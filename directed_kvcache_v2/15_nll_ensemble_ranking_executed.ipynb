{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "849114e7",
   "metadata": {},
   "source": [
    "# Exp 15: NLL Ensemble Ranking\n",
    "\n",
    "## Motivation\n",
    "\n",
    "Exp 14 found that combining bare + primed NLL marginally improves ranking\n",
    "(global ΔMRR=+0.008), but cross-validated improvement was only +0.006 (p=0.21, ns).\n",
    "The per-query oracle alpha gap (+0.050) suggests latent signal exists but a single\n",
    "primed cache can't reliably extract it.\n",
    "\n",
    "**Core hypothesis:** Diverse priming caches produce NLL estimates with partially\n",
    "independent errors. Ensembling (averaging) these estimates reduces ranking noise,\n",
    "just as averaging multiple measurements improves precision.\n",
    "\n",
    "## Design\n",
    "\n",
    "**5 Scoring Signals** (each produces a per-passage NLL):\n",
    "\n",
    "| # | Signal | Cache | Scoring Prompt | Purpose |\n",
    "|---|--------|-------|---------------|---------|\n",
    "| 1 | `bare` | Bare cache | Standard prompt | Baseline |\n",
    "| 2 | `rescore` | Bare cache | Alt prompt | **Control**: diversity without priming |\n",
    "| 3 | `sf` | Static fact prefix | Standard prompt | Replicate Exp 14 |\n",
    "| 4 | `rand` | Random text prefix | Standard prompt | Is prefix content irrelevant? |\n",
    "| 5 | `intent` | Intent prefix | Standard prompt | Different semantic angle |\n",
    "\n",
    "**Ensemble Conditions** (equal-weight NLL average, no tuning):\n",
    "\n",
    "| Ensemble | Members | Tests |\n",
    "|----------|---------|-------|\n",
    "| `ens_2_sf` | bare + sf | Replicates Exp 14 |\n",
    "| `ens_2_rand` | bare + rand | Random prefix ensemble |\n",
    "| `ens_2_rescore` | bare + rescore | **Non-priming control** |\n",
    "| `ens_3` | bare + sf + rand | 3-member ensemble |\n",
    "| `ens_4` | bare + sf + rand + intent | 4-member ensemble |\n",
    "| `ens_5_all` | all 5 signals | Maximum diversity |\n",
    "\n",
    "**Critical comparison:** `ens_2_sf` vs `ens_2_rescore`. If the control matches\n",
    "priming, then priming isn't special — any scoring diversity helps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11540b19",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T01:46:06.062095Z",
     "iopub.status.busy": "2026-02-15T01:46:06.061797Z",
     "iopub.status.idle": "2026-02-15T01:46:09.264640Z",
     "shell.execute_reply": "2026-02-15T01:46:09.263424Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEED: 42\n",
      "Results directory: results/exp15\n",
      "CUDA available: True\n",
      "GPU: NVIDIA L4\n",
      "GPU memory: 23.6 GB\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Setup\n",
    "import os\n",
    "os.umask(0o000)\n",
    "\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import gc\n",
    "from pathlib import Path\n",
    "\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "RESULTS_DIR = Path(\"results/exp15\")\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CHECKPOINT_PATH = RESULTS_DIR / \"checkpoint.json\"\n",
    "FINAL_RESULTS_PATH = RESULTS_DIR / \"results.json\"\n",
    "\n",
    "print(f\"SEED: {SEED}\")\n",
    "print(f\"Results directory: {RESULTS_DIR}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15f6f9e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T01:46:09.269001Z",
     "iopub.status.busy": "2026-02-15T01:46:09.268099Z",
     "iopub.status.idle": "2026-02-15T01:46:43.647076Z",
     "shell.execute_reply": "2026-02-15T01:46:43.646162Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading mistralai/Mistral-7B-Instruct-v0.2 (4-bit)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8badd9647e504ea4994b39d1d5a80bfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/291 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded. dtype=torch.float16, device=cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Load model (Mistral-7B 4-bit)\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, DynamicCache\n",
    "\n",
    "MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "print(f\"Loading {MODEL_NAME} (4-bit)...\")\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "print(f\"Model loaded. dtype={model.dtype}, device={model.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1b67353",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T01:46:43.652784Z",
     "iopub.status.busy": "2026-02-15T01:46:43.652014Z",
     "iopub.status.idle": "2026-02-15T01:46:44.590487Z",
     "shell.execute_reply": "2026-02-15T01:46:44.589532Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config ready\n",
      "  MAX_QUERIES: 300\n",
      "  Prefixes:\n",
      "    sf:     'What are the key facts I need to know?'\n",
      "    rand:   'The purple elephant danced gracefully on the frozen lake during twilight'\n",
      "    intent: 'What is this passage about?'\n",
      "  Alt prompt: '\n",
      "Question: ...\n",
      "Response:'\n",
      "  Signals: ['bare', 'rescore', 'sf', 'rand', 'intent']\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Config and library imports\n",
    "sys.path.insert(0, \".\")\n",
    "\n",
    "from lib.config import ExperimentConfig\n",
    "from lib.kv_cache import (\n",
    "    _get_cache_keys,\n",
    "    _get_cache_values,\n",
    "    _set_cache_keys,\n",
    "    _set_cache_values,\n",
    "    _ensure_dynamic_cache,\n",
    "    extract_and_truncate_cache_with_bos,\n",
    "    correct_rope_positions_with_bos,\n",
    "    score_answer_with_cache,\n",
    "    deepcopy_cache,\n",
    "    build_hybrid_cache,\n",
    ")\n",
    "from lib.analysis import compute_ranking_metrics\n",
    "from lib.surrogate import STATIC_SURROGATE_QUERIES\n",
    "from lib.data import count_words\n",
    "from scipy import stats\n",
    "from scipy.stats import wilcoxon\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "config = ExperimentConfig(\n",
    "    model_name=MODEL_NAME,\n",
    "    num_samples=2000,\n",
    "    seed=SEED,\n",
    ")\n",
    "\n",
    "# Templates\n",
    "SURROGATE_PREFIX_TEMPLATE = \"{surrogate}\\n\"\n",
    "DOCUMENT_TEMPLATE = \"{document}\"\n",
    "QUERY_TEMPLATE = \"\\nQuery: {query}\\nAnswer:\"\n",
    "ANSWER_TEMPLATE = \" {answer}\"\n",
    "\n",
    "# Alternative prompt for rescore control\n",
    "ALT_QUERY_TEMPLATE = \"\\nQuestion: {query}\\nResponse:\"\n",
    "\n",
    "# Prefix texts\n",
    "STATIC_FACT = STATIC_SURROGATE_QUERIES['static_factual']['query']\n",
    "RANDOM_PREFIX_TEXT = \"The purple elephant danced gracefully on the frozen lake during twilight\"\n",
    "INTENT_PREFIX_TEXT = \"What is this passage about?\"\n",
    "\n",
    "# Experiment parameters\n",
    "MAX_QUERIES = 300\n",
    "MAX_PASSAGE_WORDS = 300\n",
    "MIN_PASSAGES_PER_QUERY = 2\n",
    "CHECKPOINT_EVERY = 25\n",
    "\n",
    "SIGNAL_NAMES = ['bare', 'rescore', 'sf', 'rand', 'intent']\n",
    "\n",
    "print(\"Config ready\")\n",
    "print(f\"  MAX_QUERIES: {MAX_QUERIES}\")\n",
    "print(f\"  Prefixes:\")\n",
    "print(f\"    sf:     '{STATIC_FACT}'\")\n",
    "print(f\"    rand:   '{RANDOM_PREFIX_TEXT}'\")\n",
    "print(f\"    intent: '{INTENT_PREFIX_TEXT}'\")\n",
    "print(f\"  Alt prompt: '{ALT_QUERY_TEMPLATE.format(query='...')}'\")\n",
    "print(f\"  Signals: {SIGNAL_NAMES}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d17d2c8b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T01:46:44.594832Z",
     "iopub.status.busy": "2026-02-15T01:46:44.594183Z",
     "iopub.status.idle": "2026-02-15T01:46:45.713081Z",
     "shell.execute_reply": "2026-02-15T01:46:45.712085Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`trust_remote_code` is not supported anymore.\n",
      "Please check that the Hugging Face dataset 'microsoft/ms_marco' isn't based on a loading script and remove `trust_remote_code`.\n",
      "If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "LOADING MS MARCO v1.1 — ALL PASSAGES PER QUERY\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total items in validation: 10047\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f61b9a7e6e9348c2b55058e8b42d4915",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filtering:   0%|          | 0/10047 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Selected 300 queries (2504 total passages)\n",
      "Passages per query: mean=8.3, min=3, max=10\n",
      "Word counts: mean=71\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "192"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 4: Load MS MARCO v1.1 (same filtering as Exp 14)\n",
    "from datasets import load_dataset\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"LOADING MS MARCO v1.1 — ALL PASSAGES PER QUERY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "dataset = load_dataset(\"microsoft/ms_marco\", \"v1.1\", split=\"validation\",\n",
    "                        trust_remote_code=True)\n",
    "print(f\"Total items in validation: {len(dataset)}\")\n",
    "\n",
    "queries = []\n",
    "np.random.seed(SEED)\n",
    "\n",
    "for item in tqdm(dataset, desc=\"Filtering\"):\n",
    "    passages_info = item.get('passages', {})\n",
    "    passage_texts = passages_info.get('passage_text', [])\n",
    "    is_selected = passages_info.get('is_selected', [])\n",
    "    query = item.get('query', '')\n",
    "    answers = item.get('answers', [])\n",
    "    well_formed = item.get('wellFormedAnswers', [])\n",
    "\n",
    "    if not passage_texts or not query:\n",
    "        continue\n",
    "    if len(passage_texts) < MIN_PASSAGES_PER_QUERY:\n",
    "        continue\n",
    "    if not is_selected or sum(is_selected) == 0:\n",
    "        continue\n",
    "\n",
    "    word_counts = [count_words(p) for p in passage_texts]\n",
    "    if any(wc > MAX_PASSAGE_WORDS for wc in word_counts):\n",
    "        continue\n",
    "\n",
    "    answer = None\n",
    "    if well_formed and len(well_formed) > 0 and well_formed[0] != '[]':\n",
    "        answer = well_formed[0]\n",
    "    elif answers and len(answers) > 0 and answers[0] != 'No Answer Present.':\n",
    "        answer = answers[0]\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "    passage_list = []\n",
    "    for i, (ptext, sel) in enumerate(zip(passage_texts, is_selected)):\n",
    "        passage_list.append({\n",
    "            'passage': ptext,\n",
    "            'is_relevant': bool(sel == 1),\n",
    "            'word_count': word_counts[i],\n",
    "            'passage_idx': i,\n",
    "        })\n",
    "\n",
    "    queries.append({\n",
    "        'query': query,\n",
    "        'answer': answer,\n",
    "        'passages': passage_list,\n",
    "        'n_passages': len(passage_list),\n",
    "        'n_relevant': sum(1 for p in passage_list if p['is_relevant']),\n",
    "    })\n",
    "\n",
    "    if len(queries) >= MAX_QUERIES * 3:\n",
    "        break\n",
    "\n",
    "np.random.shuffle(queries)\n",
    "queries = queries[:MAX_QUERIES]\n",
    "N = len(queries)\n",
    "\n",
    "n_passages_list = [q['n_passages'] for q in queries]\n",
    "total_passages = sum(n_passages_list)\n",
    "\n",
    "print(f\"\\nSelected {N} queries ({total_passages} total passages)\")\n",
    "print(f\"Passages per query: mean={np.mean(n_passages_list):.1f}, \"\n",
    "      f\"min={min(n_passages_list)}, max={max(n_passages_list)}\")\n",
    "print(f\"Word counts: mean={np.mean([p['word_count'] for q in queries for p in q['passages']]):.0f}\")\n",
    "\n",
    "del dataset\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "548b0e7b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T01:46:45.716730Z",
     "iopub.status.busy": "2026-02-15T01:46:45.716398Z",
     "iopub.status.idle": "2026-02-15T01:46:45.741741Z",
     "shell.execute_reply": "2026-02-15T01:46:45.740872Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "EXPERIMENTAL CONDITIONS — NLL ENSEMBLE RANKING\n",
      "======================================================================\n",
      "\n",
      "PREFIX TOKEN LENGTHS:\n",
      "  sf        11 tokens | 'What are the key facts I need to know?'\n",
      "  rand      17 tokens | 'The purple elephant danced gracefully on the frozen lake during twilight'\n",
      "  intent     7 tokens | 'What is this passage about?'\n",
      "\n",
      "BPE BOUNDARY CHECK (first passage):\n",
      "  sf: 2/168 tokens match (1.2%)\n",
      "  rand: 2/168 tokens match (1.2%)\n",
      "  intent: 2/168 tokens match (1.2%)\n",
      "\n",
      "======================================================================\n",
      "CONDITION DETAILS\n",
      "======================================================================\n",
      "\n",
      "### bare ###\n",
      "  Standard bare cache scored with standard prompt\n",
      "  Purpose: Baseline. All other conditions are compared to this.\n",
      "\n",
      "### rescore ###\n",
      "  Bare cache scored with alt prompt ('Question:...Response:')\n",
      "  Purpose: NON-PRIMING CONTROL. Same cache, different prompt. Tests if scoring diversity alone improves ensembles, without any cache modification.\n",
      "\n",
      "### sf (static_fact) ###\n",
      "  Prefix: 'What are the key facts I need to know?'\n",
      "  Purpose: Replicates Exp 14's primed_1x. Bare keys + primed values (truncated).\n",
      "\n",
      "### rand (random) ###\n",
      "  Prefix: 'The purple elephant danced gracefully on the frozen lake during twilight'\n",
      "  Purpose: Semantically unrelated prefix. Tests if ANY prefix content works or if semantic relevance matters.\n",
      "\n",
      "### intent ###\n",
      "  Prefix: 'What is this passage about?'\n",
      "  Purpose: Different semantic angle than static_fact. Tests prefix diversity.\n",
      "\n",
      "======================================================================\n",
      "ENSEMBLE CONDITIONS (equal-weight NLL average)\n",
      "======================================================================\n",
      "  ens_2_sf:      bare + sf           (replicate Exp 14)\n",
      "  ens_2_rand:    bare + rand         (random prefix)\n",
      "  ens_2_rescore: bare + rescore      (NON-PRIMING CONTROL)\n",
      "  ens_3:         bare + sf + rand    (3-member)\n",
      "  ens_4:         bare + sf + rand + intent  (4-member)\n",
      "  ens_5_all:     all 5 signals       (maximum diversity)\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Tokenize prefixes and verify BPE boundaries\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"EXPERIMENTAL CONDITIONS — NLL ENSEMBLE RANKING\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Tokenize each prefix\n",
    "sf_str = SURROGATE_PREFIX_TEMPLATE.format(surrogate=STATIC_FACT)\n",
    "rand_str = SURROGATE_PREFIX_TEMPLATE.format(surrogate=RANDOM_PREFIX_TEXT)\n",
    "intent_str = SURROGATE_PREFIX_TEMPLATE.format(surrogate=INTENT_PREFIX_TEXT)\n",
    "\n",
    "sf_ids = tokenizer(sf_str, return_tensors=\"pt\",\n",
    "                    add_special_tokens=False)['input_ids'].to(config.device)\n",
    "rand_ids = tokenizer(rand_str, return_tensors=\"pt\",\n",
    "                      add_special_tokens=False)['input_ids'].to(config.device)\n",
    "intent_ids = tokenizer(intent_str, return_tensors=\"pt\",\n",
    "                        add_special_tokens=False)['input_ids'].to(config.device)\n",
    "\n",
    "PREFIX_CONFIGS = [\n",
    "    ('sf', STATIC_FACT, sf_str, sf_ids),\n",
    "    ('rand', RANDOM_PREFIX_TEXT, rand_str, rand_ids),\n",
    "    ('intent', INTENT_PREFIX_TEXT, intent_str, intent_ids),\n",
    "]\n",
    "\n",
    "print(\"\\nPREFIX TOKEN LENGTHS:\")\n",
    "for name, text, full_str, ids in PREFIX_CONFIGS:\n",
    "    print(f\"  {name:<8} {ids.shape[1]:>3} tokens | '{text}'\")\n",
    "\n",
    "# Verify BPE boundary consistency across prefixes\n",
    "print(\"\\nBPE BOUNDARY CHECK (first passage):\")\n",
    "example_doc = queries[0]['passages'][0]['passage']\n",
    "for name, text, full_str, ids in PREFIX_CONFIGS:\n",
    "    concat = full_str + DOCUMENT_TEMPLATE.format(document=example_doc)\n",
    "    concat_enc = tokenizer(concat, add_special_tokens=True)['input_ids']\n",
    "    prefix_enc = tokenizer(full_str, add_special_tokens=True)['input_ids']\n",
    "    doc_ids_from_concat = concat_enc[len(prefix_enc):]\n",
    "\n",
    "    bare_doc_enc = tokenizer(DOCUMENT_TEMPLATE.format(document=example_doc),\n",
    "                              add_special_tokens=False)['input_ids']\n",
    "    match = sum(1 for a, b in zip(doc_ids_from_concat, bare_doc_enc) if a == b)\n",
    "    total = max(len(bare_doc_enc), 1)\n",
    "    print(f\"  {name}: {match}/{total} tokens match ({100*match/total:.1f}%)\")\n",
    "\n",
    "# Explain conditions\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"CONDITION DETAILS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "conditions_detail = [\n",
    "    (\"bare\", \"Standard bare cache scored with standard prompt\",\n",
    "     \"Baseline. All other conditions are compared to this.\"),\n",
    "    (\"rescore\", \"Bare cache scored with alt prompt ('Question:...Response:')\",\n",
    "     \"NON-PRIMING CONTROL. Same cache, different prompt. Tests if scoring \"\n",
    "     \"diversity alone improves ensembles, without any cache modification.\"),\n",
    "    (\"sf (static_fact)\", f\"Prefix: '{STATIC_FACT}'\",\n",
    "     \"Replicates Exp 14's primed_1x. Bare keys + primed values (truncated).\"),\n",
    "    (\"rand (random)\", f\"Prefix: '{RANDOM_PREFIX_TEXT}'\",\n",
    "     \"Semantically unrelated prefix. Tests if ANY prefix content works or \"\n",
    "     \"if semantic relevance matters.\"),\n",
    "    (\"intent\", f\"Prefix: '{INTENT_PREFIX_TEXT}'\",\n",
    "     \"Different semantic angle than static_fact. Tests prefix diversity.\"),\n",
    "]\n",
    "\n",
    "for name, detail, purpose in conditions_detail:\n",
    "    print(f\"\\n### {name} ###\")\n",
    "    print(f\"  {detail}\")\n",
    "    print(f\"  Purpose: {purpose}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ENSEMBLE CONDITIONS (equal-weight NLL average)\")\n",
    "print(\"=\" * 70)\n",
    "print(\"  ens_2_sf:      bare + sf           (replicate Exp 14)\")\n",
    "print(\"  ens_2_rand:    bare + rand         (random prefix)\")\n",
    "print(\"  ens_2_rescore: bare + rescore      (NON-PRIMING CONTROL)\")\n",
    "print(\"  ens_3:         bare + sf + rand    (3-member)\")\n",
    "print(\"  ens_4:         bare + sf + rand + intent  (4-member)\")\n",
    "print(\"  ens_5_all:     all 5 signals       (maximum diversity)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "066636a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T01:46:45.745445Z",
     "iopub.status.busy": "2026-02-15T01:46:45.745122Z",
     "iopub.status.idle": "2026-02-15T03:19:14.715200Z",
     "shell.execute_reply": "2026-02-15T03:19:14.714263Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "MAIN EVALUATION (300 queries, ~2504 passages)\n",
      "======================================================================\n",
      "No checkpoint found. Starting fresh.\n",
      "Evaluating queries 0 to 299\n",
      "Per passage: 4 forward passes (bare + 3 primed) + 5 scoring passes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14a2500952b04ae78a682e2ebb95984f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Queries:   0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 25/300 | 25 done in 7.5m | ETA: 82.5 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 50/300 | 50 done in 15.2m | ETA: 75.8 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 75/300 | 75 done in 22.9m | ETA: 68.6 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 100/300 | 100 done in 30.7m | ETA: 61.5 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 125/300 | 125 done in 38.9m | ETA: 54.4 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 150/300 | 150 done in 46.4m | ETA: 46.4 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 175/300 | 175 done in 54.2m | ETA: 38.7 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 200/300 | 200 done in 61.9m | ETA: 30.9 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 225/300 | 225 done in 69.7m | ETA: 23.2 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 250/300 | 250 done in 77.5m | ETA: 15.5 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 275/300 | 275 done in 85.1m | ETA: 7.7 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 300/300 | 300 done in 92.5m | ETA: 0.0 min\n",
      "\n",
      "Evaluation complete: 300 queries in 92.5 min\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Main loop — score all passages under all conditions\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"MAIN EVALUATION ({N} queries, ~{total_passages} passages)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Checkpoint resume\n",
    "all_results = []\n",
    "start_idx = 0\n",
    "\n",
    "if CHECKPOINT_PATH.exists():\n",
    "    with open(CHECKPOINT_PATH, 'r') as f:\n",
    "        ckpt = json.load(f)\n",
    "    ckpt_queries = ckpt.get('query_texts', [])\n",
    "    current_queries = [q['query'] for q in queries]\n",
    "    if ckpt_queries == current_queries:\n",
    "        all_results = ckpt['results']\n",
    "        start_idx = len(all_results)\n",
    "        print(f\"Resuming from checkpoint: {start_idx}/{N}\")\n",
    "    else:\n",
    "        print(\"Checkpoint query mismatch. Starting fresh.\")\n",
    "else:\n",
    "    print(\"No checkpoint found. Starting fresh.\")\n",
    "\n",
    "print(f\"Evaluating queries {start_idx} to {N-1}\")\n",
    "print(f\"Per passage: 4 forward passes (bare + 3 primed) + 5 scoring passes\")\n",
    "\n",
    "t_start = time.time()\n",
    "\n",
    "for qidx in tqdm(range(start_idx, N), initial=start_idx, total=N, desc=\"Queries\"):\n",
    "    query_data = queries[qidx]\n",
    "    query = query_data['query']\n",
    "    answer = query_data['answer']\n",
    "    query_prompt = QUERY_TEMPLATE.format(query=query)\n",
    "    alt_query_prompt = ALT_QUERY_TEMPLATE.format(query=query)\n",
    "    answer_text = ANSWER_TEMPLATE.format(answer=answer)\n",
    "\n",
    "    passage_results = []\n",
    "\n",
    "    for pidx, pinfo in enumerate(query_data['passages']):\n",
    "        passage = pinfo['passage']\n",
    "        document_text = DOCUMENT_TEMPLATE.format(document=passage)\n",
    "\n",
    "        # --- Matched tokenization (using sf prefix) ---\n",
    "        full_text = sf_str + document_text\n",
    "        full_enc = tokenizer(full_text, return_tensors=\"pt\",\n",
    "                              add_special_tokens=True, padding=False, truncation=False)\n",
    "        full_ids = full_enc['input_ids'].to(config.device)\n",
    "\n",
    "        sf_prefix_enc = tokenizer(sf_str, return_tensors=\"pt\",\n",
    "                                   add_special_tokens=True, padding=False, truncation=False)\n",
    "        sf_prefix_len_matched = sf_prefix_enc['input_ids'].shape[1]\n",
    "\n",
    "        bos_id = full_ids[:, :1]\n",
    "        doc_ids = full_ids[:, sf_prefix_len_matched:]\n",
    "        doc_len = doc_ids.shape[1]\n",
    "        context_len = 1 + doc_len  # BOS + doc\n",
    "\n",
    "        del full_enc, full_ids, sf_prefix_enc\n",
    "\n",
    "        # === 1. Build bare cache ===\n",
    "        bare_input = torch.cat([bos_id, doc_ids], dim=1)\n",
    "        with torch.no_grad():\n",
    "            bare_out = model(input_ids=bare_input,\n",
    "                             attention_mask=torch.ones_like(bare_input),\n",
    "                             use_cache=True, return_dict=True)\n",
    "        bare_cache = _ensure_dynamic_cache(bare_out.past_key_values)\n",
    "        del bare_out, bare_input\n",
    "\n",
    "        # === 2. Score rescore (deepcopy bare, alt prompt) ===\n",
    "        bare_copy = deepcopy_cache(bare_cache)\n",
    "        rescore_nll = score_answer_with_cache(\n",
    "            bare_copy, context_len, alt_query_prompt, answer_text,\n",
    "            model, tokenizer, config)\n",
    "        del bare_copy\n",
    "\n",
    "        # === 3-5. For each priming prefix: build, truncate, hybrid, score ===\n",
    "        primed_nlls = {}\n",
    "        for p_name, p_text, p_str, p_ids in PREFIX_CONFIGS:\n",
    "            primed_input = torch.cat([bos_id, p_ids, doc_ids], dim=1)\n",
    "            with torch.no_grad():\n",
    "                primed_out = model(input_ids=primed_input,\n",
    "                                   attention_mask=torch.ones_like(primed_input),\n",
    "                                   use_cache=True, return_dict=True)\n",
    "            primed_full = _ensure_dynamic_cache(primed_out.past_key_values)\n",
    "            del primed_out, primed_input\n",
    "\n",
    "            # Truncate + RoPE correct\n",
    "            primed_trunc = extract_and_truncate_cache_with_bos(primed_full, doc_len)\n",
    "            correct_rope_positions_with_bos(primed_trunc, p_ids.shape[1], model)\n",
    "            del primed_full\n",
    "\n",
    "            # Hybrid: bare keys + primed values (pure value contamination)\n",
    "            hybrid = build_hybrid_cache(bare_cache, primed_trunc)\n",
    "            del primed_trunc\n",
    "\n",
    "            primed_nlls[p_name] = score_answer_with_cache(\n",
    "                hybrid, context_len, query_prompt, answer_text,\n",
    "                model, tokenizer, config)\n",
    "            del hybrid\n",
    "\n",
    "        # === 6. Score bare LAST (mutates cache) ===\n",
    "        bare_nll = score_answer_with_cache(\n",
    "            bare_cache, context_len, query_prompt, answer_text,\n",
    "            model, tokenizer, config)\n",
    "        del bare_cache\n",
    "\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        passage_results.append({\n",
    "            'passage_idx': pinfo['passage_idx'],\n",
    "            'is_relevant': pinfo['is_relevant'],\n",
    "            'word_count': pinfo['word_count'],\n",
    "            'bare_nll': bare_nll,\n",
    "            'rescore_nll': rescore_nll,\n",
    "            'sf_nll': primed_nlls['sf'],\n",
    "            'rand_nll': primed_nlls['rand'],\n",
    "            'intent_nll': primed_nlls['intent'],\n",
    "        })\n",
    "\n",
    "    all_results.append({\n",
    "        'query_idx': qidx,\n",
    "        'query': query,\n",
    "        'n_passages': len(passage_results),\n",
    "        'n_relevant': query_data['n_relevant'],\n",
    "        'passage_data': passage_results,\n",
    "    })\n",
    "\n",
    "    # Checkpoint\n",
    "    if (qidx + 1) % CHECKPOINT_EVERY == 0 or qidx == N - 1:\n",
    "        ckpt_data = {\n",
    "            'results': all_results,\n",
    "            'query_texts': [q['query'] for q in queries],\n",
    "            'completed': len(all_results),\n",
    "            'total': N,\n",
    "            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        }\n",
    "        with open(CHECKPOINT_PATH, 'w') as f:\n",
    "            json.dump(ckpt_data, f)\n",
    "        elapsed = time.time() - t_start\n",
    "        n_done = qidx - start_idx + 1\n",
    "        rate = n_done / elapsed if elapsed > 0 else 0\n",
    "        remaining = (N - qidx - 1) / rate if rate > 0 else 0\n",
    "        tqdm.write(f\"  Checkpoint {qidx+1}/{N} | {n_done} done in {elapsed/60:.1f}m | \"\n",
    "                   f\"ETA: {remaining/60:.1f} min\")\n",
    "\n",
    "elapsed_total = time.time() - t_start\n",
    "print(f\"\\nEvaluation complete: {len(all_results)} queries in {elapsed_total/60:.1f} min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8d48562",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T03:19:14.720494Z",
     "iopub.status.busy": "2026-02-15T03:19:14.720108Z",
     "iopub.status.idle": "2026-02-15T03:19:15.647429Z",
     "shell.execute_reply": "2026-02-15T03:19:15.646506Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ANALYSIS\n",
      "======================================================================\n",
      "Valid queries: 300\n",
      "\n",
      "======================================================================\n",
      "INDIVIDUAL SIGNAL RANKING\n",
      "======================================================================\n",
      "\n",
      "Signal            MRR     ΔMRR            p   Sig  Changed\n",
      "------------------------------------------------------------\n",
      "bare           0.8011       --           --    --       --\n",
      "rescore        0.7900  -0.0111    1.916e-01    ns       40\n",
      "sf             0.8058  +0.0047    5.795e-01    ns       39\n",
      "rand           0.8034  +0.0023    7.056e-01    ns       42\n",
      "intent         0.7934  -0.0077    2.875e-01    ns       43\n",
      "\n",
      "======================================================================\n",
      "ENSEMBLE RANKING (EQUAL-WEIGHT NLL AVERAGE)\n",
      "======================================================================\n",
      "\n",
      "Ensemble             Members      MRR     ΔMRR            p   Sig  Changed\n",
      "------------------------------------------------------------------------\n",
      "ens_2_sf                2   0.8060  +0.0049    3.629e-01    ns       21\n",
      "ens_2_rand              2   0.8077  +0.0066    1.599e-01    ns       20\n",
      "ens_2_intent            2   0.8013  +0.0002    9.424e-01    ns       27\n",
      "ens_2_rescore           2   0.7911  -0.0101    1.057e-01    ns       25\n",
      "ens_3                   3   0.8079  +0.0068    2.323e-01    ns       24\n",
      "ens_4                   4   0.8029  +0.0018    7.548e-01    ns       25\n",
      "ens_5_all               5   0.8056  +0.0044    4.975e-01    ns       26\n",
      "\n",
      "======================================================================\n",
      "CRITICAL: PRIMING vs NON-PRIMING CONTROL\n",
      "======================================================================\n",
      "  ens_2_sf (priming):      MRR=0.8060\n",
      "  ens_2_rescore (control): MRR=0.7911\n",
      "  Difference:              +0.0149  (p=3.890e-02, *)\n",
      "  => Priming adds value BEYOND prompt diversity\n",
      "\n",
      "======================================================================\n",
      "GREEDY SCALING CURVE: best member to add at each step\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "K         Added Ensemble                                 MRR     ΔMRR\n",
      "----------------------------------------------------------------------\n",
      "1            -- bare                                  0.8011  +0.0000\n",
      "2          rand bare+rand                             0.8077  +0.0066\n",
      "3            sf bare+rand+sf                          0.8079  +0.0068\n",
      "4       rescore bare+rand+sf+rescore                  0.8045  +0.0034\n",
      "5        intent bare+rand+sf+rescore+intent           0.8056  +0.0044\n",
      "\n",
      "======================================================================\n",
      "NLL CORRELATION MATRIX (Pearson, across all passages)\n",
      "======================================================================\n",
      "\n",
      "                   bare    rescore         sf       rand     intent\n",
      "        bare     1.0000     0.9821     0.9898     0.9891     0.9828\n",
      "     rescore     0.9821     1.0000     0.9724     0.9763     0.9785\n",
      "          sf     0.9898     0.9724     1.0000     0.9904     0.9872\n",
      "        rand     0.9891     0.9763     0.9904     1.0000     0.9876\n",
      "      intent     0.9828     0.9785     0.9872     0.9876     1.0000\n",
      "\n",
      "Note: Lower correlation = more diversity = better ensembles\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Analysis — individual signals, ensembles, significance, scaling\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "N_VALID = len(all_results)\n",
    "print(f\"Valid queries: {N_VALID}\")\n",
    "\n",
    "# --- Helper functions ---\n",
    "def mrr_for_signal(results, sig_name):\n",
    "    \"\"\"Compute per-query MRR ranking by a single NLL signal.\"\"\"\n",
    "    mrrs = []\n",
    "    for r in results:\n",
    "        pd = r['passage_data']\n",
    "        scores = {i: pd[i][f'{sig_name}_nll'] for i in range(len(pd))}\n",
    "        rel_idx = next(i for i, p in enumerate(pd) if p['is_relevant'])\n",
    "        m = compute_ranking_metrics(scores, relevant_idx=rel_idx)\n",
    "        mrrs.append(m['mrr'])\n",
    "    return np.array(mrrs)\n",
    "\n",
    "\n",
    "def mrr_for_ensemble(results, sig_names):\n",
    "    \"\"\"Compute per-query MRR ranking by equal-weight NLL average.\"\"\"\n",
    "    mrrs = []\n",
    "    for r in results:\n",
    "        pd = r['passage_data']\n",
    "        scores = {}\n",
    "        for i in range(len(pd)):\n",
    "            scores[i] = np.mean([pd[i][f'{s}_nll'] for s in sig_names])\n",
    "        rel_idx = next(i for i, p in enumerate(pd) if p['is_relevant'])\n",
    "        m = compute_ranking_metrics(scores, relevant_idx=rel_idx)\n",
    "        mrrs.append(m['mrr'])\n",
    "    return np.array(mrrs)\n",
    "\n",
    "\n",
    "def sig_test(mrrs_a, mrrs_b):\n",
    "    \"\"\"Wilcoxon signed-rank test, returns (delta, p, sig_str).\"\"\"\n",
    "    delta = float(np.mean(mrrs_a) - np.mean(mrrs_b))\n",
    "    nonzero = int(np.sum(mrrs_a != mrrs_b))\n",
    "    if nonzero > 10:\n",
    "        _, p = wilcoxon(mrrs_a, mrrs_b)\n",
    "    else:\n",
    "        p = 1.0\n",
    "    sig = \"***\" if p < 0.001 else \"**\" if p < 0.01 else \"*\" if p < 0.05 else \"ns\"\n",
    "    return delta, float(p), sig, nonzero\n",
    "\n",
    "\n",
    "# === 1. Individual signal MRR ===\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"INDIVIDUAL SIGNAL RANKING\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "individual_mrrs = {}\n",
    "for sig in SIGNAL_NAMES:\n",
    "    individual_mrrs[sig] = mrr_for_signal(all_results, sig)\n",
    "\n",
    "bare_mrrs = individual_mrrs['bare']\n",
    "print(f\"\\n{'Signal':<12} {'MRR':>8} {'ΔMRR':>8} {'p':>12} {'Sig':>5} {'Changed':>8}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "individual_stats = {}\n",
    "for sig in SIGNAL_NAMES:\n",
    "    mrrs = individual_mrrs[sig]\n",
    "    if sig == 'bare':\n",
    "        print(f\"{sig:<12} {np.mean(mrrs):>8.4f} {'--':>8} {'--':>12} {'--':>5} {'--':>8}\")\n",
    "    else:\n",
    "        d, p, s_str, n_changed = sig_test(mrrs, bare_mrrs)\n",
    "        print(f\"{sig:<12} {np.mean(mrrs):>8.4f} {d:>+8.4f} {p:>12.3e} {s_str:>5} {n_changed:>8}\")\n",
    "        individual_stats[sig] = {'delta_mrr': d, 'p_value': p, 'significant': bool(p < 0.05)}\n",
    "\n",
    "\n",
    "# === 2. Ensemble MRR ===\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ENSEMBLE RANKING (EQUAL-WEIGHT NLL AVERAGE)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "ENSEMBLE_CONFIGS = {\n",
    "    'ens_2_sf':      ['bare', 'sf'],\n",
    "    'ens_2_rand':    ['bare', 'rand'],\n",
    "    'ens_2_intent':  ['bare', 'intent'],\n",
    "    'ens_2_rescore': ['bare', 'rescore'],\n",
    "    'ens_3':         ['bare', 'sf', 'rand'],\n",
    "    'ens_4':         ['bare', 'sf', 'rand', 'intent'],\n",
    "    'ens_5_all':     ['bare', 'sf', 'rand', 'intent', 'rescore'],\n",
    "}\n",
    "\n",
    "ensemble_mrrs = {}\n",
    "ensemble_stats = {}\n",
    "print(f\"\\n{'Ensemble':<20} {'Members':>4} {'MRR':>8} {'ΔMRR':>8} {'p':>12} {'Sig':>5} {'Changed':>8}\")\n",
    "print(\"-\" * 72)\n",
    "\n",
    "for ens_name, members in ENSEMBLE_CONFIGS.items():\n",
    "    mrrs = mrr_for_ensemble(all_results, members)\n",
    "    ensemble_mrrs[ens_name] = mrrs\n",
    "    d, p, s_str, n_changed = sig_test(mrrs, bare_mrrs)\n",
    "    print(f\"{ens_name:<20} {len(members):>4} {np.mean(mrrs):>8.4f} {d:>+8.4f} \"\n",
    "          f\"{p:>12.3e} {s_str:>5} {n_changed:>8}\")\n",
    "    ensemble_stats[ens_name] = {\n",
    "        'members': members,\n",
    "        'mrr_mean': float(np.mean(mrrs)),\n",
    "        'delta_mrr': d,\n",
    "        'p_value': p,\n",
    "        'significant': bool(p < 0.05),\n",
    "        'n_changed': n_changed,\n",
    "    }\n",
    "\n",
    "\n",
    "# === 3. Critical comparison: priming vs non-priming control ===\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"CRITICAL: PRIMING vs NON-PRIMING CONTROL\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "sf_mrr = float(np.mean(ensemble_mrrs['ens_2_sf']))\n",
    "rescore_mrr = float(np.mean(ensemble_mrrs['ens_2_rescore']))\n",
    "d_sf_res, p_sf_res, s_sf_res, n_sf_res = sig_test(\n",
    "    ensemble_mrrs['ens_2_sf'], ensemble_mrrs['ens_2_rescore'])\n",
    "\n",
    "print(f\"  ens_2_sf (priming):      MRR={sf_mrr:.4f}\")\n",
    "print(f\"  ens_2_rescore (control): MRR={rescore_mrr:.4f}\")\n",
    "print(f\"  Difference:              {d_sf_res:+.4f}  (p={p_sf_res:.3e}, {s_sf_res})\")\n",
    "if sf_mrr > rescore_mrr + 0.001:\n",
    "    print(\"  => Priming adds value BEYOND prompt diversity\")\n",
    "elif rescore_mrr > sf_mrr + 0.001:\n",
    "    print(\"  => Prompt diversity alone BEATS priming\")\n",
    "else:\n",
    "    print(\"  => Priming and prompt diversity are equivalent\")\n",
    "\n",
    "\n",
    "# === 4. Greedy forward selection (scaling curve) ===\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"GREEDY SCALING CURVE: best member to add at each step\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "available = ['rescore', 'sf', 'rand', 'intent']\n",
    "selected = ['bare']\n",
    "greedy_results = [{'members': list(selected), 'mrr': float(np.mean(bare_mrrs))}]\n",
    "\n",
    "for step in range(len(available)):\n",
    "    best_next = None\n",
    "    best_mrr = -1\n",
    "    for candidate in available:\n",
    "        trial = selected + [candidate]\n",
    "        trial_mrrs = mrr_for_ensemble(all_results, trial)\n",
    "        mean_mrr = float(np.mean(trial_mrrs))\n",
    "        if mean_mrr > best_mrr:\n",
    "            best_mrr = mean_mrr\n",
    "            best_next = candidate\n",
    "    selected.append(best_next)\n",
    "    available.remove(best_next)\n",
    "    greedy_results.append({'members': list(selected), 'mrr': best_mrr})\n",
    "\n",
    "print(f\"\\n{'K':<4} {'Added':>10} {'Ensemble':<35} {'MRR':>8} {'ΔMRR':>8}\")\n",
    "print(\"-\" * 70)\n",
    "for i, gr in enumerate(greedy_results):\n",
    "    added = gr['members'][-1] if i > 0 else '--'\n",
    "    delta = gr['mrr'] - greedy_results[0]['mrr']\n",
    "    members_str = '+'.join(gr['members'])\n",
    "    print(f\"{i+1:<4} {added:>10} {members_str:<35} {gr['mrr']:>8.4f} {delta:>+8.4f}\")\n",
    "\n",
    "\n",
    "# === 5. NLL correlation matrix ===\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"NLL CORRELATION MATRIX (Pearson, across all passages)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "all_nlls = {sig: [] for sig in SIGNAL_NAMES}\n",
    "for r in all_results:\n",
    "    for p in r['passage_data']:\n",
    "        for sig in SIGNAL_NAMES:\n",
    "            all_nlls[sig].append(p[f'{sig}_nll'])\n",
    "\n",
    "all_nlls = {sig: np.array(vals) for sig, vals in all_nlls.items()}\n",
    "\n",
    "print(f\"\\n{'':>12}\", end='')\n",
    "for sig in SIGNAL_NAMES:\n",
    "    print(f\" {sig:>10}\", end='')\n",
    "print()\n",
    "\n",
    "corr_matrix = {}\n",
    "for sig_a in SIGNAL_NAMES:\n",
    "    print(f\"{sig_a:>12}\", end='')\n",
    "    for sig_b in SIGNAL_NAMES:\n",
    "        r, _ = stats.pearsonr(all_nlls[sig_a], all_nlls[sig_b])\n",
    "        corr_matrix[f'{sig_a}_{sig_b}'] = float(r)\n",
    "        print(f\" {r:>10.4f}\", end='')\n",
    "    print()\n",
    "\n",
    "print(\"\\nNote: Lower correlation = more diversity = better ensembles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9229a986",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T03:19:15.651726Z",
     "iopub.status.busy": "2026-02-15T03:19:15.650902Z",
     "iopub.status.idle": "2026-02-15T03:19:17.630639Z",
     "shell.execute_reply": "2026-02-15T03:19:17.629756Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plots saved to results/exp15/analysis_plots.png\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Plots (4-panel figure)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "colors_ens = {\n",
    "    'ens_2_sf': '#d62728',\n",
    "    'ens_2_rand': '#ff7f0e',\n",
    "    'ens_2_intent': '#9467bd',\n",
    "    'ens_2_rescore': '#2ca02c',\n",
    "    'ens_3': '#1f77b4',\n",
    "    'ens_4': '#e377c2',\n",
    "    'ens_5_all': '#17becf',\n",
    "}\n",
    "\n",
    "# --- Plot 1: Ensemble MRR bar chart ---\n",
    "ax = axes[0, 0]\n",
    "names = ['bare'] + list(ENSEMBLE_CONFIGS.keys())\n",
    "mrr_vals = [float(np.mean(bare_mrrs))] + [float(np.mean(ensemble_mrrs[e])) for e in ENSEMBLE_CONFIGS]\n",
    "bar_colors = ['#7f7f7f'] + [colors_ens.get(e, '#333') for e in ENSEMBLE_CONFIGS]\n",
    "bars = ax.bar(range(len(names)), mrr_vals, color=bar_colors, edgecolor='black', linewidth=0.5)\n",
    "for i, (n, m) in enumerate(zip(names, mrr_vals)):\n",
    "    ax.text(i, m + 0.002, f\"{m:.4f}\", ha='center', fontsize=7, rotation=45)\n",
    "ax.set_xticks(range(len(names)))\n",
    "ax.set_xticklabels(names, rotation=45, ha='right', fontsize=7)\n",
    "ax.set_ylabel(\"MRR\")\n",
    "ax.set_title(\"MRR by Condition\")\n",
    "ax.axhline(y=float(np.mean(bare_mrrs)), color='gray', linestyle='--', alpha=0.5, label='bare')\n",
    "ax.legend(fontsize=8)\n",
    "\n",
    "# --- Plot 2: Scaling curve ---\n",
    "ax = axes[0, 1]\n",
    "k_vals = list(range(1, len(greedy_results) + 1))\n",
    "mrr_curve = [gr['mrr'] for gr in greedy_results]\n",
    "ax.plot(k_vals, mrr_curve, 'o-', color='#1f77b4', linewidth=2, markersize=8)\n",
    "for i, gr in enumerate(greedy_results):\n",
    "    label = gr['members'][-1] if i > 0 else 'bare'\n",
    "    ax.annotate(f\"+{label}\" if i > 0 else label,\n",
    "                (k_vals[i], mrr_curve[i]),\n",
    "                textcoords=\"offset points\", xytext=(5, 8), fontsize=8)\n",
    "ax.axhline(y=float(np.mean(bare_mrrs)), color='gray', linestyle='--', alpha=0.5)\n",
    "ax.set_xlabel(\"Ensemble Size K\")\n",
    "ax.set_ylabel(\"MRR\")\n",
    "ax.set_title(\"Greedy Scaling Curve\")\n",
    "ax.set_xticks(k_vals)\n",
    "\n",
    "# --- Plot 3: Correlation heatmap ---\n",
    "ax = axes[1, 0]\n",
    "corr_data = np.zeros((len(SIGNAL_NAMES), len(SIGNAL_NAMES)))\n",
    "for i, sa in enumerate(SIGNAL_NAMES):\n",
    "    for j, sb in enumerate(SIGNAL_NAMES):\n",
    "        corr_data[i, j] = corr_matrix[f'{sa}_{sb}']\n",
    "im = ax.imshow(corr_data, vmin=0.9, vmax=1.0, cmap='YlOrRd', aspect='auto')\n",
    "ax.set_xticks(range(len(SIGNAL_NAMES)))\n",
    "ax.set_xticklabels(SIGNAL_NAMES, fontsize=8)\n",
    "ax.set_yticks(range(len(SIGNAL_NAMES)))\n",
    "ax.set_yticklabels(SIGNAL_NAMES, fontsize=8)\n",
    "for i in range(len(SIGNAL_NAMES)):\n",
    "    for j in range(len(SIGNAL_NAMES)):\n",
    "        ax.text(j, i, f\"{corr_data[i,j]:.3f}\", ha='center', va='center', fontsize=7)\n",
    "plt.colorbar(im, ax=ax, label=\"Pearson r\")\n",
    "ax.set_title(\"NLL Correlation Matrix\")\n",
    "\n",
    "# --- Plot 4: Per-query ΔMRR distributions ---\n",
    "ax = axes[1, 1]\n",
    "for ens_name in ['ens_2_sf', 'ens_2_rescore', 'ens_4']:\n",
    "    deltas = ensemble_mrrs[ens_name] - bare_mrrs\n",
    "    ax.hist(deltas, bins=30, alpha=0.5, label=ens_name,\n",
    "            color=colors_ens.get(ens_name, 'gray'))\n",
    "ax.axvline(x=0, color='gray', linestyle='--', alpha=0.5)\n",
    "ax.set_xlabel(\"ΔMRR (ensemble - bare)\")\n",
    "ax.set_ylabel(\"Count\")\n",
    "ax.set_title(\"Per-Query MRR Change Distribution\")\n",
    "ax.legend(fontsize=8)\n",
    "\n",
    "plt.suptitle('Exp 15: NLL Ensemble Ranking', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'analysis_plots.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"Plots saved to {RESULTS_DIR / 'analysis_plots.png'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e097f19",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T03:19:17.634445Z",
     "iopub.status.busy": "2026-02-15T03:19:17.633757Z",
     "iopub.status.idle": "2026-02-15T03:19:17.647918Z",
     "shell.execute_reply": "2026-02-15T03:19:17.647097Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to results/exp15/results.json\n",
      "File size: 44.9 KB\n",
      "\n",
      "======================================================================\n",
      "SUMMARY\n",
      "======================================================================\n",
      "Bare MRR:           0.8011\n",
      "Best ensemble:      ens_3 (MRR=0.8079, ΔMRR=+0.0068, p=2.323e-01)\n",
      "Priming vs control: +0.0149 (p=3.890e-02)\n",
      "Scaling saturates:  K=5 members, MRR=0.8056\n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Save results JSON\n",
    "final = {\n",
    "    'experiment': 'exp15_nll_ensemble_ranking',\n",
    "    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'config': {\n",
    "        'model_name': config.model_name,\n",
    "        'seed': SEED,\n",
    "        'n_queries': N,\n",
    "        'n_valid': N_VALID,\n",
    "        'max_passage_words': MAX_PASSAGE_WORDS,\n",
    "        'min_passages_per_query': MIN_PASSAGES_PER_QUERY,\n",
    "        'dataset': 'MS MARCO v1.1 validation',\n",
    "        'prefixes': {\n",
    "            'sf': STATIC_FACT,\n",
    "            'rand': RANDOM_PREFIX_TEXT,\n",
    "            'intent': INTENT_PREFIX_TEXT,\n",
    "        },\n",
    "        'alt_query_template': ALT_QUERY_TEMPLATE,\n",
    "    },\n",
    "    'signal_names': SIGNAL_NAMES,\n",
    "    'individual_mrrs': {sig: float(np.mean(individual_mrrs[sig])) for sig in SIGNAL_NAMES},\n",
    "    'individual_stats': individual_stats,\n",
    "    'ensemble_configs': {k: v for k, v in ENSEMBLE_CONFIGS.items()},\n",
    "    'ensemble_stats': ensemble_stats,\n",
    "    'priming_vs_control': {\n",
    "        'ens_2_sf_mrr': sf_mrr,\n",
    "        'ens_2_rescore_mrr': rescore_mrr,\n",
    "        'difference': float(d_sf_res),\n",
    "        'p_value': float(p_sf_res),\n",
    "        'priming_is_special': bool(sf_mrr > rescore_mrr + 0.001),\n",
    "    },\n",
    "    'greedy_scaling': greedy_results,\n",
    "    'correlation_matrix': corr_matrix,\n",
    "    'per_query_results': [\n",
    "        {k: v for k, v in r.items() if k != 'passage_data'}\n",
    "        for r in all_results\n",
    "    ],\n",
    "}\n",
    "\n",
    "with open(FINAL_RESULTS_PATH, 'w') as f:\n",
    "    json.dump(final, f, indent=2)\n",
    "\n",
    "print(f\"Results saved to {FINAL_RESULTS_PATH}\")\n",
    "print(f\"File size: {FINAL_RESULTS_PATH.stat().st_size / 1024:.1f} KB\")\n",
    "\n",
    "# Print summary\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Bare MRR:           {float(np.mean(bare_mrrs)):.4f}\")\n",
    "best_ens = max(ensemble_stats.items(), key=lambda x: x[1]['mrr_mean'])\n",
    "print(f\"Best ensemble:      {best_ens[0]} (MRR={best_ens[1]['mrr_mean']:.4f}, \"\n",
    "      f\"ΔMRR={best_ens[1]['delta_mrr']:+.4f}, p={best_ens[1]['p_value']:.3e})\")\n",
    "print(f\"Priming vs control: {d_sf_res:+.4f} (p={p_sf_res:.3e})\")\n",
    "print(f\"Scaling saturates:  K={len(greedy_results[-1]['members'])} members, \"\n",
    "      f\"MRR={greedy_results[-1]['mrr']:.4f}\")\n",
    "print(\"\\nDone!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0c1a5dfe518d410ab3f7cb2995d3c9b4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "0e68e6366011441cb43df77ea830b90b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_50c81898f8d5474685b14984ca7f4990",
       "placeholder": "​",
       "style": "IPY_MODEL_105f176d1d73430e8de16152a33dbfc0",
       "tabbable": null,
       "tooltip": null,
       "value": " 925/10047 [00:00&lt;00:02, 4248.46it/s]"
      }
     },
     "0e928cbb623c461183cea21d81ffc010": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "105f176d1d73430e8de16152a33dbfc0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "14a2500952b04ae78a682e2ebb95984f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_39134e98efda4b9888cdf580e6513e8c",
        "IPY_MODEL_4e23d5acdc9b498ca2f99d5878fa9817",
        "IPY_MODEL_d10ec3d3510a4a06a98125a00fb49841"
       ],
       "layout": "IPY_MODEL_8a37d83478e841d6bf4e8a5a6bac41d1",
       "tabbable": null,
       "tooltip": null
      }
     },
     "1ddaf647d15746949bb4f8f1f67f08a3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_0c1a5dfe518d410ab3f7cb2995d3c9b4",
       "placeholder": "​",
       "style": "IPY_MODEL_a14a661d236443d6b2d8bc45e2be835f",
       "tabbable": null,
       "tooltip": null,
       "value": "Filtering:   9%"
      }
     },
     "39134e98efda4b9888cdf580e6513e8c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_a7ac6f908c7b412c9b4cf6451670bff3",
       "placeholder": "​",
       "style": "IPY_MODEL_bca99880dbea4e4fa71324571aedbd9e",
       "tabbable": null,
       "tooltip": null,
       "value": "Queries: 100%"
      }
     },
     "3b4251c409b744e68e9925b7e7623242": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "41e477f56da3496facf11e44f5708518": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "4e23d5acdc9b498ca2f99d5878fa9817": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_7d85cbbd8582452c8afef8e76cc91c8e",
       "max": 300.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_909c42ed0882405ca637aa9fa538ba77",
       "tabbable": null,
       "tooltip": null,
       "value": 300.0
      }
     },
     "50c81898f8d5474685b14984ca7f4990": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "57e257b6935d4eb6b6a11e0186091d60": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "7d85cbbd8582452c8afef8e76cc91c8e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "7e148f0a2c1b4d20a219e6aa4be427d1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "85c0af5990df411a969f5c06e520985f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "danger",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_0e928cbb623c461183cea21d81ffc010",
       "max": 10047.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_57e257b6935d4eb6b6a11e0186091d60",
       "tabbable": null,
       "tooltip": null,
       "value": 925.0
      }
     },
     "8a37d83478e841d6bf4e8a5a6bac41d1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "8badd9647e504ea4994b39d1d5a80bfb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_8bdaea56887840769b154c19d5e5bedd",
        "IPY_MODEL_a344139b31824c33b9906ad884d14899",
        "IPY_MODEL_dd84bbb27af14c73bed6daeeec681cc9"
       ],
       "layout": "IPY_MODEL_41e477f56da3496facf11e44f5708518",
       "tabbable": null,
       "tooltip": null
      }
     },
     "8bdaea56887840769b154c19d5e5bedd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_94f141460d1244eca40ab207a98f64a5",
       "placeholder": "​",
       "style": "IPY_MODEL_fe669de53d5d4dfe82ed5efd50299167",
       "tabbable": null,
       "tooltip": null,
       "value": "Loading weights: 100%"
      }
     },
     "909c42ed0882405ca637aa9fa538ba77": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "94f141460d1244eca40ab207a98f64a5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a14a661d236443d6b2d8bc45e2be835f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "a344139b31824c33b9906ad884d14899": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_c2b90e6e396e4584bf2bb95e814ecc85",
       "max": 291.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_aa52171edfa94e4f908483ef491b6f39",
       "tabbable": null,
       "tooltip": null,
       "value": 291.0
      }
     },
     "a7ac6f908c7b412c9b4cf6451670bff3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "aa52171edfa94e4f908483ef491b6f39": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "b04b5a44cbdd46f0801e7a95b27611ea": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "bca99880dbea4e4fa71324571aedbd9e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "c2b90e6e396e4584bf2bb95e814ecc85": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d10ec3d3510a4a06a98125a00fb49841": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_3b4251c409b744e68e9925b7e7623242",
       "placeholder": "​",
       "style": "IPY_MODEL_7e148f0a2c1b4d20a219e6aa4be427d1",
       "tabbable": null,
       "tooltip": null,
       "value": " 300/300 [1:32:28&lt;00:00, 15.86s/it]"
      }
     },
     "dd84bbb27af14c73bed6daeeec681cc9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_efc711304a844149be8a6399cb2adeee",
       "placeholder": "​",
       "style": "IPY_MODEL_f95b97749c5641529f7144da2d518bdc",
       "tabbable": null,
       "tooltip": null,
       "value": " 291/291 [00:26&lt;00:00,  6.01it/s, Materializing param=model.norm.weight]"
      }
     },
     "efc711304a844149be8a6399cb2adeee": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f61b9a7e6e9348c2b55058e8b42d4915": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_1ddaf647d15746949bb4f8f1f67f08a3",
        "IPY_MODEL_85c0af5990df411a969f5c06e520985f",
        "IPY_MODEL_0e68e6366011441cb43df77ea830b90b"
       ],
       "layout": "IPY_MODEL_b04b5a44cbdd46f0801e7a95b27611ea",
       "tabbable": null,
       "tooltip": null
      }
     },
     "f95b97749c5641529f7144da2d518bdc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "fe669de53d5d4dfe82ed5efd50299167": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
