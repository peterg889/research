{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exp 10: Semantic Content Gradient \u2014 Does Content Matter?\n",
    "\n",
    "## Motivation\n",
    "\n",
    "Previous experiments show conflicting signals about whether semantic content matters:\n",
    "- **FOR**: static_fact_trunc (d=+0.472) >> random_trunc (d=+0.125) in Exp 07 \u2014 3.8\u00d7 larger effect\n",
    "- **FOR**: LLM-kw (d=+0.234) >> random (d=+0.125) in Exp 06 \u2014 coherence matters\n",
    "- **FOR**: static_fact values-only (d=+0.466) >> random values-only (d=+0.310) in Exp 09\n",
    "- **AGAINST**: oracle (d=+0.023, ns) \u2248 random (d=+0.091) in Exp 01\n",
    "- **AGAINST**: separator-only (d=+0.231) \u2248 LLM-kw-trunc (d=+0.234) in Exp 06\n",
    "\n",
    "This experiment resolves the question decisively by testing a **gradient of semantic relevance**\n",
    "in **both truncated-prefix and suffix modes**. If semantic content helps in both modes, the\n",
    "effect cannot be explained by structural artifacts alone.\n",
    "\n",
    "## Key Design Principle\n",
    "\n",
    "Each semantic level is tested in BOTH delivery modes (truncated prefix, suffix). This is\n",
    "critical because:\n",
    "- **Truncated prefix**: Affects document value vectors (value contamination)\n",
    "- **Suffix**: Document KV entries are unchanged; effect must come from query \u2192 suffix attention\n",
    "\n",
    "If the same semantic gradient appears in BOTH modes, it proves the benefit is genuinely semantic.\n",
    "\n",
    "## 16 Conditions\n",
    "\n",
    "| # | Condition | Mode | Content | Semantic Level |\n",
    "|---|-----------|------|---------|----------------|\n",
    "| 1 | bare | \u2014 | No prefix | Baseline |\n",
    "| 2 | random_trunc | Trunc | Random tokens | 0 |\n",
    "| 3 | random_words_trunc | Trunc | Random English words | 0.5 |\n",
    "| 4 | wrong_doc_llm_trunc | Trunc | LLM-kw from wrong doc | 1 |\n",
    "| 5 | tfidf_kw_trunc | Trunc | TF-IDF keywords (right doc) | 2 |\n",
    "| 6 | llm_kw_trunc | Trunc | LLM-kw (right doc) | 3 |\n",
    "| 7 | static_fact_trunc | Trunc | \"What are the key facts?\" | 4 |\n",
    "| 8 | oracle_kw_trunc | Trunc | Oracle as keywords | 5 |\n",
    "| 9 | oracle_raw_trunc | Trunc | Oracle (raw question format) | 5* |\n",
    "| 10 | random_suffix | Suffix | Random tokens | 0 |\n",
    "| 11 | random_words_suffix | Suffix | Random English words | 0.5 |\n",
    "| 12 | wrong_doc_llm_suffix | Suffix | LLM-kw from wrong doc | 1 |\n",
    "| 13 | tfidf_kw_suffix | Suffix | TF-IDF keywords (right doc) | 2 |\n",
    "| 14 | llm_kw_suffix | Suffix | LLM-kw (right doc) | 3 |\n",
    "| 15 | static_fact_suffix | Suffix | \"What are the key facts?\" | 4 |\n",
    "| 16 | oracle_kw_suffix | Suffix | Oracle as keywords | 5 |\n",
    "\n",
    "*oracle_raw_trunc uses question format (potential interference), included for comparison.\n",
    "\n",
    "## 10 Primary Comparisons (Bonferroni alpha = 0.005)\n",
    "\n",
    "| # | Comparison | Question |\n",
    "|---|-----------|----------|\n",
    "| C1 | llm_kw_trunc vs random_trunc | LLM > random in truncated? |\n",
    "| C2 | static_fact_trunc vs random_trunc | Static > random in truncated? |\n",
    "| C3 | llm_kw_trunc vs wrong_doc_llm_trunc | Right doc > wrong doc (trunc)? |\n",
    "| C4 | tfidf_kw_trunc vs random_trunc | TF-IDF > random in truncated? |\n",
    "| C5 | oracle_kw_trunc vs oracle_raw_trunc | Keyword > question format? |\n",
    "| C6 | llm_kw_suffix vs random_suffix | LLM > random in suffix? |\n",
    "| C7 | static_fact_suffix vs random_suffix | Static > random in suffix? |\n",
    "| C8 | llm_kw_suffix vs wrong_doc_llm_suffix | Right doc > wrong doc (suffix)? |\n",
    "| C9 | llm_kw_trunc vs llm_kw_suffix | Truncated > suffix (LLM content)? |\n",
    "| C10 | static_fact_trunc vs static_fact_suffix | Truncated > suffix (static)? |\n",
    "\n",
    "## Decisive Predictions\n",
    "\n",
    "If semantic content matters, we expect:\n",
    "1. **Truncated gradient**: random < wrong_doc < tfidf < llm_kw (C1, C3, C4 all sig)\n",
    "2. **Suffix gradient**: random < wrong_doc < llm_kw (C6, C8 both sig)\n",
    "3. **Static phrase dominance**: static_fact > random in BOTH modes (C2, C7 both sig)\n",
    "4. **Format effect**: oracle_kw > oracle_raw in truncated (C5 sig, replicates Exp 06)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 1: Setup \u2014 permissions, seeds, results directory\n",
    "import os\n",
    "os.umask(0o000)\n",
    "\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "RESULTS_DIR = Path(\"results/exp10\")\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "SURROGATES_DIR = RESULTS_DIR / \"surrogates\"\n",
    "SURROGATES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CHECKPOINT_PATH = RESULTS_DIR / \"checkpoint.json\"\n",
    "FINAL_RESULTS_PATH = RESULTS_DIR / \"results.json\"\n",
    "\n",
    "print(f\"SEED: {SEED}\")\n",
    "print(f\"Results directory: {RESULTS_DIR}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 2: Load model (Mistral-7B 4-bit)\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "print(f\"Loading {MODEL_NAME} (4-bit)...\")\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "print(f\"Model loaded. dtype={model.dtype}, device={model.device}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 3: Imports, config, constants, and helper functions\n",
    "sys.path.insert(0, \".\")\n",
    "\n",
    "from lib.config import ExperimentConfig\n",
    "from lib.kv_cache import (\n",
    "    deepcopy_cache,\n",
    "    extract_and_truncate_cache_with_bos,\n",
    "    correct_rope_positions_with_bos,\n",
    "    score_answer_with_cache,\n",
    "    build_suffix_kv_cache,\n",
    ")\n",
    "from lib.data import load_ms_marco, load_evaluation_samples\n",
    "from lib.analysis import cohens_d\n",
    "from lib.surrogate import generate_all_5_surrogates, STATIC_SURROGATE_QUERIES\n",
    "from scipy import stats\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "config = ExperimentConfig(\n",
    "    model_name=MODEL_NAME,\n",
    "    num_samples=2000,\n",
    "    min_passage_words=20,\n",
    "    max_passage_words=500,\n",
    "    seed=SEED,\n",
    ")\n",
    "\n",
    "# Templates \u2014 bare text, no \"Document:\\n\" framing\n",
    "SURROGATE_PREFIX_TEMPLATE = \"{surrogate}\\n\"\n",
    "DOCUMENT_TEMPLATE = \"{document}\"\n",
    "QUERY_TEMPLATE = \"\\nQuery: {query}\\nAnswer:\"\n",
    "ANSWER_TEMPLATE = \" {answer}\"\n",
    "\n",
    "N_EVAL = 1000\n",
    "N_COMPARISONS = 10\n",
    "BONFERRONI_ALPHA = 0.05 / N_COMPARISONS\n",
    "CHECKPOINT_EVERY = 50\n",
    "\n",
    "SUFFIX_SEPARATOR = \"\\n\\nRelated question: \"\n",
    "STATIC_FACTUAL_PHRASE = STATIC_SURROGATE_QUERIES['static_factual']['query']\n",
    "\n",
    "CONDITION_NAMES = [\n",
    "    'bare',\n",
    "    # Truncated gradient (8)\n",
    "    'random_trunc', 'random_words_trunc', 'wrong_doc_llm_trunc',\n",
    "    'tfidf_kw_trunc', 'llm_kw_trunc', 'static_fact_trunc',\n",
    "    'oracle_kw_trunc', 'oracle_raw_trunc',\n",
    "    # Suffix gradient (7)\n",
    "    'random_suffix', 'random_words_suffix', 'wrong_doc_llm_suffix',\n",
    "    'tfidf_kw_suffix', 'llm_kw_suffix', 'static_fact_suffix',\n",
    "    'oracle_kw_suffix',\n",
    "]\n",
    "\n",
    "# Semantic levels for gradient analysis\n",
    "SEMANTIC_LEVELS = {\n",
    "    'random': 0, 'random_words': 0.5, 'wrong_doc_llm': 1,\n",
    "    'tfidf_kw': 2, 'llm_kw': 3, 'static_fact': 4, 'oracle_kw': 5,\n",
    "}\n",
    "\n",
    "# --- Stopwords for TF-IDF and oracle-as-keywords ---\n",
    "STOPWORDS = set([\n",
    "    'the', 'a', 'an', 'is', 'are', 'was', 'were', 'be', 'been', 'being',\n",
    "    'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could',\n",
    "    'should', 'may', 'might', 'shall', 'can', 'need', 'dare', 'ought',\n",
    "    'used', 'to', 'of', 'in', 'for', 'on', 'with', 'at', 'by', 'from',\n",
    "    'as', 'into', 'through', 'during', 'before', 'after', 'above', 'below',\n",
    "    'between', 'out', 'off', 'over', 'under', 'again', 'further', 'then',\n",
    "    'once', 'and', 'but', 'or', 'nor', 'not', 'so', 'yet', 'both', 'either',\n",
    "    'neither', 'each', 'every', 'all', 'any', 'few', 'more', 'most', 'other',\n",
    "    'some', 'such', 'no', 'only', 'own', 'same', 'than', 'too', 'very',\n",
    "    'where', 'why', 'how', 'what', 'if', 'up', 'also', 'well', 'back',\n",
    "    'even', 'still', 'new', 'now', 'way', 'many', 'much', 'like', 'get',\n",
    "    'got', 'make', 'made', 'take', 'come', 'go', 'see', 'know', 'think',\n",
    "])\n",
    "\n",
    "QUESTION_STOPWORDS = STOPWORDS | set([\n",
    "    'what', 'which', 'who', 'whom', 'whose', 'when', 'where', 'why', 'how',\n",
    "    'does', 'did', 'can', 'could', 'would', 'should', 'will', 'shall',\n",
    "    'may', 'might', 'must', 'isn', 'aren', 'wasn', 'weren', 'don', 'doesn',\n",
    "    'didn', 'won', 'wouldn', 'couldn', 'shouldn',\n",
    "])\n",
    "\n",
    "# --- Common English words for random_words condition ---\n",
    "COMMON_ENGLISH_WORDS = [\n",
    "    \"apple\", \"river\", \"mountain\", \"table\", \"chair\", \"window\", \"garden\", \"flower\",\n",
    "    \"music\", \"dance\", \"piano\", \"guitar\", \"forest\", \"ocean\", \"desert\", \"island\",\n",
    "    \"bridge\", \"castle\", \"village\", \"market\", \"kitchen\", \"bedroom\", \"library\",\n",
    "    \"hospital\", \"church\", \"stadium\", \"airport\", \"highway\", \"bicycle\", \"telephone\",\n",
    "    \"calendar\", \"newspaper\", \"magazine\", \"photograph\", \"umbrella\", \"birthday\",\n",
    "    \"holiday\", \"vacation\", \"weekend\", \"summer\", \"winter\", \"autumn\", \"spring\",\n",
    "    \"morning\", \"evening\", \"midnight\", \"afternoon\", \"sunrise\", \"sunset\", \"rainbow\",\n",
    "    \"thunder\", \"lightning\", \"earthquake\", \"volcano\", \"diamond\", \"crystal\", \"silver\",\n",
    "    \"golden\", \"copper\", \"bronze\", \"wooden\", \"plastic\", \"rubber\", \"leather\", \"cotton\",\n",
    "    \"marble\", \"granite\", \"concrete\", \"gravel\", \"pebble\", \"boulder\", \"cliff\",\n",
    "    \"valley\", \"meadow\", \"jungle\", \"canyon\", \"glacier\", \"waterfall\", \"harbor\",\n",
    "    \"elephant\", \"dolphin\", \"penguin\", \"parrot\", \"butterfly\", \"crocodile\",\n",
    "    \"salmon\", \"turtle\", \"spider\", \"mosquito\", \"sandwich\", \"chocolate\", \"vanilla\",\n",
    "    \"cinnamon\", \"pepper\", \"mushroom\", \"tomato\", \"potato\", \"banana\", \"strawberry\",\n",
    "    \"blanket\", \"pillow\", \"curtain\", \"mirror\", \"compass\", \"telescope\", \"microscope\",\n",
    "    \"battery\", \"engine\", \"propeller\", \"satellite\", \"oxygen\", \"hydrogen\", \"nitrogen\",\n",
    "    \"calcium\", \"protein\", \"vitamin\", \"bacteria\", \"molecule\", \"equation\", \"triangle\",\n",
    "    \"rectangle\", \"cylinder\", \"sphere\", \"pentagon\", \"diameter\", \"fraction\", \"decimal\",\n",
    "    \"giraffe\", \"kangaroo\", \"flamingo\", \"orchestra\", \"symphony\", \"painting\",\n",
    "    \"sculpture\", \"pottery\", \"costume\", \"jewelry\", \"bracelet\", \"necklace\",\n",
    "    \"backpack\", \"suitcase\", \"envelope\", \"receipt\", \"passport\", \"notebook\",\n",
    "    \"keyboard\", \"monitor\", \"speaker\", \"printer\", \"cabinet\", \"corridor\",\n",
    "]\n",
    "\n",
    "\n",
    "def extract_tfidf_keywords(passage, n_keywords=8):\n",
    "    \"\"\"Extract top content words by frequency (stopwords removed).\"\"\"\n",
    "    words = re.findall(r'\\b[a-zA-Z]+\\b', passage.lower())\n",
    "    content_words = [w for w in words if w not in STOPWORDS and len(w) > 2]\n",
    "    return ' '.join([w for w, _ in Counter(content_words).most_common(n_keywords)])\n",
    "\n",
    "\n",
    "def oracle_to_keywords(query):\n",
    "    \"\"\"Strip question/function words from oracle query to get keyword format.\"\"\"\n",
    "    words = re.findall(r'\\b[a-zA-Z]+\\b', query)\n",
    "    return ' '.join([w for w in words if w.lower() not in QUESTION_STOPWORDS and len(w) > 2])\n",
    "\n",
    "\n",
    "def generate_random_words(rng, n_words=8):\n",
    "    \"\"\"Generate n random English words from the common words list.\"\"\"\n",
    "    indices = rng.randint(0, len(COMMON_ENGLISH_WORDS), size=n_words)\n",
    "    return ' '.join(COMMON_ENGLISH_WORDS[i] for i in indices)\n",
    "\n",
    "\n",
    "def build_primed_and_truncated(prefix_text, bos_id, doc_ids, doc_len, model, tokenizer, config):\n",
    "    \"\"\"Build a primed cache: tokenize prefix, concat [BOS][prefix][doc], forward, truncate+RoPE.\n",
    "\n",
    "    Returns:\n",
    "        (trunc_cache, prefix_token_len) where prefix_token_len includes BOS\n",
    "    \"\"\"\n",
    "    prefix_str = SURROGATE_PREFIX_TEMPLATE.format(surrogate=prefix_text)\n",
    "    prefix_enc = tokenizer(prefix_str, return_tensors=\"pt\",\n",
    "                           add_special_tokens=False, padding=False, truncation=False)\n",
    "    prefix_ids = prefix_enc['input_ids'].to(config.device)\n",
    "    prefix_token_len = 1 + prefix_ids.shape[1]  # BOS + prefix tokens\n",
    "\n",
    "    full_ids = torch.cat([bos_id, prefix_ids, doc_ids], dim=1)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = model(input_ids=full_ids,\n",
    "                    attention_mask=torch.ones_like(full_ids),\n",
    "                    use_cache=True, return_dict=True)\n",
    "\n",
    "    trunc_cache = extract_and_truncate_cache_with_bos(out.past_key_values, doc_len)\n",
    "    correct_rope_positions_with_bos(trunc_cache, prefix_token_len - 1, model)\n",
    "\n",
    "    del out\n",
    "    return trunc_cache, prefix_token_len\n",
    "\n",
    "\n",
    "print(\"Config ready\")\n",
    "print(f\"  num_samples pool: {config.num_samples}\")\n",
    "print(f\"  eval samples: {N_EVAL}\")\n",
    "print(f\"  bonferroni_alpha: {BONFERRONI_ALPHA:.4f} ({N_COMPARISONS} comparisons)\")\n",
    "print(f\"  conditions: {len(CONDITION_NAMES)}\")\n",
    "print(f\"  suffix_separator: '{SUFFIX_SEPARATOR}'\")\n",
    "print(f\"  static_factual_phrase: '{STATIC_FACTUAL_PHRASE}'\")\n",
    "print(f\"  common_english_words: {len(COMMON_ENGLISH_WORDS)} words\")\n",
    "print(f\"  semantic_levels: {SEMANTIC_LEVELS}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 4: Load MS MARCO (1000 samples)\n",
    "dataset = load_ms_marco(config)\n",
    "\n",
    "np.random.seed(SEED)\n",
    "all_samples = load_evaluation_samples(dataset, config, require_answer=True)\n",
    "\n",
    "samples = all_samples[:N_EVAL]\n",
    "N = len(samples)\n",
    "print(f\"Loaded {len(all_samples)} candidates, using first {N} for evaluation\")\n",
    "print(f\"Example passage ({len(samples[0]['passage'].split())} words): {samples[0]['passage'][:100]}...\")\n",
    "print(f\"Example query: {samples[0]['query']}\")\n",
    "print(f\"Example answer: {samples[0]['answer']}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 5: Generate LLM keyword surrogates (fresh, independent)\n",
    "print(\"=\" * 70)\n",
    "print(\"PHASE 1: LLM SURROGATE GENERATION (keyword only)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "surrogates_path = SURROGATES_DIR / \"keyword_surrogates.json\"\n",
    "\n",
    "if surrogates_path.exists():\n",
    "    with open(surrogates_path, 'r') as f:\n",
    "        surrogates_data = json.load(f)\n",
    "    keyword_surrogates = surrogates_data['surrogates']\n",
    "    print(f\"Loaded {len(keyword_surrogates)} keyword surrogates from cache\")\n",
    "else:\n",
    "    keyword_surrogates = []\n",
    "\n",
    "start_idx_gen = len(keyword_surrogates)\n",
    "if start_idx_gen < N:\n",
    "    print(f\"Generating keyword surrogates for samples {start_idx_gen} to {N-1}...\")\n",
    "    t_start = time.time()\n",
    "    for idx in tqdm(range(start_idx_gen, N), initial=start_idx_gen, total=N,\n",
    "                     desc=\"Keyword surrogates\"):\n",
    "        passage = samples[idx]['passage']\n",
    "        try:\n",
    "            s5 = generate_all_5_surrogates(passage, model, tokenizer, config)\n",
    "            kw = s5.get('keyword_query', '')\n",
    "        except Exception as e:\n",
    "            print(f\"  WARNING: Generation failed for sample {idx}: {e}\")\n",
    "            kw = \"\"\n",
    "        keyword_surrogates.append(kw)\n",
    "\n",
    "        if (idx + 1) % 100 == 0 or idx == N - 1:\n",
    "            with open(surrogates_path, 'w') as f:\n",
    "                json.dump({'surrogates': keyword_surrogates}, f)\n",
    "            elapsed = time.time() - t_start\n",
    "            rate = (idx - start_idx_gen + 1) / elapsed if elapsed > 0 else 0\n",
    "            remaining = (N - idx - 1) / rate if rate > 0 else 0\n",
    "            tqdm.write(f\"  Saved {idx+1}/{N} | {rate:.2f} s/s | ETA: {remaining/60:.1f} min\")\n",
    "\n",
    "    with open(surrogates_path, 'w') as f:\n",
    "        json.dump({'surrogates': keyword_surrogates}, f)\n",
    "    print(f\"Keyword surrogates complete: {len(keyword_surrogates)} samples\")\n",
    "else:\n",
    "    print(f\"All keyword surrogates already cached ({len(keyword_surrogates)} samples)\")\n",
    "\n",
    "n_empty = sum(1 for s in keyword_surrogates if not s.strip())\n",
    "print(f\"Empty surrogates: {n_empty}/{N}\")\n",
    "print(f\"Example: '{keyword_surrogates[0]}'\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 6: Pre-compute TF-IDF keywords and oracle-as-keywords for all samples\n",
    "print(\"=\" * 70)\n",
    "print(\"PRE-COMPUTING DERIVED SURROGATES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "tfidf_keywords = []\n",
    "oracle_keywords = []\n",
    "\n",
    "for idx in range(N):\n",
    "    passage = samples[idx]['passage']\n",
    "    query = samples[idx]['query']\n",
    "\n",
    "    tfidf_keywords.append(extract_tfidf_keywords(passage, n_keywords=8))\n",
    "    oracle_keywords.append(oracle_to_keywords(query))\n",
    "\n",
    "# Diagnostics\n",
    "print(f\"\\nTF-IDF keywords ({N} samples):\")\n",
    "print(f\"  Example 0: '{tfidf_keywords[0]}'\")\n",
    "print(f\"  Example 1: '{tfidf_keywords[1]}'\")\n",
    "print(f\"  Empty: {sum(1 for t in tfidf_keywords if not t.strip())}/{N}\")\n",
    "\n",
    "print(f\"\\nOracle-as-keywords ({N} samples):\")\n",
    "print(f\"  Example 0: '{oracle_keywords[0]}'  (from: '{samples[0]['query']}')\")\n",
    "print(f\"  Example 1: '{oracle_keywords[1]}'  (from: '{samples[1]['query']}')\")\n",
    "print(f\"  Empty: {sum(1 for o in oracle_keywords if not o.strip())}/{N}\")\n",
    "\n",
    "# Token length comparison\n",
    "llm_lens = [len(tokenizer.encode(kw, add_special_tokens=False)) for kw in keyword_surrogates]\n",
    "tfidf_lens = [len(tokenizer.encode(kw, add_special_tokens=False)) for kw in tfidf_keywords]\n",
    "oracle_kw_lens = [len(tokenizer.encode(kw, add_special_tokens=False)) for kw in oracle_keywords]\n",
    "static_len = len(tokenizer.encode(STATIC_FACTUAL_PHRASE, add_special_tokens=False))\n",
    "\n",
    "print(f\"\\nToken lengths (mean \u00b1 std):\")\n",
    "print(f\"  LLM keywords: {np.mean(llm_lens):.1f} \u00b1 {np.std(llm_lens):.1f}\")\n",
    "print(f\"  TF-IDF keywords: {np.mean(tfidf_lens):.1f} \u00b1 {np.std(tfidf_lens):.1f}\")\n",
    "print(f\"  Oracle-as-keywords: {np.mean(oracle_kw_lens):.1f} \u00b1 {np.std(oracle_kw_lens):.1f}\")\n",
    "print(f\"  Static factual: {static_len}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 7: Condition explanation with concrete examples\n",
    "print(\"=\" * 70)\n",
    "print(\"EXPERIMENTAL CONDITIONS EXPLAINED\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "ex = {\n",
    "    'passage': samples[0]['passage'][:80],\n",
    "    'query': samples[0]['query'],\n",
    "    'llm_kw': keyword_surrogates[0],\n",
    "    'tfidf_kw': tfidf_keywords[0],\n",
    "    'oracle_kw': oracle_keywords[0],\n",
    "    'random_words': generate_random_words(np.random.RandomState(SEED), 8),\n",
    "}\n",
    "\n",
    "conditions_explained = [\n",
    "    (\"1. bare\",\n",
    "     \"[BOS][doc]\",\n",
    "     \"No prefix \u2014 baseline\"),\n",
    "    (\"2. random_trunc\",\n",
    "     \"[BOS][random_tokens\\\\n][doc] \u2192 truncate + RoPE\",\n",
    "     \"Random vocabulary tokens. Semantic level: 0\"),\n",
    "    (\"3. random_words_trunc\",\n",
    "     \"[BOS][random_words\\\\n][doc] \u2192 truncate + RoPE\",\n",
    "     f\"Random English words: '{ex['random_words']}'. Semantic level: 0.5\"),\n",
    "    (\"4. wrong_doc_llm_trunc\",\n",
    "     \"[BOS][prev_kw\\\\n][doc] \u2192 truncate + RoPE\",\n",
    "     \"LLM keywords from PREVIOUS sample's doc. Right format, wrong content. Semantic level: 1\"),\n",
    "    (\"5. tfidf_kw_trunc\",\n",
    "     \"[BOS][tfidf\\\\n][doc] \u2192 truncate + RoPE\",\n",
    "     f\"TF-IDF keywords: '{ex['tfidf_kw']}'. Semantic level: 2\"),\n",
    "    (\"6. llm_kw_trunc\",\n",
    "     \"[BOS][llm_kw\\\\n][doc] \u2192 truncate + RoPE\",\n",
    "     f\"LLM-generated keywords: '{ex['llm_kw']}'. Semantic level: 3\"),\n",
    "    (\"7. static_fact_trunc\",\n",
    "     \"[BOS][static_fact\\\\n][doc] \u2192 truncate + RoPE\",\n",
    "     f\"Fixed phrase: '{STATIC_FACTUAL_PHRASE}'. Semantic level: 4\"),\n",
    "    (\"8. oracle_kw_trunc\",\n",
    "     \"[BOS][oracle_kw\\\\n][doc] \u2192 truncate + RoPE\",\n",
    "     f\"Oracle as keywords: '{ex['oracle_kw']}'. Semantic level: 5\"),\n",
    "    (\"9. oracle_raw_trunc\",\n",
    "     \"[BOS][oracle_raw\\\\n][doc] \u2192 truncate + RoPE\",\n",
    "     f\"Oracle raw question: '{ex['query']}'. Level 5 but with format interference\"),\n",
    "    (\"10-16. *_suffix\",\n",
    "     \"[BOS][doc][sep][content]\",\n",
    "     \"Same 7 content types as suffix after doc. No value contamination; pure attention.\"),\n",
    "]\n",
    "\n",
    "for name, pattern, detail in conditions_explained:\n",
    "    print(f\"\\n### {name} ###\")\n",
    "    print(f\"  Cache: {pattern}\")\n",
    "    print(f\"  Detail: {detail}\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"FORWARD PASSES PER SAMPLE: 16\")\n",
    "print(\"  Truncated (9): bare + 8 prefix types\")\n",
    "print(\"  Suffix (7): 7 suffix types via build_suffix_kv_cache\")\n",
    "print(\"  Total scoring calls: 16 (one per condition)\")\n",
    "print(f\"{'='*70}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 8: Main eval loop \u2014 16 conditions \u00d7 1000 samples\n",
    "print(\"=\" * 70)\n",
    "print(\"PHASE 2: MAIN EVALUATION (16 conditions \u00d7 1000 samples)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "results = []\n",
    "start_idx = 0\n",
    "\n",
    "if CHECKPOINT_PATH.exists():\n",
    "    with open(CHECKPOINT_PATH, 'r') as f:\n",
    "        ckpt = json.load(f)\n",
    "    ckpt_queries = ckpt.get('sample_queries', [])\n",
    "    current_queries = [s['query'] for s in samples]\n",
    "    if ckpt_queries == current_queries:\n",
    "        results = ckpt['results']\n",
    "        start_idx = len(results)\n",
    "        print(f\"Resuming from checkpoint: {start_idx}/{N}\")\n",
    "    else:\n",
    "        print(\"Checkpoint sample mismatch. Starting fresh.\")\n",
    "else:\n",
    "    print(\"No checkpoint found. Starting fresh.\")\n",
    "\n",
    "print(f\"Evaluating samples {start_idx} to {N-1}\")\n",
    "print(f\"Conditions: {len(CONDITION_NAMES)}\")\n",
    "\n",
    "t_start = time.time()\n",
    "\n",
    "for idx in tqdm(range(start_idx, N), initial=start_idx, total=N, desc=\"Evaluating\"):\n",
    "    sample = samples[idx]\n",
    "    passage = sample['passage']\n",
    "    query = sample['query']\n",
    "    answer = sample['answer']\n",
    "\n",
    "    query_prompt = QUERY_TEMPLATE.format(query=query)\n",
    "    answer_text = ANSWER_TEMPLATE.format(answer=answer)\n",
    "\n",
    "    # Get all content strings for this sample\n",
    "    llm_kw_text = keyword_surrogates[idx]\n",
    "    tfidf_kw_text = tfidf_keywords[idx]\n",
    "    oracle_kw_text = oracle_keywords[idx]\n",
    "    oracle_raw_text = query  # original question format\n",
    "\n",
    "    # Wrong-doc: use previous sample's LLM keyword\n",
    "    if idx > 0:\n",
    "        wrong_doc_text = keyword_surrogates[idx - 1]\n",
    "    else:\n",
    "        wrong_doc_text = \"\"  # sample 0: handled below\n",
    "\n",
    "    # Random tokens (deterministic per sample)\n",
    "    n_random_tokens = max(5, len(tokenizer.encode(llm_kw_text, add_special_tokens=False)))\n",
    "    rng_tokens = np.random.RandomState(SEED + idx)\n",
    "    random_ids = torch.randint(100, tokenizer.vocab_size - 100, (n_random_tokens,), device='cpu')\n",
    "    random_text = tokenizer.decode(random_ids, skip_special_tokens=True)\n",
    "\n",
    "    # Random English words (deterministic per sample)\n",
    "    rng_words = np.random.RandomState(SEED + idx + 10000)\n",
    "    random_words_text = generate_random_words(rng_words, n_words=8)\n",
    "\n",
    "    # --- Matched tokenization (for truncated conditions) ---\n",
    "    oracle_prefix = SURROGATE_PREFIX_TEMPLATE.format(surrogate=query)\n",
    "    document_text = DOCUMENT_TEMPLATE.format(document=passage)\n",
    "    full_oracle_text = oracle_prefix + document_text\n",
    "\n",
    "    full_oracle_enc = tokenizer(full_oracle_text, return_tensors=\"pt\",\n",
    "                                add_special_tokens=True, padding=False, truncation=False)\n",
    "    full_oracle_ids = full_oracle_enc['input_ids'].to(config.device)\n",
    "\n",
    "    oracle_prefix_enc = tokenizer(oracle_prefix, return_tensors=\"pt\",\n",
    "                                  add_special_tokens=True, padding=False, truncation=False)\n",
    "    oracle_prefix_len = oracle_prefix_enc['input_ids'].shape[1]\n",
    "\n",
    "    bos_id = full_oracle_ids[:, :1]\n",
    "    doc_ids = full_oracle_ids[:, oracle_prefix_len:]\n",
    "    doc_len = doc_ids.shape[1]\n",
    "\n",
    "    # ===== FORWARD PASS 1: BARE =====\n",
    "    bare_ids = torch.cat([bos_id, doc_ids], dim=1)\n",
    "    with torch.no_grad():\n",
    "        bare_out = model(input_ids=bare_ids, attention_mask=torch.ones_like(bare_ids),\n",
    "                         use_cache=True, return_dict=True)\n",
    "    bare_cache = bare_out.past_key_values\n",
    "    del bare_out\n",
    "\n",
    "    nll_bare = score_answer_with_cache(\n",
    "        deepcopy_cache(bare_cache), bare_ids.shape[1],\n",
    "        query_prompt, answer_text, model, tokenizer, config)\n",
    "\n",
    "    # ===== TRUNCATED CONDITIONS (8 forward passes) =====\n",
    "\n",
    "    # 2. random_trunc\n",
    "    trunc_cache, _ = build_primed_and_truncated(\n",
    "        random_text, bos_id, doc_ids, doc_len, model, tokenizer, config)\n",
    "    nll_random_trunc = score_answer_with_cache(\n",
    "        deepcopy_cache(trunc_cache), 1 + doc_len,\n",
    "        query_prompt, answer_text, model, tokenizer, config)\n",
    "    del trunc_cache\n",
    "\n",
    "    # 3. random_words_trunc\n",
    "    trunc_cache, _ = build_primed_and_truncated(\n",
    "        random_words_text, bos_id, doc_ids, doc_len, model, tokenizer, config)\n",
    "    nll_random_words_trunc = score_answer_with_cache(\n",
    "        deepcopy_cache(trunc_cache), 1 + doc_len,\n",
    "        query_prompt, answer_text, model, tokenizer, config)\n",
    "    del trunc_cache\n",
    "\n",
    "    # 4. wrong_doc_llm_trunc (sample 0: NLL=0)\n",
    "    if idx > 0:\n",
    "        trunc_cache, _ = build_primed_and_truncated(\n",
    "            wrong_doc_text, bos_id, doc_ids, doc_len, model, tokenizer, config)\n",
    "        nll_wrong_doc_trunc = score_answer_with_cache(\n",
    "            deepcopy_cache(trunc_cache), 1 + doc_len,\n",
    "            query_prompt, answer_text, model, tokenizer, config)\n",
    "        del trunc_cache\n",
    "    else:\n",
    "        nll_wrong_doc_trunc = 0.0\n",
    "\n",
    "    # 5. tfidf_kw_trunc\n",
    "    trunc_cache, _ = build_primed_and_truncated(\n",
    "        tfidf_kw_text, bos_id, doc_ids, doc_len, model, tokenizer, config)\n",
    "    nll_tfidf_trunc = score_answer_with_cache(\n",
    "        deepcopy_cache(trunc_cache), 1 + doc_len,\n",
    "        query_prompt, answer_text, model, tokenizer, config)\n",
    "    del trunc_cache\n",
    "\n",
    "    # 6. llm_kw_trunc\n",
    "    trunc_cache, _ = build_primed_and_truncated(\n",
    "        llm_kw_text, bos_id, doc_ids, doc_len, model, tokenizer, config)\n",
    "    nll_llm_kw_trunc = score_answer_with_cache(\n",
    "        deepcopy_cache(trunc_cache), 1 + doc_len,\n",
    "        query_prompt, answer_text, model, tokenizer, config)\n",
    "    del trunc_cache\n",
    "\n",
    "    # 7. static_fact_trunc\n",
    "    trunc_cache, _ = build_primed_and_truncated(\n",
    "        STATIC_FACTUAL_PHRASE, bos_id, doc_ids, doc_len, model, tokenizer, config)\n",
    "    nll_static_fact_trunc = score_answer_with_cache(\n",
    "        deepcopy_cache(trunc_cache), 1 + doc_len,\n",
    "        query_prompt, answer_text, model, tokenizer, config)\n",
    "    del trunc_cache\n",
    "\n",
    "    # 8. oracle_kw_trunc\n",
    "    trunc_cache, _ = build_primed_and_truncated(\n",
    "        oracle_kw_text, bos_id, doc_ids, doc_len, model, tokenizer, config)\n",
    "    nll_oracle_kw_trunc = score_answer_with_cache(\n",
    "        deepcopy_cache(trunc_cache), 1 + doc_len,\n",
    "        query_prompt, answer_text, model, tokenizer, config)\n",
    "    del trunc_cache\n",
    "\n",
    "    # 9. oracle_raw_trunc\n",
    "    trunc_cache, _ = build_primed_and_truncated(\n",
    "        oracle_raw_text, bos_id, doc_ids, doc_len, model, tokenizer, config)\n",
    "    nll_oracle_raw_trunc = score_answer_with_cache(\n",
    "        deepcopy_cache(trunc_cache), 1 + doc_len,\n",
    "        query_prompt, answer_text, model, tokenizer, config)\n",
    "    del trunc_cache\n",
    "\n",
    "    del bare_cache, bare_ids\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # ===== SUFFIX CONDITIONS (7 forward passes) =====\n",
    "\n",
    "    # 10. random_suffix\n",
    "    suf_len, suf_cache = build_suffix_kv_cache(\n",
    "        passage, random_text, model, tokenizer, config, separator=SUFFIX_SEPARATOR)\n",
    "    nll_random_suffix = score_answer_with_cache(\n",
    "        deepcopy_cache(suf_cache), suf_len,\n",
    "        query_prompt, answer_text, model, tokenizer, config)\n",
    "    del suf_cache\n",
    "\n",
    "    # 11. random_words_suffix\n",
    "    suf_len, suf_cache = build_suffix_kv_cache(\n",
    "        passage, random_words_text, model, tokenizer, config, separator=SUFFIX_SEPARATOR)\n",
    "    nll_random_words_suffix = score_answer_with_cache(\n",
    "        deepcopy_cache(suf_cache), suf_len,\n",
    "        query_prompt, answer_text, model, tokenizer, config)\n",
    "    del suf_cache\n",
    "\n",
    "    # 12. wrong_doc_llm_suffix (sample 0: NLL=0)\n",
    "    if idx > 0:\n",
    "        suf_len, suf_cache = build_suffix_kv_cache(\n",
    "            passage, wrong_doc_text, model, tokenizer, config, separator=SUFFIX_SEPARATOR)\n",
    "        nll_wrong_doc_suffix = score_answer_with_cache(\n",
    "            deepcopy_cache(suf_cache), suf_len,\n",
    "            query_prompt, answer_text, model, tokenizer, config)\n",
    "        del suf_cache\n",
    "    else:\n",
    "        nll_wrong_doc_suffix = 0.0\n",
    "\n",
    "    # 13. tfidf_kw_suffix\n",
    "    suf_len, suf_cache = build_suffix_kv_cache(\n",
    "        passage, tfidf_kw_text, model, tokenizer, config, separator=SUFFIX_SEPARATOR)\n",
    "    nll_tfidf_suffix = score_answer_with_cache(\n",
    "        deepcopy_cache(suf_cache), suf_len,\n",
    "        query_prompt, answer_text, model, tokenizer, config)\n",
    "    del suf_cache\n",
    "\n",
    "    # 14. llm_kw_suffix\n",
    "    suf_len, suf_cache = build_suffix_kv_cache(\n",
    "        passage, llm_kw_text, model, tokenizer, config, separator=SUFFIX_SEPARATOR)\n",
    "    nll_llm_kw_suffix = score_answer_with_cache(\n",
    "        deepcopy_cache(suf_cache), suf_len,\n",
    "        query_prompt, answer_text, model, tokenizer, config)\n",
    "    del suf_cache\n",
    "\n",
    "    # 15. static_fact_suffix\n",
    "    suf_len, suf_cache = build_suffix_kv_cache(\n",
    "        passage, STATIC_FACTUAL_PHRASE, model, tokenizer, config, separator=SUFFIX_SEPARATOR)\n",
    "    nll_static_fact_suffix = score_answer_with_cache(\n",
    "        deepcopy_cache(suf_cache), suf_len,\n",
    "        query_prompt, answer_text, model, tokenizer, config)\n",
    "    del suf_cache\n",
    "\n",
    "    # 16. oracle_kw_suffix\n",
    "    suf_len, suf_cache = build_suffix_kv_cache(\n",
    "        passage, oracle_kw_text, model, tokenizer, config, separator=SUFFIX_SEPARATOR)\n",
    "    nll_oracle_kw_suffix = score_answer_with_cache(\n",
    "        deepcopy_cache(suf_cache), suf_len,\n",
    "        query_prompt, answer_text, model, tokenizer, config)\n",
    "    del suf_cache\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # --- Store result ---\n",
    "    result = {\n",
    "        'idx': idx,\n",
    "        'doc_len': doc_len,\n",
    "        'passage_word_count': len(passage.split()),\n",
    "        'bare': nll_bare,\n",
    "        'random_trunc': nll_random_trunc,\n",
    "        'random_words_trunc': nll_random_words_trunc,\n",
    "        'wrong_doc_llm_trunc': nll_wrong_doc_trunc,\n",
    "        'tfidf_kw_trunc': nll_tfidf_trunc,\n",
    "        'llm_kw_trunc': nll_llm_kw_trunc,\n",
    "        'static_fact_trunc': nll_static_fact_trunc,\n",
    "        'oracle_kw_trunc': nll_oracle_kw_trunc,\n",
    "        'oracle_raw_trunc': nll_oracle_raw_trunc,\n",
    "        'random_suffix': nll_random_suffix,\n",
    "        'random_words_suffix': nll_random_words_suffix,\n",
    "        'wrong_doc_llm_suffix': nll_wrong_doc_suffix,\n",
    "        'tfidf_kw_suffix': nll_tfidf_suffix,\n",
    "        'llm_kw_suffix': nll_llm_kw_suffix,\n",
    "        'static_fact_suffix': nll_static_fact_suffix,\n",
    "        'oracle_kw_suffix': nll_oracle_kw_suffix,\n",
    "    }\n",
    "    results.append(result)\n",
    "\n",
    "    if (idx + 1) % CHECKPOINT_EVERY == 0 or idx == N - 1:\n",
    "        ckpt_data = {\n",
    "            'results': results,\n",
    "            'sample_queries': [s['query'] for s in samples],\n",
    "            'completed': len(results),\n",
    "            'total': N,\n",
    "            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        }\n",
    "        with open(CHECKPOINT_PATH, 'w') as f:\n",
    "            json.dump(ckpt_data, f)\n",
    "        elapsed = time.time() - t_start\n",
    "        rate = (idx - start_idx + 1) / elapsed if elapsed > 0 else 0\n",
    "        remaining = (N - idx - 1) / rate if rate > 0 else 0\n",
    "        tqdm.write(f\"  Checkpoint {idx+1}/{N} | {rate:.2f} s/s | ETA: {remaining/60:.1f} min\")\n",
    "\n",
    "elapsed_total = time.time() - t_start\n",
    "print(f\"\\nEvaluation complete: {len(results)} samples in {elapsed_total/60:.1f} min\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 9: Primary analysis \u2014 NLL summary + 10 comparisons\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ANALYSIS \u2014 SEMANTIC CONTENT GRADIENT\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Extract arrays and filter zero NLLs\n",
    "cond_arrays = {}\n",
    "for cname in CONDITION_NAMES:\n",
    "    cond_arrays[cname] = np.array([r[cname] for r in results])\n",
    "\n",
    "valid = np.ones(len(results), dtype=bool)\n",
    "for cname in CONDITION_NAMES:\n",
    "    valid &= (cond_arrays[cname] != 0)\n",
    "n_valid = int(np.sum(valid))\n",
    "n_excluded = int(np.sum(~valid))\n",
    "print(f\"Total: {len(results)}, Valid: {n_valid}, Excluded: {n_excluded}\")\n",
    "\n",
    "c = {}\n",
    "for cname in CONDITION_NAMES:\n",
    "    c[cname] = cond_arrays[cname][valid]\n",
    "\n",
    "# NLL summary table\n",
    "print(f\"\\n{'Condition':<30} {'Mean NLL':>10} {'Std':>10} {'d vs Bare':>10} {'Win%':>7}\")\n",
    "print(\"-\" * 72)\n",
    "for cname in CONDITION_NAMES:\n",
    "    mean_nll = np.mean(c[cname])\n",
    "    std_nll = np.std(c[cname])\n",
    "    if cname == 'bare':\n",
    "        print(f\"{cname:<30} {mean_nll:>10.4f} {std_nll:>10.4f} {'\u2014':>10} {'\u2014':>7}\")\n",
    "    else:\n",
    "        delta = c['bare'] - c[cname]\n",
    "        d = cohens_d(delta)\n",
    "        win = np.mean(delta > 0) * 100\n",
    "        _, p_val = stats.ttest_1samp(delta, 0)\n",
    "        sig = \"***\" if p_val < 0.001 else \"**\" if p_val < BONFERRONI_ALPHA else \"*\" if p_val < 0.05 else \"ns\"\n",
    "        print(f\"{cname:<30} {mean_nll:>10.4f} {std_nll:>10.4f} {d:>+10.3f} {win:>5.1f}% {sig}\")\n",
    "\n",
    "# 10 primary comparisons\n",
    "print(f\"\\n{'='*95}\")\n",
    "print(f\"10 PRIMARY COMPARISONS (Bonferroni alpha = {BONFERRONI_ALPHA:.4f})\")\n",
    "print(f\"{'='*95}\")\n",
    "\n",
    "comparisons = [\n",
    "    ('C1: llm_kw vs random (trunc)',\n",
    "     c['random_trunc'] - c['llm_kw_trunc'],\n",
    "     'LLM > random in truncated?'),\n",
    "    ('C2: static_fact vs random (trunc)',\n",
    "     c['random_trunc'] - c['static_fact_trunc'],\n",
    "     'Static > random in truncated?'),\n",
    "    ('C3: llm_kw vs wrong_doc (trunc)',\n",
    "     c['wrong_doc_llm_trunc'] - c['llm_kw_trunc'],\n",
    "     'Right doc > wrong doc (trunc)?'),\n",
    "    ('C4: tfidf vs random (trunc)',\n",
    "     c['random_trunc'] - c['tfidf_kw_trunc'],\n",
    "     'TF-IDF > random in truncated?'),\n",
    "    ('C5: oracle_kw vs oracle_raw (trunc)',\n",
    "     c['oracle_raw_trunc'] - c['oracle_kw_trunc'],\n",
    "     'Keyword > question format?'),\n",
    "    ('C6: llm_kw vs random (suffix)',\n",
    "     c['random_suffix'] - c['llm_kw_suffix'],\n",
    "     'LLM > random in suffix?'),\n",
    "    ('C7: static_fact vs random (suffix)',\n",
    "     c['random_suffix'] - c['static_fact_suffix'],\n",
    "     'Static > random in suffix?'),\n",
    "    ('C8: llm_kw vs wrong_doc (suffix)',\n",
    "     c['wrong_doc_llm_suffix'] - c['llm_kw_suffix'],\n",
    "     'Right doc > wrong doc (suffix)?'),\n",
    "    ('C9: trunc vs suffix (llm_kw)',\n",
    "     c['llm_kw_suffix'] - c['llm_kw_trunc'],\n",
    "     'Truncated > suffix (LLM content)?'),\n",
    "    ('C10: trunc vs suffix (static_fact)',\n",
    "     c['static_fact_suffix'] - c['static_fact_trunc'],\n",
    "     'Truncated > suffix (static)?'),\n",
    "]\n",
    "\n",
    "print(f\"\\n{'Comparison':<40} {'Mean delta':>10} {'d':>8} {'Win%':>7} {'t':>8} {'p':>12} {'Sig':>5}\")\n",
    "print(\"-\" * 95)\n",
    "\n",
    "comparison_results = {}\n",
    "for name, delta, question in comparisons:\n",
    "    d = cohens_d(delta)\n",
    "    win = np.mean(delta > 0) * 100\n",
    "    t_stat, p_val = stats.ttest_1samp(delta, 0)\n",
    "    sig = \"***\" if p_val < 0.001 else \"**\" if p_val < BONFERRONI_ALPHA else \"*\" if p_val < 0.05 else \"ns\"\n",
    "    print(f\"{name:<40} {np.mean(delta):>10.4f} {d:>8.3f} {win:>6.1f}% {t_stat:>8.2f} {p_val:>11.2e} {sig:>5}\")\n",
    "    comparison_results[name] = {\n",
    "        'mean_delta': float(np.mean(delta)),\n",
    "        'cohens_d': float(d),\n",
    "        'win_rate': float(win / 100),\n",
    "        't_stat': float(t_stat),\n",
    "        'p_value': float(p_val),\n",
    "        'bonferroni_significant': bool(p_val < BONFERRONI_ALPHA),\n",
    "        'question': question,\n",
    "    }\n",
    "\n",
    "# All vs Bare\n",
    "print(f\"\\n{'='*95}\")\n",
    "print(\"ALL CONDITIONS vs BARE (sorted by d)\")\n",
    "print(f\"{'='*95}\")\n",
    "all_vs_bare = {}\n",
    "all_conds = [(cn, cohens_d(c['bare'] - c[cn])) for cn in CONDITION_NAMES if cn != 'bare']\n",
    "all_conds.sort(key=lambda x: x[1], reverse=True)\n",
    "print(f\"\\n{'Condition':<30} {'d vs Bare':>10} {'Win%':>7} {'p':>12}\")\n",
    "print(\"-\" * 65)\n",
    "for cname, d in all_conds:\n",
    "    delta = c['bare'] - c[cname]\n",
    "    win = np.mean(delta > 0) * 100\n",
    "    _, p_val = stats.ttest_1samp(delta, 0)\n",
    "    sig = \"***\" if p_val < 0.001 else \"**\" if p_val < BONFERRONI_ALPHA else \"*\" if p_val < 0.05 else \"ns\"\n",
    "    print(f\"{cname:<30} {d:>10.3f} {win:>6.1f}% {p_val:>11.2e} {sig:>5}\")\n",
    "    all_vs_bare[cname] = {'cohens_d': float(d), 'win_rate': float(win/100), 'p_value': float(p_val)}"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 10: Semantic gradient analysis + hardness breakdown\n",
    "\n",
    "# --- SEMANTIC GRADIENT ---\n",
    "print(\"=\" * 70)\n",
    "print(\"SEMANTIC GRADIENT ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Build gradient data for both modes\n",
    "content_types = ['random', 'random_words', 'wrong_doc_llm', 'tfidf_kw', 'llm_kw', 'static_fact', 'oracle_kw']\n",
    "levels = [SEMANTIC_LEVELS[ct] for ct in content_types]\n",
    "\n",
    "trunc_ds = []\n",
    "suffix_ds = []\n",
    "for ct in content_types:\n",
    "    trunc_d = cohens_d(c['bare'] - c[f'{ct}_trunc'])\n",
    "    suffix_d = cohens_d(c['bare'] - c[f'{ct}_suffix'])\n",
    "    trunc_ds.append(trunc_d)\n",
    "    suffix_ds.append(suffix_d)\n",
    "\n",
    "print(f\"\\n{'Content Type':<20} {'Level':>6} {'Trunc d':>10} {'Suffix d':>10} {'Trunc > Suffix':>15}\")\n",
    "print(\"-\" * 65)\n",
    "for ct, lev, td, sd in zip(content_types, levels, trunc_ds, suffix_ds):\n",
    "    trunc_better = \"YES\" if td > sd else \"no\"\n",
    "    print(f\"{ct:<20} {lev:>6.1f} {td:>+10.3f} {sd:>+10.3f} {trunc_better:>15}\")\n",
    "\n",
    "# Correlation: semantic level vs d\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "r_trunc, p_trunc = spearmanr(levels, trunc_ds)\n",
    "r_suffix, p_suffix = spearmanr(levels, suffix_ds)\n",
    "print(f\"\\nSemantic level vs d correlation:\")\n",
    "print(f\"  Truncated: Spearman r = {r_trunc:+.3f}, p = {p_trunc:.4f}\")\n",
    "print(f\"  Suffix:    Spearman r = {r_suffix:+.3f}, p = {p_suffix:.4f}\")\n",
    "\n",
    "# Also check: do both modes agree on ranking?\n",
    "trunc_rank = np.argsort(np.argsort(trunc_ds))\n",
    "suffix_rank = np.argsort(np.argsort(suffix_ds))\n",
    "r_cross, p_cross = spearmanr(trunc_rank, suffix_rank)\n",
    "print(f\"  Cross-mode rank agreement: Spearman r = {r_cross:+.3f}, p = {p_cross:.4f}\")\n",
    "\n",
    "# Oracle raw vs oracle kw (format effect)\n",
    "d_oracle_raw = cohens_d(c['bare'] - c['oracle_raw_trunc'])\n",
    "d_oracle_kw = cohens_d(c['bare'] - c['oracle_kw_trunc'])\n",
    "print(f\"\\nFormat effect (truncated only):\")\n",
    "print(f\"  Oracle-as-keywords: d = {d_oracle_kw:+.3f}\")\n",
    "print(f\"  Oracle-raw-question: d = {d_oracle_raw:+.3f}\")\n",
    "print(f\"  Format penalty: d = {d_oracle_raw - d_oracle_kw:+.3f}\")\n",
    "\n",
    "# --- HARDNESS QUINTILE BREAKDOWN ---\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"HARDNESS QUINTILE BREAKDOWN\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "bare_valid = c['bare']\n",
    "quintile_boundaries = np.percentile(bare_valid, [20, 40, 60, 80])\n",
    "quintile_labels = ['Q1 (easy)', 'Q2', 'Q3', 'Q4', 'Q5 (hard)']\n",
    "\n",
    "def get_quintile(nll, boundaries):\n",
    "    for i, b in enumerate(boundaries):\n",
    "        if nll <= b:\n",
    "            return i\n",
    "    return len(boundaries)\n",
    "\n",
    "quintiles = np.array([get_quintile(nll, quintile_boundaries) for nll in bare_valid])\n",
    "\n",
    "# Key conditions for hardness analysis\n",
    "key_conds = [\n",
    "    'random_trunc', 'random_words_trunc', 'wrong_doc_llm_trunc',\n",
    "    'tfidf_kw_trunc', 'llm_kw_trunc', 'static_fact_trunc', 'oracle_kw_trunc',\n",
    "    'random_suffix', 'llm_kw_suffix', 'static_fact_suffix', 'oracle_kw_suffix',\n",
    "]\n",
    "\n",
    "header = f\"{'Condition':<30}\" + \"\".join(f\"{ql:>12}\" for ql in quintile_labels) + f\"{'Overall':>12}\"\n",
    "print(f\"\\n{header}\")\n",
    "print(\"-\" * (30 + 12 * 6))\n",
    "\n",
    "hardness_breakdown = {}\n",
    "for cname in key_conds:\n",
    "    row = f\"{cname:<30}\"\n",
    "    quintile_ds = []\n",
    "    for q in range(5):\n",
    "        mask_q = quintiles == q\n",
    "        if np.sum(mask_q) < 10:\n",
    "            row += f\"{'n/a':>12}\"\n",
    "            quintile_ds.append(None)\n",
    "        else:\n",
    "            delta = bare_valid[mask_q] - c[cname][mask_q]\n",
    "            d = cohens_d(delta)\n",
    "            row += f\"{d:>+12.3f}\"\n",
    "            quintile_ds.append(float(d))\n",
    "    d_all = cohens_d(bare_valid - c[cname])\n",
    "    row += f\"{d_all:>+12.3f}\"\n",
    "    print(row)\n",
    "    hardness_breakdown[cname] = {'quintile_ds': quintile_ds, 'overall_d': float(d_all)}\n",
    "\n",
    "# Verdict\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"VERDICT: DOES SEMANTIC CONTENT MATTER?\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "# Check if gradient exists in both modes\n",
    "trunc_gradient = trunc_ds[-1] - trunc_ds[0]  # oracle_kw - random\n",
    "suffix_gradient = suffix_ds[-1] - suffix_ds[0]\n",
    "both_positive = trunc_gradient > 0 and suffix_gradient > 0\n",
    "\n",
    "# Check key comparisons\n",
    "c1_sig = comparison_results['C1: llm_kw vs random (trunc)']['bonferroni_significant']\n",
    "c2_sig = comparison_results['C2: static_fact vs random (trunc)']['bonferroni_significant']\n",
    "c6_sig = comparison_results['C6: llm_kw vs random (suffix)']['bonferroni_significant']\n",
    "c7_sig = comparison_results['C7: static_fact vs random (suffix)']['bonferroni_significant']\n",
    "\n",
    "print(f\"\\n  Truncated gradient (oracle_kw - random): {trunc_gradient:+.3f}\")\n",
    "print(f\"  Suffix gradient (oracle_kw - random):    {suffix_gradient:+.3f}\")\n",
    "print(f\"  Both positive: {'YES' if both_positive else 'NO'}\")\n",
    "print(f\"\\n  C1 (llm_kw > random, trunc): {'YES ***' if c1_sig else 'NO (ns)'}\")\n",
    "print(f\"  C2 (static > random, trunc):  {'YES ***' if c2_sig else 'NO (ns)'}\")\n",
    "print(f\"  C6 (llm_kw > random, suffix): {'YES ***' if c6_sig else 'NO (ns)'}\")\n",
    "print(f\"  C7 (static > random, suffix): {'YES ***' if c7_sig else 'NO (ns)'}\")\n",
    "print(f\"\\n  Semantic level vs d (trunc): r = {r_trunc:+.3f} (p = {p_trunc:.4f})\")\n",
    "print(f\"  Semantic level vs d (suffix): r = {r_suffix:+.3f} (p = {p_suffix:.4f})\")\n",
    "\n",
    "if both_positive and c1_sig and c6_sig:\n",
    "    print(f\"\\n  *** CONCLUSION: SEMANTIC CONTENT MATTERS ***\")\n",
    "    print(f\"  The semantic gradient is positive in BOTH truncated and suffix modes.\")\n",
    "    print(f\"  LLM > random is significant in BOTH modes.\")\n",
    "    print(f\"  This cannot be explained by structural artifacts alone.\")\n",
    "elif c2_sig and c7_sig:\n",
    "    print(f\"\\n  *** CONCLUSION: STATIC FACTUAL PHRASE IS SPECIAL ***\")\n",
    "    print(f\"  Static > random in both modes, but LLM may not beat random in suffix.\")\n",
    "else:\n",
    "    print(f\"\\n  CONCLUSION: MIXED EVIDENCE\")\n",
    "    print(f\"  The gradient is not consistently significant across modes.\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 11: Plots (2x3 grid)\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(20, 13))\n",
    "\n",
    "# Color scheme: truncated = blue shades, suffix = red shades\n",
    "trunc_color = '#2166ac'\n",
    "suffix_color = '#b2182b'\n",
    "\n",
    "# --- Plot 1: All conditions bar chart (sorted by d) ---\n",
    "ax = axes[0, 0]\n",
    "all_sorted = sorted(\n",
    "    [(cn, cohens_d(c['bare'] - c[cn])) for cn in CONDITION_NAMES if cn != 'bare'],\n",
    "    key=lambda x: x[1], reverse=True\n",
    ")\n",
    "names_sorted = [x[0] for x in all_sorted]\n",
    "ds_sorted = [x[1] for x in all_sorted]\n",
    "colors_bar = [trunc_color if '_trunc' in cn else suffix_color if '_suffix' in cn else 'gray'\n",
    "              for cn in names_sorted]\n",
    "ax.barh(range(len(names_sorted)), ds_sorted, color=colors_bar, edgecolor='black', linewidth=0.5)\n",
    "ax.set_yticks(range(len(names_sorted)))\n",
    "ax.set_yticklabels(names_sorted, fontsize=7)\n",
    "ax.axvline(x=0, color='gray', linestyle='--')\n",
    "ax.set_xlabel(\"Cohen's d vs Bare\")\n",
    "ax.set_title('All Conditions vs Bare')\n",
    "ax.invert_yaxis()\n",
    "\n",
    "# --- Plot 2: Semantic Gradient \u2014 Truncated Mode ---\n",
    "ax = axes[0, 1]\n",
    "x_pos = range(len(content_types))\n",
    "ax.bar(x_pos, trunc_ds, color=trunc_color, edgecolor='black', linewidth=0.5, alpha=0.8)\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels([ct.replace('_', '\\n') for ct in content_types], fontsize=7, rotation=45, ha='right')\n",
    "ax.axhline(y=0, color='gray', linestyle='--')\n",
    "ax.set_ylabel(\"Cohen's d vs Bare\")\n",
    "ax.set_title('Semantic Gradient \u2014 Truncated Prefix')\n",
    "for i, v in enumerate(trunc_ds):\n",
    "    ax.text(i, v + 0.005, f\"{v:+.3f}\", ha='center', va='bottom', fontsize=7)\n",
    "\n",
    "# --- Plot 3: Semantic Gradient \u2014 Suffix Mode ---\n",
    "ax = axes[0, 2]\n",
    "ax.bar(x_pos, suffix_ds, color=suffix_color, edgecolor='black', linewidth=0.5, alpha=0.8)\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels([ct.replace('_', '\\n') for ct in content_types], fontsize=7, rotation=45, ha='right')\n",
    "ax.axhline(y=0, color='gray', linestyle='--')\n",
    "ax.set_ylabel(\"Cohen's d vs Bare\")\n",
    "ax.set_title('Semantic Gradient \u2014 Suffix Mode')\n",
    "for i, v in enumerate(suffix_ds):\n",
    "    ax.text(i, v + 0.005, f\"{v:+.3f}\", ha='center', va='bottom', fontsize=7)\n",
    "\n",
    "# --- Plot 4: Overlay \u2014 Both modes on same plot ---\n",
    "ax = axes[1, 0]\n",
    "width = 0.35\n",
    "x = np.arange(len(content_types))\n",
    "bars1 = ax.bar(x - width/2, trunc_ds, width, color=trunc_color, alpha=0.8, label='Truncated', edgecolor='black', linewidth=0.5)\n",
    "bars2 = ax.bar(x + width/2, suffix_ds, width, color=suffix_color, alpha=0.8, label='Suffix', edgecolor='black', linewidth=0.5)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([ct.replace('_', '\\n') for ct in content_types], fontsize=7, rotation=45, ha='right')\n",
    "ax.axhline(y=0, color='gray', linestyle='--')\n",
    "ax.set_ylabel(\"Cohen's d vs Bare\")\n",
    "ax.set_title('Truncated vs Suffix \u2014 Same Content')\n",
    "ax.legend(fontsize=9)\n",
    "\n",
    "# --- Plot 5: Semantic level scatter with fit lines ---\n",
    "ax = axes[1, 1]\n",
    "ax.scatter(levels, trunc_ds, color=trunc_color, s=80, zorder=5, label='Truncated')\n",
    "ax.scatter(levels, suffix_ds, color=suffix_color, s=80, zorder=5, marker='s', label='Suffix')\n",
    "# Fit lines\n",
    "z_trunc = np.polyfit(levels, trunc_ds, 1)\n",
    "z_suffix = np.polyfit(levels, suffix_ds, 1)\n",
    "x_fit = np.linspace(0, 5, 50)\n",
    "ax.plot(x_fit, np.polyval(z_trunc, x_fit), '--', color=trunc_color, alpha=0.5,\n",
    "        label=f'Trunc fit (slope={z_trunc[0]:.3f})')\n",
    "ax.plot(x_fit, np.polyval(z_suffix, x_fit), '--', color=suffix_color, alpha=0.5,\n",
    "        label=f'Suffix fit (slope={z_suffix[0]:.3f})')\n",
    "for ct, lev, td, sd in zip(content_types, levels, trunc_ds, suffix_ds):\n",
    "    ax.annotate(ct.replace('_', '\\n'), (lev, td), textcoords=\"offset points\",\n",
    "                xytext=(5, 5), fontsize=6)\n",
    "ax.set_xlabel('Semantic Level')\n",
    "ax.set_ylabel(\"Cohen's d vs Bare\")\n",
    "ax.set_title('Semantic Level vs Effect Size')\n",
    "ax.legend(fontsize=7)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# --- Plot 6: Hardness \u00d7 condition heatmap ---\n",
    "ax = axes[1, 2]\n",
    "hm_conds = ['random_trunc', 'wrong_doc_llm_trunc', 'tfidf_kw_trunc',\n",
    "            'llm_kw_trunc', 'static_fact_trunc', 'oracle_kw_trunc',\n",
    "            'random_suffix', 'llm_kw_suffix', 'static_fact_suffix']\n",
    "hm_data = []\n",
    "for cname in hm_conds:\n",
    "    row = []\n",
    "    for q in range(5):\n",
    "        mask_q = quintiles == q\n",
    "        if np.sum(mask_q) < 10:\n",
    "            row.append(0)\n",
    "        else:\n",
    "            delta = bare_valid[mask_q] - c[cname][mask_q]\n",
    "            row.append(cohens_d(delta))\n",
    "    hm_data.append(row)\n",
    "hm_data = np.array(hm_data)\n",
    "im = ax.imshow(hm_data, cmap='RdBu_r', vmin=-0.5, vmax=0.7, aspect='auto')\n",
    "ax.set_xticks(range(5))\n",
    "ax.set_xticklabels(quintile_labels, fontsize=7)\n",
    "ax.set_yticks(range(len(hm_conds)))\n",
    "ax.set_yticklabels(hm_conds, fontsize=7)\n",
    "for i in range(len(hm_conds)):\n",
    "    for j in range(5):\n",
    "        ax.text(j, i, f\"{hm_data[i,j]:+.2f}\", ha='center', va='center', fontsize=6)\n",
    "plt.colorbar(im, ax=ax, label=\"Cohen's d vs bare\")\n",
    "ax.set_title('Hardness \u00d7 Condition')\n",
    "\n",
    "plt.suptitle('Exp 10: Semantic Content Gradient', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'analysis_plots.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"Plots saved to {RESULTS_DIR / 'analysis_plots.png'}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 12: Save comprehensive results JSON\n",
    "\n",
    "# Gradient summaries\n",
    "gradient_trunc = {}\n",
    "gradient_suffix = {}\n",
    "for ct, lev, td, sd in zip(content_types, levels, trunc_ds, suffix_ds):\n",
    "    gradient_trunc[ct] = {'semantic_level': lev, 'cohens_d': float(td)}\n",
    "    gradient_suffix[ct] = {'semantic_level': lev, 'cohens_d': float(sd)}\n",
    "\n",
    "final = {\n",
    "    'experiment': 'exp10_semantic_content_gradient',\n",
    "    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'config': {\n",
    "        'model_name': config.model_name,\n",
    "        'seed': SEED,\n",
    "        'n_eval': N,\n",
    "        'n_valid': n_valid,\n",
    "        'n_excluded': n_excluded,\n",
    "        'min_passage_words': config.min_passage_words,\n",
    "        'max_passage_words': config.max_passage_words,\n",
    "        'n_conditions': len(CONDITION_NAMES),\n",
    "        'n_comparisons': N_COMPARISONS,\n",
    "        'bonferroni_alpha': BONFERRONI_ALPHA,\n",
    "        'suffix_separator': SUFFIX_SEPARATOR,\n",
    "    },\n",
    "    'condition_names': CONDITION_NAMES,\n",
    "    'nll_summary': {\n",
    "        cname: {\n",
    "            'mean': float(np.mean(c[cname])),\n",
    "            'std': float(np.std(c[cname])),\n",
    "            'cohens_d_vs_bare': float(cohens_d(c['bare'] - c[cname])) if cname != 'bare' else 0.0,\n",
    "        }\n",
    "        for cname in CONDITION_NAMES\n",
    "    },\n",
    "    'gradient_summaries': {\n",
    "        'truncated': gradient_trunc,\n",
    "        'suffix': gradient_suffix,\n",
    "        'spearman_trunc': {'r': float(r_trunc), 'p': float(p_trunc)},\n",
    "        'spearman_suffix': {'r': float(r_suffix), 'p': float(p_suffix)},\n",
    "        'cross_mode_rank_agreement': {'r': float(r_cross), 'p': float(p_cross)},\n",
    "    },\n",
    "    'primary_comparisons': comparison_results,\n",
    "    'all_vs_bare': all_vs_bare,\n",
    "    'hardness_breakdown': hardness_breakdown,\n",
    "    'per_sample_results': results,\n",
    "}\n",
    "\n",
    "with open(FINAL_RESULTS_PATH, 'w') as f:\n",
    "    json.dump(final, f, indent=2)\n",
    "\n",
    "print(f\"Results saved to {FINAL_RESULTS_PATH}\")\n",
    "print(f\"File size: {FINAL_RESULTS_PATH.stat().st_size / 1024:.1f} KB\")\n",
    "print(\"\\nDone!\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 13: GPU cleanup\n",
    "import gc\n",
    "\n",
    "print(\"Cleaning up GPU memory...\")\n",
    "mem_before = torch.cuda.memory_allocated() / 1e9\n",
    "\n",
    "del model\n",
    "del tokenizer\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "mem_after = torch.cuda.memory_allocated() / 1e9\n",
    "print(f\"GPU memory: {mem_before:.2f} GB -> {mem_after:.2f} GB\")\n",
    "print(\"Cleanup complete.\")"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}