{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8600d247",
   "metadata": {},
   "source": [
    "# Exp 24: Gemma Layer-Selective Mechanism Deep Dive\n",
    "\n",
    "## Motivation\n",
    "\n",
    "Exp 21 confirmed that **layer-selective value contamination** (layers 0-15, cutoff=16) produces\n",
    "d=**+0.227** on Gemma 3 4B with MS MARCO, and is more length-robust than Mistral's full priming\n",
    "(still d=+0.173 at 512 tokens, vs Mistral dropping to +0.034). The layer boundary sweep confirmed\n",
    "cutoff=16 is optimal.\n",
    "\n",
    "**Four open questions:**\n",
    "\n",
    "### Q1: Which individual layers carry the signal?\n",
    "We know layers 0-15 collectively help and 16-33 collectively hurt, but which specific layers\n",
    "matter most? Is it a smooth gradient or concentrated in a few layers?\n",
    "\n",
    "### Q2: Does this generalize to SQuAD v2?\n",
    "MS MARCO-specific effects are a known risk. SQuAD v2 has short extractive QA passages\n",
    "(median 114 words, 92% under 200 words) — ideal for Gemma's sliding window constraint.\n",
    "\n",
    "### Q3: Does prefix content matter under layer selectivity?\n",
    "Exp 16 showed static_fact, random, and oracle all fail with full-cache replacement on Gemma.\n",
    "But with layer-selective values (the method that works), does prefix content make a difference?\n",
    "\n",
    "### Q4: What makes early-layer values different?\n",
    "L2 norms, cosine similarity between bare/primed values, and delta magnitudes per layer can\n",
    "reveal why early layers carry the useful signal.\n",
    "\n",
    "## Design\n",
    "\n",
    "| Part | Data | N | Conditions | Fwd/q | Score/q |\n",
    "|------|------|---|------------|-------|---------|\n",
    "| 1+4 | MS MARCO | 300 | 34 single-layer + bare + features | 2 | 35 |\n",
    "| 2 | SQuAD v2 | 400 | bare, values_all, values_cutoff_16 | 2 | 3 |\n",
    "| 3 | MS MARCO | 300 | bare + 3 prefix types at cutoff=16 | 4 | 4 |\n",
    "\n",
    "## Reference Values\n",
    "\n",
    "| Source | Condition | d |\n",
    "|--------|-----------|---|\n",
    "| Exp 19 | values_only (all 34 layers) | +0.056 |\n",
    "| Exp 19 | values_early_layers (0-16) | +0.211 |\n",
    "| Exp 21 | values_early_layers (0-15) @ original | +0.227 |\n",
    "| Exp 21 | cutoff sweep best (cutoff=16) | +0.161 |\n",
    "| Exp 16 | static_fact full-cache | -0.031 (ns) |\n",
    "| Exp 16 | random full-cache | -0.109 (***) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29f7670b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T11:49:41.051872Z",
     "iopub.status.busy": "2026-02-16T11:49:41.051084Z",
     "iopub.status.idle": "2026-02-16T11:49:44.397525Z",
     "shell.execute_reply": "2026-02-16T11:49:44.396379Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEED: 42\n",
      "Results directory: results/exp24\n",
      "CUDA available: True\n",
      "GPU: NVIDIA L4\n",
      "GPU memory: 23.6 GB\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Setup\n",
    "import os\n",
    "os.umask(0o000)\n",
    "\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import gc\n",
    "from pathlib import Path\n",
    "\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "RESULTS_DIR = Path(\"results/exp24\")\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CHECKPOINT_P1_PATH = RESULTS_DIR / \"checkpoint_part1.json\"\n",
    "CHECKPOINT_P2_PATH = RESULTS_DIR / \"checkpoint_part2.json\"\n",
    "CHECKPOINT_P3_PATH = RESULTS_DIR / \"checkpoint_part3.json\"\n",
    "FINAL_RESULTS_PATH = RESULTS_DIR / \"results.json\"\n",
    "CSV_P1_PATH = RESULTS_DIR / \"part1_layer_map.csv\"\n",
    "CSV_P2_PATH = RESULTS_DIR / \"part2_squad.csv\"\n",
    "CSV_P3_PATH = RESULTS_DIR / \"part3_prefix_content.csv\"\n",
    "CSV_P4_PATH = RESULTS_DIR / \"part4_value_features.csv\"\n",
    "\n",
    "print(f\"SEED: {SEED}\")\n",
    "print(f\"Results directory: {RESULTS_DIR}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a1bdae7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T11:49:44.401920Z",
     "iopub.status.busy": "2026-02-16T11:49:44.401110Z",
     "iopub.status.idle": "2026-02-16T11:49:59.864872Z",
     "shell.execute_reply": "2026-02-16T11:49:59.863888Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading google/gemma-3-4b-it (4-bit, bfloat16)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13596932bf3446e28f81aaf2e2494f07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/883 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model loaded successfully.\n",
      "  Model class: Gemma3ForConditionalGeneration\n",
      "  Text config class: Gemma3TextConfig\n",
      "  Hidden size: 2560\n",
      "  Num layers: 34\n",
      "  Num attention heads: 8\n",
      "  Num KV heads: 4\n",
      "  Head dim: 256\n",
      "  BOS token ID: 2\n",
      "  Unique RoPE thetas: [10000.0, 1000000.0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Cache key dtype: torch.bfloat16\n",
      "  Cache key shape: torch.Size([1, 4, 2, 256])  (batch, kv_heads, seq, head_dim)\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Load Gemma 3 4B via load_model()\n",
    "sys.path.insert(0, \".\")\n",
    "\n",
    "from lib.config import ExperimentConfig\n",
    "from lib.model_utils import load_model\n",
    "\n",
    "MODEL_NAME = \"google/gemma-3-4b-it\"\n",
    "\n",
    "exp_config = ExperimentConfig(\n",
    "    model_name=MODEL_NAME,\n",
    "    model_type=\"gemma3\",\n",
    "    compute_dtype=\"auto\",  # resolves to bfloat16 for gemma3\n",
    "    use_4bit=True,\n",
    "    num_samples=2000,\n",
    "    seed=SEED,\n",
    ")\n",
    "\n",
    "print(f\"Loading {MODEL_NAME} (4-bit, bfloat16)...\")\n",
    "model, tokenizer = load_model(exp_config)\n",
    "\n",
    "# Architecture diagnostics\n",
    "from lib.kv_cache import _get_text_config, _get_head_dim, _get_rope_theta_for_layer, _get_cache_keys, _ensure_dynamic_cache\n",
    "\n",
    "text_config = _get_text_config(model.config)\n",
    "NUM_LAYERS = text_config.num_hidden_layers\n",
    "print(f\"\\nModel loaded successfully.\")\n",
    "print(f\"  Model class: {type(model).__name__}\")\n",
    "print(f\"  Text config class: {type(text_config).__name__}\")\n",
    "print(f\"  Hidden size: {text_config.hidden_size}\")\n",
    "print(f\"  Num layers: {NUM_LAYERS}\")\n",
    "print(f\"  Num attention heads: {text_config.num_attention_heads}\")\n",
    "print(f\"  Num KV heads: {text_config.num_key_value_heads}\")\n",
    "print(f\"  Head dim: {_get_head_dim(model.config)}\")\n",
    "print(f\"  BOS token ID: {tokenizer.bos_token_id}\")\n",
    "\n",
    "# Per-layer RoPE diagnostics\n",
    "thetas = set()\n",
    "for layer_idx in range(NUM_LAYERS):\n",
    "    thetas.add(_get_rope_theta_for_layer(model.config, layer_idx))\n",
    "print(f\"  Unique RoPE thetas: {sorted(thetas)}\")\n",
    "\n",
    "# Verify dtype\n",
    "sample_ids = tokenizer(\"test\", return_tensors=\"pt\")['input_ids'].to(exp_config.device)\n",
    "with torch.no_grad():\n",
    "    out = model(sample_ids, use_cache=True)\n",
    "    cache_check = _ensure_dynamic_cache(out.past_key_values)\n",
    "    k0 = _get_cache_keys(cache_check, 0)\n",
    "    print(f\"  Cache key dtype: {k0.dtype}\")\n",
    "    print(f\"  Cache key shape: {k0.shape}  (batch, kv_heads, seq, head_dim)\")\n",
    "del out, sample_ids\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a9e1f01",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T11:49:59.868988Z",
     "iopub.status.busy": "2026-02-16T11:49:59.868104Z",
     "iopub.status.idle": "2026-02-16T11:49:59.878297Z",
     "shell.execute_reply": "2026-02-16T11:49:59.877376Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config ready\n",
      "  Model: google/gemma-3-4b-it\n",
      "  Num layers: 34\n",
      "  Part 1+4: N=300, individual layer map + features\n",
      "  Part 2: N=400, SQuAD v2 cross-dataset\n",
      "  Part 3: N=300, prefix content x layer selectivity (cutoff=16)\n",
      "  Static fact prefix: 'What are the key facts I need to know?'\n",
      "\n",
      "Reference values:\n",
      "  Exp 19 values_only_d: +0.056\n",
      "  Exp 19 values_early_layers_d: +0.211\n",
      "  Exp 21 values_early_layers_d: +0.227\n",
      "  Exp 21 cutoff_sweep_best_d: +0.161\n",
      "  Exp 16 sf_trunc_full_d: -0.031\n",
      "  Exp 16 random_trunc_full_d: -0.109\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Lib imports + templates + constants\n",
    "from lib.kv_cache import (\n",
    "    _get_cache_keys,\n",
    "    _get_cache_values,\n",
    "    _set_cache_keys,\n",
    "    _set_cache_values,\n",
    "    _ensure_dynamic_cache,\n",
    "    extract_and_truncate_cache_with_bos,\n",
    "    correct_rope_positions_with_bos,\n",
    "    score_answer_with_cache,\n",
    "    deepcopy_cache,\n",
    "    replace_values_at_layers,\n",
    "    build_hybrid_cache,\n",
    "    _get_text_config,\n",
    ")\n",
    "from lib.analysis import cohens_d\n",
    "from lib.data import count_words\n",
    "from scipy import stats\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Templates — bare text, no \"Document:\\n\" framing\n",
    "SURROGATE_PREFIX_TEMPLATE = \"{surrogate}\\n\"\n",
    "DOCUMENT_TEMPLATE = \"{document}\"\n",
    "QUERY_TEMPLATE = \"\\nQuery: {query}\\nAnswer:\"\n",
    "ANSWER_TEMPLATE = \" {answer}\"\n",
    "\n",
    "# Prefix text\n",
    "from lib.surrogate import STATIC_SURROGATE_QUERIES\n",
    "STATIC_FACT = STATIC_SURROGATE_QUERIES['static_factual']['query']\n",
    "\n",
    "# Experiment parameters\n",
    "N_PART1 = 300  # Part 1+4: individual layer map + features\n",
    "N_PART2 = 400  # Part 2: SQuAD v2\n",
    "N_PART3 = 300  # Part 3: prefix content (same queries as Part 1)\n",
    "MAX_PASSAGE_WORDS = 300\n",
    "CHECKPOINT_EVERY = 25\n",
    "CUTOFF = 16  # layers 0-15\n",
    "\n",
    "# Reference values\n",
    "EXP19_REF = {\n",
    "    'values_only_d': 0.056,\n",
    "    'values_early_layers_d': 0.211,  # layers 0-16 (17 layers)\n",
    "}\n",
    "EXP21_REF = {\n",
    "    'values_early_layers_d': 0.227,  # layers 0-15 (16 layers)\n",
    "    'cutoff_sweep_best_d': 0.161,    # cutoff=16 on N=200\n",
    "}\n",
    "EXP16_REF = {\n",
    "    'sf_trunc_full_d': -0.031,\n",
    "    'random_trunc_full_d': -0.109,\n",
    "}\n",
    "\n",
    "print(\"Config ready\")\n",
    "print(f\"  Model: {MODEL_NAME}\")\n",
    "print(f\"  Num layers: {NUM_LAYERS}\")\n",
    "print(f\"  Part 1+4: N={N_PART1}, individual layer map + features\")\n",
    "print(f\"  Part 2: N={N_PART2}, SQuAD v2 cross-dataset\")\n",
    "print(f\"  Part 3: N={N_PART3}, prefix content x layer selectivity (cutoff={CUTOFF})\")\n",
    "print(f\"  Static fact prefix: '{STATIC_FACT}'\")\n",
    "print(f\"\\nReference values:\")\n",
    "for ref_name, ref_dict in [('Exp 19', EXP19_REF), ('Exp 21', EXP21_REF), ('Exp 16', EXP16_REF)]:\n",
    "    for k, v in ref_dict.items():\n",
    "        print(f\"  {ref_name} {k}: {v:+.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9394641e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T11:49:59.881650Z",
     "iopub.status.busy": "2026-02-16T11:49:59.881136Z",
     "iopub.status.idle": "2026-02-16T11:50:04.348241Z",
     "shell.execute_reply": "2026-02-16T11:50:04.347401Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`trust_remote_code` is not supported anymore.\n",
      "Please check that the Hugging Face dataset 'microsoft/ms_marco' isn't based on a loading script and remove `trust_remote_code`.\n",
      "If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "LOADING MS MARCO v1.1 — POSITIVE PASSAGES ONLY\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total items in validation: 10047\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ca267cca6484481b0903a1f5d195bf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filtering MARCO:   0%|          | 0/10047 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Selected 300 MS MARCO queries with positive passages\n",
      "Word counts: mean=71, min=17, max=143\n",
      "Random passage pool: 81,430\n",
      "\n",
      "======================================================================\n",
      "LOADING SQuAD v2 — HAS-ANSWER, <=300 WORDS\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total items in SQuAD v2 validation: 11873\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SQuAD v2 samples with answers & <=300 words: 5836\n",
      "Selected 400 SQuAD v2 samples\n",
      "Word counts: mean=123, median=112, min=27, max=297\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 4: Load MS MARCO (positive, <=300w) + SQuAD v2 (has-answer, <=300w)\n",
    "from datasets import load_dataset\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"LOADING MS MARCO v1.1 — POSITIVE PASSAGES ONLY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "dataset_marco = load_dataset(\"microsoft/ms_marco\", \"v1.1\", split=\"validation\",\n",
    "                              trust_remote_code=True)\n",
    "print(f\"Total items in validation: {len(dataset_marco)}\")\n",
    "\n",
    "marco_queries = []\n",
    "random_passages = []  # non-eval passages for Part 3 random prefix\n",
    "eval_passage_set = set()\n",
    "np.random.seed(SEED)\n",
    "\n",
    "for item in tqdm(dataset_marco, desc=\"Filtering MARCO\"):\n",
    "    passages_info = item.get('passages', {})\n",
    "    passage_texts = passages_info.get('passage_text', [])\n",
    "    is_selected = passages_info.get('is_selected', [])\n",
    "    query = item.get('query', '')\n",
    "    answers = item.get('answers', [])\n",
    "    well_formed = item.get('wellFormedAnswers', [])\n",
    "\n",
    "    if not passage_texts or not query:\n",
    "        continue\n",
    "\n",
    "    # Get best answer\n",
    "    answer = None\n",
    "    if well_formed and len(well_formed) > 0 and well_formed[0] != '[]':\n",
    "        answer = well_formed[0]\n",
    "    elif answers and len(answers) > 0 and answers[0] != 'No Answer Present.':\n",
    "        answer = answers[0]\n",
    "    else:\n",
    "        # Collect non-eval passages for random pool\n",
    "        for p in passage_texts:\n",
    "            if count_words(p) <= MAX_PASSAGE_WORDS:\n",
    "                random_passages.append(p)\n",
    "        continue\n",
    "\n",
    "    # Find positive passage(s)\n",
    "    for i, (ptext, sel) in enumerate(zip(passage_texts, is_selected)):\n",
    "        if sel == 1 and count_words(ptext) <= MAX_PASSAGE_WORDS:\n",
    "            if len(marco_queries) < N_PART1 * 3:\n",
    "                marco_queries.append({\n",
    "                    'query': query,\n",
    "                    'answer': answer,\n",
    "                    'passage': ptext,\n",
    "                    'word_count': count_words(ptext),\n",
    "                })\n",
    "                eval_passage_set.add(ptext)\n",
    "                break\n",
    "\n",
    "    # Collect non-eval passages for random pool\n",
    "    for p in passage_texts:\n",
    "        if p not in eval_passage_set and count_words(p) <= MAX_PASSAGE_WORDS:\n",
    "            random_passages.append(p)\n",
    "\n",
    "np.random.shuffle(marco_queries)\n",
    "marco_queries = marco_queries[:N_PART1]\n",
    "N_MARCO = len(marco_queries)\n",
    "\n",
    "print(f\"\\nSelected {N_MARCO} MS MARCO queries with positive passages\")\n",
    "print(f\"Word counts: mean={np.mean([q['word_count'] for q in marco_queries]):.0f}, \"\n",
    "      f\"min={min(q['word_count'] for q in marco_queries)}, \"\n",
    "      f\"max={max(q['word_count'] for q in marco_queries)}\")\n",
    "print(f\"Random passage pool: {len(random_passages):,}\")\n",
    "\n",
    "del dataset_marco\n",
    "gc.collect()\n",
    "\n",
    "# === SQuAD v2 ===\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"LOADING SQuAD v2 — HAS-ANSWER, <=300 WORDS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "ds_squad = load_dataset(\"rajpurkar/squad_v2\", split=\"validation\")\n",
    "print(f\"Total items in SQuAD v2 validation: {len(ds_squad)}\")\n",
    "\n",
    "squad_samples = []\n",
    "for item in ds_squad:\n",
    "    if len(item['answers']['text']) > 0 and count_words(item['context']) <= MAX_PASSAGE_WORDS:\n",
    "        squad_samples.append({\n",
    "            'query': item['question'],\n",
    "            'answer': item['answers']['text'][0],\n",
    "            'passage': item['context'],\n",
    "            'word_count': count_words(item['context']),\n",
    "        })\n",
    "\n",
    "print(f\"SQuAD v2 samples with answers & <=300 words: {len(squad_samples)}\")\n",
    "\n",
    "np.random.shuffle(squad_samples)\n",
    "squad_samples = squad_samples[:N_PART2]\n",
    "N_SQUAD = len(squad_samples)\n",
    "\n",
    "print(f\"Selected {N_SQUAD} SQuAD v2 samples\")\n",
    "print(f\"Word counts: mean={np.mean([q['word_count'] for q in squad_samples]):.0f}, \"\n",
    "      f\"median={np.median([q['word_count'] for q in squad_samples]):.0f}, \"\n",
    "      f\"min={min(q['word_count'] for q in squad_samples)}, \"\n",
    "      f\"max={max(q['word_count'] for q in squad_samples)}\")\n",
    "\n",
    "del ds_squad\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e04b941",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T11:50:04.351871Z",
     "iopub.status.busy": "2026-02-16T11:50:04.351355Z",
     "iopub.status.idle": "2026-02-16T11:50:04.367415Z",
     "shell.execute_reply": "2026-02-16T11:50:04.366495Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PREFIX TOKENIZATION — GEMMA 3 4B\n",
      "======================================================================\n",
      "\n",
      "Static fact prefix: 'What are the key facts I need to know?'\n",
      "  Formatted: 'What are the key facts I need to know?'\n",
      "  Token length: 11\n",
      "\n",
      "Random prefix passages pre-selected: 300\n",
      "  Example (first 80 chars): 'A single-gene disorder is the result of a single mutated gene. Over 4000 human d...'\n",
      "\n",
      "BPE BOUNDARY CHECK (first passage):\n",
      "  static_fact: 106/106 tokens match (100.0%)\n",
      "\n",
      "======================================================================\n",
      "EXPERIMENTAL CONDITIONS\n",
      "======================================================================\n",
      "\n",
      "### Part 1+4: Individual Layer Contribution Map + Value Features ###\n",
      "  Data: 300 MS MARCO queries\n",
      "  Per query: 2 fwd passes (bare + sf_trunc primed)\n",
      "  Then for each of 34 layers individually:\n",
      "    - Replace ONLY that layer's values from primed into bare -> score\n",
      "    - Collect value features: L2 norms, cosine sim, delta norm\n",
      "  Output: 300 x 35 NLL scores + 300 x 34 x 4 features\n",
      "\n",
      "### Part 2: Cross-Dataset — SQuAD v2 ###\n",
      "  Data: 400 SQuAD v2 queries\n",
      "  Conditions: bare, values_all (34 layers), values_cutoff_16 (layers 0-15)\n",
      "  Per query: 2 fwd passes -> score 3 conditions\n",
      "\n",
      "### Part 3: Prefix Content x Layer Selectivity ###\n",
      "  Data: 300 MS MARCO queries (same as Part 1)\n",
      "  Prefix types: static_fact, random (random MARCO passage), oracle (query text)\n",
      "  All at cutoff=16 (layers 0-15)\n",
      "  Per query: 4 fwd passes (bare + 3 prefix types) -> truncate -> RoPE -> replace values -> score 4 conditions\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Tokenize prefixes (static_fact, random pool) + condition explanations\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PREFIX TOKENIZATION — GEMMA 3 4B\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Static fact prefix\n",
    "sf_str = SURROGATE_PREFIX_TEMPLATE.format(surrogate=STATIC_FACT)\n",
    "sf_ids = tokenizer(sf_str, return_tensors=\"pt\",\n",
    "                    add_special_tokens=False)['input_ids'].to(exp_config.device)\n",
    "\n",
    "print(f\"\\nStatic fact prefix: '{STATIC_FACT}'\")\n",
    "print(f\"  Formatted: '{sf_str.strip()}'\")\n",
    "print(f\"  Token length: {sf_ids.shape[1]}\")\n",
    "\n",
    "# Pre-select random passages for Part 3 (one per query)\n",
    "np.random.seed(SEED + 1)\n",
    "random_prefix_passages = []\n",
    "rand_idx_pool = np.random.permutation(len(random_passages))\n",
    "for i in range(N_PART3):\n",
    "    rp = random_passages[rand_idx_pool[i % len(rand_idx_pool)]]\n",
    "    random_prefix_passages.append(rp)\n",
    "print(f\"\\nRandom prefix passages pre-selected: {len(random_prefix_passages)}\")\n",
    "print(f\"  Example (first 80 chars): '{random_prefix_passages[0][:80]}...'\")\n",
    "\n",
    "# BPE boundary check\n",
    "print(\"\\nBPE BOUNDARY CHECK (first passage):\")\n",
    "example_doc = marco_queries[0]['passage']\n",
    "concat = sf_str + DOCUMENT_TEMPLATE.format(document=example_doc)\n",
    "concat_enc = tokenizer(concat, add_special_tokens=True)['input_ids']\n",
    "prefix_enc = tokenizer(sf_str, add_special_tokens=True)['input_ids']\n",
    "doc_ids_from_concat = concat_enc[len(prefix_enc):]\n",
    "bare_doc_enc = tokenizer(DOCUMENT_TEMPLATE.format(document=example_doc),\n",
    "                          add_special_tokens=False)['input_ids']\n",
    "match = sum(1 for a, b in zip(doc_ids_from_concat, bare_doc_enc) if a == b)\n",
    "total = max(len(bare_doc_enc), 1)\n",
    "print(f\"  static_fact: {match}/{total} tokens match ({100*match/total:.1f}%)\")\n",
    "\n",
    "# Condition explanations\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"EXPERIMENTAL CONDITIONS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\n### Part 1+4: Individual Layer Contribution Map + Value Features ###\")\n",
    "print(f\"  Data: {N_PART1} MS MARCO queries\")\n",
    "print(\"  Per query: 2 fwd passes (bare + sf_trunc primed)\")\n",
    "print(f\"  Then for each of {NUM_LAYERS} layers individually:\")\n",
    "print(\"    - Replace ONLY that layer's values from primed into bare -> score\")\n",
    "print(\"    - Collect value features: L2 norms, cosine sim, delta norm\")\n",
    "print(f\"  Output: {N_PART1} x {1 + NUM_LAYERS} NLL scores + {N_PART1} x {NUM_LAYERS} x 4 features\")\n",
    "\n",
    "print(f\"\\n### Part 2: Cross-Dataset — SQuAD v2 ###\")\n",
    "print(f\"  Data: {N_PART2} SQuAD v2 queries\")\n",
    "print(\"  Conditions: bare, values_all (34 layers), values_cutoff_16 (layers 0-15)\")\n",
    "print(\"  Per query: 2 fwd passes -> score 3 conditions\")\n",
    "\n",
    "print(f\"\\n### Part 3: Prefix Content x Layer Selectivity ###\")\n",
    "print(f\"  Data: {N_PART3} MS MARCO queries (same as Part 1)\")\n",
    "print(f\"  Prefix types: static_fact, random (random MARCO passage), oracle (query text)\")\n",
    "print(f\"  All at cutoff={CUTOFF} (layers 0-{CUTOFF-1})\")\n",
    "print(\"  Per query: 4 fwd passes (bare + 3 prefix types) -> truncate -> RoPE -> replace values -> score 4 conditions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33006385",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T11:50:04.370906Z",
     "iopub.status.busy": "2026-02-16T11:50:04.370634Z",
     "iopub.status.idle": "2026-02-16T11:50:04.431767Z",
     "shell.execute_reply": "2026-02-16T11:50:04.430875Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PART 1+4: INDIVIDUAL LAYER MAP + VALUE FEATURES (300 queries, 34 layers)\n",
      "======================================================================\n",
      "Resuming from checkpoint: 300/300\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d193cf270bc4aebbf7b88897bab07a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Part 1+4: 100%|##########| 300/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Part 1+4 complete: 300 queries in 0.0 min\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Part 1+4 — Individual Layer Map + Value Features (300 queries, 34 layers)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"PART 1+4: INDIVIDUAL LAYER MAP + VALUE FEATURES ({N_PART1} queries, {NUM_LAYERS} layers)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Checkpoint resume\n",
    "p1_results = []\n",
    "p4_features = []\n",
    "p1_start_idx = 0\n",
    "\n",
    "if CHECKPOINT_P1_PATH.exists():\n",
    "    with open(CHECKPOINT_P1_PATH, 'r') as f:\n",
    "        ckpt = json.load(f)\n",
    "    ckpt_queries = ckpt.get('query_texts', [])\n",
    "    current_queries = [q['query'] for q in marco_queries[:N_PART1]]\n",
    "    if ckpt_queries == current_queries:\n",
    "        p1_results = ckpt['results']\n",
    "        p4_features = ckpt.get('features', [])\n",
    "        p1_start_idx = len(p1_results)\n",
    "        print(f\"Resuming from checkpoint: {p1_start_idx}/{N_PART1}\")\n",
    "    else:\n",
    "        print(\"Checkpoint query mismatch. Starting fresh.\")\n",
    "else:\n",
    "    print(\"No checkpoint found. Starting fresh.\")\n",
    "\n",
    "t_start = time.time()\n",
    "\n",
    "for qidx in tqdm(range(p1_start_idx, N_PART1), initial=p1_start_idx, total=N_PART1,\n",
    "                  desc=\"Part 1+4\"):\n",
    "    qdata = marco_queries[qidx]\n",
    "    query_prompt = QUERY_TEMPLATE.format(query=qdata['query'])\n",
    "    answer_text = ANSWER_TEMPLATE.format(answer=qdata['answer'])\n",
    "    passage = qdata['passage']\n",
    "    document_text = DOCUMENT_TEMPLATE.format(document=passage)\n",
    "\n",
    "    # Matched tokenization\n",
    "    full_text = sf_str + document_text\n",
    "    full_enc = tokenizer(full_text, return_tensors=\"pt\",\n",
    "                          add_special_tokens=True, padding=False, truncation=False)\n",
    "    full_ids = full_enc['input_ids'].to(exp_config.device)\n",
    "\n",
    "    sf_prefix_enc = tokenizer(sf_str, return_tensors=\"pt\",\n",
    "                               add_special_tokens=True, padding=False, truncation=False)\n",
    "    sf_prefix_len_matched = sf_prefix_enc['input_ids'].shape[1]\n",
    "\n",
    "    bos_id = full_ids[:, :1]\n",
    "    doc_ids = full_ids[:, sf_prefix_len_matched:]\n",
    "    doc_len = doc_ids.shape[1]\n",
    "    context_len = 1 + doc_len\n",
    "\n",
    "    del full_enc, full_ids, sf_prefix_enc\n",
    "\n",
    "    # Forward pass 1: BARE\n",
    "    bare_input = torch.cat([bos_id, doc_ids], dim=1)\n",
    "    with torch.no_grad():\n",
    "        bare_out = model(input_ids=bare_input,\n",
    "                         attention_mask=torch.ones_like(bare_input),\n",
    "                         use_cache=True, return_dict=True)\n",
    "    bare_cache = _ensure_dynamic_cache(bare_out.past_key_values)\n",
    "    del bare_out\n",
    "\n",
    "    # Forward pass 2: PRIMED (static_fact)\n",
    "    primed_input = torch.cat([bos_id, sf_ids, doc_ids], dim=1)\n",
    "    with torch.no_grad():\n",
    "        primed_out = model(input_ids=primed_input,\n",
    "                           attention_mask=torch.ones_like(primed_input),\n",
    "                           use_cache=True, return_dict=True)\n",
    "    primed_full = _ensure_dynamic_cache(primed_out.past_key_values)\n",
    "    del primed_out\n",
    "\n",
    "    # Truncate + RoPE correct\n",
    "    trunc_raw = extract_and_truncate_cache_with_bos(primed_full, doc_len)\n",
    "    prefix_offset = sf_ids.shape[1]\n",
    "    del primed_full\n",
    "\n",
    "    sf_trunc_cache = deepcopy_cache(trunc_raw)\n",
    "    correct_rope_positions_with_bos(sf_trunc_cache, prefix_offset, model)\n",
    "    del trunc_raw\n",
    "\n",
    "    # Score bare\n",
    "    bare_nll = score_answer_with_cache(\n",
    "        deepcopy_cache(bare_cache), context_len,\n",
    "        query_prompt, answer_text, model, tokenizer, exp_config)\n",
    "\n",
    "    # Part 4: Value features (cheap tensor ops on existing caches)\n",
    "    query_features = []\n",
    "    for l in range(NUM_LAYERS):\n",
    "        bare_v = _get_cache_values(bare_cache, l)[:, :, 1:, :]  # skip BOS\n",
    "        primed_v = _get_cache_values(sf_trunc_cache, l)[:, :, 1:, :]\n",
    "\n",
    "        # L2 norms (mean over positions and heads)\n",
    "        bare_l2 = bare_v.float().norm(dim=-1).mean().item()\n",
    "        primed_l2 = primed_v.float().norm(dim=-1).mean().item()\n",
    "\n",
    "        # Delta\n",
    "        delta_v = (primed_v.float() - bare_v.float())\n",
    "        delta_norm = delta_v.norm(dim=-1).mean().item()\n",
    "\n",
    "        # Cosine similarity (flatten heads and positions, compute per-vector)\n",
    "        bare_flat = bare_v.float().reshape(-1, bare_v.shape[-1])\n",
    "        primed_flat = primed_v.float().reshape(-1, primed_v.shape[-1])\n",
    "        cos_sim = torch.nn.functional.cosine_similarity(bare_flat, primed_flat, dim=-1).mean().item()\n",
    "\n",
    "        query_features.append({\n",
    "            'layer': l,\n",
    "            'bare_l2': bare_l2,\n",
    "            'primed_l2': primed_l2,\n",
    "            'delta_norm': delta_norm,\n",
    "            'cosine_sim': cos_sim,\n",
    "        })\n",
    "\n",
    "    # Part 1: Individual layer scoring\n",
    "    layer_nlls = {}\n",
    "    for l in range(NUM_LAYERS):\n",
    "        vel_cache = replace_values_at_layers(bare_cache, sf_trunc_cache, [l])\n",
    "        vel_nll = score_answer_with_cache(\n",
    "            deepcopy_cache(vel_cache), context_len,\n",
    "            query_prompt, answer_text, model, tokenizer, exp_config)\n",
    "        layer_nlls[l] = vel_nll\n",
    "        del vel_cache\n",
    "\n",
    "    del bare_cache, sf_trunc_cache, bare_input, primed_input\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    p1_results.append({\n",
    "        'query_idx': qidx,\n",
    "        'query': qdata['query'],\n",
    "        'doc_len': doc_len,\n",
    "        'bare_nll': bare_nll,\n",
    "        'layer_nlls': layer_nlls,\n",
    "    })\n",
    "    p4_features.append({\n",
    "        'query_idx': qidx,\n",
    "        'features': query_features,\n",
    "    })\n",
    "\n",
    "    # Checkpoint\n",
    "    if (qidx + 1) % CHECKPOINT_EVERY == 0 or qidx == N_PART1 - 1:\n",
    "        ckpt_data = {\n",
    "            'results': p1_results,\n",
    "            'features': p4_features,\n",
    "            'query_texts': [q['query'] for q in marco_queries[:N_PART1]],\n",
    "            'completed': len(p1_results),\n",
    "            'total': N_PART1,\n",
    "            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        }\n",
    "        with open(CHECKPOINT_P1_PATH, 'w') as f:\n",
    "            json.dump(ckpt_data, f)\n",
    "        elapsed = time.time() - t_start\n",
    "        n_done = qidx - p1_start_idx + 1\n",
    "        rate = n_done / elapsed if elapsed > 0 else 0\n",
    "        remaining = (N_PART1 - qidx - 1) / rate if rate > 0 else 0\n",
    "        tqdm.write(f\"  Checkpoint {qidx+1}/{N_PART1} | {n_done} done in {elapsed/60:.1f}m | \"\n",
    "                   f\"ETA: {remaining/60:.1f} min\")\n",
    "\n",
    "elapsed_total = time.time() - t_start\n",
    "print(f\"\\nPart 1+4 complete: {len(p1_results)} queries in {elapsed_total/60:.1f} min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3b5526d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T11:50:04.440775Z",
     "iopub.status.busy": "2026-02-16T11:50:04.440506Z",
     "iopub.status.idle": "2026-02-16T11:50:04.472132Z",
     "shell.execute_reply": "2026-02-16T11:50:04.470958Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PART 2: CROSS-DATASET SQuAD v2 (400 queries, 3 conditions)\n",
      "======================================================================\n",
      "Resuming from checkpoint: 400/400\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b217f1e32b4d4afcacc61b2b704c9c4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Part 2: 100%|##########| 400/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Part 2 complete: 400 queries in 0.0 min\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Part 2 — Cross-Dataset SQuAD v2 (400 queries, 3 conditions)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"PART 2: CROSS-DATASET SQuAD v2 ({N_SQUAD} queries, 3 conditions)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Checkpoint resume\n",
    "p2_results = []\n",
    "p2_start_idx = 0\n",
    "\n",
    "if CHECKPOINT_P2_PATH.exists():\n",
    "    with open(CHECKPOINT_P2_PATH, 'r') as f:\n",
    "        ckpt = json.load(f)\n",
    "    ckpt_queries = ckpt.get('query_texts', [])\n",
    "    current_queries = [q['query'] for q in squad_samples[:N_SQUAD]]\n",
    "    if ckpt_queries == current_queries:\n",
    "        p2_results = ckpt['results']\n",
    "        p2_start_idx = len(p2_results)\n",
    "        print(f\"Resuming from checkpoint: {p2_start_idx}/{N_SQUAD}\")\n",
    "    else:\n",
    "        print(\"Checkpoint query mismatch. Starting fresh.\")\n",
    "else:\n",
    "    print(\"No checkpoint found. Starting fresh.\")\n",
    "\n",
    "layer_indices_all = list(range(NUM_LAYERS))\n",
    "layer_indices_cutoff = list(range(CUTOFF))\n",
    "\n",
    "t_start = time.time()\n",
    "\n",
    "for qidx in tqdm(range(p2_start_idx, N_SQUAD), initial=p2_start_idx, total=N_SQUAD,\n",
    "                  desc=\"Part 2\"):\n",
    "    qdata = squad_samples[qidx]\n",
    "    query_prompt = QUERY_TEMPLATE.format(query=qdata['query'])\n",
    "    answer_text = ANSWER_TEMPLATE.format(answer=qdata['answer'])\n",
    "    passage = qdata['passage']\n",
    "    document_text = DOCUMENT_TEMPLATE.format(document=passage)\n",
    "\n",
    "    # Matched tokenization\n",
    "    full_text = sf_str + document_text\n",
    "    full_enc = tokenizer(full_text, return_tensors=\"pt\",\n",
    "                          add_special_tokens=True, padding=False, truncation=False)\n",
    "    full_ids = full_enc['input_ids'].to(exp_config.device)\n",
    "\n",
    "    sf_prefix_enc = tokenizer(sf_str, return_tensors=\"pt\",\n",
    "                               add_special_tokens=True, padding=False, truncation=False)\n",
    "    sf_prefix_len_matched = sf_prefix_enc['input_ids'].shape[1]\n",
    "\n",
    "    bos_id = full_ids[:, :1]\n",
    "    doc_ids = full_ids[:, sf_prefix_len_matched:]\n",
    "    doc_len = doc_ids.shape[1]\n",
    "    context_len = 1 + doc_len\n",
    "\n",
    "    del full_enc, full_ids, sf_prefix_enc\n",
    "\n",
    "    # Forward pass 1: BARE\n",
    "    bare_input = torch.cat([bos_id, doc_ids], dim=1)\n",
    "    with torch.no_grad():\n",
    "        bare_out = model(input_ids=bare_input,\n",
    "                         attention_mask=torch.ones_like(bare_input),\n",
    "                         use_cache=True, return_dict=True)\n",
    "    bare_cache = _ensure_dynamic_cache(bare_out.past_key_values)\n",
    "    del bare_out\n",
    "\n",
    "    # Forward pass 2: PRIMED (static_fact)\n",
    "    primed_input = torch.cat([bos_id, sf_ids, doc_ids], dim=1)\n",
    "    with torch.no_grad():\n",
    "        primed_out = model(input_ids=primed_input,\n",
    "                           attention_mask=torch.ones_like(primed_input),\n",
    "                           use_cache=True, return_dict=True)\n",
    "    primed_full = _ensure_dynamic_cache(primed_out.past_key_values)\n",
    "    del primed_out\n",
    "\n",
    "    # Truncate + RoPE correct\n",
    "    trunc_raw = extract_and_truncate_cache_with_bos(primed_full, doc_len)\n",
    "    prefix_offset = sf_ids.shape[1]\n",
    "    del primed_full\n",
    "\n",
    "    sf_trunc_cache = deepcopy_cache(trunc_raw)\n",
    "    correct_rope_positions_with_bos(sf_trunc_cache, prefix_offset, model)\n",
    "    del trunc_raw\n",
    "\n",
    "    # Score bare\n",
    "    bare_nll = score_answer_with_cache(\n",
    "        deepcopy_cache(bare_cache), context_len,\n",
    "        query_prompt, answer_text, model, tokenizer, exp_config)\n",
    "\n",
    "    # values_all: bare keys + primed values at ALL layers\n",
    "    val_all_cache = build_hybrid_cache(\n",
    "        keys_source=bare_cache,\n",
    "        values_source=sf_trunc_cache,\n",
    "    )\n",
    "    val_all_nll = score_answer_with_cache(\n",
    "        deepcopy_cache(val_all_cache), context_len,\n",
    "        query_prompt, answer_text, model, tokenizer, exp_config)\n",
    "    del val_all_cache\n",
    "\n",
    "    # values_cutoff_16: bare keys + primed values at layers 0-15\n",
    "    val_cutoff_cache = replace_values_at_layers(\n",
    "        bare_cache, sf_trunc_cache, layer_indices_cutoff)\n",
    "    val_cutoff_nll = score_answer_with_cache(\n",
    "        deepcopy_cache(val_cutoff_cache), context_len,\n",
    "        query_prompt, answer_text, model, tokenizer, exp_config)\n",
    "    del val_cutoff_cache\n",
    "\n",
    "    del bare_cache, sf_trunc_cache, bare_input, primed_input\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    p2_results.append({\n",
    "        'query_idx': qidx,\n",
    "        'query': qdata['query'],\n",
    "        'doc_len': doc_len,\n",
    "        'word_count': qdata['word_count'],\n",
    "        'bare_nll': bare_nll,\n",
    "        'values_all_nll': val_all_nll,\n",
    "        'values_cutoff_16_nll': val_cutoff_nll,\n",
    "    })\n",
    "\n",
    "    # Checkpoint\n",
    "    if (qidx + 1) % CHECKPOINT_EVERY == 0 or qidx == N_SQUAD - 1:\n",
    "        ckpt_data = {\n",
    "            'results': p2_results,\n",
    "            'query_texts': [q['query'] for q in squad_samples[:N_SQUAD]],\n",
    "            'completed': len(p2_results),\n",
    "            'total': N_SQUAD,\n",
    "            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        }\n",
    "        with open(CHECKPOINT_P2_PATH, 'w') as f:\n",
    "            json.dump(ckpt_data, f)\n",
    "        elapsed = time.time() - t_start\n",
    "        n_done = qidx - p2_start_idx + 1\n",
    "        rate = n_done / elapsed if elapsed > 0 else 0\n",
    "        remaining = (N_SQUAD - qidx - 1) / rate if rate > 0 else 0\n",
    "        tqdm.write(f\"  Checkpoint {qidx+1}/{N_SQUAD} | {n_done} done in {elapsed/60:.1f}m | \"\n",
    "                   f\"ETA: {remaining/60:.1f} min\")\n",
    "\n",
    "elapsed_total = time.time() - t_start\n",
    "print(f\"\\nPart 2 complete: {len(p2_results)} queries in {elapsed_total/60:.1f} min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "66a45ca5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T11:50:04.480269Z",
     "iopub.status.busy": "2026-02-16T11:50:04.479996Z",
     "iopub.status.idle": "2026-02-16T12:01:25.172915Z",
     "shell.execute_reply": "2026-02-16T12:01:25.172009Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PART 3: PREFIX CONTENT x LAYER SELECTIVITY (300 queries, cutoff=16)\n",
      "======================================================================\n",
      "No checkpoint found. Starting fresh.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e09b51517a74d9ca49d27c88a77fa99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Part 3:   0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 25/300 | 25 done in 1.0m | ETA: 10.6 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 50/300 | 50 done in 1.9m | ETA: 9.6 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 75/300 | 75 done in 2.9m | ETA: 8.7 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 100/300 | 100 done in 3.8m | ETA: 7.7 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 125/300 | 125 done in 4.8m | ETA: 6.7 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 150/300 | 150 done in 5.7m | ETA: 5.7 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 175/300 | 175 done in 6.7m | ETA: 4.8 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 200/300 | 200 done in 7.6m | ETA: 3.8 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 225/300 | 225 done in 8.5m | ETA: 2.8 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 250/300 | 250 done in 9.5m | ETA: 1.9 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 275/300 | 275 done in 10.4m | ETA: 0.9 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 300/300 | 300 done in 11.3m | ETA: 0.0 min\n",
      "\n",
      "Part 3 complete: 300 queries in 11.3 min\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Part 3 — Prefix Content x Layer Selectivity (300 queries, 3 prefix types at cutoff=16)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"PART 3: PREFIX CONTENT x LAYER SELECTIVITY ({N_PART3} queries, cutoff={CUTOFF})\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Checkpoint resume\n",
    "p3_results = []\n",
    "p3_start_idx = 0\n",
    "\n",
    "if CHECKPOINT_P3_PATH.exists():\n",
    "    with open(CHECKPOINT_P3_PATH, 'r') as f:\n",
    "        ckpt = json.load(f)\n",
    "    ckpt_queries = ckpt.get('query_texts', [])\n",
    "    current_queries = [q['query'] for q in marco_queries[:N_PART3]]\n",
    "    if ckpt_queries == current_queries:\n",
    "        p3_results = ckpt['results']\n",
    "        p3_start_idx = len(p3_results)\n",
    "        print(f\"Resuming from checkpoint: {p3_start_idx}/{N_PART3}\")\n",
    "    else:\n",
    "        print(\"Checkpoint query mismatch. Starting fresh.\")\n",
    "else:\n",
    "    print(\"No checkpoint found. Starting fresh.\")\n",
    "\n",
    "layer_indices_cutoff = list(range(CUTOFF))\n",
    "\n",
    "t_start = time.time()\n",
    "\n",
    "for qidx in tqdm(range(p3_start_idx, N_PART3), initial=p3_start_idx, total=N_PART3,\n",
    "                  desc=\"Part 3\"):\n",
    "    qdata = marco_queries[qidx]\n",
    "    query_prompt = QUERY_TEMPLATE.format(query=qdata['query'])\n",
    "    answer_text = ANSWER_TEMPLATE.format(answer=qdata['answer'])\n",
    "    passage = qdata['passage']\n",
    "    document_text = DOCUMENT_TEMPLATE.format(document=passage)\n",
    "\n",
    "    # === Build 3 prefix strings ===\n",
    "    # 1. static_fact\n",
    "    prefix_sf = sf_str\n",
    "    # 2. random (random MS MARCO passage)\n",
    "    prefix_random = SURROGATE_PREFIX_TEMPLATE.format(surrogate=random_prefix_passages[qidx])\n",
    "    # 3. oracle (query text)\n",
    "    prefix_oracle = SURROGATE_PREFIX_TEMPLATE.format(surrogate=qdata['query'])\n",
    "\n",
    "    prefixes = {\n",
    "        'static_fact': prefix_sf,\n",
    "        'random': prefix_random,\n",
    "        'oracle': prefix_oracle,\n",
    "    }\n",
    "\n",
    "    # Matched tokenization for each prefix type\n",
    "    prefix_caches = {}\n",
    "    prefix_offsets = {}\n",
    "\n",
    "    for pname, pstr in prefixes.items():\n",
    "        full_text = pstr + document_text\n",
    "        full_enc = tokenizer(full_text, return_tensors=\"pt\",\n",
    "                              add_special_tokens=True, padding=False, truncation=False)\n",
    "        full_ids_p = full_enc['input_ids'].to(exp_config.device)\n",
    "\n",
    "        p_prefix_enc = tokenizer(pstr, return_tensors=\"pt\",\n",
    "                                  add_special_tokens=True, padding=False, truncation=False)\n",
    "        p_prefix_len = p_prefix_enc['input_ids'].shape[1]\n",
    "        p_ids_only = tokenizer(pstr, return_tensors=\"pt\",\n",
    "                                add_special_tokens=False)['input_ids']\n",
    "        p_offset = p_ids_only.shape[1]\n",
    "\n",
    "        bos_id = full_ids_p[:, :1]\n",
    "        doc_ids_p = full_ids_p[:, p_prefix_len:]\n",
    "        doc_len_p = doc_ids_p.shape[1]\n",
    "\n",
    "        # Forward pass: PRIMED\n",
    "        p_prefix_ids = tokenizer(pstr, return_tensors=\"pt\",\n",
    "                                  add_special_tokens=False)['input_ids'].to(exp_config.device)\n",
    "        primed_input_p = torch.cat([bos_id, p_prefix_ids, doc_ids_p], dim=1)\n",
    "        with torch.no_grad():\n",
    "            primed_out_p = model(input_ids=primed_input_p,\n",
    "                                 attention_mask=torch.ones_like(primed_input_p),\n",
    "                                 use_cache=True, return_dict=True)\n",
    "        primed_full_p = _ensure_dynamic_cache(primed_out_p.past_key_values)\n",
    "        del primed_out_p\n",
    "\n",
    "        # Truncate + RoPE correct\n",
    "        trunc_raw_p = extract_and_truncate_cache_with_bos(primed_full_p, doc_len_p)\n",
    "        del primed_full_p\n",
    "\n",
    "        trunc_cache_p = deepcopy_cache(trunc_raw_p)\n",
    "        correct_rope_positions_with_bos(trunc_cache_p, p_offset, model)\n",
    "        del trunc_raw_p\n",
    "\n",
    "        prefix_caches[pname] = trunc_cache_p\n",
    "        prefix_offsets[pname] = p_offset\n",
    "\n",
    "        del full_enc, full_ids_p, p_prefix_enc, p_ids_only, primed_input_p, p_prefix_ids\n",
    "\n",
    "    # Use static_fact's matched tokenization for bare cache\n",
    "    full_text = sf_str + document_text\n",
    "    full_enc = tokenizer(full_text, return_tensors=\"pt\",\n",
    "                          add_special_tokens=True, padding=False, truncation=False)\n",
    "    full_ids = full_enc['input_ids'].to(exp_config.device)\n",
    "    sf_prefix_enc = tokenizer(sf_str, return_tensors=\"pt\",\n",
    "                               add_special_tokens=True, padding=False, truncation=False)\n",
    "    sf_prefix_len_matched = sf_prefix_enc['input_ids'].shape[1]\n",
    "    bos_id = full_ids[:, :1]\n",
    "    doc_ids = full_ids[:, sf_prefix_len_matched:]\n",
    "    doc_len = doc_ids.shape[1]\n",
    "    context_len = 1 + doc_len\n",
    "    del full_enc, full_ids, sf_prefix_enc\n",
    "\n",
    "    # Forward pass: BARE\n",
    "    bare_input = torch.cat([bos_id, doc_ids], dim=1)\n",
    "    with torch.no_grad():\n",
    "        bare_out = model(input_ids=bare_input,\n",
    "                         attention_mask=torch.ones_like(bare_input),\n",
    "                         use_cache=True, return_dict=True)\n",
    "    bare_cache = _ensure_dynamic_cache(bare_out.past_key_values)\n",
    "    del bare_out\n",
    "\n",
    "    # Score bare\n",
    "    bare_nll = score_answer_with_cache(\n",
    "        deepcopy_cache(bare_cache), context_len,\n",
    "        query_prompt, answer_text, model, tokenizer, exp_config)\n",
    "\n",
    "    # Score each prefix type with layer-selective values\n",
    "    prefix_nlls = {}\n",
    "    for pname in prefixes:\n",
    "        vel_cache = replace_values_at_layers(\n",
    "            bare_cache, prefix_caches[pname], layer_indices_cutoff)\n",
    "        vel_nll = score_answer_with_cache(\n",
    "            deepcopy_cache(vel_cache), context_len,\n",
    "            query_prompt, answer_text, model, tokenizer, exp_config)\n",
    "        prefix_nlls[pname] = vel_nll\n",
    "        del vel_cache\n",
    "\n",
    "    del bare_cache, bare_input, prefix_caches\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    p3_results.append({\n",
    "        'query_idx': qidx,\n",
    "        'query': qdata['query'],\n",
    "        'doc_len': doc_len,\n",
    "        'bare_nll': bare_nll,\n",
    "        'static_fact_nll': prefix_nlls['static_fact'],\n",
    "        'random_nll': prefix_nlls['random'],\n",
    "        'oracle_nll': prefix_nlls['oracle'],\n",
    "    })\n",
    "\n",
    "    # Checkpoint\n",
    "    if (qidx + 1) % CHECKPOINT_EVERY == 0 or qidx == N_PART3 - 1:\n",
    "        ckpt_data = {\n",
    "            'results': p3_results,\n",
    "            'query_texts': [q['query'] for q in marco_queries[:N_PART3]],\n",
    "            'completed': len(p3_results),\n",
    "            'total': N_PART3,\n",
    "            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        }\n",
    "        with open(CHECKPOINT_P3_PATH, 'w') as f:\n",
    "            json.dump(ckpt_data, f)\n",
    "        elapsed = time.time() - t_start\n",
    "        n_done = qidx - p3_start_idx + 1\n",
    "        rate = n_done / elapsed if elapsed > 0 else 0\n",
    "        remaining = (N_PART3 - qidx - 1) / rate if rate > 0 else 0\n",
    "        tqdm.write(f\"  Checkpoint {qidx+1}/{N_PART3} | {n_done} done in {elapsed/60:.1f}m | \"\n",
    "                   f\"ETA: {remaining/60:.1f} min\")\n",
    "\n",
    "elapsed_total = time.time() - t_start\n",
    "print(f\"\\nPart 3 complete: {len(p3_results)} queries in {elapsed_total/60:.1f} min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "743c51c0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T12:01:25.178102Z",
     "iopub.status.busy": "2026-02-16T12:01:25.177382Z",
     "iopub.status.idle": "2026-02-16T12:01:25.290490Z",
     "shell.execute_reply": "2026-02-16T12:01:25.289519Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PART 1 ANALYSIS: INDIVIDUAL LAYER CONTRIBUTION MAP\n",
      "======================================================================\n",
      "\n",
      "Layer        Mean D        d    Win%            p   sig\n",
      "-------------------------------------------------------\n",
      "0           -0.0003   -0.062    5.0%     2.80e-01    ns\n",
      "1           +0.0001   +0.004   34.3%     9.48e-01    ns\n",
      "2           +0.0016   +0.081   39.7%     1.61e-01    ns\n",
      "3           +0.0005   +0.026   35.0%     6.52e-01    ns\n",
      "4           +0.0011   +0.044   37.0%     4.48e-01    ns\n",
      "5           +0.0001   +0.002   42.3%     9.71e-01    ns\n",
      "6           +0.0025   +0.060   45.0%     3.01e-01    ns\n",
      "7           +0.0008   +0.037   38.0%     5.25e-01    ns\n",
      "8           +0.0034   +0.099   42.3%     8.70e-02    ns\n",
      "9           +0.0012   +0.023   46.0%     6.88e-01    ns\n",
      "10          +0.0111   +0.207   50.7%     3.89e-04   ***\n",
      "11          -0.0090   -0.083   36.3%     1.50e-01    ns\n",
      "12          +0.0171   +0.198   50.3%     6.73e-04   ***\n",
      "13          -0.0022   -0.052   33.0%     3.67e-01    ns\n",
      "14          +0.0062   +0.197   44.7%     7.45e-04   ***\n",
      "15          +0.0099   +0.238   54.3%     4.81e-05   ***\n",
      "16          -0.0024   -0.060   39.3%     3.01e-01    ns\n",
      "17          -0.0009   -0.010   42.7%     8.62e-01    ns\n",
      "18          -0.0102   -0.196   39.3%     7.78e-04   ***\n",
      "19          +0.0049   +0.084   46.0%     1.47e-01    ns\n",
      "20          +0.0056   +0.195   44.3%     8.04e-04   ***\n",
      "21          -0.0051   -0.114   24.3%     4.95e-02     *\n",
      "22          -0.0038   -0.144   29.3%     1.34e-02     *\n",
      "23          -0.0063   -0.110   36.3%     5.74e-02    ns\n",
      "24          +0.0024   +0.067   36.0%     2.47e-01    ns\n",
      "25          +0.0031   +0.098   37.3%     9.20e-02    ns\n",
      "26          +0.0011   +0.051   37.3%     3.77e-01    ns\n",
      "27          -0.0069   -0.213   37.0%     2.65e-04   ***\n",
      "28          +0.0009   +0.026   47.3%     6.47e-01    ns\n",
      "29          -0.0013   -0.044   30.7%     4.49e-01    ns\n",
      "30          -0.0010   -0.025   42.0%     6.65e-01    ns\n",
      "31          +0.0031   +0.080   32.0%     1.65e-01    ns\n",
      "32          +0.0008   +0.030   30.0%     6.03e-01    ns\n",
      "33          +0.0018   +0.164   30.0%     4.82e-03    **\n",
      "\n",
      "Top 10 layers by Cohen's d:\n",
      "  #1: layer 15  d=+0.238  win=54%  ***\n",
      "  #2: layer 10  d=+0.207  win=51%  ***\n",
      "  #3: layer 12  d=+0.198  win=50%  ***\n",
      "  #4: layer 14  d=+0.197  win=45%  ***\n",
      "  #5: layer 20  d=+0.195  win=44%  ***\n",
      "  #6: layer 33  d=+0.164  win=30%  **\n",
      "  #7: layer  8  d=+0.099  win=42%  ns\n",
      "  #8: layer 25  d=+0.098  win=37%  ns\n",
      "  #9: layer 19  d=+0.084  win=46%  ns\n",
      "  #10: layer  2  d=+0.081  win=40%  ns\n",
      "\n",
      "Bottom 5 layers (most harmful):\n",
      "  #1: layer 23  d=-0.110  win=36%  ns\n",
      "  #2: layer 21  d=-0.114  win=24%  *\n",
      "  #3: layer 22  d=-0.144  win=29%  *\n",
      "  #4: layer 18  d=-0.196  win=39%  ***\n",
      "  #5: layer 27  d=-0.213  win=37%  ***\n",
      "\n",
      "Cum Layers      Layers Added        d\n",
      "----------------------------------------\n",
      "1            layer  15           +0.238\n",
      "2            layer  10           +0.324\n",
      "3            layer  12           +0.303\n",
      "4            layer  14           +0.340\n",
      "5            layer  20           +0.347\n",
      "6            layer  33           +0.356\n",
      "7            layer   8           +0.335\n",
      "8            layer  25           +0.351\n",
      "9            layer  19           +0.323\n",
      "10           layer   2           +0.312\n",
      "11           layer  31           +0.295\n",
      "12           layer  24           +0.289\n",
      "13           layer   6           +0.275\n",
      "14           layer  26           +0.277\n",
      "15           layer   4           +0.273\n",
      "16           layer   7           +0.271\n",
      "17           layer  32           +0.255\n",
      "18           layer  28           +0.251\n",
      "19           layer   3           +0.249\n",
      "20           layer   9           +0.258\n",
      "34           layer  27           +0.085\n",
      "\n",
      "Early layers (0-15): mean d = +0.064, positive = 13/16\n",
      "Late layers (16-33):  mean d = -0.007, positive = 9/18\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Part 1 Analysis — per-layer d table, cumulative d, critical layers\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PART 1 ANALYSIS: INDIVIDUAL LAYER CONTRIBUTION MAP\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Collect per-layer deltas\n",
    "layer_deltas = {l: [] for l in range(NUM_LAYERS)}\n",
    "bare_nlls_p1 = []\n",
    "\n",
    "for r in p1_results:\n",
    "    bare_nll = r['bare_nll']\n",
    "    bare_nlls_p1.append(bare_nll)\n",
    "    for l in range(NUM_LAYERS):\n",
    "        delta = bare_nll - r['layer_nlls'][str(l)]\n",
    "        layer_deltas[l].append(delta)\n",
    "\n",
    "# Per-layer Cohen's d\n",
    "print(f\"\\n{'Layer':<8} {'Mean D':>10} {'d':>8} {'Win%':>7} {'p':>12} {'sig':>5}\")\n",
    "print(\"-\" * 55)\n",
    "\n",
    "layer_analysis = {}\n",
    "for l in range(NUM_LAYERS):\n",
    "    delta = np.array(layer_deltas[l])\n",
    "    valid = np.isfinite(delta)\n",
    "    delta = delta[valid]\n",
    "    d = cohens_d(delta)\n",
    "    win = np.mean(delta > 0) * 100\n",
    "    t_stat, p_val = stats.ttest_1samp(delta, 0)\n",
    "    sig = '***' if p_val < 0.001 else '**' if p_val < 0.01 else '*' if p_val < 0.05 else 'ns'\n",
    "    print(f\"{l:<8} {np.mean(delta):>+10.4f} {d:>+8.3f} {win:>6.1f}% {p_val:>12.2e} {sig:>5}\")\n",
    "    layer_analysis[l] = {\n",
    "        'mean_delta': float(np.mean(delta)),\n",
    "        'cohens_d': float(d),\n",
    "        'win_pct': float(win),\n",
    "        't_stat': float(t_stat),\n",
    "        'p_value': float(p_val),\n",
    "    }\n",
    "\n",
    "# Rank layers by effect size\n",
    "layer_ranking = sorted(layer_analysis.items(), key=lambda x: x[1]['cohens_d'], reverse=True)\n",
    "print(f\"\\nTop 10 layers by Cohen's d:\")\n",
    "for rank, (l, a) in enumerate(layer_ranking[:10], 1):\n",
    "    sig = '***' if a['p_value'] < 0.001 else '**' if a['p_value'] < 0.01 else '*' if a['p_value'] < 0.05 else 'ns'\n",
    "    print(f\"  #{rank}: layer {l:>2}  d={a['cohens_d']:>+.3f}  win={a['win_pct']:.0f}%  {sig}\")\n",
    "\n",
    "print(f\"\\nBottom 5 layers (most harmful):\")\n",
    "for rank, (l, a) in enumerate(layer_ranking[-5:], 1):\n",
    "    sig = '***' if a['p_value'] < 0.001 else '**' if a['p_value'] < 0.01 else '*' if a['p_value'] < 0.05 else 'ns'\n",
    "    print(f\"  #{rank}: layer {l:>2}  d={a['cohens_d']:>+.3f}  win={a['win_pct']:.0f}%  {sig}\")\n",
    "\n",
    "# Cumulative d: add layers in order of individual d (greedy)\n",
    "print(f\"\\n{'Cum Layers':<12} {'Layers Added':>15} {'d':>8}\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "ranked_layers = [l for l, _ in layer_ranking]\n",
    "cum_deltas = np.zeros(len(p1_results))\n",
    "cum_analysis = []\n",
    "\n",
    "for step, l in enumerate(ranked_layers):\n",
    "    # Can't actually compute cumulative scoring without re-running model\n",
    "    # Instead, approximate: sum of individual deltas as proxy\n",
    "    cum_deltas = cum_deltas + np.array(layer_deltas[l])\n",
    "    cum_d = cohens_d(cum_deltas)\n",
    "    cum_analysis.append({\n",
    "        'n_layers': step + 1,\n",
    "        'layer_added': l,\n",
    "        'cumulative_d_approx': float(cum_d),\n",
    "    })\n",
    "    if step < 20 or step == NUM_LAYERS - 1:\n",
    "        print(f\"{step+1:<12} layer {l:>3}         {cum_d:>+8.3f}\")\n",
    "\n",
    "# Early vs late summary\n",
    "early_ds = [layer_analysis[l]['cohens_d'] for l in range(CUTOFF)]\n",
    "late_ds = [layer_analysis[l]['cohens_d'] for l in range(CUTOFF, NUM_LAYERS)]\n",
    "print(f\"\\nEarly layers (0-{CUTOFF-1}): mean d = {np.mean(early_ds):+.3f}, \"\n",
    "      f\"positive = {sum(1 for d in early_ds if d > 0)}/{len(early_ds)}\")\n",
    "print(f\"Late layers ({CUTOFF}-{NUM_LAYERS-1}):  mean d = {np.mean(late_ds):+.3f}, \"\n",
    "      f\"positive = {sum(1 for d in late_ds if d > 0)}/{len(late_ds)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "70e49c73",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T12:01:25.294580Z",
     "iopub.status.busy": "2026-02-16T12:01:25.294042Z",
     "iopub.status.idle": "2026-02-16T12:01:25.324138Z",
     "shell.execute_reply": "2026-02-16T12:01:25.323309Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PART 2 ANALYSIS: CROSS-DATASET (SQuAD v2)\n",
      "======================================================================\n",
      "  values_all           d=-0.093  win=27.7%  p=9.49e-02  ns\n",
      "  values_cutoff_16     d=-0.031  win=24.9%  p=5.77e-01  ns\n",
      "\n",
      "Cross-dataset comparison:\n",
      "  Dataset         Condition                   d    Win%            p\n",
      "  --------------------------------------------------------------------\n",
      "  MS MARCO        values_cutoff_16     (Exp 21):  +0.227            —\n",
      "  SQuAD v2        values_all             -0.093   27.7%     9.49e-02  ns\n",
      "  SQuAD v2        values_cutoff_16       -0.031   24.9%     5.77e-01  ns\n",
      "\n",
      "======================================================================\n",
      "PART 3 ANALYSIS: PREFIX CONTENT x LAYER SELECTIVITY\n",
      "======================================================================\n",
      "\n",
      "Prefix            Mean NLL  d vs bare    Win%            p   sig\n",
      "--------------------------------------------------------------\n",
      "static_fact         0.5237     +0.217   51.2%     5.48e-04   ***\n",
      "random              0.5150     +0.095   52.7%     1.26e-01    ns\n",
      "oracle              0.5030     +0.230   51.9%     2.60e-04   ***\n",
      "\n",
      "Pairwise prefix comparisons (at cutoff=16):\n",
      "  static_fact vs random: d=-0.021, p=7.32e-01 ns\n",
      "  static_fact vs oracle: d=-0.119, p=5.56e-02 ns\n",
      "  oracle vs random: d=+0.034, p=5.86e-01 ns\n",
      "\n",
      "Comparison with Exp 16 (full-cache replacement on Gemma):\n",
      "  Prefix           Full-cache d      VEL@16 d     Gain\n",
      "  -------------------------------------------------------\n",
      "  static_fact            -0.031        +0.217   +0.248\n",
      "  random                 -0.109        +0.095   +0.204\n"
     ]
    }
   ],
   "source": [
    "# Cell 10: Part 2+3 Analysis — cross-dataset + prefix content results\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PART 2 ANALYSIS: CROSS-DATASET (SQuAD v2)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# SQuAD v2 results\n",
    "p2_bare = np.array([r['bare_nll'] for r in p2_results])\n",
    "p2_val_all = np.array([r['values_all_nll'] for r in p2_results])\n",
    "p2_val_cutoff = np.array([r['values_cutoff_16_nll'] for r in p2_results])\n",
    "\n",
    "valid_p2 = (p2_bare != 0) & np.isfinite(p2_bare) & (p2_val_all != 0) & (p2_val_cutoff != 0)\n",
    "p2_b = p2_bare[valid_p2]\n",
    "p2_va = p2_val_all[valid_p2]\n",
    "p2_vc = p2_val_cutoff[valid_p2]\n",
    "\n",
    "p2_analysis = {}\n",
    "for name, arr in [('values_all', p2_va), ('values_cutoff_16', p2_vc)]:\n",
    "    delta = p2_b - arr\n",
    "    d = cohens_d(delta)\n",
    "    win = np.mean(delta > 0) * 100\n",
    "    t_stat, p_val = stats.ttest_1samp(delta, 0)\n",
    "    sig = '***' if p_val < 0.001 else '**' if p_val < 0.01 else '*' if p_val < 0.05 else 'ns'\n",
    "    p2_analysis[name] = {\n",
    "        'n_valid': int(np.sum(valid_p2)),\n",
    "        'mean_bare': float(np.mean(p2_b)),\n",
    "        'mean_nll': float(np.mean(arr)),\n",
    "        'mean_delta': float(np.mean(delta)),\n",
    "        'cohens_d': float(d),\n",
    "        'win_pct': float(win),\n",
    "        't_stat': float(t_stat),\n",
    "        'p_value': float(p_val),\n",
    "    }\n",
    "    print(f\"  {name:<20} d={d:>+.3f}  win={win:.1f}%  p={p_val:.2e}  {sig}\")\n",
    "\n",
    "# MS MARCO comparison (from Part 1: values at all layers vs cutoff)\n",
    "# We can compute values_all and values_cutoff from Part 1 individual layer results\n",
    "# by summing all layer deltas (approx)\n",
    "p1_bare_arr = np.array(bare_nlls_p1)\n",
    "print(f\"\\nCross-dataset comparison:\")\n",
    "print(f\"  {'Dataset':<15} {'Condition':<20} {'d':>8} {'Win%':>7} {'p':>12}\")\n",
    "print(\"  \" + \"-\" * 68)\n",
    "\n",
    "# Exp 21 MS MARCO reference for comparison\n",
    "print(f\"  {'MS MARCO':<15} {'values_cutoff_16':<20} {'(Exp 21):':>8} {'+0.227':>7} {'—':>12}\")\n",
    "for name, a in p2_analysis.items():\n",
    "    sig = '***' if a['p_value'] < 0.001 else '**' if a['p_value'] < 0.01 else '*' if a['p_value'] < 0.05 else 'ns'\n",
    "    print(f\"  {'SQuAD v2':<15} {name:<20} {a['cohens_d']:>+8.3f} {a['win_pct']:>6.1f}% {a['p_value']:>12.2e}  {sig}\")\n",
    "\n",
    "# Part 3 Analysis\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"PART 3 ANALYSIS: PREFIX CONTENT x LAYER SELECTIVITY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "p3_bare = np.array([r['bare_nll'] for r in p3_results])\n",
    "p3_sf = np.array([r['static_fact_nll'] for r in p3_results])\n",
    "p3_rand = np.array([r['random_nll'] for r in p3_results])\n",
    "p3_oracle = np.array([r['oracle_nll'] for r in p3_results])\n",
    "\n",
    "valid_p3 = (p3_bare != 0) & np.isfinite(p3_bare) & (p3_sf != 0) & (p3_rand != 0) & (p3_oracle != 0)\n",
    "\n",
    "p3_analysis = {}\n",
    "print(f\"\\n{'Prefix':<15} {'Mean NLL':>10} {'d vs bare':>10} {'Win%':>7} {'p':>12} {'sig':>5}\")\n",
    "print(\"-\" * 62)\n",
    "\n",
    "for pname, parr in [('static_fact', p3_sf), ('random', p3_rand), ('oracle', p3_oracle)]:\n",
    "    b = p3_bare[valid_p3]\n",
    "    a = parr[valid_p3]\n",
    "    delta = b - a\n",
    "    d = cohens_d(delta)\n",
    "    win = np.mean(delta > 0) * 100\n",
    "    t_stat, p_val = stats.ttest_1samp(delta, 0)\n",
    "    sig = '***' if p_val < 0.001 else '**' if p_val < 0.01 else '*' if p_val < 0.05 else 'ns'\n",
    "    print(f\"{pname:<15} {np.mean(a):>10.4f} {d:>+10.3f} {win:>6.1f}% {p_val:>12.2e} {sig:>5}\")\n",
    "    p3_analysis[pname] = {\n",
    "        'n_valid': int(np.sum(valid_p3)),\n",
    "        'mean_nll': float(np.mean(a)),\n",
    "        'mean_delta': float(np.mean(delta)),\n",
    "        'cohens_d': float(d),\n",
    "        'win_pct': float(win),\n",
    "        't_stat': float(t_stat),\n",
    "        'p_value': float(p_val),\n",
    "    }\n",
    "\n",
    "# Pairwise comparisons\n",
    "print(f\"\\nPairwise prefix comparisons (at cutoff={CUTOFF}):\")\n",
    "for n1, a1, n2, a2 in [\n",
    "    ('static_fact', p3_sf, 'random', p3_rand),\n",
    "    ('static_fact', p3_sf, 'oracle', p3_oracle),\n",
    "    ('oracle', p3_oracle, 'random', p3_rand),\n",
    "]:\n",
    "    delta = a2[valid_p3] - a1[valid_p3]  # positive = n1 better\n",
    "    d = cohens_d(delta)\n",
    "    t_stat, p_val = stats.ttest_1samp(delta, 0)\n",
    "    sig = '***' if p_val < 0.001 else '**' if p_val < 0.01 else '*' if p_val < 0.05 else 'ns'\n",
    "    print(f\"  {n1} vs {n2}: d={d:+.3f}, p={p_val:.2e} {sig}\")\n",
    "\n",
    "# Exp 16 comparison (full-cache)\n",
    "print(f\"\\nComparison with Exp 16 (full-cache replacement on Gemma):\")\n",
    "vel_header = f'VEL@{CUTOFF} d'\n",
    "print(f\"  {'Prefix':<15} {'Full-cache d':>13} {vel_header:>13} {'Gain':>8}\")\n",
    "print(\"  \" + \"-\" * 55)\n",
    "exp16_map = {'static_fact': -0.031, 'random': -0.109}\n",
    "for pname in ['static_fact', 'random']:\n",
    "    if pname in exp16_map:\n",
    "        full_d = exp16_map[pname]\n",
    "        vel_d = p3_analysis[pname]['cohens_d']\n",
    "        print(f\"  {pname:<15} {full_d:>+13.3f} {vel_d:>+13.3f} {vel_d - full_d:>+8.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b5f5798d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T12:01:25.327339Z",
     "iopub.status.busy": "2026-02-16T12:01:25.327025Z",
     "iopub.status.idle": "2026-02-16T12:01:25.358493Z",
     "shell.execute_reply": "2026-02-16T12:01:25.357604Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PART 4 ANALYSIS: VALUE FEATURE ANALYSIS\n",
      "======================================================================\n",
      "\n",
      "Layer       Bare L2  Primed L2   Delta Norm   Cosine Sim  d (Part1)\n",
      "--------------------------------------------------------------------\n",
      "0           227.020    227.020       0.0804       1.0000     -0.062\n",
      "1            34.563     34.266       2.0016       0.9956     +0.004\n",
      "2            44.963     44.895       3.4098       0.9941     +0.081\n",
      "3            34.844     34.757       2.8825       0.9934     +0.026\n",
      "4            36.606     36.367       3.9894       0.9871     +0.044\n",
      "5            22.004     21.759       3.1863       0.9811     +0.002\n",
      "6            21.245     21.313       2.9072       0.9800     +0.060\n",
      "7            18.804     18.840       2.7497       0.9794     +0.037\n",
      "8            15.594     15.650       3.0159       0.9664     +0.099\n",
      "9            30.409     18.447      15.9563       0.9577     +0.023\n",
      "10           11.277     10.955       2.9910       0.9490     +0.207\n",
      "11           19.273      9.544      12.0092       0.9476     -0.083\n",
      "12           19.541     11.918      10.7103       0.9400     +0.198\n",
      "13           19.880     15.728       8.1109       0.9427     -0.052\n",
      "14           12.856     12.027       4.5203       0.9272     +0.197\n",
      "15           12.329     10.229       5.1438       0.9255     +0.238\n",
      "16           13.730     11.020       5.8792       0.9318     -0.060\n",
      "17            6.945      5.356       2.9736       0.9444     -0.010\n",
      "18           21.945     14.716      10.6906       0.9495     -0.196\n",
      "19           11.975     10.351       4.2163       0.9437     +0.084\n",
      "20           23.407     20.095       8.5689       0.9430     +0.195\n",
      "21           23.185     18.677       8.3790       0.9579     -0.114\n",
      "22           20.523     18.701       6.0695       0.9554     -0.144\n",
      "23           17.841     15.766       4.8398       0.9675     -0.110\n",
      "24           23.013     21.577       5.9023       0.9577     +0.067\n",
      "25           40.763     37.799      10.6259       0.9610     +0.098\n",
      "26           15.331     13.914       3.9326       0.9642     +0.051\n",
      "27           16.639     14.866       4.5875       0.9637     -0.213\n",
      "28           21.193     18.875       5.9575       0.9632     +0.026\n",
      "29           10.745     10.142       2.3914       0.9680     -0.044\n",
      "30           19.464     18.858       3.9109       0.9686     -0.025\n",
      "31           39.298     37.767      10.5732       0.9508     +0.080\n",
      "32           12.952     12.728       2.7709       0.9647     +0.030\n",
      "33           14.720     14.722       3.4706       0.9558     +0.164\n",
      "\n",
      "Correlation: per-layer d vs delta_norm\n",
      "  Pearson r=-0.060, p=7.37e-01\n",
      "\n",
      "Correlation: per-layer d vs cosine_sim\n",
      "  Pearson r=-0.249, p=1.56e-01\n",
      "\n",
      "Early (0-15) vs Late (16-33) features:\n",
      "  Mean delta_norm: early=5.2291, late=5.8744, ratio=0.89\n",
      "  Mean cosine_sim: early=0.9667, late=0.9562\n"
     ]
    }
   ],
   "source": [
    "# Cell 11: Part 4 Analysis — Value features + correlation with Part 1 d\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PART 4 ANALYSIS: VALUE FEATURE ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Aggregate features per layer\n",
    "feat_agg = {l: {'bare_l2': [], 'primed_l2': [], 'delta_norm': [], 'cosine_sim': []}\n",
    "            for l in range(NUM_LAYERS)}\n",
    "\n",
    "for qf in p4_features:\n",
    "    for feat in qf['features']:\n",
    "        l = feat['layer']\n",
    "        feat_agg[l]['bare_l2'].append(feat['bare_l2'])\n",
    "        feat_agg[l]['primed_l2'].append(feat['primed_l2'])\n",
    "        feat_agg[l]['delta_norm'].append(feat['delta_norm'])\n",
    "        feat_agg[l]['cosine_sim'].append(feat['cosine_sim'])\n",
    "\n",
    "# Summary table\n",
    "print(f\"\\n{'Layer':<8} {'Bare L2':>10} {'Primed L2':>10} {'Delta Norm':>12} {'Cosine Sim':>12} {'d (Part1)':>10}\")\n",
    "print(\"-\" * 68)\n",
    "\n",
    "feat_summary = {}\n",
    "layer_ds = []\n",
    "delta_norms = []\n",
    "cosine_sims = []\n",
    "\n",
    "for l in range(NUM_LAYERS):\n",
    "    mean_bare_l2 = np.mean(feat_agg[l]['bare_l2'])\n",
    "    mean_primed_l2 = np.mean(feat_agg[l]['primed_l2'])\n",
    "    mean_delta_norm = np.mean(feat_agg[l]['delta_norm'])\n",
    "    mean_cosine_sim = np.mean(feat_agg[l]['cosine_sim'])\n",
    "    d_val = layer_analysis[l]['cohens_d']\n",
    "    print(f\"{l:<8} {mean_bare_l2:>10.3f} {mean_primed_l2:>10.3f} {mean_delta_norm:>12.4f} \"\n",
    "          f\"{mean_cosine_sim:>12.4f} {d_val:>+10.3f}\")\n",
    "    feat_summary[l] = {\n",
    "        'bare_l2': float(mean_bare_l2),\n",
    "        'primed_l2': float(mean_primed_l2),\n",
    "        'delta_norm': float(mean_delta_norm),\n",
    "        'cosine_sim': float(mean_cosine_sim),\n",
    "    }\n",
    "    layer_ds.append(d_val)\n",
    "    delta_norms.append(mean_delta_norm)\n",
    "    cosine_sims.append(mean_cosine_sim)\n",
    "\n",
    "layer_ds = np.array(layer_ds)\n",
    "delta_norms = np.array(delta_norms)\n",
    "cosine_sims = np.array(cosine_sims)\n",
    "\n",
    "# Correlations\n",
    "print(f\"\\nCorrelation: per-layer d vs delta_norm\")\n",
    "r_dn, p_dn = stats.pearsonr(layer_ds, delta_norms)\n",
    "print(f\"  Pearson r={r_dn:+.3f}, p={p_dn:.2e}\")\n",
    "\n",
    "print(f\"\\nCorrelation: per-layer d vs cosine_sim\")\n",
    "r_cs, p_cs = stats.pearsonr(layer_ds, cosine_sims)\n",
    "print(f\"  Pearson r={r_cs:+.3f}, p={p_cs:.2e}\")\n",
    "\n",
    "# Early vs late feature comparison\n",
    "print(f\"\\nEarly (0-{CUTOFF-1}) vs Late ({CUTOFF}-{NUM_LAYERS-1}) features:\")\n",
    "early_delta = np.mean([feat_summary[l]['delta_norm'] for l in range(CUTOFF)])\n",
    "late_delta = np.mean([feat_summary[l]['delta_norm'] for l in range(CUTOFF, NUM_LAYERS)])\n",
    "early_cos = np.mean([feat_summary[l]['cosine_sim'] for l in range(CUTOFF)])\n",
    "late_cos = np.mean([feat_summary[l]['cosine_sim'] for l in range(CUTOFF, NUM_LAYERS)])\n",
    "print(f\"  Mean delta_norm: early={early_delta:.4f}, late={late_delta:.4f}, ratio={early_delta/late_delta:.2f}\" if late_delta > 0 else f\"  Mean delta_norm: early={early_delta:.4f}, late={late_delta:.4f}\")\n",
    "print(f\"  Mean cosine_sim: early={early_cos:.4f}, late={late_cos:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ee7a7f02",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T12:01:25.362202Z",
     "iopub.status.busy": "2026-02-16T12:01:25.361909Z",
     "iopub.status.idle": "2026-02-16T12:01:28.070443Z",
     "shell.execute_reply": "2026-02-16T12:01:28.069468Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plots saved to results/exp24/analysis_plots.png\n"
     ]
    }
   ],
   "source": [
    "# Cell 12: Plots — 6-panel summary figure (2x3)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "\n",
    "# ---- Panel 1 (top-left): Per-layer d bar chart ----\n",
    "ax = axes[0, 0]\n",
    "ds_per_layer = [layer_analysis[l]['cohens_d'] for l in range(NUM_LAYERS)]\n",
    "colors_layer = []\n",
    "for l in range(NUM_LAYERS):\n",
    "    p_val = layer_analysis[l]['p_value']\n",
    "    d_val = layer_analysis[l]['cohens_d']\n",
    "    if p_val < 0.001:\n",
    "        colors_layer.append('#2ca02c' if d_val > 0 else '#d62728')\n",
    "    elif p_val < 0.05:\n",
    "        colors_layer.append('#98df8a' if d_val > 0 else '#ff9896')\n",
    "    else:\n",
    "        colors_layer.append('#cccccc')\n",
    "\n",
    "ax.bar(range(NUM_LAYERS), ds_per_layer, color=colors_layer, edgecolor='black', linewidth=0.3)\n",
    "ax.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "ax.axvline(x=CUTOFF - 0.5, color='red', linestyle='--', linewidth=1.5, alpha=0.7,\n",
    "           label=f'Cutoff={CUTOFF}')\n",
    "ax.set_xlabel('Layer')\n",
    "ax.set_ylabel(\"Cohen's d (single layer)\")\n",
    "ax.set_title(\"Part 1: Per-Layer Effect Size\")\n",
    "ax.legend(fontsize=8)\n",
    "\n",
    "# ---- Panel 2 (top-center): Cumulative d ----\n",
    "ax = axes[0, 1]\n",
    "cum_ds = [ca['cumulative_d_approx'] for ca in cum_analysis]\n",
    "cum_labels = [ca['layer_added'] for ca in cum_analysis]\n",
    "ax.plot(range(1, NUM_LAYERS + 1), cum_ds, marker='.', markersize=3, linewidth=1.5, color='#1f77b4')\n",
    "ax.axhline(y=EXP21_REF['values_early_layers_d'], color='#9467bd', linestyle='--', linewidth=1,\n",
    "           label=f\"Exp 21 VEL d={EXP21_REF['values_early_layers_d']:+.3f}\")\n",
    "ax.axhline(y=0, color='gray', linestyle='-', alpha=0.3)\n",
    "ax.axvline(x=CUTOFF, color='red', linestyle='--', linewidth=1, alpha=0.5)\n",
    "\n",
    "# Annotate peak\n",
    "peak_idx = int(np.argmax(cum_ds))\n",
    "ax.annotate(f\"Peak: {cum_ds[peak_idx]:+.3f}\\n({peak_idx+1} layers)\",\n",
    "            (peak_idx + 1, cum_ds[peak_idx]),\n",
    "            textcoords='offset points', xytext=(15, -10),\n",
    "            fontsize=8, arrowprops=dict(arrowstyle='->', color='black'))\n",
    "\n",
    "ax.set_xlabel('Number of layers added (ranked by d)')\n",
    "ax.set_ylabel(\"Cumulative d (approx)\")\n",
    "ax.set_title(\"Part 1: Cumulative d (greedy)\")\n",
    "ax.legend(fontsize=8)\n",
    "\n",
    "# ---- Panel 3 (top-right): Cosine similarity + delta norm by layer ----\n",
    "ax = axes[0, 2]\n",
    "ax2 = ax.twinx()\n",
    "\n",
    "x = range(NUM_LAYERS)\n",
    "ln1 = ax.plot(x, cosine_sims, 'b-', linewidth=1.5, label='Cosine sim')\n",
    "ln2 = ax2.plot(x, delta_norms, 'r-', linewidth=1.5, label='Delta norm')\n",
    "\n",
    "ax.set_xlabel('Layer')\n",
    "ax.set_ylabel('Cosine similarity', color='b')\n",
    "ax2.set_ylabel('Delta norm', color='r')\n",
    "ax.axvline(x=CUTOFF - 0.5, color='gray', linestyle='--', linewidth=1, alpha=0.5)\n",
    "ax.set_title(\"Part 4: Value Features by Layer\")\n",
    "\n",
    "lns = ln1 + ln2\n",
    "labs = [l.get_label() for l in lns]\n",
    "ax.legend(lns, labs, fontsize=8, loc='center right')\n",
    "\n",
    "# ---- Panel 4 (bottom-left): SQuAD v2 vs MS MARCO ----\n",
    "ax = axes[1, 0]\n",
    "\n",
    "# Grouped bars: dataset x condition\n",
    "conditions = ['values_all', 'values_cutoff_16']\n",
    "datasets = ['MS MARCO', 'SQuAD v2']\n",
    "\n",
    "# MS MARCO reference values\n",
    "marco_ds = [EXP19_REF['values_only_d'], EXP21_REF['values_early_layers_d']]\n",
    "squad_ds = [p2_analysis['values_all']['cohens_d'], p2_analysis['values_cutoff_16']['cohens_d']]\n",
    "\n",
    "x_pos = np.arange(len(conditions))\n",
    "w = 0.35\n",
    "bars1 = ax.bar(x_pos - w/2, marco_ds, w, color='#1f77b4', edgecolor='black', linewidth=0.5,\n",
    "               label='MS MARCO')\n",
    "bars2 = ax.bar(x_pos + w/2, squad_ds, w, color='#ff7f0e', edgecolor='black', linewidth=0.5,\n",
    "               label='SQuAD v2')\n",
    "\n",
    "ax.axhline(y=0, color='gray', linestyle='-', alpha=0.3)\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels(['values_all\\n(34 layers)', f'values_cutoff_{CUTOFF}\\n(layers 0-{CUTOFF-1})'])\n",
    "ax.set_ylabel(\"Cohen's d vs Bare\")\n",
    "ax.set_title(\"Part 2: Cross-Dataset Comparison\")\n",
    "ax.legend(fontsize=8)\n",
    "\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        h = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, h + 0.005 if h >= 0 else h - 0.015,\n",
    "                f\"{h:+.3f}\", ha='center', va='bottom' if h >= 0 else 'top', fontsize=8)\n",
    "\n",
    "# ---- Panel 5 (bottom-center): Prefix content bars ----\n",
    "ax = axes[1, 1]\n",
    "\n",
    "prefix_names = ['static_fact', 'random', 'oracle']\n",
    "prefix_ds = [p3_analysis[pn]['cohens_d'] for pn in prefix_names]\n",
    "prefix_colors = ['#2ca02c', '#d62728', '#9467bd']\n",
    "\n",
    "bars = ax.bar(range(len(prefix_names)), prefix_ds, color=prefix_colors,\n",
    "              edgecolor='black', linewidth=0.5)\n",
    "\n",
    "ax.axhline(y=0, color='gray', linestyle='-', alpha=0.3)\n",
    "ax.set_xticks(range(len(prefix_names)))\n",
    "ax.set_xticklabels(prefix_names)\n",
    "ax.set_ylabel(\"Cohen's d vs Bare\")\n",
    "ax.set_title(f\"Part 3: Prefix Content (cutoff={CUTOFF})\")\n",
    "\n",
    "for i, (d_val, pn) in enumerate(zip(prefix_ds, prefix_names)):\n",
    "    p_val = p3_analysis[pn]['p_value']\n",
    "    sig = '***' if p_val < 0.001 else '**' if p_val < 0.01 else '*' if p_val < 0.05 else 'ns'\n",
    "    ax.text(i, d_val + 0.005 if d_val >= 0 else d_val - 0.015,\n",
    "            f\"{d_val:+.3f} {sig}\", ha='center',\n",
    "            va='bottom' if d_val >= 0 else 'top', fontsize=9)\n",
    "\n",
    "# ---- Panel 6 (bottom-right): Scatter per-layer d vs delta norm ----\n",
    "ax = axes[1, 2]\n",
    "\n",
    "ax.scatter(delta_norms, layer_ds, c=range(NUM_LAYERS), cmap='viridis', s=60,\n",
    "           edgecolors='black', linewidths=0.5, zorder=3)\n",
    "\n",
    "# Color bar for layer index\n",
    "sm = plt.cm.ScalarMappable(cmap='viridis', norm=plt.Normalize(0, NUM_LAYERS - 1))\n",
    "sm.set_array([])\n",
    "cbar = fig.colorbar(sm, ax=ax, shrink=0.8, label='Layer index')\n",
    "\n",
    "# Fit line\n",
    "z = np.polyfit(delta_norms, layer_ds, 1)\n",
    "x_fit = np.linspace(delta_norms.min(), delta_norms.max(), 100)\n",
    "ax.plot(x_fit, np.polyval(z, x_fit), 'r--', linewidth=1.5, alpha=0.7)\n",
    "\n",
    "ax.set_xlabel('Mean delta norm (primed - bare values)')\n",
    "ax.set_ylabel(\"Cohen's d (single layer)\")\n",
    "ax.set_title(f\"Part 4: d vs Delta Norm (r={r_dn:+.3f})\")\n",
    "ax.axhline(y=0, color='gray', linestyle='-', alpha=0.3)\n",
    "\n",
    "# Annotate a few key layers\n",
    "for l in [0, CUTOFF-1, CUTOFF, NUM_LAYERS-1]:\n",
    "    ax.annotate(f\"L{l}\", (delta_norms[l], layer_ds[l]),\n",
    "                textcoords='offset points', xytext=(5, 5), fontsize=7)\n",
    "\n",
    "plt.suptitle('Exp 24: Gemma Layer-Selective Mechanism Deep Dive', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'analysis_plots.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"Plots saved to {RESULTS_DIR / 'analysis_plots.png'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "33fc2113",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T12:01:28.074604Z",
     "iopub.status.busy": "2026-02-16T12:01:28.074053Z",
     "iopub.status.idle": "2026-02-16T12:01:28.302066Z",
     "shell.execute_reply": "2026-02-16T12:01:28.300875Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part 1 CSV saved: results/exp24/part1_layer_map.csv\n",
      "Part 2 CSV saved: results/exp24/part2_squad.csv\n",
      "Part 3 CSV saved: results/exp24/part3_prefix_content.csv\n",
      "Part 4 CSV saved: results/exp24/part4_value_features.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results saved to results/exp24/results.json\n",
      "File size: 558.1 KB\n",
      "\n",
      "======================================================================\n",
      "SUMMARY — Exp 24: Gemma Layer-Selective Mechanism Deep Dive\n",
      "======================================================================\n",
      "Model: Gemma 3 4B (34 layers, head_dim=256, bfloat16)\n",
      "\n",
      "Part 1: Individual Layer Map (300 queries)\n",
      "  Top 5 layers: L15(d=+0.238), L10(d=+0.207), L12(d=+0.198), L14(d=+0.197), L20(d=+0.195)\n",
      "  Early (0-15) mean d: +0.064\n",
      "  Late (16-33) mean d: -0.007\n",
      "\n",
      "Part 2: SQuAD v2 Cross-Dataset (400 queries)\n",
      "  values_all: d=-0.093, win=28%, ns\n",
      "  values_cutoff_16: d=-0.031, win=25%, ns\n",
      "\n",
      "Part 3: Prefix Content (cutoff=16, 300 queries)\n",
      "  static_fact: d=+0.217, win=51%, ***\n",
      "  random: d=+0.095, win=53%, ns\n",
      "  oracle: d=+0.230, win=52%, ***\n",
      "\n",
      "Part 4: Value Features\n",
      "  d vs delta_norm: r=-0.060 (p=7.37e-01)\n",
      "  d vs cosine_sim: r=-0.249 (p=1.56e-01)\n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Cell 13: Save results.json + CSVs\n",
    "import csv\n",
    "\n",
    "# --- Part 1 CSV (per query x per layer) ---\n",
    "with open(CSV_P1_PATH, 'w', newline='') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=[\n",
    "        'query_idx', 'layer', 'bare_nll', 'layer_nll', 'delta_nll', 'cohens_d'])\n",
    "    writer.writeheader()\n",
    "    for r in p1_results:\n",
    "        for l in range(NUM_LAYERS):\n",
    "            layer_nll = r['layer_nlls'][str(l)]\n",
    "            writer.writerow({\n",
    "                'query_idx': r['query_idx'],\n",
    "                'layer': l,\n",
    "                'bare_nll': r['bare_nll'],\n",
    "                'layer_nll': layer_nll,\n",
    "                'delta_nll': r['bare_nll'] - layer_nll,\n",
    "                'cohens_d': layer_analysis[l]['cohens_d'],\n",
    "            })\n",
    "print(f\"Part 1 CSV saved: {CSV_P1_PATH}\")\n",
    "\n",
    "# --- Part 2 CSV ---\n",
    "with open(CSV_P2_PATH, 'w', newline='') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=[\n",
    "        'query_idx', 'word_count', 'bare_nll', 'values_all_nll', 'values_cutoff_16_nll'])\n",
    "    writer.writeheader()\n",
    "    for r in p2_results:\n",
    "        writer.writerow({\n",
    "            'query_idx': r['query_idx'],\n",
    "            'word_count': r['word_count'],\n",
    "            'bare_nll': r['bare_nll'],\n",
    "            'values_all_nll': r['values_all_nll'],\n",
    "            'values_cutoff_16_nll': r['values_cutoff_16_nll'],\n",
    "        })\n",
    "print(f\"Part 2 CSV saved: {CSV_P2_PATH}\")\n",
    "\n",
    "# --- Part 3 CSV ---\n",
    "with open(CSV_P3_PATH, 'w', newline='') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=[\n",
    "        'query_idx', 'bare_nll', 'static_fact_nll', 'random_nll', 'oracle_nll'])\n",
    "    writer.writeheader()\n",
    "    for r in p3_results:\n",
    "        writer.writerow({\n",
    "            'query_idx': r['query_idx'],\n",
    "            'bare_nll': r['bare_nll'],\n",
    "            'static_fact_nll': r['static_fact_nll'],\n",
    "            'random_nll': r['random_nll'],\n",
    "            'oracle_nll': r['oracle_nll'],\n",
    "        })\n",
    "print(f\"Part 3 CSV saved: {CSV_P3_PATH}\")\n",
    "\n",
    "# --- Part 4 CSV ---\n",
    "with open(CSV_P4_PATH, 'w', newline='') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=[\n",
    "        'query_idx', 'layer', 'bare_l2', 'primed_l2', 'delta_norm', 'cosine_sim'])\n",
    "    writer.writeheader()\n",
    "    for qf in p4_features:\n",
    "        for feat in qf['features']:\n",
    "            writer.writerow({\n",
    "                'query_idx': qf['query_idx'],\n",
    "                'layer': feat['layer'],\n",
    "                'bare_l2': feat['bare_l2'],\n",
    "                'primed_l2': feat['primed_l2'],\n",
    "                'delta_norm': feat['delta_norm'],\n",
    "                'cosine_sim': feat['cosine_sim'],\n",
    "            })\n",
    "print(f\"Part 4 CSV saved: {CSV_P4_PATH}\")\n",
    "\n",
    "# --- Combined results.json ---\n",
    "final = {\n",
    "    'experiment': 'exp24_gemma_layer_mechanism_deep_dive',\n",
    "    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'config': {\n",
    "        'model_name': MODEL_NAME,\n",
    "        'model_type': 'gemma3',\n",
    "        'seed': SEED,\n",
    "        'dataset_marco': 'MS MARCO v1.1 validation',\n",
    "        'dataset_squad': 'SQuAD v2 validation',\n",
    "        'max_passage_words': MAX_PASSAGE_WORDS,\n",
    "        'part1': {'n_queries': N_PART1, 'n_layers': NUM_LAYERS},\n",
    "        'part2': {'n_queries': N_SQUAD, 'conditions': ['bare', 'values_all', 'values_cutoff_16']},\n",
    "        'part3': {'n_queries': N_PART3, 'cutoff': CUTOFF, 'prefix_types': ['static_fact', 'random', 'oracle']},\n",
    "        'part4': {'n_queries': N_PART1, 'features': ['bare_l2', 'primed_l2', 'delta_norm', 'cosine_sim']},\n",
    "    },\n",
    "    'gemma_architecture': {\n",
    "        'hidden_size': text_config.hidden_size,\n",
    "        'num_layers': NUM_LAYERS,\n",
    "        'num_attention_heads': text_config.num_attention_heads,\n",
    "        'num_kv_heads': text_config.num_key_value_heads,\n",
    "        'head_dim': _get_head_dim(model.config),\n",
    "        'rope_thetas': sorted(list(thetas)),\n",
    "    },\n",
    "    'part1_layer_analysis': {str(k): v for k, v in layer_analysis.items()},\n",
    "    'part1_layer_ranking': [{'layer': l, **a} for l, a in layer_ranking],\n",
    "    'part1_cumulative': cum_analysis,\n",
    "    'part1_early_vs_late': {\n",
    "        'early_mean_d': float(np.mean(early_ds)),\n",
    "        'early_positive_count': sum(1 for d in early_ds if d > 0),\n",
    "        'late_mean_d': float(np.mean(late_ds)),\n",
    "        'late_positive_count': sum(1 for d in late_ds if d > 0),\n",
    "    },\n",
    "    'part2_analysis': p2_analysis,\n",
    "    'part3_analysis': p3_analysis,\n",
    "    'part4_feature_summary': {str(k): v for k, v in feat_summary.items()},\n",
    "    'part4_correlations': {\n",
    "        'd_vs_delta_norm': {'r': float(r_dn), 'p': float(p_dn)},\n",
    "        'd_vs_cosine_sim': {'r': float(r_cs), 'p': float(p_cs)},\n",
    "    },\n",
    "    'reference_values': {\n",
    "        'exp19_gemma': EXP19_REF,\n",
    "        'exp21_gemma': EXP21_REF,\n",
    "        'exp16_gemma': EXP16_REF,\n",
    "    },\n",
    "    'part1_per_query': p1_results,\n",
    "    'part2_per_query': p2_results,\n",
    "    'part3_per_query': p3_results,\n",
    "}\n",
    "\n",
    "with open(FINAL_RESULTS_PATH, 'w') as f:\n",
    "    json.dump(final, f, indent=2)\n",
    "\n",
    "print(f\"\\nResults saved to {FINAL_RESULTS_PATH}\")\n",
    "print(f\"File size: {FINAL_RESULTS_PATH.stat().st_size / 1024:.1f} KB\")\n",
    "\n",
    "# Final summary\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"SUMMARY — Exp 24: Gemma Layer-Selective Mechanism Deep Dive\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Model: Gemma 3 4B ({NUM_LAYERS} layers, head_dim={_get_head_dim(model.config)}, bfloat16)\")\n",
    "\n",
    "print(f\"\\nPart 1: Individual Layer Map ({N_PART1} queries)\")\n",
    "top5_str = ', '.join(f'L{l}(d={a[\"cohens_d\"]:+.3f})' for l, a in layer_ranking[:5])\n",
    "print(f\"  Top 5 layers: {top5_str}\")\n",
    "print(f\"  Early (0-{CUTOFF-1}) mean d: {np.mean(early_ds):+.3f}\")\n",
    "print(f\"  Late ({CUTOFF}-{NUM_LAYERS-1}) mean d: {np.mean(late_ds):+.3f}\")\n",
    "\n",
    "print(f\"\\nPart 2: SQuAD v2 Cross-Dataset ({N_SQUAD} queries)\")\n",
    "for name, a in p2_analysis.items():\n",
    "    sig = '***' if a['p_value'] < 0.001 else '**' if a['p_value'] < 0.01 else '*' if a['p_value'] < 0.05 else 'ns'\n",
    "    print(f\"  {name}: d={a['cohens_d']:+.3f}, win={a['win_pct']:.0f}%, {sig}\")\n",
    "\n",
    "print(f\"\\nPart 3: Prefix Content (cutoff={CUTOFF}, {N_PART3} queries)\")\n",
    "for pn in ['static_fact', 'random', 'oracle']:\n",
    "    a = p3_analysis[pn]\n",
    "    sig = '***' if a['p_value'] < 0.001 else '**' if a['p_value'] < 0.01 else '*' if a['p_value'] < 0.05 else 'ns'\n",
    "    print(f\"  {pn}: d={a['cohens_d']:+.3f}, win={a['win_pct']:.0f}%, {sig}\")\n",
    "\n",
    "print(f\"\\nPart 4: Value Features\")\n",
    "print(f\"  d vs delta_norm: r={r_dn:+.3f} (p={p_dn:.2e})\")\n",
    "print(f\"  d vs cosine_sim: r={r_cs:+.3f} (p={p_cs:.2e})\")\n",
    "\n",
    "print(f\"\\nDone!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "671029d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T12:01:28.306234Z",
     "iopub.status.busy": "2026-02-16T12:01:28.305399Z",
     "iopub.status.idle": "2026-02-16T12:01:29.067600Z",
     "shell.execute_reply": "2026-02-16T12:01:29.066693Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning up GPU memory...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory: 3.24 GB -> 0.01 GB\n",
      "Cleanup complete.\n"
     ]
    }
   ],
   "source": [
    "# Cell 14: GPU cleanup\n",
    "import gc\n",
    "\n",
    "print(\"Cleaning up GPU memory...\")\n",
    "mem_before = torch.cuda.memory_allocated() / 1e9\n",
    "\n",
    "del model\n",
    "del tokenizer\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "mem_after = torch.cuda.memory_allocated() / 1e9\n",
    "print(f\"GPU memory: {mem_before:.2f} GB -> {mem_after:.2f} GB\")\n",
    "print(\"Cleanup complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "03b85c77c2ca416c85fde5983c28fcb3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_5054a4786c96404fa56b1b007448ed34",
       "placeholder": "​",
       "style": "IPY_MODEL_a6619c3cf6b646efb79d901326752c54",
       "tabbable": null,
       "tooltip": null,
       "value": " 883/883 [00:03&lt;00:00, 639.46it/s, Materializing param=model.vision_tower.vision_model.post_layernorm.weight]"
      }
     },
     "08264e0ee329487a8037deb5d0a2fdf1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "08e3944c1edd471fac4ad5725242c10b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_17318de97af941d99f044c6a2a647d6e",
       "placeholder": "​",
       "style": "IPY_MODEL_ae11e6ac55a9432481c8b771fb069907",
       "tabbable": null,
       "tooltip": null,
       "value": " 300/300 [11:20&lt;00:00,  2.29s/it]"
      }
     },
     "0a761a91f7964476b8661d371835f632": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "10e16bd4bd0c4b3ab080e5fae38d0d7b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "13596932bf3446e28f81aaf2e2494f07": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_509cd162efc44adc9908d3b0fb41e9b4",
        "IPY_MODEL_4d368dad698642d6a950079ba04273d6",
        "IPY_MODEL_03b85c77c2ca416c85fde5983c28fcb3"
       ],
       "layout": "IPY_MODEL_b7c15008250649a1aed66b21a9222005",
       "tabbable": null,
       "tooltip": null
      }
     },
     "17318de97af941d99f044c6a2a647d6e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "1b29e1226e244a38a8ea66496e0e4093": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "1d193cf270bc4aebbf7b88897bab07a0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_f07707bd86294888b75db42c23138624",
        "IPY_MODEL_c0f12df84206438682fcab73048b78dd",
        "IPY_MODEL_9873dee7ba614be38b94ebfd494d04d4"
       ],
       "layout": "IPY_MODEL_d211d884d55745d2b5629b823c5c4a8d",
       "tabbable": null,
       "tooltip": null
      }
     },
     "28ddb9e2fa254e04a791cd201a7dde31": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2b71bab0737746bdb16fb4812eaedb33": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_50184d5be0234e9e98c7be4cdd9238c6",
       "placeholder": "​",
       "style": "IPY_MODEL_805cd3876ae04b7e867f2af6eaa4339f",
       "tabbable": null,
       "tooltip": null,
       "value": " 400/400 [00:00&lt;?, ?it/s]"
      }
     },
     "2be80c5637b8487bb8d23deca0e0aa54": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2e09b51517a74d9ca49d27c88a77fa99": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_c42aeb7b7b23415fae85b522325f49ff",
        "IPY_MODEL_5b03fc28e774472681b68bbcfb84d756",
        "IPY_MODEL_08e3944c1edd471fac4ad5725242c10b"
       ],
       "layout": "IPY_MODEL_1b29e1226e244a38a8ea66496e0e4093",
       "tabbable": null,
       "tooltip": null
      }
     },
     "2ed523f07cae4ecaadebc87cf9f11b88": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "3a7f3d55df3042ac99067f1f67d395b3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_10e16bd4bd0c4b3ab080e5fae38d0d7b",
       "placeholder": "​",
       "style": "IPY_MODEL_b0583819bf1640f2906c386707369f96",
       "tabbable": null,
       "tooltip": null,
       "value": " 10047/10047 [00:01&lt;00:00, 4914.86it/s]"
      }
     },
     "3dfa16e04c7a4966995f1c98eeb22deb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "4d368dad698642d6a950079ba04273d6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_dd51f960e94349448f46938cabdf0197",
       "max": 883.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_2ed523f07cae4ecaadebc87cf9f11b88",
       "tabbable": null,
       "tooltip": null,
       "value": 883.0
      }
     },
     "50184d5be0234e9e98c7be4cdd9238c6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "5054a4786c96404fa56b1b007448ed34": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "509cd162efc44adc9908d3b0fb41e9b4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_d3f1546e828948b68907d8f4dcdccd09",
       "placeholder": "​",
       "style": "IPY_MODEL_b7cec693ac674d11984830be93484ba1",
       "tabbable": null,
       "tooltip": null,
       "value": "Loading weights: 100%"
      }
     },
     "5b03fc28e774472681b68bbcfb84d756": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_0a761a91f7964476b8661d371835f632",
       "max": 300.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_df6ae7b9d08e4c24983cfd41d3091b82",
       "tabbable": null,
       "tooltip": null,
       "value": 300.0
      }
     },
     "5e68017686e8412f86408956f914fa6f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "63502895f08b424eaaa3af3ac0580055": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_d49f364731e54a1ca1c70e56efc8ff94",
       "max": 10047.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_bdb21564484b4871b4c36abbd54283dd",
       "tabbable": null,
       "tooltip": null,
       "value": 10047.0
      }
     },
     "63f740affbe84863bcf2c32cdc553690": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "6b85efb8791a4e02b765448088c09e5e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "6ee2d4280b83496a9ec7e7701993a4e5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "805cd3876ae04b7e867f2af6eaa4339f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "865d4e8e41b044f7918d0b8a35c67849": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "8cacc20cc39940449cb7f9ec002b1f67": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_fe9f1b15e43c45e4ab2d384368507201",
       "placeholder": "​",
       "style": "IPY_MODEL_63f740affbe84863bcf2c32cdc553690",
       "tabbable": null,
       "tooltip": null,
       "value": "Filtering MARCO: 100%"
      }
     },
     "9873dee7ba614be38b94ebfd494d04d4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_28ddb9e2fa254e04a791cd201a7dde31",
       "placeholder": "​",
       "style": "IPY_MODEL_08264e0ee329487a8037deb5d0a2fdf1",
       "tabbable": null,
       "tooltip": null,
       "value": " 300/300 [00:00&lt;?, ?it/s]"
      }
     },
     "9923f9e66aaf4891b9e33d705d8c121c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_ef8db065c19b41a4a37608e387691b0c",
       "placeholder": "​",
       "style": "IPY_MODEL_6b85efb8791a4e02b765448088c09e5e",
       "tabbable": null,
       "tooltip": null,
       "value": "Part 2: 100%"
      }
     },
     "9ca267cca6484481b0903a1f5d195bf2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_8cacc20cc39940449cb7f9ec002b1f67",
        "IPY_MODEL_63502895f08b424eaaa3af3ac0580055",
        "IPY_MODEL_3a7f3d55df3042ac99067f1f67d395b3"
       ],
       "layout": "IPY_MODEL_d35868a632dc4a4caddc870dc258d302",
       "tabbable": null,
       "tooltip": null
      }
     },
     "a6619c3cf6b646efb79d901326752c54": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "ae11e6ac55a9432481c8b771fb069907": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "af7c1894932747279304d12cb4025f51": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b0583819bf1640f2906c386707369f96": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "b217f1e32b4d4afcacc61b2b704c9c4c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_9923f9e66aaf4891b9e33d705d8c121c",
        "IPY_MODEL_ea40d7122f454eb6820e6aece9ae039d",
        "IPY_MODEL_2b71bab0737746bdb16fb4812eaedb33"
       ],
       "layout": "IPY_MODEL_f00c1ae6ca21480e9e644a4781dd4a65",
       "tabbable": null,
       "tooltip": null
      }
     },
     "b7c15008250649a1aed66b21a9222005": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b7cec693ac674d11984830be93484ba1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "bdb21564484b4871b4c36abbd54283dd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "c0f12df84206438682fcab73048b78dd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_2be80c5637b8487bb8d23deca0e0aa54",
       "max": 300.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_6ee2d4280b83496a9ec7e7701993a4e5",
       "tabbable": null,
       "tooltip": null,
       "value": 300.0
      }
     },
     "c42aeb7b7b23415fae85b522325f49ff": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_af7c1894932747279304d12cb4025f51",
       "placeholder": "​",
       "style": "IPY_MODEL_3dfa16e04c7a4966995f1c98eeb22deb",
       "tabbable": null,
       "tooltip": null,
       "value": "Part 3: 100%"
      }
     },
     "d211d884d55745d2b5629b823c5c4a8d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d35868a632dc4a4caddc870dc258d302": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d3f1546e828948b68907d8f4dcdccd09": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d49f364731e54a1ca1c70e56efc8ff94": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "dd51f960e94349448f46938cabdf0197": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "df6ae7b9d08e4c24983cfd41d3091b82": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "e2980084120e43d9a906aa78f3b6a467": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ea40d7122f454eb6820e6aece9ae039d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_f512c646305c4e81be1284920b935822",
       "max": 400.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_5e68017686e8412f86408956f914fa6f",
       "tabbable": null,
       "tooltip": null,
       "value": 400.0
      }
     },
     "ef8db065c19b41a4a37608e387691b0c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f00c1ae6ca21480e9e644a4781dd4a65": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f07707bd86294888b75db42c23138624": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_e2980084120e43d9a906aa78f3b6a467",
       "placeholder": "​",
       "style": "IPY_MODEL_865d4e8e41b044f7918d0b8a35c67849",
       "tabbable": null,
       "tooltip": null,
       "value": "Part 1+4: 100%"
      }
     },
     "f512c646305c4e81be1284920b935822": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "fe9f1b15e43c45e4ab2d384368507201": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
