{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {
    "papermill": {
     "duration": 0.004923,
     "end_time": "2026-02-16T21:42:47.977456",
     "exception": false,
     "start_time": "2026-02-16T21:42:47.972533",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Exp 28: Contrastive Ranking Soft Prefix\n",
    "\n",
    "## Motivation\n",
    "\n",
    "Exps 22-23 proved that NLL-trained priming cannot improve document ranking: value contamination\n",
    "from a document-independent prefix lowers NLL equally for relevant and irrelevant passages.\n",
    "But those experiments used either discrete prefixes (static_fact) or NLL-optimized soft prefixes.\n",
    "**What if we train the soft prefix with a ranking loss instead?**\n",
    "\n",
    "## Hypothesis\n",
    "\n",
    "Training the soft prefix with a hinge-based ranking loss can create differential value\n",
    "contamination — reducing NLL more for relevant passages than irrelevant ones. The prefix\n",
    "values might learn to \"amplify\" answer-predictive tokens more than other tokens.\n",
    "\n",
    "## Core Mechanism\n",
    "\n",
    "MS MARCO provides ~8-10 candidate passages per query with relevance labels. For each step:\n",
    "1. Pick 1 relevant + 1 irrelevant passage for the same query\n",
    "2. Score both through hybrid cache (same soft prefix)\n",
    "3. Hinge loss: `max(0, margin + NLL_relevant - NLL_irrelevant)`\n",
    "4. Gradient pushes prefix to make relevant passages predict the answer better\n",
    "\n",
    "## Why This Might Fail\n",
    "\n",
    "The soft prefix is the same for all documents. It produces identical value contamination\n",
    "regardless of document content. The contrastive gradient tells it \"help relevant passages more\"\n",
    "but it has no mechanism to distinguish relevant from irrelevant at cache-build time (it doesn't\n",
    "see the query). The only hope: the prefix values create an \"amplifier\" that happens to boost\n",
    "answer-predictive tokens more than other tokens. Theoretically possible, practically unlikely.\n",
    "\n",
    "## Success Criteria\n",
    "\n",
    "- **Primary**: Contrastive prefix AUC > 0.835 (bare=0.828) or PMI AUC > 0.845 (bare PMI=0.841)\n",
    "- **Secondary**: Still helps average NLL (d > 0 vs bare)\n",
    "- **Failure is informative**: Confirms value contamination from document-independent prefix\n",
    "  fundamentally cannot create ranking signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T21:42:47.987502Z",
     "iopub.status.busy": "2026-02-16T21:42:47.986896Z",
     "iopub.status.idle": "2026-02-16T21:42:52.438886Z",
     "shell.execute_reply": "2026-02-16T21:42:52.437971Z"
    },
    "papermill": {
     "duration": 4.458839,
     "end_time": "2026-02-16T21:42:52.440574",
     "exception": false,
     "start_time": "2026-02-16T21:42:47.981735",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEED: 42\n",
      "Results directory: results/exp28\n",
      "Exp 25 soft_prefix_fact: results/exp25/soft_prefix_fact.pt (exists: True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "GPU: NVIDIA A100-SXM4-40GB\n",
      "GPU memory: 42.3 GB\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Setup\n",
    "import os\n",
    "os.umask(0o000)\n",
    "\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import csv\n",
    "import numpy as np\n",
    "import torch\n",
    "import gc\n",
    "from pathlib import Path\n",
    "from scipy import stats\n",
    "\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "RESULTS_DIR = Path(\"results/exp28\")\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "EXP25_SOFT_FACT = Path(\"results/exp25/soft_prefix_fact.pt\")\n",
    "\n",
    "CHECKPOINT_TRAIN_WARM_PATH = RESULTS_DIR / \"checkpoint_train_warm.json\"\n",
    "CHECKPOINT_TRAIN_COLD_PATH = RESULTS_DIR / \"checkpoint_train_cold.json\"\n",
    "CHECKPOINT_EVAL_PATH = RESULTS_DIR / \"checkpoint_eval.json\"\n",
    "FINAL_RESULTS_PATH = RESULTS_DIR / \"results.json\"\n",
    "CSV_EVAL_PATH = RESULTS_DIR / \"passage_scores.csv\"\n",
    "SOFT_WARM_PATH = RESULTS_DIR / \"soft_prefix_contrastive_warm.pt\"\n",
    "SOFT_COLD_PATH = RESULTS_DIR / \"soft_prefix_contrastive_cold.pt\"\n",
    "\n",
    "print(f\"SEED: {SEED}\")\n",
    "print(f\"Results directory: {RESULTS_DIR}\")\n",
    "print(f\"Exp 25 soft_prefix_fact: {EXP25_SOFT_FACT} (exists: {EXP25_SOFT_FACT.exists()})\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T21:42:52.451348Z",
     "iopub.status.busy": "2026-02-16T21:42:52.450576Z",
     "iopub.status.idle": "2026-02-16T21:43:06.553842Z",
     "shell.execute_reply": "2026-02-16T21:43:06.552902Z"
    },
    "papermill": {
     "duration": 14.110478,
     "end_time": "2026-02-16T21:43:06.555575",
     "exception": false,
     "start_time": "2026-02-16T21:42:52.445097",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading google/gemma-3-4b-it (4-bit, bfloat16)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eed2b4d8d8b54f31ae58a4571b337cd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/883 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded.\n",
      "  Layers: 34, hidden: 2560, head_dim: 256\n",
      "  KV heads: 4\n",
      "  BOS token: 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Cache dtype: torch.bfloat16\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Load Gemma 3 4B\n",
    "\n",
    "sys.path.insert(0, \".\")\n",
    "\n",
    "from lib.config import ExperimentConfig\n",
    "from lib.model_utils import load_model\n",
    "\n",
    "MODEL_NAME = \"google/gemma-3-4b-it\"\n",
    "\n",
    "exp_config = ExperimentConfig(\n",
    "    model_name=MODEL_NAME,\n",
    "    model_type=\"gemma3\",\n",
    "    compute_dtype=\"auto\",\n",
    "    use_4bit=True,\n",
    "    num_samples=500,\n",
    "    seed=SEED,\n",
    ")\n",
    "\n",
    "print(f\"Loading {MODEL_NAME} (4-bit, bfloat16)...\")\n",
    "model, tokenizer = load_model(exp_config)\n",
    "\n",
    "from lib.kv_cache import (\n",
    "    _get_text_config, _get_head_dim,\n",
    "    _get_cache_keys, _get_cache_values,\n",
    "    _set_cache_keys, _set_cache_values,\n",
    "    _ensure_dynamic_cache,\n",
    "    extract_and_truncate_cache_with_bos,\n",
    "    correct_rope_positions_with_bos,\n",
    "    score_answer_with_cache,\n",
    "    deepcopy_cache,\n",
    "    replace_values_at_layers,\n",
    ")\n",
    "\n",
    "text_config = _get_text_config(model.config)\n",
    "NUM_LAYERS = text_config.num_hidden_layers\n",
    "HIDDEN_SIZE = text_config.hidden_size\n",
    "HEAD_DIM = _get_head_dim(model.config)\n",
    "\n",
    "print(f\"Model loaded.\")\n",
    "print(f\"  Layers: {NUM_LAYERS}, hidden: {HIDDEN_SIZE}, head_dim: {HEAD_DIM}\")\n",
    "print(f\"  KV heads: {text_config.num_key_value_heads}\")\n",
    "print(f\"  BOS token: {tokenizer.bos_token_id}\")\n",
    "\n",
    "# Verify cache dtype\n",
    "sample_ids = tokenizer(\"test\", return_tensors=\"pt\")['input_ids'].to(exp_config.device)\n",
    "with torch.no_grad():\n",
    "    out = model(sample_ids, use_cache=True)\n",
    "    cache_check = _ensure_dynamic_cache(out.past_key_values)\n",
    "    k0 = _get_cache_keys(cache_check, 0)\n",
    "    print(f\"  Cache dtype: {k0.dtype}\")\n",
    "del out, sample_ids, cache_check\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T21:43:06.567033Z",
     "iopub.status.busy": "2026-02-16T21:43:06.566058Z",
     "iopub.status.idle": "2026-02-16T21:43:06.575366Z",
     "shell.execute_reply": "2026-02-16T21:43:06.574684Z"
    },
    "papermill": {
     "duration": 0.016355,
     "end_time": "2026-02-16T21:43:06.576873",
     "exception": false,
     "start_time": "2026-02-16T21:43:06.560518",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config:\n",
      "  MARGIN=0.1, LR=0.05, N_EPOCHS=5, GRAD_ACCUM=4\n",
      "  N_TRAIN=500 queries, N_EVAL=200 queries\n",
      "  CUTOFF=16 (layers 0-15)\n",
      "  PREFIX_LEN=11 tokens\n",
      "  Trainable params: 28,160\n",
      "\n",
      "Reference (Exp 22):\n",
      "  raw_bare_auc: 0.828\n",
      "  raw_primed_auc: 0.829\n",
      "  pmi_bare_auc: 0.841\n",
      "  pmi_primed_auc: 0.832\n",
      "  raw_bare_mrr: 0.860\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Constants\n",
    "\n",
    "from lib.analysis import cohens_d\n",
    "from lib.data import count_words\n",
    "from lib.surrogate import STATIC_SURROGATE_QUERIES\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Templates (same as Exp 25)\n",
    "STATIC_FACT = STATIC_SURROGATE_QUERIES['static_factual']['query']\n",
    "SURROGATE_PREFIX_TEMPLATE = \"{surrogate}\\n\"\n",
    "DOCUMENT_TEMPLATE = \"{document}\"\n",
    "QUERY_TEMPLATE = \"\\nQuery: {query}\\nAnswer:\"\n",
    "ANSWER_TEMPLATE = \" {answer}\"\n",
    "\n",
    "# Architecture\n",
    "CUTOFF = 16  # layers 0-15\n",
    "MAX_PASSAGE_WORDS = 300\n",
    "\n",
    "# Training hyperparameters\n",
    "MARGIN = 0.1        # Hinge loss margin\n",
    "LR = 0.05           # Lower than Exp 25 (0.1) for warm-start stability\n",
    "N_EPOCHS = 5\n",
    "GRAD_ACCUM = 4\n",
    "WARMUP_STEPS = 30\n",
    "N_TRAIN = 500       # queries (each with ~8 passages)\n",
    "N_EVAL = 200        # queries for ranking eval\n",
    "CHECKPOINT_EVERY = 50\n",
    "\n",
    "# Tokenize static fact prefix for matched tokenization reference\n",
    "sf_str = SURROGATE_PREFIX_TEMPLATE.format(surrogate=STATIC_FACT)\n",
    "sf_ids = tokenizer(sf_str, return_tensors=\"pt\",\n",
    "                    add_special_tokens=False)['input_ids'].to(exp_config.device)\n",
    "PREFIX_LEN = sf_ids.shape[1]\n",
    "\n",
    "# Get embedding layer\n",
    "embed_fn = model.get_input_embeddings()\n",
    "\n",
    "# Reference values from Exp 22\n",
    "EXP22_REF = {\n",
    "    'raw_bare_auc': 0.828,\n",
    "    'raw_primed_auc': 0.829,\n",
    "    'pmi_bare_auc': 0.841,\n",
    "    'pmi_primed_auc': 0.832,\n",
    "    'raw_bare_mrr': 0.860,\n",
    "}\n",
    "\n",
    "print(\"Config:\")\n",
    "print(f\"  MARGIN={MARGIN}, LR={LR}, N_EPOCHS={N_EPOCHS}, GRAD_ACCUM={GRAD_ACCUM}\")\n",
    "print(f\"  N_TRAIN={N_TRAIN} queries, N_EVAL={N_EVAL} queries\")\n",
    "print(f\"  CUTOFF={CUTOFF} (layers 0-{CUTOFF-1})\")\n",
    "print(f\"  PREFIX_LEN={PREFIX_LEN} tokens\")\n",
    "print(f\"  Trainable params: {PREFIX_LEN * HIDDEN_SIZE:,}\")\n",
    "print(f\"\\nReference (Exp 22):\")\n",
    "for k, v in EXP22_REF.items():\n",
    "    print(f\"  {k}: {v:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T21:43:06.587499Z",
     "iopub.status.busy": "2026-02-16T21:43:06.586967Z",
     "iopub.status.idle": "2026-02-16T21:43:07.871554Z",
     "shell.execute_reply": "2026-02-16T21:43:07.870652Z"
    },
    "papermill": {
     "duration": 1.292513,
     "end_time": "2026-02-16T21:43:07.873980",
     "exception": false,
     "start_time": "2026-02-16T21:43:06.581467",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "LOADING MS MARCO v1.1 TRAIN — MULTI-PASSAGE FORMAT\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total items: 82326\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08a2e54bea3f47c299b6f7b256a6514a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filtering train:   0%|          | 0/82326 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Selected 500 training queries (4116 passages)\n",
      "Passages per query: mean=8.2, min=3, max=10\n",
      "Relevant: 559 (13.6%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 4: Load MS MARCO train — multi-passage format\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"LOADING MS MARCO v1.1 TRAIN — MULTI-PASSAGE FORMAT\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "dataset = load_dataset(\"microsoft/ms_marco\", \"v1.1\", split=\"train\")\n",
    "print(f\"Total items: {len(dataset)}\")\n",
    "\n",
    "train_queries = []\n",
    "np.random.seed(SEED)\n",
    "\n",
    "for item in tqdm(dataset, desc=\"Filtering train\"):\n",
    "    passages_info = item.get('passages', {})\n",
    "    passage_texts = passages_info.get('passage_text', [])\n",
    "    is_selected = passages_info.get('is_selected', [])\n",
    "    query = item.get('query', '')\n",
    "    answers = item.get('answers', [])\n",
    "    well_formed = item.get('wellFormedAnswers', [])\n",
    "\n",
    "    if not passage_texts or not query:\n",
    "        continue\n",
    "    if not is_selected or sum(is_selected) == 0:\n",
    "        continue\n",
    "\n",
    "    # Check word counts\n",
    "    word_counts = [count_words(p) for p in passage_texts]\n",
    "    if any(wc > MAX_PASSAGE_WORDS for wc in word_counts):\n",
    "        continue\n",
    "\n",
    "    # Require answer\n",
    "    answer = None\n",
    "    if well_formed and len(well_formed) > 0 and well_formed[0] != '[]':\n",
    "        answer = well_formed[0]\n",
    "    elif answers and len(answers) > 0 and answers[0] != 'No Answer Present.':\n",
    "        answer = answers[0]\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "    # Need at least 1 relevant and 1 irrelevant\n",
    "    n_rel = sum(1 for s in is_selected if s == 1)\n",
    "    n_irr = len(is_selected) - n_rel\n",
    "    if n_rel == 0 or n_irr == 0:\n",
    "        continue\n",
    "\n",
    "    passage_list = []\n",
    "    for i, (ptext, sel) in enumerate(zip(passage_texts, is_selected)):\n",
    "        passage_list.append({\n",
    "            'passage': ptext,\n",
    "            'is_relevant': bool(sel == 1),\n",
    "            'word_count': word_counts[i],\n",
    "            'passage_idx': i,\n",
    "        })\n",
    "\n",
    "    train_queries.append({\n",
    "        'query': query,\n",
    "        'answer': answer,\n",
    "        'passages': passage_list,\n",
    "        'n_passages': len(passage_list),\n",
    "        'n_relevant': n_rel,\n",
    "    })\n",
    "\n",
    "    if len(train_queries) >= N_TRAIN * 3:\n",
    "        break\n",
    "\n",
    "np.random.shuffle(train_queries)\n",
    "train_queries = train_queries[:N_TRAIN]\n",
    "\n",
    "n_passages_train = [q['n_passages'] for q in train_queries]\n",
    "total_train_passages = sum(n_passages_train)\n",
    "total_train_rel = sum(q['n_relevant'] for q in train_queries)\n",
    "\n",
    "print(f\"\\nSelected {len(train_queries)} training queries ({total_train_passages} passages)\")\n",
    "print(f\"Passages per query: mean={np.mean(n_passages_train):.1f}, \"\n",
    "      f\"min={min(n_passages_train)}, max={max(n_passages_train)}\")\n",
    "print(f\"Relevant: {total_train_rel} ({100*total_train_rel/total_train_passages:.1f}%)\")\n",
    "\n",
    "del dataset\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T21:43:07.886080Z",
     "iopub.status.busy": "2026-02-16T21:43:07.885430Z",
     "iopub.status.idle": "2026-02-16T21:43:07.891256Z",
     "shell.execute_reply": "2026-02-16T21:43:07.890438Z"
    },
    "papermill": {
     "duration": 0.013699,
     "end_time": "2026-02-16T21:43:07.892882",
     "exception": false,
     "start_time": "2026-02-16T21:43:07.879183",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "EXPERIMENTAL CONDITIONS\n",
      "======================================================================\n",
      "\n",
      "### Training: Contrastive (Hinge) Loss ###\n",
      "\n",
      "For each training query:\n",
      "  1. Sample 1 relevant passage (R) and 1 irrelevant passage (I)\n",
      "  2. Build hybrid cache with soft prefix for R -> compute NLL_R\n",
      "  3. Build hybrid cache with soft prefix for I -> compute NLL_I\n",
      "  4. Loss = max(0, MARGIN + NLL_R - NLL_I)\n",
      "     - If NLL_R < NLL_I - MARGIN: loss = 0 (satisfied)\n",
      "     - If NLL_R > NLL_I: loss > MARGIN (violated)\n",
      "  5. Gradient pushes soft prefix to reduce NLL_R relative to NLL_I\n",
      "\n",
      "Both forward passes MUST be in the same computation graph\n",
      "(both NLLs share the same soft_prefix) for loss.backward() to work.\n",
      "\n",
      "### Two Init Conditions ###\n",
      "\n",
      "  warm: Initialize from results/exp25/soft_prefix_fact.pt\n",
      "        Tests: can we add ranking signal to an already-useful NLL prefix?\n",
      "\n",
      "  cold: Initialize from random N(0, 0.02)\n",
      "        Tests: can contrastive loss learn ranking signal from scratch?\n",
      "\n",
      "### Evaluation Conditions (5 total) ###\n",
      "\n",
      "  bare:             No prefix, no cache modification\n",
      "  exp25_fact:       Exp 25 soft_prefix_fact (NLL-optimized)\n",
      "  contrastive_warm: This experiment's warm-start prefix\n",
      "  contrastive_cold: This experiment's cold-start prefix\n",
      "  baseline:         BOS-only cache (no document) for PMI computation\n",
      "\n",
      "### Ranking Metrics ###\n",
      "\n",
      "  AUC-ROC: Can the NLL scores separate relevant from irrelevant?\n",
      "  MRR@10:  Is the first relevant passage ranked near the top?\n",
      "  Both computed for raw NLL and PMI (NLL - baseline) scoring.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Explain experimental conditions\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"EXPERIMENTAL CONDITIONS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\"\"\n",
    "### Training: Contrastive (Hinge) Loss ###\n",
    "\n",
    "For each training query:\n",
    "  1. Sample 1 relevant passage (R) and 1 irrelevant passage (I)\n",
    "  2. Build hybrid cache with soft prefix for R -> compute NLL_R\n",
    "  3. Build hybrid cache with soft prefix for I -> compute NLL_I\n",
    "  4. Loss = max(0, MARGIN + NLL_R - NLL_I)\n",
    "     - If NLL_R < NLL_I - MARGIN: loss = 0 (satisfied)\n",
    "     - If NLL_R > NLL_I: loss > MARGIN (violated)\n",
    "  5. Gradient pushes soft prefix to reduce NLL_R relative to NLL_I\n",
    "\n",
    "Both forward passes MUST be in the same computation graph\n",
    "(both NLLs share the same soft_prefix) for loss.backward() to work.\n",
    "\n",
    "### Two Init Conditions ###\n",
    "\n",
    "  warm: Initialize from results/exp25/soft_prefix_fact.pt\n",
    "        Tests: can we add ranking signal to an already-useful NLL prefix?\n",
    "\n",
    "  cold: Initialize from random N(0, 0.02)\n",
    "        Tests: can contrastive loss learn ranking signal from scratch?\n",
    "\n",
    "### Evaluation Conditions (5 total) ###\n",
    "\n",
    "  bare:             No prefix, no cache modification\n",
    "  exp25_fact:       Exp 25 soft_prefix_fact (NLL-optimized)\n",
    "  contrastive_warm: This experiment's warm-start prefix\n",
    "  contrastive_cold: This experiment's cold-start prefix\n",
    "  baseline:         BOS-only cache (no document) for PMI computation\n",
    "\n",
    "### Ranking Metrics ###\n",
    "\n",
    "  AUC-ROC: Can the NLL scores separate relevant from irrelevant?\n",
    "  MRR@10:  Is the first relevant passage ranked near the top?\n",
    "  Both computed for raw NLL and PMI (NLL - baseline) scoring.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cell-6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T21:43:07.905189Z",
     "iopub.status.busy": "2026-02-16T21:43:07.904916Z",
     "iopub.status.idle": "2026-02-16T21:43:07.921317Z",
     "shell.execute_reply": "2026-02-16T21:43:07.920412Z"
    },
    "papermill": {
     "duration": 0.024437,
     "end_time": "2026-02-16T21:43:07.922869",
     "exception": false,
     "start_time": "2026-02-16T21:43:07.898432",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding layer: Gemma3TextScaledWordEmbedding, shape=torch.Size([262208, 2560])\n",
      "differentiable_hybrid_score() defined\n",
      "  Input: soft_prefix (requires_grad), doc_ids, query, answer\n",
      "  Output: scalar NLL loss with gradients to soft_prefix\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: differentiable_hybrid_score() — reuse from Exp 25 Cell 7 verbatim\n",
    "\n",
    "from lib.kv_cache import _get_rope_theta_for_layer, _build_rope_correction, _rotate_half\n",
    "\n",
    "print(f\"Embedding layer: {type(embed_fn).__name__}, shape={embed_fn.weight.shape}\")\n",
    "\n",
    "\n",
    "def differentiable_hybrid_score(\n",
    "    soft_prefix: torch.Tensor,       # (1, prefix_len, hidden_size), requires_grad\n",
    "    doc_ids: torch.Tensor,            # (1, doc_len)\n",
    "    bos_id: torch.Tensor,             # (1, 1)\n",
    "    query_prompt: str,\n",
    "    answer_text: str,\n",
    "    model,\n",
    "    tokenizer,\n",
    "    config,\n",
    "    cutoff: int,\n",
    "):\n",
    "    \"\"\"\n",
    "    Compute answer NLL through a hybrid cache built from soft prefix embeddings.\n",
    "    Returns a scalar loss tensor with gradients flowing back to soft_prefix.\n",
    "    \"\"\"\n",
    "    device = config.device\n",
    "    doc_len = doc_ids.shape[1]\n",
    "    prefix_len = soft_prefix.shape[1]\n",
    "    context_len = 1 + doc_len  # BOS + doc\n",
    "\n",
    "    # --- Step 1: Bare cache (no gradients needed) ---\n",
    "    bare_input = torch.cat([bos_id, doc_ids], dim=1)\n",
    "    with torch.no_grad():\n",
    "        bare_out = model(input_ids=bare_input,\n",
    "                         attention_mask=torch.ones_like(bare_input),\n",
    "                         use_cache=True, return_dict=True)\n",
    "    bare_cache = bare_out.past_key_values\n",
    "    del bare_out\n",
    "\n",
    "    # --- Step 2: Primed cache via inputs_embeds (gradients enabled) ---\n",
    "    with torch.no_grad():\n",
    "        bos_emb = embed_fn(bos_id)\n",
    "        doc_emb = embed_fn(doc_ids)\n",
    "\n",
    "    soft_cast = soft_prefix.to(dtype=bos_emb.dtype)\n",
    "    inputs_embeds = torch.cat([bos_emb.detach(), soft_cast, doc_emb.detach()], dim=1)\n",
    "    total_len = inputs_embeds.shape[1]\n",
    "    attn_mask = torch.ones((1, total_len), device=device, dtype=torch.long)\n",
    "\n",
    "    primed_out = model(inputs_embeds=inputs_embeds,\n",
    "                       attention_mask=attn_mask,\n",
    "                       use_cache=True, return_dict=True)\n",
    "    primed_cache = primed_out.past_key_values\n",
    "    del primed_out\n",
    "\n",
    "    # --- Step 3+4: Build hybrid cache ---\n",
    "    primed_cache_dc = _ensure_dynamic_cache(primed_cache)\n",
    "    bare_cache_dc = _ensure_dynamic_cache(bare_cache)\n",
    "\n",
    "    from transformers import DynamicCache\n",
    "    from transformers.cache_utils import DynamicSlidingWindowLayer, DynamicLayer\n",
    "\n",
    "    hybrid_cache = DynamicCache()\n",
    "    for layer_idx in range(NUM_LAYERS):\n",
    "        k = _get_cache_keys(bare_cache_dc, layer_idx)\n",
    "\n",
    "        if layer_idx < cutoff:\n",
    "            primed_v = _get_cache_values(primed_cache_dc, layer_idx)\n",
    "            bos_v = primed_v[:, :, :1, :]\n",
    "            doc_v = primed_v[:, :, -doc_len:, :]\n",
    "            v = torch.cat([bos_v, doc_v], dim=2)\n",
    "        else:\n",
    "            v = _get_cache_values(bare_cache_dc, layer_idx)\n",
    "\n",
    "        src_layer = bare_cache_dc.layers[layer_idx]\n",
    "        if isinstance(src_layer, DynamicSlidingWindowLayer):\n",
    "            new_layer = DynamicSlidingWindowLayer(sliding_window=src_layer.sliding_window)\n",
    "            new_layer.dtype = k.dtype\n",
    "            new_layer.device = k.device\n",
    "            new_layer.keys = k\n",
    "            new_layer.values = v\n",
    "            new_layer.is_initialized = True\n",
    "            new_layer.cumulative_length = src_layer.cumulative_length\n",
    "            new_layer._sliding_window_tensor = new_layer._sliding_window_tensor.to(k.device)\n",
    "        else:\n",
    "            new_layer = DynamicLayer()\n",
    "            new_layer.dtype = k.dtype\n",
    "            new_layer.device = k.device\n",
    "            new_layer.keys = k\n",
    "            new_layer.values = v\n",
    "            new_layer.is_initialized = True\n",
    "        hybrid_cache.layers.append(new_layer)\n",
    "\n",
    "    # --- Step 5: Score answer through hybrid cache ---\n",
    "    query_ids = tokenizer(query_prompt, return_tensors=\"pt\",\n",
    "                          add_special_tokens=False)['input_ids'].to(device)\n",
    "    answer_ids = tokenizer(answer_text, return_tensors=\"pt\",\n",
    "                           add_special_tokens=False)['input_ids'].to(device)\n",
    "    query_len = query_ids.shape[1]\n",
    "    answer_len = answer_ids.shape[1]\n",
    "\n",
    "    qa_ids = torch.cat([query_ids, answer_ids], dim=1)\n",
    "    qa_len = qa_ids.shape[1]\n",
    "    qa_attn_full = torch.ones((1, context_len + qa_len), device=device)\n",
    "\n",
    "    qa_out = model(input_ids=qa_ids,\n",
    "                   attention_mask=qa_attn_full,\n",
    "                   past_key_values=hybrid_cache,\n",
    "                   use_cache=False, return_dict=True)\n",
    "\n",
    "    logits = qa_out.logits\n",
    "    answer_logits = logits[:, query_len - 1 : query_len + answer_len - 1, :]\n",
    "\n",
    "    loss = torch.nn.functional.cross_entropy(\n",
    "        answer_logits.reshape(-1, answer_logits.shape[-1]),\n",
    "        answer_ids.reshape(-1),\n",
    "        reduction='mean'\n",
    "    )\n",
    "\n",
    "    del qa_out, logits, bare_cache, bare_cache_dc, primed_cache, primed_cache_dc\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "print(\"differentiable_hybrid_score() defined\")\n",
    "print(\"  Input: soft_prefix (requires_grad), doc_ids, query, answer\")\n",
    "print(\"  Output: scalar NLL loss with gradients to soft_prefix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cell-7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T21:43:07.935765Z",
     "iopub.status.busy": "2026-02-16T21:43:07.935479Z",
     "iopub.status.idle": "2026-02-16T21:43:09.844008Z",
     "shell.execute_reply": "2026-02-16T21:43:09.843087Z"
    },
    "papermill": {
     "duration": 1.918252,
     "end_time": "2026-02-16T21:43:09.846413",
     "exception": false,
     "start_time": "2026-02-16T21:43:07.928161",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "GRADIENT FLOW SANITY CHECK — CONTRASTIVE LOSS\n",
      "======================================================================\n",
      "Test query: 'where is dachau...'\n",
      "Relevant passage: 'Dachau Altstadt. Dachau is a city with about 44,800 citizens...'\n",
      "Irrelevant passage: 'Dachau (. [ˈdaxaʊ]) is a town in Upper Bavaria, in the south...'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NLL relevant: 0.7227 (requires_grad: True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLL irrelevant: 3.1250 (requires_grad: True)\n",
      "\n",
      "Hinge loss: max(0, 0.1 + 0.7227 - 3.1250) = 0.0000\n",
      "Hinge requires_grad: True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Backward pass: SUCCESS\n",
      "Gradient shape: torch.Size([1, 11, 2560])\n",
      "Gradient norm: 0.000000\n",
      "\n",
      ">>> Hinge loss is 0 (margin satisfied). Gradient expected to be 0. <<<\n",
      ">>> This is correct behavior — try with different sample if needed. <<<\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Gradient flow sanity check with contrastive loss\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"GRADIENT FLOW SANITY CHECK — CONTRASTIVE LOSS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Create test prefix\n",
    "test_prefix = torch.randn(1, PREFIX_LEN, HIDDEN_SIZE,\n",
    "                           device=exp_config.device,\n",
    "                           dtype=torch.float32,\n",
    "                           requires_grad=True)\n",
    "\n",
    "# Pick a training query with both relevant and irrelevant passages\n",
    "test_q = train_queries[0]\n",
    "query_prompt = QUERY_TEMPLATE.format(query=test_q['query'])\n",
    "answer_text = ANSWER_TEMPLATE.format(answer=test_q['answer'])\n",
    "\n",
    "rel_passage = next(p for p in test_q['passages'] if p['is_relevant'])\n",
    "irr_passage = next(p for p in test_q['passages'] if not p['is_relevant'])\n",
    "\n",
    "print(f\"Test query: '{test_q['query'][:60]}...'\")\n",
    "print(f\"Relevant passage: '{rel_passage['passage'][:60]}...'\")\n",
    "print(f\"Irrelevant passage: '{irr_passage['passage'][:60]}...'\")\n",
    "\n",
    "# Helper to get doc_ids via matched tokenization\n",
    "def get_matched_doc_ids(passage_text):\n",
    "    doc_text = DOCUMENT_TEMPLATE.format(document=passage_text)\n",
    "    full_text = sf_str + doc_text\n",
    "    full_enc = tokenizer(full_text, return_tensors=\"pt\",\n",
    "                          add_special_tokens=True, padding=False, truncation=False)\n",
    "    full_ids = full_enc['input_ids'].to(exp_config.device)\n",
    "    sf_enc = tokenizer(sf_str, return_tensors=\"pt\",\n",
    "                        add_special_tokens=True, padding=False, truncation=False)\n",
    "    sf_len = sf_enc['input_ids'].shape[1]\n",
    "    bos_id = full_ids[:, :1]\n",
    "    doc_ids = full_ids[:, sf_len:]\n",
    "    return bos_id, doc_ids\n",
    "\n",
    "try:\n",
    "    # Score relevant passage\n",
    "    bos_rel, doc_rel = get_matched_doc_ids(rel_passage['passage'])\n",
    "    nll_rel = differentiable_hybrid_score(\n",
    "        test_prefix, doc_rel, bos_rel,\n",
    "        query_prompt, answer_text,\n",
    "        model, tokenizer, exp_config, CUTOFF)\n",
    "    print(f\"\\nNLL relevant: {nll_rel.item():.4f} (requires_grad: {nll_rel.requires_grad})\")\n",
    "\n",
    "    # Score irrelevant passage\n",
    "    bos_irr, doc_irr = get_matched_doc_ids(irr_passage['passage'])\n",
    "    nll_irr = differentiable_hybrid_score(\n",
    "        test_prefix, doc_irr, bos_irr,\n",
    "        query_prompt, answer_text,\n",
    "        model, tokenizer, exp_config, CUTOFF)\n",
    "    print(f\"NLL irrelevant: {nll_irr.item():.4f} (requires_grad: {nll_irr.requires_grad})\")\n",
    "\n",
    "    # Contrastive hinge loss\n",
    "    hinge_loss = torch.clamp(MARGIN + nll_rel - nll_irr, min=0.0)\n",
    "    print(f\"\\nHinge loss: max(0, {MARGIN} + {nll_rel.item():.4f} - {nll_irr.item():.4f}) = {hinge_loss.item():.4f}\")\n",
    "    print(f\"Hinge requires_grad: {hinge_loss.requires_grad}\")\n",
    "\n",
    "    # Backward\n",
    "    hinge_loss.backward()\n",
    "\n",
    "    print(f\"\\nBackward pass: SUCCESS\")\n",
    "    print(f\"Gradient shape: {test_prefix.grad.shape}\")\n",
    "    print(f\"Gradient norm: {test_prefix.grad.norm().item():.6f}\")\n",
    "\n",
    "    if hinge_loss.item() > 0 and test_prefix.grad.norm().item() > 0:\n",
    "        print(\"\\n>>> CONTRASTIVE GRADIENT FLOW CONFIRMED <<<\")\n",
    "    elif hinge_loss.item() == 0:\n",
    "        print(\"\\n>>> Hinge loss is 0 (margin satisfied). Gradient expected to be 0. <<<\")\n",
    "        print(\">>> This is correct behavior — try with different sample if needed. <<<\")\n",
    "    else:\n",
    "        print(\"\\n>>> WARNING: Non-zero loss but zero gradient. Debug needed. <<<\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n>>> GRADIENT FLOW FAILED: {e} <<<\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "finally:\n",
    "    del test_prefix\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cell-8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T21:43:09.868345Z",
     "iopub.status.busy": "2026-02-16T21:43:09.868002Z",
     "iopub.status.idle": "2026-02-16T21:43:09.890678Z",
     "shell.execute_reply": "2026-02-16T21:43:09.889967Z"
    },
    "papermill": {
     "duration": 0.035515,
     "end_time": "2026-02-16T21:43:09.892177",
     "exception": false,
     "start_time": "2026-02-16T21:43:09.856662",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_contrastive_soft_prefix() defined\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: train_contrastive_soft_prefix()\n",
    "\n",
    "def train_contrastive_soft_prefix(init_mode, train_data, n_epochs, lr, grad_accum,\n",
    "                                   margin, checkpoint_path, save_path, warmup_steps=30):\n",
    "    \"\"\"\n",
    "    Train soft prefix with contrastive hinge loss for ranking.\n",
    "\n",
    "    Per step: sample 1 relevant + 1 irrelevant passage per query.\n",
    "    Loss = max(0, margin + NLL_relevant - NLL_irrelevant).\n",
    "    Both scored through the SAME soft prefix (sequential forward passes).\n",
    "\n",
    "    Args:\n",
    "        init_mode: 'warm' (from exp25_fact) or 'cold' (random)\n",
    "        train_data: list of query dicts with 'passages' containing relevance labels\n",
    "        n_epochs: number of passes\n",
    "        lr: learning rate\n",
    "        grad_accum: gradient accumulation steps\n",
    "        margin: hinge loss margin\n",
    "        checkpoint_path: path to save training checkpoints\n",
    "        save_path: path to save final embeddings\n",
    "        warmup_steps: linear warmup steps\n",
    "\n",
    "    Returns:\n",
    "        dict with training history and final embeddings\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'=' * 70}\")\n",
    "    print(f\"TRAINING CONTRASTIVE SOFT PREFIX — init={init_mode}\")\n",
    "    print(f\"{'=' * 70}\")\n",
    "\n",
    "    # Initialize soft prefix\n",
    "    if init_mode == 'warm':\n",
    "        soft_prefix = torch.load(EXP25_SOFT_FACT).to(exp_config.device).float()\n",
    "        print(f\"  Loaded warm-start from {EXP25_SOFT_FACT}\")\n",
    "    elif init_mode == 'cold':\n",
    "        soft_prefix = torch.randn(1, PREFIX_LEN, HIDDEN_SIZE,\n",
    "                                   device=exp_config.device, dtype=torch.float32) * 0.02\n",
    "        print(f\"  Cold-start: random N(0, 0.02)\")\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown init_mode: {init_mode}\")\n",
    "\n",
    "    soft_prefix = soft_prefix.detach().requires_grad_(True)\n",
    "    print(f\"  Shape: {soft_prefix.shape}, norm: {soft_prefix.norm().item():.4f}\")\n",
    "\n",
    "    optimizer = torch.optim.AdamW([soft_prefix], lr=lr, weight_decay=0.01)\n",
    "\n",
    "    total_steps = n_epochs * len(train_data)\n",
    "    total_optim_steps = total_steps // grad_accum\n",
    "    print(f\"  Total steps: {total_steps}, optim steps: {total_optim_steps}\")\n",
    "\n",
    "    # Checkpoint resume\n",
    "    history = []\n",
    "    start_step = 0\n",
    "    if checkpoint_path.exists():\n",
    "        ckpt = json.loads(checkpoint_path.read_text())\n",
    "        if ckpt.get('init_mode') == init_mode and ckpt.get('total_steps') == total_steps:\n",
    "            history = ckpt['history']\n",
    "            start_step = ckpt['completed_steps']\n",
    "            soft_prefix_data = torch.tensor(ckpt['soft_prefix'],\n",
    "                                            device=exp_config.device, dtype=torch.float32)\n",
    "            soft_prefix = soft_prefix_data.requires_grad_(True)\n",
    "            optimizer = torch.optim.AdamW([soft_prefix], lr=lr, weight_decay=0.01)\n",
    "            print(f\"  Resumed from checkpoint: step {start_step}/{total_steps}\")\n",
    "\n",
    "    t_start = time.time()\n",
    "    step = 0\n",
    "    optim_step = 0\n",
    "    running_loss = 0.0\n",
    "    running_nll_gap = 0.0  # NLL_irr - NLL_rel (positive = good)\n",
    "    running_satisfied = 0  # fraction where hinge loss = 0\n",
    "    running_count = 0\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        np.random.seed(SEED + epoch)\n",
    "        epoch_indices = np.random.permutation(len(train_data))\n",
    "\n",
    "        for data_idx in epoch_indices:\n",
    "            if step < start_step:\n",
    "                step += 1\n",
    "                continue\n",
    "\n",
    "            qdata = train_data[data_idx]\n",
    "            query_prompt = QUERY_TEMPLATE.format(query=qdata['query'])\n",
    "            answer_text = ANSWER_TEMPLATE.format(answer=qdata['answer'])\n",
    "\n",
    "            # Sample 1 relevant + 1 irrelevant passage\n",
    "            rel_passages = [p for p in qdata['passages'] if p['is_relevant']]\n",
    "            irr_passages = [p for p in qdata['passages'] if not p['is_relevant']]\n",
    "            rel_p = rel_passages[np.random.randint(len(rel_passages))]\n",
    "            irr_p = irr_passages[np.random.randint(len(irr_passages))]\n",
    "\n",
    "            try:\n",
    "                # Score relevant passage\n",
    "                bos_rel, doc_rel = get_matched_doc_ids(rel_p['passage'])\n",
    "                nll_rel = differentiable_hybrid_score(\n",
    "                    soft_prefix, doc_rel, bos_rel,\n",
    "                    query_prompt, answer_text,\n",
    "                    model, tokenizer, exp_config, CUTOFF)\n",
    "\n",
    "                # Score irrelevant passage\n",
    "                bos_irr, doc_irr = get_matched_doc_ids(irr_p['passage'])\n",
    "                nll_irr = differentiable_hybrid_score(\n",
    "                    soft_prefix, doc_irr, bos_irr,\n",
    "                    query_prompt, answer_text,\n",
    "                    model, tokenizer, exp_config, CUTOFF)\n",
    "\n",
    "                # Hinge loss\n",
    "                hinge_loss = torch.clamp(margin + nll_rel - nll_irr, min=0.0)\n",
    "                scaled_loss = hinge_loss / grad_accum\n",
    "                scaled_loss.backward()\n",
    "\n",
    "                running_loss += hinge_loss.item()\n",
    "                running_nll_gap += (nll_irr.item() - nll_rel.item())\n",
    "                running_satisfied += int(hinge_loss.item() == 0)\n",
    "                running_count += 1\n",
    "\n",
    "            except RuntimeError as e:\n",
    "                print(f\"  Step {step}: RuntimeError: {e}\")\n",
    "                optimizer.zero_grad()\n",
    "                gc.collect()\n",
    "                torch.cuda.empty_cache()\n",
    "                step += 1\n",
    "                continue\n",
    "\n",
    "            # Optimizer step\n",
    "            if (step + 1) % grad_accum == 0:\n",
    "                optim_step += 1\n",
    "                if optim_step <= warmup_steps:\n",
    "                    for pg in optimizer.param_groups:\n",
    "                        pg['lr'] = lr * (optim_step / warmup_steps)\n",
    "\n",
    "                grad_norm = soft_prefix.grad.norm().item() if soft_prefix.grad is not None else 0\n",
    "                torch.nn.utils.clip_grad_norm_([soft_prefix], max_norm=1.0)\n",
    "\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                avg_loss = running_loss / running_count if running_count > 0 else 0\n",
    "                avg_gap = running_nll_gap / running_count if running_count > 0 else 0\n",
    "                sat_frac = running_satisfied / running_count if running_count > 0 else 0\n",
    "\n",
    "                history.append({\n",
    "                    'step': step,\n",
    "                    'optim_step': optim_step,\n",
    "                    'epoch': epoch,\n",
    "                    'avg_loss': avg_loss,\n",
    "                    'avg_nll_gap': avg_gap,\n",
    "                    'satisfied_frac': sat_frac,\n",
    "                    'grad_norm': grad_norm,\n",
    "                    'prefix_norm': soft_prefix.norm().item(),\n",
    "                    'lr': optimizer.param_groups[0]['lr'],\n",
    "                })\n",
    "                running_loss = 0.0\n",
    "                running_nll_gap = 0.0\n",
    "                running_satisfied = 0\n",
    "                running_count = 0\n",
    "\n",
    "            # Cleanup\n",
    "            del hinge_loss, scaled_loss, nll_rel, nll_irr\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            step += 1\n",
    "\n",
    "            # Checkpoint\n",
    "            if step % CHECKPOINT_EVERY == 0 or step == total_steps:\n",
    "                elapsed = time.time() - t_start\n",
    "                steps_done = step - start_step\n",
    "                rate = steps_done / elapsed if elapsed > 0 else 0\n",
    "                remaining = (total_steps - step) / rate if rate > 0 else 0\n",
    "\n",
    "                last = history[-1] if history else {}\n",
    "                tqdm.write(\n",
    "                    f\"  [{init_mode}] Step {step}/{total_steps} | \"\n",
    "                    f\"loss={last.get('avg_loss', 0):.4f} | \"\n",
    "                    f\"gap={last.get('avg_nll_gap', 0):.3f} | \"\n",
    "                    f\"sat={last.get('satisfied_frac', 0):.1%} | \"\n",
    "                    f\"norm={soft_prefix.norm().item():.3f} | \"\n",
    "                    f\"ETA: {remaining/60:.1f}m\")\n",
    "\n",
    "                ckpt_data = {\n",
    "                    'init_mode': init_mode,\n",
    "                    'completed_steps': step,\n",
    "                    'total_steps': total_steps,\n",
    "                    'history': history,\n",
    "                    'soft_prefix': soft_prefix.detach().cpu().tolist(),\n",
    "                    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                }\n",
    "                with open(checkpoint_path, 'w') as f:\n",
    "                    json.dump(ckpt_data, f)\n",
    "\n",
    "    # Save final\n",
    "    torch.save(soft_prefix.detach().cpu(), save_path)\n",
    "\n",
    "    elapsed = time.time() - t_start\n",
    "    print(f\"\\n  Training complete: {step} steps in {elapsed/60:.1f} min\")\n",
    "    print(f\"  Final prefix norm: {soft_prefix.norm().item():.4f}\")\n",
    "    print(f\"  Saved to: {save_path}\")\n",
    "\n",
    "    return {\n",
    "        'soft_prefix': soft_prefix.detach(),\n",
    "        'history': history,\n",
    "        'init_mode': init_mode,\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"train_contrastive_soft_prefix() defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cell-9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T21:43:09.904869Z",
     "iopub.status.busy": "2026-02-16T21:43:09.904188Z",
     "iopub.status.idle": "2026-02-16T22:53:40.300685Z",
     "shell.execute_reply": "2026-02-16T22:53:40.299777Z"
    },
    "papermill": {
     "duration": 4230.41162,
     "end_time": "2026-02-16T22:53:40.309482",
     "exception": false,
     "start_time": "2026-02-16T21:43:09.897862",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "TRAINING CONTRASTIVE SOFT PREFIX — init=warm\n",
      "======================================================================\n",
      "  Loaded warm-start from results/exp25/soft_prefix_fact.pt\n",
      "  Shape: torch.Size([1, 11, 2560]), norm: 334.9726\n",
      "  Total steps: 2500, optim steps: 625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [warm] Step 50/2500 | loss=0.0000 | gap=2.507 | sat=100.0% | norm=334.683 | ETA: 70.6m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [warm] Step 100/2500 | loss=0.0000 | gap=1.645 | sat=100.0% | norm=333.978 | ETA: 68.4m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [warm] Step 150/2500 | loss=0.0000 | gap=1.955 | sat=100.0% | norm=333.509 | ETA: 66.9m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [warm] Step 200/2500 | loss=0.0000 | gap=1.664 | sat=100.0% | norm=332.780 | ETA: 65.4m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [warm] Step 250/2500 | loss=0.0312 | gap=1.138 | sat=75.0% | norm=332.051 | ETA: 64.0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [warm] Step 300/2500 | loss=0.0000 | gap=2.088 | sat=100.0% | norm=330.789 | ETA: 62.6m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [warm] Step 350/2500 | loss=0.0322 | gap=2.453 | sat=75.0% | norm=329.457 | ETA: 61.1m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [warm] Step 400/2500 | loss=0.0000 | gap=1.862 | sat=100.0% | norm=328.163 | ETA: 59.7m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [warm] Step 450/2500 | loss=0.0000 | gap=2.853 | sat=100.0% | norm=327.368 | ETA: 58.3m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [warm] Step 500/2500 | loss=0.0000 | gap=1.796 | sat=100.0% | norm=326.831 | ETA: 56.8m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [warm] Step 550/2500 | loss=0.2539 | gap=1.621 | sat=75.0% | norm=326.148 | ETA: 55.4m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [warm] Step 600/2500 | loss=0.0000 | gap=0.947 | sat=100.0% | norm=324.683 | ETA: 54.0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [warm] Step 650/2500 | loss=0.1167 | gap=1.391 | sat=75.0% | norm=323.314 | ETA: 52.5m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [warm] Step 700/2500 | loss=0.2059 | gap=0.742 | sat=50.0% | norm=321.947 | ETA: 51.2m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [warm] Step 750/2500 | loss=0.1055 | gap=0.934 | sat=50.0% | norm=321.039 | ETA: 49.7m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [warm] Step 800/2500 | loss=0.2109 | gap=1.303 | sat=75.0% | norm=320.791 | ETA: 48.3m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [warm] Step 850/2500 | loss=0.0000 | gap=1.357 | sat=100.0% | norm=321.079 | ETA: 46.9m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [warm] Step 900/2500 | loss=0.0684 | gap=1.127 | sat=75.0% | norm=321.128 | ETA: 45.5m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [warm] Step 950/2500 | loss=0.0000 | gap=2.090 | sat=100.0% | norm=320.764 | ETA: 44.0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [warm] Step 1000/2500 | loss=0.2578 | gap=1.227 | sat=75.0% | norm=320.352 | ETA: 42.5m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [warm] Step 1050/2500 | loss=0.0000 | gap=1.436 | sat=100.0% | norm=320.699 | ETA: 41.1m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [warm] Step 1100/2500 | loss=0.0000 | gap=1.443 | sat=100.0% | norm=320.163 | ETA: 39.7m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [warm] Step 1150/2500 | loss=0.0000 | gap=1.953 | sat=100.0% | norm=320.043 | ETA: 38.2m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [warm] Step 1200/2500 | loss=0.2168 | gap=0.842 | sat=50.0% | norm=320.035 | ETA: 36.8m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [warm] Step 1250/2500 | loss=0.0312 | gap=1.880 | sat=75.0% | norm=319.994 | ETA: 35.4m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [warm] Step 1300/2500 | loss=0.0000 | gap=2.094 | sat=100.0% | norm=319.424 | ETA: 33.9m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [warm] Step 1350/2500 | loss=0.1152 | gap=1.264 | sat=75.0% | norm=319.524 | ETA: 32.5m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [warm] Step 1400/2500 | loss=0.0000 | gap=2.920 | sat=100.0% | norm=319.527 | ETA: 31.1m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [warm] Step 1450/2500 | loss=0.0000 | gap=1.550 | sat=100.0% | norm=318.806 | ETA: 29.7m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [warm] Step 1500/2500 | loss=0.2656 | gap=1.302 | sat=75.0% | norm=317.375 | ETA: 28.3m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [warm] Step 1550/2500 | loss=0.0000 | gap=2.059 | sat=100.0% | norm=316.242 | ETA: 26.8m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [warm] Step 1600/2500 | loss=0.0000 | gap=1.407 | sat=100.0% | norm=314.644 | ETA: 25.4m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [warm] Step 1650/2500 | loss=0.0977 | gap=1.658 | sat=75.0% | norm=313.501 | ETA: 24.0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [warm] Step 1700/2500 | loss=0.0410 | gap=1.482 | sat=50.0% | norm=312.557 | ETA: 22.6m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [warm] Step 1750/2500 | loss=0.0000 | gap=1.714 | sat=100.0% | norm=311.613 | ETA: 21.2m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [warm] Step 1800/2500 | loss=0.0000 | gap=1.496 | sat=100.0% | norm=310.496 | ETA: 19.8m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [warm] Step 1850/2500 | loss=0.0000 | gap=2.481 | sat=100.0% | norm=309.397 | ETA: 18.4m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [warm] Step 1900/2500 | loss=0.0195 | gap=1.358 | sat=75.0% | norm=308.167 | ETA: 16.9m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [warm] Step 1950/2500 | loss=0.0000 | gap=1.642 | sat=100.0% | norm=306.975 | ETA: 15.5m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [warm] Step 2000/2500 | loss=0.1504 | gap=0.454 | sat=50.0% | norm=305.416 | ETA: 14.1m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [warm] Step 2050/2500 | loss=0.2363 | gap=1.226 | sat=75.0% | norm=304.123 | ETA: 12.7m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [warm] Step 2100/2500 | loss=0.0039 | gap=1.800 | sat=75.0% | norm=302.522 | ETA: 11.3m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [warm] Step 2150/2500 | loss=0.0000 | gap=1.079 | sat=100.0% | norm=301.177 | ETA: 9.9m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [warm] Step 2200/2500 | loss=0.0000 | gap=1.628 | sat=100.0% | norm=299.803 | ETA: 8.5m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [warm] Step 2250/2500 | loss=0.0000 | gap=1.826 | sat=100.0% | norm=298.445 | ETA: 7.1m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [warm] Step 2300/2500 | loss=0.1035 | gap=0.809 | sat=50.0% | norm=297.178 | ETA: 5.6m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [warm] Step 2350/2500 | loss=0.0312 | gap=1.056 | sat=75.0% | norm=296.423 | ETA: 4.2m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [warm] Step 2400/2500 | loss=0.0000 | gap=2.064 | sat=100.0% | norm=296.306 | ETA: 2.8m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [warm] Step 2450/2500 | loss=0.0449 | gap=1.136 | sat=75.0% | norm=295.351 | ETA: 1.4m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [warm] Step 2500/2500 | loss=0.0059 | gap=2.322 | sat=75.0% | norm=294.352 | ETA: 0.0m\n",
      "\n",
      "  Training complete: 2500 steps in 70.5 min\n",
      "  Final prefix norm: 294.3524\n",
      "  Saved to: results/exp28/soft_prefix_contrastive_warm.pt\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Train warm start (init from exp25 soft_prefix_fact.pt)\n",
    "\n",
    "result_warm = train_contrastive_soft_prefix(\n",
    "    init_mode='warm',\n",
    "    train_data=train_queries,\n",
    "    n_epochs=N_EPOCHS,\n",
    "    lr=LR,\n",
    "    grad_accum=GRAD_ACCUM,\n",
    "    margin=MARGIN,\n",
    "    checkpoint_path=CHECKPOINT_TRAIN_WARM_PATH,\n",
    "    save_path=SOFT_WARM_PATH,\n",
    "    warmup_steps=WARMUP_STEPS,\n",
    ")\n",
    "soft_contrastive_warm = result_warm['soft_prefix']\n",
    "history_warm = result_warm['history']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cell-10",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T22:53:40.327080Z",
     "iopub.status.busy": "2026-02-16T22:53:40.326311Z",
     "iopub.status.idle": "2026-02-17T00:04:18.420932Z",
     "shell.execute_reply": "2026-02-17T00:04:18.420213Z"
    },
    "papermill": {
     "duration": 4238.115174,
     "end_time": "2026-02-17T00:04:18.432456",
     "exception": false,
     "start_time": "2026-02-16T22:53:40.317282",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "TRAINING CONTRASTIVE SOFT PREFIX — init=cold\n",
      "======================================================================\n",
      "  Cold-start: random N(0, 0.02)\n",
      "  Shape: torch.Size([1, 11, 2560]), norm: 3.3704\n",
      "  Total steps: 2500, optim steps: 625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [cold] Step 50/2500 | loss=0.0000 | gap=2.062 | sat=100.0% | norm=7.282 | ETA: 69.3m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [cold] Step 100/2500 | loss=0.2617 | gap=0.545 | sat=50.0% | norm=18.173 | ETA: 67.7m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [cold] Step 150/2500 | loss=0.0625 | gap=0.500 | sat=75.0% | norm=33.676 | ETA: 66.3m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [cold] Step 200/2500 | loss=0.0000 | gap=0.482 | sat=100.0% | norm=46.331 | ETA: 64.9m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [cold] Step 250/2500 | loss=0.0469 | gap=0.650 | sat=75.0% | norm=54.483 | ETA: 63.6m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [cold] Step 300/2500 | loss=0.0000 | gap=1.402 | sat=100.0% | norm=61.452 | ETA: 62.3m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [cold] Step 350/2500 | loss=0.0000 | gap=1.482 | sat=100.0% | norm=67.409 | ETA: 60.9m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [cold] Step 400/2500 | loss=0.0000 | gap=1.277 | sat=100.0% | norm=73.187 | ETA: 59.5m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [cold] Step 450/2500 | loss=0.0000 | gap=1.534 | sat=100.0% | norm=76.441 | ETA: 58.1m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [cold] Step 500/2500 | loss=0.0000 | gap=0.997 | sat=100.0% | norm=79.952 | ETA: 56.7m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [cold] Step 550/2500 | loss=0.0938 | gap=0.539 | sat=75.0% | norm=82.152 | ETA: 55.3m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [cold] Step 600/2500 | loss=0.0000 | gap=0.908 | sat=100.0% | norm=84.169 | ETA: 54.0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [cold] Step 650/2500 | loss=0.0469 | gap=0.865 | sat=75.0% | norm=86.202 | ETA: 52.6m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [cold] Step 700/2500 | loss=0.1211 | gap=0.619 | sat=75.0% | norm=89.323 | ETA: 51.2m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [cold] Step 750/2500 | loss=0.4531 | gap=0.324 | sat=50.0% | norm=92.940 | ETA: 49.7m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [cold] Step 800/2500 | loss=0.0000 | gap=1.344 | sat=100.0% | norm=96.414 | ETA: 48.2m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [cold] Step 850/2500 | loss=0.0000 | gap=0.900 | sat=100.0% | norm=101.040 | ETA: 46.8m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [cold] Step 900/2500 | loss=0.3398 | gap=0.348 | sat=75.0% | norm=105.872 | ETA: 45.3m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [cold] Step 950/2500 | loss=0.0000 | gap=1.566 | sat=100.0% | norm=108.934 | ETA: 43.9m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [cold] Step 1000/2500 | loss=0.0000 | gap=1.041 | sat=100.0% | norm=110.740 | ETA: 42.5m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [cold] Step 1050/2500 | loss=0.0000 | gap=1.163 | sat=100.0% | norm=111.964 | ETA: 41.1m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [cold] Step 1100/2500 | loss=0.0000 | gap=1.008 | sat=100.0% | norm=113.729 | ETA: 39.6m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [cold] Step 1150/2500 | loss=0.0000 | gap=0.703 | sat=100.0% | norm=114.926 | ETA: 38.2m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [cold] Step 1200/2500 | loss=0.1230 | gap=0.416 | sat=50.0% | norm=115.865 | ETA: 36.8m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [cold] Step 1250/2500 | loss=0.0000 | gap=1.797 | sat=100.0% | norm=117.522 | ETA: 35.4m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [cold] Step 1300/2500 | loss=0.0000 | gap=0.594 | sat=100.0% | norm=119.794 | ETA: 34.0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [cold] Step 1350/2500 | loss=0.0000 | gap=1.949 | sat=100.0% | norm=120.978 | ETA: 32.5m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [cold] Step 1400/2500 | loss=0.0000 | gap=1.949 | sat=100.0% | norm=123.086 | ETA: 31.1m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [cold] Step 1450/2500 | loss=0.5547 | gap=0.286 | sat=75.0% | norm=124.542 | ETA: 29.7m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [cold] Step 1500/2500 | loss=0.0781 | gap=0.696 | sat=75.0% | norm=125.412 | ETA: 28.3m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [cold] Step 1550/2500 | loss=0.0000 | gap=1.154 | sat=100.0% | norm=126.136 | ETA: 26.8m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [cold] Step 1600/2500 | loss=0.1367 | gap=0.500 | sat=75.0% | norm=126.545 | ETA: 25.4m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [cold] Step 1650/2500 | loss=0.0312 | gap=0.977 | sat=75.0% | norm=126.548 | ETA: 24.0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [cold] Step 1700/2500 | loss=0.7207 | gap=0.829 | sat=50.0% | norm=127.074 | ETA: 22.6m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [cold] Step 1750/2500 | loss=0.0000 | gap=1.320 | sat=100.0% | norm=127.814 | ETA: 21.2m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [cold] Step 1800/2500 | loss=0.1328 | gap=1.242 | sat=75.0% | norm=127.824 | ETA: 19.7m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [cold] Step 1850/2500 | loss=0.0000 | gap=1.434 | sat=100.0% | norm=128.101 | ETA: 18.3m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [cold] Step 1900/2500 | loss=0.0000 | gap=1.754 | sat=100.0% | norm=128.725 | ETA: 16.9m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [cold] Step 1950/2500 | loss=0.0000 | gap=2.285 | sat=100.0% | norm=130.362 | ETA: 15.5m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [cold] Step 2000/2500 | loss=0.7734 | gap=-0.629 | sat=25.0% | norm=131.439 | ETA: 14.1m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [cold] Step 2050/2500 | loss=0.0000 | gap=1.994 | sat=100.0% | norm=132.120 | ETA: 12.7m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [cold] Step 2100/2500 | loss=0.3750 | gap=1.129 | sat=75.0% | norm=133.499 | ETA: 11.3m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [cold] Step 2150/2500 | loss=0.6094 | gap=1.272 | sat=75.0% | norm=134.987 | ETA: 9.9m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [cold] Step 2200/2500 | loss=0.0000 | gap=1.557 | sat=100.0% | norm=137.939 | ETA: 8.5m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [cold] Step 2250/2500 | loss=0.0000 | gap=1.371 | sat=100.0% | norm=141.690 | ETA: 7.1m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [cold] Step 2300/2500 | loss=0.0664 | gap=0.602 | sat=75.0% | norm=144.102 | ETA: 5.6m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [cold] Step 2350/2500 | loss=0.0000 | gap=1.320 | sat=100.0% | norm=145.496 | ETA: 4.2m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [cold] Step 2400/2500 | loss=0.1055 | gap=1.389 | sat=75.0% | norm=147.852 | ETA: 2.8m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [cold] Step 2450/2500 | loss=0.1582 | gap=2.180 | sat=75.0% | norm=150.020 | ETA: 1.4m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [cold] Step 2500/2500 | loss=0.0000 | gap=2.199 | sat=100.0% | norm=152.686 | ETA: 0.0m\n",
      "\n",
      "  Training complete: 2500 steps in 70.6 min\n",
      "  Final prefix norm: 152.6860\n",
      "  Saved to: results/exp28/soft_prefix_contrastive_cold.pt\n"
     ]
    }
   ],
   "source": [
    "# Cell 10: Train cold start (init from random)\n",
    "\n",
    "result_cold = train_contrastive_soft_prefix(\n",
    "    init_mode='cold',\n",
    "    train_data=train_queries,\n",
    "    n_epochs=N_EPOCHS,\n",
    "    lr=LR,\n",
    "    grad_accum=GRAD_ACCUM,\n",
    "    margin=MARGIN,\n",
    "    checkpoint_path=CHECKPOINT_TRAIN_COLD_PATH,\n",
    "    save_path=SOFT_COLD_PATH,\n",
    "    warmup_steps=WARMUP_STEPS,\n",
    ")\n",
    "soft_contrastive_cold = result_cold['soft_prefix']\n",
    "history_cold = result_cold['history']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cell-11",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-17T00:04:18.454573Z",
     "iopub.status.busy": "2026-02-17T00:04:18.454034Z",
     "iopub.status.idle": "2026-02-17T00:04:20.266012Z",
     "shell.execute_reply": "2026-02-17T00:04:20.265269Z"
    },
    "papermill": {
     "duration": 1.824832,
     "end_time": "2026-02-17T00:04:20.267505",
     "exception": false,
     "start_time": "2026-02-17T00:04:18.442673",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to results/exp28/training_curves.png\n"
     ]
    }
   ],
   "source": [
    "# Cell 11: Training curves\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "\n",
    "for hist, label, color in [(history_warm, 'warm (exp25_fact)', '#ff7f0e'),\n",
    "                            (history_cold, 'cold (random)', '#1f77b4')]:\n",
    "    if not hist:\n",
    "        continue\n",
    "    steps = [h['optim_step'] for h in hist]\n",
    "    losses = [h['avg_loss'] for h in hist]\n",
    "    gaps = [h['avg_nll_gap'] for h in hist]\n",
    "    sats = [h['satisfied_frac'] for h in hist]\n",
    "    pnorms = [h['prefix_norm'] for h in hist]\n",
    "\n",
    "    w = min(20, len(losses) // 3 + 1)\n",
    "\n",
    "    # Panel 1: Contrastive loss\n",
    "    ax = axes[0, 0]\n",
    "    ax.plot(steps, losses, alpha=0.2, color=color)\n",
    "    if len(losses) > w:\n",
    "        smoothed = np.convolve(losses, np.ones(w)/w, mode='valid')\n",
    "        ax.plot(steps[w-1:], smoothed, linewidth=2, color=color, label=label)\n",
    "    else:\n",
    "        ax.plot(steps, losses, linewidth=2, color=color, label=label)\n",
    "\n",
    "    # Panel 2: NLL gap (irr - rel)\n",
    "    ax = axes[0, 1]\n",
    "    ax.plot(steps, gaps, alpha=0.2, color=color)\n",
    "    if len(gaps) > w:\n",
    "        smoothed = np.convolve(gaps, np.ones(w)/w, mode='valid')\n",
    "        ax.plot(steps[w-1:], smoothed, linewidth=2, color=color, label=label)\n",
    "    else:\n",
    "        ax.plot(steps, gaps, linewidth=2, color=color, label=label)\n",
    "\n",
    "    # Panel 3: Satisfied fraction\n",
    "    ax = axes[1, 0]\n",
    "    ax.plot(steps, sats, alpha=0.2, color=color)\n",
    "    if len(sats) > w:\n",
    "        smoothed = np.convolve(sats, np.ones(w)/w, mode='valid')\n",
    "        ax.plot(steps[w-1:], smoothed, linewidth=2, color=color, label=label)\n",
    "    else:\n",
    "        ax.plot(steps, sats, linewidth=2, color=color, label=label)\n",
    "\n",
    "    # Panel 4: Prefix norm\n",
    "    ax = axes[1, 1]\n",
    "    ax.plot(steps, pnorms, linewidth=2, color=color, label=label)\n",
    "\n",
    "axes[0, 0].set_xlabel('Optimizer Step')\n",
    "axes[0, 0].set_ylabel('Hinge Loss')\n",
    "axes[0, 0].set_title('Contrastive Loss')\n",
    "axes[0, 0].legend(fontsize=8)\n",
    "axes[0, 0].grid(alpha=0.3)\n",
    "\n",
    "axes[0, 1].set_xlabel('Optimizer Step')\n",
    "axes[0, 1].set_ylabel('NLL Gap (irr - rel)')\n",
    "axes[0, 1].set_title('NLL Gap (positive = correct direction)')\n",
    "axes[0, 1].axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "axes[0, 1].legend(fontsize=8)\n",
    "axes[0, 1].grid(alpha=0.3)\n",
    "\n",
    "axes[1, 0].set_xlabel('Optimizer Step')\n",
    "axes[1, 0].set_ylabel('Fraction Satisfied')\n",
    "axes[1, 0].set_title('Hinge Margin Satisfied')\n",
    "axes[1, 0].set_ylim(-0.05, 1.05)\n",
    "axes[1, 0].legend(fontsize=8)\n",
    "axes[1, 0].grid(alpha=0.3)\n",
    "\n",
    "axes[1, 1].set_xlabel('Optimizer Step')\n",
    "axes[1, 1].set_ylabel('Norm')\n",
    "axes[1, 1].set_title('Prefix Embedding Norm')\n",
    "axes[1, 1].legend(fontsize=8)\n",
    "axes[1, 1].grid(alpha=0.3)\n",
    "\n",
    "plt.suptitle('Exp 28: Contrastive Training Curves', fontsize=13)\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'training_curves.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"Saved to {RESULTS_DIR / 'training_curves.png'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cell-12",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-17T00:04:20.290122Z",
     "iopub.status.busy": "2026-02-17T00:04:20.289235Z",
     "iopub.status.idle": "2026-02-17T00:04:21.220572Z",
     "shell.execute_reply": "2026-02-17T00:04:21.219848Z"
    },
    "papermill": {
     "duration": 0.944622,
     "end_time": "2026-02-17T00:04:21.222686",
     "exception": false,
     "start_time": "2026-02-17T00:04:20.278064",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "LOADING VALIDATION QUERIES — MULTI-PASSAGE FORMAT\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total items: 10047\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "344bb38552ff42a4affb32da41877b9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filtering val:   0%|          | 0/10047 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Selected 200 val queries (1692 passages)\n",
      "Passages per query: mean=8.5\n",
      "Relevant: 221 (13.1%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 12: Load 200 validation queries — multi-passage format (like Exp 22)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"LOADING VALIDATION QUERIES — MULTI-PASSAGE FORMAT\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "dataset = load_dataset(\"microsoft/ms_marco\", \"v1.1\", split=\"validation\")\n",
    "print(f\"Total items: {len(dataset)}\")\n",
    "\n",
    "val_queries = []\n",
    "np.random.seed(SEED)  # Same seed as Exp 22 for comparability\n",
    "\n",
    "for item in tqdm(dataset, desc=\"Filtering val\"):\n",
    "    passages_info = item.get('passages', {})\n",
    "    passage_texts = passages_info.get('passage_text', [])\n",
    "    is_selected = passages_info.get('is_selected', [])\n",
    "    query = item.get('query', '')\n",
    "    answers = item.get('answers', [])\n",
    "    well_formed = item.get('wellFormedAnswers', [])\n",
    "\n",
    "    if not passage_texts or not query:\n",
    "        continue\n",
    "    if not is_selected or sum(is_selected) == 0:\n",
    "        continue\n",
    "\n",
    "    word_counts = [count_words(p) for p in passage_texts]\n",
    "    if any(wc > MAX_PASSAGE_WORDS for wc in word_counts):\n",
    "        continue\n",
    "\n",
    "    answer = None\n",
    "    if well_formed and len(well_formed) > 0 and well_formed[0] != '[]':\n",
    "        answer = well_formed[0]\n",
    "    elif answers and len(answers) > 0 and answers[0] != 'No Answer Present.':\n",
    "        answer = answers[0]\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "    passage_list = []\n",
    "    for i, (ptext, sel) in enumerate(zip(passage_texts, is_selected)):\n",
    "        passage_list.append({\n",
    "            'passage': ptext,\n",
    "            'is_relevant': bool(sel == 1),\n",
    "            'word_count': word_counts[i],\n",
    "            'passage_idx': i,\n",
    "        })\n",
    "\n",
    "    val_queries.append({\n",
    "        'query': query,\n",
    "        'answer': answer,\n",
    "        'passages': passage_list,\n",
    "        'n_passages': len(passage_list),\n",
    "        'n_relevant': sum(1 for p in passage_list if p['is_relevant']),\n",
    "    })\n",
    "\n",
    "    if len(val_queries) >= N_EVAL * 3:\n",
    "        break\n",
    "\n",
    "np.random.shuffle(val_queries)\n",
    "val_queries = val_queries[:N_EVAL]\n",
    "N = len(val_queries)\n",
    "\n",
    "n_val_passages = [q['n_passages'] for q in val_queries]\n",
    "total_val_passages = sum(n_val_passages)\n",
    "total_val_rel = sum(q['n_relevant'] for q in val_queries)\n",
    "\n",
    "print(f\"\\nSelected {N} val queries ({total_val_passages} passages)\")\n",
    "print(f\"Passages per query: mean={np.mean(n_val_passages):.1f}\")\n",
    "print(f\"Relevant: {total_val_rel} ({100*total_val_rel/total_val_passages:.1f}%)\")\n",
    "\n",
    "del dataset\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cell-13",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-17T00:04:21.246335Z",
     "iopub.status.busy": "2026-02-17T00:04:21.246007Z",
     "iopub.status.idle": "2026-02-17T01:02:18.427981Z",
     "shell.execute_reply": "2026-02-17T01:02:18.427077Z"
    },
    "papermill": {
     "duration": 3477.196292,
     "end_time": "2026-02-17T01:02:18.429906",
     "exception": false,
     "start_time": "2026-02-17T00:04:21.233614",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "RANKING EVALUATION (200 queries, 1692 passages, 5 conditions)\n",
      "======================================================================\n",
      "Loaded exp25_fact from results/exp25/soft_prefix_fact.pt\n",
      "No checkpoint found. Starting fresh.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e7b4897e3ec4c22bc5417e45bd24752",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Ranking eval:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 10/200 | 10 done in 2.8m | ETA: 53.4 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 20/200 | 20 done in 5.8m | ETA: 52.4 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 30/200 | 30 done in 8.7m | ETA: 49.2 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 40/200 | 40 done in 11.7m | ETA: 46.8 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 50/200 | 50 done in 14.5m | ETA: 43.5 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 60/200 | 60 done in 17.5m | ETA: 40.8 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 70/200 | 70 done in 20.4m | ETA: 37.9 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 80/200 | 80 done in 23.2m | ETA: 34.9 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 90/200 | 90 done in 26.1m | ETA: 31.9 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 100/200 | 100 done in 28.9m | ETA: 28.9 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 110/200 | 110 done in 31.9m | ETA: 26.1 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 120/200 | 120 done in 34.8m | ETA: 23.2 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 130/200 | 130 done in 37.7m | ETA: 20.3 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 140/200 | 140 done in 40.6m | ETA: 17.4 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 150/200 | 150 done in 43.3m | ETA: 14.4 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 160/200 | 160 done in 46.2m | ETA: 11.6 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 170/200 | 170 done in 49.1m | ETA: 8.7 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 180/200 | 180 done in 52.0m | ETA: 5.8 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 190/200 | 190 done in 54.9m | ETA: 2.9 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 200/200 | 200 done in 58.0m | ETA: 0.0 min\n",
      "\n",
      "Eval complete: 200 queries in 58.0 min\n"
     ]
    }
   ],
   "source": [
    "# Cell 13: Ranking eval — score ALL passages per query under 5 conditions\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"RANKING EVALUATION ({N} queries, {total_val_passages} passages, 5 conditions)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Load soft prefixes (in case of restart)\n",
    "if 'soft_contrastive_warm' not in dir():\n",
    "    soft_contrastive_warm = torch.load(SOFT_WARM_PATH).to(exp_config.device)\n",
    "    print(f\"Loaded contrastive_warm from {SOFT_WARM_PATH}\")\n",
    "if 'soft_contrastive_cold' not in dir():\n",
    "    soft_contrastive_cold = torch.load(SOFT_COLD_PATH).to(exp_config.device)\n",
    "    print(f\"Loaded contrastive_cold from {SOFT_COLD_PATH}\")\n",
    "\n",
    "soft_exp25_fact = torch.load(EXP25_SOFT_FACT).to(exp_config.device)\n",
    "print(f\"Loaded exp25_fact from {EXP25_SOFT_FACT}\")\n",
    "\n",
    "layer_indices = list(range(CUTOFF))\n",
    "\n",
    "def score_baseline_nll(query_prompt, answer_text):\n",
    "    \"\"\"Score answer with BOS-only cache (no document).\"\"\"\n",
    "    bos_id = torch.tensor([[tokenizer.bos_token_id]], device=exp_config.device)\n",
    "    with torch.no_grad():\n",
    "        bos_out = model(input_ids=bos_id,\n",
    "                        attention_mask=torch.ones_like(bos_id),\n",
    "                        use_cache=True, return_dict=True)\n",
    "    bos_cache = _ensure_dynamic_cache(bos_out.past_key_values)\n",
    "    del bos_out\n",
    "    nll = score_answer_with_cache(\n",
    "        bos_cache, 1, query_prompt, answer_text,\n",
    "        model, tokenizer, exp_config)\n",
    "    return nll\n",
    "\n",
    "\n",
    "def score_soft_passage(soft_embs, doc_ids, bos_id, doc_len, context_len,\n",
    "                        bare_cache, query_prompt, answer_text):\n",
    "    \"\"\"Score a passage through soft prefix hybrid cache (no grad).\"\"\"\n",
    "    with torch.no_grad():\n",
    "        bos_emb = embed_fn(bos_id)\n",
    "        doc_emb = embed_fn(doc_ids)\n",
    "        soft_cast = soft_embs.to(device=exp_config.device, dtype=bos_emb.dtype)\n",
    "\n",
    "        inputs_embeds = torch.cat([bos_emb, soft_cast, doc_emb], dim=1)\n",
    "        total_len = inputs_embeds.shape[1]\n",
    "        attn_mask = torch.ones((1, total_len), device=exp_config.device, dtype=torch.long)\n",
    "\n",
    "        soft_out = model(inputs_embeds=inputs_embeds,\n",
    "                        attention_mask=attn_mask,\n",
    "                        use_cache=True, return_dict=True)\n",
    "        soft_cache = _ensure_dynamic_cache(soft_out.past_key_values)\n",
    "        del soft_out\n",
    "\n",
    "        soft_trunc = extract_and_truncate_cache_with_bos(soft_cache, doc_len)\n",
    "        del soft_cache\n",
    "\n",
    "        vel_cache = replace_values_at_layers(bare_cache, soft_trunc, layer_indices)\n",
    "        del soft_trunc\n",
    "\n",
    "        nll = score_answer_with_cache(\n",
    "            deepcopy_cache(vel_cache), context_len,\n",
    "            query_prompt, answer_text, model, tokenizer, exp_config)\n",
    "        del vel_cache\n",
    "\n",
    "    return nll\n",
    "\n",
    "\n",
    "# Checkpoint resume\n",
    "all_results = []\n",
    "start_idx = 0\n",
    "\n",
    "if CHECKPOINT_EVAL_PATH.exists():\n",
    "    with open(CHECKPOINT_EVAL_PATH, 'r') as f:\n",
    "        ckpt = json.load(f)\n",
    "    ckpt_queries = ckpt.get('query_texts', [])\n",
    "    current_queries = [q['query'] for q in val_queries]\n",
    "    if ckpt_queries == current_queries:\n",
    "        all_results = ckpt['results']\n",
    "        start_idx = len(all_results)\n",
    "        print(f\"Resuming from checkpoint: {start_idx}/{N}\")\n",
    "    else:\n",
    "        print(\"Checkpoint query mismatch. Starting fresh.\")\n",
    "else:\n",
    "    print(\"No checkpoint found. Starting fresh.\")\n",
    "\n",
    "EVAL_CHECKPOINT_EVERY = 10\n",
    "t_start = time.time()\n",
    "\n",
    "for qidx in tqdm(range(start_idx, N), initial=start_idx, total=N, desc=\"Ranking eval\"):\n",
    "    qdata = val_queries[qidx]\n",
    "    query_prompt = QUERY_TEMPLATE.format(query=qdata['query'])\n",
    "    answer_text = ANSWER_TEMPLATE.format(answer=qdata['answer'])\n",
    "\n",
    "    # Baseline NLL (BOS-only, once per query)\n",
    "    nll_baseline = score_baseline_nll(query_prompt, answer_text)\n",
    "\n",
    "    passage_results = []\n",
    "    for pidx, pinfo in enumerate(qdata['passages']):\n",
    "        passage_text = pinfo['passage']\n",
    "        document_text = DOCUMENT_TEMPLATE.format(document=passage_text)\n",
    "\n",
    "        # Matched tokenization\n",
    "        full_text = sf_str + document_text\n",
    "        full_enc = tokenizer(full_text, return_tensors=\"pt\",\n",
    "                              add_special_tokens=True, padding=False, truncation=False)\n",
    "        full_ids = full_enc['input_ids'].to(exp_config.device)\n",
    "        sf_enc = tokenizer(sf_str, return_tensors=\"pt\",\n",
    "                            add_special_tokens=True, padding=False, truncation=False)\n",
    "        sf_len = sf_enc['input_ids'].shape[1]\n",
    "        bos_id = full_ids[:, :1]\n",
    "        doc_ids = full_ids[:, sf_len:]\n",
    "        doc_len = doc_ids.shape[1]\n",
    "        context_len = 1 + doc_len\n",
    "\n",
    "        del full_enc, full_ids, sf_enc\n",
    "\n",
    "        # Build bare cache\n",
    "        bare_input = torch.cat([bos_id, doc_ids], dim=1)\n",
    "        with torch.no_grad():\n",
    "            bare_out = model(input_ids=bare_input,\n",
    "                             attention_mask=torch.ones_like(bare_input),\n",
    "                             use_cache=True, return_dict=True)\n",
    "        bare_cache = _ensure_dynamic_cache(bare_out.past_key_values)\n",
    "        del bare_out, bare_input\n",
    "\n",
    "        # Condition 1: bare\n",
    "        nll_bare = score_answer_with_cache(\n",
    "            deepcopy_cache(bare_cache), context_len,\n",
    "            query_prompt, answer_text, model, tokenizer, exp_config)\n",
    "\n",
    "        # Condition 2: exp25_fact\n",
    "        nll_exp25 = score_soft_passage(\n",
    "            soft_exp25_fact, doc_ids, bos_id, doc_len, context_len,\n",
    "            bare_cache, query_prompt, answer_text)\n",
    "\n",
    "        # Condition 3: contrastive_warm\n",
    "        nll_warm = score_soft_passage(\n",
    "            soft_contrastive_warm, doc_ids, bos_id, doc_len, context_len,\n",
    "            bare_cache, query_prompt, answer_text)\n",
    "\n",
    "        # Condition 4: contrastive_cold\n",
    "        nll_cold = score_soft_passage(\n",
    "            soft_contrastive_cold, doc_ids, bos_id, doc_len, context_len,\n",
    "            bare_cache, query_prompt, answer_text)\n",
    "\n",
    "        del bare_cache\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        passage_results.append({\n",
    "            'passage_idx': pinfo['passage_idx'],\n",
    "            'is_relevant': pinfo['is_relevant'],\n",
    "            'word_count': pinfo['word_count'],\n",
    "            'doc_len': doc_len,\n",
    "            'nll_bare': float(nll_bare),\n",
    "            'nll_exp25_fact': float(nll_exp25),\n",
    "            'nll_contrastive_warm': float(nll_warm),\n",
    "            'nll_contrastive_cold': float(nll_cold),\n",
    "            'nll_baseline': float(nll_baseline),\n",
    "        })\n",
    "\n",
    "    all_results.append({\n",
    "        'query_idx': qidx,\n",
    "        'query': qdata['query'],\n",
    "        'answer': qdata['answer'],\n",
    "        'n_passages': len(passage_results),\n",
    "        'n_relevant': qdata['n_relevant'],\n",
    "        'nll_baseline': float(nll_baseline),\n",
    "        'passage_data': passage_results,\n",
    "    })\n",
    "\n",
    "    # Checkpoint\n",
    "    if (qidx + 1) % EVAL_CHECKPOINT_EVERY == 0 or qidx == N - 1:\n",
    "        ckpt_data = {\n",
    "            'results': all_results,\n",
    "            'query_texts': [q['query'] for q in val_queries],\n",
    "            'completed': len(all_results),\n",
    "            'total': N,\n",
    "            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        }\n",
    "        with open(CHECKPOINT_EVAL_PATH, 'w') as f:\n",
    "            json.dump(ckpt_data, f)\n",
    "        elapsed = time.time() - t_start\n",
    "        n_done = qidx - start_idx + 1\n",
    "        rate = n_done / elapsed if elapsed > 0 else 0\n",
    "        remaining = (N - qidx - 1) / rate if rate > 0 else 0\n",
    "        tqdm.write(f\"  Checkpoint {qidx+1}/{N} | {n_done} done in {elapsed/60:.1f}m | \"\n",
    "                   f\"ETA: {remaining/60:.1f} min\")\n",
    "\n",
    "elapsed_total = time.time() - t_start\n",
    "print(f\"\\nEval complete: {len(all_results)} queries in {elapsed_total/60:.1f} min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cell-14",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-17T01:02:18.457064Z",
     "iopub.status.busy": "2026-02-17T01:02:18.456697Z",
     "iopub.status.idle": "2026-02-17T01:02:18.515965Z",
     "shell.execute_reply": "2026-02-17T01:02:18.515013Z"
    },
    "papermill": {
     "duration": 0.074637,
     "end_time": "2026-02-17T01:02:18.517602",
     "exception": false,
     "start_time": "2026-02-17T01:02:18.442965",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "RANKING ANALYSIS\n",
      "======================================================================\n",
      "Total passages: 1692\n",
      "Relevant: 221 (13.1%), Irrelevant: 1471\n",
      "\n",
      "Method                    AUC\n",
      "------------------------------\n",
      "Raw bare                0.828\n",
      "Raw exp25_fact          0.839\n",
      "Raw contr_warm          0.819 <<<\n",
      "Raw contr_cold          0.735\n",
      "PMI bare                0.841\n",
      "PMI exp25_fact          0.822\n",
      "PMI contr_warm          0.787 <<<\n",
      "PMI contr_cold          0.794\n",
      "\n",
      "Method                 MRR@10\n",
      "------------------------------\n",
      "Raw bare                0.860\n",
      "Raw exp25_fact          0.872\n",
      "Raw contr_warm          0.872 <<<\n",
      "Raw contr_cold          0.808\n",
      "PMI bare                0.860\n",
      "PMI exp25_fact          0.872\n",
      "PMI contr_warm          0.872 <<<\n",
      "PMI contr_cold          0.808\n",
      "\n",
      "Method                 Mean Rel   Mean Irr       Diff        d\n",
      "--------------------------------------------------------------\n",
      "Raw bare                 0.5641     2.4836    +1.9195   +1.201\n",
      "Raw exp25_fact           0.4062     2.2412    +1.8350   +1.322\n",
      "Raw contr_warm           0.6249     2.0496    +1.4247   +1.152\n",
      "Raw contr_cold           1.8222     3.0177    +1.1955   +0.680\n",
      "PMI bare                -2.2679    -0.3045    +1.9634   +1.647\n",
      "PMI exp25_fact          -2.4258    -0.5469    +1.8789   +1.459\n",
      "PMI contr_warm          -2.2071    -0.7385    +1.4686   +1.194\n",
      "PMI contr_cold          -1.0099     0.2295    +1.2394   +0.991\n",
      "\n",
      "======================================================================\n",
      "SUMMARY TABLE\n",
      "======================================================================\n",
      "Method                    AUC   MRR@10   Diff NLL        d\n",
      "----------------------------------------------------------\n",
      "Raw bare                0.828    0.860    +1.9195   +1.201\n",
      "Raw exp25_fact          0.839    0.872    +1.8350   +1.322\n",
      "Raw contr_warm          0.819    0.872    +1.4247   +1.152\n",
      "Raw contr_cold          0.735    0.808    +1.1955   +0.680\n",
      "PMI bare                0.841    0.860    +1.9634   +1.647\n",
      "PMI exp25_fact          0.822    0.872    +1.8789   +1.459\n",
      "PMI contr_warm          0.787    0.872    +1.4686   +1.194\n",
      "PMI contr_cold          0.794    0.808    +1.2394   +0.991\n"
     ]
    }
   ],
   "source": [
    "# Cell 14: Ranking analysis — AUC-ROC, MRR@10, differential NLL\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"RANKING ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Flatten passage-level data\n",
    "is_relevant_all = []\n",
    "nll_bare_all = []\n",
    "nll_exp25_all = []\n",
    "nll_warm_all = []\n",
    "nll_cold_all = []\n",
    "nll_baseline_all = []\n",
    "\n",
    "for r in all_results:\n",
    "    for p in r['passage_data']:\n",
    "        is_relevant_all.append(int(p['is_relevant']))\n",
    "        nll_bare_all.append(p['nll_bare'])\n",
    "        nll_exp25_all.append(p['nll_exp25_fact'])\n",
    "        nll_warm_all.append(p['nll_contrastive_warm'])\n",
    "        nll_cold_all.append(p['nll_contrastive_cold'])\n",
    "        nll_baseline_all.append(p['nll_baseline'])\n",
    "\n",
    "is_relevant = np.array(is_relevant_all)\n",
    "nll_bare = np.array(nll_bare_all)\n",
    "nll_exp25 = np.array(nll_exp25_all)\n",
    "nll_warm = np.array(nll_warm_all)\n",
    "nll_cold = np.array(nll_cold_all)\n",
    "nll_baseline = np.array(nll_baseline_all)\n",
    "\n",
    "# PMI scores\n",
    "pmi_bare = nll_bare - nll_baseline\n",
    "pmi_exp25 = nll_exp25 - nll_baseline\n",
    "pmi_warm = nll_warm - nll_baseline\n",
    "pmi_cold = nll_cold - nll_baseline\n",
    "\n",
    "n_total = len(is_relevant)\n",
    "n_rel = int(is_relevant.sum())\n",
    "n_irr = n_total - n_rel\n",
    "\n",
    "print(f\"Total passages: {n_total}\")\n",
    "print(f\"Relevant: {n_rel} ({100*n_rel/n_total:.1f}%), Irrelevant: {n_irr}\")\n",
    "\n",
    "# === AUC-ROC ===\n",
    "scoring_methods = {\n",
    "    'Raw bare': nll_bare,\n",
    "    'Raw exp25_fact': nll_exp25,\n",
    "    'Raw contr_warm': nll_warm,\n",
    "    'Raw contr_cold': nll_cold,\n",
    "    'PMI bare': pmi_bare,\n",
    "    'PMI exp25_fact': pmi_exp25,\n",
    "    'PMI contr_warm': pmi_warm,\n",
    "    'PMI contr_cold': pmi_cold,\n",
    "}\n",
    "\n",
    "auc_results = {}\n",
    "print(f\"\\n{'Method':<20} {'AUC':>8}\")\n",
    "print(\"-\" * 30)\n",
    "for name, scores in scoring_methods.items():\n",
    "    auc = roc_auc_score(is_relevant, -scores)\n",
    "    auc_results[name] = float(auc)\n",
    "    marker = \" <<<\" if name in ['Raw contr_warm', 'PMI contr_warm'] else \"\"\n",
    "    print(f\"{name:<20} {auc:>8.3f}{marker}\")\n",
    "\n",
    "# === MRR@10 ===\n",
    "def compute_mrr_at_k(all_results, score_fn, k=10):\n",
    "    rr_list = []\n",
    "    for r in all_results:\n",
    "        passages = r['passage_data']\n",
    "        scored = [(score_fn(p), p['is_relevant']) for p in passages]\n",
    "        scored.sort(key=lambda x: x[0])\n",
    "        rr = 0.0\n",
    "        for rank, (score, rel) in enumerate(scored[:k], 1):\n",
    "            if rel:\n",
    "                rr = 1.0 / rank\n",
    "                break\n",
    "        rr_list.append(rr)\n",
    "    return np.mean(rr_list), rr_list\n",
    "\n",
    "mrr_fns = {\n",
    "    'Raw bare': lambda p: p['nll_bare'],\n",
    "    'Raw exp25_fact': lambda p: p['nll_exp25_fact'],\n",
    "    'Raw contr_warm': lambda p: p['nll_contrastive_warm'],\n",
    "    'Raw contr_cold': lambda p: p['nll_contrastive_cold'],\n",
    "    'PMI bare': lambda p: p['nll_bare'] - p['nll_baseline'],\n",
    "    'PMI exp25_fact': lambda p: p['nll_exp25_fact'] - p['nll_baseline'],\n",
    "    'PMI contr_warm': lambda p: p['nll_contrastive_warm'] - p['nll_baseline'],\n",
    "    'PMI contr_cold': lambda p: p['nll_contrastive_cold'] - p['nll_baseline'],\n",
    "}\n",
    "\n",
    "mrr_results = {}\n",
    "mrr_per_query = {}\n",
    "print(f\"\\n{'Method':<20} {'MRR@10':>8}\")\n",
    "print(\"-\" * 30)\n",
    "for name, fn in mrr_fns.items():\n",
    "    mrr, rr_list = compute_mrr_at_k(all_results, fn, k=10)\n",
    "    mrr_results[name] = float(mrr)\n",
    "    mrr_per_query[name] = rr_list\n",
    "    marker = \" <<<\" if name in ['Raw contr_warm', 'PMI contr_warm'] else \"\"\n",
    "    print(f\"{name:<20} {mrr:>8.3f}{marker}\")\n",
    "\n",
    "# === Differential NLL ===\n",
    "print(f\"\\n{'Method':<20} {'Mean Rel':>10} {'Mean Irr':>10} {'Diff':>10} {'d':>8}\")\n",
    "print(\"-\" * 62)\n",
    "diff_results = {}\n",
    "for name, scores in scoring_methods.items():\n",
    "    rel_vals = scores[is_relevant == 1]\n",
    "    irr_vals = scores[is_relevant == 0]\n",
    "    diff = np.mean(irr_vals) - np.mean(rel_vals)\n",
    "    pooled_std = np.sqrt(\n",
    "        (np.var(rel_vals) * (len(rel_vals)-1) + np.var(irr_vals) * (len(irr_vals)-1)) /\n",
    "        (len(rel_vals) + len(irr_vals) - 2)\n",
    "    )\n",
    "    d = diff / pooled_std if pooled_std > 0 else 0\n",
    "    t_stat, p_val = stats.ttest_ind(irr_vals, rel_vals)\n",
    "    diff_results[name] = {\n",
    "        'mean_relevant': float(np.mean(rel_vals)),\n",
    "        'mean_irrelevant': float(np.mean(irr_vals)),\n",
    "        'diff': float(diff),\n",
    "        'cohens_d': float(d),\n",
    "        't_stat': float(t_stat),\n",
    "        'p_value': float(p_val),\n",
    "    }\n",
    "    print(f\"{name:<20} {np.mean(rel_vals):>10.4f} {np.mean(irr_vals):>10.4f} \"\n",
    "          f\"{diff:>+10.4f} {d:>+8.3f}\")\n",
    "\n",
    "# === Summary ===\n",
    "print(f\"\\n{'=' * 70}\")\n",
    "print(\"SUMMARY TABLE\")\n",
    "print(f\"{'=' * 70}\")\n",
    "print(f\"{'Method':<20} {'AUC':>8} {'MRR@10':>8} {'Diff NLL':>10} {'d':>8}\")\n",
    "print(\"-\" * 58)\n",
    "for name in scoring_methods:\n",
    "    print(f\"{name:<20} {auc_results[name]:>8.3f} {mrr_results[name]:>8.3f} \"\n",
    "          f\"{diff_results[name]['diff']:>+10.4f} {diff_results[name]['cohens_d']:>+8.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cell-15",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-17T01:02:18.544377Z",
     "iopub.status.busy": "2026-02-17T01:02:18.543522Z",
     "iopub.status.idle": "2026-02-17T01:02:18.565807Z",
     "shell.execute_reply": "2026-02-17T01:02:18.564823Z"
    },
    "papermill": {
     "duration": 0.03734,
     "end_time": "2026-02-17T01:02:18.567378",
     "exception": false,
     "start_time": "2026-02-17T01:02:18.530038",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "NLL IMPROVEMENT CHECK (avg NLL vs bare, per-query paired)\n",
      "======================================================================\n",
      "\n",
      "Condition              Mean NLL      Delta        d    Win%            p\n",
      "------------------------------------------------------------------------\n",
      "bare                     0.4746\n",
      "exp25_fact               0.3449    +0.1296   +0.304   53.0%     2.64e-05 ***\n",
      "contrastive_warm         0.5730    -0.0985   -0.197   24.0%     5.91e-03 **\n",
      "contrastive_cold         1.7646    -1.2900   -0.990    3.5%     1.89e-31 ***\n"
     ]
    }
   ],
   "source": [
    "# Cell 15: NLL improvement check — does contrastive prefix still help average NLL?\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"NLL IMPROVEMENT CHECK (avg NLL vs bare, per-query paired)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Compute per-query mean NLL for each condition\n",
    "per_query_bare = []\n",
    "per_query_exp25 = []\n",
    "per_query_warm = []\n",
    "per_query_cold = []\n",
    "\n",
    "for r in all_results:\n",
    "    # Use only the relevant passage(s) for NLL comparison (like Exp 25)\n",
    "    rel_passages = [p for p in r['passage_data'] if p['is_relevant']]\n",
    "    if not rel_passages:\n",
    "        continue\n",
    "    per_query_bare.append(np.mean([p['nll_bare'] for p in rel_passages]))\n",
    "    per_query_exp25.append(np.mean([p['nll_exp25_fact'] for p in rel_passages]))\n",
    "    per_query_warm.append(np.mean([p['nll_contrastive_warm'] for p in rel_passages]))\n",
    "    per_query_cold.append(np.mean([p['nll_contrastive_cold'] for p in rel_passages]))\n",
    "\n",
    "pq_bare = np.array(per_query_bare)\n",
    "pq_exp25 = np.array(per_query_exp25)\n",
    "pq_warm = np.array(per_query_warm)\n",
    "pq_cold = np.array(per_query_cold)\n",
    "\n",
    "nll_conditions = {\n",
    "    'exp25_fact': pq_exp25,\n",
    "    'contrastive_warm': pq_warm,\n",
    "    'contrastive_cold': pq_cold,\n",
    "}\n",
    "\n",
    "print(f\"\\n{'Condition':<20} {'Mean NLL':>10} {'Delta':>10} {'d':>8} {'Win%':>7} {'p':>12}\")\n",
    "print(\"-\" * 72)\n",
    "print(f\"{'bare':<20} {np.mean(pq_bare):>10.4f}\")\n",
    "\n",
    "nll_improvement = {}\n",
    "for name, arr in nll_conditions.items():\n",
    "    delta = pq_bare - arr\n",
    "    d = cohens_d(delta)\n",
    "    win = np.mean(delta > 0) * 100\n",
    "    _, p_val = stats.ttest_1samp(delta, 0)\n",
    "    sig = '***' if p_val < 0.001 else '**' if p_val < 0.01 else '*' if p_val < 0.05 else 'ns'\n",
    "    print(f\"{name:<20} {np.mean(arr):>10.4f} {np.mean(delta):>+10.4f} \"\n",
    "          f\"{d:>+8.3f} {win:>6.1f}% {p_val:>12.2e} {sig}\")\n",
    "    nll_improvement[name] = {\n",
    "        'mean_nll': float(np.mean(arr)),\n",
    "        'mean_delta': float(np.mean(delta)),\n",
    "        'cohens_d': float(d),\n",
    "        'win_pct': float(win),\n",
    "        'p_value': float(p_val),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cell-16",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-17T01:02:18.594070Z",
     "iopub.status.busy": "2026-02-17T01:02:18.593305Z",
     "iopub.status.idle": "2026-02-17T01:02:19.862049Z",
     "shell.execute_reply": "2026-02-17T01:02:19.861000Z"
    },
    "papermill": {
     "duration": 1.283798,
     "end_time": "2026-02-17T01:02:19.863730",
     "exception": false,
     "start_time": "2026-02-17T01:02:18.579932",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to results/exp28/ranking_plots.png\n"
     ]
    }
   ],
   "source": [
    "# Cell 16: Plots — ROC curves, score distributions, MRR scatter, summary bars\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "colors = {\n",
    "    'Raw bare': '#1f77b4',\n",
    "    'Raw exp25_fact': '#2ca02c',\n",
    "    'Raw contr_warm': '#ff7f0e',\n",
    "    'Raw contr_cold': '#d62728',\n",
    "    'PMI bare': '#1f77b4',\n",
    "    'PMI exp25_fact': '#2ca02c',\n",
    "    'PMI contr_warm': '#ff7f0e',\n",
    "    'PMI contr_cold': '#d62728',\n",
    "}\n",
    "\n",
    "# --- Panel 1: ROC curves (Raw NLL) ---\n",
    "ax = axes[0, 0]\n",
    "for name in ['Raw bare', 'Raw exp25_fact', 'Raw contr_warm', 'Raw contr_cold']:\n",
    "    scores = scoring_methods[name]\n",
    "    fpr, tpr, _ = roc_curve(is_relevant, -scores)\n",
    "    ax.plot(fpr, tpr, color=colors[name], linewidth=2,\n",
    "            label=f\"{name} (AUC={auc_results[name]:.3f})\")\n",
    "ax.plot([0, 1], [0, 1], 'k--', alpha=0.3)\n",
    "ax.set_xlabel('False Positive Rate')\n",
    "ax.set_ylabel('True Positive Rate')\n",
    "ax.set_title('ROC Curves — Raw NLL')\n",
    "ax.legend(fontsize=8, loc='lower right')\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "# --- Panel 2: ROC curves (PMI) ---\n",
    "ax = axes[0, 1]\n",
    "for name in ['PMI bare', 'PMI exp25_fact', 'PMI contr_warm', 'PMI contr_cold']:\n",
    "    scores = scoring_methods[name]\n",
    "    fpr, tpr, _ = roc_curve(is_relevant, -scores)\n",
    "    ax.plot(fpr, tpr, color=colors[name], linewidth=2,\n",
    "            label=f\"{name} (AUC={auc_results[name]:.3f})\")\n",
    "ax.plot([0, 1], [0, 1], 'k--', alpha=0.3)\n",
    "ax.set_xlabel('False Positive Rate')\n",
    "ax.set_ylabel('True Positive Rate')\n",
    "ax.set_title('ROC Curves — PMI')\n",
    "ax.legend(fontsize=8, loc='lower right')\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "# --- Panel 3: AUC + MRR summary bars ---\n",
    "ax = axes[1, 0]\n",
    "methods = ['Raw bare', 'Raw exp25_fact', 'Raw contr_warm', 'Raw contr_cold']\n",
    "method_labels = ['bare', 'exp25\\nfact', 'contr\\nwarm', 'contr\\ncold']\n",
    "aucs = [auc_results[m] for m in methods]\n",
    "mrrs = [mrr_results[m] for m in methods]\n",
    "\n",
    "x = np.arange(len(methods))\n",
    "width = 0.35\n",
    "ax.bar(x - width/2, aucs, width, label='AUC', color=[colors[m] for m in methods], alpha=0.7)\n",
    "ax.bar(x + width/2, mrrs, width, label='MRR@10', color=[colors[m] for m in methods], alpha=0.4,\n",
    "       edgecolor=[colors[m] for m in methods], linewidth=2)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(method_labels, fontsize=8)\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_title('AUC & MRR@10 (Raw NLL)')\n",
    "ax.legend(fontsize=8)\n",
    "ax.set_ylim(0.5, 1.0)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "for i, (a, m) in enumerate(zip(aucs, mrrs)):\n",
    "    ax.text(i - width/2, a + 0.005, f'{a:.3f}', ha='center', va='bottom', fontsize=7)\n",
    "    ax.text(i + width/2, m + 0.005, f'{m:.3f}', ha='center', va='bottom', fontsize=7)\n",
    "\n",
    "# --- Panel 4: NLL improvement (per-query d vs bare) ---\n",
    "ax = axes[1, 1]\n",
    "nll_names = ['exp25_fact', 'contrastive_warm', 'contrastive_cold']\n",
    "nll_labels = ['exp25\\nfact', 'contr\\nwarm', 'contr\\ncold']\n",
    "nll_ds = [nll_improvement[n]['cohens_d'] for n in nll_names]\n",
    "nll_colors = ['#2ca02c', '#ff7f0e', '#d62728']\n",
    "\n",
    "bars = ax.bar(range(len(nll_names)), nll_ds, color=nll_colors,\n",
    "              edgecolor='black', linewidth=0.5)\n",
    "ax.axhline(y=0, color='gray', linestyle='-', alpha=0.5)\n",
    "ax.set_xticks(range(len(nll_names)))\n",
    "ax.set_xticklabels(nll_labels, fontsize=8)\n",
    "ax.set_ylabel(\"Cohen's d vs bare\")\n",
    "ax.set_title('NLL Improvement (relevant passages only)')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "for i, d_val in enumerate(nll_ds):\n",
    "    sig = '***' if nll_improvement[nll_names[i]]['p_value'] < 0.001 else \\\n",
    "          '**' if nll_improvement[nll_names[i]]['p_value'] < 0.01 else \\\n",
    "          '*' if nll_improvement[nll_names[i]]['p_value'] < 0.05 else 'ns'\n",
    "    ax.text(i, d_val + 0.01 if d_val >= 0 else d_val - 0.02,\n",
    "            f\"{d_val:+.3f} {sig}\", ha='center',\n",
    "            va='bottom' if d_val >= 0 else 'top', fontsize=9)\n",
    "\n",
    "plt.suptitle('Exp 28: Contrastive Ranking Soft Prefix', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'ranking_plots.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"Saved to {RESULTS_DIR / 'ranking_plots.png'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cell-17",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-17T01:02:19.890786Z",
     "iopub.status.busy": "2026-02-17T01:02:19.890442Z",
     "iopub.status.idle": "2026-02-17T01:02:19.998306Z",
     "shell.execute_reply": "2026-02-17T01:02:19.997358Z"
    },
    "papermill": {
     "duration": 0.123932,
     "end_time": "2026-02-17T01:02:19.999984",
     "exception": false,
     "start_time": "2026-02-17T01:02:19.876052",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV saved: results/exp28/passage_scores.csv (1692 rows)\n",
      "Results saved: results/exp28/results.json (983.2 KB)\n",
      "\n",
      "======================================================================\n",
      "FINAL VERDICT — Exp 28: Contrastive Ranking Soft Prefix\n",
      "======================================================================\n",
      "\n",
      "Model: Gemma 3 4B | Cutoff: 16 | Margin: 0.1\n",
      "Training: 500 queries x 5 epochs, lr=0.05\n",
      "Eval: 200 queries, 1692 passages\n",
      "\n",
      "Ranking Results (Raw NLL):\n",
      "  bare AUC:         0.828 (ref: 0.828)\n",
      "  exp25_fact AUC:   0.839\n",
      "  contr_warm AUC:   0.819\n",
      "  contr_cold AUC:   0.735\n",
      "\n",
      "Ranking Results (PMI):\n",
      "  bare PMI AUC:     0.841 (ref: 0.841)\n",
      "  exp25 PMI AUC:    0.822\n",
      "  warm PMI AUC:     0.787\n",
      "  cold PMI AUC:     0.794\n",
      "\n",
      "NLL Improvement (relevant passages, d vs bare):\n",
      "  exp25_fact           d=+0.304, win=53% ***\n",
      "  contrastive_warm     d=-0.197, win=24% **\n",
      "  contrastive_cold     d=-0.990, win=4% ***\n",
      "\n",
      "VERDICT: Contrastive training FAILS to improve ranking.\n",
      "  Best raw AUC: 0.819 (target: >0.835, bare: 0.828)\n",
      "  Best PMI AUC: 0.794 (target: >0.845, bare: 0.841)\n",
      "  CONFIRMS: Document-independent prefix cannot create query-specific\n",
      "  relevance discrimination, even with ranking-aware training.\n",
      "  Contrastive prefix still helps average NLL (secondary success).\n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Cell 17: Save results.json + CSV + final verdict\n",
    "\n",
    "# --- CSV ---\n",
    "csv_rows = []\n",
    "for r in all_results:\n",
    "    for p in r['passage_data']:\n",
    "        csv_rows.append({\n",
    "            'query_idx': r['query_idx'],\n",
    "            'passage_idx': p['passage_idx'],\n",
    "            'is_relevant': int(p['is_relevant']),\n",
    "            'nll_bare': p['nll_bare'],\n",
    "            'nll_exp25_fact': p['nll_exp25_fact'],\n",
    "            'nll_contrastive_warm': p['nll_contrastive_warm'],\n",
    "            'nll_contrastive_cold': p['nll_contrastive_cold'],\n",
    "            'nll_baseline': p['nll_baseline'],\n",
    "            'pmi_bare': p['nll_bare'] - p['nll_baseline'],\n",
    "            'pmi_exp25': p['nll_exp25_fact'] - p['nll_baseline'],\n",
    "            'pmi_warm': p['nll_contrastive_warm'] - p['nll_baseline'],\n",
    "            'pmi_cold': p['nll_contrastive_cold'] - p['nll_baseline'],\n",
    "        })\n",
    "\n",
    "with open(CSV_EVAL_PATH, 'w', newline='') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=csv_rows[0].keys())\n",
    "    writer.writeheader()\n",
    "    writer.writerows(csv_rows)\n",
    "print(f\"CSV saved: {CSV_EVAL_PATH} ({len(csv_rows)} rows)\")\n",
    "\n",
    "# --- Results JSON ---\n",
    "final = {\n",
    "    'experiment': 'exp28_contrastive_ranking_soft_prefix',\n",
    "    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'config': {\n",
    "        'model_name': MODEL_NAME,\n",
    "        'model_type': 'gemma3',\n",
    "        'seed': SEED,\n",
    "        'cutoff': CUTOFF,\n",
    "        'prefix_len': PREFIX_LEN,\n",
    "        'hidden_size': HIDDEN_SIZE,\n",
    "        'trainable_params': PREFIX_LEN * HIDDEN_SIZE,\n",
    "        'training': {\n",
    "            'margin': MARGIN,\n",
    "            'lr': LR,\n",
    "            'n_epochs': N_EPOCHS,\n",
    "            'grad_accum': GRAD_ACCUM,\n",
    "            'warmup_steps': WARMUP_STEPS,\n",
    "            'n_train_queries': N_TRAIN,\n",
    "            'loss': 'hinge: max(0, margin + NLL_rel - NLL_irrel)',\n",
    "        },\n",
    "        'eval': {\n",
    "            'n_queries': N,\n",
    "            'total_passages': total_val_passages,\n",
    "            'n_relevant': total_val_rel,\n",
    "            'conditions': ['bare', 'exp25_fact', 'contrastive_warm', 'contrastive_cold', 'baseline'],\n",
    "        },\n",
    "    },\n",
    "    'training_history': {\n",
    "        'warm': history_warm if 'history_warm' in dir() else [],\n",
    "        'cold': history_cold if 'history_cold' in dir() else [],\n",
    "    },\n",
    "    'ranking_analysis': {\n",
    "        'auc': auc_results,\n",
    "        'mrr_at_10': mrr_results,\n",
    "        'differential_nll': diff_results,\n",
    "    },\n",
    "    'nll_improvement': nll_improvement,\n",
    "    'reference_exp22': EXP22_REF,\n",
    "    'per_query_results': all_results,\n",
    "}\n",
    "\n",
    "with open(FINAL_RESULTS_PATH, 'w') as f:\n",
    "    json.dump(final, f, indent=2)\n",
    "print(f\"Results saved: {FINAL_RESULTS_PATH} ({FINAL_RESULTS_PATH.stat().st_size / 1024:.1f} KB)\")\n",
    "\n",
    "# --- Final verdict ---\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"FINAL VERDICT — Exp 28: Contrastive Ranking Soft Prefix\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\nModel: Gemma 3 4B | Cutoff: {CUTOFF} | Margin: {MARGIN}\")\n",
    "print(f\"Training: {N_TRAIN} queries x {N_EPOCHS} epochs, lr={LR}\")\n",
    "print(f\"Eval: {N} queries, {total_val_passages} passages\")\n",
    "\n",
    "print(f\"\\nRanking Results (Raw NLL):\")\n",
    "print(f\"  bare AUC:         {auc_results['Raw bare']:.3f} (ref: {EXP22_REF['raw_bare_auc']:.3f})\")\n",
    "print(f\"  exp25_fact AUC:   {auc_results['Raw exp25_fact']:.3f}\")\n",
    "print(f\"  contr_warm AUC:   {auc_results['Raw contr_warm']:.3f}\")\n",
    "print(f\"  contr_cold AUC:   {auc_results['Raw contr_cold']:.3f}\")\n",
    "\n",
    "print(f\"\\nRanking Results (PMI):\")\n",
    "print(f\"  bare PMI AUC:     {auc_results['PMI bare']:.3f} (ref: {EXP22_REF['pmi_bare_auc']:.3f})\")\n",
    "print(f\"  exp25 PMI AUC:    {auc_results['PMI exp25_fact']:.3f}\")\n",
    "print(f\"  warm PMI AUC:     {auc_results['PMI contr_warm']:.3f}\")\n",
    "print(f\"  cold PMI AUC:     {auc_results['PMI contr_cold']:.3f}\")\n",
    "\n",
    "print(f\"\\nNLL Improvement (relevant passages, d vs bare):\")\n",
    "for name, data in nll_improvement.items():\n",
    "    sig = '***' if data['p_value'] < 0.001 else '**' if data['p_value'] < 0.01 else \\\n",
    "          '*' if data['p_value'] < 0.05 else 'ns'\n",
    "    print(f\"  {name:<20} d={data['cohens_d']:+.3f}, win={data['win_pct']:.0f}% {sig}\")\n",
    "\n",
    "# Determine verdict\n",
    "best_raw_auc = max(auc_results['Raw contr_warm'], auc_results['Raw contr_cold'])\n",
    "best_pmi_auc = max(auc_results['PMI contr_warm'], auc_results['PMI contr_cold'])\n",
    "\n",
    "ranking_improved = (best_raw_auc > 0.835) or (best_pmi_auc > 0.845)\n",
    "nll_still_helps = any(d['cohens_d'] > 0 for d in nll_improvement.values())\n",
    "\n",
    "if ranking_improved:\n",
    "    print(f\"\\nVERDICT: Contrastive training IMPROVES ranking!\")\n",
    "    print(f\"  Best raw AUC: {best_raw_auc:.3f} (target: >0.835)\")\n",
    "    print(f\"  Best PMI AUC: {best_pmi_auc:.3f} (target: >0.845)\")\n",
    "    print(f\"  Value contamination CAN create differential ranking signal.\")\n",
    "else:\n",
    "    print(f\"\\nVERDICT: Contrastive training FAILS to improve ranking.\")\n",
    "    print(f\"  Best raw AUC: {best_raw_auc:.3f} (target: >0.835, bare: {auc_results['Raw bare']:.3f})\")\n",
    "    print(f\"  Best PMI AUC: {best_pmi_auc:.3f} (target: >0.845, bare: {auc_results['PMI bare']:.3f})\")\n",
    "    print(f\"  CONFIRMS: Document-independent prefix cannot create query-specific\")\n",
    "    print(f\"  relevance discrimination, even with ranking-aware training.\")\n",
    "\n",
    "if nll_still_helps:\n",
    "    print(f\"  Contrastive prefix still helps average NLL (secondary success).\")\n",
    "else:\n",
    "    print(f\"  Contrastive training also HURTS average NLL (full failure).\")\n",
    "\n",
    "print(f\"\\nDone!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cell-18",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-17T01:02:20.026470Z",
     "iopub.status.busy": "2026-02-17T01:02:20.025615Z",
     "iopub.status.idle": "2026-02-17T01:02:20.628731Z",
     "shell.execute_reply": "2026-02-17T01:02:20.628066Z"
    },
    "papermill": {
     "duration": 0.617777,
     "end_time": "2026-02-17T01:02:20.630250",
     "exception": false,
     "start_time": "2026-02-17T01:02:20.012473",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning up GPU memory...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory: 4.60 GB -> 4.36 GB\n",
      "Cleanup complete.\n"
     ]
    }
   ],
   "source": [
    "# Cell 18: GPU cleanup\n",
    "\n",
    "print(\"Cleaning up GPU memory...\")\n",
    "mem_before = torch.cuda.memory_allocated() / 1e9\n",
    "\n",
    "del model\n",
    "del tokenizer\n",
    "for var_name in ['soft_contrastive_warm', 'soft_contrastive_cold', 'soft_exp25_fact']:\n",
    "    if var_name in dir():\n",
    "        exec(f'del {var_name}')\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "mem_after = torch.cuda.memory_allocated() / 1e9\n",
    "print(f\"GPU memory: {mem_before:.2f} GB -> {mem_after:.2f} GB\")\n",
    "print(\"Cleanup complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 11977.319194,
   "end_time": "2026-02-17T01:02:24.365601",
   "environment_variables": {},
   "exception": null,
   "input_path": "28_contrastive_ranking_soft_prefix.ipynb",
   "output_path": "28_contrastive_ranking_soft_prefix_executed.ipynb",
   "parameters": {},
   "start_time": "2026-02-16T21:42:47.046407",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "00f6da8fb30e41dd98e61c18b843a648": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_517e1bb54a6f41d5964a36b64f7f7c0a",
       "placeholder": "​",
       "style": "IPY_MODEL_c35c7d5199ce44eebddd3c1e5f280e80",
       "tabbable": null,
       "tooltip": null,
       "value": "Loading weights: 100%"
      }
     },
     "08a2e54bea3f47c299b6f7b256a6514a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_1f8c5dac1f3444debc02a282e374fdf8",
        "IPY_MODEL_ca4183d574eb43a5ab8fd196920d37ba",
        "IPY_MODEL_6a8bb2f3b696412286e8fe5951a697d7"
       ],
       "layout": "IPY_MODEL_6d30495cb6ce49ddac2dbc3b880fb0ec",
       "tabbable": null,
       "tooltip": null
      }
     },
     "0e8148caa50b4b3caeb063ed79dfaf25": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "1214fbea8d7d4292887ab777758dbd52": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "13bd594b72774e6baa9c8e0af7401b42": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "15b1a14e76264b7e9391471e054a5de8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_ca7a469928a74d82bfdd6851705bd1cb",
       "placeholder": "​",
       "style": "IPY_MODEL_6090188d65124f66b508b766ef776ed1",
       "tabbable": null,
       "tooltip": null,
       "value": "Filtering val:   6%"
      }
     },
     "1f8c5dac1f3444debc02a282e374fdf8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_891c4fd2892b4ec980b15fcd22b76b3c",
       "placeholder": "​",
       "style": "IPY_MODEL_678f9b11df574b69bf1c4b7f0744c525",
       "tabbable": null,
       "tooltip": null,
       "value": "Filtering train:   2%"
      }
     },
     "2396955c42ca4e79ae9c12e733e28a06": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_3a8fa6363d934f97bf75995a965d158d",
       "placeholder": "​",
       "style": "IPY_MODEL_38c10cb05bec41ab8c277882f3346cea",
       "tabbable": null,
       "tooltip": null,
       "value": "Ranking eval: 100%"
      }
     },
     "241b65a7ae1f4ac28cf0ebb9449cb2c3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2e7b4897e3ec4c22bc5417e45bd24752": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_2396955c42ca4e79ae9c12e733e28a06",
        "IPY_MODEL_fb0735d7c0ac4d6882ef02611bdaafb5",
        "IPY_MODEL_3ae30d41a2084a879db26ef7b8e039cb"
       ],
       "layout": "IPY_MODEL_9cec4b3d4554451baabd568778a334ec",
       "tabbable": null,
       "tooltip": null
      }
     },
     "31097cf414cd4933be127281d24f3349": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "344bb38552ff42a4affb32da41877b9c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_15b1a14e76264b7e9391471e054a5de8",
        "IPY_MODEL_99e4ef84991648758961a331949c72c1",
        "IPY_MODEL_3f10393c337649bfbaceeac4e852a0aa"
       ],
       "layout": "IPY_MODEL_9448a1bcf7da44c89042d6aa67f25a8c",
       "tabbable": null,
       "tooltip": null
      }
     },
     "360ad1fb98e747f0a274220cf8bcdd67": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "38c10cb05bec41ab8c277882f3346cea": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "3a8fa6363d934f97bf75995a965d158d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3ae30d41a2084a879db26ef7b8e039cb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_c7d9ab1225c24dd2b2f6d2497c18d941",
       "placeholder": "​",
       "style": "IPY_MODEL_f4873893597547b28f2898bc2d5d9a91",
       "tabbable": null,
       "tooltip": null,
       "value": " 200/200 [57:57&lt;00:00, 17.67s/it]"
      }
     },
     "3f10393c337649bfbaceeac4e852a0aa": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_cd9068cba0bf473981770a6ec2420676",
       "placeholder": "​",
       "style": "IPY_MODEL_51200af5dcfd4e17ac05918bbd13e9b3",
       "tabbable": null,
       "tooltip": null,
       "value": " 618/10047 [00:00&lt;00:01, 5715.14it/s]"
      }
     },
     "51200af5dcfd4e17ac05918bbd13e9b3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "517e1bb54a6f41d5964a36b64f7f7c0a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6090188d65124f66b508b766ef776ed1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "654adf0ae09a4355ad5821bdc80e5994": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "678f9b11df574b69bf1c4b7f0744c525": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "6a8bb2f3b696412286e8fe5951a697d7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_ad561beabc9d49d3b99d7ea760011b76",
       "placeholder": "​",
       "style": "IPY_MODEL_654adf0ae09a4355ad5821bdc80e5994",
       "tabbable": null,
       "tooltip": null,
       "value": " 1550/82326 [00:00&lt;00:13, 6032.65it/s]"
      }
     },
     "6d30495cb6ce49ddac2dbc3b880fb0ec": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "891c4fd2892b4ec980b15fcd22b76b3c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "9448a1bcf7da44c89042d6aa67f25a8c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "99e4ef84991648758961a331949c72c1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "danger",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_e6dc761ed62946dda90955bcd0dd79cd",
       "max": 10047.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_a277f59644594fb28aac6d0f46700504",
       "tabbable": null,
       "tooltip": null,
       "value": 618.0
      }
     },
     "9cec4b3d4554451baabd568778a334ec": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "9eeeeaa29694412eaa814b2c3a291de2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a277f59644594fb28aac6d0f46700504": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "ad561beabc9d49d3b99d7ea760011b76": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ba9bafea058643d7823d411d590f7147": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "c35c7d5199ce44eebddd3c1e5f280e80": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "c7d9ab1225c24dd2b2f6d2497c18d941": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ca4183d574eb43a5ab8fd196920d37ba": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "danger",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_1214fbea8d7d4292887ab777758dbd52",
       "max": 82326.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_ba9bafea058643d7823d411d590f7147",
       "tabbable": null,
       "tooltip": null,
       "value": 1550.0
      }
     },
     "ca7a469928a74d82bfdd6851705bd1cb": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "cd9068cba0bf473981770a6ec2420676": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d93bf36076584a31af8c8b0b1bb45014": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "dcabae8a97f44ce5bf80b20fc1297cab": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_d93bf36076584a31af8c8b0b1bb45014",
       "max": 883.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_31097cf414cd4933be127281d24f3349",
       "tabbable": null,
       "tooltip": null,
       "value": 883.0
      }
     },
     "e6dc761ed62946dda90955bcd0dd79cd": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "edc23810d41146de8953e27951b0cab3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_360ad1fb98e747f0a274220cf8bcdd67",
       "placeholder": "​",
       "style": "IPY_MODEL_13bd594b72774e6baa9c8e0af7401b42",
       "tabbable": null,
       "tooltip": null,
       "value": " 883/883 [00:03&lt;00:00, 818.59it/s, Materializing param=model.vision_tower.vision_model.post_layernorm.weight]"
      }
     },
     "eed2b4d8d8b54f31ae58a4571b337cd7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_00f6da8fb30e41dd98e61c18b843a648",
        "IPY_MODEL_dcabae8a97f44ce5bf80b20fc1297cab",
        "IPY_MODEL_edc23810d41146de8953e27951b0cab3"
       ],
       "layout": "IPY_MODEL_241b65a7ae1f4ac28cf0ebb9449cb2c3",
       "tabbable": null,
       "tooltip": null
      }
     },
     "f4873893597547b28f2898bc2d5d9a91": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "fb0735d7c0ac4d6882ef02611bdaafb5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_9eeeeaa29694412eaa814b2c3a291de2",
       "max": 200.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_0e8148caa50b4b3caeb063ed79dfaf25",
       "tabbable": null,
       "tooltip": null,
       "value": 200.0
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}