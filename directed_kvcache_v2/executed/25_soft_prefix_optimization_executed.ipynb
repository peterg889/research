{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {
    "papermill": {
     "duration": 0.004945,
     "end_time": "2026-02-16T14:02:46.947696",
     "exception": false,
     "start_time": "2026-02-16T14:02:46.942751",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Exp 25: Layer-Selective Soft Surrogates for Gemma 3\n",
    "\n",
    "## Motivation\n",
    "\n",
    "Experiments 16, 19, and 24 proved that **value contamination works on Gemma 3 4B**, but\n",
    "strictly requires stripping out the primed keys and restricting the primed values to early\n",
    "layers (0-15, yielding d=**+0.227** in Exp 21). Full-cache priming fails due to late-layer\n",
    "key interference.\n",
    "\n",
    "The discrete prefix `\"What are the key facts I need to know?\"` (static_fact) was chosen\n",
    "by hand. **Can we do better by learning an optimal continuous prefix?**\n",
    "\n",
    "## Core Question\n",
    "\n",
    "Can we learn a sequence of continuous embedding vectors (a \"Soft Prompt\") that maximizes\n",
    "document value contamination for factoid QA, beating the discrete static_fact prefix when\n",
    "applied via Gemma's required layer-selective hybrid cache?\n",
    "\n",
    "## Theoretical Mechanism\n",
    "\n",
    "We combine **Soft Prompt Tuning** with the **values_early_layers** mechanism. The model\n",
    "remains completely frozen; only the soft prefix embeddings are updated. The computational\n",
    "graph flows backward from the generated answer's loss, through the hybrid cache splice,\n",
    "and into the soft prefix embeddings.\n",
    "\n",
    "## Design\n",
    "\n",
    "| Part | Phase | Data | N | Description |\n",
    "|------|-------|------|---|-------------|\n",
    "| 1 | Train | MS MARCO train | 2000 | Learn soft_prefix_embeddings via gradient descent |\n",
    "| 2 | Eval | MS MARCO val | 300 | Compare 4 conditions against Exp 21 baselines |\n",
    "\n",
    "### Training (Part 1)\n",
    "- **Trainable params**: `soft_prefix_embeddings` of shape `(prefix_len, hidden_size)`\n",
    "  where `prefix_len = 7` (matching static_fact token count)\n",
    "- **Two init conditions**: random (N(0, 0.02)) and static_fact-initialized\n",
    "- **Loss**: Mean NLL of answer tokens scored through the hybrid cache\n",
    "- **Optimizer**: AdamW, lr=0.1 (standard for soft prompt tuning)\n",
    "- **Epochs**: 3 passes over 2000 training samples\n",
    "\n",
    "### Evaluation Conditions (Part 2)\n",
    "\n",
    "| Condition | Keys | Values (L0-15) | Values (L16-33) |\n",
    "|-----------|------|----------------|------------------|\n",
    "| bare | bare | bare | bare |\n",
    "| vel_static | bare | static_fact primed | bare |\n",
    "| vel_soft_random | bare | soft (random init) primed | bare |\n",
    "| vel_soft_fact | bare | soft (fact init) primed | bare |\n",
    "\n",
    "## Reference Values\n",
    "\n",
    "| Source | Condition | d |\n",
    "|--------|-----------|---|\n",
    "| Exp 19 | values_only (all 34 layers) | +0.056 |\n",
    "| Exp 19 | values_early_layers (0-16) | +0.211 |\n",
    "| Exp 21 | values_early_layers (0-15) | +0.227 |\n",
    "| Exp 24 | static_fact @ cutoff=16 | ~+0.21 |\n",
    "\n",
    "## Success Criteria\n",
    "\n",
    "- **Primary**: Does `vel_soft_fact` achieve Cohen's d > +0.25 (beating discrete +0.227)?\n",
    "- **Secondary**: Does `vel_soft_random` learn anything useful from scratch (d > +0.10)?\n",
    "- **Diagnostic**: Track training loss curves for convergence validation\n",
    "\n",
    "## Technical Watch-Outs\n",
    "\n",
    "1. **Memory**: Backprop through full LLM attention requires large activation maps.\n",
    "   Batch size 1 + gradient accumulation mandatory.\n",
    "2. **RoPE differentiability**: `correct_rope_positions_with_bos` uses in-place ops.\n",
    "   We must reimplement the hybrid splice with pure functional ops for the training loop.\n",
    "3. **4-bit model**: BitsAndBytes 4-bit models may not support gradient computation through\n",
    "   the full forward pass. We use `inputs_embeds` to bypass the embedding lookup and let\n",
    "   gradients flow through the soft prefix only.\n",
    "4. **Learning rate**: Soft prompts typically need lr=0.1-0.3 (much higher than fine-tuning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T14:02:46.958138Z",
     "iopub.status.busy": "2026-02-16T14:02:46.957268Z",
     "iopub.status.idle": "2026-02-16T14:02:51.066482Z",
     "shell.execute_reply": "2026-02-16T14:02:51.065541Z"
    },
    "papermill": {
     "duration": 4.116041,
     "end_time": "2026-02-16T14:02:51.068206",
     "exception": false,
     "start_time": "2026-02-16T14:02:46.952165",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEED: 42\n",
      "Results directory: results/exp25\n",
      "CUDA available: True\n",
      "GPU: NVIDIA A100-SXM4-40GB\n",
      "GPU memory: 42.3 GB\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Setup\n",
    "import os\n",
    "os.umask(0o000)\n",
    "\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import gc\n",
    "from pathlib import Path\n",
    "\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "RESULTS_DIR = Path(\"results/exp25\")\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CHECKPOINT_TRAIN_RAND_PATH = RESULTS_DIR / \"checkpoint_train_random.json\"\n",
    "CHECKPOINT_TRAIN_FACT_PATH = RESULTS_DIR / \"checkpoint_train_fact.json\"\n",
    "CHECKPOINT_EVAL_PATH = RESULTS_DIR / \"checkpoint_eval.json\"\n",
    "FINAL_RESULTS_PATH = RESULTS_DIR / \"results.json\"\n",
    "CSV_EVAL_PATH = RESULTS_DIR / \"eval_results.csv\"\n",
    "SOFT_RANDOM_PATH = RESULTS_DIR / \"soft_prefix_random.pt\"\n",
    "SOFT_FACT_PATH = RESULTS_DIR / \"soft_prefix_fact.pt\"\n",
    "\n",
    "print(f\"SEED: {SEED}\")\n",
    "print(f\"Results directory: {RESULTS_DIR}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T14:02:51.077865Z",
     "iopub.status.busy": "2026-02-16T14:02:51.077364Z",
     "iopub.status.idle": "2026-02-16T14:03:05.613251Z",
     "shell.execute_reply": "2026-02-16T14:03:05.612508Z"
    },
    "papermill": {
     "duration": 14.542657,
     "end_time": "2026-02-16T14:03:05.614934",
     "exception": false,
     "start_time": "2026-02-16T14:02:51.072277",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading google/gemma-3-4b-it (4-bit, bfloat16)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1e3d473abf042d0baddcde1dee16c95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/883 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model loaded successfully.\n",
      "  Num layers: 34\n",
      "  Hidden size: 2560\n",
      "  Head dim: 256\n",
      "  Num KV heads: 4\n",
      "  BOS token ID: 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Cache key dtype: torch.bfloat16\n",
      "  Cache key shape: torch.Size([1, 4, 2, 256])\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Load Gemma 3 4B via load_model()\n",
    "sys.path.insert(0, \".\")\n",
    "\n",
    "from lib.config import ExperimentConfig\n",
    "from lib.model_utils import load_model\n",
    "\n",
    "MODEL_NAME = \"google/gemma-3-4b-it\"\n",
    "\n",
    "exp_config = ExperimentConfig(\n",
    "    model_name=MODEL_NAME,\n",
    "    model_type=\"gemma3\",\n",
    "    compute_dtype=\"auto\",  # resolves to bfloat16 for gemma3\n",
    "    use_4bit=True,\n",
    "    num_samples=2000,\n",
    "    seed=SEED,\n",
    ")\n",
    "\n",
    "print(f\"Loading {MODEL_NAME} (4-bit, bfloat16)...\")\n",
    "model, tokenizer = load_model(exp_config)\n",
    "\n",
    "# Architecture diagnostics\n",
    "from lib.kv_cache import (\n",
    "    _get_text_config, _get_head_dim, _get_rope_theta_for_layer,\n",
    "    _get_cache_keys, _get_cache_values, _set_cache_keys, _set_cache_values,\n",
    "    _ensure_dynamic_cache, extract_and_truncate_cache_with_bos,\n",
    "    correct_rope_positions_with_bos, score_answer_with_cache,\n",
    "    deepcopy_cache, replace_values_at_layers,\n",
    ")\n",
    "\n",
    "text_config = _get_text_config(model.config)\n",
    "NUM_LAYERS = text_config.num_hidden_layers\n",
    "HIDDEN_SIZE = text_config.hidden_size\n",
    "HEAD_DIM = _get_head_dim(model.config)\n",
    "\n",
    "print(f\"\\nModel loaded successfully.\")\n",
    "print(f\"  Num layers: {NUM_LAYERS}\")\n",
    "print(f\"  Hidden size: {HIDDEN_SIZE}\")\n",
    "print(f\"  Head dim: {HEAD_DIM}\")\n",
    "print(f\"  Num KV heads: {text_config.num_key_value_heads}\")\n",
    "print(f\"  BOS token ID: {tokenizer.bos_token_id}\")\n",
    "\n",
    "# Verify dtype\n",
    "sample_ids = tokenizer(\"test\", return_tensors=\"pt\")['input_ids'].to(exp_config.device)\n",
    "with torch.no_grad():\n",
    "    out = model(sample_ids, use_cache=True)\n",
    "    cache_check = _ensure_dynamic_cache(out.past_key_values)\n",
    "    k0 = _get_cache_keys(cache_check, 0)\n",
    "    print(f\"  Cache key dtype: {k0.dtype}\")\n",
    "    print(f\"  Cache key shape: {k0.shape}\")\n",
    "del out, sample_ids, cache_check\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T14:03:05.625292Z",
     "iopub.status.busy": "2026-02-16T14:03:05.624741Z",
     "iopub.status.idle": "2026-02-16T14:03:05.632609Z",
     "shell.execute_reply": "2026-02-16T14:03:05.631987Z"
    },
    "papermill": {
     "duration": 0.014547,
     "end_time": "2026-02-16T14:03:05.634121",
     "exception": false,
     "start_time": "2026-02-16T14:03:05.619574",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config ready\n",
      "  Model: google/gemma-3-4b-it\n",
      "  Num layers: 34, hidden_size: 2560\n",
      "  Cutoff: 16 (layers 0-15)\n",
      "  Soft prefix length: 7 tokens\n",
      "  Trainable params: 17,920 (7 x 2560)\n",
      "  Training: 2000 samples x 3 epochs, lr=0.1, grad_accum=4\n",
      "  Eval: 300 samples, 4 conditions\n",
      "  Static fact prefix: 'What are the key facts I need to know?'\n",
      "\n",
      "Reference values:\n",
      "  Exp 19 values_early_layers: d=+0.211\n",
      "  Exp 21 values_early_layers: d=+0.227\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Constants, templates, imports\n",
    "from lib.analysis import cohens_d\n",
    "from lib.data import count_words\n",
    "from lib.surrogate import STATIC_SURROGATE_QUERIES\n",
    "from scipy import stats\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Templates\n",
    "SURROGATE_PREFIX_TEMPLATE = \"{surrogate}\\n\"\n",
    "DOCUMENT_TEMPLATE = \"{document}\"\n",
    "QUERY_TEMPLATE = \"\\nQuery: {query}\\nAnswer:\"\n",
    "ANSWER_TEMPLATE = \" {answer}\"\n",
    "\n",
    "STATIC_FACT = STATIC_SURROGATE_QUERIES['static_factual']['query']\n",
    "\n",
    "# Experiment parameters\n",
    "N_TRAIN = 2000\n",
    "N_EVAL = 300\n",
    "MAX_PASSAGE_WORDS = 300\n",
    "CUTOFF = 16          # layers 0-15\n",
    "PREFIX_LEN = 7       # match static_fact token count\n",
    "CHECKPOINT_EVERY = 50\n",
    "\n",
    "# Training hyperparameters\n",
    "LR = 0.1\n",
    "N_EPOCHS = 3\n",
    "GRAD_ACCUM_STEPS = 4  # effective batch size = 4\n",
    "WARMUP_STEPS = 50\n",
    "\n",
    "# Reference values\n",
    "EXP19_REF = {'values_only_d': 0.056, 'values_early_layers_d': 0.211}\n",
    "EXP21_REF = {'values_early_layers_d': 0.227}\n",
    "\n",
    "print(\"Config ready\")\n",
    "print(f\"  Model: {MODEL_NAME}\")\n",
    "print(f\"  Num layers: {NUM_LAYERS}, hidden_size: {HIDDEN_SIZE}\")\n",
    "print(f\"  Cutoff: {CUTOFF} (layers 0-{CUTOFF-1})\")\n",
    "print(f\"  Soft prefix length: {PREFIX_LEN} tokens\")\n",
    "print(f\"  Trainable params: {PREFIX_LEN * HIDDEN_SIZE:,} ({PREFIX_LEN} x {HIDDEN_SIZE})\")\n",
    "print(f\"  Training: {N_TRAIN} samples x {N_EPOCHS} epochs, lr={LR}, grad_accum={GRAD_ACCUM_STEPS}\")\n",
    "print(f\"  Eval: {N_EVAL} samples, 4 conditions\")\n",
    "print(f\"  Static fact prefix: '{STATIC_FACT}'\")\n",
    "print(f\"\\nReference values:\")\n",
    "print(f\"  Exp 19 values_early_layers: d={EXP19_REF['values_early_layers_d']:+.3f}\")\n",
    "print(f\"  Exp 21 values_early_layers: d={EXP21_REF['values_early_layers_d']:+.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T14:03:05.644131Z",
     "iopub.status.busy": "2026-02-16T14:03:05.643521Z",
     "iopub.status.idle": "2026-02-16T14:03:07.856478Z",
     "shell.execute_reply": "2026-02-16T14:03:07.855799Z"
    },
    "papermill": {
     "duration": 2.219906,
     "end_time": "2026-02-16T14:03:07.858266",
     "exception": false,
     "start_time": "2026-02-16T14:03:05.638360",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`trust_remote_code` is not supported anymore.\n",
      "Please check that the Hugging Face dataset 'microsoft/ms_marco' isn't based on a loading script and remove `trust_remote_code`.\n",
      "If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "LOADING MS MARCO v1.1 — TRAINING SPLIT\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total items in train: 82326\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ea6fc2237874be0b40e620b94ed6484",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filtering train:   0%|          | 0/82326 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`trust_remote_code` is not supported anymore.\n",
      "Please check that the Hugging Face dataset 'microsoft/ms_marco' isn't based on a loading script and remove `trust_remote_code`.\n",
      "If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 2000 training samples\n",
      "Word counts: mean=72, min=9, max=185\n",
      "\n",
      "======================================================================\n",
      "LOADING MS MARCO v1.1 — VALIDATION SPLIT\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total items in validation: 10047\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6d1cd7809ce420a8012eb423d7f8fee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filtering validation:   0%|          | 0/10047 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 300 eval samples\n",
      "Word counts: mean=72, min=21, max=146\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Load MS MARCO training + validation splits\n",
    "from datasets import load_dataset\n",
    "\n",
    "def load_marco_split(split_name, n_samples, seed):\n",
    "    \"\"\"Load MS MARCO samples with positive passages.\"\"\"\n",
    "    dataset = load_dataset(\"microsoft/ms_marco\", \"v1.1\", split=split_name,\n",
    "                           trust_remote_code=True)\n",
    "    print(f\"Total items in {split_name}: {len(dataset)}\")\n",
    "\n",
    "    samples = []\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    for item in tqdm(dataset, desc=f\"Filtering {split_name}\"):\n",
    "        passages_info = item.get('passages', {})\n",
    "        passage_texts = passages_info.get('passage_text', [])\n",
    "        is_selected = passages_info.get('is_selected', [])\n",
    "        query = item.get('query', '')\n",
    "        answers = item.get('answers', [])\n",
    "        well_formed = item.get('wellFormedAnswers', [])\n",
    "\n",
    "        if not passage_texts or not query:\n",
    "            continue\n",
    "\n",
    "        answer = None\n",
    "        if well_formed and len(well_formed) > 0 and well_formed[0] != '[]':\n",
    "            answer = well_formed[0]\n",
    "        elif answers and len(answers) > 0 and answers[0] != 'No Answer Present.':\n",
    "            answer = answers[0]\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        for ptext, sel in zip(passage_texts, is_selected):\n",
    "            if sel == 1 and count_words(ptext) <= MAX_PASSAGE_WORDS:\n",
    "                samples.append({\n",
    "                    'query': query,\n",
    "                    'answer': answer,\n",
    "                    'passage': ptext,\n",
    "                    'word_count': count_words(ptext),\n",
    "                })\n",
    "                break\n",
    "\n",
    "        if len(samples) >= n_samples * 3:\n",
    "            break\n",
    "\n",
    "    np.random.shuffle(samples)\n",
    "    samples = samples[:n_samples]\n",
    "    del dataset\n",
    "    gc.collect()\n",
    "    return samples\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"LOADING MS MARCO v1.1 — TRAINING SPLIT\")\n",
    "print(\"=\" * 70)\n",
    "train_samples = load_marco_split(\"train\", N_TRAIN, SEED)\n",
    "print(f\"Selected {len(train_samples)} training samples\")\n",
    "print(f\"Word counts: mean={np.mean([q['word_count'] for q in train_samples]):.0f}, \"\n",
    "      f\"min={min(q['word_count'] for q in train_samples)}, \"\n",
    "      f\"max={max(q['word_count'] for q in train_samples)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"LOADING MS MARCO v1.1 — VALIDATION SPLIT\")\n",
    "print(\"=\" * 70)\n",
    "eval_samples = load_marco_split(\"validation\", N_EVAL, SEED + 1)\n",
    "print(f\"Selected {len(eval_samples)} eval samples\")\n",
    "print(f\"Word counts: mean={np.mean([q['word_count'] for q in eval_samples]):.0f}, \"\n",
    "      f\"min={min(q['word_count'] for q in eval_samples)}, \"\n",
    "      f\"max={max(q['word_count'] for q in eval_samples)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T14:03:07.870927Z",
     "iopub.status.busy": "2026-02-16T14:03:07.870227Z",
     "iopub.status.idle": "2026-02-16T14:03:07.880054Z",
     "shell.execute_reply": "2026-02-16T14:03:07.879367Z"
    },
    "papermill": {
     "duration": 0.017254,
     "end_time": "2026-02-16T14:03:07.881521",
     "exception": false,
     "start_time": "2026-02-16T14:03:07.864267",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PREFIX TOKENIZATION\n",
      "======================================================================\n",
      "Static fact prefix: 'What are the key facts I need to know?'\n",
      "  Formatted: 'What are the key facts I need to know?'\n",
      "  Token length: 11\n",
      "  Soft prefix length: 7 (matches: False)\n",
      "  WARNING: Updating PREFIX_LEN from 7 to 11\n",
      "\n",
      "BPE boundary check: 105/105 tokens match (100.0%)\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Tokenize static_fact prefix + BPE boundary check\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PREFIX TOKENIZATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "sf_str = SURROGATE_PREFIX_TEMPLATE.format(surrogate=STATIC_FACT)\n",
    "sf_ids = tokenizer(sf_str, return_tensors=\"pt\",\n",
    "                    add_special_tokens=False)['input_ids'].to(exp_config.device)\n",
    "SF_TOKEN_LEN = sf_ids.shape[1]\n",
    "\n",
    "print(f\"Static fact prefix: '{STATIC_FACT}'\")\n",
    "print(f\"  Formatted: '{sf_str.strip()}'\")\n",
    "print(f\"  Token length: {SF_TOKEN_LEN}\")\n",
    "print(f\"  Soft prefix length: {PREFIX_LEN} (matches: {PREFIX_LEN == SF_TOKEN_LEN})\")\n",
    "\n",
    "# If mismatch, update PREFIX_LEN to match\n",
    "if PREFIX_LEN != SF_TOKEN_LEN:\n",
    "    print(f\"  WARNING: Updating PREFIX_LEN from {PREFIX_LEN} to {SF_TOKEN_LEN}\")\n",
    "    PREFIX_LEN = SF_TOKEN_LEN\n",
    "\n",
    "# BPE boundary check\n",
    "example_doc = train_samples[0]['passage']\n",
    "concat = sf_str + DOCUMENT_TEMPLATE.format(document=example_doc)\n",
    "concat_enc = tokenizer(concat, add_special_tokens=True)['input_ids']\n",
    "prefix_enc = tokenizer(sf_str, add_special_tokens=True)['input_ids']\n",
    "doc_ids_from_concat = concat_enc[len(prefix_enc):]\n",
    "bare_doc_enc = tokenizer(DOCUMENT_TEMPLATE.format(document=example_doc),\n",
    "                          add_special_tokens=False)['input_ids']\n",
    "match = sum(1 for a, b in zip(doc_ids_from_concat, bare_doc_enc) if a == b)\n",
    "total = max(len(bare_doc_enc), 1)\n",
    "print(f\"\\nBPE boundary check: {match}/{total} tokens match ({100*match/total:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cell-6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T14:03:07.893237Z",
     "iopub.status.busy": "2026-02-16T14:03:07.892605Z",
     "iopub.status.idle": "2026-02-16T14:03:07.900099Z",
     "shell.execute_reply": "2026-02-16T14:03:07.899454Z"
    },
    "papermill": {
     "duration": 0.014773,
     "end_time": "2026-02-16T14:03:07.901526",
     "exception": false,
     "start_time": "2026-02-16T14:03:07.886753",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "EXPERIMENTAL CONDITIONS\n",
      "======================================================================\n",
      "\n",
      "### Part 1: Training the Soft Prefix ###\n",
      "  Data: 2000 MS MARCO training queries\n",
      "  Trainable: soft_prefix_embeddings (11 x 2560 = 28,160 params)\n",
      "  Two runs: random init (N(0, 0.02)) and static_fact init\n",
      "  Epochs: 3, lr=0.1, grad_accum=4\n",
      "\n",
      "  Per training step:\n",
      "    1. Build inputs_embeds = [BOS_emb] + [soft_prefix (grad)] + [doc_embs (detached)]\n",
      "    2. Forward pass -> get primed cache (gradients flow through soft prefix)\n",
      "    3. Extract primed values at layers 0-15 (functional ops, no in-place mutation)\n",
      "    4. Build bare cache (no grad)\n",
      "    5. Splice: bare keys + primed values (L0-15) + bare values (L16-33)\n",
      "    6. Score answer NLL through hybrid cache\n",
      "    7. loss.backward() -> updates only soft_prefix_embeddings\n",
      "\n",
      "### Part 2: Evaluation (4 conditions) ###\n",
      "  Data: 300 MS MARCO validation queries\n",
      "\n",
      "  bare:\n",
      "    Cache: [BOS][doc] -> score as-is\n",
      "    Baseline, no modifications.\n",
      "\n",
      "  vel_static:\n",
      "    Cache: [BOS][static_fact][doc] -> truncate -> RoPE correct\n",
      "    Replace values at layers 0-15 into bare cache\n",
      "    This is the Exp 21 condition (d=+0.227). Ceiling to beat.\n",
      "\n",
      "  vel_soft_random:\n",
      "    Cache: [BOS][soft_random_embs][doc_embs] -> forward -> extract values L0-15\n",
      "    Splice into bare cache. Tests: can random-init soft prefix learn useful values?\n",
      "\n",
      "  vel_soft_fact:\n",
      "    Cache: [BOS][soft_fact_embs][doc_embs] -> forward -> extract values L0-15\n",
      "    Splice into bare cache. Tests: can we refine static_fact in continuous space?\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Explain experimental conditions\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"EXPERIMENTAL CONDITIONS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\n### Part 1: Training the Soft Prefix ###\")\n",
    "print(f\"  Data: {N_TRAIN} MS MARCO training queries\")\n",
    "print(f\"  Trainable: soft_prefix_embeddings ({PREFIX_LEN} x {HIDDEN_SIZE} = {PREFIX_LEN * HIDDEN_SIZE:,} params)\")\n",
    "print(f\"  Two runs: random init (N(0, 0.02)) and static_fact init\")\n",
    "print(f\"  Epochs: {N_EPOCHS}, lr={LR}, grad_accum={GRAD_ACCUM_STEPS}\")\n",
    "print()\n",
    "print(\"  Per training step:\")\n",
    "print(\"    1. Build inputs_embeds = [BOS_emb] + [soft_prefix (grad)] + [doc_embs (detached)]\")\n",
    "print(\"    2. Forward pass -> get primed cache (gradients flow through soft prefix)\")\n",
    "print(\"    3. Extract primed values at layers 0-15 (functional ops, no in-place mutation)\")\n",
    "print(\"    4. Build bare cache (no grad)\")\n",
    "print(\"    5. Splice: bare keys + primed values (L0-15) + bare values (L16-33)\")\n",
    "print(\"    6. Score answer NLL through hybrid cache\")\n",
    "print(\"    7. loss.backward() -> updates only soft_prefix_embeddings\")\n",
    "\n",
    "print(\"\\n### Part 2: Evaluation (4 conditions) ###\")\n",
    "print(f\"  Data: {N_EVAL} MS MARCO validation queries\")\n",
    "print()\n",
    "print(\"  bare:\")\n",
    "print(\"    Cache: [BOS][doc] -> score as-is\")\n",
    "print(\"    Baseline, no modifications.\")\n",
    "print()\n",
    "print(\"  vel_static:\")\n",
    "print(\"    Cache: [BOS][static_fact][doc] -> truncate -> RoPE correct\")\n",
    "print(f\"    Replace values at layers 0-{CUTOFF-1} into bare cache\")\n",
    "print(f\"    This is the Exp 21 condition (d=+0.227). Ceiling to beat.\")\n",
    "print()\n",
    "print(\"  vel_soft_random:\")\n",
    "print(\"    Cache: [BOS][soft_random_embs][doc_embs] -> forward -> extract values L0-15\")\n",
    "print(f\"    Splice into bare cache. Tests: can random-init soft prefix learn useful values?\")\n",
    "print()\n",
    "print(\"  vel_soft_fact:\")\n",
    "print(\"    Cache: [BOS][soft_fact_embs][doc_embs] -> forward -> extract values L0-15\")\n",
    "print(f\"    Splice into bare cache. Tests: can we refine static_fact in continuous space?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cell-7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T14:03:07.914326Z",
     "iopub.status.busy": "2026-02-16T14:03:07.914056Z",
     "iopub.status.idle": "2026-02-16T14:03:07.929793Z",
     "shell.execute_reply": "2026-02-16T14:03:07.929118Z"
    },
    "papermill": {
     "duration": 0.024469,
     "end_time": "2026-02-16T14:03:07.931236",
     "exception": false,
     "start_time": "2026-02-16T14:03:07.906767",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding layer found: Gemma3TextScaledWordEmbedding\n",
      "Embedding dim: torch.Size([262208, 2560])\n",
      "differentiable_hybrid_score() defined\n",
      "  Input: soft_prefix (requires_grad), doc_ids, query, answer\n",
      "  Output: scalar NLL loss with gradients to soft_prefix\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Differentiable hybrid cache scoring function\n",
    "#\n",
    "# The existing lib functions use in-place ops and torch.no_grad().\n",
    "# For training we need a fully differentiable path from soft embeddings to loss.\n",
    "\n",
    "from lib.kv_cache import _get_rope_theta_for_layer, _build_rope_correction, _rotate_half\n",
    "\n",
    "# Get the embedding layer using standard HuggingFace API\n",
    "embed_fn = model.get_input_embeddings()\n",
    "print(f\"Embedding layer found: {type(embed_fn).__name__}\")\n",
    "print(f\"Embedding dim: {embed_fn.weight.shape}\")\n",
    "\n",
    "\n",
    "def differentiable_hybrid_score(\n",
    "    soft_prefix: torch.Tensor,       # (1, prefix_len, hidden_size), requires_grad\n",
    "    doc_ids: torch.Tensor,            # (1, doc_len)\n",
    "    bos_id: torch.Tensor,             # (1, 1)\n",
    "    query_prompt: str,\n",
    "    answer_text: str,\n",
    "    model,\n",
    "    tokenizer,\n",
    "    config,\n",
    "    cutoff: int,\n",
    "):\n",
    "    \"\"\"\n",
    "    Compute answer NLL through a hybrid cache built from soft prefix embeddings.\n",
    "\n",
    "    Returns a scalar loss tensor with gradients flowing back to soft_prefix.\n",
    "    \"\"\"\n",
    "    device = config.device\n",
    "    doc_len = doc_ids.shape[1]\n",
    "    prefix_len = soft_prefix.shape[1]\n",
    "    context_len = 1 + doc_len  # BOS + doc\n",
    "\n",
    "    # --- Step 1: Bare cache (no gradients needed) ---\n",
    "    bare_input = torch.cat([bos_id, doc_ids], dim=1)\n",
    "    with torch.no_grad():\n",
    "        bare_out = model(input_ids=bare_input,\n",
    "                         attention_mask=torch.ones_like(bare_input),\n",
    "                         use_cache=True, return_dict=True)\n",
    "    bare_cache = bare_out.past_key_values\n",
    "    del bare_out\n",
    "\n",
    "    # --- Step 2: Primed cache via inputs_embeds (gradients enabled) ---\n",
    "    # Get embeddings for BOS and doc tokens (detached from graph)\n",
    "    with torch.no_grad():\n",
    "        bos_emb = embed_fn(bos_id)          # (1, 1, hidden)\n",
    "        doc_emb = embed_fn(doc_ids)          # (1, doc_len, hidden)\n",
    "\n",
    "    # Cast soft prefix to model dtype for forward pass\n",
    "    soft_cast = soft_prefix.to(dtype=bos_emb.dtype)\n",
    "\n",
    "    # Concatenate: [BOS_emb, soft_prefix, doc_emb]\n",
    "    inputs_embeds = torch.cat([bos_emb.detach(), soft_cast, doc_emb.detach()], dim=1)\n",
    "    total_len = inputs_embeds.shape[1]  # 1 + prefix_len + doc_len\n",
    "    attn_mask = torch.ones((1, total_len), device=device, dtype=torch.long)\n",
    "\n",
    "    # Forward pass with gradients flowing through soft_prefix\n",
    "    primed_out = model(inputs_embeds=inputs_embeds,\n",
    "                       attention_mask=attn_mask,\n",
    "                       use_cache=True, return_dict=True)\n",
    "    primed_cache = primed_out.past_key_values\n",
    "    del primed_out\n",
    "\n",
    "    # --- Step 3+4: Build hybrid cache ---\n",
    "    # Keys: from bare cache (already at correct positions)\n",
    "    # Values L0-{cutoff-1}: from primed cache (BOS + last doc_len positions)\n",
    "    # Values L{cutoff}-{N-1}: from bare cache\n",
    "    # No RoPE correction needed — we use bare keys, and values have no RoPE.\n",
    "\n",
    "    primed_cache_dc = _ensure_dynamic_cache(primed_cache)\n",
    "    bare_cache_dc = _ensure_dynamic_cache(bare_cache)\n",
    "\n",
    "    from transformers import DynamicCache\n",
    "    from transformers.cache_utils import DynamicSlidingWindowLayer, DynamicLayer\n",
    "\n",
    "    hybrid_cache = DynamicCache()\n",
    "    for layer_idx in range(NUM_LAYERS):\n",
    "        k = _get_cache_keys(bare_cache_dc, layer_idx)\n",
    "\n",
    "        if layer_idx < cutoff:\n",
    "            primed_v = _get_cache_values(primed_cache_dc, layer_idx)\n",
    "            bos_v = primed_v[:, :, :1, :]\n",
    "            doc_v = primed_v[:, :, -doc_len:, :]\n",
    "            v = torch.cat([bos_v, doc_v], dim=2)\n",
    "        else:\n",
    "            v = _get_cache_values(bare_cache_dc, layer_idx)\n",
    "\n",
    "        src_layer = bare_cache_dc.layers[layer_idx]\n",
    "        if isinstance(src_layer, DynamicSlidingWindowLayer):\n",
    "            new_layer = DynamicSlidingWindowLayer(sliding_window=src_layer.sliding_window)\n",
    "            new_layer.dtype = k.dtype\n",
    "            new_layer.device = k.device\n",
    "            new_layer.keys = k\n",
    "            new_layer.values = v\n",
    "            new_layer.is_initialized = True\n",
    "            new_layer.cumulative_length = src_layer.cumulative_length\n",
    "            new_layer._sliding_window_tensor = new_layer._sliding_window_tensor.to(k.device)\n",
    "        else:\n",
    "            new_layer = DynamicLayer()\n",
    "            new_layer.dtype = k.dtype\n",
    "            new_layer.device = k.device\n",
    "            new_layer.keys = k\n",
    "            new_layer.values = v\n",
    "            new_layer.is_initialized = True\n",
    "        hybrid_cache.layers.append(new_layer)\n",
    "\n",
    "    # --- Step 5: Score answer through hybrid cache ---\n",
    "    query_ids = tokenizer(query_prompt, return_tensors=\"pt\",\n",
    "                          add_special_tokens=False)['input_ids'].to(device)\n",
    "    answer_ids = tokenizer(answer_text, return_tensors=\"pt\",\n",
    "                           add_special_tokens=False)['input_ids'].to(device)\n",
    "    query_len = query_ids.shape[1]\n",
    "    answer_len = answer_ids.shape[1]\n",
    "\n",
    "    # Single forward pass: query + answer together\n",
    "    qa_ids = torch.cat([query_ids, answer_ids], dim=1)\n",
    "    qa_len = qa_ids.shape[1]\n",
    "    qa_attn_full = torch.ones((1, context_len + qa_len), device=device)\n",
    "\n",
    "    qa_out = model(input_ids=qa_ids,\n",
    "                   attention_mask=qa_attn_full,\n",
    "                   past_key_values=hybrid_cache,\n",
    "                   use_cache=False, return_dict=True)\n",
    "\n",
    "    logits = qa_out.logits\n",
    "    # logit at position [query_len-1] predicts answer_ids[0]\n",
    "    answer_logits = logits[:, query_len - 1 : query_len + answer_len - 1, :]\n",
    "\n",
    "    loss = torch.nn.functional.cross_entropy(\n",
    "        answer_logits.reshape(-1, answer_logits.shape[-1]),\n",
    "        answer_ids.reshape(-1),\n",
    "        reduction='mean'\n",
    "    )\n",
    "\n",
    "    del qa_out, logits, bare_cache, bare_cache_dc, primed_cache, primed_cache_dc\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "print(\"differentiable_hybrid_score() defined\")\n",
    "print(\"  Input: soft_prefix (requires_grad), doc_ids, query, answer\")\n",
    "print(\"  Output: scalar NLL loss with gradients to soft_prefix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cell-8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T14:03:07.943760Z",
     "iopub.status.busy": "2026-02-16T14:03:07.943239Z",
     "iopub.status.idle": "2026-02-16T14:03:09.098870Z",
     "shell.execute_reply": "2026-02-16T14:03:09.098097Z"
    },
    "papermill": {
     "duration": 1.163664,
     "end_time": "2026-02-16T14:03:09.100682",
     "exception": false,
     "start_time": "2026-02-16T14:03:07.937018",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "GRADIENT FLOW SANITY CHECK\n",
      "======================================================================\n",
      "Test sample: doc_len=105, query='where is balamory filmed...'\n",
      "Test prefix shape: torch.Size([1, 11, 2560])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Forward pass: loss = 2.1875\n",
      "Loss requires_grad: True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backward pass: SUCCESS\n",
      "Gradient shape: torch.Size([1, 11, 2560])\n",
      "Gradient norm: 0.522729\n",
      "Gradient mean: -0.000005\n",
      "Gradient max: 0.078125\n",
      "\n",
      ">>> GRADIENT FLOW CONFIRMED. Training loop should work. <<<\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Gradient flow sanity check\n",
    "# Verify that gradients actually flow back to the soft prefix before committing\n",
    "# to the full training loop.\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"GRADIENT FLOW SANITY CHECK\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Create a tiny soft prefix\n",
    "test_prefix = torch.randn(1, PREFIX_LEN, HIDDEN_SIZE,\n",
    "                           device=exp_config.device,\n",
    "                           dtype=torch.float32,\n",
    "                           requires_grad=True)\n",
    "\n",
    "# Use first training sample\n",
    "sample = train_samples[0]\n",
    "sf_str_test = SURROGATE_PREFIX_TEMPLATE.format(surrogate=STATIC_FACT)\n",
    "doc_text = DOCUMENT_TEMPLATE.format(document=sample['passage'])\n",
    "\n",
    "# Matched tokenization\n",
    "full_text = sf_str_test + doc_text\n",
    "full_enc = tokenizer(full_text, return_tensors=\"pt\",\n",
    "                      add_special_tokens=True, padding=False, truncation=False)\n",
    "full_ids = full_enc['input_ids'].to(exp_config.device)\n",
    "sf_prefix_enc = tokenizer(sf_str_test, return_tensors=\"pt\",\n",
    "                           add_special_tokens=True, padding=False, truncation=False)\n",
    "sf_prefix_len_matched = sf_prefix_enc['input_ids'].shape[1]\n",
    "bos_id = full_ids[:, :1]\n",
    "doc_ids = full_ids[:, sf_prefix_len_matched:]\n",
    "\n",
    "query_prompt = QUERY_TEMPLATE.format(query=sample['query'])\n",
    "answer_text = ANSWER_TEMPLATE.format(answer=sample['answer'])\n",
    "\n",
    "print(f\"Test sample: doc_len={doc_ids.shape[1]}, query='{sample['query'][:50]}...'\")\n",
    "print(f\"Test prefix shape: {test_prefix.shape}\")\n",
    "\n",
    "test_loss = None\n",
    "try:\n",
    "    test_loss = differentiable_hybrid_score(\n",
    "        test_prefix, doc_ids, bos_id,\n",
    "        query_prompt, answer_text,\n",
    "        model, tokenizer, exp_config, CUTOFF)\n",
    "\n",
    "    print(f\"\\nForward pass: loss = {test_loss.item():.4f}\")\n",
    "    print(f\"Loss requires_grad: {test_loss.requires_grad}\")\n",
    "\n",
    "    test_loss.backward()\n",
    "\n",
    "    print(f\"Backward pass: SUCCESS\")\n",
    "    print(f\"Gradient shape: {test_prefix.grad.shape}\")\n",
    "    print(f\"Gradient norm: {test_prefix.grad.norm().item():.6f}\")\n",
    "    print(f\"Gradient mean: {test_prefix.grad.mean().item():.6f}\")\n",
    "    print(f\"Gradient max: {test_prefix.grad.abs().max().item():.6f}\")\n",
    "\n",
    "    if test_prefix.grad.norm().item() > 0:\n",
    "        print(\"\\n>>> GRADIENT FLOW CONFIRMED. Training loop should work. <<<\")\n",
    "    else:\n",
    "        print(\"\\n>>> WARNING: Zero gradients. Check computational graph. <<<\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n>>> GRADIENT FLOW FAILED: {e} <<<\")\n",
    "    print(\"Training will not work. Need to debug the differentiable path.\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "finally:\n",
    "    del test_prefix\n",
    "    if test_loss is not None:\n",
    "        del test_loss\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cell-9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T14:03:09.113911Z",
     "iopub.status.busy": "2026-02-16T14:03:09.113556Z",
     "iopub.status.idle": "2026-02-16T14:03:09.134390Z",
     "shell.execute_reply": "2026-02-16T14:03:09.133687Z"
    },
    "papermill": {
     "duration": 0.029308,
     "end_time": "2026-02-16T14:03:09.135829",
     "exception": false,
     "start_time": "2026-02-16T14:03:09.106521",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_soft_prefix() defined\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Training loop\n",
    "\n",
    "def train_soft_prefix(init_mode, train_data, n_epochs, lr, grad_accum,\n",
    "                       checkpoint_path, save_path, warmup_steps=50):\n",
    "    \"\"\"\n",
    "    Train soft prefix embeddings via gradient descent on answer NLL.\n",
    "\n",
    "    Args:\n",
    "        init_mode: 'random' or 'fact' (initialize from static_fact embeddings)\n",
    "        train_data: list of query dicts\n",
    "        n_epochs: number of passes over data\n",
    "        lr: learning rate\n",
    "        grad_accum: gradient accumulation steps\n",
    "        checkpoint_path: path to save training checkpoints\n",
    "        save_path: path to save final trained embeddings\n",
    "        warmup_steps: linear warmup steps\n",
    "\n",
    "    Returns:\n",
    "        dict with training history and final embeddings\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'=' * 70}\")\n",
    "    print(f\"TRAINING SOFT PREFIX — init={init_mode}\")\n",
    "    print(f\"{'=' * 70}\")\n",
    "\n",
    "    # Initialize soft prefix\n",
    "    if init_mode == 'random':\n",
    "        soft_prefix = torch.randn(1, PREFIX_LEN, HIDDEN_SIZE,\n",
    "                                   device=exp_config.device, dtype=torch.float32) * 0.02\n",
    "    elif init_mode == 'fact':\n",
    "        # Get embeddings for static_fact tokens\n",
    "        with torch.no_grad():\n",
    "            fact_emb = embed_fn(sf_ids)  # (1, prefix_len, hidden)\n",
    "        soft_prefix = fact_emb.float().clone()\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown init_mode: {init_mode}\")\n",
    "\n",
    "    soft_prefix = soft_prefix.detach().requires_grad_(True)\n",
    "\n",
    "    print(f\"  Soft prefix shape: {soft_prefix.shape}\")\n",
    "    print(f\"  Soft prefix dtype: {soft_prefix.dtype}\")\n",
    "    print(f\"  Soft prefix norm: {soft_prefix.norm().item():.4f}\")\n",
    "\n",
    "    optimizer = torch.optim.AdamW([soft_prefix], lr=lr, weight_decay=0.01)\n",
    "\n",
    "    total_steps = n_epochs * len(train_data)\n",
    "    total_optim_steps = total_steps // grad_accum\n",
    "    print(f\"  Total forward passes: {total_steps}\")\n",
    "    print(f\"  Total optimizer steps: {total_optim_steps}\")\n",
    "    print(f\"  Warmup steps: {warmup_steps}\")\n",
    "\n",
    "    # Checkpoint resume\n",
    "    history = []\n",
    "    start_step = 0\n",
    "    if checkpoint_path.exists():\n",
    "        ckpt = json.loads(checkpoint_path.read_text())\n",
    "        if ckpt.get('init_mode') == init_mode and ckpt.get('total_steps') == total_steps:\n",
    "            history = ckpt['history']\n",
    "            start_step = ckpt['completed_steps']\n",
    "            # Restore soft_prefix\n",
    "            soft_prefix_data = torch.tensor(ckpt['soft_prefix'],\n",
    "                                            device=exp_config.device, dtype=torch.float32)\n",
    "            soft_prefix = soft_prefix_data.requires_grad_(True)\n",
    "            optimizer = torch.optim.AdamW([soft_prefix], lr=lr, weight_decay=0.01)\n",
    "            print(f\"  Resumed from checkpoint: step {start_step}/{total_steps}\")\n",
    "\n",
    "    t_start = time.time()\n",
    "    step = 0\n",
    "    optim_step = 0\n",
    "    running_loss = 0.0\n",
    "    running_count = 0\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        # Shuffle training data each epoch\n",
    "        np.random.seed(SEED + epoch)\n",
    "        epoch_indices = np.random.permutation(len(train_data))\n",
    "\n",
    "        for idx_in_epoch, data_idx in enumerate(epoch_indices):\n",
    "            # Skip already-completed steps\n",
    "            if step < start_step:\n",
    "                step += 1\n",
    "                continue\n",
    "\n",
    "            sample = train_data[data_idx]\n",
    "            doc_text = DOCUMENT_TEMPLATE.format(document=sample['passage'])\n",
    "            query_prompt = QUERY_TEMPLATE.format(query=sample['query'])\n",
    "            answer_text = ANSWER_TEMPLATE.format(answer=sample['answer'])\n",
    "\n",
    "            # Matched tokenization using sf_str as reference\n",
    "            full_text = sf_str + doc_text\n",
    "            full_enc = tokenizer(full_text, return_tensors=\"pt\",\n",
    "                                  add_special_tokens=True, padding=False, truncation=False)\n",
    "            full_ids_t = full_enc['input_ids'].to(exp_config.device)\n",
    "            sf_enc = tokenizer(sf_str, return_tensors=\"pt\",\n",
    "                                add_special_tokens=True, padding=False, truncation=False)\n",
    "            sf_len = sf_enc['input_ids'].shape[1]\n",
    "            bos_t = full_ids_t[:, :1]\n",
    "            doc_ids_t = full_ids_t[:, sf_len:]\n",
    "\n",
    "            try:\n",
    "                loss = differentiable_hybrid_score(\n",
    "                    soft_prefix, doc_ids_t, bos_t,\n",
    "                    query_prompt, answer_text,\n",
    "                    model, tokenizer, exp_config, CUTOFF)\n",
    "\n",
    "                # Scale loss for gradient accumulation\n",
    "                scaled_loss = loss / grad_accum\n",
    "                scaled_loss.backward()\n",
    "\n",
    "                running_loss += loss.item()\n",
    "                running_count += 1\n",
    "\n",
    "            except RuntimeError as e:\n",
    "                print(f\"  Step {step}: RuntimeError: {e}\")\n",
    "                optimizer.zero_grad()\n",
    "                gc.collect()\n",
    "                torch.cuda.empty_cache()\n",
    "                step += 1\n",
    "                continue\n",
    "\n",
    "            # Optimizer step\n",
    "            if (step + 1) % grad_accum == 0:\n",
    "                # Linear warmup\n",
    "                optim_step += 1\n",
    "                if optim_step <= warmup_steps:\n",
    "                    warmup_factor = optim_step / warmup_steps\n",
    "                    for pg in optimizer.param_groups:\n",
    "                        pg['lr'] = lr * warmup_factor\n",
    "\n",
    "                # Gradient clipping\n",
    "                grad_norm = soft_prefix.grad.norm().item() if soft_prefix.grad is not None else 0\n",
    "                torch.nn.utils.clip_grad_norm_([soft_prefix], max_norm=1.0)\n",
    "\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                avg_loss = running_loss / running_count if running_count > 0 else 0\n",
    "                history.append({\n",
    "                    'step': step,\n",
    "                    'optim_step': optim_step,\n",
    "                    'epoch': epoch,\n",
    "                    'avg_loss': avg_loss,\n",
    "                    'grad_norm': grad_norm,\n",
    "                    'prefix_norm': soft_prefix.norm().item(),\n",
    "                    'lr': optimizer.param_groups[0]['lr'],\n",
    "                })\n",
    "                running_loss = 0.0\n",
    "                running_count = 0\n",
    "\n",
    "            # Cleanup\n",
    "            del loss, scaled_loss\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            step += 1\n",
    "\n",
    "            # Checkpoint\n",
    "            if step % CHECKPOINT_EVERY == 0 or step == total_steps:\n",
    "                elapsed = time.time() - t_start\n",
    "                steps_done = step - start_step\n",
    "                rate = steps_done / elapsed if elapsed > 0 else 0\n",
    "                remaining = (total_steps - step) / rate if rate > 0 else 0\n",
    "\n",
    "                last_loss = history[-1]['avg_loss'] if history else 0\n",
    "                tqdm.write(f\"  [{init_mode}] Step {step}/{total_steps} | \"\n",
    "                           f\"loss={last_loss:.4f} | \"\n",
    "                           f\"prefix_norm={soft_prefix.norm().item():.3f} | \"\n",
    "                           f\"ETA: {remaining/60:.1f}m\")\n",
    "\n",
    "                ckpt_data = {\n",
    "                    'init_mode': init_mode,\n",
    "                    'completed_steps': step,\n",
    "                    'total_steps': total_steps,\n",
    "                    'history': history,\n",
    "                    'soft_prefix': soft_prefix.detach().cpu().tolist(),\n",
    "                    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                }\n",
    "                with open(checkpoint_path, 'w') as f:\n",
    "                    json.dump(ckpt_data, f)\n",
    "\n",
    "    # Save final embeddings\n",
    "    torch.save(soft_prefix.detach().cpu(), save_path)\n",
    "\n",
    "    elapsed = time.time() - t_start\n",
    "    print(f\"\\n  Training complete: {step} steps in {elapsed/60:.1f} min\")\n",
    "    print(f\"  Final prefix norm: {soft_prefix.norm().item():.4f}\")\n",
    "    print(f\"  Saved to: {save_path}\")\n",
    "\n",
    "    return {\n",
    "        'soft_prefix': soft_prefix.detach(),\n",
    "        'history': history,\n",
    "        'init_mode': init_mode,\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"train_soft_prefix() defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cell-10",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T14:03:09.148700Z",
     "iopub.status.busy": "2026-02-16T14:03:09.148055Z",
     "iopub.status.idle": "2026-02-16T15:41:11.410378Z",
     "shell.execute_reply": "2026-02-16T15:41:11.409647Z"
    },
    "papermill": {
     "duration": 5882.281143,
     "end_time": "2026-02-16T15:41:11.422607",
     "exception": false,
     "start_time": "2026-02-16T14:03:09.141464",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "TRAINING SOFT PREFIX — init=random\n",
      "======================================================================\n",
      "  Soft prefix shape: torch.Size([1, 11, 2560])\n",
      "  Soft prefix dtype: torch.float32\n",
      "  Soft prefix norm: 3.3704\n",
      "  Total forward passes: 6000\n",
      "  Total optimizer steps: 1500\n",
      "  Warmup steps: 50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [random] Step 50/6000 | loss=4.3281 | prefix_norm=8.514 | ETA: 99.9m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [random] Step 100/6000 | loss=3.1680 | prefix_norm=21.994 | ETA: 99.2m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [random] Step 150/6000 | loss=4.5684 | prefix_norm=38.400 | ETA: 98.1m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [random] Step 200/6000 | loss=2.4883 | prefix_norm=59.334 | ETA: 97.2m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [random] Step 250/6000 | loss=1.0005 | prefix_norm=79.002 | ETA: 96.3m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [random] Step 300/6000 | loss=1.3291 | prefix_norm=97.345 | ETA: 95.4m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [random] Step 350/6000 | loss=1.7422 | prefix_norm=109.907 | ETA: 94.3m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [random] Step 400/6000 | loss=1.5088 | prefix_norm=120.991 | ETA: 93.4m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [random] Step 450/6000 | loss=0.6938 | prefix_norm=126.222 | ETA: 92.5m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [random] Step 500/6000 | loss=1.1221 | prefix_norm=129.091 | ETA: 91.7m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [random] Step 550/6000 | loss=2.0127 | prefix_norm=130.593 | ETA: 90.7m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [random] Step 600/6000 | loss=0.9299 | prefix_norm=135.107 | ETA: 89.7m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [random] Step 650/6000 | loss=1.8916 | prefix_norm=144.965 | ETA: 88.7m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [random] Step 700/6000 | loss=0.8594 | prefix_norm=155.810 | ETA: 87.8m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [random] Step 750/6000 | loss=1.0601 | prefix_norm=163.023 | ETA: 86.9m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [random] Step 800/6000 | loss=1.5527 | prefix_norm=167.556 | ETA: 85.9m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [random] Step 850/6000 | loss=1.1543 | prefix_norm=171.245 | ETA: 85.0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [random] Step 900/6000 | loss=0.5108 | prefix_norm=175.312 | ETA: 84.1m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [random] Step 950/6000 | loss=0.4312 | prefix_norm=180.946 | ETA: 83.2m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [random] Step 1000/6000 | loss=2.9595 | prefix_norm=188.277 | ETA: 82.3m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [random] Step 1050/6000 | loss=1.0264 | prefix_norm=194.935 | ETA: 81.4m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [random] Step 1100/6000 | loss=0.6096 | prefix_norm=200.719 | ETA: 80.6m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [random] Step 1150/6000 | loss=1.0903 | prefix_norm=207.757 | ETA: 79.7m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [random] Step 1200/6000 | loss=0.6265 | prefix_norm=212.233 | ETA: 78.9m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [random] Step 1250/6000 | loss=0.7678 | prefix_norm=216.087 | ETA: 78.0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [random] Step 1300/6000 | loss=0.5039 | prefix_norm=221.762 | ETA: 77.1m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [random] Step 1350/6000 | loss=1.6123 | prefix_norm=225.999 | ETA: 76.3m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [random] Step 1400/6000 | loss=0.4556 | prefix_norm=230.831 | ETA: 75.5m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [random] Step 1450/6000 | loss=1.0457 | prefix_norm=233.357 | ETA: 74.6m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [random] Step 1500/6000 | loss=0.2361 | prefix_norm=233.844 | ETA: 73.8m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [random] Step 1550/6000 | loss=1.8896 | prefix_norm=234.322 | ETA: 73.0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [random] Step 1600/6000 | loss=0.4958 | prefix_norm=236.968 | ETA: 72.1m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [random] Step 1650/6000 | loss=1.1982 | prefix_norm=237.597 | ETA: 71.3m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [random] Step 1700/6000 | loss=0.7468 | prefix_norm=237.114 | ETA: 70.5m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [random] Step 1750/6000 | loss=0.8198 | prefix_norm=235.925 | ETA: 69.7m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [random] Step 1800/6000 | loss=1.5791 | prefix_norm=236.208 | ETA: 68.9m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [random] Step 1850/6000 | loss=1.9072 | prefix_norm=237.040 | ETA: 68.1m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [random] Step 1900/6000 | loss=1.0381 | prefix_norm=238.781 | ETA: 67.2m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [random] Step 1950/6000 | loss=0.9686 | prefix_norm=240.070 | ETA: 66.4m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [random] Step 2000/6000 | loss=1.9292 | prefix_norm=240.983 | ETA: 65.6m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [random] Step 2050/6000 | loss=0.4219 | prefix_norm=241.674 | ETA: 64.8m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [random] Step 2100/6000 | loss=0.6133 | prefix_norm=241.682 | ETA: 63.9m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [random] Step 2150/6000 | loss=1.1306 | prefix_norm=245.653 | ETA: 63.1m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [random] Step 2200/6000 | loss=0.6733 | prefix_norm=249.710 | ETA: 62.3m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [random] Step 2250/6000 | loss=1.0967 | prefix_norm=253.955 | ETA: 61.4m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [random] Step 2300/6000 | loss=0.7767 | prefix_norm=257.498 | ETA: 60.6m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [random] Step 2350/6000 | loss=0.7935 | prefix_norm=260.198 | ETA: 59.8m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [random] Step 2400/6000 | loss=1.3330 | prefix_norm=263.868 | ETA: 59.0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [random] Step 2450/6000 | loss=0.5945 | prefix_norm=266.659 | ETA: 58.2m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [random] Step 2500/6000 | loss=0.5818 | prefix_norm=268.590 | ETA: 57.3m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [random] Step 2550/6000 | loss=0.9957 | prefix_norm=268.466 | ETA: 56.5m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [random] Step 2600/6000 | loss=0.8276 | prefix_norm=268.985 | ETA: 55.7m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [random] Step 2650/6000 | loss=0.7240 | prefix_norm=269.838 | ETA: 54.9m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [random] Step 2700/6000 | loss=0.4751 | prefix_norm=272.497 | ETA: 54.1m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [random] Step 2750/6000 | loss=0.3572 | prefix_norm=275.604 | ETA: 53.2m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [random] Step 2800/6000 | loss=0.3898 | prefix_norm=276.629 | ETA: 52.4m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [random] Step 2850/6000 | loss=1.1650 | prefix_norm=276.467 | ETA: 51.6m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [random] Step 2900/6000 | loss=1.0706 | prefix_norm=275.268 | ETA: 50.8m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [random] Step 2950/6000 | loss=0.6626 | prefix_norm=273.397 | ETA: 49.9m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [random] Step 3000/6000 | loss=3.7054 | prefix_norm=271.935 | ETA: 49.1m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [random] Step 3050/6000 | loss=2.5156 | prefix_norm=271.881 | ETA: 48.3m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [random] Step 3100/6000 | loss=2.3135 | prefix_norm=270.348 | ETA: 47.5m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [random] Step 3150/6000 | loss=0.4333 | prefix_norm=269.274 | ETA: 46.7m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [random] Step 3200/6000 | loss=1.3892 | prefix_norm=268.024 | ETA: 45.9m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [random] Step 3250/6000 | loss=0.3653 | prefix_norm=267.654 | ETA: 45.0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [random] Step 3300/6000 | loss=0.4308 | prefix_norm=266.720 | ETA: 44.2m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [random] Step 3350/6000 | loss=1.7095 | prefix_norm=265.763 | ETA: 43.4m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [random] Step 3400/6000 | loss=2.0437 | prefix_norm=264.331 | ETA: 42.6m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [random] Step 3450/6000 | loss=2.1021 | prefix_norm=262.707 | ETA: 41.8m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [random] Step 3500/6000 | loss=0.8105 | prefix_norm=261.635 | ETA: 40.9m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [random] Step 3550/6000 | loss=0.9937 | prefix_norm=260.321 | ETA: 40.1m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [random] Step 3600/6000 | loss=1.6986 | prefix_norm=258.491 | ETA: 39.3m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [random] Step 3650/6000 | loss=2.2783 | prefix_norm=256.861 | ETA: 38.5m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [random] Step 3700/6000 | loss=0.6938 | prefix_norm=254.885 | ETA: 37.6m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [random] Step 3750/6000 | loss=2.8628 | prefix_norm=252.822 | ETA: 36.8m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [random] Step 3800/6000 | loss=0.8428 | prefix_norm=252.481 | ETA: 36.0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [random] Step 3850/6000 | loss=0.5315 | prefix_norm=251.817 | ETA: 35.2m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [random] Step 3900/6000 | loss=0.5833 | prefix_norm=250.882 | ETA: 34.4m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [random] Step 3950/6000 | loss=0.7427 | prefix_norm=249.360 | ETA: 33.5m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [random] Step 4000/6000 | loss=0.7471 | prefix_norm=247.296 | ETA: 32.7m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [random] Step 4050/6000 | loss=0.8701 | prefix_norm=245.767 | ETA: 31.9m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [random] Step 4100/6000 | loss=0.5746 | prefix_norm=244.512 | ETA: 31.1m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [random] Step 4150/6000 | loss=1.2357 | prefix_norm=244.477 | ETA: 30.3m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [random] Step 4200/6000 | loss=0.2323 | prefix_norm=243.836 | ETA: 29.5m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [random] Step 4250/6000 | loss=0.6763 | prefix_norm=242.682 | ETA: 28.6m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [random] Step 4300/6000 | loss=0.2386 | prefix_norm=240.707 | ETA: 27.8m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [random] Step 4350/6000 | loss=0.5569 | prefix_norm=239.386 | ETA: 27.0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [random] Step 4400/6000 | loss=1.0536 | prefix_norm=238.314 | ETA: 26.2m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [random] Step 4450/6000 | loss=0.9536 | prefix_norm=236.879 | ETA: 25.3m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [random] Step 4500/6000 | loss=0.2538 | prefix_norm=236.360 | ETA: 24.5m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [random] Step 4550/6000 | loss=0.3458 | prefix_norm=235.353 | ETA: 23.7m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [random] Step 4600/6000 | loss=0.4147 | prefix_norm=235.860 | ETA: 22.9m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [random] Step 4650/6000 | loss=1.2197 | prefix_norm=237.663 | ETA: 22.1m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [random] Step 4700/6000 | loss=2.1143 | prefix_norm=238.535 | ETA: 21.3m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [random] Step 4750/6000 | loss=0.4125 | prefix_norm=237.575 | ETA: 20.4m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [random] Step 4800/6000 | loss=1.6951 | prefix_norm=236.013 | ETA: 19.6m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [random] Step 4850/6000 | loss=0.6720 | prefix_norm=234.818 | ETA: 18.8m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [random] Step 4900/6000 | loss=0.8135 | prefix_norm=233.207 | ETA: 18.0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [random] Step 4950/6000 | loss=0.2453 | prefix_norm=231.586 | ETA: 17.2m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [random] Step 5000/6000 | loss=1.4818 | prefix_norm=230.151 | ETA: 16.3m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [random] Step 5050/6000 | loss=0.1896 | prefix_norm=229.066 | ETA: 15.5m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [random] Step 5100/6000 | loss=0.5469 | prefix_norm=227.802 | ETA: 14.7m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [random] Step 5150/6000 | loss=0.7786 | prefix_norm=227.310 | ETA: 13.9m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [random] Step 5200/6000 | loss=0.2618 | prefix_norm=226.812 | ETA: 13.1m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [random] Step 5250/6000 | loss=0.7766 | prefix_norm=225.947 | ETA: 12.3m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [random] Step 5300/6000 | loss=1.3604 | prefix_norm=225.053 | ETA: 11.4m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [random] Step 5350/6000 | loss=0.3140 | prefix_norm=224.023 | ETA: 10.6m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [random] Step 5400/6000 | loss=0.5007 | prefix_norm=224.362 | ETA: 9.8m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [random] Step 5450/6000 | loss=0.8826 | prefix_norm=225.626 | ETA: 9.0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [random] Step 5500/6000 | loss=1.5181 | prefix_norm=228.594 | ETA: 8.2m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [random] Step 5550/6000 | loss=1.8487 | prefix_norm=230.081 | ETA: 7.4m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [random] Step 5600/6000 | loss=0.6943 | prefix_norm=230.084 | ETA: 6.5m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [random] Step 5650/6000 | loss=0.3636 | prefix_norm=229.059 | ETA: 5.7m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [random] Step 5700/6000 | loss=1.3046 | prefix_norm=227.506 | ETA: 4.9m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [random] Step 5750/6000 | loss=0.6770 | prefix_norm=226.832 | ETA: 4.1m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [random] Step 5800/6000 | loss=0.4604 | prefix_norm=227.099 | ETA: 3.3m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [random] Step 5850/6000 | loss=0.4641 | prefix_norm=227.707 | ETA: 2.5m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [random] Step 5900/6000 | loss=0.7998 | prefix_norm=226.922 | ETA: 1.6m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [random] Step 5950/6000 | loss=0.2055 | prefix_norm=225.438 | ETA: 0.8m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [random] Step 6000/6000 | loss=0.8070 | prefix_norm=224.251 | ETA: 0.0m\n",
      "\n",
      "  Training complete: 6000 steps in 98.0 min\n",
      "  Final prefix norm: 224.2508\n",
      "  Saved to: results/exp25/soft_prefix_random.pt\n"
     ]
    }
   ],
   "source": [
    "# Cell 10: Train random-init soft prefix\n",
    "\n",
    "result_random = train_soft_prefix(\n",
    "    init_mode='random',\n",
    "    train_data=train_samples,\n",
    "    n_epochs=N_EPOCHS,\n",
    "    lr=LR,\n",
    "    grad_accum=GRAD_ACCUM_STEPS,\n",
    "    checkpoint_path=CHECKPOINT_TRAIN_RAND_PATH,\n",
    "    save_path=SOFT_RANDOM_PATH,\n",
    "    warmup_steps=WARMUP_STEPS,\n",
    ")\n",
    "soft_prefix_random = result_random['soft_prefix']\n",
    "history_random = result_random['history']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cell-11",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T15:41:11.446107Z",
     "iopub.status.busy": "2026-02-16T15:41:11.445766Z",
     "iopub.status.idle": "2026-02-16T17:21:11.975792Z",
     "shell.execute_reply": "2026-02-16T17:21:11.974866Z"
    },
    "papermill": {
     "duration": 6000.561188,
     "end_time": "2026-02-16T17:21:11.994915",
     "exception": false,
     "start_time": "2026-02-16T15:41:11.433727",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "TRAINING SOFT PREFIX — init=fact\n",
      "======================================================================\n",
      "  Soft prefix shape: torch.Size([1, 11, 2560])\n",
      "  Soft prefix dtype: torch.float32\n",
      "  Soft prefix norm: 173.1110\n",
      "  Total forward passes: 6000\n",
      "  Total optimizer steps: 1500\n",
      "  Warmup steps: 50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [fact] Step 50/6000 | loss=4.0989 | prefix_norm=173.043 | ETA: 96.8m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [fact] Step 100/6000 | loss=1.4463 | prefix_norm=173.563 | ETA: 96.6m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [fact] Step 150/6000 | loss=1.8144 | prefix_norm=174.901 | ETA: 95.6m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [fact] Step 200/6000 | loss=2.2485 | prefix_norm=178.937 | ETA: 94.8m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [fact] Step 250/6000 | loss=0.6027 | prefix_norm=183.207 | ETA: 94.2m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [fact] Step 300/6000 | loss=0.5479 | prefix_norm=189.185 | ETA: 93.3m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [fact] Step 350/6000 | loss=0.6743 | prefix_norm=196.577 | ETA: 92.4m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [fact] Step 400/6000 | loss=0.7170 | prefix_norm=206.427 | ETA: 91.5m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [fact] Step 450/6000 | loss=0.5270 | prefix_norm=212.221 | ETA: 90.7m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [fact] Step 500/6000 | loss=0.8363 | prefix_norm=216.312 | ETA: 89.8m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [fact] Step 550/6000 | loss=0.9771 | prefix_norm=218.655 | ETA: 89.1m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [fact] Step 600/6000 | loss=0.7285 | prefix_norm=223.597 | ETA: 88.2m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [fact] Step 650/6000 | loss=2.0210 | prefix_norm=230.466 | ETA: 87.5m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [fact] Step 700/6000 | loss=0.4775 | prefix_norm=236.178 | ETA: 86.7m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [fact] Step 750/6000 | loss=0.5046 | prefix_norm=240.370 | ETA: 86.0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [fact] Step 800/6000 | loss=1.5361 | prefix_norm=245.301 | ETA: 85.2m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [fact] Step 850/6000 | loss=0.8096 | prefix_norm=251.163 | ETA: 84.5m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [fact] Step 900/6000 | loss=0.1721 | prefix_norm=255.664 | ETA: 83.7m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [fact] Step 950/6000 | loss=0.4282 | prefix_norm=259.967 | ETA: 83.0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [fact] Step 1000/6000 | loss=1.9426 | prefix_norm=265.393 | ETA: 82.2m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [fact] Step 1050/6000 | loss=1.2310 | prefix_norm=271.878 | ETA: 81.4m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [fact] Step 1100/6000 | loss=1.9163 | prefix_norm=283.728 | ETA: 80.6m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [fact] Step 1150/6000 | loss=0.9087 | prefix_norm=293.409 | ETA: 79.9m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [fact] Step 1200/6000 | loss=0.6240 | prefix_norm=299.588 | ETA: 79.3m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [fact] Step 1250/6000 | loss=0.5367 | prefix_norm=303.550 | ETA: 78.6m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [fact] Step 1300/6000 | loss=0.3469 | prefix_norm=306.742 | ETA: 77.8m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [fact] Step 1350/6000 | loss=1.2173 | prefix_norm=311.850 | ETA: 77.1m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [fact] Step 1400/6000 | loss=0.1074 | prefix_norm=323.000 | ETA: 76.3m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [fact] Step 1450/6000 | loss=1.0303 | prefix_norm=326.750 | ETA: 75.5m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [fact] Step 1500/6000 | loss=0.1704 | prefix_norm=326.177 | ETA: 74.6m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [fact] Step 1550/6000 | loss=1.6221 | prefix_norm=325.669 | ETA: 73.8m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [fact] Step 1600/6000 | loss=0.4321 | prefix_norm=328.198 | ETA: 73.1m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [fact] Step 1650/6000 | loss=1.1418 | prefix_norm=328.895 | ETA: 72.3m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [fact] Step 1700/6000 | loss=0.5713 | prefix_norm=328.215 | ETA: 71.6m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [fact] Step 1750/6000 | loss=0.4651 | prefix_norm=327.645 | ETA: 70.8m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [fact] Step 1800/6000 | loss=0.6987 | prefix_norm=325.846 | ETA: 70.0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [fact] Step 1850/6000 | loss=1.9966 | prefix_norm=327.860 | ETA: 69.2m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [fact] Step 1900/6000 | loss=0.6436 | prefix_norm=330.492 | ETA: 68.4m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [fact] Step 1950/6000 | loss=0.5859 | prefix_norm=330.516 | ETA: 67.6m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [fact] Step 2000/6000 | loss=0.7690 | prefix_norm=329.800 | ETA: 66.7m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [fact] Step 2050/6000 | loss=0.2565 | prefix_norm=329.461 | ETA: 65.9m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [fact] Step 2100/6000 | loss=0.7124 | prefix_norm=327.900 | ETA: 65.1m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [fact] Step 2150/6000 | loss=0.7848 | prefix_norm=326.024 | ETA: 64.2m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [fact] Step 2200/6000 | loss=0.3357 | prefix_norm=324.106 | ETA: 63.3m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [fact] Step 2250/6000 | loss=0.6050 | prefix_norm=323.817 | ETA: 62.5m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [fact] Step 2300/6000 | loss=0.6779 | prefix_norm=324.256 | ETA: 61.6m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [fact] Step 2350/6000 | loss=0.5959 | prefix_norm=322.962 | ETA: 60.8m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [fact] Step 2400/6000 | loss=1.8389 | prefix_norm=321.386 | ETA: 59.9m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [fact] Step 2450/6000 | loss=0.4082 | prefix_norm=322.148 | ETA: 59.1m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [fact] Step 2500/6000 | loss=0.5205 | prefix_norm=322.958 | ETA: 58.2m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [fact] Step 2550/6000 | loss=0.8324 | prefix_norm=322.284 | ETA: 57.3m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [fact] Step 2600/6000 | loss=0.6562 | prefix_norm=320.524 | ETA: 56.5m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [fact] Step 2650/6000 | loss=1.0406 | prefix_norm=318.758 | ETA: 55.6m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [fact] Step 2700/6000 | loss=0.4702 | prefix_norm=319.150 | ETA: 54.8m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [fact] Step 2750/6000 | loss=0.1854 | prefix_norm=320.163 | ETA: 54.0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [fact] Step 2800/6000 | loss=0.4631 | prefix_norm=319.318 | ETA: 53.1m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [fact] Step 2850/6000 | loss=2.6787 | prefix_norm=317.954 | ETA: 52.3m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [fact] Step 2900/6000 | loss=1.0909 | prefix_norm=317.911 | ETA: 51.4m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [fact] Step 2950/6000 | loss=0.6544 | prefix_norm=317.147 | ETA: 50.6m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [fact] Step 3000/6000 | loss=1.1876 | prefix_norm=316.813 | ETA: 49.7m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [fact] Step 3050/6000 | loss=2.3193 | prefix_norm=318.388 | ETA: 48.9m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [fact] Step 3100/6000 | loss=2.3428 | prefix_norm=319.575 | ETA: 48.1m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [fact] Step 3150/6000 | loss=0.4014 | prefix_norm=324.655 | ETA: 47.2m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [fact] Step 3200/6000 | loss=1.3223 | prefix_norm=328.132 | ETA: 46.4m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [fact] Step 3250/6000 | loss=0.4204 | prefix_norm=328.415 | ETA: 45.5m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [fact] Step 3300/6000 | loss=0.3298 | prefix_norm=326.609 | ETA: 44.7m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [fact] Step 3350/6000 | loss=1.5812 | prefix_norm=325.338 | ETA: 43.9m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [fact] Step 3400/6000 | loss=1.7153 | prefix_norm=325.029 | ETA: 43.0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [fact] Step 3450/6000 | loss=2.5791 | prefix_norm=324.093 | ETA: 42.2m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [fact] Step 3500/6000 | loss=0.7828 | prefix_norm=323.470 | ETA: 41.4m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [fact] Step 3550/6000 | loss=1.2161 | prefix_norm=322.067 | ETA: 40.5m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [fact] Step 3600/6000 | loss=1.4824 | prefix_norm=319.965 | ETA: 39.7m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [fact] Step 3650/6000 | loss=2.5149 | prefix_norm=318.809 | ETA: 38.9m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [fact] Step 3700/6000 | loss=0.6133 | prefix_norm=317.920 | ETA: 38.1m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [fact] Step 3750/6000 | loss=3.2217 | prefix_norm=316.801 | ETA: 37.3m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [fact] Step 3800/6000 | loss=0.7349 | prefix_norm=317.226 | ETA: 36.5m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [fact] Step 3850/6000 | loss=0.5710 | prefix_norm=318.048 | ETA: 35.6m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [fact] Step 3900/6000 | loss=0.5078 | prefix_norm=319.285 | ETA: 34.8m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [fact] Step 3950/6000 | loss=0.8566 | prefix_norm=319.376 | ETA: 34.0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [fact] Step 4000/6000 | loss=0.8125 | prefix_norm=318.890 | ETA: 33.1m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [fact] Step 4050/6000 | loss=0.8712 | prefix_norm=320.949 | ETA: 32.3m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [fact] Step 4100/6000 | loss=0.9366 | prefix_norm=323.952 | ETA: 31.5m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [fact] Step 4150/6000 | loss=1.6975 | prefix_norm=329.940 | ETA: 30.6m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [fact] Step 4200/6000 | loss=0.3486 | prefix_norm=333.770 | ETA: 29.8m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [fact] Step 4250/6000 | loss=0.7673 | prefix_norm=334.414 | ETA: 29.0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [fact] Step 4300/6000 | loss=0.3282 | prefix_norm=332.645 | ETA: 28.2m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [fact] Step 4350/6000 | loss=0.5969 | prefix_norm=330.933 | ETA: 27.3m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [fact] Step 4400/6000 | loss=1.0804 | prefix_norm=329.255 | ETA: 26.5m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [fact] Step 4450/6000 | loss=0.7456 | prefix_norm=327.418 | ETA: 25.7m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [fact] Step 4500/6000 | loss=0.2810 | prefix_norm=325.389 | ETA: 24.8m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [fact] Step 4550/6000 | loss=0.3907 | prefix_norm=323.150 | ETA: 24.0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [fact] Step 4600/6000 | loss=0.3221 | prefix_norm=320.658 | ETA: 23.2m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [fact] Step 4650/6000 | loss=0.9478 | prefix_norm=321.041 | ETA: 22.4m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [fact] Step 4700/6000 | loss=2.3209 | prefix_norm=331.541 | ETA: 21.5m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [fact] Step 4750/6000 | loss=0.3109 | prefix_norm=335.202 | ETA: 20.7m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [fact] Step 4800/6000 | loss=1.1671 | prefix_norm=335.312 | ETA: 19.9m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [fact] Step 4850/6000 | loss=0.4681 | prefix_norm=335.539 | ETA: 19.1m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [fact] Step 4900/6000 | loss=0.6709 | prefix_norm=334.042 | ETA: 18.2m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [fact] Step 4950/6000 | loss=0.2158 | prefix_norm=331.957 | ETA: 17.4m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [fact] Step 5000/6000 | loss=1.6180 | prefix_norm=329.675 | ETA: 16.6m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [fact] Step 5050/6000 | loss=0.2758 | prefix_norm=328.440 | ETA: 15.8m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [fact] Step 5100/6000 | loss=0.5076 | prefix_norm=328.979 | ETA: 14.9m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [fact] Step 5150/6000 | loss=0.7273 | prefix_norm=329.544 | ETA: 14.1m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [fact] Step 5200/6000 | loss=0.2453 | prefix_norm=328.454 | ETA: 13.3m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [fact] Step 5250/6000 | loss=0.6374 | prefix_norm=327.011 | ETA: 12.5m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [fact] Step 5300/6000 | loss=1.0811 | prefix_norm=329.693 | ETA: 11.6m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [fact] Step 5350/6000 | loss=0.3120 | prefix_norm=333.066 | ETA: 10.8m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [fact] Step 5400/6000 | loss=0.5459 | prefix_norm=337.212 | ETA: 10.0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [fact] Step 5450/6000 | loss=0.6583 | prefix_norm=338.643 | ETA: 9.1m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [fact] Step 5500/6000 | loss=1.1802 | prefix_norm=337.436 | ETA: 8.3m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [fact] Step 5550/6000 | loss=1.9827 | prefix_norm=337.008 | ETA: 7.5m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [fact] Step 5600/6000 | loss=0.8118 | prefix_norm=338.236 | ETA: 6.7m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [fact] Step 5650/6000 | loss=0.2562 | prefix_norm=337.392 | ETA: 5.8m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [fact] Step 5700/6000 | loss=1.2173 | prefix_norm=335.595 | ETA: 5.0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [fact] Step 5750/6000 | loss=0.7593 | prefix_norm=337.288 | ETA: 4.2m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [fact] Step 5800/6000 | loss=0.2650 | prefix_norm=339.438 | ETA: 3.3m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [fact] Step 5850/6000 | loss=0.4082 | prefix_norm=339.450 | ETA: 2.5m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [fact] Step 5900/6000 | loss=0.8523 | prefix_norm=338.312 | ETA: 1.7m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [fact] Step 5950/6000 | loss=0.2150 | prefix_norm=337.011 | ETA: 0.8m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [fact] Step 6000/6000 | loss=0.6031 | prefix_norm=334.973 | ETA: 0.0m\n",
      "\n",
      "  Training complete: 6000 steps in 100.0 min\n",
      "  Final prefix norm: 334.9726\n",
      "  Saved to: results/exp25/soft_prefix_fact.pt\n"
     ]
    }
   ],
   "source": [
    "# Cell 11: Train fact-init soft prefix\n",
    "\n",
    "result_fact = train_soft_prefix(\n",
    "    init_mode='fact',\n",
    "    train_data=train_samples,\n",
    "    n_epochs=N_EPOCHS,\n",
    "    lr=LR,\n",
    "    grad_accum=GRAD_ACCUM_STEPS,\n",
    "    checkpoint_path=CHECKPOINT_TRAIN_FACT_PATH,\n",
    "    save_path=SOFT_FACT_PATH,\n",
    "    warmup_steps=WARMUP_STEPS,\n",
    ")\n",
    "soft_prefix_fact = result_fact['soft_prefix']\n",
    "history_fact = result_fact['history']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cell-12",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T17:21:12.031050Z",
     "iopub.status.busy": "2026-02-16T17:21:12.030213Z",
     "iopub.status.idle": "2026-02-16T17:21:13.448140Z",
     "shell.execute_reply": "2026-02-16T17:21:13.447072Z"
    },
    "papermill": {
     "duration": 1.438267,
     "end_time": "2026-02-16T17:21:13.450099",
     "exception": false,
     "start_time": "2026-02-16T17:21:12.011832",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training curves saved to results/exp25/training_curves.png\n"
     ]
    }
   ],
   "source": [
    "# Cell 12: Training curves visualization\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for hist, label, color in [(history_random, 'random init', '#1f77b4'),\n",
    "                            (history_fact, 'fact init', '#ff7f0e')]:\n",
    "    if not hist:\n",
    "        continue\n",
    "    steps = [h['optim_step'] for h in hist]\n",
    "    losses = [h['avg_loss'] for h in hist]\n",
    "    gnorms = [h['grad_norm'] for h in hist]\n",
    "    pnorms = [h['prefix_norm'] for h in hist]\n",
    "\n",
    "    # Smoothed loss (rolling average window=20)\n",
    "    w = min(20, len(losses) // 3 + 1)\n",
    "    smoothed = np.convolve(losses, np.ones(w)/w, mode='valid') if len(losses) > w else losses\n",
    "    smooth_steps = steps[w-1:] if len(losses) > w else steps\n",
    "\n",
    "    axes[0].plot(steps, losses, alpha=0.2, color=color)\n",
    "    axes[0].plot(smooth_steps, smoothed, linewidth=2, color=color, label=label)\n",
    "    axes[1].plot(steps, gnorms, alpha=0.5, linewidth=1, color=color, label=label)\n",
    "    axes[2].plot(steps, pnorms, linewidth=2, color=color, label=label)\n",
    "\n",
    "axes[0].set_xlabel('Optimizer Step')\n",
    "axes[0].set_ylabel('Loss (NLL)')\n",
    "axes[0].set_title('Training Loss')\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].set_xlabel('Optimizer Step')\n",
    "axes[1].set_ylabel('Gradient Norm')\n",
    "axes[1].set_title('Gradient Norm')\n",
    "axes[1].legend()\n",
    "\n",
    "axes[2].set_xlabel('Optimizer Step')\n",
    "axes[2].set_ylabel('Prefix Embedding Norm')\n",
    "axes[2].set_title('Prefix Norm')\n",
    "axes[2].legend()\n",
    "\n",
    "plt.suptitle('Exp 25: Soft Prefix Training Curves', fontsize=13)\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'training_curves.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"Training curves saved to {RESULTS_DIR / 'training_curves.png'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cell-13",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T17:21:13.486931Z",
     "iopub.status.busy": "2026-02-16T17:21:13.486258Z",
     "iopub.status.idle": "2026-02-16T17:31:45.551026Z",
     "shell.execute_reply": "2026-02-16T17:31:45.550096Z"
    },
    "papermill": {
     "duration": 632.103966,
     "end_time": "2026-02-16T17:31:45.571368",
     "exception": false,
     "start_time": "2026-02-16T17:21:13.467402",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PART 2: EVALUATION (300 queries, 4 conditions)\n",
      "======================================================================\n",
      "No checkpoint found. Starting fresh.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "808f34f5b6164be7ab86f1fff629f9f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval:   0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 50/300 | 50 done in 1.8m | ETA: 8.8 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 100/300 | 100 done in 3.5m | ETA: 7.0 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 150/300 | 150 done in 5.3m | ETA: 5.3 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 200/300 | 200 done in 7.0m | ETA: 3.5 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 250/300 | 250 done in 8.8m | ETA: 1.8 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 300/300 | 300 done in 10.5m | ETA: 0.0 min\n",
      "\n",
      "Eval complete: 300 queries in 10.5 min\n"
     ]
    }
   ],
   "source": [
    "# Cell 13: Part 2 — Evaluation (300 validation queries, 4 conditions)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"PART 2: EVALUATION ({N_EVAL} queries, 4 conditions)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Load trained soft prefixes (in case of restart)\n",
    "if 'soft_prefix_random' not in dir():\n",
    "    soft_prefix_random = torch.load(SOFT_RANDOM_PATH).to(exp_config.device)\n",
    "    print(f\"Loaded soft_prefix_random from {SOFT_RANDOM_PATH}\")\n",
    "if 'soft_prefix_fact' not in dir():\n",
    "    soft_prefix_fact = torch.load(SOFT_FACT_PATH).to(exp_config.device)\n",
    "    print(f\"Loaded soft_prefix_fact from {SOFT_FACT_PATH}\")\n",
    "\n",
    "# Checkpoint resume\n",
    "eval_results = []\n",
    "eval_start_idx = 0\n",
    "\n",
    "if CHECKPOINT_EVAL_PATH.exists():\n",
    "    with open(CHECKPOINT_EVAL_PATH, 'r') as f:\n",
    "        ckpt = json.load(f)\n",
    "    ckpt_queries = ckpt.get('query_texts', [])\n",
    "    current_queries = [q['query'] for q in eval_samples[:N_EVAL]]\n",
    "    if ckpt_queries == current_queries:\n",
    "        eval_results = ckpt['results']\n",
    "        eval_start_idx = len(eval_results)\n",
    "        print(f\"Resuming from checkpoint: {eval_start_idx}/{N_EVAL}\")\n",
    "    else:\n",
    "        print(\"Checkpoint query mismatch. Starting fresh.\")\n",
    "else:\n",
    "    print(\"No checkpoint found. Starting fresh.\")\n",
    "\n",
    "layer_indices = list(range(CUTOFF))\n",
    "t_start = time.time()\n",
    "\n",
    "for qidx in tqdm(range(eval_start_idx, N_EVAL), initial=eval_start_idx, total=N_EVAL,\n",
    "                  desc=\"Eval\"):\n",
    "    qdata = eval_samples[qidx]\n",
    "    query_prompt = QUERY_TEMPLATE.format(query=qdata['query'])\n",
    "    answer_text = ANSWER_TEMPLATE.format(answer=qdata['answer'])\n",
    "    passage = qdata['passage']\n",
    "    document_text = DOCUMENT_TEMPLATE.format(document=passage)\n",
    "\n",
    "    # Matched tokenization (using sf_str as reference for BPE boundaries)\n",
    "    full_text = sf_str + document_text\n",
    "    full_enc = tokenizer(full_text, return_tensors=\"pt\",\n",
    "                          add_special_tokens=True, padding=False, truncation=False)\n",
    "    full_ids = full_enc['input_ids'].to(exp_config.device)\n",
    "    sf_prefix_enc = tokenizer(sf_str, return_tensors=\"pt\",\n",
    "                               add_special_tokens=True, padding=False, truncation=False)\n",
    "    sf_prefix_len_matched = sf_prefix_enc['input_ids'].shape[1]\n",
    "    bos_id = full_ids[:, :1]\n",
    "    doc_ids = full_ids[:, sf_prefix_len_matched:]\n",
    "    doc_len = doc_ids.shape[1]\n",
    "    context_len = 1 + doc_len\n",
    "\n",
    "    del full_enc, full_ids, sf_prefix_enc\n",
    "\n",
    "    # --- Condition 1: bare ---\n",
    "    bare_input = torch.cat([bos_id, doc_ids], dim=1)\n",
    "    with torch.no_grad():\n",
    "        bare_out = model(input_ids=bare_input,\n",
    "                         attention_mask=torch.ones_like(bare_input),\n",
    "                         use_cache=True, return_dict=True)\n",
    "    bare_cache = _ensure_dynamic_cache(bare_out.past_key_values)\n",
    "    del bare_out\n",
    "\n",
    "    bare_nll = score_answer_with_cache(\n",
    "        deepcopy_cache(bare_cache), context_len,\n",
    "        query_prompt, answer_text, model, tokenizer, exp_config)\n",
    "\n",
    "    # --- Condition 2: vel_static (discrete static_fact prefix) ---\n",
    "    primed_input = torch.cat([bos_id, sf_ids, doc_ids], dim=1)\n",
    "    with torch.no_grad():\n",
    "        primed_out = model(input_ids=primed_input,\n",
    "                           attention_mask=torch.ones_like(primed_input),\n",
    "                           use_cache=True, return_dict=True)\n",
    "    primed_full = _ensure_dynamic_cache(primed_out.past_key_values)\n",
    "    del primed_out\n",
    "\n",
    "    trunc_raw = extract_and_truncate_cache_with_bos(primed_full, doc_len)\n",
    "    sf_trunc_cache = deepcopy_cache(trunc_raw)\n",
    "    correct_rope_positions_with_bos(sf_trunc_cache, sf_ids.shape[1], model)\n",
    "    del primed_full, trunc_raw\n",
    "\n",
    "    vel_static_cache = replace_values_at_layers(bare_cache, sf_trunc_cache, layer_indices)\n",
    "    vel_static_nll = score_answer_with_cache(\n",
    "        deepcopy_cache(vel_static_cache), context_len,\n",
    "        query_prompt, answer_text, model, tokenizer, exp_config)\n",
    "    del sf_trunc_cache, vel_static_cache\n",
    "\n",
    "    # --- Helper: score soft prefix condition ---\n",
    "    def score_soft_condition(soft_embs):\n",
    "        \"\"\"Build hybrid cache from soft embeddings and score.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            bos_emb = embed_fn(bos_id)\n",
    "            doc_emb = embed_fn(doc_ids)\n",
    "            soft_cast = soft_embs.to(device=exp_config.device, dtype=bos_emb.dtype)\n",
    "\n",
    "            inputs_embeds = torch.cat([bos_emb, soft_cast, doc_emb], dim=1)\n",
    "            total_len = inputs_embeds.shape[1]\n",
    "            attn_mask = torch.ones((1, total_len), device=exp_config.device, dtype=torch.long)\n",
    "\n",
    "            soft_out = model(inputs_embeds=inputs_embeds,\n",
    "                            attention_mask=attn_mask,\n",
    "                            use_cache=True, return_dict=True)\n",
    "            soft_cache = _ensure_dynamic_cache(soft_out.past_key_values)\n",
    "            del soft_out\n",
    "\n",
    "            # Extract BOS + doc values from soft cache, splice into bare\n",
    "            soft_trunc = extract_and_truncate_cache_with_bos(soft_cache, doc_len)\n",
    "            # No RoPE correction needed because we use bare keys\n",
    "            # (values don't have positional encoding)\n",
    "            del soft_cache\n",
    "\n",
    "            vel_soft_cache = replace_values_at_layers(bare_cache, soft_trunc, layer_indices)\n",
    "            del soft_trunc\n",
    "\n",
    "            nll = score_answer_with_cache(\n",
    "                deepcopy_cache(vel_soft_cache), context_len,\n",
    "                query_prompt, answer_text, model, tokenizer, exp_config)\n",
    "            del vel_soft_cache\n",
    "\n",
    "        return nll\n",
    "\n",
    "    # --- Condition 3: vel_soft_random ---\n",
    "    vel_soft_random_nll = score_soft_condition(soft_prefix_random)\n",
    "\n",
    "    # --- Condition 4: vel_soft_fact ---\n",
    "    vel_soft_fact_nll = score_soft_condition(soft_prefix_fact)\n",
    "\n",
    "    del bare_cache, bare_input\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    eval_results.append({\n",
    "        'query_idx': qidx,\n",
    "        'query': qdata['query'],\n",
    "        'doc_len': doc_len,\n",
    "        'bare_nll': bare_nll,\n",
    "        'vel_static_nll': vel_static_nll,\n",
    "        'vel_soft_random_nll': vel_soft_random_nll,\n",
    "        'vel_soft_fact_nll': vel_soft_fact_nll,\n",
    "    })\n",
    "\n",
    "    # Checkpoint\n",
    "    if (qidx + 1) % CHECKPOINT_EVERY == 0 or qidx == N_EVAL - 1:\n",
    "        ckpt_data = {\n",
    "            'results': eval_results,\n",
    "            'query_texts': [q['query'] for q in eval_samples[:N_EVAL]],\n",
    "            'completed': len(eval_results),\n",
    "            'total': N_EVAL,\n",
    "            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        }\n",
    "        with open(CHECKPOINT_EVAL_PATH, 'w') as f:\n",
    "            json.dump(ckpt_data, f)\n",
    "        elapsed = time.time() - t_start\n",
    "        n_done = qidx - eval_start_idx + 1\n",
    "        rate = n_done / elapsed if elapsed > 0 else 0\n",
    "        remaining = (N_EVAL - qidx - 1) / rate if rate > 0 else 0\n",
    "        tqdm.write(f\"  Checkpoint {qidx+1}/{N_EVAL} | {n_done} done in {elapsed/60:.1f}m | \"\n",
    "                   f\"ETA: {remaining/60:.1f} min\")\n",
    "\n",
    "elapsed_total = time.time() - t_start\n",
    "print(f\"\\nEval complete: {len(eval_results)} queries in {elapsed_total/60:.1f} min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cell-14",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T17:31:45.607790Z",
     "iopub.status.busy": "2026-02-16T17:31:45.607156Z",
     "iopub.status.idle": "2026-02-16T17:31:45.631569Z",
     "shell.execute_reply": "2026-02-16T17:31:45.630824Z"
    },
    "papermill": {
     "duration": 0.044705,
     "end_time": "2026-02-16T17:31:45.633161",
     "exception": false,
     "start_time": "2026-02-16T17:31:45.588456",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "EVALUATION ANALYSIS\n",
      "======================================================================\n",
      "\n",
      "Valid samples: 275/300\n",
      "\n",
      "Condition              Mean NLL     Mean D        d    Win%            p   sig\n",
      "------------------------------------------------------------------------------\n",
      "bare                     0.6982          —        —       —            —     —\n",
      "vel_static               0.6539    +0.0443   +0.195   58.9%     1.38e-03    **\n",
      "vel_soft_random          0.5000    +0.1981   +0.274   62.5%     8.09e-06   ***\n",
      "vel_soft_fact            0.4980    +0.2002   +0.288   64.0%     2.87e-06   ***\n",
      "\n",
      "Pairwise comparisons:\n",
      "  vel_soft_fact vs vel_static: d=+0.273, p=8.83e-06 ***\n",
      "  vel_soft_random vs vel_static: d=+0.245, p=6.17e-05 ***\n",
      "  vel_soft_fact vs vel_soft_random: d=+0.008, p=8.89e-01 ns\n",
      "\n",
      "Reference comparison:\n",
      "  Exp 21 vel_static d: +0.227\n",
      "  This exp vel_static d: +0.195\n",
      "  This exp vel_soft_fact d: +0.288\n",
      "  This exp vel_soft_random d: +0.274\n",
      "\n",
      "Hardness gradient (bare NLL quintiles):\n",
      "Condition                Q1 easy          Q2          Q3          Q4     Q5 hard\n",
      "--------------------------------------------------------------------------------\n",
      "vel_static                -0.335      +0.019      -0.099      +0.368      +0.562\n",
      "vel_soft_random           -0.315      -0.320      +0.119      +0.742      +0.640\n",
      "vel_soft_fact             -0.429      -0.132      +0.009      +0.850      +0.613\n"
     ]
    }
   ],
   "source": [
    "# Cell 14: Evaluation analysis\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"EVALUATION ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "bare_arr = np.array([r['bare_nll'] for r in eval_results])\n",
    "static_arr = np.array([r['vel_static_nll'] for r in eval_results])\n",
    "soft_rand_arr = np.array([r['vel_soft_random_nll'] for r in eval_results])\n",
    "soft_fact_arr = np.array([r['vel_soft_fact_nll'] for r in eval_results])\n",
    "\n",
    "# Filter valid results\n",
    "valid = (\n",
    "    (bare_arr != 0) & np.isfinite(bare_arr) &\n",
    "    (static_arr != 0) & np.isfinite(static_arr) &\n",
    "    (soft_rand_arr != 0) & np.isfinite(soft_rand_arr) &\n",
    "    (soft_fact_arr != 0) & np.isfinite(soft_fact_arr)\n",
    ")\n",
    "\n",
    "b = bare_arr[valid]\n",
    "conditions = {\n",
    "    'vel_static': static_arr[valid],\n",
    "    'vel_soft_random': soft_rand_arr[valid],\n",
    "    'vel_soft_fact': soft_fact_arr[valid],\n",
    "}\n",
    "\n",
    "print(f\"\\nValid samples: {np.sum(valid)}/{len(eval_results)}\")\n",
    "print(f\"\\n{'Condition':<20} {'Mean NLL':>10} {'Mean D':>10} {'d':>8} {'Win%':>7} {'p':>12} {'sig':>5}\")\n",
    "print(\"-\" * 78)\n",
    "print(f\"{'bare':<20} {np.mean(b):>10.4f} {'—':>10} {'—':>8} {'—':>7} {'—':>12} {'—':>5}\")\n",
    "\n",
    "eval_analysis = {}\n",
    "for cname, carr in conditions.items():\n",
    "    delta = b - carr\n",
    "    d = cohens_d(delta)\n",
    "    win = np.mean(delta > 0) * 100\n",
    "    t_stat, p_val = stats.ttest_1samp(delta, 0)\n",
    "    sig = '***' if p_val < 0.001 else '**' if p_val < 0.01 else '*' if p_val < 0.05 else 'ns'\n",
    "    print(f\"{cname:<20} {np.mean(carr):>10.4f} {np.mean(delta):>+10.4f} \"\n",
    "          f\"{d:>+8.3f} {win:>6.1f}% {p_val:>12.2e} {sig:>5}\")\n",
    "    eval_analysis[cname] = {\n",
    "        'n_valid': int(np.sum(valid)),\n",
    "        'mean_nll': float(np.mean(carr)),\n",
    "        'mean_delta': float(np.mean(delta)),\n",
    "        'cohens_d': float(d),\n",
    "        'win_pct': float(win),\n",
    "        't_stat': float(t_stat),\n",
    "        'p_value': float(p_val),\n",
    "    }\n",
    "\n",
    "# Pairwise: soft vs static\n",
    "print(\"\\nPairwise comparisons:\")\n",
    "for n1, a1, n2, a2 in [\n",
    "    ('vel_soft_fact', conditions['vel_soft_fact'], 'vel_static', conditions['vel_static']),\n",
    "    ('vel_soft_random', conditions['vel_soft_random'], 'vel_static', conditions['vel_static']),\n",
    "    ('vel_soft_fact', conditions['vel_soft_fact'], 'vel_soft_random', conditions['vel_soft_random']),\n",
    "]:\n",
    "    delta = a2 - a1  # positive = n1 better (lower NLL)\n",
    "    d = cohens_d(delta)\n",
    "    t_stat, p_val = stats.ttest_1samp(delta, 0)\n",
    "    sig = '***' if p_val < 0.001 else '**' if p_val < 0.01 else '*' if p_val < 0.05 else 'ns'\n",
    "    print(f\"  {n1} vs {n2}: d={d:+.3f}, p={p_val:.2e} {sig}\")\n",
    "\n",
    "# Reference comparison\n",
    "print(f\"\\nReference comparison:\")\n",
    "print(f\"  Exp 21 vel_static d: {EXP21_REF['values_early_layers_d']:+.3f}\")\n",
    "print(f\"  This exp vel_static d: {eval_analysis['vel_static']['cohens_d']:+.3f}\")\n",
    "print(f\"  This exp vel_soft_fact d: {eval_analysis['vel_soft_fact']['cohens_d']:+.3f}\")\n",
    "print(f\"  This exp vel_soft_random d: {eval_analysis['vel_soft_random']['cohens_d']:+.3f}\")\n",
    "\n",
    "# Hardness gradient (quintiles)\n",
    "print(\"\\nHardness gradient (bare NLL quintiles):\")\n",
    "quintile_bounds = np.percentile(b, [20, 40, 60, 80])\n",
    "qlabels = ['Q1 easy', 'Q2', 'Q3', 'Q4', 'Q5 hard']\n",
    "quintiles = np.digitize(b, quintile_bounds)\n",
    "\n",
    "print(f\"{'Condition':<20}\", end='')\n",
    "for ql in qlabels:\n",
    "    print(f\"{ql:>12}\", end='')\n",
    "print()\n",
    "print(\"-\" * (20 + 12 * 5))\n",
    "\n",
    "hardness_data = {}\n",
    "for cname, carr in conditions.items():\n",
    "    delta = b - carr\n",
    "    row = []\n",
    "    print(f\"{cname:<20}\", end='')\n",
    "    for q in range(5):\n",
    "        mask = quintiles == q\n",
    "        if np.sum(mask) < 5:\n",
    "            print(f\"{'n/a':>12}\", end='')\n",
    "            row.append(None)\n",
    "        else:\n",
    "            d_q = cohens_d(delta[mask])\n",
    "            print(f\"{d_q:>+12.3f}\", end='')\n",
    "            row.append(float(d_q))\n",
    "    print()\n",
    "    hardness_data[cname] = row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cell-15",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T17:31:45.669225Z",
     "iopub.status.busy": "2026-02-16T17:31:45.668809Z",
     "iopub.status.idle": "2026-02-16T17:31:47.208071Z",
     "shell.execute_reply": "2026-02-16T17:31:47.207348Z"
    },
    "papermill": {
     "duration": 1.559196,
     "end_time": "2026-02-16T17:31:47.209597",
     "exception": false,
     "start_time": "2026-02-16T17:31:45.650401",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval plots saved to results/exp25/eval_plots.png\n"
     ]
    }
   ],
   "source": [
    "# Cell 15: Evaluation plots (4-panel)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# --- Panel 1: Bar chart of Cohen's d ---\n",
    "ax = axes[0, 0]\n",
    "cond_names = ['vel_static', 'vel_soft_random', 'vel_soft_fact']\n",
    "cond_labels = ['static_fact\\n(discrete)', 'soft_random\\n(random init)', 'soft_fact\\n(fact init)']\n",
    "cond_ds = [eval_analysis[cn]['cohens_d'] for cn in cond_names]\n",
    "cond_colors = ['#2ca02c', '#1f77b4', '#ff7f0e']\n",
    "\n",
    "bars = ax.bar(range(len(cond_names)), cond_ds, color=cond_colors,\n",
    "              edgecolor='black', linewidth=0.5)\n",
    "ax.axhline(y=EXP21_REF['values_early_layers_d'], color='#9467bd', linestyle='--',\n",
    "           linewidth=1.5, label=f\"Exp 21 d={EXP21_REF['values_early_layers_d']:+.3f}\")\n",
    "ax.axhline(y=0, color='gray', linestyle='-', alpha=0.3)\n",
    "\n",
    "ax.set_xticks(range(len(cond_names)))\n",
    "ax.set_xticklabels(cond_labels)\n",
    "ax.set_ylabel(\"Cohen's d vs Bare\")\n",
    "ax.set_title(\"Effect Size by Condition\")\n",
    "ax.legend(fontsize=8)\n",
    "\n",
    "for i, d_val in enumerate(cond_ds):\n",
    "    p_val = eval_analysis[cond_names[i]]['p_value']\n",
    "    sig = '***' if p_val < 0.001 else '**' if p_val < 0.01 else '*' if p_val < 0.05 else 'ns'\n",
    "    ax.text(i, d_val + 0.005 if d_val >= 0 else d_val - 0.015,\n",
    "            f\"{d_val:+.3f} {sig}\", ha='center',\n",
    "            va='bottom' if d_val >= 0 else 'top', fontsize=9)\n",
    "\n",
    "# --- Panel 2: Per-sample scatter (soft_fact vs static) ---\n",
    "ax = axes[0, 1]\n",
    "ax.scatter(conditions['vel_static'], conditions['vel_soft_fact'],\n",
    "           alpha=0.4, s=20, color='#ff7f0e', edgecolors='none')\n",
    "lims = [min(conditions['vel_static'].min(), conditions['vel_soft_fact'].min()),\n",
    "        max(conditions['vel_static'].max(), conditions['vel_soft_fact'].max())]\n",
    "ax.plot(lims, lims, 'k--', alpha=0.5, linewidth=1, label='y=x')\n",
    "ax.set_xlabel('vel_static NLL (discrete)')\n",
    "ax.set_ylabel('vel_soft_fact NLL (learned)')\n",
    "ax.set_title('Per-Sample: Soft Fact vs Static')\n",
    "ax.legend(fontsize=8)\n",
    "\n",
    "# Count wins\n",
    "soft_wins = np.sum(conditions['vel_soft_fact'] < conditions['vel_static'])\n",
    "static_wins = np.sum(conditions['vel_soft_fact'] > conditions['vel_static'])\n",
    "ties = np.sum(conditions['vel_soft_fact'] == conditions['vel_static'])\n",
    "ax.text(0.05, 0.95, f\"Soft wins: {soft_wins}\\nStatic wins: {static_wins}\\nTies: {ties}\",\n",
    "        transform=ax.transAxes, fontsize=9, va='top',\n",
    "        bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "# --- Panel 3: Hardness gradient heatmap ---\n",
    "ax = axes[1, 0]\n",
    "heatmap = np.zeros((len(cond_names), 5))\n",
    "for i, cn in enumerate(cond_names):\n",
    "    for q in range(5):\n",
    "        val = hardness_data[cn][q]\n",
    "        heatmap[i, q] = val if val is not None else np.nan\n",
    "\n",
    "im = ax.imshow(heatmap, cmap='RdBu', aspect='auto', vmin=-0.5, vmax=0.5)\n",
    "ax.set_xticks(range(5))\n",
    "ax.set_xticklabels(qlabels, fontsize=8)\n",
    "ax.set_yticks(range(len(cond_names)))\n",
    "ax.set_yticklabels(['static', 'soft_random', 'soft_fact'])\n",
    "ax.set_xlabel('Difficulty Quintile')\n",
    "ax.set_title(\"Hardness x Condition (Cohen's d)\")\n",
    "\n",
    "for i in range(len(cond_names)):\n",
    "    for j in range(5):\n",
    "        val = heatmap[i, j]\n",
    "        if not np.isnan(val):\n",
    "            ax.text(j, i, f\"{val:+.2f}\", ha='center', va='center',\n",
    "                    fontsize=8, color='white' if abs(val) > 0.25 else 'black')\n",
    "fig.colorbar(im, ax=ax, shrink=0.8)\n",
    "\n",
    "# --- Panel 4: NLL distribution comparison ---\n",
    "ax = axes[1, 1]\n",
    "delta_static = b - conditions['vel_static']\n",
    "delta_soft_fact = b - conditions['vel_soft_fact']\n",
    "delta_soft_random = b - conditions['vel_soft_random']\n",
    "\n",
    "ax.hist(delta_static, bins=40, alpha=0.5, color='#2ca02c', label='static', density=True)\n",
    "ax.hist(delta_soft_fact, bins=40, alpha=0.5, color='#ff7f0e', label='soft_fact', density=True)\n",
    "ax.hist(delta_soft_random, bins=40, alpha=0.3, color='#1f77b4', label='soft_random', density=True)\n",
    "ax.axvline(x=0, color='black', linestyle='-', linewidth=0.5)\n",
    "ax.set_xlabel('NLL Delta (bare - condition, positive = helps)')\n",
    "ax.set_ylabel('Density')\n",
    "ax.set_title('Delta Distribution')\n",
    "ax.legend(fontsize=8)\n",
    "\n",
    "plt.suptitle('Exp 25: Soft Prefix Evaluation', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'eval_plots.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"Eval plots saved to {RESULTS_DIR / 'eval_plots.png'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cell-16",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T17:31:47.246823Z",
     "iopub.status.busy": "2026-02-16T17:31:47.246438Z",
     "iopub.status.idle": "2026-02-16T17:31:47.314827Z",
     "shell.execute_reply": "2026-02-16T17:31:47.314107Z"
    },
    "papermill": {
     "duration": 0.089003,
     "end_time": "2026-02-16T17:31:47.316377",
     "exception": false,
     "start_time": "2026-02-16T17:31:47.227374",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval CSV saved: results/exp25/eval_results.csv\n",
      "\n",
      "Results saved to results/exp25/results.json\n",
      "File size: 737.7 KB\n",
      "\n",
      "======================================================================\n",
      "SUMMARY — Exp 25: Soft Prefix Optimization\n",
      "======================================================================\n",
      "Model: Gemma 3 4B (34 layers, hidden=2560, bfloat16)\n",
      "Soft prefix: 11 vectors x 2560 dims = 28,160 params\n",
      "Training: 2000 samples x 3 epochs, lr=0.1\n",
      "\n",
      "Evaluation (300 queries):\n",
      "  vel_static           d=+0.195  win=59%  **\n",
      "  vel_soft_random      d=+0.274  win=63%  ***\n",
      "  vel_soft_fact        d=+0.288  win=64%  ***\n",
      "\n",
      "VERDICT: Soft fact-init BEATS discrete static_fact (+0.288 vs +0.195). Continuous optimization improves value contamination.\n",
      "  Random-init learned useful signal from scratch (d=+0.274).\n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Cell 16: Save results.json + CSV\n",
    "import csv\n",
    "\n",
    "# --- Eval CSV ---\n",
    "with open(CSV_EVAL_PATH, 'w', newline='') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=[\n",
    "        'query_idx', 'doc_len', 'bare_nll',\n",
    "        'vel_static_nll', 'vel_soft_random_nll', 'vel_soft_fact_nll'])\n",
    "    writer.writeheader()\n",
    "    for r in eval_results:\n",
    "        writer.writerow({\n",
    "            'query_idx': r['query_idx'],\n",
    "            'doc_len': r['doc_len'],\n",
    "            'bare_nll': r['bare_nll'],\n",
    "            'vel_static_nll': r['vel_static_nll'],\n",
    "            'vel_soft_random_nll': r['vel_soft_random_nll'],\n",
    "            'vel_soft_fact_nll': r['vel_soft_fact_nll'],\n",
    "        })\n",
    "print(f\"Eval CSV saved: {CSV_EVAL_PATH}\")\n",
    "\n",
    "# --- Combined results.json ---\n",
    "final = {\n",
    "    'experiment': 'exp25_soft_prefix_optimization',\n",
    "    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'config': {\n",
    "        'model_name': MODEL_NAME,\n",
    "        'model_type': 'gemma3',\n",
    "        'seed': SEED,\n",
    "        'cutoff': CUTOFF,\n",
    "        'prefix_len': PREFIX_LEN,\n",
    "        'hidden_size': HIDDEN_SIZE,\n",
    "        'trainable_params': PREFIX_LEN * HIDDEN_SIZE,\n",
    "        'training': {\n",
    "            'n_samples': N_TRAIN,\n",
    "            'n_epochs': N_EPOCHS,\n",
    "            'lr': LR,\n",
    "            'grad_accum': GRAD_ACCUM_STEPS,\n",
    "            'warmup_steps': WARMUP_STEPS,\n",
    "            'dataset': 'MS MARCO v1.1 train',\n",
    "        },\n",
    "        'eval': {\n",
    "            'n_samples': N_EVAL,\n",
    "            'dataset': 'MS MARCO v1.1 validation',\n",
    "            'conditions': ['bare', 'vel_static', 'vel_soft_random', 'vel_soft_fact'],\n",
    "        },\n",
    "    },\n",
    "    'training_history': {\n",
    "        'random': history_random if 'history_random' in dir() else [],\n",
    "        'fact': history_fact if 'history_fact' in dir() else [],\n",
    "    },\n",
    "    'eval_analysis': eval_analysis,\n",
    "    'eval_hardness': hardness_data,\n",
    "    'reference_values': {\n",
    "        'exp19_gemma': EXP19_REF,\n",
    "        'exp21_gemma': EXP21_REF,\n",
    "    },\n",
    "    'eval_per_query': eval_results,\n",
    "}\n",
    "\n",
    "with open(FINAL_RESULTS_PATH, 'w') as f:\n",
    "    json.dump(final, f, indent=2)\n",
    "\n",
    "print(f\"\\nResults saved to {FINAL_RESULTS_PATH}\")\n",
    "print(f\"File size: {FINAL_RESULTS_PATH.stat().st_size / 1024:.1f} KB\")\n",
    "\n",
    "# Final summary\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"SUMMARY — Exp 25: Soft Prefix Optimization\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Model: Gemma 3 4B ({NUM_LAYERS} layers, hidden={HIDDEN_SIZE}, bfloat16)\")\n",
    "print(f\"Soft prefix: {PREFIX_LEN} vectors x {HIDDEN_SIZE} dims = {PREFIX_LEN * HIDDEN_SIZE:,} params\")\n",
    "print(f\"Training: {N_TRAIN} samples x {N_EPOCHS} epochs, lr={LR}\")\n",
    "\n",
    "print(f\"\\nEvaluation ({N_EVAL} queries):\")\n",
    "for cn in ['vel_static', 'vel_soft_random', 'vel_soft_fact']:\n",
    "    a = eval_analysis[cn]\n",
    "    sig = '***' if a['p_value'] < 0.001 else '**' if a['p_value'] < 0.01 else '*' if a['p_value'] < 0.05 else 'ns'\n",
    "    print(f\"  {cn:<20} d={a['cohens_d']:>+.3f}  win={a['win_pct']:.0f}%  {sig}\")\n",
    "\n",
    "d_static = eval_analysis['vel_static']['cohens_d']\n",
    "d_soft_fact = eval_analysis['vel_soft_fact']['cohens_d']\n",
    "d_soft_random = eval_analysis['vel_soft_random']['cohens_d']\n",
    "\n",
    "if d_soft_fact > d_static + 0.02:\n",
    "    print(f\"\\nVERDICT: Soft fact-init BEATS discrete static_fact \"\n",
    "          f\"({d_soft_fact:+.3f} vs {d_static:+.3f}). \"\n",
    "          f\"Continuous optimization improves value contamination.\")\n",
    "elif d_soft_fact > d_static - 0.02:\n",
    "    print(f\"\\nVERDICT: Soft fact-init MATCHES discrete static_fact \"\n",
    "          f\"({d_soft_fact:+.3f} vs {d_static:+.3f}). \"\n",
    "          f\"Continuous space adds no benefit beyond the discrete prefix.\")\n",
    "else:\n",
    "    print(f\"\\nVERDICT: Soft fact-init WORSE than discrete static_fact \"\n",
    "          f\"({d_soft_fact:+.3f} vs {d_static:+.3f}). \"\n",
    "          f\"Gradient optimization may be disrupting the prefix signal.\")\n",
    "\n",
    "if d_soft_random > 0.10:\n",
    "    print(f\"  Random-init learned useful signal from scratch (d={d_soft_random:+.3f}).\")\n",
    "else:\n",
    "    print(f\"  Random-init did NOT learn useful signal (d={d_soft_random:+.3f}).\")\n",
    "\n",
    "print(f\"\\nDone!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cell-17",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T17:31:47.353963Z",
     "iopub.status.busy": "2026-02-16T17:31:47.353182Z",
     "iopub.status.idle": "2026-02-16T17:31:48.019169Z",
     "shell.execute_reply": "2026-02-16T17:31:48.018421Z"
    },
    "papermill": {
     "duration": 0.686425,
     "end_time": "2026-02-16T17:31:48.020719",
     "exception": false,
     "start_time": "2026-02-16T17:31:47.334294",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning up GPU memory...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory: 4.60 GB -> 2.70 GB\n",
      "Cleanup complete.\n"
     ]
    }
   ],
   "source": [
    "# Cell 17: GPU cleanup\n",
    "import gc\n",
    "\n",
    "print(\"Cleaning up GPU memory...\")\n",
    "mem_before = torch.cuda.memory_allocated() / 1e9\n",
    "\n",
    "del model\n",
    "del tokenizer\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "mem_after = torch.cuda.memory_allocated() / 1e9\n",
    "print(f\"GPU memory: {mem_before:.2f} GB -> {mem_after:.2f} GB\")\n",
    "print(\"Cleanup complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 12545.566029,
   "end_time": "2026-02-16T17:31:51.570438",
   "environment_variables": {},
   "exception": null,
   "input_path": "25_soft_prefix_optimization.ipynb",
   "output_path": "25_soft_prefix_optimization_executed.ipynb",
   "parameters": {},
   "start_time": "2026-02-16T14:02:46.004409",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "004c973298c7473db5bdddaaf9de0126": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "0821ebaf1b164012a8d3eb5b2506d394": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "0ce82c813f2040019cc45e2581529841": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "danger",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_db4ed2b1a3a943a1b4cc3a51264b3a71",
       "max": 10047.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_004c973298c7473db5bdddaaf9de0126",
       "tabbable": null,
       "tooltip": null,
       "value": 925.0
      }
     },
     "0e297b672c11470e818cd03586b47d35": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "12e4d29be044492f8fba58b4b6ca9c58": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "265dcc8fd51646dead5aceb4e891656f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "3d212dc112ea41e1bc194fdc12338deb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_46cf87222fc84c1a975139bdfc13a6fb",
       "placeholder": "​",
       "style": "IPY_MODEL_da0e3e6fbb1c468c81f8bff6a54ac3f5",
       "tabbable": null,
       "tooltip": null,
       "value": " 883/883 [00:03&lt;00:00, 852.59it/s, Materializing param=model.vision_tower.vision_model.post_layernorm.weight]"
      }
     },
     "43e4886de2dc4d36ada6ea7c97f5bd05": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "46cf87222fc84c1a975139bdfc13a6fb": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "479ff4183e5348b3bae86a25e5008c59": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "4c34f82f0f2143ad8cf41601e1f9ded2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "4f78da06faf743b1baa9985fa491114f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "510aa0c9a2824ea5bc4aed3e923a3bce": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_c31fb5a305354351acfaa581b8c6ed9a",
       "placeholder": "​",
       "style": "IPY_MODEL_f4c8b6c384664234987312f25f5991fd",
       "tabbable": null,
       "tooltip": null,
       "value": "Filtering train:   8%"
      }
     },
     "513bccea2c064e9c8e498f6be626eb52": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "5b7486c5c7324771ad70c00ffbf78496": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "danger",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_0e297b672c11470e818cd03586b47d35",
       "max": 82326.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_4c34f82f0f2143ad8cf41601e1f9ded2",
       "tabbable": null,
       "tooltip": null,
       "value": 6205.0
      }
     },
     "5ea6fc2237874be0b40e620b94ed6484": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_510aa0c9a2824ea5bc4aed3e923a3bce",
        "IPY_MODEL_5b7486c5c7324771ad70c00ffbf78496",
        "IPY_MODEL_9d0514ea3fbe452bb3eebcda1d796614"
       ],
       "layout": "IPY_MODEL_fc3e1360944a4a178d506bee5d10005d",
       "tabbable": null,
       "tooltip": null
      }
     },
     "729e351ae36c45f99fece7d2fd30b9d5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_ecf8c71bb4ab48ba82db8fcc37ddacd7",
       "max": 883.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_e6778b7417eb4ac3acc521bc74d0c1f5",
       "tabbable": null,
       "tooltip": null,
       "value": 883.0
      }
     },
     "7571c0dfcee742d9a3602119ff635d31": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "78ce434d19374d5ca800222bd698d46a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "7ad8b0d88da84bb2ae4a5b78656a0133": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_7571c0dfcee742d9a3602119ff635d31",
       "placeholder": "​",
       "style": "IPY_MODEL_ee0df397f25c407997dc7b79d2acf8db",
       "tabbable": null,
       "tooltip": null,
       "value": "Loading weights: 100%"
      }
     },
     "808f34f5b6164be7ab86f1fff629f9f9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_e717d2cfcd7f4c129fae53b13ed8d3b5",
        "IPY_MODEL_f3b5e5f25e4a4fda8a2763b6b8685cb0",
        "IPY_MODEL_d8c00a20fee64a91b538d7e2167afe70"
       ],
       "layout": "IPY_MODEL_12e4d29be044492f8fba58b4b6ca9c58",
       "tabbable": null,
       "tooltip": null
      }
     },
     "8a01f0ba52bf411594679aee7954fd54": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "9aec4787920e4afd8b08d0cc5cca7960": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "9d0514ea3fbe452bb3eebcda1d796614": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_513bccea2c064e9c8e498f6be626eb52",
       "placeholder": "​",
       "style": "IPY_MODEL_e5f2bb4534064b10bf7554f317e57cd6",
       "tabbable": null,
       "tooltip": null,
       "value": " 6205/82326 [00:00&lt;00:08, 9362.78it/s]"
      }
     },
     "a24c31bd179542b186566451b687f9ff": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a58260b8d69441a394e3a784c423df25": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_a24c31bd179542b186566451b687f9ff",
       "placeholder": "​",
       "style": "IPY_MODEL_479ff4183e5348b3bae86a25e5008c59",
       "tabbable": null,
       "tooltip": null,
       "value": "Filtering validation:   9%"
      }
     },
     "b1a02bb1b3b640f0855b053ad35d0214": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "bce2ede5be7e40a090cf3fcc938ce32c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c2d25853bd5d423484008c747262d4bb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_bce2ede5be7e40a090cf3fcc938ce32c",
       "placeholder": "​",
       "style": "IPY_MODEL_b1a02bb1b3b640f0855b053ad35d0214",
       "tabbable": null,
       "tooltip": null,
       "value": " 925/10047 [00:00&lt;00:01, 8418.22it/s]"
      }
     },
     "c31fb5a305354351acfaa581b8c6ed9a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d8c00a20fee64a91b538d7e2167afe70": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_9aec4787920e4afd8b08d0cc5cca7960",
       "placeholder": "​",
       "style": "IPY_MODEL_4f78da06faf743b1baa9985fa491114f",
       "tabbable": null,
       "tooltip": null,
       "value": " 300/300 [10:32&lt;00:00,  2.09s/it]"
      }
     },
     "da0e3e6fbb1c468c81f8bff6a54ac3f5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "db4ed2b1a3a943a1b4cc3a51264b3a71": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e5f2bb4534064b10bf7554f317e57cd6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "e6778b7417eb4ac3acc521bc74d0c1f5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "e6d1cd7809ce420a8012eb423d7f8fee": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_a58260b8d69441a394e3a784c423df25",
        "IPY_MODEL_0ce82c813f2040019cc45e2581529841",
        "IPY_MODEL_c2d25853bd5d423484008c747262d4bb"
       ],
       "layout": "IPY_MODEL_0821ebaf1b164012a8d3eb5b2506d394",
       "tabbable": null,
       "tooltip": null
      }
     },
     "e717d2cfcd7f4c129fae53b13ed8d3b5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_8a01f0ba52bf411594679aee7954fd54",
       "placeholder": "​",
       "style": "IPY_MODEL_78ce434d19374d5ca800222bd698d46a",
       "tabbable": null,
       "tooltip": null,
       "value": "Eval: 100%"
      }
     },
     "ecf8c71bb4ab48ba82db8fcc37ddacd7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ee0df397f25c407997dc7b79d2acf8db": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "f1e3d473abf042d0baddcde1dee16c95": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_7ad8b0d88da84bb2ae4a5b78656a0133",
        "IPY_MODEL_729e351ae36c45f99fece7d2fd30b9d5",
        "IPY_MODEL_3d212dc112ea41e1bc194fdc12338deb"
       ],
       "layout": "IPY_MODEL_43e4886de2dc4d36ada6ea7c97f5bd05",
       "tabbable": null,
       "tooltip": null
      }
     },
     "f3b5e5f25e4a4fda8a2763b6b8685cb0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_f3f29e36192346afa5c6b4630d6fd70c",
       "max": 300.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_265dcc8fd51646dead5aceb4e891656f",
       "tabbable": null,
       "tooltip": null,
       "value": 300.0
      }
     },
     "f3f29e36192346afa5c6b4630d6fd70c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f4c8b6c384664234987312f25f5991fd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "fc3e1360944a4a178d506bee5d10005d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}