{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51da059e",
   "metadata": {},
   "source": [
    "# Experiment 33b: Corrected Surrogate Transfer Test## Query in encoder ONLY -- encoder is the sole source of context### Why Exp 33 was wrongIn Exp 33, the decoder target was \"[query] Answer: [answer]\". The decoderalready had the query via cross-attention to its own input tokens. Addingthe query to the encoder was redundant -- the decoder didn't NEED the encoderto know about the query.### The ad-serving scenario- **Offline**: Pre-compute encoder representations for each ad/document- **Online**: User query arrives. We need the encoder representation to CARRY  the context for answering. The decoder generates an answer based solely on  what the encoder provides.### Corrected setup- **Encoder input**: varies by condition (document ± query/surrogate)- **Decoder target**: just \"[answer]\" (NO query in decoder)- The decoder's ONLY source of information is the encoder output- Therefore, query-aware encoding SHOULD beat bare encoding### Conditions1. **bare**: encoder(\"[document]\") → decoder NLL(\"[answer]\")2. **oracle**: encoder(\"[query]\\n[document]\") → decoder NLL(\"[answer]\")3. **static**: encoder(\"What are the key facts?\\n[document]\") → decoder NLL(\"[answer]\")4. **surr_para**: encoder(\"[paraphrased_query]\\n[document]\") → decoder NLL(\"[answer]\")5. **surr_doc**: encoder(\"[doc_keywords]\\n[document]\") → decoder NLL(\"[answer]\")### Expected hierarchyoracle > surr_para > surr_doc ≈ static > bareIf surrogate captures >30% of the oracle-bare gap, the core idea works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85d47ba8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-17T20:06:31.343539Z",
     "iopub.status.busy": "2026-02-17T20:06:31.343041Z",
     "iopub.status.idle": "2026-02-17T20:06:35.897181Z",
     "shell.execute_reply": "2026-02-17T20:06:35.896273Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment 33b: Corrected Surrogate Transfer\n",
      "Model: google/t5gemma-2-4b-4b\n",
      "N: 200\n",
      "CUDA: NVIDIA A100-SXM4-40GB\n",
      "GPU memory: 42.3 GB\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Setup\n",
    "import os\n",
    "os.umask(0o000)\n",
    "\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "import gc\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from scipy import stats\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "sys.path.insert(0, \".\")\n",
    "\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "RESULTS_DIR = Path(\"results/exp33b\")\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CHECKPOINT_PATH = RESULTS_DIR / \"checkpoint.json\"\n",
    "\n",
    "N_SAMPLES = 200\n",
    "MODEL_NAME = \"google/t5gemma-2-4b-4b\"\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "HF_TOKEN = os.environ.get(\"HF_TOKEN\")\n",
    "\n",
    "print(f\"Experiment 33b: Corrected Surrogate Transfer\")\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"N: {N_SAMPLES}\")\n",
    "print(f\"CUDA: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26dbe78f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-17T20:06:35.900811Z",
     "iopub.status.busy": "2026-02-17T20:06:35.900387Z",
     "iopub.status.idle": "2026-02-17T20:06:53.013583Z",
     "shell.execute_reply": "2026-02-17T20:06:53.012504Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading google/t5gemma-2-4b-4b...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13161b653a264fd095e892205bb0d03a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/1327 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded. dtype=torch.bfloat16\n",
      "GPU memory used: 15.02 GB\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Load model\n",
    "from transformers import AutoProcessor, AutoModelForSeq2SeqLM\n",
    "\n",
    "print(f\"Loading {MODEL_NAME}...\")\n",
    "processor = AutoProcessor.from_pretrained(MODEL_NAME, token=HF_TOKEN)\n",
    "tokenizer = processor.tokenizer\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    token=HF_TOKEN,\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "DEVICE = next(model.parameters()).device\n",
    "print(f\"Model loaded. dtype={next(model.parameters()).dtype}\")\n",
    "print(f\"GPU memory used: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92cf5dba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-17T20:06:53.017147Z",
     "iopub.status.busy": "2026-02-17T20:06:53.016721Z",
     "iopub.status.idle": "2026-02-17T20:06:53.030014Z",
     "shell.execute_reply": "2026-02-17T20:06:53.029298Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helpers defined.\n",
      "CRITICAL DIFFERENCE from Exp 33:\n",
      "  score_answer_nll(encoder_text, answer_text)\n",
      "  Decoder sees ONLY the answer -- NO query in decoder\n",
      "  Encoder representation is the SOLE source of context\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Scoring helper -- CORRECTED: answer-only in decoder\n",
    "\n",
    "def score_answer_nll(encoder_text, answer_text):\n",
    "    '''Score NLL of answer tokens.\n",
    "\n",
    "    Encoder: encoder_text (contains document, optionally query/surrogate)\n",
    "    Decoder: answer_text ONLY (no query -- encoder is sole context source)\n",
    "\n",
    "    This is the corrected version: the decoder's ONLY information about\n",
    "    what to answer comes from the encoder representation.\n",
    "    '''\n",
    "    enc_ids = tokenizer(encoder_text, return_tensors=\"pt\",\n",
    "                        add_special_tokens=True, truncation=True,\n",
    "                        max_length=2048).input_ids.to(DEVICE)\n",
    "    enc_mask = torch.ones_like(enc_ids)\n",
    "\n",
    "    # Decoder target: just the answer\n",
    "    ans_ids = tokenizer(answer_text, return_tensors=\"pt\",\n",
    "                        add_special_tokens=False, truncation=True,\n",
    "                        max_length=256).input_ids.to(DEVICE)\n",
    "\n",
    "    if ans_ids.shape[1] == 0:\n",
    "        return 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        encoder_outputs = model.get_encoder()(\n",
    "            input_ids=enc_ids, attention_mask=enc_mask\n",
    "        )\n",
    "        outputs = model(\n",
    "            encoder_outputs=encoder_outputs,\n",
    "            attention_mask=enc_mask,\n",
    "            labels=ans_ids,\n",
    "        )\n",
    "\n",
    "    # Per-token NLL over answer\n",
    "    logits = outputs.logits\n",
    "    log_probs = F.log_softmax(logits, dim=-1)\n",
    "    token_log_probs = log_probs[0].gather(1, ans_ids[0].unsqueeze(1)).squeeze(1)\n",
    "    mean_nll = -token_log_probs.mean().item()\n",
    "\n",
    "    del encoder_outputs, outputs, logits, log_probs\n",
    "    return mean_nll\n",
    "\n",
    "\n",
    "# === Surrogate query generation ===\n",
    "STOP_WORDS = {\n",
    "    'a', 'an', 'the', 'is', 'are', 'was', 'were', 'be', 'been', 'being',\n",
    "    'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could',\n",
    "    'should', 'may', 'might', 'can', 'shall', 'to', 'of', 'in', 'for',\n",
    "    'on', 'with', 'at', 'by', 'from', 'as', 'into', 'through', 'during',\n",
    "    'before', 'after', 'above', 'below', 'between', 'and', 'but', 'or',\n",
    "    'not', 'no', 'if', 'then', 'than', 'so', 'up', 'out', 'about',\n",
    "    'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those',\n",
    "    'it', 'its', 'i', 'me', 'my', 'we', 'our', 'you', 'your', 'he',\n",
    "    'him', 'his', 'she', 'her', 'they', 'them', 'their', 'how', 'when',\n",
    "    'where', 'why', 'much', 'many', 'some', 'any', 'all', 'each',\n",
    "    'does', 'also', 'just', 'more', 'most', 'very', 'too', 'only',\n",
    "}\n",
    "\n",
    "def extract_keywords(text):\n",
    "    words = re.sub(r'[^\\w\\s]', '', text.lower()).split()\n",
    "    return [w for w in words if w not in STOP_WORDS and len(w) > 2]\n",
    "\n",
    "def make_surrogate_paraphrase(query):\n",
    "    '''Paraphrase: reverse keyword order.'''\n",
    "    keywords = extract_keywords(query)\n",
    "    return \" \".join(keywords[::-1]) if keywords else query\n",
    "\n",
    "def make_surrogate_from_doc(passage):\n",
    "    '''Extract top-5 keywords from document.'''\n",
    "    content_words = extract_keywords(passage)\n",
    "    if not content_words:\n",
    "        return \"information\"\n",
    "    counts = Counter(content_words)\n",
    "    return \" \".join(w for w, _ in counts.most_common(5))\n",
    "\n",
    "STATIC_PREFIX = \"What are the key facts?\"\n",
    "\n",
    "print(\"Helpers defined.\")\n",
    "print(\"CRITICAL DIFFERENCE from Exp 33:\")\n",
    "print(\"  score_answer_nll(encoder_text, answer_text)\")\n",
    "print(\"  Decoder sees ONLY the answer -- NO query in decoder\")\n",
    "print(\"  Encoder representation is the SOLE source of context\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f9bf231",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-17T20:06:53.032967Z",
     "iopub.status.busy": "2026-02-17T20:06:53.032439Z",
     "iopub.status.idle": "2026-02-17T20:06:54.570124Z",
     "shell.execute_reply": "2026-02-17T20:06:54.569217Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MS MARCO v1.1 validation...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 200 samples\n",
      "Word counts: mean=74\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Load data\n",
    "from lib.data import count_words\n",
    "from datasets import load_dataset\n",
    "\n",
    "print(\"Loading MS MARCO v1.1 validation...\")\n",
    "ds = load_dataset(\"microsoft/ms_marco\", \"v1.1\", split=\"validation\")\n",
    "\n",
    "samples = []\n",
    "for item in ds:\n",
    "    if len(samples) >= N_SAMPLES * 3:\n",
    "        break\n",
    "    passages = item.get('passages', {})\n",
    "    ptexts = passages.get('passage_text', [])\n",
    "    is_sel = passages.get('is_selected', [])\n",
    "    query = item.get('query', '')\n",
    "    answers = item.get('answers', [])\n",
    "    well_formed = item.get('wellFormedAnswers', [])\n",
    "\n",
    "    answer = None\n",
    "    if well_formed and len(well_formed) > 0 and well_formed[0] not in ('[]', ''):\n",
    "        answer = well_formed[0]\n",
    "    elif answers and len(answers) > 0 and answers[0] != 'No Answer Present.':\n",
    "        answer = answers[0]\n",
    "    if not answer:\n",
    "        continue\n",
    "\n",
    "    for pt, sel in zip(ptexts, is_sel):\n",
    "        wc = count_words(pt)\n",
    "        if sel == 1 and 30 <= wc <= 300:\n",
    "            samples.append({\n",
    "                'passage': pt, 'query': query, 'answer': answer,\n",
    "                'word_count': wc\n",
    "            })\n",
    "            break\n",
    "\n",
    "np.random.seed(SEED)\n",
    "np.random.shuffle(samples)\n",
    "samples = samples[:N_SAMPLES]\n",
    "del ds\n",
    "gc.collect()\n",
    "\n",
    "for s in samples:\n",
    "    s['surrogate_para'] = make_surrogate_paraphrase(s['query'])\n",
    "    s['surrogate_doc_kw'] = make_surrogate_from_doc(s['passage'])\n",
    "\n",
    "print(f\"Selected {len(samples)} samples\")\n",
    "print(f\"Word counts: mean={np.mean([s['word_count'] for s in samples]):.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99c2e25d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-17T20:06:54.573585Z",
     "iopub.status.busy": "2026-02-17T20:06:54.573138Z",
     "iopub.status.idle": "2026-02-17T20:06:54.584722Z",
     "shell.execute_reply": "2026-02-17T20:06:54.584019Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "EXPERIMENTAL CONDITIONS -- CORRECTED\n",
      "======================================================================\n",
      "\n",
      "Example query:  when did the triceratops appear in the fossil record\n",
      "Example answer: They first appeared during the late Maastrichtian stage of the late Cr\n",
      "\n",
      "### bare ###\n",
      "  Encoder (155 tok): Triceratops is a genus of herbivorous ceratopsid dinosaur that first appeared during the late Maastr...\n",
      "\n",
      "### oracle ###\n",
      "  Encoder (168 tok): when did the triceratops appear in the fossil record\n",
      "Triceratops is a genus of herbivorous ceratopsi...\n",
      "\n",
      "### static ###\n",
      "  Encoder (162 tok): What are the key facts?\n",
      "Triceratops is a genus of herbivorous ceratopsid dinosaur that first appeare...\n",
      "\n",
      "### surr_para ###\n",
      "  Encoder (163 tok): record fossil appear triceratops\n",
      "Triceratops is a genus of herbivorous ceratopsid dinosaur that firs...\n",
      "\n",
      "### surr_doc ###\n",
      "  Encoder (167 tok): triceratops late genus herbivorous ceratopsid\n",
      "Triceratops is a genus of herbivorous ceratopsid dinos...\n",
      "\n",
      "### Decoder (same for ALL conditions) ###\n",
      "  Target: 'They first appeared during the late Maastrichtian stage of the late Cretaceous p...'\n",
      "  NO query in decoder -- encoder is the ONLY context source!\n",
      "  This means:\n",
      "    - bare:   decoder must produce answer knowing only the document\n",
      "    - oracle: decoder knows what question to answer (via encoder)\n",
      "    - surr:   decoder has approximate question info (via encoder)\n",
      "\n",
      "--- Surrogate examples ---\n",
      "\n",
      "  Real query: when did the triceratops appear in the fossil record\n",
      "  Paraphrase: record fossil appear triceratops\n",
      "  Doc KW:     triceratops late genus herbivorous ceratopsid\n",
      "\n",
      "  Real query: kilaya name meaning\n",
      "  Paraphrase: meaning name kilaya\n",
      "  Doc KW:     vajrakilaya daggerspike point phurba power\n",
      "\n",
      "  Real query: what biome has plants spaced far apart\n",
      "  Paraphrase: apart far spaced plants biome\n",
      "  Doc KW:     plants desert plant developed horizontal\n",
      "\n",
      "  Real query: what does blue colour signify\n",
      "  Paraphrase: signify colour blue\n",
      "  Doc KW:     blue represents color body helps\n",
      "\n",
      "  Real query: does poppy seed mean mustard seeds\n",
      "  Paraphrase: seeds mustard mean seed poppy\n",
      "  Doc KW:     poppy seed seeds opium ground\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Explain experimental conditions\n",
    "print(\"=\" * 70)\n",
    "print(\"EXPERIMENTAL CONDITIONS -- CORRECTED\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "CONDITIONS = {\n",
    "    'bare':      lambda s: s['passage'],\n",
    "    'oracle':    lambda s: s['query'] + \"\\n\" + s['passage'],\n",
    "    'static':    lambda s: STATIC_PREFIX + \"\\n\" + s['passage'],\n",
    "    'surr_para': lambda s: s['surrogate_para'] + \"\\n\" + s['passage'],\n",
    "    'surr_doc':  lambda s: s['surrogate_doc_kw'] + \"\\n\" + s['passage'],\n",
    "}\n",
    "\n",
    "ex = samples[0]\n",
    "print(f\"\\nExample query:  {ex['query'][:70]}\")\n",
    "print(f\"Example answer: {ex['answer'][:70]}\")\n",
    "\n",
    "for name, fn in CONDITIONS.items():\n",
    "    enc_input = fn(ex)\n",
    "    n_tokens = len(tokenizer(enc_input, add_special_tokens=True).input_ids)\n",
    "    print(f\"\\n### {name} ###\")\n",
    "    print(f\"  Encoder ({n_tokens} tok): {enc_input[:100]}...\")\n",
    "\n",
    "print(f\"\\n### Decoder (same for ALL conditions) ###\")\n",
    "print(f\"  Target: '{ex['answer'][:80]}...'\")\n",
    "print(f\"  NO query in decoder -- encoder is the ONLY context source!\")\n",
    "print(f\"  This means:\")\n",
    "print(f\"    - bare:   decoder must produce answer knowing only the document\")\n",
    "print(f\"    - oracle: decoder knows what question to answer (via encoder)\")\n",
    "print(f\"    - surr:   decoder has approximate question info (via encoder)\")\n",
    "\n",
    "print(f\"\\n--- Surrogate examples ---\")\n",
    "for i in range(5):\n",
    "    s = samples[i]\n",
    "    print(f\"\\n  Real query: {s['query'][:55]}\")\n",
    "    print(f\"  Paraphrase: {s['surrogate_para'][:55]}\")\n",
    "    print(f\"  Doc KW:     {s['surrogate_doc_kw'][:55]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0fcccd0b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-17T20:06:54.587927Z",
     "iopub.status.busy": "2026-02-17T20:06:54.587406Z",
     "iopub.status.idle": "2026-02-17T20:10:44.738537Z",
     "shell.execute_reply": "2026-02-17T20:10:44.737831Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "RUNNING EXPERIMENT\n",
      "======================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf4aca452ee54910b622abc4665ddfa6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Scoring:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 20/200 | 0.4m | ETA 3.5m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 40/200 | 0.8m | ETA 3.1m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 60/200 | 1.1m | ETA 2.7m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 80/200 | 1.5m | ETA 2.3m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 100/200 | 1.9m | ETA 1.9m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 120/200 | 2.3m | ETA 1.5m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 140/200 | 2.7m | ETA 1.1m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 160/200 | 3.1m | ETA 0.8m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 180/200 | 3.5m | ETA 0.4m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 200/200 | 3.8m | ETA 0.0m\n",
      "\n",
      "Scoring complete: 200 samples in 3.8 min\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Run scoring\n",
    "print(\"=\" * 70)\n",
    "print(\"RUNNING EXPERIMENT\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "cond_names = list(CONDITIONS.keys())\n",
    "\n",
    "all_results = []\n",
    "start_idx = 0\n",
    "\n",
    "if CHECKPOINT_PATH.exists():\n",
    "    ckpt = json.loads(CHECKPOINT_PATH.read_text())\n",
    "    if ckpt.get('n_total') == N_SAMPLES and len(ckpt.get('results', [])) > 0:\n",
    "        saved_queries = [r['query'][:50] for r in ckpt['results']]\n",
    "        current_queries = [s['query'][:50] for s in samples[:len(saved_queries)]]\n",
    "        if saved_queries == current_queries:\n",
    "            all_results = ckpt['results']\n",
    "            start_idx = len(all_results)\n",
    "            print(f\"Resuming from checkpoint: {start_idx}/{N_SAMPLES}\")\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "for i in tqdm(range(start_idx, N_SAMPLES), initial=start_idx, total=N_SAMPLES, desc=\"Scoring\"):\n",
    "    s = samples[i]\n",
    "\n",
    "    result = {\n",
    "        'query': s['query'],\n",
    "        'answer': s['answer'],\n",
    "        'passage_words': s['word_count'],\n",
    "        'surrogate_para': s['surrogate_para'],\n",
    "        'surrogate_doc_kw': s['surrogate_doc_kw'],\n",
    "    }\n",
    "\n",
    "    for cond_name, cond_fn in CONDITIONS.items():\n",
    "        encoder_text = cond_fn(s)\n",
    "        # CORRECTED: decoder gets answer ONLY, no query\n",
    "        nll = score_answer_nll(encoder_text, s['answer'])\n",
    "        result[f'nll_{cond_name}'] = nll\n",
    "\n",
    "    all_results.append(result)\n",
    "\n",
    "    if (i + 1) % 20 == 0 or i == N_SAMPLES - 1:\n",
    "        ckpt = {\n",
    "            'n_total': N_SAMPLES, 'results': all_results,\n",
    "            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        }\n",
    "        CHECKPOINT_PATH.write_text(json.dumps(ckpt))\n",
    "        elapsed = time.time() - t0\n",
    "        done = i - start_idx + 1\n",
    "        eta = (N_SAMPLES - i - 1) * elapsed / done if done > 0 else 0\n",
    "        tqdm.write(f\"  Checkpoint {i+1}/{N_SAMPLES} | {elapsed/60:.1f}m | ETA {eta/60:.1f}m\")\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "elapsed = time.time() - t0\n",
    "print(f\"\\nScoring complete: {len(all_results)} samples in {elapsed/60:.1f} min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61acacfc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-17T20:10:44.742100Z",
     "iopub.status.busy": "2026-02-17T20:10:44.741489Z",
     "iopub.status.idle": "2026-02-17T20:10:44.759494Z",
     "shell.execute_reply": "2026-02-17T20:10:44.758876Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "RESULTS (N=200)\n",
      "======================================================================\n",
      "\n",
      "Condition         Mean NLL    vs Bare        d     Win%            p   sig\n",
      "-------------------------------------------------------------------------\n",
      "bare                3.7188         --       --       --           --    --\n",
      "oracle              3.4306    +0.2882   +0.345    81.5%     2.14e-06   ***\n",
      "static              3.6916    +0.0273   +0.103    56.5%     1.47e-01    ns\n",
      "surr_para           3.5175    +0.2013   +0.293    72.0%     5.06e-05   ***\n",
      "surr_doc            3.4435    +0.2753   +0.312    79.5%     1.64e-05   ***\n",
      "\n",
      "--- Pairwise Cohen's d (row better than column = positive) ---\n",
      "                        bare       oracle       static    surr_para     surr_doc\n",
      "bare                      --       -0.345       -0.103       -0.293       -0.312\n",
      "oracle                +0.345           --       +0.314       +0.275       +0.031\n",
      "static                +0.103       -0.314           --       -0.254       -0.272\n",
      "surr_para             +0.293       -0.275       +0.254           --       -0.155\n",
      "surr_doc              +0.312       -0.031       +0.272       +0.155           --\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Results\n",
    "from lib.analysis import cohens_d\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"RESULTS (N={len(all_results)})\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "bare_nlls = np.array([r['nll_bare'] for r in all_results])\n",
    "\n",
    "print(f\"\\n{'Condition':<15} {'Mean NLL':>10} {'vs Bare':>10} {'d':>8} {'Win%':>8} {'p':>12} {'sig':>5}\")\n",
    "print(\"-\" * 73)\n",
    "\n",
    "analysis = {}\n",
    "for cond in cond_names:\n",
    "    nlls = np.array([r[f'nll_{cond}'] for r in all_results])\n",
    "    mean_nll = nlls.mean()\n",
    "    diff = bare_nlls - nlls  # positive = condition better (lower NLL)\n",
    "    d = cohens_d(diff)\n",
    "    win_pct = 100 * np.mean(diff > 0)\n",
    "\n",
    "    if cond == 'bare':\n",
    "        print(f\"{cond:<15} {mean_nll:>10.4f} {'--':>10} {'--':>8} {'--':>8} {'--':>12} {'--':>5}\")\n",
    "        analysis[cond] = {'mean_nll': float(mean_nll)}\n",
    "    else:\n",
    "        t_stat, p_val = stats.ttest_1samp(diff, 0)\n",
    "        sig = '***' if p_val < 0.001 else '**' if p_val < 0.01 else '*' if p_val < 0.05 else 'ns'\n",
    "        print(f\"{cond:<15} {mean_nll:>10.4f} {diff.mean():>+10.4f} {d:>+8.3f} {win_pct:>7.1f}% {p_val:>12.2e} {sig:>5}\")\n",
    "        analysis[cond] = {\n",
    "            'mean_nll': float(mean_nll), 'delta_vs_bare': float(diff.mean()),\n",
    "            'cohens_d': float(d), 'win_pct': float(win_pct), 'p_value': float(p_val),\n",
    "        }\n",
    "\n",
    "# Pairwise\n",
    "print(f\"\\n--- Pairwise Cohen's d (row better than column = positive) ---\")\n",
    "print(f\"{'':>15}\", end='')\n",
    "for c in cond_names:\n",
    "    print(f\" {c:>12}\", end='')\n",
    "print()\n",
    "for c1 in cond_names:\n",
    "    nlls1 = np.array([r[f'nll_{c1}'] for r in all_results])\n",
    "    print(f\"{c1:<15}\", end='')\n",
    "    for c2 in cond_names:\n",
    "        if c1 == c2:\n",
    "            print(f\" {'--':>12}\", end='')\n",
    "        else:\n",
    "            nlls2 = np.array([r[f'nll_{c2}'] for r in all_results])\n",
    "            diff = nlls2 - nlls1\n",
    "            d = cohens_d(diff)\n",
    "            print(f\" {d:>+12.3f}\", end='')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7819b08",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-17T20:10:44.762469Z",
     "iopub.status.busy": "2026-02-17T20:10:44.761940Z",
     "iopub.status.idle": "2026-02-17T20:10:44.776749Z",
     "shell.execute_reply": "2026-02-17T20:10:44.776128Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "TRANSFER ANALYSIS\n",
      "======================================================================\n",
      "\n",
      "Oracle-bare NLL gap: +0.2882\n",
      "  (positive = oracle better, this is the 'prize' to capture)\n",
      "\n",
      "  static:\n",
      "    Gap captured: +0.0273 (9% of oracle gap)\n",
      "\n",
      "  surr_para:\n",
      "    Gap captured: +0.2013 (70% of oracle gap)\n",
      "\n",
      "  surr_doc:\n",
      "    Gap captured: +0.2753 (96% of oracle gap)\n",
      "\n",
      "--- Per-sample correlations ---\n",
      "  oracle vs surr_para: r=0.932 (p=4.77e-89)\n",
      "  oracle vs surr_doc:  r=0.882 (p=1.13e-66)\n",
      "  surr_para vs surr_doc: r=0.844 (p=2.11e-55)\n",
      "\n",
      "--- Hardness gradient (by bare NLL quintile) ---\n",
      "Quintile        N       bare     oracle  surr_para   surr_doc   orc-bare    sp-bare\n",
      "------------------------------------------------------------------------------\n",
      "Q1 easy        40     0.5338     0.4804     0.5028     0.4990    +0.0534    +0.0310\n",
      "Q2             40     0.9906     0.9307     0.9588     0.9361    +0.0600    +0.0318\n",
      "Q3             40     1.7232     1.6213     1.6656     1.6537    +0.1020    +0.0576\n",
      "Q4             40     3.0816     2.9090     2.8771     2.8564    +0.1727    +0.2045\n",
      "Q5 hard        40    12.2648    11.2117    11.5832    11.2723    +1.0531    +0.6816\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Transfer analysis\n",
    "print(\"=\" * 70)\n",
    "print(\"TRANSFER ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "oracle_nlls = np.array([r['nll_oracle'] for r in all_results])\n",
    "surr_para_nlls = np.array([r['nll_surr_para'] for r in all_results])\n",
    "surr_doc_nlls = np.array([r['nll_surr_doc'] for r in all_results])\n",
    "static_nlls = np.array([r['nll_static'] for r in all_results])\n",
    "\n",
    "# Oracle-bare gap = the total benefit of query awareness\n",
    "oracle_gap = bare_nlls.mean() - oracle_nlls.mean()\n",
    "print(f\"\\nOracle-bare NLL gap: {oracle_gap:+.4f}\")\n",
    "print(f\"  (positive = oracle better, this is the 'prize' to capture)\")\n",
    "\n",
    "# How much of the gap does each surrogate capture?\n",
    "for name, nlls in [('static', static_nlls), ('surr_para', surr_para_nlls),\n",
    "                    ('surr_doc', surr_doc_nlls)]:\n",
    "    gap = bare_nlls.mean() - nlls.mean()\n",
    "    if oracle_gap > 0:\n",
    "        ratio = gap / oracle_gap * 100\n",
    "    else:\n",
    "        ratio = float('nan')\n",
    "    print(f\"\\n  {name}:\")\n",
    "    print(f\"    Gap captured: {gap:+.4f} ({ratio:.0f}% of oracle gap)\")\n",
    "\n",
    "# Per-sample correlations\n",
    "oracle_delta = bare_nlls - oracle_nlls\n",
    "surr_para_delta = bare_nlls - surr_para_nlls\n",
    "surr_doc_delta = bare_nlls - surr_doc_nlls\n",
    "\n",
    "r_op, p_op = stats.pearsonr(oracle_delta, surr_para_delta)\n",
    "r_od, p_od = stats.pearsonr(oracle_delta, surr_doc_delta)\n",
    "r_pd, p_pd = stats.pearsonr(surr_para_delta, surr_doc_delta)\n",
    "\n",
    "print(f\"\\n--- Per-sample correlations ---\")\n",
    "print(f\"  oracle vs surr_para: r={r_op:.3f} (p={p_op:.2e})\")\n",
    "print(f\"  oracle vs surr_doc:  r={r_od:.3f} (p={p_od:.2e})\")\n",
    "print(f\"  surr_para vs surr_doc: r={r_pd:.3f} (p={p_pd:.2e})\")\n",
    "\n",
    "# Hardness gradient\n",
    "print(f\"\\n--- Hardness gradient (by bare NLL quintile) ---\")\n",
    "quintile_bounds = np.percentile(bare_nlls, [20, 40, 60, 80])\n",
    "quintiles = np.digitize(bare_nlls, quintile_bounds)\n",
    "\n",
    "print(f\"{'Quintile':<12} {'N':>4} {'bare':>10} {'oracle':>10} {'surr_para':>10} {'surr_doc':>10} {'orc-bare':>10} {'sp-bare':>10}\")\n",
    "print(\"-\" * 78)\n",
    "\n",
    "for q in range(5):\n",
    "    mask = quintiles == q\n",
    "    n_q = mask.sum()\n",
    "    if n_q < 3:\n",
    "        continue\n",
    "    qlabel = ['Q1 easy', 'Q2', 'Q3', 'Q4', 'Q5 hard'][q]\n",
    "    b = bare_nlls[mask].mean()\n",
    "    o = oracle_nlls[mask].mean()\n",
    "    sp = surr_para_nlls[mask].mean()\n",
    "    sd = surr_doc_nlls[mask].mean()\n",
    "    print(f\"{qlabel:<12} {n_q:>4} {b:>10.4f} {o:>10.4f} {sp:>10.4f} {sd:>10.4f} {b-o:>+10.4f} {b-sp:>+10.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "969bd1ac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-17T20:10:44.780436Z",
     "iopub.status.busy": "2026-02-17T20:10:44.780199Z",
     "iopub.status.idle": "2026-02-17T20:10:44.791366Z",
     "shell.execute_reply": "2026-02-17T20:10:44.790761Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "VERDICT -- Exp 33b: Corrected Surrogate Transfer\n",
      "======================================================================\n",
      "\n",
      "Model: google/t5gemma-2-4b-4b\n",
      "N: 200 samples\n",
      "Setup: query in encoder ONLY, decoder sees answer ONLY\n",
      "\n",
      "--- Core question: does query in encoder help? ---\n",
      "  oracle d = +0.345\n",
      "  YES -- strong benefit. Query-aware encoding helps the decoder.\n",
      "\n",
      "--- Surrogate transfer ---\n",
      "  surr_para: d=+0.293, captures 70% of oracle gap\n",
      "  surr_doc: d=+0.312, captures 96% of oracle gap\n",
      "  static: d=+0.103, captures 9% of oracle gap\n",
      "\n",
      "--- Expected vs actual ranking (best to worst NLL) ---\n",
      "  Expected: oracle > surr_para > surr_doc > static > bare\n",
      "  Actual:   oracle > surr_doc > surr_para > static > bare\n",
      "\n",
      "--- Exp 33 vs 33b comparison ---\n",
      "  Exp 33  (query in decoder): oracle d = -0.175 (HURT)\n",
      "  Exp 33b (query in encoder): oracle d = +0.345\n",
      "  CONFIRMED: removing query from decoder reveals the encoder benefit\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Results saved to results/exp33b/results.json\n"
     ]
    }
   ],
   "source": [
    "# Cell 10: Verdict and save\n",
    "print(\"=\" * 70)\n",
    "print(\"VERDICT -- Exp 33b: Corrected Surrogate Transfer\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\nModel: {MODEL_NAME}\")\n",
    "print(f\"N: {len(all_results)} samples\")\n",
    "print(f\"Setup: query in encoder ONLY, decoder sees answer ONLY\")\n",
    "\n",
    "oracle_d = analysis.get('oracle', {}).get('cohens_d', 0)\n",
    "static_d = analysis.get('static', {}).get('cohens_d', 0)\n",
    "surr_para_d = analysis.get('surr_para', {}).get('cohens_d', 0)\n",
    "surr_doc_d = analysis.get('surr_doc', {}).get('cohens_d', 0)\n",
    "\n",
    "oracle_gap = bare_nlls.mean() - oracle_nlls.mean()\n",
    "\n",
    "print(f\"\\n--- Core question: does query in encoder help? ---\")\n",
    "print(f\"  oracle d = {oracle_d:+.3f}\")\n",
    "if oracle_d > 0.2:\n",
    "    print(f\"  YES -- strong benefit. Query-aware encoding helps the decoder.\")\n",
    "elif oracle_d > 0.05:\n",
    "    print(f\"  MODERATE -- some benefit from query-aware encoding.\")\n",
    "elif oracle_d > 0:\n",
    "    print(f\"  MARGINAL -- barely helps.\")\n",
    "else:\n",
    "    print(f\"  NO -- even with query as sole context source, encoding doesn't help.\")\n",
    "\n",
    "print(f\"\\n--- Surrogate transfer ---\")\n",
    "if oracle_gap > 0:\n",
    "    for name, d_val in [('surr_para', surr_para_d), ('surr_doc', surr_doc_d), ('static', static_d)]:\n",
    "        gap = analysis.get(name, {}).get('delta_vs_bare', 0)\n",
    "        ratio = gap / oracle_gap * 100 if oracle_gap > 0 else 0\n",
    "        print(f\"  {name}: d={d_val:+.3f}, captures {ratio:.0f}% of oracle gap\")\n",
    "else:\n",
    "    print(f\"  Oracle gap is zero or negative -- no benefit to transfer.\")\n",
    "\n",
    "# Expected hierarchy check\n",
    "expected = ['oracle', 'surr_para', 'surr_doc', 'static', 'bare']\n",
    "actual_order = sorted(cond_names, key=lambda c: np.array([r[f'nll_{c}'] for r in all_results]).mean())\n",
    "print(f\"\\n--- Expected vs actual ranking (best to worst NLL) ---\")\n",
    "print(f\"  Expected: {' > '.join(expected)}\")\n",
    "print(f\"  Actual:   {' > '.join(actual_order)}\")\n",
    "\n",
    "# Comparison to Exp 33 (wrong setup)\n",
    "print(f\"\\n--- Exp 33 vs 33b comparison ---\")\n",
    "print(f\"  Exp 33  (query in decoder): oracle d = -0.175 (HURT)\")\n",
    "print(f\"  Exp 33b (query in encoder): oracle d = {oracle_d:+.3f}\")\n",
    "if oracle_d > 0:\n",
    "    print(f\"  CONFIRMED: removing query from decoder reveals the encoder benefit\")\n",
    "else:\n",
    "    print(f\"  Even with corrected setup, encoder query-awareness doesn't help\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "\n",
    "# Save\n",
    "final_results = {\n",
    "    'experiment': 'exp33b_corrected_surrogate',\n",
    "    'model': MODEL_NAME,\n",
    "    'n_samples': len(all_results),\n",
    "    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'setup': 'query in encoder ONLY, decoder scores answer ONLY',\n",
    "    'analysis': analysis,\n",
    "    'oracle_bare_gap': float(oracle_gap),\n",
    "    'correlations': {\n",
    "        'oracle_vs_surr_para': float(r_op),\n",
    "        'oracle_vs_surr_doc': float(r_od),\n",
    "    },\n",
    "}\n",
    "\n",
    "with open(RESULTS_DIR / 'results.json', 'w') as f:\n",
    "    json.dump(final_results, f, indent=2)\n",
    "print(f\"\\nResults saved to {RESULTS_DIR / 'results.json'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd7172d2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-17T20:10:44.794495Z",
     "iopub.status.busy": "2026-02-17T20:10:44.794022Z",
     "iopub.status.idle": "2026-02-17T20:10:45.301325Z",
     "shell.execute_reply": "2026-02-17T20:10:45.300389Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning up GPU memory...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory: 15.03 GB -> 0.01 GB\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Cell 11: Cleanup\n",
    "print(\"Cleaning up GPU memory...\")\n",
    "mem_before = torch.cuda.memory_allocated() / 1e9\n",
    "del model, processor, tokenizer\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "mem_after = torch.cuda.memory_allocated() / 1e9\n",
    "print(f\"GPU memory: {mem_before:.2f} GB -> {mem_after:.2f} GB\")\n",
    "print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0541a63f31864c43a684f9b5704bf49e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "13161b653a264fd095e892205bb0d03a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_afcf8082db1644868b5cf44cee359f8e",
        "IPY_MODEL_5ee1ac5e7e7e43b7b9b57b1421fc75c6",
        "IPY_MODEL_7b43130222644fcdaeb232833c57c41e"
       ],
       "layout": "IPY_MODEL_e1c185fb89e944a4a786d92f4f89352f",
       "tabbable": null,
       "tooltip": null
      }
     },
     "1b32d470eb674b5c8452a3a71d69e970": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "32697bb7d9ba45519084d39b876ecafe": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "46a1b5fea3f64582a30d921b3e5a585b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "5ee1ac5e7e7e43b7b9b57b1421fc75c6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_9f4da89ab2a54b3ea832f2ab2d7b4e1a",
       "max": 1327.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_75ba049591d84b718d1df05c40ae8316",
       "tabbable": null,
       "tooltip": null,
       "value": 1327.0
      }
     },
     "75ba049591d84b718d1df05c40ae8316": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "797b7ab2f62340d0b770baf7e9b373e6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "7b43130222644fcdaeb232833c57c41e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_32697bb7d9ba45519084d39b876ecafe",
       "placeholder": "​",
       "style": "IPY_MODEL_d1df3ec22b7240919ad247d92c83b32e",
       "tabbable": null,
       "tooltip": null,
       "value": " 1327/1327 [00:04&lt;00:00, 702.84it/s, Materializing param=model.encoder.vision_tower.vision_model.post_layernorm.weight]"
      }
     },
     "832eaa91b7fd4faf832a6a6ed374eef7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "90979850938b46c3b24019db80f4e911": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_a2af3a27b7604cec834e1c8669b16ea1",
       "placeholder": "​",
       "style": "IPY_MODEL_797b7ab2f62340d0b770baf7e9b373e6",
       "tabbable": null,
       "tooltip": null,
       "value": "Scoring: 100%"
      }
     },
     "9f4da89ab2a54b3ea832f2ab2d7b4e1a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a2af3a27b7604cec834e1c8669b16ea1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "aca4d0c8753a4d0bbd9568064a9c903b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "afcf8082db1644868b5cf44cee359f8e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_832eaa91b7fd4faf832a6a6ed374eef7",
       "placeholder": "​",
       "style": "IPY_MODEL_46a1b5fea3f64582a30d921b3e5a585b",
       "tabbable": null,
       "tooltip": null,
       "value": "Loading weights: 100%"
      }
     },
     "bf4aca452ee54910b622abc4665ddfa6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_90979850938b46c3b24019db80f4e911",
        "IPY_MODEL_fedb67a7f45c4cacb3892394c2f375c9",
        "IPY_MODEL_c7cbb866d9e241d69abd27388a214bb2"
       ],
       "layout": "IPY_MODEL_aca4d0c8753a4d0bbd9568064a9c903b",
       "tabbable": null,
       "tooltip": null
      }
     },
     "c7cbb866d9e241d69abd27388a214bb2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_1b32d470eb674b5c8452a3a71d69e970",
       "placeholder": "​",
       "style": "IPY_MODEL_ec643804d53c466498e9afb4469420b2",
       "tabbable": null,
       "tooltip": null,
       "value": " 200/200 [03:50&lt;00:00,  1.13s/it]"
      }
     },
     "d1df3ec22b7240919ad247d92c83b32e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "da6585a3fe454754bbd2c3b5cfb1dac4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e1c185fb89e944a4a786d92f4f89352f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ec643804d53c466498e9afb4469420b2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "fedb67a7f45c4cacb3892394c2f375c9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_da6585a3fe454754bbd2c3b5cfb1dac4",
       "max": 200.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_0541a63f31864c43a684f9b5704bf49e",
       "tabbable": null,
       "tooltip": null,
       "value": 200.0
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
