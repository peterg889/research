{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {
    "papermill": {
     "duration": 0.00514,
     "end_time": "2026-02-15T21:13:51.212950",
     "exception": false,
     "start_time": "2026-02-15T21:13:51.207810",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Exp 22: Ranking Evaluation & Contrastive Scoring (PMI)\n",
    "\n",
    "## Motivation\n",
    "\n",
    "Exp 19 showed that `values_early_layers` (layers 0-16) is the best-known mechanism for\n",
    "Gemma 3 4B priming (d=+0.211, vs values_only d=+0.056). But all prior experiments measured\n",
    "*average NLL improvement* \u2014 they never tested whether the signal actually **correlates with\n",
    "document relevance** for ranking. This experiment validates the ranking hypothesis: does\n",
    "values_early_layers produce lower NLL for relevant documents than irrelevant ones?\n",
    "\n",
    "Additionally, raw NLL is biased by how \"easy\" the answer is for the model. We introduce\n",
    "**Contrastive Scoring (PMI)** to subtract the model's prior:\n",
    "\n",
    "```\n",
    "Score_PMI = NLL(Answer | Query, Document) - NLL(Answer | Query, Empty)\n",
    "```\n",
    "\n",
    "If the document helps predict the answer, NLL drops, making Score_PMI more negative.\n",
    "Relevant documents should have more negative Score_PMI than irrelevant ones.\n",
    "\n",
    "## Design\n",
    "\n",
    "| Parameter | Value |\n",
    "|-----------|-------|\n",
    "| Model | Gemma 3 4B (`google/gemma-3-4b-it`, 4-bit, bfloat16) |\n",
    "| Method | `values_early_layers` (layers 0-16 of 34), from Exp 19 |\n",
    "| Dataset | MS MARCO v1.1 validation (~8-10 candidate passages per query) |\n",
    "| N queries | 200 |\n",
    "| Checkpoint | Every 10 queries |\n",
    "\n",
    "### Scoring per passage (3 NLL values)\n",
    "\n",
    "1. `nll_primed`: values_early_layers cache \u2192 score answer\n",
    "2. `nll_bare`: bare cache (no priming) \u2192 score answer\n",
    "3. `nll_baseline`: BOS-only cache (no document) \u2192 score answer (computed ONCE per query)\n",
    "\n",
    "### Derived scores\n",
    "\n",
    "- `score_pmi_primed = nll_primed - nll_baseline`\n",
    "- `score_pmi_bare = nll_bare - nll_baseline`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T21:13:51.223254Z",
     "iopub.status.busy": "2026-02-15T21:13:51.222942Z",
     "iopub.status.idle": "2026-02-15T21:14:10.056523Z",
     "shell.execute_reply": "2026-02-15T21:14:10.055633Z"
    },
    "papermill": {
     "duration": 18.841099,
     "end_time": "2026-02-15T21:14:10.058284",
     "exception": false,
     "start_time": "2026-02-15T21:13:51.217185",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEED: 42\n",
      "Results directory: results/exp22\n",
      "CUDA available: True\n",
      "GPU: NVIDIA A100-SXM4-40GB\n",
      "GPU memory: 42.3 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading google/gemma-3-4b-it (4-bit, bfloat16)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c02d11f865e4e2aab67d4e62e1ff329",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/883 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model loaded successfully.\n",
      "  Model class: Gemma3ForConditionalGeneration\n",
      "  Text config class: Gemma3TextConfig\n",
      "  Hidden size: 2560\n",
      "  Num layers: 34\n",
      "  Num attention heads: 8\n",
      "  Num KV heads: 4\n",
      "  Head dim: 256\n",
      "  BOS token ID: 2\n",
      "  EOS token ID: 1\n",
      "  Unique RoPE thetas: [10000.0, 1000000.0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Cache key dtype: torch.bfloat16\n",
      "  Cache key shape: torch.Size([1, 4, 2, 256])  (batch, kv_heads, seq, head_dim)\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Setup & Imports\n",
    "import os\n",
    "os.umask(0o000)\n",
    "os.environ['HF_TOKEN'] = 'REDACTED'\n",
    "\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import gc\n",
    "from pathlib import Path\n",
    "\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "RESULTS_DIR = Path(\"results/exp22\")\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "(RESULTS_DIR / \"figures\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CHECKPOINT_PATH = RESULTS_DIR / \"checkpoint.json\"\n",
    "FINAL_RESULTS_PATH = RESULTS_DIR / \"results.json\"\n",
    "CSV_PATH = RESULTS_DIR / \"passage_scores.csv\"\n",
    "\n",
    "print(f\"SEED: {SEED}\")\n",
    "print(f\"Results directory: {RESULTS_DIR}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "\n",
    "# Load model\n",
    "sys.path.insert(0, \".\")\n",
    "\n",
    "from lib.config import ExperimentConfig\n",
    "from lib.model_utils import load_model\n",
    "\n",
    "MODEL_NAME = \"google/gemma-3-4b-it\"\n",
    "\n",
    "exp_config = ExperimentConfig(\n",
    "    model_name=MODEL_NAME,\n",
    "    model_type=\"gemma3\",\n",
    "    compute_dtype=\"auto\",\n",
    "    use_4bit=True,\n",
    "    num_samples=2000,\n",
    "    seed=SEED,\n",
    ")\n",
    "\n",
    "print(f\"\\nLoading {MODEL_NAME} (4-bit, bfloat16)...\")\n",
    "model, tokenizer = load_model(exp_config)\n",
    "\n",
    "# Architecture diagnostics\n",
    "from lib.kv_cache import _get_text_config, _get_head_dim, _get_rope_theta_for_layer, _get_cache_keys, _ensure_dynamic_cache\n",
    "\n",
    "text_config = _get_text_config(model.config)\n",
    "print(f\"\\nModel loaded successfully.\")\n",
    "print(f\"  Model class: {type(model).__name__}\")\n",
    "print(f\"  Text config class: {type(text_config).__name__}\")\n",
    "print(f\"  Hidden size: {text_config.hidden_size}\")\n",
    "print(f\"  Num layers: {text_config.num_hidden_layers}\")\n",
    "print(f\"  Num attention heads: {text_config.num_attention_heads}\")\n",
    "print(f\"  Num KV heads: {text_config.num_key_value_heads}\")\n",
    "print(f\"  Head dim: {_get_head_dim(model.config)}\")\n",
    "print(f\"  BOS token ID: {tokenizer.bos_token_id}\")\n",
    "print(f\"  EOS token ID: {tokenizer.eos_token_id}\")\n",
    "\n",
    "# Per-layer RoPE diagnostics\n",
    "thetas = set()\n",
    "for layer_idx in range(text_config.num_hidden_layers):\n",
    "    thetas.add(_get_rope_theta_for_layer(model.config, layer_idx))\n",
    "print(f\"  Unique RoPE thetas: {sorted(thetas)}\")\n",
    "\n",
    "# Verify dtype\n",
    "sample_ids = tokenizer(\"test\", return_tensors=\"pt\")['input_ids'].to(exp_config.device)\n",
    "with torch.no_grad():\n",
    "    out = model(sample_ids, use_cache=True)\n",
    "    cache_check = _ensure_dynamic_cache(out.past_key_values)\n",
    "    k0 = _get_cache_keys(cache_check, 0)\n",
    "    print(f\"  Cache key dtype: {k0.dtype}\")\n",
    "    print(f\"  Cache key shape: {k0.shape}  (batch, kv_heads, seq, head_dim)\")\n",
    "del out, sample_ids\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T21:14:10.066013Z",
     "iopub.status.busy": "2026-02-15T21:14:10.065428Z",
     "iopub.status.idle": "2026-02-15T21:14:10.072453Z",
     "shell.execute_reply": "2026-02-15T21:14:10.071611Z"
    },
    "papermill": {
     "duration": 0.012445,
     "end_time": "2026-02-15T21:14:10.073928",
     "exception": false,
     "start_time": "2026-02-15T21:14:10.061483",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "  N_QUERIES: 200\n",
      "  LAYER_CUTOFF: 17 (layers 0-16)\n",
      "  CHECKPOINT_EVERY: 10\n",
      "  STATIC_FACT: 'What are the key facts I need to know?'\n",
      "  MAX_PASSAGE_WORDS: 300\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Configuration & lib imports\n",
    "from lib.kv_cache import (\n",
    "    _get_cache_keys,\n",
    "    _get_cache_values,\n",
    "    _set_cache_keys,\n",
    "    _set_cache_values,\n",
    "    _ensure_dynamic_cache,\n",
    "    extract_and_truncate_cache_with_bos,\n",
    "    correct_rope_positions_with_bos,\n",
    "    score_answer_with_cache,\n",
    "    deepcopy_cache,\n",
    "    replace_values_at_layers,\n",
    ")\n",
    "from lib.surrogate import STATIC_SURROGATE_QUERIES\n",
    "from lib.data import count_words\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "N_QUERIES = 200\n",
    "LAYER_CUTOFF = 17  # layers 0-16 inclusive = list(range(17))\n",
    "CHECKPOINT_EVERY = 10\n",
    "MAX_PASSAGE_WORDS = 300\n",
    "\n",
    "STATIC_FACT = STATIC_SURROGATE_QUERIES['static_factual']['query']\n",
    "SURROGATE_PREFIX_TEMPLATE = \"{surrogate}\\n\"\n",
    "QUERY_TEMPLATE = \"\\nQuery: {query}\\nAnswer:\"\n",
    "ANSWER_TEMPLATE = \" {answer}\"\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  N_QUERIES: {N_QUERIES}\")\n",
    "print(f\"  LAYER_CUTOFF: {LAYER_CUTOFF} (layers 0-{LAYER_CUTOFF-1})\")\n",
    "print(f\"  CHECKPOINT_EVERY: {CHECKPOINT_EVERY}\")\n",
    "print(f\"  STATIC_FACT: '{STATIC_FACT}'\")\n",
    "print(f\"  MAX_PASSAGE_WORDS: {MAX_PASSAGE_WORDS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T21:14:10.081365Z",
     "iopub.status.busy": "2026-02-15T21:14:10.080843Z",
     "iopub.status.idle": "2026-02-15T21:14:11.105320Z",
     "shell.execute_reply": "2026-02-15T21:14:11.104607Z"
    },
    "papermill": {
     "duration": 1.030749,
     "end_time": "2026-02-15T21:14:11.107582",
     "exception": false,
     "start_time": "2026-02-15T21:14:10.076833",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`trust_remote_code` is not supported anymore.\n",
      "Please check that the Hugging Face dataset 'microsoft/ms_marco' isn't based on a loading script and remove `trust_remote_code`.\n",
      "If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "LOADING MS MARCO v1.1 \u2014 ALL PASSAGES PER QUERY\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total items in validation: 10047\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd3be5cae5a04ef6a228bd9f243b4146",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filtering:   0%|          | 0/10047 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Selected 200 queries (1692 total passages)\n",
      "Passages per query: mean=8.5, min=5, max=10\n",
      "Word counts: mean=71\n",
      "Relevant passages: 221 (13.1%)\n",
      "Irrelevant passages: 1471 (86.9%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 3: Load MS MARCO Ranking Data\n",
    "from datasets import load_dataset\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"LOADING MS MARCO v1.1 \u2014 ALL PASSAGES PER QUERY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "dataset = load_dataset(\"microsoft/ms_marco\", \"v1.1\", split=\"validation\",\n",
    "                        trust_remote_code=True)\n",
    "print(f\"Total items in validation: {len(dataset)}\")\n",
    "\n",
    "queries = []\n",
    "np.random.seed(SEED)\n",
    "\n",
    "for item in tqdm(dataset, desc=\"Filtering\"):\n",
    "    passages_info = item.get('passages', {})\n",
    "    passage_texts = passages_info.get('passage_text', [])\n",
    "    is_selected = passages_info.get('is_selected', [])\n",
    "    query = item.get('query', '')\n",
    "    answers = item.get('answers', [])\n",
    "    well_formed = item.get('wellFormedAnswers', [])\n",
    "\n",
    "    if not passage_texts or not query:\n",
    "        continue\n",
    "    # Require at least 1 relevant passage\n",
    "    if not is_selected or sum(is_selected) == 0:\n",
    "        continue\n",
    "\n",
    "    # All passages must be <= MAX_PASSAGE_WORDS\n",
    "    word_counts = [count_words(p) for p in passage_texts]\n",
    "    if any(wc > MAX_PASSAGE_WORDS for wc in word_counts):\n",
    "        continue\n",
    "\n",
    "    # Require valid answer\n",
    "    answer = None\n",
    "    if well_formed and len(well_formed) > 0 and well_formed[0] != '[]':\n",
    "        answer = well_formed[0]\n",
    "    elif answers and len(answers) > 0 and answers[0] != 'No Answer Present.':\n",
    "        answer = answers[0]\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "    passage_list = []\n",
    "    for i, (ptext, sel) in enumerate(zip(passage_texts, is_selected)):\n",
    "        passage_list.append({\n",
    "            'passage': ptext,\n",
    "            'is_relevant': bool(sel == 1),\n",
    "            'word_count': word_counts[i],\n",
    "            'passage_idx': i,\n",
    "        })\n",
    "\n",
    "    queries.append({\n",
    "        'query': query,\n",
    "        'answer': answer,\n",
    "        'passages': passage_list,\n",
    "        'n_passages': len(passage_list),\n",
    "        'n_relevant': sum(1 for p in passage_list if p['is_relevant']),\n",
    "    })\n",
    "\n",
    "    if len(queries) >= N_QUERIES * 3:\n",
    "        break\n",
    "\n",
    "np.random.shuffle(queries)\n",
    "queries = queries[:N_QUERIES]\n",
    "N = len(queries)\n",
    "\n",
    "n_passages_list = [q['n_passages'] for q in queries]\n",
    "total_passages = sum(n_passages_list)\n",
    "total_relevant = sum(q['n_relevant'] for q in queries)\n",
    "total_irrelevant = total_passages - total_relevant\n",
    "\n",
    "print(f\"\\nSelected {N} queries ({total_passages} total passages)\")\n",
    "print(f\"Passages per query: mean={np.mean(n_passages_list):.1f}, \"\n",
    "      f\"min={min(n_passages_list)}, max={max(n_passages_list)}\")\n",
    "print(f\"Word counts: mean={np.mean([p['word_count'] for q in queries for p in q['passages']]):.0f}\")\n",
    "print(f\"Relevant passages: {total_relevant} ({100*total_relevant/total_passages:.1f}%)\")\n",
    "print(f\"Irrelevant passages: {total_irrelevant} ({100*total_irrelevant/total_passages:.1f}%)\")\n",
    "\n",
    "del dataset\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T21:14:11.117050Z",
     "iopub.status.busy": "2026-02-15T21:14:11.116081Z",
     "iopub.status.idle": "2026-02-15T21:14:11.122661Z",
     "shell.execute_reply": "2026-02-15T21:14:11.122013Z"
    },
    "papermill": {
     "duration": 0.012908,
     "end_time": "2026-02-15T21:14:11.124051",
     "exception": false,
     "start_time": "2026-02-15T21:14:11.111143",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "EXPERIMENTAL CONDITIONS EXPLAINED\n",
      "======================================================================\n",
      "\n",
      "Example query:   \"what is the bride entrance called\"\n",
      "Example answer:  \"Procession...\"\n",
      "Example passage: \"Check out our ultimate guide below, beginning with Mom and ending with the bride...\"\n",
      "Static fact:     \"What are the key facts I need to know?\"\n",
      "\n",
      "### Pass A \u2014 Primed (values_early_layers) ###\n",
      "  Step 1: Build bare cache     = model([BOS][doc_ids])\n",
      "  Step 2: Build primed cache   = model([BOS][sf_ids][doc_ids])\n",
      "  Step 3: Truncate primed      = keep [BOS] + last doc_len positions\n",
      "  Step 4: RoPE correct         = correct_rope_positions_with_bos(truncated, offset)\n",
      "  Step 5: Hybrid cache         = replace_values_at_layers(bare, primed_corrected, range(17))\n",
      "           (bare keys everywhere, primed values in layers 0-16 only)\n",
      "  Step 6: Score                = score_answer_with_cache(hybrid, query+answer)\n",
      "  Result: nll_primed\n",
      "\n",
      "### Pass B \u2014 Bare ###\n",
      "  Step 1: Reuse bare cache from Pass A\n",
      "  Step 2: Score                = score_answer_with_cache(bare, query+answer)\n",
      "  Result: nll_bare\n",
      "\n",
      "### Pass C \u2014 Baseline (computed ONCE per query) ###\n",
      "  Step 1: Build BOS-only cache = model([BOS])\n",
      "  Step 2: Score                = score_answer_with_cache(bos_cache, query+answer)\n",
      "  Result: nll_baseline\n",
      "  Note: Same for ALL passages under this query (no document context)\n",
      "\n",
      "### PMI Formula ###\n",
      "  score_pmi_primed = nll_primed - nll_baseline\n",
      "  score_pmi_bare   = nll_bare   - nll_baseline\n",
      "\n",
      "  If document helps -> NLL drops below baseline -> PMI is NEGATIVE\n",
      "  More negative PMI = document is more helpful for predicting the answer\n",
      "  Relevant docs should have more negative PMI than irrelevant docs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Explain Experimental Conditions\n",
    "print(\"=\" * 70)\n",
    "print(\"EXPERIMENTAL CONDITIONS EXPLAINED\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "example_query = queries[0]['query']\n",
    "example_answer = queries[0]['answer']\n",
    "example_passage = queries[0]['passages'][0]['passage'][:80] + \"...\"\n",
    "sf_str = SURROGATE_PREFIX_TEMPLATE.format(surrogate=STATIC_FACT)\n",
    "\n",
    "print(f\"\"\"\n",
    "Example query:   \\\"{example_query}\\\"\n",
    "Example answer:  \\\"{example_answer[:60]}...\\\"\n",
    "Example passage: \\\"{example_passage}\\\"\n",
    "Static fact:     \\\"{STATIC_FACT}\\\"\n",
    "\n",
    "### Pass A \u2014 Primed (values_early_layers) ###\n",
    "  Step 1: Build bare cache     = model([BOS][doc_ids])\n",
    "  Step 2: Build primed cache   = model([BOS][sf_ids][doc_ids])\n",
    "  Step 3: Truncate primed      = keep [BOS] + last doc_len positions\n",
    "  Step 4: RoPE correct         = correct_rope_positions_with_bos(truncated, offset)\n",
    "  Step 5: Hybrid cache         = replace_values_at_layers(bare, primed_corrected, range({LAYER_CUTOFF}))\n",
    "           (bare keys everywhere, primed values in layers 0-{LAYER_CUTOFF-1} only)\n",
    "  Step 6: Score                = score_answer_with_cache(hybrid, query+answer)\n",
    "  Result: nll_primed\n",
    "\n",
    "### Pass B \u2014 Bare ###\n",
    "  Step 1: Reuse bare cache from Pass A\n",
    "  Step 2: Score                = score_answer_with_cache(bare, query+answer)\n",
    "  Result: nll_bare\n",
    "\n",
    "### Pass C \u2014 Baseline (computed ONCE per query) ###\n",
    "  Step 1: Build BOS-only cache = model([BOS])\n",
    "  Step 2: Score                = score_answer_with_cache(bos_cache, query+answer)\n",
    "  Result: nll_baseline\n",
    "  Note: Same for ALL passages under this query (no document context)\n",
    "\n",
    "### PMI Formula ###\n",
    "  score_pmi_primed = nll_primed - nll_baseline\n",
    "  score_pmi_bare   = nll_bare   - nll_baseline\n",
    "\n",
    "  If document helps -> NLL drops below baseline -> PMI is NEGATIVE\n",
    "  More negative PMI = document is more helpful for predicting the answer\n",
    "  Relevant docs should have more negative PMI than irrelevant docs\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T21:14:11.132547Z",
     "iopub.status.busy": "2026-02-15T21:14:11.132181Z",
     "iopub.status.idle": "2026-02-15T21:14:12.814341Z",
     "shell.execute_reply": "2026-02-15T21:14:12.813574Z"
    },
    "papermill": {
     "duration": 1.688327,
     "end_time": "2026-02-15T21:14:12.815844",
     "exception": false,
     "start_time": "2026-02-15T21:14:11.127517",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smoke-testing helper functions...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Baseline NLL (BOS-only): 4.7500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Primed NLL: 9.6250\n",
      "  Bare NLL:   8.1250\n",
      "  Doc len:    151\n",
      "  PMI primed: 4.8750\n",
      "  PMI bare:   3.3750\n",
      "\n",
      "Smoke test passed.\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Helper Functions\n",
    "\n",
    "def score_baseline_nll(query_prompt, answer_text, model, tokenizer, config):\n",
    "    \"\"\"Score answer with NO document context (BOS-only cache).\n",
    "\n",
    "    Builds a cache from just the BOS token, then scores query+answer.\n",
    "    This gives NLL(Answer | Query, Empty) \u2014 the model's prior.\n",
    "\n",
    "    Returns: float (mean NLL per token)\n",
    "    \"\"\"\n",
    "    bos_id = torch.tensor([[tokenizer.bos_token_id]], device=config.device)\n",
    "    with torch.no_grad():\n",
    "        bos_out = model(input_ids=bos_id,\n",
    "                        attention_mask=torch.ones_like(bos_id),\n",
    "                        use_cache=True, return_dict=True)\n",
    "    bos_cache = _ensure_dynamic_cache(bos_out.past_key_values)\n",
    "    del bos_out\n",
    "\n",
    "    context_len = 1  # just BOS\n",
    "    nll = score_answer_with_cache(\n",
    "        bos_cache, context_len, query_prompt, answer_text,\n",
    "        model, tokenizer, config)\n",
    "    return nll\n",
    "\n",
    "\n",
    "def build_and_score_passage(passage_text, sf_str, query_prompt, answer_text,\n",
    "                            model, tokenizer, config, layer_cutoff):\n",
    "    \"\"\"Build values_early_layers + bare caches, score both.\n",
    "\n",
    "    Uses matched tokenization from Exp 19: tokenize prefix+doc together,\n",
    "    extract doc_ids from the concatenated encoding.\n",
    "\n",
    "    Returns: {'nll_primed': float, 'nll_bare': float, 'doc_len': int}\n",
    "    \"\"\"\n",
    "    # --- Matched tokenization ---\n",
    "    full_text = sf_str + passage_text\n",
    "    full_enc = tokenizer(full_text, return_tensors=\"pt\",\n",
    "                          add_special_tokens=True, padding=False, truncation=False)\n",
    "    full_ids = full_enc['input_ids'].to(config.device)\n",
    "\n",
    "    sf_prefix_enc = tokenizer(sf_str, return_tensors=\"pt\",\n",
    "                               add_special_tokens=True, padding=False, truncation=False)\n",
    "    sf_prefix_len_matched = sf_prefix_enc['input_ids'].shape[1]\n",
    "\n",
    "    bos_id = full_ids[:, :1]\n",
    "    sf_ids = full_ids[:, 1:sf_prefix_len_matched]  # sf tokens (without BOS)\n",
    "    doc_ids = full_ids[:, sf_prefix_len_matched:]\n",
    "    doc_len = doc_ids.shape[1]\n",
    "    context_len = 1 + doc_len  # BOS + doc\n",
    "\n",
    "    del full_enc, sf_prefix_enc\n",
    "\n",
    "    # === Forward pass 1: BARE cache ===\n",
    "    bare_input = torch.cat([bos_id, doc_ids], dim=1)\n",
    "    with torch.no_grad():\n",
    "        bare_out = model(input_ids=bare_input,\n",
    "                         attention_mask=torch.ones_like(bare_input),\n",
    "                         use_cache=True, return_dict=True)\n",
    "    bare_cache = _ensure_dynamic_cache(bare_out.past_key_values)\n",
    "    del bare_out, bare_input\n",
    "\n",
    "    # === Forward pass 2: PRIMED cache ===\n",
    "    primed_input = torch.cat([bos_id, sf_ids, doc_ids], dim=1)\n",
    "    with torch.no_grad():\n",
    "        primed_out = model(input_ids=primed_input,\n",
    "                           attention_mask=torch.ones_like(primed_input),\n",
    "                           use_cache=True, return_dict=True)\n",
    "    primed_full = _ensure_dynamic_cache(primed_out.past_key_values)\n",
    "    del primed_out, primed_input\n",
    "\n",
    "    # === Truncate + RoPE correct ===\n",
    "    trunc_cache = extract_and_truncate_cache_with_bos(primed_full, doc_len)\n",
    "    prefix_offset = sf_ids.shape[1]\n",
    "    del primed_full\n",
    "\n",
    "    correct_rope_positions_with_bos(trunc_cache, prefix_offset, model)\n",
    "\n",
    "    # === Build values_early_layers ===\n",
    "    vel_cache = replace_values_at_layers(\n",
    "        bare_cache, trunc_cache, list(range(layer_cutoff))\n",
    "    )\n",
    "    del trunc_cache\n",
    "\n",
    "    # === Score primed (values_early_layers) ===\n",
    "    nll_primed = score_answer_with_cache(\n",
    "        deepcopy_cache(vel_cache), context_len,\n",
    "        query_prompt, answer_text, model, tokenizer, config)\n",
    "    del vel_cache\n",
    "\n",
    "    # === Score bare (mutates cache, so do last) ===\n",
    "    nll_bare = score_answer_with_cache(\n",
    "        bare_cache, context_len,\n",
    "        query_prompt, answer_text, model, tokenizer, config)\n",
    "    del bare_cache\n",
    "\n",
    "    del bos_id, sf_ids, doc_ids, full_ids\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return {'nll_primed': nll_primed, 'nll_bare': nll_bare, 'doc_len': doc_len}\n",
    "\n",
    "\n",
    "# Quick smoke test\n",
    "print(\"Smoke-testing helper functions...\")\n",
    "test_q = queries[0]\n",
    "test_query_prompt = QUERY_TEMPLATE.format(query=test_q['query'])\n",
    "test_answer_text = ANSWER_TEMPLATE.format(answer=test_q['answer'])\n",
    "test_sf_str = SURROGATE_PREFIX_TEMPLATE.format(surrogate=STATIC_FACT)\n",
    "\n",
    "# Test baseline\n",
    "test_baseline = score_baseline_nll(test_query_prompt, test_answer_text, model, tokenizer, exp_config)\n",
    "print(f\"  Baseline NLL (BOS-only): {test_baseline:.4f}\")\n",
    "assert np.isfinite(test_baseline), \"Baseline NLL is not finite!\"\n",
    "\n",
    "# Test passage scoring\n",
    "test_result = build_and_score_passage(\n",
    "    test_q['passages'][0]['passage'], test_sf_str,\n",
    "    test_query_prompt, test_answer_text,\n",
    "    model, tokenizer, exp_config, LAYER_CUTOFF)\n",
    "print(f\"  Primed NLL: {test_result['nll_primed']:.4f}\")\n",
    "print(f\"  Bare NLL:   {test_result['nll_bare']:.4f}\")\n",
    "print(f\"  Doc len:    {test_result['doc_len']}\")\n",
    "print(f\"  PMI primed: {test_result['nll_primed'] - test_baseline:.4f}\")\n",
    "print(f\"  PMI bare:   {test_result['nll_bare'] - test_baseline:.4f}\")\n",
    "assert np.isfinite(test_result['nll_primed']), \"Primed NLL not finite!\"\n",
    "assert np.isfinite(test_result['nll_bare']), \"Bare NLL not finite!\"\n",
    "print(\"\\nSmoke test passed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cell-6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T21:14:12.825874Z",
     "iopub.status.busy": "2026-02-15T21:14:12.825555Z",
     "iopub.status.idle": "2026-02-15T21:37:50.038588Z",
     "shell.execute_reply": "2026-02-15T21:37:50.037853Z"
    },
    "papermill": {
     "duration": 1417.220923,
     "end_time": "2026-02-15T21:37:50.040719",
     "exception": false,
     "start_time": "2026-02-15T21:14:12.819796",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "MAIN EVALUATION (200 queries, ~1692 passages)\n",
      "Model: Gemma 3 4B | values_early_layers (layers 0-16)\n",
      "======================================================================\n",
      "Resuming from checkpoint: 60/200\n",
      "Evaluating queries 60 to 199\n",
      "Per passage: 2 forward passes (bare + primed) + 2 scoring passes\n",
      "Per query: +1 baseline scoring pass\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94bb6761f5bd463994db08eb85053c64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Queries:  30%|###       | 60/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 70/200 | 10 done in 1.7m | ETA: 21.8 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 80/200 | 20 done in 3.3m | ETA: 19.9 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 90/200 | 30 done in 5.0m | ETA: 18.3 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 100/200 | 40 done in 6.6m | ETA: 16.5 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 110/200 | 50 done in 8.4m | ETA: 15.1 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 120/200 | 60 done in 10.1m | ETA: 13.4 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 130/200 | 70 done in 11.8m | ETA: 11.8 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 140/200 | 80 done in 13.5m | ETA: 10.1 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 150/200 | 90 done in 15.1m | ETA: 8.4 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 160/200 | 100 done in 16.8m | ETA: 6.7 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 170/200 | 110 done in 18.5m | ETA: 5.0 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 180/200 | 120 done in 20.2m | ETA: 3.4 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 190/200 | 130 done in 21.9m | ETA: 1.7 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 200/200 | 140 done in 23.6m | ETA: 0.0 min\n",
      "\n",
      "Evaluation complete: 200 queries in 23.6 min\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Evaluation Loop\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"MAIN EVALUATION ({N} queries, ~{total_passages} passages)\")\n",
    "print(\"Model: Gemma 3 4B | values_early_layers (layers 0-16)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "sf_str = SURROGATE_PREFIX_TEMPLATE.format(surrogate=STATIC_FACT)\n",
    "\n",
    "# Checkpoint resume\n",
    "all_results = []\n",
    "start_idx = 0\n",
    "\n",
    "if CHECKPOINT_PATH.exists():\n",
    "    with open(CHECKPOINT_PATH, 'r') as f:\n",
    "        ckpt = json.load(f)\n",
    "    ckpt_queries = ckpt.get('query_texts', [])\n",
    "    current_queries = [q['query'] for q in queries]\n",
    "    if ckpt_queries == current_queries:\n",
    "        all_results = ckpt['results']\n",
    "        start_idx = len(all_results)\n",
    "        print(f\"Resuming from checkpoint: {start_idx}/{N}\")\n",
    "    else:\n",
    "        print(\"Checkpoint query mismatch. Starting fresh.\")\n",
    "else:\n",
    "    print(\"No checkpoint found. Starting fresh.\")\n",
    "\n",
    "print(f\"Evaluating queries {start_idx} to {N-1}\")\n",
    "print(f\"Per passage: 2 forward passes (bare + primed) + 2 scoring passes\")\n",
    "print(f\"Per query: +1 baseline scoring pass\")\n",
    "\n",
    "t_start = time.time()\n",
    "\n",
    "for qidx in tqdm(range(start_idx, N), initial=start_idx, total=N, desc=\"Queries\"):\n",
    "    query_data = queries[qidx]\n",
    "    query = query_data['query']\n",
    "    answer = query_data['answer']\n",
    "    query_prompt = QUERY_TEMPLATE.format(query=query)\n",
    "    answer_text = ANSWER_TEMPLATE.format(answer=answer)\n",
    "\n",
    "    # 1. Compute baseline NLL once per query\n",
    "    nll_baseline = score_baseline_nll(query_prompt, answer_text, model, tokenizer, exp_config)\n",
    "\n",
    "    # 2. Score each passage\n",
    "    passage_results = []\n",
    "    for pidx, pinfo in enumerate(query_data['passages']):\n",
    "        result = build_and_score_passage(\n",
    "            pinfo['passage'], sf_str, query_prompt, answer_text,\n",
    "            model, tokenizer, exp_config, LAYER_CUTOFF)\n",
    "\n",
    "        passage_results.append({\n",
    "            'passage_idx': pinfo['passage_idx'],\n",
    "            'is_relevant': pinfo['is_relevant'],\n",
    "            'word_count': pinfo['word_count'],\n",
    "            'doc_len': result['doc_len'],\n",
    "            'nll_primed': result['nll_primed'],\n",
    "            'nll_bare': result['nll_bare'],\n",
    "            'nll_baseline': nll_baseline,\n",
    "        })\n",
    "\n",
    "    all_results.append({\n",
    "        'query_idx': qidx,\n",
    "        'query': query,\n",
    "        'answer': answer,\n",
    "        'n_passages': len(passage_results),\n",
    "        'n_relevant': query_data['n_relevant'],\n",
    "        'nll_baseline': nll_baseline,\n",
    "        'passage_data': passage_results,\n",
    "    })\n",
    "\n",
    "    # 3. Checkpoint\n",
    "    if (qidx + 1) % CHECKPOINT_EVERY == 0 or qidx == N - 1:\n",
    "        ckpt_data = {\n",
    "            'results': all_results,\n",
    "            'query_texts': [q['query'] for q in queries],\n",
    "            'completed': len(all_results),\n",
    "            'total': N,\n",
    "            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        }\n",
    "        with open(CHECKPOINT_PATH, 'w') as f:\n",
    "            json.dump(ckpt_data, f)\n",
    "        elapsed = time.time() - t_start\n",
    "        n_done = qidx - start_idx + 1\n",
    "        rate = n_done / elapsed if elapsed > 0 else 0\n",
    "        remaining = (N - qidx - 1) / rate if rate > 0 else 0\n",
    "        tqdm.write(f\"  Checkpoint {qidx+1}/{N} | {n_done} done in {elapsed/60:.1f}m | \"\n",
    "                   f\"ETA: {remaining/60:.1f} min\")\n",
    "\n",
    "elapsed_total = time.time() - t_start\n",
    "print(f\"\\nEvaluation complete: {len(all_results)} queries in {elapsed_total/60:.1f} min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cell-7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T21:37:50.051740Z",
     "iopub.status.busy": "2026-02-15T21:37:50.051417Z",
     "iopub.status.idle": "2026-02-15T21:37:50.091066Z",
     "shell.execute_reply": "2026-02-15T21:37:50.090366Z"
    },
    "papermill": {
     "duration": 0.047195,
     "end_time": "2026-02-15T21:37:50.092613",
     "exception": false,
     "start_time": "2026-02-15T21:37:50.045418",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ANALYSIS \u2014 RANKING EVALUATION & PMI SCORING\n",
      "======================================================================\n",
      "\n",
      "Total passages: 1692\n",
      "Relevant: 221 (13.1%)\n",
      "Irrelevant: 1471 (86.9%)\n",
      "\n",
      "======================================================================\n",
      "METRIC 1: DIFFERENTIAL NLL (relevant vs irrelevant)\n",
      "======================================================================\n",
      "\n",
      "Method                 Mean Rel   Mean Irr       Diff        d\n",
      "--------------------------------------------------------------\n",
      "Raw bare NLL             0.5641     2.4836    +1.9195   +1.201\n",
      "Raw primed NLL           0.5398     2.4138    +1.8740   +1.228\n",
      "PMI bare                -2.2679    -0.3045    +1.9634   +1.647\n",
      "PMI primed              -2.2922    -0.3743    +1.9179   +1.588\n",
      "\n",
      "======================================================================\n",
      "METRIC 2: AUC-ROC\n",
      "======================================================================\n",
      "\n",
      "Method                    AUC\n",
      "------------------------------\n",
      "Raw bare NLL            0.828\n",
      "Raw primed NLL          0.829\n",
      "PMI bare                0.841\n",
      "PMI primed              0.832\n",
      "\n",
      "======================================================================\n",
      "METRIC 3: MRR@10\n",
      "======================================================================\n",
      "\n",
      "Method                 MRR@10\n",
      "------------------------------\n",
      "Raw bare NLL            0.860\n",
      "Raw primed NLL          0.853\n",
      "PMI bare                0.860\n",
      "PMI primed              0.853\n",
      "\n",
      "======================================================================\n",
      "SUMMARY TABLE\n",
      "======================================================================\n",
      "\n",
      "Method                    AUC   MRR@10   Diff NLL        d\n",
      "----------------------------------------------------------\n",
      "Raw bare NLL            0.828    0.860    +1.9195   +1.201\n",
      "Raw primed NLL          0.829    0.853    +1.8740   +1.228\n",
      "PMI bare                0.841    0.860    +1.9634   +1.647\n",
      "PMI primed              0.832    0.853    +1.9179   +1.588\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Analysis\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from scipy import stats\n",
    "from lib.analysis import cohens_d\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ANALYSIS \u2014 RANKING EVALUATION & PMI SCORING\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# --- Flatten passage-level data ---\n",
    "is_relevant_all = []\n",
    "nll_primed_all = []\n",
    "nll_bare_all = []\n",
    "nll_baseline_all = []\n",
    "\n",
    "for r in all_results:\n",
    "    for p in r['passage_data']:\n",
    "        is_relevant_all.append(int(p['is_relevant']))\n",
    "        nll_primed_all.append(p['nll_primed'])\n",
    "        nll_bare_all.append(p['nll_bare'])\n",
    "        nll_baseline_all.append(p['nll_baseline'])\n",
    "\n",
    "is_relevant = np.array(is_relevant_all)\n",
    "nll_primed = np.array(nll_primed_all)\n",
    "nll_bare = np.array(nll_bare_all)\n",
    "nll_baseline = np.array(nll_baseline_all)\n",
    "\n",
    "# Derived PMI scores\n",
    "score_pmi_primed = nll_primed - nll_baseline\n",
    "score_pmi_bare = nll_bare - nll_baseline\n",
    "\n",
    "n_total = len(is_relevant)\n",
    "n_rel = int(is_relevant.sum())\n",
    "n_irr = n_total - n_rel\n",
    "\n",
    "print(f\"\\nTotal passages: {n_total}\")\n",
    "print(f\"Relevant: {n_rel} ({100*n_rel/n_total:.1f}%)\")\n",
    "print(f\"Irrelevant: {n_irr} ({100*n_irr/n_total:.1f}%)\")\n",
    "\n",
    "# === Metric 1: Differential NLL ===\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"METRIC 1: DIFFERENTIAL NLL (relevant vs irrelevant)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "scoring_methods = {\n",
    "    'Raw bare NLL': nll_bare,\n",
    "    'Raw primed NLL': nll_primed,\n",
    "    'PMI bare': score_pmi_bare,\n",
    "    'PMI primed': score_pmi_primed,\n",
    "}\n",
    "\n",
    "print(f\"\\n{'Method':<20} {'Mean Rel':>10} {'Mean Irr':>10} {'Diff':>10} {'d':>8}\")\n",
    "print(\"-\" * 62)\n",
    "\n",
    "diff_nll_results = {}\n",
    "for name, scores in scoring_methods.items():\n",
    "    mean_rel = np.mean(scores[is_relevant == 1])\n",
    "    mean_irr = np.mean(scores[is_relevant == 0])\n",
    "    diff = mean_irr - mean_rel  # positive = relevant gets lower score\n",
    "    # Cohen's d: relevant vs irrelevant (two independent groups)\n",
    "    rel_vals = scores[is_relevant == 1]\n",
    "    irr_vals = scores[is_relevant == 0]\n",
    "    pooled_std = np.sqrt((np.var(rel_vals) * (len(rel_vals)-1) + np.var(irr_vals) * (len(irr_vals)-1)) /\n",
    "                         (len(rel_vals) + len(irr_vals) - 2))\n",
    "    d = diff / pooled_std if pooled_std > 0 else 0\n",
    "    t_stat, p_val = stats.ttest_ind(irr_vals, rel_vals)\n",
    "    print(f\"{name:<20} {mean_rel:>10.4f} {mean_irr:>10.4f} {diff:>+10.4f} {d:>+8.3f}\")\n",
    "    diff_nll_results[name] = {\n",
    "        'mean_relevant': float(mean_rel),\n",
    "        'mean_irrelevant': float(mean_irr),\n",
    "        'diff': float(diff),\n",
    "        'cohens_d': float(d),\n",
    "        't_stat': float(t_stat),\n",
    "        'p_value': float(p_val),\n",
    "    }\n",
    "\n",
    "# === Metric 2: AUC ===\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"METRIC 2: AUC-ROC\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# For NLL/PMI scores: lower = more relevant, so negate for AUC\n",
    "auc_results = {}\n",
    "print(f\"\\n{'Method':<20} {'AUC':>8}\")\n",
    "print(\"-\" * 30)\n",
    "for name, scores in scoring_methods.items():\n",
    "    auc = roc_auc_score(is_relevant, -scores)\n",
    "    print(f\"{name:<20} {auc:>8.3f}\")\n",
    "    auc_results[name] = float(auc)\n",
    "\n",
    "# === Metric 3: MRR@10 ===\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"METRIC 3: MRR@10\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "def compute_mrr_at_k(all_results, score_fn, k=10):\n",
    "    \"\"\"Compute MRR@k across queries.\n",
    "\n",
    "    score_fn: callable that takes passage_data dict -> score (lower = more relevant)\n",
    "    \"\"\"\n",
    "    rr_list = []\n",
    "    for r in all_results:\n",
    "        passages = r['passage_data']\n",
    "        scored = [(score_fn(p), p['is_relevant']) for p in passages]\n",
    "        # Sort by score ascending (lower = more relevant)\n",
    "        scored.sort(key=lambda x: x[0])\n",
    "        # Find rank of first relevant\n",
    "        rr = 0.0\n",
    "        for rank, (score, rel) in enumerate(scored[:k], 1):\n",
    "            if rel:\n",
    "                rr = 1.0 / rank\n",
    "                break\n",
    "        rr_list.append(rr)\n",
    "    return np.mean(rr_list), rr_list\n",
    "\n",
    "mrr_results = {}\n",
    "mrr_per_query = {}\n",
    "\n",
    "score_fns = {\n",
    "    'Raw bare NLL': lambda p: p['nll_bare'],\n",
    "    'Raw primed NLL': lambda p: p['nll_primed'],\n",
    "    'PMI bare': lambda p: p['nll_bare'] - p['nll_baseline'],\n",
    "    'PMI primed': lambda p: p['nll_primed'] - p['nll_baseline'],\n",
    "}\n",
    "\n",
    "print(f\"\\n{'Method':<20} {'MRR@10':>8}\")\n",
    "print(\"-\" * 30)\n",
    "for name, fn in score_fns.items():\n",
    "    mrr, rr_list = compute_mrr_at_k(all_results, fn, k=10)\n",
    "    print(f\"{name:<20} {mrr:>8.3f}\")\n",
    "    mrr_results[name] = float(mrr)\n",
    "    mrr_per_query[name] = rr_list\n",
    "\n",
    "# === Summary Table ===\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"SUMMARY TABLE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\n{'Method':<20} {'AUC':>8} {'MRR@10':>8} {'Diff NLL':>10} {'d':>8}\")\n",
    "print(\"-\" * 58)\n",
    "for name in scoring_methods:\n",
    "    print(f\"{name:<20} {auc_results[name]:>8.3f} {mrr_results[name]:>8.3f} \"\n",
    "          f\"{diff_nll_results[name]['diff']:>+10.4f} {diff_nll_results[name]['cohens_d']:>+8.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cell-8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T21:37:50.103709Z",
     "iopub.status.busy": "2026-02-15T21:37:50.103393Z",
     "iopub.status.idle": "2026-02-15T21:37:52.356745Z",
     "shell.execute_reply": "2026-02-15T21:37:52.356059Z"
    },
    "papermill": {
     "duration": 2.261008,
     "end_time": "2026-02-15T21:37:52.358382",
     "exception": false,
     "start_time": "2026-02-15T21:37:50.097374",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved combined figure\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saved roc_curves.png\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saved score_distributions.png\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saved mrr_scatter.png\n",
      "\n",
      "All figures saved to results/exp22/figures\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Figures & Summary\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 11))\n",
    "\n",
    "colors = {\n",
    "    'Raw bare NLL': '#1f77b4',\n",
    "    'Raw primed NLL': '#2ca02c',\n",
    "    'PMI bare': '#ff7f0e',\n",
    "    'PMI primed': '#d62728',\n",
    "}\n",
    "\n",
    "# --- Panel 1: ROC Curves ---\n",
    "ax = axes[0, 0]\n",
    "for name, scores in scoring_methods.items():\n",
    "    fpr, tpr, _ = roc_curve(is_relevant, -scores)\n",
    "    ax.plot(fpr, tpr, color=colors[name], linewidth=2,\n",
    "            label=f\"{name} (AUC={auc_results[name]:.3f})\")\n",
    "ax.plot([0, 1], [0, 1], 'k--', alpha=0.3, label='Random')\n",
    "ax.set_xlabel('False Positive Rate')\n",
    "ax.set_ylabel('True Positive Rate')\n",
    "ax.set_title('ROC Curves')\n",
    "ax.legend(fontsize=8, loc='lower right')\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "# --- Panel 2: Score Distributions (PMI) ---\n",
    "ax = axes[0, 1]\n",
    "pmi_data = [\n",
    "    score_pmi_bare[is_relevant == 1],\n",
    "    score_pmi_bare[is_relevant == 0],\n",
    "    score_pmi_primed[is_relevant == 1],\n",
    "    score_pmi_primed[is_relevant == 0],\n",
    "]\n",
    "pmi_labels = ['PMI bare\\nRelevant', 'PMI bare\\nIrrelevant',\n",
    "              'PMI primed\\nRelevant', 'PMI primed\\nIrrelevant']\n",
    "pmi_colors = ['#ff7f0e', '#ffbb78', '#d62728', '#ff9896']\n",
    "\n",
    "bp = ax.boxplot(pmi_data, labels=pmi_labels, patch_artist=True, widths=0.6)\n",
    "for patch, color in zip(bp['boxes'], pmi_colors):\n",
    "    patch.set_facecolor(color)\n",
    "    patch.set_alpha(0.7)\n",
    "ax.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "ax.set_ylabel('PMI Score (lower = more helpful)')\n",
    "ax.set_title('PMI Score Distributions')\n",
    "ax.tick_params(axis='x', labelsize=8)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# --- Panel 3: Per-query MRR Scatter (primed vs bare) ---\n",
    "ax = axes[1, 0]\n",
    "rr_bare = np.array(mrr_per_query['PMI bare'])\n",
    "rr_primed = np.array(mrr_per_query['PMI primed'])\n",
    "\n",
    "# Jitter for visibility\n",
    "jitter = np.random.RandomState(42).normal(0, 0.015, size=len(rr_bare))\n",
    "ax.scatter(rr_bare + jitter, rr_primed + jitter, alpha=0.4, s=20, c='#9467bd', edgecolors='none')\n",
    "ax.plot([0, 1], [0, 1], 'k--', alpha=0.3, label='y=x')\n",
    "ax.set_xlabel('MRR (PMI bare)')\n",
    "ax.set_ylabel('MRR (PMI primed)')\n",
    "ax.set_title('Per-query MRR: PMI Primed vs PMI Bare')\n",
    "ax.set_xlim(-0.05, 1.05)\n",
    "ax.set_ylim(-0.05, 1.05)\n",
    "ax.legend(fontsize=8)\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "# Wins/ties/losses\n",
    "wins = int(np.sum(rr_primed > rr_bare))\n",
    "ties = int(np.sum(rr_primed == rr_bare))\n",
    "losses = int(np.sum(rr_primed < rr_bare))\n",
    "ax.text(0.05, 0.95, f'Primed wins: {wins}\\nTies: {ties}\\nBare wins: {losses}',\n",
    "        transform=ax.transAxes, fontsize=9, va='top',\n",
    "        bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "# --- Panel 4: Summary Bar Chart ---\n",
    "ax = axes[1, 1]\n",
    "methods = list(scoring_methods.keys())\n",
    "aucs = [auc_results[m] for m in methods]\n",
    "mrrs = [mrr_results[m] for m in methods]\n",
    "\n",
    "x = np.arange(len(methods))\n",
    "width = 0.35\n",
    "bars1 = ax.bar(x - width/2, aucs, width, label='AUC', color=[colors[m] for m in methods], alpha=0.7)\n",
    "bars2 = ax.bar(x + width/2, mrrs, width, label='MRR@10', color=[colors[m] for m in methods], alpha=0.4,\n",
    "               edgecolor=[colors[m] for m in methods], linewidth=2)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([m.replace(' ', '\\n') for m in methods], fontsize=8)\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_title('AUC & MRR@10 Comparison')\n",
    "ax.legend(fontsize=9)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.axhline(y=0.5, color='gray', linestyle='--', alpha=0.3, label='Random')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "for bar in bars1:\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "            f'{bar.get_height():.3f}', ha='center', va='bottom', fontsize=7)\n",
    "for bar in bars2:\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "            f'{bar.get_height():.3f}', ha='center', va='bottom', fontsize=7)\n",
    "\n",
    "plt.suptitle('Exp 22: Ranking Evaluation & Contrastive Scoring (PMI)', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save individual figures\n",
    "fig.savefig(RESULTS_DIR / 'figures' / 'roc_curves.png', dpi=150, bbox_inches='tight')\n",
    "print(f\"Saved combined figure\")\n",
    "\n",
    "# Also save individual panels\n",
    "for idx, (panel_name, panel_ax) in enumerate([\n",
    "    ('roc_curves', axes[0, 0]),\n",
    "    ('score_distributions', axes[0, 1]),\n",
    "    ('mrr_scatter', axes[1, 0]),\n",
    "]):\n",
    "    fig_single, ax_single = plt.subplots(figsize=(7, 5.5))\n",
    "    # Re-draw individual panels\n",
    "    if panel_name == 'roc_curves':\n",
    "        for name, scores in scoring_methods.items():\n",
    "            fpr, tpr, _ = roc_curve(is_relevant, -scores)\n",
    "            ax_single.plot(fpr, tpr, color=colors[name], linewidth=2,\n",
    "                           label=f\"{name} (AUC={auc_results[name]:.3f})\")\n",
    "        ax_single.plot([0, 1], [0, 1], 'k--', alpha=0.3, label='Random')\n",
    "        ax_single.set_xlabel('False Positive Rate')\n",
    "        ax_single.set_ylabel('True Positive Rate')\n",
    "        ax_single.set_title('ROC Curves \u2014 Ranking by NLL / PMI')\n",
    "        ax_single.legend(fontsize=9)\n",
    "        ax_single.grid(alpha=0.3)\n",
    "    elif panel_name == 'score_distributions':\n",
    "        bp2 = ax_single.boxplot(pmi_data, labels=pmi_labels, patch_artist=True, widths=0.6)\n",
    "        for patch, color in zip(bp2['boxes'], pmi_colors):\n",
    "            patch.set_facecolor(color)\n",
    "            patch.set_alpha(0.7)\n",
    "        ax_single.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "        ax_single.set_ylabel('PMI Score (lower = more helpful)')\n",
    "        ax_single.set_title('PMI Score Distributions: Relevant vs Irrelevant')\n",
    "        ax_single.grid(axis='y', alpha=0.3)\n",
    "    elif panel_name == 'mrr_scatter':\n",
    "        ax_single.scatter(rr_bare + jitter, rr_primed + jitter, alpha=0.4, s=25,\n",
    "                          c='#9467bd', edgecolors='none')\n",
    "        ax_single.plot([0, 1], [0, 1], 'k--', alpha=0.3, label='y=x')\n",
    "        ax_single.set_xlabel('Reciprocal Rank (PMI bare)')\n",
    "        ax_single.set_ylabel('Reciprocal Rank (PMI primed)')\n",
    "        ax_single.set_title('Per-query MRR: PMI Primed vs PMI Bare')\n",
    "        ax_single.set_xlim(-0.05, 1.05)\n",
    "        ax_single.set_ylim(-0.05, 1.05)\n",
    "        ax_single.legend(fontsize=9)\n",
    "        ax_single.grid(alpha=0.3)\n",
    "        ax_single.text(0.05, 0.95, f'Primed wins: {wins}\\nTies: {ties}\\nBare wins: {losses}',\n",
    "                        transform=ax_single.transAxes, fontsize=10, va='top',\n",
    "                        bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "    fig_single.tight_layout()\n",
    "    fig_single.savefig(RESULTS_DIR / 'figures' / f'{panel_name}.png', dpi=150, bbox_inches='tight')\n",
    "    plt.close(fig_single)\n",
    "    print(f\"  Saved {panel_name}.png\")\n",
    "\n",
    "plt.show()\n",
    "print(f\"\\nAll figures saved to {RESULTS_DIR / 'figures'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cell-9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T21:37:52.370864Z",
     "iopub.status.busy": "2026-02-15T21:37:52.370308Z",
     "iopub.status.idle": "2026-02-15T21:37:52.427228Z",
     "shell.execute_reply": "2026-02-15T21:37:52.426551Z"
    },
    "papermill": {
     "duration": 0.064628,
     "end_time": "2026-02-15T21:37:52.428667",
     "exception": false,
     "start_time": "2026-02-15T21:37:52.364039",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV saved to results/exp22/passage_scores.csv (1692 rows)\n",
      "JSON saved to results/exp22/results.json (488.6 KB)\n",
      "\n",
      "======================================================================\n",
      "FINAL SUMMARY \u2014 Exp 22\n",
      "======================================================================\n",
      "Model: Gemma 3 4B | Method: values_early_layers (layers 0-16)\n",
      "Dataset: MS MARCO v1.1 (200 queries, 1692 passages)\n",
      "\n",
      "Method                    AUC   MRR@10   Diff NLL        d\n",
      "----------------------------------------------------------\n",
      "Raw bare NLL            0.828    0.860    +1.9195   +1.201\n",
      "Raw primed NLL          0.829    0.853    +1.8740   +1.228\n",
      "PMI bare                0.841    0.860    +1.9634   +1.647\n",
      "PMI primed              0.832    0.853    +1.9179   +1.588\n",
      "\n",
      "Primed vs Bare MRR (PMI): 6 wins / 185 ties / 9 losses\n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Save CSV + JSON\n",
    "import csv\n",
    "\n",
    "# --- CSV: per-passage scores ---\n",
    "csv_rows = []\n",
    "for r in all_results:\n",
    "    for p in r['passage_data']:\n",
    "        csv_rows.append({\n",
    "            'query_idx': r['query_idx'],\n",
    "            'passage_idx': p['passage_idx'],\n",
    "            'is_relevant': int(p['is_relevant']),\n",
    "            'nll_primed': p['nll_primed'],\n",
    "            'nll_bare': p['nll_bare'],\n",
    "            'nll_baseline': p['nll_baseline'],\n",
    "            'score_pmi_primed': p['nll_primed'] - p['nll_baseline'],\n",
    "            'score_pmi_bare': p['nll_bare'] - p['nll_baseline'],\n",
    "        })\n",
    "\n",
    "with open(CSV_PATH, 'w', newline='') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=csv_rows[0].keys())\n",
    "    writer.writeheader()\n",
    "    writer.writerows(csv_rows)\n",
    "print(f\"CSV saved to {CSV_PATH} ({len(csv_rows)} rows)\")\n",
    "\n",
    "# --- JSON: full results ---\n",
    "final = {\n",
    "    'experiment': 'exp22_ranking_and_pmi',\n",
    "    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'config': {\n",
    "        'model_name': MODEL_NAME,\n",
    "        'model_type': 'gemma3',\n",
    "        'seed': SEED,\n",
    "        'n_queries': N,\n",
    "        'total_passages': n_total,\n",
    "        'n_relevant': n_rel,\n",
    "        'n_irrelevant': n_irr,\n",
    "        'layer_cutoff': LAYER_CUTOFF,\n",
    "        'max_passage_words': MAX_PASSAGE_WORDS,\n",
    "        'dataset': 'MS MARCO v1.1 validation',\n",
    "        'method': 'values_early_layers (layers 0-16)',\n",
    "        'static_fact': STATIC_FACT,\n",
    "    },\n",
    "    'analysis': {\n",
    "        'differential_nll': diff_nll_results,\n",
    "        'auc': auc_results,\n",
    "        'mrr_at_10': mrr_results,\n",
    "        'mrr_per_query': {name: [float(x) for x in rr] for name, rr in mrr_per_query.items()},\n",
    "        'primed_vs_bare_mrr': {\n",
    "            'primed_wins': wins,\n",
    "            'ties': ties,\n",
    "            'bare_wins': losses,\n",
    "        },\n",
    "    },\n",
    "    'per_query_results': all_results,\n",
    "}\n",
    "\n",
    "with open(FINAL_RESULTS_PATH, 'w') as f:\n",
    "    json.dump(final, f, indent=2)\n",
    "print(f\"JSON saved to {FINAL_RESULTS_PATH} ({FINAL_RESULTS_PATH.stat().st_size / 1024:.1f} KB)\")\n",
    "\n",
    "# --- Final Summary ---\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"FINAL SUMMARY \u2014 Exp 22\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Model: Gemma 3 4B | Method: values_early_layers (layers 0-{LAYER_CUTOFF-1})\")\n",
    "print(f\"Dataset: MS MARCO v1.1 ({N} queries, {n_total} passages)\")\n",
    "print(f\"\")\n",
    "print(f\"{'Method':<20} {'AUC':>8} {'MRR@10':>8} {'Diff NLL':>10} {'d':>8}\")\n",
    "print(\"-\" * 58)\n",
    "for name in scoring_methods:\n",
    "    print(f\"{name:<20} {auc_results[name]:>8.3f} {mrr_results[name]:>8.3f} \"\n",
    "          f\"{diff_nll_results[name]['diff']:>+10.4f} {diff_nll_results[name]['cohens_d']:>+8.3f}\")\n",
    "print(f\"\\nPrimed vs Bare MRR (PMI): {wins} wins / {ties} ties / {losses} losses\")\n",
    "print(\"\\nDone!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1445.725119,
   "end_time": "2026-02-15T21:37:56.009822",
   "environment_variables": {},
   "exception": null,
   "input_path": "22_ranking_and_pmi.ipynb",
   "output_path": "22_ranking_and_pmi_executed.ipynb",
   "parameters": {},
   "start_time": "2026-02-15T21:13:50.284703",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "03d84b3484d04345aa098e515547a64d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "0509e93a51e74e33b052db81e19765a6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_077b84a262254b0da24ae8afb903501e",
       "placeholder": "\u200b",
       "style": "IPY_MODEL_7e73ebffb76c4127bd98386255a8bc06",
       "tabbable": null,
       "tooltip": null,
       "value": "\u2007883/883\u2007[00:03&lt;00:00,\u2007842.45it/s,\u2007Materializing\u2007param=model.vision_tower.vision_model.post_layernorm.weight]"
      }
     },
     "077b84a262254b0da24ae8afb903501e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "0c02d11f865e4e2aab67d4e62e1ff329": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_291fb4d0dd204c79972090123e49bdd2",
        "IPY_MODEL_1ce91cfd5c0b4f8da8dae75b16337794",
        "IPY_MODEL_0509e93a51e74e33b052db81e19765a6"
       ],
       "layout": "IPY_MODEL_f9e387b5a4b74117957c085430bd5f26",
       "tabbable": null,
       "tooltip": null
      }
     },
     "13306f862f754b9ca43cb70b678c82a1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "1ce91cfd5c0b4f8da8dae75b16337794": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_ffc04e383e844db99323e4919899958e",
       "max": 883.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_ffd3adb9d1e140fabae89c93096c5202",
       "tabbable": null,
       "tooltip": null,
       "value": 883.0
      }
     },
     "21611407a3f44a0db2205b36e9b94839": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "291fb4d0dd204c79972090123e49bdd2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_13306f862f754b9ca43cb70b678c82a1",
       "placeholder": "\u200b",
       "style": "IPY_MODEL_03d84b3484d04345aa098e515547a64d",
       "tabbable": null,
       "tooltip": null,
       "value": "Loading\u2007weights:\u2007100%"
      }
     },
     "44c6d71c46e84b3699d701271e22c705": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "538d3463591d4f0babb8a5881ed6ceeb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "6921cee519c34419a48dc2c0c4582911": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "danger",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_e51e2aa12d09475c91f550357fcf71de",
       "max": 10047.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_794ef50635ec4ff89397ed822abe36b9",
       "tabbable": null,
       "tooltip": null,
       "value": 618.0
      }
     },
     "76d452f7b819415c97cfe9f9ef461c28": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "794ef50635ec4ff89397ed822abe36b9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "7ca2aa6f28534712b86a5a88a160c317": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "7e73ebffb76c4127bd98386255a8bc06": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "84b22f9f90e4470990f2a300cc4629ed": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_e6f20b5dd40f4a298b04388a9cb6b6e6",
       "max": 200.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_be25d3bcd2a04424b3402bc60e8c8ada",
       "tabbable": null,
       "tooltip": null,
       "value": 200.0
      }
     },
     "90b4454fae414486b86c91d99577693a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_7ca2aa6f28534712b86a5a88a160c317",
       "placeholder": "\u200b",
       "style": "IPY_MODEL_93c30d40e6b543d497e8090a09ba8283",
       "tabbable": null,
       "tooltip": null,
       "value": "Filtering:\u2007\u2007\u20076%"
      }
     },
     "93c30d40e6b543d497e8090a09ba8283": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "94bb6761f5bd463994db08eb85053c64": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_98fb063c3635482088413a3413807124",
        "IPY_MODEL_84b22f9f90e4470990f2a300cc4629ed",
        "IPY_MODEL_c2fea4599c5942928fa7581168f40500"
       ],
       "layout": "IPY_MODEL_76d452f7b819415c97cfe9f9ef461c28",
       "tabbable": null,
       "tooltip": null
      }
     },
     "96582fbc443b4308bca364a79f84b913": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "98fb063c3635482088413a3413807124": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_44c6d71c46e84b3699d701271e22c705",
       "placeholder": "\u200b",
       "style": "IPY_MODEL_96582fbc443b4308bca364a79f84b913",
       "tabbable": null,
       "tooltip": null,
       "value": "Queries:\u2007100%"
      }
     },
     "be25d3bcd2a04424b3402bc60e8c8ada": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "c1fb1528f82648cd9c6c0b5b086b763d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c201cc9fce8341aa941715748a513cb3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c2fea4599c5942928fa7581168f40500": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_f0701410456d4f9d9872a8dc5afaf1bb",
       "placeholder": "\u200b",
       "style": "IPY_MODEL_538d3463591d4f0babb8a5881ed6ceeb",
       "tabbable": null,
       "tooltip": null,
       "value": "\u2007200/200\u2007[23:37&lt;00:00,\u200710.04s/it]"
      }
     },
     "ca3b6caa7e504037be5f0ad5e726bf47": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_c201cc9fce8341aa941715748a513cb3",
       "placeholder": "\u200b",
       "style": "IPY_MODEL_21611407a3f44a0db2205b36e9b94839",
       "tabbable": null,
       "tooltip": null,
       "value": "\u2007618/10047\u2007[00:00&lt;00:01,\u20075814.24it/s]"
      }
     },
     "dd3be5cae5a04ef6a228bd9f243b4146": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_90b4454fae414486b86c91d99577693a",
        "IPY_MODEL_6921cee519c34419a48dc2c0c4582911",
        "IPY_MODEL_ca3b6caa7e504037be5f0ad5e726bf47"
       ],
       "layout": "IPY_MODEL_c1fb1528f82648cd9c6c0b5b086b763d",
       "tabbable": null,
       "tooltip": null
      }
     },
     "e51e2aa12d09475c91f550357fcf71de": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e6f20b5dd40f4a298b04388a9cb6b6e6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f0701410456d4f9d9872a8dc5afaf1bb": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f9e387b5a4b74117957c085430bd5f26": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ffc04e383e844db99323e4919899958e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ffd3adb9d1e140fabae89c93096c5202": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}