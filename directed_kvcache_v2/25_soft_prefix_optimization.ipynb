{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Exp 25: Layer-Selective Soft Surrogates for Gemma 3\n",
    "\n",
    "## Motivation\n",
    "\n",
    "Experiments 16, 19, and 24 proved that **value contamination works on Gemma 3 4B**, but\n",
    "strictly requires stripping out the primed keys and restricting the primed values to early\n",
    "layers (0-15, yielding d=**+0.227** in Exp 21). Full-cache priming fails due to late-layer\n",
    "key interference.\n",
    "\n",
    "The discrete prefix `\"What are the key facts I need to know?\"` (static_fact) was chosen\n",
    "by hand. **Can we do better by learning an optimal continuous prefix?**\n",
    "\n",
    "## Core Question\n",
    "\n",
    "Can we learn a sequence of continuous embedding vectors (a \"Soft Prompt\") that maximizes\n",
    "document value contamination for factoid QA, beating the discrete static_fact prefix when\n",
    "applied via Gemma's required layer-selective hybrid cache?\n",
    "\n",
    "## Theoretical Mechanism\n",
    "\n",
    "We combine **Soft Prompt Tuning** with the **values_early_layers** mechanism. The model\n",
    "remains completely frozen; only the soft prefix embeddings are updated. The computational\n",
    "graph flows backward from the generated answer's loss, through the hybrid cache splice,\n",
    "and into the soft prefix embeddings.\n",
    "\n",
    "## Design\n",
    "\n",
    "| Part | Phase | Data | N | Description |\n",
    "|------|-------|------|---|-------------|\n",
    "| 1 | Train | MS MARCO train | 2000 | Learn soft_prefix_embeddings via gradient descent |\n",
    "| 2 | Eval | MS MARCO val | 300 | Compare 4 conditions against Exp 21 baselines |\n",
    "\n",
    "### Training (Part 1)\n",
    "- **Trainable params**: `soft_prefix_embeddings` of shape `(prefix_len, hidden_size)`\n",
    "  where `prefix_len = 7` (matching static_fact token count)\n",
    "- **Two init conditions**: random (N(0, 0.02)) and static_fact-initialized\n",
    "- **Loss**: Mean NLL of answer tokens scored through the hybrid cache\n",
    "- **Optimizer**: AdamW, lr=0.1 (standard for soft prompt tuning)\n",
    "- **Epochs**: 3 passes over 2000 training samples\n",
    "\n",
    "### Evaluation Conditions (Part 2)\n",
    "\n",
    "| Condition | Keys | Values (L0-15) | Values (L16-33) |\n",
    "|-----------|------|----------------|------------------|\n",
    "| bare | bare | bare | bare |\n",
    "| vel_static | bare | static_fact primed | bare |\n",
    "| vel_soft_random | bare | soft (random init) primed | bare |\n",
    "| vel_soft_fact | bare | soft (fact init) primed | bare |\n",
    "\n",
    "## Reference Values\n",
    "\n",
    "| Source | Condition | d |\n",
    "|--------|-----------|---|\n",
    "| Exp 19 | values_only (all 34 layers) | +0.056 |\n",
    "| Exp 19 | values_early_layers (0-16) | +0.211 |\n",
    "| Exp 21 | values_early_layers (0-15) | +0.227 |\n",
    "| Exp 24 | static_fact @ cutoff=16 | ~+0.21 |\n",
    "\n",
    "## Success Criteria\n",
    "\n",
    "- **Primary**: Does `vel_soft_fact` achieve Cohen's d > +0.25 (beating discrete +0.227)?\n",
    "- **Secondary**: Does `vel_soft_random` learn anything useful from scratch (d > +0.10)?\n",
    "- **Diagnostic**: Track training loss curves for convergence validation\n",
    "\n",
    "## Technical Watch-Outs\n",
    "\n",
    "1. **Memory**: Backprop through full LLM attention requires large activation maps.\n",
    "   Batch size 1 + gradient accumulation mandatory.\n",
    "2. **RoPE differentiability**: `correct_rope_positions_with_bos` uses in-place ops.\n",
    "   We must reimplement the hybrid splice with pure functional ops for the training loop.\n",
    "3. **4-bit model**: BitsAndBytes 4-bit models may not support gradient computation through\n",
    "   the full forward pass. We use `inputs_embeds` to bypass the embedding lookup and let\n",
    "   gradients flow through the soft prefix only.\n",
    "4. **Learning rate**: Soft prompts typically need lr=0.1-0.3 (much higher than fine-tuning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": "# Cell 1: Setup\nimport os\nos.umask(0o000)\n\nimport sys\nimport json\nimport time\nimport numpy as np\nimport torch\nimport gc\nfrom pathlib import Path\n\nSEED = 42\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed_all(SEED)\nnp.random.seed(SEED)\n\nRESULTS_DIR = Path(\"results/exp25\")\nRESULTS_DIR.mkdir(parents=True, exist_ok=True)\n\nCHECKPOINT_TRAIN_RAND_PATH = RESULTS_DIR / \"checkpoint_train_random.json\"\nCHECKPOINT_TRAIN_FACT_PATH = RESULTS_DIR / \"checkpoint_train_fact.json\"\nCHECKPOINT_EVAL_PATH = RESULTS_DIR / \"checkpoint_eval.json\"\nFINAL_RESULTS_PATH = RESULTS_DIR / \"results.json\"\nCSV_EVAL_PATH = RESULTS_DIR / \"eval_results.csv\"\nSOFT_RANDOM_PATH = RESULTS_DIR / \"soft_prefix_random.pt\"\nSOFT_FACT_PATH = RESULTS_DIR / \"soft_prefix_fact.pt\"\n\nprint(f\"SEED: {SEED}\")\nprint(f\"Results directory: {RESULTS_DIR}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Load Gemma 3 4B via load_model()\n",
    "sys.path.insert(0, \".\")\n",
    "\n",
    "from lib.config import ExperimentConfig\n",
    "from lib.model_utils import load_model\n",
    "\n",
    "MODEL_NAME = \"google/gemma-3-4b-it\"\n",
    "\n",
    "exp_config = ExperimentConfig(\n",
    "    model_name=MODEL_NAME,\n",
    "    model_type=\"gemma3\",\n",
    "    compute_dtype=\"auto\",  # resolves to bfloat16 for gemma3\n",
    "    use_4bit=True,\n",
    "    num_samples=2000,\n",
    "    seed=SEED,\n",
    ")\n",
    "\n",
    "print(f\"Loading {MODEL_NAME} (4-bit, bfloat16)...\")\n",
    "model, tokenizer = load_model(exp_config)\n",
    "\n",
    "# Architecture diagnostics\n",
    "from lib.kv_cache import (\n",
    "    _get_text_config, _get_head_dim, _get_rope_theta_for_layer,\n",
    "    _get_cache_keys, _get_cache_values, _set_cache_keys, _set_cache_values,\n",
    "    _ensure_dynamic_cache, extract_and_truncate_cache_with_bos,\n",
    "    correct_rope_positions_with_bos, score_answer_with_cache,\n",
    "    deepcopy_cache, replace_values_at_layers,\n",
    ")\n",
    "\n",
    "text_config = _get_text_config(model.config)\n",
    "NUM_LAYERS = text_config.num_hidden_layers\n",
    "HIDDEN_SIZE = text_config.hidden_size\n",
    "HEAD_DIM = _get_head_dim(model.config)\n",
    "\n",
    "print(f\"\\nModel loaded successfully.\")\n",
    "print(f\"  Num layers: {NUM_LAYERS}\")\n",
    "print(f\"  Hidden size: {HIDDEN_SIZE}\")\n",
    "print(f\"  Head dim: {HEAD_DIM}\")\n",
    "print(f\"  Num KV heads: {text_config.num_key_value_heads}\")\n",
    "print(f\"  BOS token ID: {tokenizer.bos_token_id}\")\n",
    "\n",
    "# Verify dtype\n",
    "sample_ids = tokenizer(\"test\", return_tensors=\"pt\")['input_ids'].to(exp_config.device)\n",
    "with torch.no_grad():\n",
    "    out = model(sample_ids, use_cache=True)\n",
    "    cache_check = _ensure_dynamic_cache(out.past_key_values)\n",
    "    k0 = _get_cache_keys(cache_check, 0)\n",
    "    print(f\"  Cache key dtype: {k0.dtype}\")\n",
    "    print(f\"  Cache key shape: {k0.shape}\")\n",
    "del out, sample_ids, cache_check\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Constants, templates, imports\n",
    "from lib.analysis import cohens_d\n",
    "from lib.data import count_words\n",
    "from lib.surrogate import STATIC_SURROGATE_QUERIES\n",
    "from scipy import stats\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Templates\n",
    "SURROGATE_PREFIX_TEMPLATE = \"{surrogate}\\n\"\n",
    "DOCUMENT_TEMPLATE = \"{document}\"\n",
    "QUERY_TEMPLATE = \"\\nQuery: {query}\\nAnswer:\"\n",
    "ANSWER_TEMPLATE = \" {answer}\"\n",
    "\n",
    "STATIC_FACT = STATIC_SURROGATE_QUERIES['static_factual']['query']\n",
    "\n",
    "# Experiment parameters\n",
    "N_TRAIN = 2000\n",
    "N_EVAL = 300\n",
    "MAX_PASSAGE_WORDS = 300\n",
    "CUTOFF = 16          # layers 0-15\n",
    "PREFIX_LEN = 7       # match static_fact token count\n",
    "CHECKPOINT_EVERY = 50\n",
    "\n",
    "# Training hyperparameters\n",
    "LR = 0.1\n",
    "N_EPOCHS = 3\n",
    "GRAD_ACCUM_STEPS = 4  # effective batch size = 4\n",
    "WARMUP_STEPS = 50\n",
    "\n",
    "# Reference values\n",
    "EXP19_REF = {'values_only_d': 0.056, 'values_early_layers_d': 0.211}\n",
    "EXP21_REF = {'values_early_layers_d': 0.227}\n",
    "\n",
    "print(\"Config ready\")\n",
    "print(f\"  Model: {MODEL_NAME}\")\n",
    "print(f\"  Num layers: {NUM_LAYERS}, hidden_size: {HIDDEN_SIZE}\")\n",
    "print(f\"  Cutoff: {CUTOFF} (layers 0-{CUTOFF-1})\")\n",
    "print(f\"  Soft prefix length: {PREFIX_LEN} tokens\")\n",
    "print(f\"  Trainable params: {PREFIX_LEN * HIDDEN_SIZE:,} ({PREFIX_LEN} x {HIDDEN_SIZE})\")\n",
    "print(f\"  Training: {N_TRAIN} samples x {N_EPOCHS} epochs, lr={LR}, grad_accum={GRAD_ACCUM_STEPS}\")\n",
    "print(f\"  Eval: {N_EVAL} samples, 4 conditions\")\n",
    "print(f\"  Static fact prefix: '{STATIC_FACT}'\")\n",
    "print(f\"\\nReference values:\")\n",
    "print(f\"  Exp 19 values_early_layers: d={EXP19_REF['values_early_layers_d']:+.3f}\")\n",
    "print(f\"  Exp 21 values_early_layers: d={EXP21_REF['values_early_layers_d']:+.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Load MS MARCO training + validation splits\n",
    "from datasets import load_dataset\n",
    "\n",
    "def load_marco_split(split_name, n_samples, seed):\n",
    "    \"\"\"Load MS MARCO samples with positive passages.\"\"\"\n",
    "    dataset = load_dataset(\"microsoft/ms_marco\", \"v1.1\", split=split_name,\n",
    "                           trust_remote_code=True)\n",
    "    print(f\"Total items in {split_name}: {len(dataset)}\")\n",
    "\n",
    "    samples = []\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    for item in tqdm(dataset, desc=f\"Filtering {split_name}\"):\n",
    "        passages_info = item.get('passages', {})\n",
    "        passage_texts = passages_info.get('passage_text', [])\n",
    "        is_selected = passages_info.get('is_selected', [])\n",
    "        query = item.get('query', '')\n",
    "        answers = item.get('answers', [])\n",
    "        well_formed = item.get('wellFormedAnswers', [])\n",
    "\n",
    "        if not passage_texts or not query:\n",
    "            continue\n",
    "\n",
    "        answer = None\n",
    "        if well_formed and len(well_formed) > 0 and well_formed[0] != '[]':\n",
    "            answer = well_formed[0]\n",
    "        elif answers and len(answers) > 0 and answers[0] != 'No Answer Present.':\n",
    "            answer = answers[0]\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        for ptext, sel in zip(passage_texts, is_selected):\n",
    "            if sel == 1 and count_words(ptext) <= MAX_PASSAGE_WORDS:\n",
    "                samples.append({\n",
    "                    'query': query,\n",
    "                    'answer': answer,\n",
    "                    'passage': ptext,\n",
    "                    'word_count': count_words(ptext),\n",
    "                })\n",
    "                break\n",
    "\n",
    "        if len(samples) >= n_samples * 3:\n",
    "            break\n",
    "\n",
    "    np.random.shuffle(samples)\n",
    "    samples = samples[:n_samples]\n",
    "    del dataset\n",
    "    gc.collect()\n",
    "    return samples\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"LOADING MS MARCO v1.1 — TRAINING SPLIT\")\n",
    "print(\"=\" * 70)\n",
    "train_samples = load_marco_split(\"train\", N_TRAIN, SEED)\n",
    "print(f\"Selected {len(train_samples)} training samples\")\n",
    "print(f\"Word counts: mean={np.mean([q['word_count'] for q in train_samples]):.0f}, \"\n",
    "      f\"min={min(q['word_count'] for q in train_samples)}, \"\n",
    "      f\"max={max(q['word_count'] for q in train_samples)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"LOADING MS MARCO v1.1 — VALIDATION SPLIT\")\n",
    "print(\"=\" * 70)\n",
    "eval_samples = load_marco_split(\"validation\", N_EVAL, SEED + 1)\n",
    "print(f\"Selected {len(eval_samples)} eval samples\")\n",
    "print(f\"Word counts: mean={np.mean([q['word_count'] for q in eval_samples]):.0f}, \"\n",
    "      f\"min={min(q['word_count'] for q in eval_samples)}, \"\n",
    "      f\"max={max(q['word_count'] for q in eval_samples)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Tokenize static_fact prefix + BPE boundary check\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PREFIX TOKENIZATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "sf_str = SURROGATE_PREFIX_TEMPLATE.format(surrogate=STATIC_FACT)\n",
    "sf_ids = tokenizer(sf_str, return_tensors=\"pt\",\n",
    "                    add_special_tokens=False)['input_ids'].to(exp_config.device)\n",
    "SF_TOKEN_LEN = sf_ids.shape[1]\n",
    "\n",
    "print(f\"Static fact prefix: '{STATIC_FACT}'\")\n",
    "print(f\"  Formatted: '{sf_str.strip()}'\")\n",
    "print(f\"  Token length: {SF_TOKEN_LEN}\")\n",
    "print(f\"  Soft prefix length: {PREFIX_LEN} (matches: {PREFIX_LEN == SF_TOKEN_LEN})\")\n",
    "\n",
    "# If mismatch, update PREFIX_LEN to match\n",
    "if PREFIX_LEN != SF_TOKEN_LEN:\n",
    "    print(f\"  WARNING: Updating PREFIX_LEN from {PREFIX_LEN} to {SF_TOKEN_LEN}\")\n",
    "    PREFIX_LEN = SF_TOKEN_LEN\n",
    "\n",
    "# BPE boundary check\n",
    "example_doc = train_samples[0]['passage']\n",
    "concat = sf_str + DOCUMENT_TEMPLATE.format(document=example_doc)\n",
    "concat_enc = tokenizer(concat, add_special_tokens=True)['input_ids']\n",
    "prefix_enc = tokenizer(sf_str, add_special_tokens=True)['input_ids']\n",
    "doc_ids_from_concat = concat_enc[len(prefix_enc):]\n",
    "bare_doc_enc = tokenizer(DOCUMENT_TEMPLATE.format(document=example_doc),\n",
    "                          add_special_tokens=False)['input_ids']\n",
    "match = sum(1 for a, b in zip(doc_ids_from_concat, bare_doc_enc) if a == b)\n",
    "total = max(len(bare_doc_enc), 1)\n",
    "print(f\"\\nBPE boundary check: {match}/{total} tokens match ({100*match/total:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Explain experimental conditions\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"EXPERIMENTAL CONDITIONS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\n### Part 1: Training the Soft Prefix ###\")\n",
    "print(f\"  Data: {N_TRAIN} MS MARCO training queries\")\n",
    "print(f\"  Trainable: soft_prefix_embeddings ({PREFIX_LEN} x {HIDDEN_SIZE} = {PREFIX_LEN * HIDDEN_SIZE:,} params)\")\n",
    "print(f\"  Two runs: random init (N(0, 0.02)) and static_fact init\")\n",
    "print(f\"  Epochs: {N_EPOCHS}, lr={LR}, grad_accum={GRAD_ACCUM_STEPS}\")\n",
    "print()\n",
    "print(\"  Per training step:\")\n",
    "print(\"    1. Build inputs_embeds = [BOS_emb] + [soft_prefix (grad)] + [doc_embs (detached)]\")\n",
    "print(\"    2. Forward pass -> get primed cache (gradients flow through soft prefix)\")\n",
    "print(\"    3. Extract primed values at layers 0-15 (functional ops, no in-place mutation)\")\n",
    "print(\"    4. Build bare cache (no grad)\")\n",
    "print(\"    5. Splice: bare keys + primed values (L0-15) + bare values (L16-33)\")\n",
    "print(\"    6. Score answer NLL through hybrid cache\")\n",
    "print(\"    7. loss.backward() -> updates only soft_prefix_embeddings\")\n",
    "\n",
    "print(\"\\n### Part 2: Evaluation (4 conditions) ###\")\n",
    "print(f\"  Data: {N_EVAL} MS MARCO validation queries\")\n",
    "print()\n",
    "print(\"  bare:\")\n",
    "print(\"    Cache: [BOS][doc] -> score as-is\")\n",
    "print(\"    Baseline, no modifications.\")\n",
    "print()\n",
    "print(\"  vel_static:\")\n",
    "print(\"    Cache: [BOS][static_fact][doc] -> truncate -> RoPE correct\")\n",
    "print(f\"    Replace values at layers 0-{CUTOFF-1} into bare cache\")\n",
    "print(f\"    This is the Exp 21 condition (d=+0.227). Ceiling to beat.\")\n",
    "print()\n",
    "print(\"  vel_soft_random:\")\n",
    "print(\"    Cache: [BOS][soft_random_embs][doc_embs] -> forward -> extract values L0-15\")\n",
    "print(f\"    Splice into bare cache. Tests: can random-init soft prefix learn useful values?\")\n",
    "print()\n",
    "print(\"  vel_soft_fact:\")\n",
    "print(\"    Cache: [BOS][soft_fact_embs][doc_embs] -> forward -> extract values L0-15\")\n",
    "print(f\"    Splice into bare cache. Tests: can we refine static_fact in continuous space?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": "# Cell 7: Differentiable hybrid cache scoring function\n#\n# The existing lib functions use in-place ops and torch.no_grad().\n# For training we need a fully differentiable path from soft embeddings to loss.\n\nfrom lib.kv_cache import _get_rope_theta_for_layer, _build_rope_correction, _rotate_half\n\n# Get the embedding layer using standard HuggingFace API\nembed_fn = model.get_input_embeddings()\nprint(f\"Embedding layer found: {type(embed_fn).__name__}\")\nprint(f\"Embedding dim: {embed_fn.weight.shape}\")\n\n\ndef differentiable_hybrid_score(\n    soft_prefix: torch.Tensor,       # (1, prefix_len, hidden_size), requires_grad\n    doc_ids: torch.Tensor,            # (1, doc_len)\n    bos_id: torch.Tensor,             # (1, 1)\n    query_prompt: str,\n    answer_text: str,\n    model,\n    tokenizer,\n    config,\n    cutoff: int,\n):\n    \"\"\"\n    Compute answer NLL through a hybrid cache built from soft prefix embeddings.\n\n    Returns a scalar loss tensor with gradients flowing back to soft_prefix.\n    \"\"\"\n    device = config.device\n    doc_len = doc_ids.shape[1]\n    prefix_len = soft_prefix.shape[1]\n    context_len = 1 + doc_len  # BOS + doc\n\n    # --- Step 1: Bare cache (no gradients needed) ---\n    bare_input = torch.cat([bos_id, doc_ids], dim=1)\n    with torch.no_grad():\n        bare_out = model(input_ids=bare_input,\n                         attention_mask=torch.ones_like(bare_input),\n                         use_cache=True, return_dict=True)\n    bare_cache = bare_out.past_key_values\n    del bare_out\n\n    # --- Step 2: Primed cache via inputs_embeds (gradients enabled) ---\n    # Get embeddings for BOS and doc tokens (detached from graph)\n    with torch.no_grad():\n        bos_emb = embed_fn(bos_id)          # (1, 1, hidden)\n        doc_emb = embed_fn(doc_ids)          # (1, doc_len, hidden)\n\n    # Cast soft prefix to model dtype for forward pass\n    soft_cast = soft_prefix.to(dtype=bos_emb.dtype)\n\n    # Concatenate: [BOS_emb, soft_prefix, doc_emb]\n    inputs_embeds = torch.cat([bos_emb.detach(), soft_cast, doc_emb.detach()], dim=1)\n    total_len = inputs_embeds.shape[1]  # 1 + prefix_len + doc_len\n    attn_mask = torch.ones((1, total_len), device=device, dtype=torch.long)\n\n    # Forward pass with gradients flowing through soft_prefix\n    primed_out = model(inputs_embeds=inputs_embeds,\n                       attention_mask=attn_mask,\n                       use_cache=True, return_dict=True)\n    primed_cache = primed_out.past_key_values\n    del primed_out\n\n    # --- Step 3+4: Build hybrid cache ---\n    # Keys: from bare cache (already at correct positions)\n    # Values L0-{cutoff-1}: from primed cache (BOS + last doc_len positions)\n    # Values L{cutoff}-{N-1}: from bare cache\n    # No RoPE correction needed — we use bare keys, and values have no RoPE.\n\n    primed_cache_dc = _ensure_dynamic_cache(primed_cache)\n    bare_cache_dc = _ensure_dynamic_cache(bare_cache)\n\n    from transformers import DynamicCache\n    from transformers.cache_utils import DynamicSlidingWindowLayer, DynamicLayer\n\n    hybrid_cache = DynamicCache()\n    for layer_idx in range(NUM_LAYERS):\n        k = _get_cache_keys(bare_cache_dc, layer_idx)\n\n        if layer_idx < cutoff:\n            primed_v = _get_cache_values(primed_cache_dc, layer_idx)\n            bos_v = primed_v[:, :, :1, :]\n            doc_v = primed_v[:, :, -doc_len:, :]\n            v = torch.cat([bos_v, doc_v], dim=2)\n        else:\n            v = _get_cache_values(bare_cache_dc, layer_idx)\n\n        src_layer = bare_cache_dc.layers[layer_idx]\n        if isinstance(src_layer, DynamicSlidingWindowLayer):\n            new_layer = DynamicSlidingWindowLayer(sliding_window=src_layer.sliding_window)\n            new_layer.dtype = k.dtype\n            new_layer.device = k.device\n            new_layer.keys = k\n            new_layer.values = v\n            new_layer.is_initialized = True\n            new_layer.cumulative_length = src_layer.cumulative_length\n            new_layer._sliding_window_tensor = new_layer._sliding_window_tensor.to(k.device)\n        else:\n            new_layer = DynamicLayer()\n            new_layer.dtype = k.dtype\n            new_layer.device = k.device\n            new_layer.keys = k\n            new_layer.values = v\n            new_layer.is_initialized = True\n        hybrid_cache.layers.append(new_layer)\n\n    # --- Step 5: Score answer through hybrid cache ---\n    query_ids = tokenizer(query_prompt, return_tensors=\"pt\",\n                          add_special_tokens=False)['input_ids'].to(device)\n    answer_ids = tokenizer(answer_text, return_tensors=\"pt\",\n                           add_special_tokens=False)['input_ids'].to(device)\n    query_len = query_ids.shape[1]\n    answer_len = answer_ids.shape[1]\n\n    # Single forward pass: query + answer together\n    qa_ids = torch.cat([query_ids, answer_ids], dim=1)\n    qa_len = qa_ids.shape[1]\n    qa_attn_full = torch.ones((1, context_len + qa_len), device=device)\n\n    qa_out = model(input_ids=qa_ids,\n                   attention_mask=qa_attn_full,\n                   past_key_values=hybrid_cache,\n                   use_cache=False, return_dict=True)\n\n    logits = qa_out.logits\n    # logit at position [query_len-1] predicts answer_ids[0]\n    answer_logits = logits[:, query_len - 1 : query_len + answer_len - 1, :]\n\n    loss = torch.nn.functional.cross_entropy(\n        answer_logits.reshape(-1, answer_logits.shape[-1]),\n        answer_ids.reshape(-1),\n        reduction='mean'\n    )\n\n    del qa_out, logits, bare_cache, bare_cache_dc, primed_cache, primed_cache_dc\n\n    return loss\n\n\nprint(\"differentiable_hybrid_score() defined\")\nprint(\"  Input: soft_prefix (requires_grad), doc_ids, query, answer\")\nprint(\"  Output: scalar NLL loss with gradients to soft_prefix\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": "# Cell 8: Gradient flow sanity check\n# Verify that gradients actually flow back to the soft prefix before committing\n# to the full training loop.\n\nprint(\"=\" * 70)\nprint(\"GRADIENT FLOW SANITY CHECK\")\nprint(\"=\" * 70)\n\n# Create a tiny soft prefix\ntest_prefix = torch.randn(1, PREFIX_LEN, HIDDEN_SIZE,\n                           device=exp_config.device,\n                           dtype=torch.float32,\n                           requires_grad=True)\n\n# Use first training sample\nsample = train_samples[0]\nsf_str_test = SURROGATE_PREFIX_TEMPLATE.format(surrogate=STATIC_FACT)\ndoc_text = DOCUMENT_TEMPLATE.format(document=sample['passage'])\n\n# Matched tokenization\nfull_text = sf_str_test + doc_text\nfull_enc = tokenizer(full_text, return_tensors=\"pt\",\n                      add_special_tokens=True, padding=False, truncation=False)\nfull_ids = full_enc['input_ids'].to(exp_config.device)\nsf_prefix_enc = tokenizer(sf_str_test, return_tensors=\"pt\",\n                           add_special_tokens=True, padding=False, truncation=False)\nsf_prefix_len_matched = sf_prefix_enc['input_ids'].shape[1]\nbos_id = full_ids[:, :1]\ndoc_ids = full_ids[:, sf_prefix_len_matched:]\n\nquery_prompt = QUERY_TEMPLATE.format(query=sample['query'])\nanswer_text = ANSWER_TEMPLATE.format(answer=sample['answer'])\n\nprint(f\"Test sample: doc_len={doc_ids.shape[1]}, query='{sample['query'][:50]}...'\")\nprint(f\"Test prefix shape: {test_prefix.shape}\")\n\ntest_loss = None\ntry:\n    test_loss = differentiable_hybrid_score(\n        test_prefix, doc_ids, bos_id,\n        query_prompt, answer_text,\n        model, tokenizer, exp_config, CUTOFF)\n\n    print(f\"\\nForward pass: loss = {test_loss.item():.4f}\")\n    print(f\"Loss requires_grad: {test_loss.requires_grad}\")\n\n    test_loss.backward()\n\n    print(f\"Backward pass: SUCCESS\")\n    print(f\"Gradient shape: {test_prefix.grad.shape}\")\n    print(f\"Gradient norm: {test_prefix.grad.norm().item():.6f}\")\n    print(f\"Gradient mean: {test_prefix.grad.mean().item():.6f}\")\n    print(f\"Gradient max: {test_prefix.grad.abs().max().item():.6f}\")\n\n    if test_prefix.grad.norm().item() > 0:\n        print(\"\\n>>> GRADIENT FLOW CONFIRMED. Training loop should work. <<<\")\n    else:\n        print(\"\\n>>> WARNING: Zero gradients. Check computational graph. <<<\")\n\nexcept Exception as e:\n    print(f\"\\n>>> GRADIENT FLOW FAILED: {e} <<<\")\n    print(\"Training will not work. Need to debug the differentiable path.\")\n    import traceback\n    traceback.print_exc()\n\nfinally:\n    del test_prefix\n    if test_loss is not None:\n        del test_loss\n    gc.collect()\n    torch.cuda.empty_cache()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": "# Cell 9: Training loop\n\ndef train_soft_prefix(init_mode, train_data, n_epochs, lr, grad_accum,\n                       checkpoint_path, save_path, warmup_steps=50):\n    \"\"\"\n    Train soft prefix embeddings via gradient descent on answer NLL.\n\n    Args:\n        init_mode: 'random' or 'fact' (initialize from static_fact embeddings)\n        train_data: list of query dicts\n        n_epochs: number of passes over data\n        lr: learning rate\n        grad_accum: gradient accumulation steps\n        checkpoint_path: path to save training checkpoints\n        save_path: path to save final trained embeddings\n        warmup_steps: linear warmup steps\n\n    Returns:\n        dict with training history and final embeddings\n    \"\"\"\n    print(f\"\\n{'=' * 70}\")\n    print(f\"TRAINING SOFT PREFIX — init={init_mode}\")\n    print(f\"{'=' * 70}\")\n\n    # Initialize soft prefix\n    if init_mode == 'random':\n        soft_prefix = torch.randn(1, PREFIX_LEN, HIDDEN_SIZE,\n                                   device=exp_config.device, dtype=torch.float32) * 0.02\n    elif init_mode == 'fact':\n        # Get embeddings for static_fact tokens\n        with torch.no_grad():\n            fact_emb = embed_fn(sf_ids)  # (1, prefix_len, hidden)\n        soft_prefix = fact_emb.float().clone()\n    else:\n        raise ValueError(f\"Unknown init_mode: {init_mode}\")\n\n    soft_prefix = soft_prefix.detach().requires_grad_(True)\n\n    print(f\"  Soft prefix shape: {soft_prefix.shape}\")\n    print(f\"  Soft prefix dtype: {soft_prefix.dtype}\")\n    print(f\"  Soft prefix norm: {soft_prefix.norm().item():.4f}\")\n\n    optimizer = torch.optim.AdamW([soft_prefix], lr=lr, weight_decay=0.01)\n\n    total_steps = n_epochs * len(train_data)\n    total_optim_steps = total_steps // grad_accum\n    print(f\"  Total forward passes: {total_steps}\")\n    print(f\"  Total optimizer steps: {total_optim_steps}\")\n    print(f\"  Warmup steps: {warmup_steps}\")\n\n    # Checkpoint resume\n    history = []\n    start_step = 0\n    if checkpoint_path.exists():\n        ckpt = json.loads(checkpoint_path.read_text())\n        if ckpt.get('init_mode') == init_mode and ckpt.get('total_steps') == total_steps:\n            history = ckpt['history']\n            start_step = ckpt['completed_steps']\n            # Restore soft_prefix\n            soft_prefix_data = torch.tensor(ckpt['soft_prefix'],\n                                            device=exp_config.device, dtype=torch.float32)\n            soft_prefix = soft_prefix_data.requires_grad_(True)\n            optimizer = torch.optim.AdamW([soft_prefix], lr=lr, weight_decay=0.01)\n            print(f\"  Resumed from checkpoint: step {start_step}/{total_steps}\")\n\n    t_start = time.time()\n    step = 0\n    optim_step = 0\n    running_loss = 0.0\n    running_count = 0\n\n    for epoch in range(n_epochs):\n        # Shuffle training data each epoch\n        np.random.seed(SEED + epoch)\n        epoch_indices = np.random.permutation(len(train_data))\n\n        for idx_in_epoch, data_idx in enumerate(epoch_indices):\n            # Skip already-completed steps\n            if step < start_step:\n                step += 1\n                continue\n\n            sample = train_data[data_idx]\n            doc_text = DOCUMENT_TEMPLATE.format(document=sample['passage'])\n            query_prompt = QUERY_TEMPLATE.format(query=sample['query'])\n            answer_text = ANSWER_TEMPLATE.format(answer=sample['answer'])\n\n            # Matched tokenization using sf_str as reference\n            full_text = sf_str + doc_text\n            full_enc = tokenizer(full_text, return_tensors=\"pt\",\n                                  add_special_tokens=True, padding=False, truncation=False)\n            full_ids_t = full_enc['input_ids'].to(exp_config.device)\n            sf_enc = tokenizer(sf_str, return_tensors=\"pt\",\n                                add_special_tokens=True, padding=False, truncation=False)\n            sf_len = sf_enc['input_ids'].shape[1]\n            bos_t = full_ids_t[:, :1]\n            doc_ids_t = full_ids_t[:, sf_len:]\n\n            try:\n                loss = differentiable_hybrid_score(\n                    soft_prefix, doc_ids_t, bos_t,\n                    query_prompt, answer_text,\n                    model, tokenizer, exp_config, CUTOFF)\n\n                # Scale loss for gradient accumulation\n                scaled_loss = loss / grad_accum\n                scaled_loss.backward()\n\n                running_loss += loss.item()\n                running_count += 1\n\n            except RuntimeError as e:\n                print(f\"  Step {step}: RuntimeError: {e}\")\n                optimizer.zero_grad()\n                gc.collect()\n                torch.cuda.empty_cache()\n                step += 1\n                continue\n\n            # Optimizer step\n            if (step + 1) % grad_accum == 0:\n                # Linear warmup\n                optim_step += 1\n                if optim_step <= warmup_steps:\n                    warmup_factor = optim_step / warmup_steps\n                    for pg in optimizer.param_groups:\n                        pg['lr'] = lr * warmup_factor\n\n                # Gradient clipping\n                grad_norm = soft_prefix.grad.norm().item() if soft_prefix.grad is not None else 0\n                torch.nn.utils.clip_grad_norm_([soft_prefix], max_norm=1.0)\n\n                optimizer.step()\n                optimizer.zero_grad()\n\n                avg_loss = running_loss / running_count if running_count > 0 else 0\n                history.append({\n                    'step': step,\n                    'optim_step': optim_step,\n                    'epoch': epoch,\n                    'avg_loss': avg_loss,\n                    'grad_norm': grad_norm,\n                    'prefix_norm': soft_prefix.norm().item(),\n                    'lr': optimizer.param_groups[0]['lr'],\n                })\n                running_loss = 0.0\n                running_count = 0\n\n            # Cleanup\n            del loss, scaled_loss\n            gc.collect()\n            torch.cuda.empty_cache()\n\n            step += 1\n\n            # Checkpoint\n            if step % CHECKPOINT_EVERY == 0 or step == total_steps:\n                elapsed = time.time() - t_start\n                steps_done = step - start_step\n                rate = steps_done / elapsed if elapsed > 0 else 0\n                remaining = (total_steps - step) / rate if rate > 0 else 0\n\n                last_loss = history[-1]['avg_loss'] if history else 0\n                tqdm.write(f\"  [{init_mode}] Step {step}/{total_steps} | \"\n                           f\"loss={last_loss:.4f} | \"\n                           f\"prefix_norm={soft_prefix.norm().item():.3f} | \"\n                           f\"ETA: {remaining/60:.1f}m\")\n\n                ckpt_data = {\n                    'init_mode': init_mode,\n                    'completed_steps': step,\n                    'total_steps': total_steps,\n                    'history': history,\n                    'soft_prefix': soft_prefix.detach().cpu().tolist(),\n                    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n                }\n                with open(checkpoint_path, 'w') as f:\n                    json.dump(ckpt_data, f)\n\n    # Save final embeddings\n    torch.save(soft_prefix.detach().cpu(), save_path)\n\n    elapsed = time.time() - t_start\n    print(f\"\\n  Training complete: {step} steps in {elapsed/60:.1f} min\")\n    print(f\"  Final prefix norm: {soft_prefix.norm().item():.4f}\")\n    print(f\"  Saved to: {save_path}\")\n\n    return {\n        'soft_prefix': soft_prefix.detach(),\n        'history': history,\n        'init_mode': init_mode,\n    }\n\n\nprint(\"train_soft_prefix() defined\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Train random-init soft prefix\n",
    "\n",
    "result_random = train_soft_prefix(\n",
    "    init_mode='random',\n",
    "    train_data=train_samples,\n",
    "    n_epochs=N_EPOCHS,\n",
    "    lr=LR,\n",
    "    grad_accum=GRAD_ACCUM_STEPS,\n",
    "    checkpoint_path=CHECKPOINT_TRAIN_RAND_PATH,\n",
    "    save_path=SOFT_RANDOM_PATH,\n",
    "    warmup_steps=WARMUP_STEPS,\n",
    ")\n",
    "soft_prefix_random = result_random['soft_prefix']\n",
    "history_random = result_random['history']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Train fact-init soft prefix\n",
    "\n",
    "result_fact = train_soft_prefix(\n",
    "    init_mode='fact',\n",
    "    train_data=train_samples,\n",
    "    n_epochs=N_EPOCHS,\n",
    "    lr=LR,\n",
    "    grad_accum=GRAD_ACCUM_STEPS,\n",
    "    checkpoint_path=CHECKPOINT_TRAIN_FACT_PATH,\n",
    "    save_path=SOFT_FACT_PATH,\n",
    "    warmup_steps=WARMUP_STEPS,\n",
    ")\n",
    "soft_prefix_fact = result_fact['soft_prefix']\n",
    "history_fact = result_fact['history']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Training curves visualization\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for hist, label, color in [(history_random, 'random init', '#1f77b4'),\n",
    "                            (history_fact, 'fact init', '#ff7f0e')]:\n",
    "    if not hist:\n",
    "        continue\n",
    "    steps = [h['optim_step'] for h in hist]\n",
    "    losses = [h['avg_loss'] for h in hist]\n",
    "    gnorms = [h['grad_norm'] for h in hist]\n",
    "    pnorms = [h['prefix_norm'] for h in hist]\n",
    "\n",
    "    # Smoothed loss (rolling average window=20)\n",
    "    w = min(20, len(losses) // 3 + 1)\n",
    "    smoothed = np.convolve(losses, np.ones(w)/w, mode='valid') if len(losses) > w else losses\n",
    "    smooth_steps = steps[w-1:] if len(losses) > w else steps\n",
    "\n",
    "    axes[0].plot(steps, losses, alpha=0.2, color=color)\n",
    "    axes[0].plot(smooth_steps, smoothed, linewidth=2, color=color, label=label)\n",
    "    axes[1].plot(steps, gnorms, alpha=0.5, linewidth=1, color=color, label=label)\n",
    "    axes[2].plot(steps, pnorms, linewidth=2, color=color, label=label)\n",
    "\n",
    "axes[0].set_xlabel('Optimizer Step')\n",
    "axes[0].set_ylabel('Loss (NLL)')\n",
    "axes[0].set_title('Training Loss')\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].set_xlabel('Optimizer Step')\n",
    "axes[1].set_ylabel('Gradient Norm')\n",
    "axes[1].set_title('Gradient Norm')\n",
    "axes[1].legend()\n",
    "\n",
    "axes[2].set_xlabel('Optimizer Step')\n",
    "axes[2].set_ylabel('Prefix Embedding Norm')\n",
    "axes[2].set_title('Prefix Norm')\n",
    "axes[2].legend()\n",
    "\n",
    "plt.suptitle('Exp 25: Soft Prefix Training Curves', fontsize=13)\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'training_curves.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"Training curves saved to {RESULTS_DIR / 'training_curves.png'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": "# Cell 13: Part 2 — Evaluation (300 validation queries, 4 conditions)\n\nprint(\"=\" * 70)\nprint(f\"PART 2: EVALUATION ({N_EVAL} queries, 4 conditions)\")\nprint(\"=\" * 70)\n\n# Load trained soft prefixes (in case of restart)\nif 'soft_prefix_random' not in dir():\n    soft_prefix_random = torch.load(SOFT_RANDOM_PATH).to(exp_config.device)\n    print(f\"Loaded soft_prefix_random from {SOFT_RANDOM_PATH}\")\nif 'soft_prefix_fact' not in dir():\n    soft_prefix_fact = torch.load(SOFT_FACT_PATH).to(exp_config.device)\n    print(f\"Loaded soft_prefix_fact from {SOFT_FACT_PATH}\")\n\n# Checkpoint resume\neval_results = []\neval_start_idx = 0\n\nif CHECKPOINT_EVAL_PATH.exists():\n    with open(CHECKPOINT_EVAL_PATH, 'r') as f:\n        ckpt = json.load(f)\n    ckpt_queries = ckpt.get('query_texts', [])\n    current_queries = [q['query'] for q in eval_samples[:N_EVAL]]\n    if ckpt_queries == current_queries:\n        eval_results = ckpt['results']\n        eval_start_idx = len(eval_results)\n        print(f\"Resuming from checkpoint: {eval_start_idx}/{N_EVAL}\")\n    else:\n        print(\"Checkpoint query mismatch. Starting fresh.\")\nelse:\n    print(\"No checkpoint found. Starting fresh.\")\n\nlayer_indices = list(range(CUTOFF))\nt_start = time.time()\n\nfor qidx in tqdm(range(eval_start_idx, N_EVAL), initial=eval_start_idx, total=N_EVAL,\n                  desc=\"Eval\"):\n    qdata = eval_samples[qidx]\n    query_prompt = QUERY_TEMPLATE.format(query=qdata['query'])\n    answer_text = ANSWER_TEMPLATE.format(answer=qdata['answer'])\n    passage = qdata['passage']\n    document_text = DOCUMENT_TEMPLATE.format(document=passage)\n\n    # Matched tokenization (using sf_str as reference for BPE boundaries)\n    full_text = sf_str + document_text\n    full_enc = tokenizer(full_text, return_tensors=\"pt\",\n                          add_special_tokens=True, padding=False, truncation=False)\n    full_ids = full_enc['input_ids'].to(exp_config.device)\n    sf_prefix_enc = tokenizer(sf_str, return_tensors=\"pt\",\n                               add_special_tokens=True, padding=False, truncation=False)\n    sf_prefix_len_matched = sf_prefix_enc['input_ids'].shape[1]\n    bos_id = full_ids[:, :1]\n    doc_ids = full_ids[:, sf_prefix_len_matched:]\n    doc_len = doc_ids.shape[1]\n    context_len = 1 + doc_len\n\n    del full_enc, full_ids, sf_prefix_enc\n\n    # --- Condition 1: bare ---\n    bare_input = torch.cat([bos_id, doc_ids], dim=1)\n    with torch.no_grad():\n        bare_out = model(input_ids=bare_input,\n                         attention_mask=torch.ones_like(bare_input),\n                         use_cache=True, return_dict=True)\n    bare_cache = _ensure_dynamic_cache(bare_out.past_key_values)\n    del bare_out\n\n    bare_nll = score_answer_with_cache(\n        deepcopy_cache(bare_cache), context_len,\n        query_prompt, answer_text, model, tokenizer, exp_config)\n\n    # --- Condition 2: vel_static (discrete static_fact prefix) ---\n    primed_input = torch.cat([bos_id, sf_ids, doc_ids], dim=1)\n    with torch.no_grad():\n        primed_out = model(input_ids=primed_input,\n                           attention_mask=torch.ones_like(primed_input),\n                           use_cache=True, return_dict=True)\n    primed_full = _ensure_dynamic_cache(primed_out.past_key_values)\n    del primed_out\n\n    trunc_raw = extract_and_truncate_cache_with_bos(primed_full, doc_len)\n    sf_trunc_cache = deepcopy_cache(trunc_raw)\n    correct_rope_positions_with_bos(sf_trunc_cache, sf_ids.shape[1], model)\n    del primed_full, trunc_raw\n\n    vel_static_cache = replace_values_at_layers(bare_cache, sf_trunc_cache, layer_indices)\n    vel_static_nll = score_answer_with_cache(\n        deepcopy_cache(vel_static_cache), context_len,\n        query_prompt, answer_text, model, tokenizer, exp_config)\n    del sf_trunc_cache, vel_static_cache\n\n    # --- Helper: score soft prefix condition ---\n    def score_soft_condition(soft_embs):\n        \"\"\"Build hybrid cache from soft embeddings and score.\"\"\"\n        with torch.no_grad():\n            bos_emb = embed_fn(bos_id)\n            doc_emb = embed_fn(doc_ids)\n            soft_cast = soft_embs.to(device=exp_config.device, dtype=bos_emb.dtype)\n\n            inputs_embeds = torch.cat([bos_emb, soft_cast, doc_emb], dim=1)\n            total_len = inputs_embeds.shape[1]\n            attn_mask = torch.ones((1, total_len), device=exp_config.device, dtype=torch.long)\n\n            soft_out = model(inputs_embeds=inputs_embeds,\n                            attention_mask=attn_mask,\n                            use_cache=True, return_dict=True)\n            soft_cache = _ensure_dynamic_cache(soft_out.past_key_values)\n            del soft_out\n\n            # Extract BOS + doc values from soft cache, splice into bare\n            soft_trunc = extract_and_truncate_cache_with_bos(soft_cache, doc_len)\n            # No RoPE correction needed because we use bare keys\n            # (values don't have positional encoding)\n            del soft_cache\n\n            vel_soft_cache = replace_values_at_layers(bare_cache, soft_trunc, layer_indices)\n            del soft_trunc\n\n            nll = score_answer_with_cache(\n                deepcopy_cache(vel_soft_cache), context_len,\n                query_prompt, answer_text, model, tokenizer, exp_config)\n            del vel_soft_cache\n\n        return nll\n\n    # --- Condition 3: vel_soft_random ---\n    vel_soft_random_nll = score_soft_condition(soft_prefix_random)\n\n    # --- Condition 4: vel_soft_fact ---\n    vel_soft_fact_nll = score_soft_condition(soft_prefix_fact)\n\n    del bare_cache, bare_input\n    gc.collect()\n    torch.cuda.empty_cache()\n\n    eval_results.append({\n        'query_idx': qidx,\n        'query': qdata['query'],\n        'doc_len': doc_len,\n        'bare_nll': bare_nll,\n        'vel_static_nll': vel_static_nll,\n        'vel_soft_random_nll': vel_soft_random_nll,\n        'vel_soft_fact_nll': vel_soft_fact_nll,\n    })\n\n    # Checkpoint\n    if (qidx + 1) % CHECKPOINT_EVERY == 0 or qidx == N_EVAL - 1:\n        ckpt_data = {\n            'results': eval_results,\n            'query_texts': [q['query'] for q in eval_samples[:N_EVAL]],\n            'completed': len(eval_results),\n            'total': N_EVAL,\n            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n        }\n        with open(CHECKPOINT_EVAL_PATH, 'w') as f:\n            json.dump(ckpt_data, f)\n        elapsed = time.time() - t_start\n        n_done = qidx - eval_start_idx + 1\n        rate = n_done / elapsed if elapsed > 0 else 0\n        remaining = (N_EVAL - qidx - 1) / rate if rate > 0 else 0\n        tqdm.write(f\"  Checkpoint {qidx+1}/{N_EVAL} | {n_done} done in {elapsed/60:.1f}m | \"\n                   f\"ETA: {remaining/60:.1f} min\")\n\nelapsed_total = time.time() - t_start\nprint(f\"\\nEval complete: {len(eval_results)} queries in {elapsed_total/60:.1f} min\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 14: Evaluation analysis\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"EVALUATION ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "bare_arr = np.array([r['bare_nll'] for r in eval_results])\n",
    "static_arr = np.array([r['vel_static_nll'] for r in eval_results])\n",
    "soft_rand_arr = np.array([r['vel_soft_random_nll'] for r in eval_results])\n",
    "soft_fact_arr = np.array([r['vel_soft_fact_nll'] for r in eval_results])\n",
    "\n",
    "# Filter valid results\n",
    "valid = (\n",
    "    (bare_arr != 0) & np.isfinite(bare_arr) &\n",
    "    (static_arr != 0) & np.isfinite(static_arr) &\n",
    "    (soft_rand_arr != 0) & np.isfinite(soft_rand_arr) &\n",
    "    (soft_fact_arr != 0) & np.isfinite(soft_fact_arr)\n",
    ")\n",
    "\n",
    "b = bare_arr[valid]\n",
    "conditions = {\n",
    "    'vel_static': static_arr[valid],\n",
    "    'vel_soft_random': soft_rand_arr[valid],\n",
    "    'vel_soft_fact': soft_fact_arr[valid],\n",
    "}\n",
    "\n",
    "print(f\"\\nValid samples: {np.sum(valid)}/{len(eval_results)}\")\n",
    "print(f\"\\n{'Condition':<20} {'Mean NLL':>10} {'Mean D':>10} {'d':>8} {'Win%':>7} {'p':>12} {'sig':>5}\")\n",
    "print(\"-\" * 78)\n",
    "print(f\"{'bare':<20} {np.mean(b):>10.4f} {'—':>10} {'—':>8} {'—':>7} {'—':>12} {'—':>5}\")\n",
    "\n",
    "eval_analysis = {}\n",
    "for cname, carr in conditions.items():\n",
    "    delta = b - carr\n",
    "    d = cohens_d(delta)\n",
    "    win = np.mean(delta > 0) * 100\n",
    "    t_stat, p_val = stats.ttest_1samp(delta, 0)\n",
    "    sig = '***' if p_val < 0.001 else '**' if p_val < 0.01 else '*' if p_val < 0.05 else 'ns'\n",
    "    print(f\"{cname:<20} {np.mean(carr):>10.4f} {np.mean(delta):>+10.4f} \"\n",
    "          f\"{d:>+8.3f} {win:>6.1f}% {p_val:>12.2e} {sig:>5}\")\n",
    "    eval_analysis[cname] = {\n",
    "        'n_valid': int(np.sum(valid)),\n",
    "        'mean_nll': float(np.mean(carr)),\n",
    "        'mean_delta': float(np.mean(delta)),\n",
    "        'cohens_d': float(d),\n",
    "        'win_pct': float(win),\n",
    "        't_stat': float(t_stat),\n",
    "        'p_value': float(p_val),\n",
    "    }\n",
    "\n",
    "# Pairwise: soft vs static\n",
    "print(\"\\nPairwise comparisons:\")\n",
    "for n1, a1, n2, a2 in [\n",
    "    ('vel_soft_fact', conditions['vel_soft_fact'], 'vel_static', conditions['vel_static']),\n",
    "    ('vel_soft_random', conditions['vel_soft_random'], 'vel_static', conditions['vel_static']),\n",
    "    ('vel_soft_fact', conditions['vel_soft_fact'], 'vel_soft_random', conditions['vel_soft_random']),\n",
    "]:\n",
    "    delta = a2 - a1  # positive = n1 better (lower NLL)\n",
    "    d = cohens_d(delta)\n",
    "    t_stat, p_val = stats.ttest_1samp(delta, 0)\n",
    "    sig = '***' if p_val < 0.001 else '**' if p_val < 0.01 else '*' if p_val < 0.05 else 'ns'\n",
    "    print(f\"  {n1} vs {n2}: d={d:+.3f}, p={p_val:.2e} {sig}\")\n",
    "\n",
    "# Reference comparison\n",
    "print(f\"\\nReference comparison:\")\n",
    "print(f\"  Exp 21 vel_static d: {EXP21_REF['values_early_layers_d']:+.3f}\")\n",
    "print(f\"  This exp vel_static d: {eval_analysis['vel_static']['cohens_d']:+.3f}\")\n",
    "print(f\"  This exp vel_soft_fact d: {eval_analysis['vel_soft_fact']['cohens_d']:+.3f}\")\n",
    "print(f\"  This exp vel_soft_random d: {eval_analysis['vel_soft_random']['cohens_d']:+.3f}\")\n",
    "\n",
    "# Hardness gradient (quintiles)\n",
    "print(\"\\nHardness gradient (bare NLL quintiles):\")\n",
    "quintile_bounds = np.percentile(b, [20, 40, 60, 80])\n",
    "qlabels = ['Q1 easy', 'Q2', 'Q3', 'Q4', 'Q5 hard']\n",
    "quintiles = np.digitize(b, quintile_bounds)\n",
    "\n",
    "print(f\"{'Condition':<20}\", end='')\n",
    "for ql in qlabels:\n",
    "    print(f\"{ql:>12}\", end='')\n",
    "print()\n",
    "print(\"-\" * (20 + 12 * 5))\n",
    "\n",
    "hardness_data = {}\n",
    "for cname, carr in conditions.items():\n",
    "    delta = b - carr\n",
    "    row = []\n",
    "    print(f\"{cname:<20}\", end='')\n",
    "    for q in range(5):\n",
    "        mask = quintiles == q\n",
    "        if np.sum(mask) < 5:\n",
    "            print(f\"{'n/a':>12}\", end='')\n",
    "            row.append(None)\n",
    "        else:\n",
    "            d_q = cohens_d(delta[mask])\n",
    "            print(f\"{d_q:>+12.3f}\", end='')\n",
    "            row.append(float(d_q))\n",
    "    print()\n",
    "    hardness_data[cname] = row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 15: Evaluation plots (4-panel)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# --- Panel 1: Bar chart of Cohen's d ---\n",
    "ax = axes[0, 0]\n",
    "cond_names = ['vel_static', 'vel_soft_random', 'vel_soft_fact']\n",
    "cond_labels = ['static_fact\\n(discrete)', 'soft_random\\n(random init)', 'soft_fact\\n(fact init)']\n",
    "cond_ds = [eval_analysis[cn]['cohens_d'] for cn in cond_names]\n",
    "cond_colors = ['#2ca02c', '#1f77b4', '#ff7f0e']\n",
    "\n",
    "bars = ax.bar(range(len(cond_names)), cond_ds, color=cond_colors,\n",
    "              edgecolor='black', linewidth=0.5)\n",
    "ax.axhline(y=EXP21_REF['values_early_layers_d'], color='#9467bd', linestyle='--',\n",
    "           linewidth=1.5, label=f\"Exp 21 d={EXP21_REF['values_early_layers_d']:+.3f}\")\n",
    "ax.axhline(y=0, color='gray', linestyle='-', alpha=0.3)\n",
    "\n",
    "ax.set_xticks(range(len(cond_names)))\n",
    "ax.set_xticklabels(cond_labels)\n",
    "ax.set_ylabel(\"Cohen's d vs Bare\")\n",
    "ax.set_title(\"Effect Size by Condition\")\n",
    "ax.legend(fontsize=8)\n",
    "\n",
    "for i, d_val in enumerate(cond_ds):\n",
    "    p_val = eval_analysis[cond_names[i]]['p_value']\n",
    "    sig = '***' if p_val < 0.001 else '**' if p_val < 0.01 else '*' if p_val < 0.05 else 'ns'\n",
    "    ax.text(i, d_val + 0.005 if d_val >= 0 else d_val - 0.015,\n",
    "            f\"{d_val:+.3f} {sig}\", ha='center',\n",
    "            va='bottom' if d_val >= 0 else 'top', fontsize=9)\n",
    "\n",
    "# --- Panel 2: Per-sample scatter (soft_fact vs static) ---\n",
    "ax = axes[0, 1]\n",
    "ax.scatter(conditions['vel_static'], conditions['vel_soft_fact'],\n",
    "           alpha=0.4, s=20, color='#ff7f0e', edgecolors='none')\n",
    "lims = [min(conditions['vel_static'].min(), conditions['vel_soft_fact'].min()),\n",
    "        max(conditions['vel_static'].max(), conditions['vel_soft_fact'].max())]\n",
    "ax.plot(lims, lims, 'k--', alpha=0.5, linewidth=1, label='y=x')\n",
    "ax.set_xlabel('vel_static NLL (discrete)')\n",
    "ax.set_ylabel('vel_soft_fact NLL (learned)')\n",
    "ax.set_title('Per-Sample: Soft Fact vs Static')\n",
    "ax.legend(fontsize=8)\n",
    "\n",
    "# Count wins\n",
    "soft_wins = np.sum(conditions['vel_soft_fact'] < conditions['vel_static'])\n",
    "static_wins = np.sum(conditions['vel_soft_fact'] > conditions['vel_static'])\n",
    "ties = np.sum(conditions['vel_soft_fact'] == conditions['vel_static'])\n",
    "ax.text(0.05, 0.95, f\"Soft wins: {soft_wins}\\nStatic wins: {static_wins}\\nTies: {ties}\",\n",
    "        transform=ax.transAxes, fontsize=9, va='top',\n",
    "        bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "# --- Panel 3: Hardness gradient heatmap ---\n",
    "ax = axes[1, 0]\n",
    "heatmap = np.zeros((len(cond_names), 5))\n",
    "for i, cn in enumerate(cond_names):\n",
    "    for q in range(5):\n",
    "        val = hardness_data[cn][q]\n",
    "        heatmap[i, q] = val if val is not None else np.nan\n",
    "\n",
    "im = ax.imshow(heatmap, cmap='RdBu', aspect='auto', vmin=-0.5, vmax=0.5)\n",
    "ax.set_xticks(range(5))\n",
    "ax.set_xticklabels(qlabels, fontsize=8)\n",
    "ax.set_yticks(range(len(cond_names)))\n",
    "ax.set_yticklabels(['static', 'soft_random', 'soft_fact'])\n",
    "ax.set_xlabel('Difficulty Quintile')\n",
    "ax.set_title(\"Hardness x Condition (Cohen's d)\")\n",
    "\n",
    "for i in range(len(cond_names)):\n",
    "    for j in range(5):\n",
    "        val = heatmap[i, j]\n",
    "        if not np.isnan(val):\n",
    "            ax.text(j, i, f\"{val:+.2f}\", ha='center', va='center',\n",
    "                    fontsize=8, color='white' if abs(val) > 0.25 else 'black')\n",
    "fig.colorbar(im, ax=ax, shrink=0.8)\n",
    "\n",
    "# --- Panel 4: NLL distribution comparison ---\n",
    "ax = axes[1, 1]\n",
    "delta_static = b - conditions['vel_static']\n",
    "delta_soft_fact = b - conditions['vel_soft_fact']\n",
    "delta_soft_random = b - conditions['vel_soft_random']\n",
    "\n",
    "ax.hist(delta_static, bins=40, alpha=0.5, color='#2ca02c', label='static', density=True)\n",
    "ax.hist(delta_soft_fact, bins=40, alpha=0.5, color='#ff7f0e', label='soft_fact', density=True)\n",
    "ax.hist(delta_soft_random, bins=40, alpha=0.3, color='#1f77b4', label='soft_random', density=True)\n",
    "ax.axvline(x=0, color='black', linestyle='-', linewidth=0.5)\n",
    "ax.set_xlabel('NLL Delta (bare - condition, positive = helps)')\n",
    "ax.set_ylabel('Density')\n",
    "ax.set_title('Delta Distribution')\n",
    "ax.legend(fontsize=8)\n",
    "\n",
    "plt.suptitle('Exp 25: Soft Prefix Evaluation', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'eval_plots.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"Eval plots saved to {RESULTS_DIR / 'eval_plots.png'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 16: Save results.json + CSV\n",
    "import csv\n",
    "\n",
    "# --- Eval CSV ---\n",
    "with open(CSV_EVAL_PATH, 'w', newline='') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=[\n",
    "        'query_idx', 'doc_len', 'bare_nll',\n",
    "        'vel_static_nll', 'vel_soft_random_nll', 'vel_soft_fact_nll'])\n",
    "    writer.writeheader()\n",
    "    for r in eval_results:\n",
    "        writer.writerow({\n",
    "            'query_idx': r['query_idx'],\n",
    "            'doc_len': r['doc_len'],\n",
    "            'bare_nll': r['bare_nll'],\n",
    "            'vel_static_nll': r['vel_static_nll'],\n",
    "            'vel_soft_random_nll': r['vel_soft_random_nll'],\n",
    "            'vel_soft_fact_nll': r['vel_soft_fact_nll'],\n",
    "        })\n",
    "print(f\"Eval CSV saved: {CSV_EVAL_PATH}\")\n",
    "\n",
    "# --- Combined results.json ---\n",
    "final = {\n",
    "    'experiment': 'exp25_soft_prefix_optimization',\n",
    "    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'config': {\n",
    "        'model_name': MODEL_NAME,\n",
    "        'model_type': 'gemma3',\n",
    "        'seed': SEED,\n",
    "        'cutoff': CUTOFF,\n",
    "        'prefix_len': PREFIX_LEN,\n",
    "        'hidden_size': HIDDEN_SIZE,\n",
    "        'trainable_params': PREFIX_LEN * HIDDEN_SIZE,\n",
    "        'training': {\n",
    "            'n_samples': N_TRAIN,\n",
    "            'n_epochs': N_EPOCHS,\n",
    "            'lr': LR,\n",
    "            'grad_accum': GRAD_ACCUM_STEPS,\n",
    "            'warmup_steps': WARMUP_STEPS,\n",
    "            'dataset': 'MS MARCO v1.1 train',\n",
    "        },\n",
    "        'eval': {\n",
    "            'n_samples': N_EVAL,\n",
    "            'dataset': 'MS MARCO v1.1 validation',\n",
    "            'conditions': ['bare', 'vel_static', 'vel_soft_random', 'vel_soft_fact'],\n",
    "        },\n",
    "    },\n",
    "    'training_history': {\n",
    "        'random': history_random if 'history_random' in dir() else [],\n",
    "        'fact': history_fact if 'history_fact' in dir() else [],\n",
    "    },\n",
    "    'eval_analysis': eval_analysis,\n",
    "    'eval_hardness': hardness_data,\n",
    "    'reference_values': {\n",
    "        'exp19_gemma': EXP19_REF,\n",
    "        'exp21_gemma': EXP21_REF,\n",
    "    },\n",
    "    'eval_per_query': eval_results,\n",
    "}\n",
    "\n",
    "with open(FINAL_RESULTS_PATH, 'w') as f:\n",
    "    json.dump(final, f, indent=2)\n",
    "\n",
    "print(f\"\\nResults saved to {FINAL_RESULTS_PATH}\")\n",
    "print(f\"File size: {FINAL_RESULTS_PATH.stat().st_size / 1024:.1f} KB\")\n",
    "\n",
    "# Final summary\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"SUMMARY — Exp 25: Soft Prefix Optimization\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Model: Gemma 3 4B ({NUM_LAYERS} layers, hidden={HIDDEN_SIZE}, bfloat16)\")\n",
    "print(f\"Soft prefix: {PREFIX_LEN} vectors x {HIDDEN_SIZE} dims = {PREFIX_LEN * HIDDEN_SIZE:,} params\")\n",
    "print(f\"Training: {N_TRAIN} samples x {N_EPOCHS} epochs, lr={LR}\")\n",
    "\n",
    "print(f\"\\nEvaluation ({N_EVAL} queries):\")\n",
    "for cn in ['vel_static', 'vel_soft_random', 'vel_soft_fact']:\n",
    "    a = eval_analysis[cn]\n",
    "    sig = '***' if a['p_value'] < 0.001 else '**' if a['p_value'] < 0.01 else '*' if a['p_value'] < 0.05 else 'ns'\n",
    "    print(f\"  {cn:<20} d={a['cohens_d']:>+.3f}  win={a['win_pct']:.0f}%  {sig}\")\n",
    "\n",
    "d_static = eval_analysis['vel_static']['cohens_d']\n",
    "d_soft_fact = eval_analysis['vel_soft_fact']['cohens_d']\n",
    "d_soft_random = eval_analysis['vel_soft_random']['cohens_d']\n",
    "\n",
    "if d_soft_fact > d_static + 0.02:\n",
    "    print(f\"\\nVERDICT: Soft fact-init BEATS discrete static_fact \"\n",
    "          f\"({d_soft_fact:+.3f} vs {d_static:+.3f}). \"\n",
    "          f\"Continuous optimization improves value contamination.\")\n",
    "elif d_soft_fact > d_static - 0.02:\n",
    "    print(f\"\\nVERDICT: Soft fact-init MATCHES discrete static_fact \"\n",
    "          f\"({d_soft_fact:+.3f} vs {d_static:+.3f}). \"\n",
    "          f\"Continuous space adds no benefit beyond the discrete prefix.\")\n",
    "else:\n",
    "    print(f\"\\nVERDICT: Soft fact-init WORSE than discrete static_fact \"\n",
    "          f\"({d_soft_fact:+.3f} vs {d_static:+.3f}). \"\n",
    "          f\"Gradient optimization may be disrupting the prefix signal.\")\n",
    "\n",
    "if d_soft_random > 0.10:\n",
    "    print(f\"  Random-init learned useful signal from scratch (d={d_soft_random:+.3f}).\")\n",
    "else:\n",
    "    print(f\"  Random-init did NOT learn useful signal (d={d_soft_random:+.3f}).\")\n",
    "\n",
    "print(f\"\\nDone!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 17: GPU cleanup\n",
    "import gc\n",
    "\n",
    "print(\"Cleaning up GPU memory...\")\n",
    "mem_before = torch.cuda.memory_allocated() / 1e9\n",
    "\n",
    "del model\n",
    "del tokenizer\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "mem_after = torch.cuda.memory_allocated() / 1e9\n",
    "print(f\"GPU memory: {mem_before:.2f} GB -> {mem_after:.2f} GB\")\n",
    "print(\"Cleanup complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}