{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 33b: Corrected Surrogate Transfer Test",
    "## Query in encoder ONLY -- encoder is the sole source of context",
    "",
    "### Why Exp 33 was wrong",
    "In Exp 33, the decoder target was \"[query] Answer: [answer]\". The decoder",
    "already had the query via cross-attention to its own input tokens. Adding",
    "the query to the encoder was redundant -- the decoder didn't NEED the encoder",
    "to know about the query.",
    "",
    "### The ad-serving scenario",
    "- **Offline**: Pre-compute encoder representations for each ad/document",
    "- **Online**: User query arrives. We need the encoder representation to CARRY",
    "  the context for answering. The decoder generates an answer based solely on",
    "  what the encoder provides.",
    "",
    "### Corrected setup",
    "- **Encoder input**: varies by condition (document \u00b1 query/surrogate)",
    "- **Decoder target**: just \"[answer]\" (NO query in decoder)",
    "- The decoder's ONLY source of information is the encoder output",
    "- Therefore, query-aware encoding SHOULD beat bare encoding",
    "",
    "### Conditions",
    "1. **bare**: encoder(\"[document]\") \u2192 decoder NLL(\"[answer]\")",
    "2. **oracle**: encoder(\"[query]\\n[document]\") \u2192 decoder NLL(\"[answer]\")",
    "3. **static**: encoder(\"What are the key facts?\\n[document]\") \u2192 decoder NLL(\"[answer]\")",
    "4. **surr_para**: encoder(\"[paraphrased_query]\\n[document]\") \u2192 decoder NLL(\"[answer]\")",
    "5. **surr_doc**: encoder(\"[doc_keywords]\\n[document]\") \u2192 decoder NLL(\"[answer]\")",
    "",
    "### Expected hierarchy",
    "oracle > surr_para > surr_doc \u2248 static > bare",
    "",
    "If surrogate captures >30% of the oracle-bare gap, the core idea works.",
    ""
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 2: Setup\n",
    "import os\n",
    "os.umask(0o000)\n",
    "\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "import gc\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from scipy import stats\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "sys.path.insert(0, \".\")\n",
    "\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "RESULTS_DIR = Path(\"results/exp33b\")\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CHECKPOINT_PATH = RESULTS_DIR / \"checkpoint.json\"\n",
    "\n",
    "N_SAMPLES = 200\n",
    "MODEL_NAME = \"google/t5gemma-2-4b-4b\"\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "HF_TOKEN = os.environ.get(\"HF_TOKEN\")\n",
    "\n",
    "print(f\"Experiment 33b: Corrected Surrogate Transfer\")\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"N: {N_SAMPLES}\")\n",
    "print(f\"CUDA: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 3: Load model\n",
    "from transformers import AutoProcessor, AutoModelForSeq2SeqLM\n",
    "\n",
    "print(f\"Loading {MODEL_NAME}...\")\n",
    "processor = AutoProcessor.from_pretrained(MODEL_NAME, token=HF_TOKEN)\n",
    "tokenizer = processor.tokenizer\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    token=HF_TOKEN,\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "DEVICE = next(model.parameters()).device\n",
    "print(f\"Model loaded. dtype={next(model.parameters()).dtype}\")\n",
    "print(f\"GPU memory used: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 4: Scoring helper -- CORRECTED: answer-only in decoder\n",
    "\n",
    "def score_answer_nll(encoder_text, answer_text):\n",
    "    '''Score NLL of answer tokens.\n",
    "\n",
    "    Encoder: encoder_text (contains document, optionally query/surrogate)\n",
    "    Decoder: answer_text ONLY (no query -- encoder is sole context source)\n",
    "\n",
    "    This is the corrected version: the decoder's ONLY information about\n",
    "    what to answer comes from the encoder representation.\n",
    "    '''\n",
    "    enc_ids = tokenizer(encoder_text, return_tensors=\"pt\",\n",
    "                        add_special_tokens=True, truncation=True,\n",
    "                        max_length=2048).input_ids.to(DEVICE)\n",
    "    enc_mask = torch.ones_like(enc_ids)\n",
    "\n",
    "    # Decoder target: just the answer\n",
    "    ans_ids = tokenizer(answer_text, return_tensors=\"pt\",\n",
    "                        add_special_tokens=False, truncation=True,\n",
    "                        max_length=256).input_ids.to(DEVICE)\n",
    "\n",
    "    if ans_ids.shape[1] == 0:\n",
    "        return 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        encoder_outputs = model.get_encoder()(\n",
    "            input_ids=enc_ids, attention_mask=enc_mask\n",
    "        )\n",
    "        outputs = model(\n",
    "            encoder_outputs=encoder_outputs,\n",
    "            attention_mask=enc_mask,\n",
    "            labels=ans_ids,\n",
    "        )\n",
    "\n",
    "    # Per-token NLL over answer\n",
    "    logits = outputs.logits\n",
    "    log_probs = F.log_softmax(logits, dim=-1)\n",
    "    token_log_probs = log_probs[0].gather(1, ans_ids[0].unsqueeze(1)).squeeze(1)\n",
    "    mean_nll = -token_log_probs.mean().item()\n",
    "\n",
    "    del encoder_outputs, outputs, logits, log_probs\n",
    "    return mean_nll\n",
    "\n",
    "\n",
    "# === Surrogate query generation ===\n",
    "STOP_WORDS = {\n",
    "    'a', 'an', 'the', 'is', 'are', 'was', 'were', 'be', 'been', 'being',\n",
    "    'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could',\n",
    "    'should', 'may', 'might', 'can', 'shall', 'to', 'of', 'in', 'for',\n",
    "    'on', 'with', 'at', 'by', 'from', 'as', 'into', 'through', 'during',\n",
    "    'before', 'after', 'above', 'below', 'between', 'and', 'but', 'or',\n",
    "    'not', 'no', 'if', 'then', 'than', 'so', 'up', 'out', 'about',\n",
    "    'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those',\n",
    "    'it', 'its', 'i', 'me', 'my', 'we', 'our', 'you', 'your', 'he',\n",
    "    'him', 'his', 'she', 'her', 'they', 'them', 'their', 'how', 'when',\n",
    "    'where', 'why', 'much', 'many', 'some', 'any', 'all', 'each',\n",
    "    'does', 'also', 'just', 'more', 'most', 'very', 'too', 'only',\n",
    "}\n",
    "\n",
    "def extract_keywords(text):\n",
    "    words = re.sub(r'[^\\w\\s]', '', text.lower()).split()\n",
    "    return [w for w in words if w not in STOP_WORDS and len(w) > 2]\n",
    "\n",
    "def make_surrogate_paraphrase(query):\n",
    "    '''Paraphrase: reverse keyword order.'''\n",
    "    keywords = extract_keywords(query)\n",
    "    return \" \".join(keywords[::-1]) if keywords else query\n",
    "\n",
    "def make_surrogate_from_doc(passage):\n",
    "    '''Extract top-5 keywords from document.'''\n",
    "    content_words = extract_keywords(passage)\n",
    "    if not content_words:\n",
    "        return \"information\"\n",
    "    counts = Counter(content_words)\n",
    "    return \" \".join(w for w, _ in counts.most_common(5))\n",
    "\n",
    "STATIC_PREFIX = \"What are the key facts?\"\n",
    "\n",
    "print(\"Helpers defined.\")\n",
    "print(\"CRITICAL DIFFERENCE from Exp 33:\")\n",
    "print(\"  score_answer_nll(encoder_text, answer_text)\")\n",
    "print(\"  Decoder sees ONLY the answer -- NO query in decoder\")\n",
    "print(\"  Encoder representation is the SOLE source of context\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 5: Load data\n",
    "from lib.data import count_words\n",
    "from datasets import load_dataset\n",
    "\n",
    "print(\"Loading MS MARCO v1.1 validation...\")\n",
    "ds = load_dataset(\"microsoft/ms_marco\", \"v1.1\", split=\"validation\")\n",
    "\n",
    "samples = []\n",
    "for item in ds:\n",
    "    if len(samples) >= N_SAMPLES * 3:\n",
    "        break\n",
    "    passages = item.get('passages', {})\n",
    "    ptexts = passages.get('passage_text', [])\n",
    "    is_sel = passages.get('is_selected', [])\n",
    "    query = item.get('query', '')\n",
    "    answers = item.get('answers', [])\n",
    "    well_formed = item.get('wellFormedAnswers', [])\n",
    "\n",
    "    answer = None\n",
    "    if well_formed and len(well_formed) > 0 and well_formed[0] not in ('[]', ''):\n",
    "        answer = well_formed[0]\n",
    "    elif answers and len(answers) > 0 and answers[0] != 'No Answer Present.':\n",
    "        answer = answers[0]\n",
    "    if not answer:\n",
    "        continue\n",
    "\n",
    "    for pt, sel in zip(ptexts, is_sel):\n",
    "        wc = count_words(pt)\n",
    "        if sel == 1 and 30 <= wc <= 300:\n",
    "            samples.append({\n",
    "                'passage': pt, 'query': query, 'answer': answer,\n",
    "                'word_count': wc\n",
    "            })\n",
    "            break\n",
    "\n",
    "np.random.seed(SEED)\n",
    "np.random.shuffle(samples)\n",
    "samples = samples[:N_SAMPLES]\n",
    "del ds\n",
    "gc.collect()\n",
    "\n",
    "for s in samples:\n",
    "    s['surrogate_para'] = make_surrogate_paraphrase(s['query'])\n",
    "    s['surrogate_doc_kw'] = make_surrogate_from_doc(s['passage'])\n",
    "\n",
    "print(f\"Selected {len(samples)} samples\")\n",
    "print(f\"Word counts: mean={np.mean([s['word_count'] for s in samples]):.0f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 6: Explain experimental conditions\n",
    "print(\"=\" * 70)\n",
    "print(\"EXPERIMENTAL CONDITIONS -- CORRECTED\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "CONDITIONS = {\n",
    "    'bare':      lambda s: s['passage'],\n",
    "    'oracle':    lambda s: s['query'] + \"\\n\" + s['passage'],\n",
    "    'static':    lambda s: STATIC_PREFIX + \"\\n\" + s['passage'],\n",
    "    'surr_para': lambda s: s['surrogate_para'] + \"\\n\" + s['passage'],\n",
    "    'surr_doc':  lambda s: s['surrogate_doc_kw'] + \"\\n\" + s['passage'],\n",
    "}\n",
    "\n",
    "ex = samples[0]\n",
    "print(f\"\\nExample query:  {ex['query'][:70]}\")\n",
    "print(f\"Example answer: {ex['answer'][:70]}\")\n",
    "\n",
    "for name, fn in CONDITIONS.items():\n",
    "    enc_input = fn(ex)\n",
    "    n_tokens = len(tokenizer(enc_input, add_special_tokens=True).input_ids)\n",
    "    print(f\"\\n### {name} ###\")\n",
    "    print(f\"  Encoder ({n_tokens} tok): {enc_input[:100]}...\")\n",
    "\n",
    "print(f\"\\n### Decoder (same for ALL conditions) ###\")\n",
    "print(f\"  Target: '{ex['answer'][:80]}...'\")\n",
    "print(f\"  NO query in decoder -- encoder is the ONLY context source!\")\n",
    "print(f\"  This means:\")\n",
    "print(f\"    - bare:   decoder must produce answer knowing only the document\")\n",
    "print(f\"    - oracle: decoder knows what question to answer (via encoder)\")\n",
    "print(f\"    - surr:   decoder has approximate question info (via encoder)\")\n",
    "\n",
    "print(f\"\\n--- Surrogate examples ---\")\n",
    "for i in range(5):\n",
    "    s = samples[i]\n",
    "    print(f\"\\n  Real query: {s['query'][:55]}\")\n",
    "    print(f\"  Paraphrase: {s['surrogate_para'][:55]}\")\n",
    "    print(f\"  Doc KW:     {s['surrogate_doc_kw'][:55]}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 7: Run scoring\n",
    "print(\"=\" * 70)\n",
    "print(\"RUNNING EXPERIMENT\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "cond_names = list(CONDITIONS.keys())\n",
    "\n",
    "all_results = []\n",
    "start_idx = 0\n",
    "\n",
    "if CHECKPOINT_PATH.exists():\n",
    "    ckpt = json.loads(CHECKPOINT_PATH.read_text())\n",
    "    if ckpt.get('n_total') == N_SAMPLES and len(ckpt.get('results', [])) > 0:\n",
    "        saved_queries = [r['query'][:50] for r in ckpt['results']]\n",
    "        current_queries = [s['query'][:50] for s in samples[:len(saved_queries)]]\n",
    "        if saved_queries == current_queries:\n",
    "            all_results = ckpt['results']\n",
    "            start_idx = len(all_results)\n",
    "            print(f\"Resuming from checkpoint: {start_idx}/{N_SAMPLES}\")\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "for i in tqdm(range(start_idx, N_SAMPLES), initial=start_idx, total=N_SAMPLES, desc=\"Scoring\"):\n",
    "    s = samples[i]\n",
    "\n",
    "    result = {\n",
    "        'query': s['query'],\n",
    "        'answer': s['answer'],\n",
    "        'passage_words': s['word_count'],\n",
    "        'surrogate_para': s['surrogate_para'],\n",
    "        'surrogate_doc_kw': s['surrogate_doc_kw'],\n",
    "    }\n",
    "\n",
    "    for cond_name, cond_fn in CONDITIONS.items():\n",
    "        encoder_text = cond_fn(s)\n",
    "        # CORRECTED: decoder gets answer ONLY, no query\n",
    "        nll = score_answer_nll(encoder_text, s['answer'])\n",
    "        result[f'nll_{cond_name}'] = nll\n",
    "\n",
    "    all_results.append(result)\n",
    "\n",
    "    if (i + 1) % 20 == 0 or i == N_SAMPLES - 1:\n",
    "        ckpt = {\n",
    "            'n_total': N_SAMPLES, 'results': all_results,\n",
    "            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        }\n",
    "        CHECKPOINT_PATH.write_text(json.dumps(ckpt))\n",
    "        elapsed = time.time() - t0\n",
    "        done = i - start_idx + 1\n",
    "        eta = (N_SAMPLES - i - 1) * elapsed / done if done > 0 else 0\n",
    "        tqdm.write(f\"  Checkpoint {i+1}/{N_SAMPLES} | {elapsed/60:.1f}m | ETA {eta/60:.1f}m\")\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "elapsed = time.time() - t0\n",
    "print(f\"\\nScoring complete: {len(all_results)} samples in {elapsed/60:.1f} min\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 8: Results\n",
    "from lib.analysis import cohens_d\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"RESULTS (N={len(all_results)})\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "bare_nlls = np.array([r['nll_bare'] for r in all_results])\n",
    "\n",
    "print(f\"\\n{'Condition':<15} {'Mean NLL':>10} {'vs Bare':>10} {'d':>8} {'Win%':>8} {'p':>12} {'sig':>5}\")\n",
    "print(\"-\" * 73)\n",
    "\n",
    "analysis = {}\n",
    "for cond in cond_names:\n",
    "    nlls = np.array([r[f'nll_{cond}'] for r in all_results])\n",
    "    mean_nll = nlls.mean()\n",
    "    diff = bare_nlls - nlls  # positive = condition better (lower NLL)\n",
    "    d = cohens_d(diff)\n",
    "    win_pct = 100 * np.mean(diff > 0)\n",
    "\n",
    "    if cond == 'bare':\n",
    "        print(f\"{cond:<15} {mean_nll:>10.4f} {'--':>10} {'--':>8} {'--':>8} {'--':>12} {'--':>5}\")\n",
    "        analysis[cond] = {'mean_nll': float(mean_nll)}\n",
    "    else:\n",
    "        t_stat, p_val = stats.ttest_1samp(diff, 0)\n",
    "        sig = '***' if p_val < 0.001 else '**' if p_val < 0.01 else '*' if p_val < 0.05 else 'ns'\n",
    "        print(f\"{cond:<15} {mean_nll:>10.4f} {diff.mean():>+10.4f} {d:>+8.3f} {win_pct:>7.1f}% {p_val:>12.2e} {sig:>5}\")\n",
    "        analysis[cond] = {\n",
    "            'mean_nll': float(mean_nll), 'delta_vs_bare': float(diff.mean()),\n",
    "            'cohens_d': float(d), 'win_pct': float(win_pct), 'p_value': float(p_val),\n",
    "        }\n",
    "\n",
    "# Pairwise\n",
    "print(f\"\\n--- Pairwise Cohen's d (row better than column = positive) ---\")\n",
    "print(f\"{'':>15}\", end='')\n",
    "for c in cond_names:\n",
    "    print(f\" {c:>12}\", end='')\n",
    "print()\n",
    "for c1 in cond_names:\n",
    "    nlls1 = np.array([r[f'nll_{c1}'] for r in all_results])\n",
    "    print(f\"{c1:<15}\", end='')\n",
    "    for c2 in cond_names:\n",
    "        if c1 == c2:\n",
    "            print(f\" {'--':>12}\", end='')\n",
    "        else:\n",
    "            nlls2 = np.array([r[f'nll_{c2}'] for r in all_results])\n",
    "            diff = nlls2 - nlls1\n",
    "            d = cohens_d(diff)\n",
    "            print(f\" {d:>+12.3f}\", end='')\n",
    "    print()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 9: Transfer analysis\n",
    "print(\"=\" * 70)\n",
    "print(\"TRANSFER ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "oracle_nlls = np.array([r['nll_oracle'] for r in all_results])\n",
    "surr_para_nlls = np.array([r['nll_surr_para'] for r in all_results])\n",
    "surr_doc_nlls = np.array([r['nll_surr_doc'] for r in all_results])\n",
    "static_nlls = np.array([r['nll_static'] for r in all_results])\n",
    "\n",
    "# Oracle-bare gap = the total benefit of query awareness\n",
    "oracle_gap = bare_nlls.mean() - oracle_nlls.mean()\n",
    "print(f\"\\nOracle-bare NLL gap: {oracle_gap:+.4f}\")\n",
    "print(f\"  (positive = oracle better, this is the 'prize' to capture)\")\n",
    "\n",
    "# How much of the gap does each surrogate capture?\n",
    "for name, nlls in [('static', static_nlls), ('surr_para', surr_para_nlls),\n",
    "                    ('surr_doc', surr_doc_nlls)]:\n",
    "    gap = bare_nlls.mean() - nlls.mean()\n",
    "    if oracle_gap > 0:\n",
    "        ratio = gap / oracle_gap * 100\n",
    "    else:\n",
    "        ratio = float('nan')\n",
    "    print(f\"\\n  {name}:\")\n",
    "    print(f\"    Gap captured: {gap:+.4f} ({ratio:.0f}% of oracle gap)\")\n",
    "\n",
    "# Per-sample correlations\n",
    "oracle_delta = bare_nlls - oracle_nlls\n",
    "surr_para_delta = bare_nlls - surr_para_nlls\n",
    "surr_doc_delta = bare_nlls - surr_doc_nlls\n",
    "\n",
    "r_op, p_op = stats.pearsonr(oracle_delta, surr_para_delta)\n",
    "r_od, p_od = stats.pearsonr(oracle_delta, surr_doc_delta)\n",
    "r_pd, p_pd = stats.pearsonr(surr_para_delta, surr_doc_delta)\n",
    "\n",
    "print(f\"\\n--- Per-sample correlations ---\")\n",
    "print(f\"  oracle vs surr_para: r={r_op:.3f} (p={p_op:.2e})\")\n",
    "print(f\"  oracle vs surr_doc:  r={r_od:.3f} (p={p_od:.2e})\")\n",
    "print(f\"  surr_para vs surr_doc: r={r_pd:.3f} (p={p_pd:.2e})\")\n",
    "\n",
    "# Hardness gradient\n",
    "print(f\"\\n--- Hardness gradient (by bare NLL quintile) ---\")\n",
    "quintile_bounds = np.percentile(bare_nlls, [20, 40, 60, 80])\n",
    "quintiles = np.digitize(bare_nlls, quintile_bounds)\n",
    "\n",
    "print(f\"{'Quintile':<12} {'N':>4} {'bare':>10} {'oracle':>10} {'surr_para':>10} {'surr_doc':>10} {'orc-bare':>10} {'sp-bare':>10}\")\n",
    "print(\"-\" * 78)\n",
    "\n",
    "for q in range(5):\n",
    "    mask = quintiles == q\n",
    "    n_q = mask.sum()\n",
    "    if n_q < 3:\n",
    "        continue\n",
    "    qlabel = ['Q1 easy', 'Q2', 'Q3', 'Q4', 'Q5 hard'][q]\n",
    "    b = bare_nlls[mask].mean()\n",
    "    o = oracle_nlls[mask].mean()\n",
    "    sp = surr_para_nlls[mask].mean()\n",
    "    sd = surr_doc_nlls[mask].mean()\n",
    "    print(f\"{qlabel:<12} {n_q:>4} {b:>10.4f} {o:>10.4f} {sp:>10.4f} {sd:>10.4f} {b-o:>+10.4f} {b-sp:>+10.4f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 10: Verdict and save\n",
    "print(\"=\" * 70)\n",
    "print(\"VERDICT -- Exp 33b: Corrected Surrogate Transfer\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\nModel: {MODEL_NAME}\")\n",
    "print(f\"N: {len(all_results)} samples\")\n",
    "print(f\"Setup: query in encoder ONLY, decoder sees answer ONLY\")\n",
    "\n",
    "oracle_d = analysis.get('oracle', {}).get('cohens_d', 0)\n",
    "static_d = analysis.get('static', {}).get('cohens_d', 0)\n",
    "surr_para_d = analysis.get('surr_para', {}).get('cohens_d', 0)\n",
    "surr_doc_d = analysis.get('surr_doc', {}).get('cohens_d', 0)\n",
    "\n",
    "oracle_gap = bare_nlls.mean() - oracle_nlls.mean()\n",
    "\n",
    "print(f\"\\n--- Core question: does query in encoder help? ---\")\n",
    "print(f\"  oracle d = {oracle_d:+.3f}\")\n",
    "if oracle_d > 0.2:\n",
    "    print(f\"  YES -- strong benefit. Query-aware encoding helps the decoder.\")\n",
    "elif oracle_d > 0.05:\n",
    "    print(f\"  MODERATE -- some benefit from query-aware encoding.\")\n",
    "elif oracle_d > 0:\n",
    "    print(f\"  MARGINAL -- barely helps.\")\n",
    "else:\n",
    "    print(f\"  NO -- even with query as sole context source, encoding doesn't help.\")\n",
    "\n",
    "print(f\"\\n--- Surrogate transfer ---\")\n",
    "if oracle_gap > 0:\n",
    "    for name, d_val in [('surr_para', surr_para_d), ('surr_doc', surr_doc_d), ('static', static_d)]:\n",
    "        gap = analysis.get(name, {}).get('delta_vs_bare', 0)\n",
    "        ratio = gap / oracle_gap * 100 if oracle_gap > 0 else 0\n",
    "        print(f\"  {name}: d={d_val:+.3f}, captures {ratio:.0f}% of oracle gap\")\n",
    "else:\n",
    "    print(f\"  Oracle gap is zero or negative -- no benefit to transfer.\")\n",
    "\n",
    "# Expected hierarchy check\n",
    "expected = ['oracle', 'surr_para', 'surr_doc', 'static', 'bare']\n",
    "actual_order = sorted(cond_names, key=lambda c: np.array([r[f'nll_{c}'] for r in all_results]).mean())\n",
    "print(f\"\\n--- Expected vs actual ranking (best to worst NLL) ---\")\n",
    "print(f\"  Expected: {' > '.join(expected)}\")\n",
    "print(f\"  Actual:   {' > '.join(actual_order)}\")\n",
    "\n",
    "# Comparison to Exp 33 (wrong setup)\n",
    "print(f\"\\n--- Exp 33 vs 33b comparison ---\")\n",
    "print(f\"  Exp 33  (query in decoder): oracle d = -0.175 (HURT)\")\n",
    "print(f\"  Exp 33b (query in encoder): oracle d = {oracle_d:+.3f}\")\n",
    "if oracle_d > 0:\n",
    "    print(f\"  CONFIRMED: removing query from decoder reveals the encoder benefit\")\n",
    "else:\n",
    "    print(f\"  Even with corrected setup, encoder query-awareness doesn't help\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "\n",
    "# Save\n",
    "final_results = {\n",
    "    'experiment': 'exp33b_corrected_surrogate',\n",
    "    'model': MODEL_NAME,\n",
    "    'n_samples': len(all_results),\n",
    "    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'setup': 'query in encoder ONLY, decoder scores answer ONLY',\n",
    "    'analysis': analysis,\n",
    "    'oracle_bare_gap': float(oracle_gap),\n",
    "    'correlations': {\n",
    "        'oracle_vs_surr_para': float(r_op),\n",
    "        'oracle_vs_surr_doc': float(r_od),\n",
    "    },\n",
    "}\n",
    "\n",
    "with open(RESULTS_DIR / 'results.json', 'w') as f:\n",
    "    json.dump(final_results, f, indent=2)\n",
    "print(f\"\\nResults saved to {RESULTS_DIR / 'results.json'}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 11: Cleanup\n",
    "print(\"Cleaning up GPU memory...\")\n",
    "mem_before = torch.cuda.memory_allocated() / 1e9\n",
    "del model, processor, tokenizer\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "mem_after = torch.cuda.memory_allocated() / 1e9\n",
    "print(f\"GPU memory: {mem_before:.2f} GB -> {mem_after:.2f} GB\")\n",
    "print(\"Done!\")"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}