{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 31: Query-Likelihood Ranking with PMI\n",
    "\n",
    "## Motivation\n",
    "\n",
    "Every ranking experiment so far (Exps 14, 15, 22, 23, 28) scored **NLL(answer | passage + query)**.\n",
    "But in ad serving, there IS no answer at query time — there's only the user's query and the ads.\n",
    "\n",
    "**Query-likelihood** — NLL(query | passage) — is a classic IR scoring function that measures\n",
    "how well a document predicts the query. This has never been tested in our framework.\n",
    "\n",
    "## Design\n",
    "\n",
    "Model: Gemma 3 4B (4-bit). Data: MS MARCO v1.1 validation, 200 queries × ~8 passages each.\n",
    "All caches are **bare** (no priming). This experiment is purely about scoring method comparison.\n",
    "\n",
    "| # | Method | Score | PMI baseline |\n",
    "|---|--------|-------|--------------|\n",
    "| 1 | `al` | NLL(answer \\| passage + query_template) | — |\n",
    "| 2 | `pmi_al` | al − NLL(answer \\| BOS + query_template) | BOS-only |\n",
    "| 3 | `ql` | NLL(query \\| passage + \"\\n\") | — |\n",
    "| 4 | `pmi_ql` | ql − NLL(query \\| BOS + \"\\n\") | BOS-only |\n",
    "| 5 | `ql_search` | NLL(query \\| passage + \"\\nSearch query: \") | — |\n",
    "| 6 | `pmi_ql_search` | ql_search − NLL(query \\| BOS + \"\\nSearch query: \") | BOS-only |\n",
    "\n",
    "## Success Criteria\n",
    "\n",
    "- **Primary:** Does any query-likelihood method achieve AUC > 0.80? (bare answer-likelihood = 0.828)\n",
    "- **Secondary:** Does PMI improve query-likelihood as it does for answer-likelihood?\n",
    "- **Informational:** How does query-likelihood correlate with answer-likelihood?\n",
    "\n",
    "## Reference Values (from Exp 22, Gemma 3 4B)\n",
    "\n",
    "| Method | AUC | MRR@10 |\n",
    "|--------|-----|--------|\n",
    "| Raw bare NLL (answer) | 0.828 | 0.860 |\n",
    "| PMI bare (answer) | 0.841 | 0.860 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 1: Setup & Imports\nimport os\nos.umask(0o000)\n\nimport sys\nimport json\nimport time\nimport csv\nimport numpy as np\nimport torch\nimport gc\nfrom pathlib import Path\nfrom scipy import stats\nfrom sklearn.metrics import roc_auc_score, roc_curve\nfrom datasets import load_dataset\nfrom tqdm.auto import tqdm\n\nsys.path.insert(0, \".\")\n\nfrom lib.config import ExperimentConfig\nfrom lib.model_utils import load_model\nfrom lib.kv_cache import (\n    _ensure_dynamic_cache,\n    score_answer_with_cache,\n    deepcopy_cache,\n)\nfrom lib.data import count_words\nfrom lib.analysis import cohens_d\n\nimport matplotlib\nmatplotlib.use('Agg')\nimport matplotlib.pyplot as plt\n\n# === Constants ===\nSEED = 42\nN_QUERIES = 200\nMAX_PASSAGE_WORDS = 300\nCHECKPOINT_EVERY = 10\n\nRESULTS_DIR = Path(\"results/exp31\")\nRESULTS_DIR.mkdir(parents=True, exist_ok=True)\nCHECKPOINT_PATH = RESULTS_DIR / \"checkpoint_eval.json\"\n\n# Scoring templates\nQUERY_TEMPLATE = \"\\nQuery: {query}\\nAnswer:\"\nANSWER_TEMPLATE = \" {answer}\"\nQL_NEWLINE_SEP = \"\\n\"                     # minimal separator for query-likelihood\nQL_SEARCH_SEP = \"\\nSearch query: \"         # search-framed query-likelihood\n\n# Reference values from Exp 22 (Gemma 3 4B)\nREF = {\n    'raw_bare_auc': 0.828,\n    'pmi_bare_auc': 0.841,\n    'raw_bare_mrr': 0.860,\n}\n\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\n\nprint(f\"Results dir: {RESULTS_DIR}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name()}\")\n    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Load Model\n",
    "exp_config = ExperimentConfig(\n",
    "    model_name=\"google/gemma-3-4b-it\",\n",
    "    model_type=\"gemma3\",\n",
    "    use_4bit=True,\n",
    "    compute_dtype=\"bfloat16\",\n",
    "    seed=SEED,\n",
    ")\n",
    "\n",
    "print(\"Loading Gemma 3 4B...\")\n",
    "model, tokenizer = load_model(exp_config)\n",
    "print(f\"Model loaded. dtype={model.dtype}\")\n",
    "print(f\"GPU memory: {torch.cuda.memory_allocated()/1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Load MS MARCO Multi-Passage Validation Data\n",
    "print(\"Loading MS MARCO v1.1 validation (multi-passage format)...\")\n",
    "dataset = load_dataset(\"microsoft/ms_marco\", \"v1.1\", split=\"validation\",\n",
    "                        trust_remote_code=True)\n",
    "print(f\"Total items: {len(dataset)}\")\n",
    "\n",
    "queries = []\n",
    "np.random.seed(SEED)\n",
    "\n",
    "for item in tqdm(dataset, desc=\"Filtering\"):\n",
    "    passages_info = item.get('passages', {})\n",
    "    passage_texts = passages_info.get('passage_text', [])\n",
    "    is_selected = passages_info.get('is_selected', [])\n",
    "    query = item.get('query', '')\n",
    "    answers = item.get('answers', [])\n",
    "    well_formed = item.get('wellFormedAnswers', [])\n",
    "\n",
    "    if not passage_texts or not query:\n",
    "        continue\n",
    "    # Require at least 1 relevant passage\n",
    "    if not is_selected or sum(is_selected) == 0:\n",
    "        continue\n",
    "    # All passages must be within word limits\n",
    "    word_counts = [count_words(p) for p in passage_texts]\n",
    "    if any(wc > MAX_PASSAGE_WORDS for wc in word_counts):\n",
    "        continue\n",
    "\n",
    "    # Require valid answer (needed for answer-likelihood reference)\n",
    "    answer = None\n",
    "    if well_formed and len(well_formed) > 0 and well_formed[0] != '[]':\n",
    "        answer = well_formed[0]\n",
    "    elif answers and len(answers) > 0 and answers[0] != 'No Answer Present.':\n",
    "        answer = answers[0]\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "    passage_list = []\n",
    "    for i, (ptext, sel) in enumerate(zip(passage_texts, is_selected)):\n",
    "        passage_list.append({\n",
    "            'passage': ptext,\n",
    "            'is_relevant': bool(sel == 1),\n",
    "            'word_count': word_counts[i],\n",
    "            'passage_idx': i,\n",
    "        })\n",
    "\n",
    "    queries.append({\n",
    "        'query': query,\n",
    "        'answer': answer,\n",
    "        'passages': passage_list,\n",
    "        'n_passages': len(passage_list),\n",
    "        'n_relevant': sum(1 for p in passage_list if p['is_relevant']),\n",
    "    })\n",
    "\n",
    "    if len(queries) >= N_QUERIES * 3:\n",
    "        break\n",
    "\n",
    "np.random.shuffle(queries)\n",
    "queries = queries[:N_QUERIES]\n",
    "\n",
    "# Stats\n",
    "total_passages = sum(q['n_passages'] for q in queries)\n",
    "total_relevant = sum(q['n_relevant'] for q in queries)\n",
    "print(f\"\\nLoaded {len(queries)} queries\")\n",
    "print(f\"Total passages: {total_passages} (mean {total_passages/len(queries):.1f}/query)\")\n",
    "print(f\"Relevant: {total_relevant} ({100*total_relevant/total_passages:.1f}%)\")\n",
    "print(f\"Query lengths (words): mean={np.mean([count_words(q['query']) for q in queries]):.1f}\")\n",
    "print(f\"Answer lengths (words): mean={np.mean([count_words(q['answer']) for q in queries]):.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Explain Experimental Conditions\n",
    "print(\"=\" * 70)\n",
    "print(\"EXPERIMENTAL CONDITIONS EXPLAINED\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "ex_query = queries[0]['query']\n",
    "ex_answer = queries[0]['answer']\n",
    "ex_passage = queries[0]['passages'][0]['passage'][:100] + \"...\"\n",
    "\n",
    "print(f\"\\nExample query:   {ex_query}\")\n",
    "print(f\"Example answer:  {ex_answer[:80]}...\")\n",
    "print(f\"Example passage: {ex_passage}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ANSWER-LIKELIHOOD (standard — reference from Exp 22)\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Cache: [BOS][passage tokens]\")\n",
    "print(f\"Score: NLL( \\\"{ANSWER_TEMPLATE.format(answer=ex_answer[:40])}...\\\"\")\n",
    "print(f\"       | passage + \\\"\\\\nQuery: {ex_query}\\\\nAnswer:\\\" )\")\n",
    "print(f\"PMI baseline: NLL(answer | BOS + query_template)  [no passage]\")\n",
    "print(f\"Key insight: Measures how much the passage helps predict the ANSWER.\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"QUERY-LIKELIHOOD with newline separator (NEW)\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Cache: [BOS][passage tokens]\")\n",
    "print(f\"Score: NLL( \\\"{ex_query}\\\" | passage + \\\"\\\\n\\\" )\")\n",
    "print(f\"PMI baseline: NLL(query | BOS + \\\"\\\\n\\\")  [no passage]\")\n",
    "print(f\"Key insight: Measures how much the passage helps predict the QUERY.\")\n",
    "print(f\"  If passage is about running shoes and query is 'best running shoes',\")\n",
    "print(f\"  the passage should lower NLL(query) = more relevant.\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"QUERY-LIKELIHOOD with search frame (NEW)\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Cache: [BOS][passage tokens]\")\n",
    "print(f\"Score: NLL( \\\"{ex_query}\\\" | passage + \\\"\\\\nSearch query: \\\" )\")\n",
    "print(f\"PMI baseline: NLL(query | BOS + \\\"\\\\nSearch query: \\\")  [no passage]\")\n",
    "print(f\"Key insight: Same as above but with explicit search framing.\")\n",
    "print(f\"  The frame 'Search query: ' may tell the model to predict search terms.\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(f\"TOTAL: 6 scoring methods (3 raw + 3 PMI)\")\n",
    "print(f\"Per passage: 1 cache build + 3 scoring calls\")\n",
    "print(f\"Per query: 3 baseline scoring calls\")\n",
    "print(f\"Total: {N_QUERIES} queries × ~8 passages = ~{N_QUERIES * 8} passage scores\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Helper Functions\n",
    "\n",
    "def build_bare_cache(passage_text, model, tokenizer, config):\n",
    "    \"\"\"Build bare (no prefix) KV cache for a passage.\n",
    "    Returns: (cache, context_len)\n",
    "    \"\"\"\n",
    "    enc = tokenizer(passage_text, return_tensors=\"pt\",\n",
    "                    add_special_tokens=True, padding=False, truncation=False)\n",
    "    input_ids = enc['input_ids'].to(config.device)\n",
    "    with torch.no_grad():\n",
    "        out = model(input_ids=input_ids,\n",
    "                    attention_mask=torch.ones_like(input_ids),\n",
    "                    use_cache=True, return_dict=True)\n",
    "    cache = _ensure_dynamic_cache(out.past_key_values)\n",
    "    context_len = input_ids.shape[1]\n",
    "    del out, input_ids, enc\n",
    "    return cache, context_len\n",
    "\n",
    "\n",
    "def build_bos_cache(model, tokenizer, config):\n",
    "    \"\"\"Build BOS-only cache for PMI baselines.\n",
    "    Returns: (cache, context_len=1)\n",
    "    \"\"\"\n",
    "    bos_id = torch.tensor([[tokenizer.bos_token_id]], device=config.device)\n",
    "    with torch.no_grad():\n",
    "        out = model(input_ids=bos_id,\n",
    "                    attention_mask=torch.ones_like(bos_id),\n",
    "                    use_cache=True, return_dict=True)\n",
    "    cache = _ensure_dynamic_cache(out.past_key_values)\n",
    "    del out\n",
    "    return cache, 1\n",
    "\n",
    "\n",
    "def score_all_methods(cache, context_len, query, answer, model, tokenizer, config):\n",
    "    \"\"\"Score a passage with all 3 scoring methods (answer-likelihood + 2 query-likelihood).\n",
    "    Requires 3 deepcopy calls (cache is mutated by scoring).\n",
    "    Returns: dict with nll_al, nll_ql, nll_ql_search\n",
    "    \"\"\"\n",
    "    query_prompt = QUERY_TEMPLATE.format(query=query)\n",
    "    answer_text = ANSWER_TEMPLATE.format(answer=answer)\n",
    "\n",
    "    # 1. Answer-likelihood: NLL(answer | passage + query_template)\n",
    "    nll_al = score_answer_with_cache(\n",
    "        deepcopy_cache(cache), context_len,\n",
    "        query_prompt, answer_text, model, tokenizer, config)\n",
    "\n",
    "    # 2. Query-likelihood (newline): NLL(query | passage + \"\\n\")\n",
    "    nll_ql = score_answer_with_cache(\n",
    "        deepcopy_cache(cache), context_len,\n",
    "        QL_NEWLINE_SEP, query, model, tokenizer, config)\n",
    "\n",
    "    # 3. Query-likelihood (search frame): NLL(query | passage + \"\\nSearch query: \")\n",
    "    nll_ql_search = score_answer_with_cache(\n",
    "        deepcopy_cache(cache), context_len,\n",
    "        QL_SEARCH_SEP, query, model, tokenizer, config)\n",
    "\n",
    "    return {\n",
    "        'nll_al': nll_al,\n",
    "        'nll_ql': nll_ql,\n",
    "        'nll_ql_search': nll_ql_search,\n",
    "    }\n",
    "\n",
    "\n",
    "def score_baselines(query, answer, model, tokenizer, config):\n",
    "    \"\"\"Compute BOS-only baselines for PMI normalization (once per query).\n",
    "    Returns: dict with baseline NLLs for all 3 methods\n",
    "    \"\"\"\n",
    "    bos_cache, bos_len = build_bos_cache(model, tokenizer, config)\n",
    "\n",
    "    query_prompt = QUERY_TEMPLATE.format(query=query)\n",
    "    answer_text = ANSWER_TEMPLATE.format(answer=answer)\n",
    "\n",
    "    # Answer baseline\n",
    "    bl_al = score_answer_with_cache(\n",
    "        deepcopy_cache(bos_cache), bos_len,\n",
    "        query_prompt, answer_text, model, tokenizer, config)\n",
    "\n",
    "    # Query baseline (newline)\n",
    "    bl_ql = score_answer_with_cache(\n",
    "        deepcopy_cache(bos_cache), bos_len,\n",
    "        QL_NEWLINE_SEP, query, model, tokenizer, config)\n",
    "\n",
    "    # Query baseline (search frame)\n",
    "    bl_ql_search = score_answer_with_cache(\n",
    "        bos_cache, bos_len,  # last call, no need to copy\n",
    "        QL_SEARCH_SEP, query, model, tokenizer, config)\n",
    "\n",
    "    return {\n",
    "        'bl_al': bl_al,\n",
    "        'bl_ql': bl_ql,\n",
    "        'bl_ql_search': bl_ql_search,\n",
    "    }\n",
    "\n",
    "\n",
    "# === Smoke Test ===\n",
    "print(\"Smoke test on first query, first passage...\")\n",
    "q0 = queries[0]\n",
    "p0 = q0['passages'][0]\n",
    "\n",
    "cache_test, ctx_test = build_bare_cache(p0['passage'], model, tokenizer, exp_config)\n",
    "scores_test = score_all_methods(cache_test, ctx_test, q0['query'], q0['answer'],\n",
    "                                 model, tokenizer, exp_config)\n",
    "baselines_test = score_baselines(q0['query'], q0['answer'], model, tokenizer, exp_config)\n",
    "del cache_test\n",
    "\n",
    "print(f\"  Passage ({p0['word_count']} words, relevant={p0['is_relevant']}):\")\n",
    "print(f\"    Answer-likelihood:  NLL={scores_test['nll_al']:.4f}  (baseline={baselines_test['bl_al']:.4f})\")\n",
    "print(f\"    Query-likelihood:   NLL={scores_test['nll_ql']:.4f}  (baseline={baselines_test['bl_ql']:.4f})\")\n",
    "print(f\"    Query-lik (search): NLL={scores_test['nll_ql_search']:.4f}  (baseline={baselines_test['bl_ql_search']:.4f})\")\n",
    "print(f\"  PMI_al = {scores_test['nll_al'] - baselines_test['bl_al']:.4f}\")\n",
    "print(f\"  PMI_ql = {scores_test['nll_ql'] - baselines_test['bl_ql']:.4f}\")\n",
    "print(f\"  PMI_ql_search = {scores_test['nll_ql_search'] - baselines_test['bl_ql_search']:.4f}\")\n",
    "print(\"Smoke test passed!\")\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Main Evaluation Loop\n",
    "print(f\"\\nEvaluating {N_QUERIES} queries...\")\n",
    "print(f\"Checkpoint every {CHECKPOINT_EVERY} queries to {CHECKPOINT_PATH}\")\n",
    "\n",
    "all_results = []\n",
    "start_idx = 0\n",
    "\n",
    "# Resume from checkpoint\n",
    "if CHECKPOINT_PATH.exists():\n",
    "    with open(CHECKPOINT_PATH, 'r') as f:\n",
    "        ckpt = json.load(f)\n",
    "    ckpt_queries = ckpt.get('query_texts', [])\n",
    "    current_queries = [q['query'] for q in queries]\n",
    "    if ckpt_queries == current_queries:\n",
    "        all_results = ckpt['results']\n",
    "        start_idx = len(all_results)\n",
    "        print(f\"Resumed from checkpoint: {start_idx}/{N_QUERIES}\")\n",
    "    else:\n",
    "        print(\"Checkpoint query mismatch — starting fresh.\")\n",
    "\n",
    "t_start = time.time()\n",
    "\n",
    "for qidx in tqdm(range(start_idx, N_QUERIES), initial=start_idx, total=N_QUERIES,\n",
    "                  desc=\"Queries\"):\n",
    "    q = queries[qidx]\n",
    "    query_text = q['query']\n",
    "    answer_text = q['answer']\n",
    "\n",
    "    # Compute baselines (once per query)\n",
    "    baselines = score_baselines(query_text, answer_text, model, tokenizer, exp_config)\n",
    "\n",
    "    passage_data = []\n",
    "    for pidx, p in enumerate(q['passages']):\n",
    "        # Build bare cache\n",
    "        cache, ctx_len = build_bare_cache(p['passage'], model, tokenizer, exp_config)\n",
    "\n",
    "        # Score all methods\n",
    "        scores = score_all_methods(cache, ctx_len, query_text, answer_text,\n",
    "                                    model, tokenizer, exp_config)\n",
    "        del cache\n",
    "\n",
    "        passage_data.append({\n",
    "            'passage_idx': p['passage_idx'],\n",
    "            'is_relevant': p['is_relevant'],\n",
    "            'word_count': p['word_count'],\n",
    "            'doc_len': ctx_len - 1,  # exclude BOS\n",
    "            'nll_al': scores['nll_al'],\n",
    "            'nll_ql': scores['nll_ql'],\n",
    "            'nll_ql_search': scores['nll_ql_search'],\n",
    "            'bl_al': baselines['bl_al'],\n",
    "            'bl_ql': baselines['bl_ql'],\n",
    "            'bl_ql_search': baselines['bl_ql_search'],\n",
    "        })\n",
    "\n",
    "    all_results.append({\n",
    "        'query_idx': qidx,\n",
    "        'query': query_text,\n",
    "        'answer': answer_text,\n",
    "        'n_passages': q['n_passages'],\n",
    "        'n_relevant': q['n_relevant'],\n",
    "        'baselines': baselines,\n",
    "        'passage_data': passage_data,\n",
    "    })\n",
    "\n",
    "    # Checkpoint\n",
    "    if (qidx + 1) % CHECKPOINT_EVERY == 0 or qidx == N_QUERIES - 1:\n",
    "        ckpt_data = {\n",
    "            'results': all_results,\n",
    "            'query_texts': [q['query'] for q in queries],\n",
    "            'completed': len(all_results),\n",
    "            'total': N_QUERIES,\n",
    "            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        }\n",
    "        with open(CHECKPOINT_PATH, 'w') as f:\n",
    "            json.dump(ckpt_data, f)\n",
    "\n",
    "    # Periodic cleanup\n",
    "    if (qidx + 1) % 20 == 0:\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "elapsed = time.time() - t_start\n",
    "print(f\"\\nDone! {len(all_results)} queries scored in {elapsed/60:.1f} min\")\n",
    "print(f\"Speed: {elapsed/len(all_results):.1f} s/query\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Analysis — AUC, MRR, Differential NLL\n",
    "\n",
    "# === Flatten passage-level data ===\n",
    "is_relevant_all = []\n",
    "nll_al_all, nll_ql_all, nll_ql_search_all = [], [], []\n",
    "bl_al_all, bl_ql_all, bl_ql_search_all = [], [], []\n",
    "\n",
    "for r in all_results:\n",
    "    for p in r['passage_data']:\n",
    "        is_relevant_all.append(int(p['is_relevant']))\n",
    "        nll_al_all.append(p['nll_al'])\n",
    "        nll_ql_all.append(p['nll_ql'])\n",
    "        nll_ql_search_all.append(p['nll_ql_search'])\n",
    "        bl_al_all.append(p['bl_al'])\n",
    "        bl_ql_all.append(p['bl_ql'])\n",
    "        bl_ql_search_all.append(p['bl_ql_search'])\n",
    "\n",
    "is_relevant = np.array(is_relevant_all)\n",
    "nll_al = np.array(nll_al_all)\n",
    "nll_ql = np.array(nll_ql_all)\n",
    "nll_ql_search = np.array(nll_ql_search_all)\n",
    "bl_al = np.array(bl_al_all)\n",
    "bl_ql = np.array(bl_ql_all)\n",
    "bl_ql_search = np.array(bl_ql_search_all)\n",
    "\n",
    "# PMI scores\n",
    "pmi_al = nll_al - bl_al\n",
    "pmi_ql = nll_ql - bl_ql\n",
    "pmi_ql_search = nll_ql_search - bl_ql_search\n",
    "\n",
    "n_total = len(is_relevant)\n",
    "n_rel = is_relevant.sum()\n",
    "n_irr = n_total - n_rel\n",
    "print(f\"Total passages: {n_total} ({n_rel} relevant, {n_irr} irrelevant)\")\n",
    "\n",
    "# === AUC-ROC ===\n",
    "scoring_methods = {\n",
    "    'Raw AL (answer-likelihood)': nll_al,\n",
    "    'PMI AL': pmi_al,\n",
    "    'Raw QL (query-likelihood)': nll_ql,\n",
    "    'PMI QL': pmi_ql,\n",
    "    'Raw QL-search': nll_ql_search,\n",
    "    'PMI QL-search': pmi_ql_search,\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"AUC-ROC RESULTS\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Method':<35} {'AUC':>8} {'vs Exp22 ref':>15}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "auc_results = {}\n",
    "for name, scores in scoring_methods.items():\n",
    "    auc = roc_auc_score(is_relevant, -scores)  # negate: lower NLL = more relevant\n",
    "    auc_results[name] = float(auc)\n",
    "    ref_str = \"\"\n",
    "    if 'AL' in name and 'PMI' not in name:\n",
    "        ref_str = f\"(ref: {REF['raw_bare_auc']:.3f})\"\n",
    "    elif 'PMI AL' in name:\n",
    "        ref_str = f\"(ref: {REF['pmi_bare_auc']:.3f})\"\n",
    "    print(f\"{name:<35} {auc:>8.3f} {ref_str:>15}\")\n",
    "\n",
    "# === MRR@10 ===\n",
    "def compute_mrr_at_k(all_results, score_fn, k=10):\n",
    "    rr_list = []\n",
    "    for r in all_results:\n",
    "        passages = r['passage_data']\n",
    "        scored = [(score_fn(p), p['is_relevant']) for p in passages]\n",
    "        scored.sort(key=lambda x: x[0])  # lower = more relevant\n",
    "        rr = 0.0\n",
    "        for rank, (score, rel) in enumerate(scored[:k], 1):\n",
    "            if rel:\n",
    "                rr = 1.0 / rank\n",
    "                break\n",
    "        rr_list.append(rr)\n",
    "    return np.mean(rr_list), rr_list\n",
    "\n",
    "mrr_fns = {\n",
    "    'Raw AL': lambda p: p['nll_al'],\n",
    "    'PMI AL': lambda p: p['nll_al'] - p['bl_al'],\n",
    "    'Raw QL': lambda p: p['nll_ql'],\n",
    "    'PMI QL': lambda p: p['nll_ql'] - p['bl_ql'],\n",
    "    'Raw QL-search': lambda p: p['nll_ql_search'],\n",
    "    'PMI QL-search': lambda p: p['nll_ql_search'] - p['bl_ql_search'],\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"MRR@10 RESULTS\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Method':<35} {'MRR@10':>8} {'vs Exp22 ref':>15}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "mrr_results = {}\n",
    "mrr_per_query = {}\n",
    "for name, fn in mrr_fns.items():\n",
    "    mrr, rr_list = compute_mrr_at_k(all_results, fn, k=10)\n",
    "    mrr_results[name] = float(mrr)\n",
    "    mrr_per_query[name] = rr_list\n",
    "    ref_str = \"\"\n",
    "    if name == 'Raw AL':\n",
    "        ref_str = f\"(ref: {REF['raw_bare_mrr']:.3f})\"\n",
    "    print(f\"{name:<35} {mrr:>8.3f} {ref_str:>15}\")\n",
    "\n",
    "# === Differential NLL (Cohen's d between relevant vs irrelevant) ===\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"DIFFERENTIAL NLL (relevant vs irrelevant)\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Method':<25} {'Mean Rel':>10} {'Mean Irr':>10} {'Gap':>8} {'d':>8} {'p':>12}\")\n",
    "print(\"-\" * 75)\n",
    "\n",
    "diff_results = {}\n",
    "for name, scores in scoring_methods.items():\n",
    "    short_name = name.replace(' (answer-likelihood)', '').replace(' (query-likelihood)', '')\n",
    "    rel_vals = scores[is_relevant == 1]\n",
    "    irr_vals = scores[is_relevant == 0]\n",
    "    mean_rel = np.mean(rel_vals)\n",
    "    mean_irr = np.mean(irr_vals)\n",
    "    gap = mean_irr - mean_rel  # positive = relevant gets lower score (good)\n",
    "    pooled_std = np.sqrt(\n",
    "        (np.var(rel_vals, ddof=1) * (len(rel_vals)-1) + np.var(irr_vals, ddof=1) * (len(irr_vals)-1)) /\n",
    "        (len(rel_vals) + len(irr_vals) - 2)\n",
    "    )\n",
    "    d = gap / pooled_std if pooled_std > 0 else 0\n",
    "    t_stat, p_val = stats.ttest_ind(irr_vals, rel_vals)\n",
    "    diff_results[name] = {'mean_rel': mean_rel, 'mean_irr': mean_irr, 'gap': gap, 'd': d, 'p': p_val}\n",
    "    print(f\"{short_name:<25} {mean_rel:>10.4f} {mean_irr:>10.4f} {gap:>+8.4f} {d:>+8.3f} {p_val:>12.2e}\")\n",
    "\n",
    "# === Correlation between methods ===\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"INTER-METHOD CORRELATIONS (Pearson r)\")\n",
    "print(\"=\" * 70)\n",
    "methods_for_corr = ['Raw AL (answer-likelihood)', 'Raw QL (query-likelihood)', 'Raw QL-search',\n",
    "                     'PMI AL', 'PMI QL', 'PMI QL-search']\n",
    "corr_arrays = [scoring_methods[m] for m in methods_for_corr]\n",
    "short_labels = ['Raw AL', 'Raw QL', 'Raw QL-s', 'PMI AL', 'PMI QL', 'PMI QL-s']\n",
    "\n",
    "print(f\"{'':>12}\", end=\"\")\n",
    "for sl in short_labels:\n",
    "    print(f\"{sl:>10}\", end=\"\")\n",
    "print()\n",
    "\n",
    "for i, (sl_i, arr_i) in enumerate(zip(short_labels, corr_arrays)):\n",
    "    print(f\"{sl_i:>12}\", end=\"\")\n",
    "    for j, arr_j in enumerate(corr_arrays):\n",
    "        r = np.corrcoef(arr_i, arr_j)[0, 1]\n",
    "        print(f\"{r:>10.3f}\", end=\"\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Plots\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "\n",
    "# --- Row 1: ROC curves ---\n",
    "# Panel 1: Raw scores\n",
    "ax = axes[0, 0]\n",
    "for name, color in [('Raw AL (answer-likelihood)', 'blue'),\n",
    "                      ('Raw QL (query-likelihood)', 'red'),\n",
    "                      ('Raw QL-search', 'orange')]:\n",
    "    fpr, tpr, _ = roc_curve(is_relevant, -scoring_methods[name])\n",
    "    ax.plot(fpr, tpr, color=color, label=f\"{name.split(' (')[0]} (AUC={auc_results[name]:.3f})\")\n",
    "ax.plot([0,1], [0,1], 'k--', alpha=0.3)\n",
    "ax.set_xlabel('FPR'); ax.set_ylabel('TPR')\n",
    "ax.set_title('ROC — Raw Scores')\n",
    "ax.legend(fontsize=8)\n",
    "\n",
    "# Panel 2: PMI scores\n",
    "ax = axes[0, 1]\n",
    "for name, color in [('PMI AL', 'blue'), ('PMI QL', 'red'), ('PMI QL-search', 'orange')]:\n",
    "    fpr, tpr, _ = roc_curve(is_relevant, -scoring_methods[name])\n",
    "    ax.plot(fpr, tpr, color=color, label=f\"{name} (AUC={auc_results[name]:.3f})\")\n",
    "ax.plot([0,1], [0,1], 'k--', alpha=0.3)\n",
    "ax.set_xlabel('FPR'); ax.set_ylabel('TPR')\n",
    "ax.set_title('ROC — PMI Scores')\n",
    "ax.legend(fontsize=8)\n",
    "\n",
    "# Panel 3: AUC comparison bar chart\n",
    "ax = axes[0, 2]\n",
    "names_short = ['Raw AL', 'PMI AL', 'Raw QL', 'PMI QL', 'Raw QL-s', 'PMI QL-s']\n",
    "aucs = [auc_results[m] for m in scoring_methods.keys()]\n",
    "colors = ['steelblue', 'steelblue', 'coral', 'coral', 'orange', 'orange']\n",
    "bars = ax.bar(range(len(names_short)), aucs, color=colors, alpha=0.8)\n",
    "ax.set_xticks(range(len(names_short)))\n",
    "ax.set_xticklabels(names_short, rotation=30, ha='right', fontsize=8)\n",
    "ax.set_ylabel('AUC')\n",
    "ax.set_title('AUC Comparison')\n",
    "ax.axhline(y=REF['raw_bare_auc'], color='gray', linestyle='--', alpha=0.5, label=f\"Exp22 bare={REF['raw_bare_auc']}\")\n",
    "ax.axhline(y=REF['pmi_bare_auc'], color='gray', linestyle=':', alpha=0.5, label=f\"Exp22 PMI={REF['pmi_bare_auc']}\")\n",
    "ax.legend(fontsize=7)\n",
    "for bar, val in zip(bars, aucs):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.003,\n",
    "            f'{val:.3f}', ha='center', va='bottom', fontsize=8)\n",
    "ax.set_ylim(0.5, 1.0)\n",
    "\n",
    "# --- Row 2: Score distributions and MRR ---\n",
    "# Panel 4: NLL distributions (relevant vs irrelevant)\n",
    "ax = axes[1, 0]\n",
    "for name, color, offset in [('nll_al', 'blue', -0.2), ('nll_ql', 'red', 0), ('nll_ql_search', 'orange', 0.2)]:\n",
    "    arr = {'nll_al': nll_al, 'nll_ql': nll_ql, 'nll_ql_search': nll_ql_search}[name]\n",
    "    label = name.replace('nll_', '')\n",
    "    rel_m = np.mean(arr[is_relevant == 1])\n",
    "    irr_m = np.mean(arr[is_relevant == 0])\n",
    "    ax.bar([0 + offset], [rel_m], width=0.18, color=color, alpha=0.7, label=f\"{label} (rel)\")\n",
    "    ax.bar([1 + offset], [irr_m], width=0.18, color=color, alpha=0.4)\n",
    "ax.set_xticks([0, 1])\n",
    "ax.set_xticklabels(['Relevant', 'Irrelevant'])\n",
    "ax.set_ylabel('Mean NLL')\n",
    "ax.set_title('Mean NLL by Relevance')\n",
    "ax.legend(fontsize=8)\n",
    "\n",
    "# Panel 5: MRR comparison\n",
    "ax = axes[1, 1]\n",
    "mrr_names = list(mrr_results.keys())\n",
    "mrr_vals = [mrr_results[n] for n in mrr_names]\n",
    "colors_mrr = ['steelblue', 'steelblue', 'coral', 'coral', 'orange', 'orange']\n",
    "bars = ax.bar(range(len(mrr_names)), mrr_vals, color=colors_mrr, alpha=0.8)\n",
    "ax.set_xticks(range(len(mrr_names)))\n",
    "ax.set_xticklabels(mrr_names, rotation=30, ha='right', fontsize=8)\n",
    "ax.set_ylabel('MRR@10')\n",
    "ax.set_title('MRR@10 Comparison')\n",
    "ax.axhline(y=REF['raw_bare_mrr'], color='gray', linestyle='--', alpha=0.5)\n",
    "for bar, val in zip(bars, mrr_vals):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.005,\n",
    "            f'{val:.3f}', ha='center', va='bottom', fontsize=8)\n",
    "ax.set_ylim(0.5, 1.0)\n",
    "\n",
    "# Panel 6: AL vs QL scatter (per-passage)\n",
    "ax = axes[1, 2]\n",
    "sample_idx = np.random.choice(len(nll_al), size=min(2000, len(nll_al)), replace=False)\n",
    "ax.scatter(nll_al[sample_idx][is_relevant[sample_idx] == 0],\n",
    "           nll_ql[sample_idx][is_relevant[sample_idx] == 0],\n",
    "           c='gray', alpha=0.2, s=10, label='Irrelevant')\n",
    "ax.scatter(nll_al[sample_idx][is_relevant[sample_idx] == 1],\n",
    "           nll_ql[sample_idx][is_relevant[sample_idx] == 1],\n",
    "           c='red', alpha=0.6, s=20, label='Relevant')\n",
    "r_corr = np.corrcoef(nll_al, nll_ql)[0, 1]\n",
    "ax.set_xlabel('NLL (answer-likelihood)')\n",
    "ax.set_ylabel('NLL (query-likelihood)')\n",
    "ax.set_title(f'AL vs QL per passage (r={r_corr:.3f})')\n",
    "ax.legend(fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'ranking_plots.png', dpi=150, bbox_inches='tight')\n",
    "print(f\"Saved to {RESULTS_DIR / 'ranking_plots.png'}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Save Results + Final Verdict\n",
    "\n",
    "# Save passage-level CSV\n",
    "csv_path = RESULTS_DIR / 'passage_scores.csv'\n",
    "with open(csv_path, 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['query_idx', 'passage_idx', 'is_relevant', 'word_count', 'doc_len',\n",
    "                      'nll_al', 'nll_ql', 'nll_ql_search',\n",
    "                      'bl_al', 'bl_ql', 'bl_ql_search',\n",
    "                      'pmi_al', 'pmi_ql', 'pmi_ql_search'])\n",
    "    for r in all_results:\n",
    "        for p in r['passage_data']:\n",
    "            writer.writerow([\n",
    "                r['query_idx'], p['passage_idx'], int(p['is_relevant']),\n",
    "                p['word_count'], p['doc_len'],\n",
    "                f\"{p['nll_al']:.6f}\", f\"{p['nll_ql']:.6f}\", f\"{p['nll_ql_search']:.6f}\",\n",
    "                f\"{p['bl_al']:.6f}\", f\"{p['bl_ql']:.6f}\", f\"{p['bl_ql_search']:.6f}\",\n",
    "                f\"{p['nll_al'] - p['bl_al']:.6f}\",\n",
    "                f\"{p['nll_ql'] - p['bl_ql']:.6f}\",\n",
    "                f\"{p['nll_ql_search'] - p['bl_ql_search']:.6f}\",\n",
    "            ])\n",
    "print(f\"CSV saved: {csv_path} ({n_total} rows)\")\n",
    "\n",
    "# Save full results JSON\n",
    "results_json = {\n",
    "    'experiment': 'Exp 31: Query-Likelihood Ranking with PMI',\n",
    "    'model': 'google/gemma-3-4b-it',\n",
    "    'n_queries': len(all_results),\n",
    "    'n_passages': n_total,\n",
    "    'n_relevant': int(n_rel),\n",
    "    'seed': SEED,\n",
    "    'auc_results': auc_results,\n",
    "    'mrr_results': mrr_results,\n",
    "    'diff_results': {k: {kk: float(vv) for kk, vv in v.items()} for k, v in diff_results.items()},\n",
    "    'reference_exp22': REF,\n",
    "    'all_results': all_results,\n",
    "}\n",
    "json_path = RESULTS_DIR / 'results.json'\n",
    "with open(json_path, 'w') as f:\n",
    "    json.dump(results_json, f, indent=2)\n",
    "print(f\"Results saved: {json_path} ({os.path.getsize(json_path)/1024:.1f} KB)\")\n",
    "\n",
    "# === FINAL VERDICT ===\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"FINAL VERDICT — Exp 31: Query-Likelihood Ranking\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "best_ql_auc = max(auc_results['Raw QL (query-likelihood)'],\n",
    "                   auc_results['PMI QL'],\n",
    "                   auc_results['Raw QL-search'],\n",
    "                   auc_results['PMI QL-search'])\n",
    "best_al_auc = max(auc_results['Raw AL (answer-likelihood)'],\n",
    "                   auc_results['PMI AL'])\n",
    "\n",
    "print(f\"\\nModel: Gemma 3 4B | N={len(all_results)} queries, {n_total} passages\")\n",
    "print(f\"\\nAnswer-Likelihood (reference):\")\n",
    "print(f\"  Raw AL AUC:  {auc_results['Raw AL (answer-likelihood)']:.3f}  (Exp22 ref: {REF['raw_bare_auc']:.3f})\")\n",
    "print(f\"  PMI AL AUC:  {auc_results['PMI AL']:.3f}  (Exp22 ref: {REF['pmi_bare_auc']:.3f})\")\n",
    "print(f\"\\nQuery-Likelihood (NEW):\")\n",
    "print(f\"  Raw QL AUC:       {auc_results['Raw QL (query-likelihood)']:.3f}\")\n",
    "print(f\"  PMI QL AUC:       {auc_results['PMI QL']:.3f}\")\n",
    "print(f\"  Raw QL-search AUC: {auc_results['Raw QL-search']:.3f}\")\n",
    "print(f\"  PMI QL-search AUC: {auc_results['PMI QL-search']:.3f}\")\n",
    "\n",
    "print(f\"\\nMRR@10:\")\n",
    "for name, val in mrr_results.items():\n",
    "    print(f\"  {name:<20} {val:.3f}\")\n",
    "\n",
    "ql_passes_primary = best_ql_auc > 0.80\n",
    "ql_beats_al = best_ql_auc > best_al_auc\n",
    "pmi_helps_ql = (auc_results['PMI QL'] > auc_results['Raw QL (query-likelihood)'] or\n",
    "                auc_results['PMI QL-search'] > auc_results['Raw QL-search'])\n",
    "\n",
    "print(f\"\\nPrimary:   QL AUC > 0.80?  {'YES' if ql_passes_primary else 'NO'} (best={best_ql_auc:.3f})\")\n",
    "print(f\"Secondary: PMI helps QL?    {'YES' if pmi_helps_ql else 'NO'}\")\n",
    "print(f\"Compare:   QL beats AL?     {'YES' if ql_beats_al else 'NO'} (best QL={best_ql_auc:.3f}, best AL={best_al_auc:.3f})\")\n",
    "\n",
    "r_al_ql = np.corrcoef(nll_al, nll_ql)[0, 1]\n",
    "print(f\"\\nAL-QL correlation: r={r_al_ql:.3f}\")\n",
    "\n",
    "if ql_passes_primary:\n",
    "    print(\"\\nVERDICT: Query-likelihood IS a viable ranking signal.\")\n",
    "    if ql_beats_al:\n",
    "        print(\"  Query-likelihood BEATS answer-likelihood — pursue for ad ranking.\")\n",
    "    else:\n",
    "        print(\"  Query-likelihood works but doesn't beat answer-likelihood.\")\n",
    "        print(\"  May still be valuable for ad serving where answers are unavailable.\")\n",
    "else:\n",
    "    print(\"\\nVERDICT: Query-likelihood FAILS as a ranking signal (AUC < 0.80).\")\n",
    "    print(\"  The model cannot reliably predict queries from passages.\")\n",
    "\n",
    "print(\"\\nDone!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: GPU Cleanup\n",
    "print(\"Cleaning up GPU memory...\")\n",
    "mem_before = torch.cuda.memory_allocated() / 1e9\n",
    "\n",
    "del model\n",
    "del tokenizer\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "mem_after = torch.cuda.memory_allocated() / 1e9\n",
    "print(f\"GPU memory: {mem_before:.2f} GB -> {mem_after:.2f} GB\")\n",
    "print(\"Cleanup complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}