{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 20: Length-Controlled Padding\n",
    "\n",
    "**Question**: Is the priming failure on long NQ docs a **length effect** or a **dataset effect**?\n",
    "\n",
    "MS MARCO and NQ differ in many ways (passage style, question type, answer format). Exp 18 showed\n",
    "priming works on short MS MARCO (d=+0.30) and short NQ (d=+0.33) but fails on long NQ (medium/long/very_long all negative).\n",
    "Exp 18 also ruled out the distance hypothesis (periodic beacons didn't help).\n",
    "\n",
    "**Design**: Take MS MARCO samples where priming works, artificially pad them with unrelated MS MARCO\n",
    "passages to reach target token lengths, and test whether the benefit disappears as length increases.\n",
    "This isolates the length variable while keeping domain/style constant.\n",
    "\n",
    "| Target Length | What it tests |\n",
    "|---|---|\n",
    "| original (~80-130 tok) | Baseline where priming works (d≈+0.30) |\n",
    "| 256 | Still short |\n",
    "| 512 | Medium (~NQ medium bin) |\n",
    "| 1024 | Long (~NQ long bin) |\n",
    "| 2048 | Very long (~NQ very_long bin) |\n",
    "\n",
    "2 conditions (bare, single_prefix) × 5 lengths = 10 scores per sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Setup & Imports\n",
    "import os\n",
    "os.umask(0o000)  # File permission safety (two-user environment)\n",
    "\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from scipy import stats\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from lib.config import ExperimentConfig\n",
    "from lib.kv_cache import (\n",
    "    extract_and_truncate_cache_with_bos,\n",
    "    correct_rope_positions_with_bos,\n",
    "    score_answer_with_cache,\n",
    "    deepcopy_cache,\n",
    ")\n",
    "from lib.analysis import cohens_d\n",
    "from lib.surrogate import STATIC_SURROGATE_QUERIES\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Paths\n",
    "RESULTS_DIR = Path('results/exp20')\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "FIGURES_DIR = RESULTS_DIR / 'figures'\n",
    "FIGURES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Model\n",
    "MODEL_NAME = 'mistralai/Mistral-7B-Instruct-v0.2'\n",
    "bnb_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.float16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME, quantization_config=bnb_config, device_map='auto'\n",
    ")\n",
    "config = ExperimentConfig(device='cuda')\n",
    "\n",
    "print(f'Model loaded: {MODEL_NAME}')\n",
    "print(f'Device: {config.device}')\n",
    "print(f'Results dir: {RESULTS_DIR}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Configuration\n",
    "\n",
    "N_SAMPLES = 300\n",
    "TARGET_LENGTHS = [None, 256, 512, 1024, 2048]  # None = original length\n",
    "CHECKPOINT_EVERY = 25\n",
    "\n",
    "# Prefix text (static_factual — best performer, same as Exp 07/17/18)\n",
    "BEACON_TEXT = STATIC_SURROGATE_QUERIES['static_factual']['query']\n",
    "\n",
    "# Templates (matched to Exp 07/17/18)\n",
    "QUERY_TEMPLATE = '\\nQuery: {query}\\nAnswer:'\n",
    "ANSWER_TEMPLATE = ' {answer}'\n",
    "\n",
    "# Paths\n",
    "CHECKPOINT_PATH = RESULTS_DIR / 'checkpoint.json'\n",
    "RESULTS_PATH = RESULTS_DIR / 'results.json'\n",
    "\n",
    "print(f'N_SAMPLES: {N_SAMPLES}')\n",
    "print(f'TARGET_LENGTHS: {TARGET_LENGTHS}')\n",
    "print(f'Prefix text: \"{BEACON_TEXT}\"')\n",
    "print(f'Checkpoint every: {CHECKPOINT_EVERY} samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 3: Build Padding Pool\n\nfrom lib.data import load_ms_marco, load_evaluation_samples\n\n# Load MS MARCO\nmsmarco_dataset = load_ms_marco(config)\nall_samples = load_evaluation_samples(msmarco_dataset, config, require_answer=True)\n\n# First 300 = evaluation set (same as Exp 17/18)\neval_samples = all_samples[:N_SAMPLES]\nprint(f'Evaluation samples: {len(eval_samples)}')\n\n# Build padding pool from ALL passages in the raw dataset (not just selected ones).\n# We need ~614K tokens but load_evaluation_samples only yields ~2500 samples (~275K tokens).\n# The raw dataset has ~10K rows × ~10 passages each = plenty of text.\neval_passages = set(s['passage'] for s in eval_samples)\npadding_passages = []\n\nfor item in msmarco_dataset:\n    passages = item.get('passages', {})\n    passage_texts = passages.get('passage_text', [])\n    for p in passage_texts:\n        if p not in eval_passages:\n            padding_passages.append(p)\n\nprint(f'Padding pool passages: {len(padding_passages):,}')\n\n# Concatenate and pre-tokenize\npadding_text = ' '.join(padding_passages)\nprint(f'Padding text length: {len(padding_text):,} characters')\n\npadding_ids = tokenizer.encode(padding_text, add_special_tokens=False)\nprint(f'Padding pool tokens: {len(padding_ids):,}')\n\n# Verify pool is large enough (need at most 2048 * 300 = 614,400)\nmax_needed = 2048 * N_SAMPLES\nprint(f'Max tokens needed: {max_needed:,}')\nassert len(padding_ids) > max_needed, (\n    f'Padding pool too small: {len(padding_ids):,} < {max_needed:,}. '\n    f'Need more padding samples.'\n)\nprint(f'Pool is {len(padding_ids) / max_needed:.1f}x the max needed. OK.')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 4: Explain Experimental Conditions\nprint('=' * 70)\nprint('EXPERIMENTAL CONDITIONS EXPLAINED')\nprint('=' * 70)\n\n# Concrete example with a real eval sample\nex = eval_samples[0]\nex_ids = tokenizer.encode(ex['passage'], add_special_tokens=False)\nprefix_ids = tokenizer.encode(BEACON_TEXT + '\\n', add_special_tokens=False)\n\nprint(f'\\nExample passage: {len(ex_ids)} tokens')\nprint(f'  \"{ex[\"passage\"][:80]}...\"')\nprint(f'Prefix: {len(prefix_ids)} tokens (\"{BEACON_TEXT}\")')\nprint()\n\nprint('### At each target length, we score two conditions: ###')\nprint()\nprint('  bare:          [BOS] + [doc_ids] -> forward -> score')\nprint('  single_prefix: [BOS] + [prefix_ids] + [doc_ids] -> forward -> truncate -> RoPE correct -> score')\nprint()\nprint('Both use IDENTICAL doc_ids at each length. Only the prefix differs.')\nprint()\n\nn_ex = len(ex_ids)\nn_pfx = len(prefix_ids)\nprint(f'--- Target: original ({n_ex} tokens) ---')\nprint(f'  bare:          [BOS][doc({n_ex})] = {1 + n_ex} tokens total')\nprint(f'  single_prefix: [BOS][prefix({n_pfx})][doc({n_ex})] -> truncate -> [BOS][doc_primed({n_ex})]')\nprint(f'  This is the baseline where priming works (d~+0.30).')\nprint()\n\nfor tl in [256, 512, 2048]:\n    pad_needed = max(0, tl - n_ex)\n    actual_len = max(tl, n_ex)\n    print(f'--- Target: {tl} tokens ---')\n    if pad_needed > 0:\n        print(f'  doc_ids = original({n_ex}) + padding({pad_needed}) = {actual_len} tokens')\n        print(f'  Padding: unrelated MS MARCO passages appended at token level')\n    else:\n        print(f'  doc_ids = original({n_ex}) (already >= {tl}, no padding needed)')\n    print(f'  bare:          [BOS][doc({actual_len})] = {1 + actual_len} tokens total')\n    print(f'  single_prefix: [BOS][prefix({n_pfx})][doc({actual_len})] -> truncate -> [BOS][doc_primed({actual_len})]')\n    print(f'  Answer span stays at the BEGINNING -- only total cache size changes.')\n    print()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Helper Function\n",
    "\n",
    "def score_at_length(passage_ids, padding_ids_pool, target_len,\n",
    "                    beacon_text, model, tokenizer, config,\n",
    "                    query_prompt, answer_text):\n",
    "    \"\"\"Score bare and single_prefix for a passage padded to target_len tokens.\n",
    "\n",
    "    If target_len is None, use original passage length.\n",
    "    If passage is already >= target_len, use original (no padding needed).\n",
    "\n",
    "    Returns: {'bare': nll, 'single_prefix': nll, 'actual_len': int}\n",
    "    \"\"\"\n",
    "    doc_ids = list(passage_ids)  # copy\n",
    "\n",
    "    # Pad if needed\n",
    "    if target_len is not None and len(doc_ids) < target_len:\n",
    "        pad_needed = target_len - len(doc_ids)\n",
    "        # Random start offset into padding pool for diversity\n",
    "        max_start = len(padding_ids_pool) - pad_needed\n",
    "        start = np.random.randint(0, max_start)\n",
    "        doc_ids.extend(padding_ids_pool[start:start + pad_needed])\n",
    "\n",
    "    actual_len = len(doc_ids)\n",
    "    bos_id = tokenizer.bos_token_id if tokenizer.bos_token_id is not None else 1\n",
    "    bos_tensor = torch.tensor([[bos_id]], device=config.device)\n",
    "    doc_tensor = torch.tensor([doc_ids], device=config.device)\n",
    "\n",
    "    results = {'actual_len': actual_len}\n",
    "\n",
    "    # --- bare ---\n",
    "    bare_input = torch.cat([bos_tensor, doc_tensor], dim=1)\n",
    "    with torch.no_grad():\n",
    "        out = model(\n",
    "            input_ids=bare_input,\n",
    "            attention_mask=torch.ones_like(bare_input),\n",
    "            use_cache=True, return_dict=True,\n",
    "        )\n",
    "    results['bare'] = score_answer_with_cache(\n",
    "        deepcopy_cache(out.past_key_values), bare_input.shape[1],\n",
    "        query_prompt, answer_text, model, tokenizer, config,\n",
    "    )\n",
    "    del out\n",
    "\n",
    "    # --- single_prefix ---\n",
    "    prefix_text = beacon_text + '\\n'\n",
    "    prefix_ids = tokenizer.encode(prefix_text, add_special_tokens=False)\n",
    "    prefix_tensor = torch.tensor([prefix_ids], device=config.device)\n",
    "    full_input = torch.cat([bos_tensor, prefix_tensor, doc_tensor], dim=1)\n",
    "    prefix_token_len = 1 + len(prefix_ids)  # BOS + prefix\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = model(\n",
    "            input_ids=full_input,\n",
    "            attention_mask=torch.ones_like(full_input),\n",
    "            use_cache=True, return_dict=True,\n",
    "        )\n",
    "    truncated = extract_and_truncate_cache_with_bos(out.past_key_values, actual_len)\n",
    "    surrogate_offset = prefix_token_len - 1\n",
    "    correct_rope_positions_with_bos(truncated, surrogate_offset, model)\n",
    "    keep_len = 1 + actual_len\n",
    "    results['single_prefix'] = score_answer_with_cache(\n",
    "        deepcopy_cache(truncated), keep_len,\n",
    "        query_prompt, answer_text, model, tokenizer, config,\n",
    "    )\n",
    "    del out, truncated\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    return results\n",
    "\n",
    "\n",
    "print('Helper function defined.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Evaluation Loop\n",
    "\n",
    "all_results = []\n",
    "start_idx = 0\n",
    "\n",
    "# Resume from checkpoint if available\n",
    "if CHECKPOINT_PATH.exists():\n",
    "    with open(CHECKPOINT_PATH, 'r') as f:\n",
    "        ckpt = json.load(f)\n",
    "    ckpt_queries = ckpt.get('sample_queries', [])\n",
    "    current_queries = [s['query'] for s in eval_samples]\n",
    "    if ckpt_queries == current_queries:\n",
    "        all_results = ckpt['results']\n",
    "        start_idx = len(all_results)\n",
    "        print(f'Resuming from checkpoint: {start_idx}/{N_SAMPLES}')\n",
    "    else:\n",
    "        print('Checkpoint sample mismatch. Starting fresh.')\n",
    "\n",
    "for idx in tqdm(range(start_idx, N_SAMPLES), initial=start_idx, total=N_SAMPLES,\n",
    "                desc='Eval'):\n",
    "    sample = eval_samples[idx]\n",
    "    query_prompt = QUERY_TEMPLATE.format(query=sample['query'])\n",
    "    answer_text = ANSWER_TEMPLATE.format(answer=sample['answer'])\n",
    "\n",
    "    # Tokenize passage once\n",
    "    passage_ids = tokenizer.encode(sample['passage'], add_special_tokens=False)\n",
    "\n",
    "    sample_result = {\n",
    "        'idx': idx,\n",
    "        'word_count': len(sample['passage'].split()),\n",
    "        'original_token_len': len(passage_ids),\n",
    "    }\n",
    "\n",
    "    for tl in TARGET_LENGTHS:\n",
    "        length_key = 'original' if tl is None else str(tl)\n",
    "        scores = score_at_length(\n",
    "            passage_ids, padding_ids, tl,\n",
    "            BEACON_TEXT, model, tokenizer, config,\n",
    "            query_prompt, answer_text,\n",
    "        )\n",
    "        sample_result[f'bare_{length_key}'] = scores['bare']\n",
    "        sample_result[f'prefix_{length_key}'] = scores['single_prefix']\n",
    "        sample_result[f'actual_len_{length_key}'] = scores['actual_len']\n",
    "\n",
    "    all_results.append(sample_result)\n",
    "\n",
    "    # Checkpoint\n",
    "    if (idx + 1) % CHECKPOINT_EVERY == 0 or idx == N_SAMPLES - 1:\n",
    "        ckpt_data = {\n",
    "            'results': all_results,\n",
    "            'sample_queries': [s['query'] for s in eval_samples],\n",
    "            'completed': len(all_results),\n",
    "            'total': N_SAMPLES,\n",
    "            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        }\n",
    "        with open(CHECKPOINT_PATH, 'w') as f:\n",
    "            json.dump(ckpt_data, f)\n",
    "\n",
    "print(f'Evaluation complete: {len(all_results)} samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Analysis\n",
    "\n",
    "length_keys = ['original', '256', '512', '1024', '2048']\n",
    "\n",
    "print('=' * 70)\n",
    "print('LENGTH-CONTROLLED PADDING ANALYSIS')\n",
    "print('=' * 70)\n",
    "print()\n",
    "print(f'{\"Length\":<12} {\"Mean Bare\":>10} {\"Mean Prefix\":>12} {\"Mean Δ\":>10} {\"d\":>8} {\"Win%\":>7} {\"p\":>12} {\"sig\":>5}')\n",
    "print('-' * 80)\n",
    "\n",
    "analysis = {}\n",
    "\n",
    "for lk in length_keys:\n",
    "    bare_arr = np.array([r[f'bare_{lk}'] for r in all_results])\n",
    "    prefix_arr = np.array([r[f'prefix_{lk}'] for r in all_results])\n",
    "    actual_lens = np.array([r[f'actual_len_{lk}'] for r in all_results])\n",
    "\n",
    "    # Filter invalid\n",
    "    valid = np.isfinite(bare_arr) & np.isfinite(prefix_arr) & (bare_arr != 0) & (prefix_arr != 0)\n",
    "    bare = bare_arr[valid]\n",
    "    prefix = prefix_arr[valid]\n",
    "    lens = actual_lens[valid]\n",
    "    n_valid = int(np.sum(valid))\n",
    "\n",
    "    delta = bare - prefix  # positive = prefix helps\n",
    "    d = cohens_d(delta)\n",
    "    win = np.mean(delta > 0) * 100\n",
    "    t_stat, p_val = stats.ttest_1samp(delta, 0)\n",
    "    sig = '***' if p_val < 0.001 else '**' if p_val < 0.01 else '*' if p_val < 0.05 else 'ns'\n",
    "\n",
    "    label = lk if lk == 'original' else f'{lk} tok'\n",
    "    print(f'{label:<12} {np.mean(bare):>10.4f} {np.mean(prefix):>12.4f} {np.mean(delta):>+10.4f} {d:>+8.3f} {win:>6.1f}% {p_val:>12.2e} {sig:>5}')\n",
    "\n",
    "    analysis[lk] = {\n",
    "        'n_valid': n_valid,\n",
    "        'mean_bare': float(np.mean(bare)),\n",
    "        'mean_prefix': float(np.mean(prefix)),\n",
    "        'mean_delta': float(np.mean(delta)),\n",
    "        'cohens_d': float(d),\n",
    "        'win_pct': float(win),\n",
    "        't_stat': float(t_stat),\n",
    "        'p_value': float(p_val),\n",
    "        'mean_actual_len': float(np.mean(lens)),\n",
    "    }\n",
    "\n",
    "print()\n",
    "print('Prediction: d should decay from ~+0.30 at original to ~0 or negative at 2048.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Figures & Summary\n",
    "\n",
    "length_keys = ['original', '256', '512', '1024', '2048']\n",
    "\n",
    "# Compute arrays for plotting\n",
    "ds_plot = []\n",
    "wins_plot = []\n",
    "x_lengths = []\n",
    "ci_lo = []\n",
    "ci_hi = []\n",
    "\n",
    "# Also collect per-sample deltas for scatter\n",
    "deltas_by_length = {}\n",
    "\n",
    "for lk in length_keys:\n",
    "    bare_arr = np.array([r[f'bare_{lk}'] for r in all_results])\n",
    "    prefix_arr = np.array([r[f'prefix_{lk}'] for r in all_results])\n",
    "    valid = np.isfinite(bare_arr) & np.isfinite(prefix_arr) & (bare_arr != 0) & (prefix_arr != 0)\n",
    "    bare = bare_arr[valid]\n",
    "    prefix = prefix_arr[valid]\n",
    "    delta = bare - prefix\n",
    "\n",
    "    d = cohens_d(delta)\n",
    "    win = np.mean(delta > 0) * 100\n",
    "\n",
    "    # Bootstrap 95% CI for Cohen's d\n",
    "    np.random.seed(SEED)\n",
    "    boot_ds = []\n",
    "    for _ in range(2000):\n",
    "        idx_boot = np.random.randint(0, len(delta), size=len(delta))\n",
    "        boot_ds.append(cohens_d(delta[idx_boot]))\n",
    "    boot_ds = np.array(boot_ds)\n",
    "    ci_lo.append(np.percentile(boot_ds, 2.5))\n",
    "    ci_hi.append(np.percentile(boot_ds, 97.5))\n",
    "\n",
    "    ds_plot.append(d)\n",
    "    wins_plot.append(win)\n",
    "    actual_lens = np.array([r[f'actual_len_{lk}'] for r in all_results])[valid]\n",
    "    x_lengths.append(np.mean(actual_lens))\n",
    "    deltas_by_length[lk] = delta\n",
    "\n",
    "ds_plot = np.array(ds_plot)\n",
    "wins_plot = np.array(wins_plot)\n",
    "x_lengths = np.array(x_lengths)\n",
    "ci_lo = np.array(ci_lo)\n",
    "ci_hi = np.array(ci_hi)\n",
    "\n",
    "# --- Figure 1: Cohen's d vs Target Length ---\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "ax.errorbar(x_lengths, ds_plot,\n",
    "            yerr=[ds_plot - ci_lo, ci_hi - ds_plot],\n",
    "            marker='o', markersize=8, linewidth=2, capsize=5,\n",
    "            color='#2196F3', ecolor='#90CAF9')\n",
    "\n",
    "# Annotate significance\n",
    "for i, lk in enumerate(length_keys):\n",
    "    p = analysis[lk]['p_value']\n",
    "    sig = '***' if p < 0.001 else '**' if p < 0.01 else '*' if p < 0.05 else 'ns'\n",
    "    label = lk if lk == 'original' else f'{lk}'\n",
    "    ax.annotate(f'{label}\\nd={ds_plot[i]:+.3f} {sig}',\n",
    "                (x_lengths[i], ds_plot[i]),\n",
    "                textcoords='offset points', xytext=(0, 18),\n",
    "                ha='center', fontsize=8)\n",
    "\n",
    "ax.axhline(y=0, color='black', linestyle='-', linewidth=0.8)\n",
    "ax.set_xscale('log')\n",
    "ax.set_xlabel('Mean Document Token Length', fontsize=11)\n",
    "ax.set_ylabel(\"Cohen's d (positive = prefix helps)\", fontsize=11)\n",
    "ax.set_title('Exp 20: Priming Effect vs Document Length\\n(MS MARCO passages with padding)', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'd_vs_length.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f'Saved: {FIGURES_DIR / \"d_vs_length.png\"}')\n",
    "\n",
    "# --- Figure 2: Win Rate vs Length ---\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "ax.plot(x_lengths, wins_plot, marker='s', markersize=8, linewidth=2, color='#4CAF50')\n",
    "ax.axhline(y=50, color='black', linestyle='--', linewidth=0.8, label='Chance (50%)')\n",
    "for i, lk in enumerate(length_keys):\n",
    "    label = lk if lk == 'original' else f'{lk}'\n",
    "    ax.annotate(f'{label}\\n{wins_plot[i]:.1f}%',\n",
    "                (x_lengths[i], wins_plot[i]),\n",
    "                textcoords='offset points', xytext=(0, 12),\n",
    "                ha='center', fontsize=8)\n",
    "ax.set_xscale('log')\n",
    "ax.set_xlabel('Mean Document Token Length', fontsize=11)\n",
    "ax.set_ylabel('Win Rate (% samples where prefix helps)', fontsize=11)\n",
    "ax.set_title('Exp 20: Prefix Win Rate vs Document Length', fontsize=12)\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'winrate_vs_length.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f'Saved: {FIGURES_DIR / \"winrate_vs_length.png\"}')\n",
    "\n",
    "# --- Figure 3: Per-sample scatter (delta at 2048 vs delta at original) ---\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "d_orig = deltas_by_length['original']\n",
    "d_2048 = deltas_by_length['2048']\n",
    "# Align lengths (both should be same if all valid, but intersect to be safe)\n",
    "n_scatter = min(len(d_orig), len(d_2048))\n",
    "ax.scatter(d_orig[:n_scatter], d_2048[:n_scatter], alpha=0.3, s=15, color='#FF9800')\n",
    "ax.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "ax.axvline(x=0, color='black', linestyle='-', linewidth=0.5)\n",
    "# Diagonal\n",
    "lims = [min(ax.get_xlim()[0], ax.get_ylim()[0]), max(ax.get_xlim()[1], ax.get_ylim()[1])]\n",
    "ax.plot(lims, lims, 'k--', alpha=0.3, linewidth=0.8)\n",
    "ax.set_xlabel('Delta at original length (bare - prefix)', fontsize=10)\n",
    "ax.set_ylabel('Delta at 2048 tokens (bare - prefix)', fontsize=10)\n",
    "ax.set_title('Per-Sample: Does short-doc benefit predict long-doc benefit?', fontsize=11)\n",
    "from scipy.stats import pearsonr\n",
    "r, p = pearsonr(d_orig[:n_scatter], d_2048[:n_scatter])\n",
    "ax.annotate(f'r={r:.3f}, p={p:.2e}', xy=(0.05, 0.95), xycoords='axes fraction',\n",
    "            fontsize=10, va='top')\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'delta_scatter.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f'Saved: {FIGURES_DIR / \"delta_scatter.png\"}')\n",
    "\n",
    "# --- Summary ---\n",
    "print()\n",
    "print('=' * 70)\n",
    "print('EXPERIMENT 20 SUMMARY')\n",
    "print('=' * 70)\n",
    "print()\n",
    "print('Question: Is priming failure on long docs a LENGTH effect or DATASET effect?')\n",
    "print()\n",
    "for lk in length_keys:\n",
    "    a = analysis[lk]\n",
    "    label = lk if lk == 'original' else f'{lk} tok'\n",
    "    sig = '***' if a['p_value'] < 0.001 else '**' if a['p_value'] < 0.01 else '*' if a['p_value'] < 0.05 else 'ns'\n",
    "    verdict = 'HELPS' if a['cohens_d'] > 0 and a['p_value'] < 0.05 else 'HURTS' if a['cohens_d'] < 0 and a['p_value'] < 0.05 else 'NEUTRAL'\n",
    "    print(f'  {label:<12} d={a[\"cohens_d\"]:+.3f}  win={a[\"win_pct\"]:.0f}%  p={a[\"p_value\"]:.2e}  {sig}  → {verdict}')\n",
    "\n",
    "print()\n",
    "if analysis['original']['cohens_d'] > 0.1 and analysis['2048']['cohens_d'] < 0.05:\n",
    "    print('CONCLUSION: LENGTH is likely the primary factor. Priming benefit decays')\n",
    "    print('with document length even within MS MARCO, not just across datasets.')\n",
    "elif analysis['2048']['cohens_d'] > 0.1:\n",
    "    print('CONCLUSION: DATASET is likely the primary factor. Priming benefit persists')\n",
    "    print('even at NQ-like lengths when the content is MS MARCO-style.')\n",
    "else:\n",
    "    print('CONCLUSION: Mixed signal. See detailed numbers above.')\n",
    "\n",
    "# Save analysis\n",
    "final = {\n",
    "    'experiment': 'exp20_length_controlled_padding',\n",
    "    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'config': {\n",
    "        'model_name': MODEL_NAME, 'seed': SEED, 'n_eval': N_SAMPLES,\n",
    "        'dataset': 'MS MARCO v1.1', 'prefix_text': BEACON_TEXT,\n",
    "        'target_lengths': [str(tl) if tl else 'original' for tl in TARGET_LENGTHS],\n",
    "    },\n",
    "    'analysis': analysis,\n",
    "    'per_sample_results': all_results,\n",
    "}\n",
    "with open(RESULTS_PATH, 'w') as f:\n",
    "    json.dump(final, f, indent=2)\n",
    "print(f'\\nAll results saved to {RESULTS_DIR}/')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}