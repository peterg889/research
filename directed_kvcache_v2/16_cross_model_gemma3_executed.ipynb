{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "175acf85",
   "metadata": {},
   "source": [
    "# Exp 16: Cross-Model Priming Replication (Gemma 3 4B)\n",
    "\n",
    "## Motivation\n",
    "\n",
    "All 15 prior experiments used Mistral-7B-Instruct-v0.2 exclusively. The critical open\n",
    "question: **is value contamination via priming a universal transformer mechanism, or\n",
    "Mistral-specific?**\n",
    "\n",
    "Gemma 3 4B is an ideal comparison model:\n",
    "- Different architecture: 34 layers (vs 32), head_dim=256 (vs 128), 4 KV heads (vs 8)\n",
    "- Per-layer RoPE: sliding_attention=10k theta, full_attention=1M theta (vs uniform 1M)\n",
    "- GQA with different grouping ratio\n",
    "- bfloat16 required (float16 produces garbage)\n",
    "- All lib functions already support it\n",
    "\n",
    "## Architecture Comparison\n",
    "\n",
    "| Property | Mistral-7B | Gemma 3 4B |\n",
    "|----------|-----------|------------|\n",
    "| Parameters | 7B | 4B |\n",
    "| Layers | 32 | 34 |\n",
    "| Hidden size | 4096 | 2560 |\n",
    "| Attention heads | 32 | 8 |\n",
    "| KV heads | 8 | 4 |\n",
    "| Head dim | 128 | 256 |\n",
    "| RoPE theta | 1M (uniform) | 10k/1M (per-layer) |\n",
    "| Dtype | float16 | bfloat16 |\n",
    "\n",
    "## Design\n",
    "\n",
    "**5 conditions on MS MARCO v1.1 (where priming works on Mistral), N=300 queries:**\n",
    "\n",
    "| # | Condition | Description | Tests |\n",
    "|---|-----------|-------------|-------|\n",
    "| 1 | bare | Baseline — no prefix | — |\n",
    "| 2 | static_fact_trunc | \"What are the key facts?\" prefix, truncated+RoPE | Best Mistral condition on Gemma? |\n",
    "| 3 | random_trunc | Random text prefix, truncated+RoPE | Does any prefix help? |\n",
    "| 4 | oracle_trunc | Actual query as prefix, truncated+RoPE | Is random > oracle on Gemma too? |\n",
    "| 5 | values_only | Bare keys + sf primed values (hybrid cache) | Is it value contamination on Gemma? |\n",
    "\n",
    "## Mistral Reference Values (from Exp 01 + 08)\n",
    "\n",
    "| Metric | Mistral Value | Source |\n",
    "|--------|--------------|--------|\n",
    "| static_fact_trunc d | +0.472 | Exp 07 |\n",
    "| random_trunc d | +0.091 | Exp 01 |\n",
    "| oracle_trunc d | +0.023 (ns) | Exp 01 |\n",
    "| values_only fraction | 108% | Exp 08 |\n",
    "| keys_only fraction | -4% | Exp 08 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69a8764d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T12:31:35.851089Z",
     "iopub.status.busy": "2026-02-15T12:31:35.850684Z",
     "iopub.status.idle": "2026-02-15T12:31:38.285074Z",
     "shell.execute_reply": "2026-02-15T12:31:38.283936Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEED: 42\n",
      "Results directory: results/exp16\n",
      "CUDA available: True\n",
      "GPU: NVIDIA L4\n",
      "GPU memory: 23.6 GB\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Setup\n",
    "import os\n",
    "os.umask(0o000)\n",
    "\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import gc\n",
    "from pathlib import Path\n",
    "\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "RESULTS_DIR = Path(\"results/exp16\")\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CHECKPOINT_PATH = RESULTS_DIR / \"checkpoint.json\"\n",
    "FINAL_RESULTS_PATH = RESULTS_DIR / \"results.json\"\n",
    "\n",
    "print(f\"SEED: {SEED}\")\n",
    "print(f\"Results directory: {RESULTS_DIR}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad84d777",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T12:31:38.289511Z",
     "iopub.status.busy": "2026-02-15T12:31:38.289066Z",
     "iopub.status.idle": "2026-02-15T12:31:56.841244Z",
     "shell.execute_reply": "2026-02-15T12:31:56.840251Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading google/gemma-3-4b-it (4-bit, bfloat16)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "318ec4591b784c248daf8d3b2ed9d6ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/883 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model loaded successfully.\n",
      "  Model class: Gemma3ForConditionalGeneration\n",
      "  Text config class: Gemma3TextConfig\n",
      "  Hidden size: 2560\n",
      "  Num layers: 34\n",
      "  Num attention heads: 8\n",
      "  Num KV heads: 4\n",
      "  Head dim: 256\n",
      "  BOS token ID: 2\n",
      "  EOS token ID: 1\n",
      "  Unique RoPE thetas: [10000.0, 1000000.0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Cache key dtype: torch.bfloat16\n",
      "  Cache key shape: torch.Size([1, 4, 2, 256])  (batch, kv_heads, seq, head_dim)\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Load Gemma 3 4B via load_model()\n",
    "sys.path.insert(0, \".\")\n",
    "\n",
    "from lib.config import ExperimentConfig\n",
    "from lib.model_utils import load_model\n",
    "\n",
    "MODEL_NAME = \"google/gemma-3-4b-it\"\n",
    "\n",
    "exp_config = ExperimentConfig(\n",
    "    model_name=MODEL_NAME,\n",
    "    model_type=\"gemma3\",\n",
    "    compute_dtype=\"auto\",  # resolves to bfloat16 for gemma3\n",
    "    use_4bit=True,\n",
    "    num_samples=2000,\n",
    "    seed=SEED,\n",
    ")\n",
    "\n",
    "print(f\"Loading {MODEL_NAME} (4-bit, bfloat16)...\")\n",
    "model, tokenizer = load_model(exp_config)\n",
    "\n",
    "# Architecture diagnostics\n",
    "from lib.kv_cache import _get_text_config, _get_head_dim, _get_rope_theta_for_layer, _get_cache_keys, _ensure_dynamic_cache\n",
    "\n",
    "text_config = _get_text_config(model.config)\n",
    "print(f\"\\nModel loaded successfully.\")\n",
    "print(f\"  Model class: {type(model).__name__}\")\n",
    "print(f\"  Text config class: {type(text_config).__name__}\")\n",
    "print(f\"  Hidden size: {text_config.hidden_size}\")\n",
    "print(f\"  Num layers: {text_config.num_hidden_layers}\")\n",
    "print(f\"  Num attention heads: {text_config.num_attention_heads}\")\n",
    "print(f\"  Num KV heads: {text_config.num_key_value_heads}\")\n",
    "print(f\"  Head dim: {_get_head_dim(model.config)}\")\n",
    "print(f\"  BOS token ID: {tokenizer.bos_token_id}\")\n",
    "print(f\"  EOS token ID: {tokenizer.eos_token_id}\")\n",
    "\n",
    "# Per-layer RoPE diagnostics\n",
    "thetas = set()\n",
    "for layer_idx in range(text_config.num_hidden_layers):\n",
    "    thetas.add(_get_rope_theta_for_layer(model.config, layer_idx))\n",
    "print(f\"  Unique RoPE thetas: {sorted(thetas)}\")\n",
    "\n",
    "# Verify dtype\n",
    "sample_ids = tokenizer(\"test\", return_tensors=\"pt\")['input_ids'].to(exp_config.device)\n",
    "with torch.no_grad():\n",
    "    out = model(sample_ids, use_cache=True)\n",
    "    cache_check = _ensure_dynamic_cache(out.past_key_values)\n",
    "    k0 = _get_cache_keys(cache_check, 0)\n",
    "    print(f\"  Cache key dtype: {k0.dtype}\")\n",
    "    print(f\"  Cache key shape: {k0.shape}  (batch, kv_heads, seq, head_dim)\")\n",
    "del out, sample_ids\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37ff07b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T12:31:56.845959Z",
     "iopub.status.busy": "2026-02-15T12:31:56.845374Z",
     "iopub.status.idle": "2026-02-15T12:31:56.855160Z",
     "shell.execute_reply": "2026-02-15T12:31:56.854029Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config ready\n",
      "  Model: google/gemma-3-4b-it\n",
      "  MAX_QUERIES: 300\n",
      "  Conditions: ['bare', 'static_fact_trunc', 'random_trunc', 'oracle_trunc', 'values_only']\n",
      "  Prefixes:\n",
      "    static_fact: 'What are the key facts I need to know?'\n",
      "    random:      'The purple elephant danced gracefully on the frozen lake during twilight'\n",
      "    oracle:      (actual query text per sample)\n",
      "  Mistral reference d values:\n",
      "    random_trunc_d: +0.091\n",
      "    oracle_trunc_d: +0.023\n",
      "    static_fact_trunc_d: +0.472\n",
      "    values_fraction: +1.083\n",
      "    keys_fraction: -0.036\n",
      "    d_full_trunc: +0.254\n",
      "    d_values_only: +0.275\n",
      "    d_keys_only: -0.009\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Config and library imports\n",
    "from lib.kv_cache import (\n",
    "    _get_cache_keys,\n",
    "    _get_cache_values,\n",
    "    _set_cache_keys,\n",
    "    _set_cache_values,\n",
    "    _ensure_dynamic_cache,\n",
    "    extract_and_truncate_cache_with_bos,\n",
    "    correct_rope_positions_with_bos,\n",
    "    score_answer_with_cache,\n",
    "    deepcopy_cache,\n",
    "    build_hybrid_cache,\n",
    ")\n",
    "from lib.analysis import cohens_d\n",
    "from lib.data import count_words\n",
    "from scipy import stats\n",
    "from scipy.stats import wilcoxon\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Templates — bare text, no \"Document:\\n\" framing\n",
    "SURROGATE_PREFIX_TEMPLATE = \"{surrogate}\\n\"\n",
    "DOCUMENT_TEMPLATE = \"{document}\"\n",
    "QUERY_TEMPLATE = \"\\nQuery: {query}\\nAnswer:\"\n",
    "ANSWER_TEMPLATE = \" {answer}\"\n",
    "\n",
    "# Prefix texts\n",
    "from lib.surrogate import STATIC_SURROGATE_QUERIES\n",
    "STATIC_FACT = STATIC_SURROGATE_QUERIES['static_factual']['query']\n",
    "RANDOM_PREFIX_TEXT = \"The purple elephant danced gracefully on the frozen lake during twilight\"\n",
    "\n",
    "# Experiment parameters\n",
    "MAX_QUERIES = 300\n",
    "MAX_PASSAGE_WORDS = 300\n",
    "MIN_PASSAGES_PER_QUERY = 2\n",
    "CHECKPOINT_EVERY = 25\n",
    "\n",
    "CONDITION_NAMES = ['bare', 'static_fact_trunc', 'random_trunc', 'oracle_trunc', 'values_only']\n",
    "\n",
    "# Mistral reference values (from Exp 01 + 08)\n",
    "MISTRAL_REF = {\n",
    "    'random_trunc_d': 0.091,\n",
    "    'oracle_trunc_d': 0.023,\n",
    "    'static_fact_trunc_d': 0.472,\n",
    "    'values_fraction': 1.083,\n",
    "    'keys_fraction': -0.036,\n",
    "    'd_full_trunc': 0.254,\n",
    "    'd_values_only': 0.275,\n",
    "    'd_keys_only': -0.009,\n",
    "}\n",
    "\n",
    "print(\"Config ready\")\n",
    "print(f\"  Model: {MODEL_NAME}\")\n",
    "print(f\"  MAX_QUERIES: {MAX_QUERIES}\")\n",
    "print(f\"  Conditions: {CONDITION_NAMES}\")\n",
    "print(f\"  Prefixes:\")\n",
    "print(f\"    static_fact: '{STATIC_FACT}'\")\n",
    "print(f\"    random:      '{RANDOM_PREFIX_TEXT}'\")\n",
    "print(f\"    oracle:      (actual query text per sample)\")\n",
    "print(f\"  Mistral reference d values:\")\n",
    "for k, v in MISTRAL_REF.items():\n",
    "    print(f\"    {k}: {v:+.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3583f6de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T12:31:56.858376Z",
     "iopub.status.busy": "2026-02-15T12:31:56.858106Z",
     "iopub.status.idle": "2026-02-15T12:31:57.934399Z",
     "shell.execute_reply": "2026-02-15T12:31:57.933345Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`trust_remote_code` is not supported anymore.\n",
      "Please check that the Hugging Face dataset 'microsoft/ms_marco' isn't based on a loading script and remove `trust_remote_code`.\n",
      "If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "LOADING MS MARCO v1.1 — ALL PASSAGES PER QUERY\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total items in validation: 10047\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b2df4a5ebe644b5b282a66b44c4d329",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filtering:   0%|          | 0/10047 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Selected 300 queries (2504 total passages)\n",
      "Passages per query: mean=8.3, min=3, max=10\n",
      "Word counts: mean=71\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "94"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 4: Load MS MARCO v1.1 (same filtering as previous ranking experiments)\n",
    "from datasets import load_dataset\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"LOADING MS MARCO v1.1 — ALL PASSAGES PER QUERY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "dataset = load_dataset(\"microsoft/ms_marco\", \"v1.1\", split=\"validation\",\n",
    "                        trust_remote_code=True)\n",
    "print(f\"Total items in validation: {len(dataset)}\")\n",
    "\n",
    "queries = []\n",
    "np.random.seed(SEED)\n",
    "\n",
    "for item in tqdm(dataset, desc=\"Filtering\"):\n",
    "    passages_info = item.get('passages', {})\n",
    "    passage_texts = passages_info.get('passage_text', [])\n",
    "    is_selected = passages_info.get('is_selected', [])\n",
    "    query = item.get('query', '')\n",
    "    answers = item.get('answers', [])\n",
    "    well_formed = item.get('wellFormedAnswers', [])\n",
    "\n",
    "    if not passage_texts or not query:\n",
    "        continue\n",
    "    if len(passage_texts) < MIN_PASSAGES_PER_QUERY:\n",
    "        continue\n",
    "    if not is_selected or sum(is_selected) == 0:\n",
    "        continue\n",
    "\n",
    "    word_counts = [count_words(p) for p in passage_texts]\n",
    "    if any(wc > MAX_PASSAGE_WORDS for wc in word_counts):\n",
    "        continue\n",
    "\n",
    "    answer = None\n",
    "    if well_formed and len(well_formed) > 0 and well_formed[0] != '[]':\n",
    "        answer = well_formed[0]\n",
    "    elif answers and len(answers) > 0 and answers[0] != 'No Answer Present.':\n",
    "        answer = answers[0]\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "    passage_list = []\n",
    "    for i, (ptext, sel) in enumerate(zip(passage_texts, is_selected)):\n",
    "        passage_list.append({\n",
    "            'passage': ptext,\n",
    "            'is_relevant': bool(sel == 1),\n",
    "            'word_count': word_counts[i],\n",
    "            'passage_idx': i,\n",
    "        })\n",
    "\n",
    "    queries.append({\n",
    "        'query': query,\n",
    "        'answer': answer,\n",
    "        'passages': passage_list,\n",
    "        'n_passages': len(passage_list),\n",
    "        'n_relevant': sum(1 for p in passage_list if p['is_relevant']),\n",
    "    })\n",
    "\n",
    "    if len(queries) >= MAX_QUERIES * 3:\n",
    "        break\n",
    "\n",
    "np.random.shuffle(queries)\n",
    "queries = queries[:MAX_QUERIES]\n",
    "N = len(queries)\n",
    "\n",
    "n_passages_list = [q['n_passages'] for q in queries]\n",
    "total_passages = sum(n_passages_list)\n",
    "\n",
    "print(f\"\\nSelected {N} queries ({total_passages} total passages)\")\n",
    "print(f\"Passages per query: mean={np.mean(n_passages_list):.1f}, \"\n",
    "      f\"min={min(n_passages_list)}, max={max(n_passages_list)}\")\n",
    "print(f\"Word counts: mean={np.mean([p['word_count'] for q in queries for p in q['passages']]):.0f}\")\n",
    "\n",
    "del dataset\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b006569",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T12:31:57.939457Z",
     "iopub.status.busy": "2026-02-15T12:31:57.938990Z",
     "iopub.status.idle": "2026-02-15T12:31:57.962774Z",
     "shell.execute_reply": "2026-02-15T12:31:57.961541Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PREFIX TOKENIZATION — GEMMA 3 4B\n",
      "======================================================================\n",
      "\n",
      "PREFIX TOKEN LENGTHS (Gemma tokenizer):\n",
      "  static_fact      11 tokens | 'What are the key facts I need to know?'\n",
      "  random           12 tokens | 'The purple elephant danced gracefully on the frozen lake during twilight'\n",
      "  oracle (ex)      11 tokens | 'how long to cook lamb chops in nuwave oven'\n",
      "\n",
      "BPE BOUNDARY CHECK (first passage):\n",
      "  static_fact    : 151/151 tokens match (100.0%)\n",
      "  random         : 151/151 tokens match (100.0%)\n",
      "  oracle (ex)    : 151/151 tokens match (100.0%)\n",
      "\n",
      "======================================================================\n",
      "CONDITION DETAILS\n",
      "======================================================================\n",
      "\n",
      "### bare ###\n",
      "  Standard bare cache: [BOS][doc]\n",
      "  Purpose: Baseline. All other conditions compared to this.\n",
      "\n",
      "### static_fact_trunc ###\n",
      "  Prefix: 'What are the key facts I need to know?' → truncate + RoPE correct\n",
      "  Purpose: Best single Mistral condition (d=+0.472). Does it replicate on Gemma?\n",
      "\n",
      "### random_trunc ###\n",
      "  Prefix: 'The purple elephant danced gracefully on the frozen lake during twilight' → truncate + RoPE correct\n",
      "  Purpose: Random prefix (d=+0.091 on Mistral). Does ANY prefix help on Gemma?\n",
      "\n",
      "### oracle_trunc ###\n",
      "  Prefix: actual query text → truncate + RoPE correct\n",
      "  Purpose: Oracle prefix (d=+0.023 ns on Mistral). Is random > oracle on Gemma too?\n",
      "\n",
      "### values_only ###\n",
      "  Bare keys + static_fact primed values (hybrid cache)\n",
      "  Purpose: Tests value contamination mechanism. On Mistral: values=108%, keys=-4%.\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Tokenize prefixes and verify BPE boundaries\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PREFIX TOKENIZATION — GEMMA 3 4B\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Tokenize each prefix\n",
    "sf_str = SURROGATE_PREFIX_TEMPLATE.format(surrogate=STATIC_FACT)\n",
    "rand_str = SURROGATE_PREFIX_TEMPLATE.format(surrogate=RANDOM_PREFIX_TEXT)\n",
    "\n",
    "sf_ids = tokenizer(sf_str, return_tensors=\"pt\",\n",
    "                    add_special_tokens=False)['input_ids'].to(exp_config.device)\n",
    "rand_ids = tokenizer(rand_str, return_tensors=\"pt\",\n",
    "                      add_special_tokens=False)['input_ids'].to(exp_config.device)\n",
    "\n",
    "# Example oracle prefix\n",
    "example_oracle_str = SURROGATE_PREFIX_TEMPLATE.format(surrogate=queries[0]['query'])\n",
    "oracle_example_ids = tokenizer(example_oracle_str, return_tensors=\"pt\",\n",
    "                                add_special_tokens=False)['input_ids'].to(exp_config.device)\n",
    "\n",
    "PREFIX_CONFIGS = [\n",
    "    ('static_fact', STATIC_FACT, sf_str, sf_ids),\n",
    "    ('random', RANDOM_PREFIX_TEXT, rand_str, rand_ids),\n",
    "    ('oracle (ex)', queries[0]['query'], example_oracle_str, oracle_example_ids),\n",
    "]\n",
    "\n",
    "print(\"\\nPREFIX TOKEN LENGTHS (Gemma tokenizer):\")\n",
    "for name, text, full_str, ids in PREFIX_CONFIGS:\n",
    "    print(f\"  {name:<15} {ids.shape[1]:>3} tokens | '{text}'\")\n",
    "\n",
    "# Verify BPE boundary consistency across prefixes\n",
    "print(\"\\nBPE BOUNDARY CHECK (first passage):\")\n",
    "example_doc = queries[0]['passages'][0]['passage']\n",
    "for name, text, full_str, ids in PREFIX_CONFIGS:\n",
    "    concat = full_str + DOCUMENT_TEMPLATE.format(document=example_doc)\n",
    "    concat_enc = tokenizer(concat, add_special_tokens=True)['input_ids']\n",
    "    prefix_enc = tokenizer(full_str, add_special_tokens=True)['input_ids']\n",
    "    doc_ids_from_concat = concat_enc[len(prefix_enc):]\n",
    "\n",
    "    bare_doc_enc = tokenizer(DOCUMENT_TEMPLATE.format(document=example_doc),\n",
    "                              add_special_tokens=False)['input_ids']\n",
    "    match = sum(1 for a, b in zip(doc_ids_from_concat, bare_doc_enc) if a == b)\n",
    "    total = max(len(bare_doc_enc), 1)\n",
    "    print(f\"  {name:<15}: {match}/{total} tokens match ({100*match/total:.1f}%)\")\n",
    "\n",
    "# Condition summary\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"CONDITION DETAILS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "conditions_detail = [\n",
    "    (\"bare\",\n",
    "     \"Standard bare cache: [BOS][doc]\",\n",
    "     \"Baseline. All other conditions compared to this.\"),\n",
    "    (\"static_fact_trunc\",\n",
    "     f\"Prefix: '{STATIC_FACT}' → truncate + RoPE correct\",\n",
    "     \"Best single Mistral condition (d=+0.472). Does it replicate on Gemma?\"),\n",
    "    (\"random_trunc\",\n",
    "     f\"Prefix: '{RANDOM_PREFIX_TEXT}' → truncate + RoPE correct\",\n",
    "     \"Random prefix (d=+0.091 on Mistral). Does ANY prefix help on Gemma?\"),\n",
    "    (\"oracle_trunc\",\n",
    "     \"Prefix: actual query text → truncate + RoPE correct\",\n",
    "     \"Oracle prefix (d=+0.023 ns on Mistral). Is random > oracle on Gemma too?\"),\n",
    "    (\"values_only\",\n",
    "     \"Bare keys + static_fact primed values (hybrid cache)\",\n",
    "     \"Tests value contamination mechanism. On Mistral: values=108%, keys=-4%.\"),\n",
    "]\n",
    "\n",
    "for name, detail, purpose in conditions_detail:\n",
    "    print(f\"\\n### {name} ###\")\n",
    "    print(f\"  {detail}\")\n",
    "    print(f\"  Purpose: {purpose}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3a5d26b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T12:31:57.967487Z",
     "iopub.status.busy": "2026-02-15T12:31:57.967131Z",
     "iopub.status.idle": "2026-02-15T13:59:44.079007Z",
     "shell.execute_reply": "2026-02-15T13:59:44.077951Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "MAIN EVALUATION (300 queries, ~2504 passages)\n",
      "Model: Gemma 3 4B\n",
      "======================================================================\n",
      "Resuming from checkpoint: 75/300\n",
      "Evaluating queries 75 to 299\n",
      "Per passage: 4 forward passes (bare + 3 primed) + 5 scoring passes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca0179e33a8a44269aec78a43dd9ad9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Queries:  25%|##5       | 75/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 100/300 | 25 done in 9.9m | ETA: 79.1 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 125/300 | 50 done in 20.0m | ETA: 70.1 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 150/300 | 75 done in 29.5m | ETA: 59.1 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 175/300 | 100 done in 39.6m | ETA: 49.5 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 200/300 | 125 done in 49.2m | ETA: 39.4 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 225/300 | 150 done in 59.2m | ETA: 29.6 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 250/300 | 175 done in 68.9m | ETA: 19.7 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 275/300 | 200 done in 78.6m | ETA: 9.8 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 300/300 | 225 done in 87.8m | ETA: 0.0 min\n",
      "\n",
      "Evaluation complete: 300 queries in 87.8 min\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Main loop — score all passages under all 5 conditions\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"MAIN EVALUATION ({N} queries, ~{total_passages} passages)\")\n",
    "print(\"Model: Gemma 3 4B\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Checkpoint resume\n",
    "all_results = []\n",
    "start_idx = 0\n",
    "\n",
    "if CHECKPOINT_PATH.exists():\n",
    "    with open(CHECKPOINT_PATH, 'r') as f:\n",
    "        ckpt = json.load(f)\n",
    "    ckpt_queries = ckpt.get('query_texts', [])\n",
    "    current_queries = [q['query'] for q in queries]\n",
    "    if ckpt_queries == current_queries:\n",
    "        all_results = ckpt['results']\n",
    "        start_idx = len(all_results)\n",
    "        print(f\"Resuming from checkpoint: {start_idx}/{N}\")\n",
    "    else:\n",
    "        print(\"Checkpoint query mismatch. Starting fresh.\")\n",
    "else:\n",
    "    print(\"No checkpoint found. Starting fresh.\")\n",
    "\n",
    "print(f\"Evaluating queries {start_idx} to {N-1}\")\n",
    "print(f\"Per passage: 4 forward passes (bare + 3 primed) + 5 scoring passes\")\n",
    "\n",
    "t_start = time.time()\n",
    "\n",
    "for qidx in tqdm(range(start_idx, N), initial=start_idx, total=N, desc=\"Queries\"):\n",
    "    query_data = queries[qidx]\n",
    "    query = query_data['query']\n",
    "    answer = query_data['answer']\n",
    "    query_prompt = QUERY_TEMPLATE.format(query=query)\n",
    "    answer_text = ANSWER_TEMPLATE.format(answer=answer)\n",
    "\n",
    "    passage_results = []\n",
    "\n",
    "    for pidx, pinfo in enumerate(query_data['passages']):\n",
    "        passage = pinfo['passage']\n",
    "        document_text = DOCUMENT_TEMPLATE.format(document=passage)\n",
    "\n",
    "        # --- Matched tokenization (using sf prefix as reference) ---\n",
    "        full_text = sf_str + document_text\n",
    "        full_enc = tokenizer(full_text, return_tensors=\"pt\",\n",
    "                              add_special_tokens=True, padding=False, truncation=False)\n",
    "        full_ids = full_enc['input_ids'].to(exp_config.device)\n",
    "\n",
    "        sf_prefix_enc = tokenizer(sf_str, return_tensors=\"pt\",\n",
    "                                   add_special_tokens=True, padding=False, truncation=False)\n",
    "        sf_prefix_len_matched = sf_prefix_enc['input_ids'].shape[1]\n",
    "\n",
    "        bos_id = full_ids[:, :1]\n",
    "        doc_ids = full_ids[:, sf_prefix_len_matched:]\n",
    "        doc_len = doc_ids.shape[1]\n",
    "        context_len = 1 + doc_len  # BOS + doc\n",
    "\n",
    "        del full_enc, full_ids, sf_prefix_enc\n",
    "\n",
    "        # === 1. Build bare cache ===\n",
    "        bare_input = torch.cat([bos_id, doc_ids], dim=1)\n",
    "        with torch.no_grad():\n",
    "            bare_out = model(input_ids=bare_input,\n",
    "                             attention_mask=torch.ones_like(bare_input),\n",
    "                             use_cache=True, return_dict=True)\n",
    "        bare_cache = _ensure_dynamic_cache(bare_out.past_key_values)\n",
    "        del bare_out, bare_input\n",
    "\n",
    "        # === 2-4. For each priming prefix: build primed cache, truncate + RoPE ===\n",
    "        # Prefix configs for this passage\n",
    "        oracle_str = SURROGATE_PREFIX_TEMPLATE.format(surrogate=query)\n",
    "        oracle_ids_local = tokenizer(oracle_str, return_tensors=\"pt\",\n",
    "                                      add_special_tokens=False)['input_ids'].to(exp_config.device)\n",
    "\n",
    "        prefix_configs_local = [\n",
    "            ('static_fact_trunc', sf_ids),\n",
    "            ('random_trunc', rand_ids),\n",
    "            ('oracle_trunc', oracle_ids_local),\n",
    "        ]\n",
    "\n",
    "        primed_caches = {}  # Store truncated+corrected caches\n",
    "\n",
    "        for p_name, p_ids in prefix_configs_local:\n",
    "            primed_input = torch.cat([bos_id, p_ids, doc_ids], dim=1)\n",
    "            with torch.no_grad():\n",
    "                primed_out = model(input_ids=primed_input,\n",
    "                                   attention_mask=torch.ones_like(primed_input),\n",
    "                                   use_cache=True, return_dict=True)\n",
    "            primed_full = _ensure_dynamic_cache(primed_out.past_key_values)\n",
    "            del primed_out, primed_input\n",
    "\n",
    "            # Truncate: keep [BOS] + [last doc_len positions]\n",
    "            primed_trunc = extract_and_truncate_cache_with_bos(primed_full, doc_len)\n",
    "            # RoPE correct: shift doc positions back by prefix_len\n",
    "            correct_rope_positions_with_bos(primed_trunc, p_ids.shape[1], model)\n",
    "            del primed_full\n",
    "\n",
    "            primed_caches[p_name] = primed_trunc\n",
    "\n",
    "        del oracle_ids_local\n",
    "\n",
    "        # === 5. Build values_only hybrid cache (bare keys + sf primed values) ===\n",
    "        hybrid_values = build_hybrid_cache(\n",
    "            keys_source=bare_cache,\n",
    "            values_source=primed_caches['static_fact_trunc'],\n",
    "        )\n",
    "\n",
    "        # === Score all conditions ===\n",
    "        # Score hybrid and primed conditions first (need deepcopy for each)\n",
    "        nll_values_only = score_answer_with_cache(\n",
    "            deepcopy_cache(hybrid_values), context_len, query_prompt, answer_text,\n",
    "            model, tokenizer, exp_config)\n",
    "        del hybrid_values\n",
    "\n",
    "        nll_sf = score_answer_with_cache(\n",
    "            deepcopy_cache(primed_caches['static_fact_trunc']), context_len,\n",
    "            query_prompt, answer_text, model, tokenizer, exp_config)\n",
    "\n",
    "        nll_rand = score_answer_with_cache(\n",
    "            deepcopy_cache(primed_caches['random_trunc']), context_len,\n",
    "            query_prompt, answer_text, model, tokenizer, exp_config)\n",
    "\n",
    "        nll_oracle = score_answer_with_cache(\n",
    "            deepcopy_cache(primed_caches['oracle_trunc']), context_len,\n",
    "            query_prompt, answer_text, model, tokenizer, exp_config)\n",
    "\n",
    "        del primed_caches\n",
    "\n",
    "        # Score bare LAST (mutates cache)\n",
    "        nll_bare = score_answer_with_cache(\n",
    "            bare_cache, context_len, query_prompt, answer_text,\n",
    "            model, tokenizer, exp_config)\n",
    "        del bare_cache\n",
    "\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        passage_results.append({\n",
    "            'passage_idx': pinfo['passage_idx'],\n",
    "            'is_relevant': pinfo['is_relevant'],\n",
    "            'word_count': pinfo['word_count'],\n",
    "            'bare_nll': nll_bare,\n",
    "            'static_fact_trunc_nll': nll_sf,\n",
    "            'random_trunc_nll': nll_rand,\n",
    "            'oracle_trunc_nll': nll_oracle,\n",
    "            'values_only_nll': nll_values_only,\n",
    "        })\n",
    "\n",
    "    all_results.append({\n",
    "        'query_idx': qidx,\n",
    "        'query': query,\n",
    "        'n_passages': len(passage_results),\n",
    "        'n_relevant': query_data['n_relevant'],\n",
    "        'passage_data': passage_results,\n",
    "    })\n",
    "\n",
    "    # Checkpoint\n",
    "    if (qidx + 1) % CHECKPOINT_EVERY == 0 or qidx == N - 1:\n",
    "        ckpt_data = {\n",
    "            'results': all_results,\n",
    "            'query_texts': [q['query'] for q in queries],\n",
    "            'completed': len(all_results),\n",
    "            'total': N,\n",
    "            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        }\n",
    "        with open(CHECKPOINT_PATH, 'w') as f:\n",
    "            json.dump(ckpt_data, f)\n",
    "        elapsed = time.time() - t_start\n",
    "        n_done = qidx - start_idx + 1\n",
    "        rate = n_done / elapsed if elapsed > 0 else 0\n",
    "        remaining = (N - qidx - 1) / rate if rate > 0 else 0\n",
    "        tqdm.write(f\"  Checkpoint {qidx+1}/{N} | {n_done} done in {elapsed/60:.1f}m | \"\n",
    "                   f\"ETA: {remaining/60:.1f} min\")\n",
    "\n",
    "elapsed_total = time.time() - t_start\n",
    "print(f\"\\nEvaluation complete: {len(all_results)} queries in {elapsed_total/60:.1f} min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb81b44a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T13:59:44.083752Z",
     "iopub.status.busy": "2026-02-15T13:59:44.083442Z",
     "iopub.status.idle": "2026-02-15T13:59:44.465167Z",
     "shell.execute_reply": "2026-02-15T13:59:44.464235Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ANALYSIS — CROSS-MODEL PRIMING REPLICATION\n",
      "======================================================================\n",
      "Valid queries: 300\n",
      "Total passages: 2504, Valid: 2174, Excluded: 330\n",
      "\n",
      "======================================================================\n",
      "NLL SUMMARY (per-passage, Gemma 3 4B)\n",
      "======================================================================\n",
      "\n",
      "Condition                   Mean NLL        Std  d vs Bare     Win%\n",
      "--------------------------------------------------------------------\n",
      "bare                          2.2966     1.7849          —        —\n",
      "static_fact_trunc             2.3138     1.8009     -0.031    45.2%\n",
      "random_trunc                  2.3586     1.8236     -0.109    40.0%\n",
      "oracle_trunc                  2.3077     1.8436     -0.020    46.6%\n",
      "values_only                   2.2693     1.7790     +0.056    49.2%\n",
      "\n",
      "======================================================================\n",
      "STATISTICAL TESTS (paired t-test, per-passage)\n",
      "======================================================================\n",
      "\n",
      "Condition                  Mean ΔNLL        d        t            p   Sig\n",
      "----------------------------------------------------------------------\n",
      "static_fact_trunc            -0.0172   -0.031    -1.46    1.45e-01    ns\n",
      "random_trunc                 -0.0620   -0.109    -5.07    4.36e-07   ***\n",
      "oracle_trunc                 -0.0111   -0.020    -0.95    3.41e-01    ns\n",
      "values_only                   0.0273   +0.056     2.62    8.73e-03    **\n",
      "\n",
      "======================================================================\n",
      "MECHANISM DECOMPOSITION (Gemma 3 4B)\n",
      "======================================================================\n",
      "\n",
      "  Full static_fact_trunc (keys+values): d = -0.031\n",
      "  Values-only (bare keys + sf values):   d = +0.056\n",
      "  Values fraction of full effect:        -180.0%\n",
      "\n",
      "  => Values contribute (-180%) but keys also matter on Gemma\n",
      "\n",
      "======================================================================\n",
      "CROSS-MODEL COMPARISON: Gemma 3 4B vs Mistral 7B\n",
      "======================================================================\n",
      "\n",
      "Condition                  Mistral d    Gemma d    Ratio   Same Sign?\n",
      "----------------------------------------------------------------------\n",
      "static_fact_trunc             +0.472     -0.031    -0.07           NO\n",
      "random_trunc                  +0.091     -0.109    -1.19           NO\n",
      "oracle_trunc                  +0.023     -0.020    -0.89           NO\n",
      "\n",
      "Mechanism                    Mistral      Gemma\n",
      "--------------------------------------------------\n",
      "Values fraction               108.3%    -180.0%\n",
      "\n",
      "======================================================================\n",
      "VERDICT\n",
      "======================================================================\n",
      "  MISTRAL-SPECIFIC: Priming does NOT replicate on Gemma 3 4B.\n",
      "\n",
      "======================================================================\n",
      "HARDNESS INTERACTION\n",
      "======================================================================\n",
      "\n",
      "Condition                     Q1 (easy)            Q2            Q3            Q4     Q5 (hard)       Overall\n",
      "-------------------------------------------------------------------------------------------------------------\n",
      "static_fact_trunc                -0.275        -0.248        -0.134        -0.036        +0.148        -0.031\n",
      "random_trunc                     -0.243        -0.372        -0.254        -0.145        +0.051        -0.109\n",
      "oracle_trunc                     -0.243        -0.202        +0.021        -0.062        +0.127        -0.020\n",
      "values_only                      -0.242        -0.083        +0.020        +0.083        +0.219        +0.056\n",
      "\n",
      "Hardness correlation (bare NLL vs delta):\n",
      "  static_fact_trunc         r=+0.125  p=5.69e-09  ***\n",
      "  random_trunc              r=+0.091  p=2.06e-05  ***\n",
      "  oracle_trunc              r=+0.043  p=4.35e-02  *\n",
      "  values_only               r=+0.148  p=4.18e-12  ***\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Analysis — per-condition stats, Cohen's d, Wilcoxon, cross-model comparison\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ANALYSIS — CROSS-MODEL PRIMING REPLICATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "N_VALID = len(all_results)\n",
    "print(f\"Valid queries: {N_VALID}\")\n",
    "\n",
    "# --- Collect per-passage NLLs ---\n",
    "cond_nlls = {cn: [] for cn in CONDITION_NAMES}\n",
    "for r in all_results:\n",
    "    for p in r['passage_data']:\n",
    "        for cn in CONDITION_NAMES:\n",
    "            cond_nlls[cn].append(p[f'{cn}_nll'])\n",
    "\n",
    "cond_arrays = {cn: np.array(vals) for cn, vals in cond_nlls.items()}\n",
    "\n",
    "# Filter zero NLLs\n",
    "valid = np.ones(len(cond_arrays['bare']), dtype=bool)\n",
    "for cn in CONDITION_NAMES:\n",
    "    valid &= (cond_arrays[cn] != 0)\n",
    "n_passages_valid = int(np.sum(valid))\n",
    "n_excluded = int(np.sum(~valid))\n",
    "print(f\"Total passages: {len(valid)}, Valid: {n_passages_valid}, Excluded: {n_excluded}\")\n",
    "\n",
    "c = {}\n",
    "for cn in CONDITION_NAMES:\n",
    "    c[cn] = cond_arrays[cn][valid]\n",
    "\n",
    "# === 1. NLL Summary Table ===\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"NLL SUMMARY (per-passage, Gemma 3 4B)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\n{'Condition':<25} {'Mean NLL':>10} {'Std':>10} {'d vs Bare':>10} {'Win%':>8}\")\n",
    "print(\"-\" * 68)\n",
    "\n",
    "gemma_ds = {}\n",
    "for cn in CONDITION_NAMES:\n",
    "    mean_nll = np.mean(c[cn])\n",
    "    std_nll = np.std(c[cn])\n",
    "    if cn == 'bare':\n",
    "        print(f\"{cn:<25} {mean_nll:>10.4f} {std_nll:>10.4f} {'—':>10} {'—':>8}\")\n",
    "    else:\n",
    "        delta = c['bare'] - c[cn]\n",
    "        d = cohens_d(delta)\n",
    "        win = np.mean(delta > 0) * 100\n",
    "        gemma_ds[cn] = d\n",
    "        print(f\"{cn:<25} {mean_nll:>10.4f} {std_nll:>10.4f} {d:>+10.3f} {win:>7.1f}%\")\n",
    "\n",
    "# === 2. Statistical Tests ===\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"STATISTICAL TESTS (paired t-test, per-passage)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\n{'Condition':<25} {'Mean ΔNLL':>10} {'d':>8} {'t':>8} {'p':>12} {'Sig':>5}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "stat_results = {}\n",
    "for cn in CONDITION_NAMES:\n",
    "    if cn == 'bare':\n",
    "        continue\n",
    "    delta = c['bare'] - c[cn]\n",
    "    d = cohens_d(delta)\n",
    "    t_stat, p_val = stats.ttest_1samp(delta, 0)\n",
    "    sig = \"***\" if p_val < 0.001 else \"**\" if p_val < 0.01 else \"*\" if p_val < 0.05 else \"ns\"\n",
    "    print(f\"{cn:<25} {np.mean(delta):>10.4f} {d:>+8.3f} {t_stat:>8.2f} {p_val:>11.2e} {sig:>5}\")\n",
    "    stat_results[cn] = {\n",
    "        'mean_delta': float(np.mean(delta)),\n",
    "        'cohens_d': float(d),\n",
    "        't_stat': float(t_stat),\n",
    "        'p_value': float(p_val),\n",
    "        'significant': bool(p_val < 0.05),\n",
    "        'win_rate': float(np.mean(delta > 0)),\n",
    "    }\n",
    "\n",
    "# === 3. Mechanism Decomposition ===\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"MECHANISM DECOMPOSITION (Gemma 3 4B)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "d_sf = gemma_ds.get('static_fact_trunc', 0)\n",
    "d_vo = gemma_ds.get('values_only', 0)\n",
    "\n",
    "values_fraction = d_vo / d_sf if d_sf != 0 else float('nan')\n",
    "\n",
    "print(f\"\\n  Full static_fact_trunc (keys+values): d = {d_sf:+.3f}\")\n",
    "print(f\"  Values-only (bare keys + sf values):   d = {d_vo:+.3f}\")\n",
    "if d_sf != 0:\n",
    "    print(f\"  Values fraction of full effect:        {values_fraction:.1%}\")\n",
    "\n",
    "if d_vo > 0 and values_fraction > 0.8:\n",
    "    print(f\"\\n  => VALUE CONTAMINATION confirmed on Gemma 3 4B\")\n",
    "    print(f\"     (values carry {values_fraction:.0%} of the effect, same as Mistral's {MISTRAL_REF['values_fraction']:.0%})\")\n",
    "elif d_vo > 0:\n",
    "    print(f\"\\n  => Values contribute ({values_fraction:.0%}) but keys also matter on Gemma\")\n",
    "else:\n",
    "    print(f\"\\n  => Values-only does NOT help on Gemma — different mechanism?\")\n",
    "\n",
    "# === 4. Cross-Model Comparison Table ===\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"CROSS-MODEL COMPARISON: Gemma 3 4B vs Mistral 7B\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "mistral_ref_ds = {\n",
    "    'static_fact_trunc': MISTRAL_REF['static_fact_trunc_d'],\n",
    "    'random_trunc': MISTRAL_REF['random_trunc_d'],\n",
    "    'oracle_trunc': MISTRAL_REF['oracle_trunc_d'],\n",
    "}\n",
    "\n",
    "print(f\"\\n{'Condition':<25} {'Mistral d':>10} {'Gemma d':>10} {'Ratio':>8} {'Same Sign?':>12}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for cn in ['static_fact_trunc', 'random_trunc', 'oracle_trunc']:\n",
    "    m_d = mistral_ref_ds.get(cn, 0)\n",
    "    g_d = gemma_ds.get(cn, 0)\n",
    "    ratio = g_d / m_d if m_d != 0 else float('nan')\n",
    "    same_sign = \"Yes\" if (m_d > 0 and g_d > 0) or (m_d < 0 and g_d < 0) or (m_d == 0 and g_d == 0) else \"NO\"\n",
    "    print(f\"{cn:<25} {m_d:>+10.3f} {g_d:>+10.3f} {ratio:>8.2f} {same_sign:>12}\")\n",
    "\n",
    "print(f\"\\n{'Mechanism':<25} {'Mistral':>10} {'Gemma':>10}\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"{'Values fraction':<25} {MISTRAL_REF['values_fraction']:>10.1%} {values_fraction:>10.1%}\")\n",
    "\n",
    "# Verdict\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"VERDICT\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "sf_replicates = gemma_ds.get('static_fact_trunc', 0) > 0 and stat_results.get('static_fact_trunc', {}).get('significant', False)\n",
    "rand_helps = gemma_ds.get('random_trunc', 0) > 0\n",
    "oracle_worse_than_rand = gemma_ds.get('random_trunc', 0) > gemma_ds.get('oracle_trunc', 0)\n",
    "values_mechanism = d_vo > 0 and values_fraction > 0.5\n",
    "\n",
    "if sf_replicates and values_mechanism:\n",
    "    print(\"  UNIVERSAL MECHANISM: Priming via value contamination replicates on Gemma 3 4B.\")\n",
    "    print(f\"  static_fact_trunc: d={gemma_ds.get('static_fact_trunc', 0):+.3f} (Mistral: d={MISTRAL_REF['static_fact_trunc_d']:+.3f})\")\n",
    "    print(f\"  Value contamination: {values_fraction:.0%} (Mistral: {MISTRAL_REF['values_fraction']:.0%})\")\n",
    "elif sf_replicates:\n",
    "    print(\"  PARTIAL REPLICATION: Priming helps but mechanism may differ.\")\n",
    "else:\n",
    "    print(\"  MISTRAL-SPECIFIC: Priming does NOT replicate on Gemma 3 4B.\")\n",
    "\n",
    "if rand_helps:\n",
    "    print(f\"  Random prefix helps: d={gemma_ds.get('random_trunc', 0):+.3f}\")\n",
    "if oracle_worse_than_rand:\n",
    "    print(f\"  Random > Oracle replicates: rand d={gemma_ds.get('random_trunc', 0):+.3f} > oracle d={gemma_ds.get('oracle_trunc', 0):+.3f}\")\n",
    "\n",
    "# === 5. Hardness Interaction ===\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"HARDNESS INTERACTION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "bare_all = c['bare']\n",
    "quintile_boundaries = np.percentile(bare_all, [20, 40, 60, 80])\n",
    "quintile_labels = ['Q1 (easy)', 'Q2', 'Q3', 'Q4', 'Q5 (hard)']\n",
    "\n",
    "def get_quintile(nll, boundaries):\n",
    "    for i, b in enumerate(boundaries):\n",
    "        if nll <= b:\n",
    "            return i\n",
    "    return len(boundaries)\n",
    "\n",
    "quintiles = np.array([get_quintile(nll, quintile_boundaries) for nll in bare_all])\n",
    "\n",
    "hardness_conds = ['static_fact_trunc', 'random_trunc', 'oracle_trunc', 'values_only']\n",
    "\n",
    "header = f\"{'Condition':<25}\" + \"\".join(f\"{ql:>14}\" for ql in quintile_labels) + f\"{'Overall':>14}\"\n",
    "print(f\"\\n{header}\")\n",
    "print(\"-\" * (25 + 14 * 6))\n",
    "\n",
    "hardness_breakdown = {}\n",
    "for cn in hardness_conds:\n",
    "    row = f\"{cn:<25}\"\n",
    "    quintile_ds = []\n",
    "    for q in range(5):\n",
    "        mask_q = quintiles == q\n",
    "        if np.sum(mask_q) < 10:\n",
    "            row += f\"{'n/a':>14}\"\n",
    "            quintile_ds.append(None)\n",
    "        else:\n",
    "            delta = bare_all[mask_q] - c[cn][mask_q]\n",
    "            d = cohens_d(delta)\n",
    "            row += f\"{d:>+14.3f}\"\n",
    "            quintile_ds.append(float(d))\n",
    "    d_all = cohens_d(bare_all - c[cn])\n",
    "    row += f\"{d_all:>+14.3f}\"\n",
    "    print(row)\n",
    "    hardness_breakdown[cn] = {'quintile_ds': quintile_ds, 'overall_d': float(d_all)}\n",
    "\n",
    "# Hardness correlation\n",
    "print(\"\\nHardness correlation (bare NLL vs delta):\")\n",
    "for cn in hardness_conds:\n",
    "    delta = bare_all - c[cn]\n",
    "    r, p = stats.pearsonr(bare_all, delta)\n",
    "    sig = \"***\" if p < 0.001 else \"**\" if p < 0.01 else \"*\" if p < 0.05 else \"ns\"\n",
    "    print(f\"  {cn:<25} r={r:+.3f}  p={p:.2e}  {sig}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "331dac0e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T13:59:44.470059Z",
     "iopub.status.busy": "2026-02-15T13:59:44.469414Z",
     "iopub.status.idle": "2026-02-15T13:59:46.625745Z",
     "shell.execute_reply": "2026-02-15T13:59:46.624891Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plots saved to results/exp16/analysis_plots.png\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Plots — 4-panel figure\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Color scheme\n",
    "colors = {\n",
    "    'bare': '#7f7f7f',\n",
    "    'static_fact_trunc': '#d62728',\n",
    "    'random_trunc': '#ff7f0e',\n",
    "    'oracle_trunc': '#2ca02c',\n",
    "    'values_only': '#1f77b4',\n",
    "}\n",
    "\n",
    "# --- Plot 1: Cohen's d bar chart with Mistral reference ---\n",
    "ax = axes[0, 0]\n",
    "conds_plot = ['static_fact_trunc', 'random_trunc', 'oracle_trunc', 'values_only']\n",
    "gemma_d_vals = [gemma_ds.get(cn, 0) for cn in conds_plot]\n",
    "mistral_d_vals = [mistral_ref_ds.get(cn, MISTRAL_REF.get(f'd_{cn}', 0)) for cn in conds_plot]\n",
    "\n",
    "# For values_only, use Mistral d_values_only\n",
    "mistral_d_vals[3] = MISTRAL_REF['d_values_only']\n",
    "\n",
    "x = np.arange(len(conds_plot))\n",
    "width = 0.35\n",
    "bars_g = ax.bar(x - width/2, gemma_d_vals, width, label='Gemma 3 4B',\n",
    "                color=[colors[cn] for cn in conds_plot], edgecolor='black', linewidth=0.5)\n",
    "bars_m = ax.bar(x + width/2, mistral_d_vals, width, label='Mistral 7B (ref)',\n",
    "                color=[colors[cn] for cn in conds_plot], edgecolor='black', linewidth=0.5,\n",
    "                alpha=0.4, hatch='//')\n",
    "\n",
    "for i, (gd, md) in enumerate(zip(gemma_d_vals, mistral_d_vals)):\n",
    "    ax.text(i - width/2, gd + 0.01, f\"{gd:+.3f}\", ha='center', fontsize=7, rotation=45)\n",
    "    ax.text(i + width/2, md + 0.01, f\"{md:+.3f}\", ha='center', fontsize=7, rotation=45, alpha=0.6)\n",
    "\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([cn.replace('_trunc', '').replace('_', '\\n') for cn in conds_plot],\n",
    "                    fontsize=8)\n",
    "ax.set_ylabel(\"Cohen's d vs Bare\")\n",
    "ax.set_title(\"Cross-Model: Gemma vs Mistral Effect Sizes\")\n",
    "ax.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "ax.legend(fontsize=8)\n",
    "\n",
    "# --- Plot 2: Hardness scatter (static_fact_trunc) ---\n",
    "ax = axes[0, 1]\n",
    "delta_sf = c['bare'] - c['static_fact_trunc']\n",
    "ax.scatter(c['bare'], delta_sf, alpha=0.15, s=8, color=colors['static_fact_trunc'])\n",
    "# Fit line\n",
    "z = np.polyfit(c['bare'], delta_sf, 1)\n",
    "x_fit = np.linspace(c['bare'].min(), c['bare'].max(), 100)\n",
    "ax.plot(x_fit, np.polyval(z, x_fit), color='black', linewidth=2, linestyle='--')\n",
    "r_sf, p_sf = stats.pearsonr(c['bare'], delta_sf)\n",
    "ax.set_xlabel(\"Bare NLL (difficulty)\")\n",
    "ax.set_ylabel(\"ΔNLL (bare - static_fact)\")\n",
    "ax.set_title(f\"Hardness Interaction (r={r_sf:+.3f}, p={p_sf:.1e})\")\n",
    "ax.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "\n",
    "# --- Plot 3: Mechanism decomposition ---\n",
    "ax = axes[1, 0]\n",
    "decomp_labels = ['Full\\n(K+V)', 'Values\\nonly']\n",
    "gemma_decomp = [d_sf, d_vo]\n",
    "mistral_decomp = [MISTRAL_REF['d_full_trunc'], MISTRAL_REF['d_values_only']]\n",
    "\n",
    "x_dec = np.arange(len(decomp_labels))\n",
    "width_dec = 0.35\n",
    "ax.bar(x_dec - width_dec/2, gemma_decomp, width_dec, label='Gemma 3 4B',\n",
    "       color=['#d62728', '#1f77b4'], edgecolor='black', linewidth=0.5)\n",
    "ax.bar(x_dec + width_dec/2, mistral_decomp, width_dec, label='Mistral 7B (ref)',\n",
    "       color=['#d62728', '#1f77b4'], edgecolor='black', linewidth=0.5,\n",
    "       alpha=0.4, hatch='//')\n",
    "\n",
    "for i, (gd, md) in enumerate(zip(gemma_decomp, mistral_decomp)):\n",
    "    ax.text(i - width_dec/2, gd + 0.005, f\"{gd:+.3f}\", ha='center', va='bottom', fontsize=9)\n",
    "    ax.text(i + width_dec/2, md + 0.005, f\"{md:+.3f}\", ha='center', va='bottom', fontsize=9, alpha=0.6)\n",
    "\n",
    "ax.set_xticks(x_dec)\n",
    "ax.set_xticklabels(decomp_labels, fontsize=9)\n",
    "ax.axhline(y=0, color='gray', linestyle='--')\n",
    "ax.set_ylabel(\"Cohen's d vs Bare\")\n",
    "ax.set_title(\"Mechanism: Value Contamination Decomposition\")\n",
    "ax.legend(fontsize=8)\n",
    "\n",
    "# --- Plot 4: NLL distributions (all conditions) ---\n",
    "ax = axes[1, 1]\n",
    "for cn in CONDITION_NAMES:\n",
    "    ax.hist(c[cn], bins=50, alpha=0.4, label=cn, color=colors.get(cn, 'gray'))\n",
    "ax.set_xlabel(\"NLL\")\n",
    "ax.set_ylabel(\"Count\")\n",
    "ax.set_title(\"NLL Distributions (all conditions)\")\n",
    "ax.legend(fontsize=7)\n",
    "\n",
    "plt.suptitle('Exp 16: Cross-Model Priming — Gemma 3 4B vs Mistral 7B', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'analysis_plots.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"Plots saved to {RESULTS_DIR / 'analysis_plots.png'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3ea597a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T13:59:46.629568Z",
     "iopub.status.busy": "2026-02-15T13:59:46.628818Z",
     "iopub.status.idle": "2026-02-15T13:59:46.699629Z",
     "shell.execute_reply": "2026-02-15T13:59:46.698586Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to results/exp16/results.json\n",
      "File size: 849.5 KB\n",
      "\n",
      "======================================================================\n",
      "SUMMARY — Exp 16: Cross-Model Priming Replication\n",
      "======================================================================\n",
      "Model: Gemma 3 4B (34 layers, head_dim=256, bfloat16)\n",
      "Dataset: MS MARCO v1.1 (300 queries, 2174 passages)\n",
      "\n",
      "Effect sizes (Cohen's d vs bare):\n",
      "  static_fact_trunc         d=-0.031  (ns)\n",
      "  random_trunc              d=-0.109  (sig)\n",
      "  oracle_trunc              d=-0.020  (ns)\n",
      "  values_only               d=+0.056  (sig)\n",
      "\n",
      "Mechanism: values carry -180% of the effect\n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Save results JSON\n",
    "final = {\n",
    "    'experiment': 'exp16_cross_model_gemma3',\n",
    "    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'config': {\n",
    "        'model_name': MODEL_NAME,\n",
    "        'model_type': 'gemma3',\n",
    "        'seed': SEED,\n",
    "        'n_queries': N,\n",
    "        'n_valid': N_VALID,\n",
    "        'n_passages_valid': n_passages_valid,\n",
    "        'n_passages_excluded': n_excluded,\n",
    "        'max_passage_words': MAX_PASSAGE_WORDS,\n",
    "        'min_passages_per_query': MIN_PASSAGES_PER_QUERY,\n",
    "        'dataset': 'MS MARCO v1.1 validation',\n",
    "        'prefixes': {\n",
    "            'static_fact': STATIC_FACT,\n",
    "            'random': RANDOM_PREFIX_TEXT,\n",
    "            'oracle': '(actual query per sample)',\n",
    "        },\n",
    "    },\n",
    "    'gemma_architecture': {\n",
    "        'hidden_size': text_config.hidden_size,\n",
    "        'num_layers': text_config.num_hidden_layers,\n",
    "        'num_attention_heads': text_config.num_attention_heads,\n",
    "        'num_kv_heads': text_config.num_key_value_heads,\n",
    "        'head_dim': _get_head_dim(model.config),\n",
    "        'rope_thetas': sorted(list(thetas)),\n",
    "    },\n",
    "    'condition_names': CONDITION_NAMES,\n",
    "    'nll_summary': {\n",
    "        cn: {\n",
    "            'mean': float(np.mean(c[cn])),\n",
    "            'std': float(np.std(c[cn])),\n",
    "            'cohens_d_vs_bare': float(cohens_d(c['bare'] - c[cn])) if cn != 'bare' else 0.0,\n",
    "        }\n",
    "        for cn in CONDITION_NAMES\n",
    "    },\n",
    "    'statistical_tests': stat_results,\n",
    "    'mechanism_decomposition': {\n",
    "        'd_full_sf': float(d_sf),\n",
    "        'd_values_only': float(d_vo),\n",
    "        'values_fraction': float(values_fraction) if not np.isnan(values_fraction) else None,\n",
    "    },\n",
    "    'cross_model_comparison': {\n",
    "        'mistral_reference': MISTRAL_REF,\n",
    "        'gemma_ds': {cn: float(gemma_ds[cn]) for cn in gemma_ds},\n",
    "    },\n",
    "    'hardness_breakdown': hardness_breakdown,\n",
    "    'per_query_results': all_results,\n",
    "}\n",
    "\n",
    "with open(FINAL_RESULTS_PATH, 'w') as f:\n",
    "    json.dump(final, f, indent=2)\n",
    "\n",
    "print(f\"Results saved to {FINAL_RESULTS_PATH}\")\n",
    "print(f\"File size: {FINAL_RESULTS_PATH.stat().st_size / 1024:.1f} KB\")\n",
    "\n",
    "# Print summary\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"SUMMARY — Exp 16: Cross-Model Priming Replication\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Model: Gemma 3 4B (34 layers, head_dim=256, bfloat16)\")\n",
    "print(f\"Dataset: MS MARCO v1.1 ({N} queries, {n_passages_valid} passages)\")\n",
    "print(f\"\\nEffect sizes (Cohen's d vs bare):\")\n",
    "for cn in ['static_fact_trunc', 'random_trunc', 'oracle_trunc', 'values_only']:\n",
    "    g_d = gemma_ds.get(cn, 0)\n",
    "    sig = stat_results.get(cn, {}).get('significant', False)\n",
    "    sig_str = \"(sig)\" if sig else \"(ns)\"\n",
    "    print(f\"  {cn:<25} d={g_d:>+.3f}  {sig_str}\")\n",
    "print(f\"\\nMechanism: values carry {values_fraction:.0%} of the effect\")\n",
    "print(\"\\nDone!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "016ab1cbf4124f728b84604aec2cdf65": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "04f5f2d5ad30440aa461194bd2df37ad": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_d9b699c8fe0b4448b1b66d9b590912b2",
       "placeholder": "​",
       "style": "IPY_MODEL_f721dbb6a2194f62959988079800d2a3",
       "tabbable": null,
       "tooltip": null,
       "value": "Filtering:   9%"
      }
     },
     "060028a018c24799a73e09760c6bf3e3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "318ec4591b784c248daf8d3b2ed9d6ad": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_c0496d15c05e41cfa03a7192ce61deae",
        "IPY_MODEL_d2dd14d7cb7749599151e863dcc959ef",
        "IPY_MODEL_34de8aac0b2947b380aa30cdbaeb761f"
       ],
       "layout": "IPY_MODEL_aa6fe5ef684d4a428eb270d9a0d306d1",
       "tabbable": null,
       "tooltip": null
      }
     },
     "34de8aac0b2947b380aa30cdbaeb761f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_fffb469b891b4386b496b82ce22be48b",
       "placeholder": "​",
       "style": "IPY_MODEL_85c2eff5d2424dd986d2a97d4f5c1925",
       "tabbable": null,
       "tooltip": null,
       "value": " 883/883 [00:04&lt;00:00, 620.78it/s, Materializing param=model.vision_tower.vision_model.post_layernorm.weight]"
      }
     },
     "3a3d427e39604daab3eb2ce2fa254cc6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "4b2df4a5ebe644b5b282a66b44c4d329": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_04f5f2d5ad30440aa461194bd2df37ad",
        "IPY_MODEL_fb9c2e5819b142ea848fbc4b96459569",
        "IPY_MODEL_9a85af79fd764443a7ae90863f98b041"
       ],
       "layout": "IPY_MODEL_6cb701776a314c66b220f1741c0ea05a",
       "tabbable": null,
       "tooltip": null
      }
     },
     "6c7671ee19814a48a3496530f5b2b8ee": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "6cb701776a314c66b220f1741c0ea05a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6dcdfcd2f14d407ea41d72de8946cdb3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "7a3610703ca3420f93e358b8aa69c7d6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "85c2eff5d2424dd986d2a97d4f5c1925": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "8f9fec1bbd604136985a85854260c096": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "95e223222d104ee9afd20dd43725b74d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "9a85af79fd764443a7ae90863f98b041": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_d462aff032144e97830403e933b2d69a",
       "placeholder": "​",
       "style": "IPY_MODEL_e32c7a3341334198981ff6150ceab7c6",
       "tabbable": null,
       "tooltip": null,
       "value": " 925/10047 [00:00&lt;00:01, 4971.64it/s]"
      }
     },
     "a1bfbae1f65b485a83922b48c5fbcae5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "aa6fe5ef684d4a428eb270d9a0d306d1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b4a87a5d4a3842fcb16d109d8afbf539": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_eedeca0ef8f94597b1600bfd41454eb2",
       "max": 300.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_3a3d427e39604daab3eb2ce2fa254cc6",
       "tabbable": null,
       "tooltip": null,
       "value": 300.0
      }
     },
     "b621de11f24341a3b031a1dcaca451b0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_7a3610703ca3420f93e358b8aa69c7d6",
       "placeholder": "​",
       "style": "IPY_MODEL_cdccb42976dc435b9ae6cd4ccdc94bcc",
       "tabbable": null,
       "tooltip": null,
       "value": " 300/300 [1:27:46&lt;00:00, 18.41s/it]"
      }
     },
     "c0496d15c05e41cfa03a7192ce61deae": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_dc8b5b4581c94cd8bb044ac15c4d0ff4",
       "placeholder": "​",
       "style": "IPY_MODEL_a1bfbae1f65b485a83922b48c5fbcae5",
       "tabbable": null,
       "tooltip": null,
       "value": "Loading weights: 100%"
      }
     },
     "ca0179e33a8a44269aec78a43dd9ad9c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_d7a88880fb2947488ad1f259ef6a5a35",
        "IPY_MODEL_b4a87a5d4a3842fcb16d109d8afbf539",
        "IPY_MODEL_b621de11f24341a3b031a1dcaca451b0"
       ],
       "layout": "IPY_MODEL_f346881d1501453b8efe50adc9b9eb93",
       "tabbable": null,
       "tooltip": null
      }
     },
     "cdccb42976dc435b9ae6cd4ccdc94bcc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "d2dd14d7cb7749599151e863dcc959ef": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_6dcdfcd2f14d407ea41d72de8946cdb3",
       "max": 883.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_8f9fec1bbd604136985a85854260c096",
       "tabbable": null,
       "tooltip": null,
       "value": 883.0
      }
     },
     "d462aff032144e97830403e933b2d69a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d7a88880fb2947488ad1f259ef6a5a35": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_016ab1cbf4124f728b84604aec2cdf65",
       "placeholder": "​",
       "style": "IPY_MODEL_6c7671ee19814a48a3496530f5b2b8ee",
       "tabbable": null,
       "tooltip": null,
       "value": "Queries: 100%"
      }
     },
     "d9b699c8fe0b4448b1b66d9b590912b2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "dc8b5b4581c94cd8bb044ac15c4d0ff4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e32c7a3341334198981ff6150ceab7c6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "eedeca0ef8f94597b1600bfd41454eb2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f346881d1501453b8efe50adc9b9eb93": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f721dbb6a2194f62959988079800d2a3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "fb9c2e5819b142ea848fbc4b96459569": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "danger",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_060028a018c24799a73e09760c6bf3e3",
       "max": 10047.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_95e223222d104ee9afd20dd43725b74d",
       "tabbable": null,
       "tooltip": null,
       "value": 925.0
      }
     },
     "fffb469b891b4386b496b82ce22be48b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
