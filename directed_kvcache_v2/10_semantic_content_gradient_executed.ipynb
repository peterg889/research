{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8660486",
   "metadata": {},
   "source": [
    "# Exp 10: Semantic Content Gradient — Does Content Matter?\n",
    "\n",
    "## Motivation\n",
    "\n",
    "Previous experiments show conflicting signals about whether semantic content matters:\n",
    "- **FOR**: static_fact_trunc (d=+0.472) >> random_trunc (d=+0.125) in Exp 07 — 3.8× larger effect\n",
    "- **FOR**: LLM-kw (d=+0.234) >> random (d=+0.125) in Exp 06 — coherence matters\n",
    "- **FOR**: static_fact values-only (d=+0.466) >> random values-only (d=+0.310) in Exp 09\n",
    "- **AGAINST**: oracle (d=+0.023, ns) ≈ random (d=+0.091) in Exp 01\n",
    "- **AGAINST**: separator-only (d=+0.231) ≈ LLM-kw-trunc (d=+0.234) in Exp 06\n",
    "\n",
    "This experiment resolves the question decisively by testing a **gradient of semantic relevance**\n",
    "in **both truncated-prefix and suffix modes**. If semantic content helps in both modes, the\n",
    "effect cannot be explained by structural artifacts alone.\n",
    "\n",
    "## Key Design Principle\n",
    "\n",
    "Each semantic level is tested in BOTH delivery modes (truncated prefix, suffix). This is\n",
    "critical because:\n",
    "- **Truncated prefix**: Affects document value vectors (value contamination)\n",
    "- **Suffix**: Document KV entries are unchanged; effect must come from query → suffix attention\n",
    "\n",
    "If the same semantic gradient appears in BOTH modes, it proves the benefit is genuinely semantic.\n",
    "\n",
    "## 16 Conditions\n",
    "\n",
    "| # | Condition | Mode | Content | Semantic Level |\n",
    "|---|-----------|------|---------|----------------|\n",
    "| 1 | bare | — | No prefix | Baseline |\n",
    "| 2 | random_trunc | Trunc | Random tokens | 0 |\n",
    "| 3 | random_words_trunc | Trunc | Random English words | 0.5 |\n",
    "| 4 | wrong_doc_llm_trunc | Trunc | LLM-kw from wrong doc | 1 |\n",
    "| 5 | tfidf_kw_trunc | Trunc | TF-IDF keywords (right doc) | 2 |\n",
    "| 6 | llm_kw_trunc | Trunc | LLM-kw (right doc) | 3 |\n",
    "| 7 | static_fact_trunc | Trunc | \"What are the key facts?\" | 4 |\n",
    "| 8 | oracle_kw_trunc | Trunc | Oracle as keywords | 5 |\n",
    "| 9 | oracle_raw_trunc | Trunc | Oracle (raw question format) | 5* |\n",
    "| 10 | random_suffix | Suffix | Random tokens | 0 |\n",
    "| 11 | random_words_suffix | Suffix | Random English words | 0.5 |\n",
    "| 12 | wrong_doc_llm_suffix | Suffix | LLM-kw from wrong doc | 1 |\n",
    "| 13 | tfidf_kw_suffix | Suffix | TF-IDF keywords (right doc) | 2 |\n",
    "| 14 | llm_kw_suffix | Suffix | LLM-kw (right doc) | 3 |\n",
    "| 15 | static_fact_suffix | Suffix | \"What are the key facts?\" | 4 |\n",
    "| 16 | oracle_kw_suffix | Suffix | Oracle as keywords | 5 |\n",
    "\n",
    "*oracle_raw_trunc uses question format (potential interference), included for comparison.\n",
    "\n",
    "## 10 Primary Comparisons (Bonferroni alpha = 0.005)\n",
    "\n",
    "| # | Comparison | Question |\n",
    "|---|-----------|----------|\n",
    "| C1 | llm_kw_trunc vs random_trunc | LLM > random in truncated? |\n",
    "| C2 | static_fact_trunc vs random_trunc | Static > random in truncated? |\n",
    "| C3 | llm_kw_trunc vs wrong_doc_llm_trunc | Right doc > wrong doc (trunc)? |\n",
    "| C4 | tfidf_kw_trunc vs random_trunc | TF-IDF > random in truncated? |\n",
    "| C5 | oracle_kw_trunc vs oracle_raw_trunc | Keyword > question format? |\n",
    "| C6 | llm_kw_suffix vs random_suffix | LLM > random in suffix? |\n",
    "| C7 | static_fact_suffix vs random_suffix | Static > random in suffix? |\n",
    "| C8 | llm_kw_suffix vs wrong_doc_llm_suffix | Right doc > wrong doc (suffix)? |\n",
    "| C9 | llm_kw_trunc vs llm_kw_suffix | Truncated > suffix (LLM content)? |\n",
    "| C10 | static_fact_trunc vs static_fact_suffix | Truncated > suffix (static)? |\n",
    "\n",
    "## Decisive Predictions\n",
    "\n",
    "If semantic content matters, we expect:\n",
    "1. **Truncated gradient**: random < wrong_doc < tfidf < llm_kw (C1, C3, C4 all sig)\n",
    "2. **Suffix gradient**: random < wrong_doc < llm_kw (C6, C8 both sig)\n",
    "3. **Static phrase dominance**: static_fact > random in BOTH modes (C2, C7 both sig)\n",
    "4. **Format effect**: oracle_kw > oracle_raw in truncated (C5 sig, replicates Exp 06)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5750307",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-11T03:39:36.222034Z",
     "iopub.status.busy": "2026-02-11T03:39:36.221247Z",
     "iopub.status.idle": "2026-02-11T03:39:39.509980Z",
     "shell.execute_reply": "2026-02-11T03:39:39.508818Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEED: 42\n",
      "Results directory: results/exp10\n",
      "CUDA available: True\n",
      "GPU: NVIDIA L4\n",
      "GPU memory: 23.6 GB\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Setup — permissions, seeds, results directory\n",
    "import os\n",
    "os.umask(0o000)\n",
    "\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "RESULTS_DIR = Path(\"results/exp10\")\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "SURROGATES_DIR = RESULTS_DIR / \"surrogates\"\n",
    "SURROGATES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CHECKPOINT_PATH = RESULTS_DIR / \"checkpoint.json\"\n",
    "FINAL_RESULTS_PATH = RESULTS_DIR / \"results.json\"\n",
    "\n",
    "print(f\"SEED: {SEED}\")\n",
    "print(f\"Results directory: {RESULTS_DIR}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d8a70dc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-11T03:39:39.514389Z",
     "iopub.status.busy": "2026-02-11T03:39:39.513621Z",
     "iopub.status.idle": "2026-02-11T03:40:39.136983Z",
     "shell.execute_reply": "2026-02-11T03:40:39.136034Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading mistralai/Mistral-7B-Instruct-v0.2 (4-bit)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7eff2efecf934b5d8f2df18126093b77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/291 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded. dtype=torch.float16, device=cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Load model (Mistral-7B 4-bit)\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "print(f\"Loading {MODEL_NAME} (4-bit)...\")\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "print(f\"Model loaded. dtype={model.dtype}, device={model.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "434d87f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-11T03:40:39.141029Z",
     "iopub.status.busy": "2026-02-11T03:40:39.140539Z",
     "iopub.status.idle": "2026-02-11T03:40:39.590609Z",
     "shell.execute_reply": "2026-02-11T03:40:39.589710Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config ready\n",
      "  num_samples pool: 2000\n",
      "  eval samples: 1000\n",
      "  bonferroni_alpha: 0.0050 (10 comparisons)\n",
      "  conditions: 16\n",
      "  suffix_separator: '\n",
      "\n",
      "Related question: '\n",
      "  static_factual_phrase: 'What are the key facts I need to know?'\n",
      "  common_english_words: 151 words\n",
      "  semantic_levels: {'random': 0, 'random_words': 0.5, 'wrong_doc_llm': 1, 'tfidf_kw': 2, 'llm_kw': 3, 'static_fact': 4, 'oracle_kw': 5}\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Imports, config, constants, and helper functions\n",
    "sys.path.insert(0, \".\")\n",
    "\n",
    "from lib.config import ExperimentConfig\n",
    "from lib.kv_cache import (\n",
    "    deepcopy_cache,\n",
    "    extract_and_truncate_cache_with_bos,\n",
    "    correct_rope_positions_with_bos,\n",
    "    score_answer_with_cache,\n",
    "    build_suffix_kv_cache,\n",
    ")\n",
    "from lib.data import load_ms_marco, load_evaluation_samples\n",
    "from lib.analysis import cohens_d\n",
    "from lib.surrogate import generate_all_5_surrogates, STATIC_SURROGATE_QUERIES\n",
    "from scipy import stats\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "config = ExperimentConfig(\n",
    "    model_name=MODEL_NAME,\n",
    "    num_samples=2000,\n",
    "    min_passage_words=20,\n",
    "    max_passage_words=500,\n",
    "    seed=SEED,\n",
    ")\n",
    "\n",
    "# Templates — bare text, no \"Document:\\n\" framing\n",
    "SURROGATE_PREFIX_TEMPLATE = \"{surrogate}\\n\"\n",
    "DOCUMENT_TEMPLATE = \"{document}\"\n",
    "QUERY_TEMPLATE = \"\\nQuery: {query}\\nAnswer:\"\n",
    "ANSWER_TEMPLATE = \" {answer}\"\n",
    "\n",
    "N_EVAL = 1000\n",
    "N_COMPARISONS = 10\n",
    "BONFERRONI_ALPHA = 0.05 / N_COMPARISONS\n",
    "CHECKPOINT_EVERY = 50\n",
    "\n",
    "SUFFIX_SEPARATOR = \"\\n\\nRelated question: \"\n",
    "STATIC_FACTUAL_PHRASE = STATIC_SURROGATE_QUERIES['static_factual']['query']\n",
    "\n",
    "CONDITION_NAMES = [\n",
    "    'bare',\n",
    "    # Truncated gradient (8)\n",
    "    'random_trunc', 'random_words_trunc', 'wrong_doc_llm_trunc',\n",
    "    'tfidf_kw_trunc', 'llm_kw_trunc', 'static_fact_trunc',\n",
    "    'oracle_kw_trunc', 'oracle_raw_trunc',\n",
    "    # Suffix gradient (7)\n",
    "    'random_suffix', 'random_words_suffix', 'wrong_doc_llm_suffix',\n",
    "    'tfidf_kw_suffix', 'llm_kw_suffix', 'static_fact_suffix',\n",
    "    'oracle_kw_suffix',\n",
    "]\n",
    "\n",
    "# Semantic levels for gradient analysis\n",
    "SEMANTIC_LEVELS = {\n",
    "    'random': 0, 'random_words': 0.5, 'wrong_doc_llm': 1,\n",
    "    'tfidf_kw': 2, 'llm_kw': 3, 'static_fact': 4, 'oracle_kw': 5,\n",
    "}\n",
    "\n",
    "# --- Stopwords for TF-IDF and oracle-as-keywords ---\n",
    "STOPWORDS = set([\n",
    "    'the', 'a', 'an', 'is', 'are', 'was', 'were', 'be', 'been', 'being',\n",
    "    'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could',\n",
    "    'should', 'may', 'might', 'shall', 'can', 'need', 'dare', 'ought',\n",
    "    'used', 'to', 'of', 'in', 'for', 'on', 'with', 'at', 'by', 'from',\n",
    "    'as', 'into', 'through', 'during', 'before', 'after', 'above', 'below',\n",
    "    'between', 'out', 'off', 'over', 'under', 'again', 'further', 'then',\n",
    "    'once', 'and', 'but', 'or', 'nor', 'not', 'so', 'yet', 'both', 'either',\n",
    "    'neither', 'each', 'every', 'all', 'any', 'few', 'more', 'most', 'other',\n",
    "    'some', 'such', 'no', 'only', 'own', 'same', 'than', 'too', 'very',\n",
    "    'where', 'why', 'how', 'what', 'if', 'up', 'also', 'well', 'back',\n",
    "    'even', 'still', 'new', 'now', 'way', 'many', 'much', 'like', 'get',\n",
    "    'got', 'make', 'made', 'take', 'come', 'go', 'see', 'know', 'think',\n",
    "])\n",
    "\n",
    "QUESTION_STOPWORDS = STOPWORDS | set([\n",
    "    'what', 'which', 'who', 'whom', 'whose', 'when', 'where', 'why', 'how',\n",
    "    'does', 'did', 'can', 'could', 'would', 'should', 'will', 'shall',\n",
    "    'may', 'might', 'must', 'isn', 'aren', 'wasn', 'weren', 'don', 'doesn',\n",
    "    'didn', 'won', 'wouldn', 'couldn', 'shouldn',\n",
    "])\n",
    "\n",
    "# --- Common English words for random_words condition ---\n",
    "COMMON_ENGLISH_WORDS = [\n",
    "    \"apple\", \"river\", \"mountain\", \"table\", \"chair\", \"window\", \"garden\", \"flower\",\n",
    "    \"music\", \"dance\", \"piano\", \"guitar\", \"forest\", \"ocean\", \"desert\", \"island\",\n",
    "    \"bridge\", \"castle\", \"village\", \"market\", \"kitchen\", \"bedroom\", \"library\",\n",
    "    \"hospital\", \"church\", \"stadium\", \"airport\", \"highway\", \"bicycle\", \"telephone\",\n",
    "    \"calendar\", \"newspaper\", \"magazine\", \"photograph\", \"umbrella\", \"birthday\",\n",
    "    \"holiday\", \"vacation\", \"weekend\", \"summer\", \"winter\", \"autumn\", \"spring\",\n",
    "    \"morning\", \"evening\", \"midnight\", \"afternoon\", \"sunrise\", \"sunset\", \"rainbow\",\n",
    "    \"thunder\", \"lightning\", \"earthquake\", \"volcano\", \"diamond\", \"crystal\", \"silver\",\n",
    "    \"golden\", \"copper\", \"bronze\", \"wooden\", \"plastic\", \"rubber\", \"leather\", \"cotton\",\n",
    "    \"marble\", \"granite\", \"concrete\", \"gravel\", \"pebble\", \"boulder\", \"cliff\",\n",
    "    \"valley\", \"meadow\", \"jungle\", \"canyon\", \"glacier\", \"waterfall\", \"harbor\",\n",
    "    \"elephant\", \"dolphin\", \"penguin\", \"parrot\", \"butterfly\", \"crocodile\",\n",
    "    \"salmon\", \"turtle\", \"spider\", \"mosquito\", \"sandwich\", \"chocolate\", \"vanilla\",\n",
    "    \"cinnamon\", \"pepper\", \"mushroom\", \"tomato\", \"potato\", \"banana\", \"strawberry\",\n",
    "    \"blanket\", \"pillow\", \"curtain\", \"mirror\", \"compass\", \"telescope\", \"microscope\",\n",
    "    \"battery\", \"engine\", \"propeller\", \"satellite\", \"oxygen\", \"hydrogen\", \"nitrogen\",\n",
    "    \"calcium\", \"protein\", \"vitamin\", \"bacteria\", \"molecule\", \"equation\", \"triangle\",\n",
    "    \"rectangle\", \"cylinder\", \"sphere\", \"pentagon\", \"diameter\", \"fraction\", \"decimal\",\n",
    "    \"giraffe\", \"kangaroo\", \"flamingo\", \"orchestra\", \"symphony\", \"painting\",\n",
    "    \"sculpture\", \"pottery\", \"costume\", \"jewelry\", \"bracelet\", \"necklace\",\n",
    "    \"backpack\", \"suitcase\", \"envelope\", \"receipt\", \"passport\", \"notebook\",\n",
    "    \"keyboard\", \"monitor\", \"speaker\", \"printer\", \"cabinet\", \"corridor\",\n",
    "]\n",
    "\n",
    "\n",
    "def extract_tfidf_keywords(passage, n_keywords=8):\n",
    "    \"\"\"Extract top content words by frequency (stopwords removed).\"\"\"\n",
    "    words = re.findall(r'\\b[a-zA-Z]+\\b', passage.lower())\n",
    "    content_words = [w for w in words if w not in STOPWORDS and len(w) > 2]\n",
    "    return ' '.join([w for w, _ in Counter(content_words).most_common(n_keywords)])\n",
    "\n",
    "\n",
    "def oracle_to_keywords(query):\n",
    "    \"\"\"Strip question/function words from oracle query to get keyword format.\"\"\"\n",
    "    words = re.findall(r'\\b[a-zA-Z]+\\b', query)\n",
    "    return ' '.join([w for w in words if w.lower() not in QUESTION_STOPWORDS and len(w) > 2])\n",
    "\n",
    "\n",
    "def generate_random_words(rng, n_words=8):\n",
    "    \"\"\"Generate n random English words from the common words list.\"\"\"\n",
    "    indices = rng.randint(0, len(COMMON_ENGLISH_WORDS), size=n_words)\n",
    "    return ' '.join(COMMON_ENGLISH_WORDS[i] for i in indices)\n",
    "\n",
    "\n",
    "def build_primed_and_truncated(prefix_text, bos_id, doc_ids, doc_len, model, tokenizer, config):\n",
    "    \"\"\"Build a primed cache: tokenize prefix, concat [BOS][prefix][doc], forward, truncate+RoPE.\n",
    "\n",
    "    Returns:\n",
    "        (trunc_cache, prefix_token_len) where prefix_token_len includes BOS\n",
    "    \"\"\"\n",
    "    prefix_str = SURROGATE_PREFIX_TEMPLATE.format(surrogate=prefix_text)\n",
    "    prefix_enc = tokenizer(prefix_str, return_tensors=\"pt\",\n",
    "                           add_special_tokens=False, padding=False, truncation=False)\n",
    "    prefix_ids = prefix_enc['input_ids'].to(config.device)\n",
    "    prefix_token_len = 1 + prefix_ids.shape[1]  # BOS + prefix tokens\n",
    "\n",
    "    full_ids = torch.cat([bos_id, prefix_ids, doc_ids], dim=1)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = model(input_ids=full_ids,\n",
    "                    attention_mask=torch.ones_like(full_ids),\n",
    "                    use_cache=True, return_dict=True)\n",
    "\n",
    "    trunc_cache = extract_and_truncate_cache_with_bos(out.past_key_values, doc_len)\n",
    "    correct_rope_positions_with_bos(trunc_cache, prefix_token_len - 1, model)\n",
    "\n",
    "    del out\n",
    "    return trunc_cache, prefix_token_len\n",
    "\n",
    "\n",
    "print(\"Config ready\")\n",
    "print(f\"  num_samples pool: {config.num_samples}\")\n",
    "print(f\"  eval samples: {N_EVAL}\")\n",
    "print(f\"  bonferroni_alpha: {BONFERRONI_ALPHA:.4f} ({N_COMPARISONS} comparisons)\")\n",
    "print(f\"  conditions: {len(CONDITION_NAMES)}\")\n",
    "print(f\"  suffix_separator: '{SUFFIX_SEPARATOR}'\")\n",
    "print(f\"  static_factual_phrase: '{STATIC_FACTUAL_PHRASE}'\")\n",
    "print(f\"  common_english_words: {len(COMMON_ENGLISH_WORDS)} words\")\n",
    "print(f\"  semantic_levels: {SEMANTIC_LEVELS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79c2b160",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-11T03:40:39.594208Z",
     "iopub.status.busy": "2026-02-11T03:40:39.593650Z",
     "iopub.status.idle": "2026-02-11T03:40:40.837512Z",
     "shell.execute_reply": "2026-02-11T03:40:40.836627Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`trust_remote_code` is not supported anymore.\n",
      "Please check that the Hugging Face dataset 'microsoft/ms_marco' isn't based on a loading script and remove `trust_remote_code`.\n",
      "If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading microsoft/ms_marco dataset...\n",
      "Dataset loaded: 10047 samples\n",
      "Filtering samples...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9a8661fb8cc4cf9a2e4a1b2b99162ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filtering:   0%|          | 0/10047 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 2000 samples\n",
      "Loaded 2000 candidates, using first 1000 for evaluation\n",
      "Example passage (92 words): The word totem derives from the Algonquian (most likely Ojibwe) word odoodem [ oˈtuːtɛm ], his kinsh...\n",
      "Example query: what do the carvings on a totem pole mean\n",
      "Example answer: Represent characters or events in a story.\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Load MS MARCO (1000 samples)\n",
    "dataset = load_ms_marco(config)\n",
    "\n",
    "np.random.seed(SEED)\n",
    "all_samples = load_evaluation_samples(dataset, config, require_answer=True)\n",
    "\n",
    "samples = all_samples[:N_EVAL]\n",
    "N = len(samples)\n",
    "print(f\"Loaded {len(all_samples)} candidates, using first {N} for evaluation\")\n",
    "print(f\"Example passage ({len(samples[0]['passage'].split())} words): {samples[0]['passage'][:100]}...\")\n",
    "print(f\"Example query: {samples[0]['query']}\")\n",
    "print(f\"Example answer: {samples[0]['answer']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fab4f7af",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-11T03:40:40.841657Z",
     "iopub.status.busy": "2026-02-11T03:40:40.840899Z",
     "iopub.status.idle": "2026-02-11T07:23:05.658004Z",
     "shell.execute_reply": "2026-02-11T07:23:05.657203Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PHASE 1: LLM SURROGATE GENERATION (keyword only)\n",
      "======================================================================\n",
      "Generating keyword surrogates for samples 0 to 999...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44022585011048f08254a1e21aa2583d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Keyword surrogates:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saved 100/1000 | 0.07 s/s | ETA: 202.8 min\n",
      "  Saved 200/1000 | 0.07 s/s | ETA: 179.0 min\n",
      "  Saved 300/1000 | 0.07 s/s | ETA: 156.1 min\n",
      "  Saved 400/1000 | 0.07 s/s | ETA: 134.1 min\n",
      "  Saved 500/1000 | 0.07 s/s | ETA: 111.5 min\n",
      "  Saved 600/1000 | 0.07 s/s | ETA: 88.9 min\n",
      "  Saved 700/1000 | 0.07 s/s | ETA: 66.9 min\n",
      "  Saved 800/1000 | 0.07 s/s | ETA: 44.5 min\n",
      "  Saved 900/1000 | 0.07 s/s | ETA: 22.3 min\n",
      "  Saved 1000/1000 | 0.07 s/s | ETA: 0.0 min\n",
      "Keyword surrogates complete: 1000 samples\n",
      "Empty surrogates: 0/1000\n",
      "Example: 'totem, algonquian, odoodem, carvings, animals, characters, story, family legends, coastal Pacific Northwest, native culture, european explorers, history, decorative car'\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Generate LLM keyword surrogates (fresh, independent)\n",
    "print(\"=\" * 70)\n",
    "print(\"PHASE 1: LLM SURROGATE GENERATION (keyword only)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "surrogates_path = SURROGATES_DIR / \"keyword_surrogates.json\"\n",
    "\n",
    "if surrogates_path.exists():\n",
    "    with open(surrogates_path, 'r') as f:\n",
    "        surrogates_data = json.load(f)\n",
    "    keyword_surrogates = surrogates_data['surrogates']\n",
    "    print(f\"Loaded {len(keyword_surrogates)} keyword surrogates from cache\")\n",
    "else:\n",
    "    keyword_surrogates = []\n",
    "\n",
    "start_idx_gen = len(keyword_surrogates)\n",
    "if start_idx_gen < N:\n",
    "    print(f\"Generating keyword surrogates for samples {start_idx_gen} to {N-1}...\")\n",
    "    t_start = time.time()\n",
    "    for idx in tqdm(range(start_idx_gen, N), initial=start_idx_gen, total=N,\n",
    "                     desc=\"Keyword surrogates\"):\n",
    "        passage = samples[idx]['passage']\n",
    "        try:\n",
    "            s5 = generate_all_5_surrogates(passage, model, tokenizer, config)\n",
    "            kw = s5.get('keyword_query', '')\n",
    "        except Exception as e:\n",
    "            print(f\"  WARNING: Generation failed for sample {idx}: {e}\")\n",
    "            kw = \"\"\n",
    "        keyword_surrogates.append(kw)\n",
    "\n",
    "        if (idx + 1) % 100 == 0 or idx == N - 1:\n",
    "            with open(surrogates_path, 'w') as f:\n",
    "                json.dump({'surrogates': keyword_surrogates}, f)\n",
    "            elapsed = time.time() - t_start\n",
    "            rate = (idx - start_idx_gen + 1) / elapsed if elapsed > 0 else 0\n",
    "            remaining = (N - idx - 1) / rate if rate > 0 else 0\n",
    "            tqdm.write(f\"  Saved {idx+1}/{N} | {rate:.2f} s/s | ETA: {remaining/60:.1f} min\")\n",
    "\n",
    "    with open(surrogates_path, 'w') as f:\n",
    "        json.dump({'surrogates': keyword_surrogates}, f)\n",
    "    print(f\"Keyword surrogates complete: {len(keyword_surrogates)} samples\")\n",
    "else:\n",
    "    print(f\"All keyword surrogates already cached ({len(keyword_surrogates)} samples)\")\n",
    "\n",
    "n_empty = sum(1 for s in keyword_surrogates if not s.strip())\n",
    "print(f\"Empty surrogates: {n_empty}/{N}\")\n",
    "print(f\"Example: '{keyword_surrogates[0]}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5c39c82",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-11T07:23:05.661503Z",
     "iopub.status.busy": "2026-02-11T07:23:05.661202Z",
     "iopub.status.idle": "2026-02-11T07:23:05.913699Z",
     "shell.execute_reply": "2026-02-11T07:23:05.912790Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PRE-COMPUTING DERIVED SURROGATES\n",
      "======================================================================\n",
      "\n",
      "TF-IDF keywords (1000 samples):\n",
      "  Example 0: 'totem poles word likely objects animals characters events'\n",
      "  Example 1: 'album sea cash shining released wikipedia free encyclopedia'\n",
      "  Empty: 0/1000\n",
      "\n",
      "Oracle-as-keywords (1000 samples):\n",
      "  Example 0: 'carvings totem pole mean'  (from: 'what do the carvings on a totem pole mean')\n",
      "  Example 1: 'wrote sea shining sea'  (from: 'who wrote sea to shining sea')\n",
      "  Empty: 0/1000\n",
      "\n",
      "Token lengths (mean ± std):\n",
      "  LLM keywords: 23.8 ± 11.9\n",
      "  TF-IDF keywords: 11.5 ± 2.8\n",
      "  Oracle-as-keywords: 5.0 ± 2.0\n",
      "  Static factual: 10\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Pre-compute TF-IDF keywords and oracle-as-keywords for all samples\n",
    "print(\"=\" * 70)\n",
    "print(\"PRE-COMPUTING DERIVED SURROGATES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "tfidf_keywords = []\n",
    "oracle_keywords = []\n",
    "\n",
    "for idx in range(N):\n",
    "    passage = samples[idx]['passage']\n",
    "    query = samples[idx]['query']\n",
    "\n",
    "    tfidf_keywords.append(extract_tfidf_keywords(passage, n_keywords=8))\n",
    "    oracle_keywords.append(oracle_to_keywords(query))\n",
    "\n",
    "# Diagnostics\n",
    "print(f\"\\nTF-IDF keywords ({N} samples):\")\n",
    "print(f\"  Example 0: '{tfidf_keywords[0]}'\")\n",
    "print(f\"  Example 1: '{tfidf_keywords[1]}'\")\n",
    "print(f\"  Empty: {sum(1 for t in tfidf_keywords if not t.strip())}/{N}\")\n",
    "\n",
    "print(f\"\\nOracle-as-keywords ({N} samples):\")\n",
    "print(f\"  Example 0: '{oracle_keywords[0]}'  (from: '{samples[0]['query']}')\")\n",
    "print(f\"  Example 1: '{oracle_keywords[1]}'  (from: '{samples[1]['query']}')\")\n",
    "print(f\"  Empty: {sum(1 for o in oracle_keywords if not o.strip())}/{N}\")\n",
    "\n",
    "# Token length comparison\n",
    "llm_lens = [len(tokenizer.encode(kw, add_special_tokens=False)) for kw in keyword_surrogates]\n",
    "tfidf_lens = [len(tokenizer.encode(kw, add_special_tokens=False)) for kw in tfidf_keywords]\n",
    "oracle_kw_lens = [len(tokenizer.encode(kw, add_special_tokens=False)) for kw in oracle_keywords]\n",
    "static_len = len(tokenizer.encode(STATIC_FACTUAL_PHRASE, add_special_tokens=False))\n",
    "\n",
    "print(f\"\\nToken lengths (mean ± std):\")\n",
    "print(f\"  LLM keywords: {np.mean(llm_lens):.1f} ± {np.std(llm_lens):.1f}\")\n",
    "print(f\"  TF-IDF keywords: {np.mean(tfidf_lens):.1f} ± {np.std(tfidf_lens):.1f}\")\n",
    "print(f\"  Oracle-as-keywords: {np.mean(oracle_kw_lens):.1f} ± {np.std(oracle_kw_lens):.1f}\")\n",
    "print(f\"  Static factual: {static_len}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f508f7e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-11T07:23:05.917657Z",
     "iopub.status.busy": "2026-02-11T07:23:05.916742Z",
     "iopub.status.idle": "2026-02-11T07:23:05.926577Z",
     "shell.execute_reply": "2026-02-11T07:23:05.925776Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "EXPERIMENTAL CONDITIONS EXPLAINED\n",
      "======================================================================\n",
      "\n",
      "### 1. bare ###\n",
      "  Cache: [BOS][doc]\n",
      "  Detail: No prefix — baseline\n",
      "\n",
      "### 2. random_trunc ###\n",
      "  Cache: [BOS][random_tokens\\n][doc] → truncate + RoPE\n",
      "  Detail: Random vocabulary tokens. Semantic level: 0\n",
      "\n",
      "### 3. random_words_trunc ###\n",
      "  Cache: [BOS][random_words\\n][doc] → truncate + RoPE\n",
      "  Detail: Random English words: 'mirror cinnamon desert battery cliff kitchen mirror cylinder'. Semantic level: 0.5\n",
      "\n",
      "### 4. wrong_doc_llm_trunc ###\n",
      "  Cache: [BOS][prev_kw\\n][doc] → truncate + RoPE\n",
      "  Detail: LLM keywords from PREVIOUS sample's doc. Right format, wrong content. Semantic level: 1\n",
      "\n",
      "### 5. tfidf_kw_trunc ###\n",
      "  Cache: [BOS][tfidf\\n][doc] → truncate + RoPE\n",
      "  Detail: TF-IDF keywords: 'totem poles word likely objects animals characters events'. Semantic level: 2\n",
      "\n",
      "### 6. llm_kw_trunc ###\n",
      "  Cache: [BOS][llm_kw\\n][doc] → truncate + RoPE\n",
      "  Detail: LLM-generated keywords: 'totem, algonquian, odoodem, carvings, animals, characters, story, family legends, coastal Pacific Northwest, native culture, european explorers, history, decorative car'. Semantic level: 3\n",
      "\n",
      "### 7. static_fact_trunc ###\n",
      "  Cache: [BOS][static_fact\\n][doc] → truncate + RoPE\n",
      "  Detail: Fixed phrase: 'What are the key facts I need to know?'. Semantic level: 4\n",
      "\n",
      "### 8. oracle_kw_trunc ###\n",
      "  Cache: [BOS][oracle_kw\\n][doc] → truncate + RoPE\n",
      "  Detail: Oracle as keywords: 'carvings totem pole mean'. Semantic level: 5\n",
      "\n",
      "### 9. oracle_raw_trunc ###\n",
      "  Cache: [BOS][oracle_raw\\n][doc] → truncate + RoPE\n",
      "  Detail: Oracle raw question: 'what do the carvings on a totem pole mean'. Level 5 but with format interference\n",
      "\n",
      "### 10-16. *_suffix ###\n",
      "  Cache: [BOS][doc][sep][content]\n",
      "  Detail: Same 7 content types as suffix after doc. No value contamination; pure attention.\n",
      "\n",
      "======================================================================\n",
      "FORWARD PASSES PER SAMPLE: 16\n",
      "  Truncated (9): bare + 8 prefix types\n",
      "  Suffix (7): 7 suffix types via build_suffix_kv_cache\n",
      "  Total scoring calls: 16 (one per condition)\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Condition explanation with concrete examples\n",
    "print(\"=\" * 70)\n",
    "print(\"EXPERIMENTAL CONDITIONS EXPLAINED\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "ex = {\n",
    "    'passage': samples[0]['passage'][:80],\n",
    "    'query': samples[0]['query'],\n",
    "    'llm_kw': keyword_surrogates[0],\n",
    "    'tfidf_kw': tfidf_keywords[0],\n",
    "    'oracle_kw': oracle_keywords[0],\n",
    "    'random_words': generate_random_words(np.random.RandomState(SEED), 8),\n",
    "}\n",
    "\n",
    "conditions_explained = [\n",
    "    (\"1. bare\",\n",
    "     \"[BOS][doc]\",\n",
    "     \"No prefix — baseline\"),\n",
    "    (\"2. random_trunc\",\n",
    "     \"[BOS][random_tokens\\\\n][doc] → truncate + RoPE\",\n",
    "     \"Random vocabulary tokens. Semantic level: 0\"),\n",
    "    (\"3. random_words_trunc\",\n",
    "     \"[BOS][random_words\\\\n][doc] → truncate + RoPE\",\n",
    "     f\"Random English words: '{ex['random_words']}'. Semantic level: 0.5\"),\n",
    "    (\"4. wrong_doc_llm_trunc\",\n",
    "     \"[BOS][prev_kw\\\\n][doc] → truncate + RoPE\",\n",
    "     \"LLM keywords from PREVIOUS sample's doc. Right format, wrong content. Semantic level: 1\"),\n",
    "    (\"5. tfidf_kw_trunc\",\n",
    "     \"[BOS][tfidf\\\\n][doc] → truncate + RoPE\",\n",
    "     f\"TF-IDF keywords: '{ex['tfidf_kw']}'. Semantic level: 2\"),\n",
    "    (\"6. llm_kw_trunc\",\n",
    "     \"[BOS][llm_kw\\\\n][doc] → truncate + RoPE\",\n",
    "     f\"LLM-generated keywords: '{ex['llm_kw']}'. Semantic level: 3\"),\n",
    "    (\"7. static_fact_trunc\",\n",
    "     \"[BOS][static_fact\\\\n][doc] → truncate + RoPE\",\n",
    "     f\"Fixed phrase: '{STATIC_FACTUAL_PHRASE}'. Semantic level: 4\"),\n",
    "    (\"8. oracle_kw_trunc\",\n",
    "     \"[BOS][oracle_kw\\\\n][doc] → truncate + RoPE\",\n",
    "     f\"Oracle as keywords: '{ex['oracle_kw']}'. Semantic level: 5\"),\n",
    "    (\"9. oracle_raw_trunc\",\n",
    "     \"[BOS][oracle_raw\\\\n][doc] → truncate + RoPE\",\n",
    "     f\"Oracle raw question: '{ex['query']}'. Level 5 but with format interference\"),\n",
    "    (\"10-16. *_suffix\",\n",
    "     \"[BOS][doc][sep][content]\",\n",
    "     \"Same 7 content types as suffix after doc. No value contamination; pure attention.\"),\n",
    "]\n",
    "\n",
    "for name, pattern, detail in conditions_explained:\n",
    "    print(f\"\\n### {name} ###\")\n",
    "    print(f\"  Cache: {pattern}\")\n",
    "    print(f\"  Detail: {detail}\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"FORWARD PASSES PER SAMPLE: 16\")\n",
    "print(\"  Truncated (9): bare + 8 prefix types\")\n",
    "print(\"  Suffix (7): 7 suffix types via build_suffix_kv_cache\")\n",
    "print(\"  Total scoring calls: 16 (one per condition)\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c59fa71e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-11T07:23:05.930147Z",
     "iopub.status.busy": "2026-02-11T07:23:05.929844Z",
     "iopub.status.idle": "2026-02-11T09:18:48.811889Z",
     "shell.execute_reply": "2026-02-11T09:18:48.810894Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PHASE 2: MAIN EVALUATION (16 conditions × 1000 samples)\n",
      "======================================================================\n",
      "No checkpoint found. Starting fresh.\n",
      "Evaluating samples 0 to 999\n",
      "Conditions: 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53e93bb6461a41be8a2b6ddde4e6dbe2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 50/1000 | 0.15 s/s | ETA: 109.0 min\n",
      "  Checkpoint 100/1000 | 0.14 s/s | ETA: 104.1 min\n",
      "  Checkpoint 150/1000 | 0.14 s/s | ETA: 98.2 min\n",
      "  Checkpoint 200/1000 | 0.14 s/s | ETA: 92.5 min\n",
      "  Checkpoint 250/1000 | 0.14 s/s | ETA: 86.9 min\n",
      "  Checkpoint 300/1000 | 0.14 s/s | ETA: 81.0 min\n",
      "  Checkpoint 350/1000 | 0.14 s/s | ETA: 75.3 min\n",
      "  Checkpoint 400/1000 | 0.14 s/s | ETA: 69.5 min\n",
      "  Checkpoint 450/1000 | 0.14 s/s | ETA: 63.7 min\n",
      "  Checkpoint 500/1000 | 0.14 s/s | ETA: 57.9 min\n",
      "  Checkpoint 550/1000 | 0.14 s/s | ETA: 52.1 min\n",
      "  Checkpoint 600/1000 | 0.14 s/s | ETA: 46.3 min\n",
      "  Checkpoint 650/1000 | 0.14 s/s | ETA: 40.5 min\n",
      "  Checkpoint 700/1000 | 0.14 s/s | ETA: 34.7 min\n",
      "  Checkpoint 750/1000 | 0.14 s/s | ETA: 29.0 min\n",
      "  Checkpoint 800/1000 | 0.14 s/s | ETA: 23.2 min\n",
      "  Checkpoint 850/1000 | 0.14 s/s | ETA: 17.4 min\n",
      "  Checkpoint 900/1000 | 0.14 s/s | ETA: 11.6 min\n",
      "  Checkpoint 950/1000 | 0.14 s/s | ETA: 5.8 min\n",
      "  Checkpoint 1000/1000 | 0.14 s/s | ETA: 0.0 min\n",
      "\n",
      "Evaluation complete: 1000 samples in 115.7 min\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Main eval loop — 16 conditions × 1000 samples\n",
    "print(\"=\" * 70)\n",
    "print(\"PHASE 2: MAIN EVALUATION (16 conditions × 1000 samples)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "results = []\n",
    "start_idx = 0\n",
    "\n",
    "if CHECKPOINT_PATH.exists():\n",
    "    with open(CHECKPOINT_PATH, 'r') as f:\n",
    "        ckpt = json.load(f)\n",
    "    ckpt_queries = ckpt.get('sample_queries', [])\n",
    "    current_queries = [s['query'] for s in samples]\n",
    "    if ckpt_queries == current_queries:\n",
    "        results = ckpt['results']\n",
    "        start_idx = len(results)\n",
    "        print(f\"Resuming from checkpoint: {start_idx}/{N}\")\n",
    "    else:\n",
    "        print(\"Checkpoint sample mismatch. Starting fresh.\")\n",
    "else:\n",
    "    print(\"No checkpoint found. Starting fresh.\")\n",
    "\n",
    "print(f\"Evaluating samples {start_idx} to {N-1}\")\n",
    "print(f\"Conditions: {len(CONDITION_NAMES)}\")\n",
    "\n",
    "t_start = time.time()\n",
    "\n",
    "for idx in tqdm(range(start_idx, N), initial=start_idx, total=N, desc=\"Evaluating\"):\n",
    "    sample = samples[idx]\n",
    "    passage = sample['passage']\n",
    "    query = sample['query']\n",
    "    answer = sample['answer']\n",
    "\n",
    "    query_prompt = QUERY_TEMPLATE.format(query=query)\n",
    "    answer_text = ANSWER_TEMPLATE.format(answer=answer)\n",
    "\n",
    "    # Get all content strings for this sample\n",
    "    llm_kw_text = keyword_surrogates[idx]\n",
    "    tfidf_kw_text = tfidf_keywords[idx]\n",
    "    oracle_kw_text = oracle_keywords[idx]\n",
    "    oracle_raw_text = query  # original question format\n",
    "\n",
    "    # Wrong-doc: use previous sample's LLM keyword\n",
    "    if idx > 0:\n",
    "        wrong_doc_text = keyword_surrogates[idx - 1]\n",
    "    else:\n",
    "        wrong_doc_text = \"\"  # sample 0: handled below\n",
    "\n",
    "    # Random tokens (deterministic per sample)\n",
    "    n_random_tokens = max(5, len(tokenizer.encode(llm_kw_text, add_special_tokens=False)))\n",
    "    rng_tokens = np.random.RandomState(SEED + idx)\n",
    "    random_ids = torch.randint(100, tokenizer.vocab_size - 100, (n_random_tokens,), device='cpu')\n",
    "    random_text = tokenizer.decode(random_ids, skip_special_tokens=True)\n",
    "\n",
    "    # Random English words (deterministic per sample)\n",
    "    rng_words = np.random.RandomState(SEED + idx + 10000)\n",
    "    random_words_text = generate_random_words(rng_words, n_words=8)\n",
    "\n",
    "    # --- Matched tokenization (for truncated conditions) ---\n",
    "    oracle_prefix = SURROGATE_PREFIX_TEMPLATE.format(surrogate=query)\n",
    "    document_text = DOCUMENT_TEMPLATE.format(document=passage)\n",
    "    full_oracle_text = oracle_prefix + document_text\n",
    "\n",
    "    full_oracle_enc = tokenizer(full_oracle_text, return_tensors=\"pt\",\n",
    "                                add_special_tokens=True, padding=False, truncation=False)\n",
    "    full_oracle_ids = full_oracle_enc['input_ids'].to(config.device)\n",
    "\n",
    "    oracle_prefix_enc = tokenizer(oracle_prefix, return_tensors=\"pt\",\n",
    "                                  add_special_tokens=True, padding=False, truncation=False)\n",
    "    oracle_prefix_len = oracle_prefix_enc['input_ids'].shape[1]\n",
    "\n",
    "    bos_id = full_oracle_ids[:, :1]\n",
    "    doc_ids = full_oracle_ids[:, oracle_prefix_len:]\n",
    "    doc_len = doc_ids.shape[1]\n",
    "\n",
    "    # ===== FORWARD PASS 1: BARE =====\n",
    "    bare_ids = torch.cat([bos_id, doc_ids], dim=1)\n",
    "    with torch.no_grad():\n",
    "        bare_out = model(input_ids=bare_ids, attention_mask=torch.ones_like(bare_ids),\n",
    "                         use_cache=True, return_dict=True)\n",
    "    bare_cache = bare_out.past_key_values\n",
    "    del bare_out\n",
    "\n",
    "    nll_bare = score_answer_with_cache(\n",
    "        deepcopy_cache(bare_cache), bare_ids.shape[1],\n",
    "        query_prompt, answer_text, model, tokenizer, config)\n",
    "\n",
    "    # ===== TRUNCATED CONDITIONS (8 forward passes) =====\n",
    "\n",
    "    # 2. random_trunc\n",
    "    trunc_cache, _ = build_primed_and_truncated(\n",
    "        random_text, bos_id, doc_ids, doc_len, model, tokenizer, config)\n",
    "    nll_random_trunc = score_answer_with_cache(\n",
    "        deepcopy_cache(trunc_cache), 1 + doc_len,\n",
    "        query_prompt, answer_text, model, tokenizer, config)\n",
    "    del trunc_cache\n",
    "\n",
    "    # 3. random_words_trunc\n",
    "    trunc_cache, _ = build_primed_and_truncated(\n",
    "        random_words_text, bos_id, doc_ids, doc_len, model, tokenizer, config)\n",
    "    nll_random_words_trunc = score_answer_with_cache(\n",
    "        deepcopy_cache(trunc_cache), 1 + doc_len,\n",
    "        query_prompt, answer_text, model, tokenizer, config)\n",
    "    del trunc_cache\n",
    "\n",
    "    # 4. wrong_doc_llm_trunc (sample 0: NLL=0)\n",
    "    if idx > 0:\n",
    "        trunc_cache, _ = build_primed_and_truncated(\n",
    "            wrong_doc_text, bos_id, doc_ids, doc_len, model, tokenizer, config)\n",
    "        nll_wrong_doc_trunc = score_answer_with_cache(\n",
    "            deepcopy_cache(trunc_cache), 1 + doc_len,\n",
    "            query_prompt, answer_text, model, tokenizer, config)\n",
    "        del trunc_cache\n",
    "    else:\n",
    "        nll_wrong_doc_trunc = 0.0\n",
    "\n",
    "    # 5. tfidf_kw_trunc\n",
    "    trunc_cache, _ = build_primed_and_truncated(\n",
    "        tfidf_kw_text, bos_id, doc_ids, doc_len, model, tokenizer, config)\n",
    "    nll_tfidf_trunc = score_answer_with_cache(\n",
    "        deepcopy_cache(trunc_cache), 1 + doc_len,\n",
    "        query_prompt, answer_text, model, tokenizer, config)\n",
    "    del trunc_cache\n",
    "\n",
    "    # 6. llm_kw_trunc\n",
    "    trunc_cache, _ = build_primed_and_truncated(\n",
    "        llm_kw_text, bos_id, doc_ids, doc_len, model, tokenizer, config)\n",
    "    nll_llm_kw_trunc = score_answer_with_cache(\n",
    "        deepcopy_cache(trunc_cache), 1 + doc_len,\n",
    "        query_prompt, answer_text, model, tokenizer, config)\n",
    "    del trunc_cache\n",
    "\n",
    "    # 7. static_fact_trunc\n",
    "    trunc_cache, _ = build_primed_and_truncated(\n",
    "        STATIC_FACTUAL_PHRASE, bos_id, doc_ids, doc_len, model, tokenizer, config)\n",
    "    nll_static_fact_trunc = score_answer_with_cache(\n",
    "        deepcopy_cache(trunc_cache), 1 + doc_len,\n",
    "        query_prompt, answer_text, model, tokenizer, config)\n",
    "    del trunc_cache\n",
    "\n",
    "    # 8. oracle_kw_trunc\n",
    "    trunc_cache, _ = build_primed_and_truncated(\n",
    "        oracle_kw_text, bos_id, doc_ids, doc_len, model, tokenizer, config)\n",
    "    nll_oracle_kw_trunc = score_answer_with_cache(\n",
    "        deepcopy_cache(trunc_cache), 1 + doc_len,\n",
    "        query_prompt, answer_text, model, tokenizer, config)\n",
    "    del trunc_cache\n",
    "\n",
    "    # 9. oracle_raw_trunc\n",
    "    trunc_cache, _ = build_primed_and_truncated(\n",
    "        oracle_raw_text, bos_id, doc_ids, doc_len, model, tokenizer, config)\n",
    "    nll_oracle_raw_trunc = score_answer_with_cache(\n",
    "        deepcopy_cache(trunc_cache), 1 + doc_len,\n",
    "        query_prompt, answer_text, model, tokenizer, config)\n",
    "    del trunc_cache\n",
    "\n",
    "    del bare_cache, bare_ids\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # ===== SUFFIX CONDITIONS (7 forward passes) =====\n",
    "\n",
    "    # 10. random_suffix\n",
    "    suf_len, suf_cache = build_suffix_kv_cache(\n",
    "        passage, random_text, model, tokenizer, config, separator=SUFFIX_SEPARATOR)\n",
    "    nll_random_suffix = score_answer_with_cache(\n",
    "        deepcopy_cache(suf_cache), suf_len,\n",
    "        query_prompt, answer_text, model, tokenizer, config)\n",
    "    del suf_cache\n",
    "\n",
    "    # 11. random_words_suffix\n",
    "    suf_len, suf_cache = build_suffix_kv_cache(\n",
    "        passage, random_words_text, model, tokenizer, config, separator=SUFFIX_SEPARATOR)\n",
    "    nll_random_words_suffix = score_answer_with_cache(\n",
    "        deepcopy_cache(suf_cache), suf_len,\n",
    "        query_prompt, answer_text, model, tokenizer, config)\n",
    "    del suf_cache\n",
    "\n",
    "    # 12. wrong_doc_llm_suffix (sample 0: NLL=0)\n",
    "    if idx > 0:\n",
    "        suf_len, suf_cache = build_suffix_kv_cache(\n",
    "            passage, wrong_doc_text, model, tokenizer, config, separator=SUFFIX_SEPARATOR)\n",
    "        nll_wrong_doc_suffix = score_answer_with_cache(\n",
    "            deepcopy_cache(suf_cache), suf_len,\n",
    "            query_prompt, answer_text, model, tokenizer, config)\n",
    "        del suf_cache\n",
    "    else:\n",
    "        nll_wrong_doc_suffix = 0.0\n",
    "\n",
    "    # 13. tfidf_kw_suffix\n",
    "    suf_len, suf_cache = build_suffix_kv_cache(\n",
    "        passage, tfidf_kw_text, model, tokenizer, config, separator=SUFFIX_SEPARATOR)\n",
    "    nll_tfidf_suffix = score_answer_with_cache(\n",
    "        deepcopy_cache(suf_cache), suf_len,\n",
    "        query_prompt, answer_text, model, tokenizer, config)\n",
    "    del suf_cache\n",
    "\n",
    "    # 14. llm_kw_suffix\n",
    "    suf_len, suf_cache = build_suffix_kv_cache(\n",
    "        passage, llm_kw_text, model, tokenizer, config, separator=SUFFIX_SEPARATOR)\n",
    "    nll_llm_kw_suffix = score_answer_with_cache(\n",
    "        deepcopy_cache(suf_cache), suf_len,\n",
    "        query_prompt, answer_text, model, tokenizer, config)\n",
    "    del suf_cache\n",
    "\n",
    "    # 15. static_fact_suffix\n",
    "    suf_len, suf_cache = build_suffix_kv_cache(\n",
    "        passage, STATIC_FACTUAL_PHRASE, model, tokenizer, config, separator=SUFFIX_SEPARATOR)\n",
    "    nll_static_fact_suffix = score_answer_with_cache(\n",
    "        deepcopy_cache(suf_cache), suf_len,\n",
    "        query_prompt, answer_text, model, tokenizer, config)\n",
    "    del suf_cache\n",
    "\n",
    "    # 16. oracle_kw_suffix\n",
    "    suf_len, suf_cache = build_suffix_kv_cache(\n",
    "        passage, oracle_kw_text, model, tokenizer, config, separator=SUFFIX_SEPARATOR)\n",
    "    nll_oracle_kw_suffix = score_answer_with_cache(\n",
    "        deepcopy_cache(suf_cache), suf_len,\n",
    "        query_prompt, answer_text, model, tokenizer, config)\n",
    "    del suf_cache\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # --- Store result ---\n",
    "    result = {\n",
    "        'idx': idx,\n",
    "        'doc_len': doc_len,\n",
    "        'passage_word_count': len(passage.split()),\n",
    "        'bare': nll_bare,\n",
    "        'random_trunc': nll_random_trunc,\n",
    "        'random_words_trunc': nll_random_words_trunc,\n",
    "        'wrong_doc_llm_trunc': nll_wrong_doc_trunc,\n",
    "        'tfidf_kw_trunc': nll_tfidf_trunc,\n",
    "        'llm_kw_trunc': nll_llm_kw_trunc,\n",
    "        'static_fact_trunc': nll_static_fact_trunc,\n",
    "        'oracle_kw_trunc': nll_oracle_kw_trunc,\n",
    "        'oracle_raw_trunc': nll_oracle_raw_trunc,\n",
    "        'random_suffix': nll_random_suffix,\n",
    "        'random_words_suffix': nll_random_words_suffix,\n",
    "        'wrong_doc_llm_suffix': nll_wrong_doc_suffix,\n",
    "        'tfidf_kw_suffix': nll_tfidf_suffix,\n",
    "        'llm_kw_suffix': nll_llm_kw_suffix,\n",
    "        'static_fact_suffix': nll_static_fact_suffix,\n",
    "        'oracle_kw_suffix': nll_oracle_kw_suffix,\n",
    "    }\n",
    "    results.append(result)\n",
    "\n",
    "    if (idx + 1) % CHECKPOINT_EVERY == 0 or idx == N - 1:\n",
    "        ckpt_data = {\n",
    "            'results': results,\n",
    "            'sample_queries': [s['query'] for s in samples],\n",
    "            'completed': len(results),\n",
    "            'total': N,\n",
    "            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        }\n",
    "        with open(CHECKPOINT_PATH, 'w') as f:\n",
    "            json.dump(ckpt_data, f)\n",
    "        elapsed = time.time() - t_start\n",
    "        rate = (idx - start_idx + 1) / elapsed if elapsed > 0 else 0\n",
    "        remaining = (N - idx - 1) / rate if rate > 0 else 0\n",
    "        tqdm.write(f\"  Checkpoint {idx+1}/{N} | {rate:.2f} s/s | ETA: {remaining/60:.1f} min\")\n",
    "\n",
    "elapsed_total = time.time() - t_start\n",
    "print(f\"\\nEvaluation complete: {len(results)} samples in {elapsed_total/60:.1f} min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45d7ff3e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-11T09:18:48.816859Z",
     "iopub.status.busy": "2026-02-11T09:18:48.816550Z",
     "iopub.status.idle": "2026-02-11T09:18:49.289341Z",
     "shell.execute_reply": "2026-02-11T09:18:49.288413Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ANALYSIS — SEMANTIC CONTENT GRADIENT\n",
      "======================================================================\n",
      "Total: 1000, Valid: 929, Excluded: 71\n",
      "\n",
      "Condition                        Mean NLL        Std  d vs Bare    Win%\n",
      "------------------------------------------------------------------------\n",
      "bare                               1.1629     1.7280          —       —\n",
      "random_trunc                       1.1066     1.7179     +0.170  67.1% ***\n",
      "random_words_trunc                 1.1135     1.6964     +0.179  62.5% ***\n",
      "wrong_doc_llm_trunc                1.0892     1.6478     +0.239  62.9% ***\n",
      "tfidf_kw_trunc                     1.1197     1.6304     +0.124  60.4% ***\n",
      "llm_kw_trunc                       1.0761     1.6449     +0.253  69.2% ***\n",
      "static_fact_trunc                  0.9815     1.5230     +0.438  81.8% ***\n",
      "oracle_kw_trunc                    1.1226     1.6866     +0.124  60.6% ***\n",
      "oracle_raw_trunc                   1.1361     1.6890     +0.069  52.5% *\n",
      "random_suffix                      1.0569     1.5994     +0.294  65.9% ***\n",
      "random_words_suffix                1.0934     1.6750     +0.201  61.4% ***\n",
      "wrong_doc_llm_suffix               1.0983     1.6720     +0.166  60.9% ***\n",
      "tfidf_kw_suffix                    1.0194     1.6064     +0.349  70.6% ***\n",
      "llm_kw_suffix                      1.0864     1.7115     +0.179  60.8% ***\n",
      "static_fact_suffix                 1.0843     1.6386     +0.188  63.3% ***\n",
      "oracle_kw_suffix                   1.0453     1.5496     +0.283  64.9% ***\n",
      "\n",
      "===============================================================================================\n",
      "10 PRIMARY COMPARISONS (Bonferroni alpha = 0.0050)\n",
      "===============================================================================================\n",
      "\n",
      "Comparison                               Mean delta        d    Win%        t            p   Sig\n",
      "-----------------------------------------------------------------------------------------------\n",
      "C1: llm_kw vs random (trunc)                 0.0305    0.073   53.2%     2.23    2.59e-02     *\n",
      "C2: static_fact vs random (trunc)            0.1251    0.297   65.9%     9.06    7.43e-19   ***\n",
      "C3: llm_kw vs wrong_doc (trunc)              0.0131    0.035   53.9%     1.07    2.85e-01    ns\n",
      "C4: tfidf vs random (trunc)                 -0.0131   -0.033   45.7%    -1.02    3.10e-01    ns\n",
      "C5: oracle_kw vs oracle_raw (trunc)          0.0134    0.050   54.5%     1.51    1.31e-01    ns\n",
      "C6: llm_kw vs random (suffix)               -0.0295   -0.080   48.0%    -2.44    1.47e-02     *\n",
      "C7: static_fact vs random (suffix)          -0.0274   -0.081   49.6%    -2.46    1.40e-02     *\n",
      "C8: llm_kw vs wrong_doc (suffix)             0.0119    0.028   53.9%     0.86    3.89e-01    ns\n",
      "C9: trunc vs suffix (llm_kw)                 0.0103    0.021   53.3%     0.65    5.17e-01    ns\n",
      "C10: trunc vs suffix (static_fact)           0.1028    0.222   62.6%     6.76    2.38e-11   ***\n",
      "\n",
      "===============================================================================================\n",
      "ALL CONDITIONS vs BARE (sorted by d)\n",
      "===============================================================================================\n",
      "\n",
      "Condition                       d vs Bare    Win%            p\n",
      "-----------------------------------------------------------------\n",
      "static_fact_trunc                   0.438   81.8%    2.72e-37   ***\n",
      "tfidf_kw_suffix                     0.349   70.6%    5.05e-25   ***\n",
      "random_suffix                       0.294   65.9%    1.86e-18   ***\n",
      "oracle_kw_suffix                    0.283   64.9%    2.69e-17   ***\n",
      "llm_kw_trunc                        0.253   69.2%    3.11e-14   ***\n",
      "wrong_doc_llm_trunc                 0.239   62.9%    7.60e-13   ***\n",
      "random_words_suffix                 0.201   61.4%    1.44e-09   ***\n",
      "static_fact_suffix                  0.188   63.3%    1.30e-08   ***\n",
      "random_words_trunc                  0.179   62.5%    5.82e-08   ***\n",
      "llm_kw_suffix                       0.179   60.8%    5.86e-08   ***\n",
      "random_trunc                        0.170   67.1%    2.65e-07   ***\n",
      "wrong_doc_llm_suffix                0.166   60.9%    4.84e-07   ***\n",
      "tfidf_kw_trunc                      0.124   60.4%    1.62e-04   ***\n",
      "oracle_kw_trunc                     0.124   60.6%    1.77e-04   ***\n",
      "oracle_raw_trunc                    0.069   52.5%    3.57e-02     *\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Primary analysis — NLL summary + 10 comparisons\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ANALYSIS — SEMANTIC CONTENT GRADIENT\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Extract arrays and filter zero NLLs\n",
    "cond_arrays = {}\n",
    "for cname in CONDITION_NAMES:\n",
    "    cond_arrays[cname] = np.array([r[cname] for r in results])\n",
    "\n",
    "valid = np.ones(len(results), dtype=bool)\n",
    "for cname in CONDITION_NAMES:\n",
    "    valid &= (cond_arrays[cname] != 0)\n",
    "n_valid = int(np.sum(valid))\n",
    "n_excluded = int(np.sum(~valid))\n",
    "print(f\"Total: {len(results)}, Valid: {n_valid}, Excluded: {n_excluded}\")\n",
    "\n",
    "c = {}\n",
    "for cname in CONDITION_NAMES:\n",
    "    c[cname] = cond_arrays[cname][valid]\n",
    "\n",
    "# NLL summary table\n",
    "print(f\"\\n{'Condition':<30} {'Mean NLL':>10} {'Std':>10} {'d vs Bare':>10} {'Win%':>7}\")\n",
    "print(\"-\" * 72)\n",
    "for cname in CONDITION_NAMES:\n",
    "    mean_nll = np.mean(c[cname])\n",
    "    std_nll = np.std(c[cname])\n",
    "    if cname == 'bare':\n",
    "        print(f\"{cname:<30} {mean_nll:>10.4f} {std_nll:>10.4f} {'—':>10} {'—':>7}\")\n",
    "    else:\n",
    "        delta = c['bare'] - c[cname]\n",
    "        d = cohens_d(delta)\n",
    "        win = np.mean(delta > 0) * 100\n",
    "        _, p_val = stats.ttest_1samp(delta, 0)\n",
    "        sig = \"***\" if p_val < 0.001 else \"**\" if p_val < BONFERRONI_ALPHA else \"*\" if p_val < 0.05 else \"ns\"\n",
    "        print(f\"{cname:<30} {mean_nll:>10.4f} {std_nll:>10.4f} {d:>+10.3f} {win:>5.1f}% {sig}\")\n",
    "\n",
    "# 10 primary comparisons\n",
    "print(f\"\\n{'='*95}\")\n",
    "print(f\"10 PRIMARY COMPARISONS (Bonferroni alpha = {BONFERRONI_ALPHA:.4f})\")\n",
    "print(f\"{'='*95}\")\n",
    "\n",
    "comparisons = [\n",
    "    ('C1: llm_kw vs random (trunc)',\n",
    "     c['random_trunc'] - c['llm_kw_trunc'],\n",
    "     'LLM > random in truncated?'),\n",
    "    ('C2: static_fact vs random (trunc)',\n",
    "     c['random_trunc'] - c['static_fact_trunc'],\n",
    "     'Static > random in truncated?'),\n",
    "    ('C3: llm_kw vs wrong_doc (trunc)',\n",
    "     c['wrong_doc_llm_trunc'] - c['llm_kw_trunc'],\n",
    "     'Right doc > wrong doc (trunc)?'),\n",
    "    ('C4: tfidf vs random (trunc)',\n",
    "     c['random_trunc'] - c['tfidf_kw_trunc'],\n",
    "     'TF-IDF > random in truncated?'),\n",
    "    ('C5: oracle_kw vs oracle_raw (trunc)',\n",
    "     c['oracle_raw_trunc'] - c['oracle_kw_trunc'],\n",
    "     'Keyword > question format?'),\n",
    "    ('C6: llm_kw vs random (suffix)',\n",
    "     c['random_suffix'] - c['llm_kw_suffix'],\n",
    "     'LLM > random in suffix?'),\n",
    "    ('C7: static_fact vs random (suffix)',\n",
    "     c['random_suffix'] - c['static_fact_suffix'],\n",
    "     'Static > random in suffix?'),\n",
    "    ('C8: llm_kw vs wrong_doc (suffix)',\n",
    "     c['wrong_doc_llm_suffix'] - c['llm_kw_suffix'],\n",
    "     'Right doc > wrong doc (suffix)?'),\n",
    "    ('C9: trunc vs suffix (llm_kw)',\n",
    "     c['llm_kw_suffix'] - c['llm_kw_trunc'],\n",
    "     'Truncated > suffix (LLM content)?'),\n",
    "    ('C10: trunc vs suffix (static_fact)',\n",
    "     c['static_fact_suffix'] - c['static_fact_trunc'],\n",
    "     'Truncated > suffix (static)?'),\n",
    "]\n",
    "\n",
    "print(f\"\\n{'Comparison':<40} {'Mean delta':>10} {'d':>8} {'Win%':>7} {'t':>8} {'p':>12} {'Sig':>5}\")\n",
    "print(\"-\" * 95)\n",
    "\n",
    "comparison_results = {}\n",
    "for name, delta, question in comparisons:\n",
    "    d = cohens_d(delta)\n",
    "    win = np.mean(delta > 0) * 100\n",
    "    t_stat, p_val = stats.ttest_1samp(delta, 0)\n",
    "    sig = \"***\" if p_val < 0.001 else \"**\" if p_val < BONFERRONI_ALPHA else \"*\" if p_val < 0.05 else \"ns\"\n",
    "    print(f\"{name:<40} {np.mean(delta):>10.4f} {d:>8.3f} {win:>6.1f}% {t_stat:>8.2f} {p_val:>11.2e} {sig:>5}\")\n",
    "    comparison_results[name] = {\n",
    "        'mean_delta': float(np.mean(delta)),\n",
    "        'cohens_d': float(d),\n",
    "        'win_rate': float(win / 100),\n",
    "        't_stat': float(t_stat),\n",
    "        'p_value': float(p_val),\n",
    "        'bonferroni_significant': bool(p_val < BONFERRONI_ALPHA),\n",
    "        'question': question,\n",
    "    }\n",
    "\n",
    "# All vs Bare\n",
    "print(f\"\\n{'='*95}\")\n",
    "print(\"ALL CONDITIONS vs BARE (sorted by d)\")\n",
    "print(f\"{'='*95}\")\n",
    "all_vs_bare = {}\n",
    "all_conds = [(cn, cohens_d(c['bare'] - c[cn])) for cn in CONDITION_NAMES if cn != 'bare']\n",
    "all_conds.sort(key=lambda x: x[1], reverse=True)\n",
    "print(f\"\\n{'Condition':<30} {'d vs Bare':>10} {'Win%':>7} {'p':>12}\")\n",
    "print(\"-\" * 65)\n",
    "for cname, d in all_conds:\n",
    "    delta = c['bare'] - c[cname]\n",
    "    win = np.mean(delta > 0) * 100\n",
    "    _, p_val = stats.ttest_1samp(delta, 0)\n",
    "    sig = \"***\" if p_val < 0.001 else \"**\" if p_val < BONFERRONI_ALPHA else \"*\" if p_val < 0.05 else \"ns\"\n",
    "    print(f\"{cname:<30} {d:>10.3f} {win:>6.1f}% {p_val:>11.2e} {sig:>5}\")\n",
    "    all_vs_bare[cname] = {'cohens_d': float(d), 'win_rate': float(win/100), 'p_value': float(p_val)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd1e63a2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-11T09:18:49.293405Z",
     "iopub.status.busy": "2026-02-11T09:18:49.292596Z",
     "iopub.status.idle": "2026-02-11T09:18:49.325742Z",
     "shell.execute_reply": "2026-02-11T09:18:49.324868Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "SEMANTIC GRADIENT ANALYSIS\n",
      "======================================================================\n",
      "\n",
      "Content Type          Level    Trunc d   Suffix d  Trunc > Suffix\n",
      "-----------------------------------------------------------------\n",
      "random                  0.0     +0.170     +0.294              no\n",
      "random_words            0.5     +0.179     +0.201              no\n",
      "wrong_doc_llm           1.0     +0.239     +0.166             YES\n",
      "tfidf_kw                2.0     +0.124     +0.349              no\n",
      "llm_kw                  3.0     +0.253     +0.179             YES\n",
      "static_fact             4.0     +0.438     +0.188             YES\n",
      "oracle_kw               5.0     +0.124     +0.283              no\n",
      "\n",
      "Semantic level vs d correlation:\n",
      "  Truncated: Spearman r = +0.036, p = 0.9394\n",
      "  Suffix:    Spearman r = -0.143, p = 0.7599\n",
      "  Cross-mode rank agreement: Spearman r = -0.750, p = 0.0522\n",
      "\n",
      "Format effect (truncated only):\n",
      "  Oracle-as-keywords: d = +0.124\n",
      "  Oracle-raw-question: d = +0.069\n",
      "  Format penalty: d = -0.055\n",
      "\n",
      "======================================================================\n",
      "HARDNESS QUINTILE BREAKDOWN\n",
      "======================================================================\n",
      "\n",
      "Condition                        Q1 (easy)          Q2          Q3          Q4   Q5 (hard)     Overall\n",
      "------------------------------------------------------------------------------------------------------\n",
      "random_trunc                        +0.256      +0.257      +0.233      +0.449      +0.137      +0.170\n",
      "random_words_trunc                  +0.135      +0.253      +0.255      +0.349      +0.203      +0.179\n",
      "wrong_doc_llm_trunc                 +0.084      +0.119      +0.151      +0.424      +0.378      +0.239\n",
      "tfidf_kw_trunc                      -0.083      +0.105      +0.160      +0.191      +0.190      +0.124\n",
      "llm_kw_trunc                        -0.030      +0.192      +0.280      +0.524      +0.342      +0.253\n",
      "static_fact_trunc                   +0.256      +0.453      +1.101      +1.050      +0.604      +0.438\n",
      "oracle_kw_trunc                     -0.102      +0.042      +0.173      +0.278      +0.166      +0.124\n",
      "random_suffix                       -0.056      +0.204      +0.350      +0.378      +0.503      +0.294\n",
      "llm_kw_suffix                       -0.173      -0.047      +0.221      +0.324      +0.303      +0.179\n",
      "static_fact_suffix                  -0.255      -0.075      +0.243      +0.331      +0.333      +0.188\n",
      "oracle_kw_suffix                    -0.344      +0.029      +0.180      +0.521      +0.585      +0.283\n",
      "\n",
      "======================================================================\n",
      "VERDICT: DOES SEMANTIC CONTENT MATTER?\n",
      "======================================================================\n",
      "\n",
      "  Truncated gradient (oracle_kw - random): -0.047\n",
      "  Suffix gradient (oracle_kw - random):    -0.011\n",
      "  Both positive: NO\n",
      "\n",
      "  C1 (llm_kw > random, trunc): NO (ns)\n",
      "  C2 (static > random, trunc):  YES ***\n",
      "  C6 (llm_kw > random, suffix): NO (ns)\n",
      "  C7 (static > random, suffix): NO (ns)\n",
      "\n",
      "  Semantic level vs d (trunc): r = +0.036 (p = 0.9394)\n",
      "  Semantic level vs d (suffix): r = -0.143 (p = 0.7599)\n",
      "\n",
      "  CONCLUSION: MIXED EVIDENCE\n",
      "  The gradient is not consistently significant across modes.\n"
     ]
    }
   ],
   "source": [
    "# Cell 10: Semantic gradient analysis + hardness breakdown\n",
    "\n",
    "# --- SEMANTIC GRADIENT ---\n",
    "print(\"=\" * 70)\n",
    "print(\"SEMANTIC GRADIENT ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Build gradient data for both modes\n",
    "content_types = ['random', 'random_words', 'wrong_doc_llm', 'tfidf_kw', 'llm_kw', 'static_fact', 'oracle_kw']\n",
    "levels = [SEMANTIC_LEVELS[ct] for ct in content_types]\n",
    "\n",
    "trunc_ds = []\n",
    "suffix_ds = []\n",
    "for ct in content_types:\n",
    "    trunc_d = cohens_d(c['bare'] - c[f'{ct}_trunc'])\n",
    "    suffix_d = cohens_d(c['bare'] - c[f'{ct}_suffix'])\n",
    "    trunc_ds.append(trunc_d)\n",
    "    suffix_ds.append(suffix_d)\n",
    "\n",
    "print(f\"\\n{'Content Type':<20} {'Level':>6} {'Trunc d':>10} {'Suffix d':>10} {'Trunc > Suffix':>15}\")\n",
    "print(\"-\" * 65)\n",
    "for ct, lev, td, sd in zip(content_types, levels, trunc_ds, suffix_ds):\n",
    "    trunc_better = \"YES\" if td > sd else \"no\"\n",
    "    print(f\"{ct:<20} {lev:>6.1f} {td:>+10.3f} {sd:>+10.3f} {trunc_better:>15}\")\n",
    "\n",
    "# Correlation: semantic level vs d\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "r_trunc, p_trunc = spearmanr(levels, trunc_ds)\n",
    "r_suffix, p_suffix = spearmanr(levels, suffix_ds)\n",
    "print(f\"\\nSemantic level vs d correlation:\")\n",
    "print(f\"  Truncated: Spearman r = {r_trunc:+.3f}, p = {p_trunc:.4f}\")\n",
    "print(f\"  Suffix:    Spearman r = {r_suffix:+.3f}, p = {p_suffix:.4f}\")\n",
    "\n",
    "# Also check: do both modes agree on ranking?\n",
    "trunc_rank = np.argsort(np.argsort(trunc_ds))\n",
    "suffix_rank = np.argsort(np.argsort(suffix_ds))\n",
    "r_cross, p_cross = spearmanr(trunc_rank, suffix_rank)\n",
    "print(f\"  Cross-mode rank agreement: Spearman r = {r_cross:+.3f}, p = {p_cross:.4f}\")\n",
    "\n",
    "# Oracle raw vs oracle kw (format effect)\n",
    "d_oracle_raw = cohens_d(c['bare'] - c['oracle_raw_trunc'])\n",
    "d_oracle_kw = cohens_d(c['bare'] - c['oracle_kw_trunc'])\n",
    "print(f\"\\nFormat effect (truncated only):\")\n",
    "print(f\"  Oracle-as-keywords: d = {d_oracle_kw:+.3f}\")\n",
    "print(f\"  Oracle-raw-question: d = {d_oracle_raw:+.3f}\")\n",
    "print(f\"  Format penalty: d = {d_oracle_raw - d_oracle_kw:+.3f}\")\n",
    "\n",
    "# --- HARDNESS QUINTILE BREAKDOWN ---\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"HARDNESS QUINTILE BREAKDOWN\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "bare_valid = c['bare']\n",
    "quintile_boundaries = np.percentile(bare_valid, [20, 40, 60, 80])\n",
    "quintile_labels = ['Q1 (easy)', 'Q2', 'Q3', 'Q4', 'Q5 (hard)']\n",
    "\n",
    "def get_quintile(nll, boundaries):\n",
    "    for i, b in enumerate(boundaries):\n",
    "        if nll <= b:\n",
    "            return i\n",
    "    return len(boundaries)\n",
    "\n",
    "quintiles = np.array([get_quintile(nll, quintile_boundaries) for nll in bare_valid])\n",
    "\n",
    "# Key conditions for hardness analysis\n",
    "key_conds = [\n",
    "    'random_trunc', 'random_words_trunc', 'wrong_doc_llm_trunc',\n",
    "    'tfidf_kw_trunc', 'llm_kw_trunc', 'static_fact_trunc', 'oracle_kw_trunc',\n",
    "    'random_suffix', 'llm_kw_suffix', 'static_fact_suffix', 'oracle_kw_suffix',\n",
    "]\n",
    "\n",
    "header = f\"{'Condition':<30}\" + \"\".join(f\"{ql:>12}\" for ql in quintile_labels) + f\"{'Overall':>12}\"\n",
    "print(f\"\\n{header}\")\n",
    "print(\"-\" * (30 + 12 * 6))\n",
    "\n",
    "hardness_breakdown = {}\n",
    "for cname in key_conds:\n",
    "    row = f\"{cname:<30}\"\n",
    "    quintile_ds = []\n",
    "    for q in range(5):\n",
    "        mask_q = quintiles == q\n",
    "        if np.sum(mask_q) < 10:\n",
    "            row += f\"{'n/a':>12}\"\n",
    "            quintile_ds.append(None)\n",
    "        else:\n",
    "            delta = bare_valid[mask_q] - c[cname][mask_q]\n",
    "            d = cohens_d(delta)\n",
    "            row += f\"{d:>+12.3f}\"\n",
    "            quintile_ds.append(float(d))\n",
    "    d_all = cohens_d(bare_valid - c[cname])\n",
    "    row += f\"{d_all:>+12.3f}\"\n",
    "    print(row)\n",
    "    hardness_breakdown[cname] = {'quintile_ds': quintile_ds, 'overall_d': float(d_all)}\n",
    "\n",
    "# Verdict\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"VERDICT: DOES SEMANTIC CONTENT MATTER?\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "# Check if gradient exists in both modes\n",
    "trunc_gradient = trunc_ds[-1] - trunc_ds[0]  # oracle_kw - random\n",
    "suffix_gradient = suffix_ds[-1] - suffix_ds[0]\n",
    "both_positive = trunc_gradient > 0 and suffix_gradient > 0\n",
    "\n",
    "# Check key comparisons\n",
    "c1_sig = comparison_results['C1: llm_kw vs random (trunc)']['bonferroni_significant']\n",
    "c2_sig = comparison_results['C2: static_fact vs random (trunc)']['bonferroni_significant']\n",
    "c6_sig = comparison_results['C6: llm_kw vs random (suffix)']['bonferroni_significant']\n",
    "c7_sig = comparison_results['C7: static_fact vs random (suffix)']['bonferroni_significant']\n",
    "\n",
    "print(f\"\\n  Truncated gradient (oracle_kw - random): {trunc_gradient:+.3f}\")\n",
    "print(f\"  Suffix gradient (oracle_kw - random):    {suffix_gradient:+.3f}\")\n",
    "print(f\"  Both positive: {'YES' if both_positive else 'NO'}\")\n",
    "print(f\"\\n  C1 (llm_kw > random, trunc): {'YES ***' if c1_sig else 'NO (ns)'}\")\n",
    "print(f\"  C2 (static > random, trunc):  {'YES ***' if c2_sig else 'NO (ns)'}\")\n",
    "print(f\"  C6 (llm_kw > random, suffix): {'YES ***' if c6_sig else 'NO (ns)'}\")\n",
    "print(f\"  C7 (static > random, suffix): {'YES ***' if c7_sig else 'NO (ns)'}\")\n",
    "print(f\"\\n  Semantic level vs d (trunc): r = {r_trunc:+.3f} (p = {p_trunc:.4f})\")\n",
    "print(f\"  Semantic level vs d (suffix): r = {r_suffix:+.3f} (p = {p_suffix:.4f})\")\n",
    "\n",
    "if both_positive and c1_sig and c6_sig:\n",
    "    print(f\"\\n  *** CONCLUSION: SEMANTIC CONTENT MATTERS ***\")\n",
    "    print(f\"  The semantic gradient is positive in BOTH truncated and suffix modes.\")\n",
    "    print(f\"  LLM > random is significant in BOTH modes.\")\n",
    "    print(f\"  This cannot be explained by structural artifacts alone.\")\n",
    "elif c2_sig and c7_sig:\n",
    "    print(f\"\\n  *** CONCLUSION: STATIC FACTUAL PHRASE IS SPECIAL ***\")\n",
    "    print(f\"  Static > random in both modes, but LLM may not beat random in suffix.\")\n",
    "else:\n",
    "    print(f\"\\n  CONCLUSION: MIXED EVIDENCE\")\n",
    "    print(f\"  The gradient is not consistently significant across modes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "41e29b11",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-11T09:18:49.329312Z",
     "iopub.status.busy": "2026-02-11T09:18:49.329017Z",
     "iopub.status.idle": "2026-02-11T09:18:52.180106Z",
     "shell.execute_reply": "2026-02-11T09:18:52.179185Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plots saved to results/exp10/analysis_plots.png\n"
     ]
    }
   ],
   "source": [
    "# Cell 11: Plots (2x3 grid)\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(20, 13))\n",
    "\n",
    "# Color scheme: truncated = blue shades, suffix = red shades\n",
    "trunc_color = '#2166ac'\n",
    "suffix_color = '#b2182b'\n",
    "\n",
    "# --- Plot 1: All conditions bar chart (sorted by d) ---\n",
    "ax = axes[0, 0]\n",
    "all_sorted = sorted(\n",
    "    [(cn, cohens_d(c['bare'] - c[cn])) for cn in CONDITION_NAMES if cn != 'bare'],\n",
    "    key=lambda x: x[1], reverse=True\n",
    ")\n",
    "names_sorted = [x[0] for x in all_sorted]\n",
    "ds_sorted = [x[1] for x in all_sorted]\n",
    "colors_bar = [trunc_color if '_trunc' in cn else suffix_color if '_suffix' in cn else 'gray'\n",
    "              for cn in names_sorted]\n",
    "ax.barh(range(len(names_sorted)), ds_sorted, color=colors_bar, edgecolor='black', linewidth=0.5)\n",
    "ax.set_yticks(range(len(names_sorted)))\n",
    "ax.set_yticklabels(names_sorted, fontsize=7)\n",
    "ax.axvline(x=0, color='gray', linestyle='--')\n",
    "ax.set_xlabel(\"Cohen's d vs Bare\")\n",
    "ax.set_title('All Conditions vs Bare')\n",
    "ax.invert_yaxis()\n",
    "\n",
    "# --- Plot 2: Semantic Gradient — Truncated Mode ---\n",
    "ax = axes[0, 1]\n",
    "x_pos = range(len(content_types))\n",
    "ax.bar(x_pos, trunc_ds, color=trunc_color, edgecolor='black', linewidth=0.5, alpha=0.8)\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels([ct.replace('_', '\\n') for ct in content_types], fontsize=7, rotation=45, ha='right')\n",
    "ax.axhline(y=0, color='gray', linestyle='--')\n",
    "ax.set_ylabel(\"Cohen's d vs Bare\")\n",
    "ax.set_title('Semantic Gradient — Truncated Prefix')\n",
    "for i, v in enumerate(trunc_ds):\n",
    "    ax.text(i, v + 0.005, f\"{v:+.3f}\", ha='center', va='bottom', fontsize=7)\n",
    "\n",
    "# --- Plot 3: Semantic Gradient — Suffix Mode ---\n",
    "ax = axes[0, 2]\n",
    "ax.bar(x_pos, suffix_ds, color=suffix_color, edgecolor='black', linewidth=0.5, alpha=0.8)\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels([ct.replace('_', '\\n') for ct in content_types], fontsize=7, rotation=45, ha='right')\n",
    "ax.axhline(y=0, color='gray', linestyle='--')\n",
    "ax.set_ylabel(\"Cohen's d vs Bare\")\n",
    "ax.set_title('Semantic Gradient — Suffix Mode')\n",
    "for i, v in enumerate(suffix_ds):\n",
    "    ax.text(i, v + 0.005, f\"{v:+.3f}\", ha='center', va='bottom', fontsize=7)\n",
    "\n",
    "# --- Plot 4: Overlay — Both modes on same plot ---\n",
    "ax = axes[1, 0]\n",
    "width = 0.35\n",
    "x = np.arange(len(content_types))\n",
    "bars1 = ax.bar(x - width/2, trunc_ds, width, color=trunc_color, alpha=0.8, label='Truncated', edgecolor='black', linewidth=0.5)\n",
    "bars2 = ax.bar(x + width/2, suffix_ds, width, color=suffix_color, alpha=0.8, label='Suffix', edgecolor='black', linewidth=0.5)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([ct.replace('_', '\\n') for ct in content_types], fontsize=7, rotation=45, ha='right')\n",
    "ax.axhline(y=0, color='gray', linestyle='--')\n",
    "ax.set_ylabel(\"Cohen's d vs Bare\")\n",
    "ax.set_title('Truncated vs Suffix — Same Content')\n",
    "ax.legend(fontsize=9)\n",
    "\n",
    "# --- Plot 5: Semantic level scatter with fit lines ---\n",
    "ax = axes[1, 1]\n",
    "ax.scatter(levels, trunc_ds, color=trunc_color, s=80, zorder=5, label='Truncated')\n",
    "ax.scatter(levels, suffix_ds, color=suffix_color, s=80, zorder=5, marker='s', label='Suffix')\n",
    "# Fit lines\n",
    "z_trunc = np.polyfit(levels, trunc_ds, 1)\n",
    "z_suffix = np.polyfit(levels, suffix_ds, 1)\n",
    "x_fit = np.linspace(0, 5, 50)\n",
    "ax.plot(x_fit, np.polyval(z_trunc, x_fit), '--', color=trunc_color, alpha=0.5,\n",
    "        label=f'Trunc fit (slope={z_trunc[0]:.3f})')\n",
    "ax.plot(x_fit, np.polyval(z_suffix, x_fit), '--', color=suffix_color, alpha=0.5,\n",
    "        label=f'Suffix fit (slope={z_suffix[0]:.3f})')\n",
    "for ct, lev, td, sd in zip(content_types, levels, trunc_ds, suffix_ds):\n",
    "    ax.annotate(ct.replace('_', '\\n'), (lev, td), textcoords=\"offset points\",\n",
    "                xytext=(5, 5), fontsize=6)\n",
    "ax.set_xlabel('Semantic Level')\n",
    "ax.set_ylabel(\"Cohen's d vs Bare\")\n",
    "ax.set_title('Semantic Level vs Effect Size')\n",
    "ax.legend(fontsize=7)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# --- Plot 6: Hardness × condition heatmap ---\n",
    "ax = axes[1, 2]\n",
    "hm_conds = ['random_trunc', 'wrong_doc_llm_trunc', 'tfidf_kw_trunc',\n",
    "            'llm_kw_trunc', 'static_fact_trunc', 'oracle_kw_trunc',\n",
    "            'random_suffix', 'llm_kw_suffix', 'static_fact_suffix']\n",
    "hm_data = []\n",
    "for cname in hm_conds:\n",
    "    row = []\n",
    "    for q in range(5):\n",
    "        mask_q = quintiles == q\n",
    "        if np.sum(mask_q) < 10:\n",
    "            row.append(0)\n",
    "        else:\n",
    "            delta = bare_valid[mask_q] - c[cname][mask_q]\n",
    "            row.append(cohens_d(delta))\n",
    "    hm_data.append(row)\n",
    "hm_data = np.array(hm_data)\n",
    "im = ax.imshow(hm_data, cmap='RdBu_r', vmin=-0.5, vmax=0.7, aspect='auto')\n",
    "ax.set_xticks(range(5))\n",
    "ax.set_xticklabels(quintile_labels, fontsize=7)\n",
    "ax.set_yticks(range(len(hm_conds)))\n",
    "ax.set_yticklabels(hm_conds, fontsize=7)\n",
    "for i in range(len(hm_conds)):\n",
    "    for j in range(5):\n",
    "        ax.text(j, i, f\"{hm_data[i,j]:+.2f}\", ha='center', va='center', fontsize=6)\n",
    "plt.colorbar(im, ax=ax, label=\"Cohen's d vs bare\")\n",
    "ax.set_title('Hardness × Condition')\n",
    "\n",
    "plt.suptitle('Exp 10: Semantic Content Gradient', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'analysis_plots.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"Plots saved to {RESULTS_DIR / 'analysis_plots.png'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dbdaaf6c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-11T09:18:52.183458Z",
     "iopub.status.busy": "2026-02-11T09:18:52.183135Z",
     "iopub.status.idle": "2026-02-11T09:18:52.237284Z",
     "shell.execute_reply": "2026-02-11T09:18:52.236496Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to results/exp10/results.json\n",
      "File size: 749.6 KB\n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Cell 12: Save comprehensive results JSON\n",
    "\n",
    "# Gradient summaries\n",
    "gradient_trunc = {}\n",
    "gradient_suffix = {}\n",
    "for ct, lev, td, sd in zip(content_types, levels, trunc_ds, suffix_ds):\n",
    "    gradient_trunc[ct] = {'semantic_level': lev, 'cohens_d': float(td)}\n",
    "    gradient_suffix[ct] = {'semantic_level': lev, 'cohens_d': float(sd)}\n",
    "\n",
    "final = {\n",
    "    'experiment': 'exp10_semantic_content_gradient',\n",
    "    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'config': {\n",
    "        'model_name': config.model_name,\n",
    "        'seed': SEED,\n",
    "        'n_eval': N,\n",
    "        'n_valid': n_valid,\n",
    "        'n_excluded': n_excluded,\n",
    "        'min_passage_words': config.min_passage_words,\n",
    "        'max_passage_words': config.max_passage_words,\n",
    "        'n_conditions': len(CONDITION_NAMES),\n",
    "        'n_comparisons': N_COMPARISONS,\n",
    "        'bonferroni_alpha': BONFERRONI_ALPHA,\n",
    "        'suffix_separator': SUFFIX_SEPARATOR,\n",
    "    },\n",
    "    'condition_names': CONDITION_NAMES,\n",
    "    'nll_summary': {\n",
    "        cname: {\n",
    "            'mean': float(np.mean(c[cname])),\n",
    "            'std': float(np.std(c[cname])),\n",
    "            'cohens_d_vs_bare': float(cohens_d(c['bare'] - c[cname])) if cname != 'bare' else 0.0,\n",
    "        }\n",
    "        for cname in CONDITION_NAMES\n",
    "    },\n",
    "    'gradient_summaries': {\n",
    "        'truncated': gradient_trunc,\n",
    "        'suffix': gradient_suffix,\n",
    "        'spearman_trunc': {'r': float(r_trunc), 'p': float(p_trunc)},\n",
    "        'spearman_suffix': {'r': float(r_suffix), 'p': float(p_suffix)},\n",
    "        'cross_mode_rank_agreement': {'r': float(r_cross), 'p': float(p_cross)},\n",
    "    },\n",
    "    'primary_comparisons': comparison_results,\n",
    "    'all_vs_bare': all_vs_bare,\n",
    "    'hardness_breakdown': hardness_breakdown,\n",
    "    'per_sample_results': results,\n",
    "}\n",
    "\n",
    "with open(FINAL_RESULTS_PATH, 'w') as f:\n",
    "    json.dump(final, f, indent=2)\n",
    "\n",
    "print(f\"Results saved to {FINAL_RESULTS_PATH}\")\n",
    "print(f\"File size: {FINAL_RESULTS_PATH.stat().st_size / 1024:.1f} KB\")\n",
    "print(\"\\nDone!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a17414e1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-11T09:18:52.240595Z",
     "iopub.status.busy": "2026-02-11T09:18:52.240058Z",
     "iopub.status.idle": "2026-02-11T09:18:52.777633Z",
     "shell.execute_reply": "2026-02-11T09:18:52.776532Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning up GPU memory...\n",
      "GPU memory: 4.14 GB -> 0.01 GB\n",
      "Cleanup complete.\n"
     ]
    }
   ],
   "source": [
    "# Cell 13: GPU cleanup\n",
    "import gc\n",
    "\n",
    "print(\"Cleaning up GPU memory...\")\n",
    "mem_before = torch.cuda.memory_allocated() / 1e9\n",
    "\n",
    "del model\n",
    "del tokenizer\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "mem_after = torch.cuda.memory_allocated() / 1e9\n",
    "print(f\"GPU memory: {mem_before:.2f} GB -> {mem_after:.2f} GB\")\n",
    "print(\"Cleanup complete.\")"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cu124.m127",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/base-cu124:m127"
  },
  "kernelspec": {
   "display_name": "Python 3 (Local)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0e89405e9f05402bba9c263f83a4191e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_526c4382f11d46e9b531a882cb89d6c5",
       "max": 1000,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_75258edfc2fc47dfbb5a222b3882377a",
       "tabbable": null,
       "tooltip": null,
       "value": 1000
      }
     },
     "129bf11ca02c41cdbf3580783cd3361b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "129eff89d643467ab7bc0745769fb2be": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "25963be243a641028779f435c19034bf": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "265b2ac28dfb40608cbb523406ead6f3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2dc7e05895224cc89e67e8827dffc891": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "31b81c8c27bc4044ba7dad497b6e7846": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_265b2ac28dfb40608cbb523406ead6f3",
       "placeholder": "​",
       "style": "IPY_MODEL_7a23bf2a8a3346bea4b4722f634594e9",
       "tabbable": null,
       "tooltip": null,
       "value": "Filtering:  41%"
      }
     },
     "44022585011048f08254a1e21aa2583d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_f699e135de2b4de49d5503f9af9fb8a5",
        "IPY_MODEL_0e89405e9f05402bba9c263f83a4191e",
        "IPY_MODEL_6abb5761c65c4e5fa84726ccd857413d"
       ],
       "layout": "IPY_MODEL_129eff89d643467ab7bc0745769fb2be",
       "tabbable": null,
       "tooltip": null
      }
     },
     "499636245c4942c296adfbb7a44f6cdb": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "500448937bd04c51bd38115bb29b82dc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "526c4382f11d46e9b531a882cb89d6c5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "53e93bb6461a41be8a2b6ddde4e6dbe2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_ab254355856f4c64be1e11c6dec5ae09",
        "IPY_MODEL_bbfd7b213c734cb9a56bece73c656682",
        "IPY_MODEL_fc9e97746dac4d0caa846980185076e0"
       ],
       "layout": "IPY_MODEL_5a14d2bff3b24878b51f41bf6e5c2850",
       "tabbable": null,
       "tooltip": null
      }
     },
     "5a14d2bff3b24878b51f41bf6e5c2850": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "60cdca271a59403b81a303881ac35fe0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_c5b7b4ef9d804dc8a7aa2fdc285fdc4e",
       "placeholder": "​",
       "style": "IPY_MODEL_75e4c0ca01d84cf0942731c4bbd798e4",
       "tabbable": null,
       "tooltip": null,
       "value": "Loading weights: 100%"
      }
     },
     "6abb5761c65c4e5fa84726ccd857413d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_25963be243a641028779f435c19034bf",
       "placeholder": "​",
       "style": "IPY_MODEL_2dc7e05895224cc89e67e8827dffc891",
       "tabbable": null,
       "tooltip": null,
       "value": " 1000/1000 [3:42:24&lt;00:00, 12.36s/it]"
      }
     },
     "733ea010b38444bc8f139c0128dabf0c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "75258edfc2fc47dfbb5a222b3882377a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "75e4c0ca01d84cf0942731c4bbd798e4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "7a23bf2a8a3346bea4b4722f634594e9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "7eff2efecf934b5d8f2df18126093b77": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_60cdca271a59403b81a303881ac35fe0",
        "IPY_MODEL_c53e237862c944cc93c6d74b1041fc54",
        "IPY_MODEL_985e18d1ad97422099e9643842869eb3"
       ],
       "layout": "IPY_MODEL_dda2b864058044a5b29fdaae1551f2b2",
       "tabbable": null,
       "tooltip": null
      }
     },
     "8fd42b9570f44f67bae146c09cbfc4ae": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "985e18d1ad97422099e9643842869eb3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_bf6171cd81dc4de3a00d89e35c25a675",
       "placeholder": "​",
       "style": "IPY_MODEL_cb182b5cb2d8479ab06b5495dfc7d007",
       "tabbable": null,
       "tooltip": null,
       "value": " 291/291 [00:50&lt;00:00, 16.23it/s, Materializing param=model.norm.weight]"
      }
     },
     "9cd55356b16046318996d8ac620fa5c3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "a9fcf71455a349c787e667135f1deeda": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "ab254355856f4c64be1e11c6dec5ae09": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_f047d59d83fa404cb59257ff3bbc16d6",
       "placeholder": "​",
       "style": "IPY_MODEL_a9fcf71455a349c787e667135f1deeda",
       "tabbable": null,
       "tooltip": null,
       "value": "Evaluating: 100%"
      }
     },
     "afa122019ef74f8b9c86460bde338931": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_8fd42b9570f44f67bae146c09cbfc4ae",
       "placeholder": "​",
       "style": "IPY_MODEL_500448937bd04c51bd38115bb29b82dc",
       "tabbable": null,
       "tooltip": null,
       "value": " 4153/10047 [00:00&lt;00:00, 7044.34it/s]"
      }
     },
     "b0f8f844dbe94fc1b6885cecb91bee60": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "bbfd7b213c734cb9a56bece73c656682": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_499636245c4942c296adfbb7a44f6cdb",
       "max": 1000,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_ffbcde07b51843ebbdec8e82331512b9",
       "tabbable": null,
       "tooltip": null,
       "value": 1000
      }
     },
     "bf6171cd81dc4de3a00d89e35c25a675": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c53e237862c944cc93c6d74b1041fc54": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_d0d80f0c182449a39d77041c847ff85c",
       "max": 291,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_b0f8f844dbe94fc1b6885cecb91bee60",
       "tabbable": null,
       "tooltip": null,
       "value": 291
      }
     },
     "c5b7b4ef9d804dc8a7aa2fdc285fdc4e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c7d1120042cd4a46b1467c034b08e38f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "cb182b5cb2d8479ab06b5495dfc7d007": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "d0d80f0c182449a39d77041c847ff85c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d638f17023514e65994b41e498ea19a8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "danger",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_e39595692afe47839226a639f93b155e",
       "max": 10047,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_733ea010b38444bc8f139c0128dabf0c",
       "tabbable": null,
       "tooltip": null,
       "value": 4153
      }
     },
     "db221b93984d44e192d20e1240d712c8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "dda2b864058044a5b29fdaae1551f2b2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e39595692afe47839226a639f93b155e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e9a8661fb8cc4cf9a2e4a1b2b99162ad": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_31b81c8c27bc4044ba7dad497b6e7846",
        "IPY_MODEL_d638f17023514e65994b41e498ea19a8",
        "IPY_MODEL_afa122019ef74f8b9c86460bde338931"
       ],
       "layout": "IPY_MODEL_db221b93984d44e192d20e1240d712c8",
       "tabbable": null,
       "tooltip": null
      }
     },
     "f047d59d83fa404cb59257ff3bbc16d6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f699e135de2b4de49d5503f9af9fb8a5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_c7d1120042cd4a46b1467c034b08e38f",
       "placeholder": "​",
       "style": "IPY_MODEL_9cd55356b16046318996d8ac620fa5c3",
       "tabbable": null,
       "tooltip": null,
       "value": "Keyword surrogates: 100%"
      }
     },
     "fc9e97746dac4d0caa846980185076e0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_129bf11ca02c41cdbf3580783cd3361b",
       "placeholder": "​",
       "style": "IPY_MODEL_ff2ac376c0e042189316fcc791eef685",
       "tabbable": null,
       "tooltip": null,
       "value": " 1000/1000 [1:55:42&lt;00:00,  6.91s/it]"
      }
     },
     "ff2ac376c0e042189316fcc791eef685": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "ffbcde07b51843ebbdec8e82331512b9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
