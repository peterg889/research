{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45bd8ea3",
   "metadata": {},
   "source": [
    "# Exp 26: Attention Forcing for Long Documents\n",
    "\n",
    "## Motivation\n",
    "\n",
    "Experiments 11 and 20 proved that **value contamination fails on documents longer than 256 tokens**.\n",
    "The prefix signal is diluted because attention weights for the prefix become infinitesimally small\n",
    "as document length grows. Experiment 12 attempted to fix this by repeating the prefix 20 times,\n",
    "but it failed — the model's attention mechanism naturally learns to ignore redundant tokens.\n",
    "\n",
    "## Core Question\n",
    "\n",
    "If we bypass the model's natural attention decay and **mathematically force** document tokens to\n",
    "pay attention to the prefix during cache generation, can we recover the priming benefit?\n",
    "\n",
    "## Theoretical Mechanism\n",
    "\n",
    "In SDPA, the attention scores (pre-softmax) determine how much information flows from previous tokens\n",
    "into the current token's representation. Normally, we pass a boolean causal mask (0 for visible,\n",
    "-inf for masked). Instead, we pass a **float-based attention mask** with a positive logit bias\n",
    "(e.g., +5.0) exclusively at the intersection of document token queries and prefix token keys.\n",
    "\n",
    "This forces every document token to aggressively mix prefix semantics into its value vector,\n",
    "artificially counteracting long-document dilution.\n",
    "\n",
    "## Conditions (on 1024-token padded MS MARCO)\n",
    "\n",
    "| Condition | Description |\n",
    "|-----------|-------------|\n",
    "| `bare` | BOS + doc cache (control) |\n",
    "| `bias_0.0` | Standard priming, no bias (failure baseline, expected d ≈ 0) |\n",
    "| `bias_2.0` | +2.0 logit bias on doc→prefix attention |\n",
    "| `bias_5.0` | +5.0 logit bias on doc→prefix attention |\n",
    "| `bias_10.0` | +10.0 logit bias on doc→prefix attention |\n",
    "\n",
    "## Reference Values\n",
    "\n",
    "| Source | Condition | d |\n",
    "|--------|-----------|---|\n",
    "| Exp 20 (Mistral) | full priming @ original (~130 tok) | +0.303 |\n",
    "| Exp 20 (Mistral) | full priming @ 256 tok | +0.114 (ns) |\n",
    "| Exp 20 (Mistral) | full priming @ 1024 tok | -0.043 (ns) |\n",
    "\n",
    "## Success Criteria\n",
    "\n",
    "1. Does any bias level recover a positive Cohen's d (> +0.20) at 1024 tokens?\n",
    "2. At what bias does the document representation corrupt (NLL explodes)?\n",
    "3. What is the optimal bias on the tuning curve?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d473c33e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T13:56:53.594995Z",
     "iopub.status.busy": "2026-02-16T13:56:53.594704Z",
     "iopub.status.idle": "2026-02-16T13:56:56.956140Z",
     "shell.execute_reply": "2026-02-16T13:56:56.955023Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEED: 42\n",
      "Results directory: results/exp26\n",
      "CUDA available: True\n",
      "GPU: NVIDIA L4\n",
      "GPU memory: 23.6 GB\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Setup\n",
    "import os\n",
    "os.umask(0o000)\n",
    "\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import gc\n",
    "from pathlib import Path\n",
    "\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "RESULTS_DIR = Path(\"results/exp26\")\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CHECKPOINT_PATH = RESULTS_DIR / \"checkpoint.json\"\n",
    "FINAL_RESULTS_PATH = RESULTS_DIR / \"results.json\"\n",
    "CSV_PATH = RESULTS_DIR / \"results.csv\"\n",
    "\n",
    "print(f\"SEED: {SEED}\")\n",
    "print(f\"Results directory: {RESULTS_DIR}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ab23dd8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T13:56:56.960508Z",
     "iopub.status.busy": "2026-02-16T13:56:56.959589Z",
     "iopub.status.idle": "2026-02-16T13:58:01.451994Z",
     "shell.execute_reply": "2026-02-16T13:58:01.450859Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading mistralai/Mistral-7B-Instruct-v0.2 (4-bit, float16)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9a170f487ce4525855f3a9b55047d43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/291 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model loaded successfully.\n",
      "  Model class: MistralForCausalLM\n",
      "  Hidden size: 4096\n",
      "  Num layers: 32\n",
      "  Num attention heads: 32\n",
      "  Num KV heads: 8\n",
      "  Head dim: 128\n",
      "  Model dtype: torch.float16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Cache key dtype: torch.float16\n",
      "  Cache key shape: torch.Size([1, 8, 2, 128])  (batch, kv_heads, seq, head_dim)\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Load Mistral 7B via load_model()\n",
    "sys.path.insert(0, \".\")\n",
    "\n",
    "from lib.config import ExperimentConfig\n",
    "from lib.model_utils import load_model\n",
    "\n",
    "MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "exp_config = ExperimentConfig(\n",
    "    model_name=MODEL_NAME,\n",
    "    model_type=\"mistral\",\n",
    "    compute_dtype=\"auto\",  # resolves to float16 for Mistral\n",
    "    use_4bit=True,\n",
    "    num_samples=2000,\n",
    "    seed=SEED,\n",
    ")\n",
    "\n",
    "print(f\"Loading {MODEL_NAME} (4-bit, float16)...\")\n",
    "model, tokenizer = load_model(exp_config)\n",
    "\n",
    "from lib.kv_cache import _get_text_config, _get_head_dim, _ensure_dynamic_cache, _get_cache_keys\n",
    "\n",
    "text_config = _get_text_config(model.config)\n",
    "print(f\"\\nModel loaded successfully.\")\n",
    "print(f\"  Model class: {type(model).__name__}\")\n",
    "print(f\"  Hidden size: {text_config.hidden_size}\")\n",
    "print(f\"  Num layers: {text_config.num_hidden_layers}\")\n",
    "print(f\"  Num attention heads: {text_config.num_attention_heads}\")\n",
    "print(f\"  Num KV heads: {text_config.num_key_value_heads}\")\n",
    "print(f\"  Head dim: {_get_head_dim(model.config)}\")\n",
    "print(f\"  Model dtype: {model.dtype}\")\n",
    "\n",
    "# Verify dtype with a test forward pass\n",
    "sample_ids = tokenizer(\"test\", return_tensors=\"pt\")['input_ids'].to(exp_config.device)\n",
    "with torch.no_grad():\n",
    "    out = model(sample_ids, use_cache=True)\n",
    "    cache_check = _ensure_dynamic_cache(out.past_key_values)\n",
    "    k0 = _get_cache_keys(cache_check, 0)\n",
    "    print(f\"  Cache key dtype: {k0.dtype}\")\n",
    "    print(f\"  Cache key shape: {k0.shape}  (batch, kv_heads, seq, head_dim)\")\n",
    "del out, sample_ids, cache_check\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01bd6eb2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T13:58:01.457198Z",
     "iopub.status.busy": "2026-02-16T13:58:01.455931Z",
     "iopub.status.idle": "2026-02-16T13:58:01.468199Z",
     "shell.execute_reply": "2026-02-16T13:58:01.467121Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config ready\n",
      "  Model: mistralai/Mistral-7B-Instruct-v0.2\n",
      "  N_QUERIES: 500\n",
      "  PAD_TARGET: 1024 tokens\n",
      "  BIAS_VALUES: [0.0, 2.0, 5.0, 10.0]\n",
      "  Static fact prefix: 'What are the key facts I need to know?'\n",
      "\n",
      "Exp 20 reference (Mistral, standard priming):\n",
      "    original_d: +0.303\n",
      "    256_d: +0.114\n",
      "    1024_d: -0.043\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Lib imports + templates + constants\n",
    "from lib.kv_cache import (\n",
    "    _get_cache_keys,\n",
    "    _get_cache_values,\n",
    "    _set_cache_keys,\n",
    "    _set_cache_values,\n",
    "    _ensure_dynamic_cache,\n",
    "    extract_and_truncate_cache_with_bos,\n",
    "    correct_rope_positions_with_bos,\n",
    "    score_answer_with_cache,\n",
    "    deepcopy_cache,\n",
    ")\n",
    "from lib.analysis import cohens_d\n",
    "from lib.data import count_words\n",
    "from scipy import stats\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Templates -- bare text, no \"Document:\\n\" framing\n",
    "SURROGATE_PREFIX_TEMPLATE = \"{surrogate}\\n\"\n",
    "DOCUMENT_TEMPLATE = \"{document}\"\n",
    "QUERY_TEMPLATE = \"\\nQuery: {query}\\nAnswer:\"\n",
    "ANSWER_TEMPLATE = \" {answer}\"\n",
    "\n",
    "# Prefix text\n",
    "from lib.surrogate import STATIC_SURROGATE_QUERIES\n",
    "STATIC_FACT = STATIC_SURROGATE_QUERIES['static_factual']['query']\n",
    "\n",
    "# Experiment parameters\n",
    "N_QUERIES = 500\n",
    "MAX_PASSAGE_WORDS = 300\n",
    "PAD_TARGET = 1024  # pad all documents to this token length\n",
    "CHECKPOINT_EVERY = 25\n",
    "\n",
    "# Bias values to sweep (0.0 = standard priming, no bias)\n",
    "BIAS_VALUES = [0.0, 2.0, 5.0, 10.0]\n",
    "\n",
    "# Reference values from Exp 20 (Mistral)\n",
    "EXP20_REF = {\n",
    "    'original_d': 0.303,\n",
    "    '256_d': 0.114,\n",
    "    '1024_d': -0.043,\n",
    "}\n",
    "\n",
    "print(\"Config ready\")\n",
    "print(f\"  Model: {MODEL_NAME}\")\n",
    "print(f\"  N_QUERIES: {N_QUERIES}\")\n",
    "print(f\"  PAD_TARGET: {PAD_TARGET} tokens\")\n",
    "print(f\"  BIAS_VALUES: {BIAS_VALUES}\")\n",
    "print(f\"  Static fact prefix: '{STATIC_FACT}'\")\n",
    "print(f\"\\nExp 20 reference (Mistral, standard priming):\")\n",
    "for k, v in EXP20_REF.items():\n",
    "    print(f\"    {k}: {v:+.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba85cfde",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T13:58:01.471611Z",
     "iopub.status.busy": "2026-02-16T13:58:01.471327Z",
     "iopub.status.idle": "2026-02-16T13:59:00.629214Z",
     "shell.execute_reply": "2026-02-16T13:59:00.628297Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`trust_remote_code` is not supported anymore.\n",
      "Please check that the Hugging Face dataset 'microsoft/ms_marco' isn't based on a loading script and remove `trust_remote_code`.\n",
      "If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "LOADING MS MARCO v1.1 -- POSITIVE PASSAGES ONLY\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total items in validation: 10047\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e79a5a06f4bf454b8b96e34965968dcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filtering:   0%|          | 0/10047 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Selected 500 queries with positive passages\n",
      "Word counts: mean=72, min=15, max=157\n",
      "\n",
      "Padding pool passages: 80,814\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padding pool tokens: 8,628,937\n",
      "Max tokens needed: 512,000\n",
      "Pool is 16.9x the max needed. OK.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "92"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 4: Load MS MARCO v1.1, filter positive passages, build padding pool\n",
    "from datasets import load_dataset\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"LOADING MS MARCO v1.1 -- POSITIVE PASSAGES ONLY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "dataset = load_dataset(\"microsoft/ms_marco\", \"v1.1\", split=\"validation\",\n",
    "                        trust_remote_code=True)\n",
    "print(f\"Total items in validation: {len(dataset)}\")\n",
    "\n",
    "queries = []\n",
    "padding_passages = []\n",
    "eval_passage_set = set()\n",
    "np.random.seed(SEED)\n",
    "\n",
    "for item in tqdm(dataset, desc=\"Filtering\"):\n",
    "    passages_info = item.get('passages', {})\n",
    "    passage_texts = passages_info.get('passage_text', [])\n",
    "    is_selected = passages_info.get('is_selected', [])\n",
    "    query = item.get('query', '')\n",
    "    answers = item.get('answers', [])\n",
    "    well_formed = item.get('wellFormedAnswers', [])\n",
    "\n",
    "    if not passage_texts or not query:\n",
    "        continue\n",
    "\n",
    "    # Get best answer\n",
    "    answer = None\n",
    "    if well_formed and len(well_formed) > 0 and well_formed[0] != '[]':\n",
    "        answer = well_formed[0]\n",
    "    elif answers and len(answers) > 0 and answers[0] != 'No Answer Present.':\n",
    "        answer = answers[0]\n",
    "    else:\n",
    "        # Still collect non-eval passages for padding pool\n",
    "        for p in passage_texts:\n",
    "            if count_words(p) <= MAX_PASSAGE_WORDS:\n",
    "                padding_passages.append(p)\n",
    "        continue\n",
    "\n",
    "    # Find positive passage(s)\n",
    "    for i, (ptext, sel) in enumerate(zip(passage_texts, is_selected)):\n",
    "        if sel == 1 and count_words(ptext) <= MAX_PASSAGE_WORDS:\n",
    "            if len(queries) < N_QUERIES * 3:  # collect 3x for shuffling\n",
    "                queries.append({\n",
    "                    'query': query,\n",
    "                    'answer': answer,\n",
    "                    'passage': ptext,\n",
    "                    'word_count': count_words(ptext),\n",
    "                })\n",
    "                eval_passage_set.add(ptext)\n",
    "                break\n",
    "\n",
    "    # Collect non-eval passages for padding pool\n",
    "    for p in passage_texts:\n",
    "        if p not in eval_passage_set and count_words(p) <= MAX_PASSAGE_WORDS:\n",
    "            padding_passages.append(p)\n",
    "\n",
    "np.random.shuffle(queries)\n",
    "queries = queries[:N_QUERIES]\n",
    "N = len(queries)\n",
    "\n",
    "print(f\"\\nSelected {N} queries with positive passages\")\n",
    "print(f\"Word counts: mean={np.mean([q['word_count'] for q in queries]):.0f}, \"\n",
    "      f\"min={min(q['word_count'] for q in queries)}, \"\n",
    "      f\"max={max(q['word_count'] for q in queries)}\")\n",
    "\n",
    "# Build padding pool (pre-tokenize)\n",
    "print(f\"\\nPadding pool passages: {len(padding_passages):,}\")\n",
    "padding_text = ' '.join(padding_passages)\n",
    "padding_ids = tokenizer.encode(padding_text, add_special_tokens=False)\n",
    "print(f\"Padding pool tokens: {len(padding_ids):,}\")\n",
    "\n",
    "max_needed = PAD_TARGET * N_QUERIES\n",
    "print(f\"Max tokens needed: {max_needed:,}\")\n",
    "assert len(padding_ids) > max_needed, (\n",
    "    f\"Padding pool too small: {len(padding_ids):,} < {max_needed:,}\"\n",
    ")\n",
    "print(f\"Pool is {len(padding_ids) / max_needed:.1f}x the max needed. OK.\")\n",
    "\n",
    "del dataset\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76940160",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T13:59:00.633010Z",
     "iopub.status.busy": "2026-02-16T13:59:00.632734Z",
     "iopub.status.idle": "2026-02-16T13:59:00.645037Z",
     "shell.execute_reply": "2026-02-16T13:59:00.644280Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PREFIX TOKENIZATION\n",
      "======================================================================\n",
      "\n",
      "Static fact prefix: 'What are the key facts I need to know?'\n",
      "  Formatted: 'What are the key facts I need to know?'\n",
      "  Token length (no BOS): 11\n",
      "\n",
      "BPE BOUNDARY CHECK (first passage):\n",
      "  Token match: 74/75 (98.7%)\n",
      "\n",
      "======================================================================\n",
      "EXPERIMENTAL CONDITIONS\n",
      "======================================================================\n",
      "\n",
      "All conditions use 1024-token padded documents from MS MARCO.\n",
      "Prefix: static_fact_trunc ('What are the key facts I need to know?')\n",
      "Prefix tokens: 11 (plus BOS = 12 total prefix positions)\n",
      "Document tokens: 1024\n",
      "Total sequence for primed passes: 1 + 11 + 1024 = 1036\n",
      "\n",
      "### bare ###\n",
      "  Forward: [BOS][doc_1024]\n",
      "  Cache:   Standard causal attention, no prefix\n",
      "  Score:   Standard scoring against bare cache\n",
      "\n",
      "### bias_0.0 ###\n",
      "  Forward: [BOS][prefix_11][doc_1024]\n",
      "  Mask:    Standard causal (no bias)\n",
      "  Note:    This is standard priming -- the failure baseline at 1024 tokens\n",
      "  Post:    Truncate prefix -> RoPE correct -> score\n",
      "\n",
      "### bias_2.0 ###\n",
      "  Forward: [BOS][prefix_11][doc_1024]\n",
      "  Mask:    Causal + +2.0 logit boost on doc->prefix attention\n",
      "           Every doc token gets +2.0 added to its pre-softmax\n",
      "           attention scores for the 11 prefix positions\n",
      "  Post:    Truncate prefix -> RoPE correct -> score\n",
      "\n",
      "### bias_5.0 ###\n",
      "  Forward: [BOS][prefix_11][doc_1024]\n",
      "  Mask:    Causal + +5.0 logit boost on doc->prefix attention\n",
      "           Every doc token gets +5.0 added to its pre-softmax\n",
      "           attention scores for the 11 prefix positions\n",
      "  Post:    Truncate prefix -> RoPE correct -> score\n",
      "\n",
      "### bias_10.0 ###\n",
      "  Forward: [BOS][prefix_11][doc_1024]\n",
      "  Mask:    Causal + +10.0 logit boost on doc->prefix attention\n",
      "           Every doc token gets +10.0 added to its pre-softmax\n",
      "           attention scores for the 11 prefix positions\n",
      "  Post:    Truncate prefix -> RoPE correct -> score\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Tokenize prefix and explain experimental conditions\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PREFIX TOKENIZATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "sf_str = SURROGATE_PREFIX_TEMPLATE.format(surrogate=STATIC_FACT)\n",
    "\n",
    "sf_ids = tokenizer(sf_str, return_tensors=\"pt\",\n",
    "                    add_special_tokens=False)['input_ids'].to(exp_config.device)\n",
    "PREFIX_TOKEN_LEN = sf_ids.shape[1]\n",
    "\n",
    "print(f\"\\nStatic fact prefix: '{STATIC_FACT}'\")\n",
    "print(f\"  Formatted: '{sf_str.strip()}'\")\n",
    "print(f\"  Token length (no BOS): {PREFIX_TOKEN_LEN}\")\n",
    "\n",
    "# Verify BPE boundary consistency\n",
    "print(\"\\nBPE BOUNDARY CHECK (first passage):\")\n",
    "example_doc = queries[0]['passage']\n",
    "concat = sf_str + DOCUMENT_TEMPLATE.format(document=example_doc)\n",
    "concat_enc = tokenizer(concat, add_special_tokens=True)['input_ids']\n",
    "prefix_enc = tokenizer(sf_str, add_special_tokens=True)['input_ids']\n",
    "doc_ids_from_concat = concat_enc[len(prefix_enc):]\n",
    "\n",
    "bare_doc_enc = tokenizer(DOCUMENT_TEMPLATE.format(document=example_doc),\n",
    "                          add_special_tokens=False)['input_ids']\n",
    "match = sum(1 for a, b in zip(doc_ids_from_concat, bare_doc_enc) if a == b)\n",
    "total = max(len(bare_doc_enc), 1)\n",
    "print(f\"  Token match: {match}/{total} ({100*match/total:.1f}%)\")\n",
    "\n",
    "# Condition explanations\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"EXPERIMENTAL CONDITIONS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nAll conditions use 1024-token padded documents from MS MARCO.\")\n",
    "print(\"Prefix: static_fact_trunc ('What are the key facts I need to know?')\")\n",
    "print(f\"Prefix tokens: {PREFIX_TOKEN_LEN} (plus BOS = {PREFIX_TOKEN_LEN + 1} total prefix positions)\")\n",
    "print(f\"Document tokens: {PAD_TARGET}\")\n",
    "print(f\"Total sequence for primed passes: 1 + {PREFIX_TOKEN_LEN} + {PAD_TARGET} = {1 + PREFIX_TOKEN_LEN + PAD_TARGET}\")\n",
    "\n",
    "print(\"\\n### bare ###\")\n",
    "print(\"  Forward: [BOS][doc_1024]\")\n",
    "print(\"  Cache:   Standard causal attention, no prefix\")\n",
    "print(\"  Score:   Standard scoring against bare cache\")\n",
    "\n",
    "for bias in BIAS_VALUES:\n",
    "    label = f\"bias_{bias:.1f}\"\n",
    "    print(f\"\\n### {label} ###\")\n",
    "    print(f\"  Forward: [BOS][prefix_{PREFIX_TOKEN_LEN}][doc_{PAD_TARGET}]\")\n",
    "    if bias == 0.0:\n",
    "        print(\"  Mask:    Standard causal (no bias)\")\n",
    "        print(\"  Note:    This is standard priming -- the failure baseline at 1024 tokens\")\n",
    "    else:\n",
    "        print(f\"  Mask:    Causal + {bias:+.1f} logit boost on doc->prefix attention\")\n",
    "        print(f\"           Every doc token gets +{bias:.1f} added to its pre-softmax\")\n",
    "        print(f\"           attention scores for the {PREFIX_TOKEN_LEN} prefix positions\")\n",
    "    print(\"  Post:    Truncate prefix -> RoPE correct -> score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2f19248",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T13:59:00.648313Z",
     "iopub.status.busy": "2026-02-16T13:59:00.648027Z",
     "iopub.status.idle": "2026-02-16T13:59:00.668143Z",
     "shell.execute_reply": "2026-02-16T13:59:00.667280Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "MASK VERIFICATION (toy example)\n",
      "======================================================================\n",
      "\n",
      "Mask shape: torch.Size([1, 1, 9, 9])\n",
      "Dtype: torch.float16\n",
      "\n",
      "Positions: [BOS=0, P1=1, P2=2, P3=3, D1=4, D2=5, D3=6, D4=7, D5=8]\n",
      "\n",
      "Mask values (rows=queries, cols=keys):\n",
      "         BOS   P1    P2    P3    D1    D2    D3    D4    D5\n",
      "  BOS:   0.0   -inf   -inf   -inf   -inf   -inf   -inf   -inf   -inf\n",
      "  P1 :   0.0    0.0   -inf   -inf   -inf   -inf   -inf   -inf   -inf\n",
      "  P2 :   0.0    0.0    0.0   -inf   -inf   -inf   -inf   -inf   -inf\n",
      "  P3 :   0.0    0.0    0.0    0.0   -inf   -inf   -inf   -inf   -inf\n",
      "  D1 :   0.0   +5.0   +5.0   +5.0    0.0   -inf   -inf   -inf   -inf\n",
      "  D2 :   0.0   +5.0   +5.0   +5.0    0.0    0.0   -inf   -inf   -inf\n",
      "  D3 :   0.0   +5.0   +5.0   +5.0    0.0    0.0    0.0   -inf   -inf\n",
      "  D4 :   0.0   +5.0   +5.0   +5.0    0.0    0.0    0.0    0.0   -inf\n",
      "  D5 :   0.0   +5.0   +5.0   +5.0    0.0    0.0    0.0    0.0    0.0\n",
      "\n",
      "Key observations:\n",
      "  - BOS (row 0) can only attend to itself (0.0)\n",
      "  - Prefix tokens (rows 1-3) attend causally to BOS + prior prefix (0.0)\n",
      "  - Doc tokens (rows 4-8) attend to prefix with +5.0 bias\n",
      "  - Doc tokens attend to BOS and other doc tokens normally (0.0)\n",
      "  - Upper triangle is -inf (causal masking)\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Helper function for building biased attention masks\n",
    "\n",
    "def build_biased_causal_mask(total_len, prefix_start, prefix_end, bias_value, dtype, device):\n",
    "    \"\"\"Build a 4D causal attention mask with logit bias on doc->prefix attention.\n",
    "\n",
    "    Creates a standard causal mask (lower-triangular = 0, upper-triangular = -inf),\n",
    "    then adds a positive bias to the attention scores at the intersection of\n",
    "    document token queries (rows) and prefix token keys (columns).\n",
    "\n",
    "    Args:\n",
    "        total_len: Total sequence length [BOS + prefix + doc]\n",
    "        prefix_start: Start index of prefix tokens (typically 1, after BOS)\n",
    "        prefix_end: End index of prefix tokens (exclusive)\n",
    "        bias_value: Positive float to add to doc->prefix attention scores.\n",
    "            0.0 = standard causal mask (no bias).\n",
    "        dtype: Model dtype (e.g., torch.float16)\n",
    "        device: Model device\n",
    "\n",
    "    Returns:\n",
    "        Tensor of shape (1, 1, total_len, total_len)\n",
    "    \"\"\"\n",
    "    # Standard causal mask: 0 for attend, -inf for future positions\n",
    "    mask = torch.zeros((total_len, total_len), dtype=dtype, device=device)\n",
    "    causal = torch.triu(\n",
    "        torch.ones(total_len, total_len, dtype=torch.bool, device=device),\n",
    "        diagonal=1\n",
    "    )\n",
    "    mask.masked_fill_(causal, float('-inf'))\n",
    "\n",
    "    # Apply positive bias to doc->prefix attention\n",
    "    # Doc tokens start at prefix_end, prefix tokens at [prefix_start, prefix_end)\n",
    "    if bias_value != 0.0:\n",
    "        doc_start = prefix_end\n",
    "        mask[doc_start:, prefix_start:prefix_end] += bias_value\n",
    "\n",
    "    return mask.unsqueeze(0).unsqueeze(0)  # (1, 1, total_len, total_len)\n",
    "\n",
    "\n",
    "# Verify mask shape and values for a toy example\n",
    "print(\"=\" * 70)\n",
    "print(\"MASK VERIFICATION (toy example)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Toy: BOS + 3 prefix tokens + 5 doc tokens = 9 total\n",
    "toy_mask = build_biased_causal_mask(\n",
    "    total_len=9, prefix_start=1, prefix_end=4,\n",
    "    bias_value=5.0, dtype=model.dtype, device='cpu'\n",
    ")\n",
    "m = toy_mask.squeeze()  # (9, 9)\n",
    "\n",
    "print(f\"\\nMask shape: {toy_mask.shape}\")\n",
    "print(f\"Dtype: {toy_mask.dtype}\")\n",
    "print(\"\\nPositions: [BOS=0, P1=1, P2=2, P3=3, D1=4, D2=5, D3=6, D4=7, D5=8]\")\n",
    "print(\"\\nMask values (rows=queries, cols=keys):\")\n",
    "print(\"         BOS   P1    P2    P3    D1    D2    D3    D4    D5\")\n",
    "labels = ['BOS', 'P1 ', 'P2 ', 'P3 ', 'D1 ', 'D2 ', 'D3 ', 'D4 ', 'D5 ']\n",
    "for i, label in enumerate(labels):\n",
    "    row_vals = []\n",
    "    for j in range(9):\n",
    "        v = m[i, j].item()\n",
    "        if v == float('-inf'):\n",
    "            row_vals.append(' -inf')\n",
    "        elif v == 0.0:\n",
    "            row_vals.append('  0.0')\n",
    "        else:\n",
    "            row_vals.append(f'{v:+5.1f}')\n",
    "    print(f\"  {label}: {'  '.join(row_vals)}\")\n",
    "\n",
    "print(\"\\nKey observations:\")\n",
    "print(\"  - BOS (row 0) can only attend to itself (0.0)\")\n",
    "print(\"  - Prefix tokens (rows 1-3) attend causally to BOS + prior prefix (0.0)\")\n",
    "print(\"  - Doc tokens (rows 4-8) attend to prefix with +5.0 bias\")\n",
    "print(\"  - Doc tokens attend to BOS and other doc tokens normally (0.0)\")\n",
    "print(\"  - Upper triangle is -inf (causal masking)\")\n",
    "\n",
    "del toy_mask, m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02d90d4a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T13:59:00.671536Z",
     "iopub.status.busy": "2026-02-16T13:59:00.671260Z",
     "iopub.status.idle": "2026-02-16T14:32:59.392610Z",
     "shell.execute_reply": "2026-02-16T14:32:59.391723Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "EXPERIMENT: 500 queries, 4 bias levels + bare\n",
      "Document length: 1024 tokens (padded)\n",
      "======================================================================\n",
      "No checkpoint found. Starting fresh.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffc5d72a230f428aad1259046e83dae2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Exp 26:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 25/500 | 25 done in 1.6m | ETA: 30.7 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 50/500 | 50 done in 3.3m | ETA: 29.4 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 75/500 | 75 done in 5.0m | ETA: 28.1 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 100/500 | 100 done in 6.7m | ETA: 26.6 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 125/500 | 125 done in 8.4m | ETA: 25.1 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 150/500 | 150 done in 10.1m | ETA: 23.5 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 175/500 | 175 done in 11.8m | ETA: 21.9 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 200/500 | 200 done in 13.5m | ETA: 20.3 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 225/500 | 225 done in 15.2m | ETA: 18.6 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 250/500 | 250 done in 16.9m | ETA: 16.9 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 275/500 | 275 done in 18.6m | ETA: 15.2 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 300/500 | 300 done in 20.3m | ETA: 13.6 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 325/500 | 325 done in 22.0m | ETA: 11.9 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 350/500 | 350 done in 23.8m | ETA: 10.2 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 375/500 | 375 done in 25.5m | ETA: 8.5 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 400/500 | 400 done in 27.2m | ETA: 6.8 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 425/500 | 425 done in 28.9m | ETA: 5.1 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 450/500 | 450 done in 30.6m | ETA: 3.4 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 475/500 | 475 done in 32.3m | ETA: 1.7 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 500/500 | 500 done in 34.0m | ETA: 0.0 min\n",
      "\n",
      "Experiment complete: 500 queries in 34.0 min\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Main experiment loop\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"EXPERIMENT: {N_QUERIES} queries, {len(BIAS_VALUES)} bias levels + bare\")\n",
    "print(f\"Document length: {PAD_TARGET} tokens (padded)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Checkpoint resume\n",
    "all_results = []\n",
    "start_idx = 0\n",
    "\n",
    "if CHECKPOINT_PATH.exists():\n",
    "    with open(CHECKPOINT_PATH, 'r') as f:\n",
    "        ckpt = json.load(f)\n",
    "    ckpt_queries = ckpt.get('query_texts', [])\n",
    "    current_queries = [q['query'] for q in queries[:N_QUERIES]]\n",
    "    if ckpt_queries == current_queries:\n",
    "        all_results = ckpt['results']\n",
    "        start_idx = len(all_results)\n",
    "        print(f\"Resuming from checkpoint: {start_idx}/{N_QUERIES}\")\n",
    "    else:\n",
    "        print(\"Checkpoint query mismatch. Starting fresh.\")\n",
    "else:\n",
    "    print(\"No checkpoint found. Starting fresh.\")\n",
    "\n",
    "t_start = time.time()\n",
    "\n",
    "for qidx in tqdm(range(start_idx, N_QUERIES), initial=start_idx, total=N_QUERIES,\n",
    "                  desc=\"Exp 26\"):\n",
    "    qdata = queries[qidx]\n",
    "    query_prompt = QUERY_TEMPLATE.format(query=qdata['query'])\n",
    "    answer_text = ANSWER_TEMPLATE.format(answer=qdata['answer'])\n",
    "    passage = qdata['passage']\n",
    "    document_text = DOCUMENT_TEMPLATE.format(document=passage)\n",
    "\n",
    "    # === Matched tokenization ===\n",
    "    # Tokenize concatenated prefix+doc to get matched BPE boundaries\n",
    "    full_text = sf_str + document_text\n",
    "    full_enc = tokenizer(full_text, return_tensors=\"pt\",\n",
    "                          add_special_tokens=True, padding=False, truncation=False)\n",
    "    full_ids = full_enc['input_ids'].to(exp_config.device)\n",
    "\n",
    "    sf_prefix_enc = tokenizer(sf_str, return_tensors=\"pt\",\n",
    "                               add_special_tokens=True, padding=False, truncation=False)\n",
    "    sf_prefix_len_with_bos = sf_prefix_enc['input_ids'].shape[1]\n",
    "\n",
    "    bos_id = full_ids[:, :1]\n",
    "    base_doc_ids = full_ids[:, sf_prefix_len_with_bos:]\n",
    "    base_doc_len = base_doc_ids.shape[1]\n",
    "\n",
    "    del full_enc, full_ids, sf_prefix_enc\n",
    "\n",
    "    # === Pad doc to PAD_TARGET tokens ===\n",
    "    if base_doc_len < PAD_TARGET:\n",
    "        pad_needed = PAD_TARGET - base_doc_len\n",
    "        max_start = len(padding_ids) - pad_needed\n",
    "        start = np.random.randint(0, max_start)\n",
    "        pad_tensor = torch.tensor([padding_ids[start:start + pad_needed]],\n",
    "                                   device=exp_config.device)\n",
    "        doc_ids = torch.cat([base_doc_ids, pad_tensor], dim=1)\n",
    "    else:\n",
    "        doc_ids = base_doc_ids[:, :PAD_TARGET]\n",
    "\n",
    "    doc_len = doc_ids.shape[1]\n",
    "    context_len = 1 + doc_len  # BOS + doc\n",
    "\n",
    "    # === Forward pass: BARE ===\n",
    "    bare_input = torch.cat([bos_id, doc_ids], dim=1)\n",
    "    with torch.no_grad():\n",
    "        bare_out = model(input_ids=bare_input,\n",
    "                         attention_mask=torch.ones_like(bare_input),\n",
    "                         use_cache=True, return_dict=True)\n",
    "    bare_cache = _ensure_dynamic_cache(bare_out.past_key_values)\n",
    "    del bare_out\n",
    "\n",
    "    # Score bare\n",
    "    bare_nll = score_answer_with_cache(\n",
    "        deepcopy_cache(bare_cache), context_len,\n",
    "        query_prompt, answer_text, model, tokenizer, exp_config)\n",
    "\n",
    "    # === Forward passes: BIASED (one per bias level) ===\n",
    "    primed_input = torch.cat([bos_id, sf_ids, doc_ids], dim=1)\n",
    "    total_seq_len = primed_input.shape[1]\n",
    "    prefix_start = 1  # prefix starts after BOS\n",
    "    prefix_end = 1 + sf_ids.shape[1]  # prefix ends before doc\n",
    "    prefix_offset = sf_ids.shape[1]  # for RoPE correction\n",
    "\n",
    "    query_rows = []\n",
    "\n",
    "    for bias_value in BIAS_VALUES:\n",
    "        # Build 4D attention mask with bias\n",
    "        mask_4d = build_biased_causal_mask(\n",
    "            total_seq_len, prefix_start, prefix_end,\n",
    "            bias_value, model.dtype, exp_config.device)\n",
    "\n",
    "        # Forward pass with custom mask\n",
    "        with torch.no_grad():\n",
    "            primed_out = model(input_ids=primed_input,\n",
    "                               attention_mask=mask_4d,\n",
    "                               use_cache=True, return_dict=True)\n",
    "        primed_full = _ensure_dynamic_cache(primed_out.past_key_values)\n",
    "        del primed_out, mask_4d\n",
    "\n",
    "        # Truncate: keep [BOS] + [last doc_len positions]\n",
    "        trunc_raw = extract_and_truncate_cache_with_bos(primed_full, doc_len)\n",
    "        del primed_full\n",
    "\n",
    "        # RoPE correct\n",
    "        sf_trunc_cache = deepcopy_cache(trunc_raw)\n",
    "        correct_rope_positions_with_bos(sf_trunc_cache, prefix_offset, model)\n",
    "        del trunc_raw\n",
    "\n",
    "        # Score\n",
    "        biased_nll = score_answer_with_cache(\n",
    "            deepcopy_cache(sf_trunc_cache), context_len,\n",
    "            query_prompt, answer_text, model, tokenizer, exp_config)\n",
    "        del sf_trunc_cache\n",
    "\n",
    "        query_rows.append({\n",
    "            'query_idx': qidx,\n",
    "            'bias_value': bias_value,\n",
    "            'bias_label': f\"bias_{bias_value:.1f}\",\n",
    "            'actual_doc_len': doc_len,\n",
    "            'bare_nll': bare_nll,\n",
    "            'primed_nll': biased_nll,\n",
    "            'delta_nll': bare_nll - biased_nll,\n",
    "        })\n",
    "\n",
    "    del bare_cache, bare_input, primed_input\n",
    "    if base_doc_len < PAD_TARGET:\n",
    "        del pad_tensor\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    all_results.append({\n",
    "        'query_idx': qidx,\n",
    "        'query': qdata['query'],\n",
    "        'base_doc_len': base_doc_len,\n",
    "        'padded_doc_len': doc_len,\n",
    "        'bare_nll': bare_nll,\n",
    "        'rows': query_rows,\n",
    "    })\n",
    "\n",
    "    # Checkpoint\n",
    "    if (qidx + 1) % CHECKPOINT_EVERY == 0 or qidx == N_QUERIES - 1:\n",
    "        ckpt_data = {\n",
    "            'results': all_results,\n",
    "            'query_texts': [q['query'] for q in queries[:N_QUERIES]],\n",
    "            'completed': len(all_results),\n",
    "            'total': N_QUERIES,\n",
    "            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        }\n",
    "        with open(CHECKPOINT_PATH, 'w') as f:\n",
    "            json.dump(ckpt_data, f)\n",
    "        elapsed = time.time() - t_start\n",
    "        n_done = qidx - start_idx + 1\n",
    "        rate = n_done / elapsed if elapsed > 0 else 0\n",
    "        remaining = (N_QUERIES - qidx - 1) / rate if rate > 0 else 0\n",
    "        tqdm.write(f\"  Checkpoint {qidx+1}/{N_QUERIES} | {n_done} done in {elapsed/60:.1f}m | \"\n",
    "                   f\"ETA: {remaining/60:.1f} min\")\n",
    "\n",
    "elapsed_total = time.time() - t_start\n",
    "print(f\"\\nExperiment complete: {len(all_results)} queries in {elapsed_total/60:.1f} min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5641802",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T14:32:59.396565Z",
     "iopub.status.busy": "2026-02-16T14:32:59.396270Z",
     "iopub.status.idle": "2026-02-16T14:32:59.501219Z",
     "shell.execute_reply": "2026-02-16T14:32:59.500335Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ANALYSIS: ATTENTION FORCING RESULTS\n",
      "======================================================================\n",
      "\n",
      "Condition          N  Mean Bare  Mean Primed     Mean D        d    Win%            p   sig\n",
      "-----------------------------------------------------------------------------------------------\n",
      "bias_0.0         458     0.8888       0.8662    +0.0226   +0.144   64.0%     2.15e-03    **\n",
      "bias_2.0         458     0.8888       0.7747    +0.1140   +0.291   73.6%     1.02e-09   ***\n",
      "bias_5.0         458     0.8888       0.9842    -0.0954   -0.076   31.7%     1.06e-01    ns\n",
      "bias_10.0        458     0.8888       2.1598    -1.2710   -0.780   11.8%     3.32e-49   ***\n",
      "\n",
      "======================================================================\n",
      "PERPLEXITY CHECK\n",
      "======================================================================\n",
      "\n",
      "Does attention forcing corrupt the document representation?\n",
      "If mean primed NLL is much higher than mean bare NLL, the bias is too aggressive.\n",
      "\n",
      "  bias_0.0: bare=0.8888, primed=0.8662, ratio=0.97x  [OK]\n",
      "  bias_2.0: bare=0.8888, primed=0.7747, ratio=0.87x  [OK]\n",
      "  bias_5.0: bare=0.8888, primed=0.9842, ratio=1.11x  [OK]\n",
      "  bias_10.0: bare=0.8888, primed=2.1598, ratio=2.43x  [WARNING]\n",
      "\n",
      "======================================================================\n",
      "COMPARISON WITH EXP 20 (standard priming)\n",
      "======================================================================\n",
      "\n",
      "Exp 20 standard priming @ 1024 tok: d=-0.043\n",
      "This exp bias_0.0 (standard priming):  d=+0.144\n",
      "\n",
      "Best bias: bias_2.0 (d=+0.291)\n",
      "\n",
      "VERDICT: SUCCESS: Attention forcing recovers a meaningful effect (d=+0.291) at 1024 tokens with bias=2.0\n",
      "\n",
      "======================================================================\n",
      "HARDNESS INTERACTION (quintiles by bare NLL)\n",
      "======================================================================\n",
      "\n",
      "Condition          Q1 (easy)            Q2            Q3            Q4     Q5 (hard)       Overall\n",
      "--------------------------------------------------------------------------------------------------\n",
      "bias_0.0              -0.053        +0.210        +0.286        +0.228        +0.158        +0.138\n",
      "bias_2.0              -0.092        +0.195        +0.520        +0.549        +0.439        +0.278\n",
      "bias_5.0              -0.488        -0.889        -1.063        -0.411        +0.612        -0.072\n",
      "bias_10.0             -0.744        -2.251        -2.189        -1.592        +0.076        -0.728\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Analysis -- Cohen's d, statistical tests, perplexity check\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ANALYSIS: ATTENTION FORCING RESULTS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Collect per-sample deltas by bias level\n",
    "bias_deltas = {}\n",
    "bias_bare = {}\n",
    "bias_primed = {}\n",
    "for bv in BIAS_VALUES:\n",
    "    label = f\"bias_{bv:.1f}\"\n",
    "    bias_deltas[label] = []\n",
    "    bias_bare[label] = []\n",
    "    bias_primed[label] = []\n",
    "\n",
    "for r in all_results:\n",
    "    for row in r['rows']:\n",
    "        label = row['bias_label']\n",
    "        if label in bias_deltas:\n",
    "            bias_deltas[label].append(row['delta_nll'])\n",
    "            bias_bare[label].append(row['bare_nll'])\n",
    "            bias_primed[label].append(row['primed_nll'])\n",
    "\n",
    "# Convert to arrays and filter invalid values\n",
    "bias_arrays = {}\n",
    "for label in bias_deltas:\n",
    "    bare = np.array(bias_bare[label])\n",
    "    primed = np.array(bias_primed[label])\n",
    "    delta = np.array(bias_deltas[label])\n",
    "    valid = (bare != 0) & (primed != 0) & np.isfinite(bare) & np.isfinite(primed)\n",
    "    bias_arrays[label] = {\n",
    "        'bare': bare[valid],\n",
    "        'primed': primed[valid],\n",
    "        'delta': delta[valid],\n",
    "        'n_valid': int(np.sum(valid)),\n",
    "    }\n",
    "\n",
    "# Summary table\n",
    "print(f\"\\n{'Condition':<14} {'N':>5} {'Mean Bare':>10} {'Mean Primed':>12} \"\n",
    "      f\"{'Mean D':>10} {'d':>8} {'Win%':>7} {'p':>12} {'sig':>5}\")\n",
    "print(\"-\" * 95)\n",
    "\n",
    "analysis = {}\n",
    "for bv in BIAS_VALUES:\n",
    "    label = f\"bias_{bv:.1f}\"\n",
    "    a = bias_arrays[label]\n",
    "    d = cohens_d(a['delta'])\n",
    "    win = np.mean(a['delta'] > 0) * 100\n",
    "    t_stat, p_val = stats.ttest_1samp(a['delta'], 0)\n",
    "    sig = '***' if p_val < 0.001 else '**' if p_val < 0.01 else '*' if p_val < 0.05 else 'ns'\n",
    "    print(f\"{label:<14} {a['n_valid']:>5} {np.mean(a['bare']):>10.4f} \"\n",
    "          f\"{np.mean(a['primed']):>12.4f} {np.mean(a['delta']):>+10.4f} \"\n",
    "          f\"{d:>+8.3f} {win:>6.1f}% {p_val:>12.2e} {sig:>5}\")\n",
    "    analysis[label] = {\n",
    "        'bias_value': bv,\n",
    "        'n_valid': a['n_valid'],\n",
    "        'mean_bare': float(np.mean(a['bare'])),\n",
    "        'mean_primed': float(np.mean(a['primed'])),\n",
    "        'mean_delta': float(np.mean(a['delta'])),\n",
    "        'std_delta': float(np.std(a['delta'])),\n",
    "        'cohens_d': float(d),\n",
    "        'win_pct': float(win),\n",
    "        't_stat': float(t_stat),\n",
    "        'p_value': float(p_val),\n",
    "    }\n",
    "\n",
    "# Perplexity check\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"PERPLEXITY CHECK\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nDoes attention forcing corrupt the document representation?\")\n",
    "print(\"If mean primed NLL is much higher than mean bare NLL, the bias is too aggressive.\")\n",
    "print()\n",
    "\n",
    "bare_mean = np.mean(bias_arrays['bias_0.0']['bare'])\n",
    "for bv in BIAS_VALUES:\n",
    "    label = f\"bias_{bv:.1f}\"\n",
    "    a = bias_arrays[label]\n",
    "    primed_mean = np.mean(a['primed'])\n",
    "    ratio = primed_mean / bare_mean if bare_mean > 0 else float('inf')\n",
    "    corruption = \"OK\" if ratio < 1.5 else \"WARNING\" if ratio < 3.0 else \"CORRUPTED\"\n",
    "    print(f\"  {label}: bare={bare_mean:.4f}, primed={primed_mean:.4f}, \"\n",
    "          f\"ratio={ratio:.2f}x  [{corruption}]\")\n",
    "\n",
    "# Exp 20 comparison\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"COMPARISON WITH EXP 20 (standard priming)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "d_standard = analysis['bias_0.0']['cohens_d']\n",
    "print(f\"\\nExp 20 standard priming @ 1024 tok: d={EXP20_REF['1024_d']:+.3f}\")\n",
    "print(f\"This exp bias_0.0 (standard priming):  d={d_standard:+.3f}\")\n",
    "\n",
    "best_label = max(analysis, key=lambda k: analysis[k]['cohens_d'])\n",
    "best_d = analysis[best_label]['cohens_d']\n",
    "best_bias = analysis[best_label]['bias_value']\n",
    "\n",
    "print(f\"\\nBest bias: {best_label} (d={best_d:+.3f})\")\n",
    "if best_d > 0.20:\n",
    "    verdict = (f\"SUCCESS: Attention forcing recovers a meaningful effect (d={best_d:+.3f}) \"\n",
    "               f\"at 1024 tokens with bias={best_bias:.1f}\")\n",
    "elif best_d > 0.05:\n",
    "    verdict = (f\"PARTIAL: Some recovery (d={best_d:+.3f}) but below the +0.20 target. \"\n",
    "               f\"Best bias={best_bias:.1f}\")\n",
    "elif best_d > d_standard + 0.05:\n",
    "    verdict = (f\"MARGINAL: Bias helps vs standard (d={best_d:+.3f} vs {d_standard:+.3f}) \"\n",
    "               f\"but effect is small\")\n",
    "else:\n",
    "    verdict = (f\"FAILURE: Attention forcing does not recover priming at 1024 tokens. \"\n",
    "               f\"Best d={best_d:+.3f}, standard d={d_standard:+.3f}\")\n",
    "\n",
    "print(f\"\\nVERDICT: {verdict}\")\n",
    "\n",
    "# Hardness interaction\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"HARDNESS INTERACTION (quintiles by bare NLL)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "all_bare_nlls = np.array([r['bare_nll'] for r in all_results])\n",
    "quintile_boundaries = np.percentile(all_bare_nlls, [20, 40, 60, 80])\n",
    "quintile_labels = ['Q1 (easy)', 'Q2', 'Q3', 'Q4', 'Q5 (hard)']\n",
    "\n",
    "def get_quintile(nll, boundaries):\n",
    "    for i, b in enumerate(boundaries):\n",
    "        if nll <= b:\n",
    "            return i\n",
    "    return len(boundaries)\n",
    "\n",
    "quintiles = np.array([get_quintile(nll, quintile_boundaries) for nll in all_bare_nlls])\n",
    "\n",
    "header = f\"{'Condition':<14}\" + \"\".join(f\"{ql:>14}\" for ql in quintile_labels) + f\"{'Overall':>14}\"\n",
    "print(f\"\\n{header}\")\n",
    "print(\"-\" * (14 + 14 * 6))\n",
    "\n",
    "hardness_data = {}\n",
    "for bv in BIAS_VALUES:\n",
    "    label = f\"bias_{bv:.1f}\"\n",
    "    delta = np.array(bias_deltas[label])\n",
    "    row_str = f\"{label:<14}\"\n",
    "    quintile_ds = []\n",
    "    for q in range(5):\n",
    "        mask_q = quintiles == q\n",
    "        if np.sum(mask_q) < 5:\n",
    "            row_str += f\"{'n/a':>14}\"\n",
    "            quintile_ds.append(None)\n",
    "        else:\n",
    "            d_q = cohens_d(delta[mask_q])\n",
    "            row_str += f\"{d_q:>+14.3f}\"\n",
    "            quintile_ds.append(float(d_q))\n",
    "    d_all = cohens_d(delta)\n",
    "    row_str += f\"{d_all:>+14.3f}\"\n",
    "    print(row_str)\n",
    "    hardness_data[label] = quintile_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "861e1fd2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T14:32:59.505187Z",
     "iopub.status.busy": "2026-02-16T14:32:59.504655Z",
     "iopub.status.idle": "2026-02-16T14:33:02.220368Z",
     "shell.execute_reply": "2026-02-16T14:33:02.219228Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plots saved to results/exp26/analysis_plots.png\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Plots -- 4-panel figure\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# ---- Panel 1 (top-left): Bias Tuning Curve (d vs bias) ----\n",
    "ax = axes[0, 0]\n",
    "\n",
    "bias_vals_plot = BIAS_VALUES\n",
    "d_vals = [analysis[f\"bias_{bv:.1f}\"]['cohens_d'] for bv in bias_vals_plot]\n",
    "\n",
    "# Bootstrap 95% CI\n",
    "np.random.seed(SEED)\n",
    "ci_lo, ci_hi = [], []\n",
    "for bv in bias_vals_plot:\n",
    "    label = f\"bias_{bv:.1f}\"\n",
    "    delta = bias_arrays[label]['delta']\n",
    "    boot_ds = []\n",
    "    for _ in range(2000):\n",
    "        idx_boot = np.random.randint(0, len(delta), size=len(delta))\n",
    "        boot_ds.append(cohens_d(delta[idx_boot]))\n",
    "    boot_ds = np.array(boot_ds)\n",
    "    ci_lo.append(np.percentile(boot_ds, 2.5))\n",
    "    ci_hi.append(np.percentile(boot_ds, 97.5))\n",
    "ci_lo = np.array(ci_lo)\n",
    "ci_hi = np.array(ci_hi)\n",
    "\n",
    "ax.errorbar(bias_vals_plot, d_vals,\n",
    "            yerr=[np.array(d_vals) - ci_lo, ci_hi - np.array(d_vals)],\n",
    "            marker='o', markersize=8, linewidth=2, capsize=5,\n",
    "            color='#1f77b4', ecolor='#aec7e8')\n",
    "\n",
    "# Reference lines\n",
    "ax.axhline(y=0, color='gray', linestyle='-', alpha=0.3)\n",
    "ax.axhline(y=EXP20_REF['1024_d'], color='#d62728', linestyle='--', linewidth=1.5,\n",
    "           label=f\"Exp 20 standard @ 1024tok (d={EXP20_REF['1024_d']:+.3f})\")\n",
    "ax.axhline(y=EXP20_REF['original_d'], color='#2ca02c', linestyle=':', linewidth=1.5,\n",
    "           label=f\"Exp 20 standard @ original (d={EXP20_REF['original_d']:+.3f})\")\n",
    "\n",
    "for i, bv in enumerate(bias_vals_plot):\n",
    "    label = f\"bias_{bv:.1f}\"\n",
    "    p_val = analysis[label]['p_value']\n",
    "    sig = '***' if p_val < 0.001 else '**' if p_val < 0.01 else '*' if p_val < 0.05 else 'ns'\n",
    "    ax.annotate(f'd={d_vals[i]:+.3f} {sig}',\n",
    "                (bv, d_vals[i]),\n",
    "                textcoords='offset points', xytext=(0, 18),\n",
    "                ha='center', fontsize=8)\n",
    "\n",
    "ax.set_xlabel('Attention Bias Value')\n",
    "ax.set_ylabel(\"Cohen's d (positive = helps)\")\n",
    "ax.set_title(\"Bias Tuning Curve: d vs Attention Bias\")\n",
    "ax.legend(fontsize=7)\n",
    "\n",
    "# ---- Panel 2 (top-right): Mean NLL by condition (perplexity check) ----\n",
    "ax = axes[0, 1]\n",
    "\n",
    "cond_labels = ['bare'] + [f\"bias_{bv:.1f}\" for bv in BIAS_VALUES]\n",
    "bare_mean_nll = float(np.mean(all_bare_nlls))\n",
    "mean_nlls = [bare_mean_nll] + [analysis[f\"bias_{bv:.1f}\"]['mean_primed'] for bv in BIAS_VALUES]\n",
    "\n",
    "colors_bar = ['#7f7f7f'] + ['#1f77b4' if bv < 10 else '#ff7f0e' for bv in BIAS_VALUES]\n",
    "bars = ax.bar(range(len(cond_labels)), mean_nlls, color=colors_bar, edgecolor='black', linewidth=0.5)\n",
    "ax.set_xticks(range(len(cond_labels)))\n",
    "ax.set_xticklabels(cond_labels, rotation=30, ha='right', fontsize=8)\n",
    "ax.set_ylabel('Mean NLL')\n",
    "ax.set_title('Perplexity Check: Mean NLL by Condition')\n",
    "\n",
    "for i, v in enumerate(mean_nlls):\n",
    "    ax.text(i, v + 0.01, f'{v:.3f}', ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "# ---- Panel 3 (bottom-left): Win Rate vs Bias ----\n",
    "ax = axes[1, 0]\n",
    "\n",
    "win_rates = [analysis[f\"bias_{bv:.1f}\"]['win_pct'] for bv in BIAS_VALUES]\n",
    "ax.plot(BIAS_VALUES, win_rates, marker='s', markersize=8, linewidth=2, color='#2ca02c')\n",
    "ax.axhline(y=50, color='black', linestyle='--', linewidth=0.8, label='Chance (50%)')\n",
    "\n",
    "for i, bv in enumerate(BIAS_VALUES):\n",
    "    ax.annotate(f'{win_rates[i]:.1f}%',\n",
    "                (bv, win_rates[i]),\n",
    "                textcoords='offset points', xytext=(0, 12),\n",
    "                ha='center', fontsize=9)\n",
    "\n",
    "ax.set_xlabel('Attention Bias Value')\n",
    "ax.set_ylabel('Win Rate (% where primed < bare)')\n",
    "ax.set_title('Win Rate vs Bias Level')\n",
    "ax.legend(fontsize=8)\n",
    "\n",
    "# ---- Panel 4 (bottom-right): Hardness x Bias heatmap ----\n",
    "ax = axes[1, 1]\n",
    "\n",
    "heatmap_data = np.zeros((len(BIAS_VALUES), 5))\n",
    "for i, bv in enumerate(BIAS_VALUES):\n",
    "    label = f\"bias_{bv:.1f}\"\n",
    "    for q in range(5):\n",
    "        val = hardness_data[label][q]\n",
    "        heatmap_data[i, q] = val if val is not None else np.nan\n",
    "\n",
    "im = ax.imshow(heatmap_data, cmap='RdBu', aspect='auto',\n",
    "               vmin=-0.5, vmax=0.5)\n",
    "ax.set_xticks(range(5))\n",
    "ax.set_xticklabels(quintile_labels, fontsize=8)\n",
    "ax.set_yticks(range(len(BIAS_VALUES)))\n",
    "ax.set_yticklabels([f\"bias_{bv:.1f}\" for bv in BIAS_VALUES])\n",
    "ax.set_xlabel('Difficulty Quintile')\n",
    "ax.set_ylabel('Bias Level')\n",
    "ax.set_title(\"Hardness x Bias Interaction (Cohen's d)\")\n",
    "\n",
    "for i in range(len(BIAS_VALUES)):\n",
    "    for j in range(5):\n",
    "        val = heatmap_data[i, j]\n",
    "        if not np.isnan(val):\n",
    "            ax.text(j, i, f\"{val:+.2f}\", ha='center', va='center',\n",
    "                    fontsize=8, color='white' if abs(val) > 0.25 else 'black')\n",
    "\n",
    "fig.colorbar(im, ax=ax, shrink=0.8, label=\"Cohen's d\")\n",
    "\n",
    "plt.suptitle('Exp 26: Attention Forcing for Long Documents (1024 tok)', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'analysis_plots.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"Plots saved to {RESULTS_DIR / 'analysis_plots.png'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8acbe32a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T14:33:02.224302Z",
     "iopub.status.busy": "2026-02-16T14:33:02.223964Z",
     "iopub.status.idle": "2026-02-16T14:33:02.288353Z",
     "shell.execute_reply": "2026-02-16T14:33:02.287325Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV saved: results/exp26/results.csv\n",
      "\n",
      "Results saved to results/exp26/results.json\n",
      "File size: 626.8 KB\n",
      "\n",
      "======================================================================\n",
      "SUMMARY -- Exp 26: Attention Forcing for Long Documents\n",
      "======================================================================\n",
      "Model: Mistral 7B (4-bit, float16)\n",
      "Document length: 1024 tokens (padded)\n",
      "Prefix: static_fact_trunc (11 tokens)\n",
      "\n",
      "  bias_0.0       d=+0.144  win=64%  NLL=0.8662  **\n",
      "  bias_2.0       d=+0.291  win=74%  NLL=0.7747  *** <-- BEST\n",
      "  bias_5.0       d=-0.076  win=32%  NLL=0.9842  ns\n",
      "  bias_10.0      d=-0.780  win=12%  NLL=2.1598  ***\n",
      "\n",
      "Exp 20 standard @ 1024 tok: d=-0.043\n",
      "Exp 20 standard @ original:  d=+0.303\n",
      "\n",
      "VERDICT: SUCCESS: Attention forcing recovers a meaningful effect (d=+0.291) at 1024 tokens with bias=2.0\n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Cell 10: Save results.json + CSV\n",
    "import csv\n",
    "\n",
    "# --- CSV ---\n",
    "with open(CSV_PATH, 'w', newline='') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=[\n",
    "        'query_idx', 'bias_value', 'bias_label', 'actual_doc_len',\n",
    "        'bare_nll', 'primed_nll', 'delta_nll'])\n",
    "    writer.writeheader()\n",
    "    for r in all_results:\n",
    "        for row in r['rows']:\n",
    "            writer.writerow(row)\n",
    "print(f\"CSV saved: {CSV_PATH}\")\n",
    "\n",
    "# --- results.json ---\n",
    "final = {\n",
    "    'experiment': 'exp26_attention_forcing',\n",
    "    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'config': {\n",
    "        'model_name': MODEL_NAME,\n",
    "        'model_type': 'mistral',\n",
    "        'seed': SEED,\n",
    "        'dataset': 'MS MARCO v1.1 validation',\n",
    "        'max_passage_words': MAX_PASSAGE_WORDS,\n",
    "        'n_queries': N_QUERIES,\n",
    "        'pad_target': PAD_TARGET,\n",
    "        'bias_values': BIAS_VALUES,\n",
    "        'prefix': STATIC_FACT,\n",
    "        'prefix_token_len': PREFIX_TOKEN_LEN,\n",
    "    },\n",
    "    'analysis': analysis,\n",
    "    'verdict': verdict,\n",
    "    'hardness_data': hardness_data,\n",
    "    'reference_values': {\n",
    "        'exp20_mistral': EXP20_REF,\n",
    "    },\n",
    "    'per_query': all_results,\n",
    "}\n",
    "\n",
    "with open(FINAL_RESULTS_PATH, 'w') as f:\n",
    "    json.dump(final, f, indent=2)\n",
    "\n",
    "print(f\"\\nResults saved to {FINAL_RESULTS_PATH}\")\n",
    "print(f\"File size: {FINAL_RESULTS_PATH.stat().st_size / 1024:.1f} KB\")\n",
    "\n",
    "# Final summary\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"SUMMARY -- Exp 26: Attention Forcing for Long Documents\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Model: Mistral 7B (4-bit, float16)\")\n",
    "print(f\"Document length: {PAD_TARGET} tokens (padded)\")\n",
    "print(f\"Prefix: static_fact_trunc ({PREFIX_TOKEN_LEN} tokens)\")\n",
    "print()\n",
    "for bv in BIAS_VALUES:\n",
    "    label = f\"bias_{bv:.1f}\"\n",
    "    a = analysis[label]\n",
    "    sig = '***' if a['p_value'] < 0.001 else '**' if a['p_value'] < 0.01 else '*' if a['p_value'] < 0.05 else 'ns'\n",
    "    marker = \" <-- BEST\" if label == best_label else \"\"\n",
    "    print(f\"  {label:<14} d={a['cohens_d']:>+.3f}  win={a['win_pct']:.0f}%  \"\n",
    "          f\"NLL={a['mean_primed']:.4f}  {sig}{marker}\")\n",
    "print(f\"\\nExp 20 standard @ 1024 tok: d={EXP20_REF['1024_d']:+.3f}\")\n",
    "print(f\"Exp 20 standard @ original:  d={EXP20_REF['original_d']:+.3f}\")\n",
    "print(f\"\\nVERDICT: {verdict}\")\n",
    "print(f\"\\nDone!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "688616f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T14:33:02.292566Z",
     "iopub.status.busy": "2026-02-16T14:33:02.291745Z",
     "iopub.status.idle": "2026-02-16T14:33:03.078770Z",
     "shell.execute_reply": "2026-02-16T14:33:03.077828Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning up GPU memory...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory: 4.13 GB -> 0.01 GB\n",
      "Cleanup complete.\n"
     ]
    }
   ],
   "source": [
    "# Cell 11: GPU cleanup\n",
    "import gc\n",
    "\n",
    "print(\"Cleaning up GPU memory...\")\n",
    "mem_before = torch.cuda.memory_allocated() / 1e9\n",
    "\n",
    "del model\n",
    "del tokenizer\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "mem_after = torch.cuda.memory_allocated() / 1e9\n",
    "print(f\"GPU memory: {mem_before:.2f} GB -> {mem_after:.2f} GB\")\n",
    "print(\"Cleanup complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0795b9acd2e146169ad5b2c85637a358": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "15e3b790de764b59845d2087abde0576": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_b65666d623394bad8862089f9aa5422d",
       "placeholder": "​",
       "style": "IPY_MODEL_d025e7f9327c4769b0b03937d5e586ae",
       "tabbable": null,
       "tooltip": null,
       "value": " 500/500 [33:58&lt;00:00,  4.13s/it]"
      }
     },
     "1ab03fac69484e74bf9927354cf25470": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_60790e51126443adb239ff4dfd27d2e5",
       "placeholder": "​",
       "style": "IPY_MODEL_d526cc366a5a4742ab88c9c1a10231f4",
       "tabbable": null,
       "tooltip": null,
       "value": " 10047/10047 [00:01&lt;00:00, 5979.36it/s]"
      }
     },
     "2759d74e02894f02ad0f5adb68399731": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "34f582ad4ef548ca81bcd2373b143b38": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3e8225f2d5f84f29b766f75216fc3157": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_458630ffec714b04a45ffbe47d602ff4",
       "placeholder": "​",
       "style": "IPY_MODEL_c71632aaa1d844cfb0269d90dc4c3e5b",
       "tabbable": null,
       "tooltip": null,
       "value": "Filtering: 100%"
      }
     },
     "458630ffec714b04a45ffbe47d602ff4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "5bffa2a154aa48c4bdfd21397c0ae5b9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "60790e51126443adb239ff4dfd27d2e5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "63659a1d2f78447ea657291db0a930bd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_9f17ed424b70413d8d6e70914b07ae7d",
       "max": 291.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_e2836be5e496432e8f64d8e9f1ba2e61",
       "tabbable": null,
       "tooltip": null,
       "value": 291.0
      }
     },
     "65187bdc4beb40af8b4b749ed0697e55": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "6740e68a1052469790e924eec7e757c8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_945f5511d62442d2817227c7c3932c8c",
       "max": 500.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_65187bdc4beb40af8b4b749ed0697e55",
       "tabbable": null,
       "tooltip": null,
       "value": 500.0
      }
     },
     "6a63d7b213714fe7b282f57ec5be758e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "749d47950d434c6b8e83adae52b6605c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_c19c1355b1a54e9c9f2c86581bcb1915",
       "placeholder": "​",
       "style": "IPY_MODEL_fe1459c478814ded9b3416d7d1ed02c2",
       "tabbable": null,
       "tooltip": null,
       "value": " 291/291 [00:56&lt;00:00,  5.76it/s, Materializing param=model.norm.weight]"
      }
     },
     "93ed3569165344259c1148613caa8ccf": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "945f5511d62442d2817227c7c3932c8c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "9c751e8aa3be4e648a95877bee6d32a5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "9f17ed424b70413d8d6e70914b07ae7d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a9a170f487ce4525855f3a9b55047d43": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_f8ebb8f5d93e4784b7264882460b75c1",
        "IPY_MODEL_63659a1d2f78447ea657291db0a930bd",
        "IPY_MODEL_749d47950d434c6b8e83adae52b6605c"
       ],
       "layout": "IPY_MODEL_aecde07a646f4f69be1cb2a2cf056cb8",
       "tabbable": null,
       "tooltip": null
      }
     },
     "ae7a28d9669d4403937c7a31d12a7728": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "aecde07a646f4f69be1cb2a2cf056cb8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b65666d623394bad8862089f9aa5422d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b9e2e16cc5564a009e4735fc95349ce3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_ae7a28d9669d4403937c7a31d12a7728",
       "max": 10047.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_2759d74e02894f02ad0f5adb68399731",
       "tabbable": null,
       "tooltip": null,
       "value": 10047.0
      }
     },
     "c19c1355b1a54e9c9f2c86581bcb1915": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c71632aaa1d844cfb0269d90dc4c3e5b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "d025e7f9327c4769b0b03937d5e586ae": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "d526cc366a5a4742ab88c9c1a10231f4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "dcf9a1eecdf84ab29ac747fe31f0e60f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_93ed3569165344259c1148613caa8ccf",
       "placeholder": "​",
       "style": "IPY_MODEL_9c751e8aa3be4e648a95877bee6d32a5",
       "tabbable": null,
       "tooltip": null,
       "value": "Exp 26: 100%"
      }
     },
     "e2836be5e496432e8f64d8e9f1ba2e61": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "e79a5a06f4bf454b8b96e34965968dcf": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_3e8225f2d5f84f29b766f75216fc3157",
        "IPY_MODEL_b9e2e16cc5564a009e4735fc95349ce3",
        "IPY_MODEL_1ab03fac69484e74bf9927354cf25470"
       ],
       "layout": "IPY_MODEL_5bffa2a154aa48c4bdfd21397c0ae5b9",
       "tabbable": null,
       "tooltip": null
      }
     },
     "f8ebb8f5d93e4784b7264882460b75c1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_34f582ad4ef548ca81bcd2373b143b38",
       "placeholder": "​",
       "style": "IPY_MODEL_6a63d7b213714fe7b282f57ec5be758e",
       "tabbable": null,
       "tooltip": null,
       "value": "Loading weights: 100%"
      }
     },
     "fe1459c478814ded9b3416d7d1ed02c2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "ffc5d72a230f428aad1259046e83dae2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_dcf9a1eecdf84ab29ac747fe31f0e60f",
        "IPY_MODEL_6740e68a1052469790e924eec7e757c8",
        "IPY_MODEL_15e3b790de764b59845d2087abde0576"
       ],
       "layout": "IPY_MODEL_0795b9acd2e146169ad5b2c85637a358",
       "tabbable": null,
       "tooltip": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
