{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exp 23: Multi-Signal Ranking & Query-Time Enhancement\n",
    "\n",
    "## Motivation\n",
    "\n",
    "Exp 22 showed that static-fact `values_early_layers` (VEL) provides **zero ranking benefit**\n",
    "(AUC 0.832 vs bare 0.841 with PMI). The hypothesis: a static, query-independent prefix cannot\n",
    "create query-specific relevance discrimination. This experiment tests 4 new ideas:\n",
    "\n",
    "1. **Cached Answer Templates** \u2014 Score a relevance template instead of the actual answer\n",
    "2. **Query-Time Value Injection (QVI)** \u2014 Inject query info at scoring time via value blending\n",
    "3. **Two-Stage Pipeline** \u2014 Bare PMI rank \u2192 re-rank top-k with oracle VEL\n",
    "4. **Query-Clustered Priming** \u2014 5 intent-specialized prefixes, route to best\n",
    "\n",
    "## Design\n",
    "\n",
    "| Parameter | Value |\n",
    "|-----------|-------|\n",
    "| Model | Gemma 3 4B (`google/gemma-3-4b-it`, 4-bit, bfloat16) |\n",
    "| Dataset | MS MARCO v1.1 validation (same filtering as Exp 22) |\n",
    "| Scale | 200 queries \u00d7 ~8.5 passages \u2248 1,700 passages |\n",
    "| Checkpoint | Every 5 queries |\n",
    "\n",
    "### 13 Cache Conditions\n",
    "\n",
    "| # | Name | Cache Build | FP Cost |\n",
    "|---|------|-------------|--------|\n",
    "| 1 | `bare` | `[BOS][doc]` | 1 FP (shared) |\n",
    "| 2 | `fact_vel` | VEL with static_factual, L0-16 | 1 FP |\n",
    "| 3 | `def_vel` | VEL with static_definitional, L0-16 | 1 FP |\n",
    "| 4 | `proc_vel` | VEL with static_procedural, L0-16 | 1 FP |\n",
    "| 5 | `quant_vel` | VEL with static_quantitative, L0-16 | 1 FP |\n",
    "| 6 | `prob_vel` | VEL with static_problem, L0-16 | 1 FP |\n",
    "| 7 | `oracle_vel` | VEL with actual query, L0-16 | 1 FP |\n",
    "| 8 | `oracle_vel_low` | VEL with actual query, L0-8 | reuse #7 FP |\n",
    "| 9 | `oracle_interp` | `interpolate_values(bare, oracle_corr, \u03b1=0.25)` all layers | reuse #7 FP |\n",
    "| 10 | `oracle_full` | Full `[BOS][query][doc]` context (no truncation) | reuse #7 FP |\n",
    "| 11 | `qvi_010` | QVI \u03b1=0.10, L0-16 | reuse bare+query |\n",
    "| 12 | `qvi_025` | QVI \u03b1=0.25, L0-16 | reuse bare+query |\n",
    "| 13 | `qvi_050` | QVI \u03b1=0.50, L0-16 | reuse bare+query |\n",
    "\n",
    "### 3 Scoring Targets\n",
    "\n",
    "| Target | query_prompt | answer_text | Purpose |\n",
    "|--------|-------------|-------------|--------|\n",
    "| `answer` | `\"\\nQuery: {query}\\nAnswer:\"` | `\" {answer}\"` | Standard: can cache predict the answer? |\n",
    "| `qdoc` | `\"\\nThis document is about:\"` | `\" {query}\"` | Bidirectional: does doc predict the query? |\n",
    "| `relevance` | `\"\\nQuery: {query}\\nIs this document relevant?\"` | `\" Yes, this document is relevant to the query\"` | Template: explicit relevance judgment |\n",
    "\n",
    "### Per-Passage: 13 conditions \u00d7 3 targets = 39 scores\n",
    "\n",
    "### Analysis-Time Derived Scores (no extra data)\n",
    "\n",
    "1. **PMI versions**: `score - baseline` for all 39 combinations\n",
    "2. **Best-intent routing**: `min(fact, def, proc, quant, prob)` per passage per target\n",
    "3. **Two-stage pipeline**: Rank by bare PMI \u2192 re-rank top-k with oracle_vel PMI\n",
    "4. **Per-query intent routing**: For each query, pick intent with lowest mean NLL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T23:25:48.115563Z",
     "iopub.status.busy": "2026-02-15T23:25:48.115070Z",
     "iopub.status.idle": "2026-02-15T23:26:07.134481Z",
     "shell.execute_reply": "2026-02-15T23:26:07.133699Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEED: 42\n",
      "Results directory: results/exp23\n",
      "CUDA available: True\n",
      "GPU: NVIDIA A100-SXM4-40GB\n",
      "GPU memory: 42.3 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading google/gemma-3-4b-it (4-bit, bfloat16)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45e556d733854180ba7daf5115f2e369",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/883 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model loaded successfully.\n",
      "  Model class: Gemma3ForConditionalGeneration\n",
      "  Text config class: Gemma3TextConfig\n",
      "  Hidden size: 2560\n",
      "  Num layers: 34\n",
      "  Num attention heads: 8\n",
      "  Num KV heads: 4\n",
      "  Head dim: 256\n",
      "  BOS token ID: 2\n",
      "  EOS token ID: 1\n",
      "  Unique RoPE thetas: [10000.0, 1000000.0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Cache key dtype: torch.bfloat16\n",
      "  Cache key shape: torch.Size([1, 4, 2, 256])  (batch, kv_heads, seq, head_dim)\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Setup & Imports\n",
    "import os\n",
    "os.umask(0o000)\n",
    "os.environ['HF_TOKEN'] = os.environ.get('HF_TOKEN', '')\n",
    "\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import gc\n",
    "from pathlib import Path\n",
    "\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "RESULTS_DIR = Path(\"results/exp23\")\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "(RESULTS_DIR / \"figures\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CHECKPOINT_PATH = RESULTS_DIR / \"checkpoint.json\"\n",
    "FINAL_RESULTS_PATH = RESULTS_DIR / \"results.json\"\n",
    "CSV_PATH = RESULTS_DIR / \"passage_scores.csv\"\n",
    "\n",
    "print(f\"SEED: {SEED}\")\n",
    "print(f\"Results directory: {RESULTS_DIR}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "\n",
    "# Load model\n",
    "sys.path.insert(0, \".\")\n",
    "\n",
    "from lib.config import ExperimentConfig\n",
    "from lib.model_utils import load_model\n",
    "\n",
    "MODEL_NAME = \"google/gemma-3-4b-it\"\n",
    "\n",
    "exp_config = ExperimentConfig(\n",
    "    model_name=MODEL_NAME,\n",
    "    model_type=\"gemma3\",\n",
    "    compute_dtype=\"auto\",\n",
    "    use_4bit=True,\n",
    "    num_samples=2000,\n",
    "    seed=SEED,\n",
    ")\n",
    "\n",
    "print(f\"\\nLoading {MODEL_NAME} (4-bit, bfloat16)...\")\n",
    "model, tokenizer = load_model(exp_config)\n",
    "\n",
    "# Architecture diagnostics\n",
    "from lib.kv_cache import (\n",
    "    _get_text_config, _get_head_dim, _get_rope_theta_for_layer,\n",
    "    _get_cache_keys, _ensure_dynamic_cache,\n",
    ")\n",
    "\n",
    "text_config = _get_text_config(model.config)\n",
    "print(f\"\\nModel loaded successfully.\")\n",
    "print(f\"  Model class: {type(model).__name__}\")\n",
    "print(f\"  Text config class: {type(text_config).__name__}\")\n",
    "print(f\"  Hidden size: {text_config.hidden_size}\")\n",
    "print(f\"  Num layers: {text_config.num_hidden_layers}\")\n",
    "print(f\"  Num attention heads: {text_config.num_attention_heads}\")\n",
    "print(f\"  Num KV heads: {text_config.num_key_value_heads}\")\n",
    "print(f\"  Head dim: {_get_head_dim(model.config)}\")\n",
    "print(f\"  BOS token ID: {tokenizer.bos_token_id}\")\n",
    "print(f\"  EOS token ID: {tokenizer.eos_token_id}\")\n",
    "\n",
    "# Per-layer RoPE diagnostics\n",
    "thetas = set()\n",
    "for layer_idx in range(text_config.num_hidden_layers):\n",
    "    thetas.add(_get_rope_theta_for_layer(model.config, layer_idx))\n",
    "print(f\"  Unique RoPE thetas: {sorted(thetas)}\")\n",
    "\n",
    "# Verify dtype\n",
    "sample_ids = tokenizer(\"test\", return_tensors=\"pt\")['input_ids'].to(exp_config.device)\n",
    "with torch.no_grad():\n",
    "    out = model(sample_ids, use_cache=True)\n",
    "    cache_check = _ensure_dynamic_cache(out.past_key_values)\n",
    "    k0 = _get_cache_keys(cache_check, 0)\n",
    "    print(f\"  Cache key dtype: {k0.dtype}\")\n",
    "    print(f\"  Cache key shape: {k0.shape}  (batch, kv_heads, seq, head_dim)\")\n",
    "del out, sample_ids, cache_check\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T23:26:07.138022Z",
     "iopub.status.busy": "2026-02-15T23:26:07.137450Z",
     "iopub.status.idle": "2026-02-15T23:26:07.147949Z",
     "shell.execute_reply": "2026-02-15T23:26:07.147275Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "  N_QUERIES: 200\n",
      "  LAYER_CUTOFF: 17 (layers 0-16)\n",
      "  LAYER_CUTOFF_LOW: 9 (layers 0-8)\n",
      "  CHECKPOINT_EVERY: 5\n",
      "  QVI_ALPHAS: [0.1, 0.25, 0.5]\n",
      "  INTERP_ALPHA: 0.25\n",
      "  MAX_PASSAGE_WORDS: 300\n",
      "\n",
      "Intent prefixes:\n",
      "  fact: 'What are the key facts I need to know?'\n",
      "  def: 'What is this and what does it mean?'\n",
      "  proc: 'How do I do this step by step?'\n",
      "  quant: 'How much does this cost or how long does it take?'\n",
      "  prob: 'What problem does this solve?'\n",
      "\n",
      "Scoring targets:\n",
      "  answer:    QP='\n",
      "Query: {query}\n",
      "Answer:'  AT=' {answer}'\n",
      "  qdoc:      QP='\n",
      "This document is about:'  AT=' {query}'\n",
      "  relevance: QP='\n",
      "Query: {query}\n",
      "Is this document relevant?'  AT=' Yes, this document is relevant to the query'\n",
      "\n",
      "13 conditions \u00d7 3 targets = 39 scores per passage\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Configuration\n",
    "from lib.kv_cache import (\n",
    "    _get_cache_keys,\n",
    "    _get_cache_values,\n",
    "    _set_cache_keys,\n",
    "    _set_cache_values,\n",
    "    _ensure_dynamic_cache,\n",
    "    extract_and_truncate_cache_with_bos,\n",
    "    correct_rope_positions_with_bos,\n",
    "    score_answer_with_cache,\n",
    "    deepcopy_cache,\n",
    "    replace_values_at_layers,\n",
    "    interpolate_values,\n",
    ")\n",
    "from lib.surrogate import STATIC_SURROGATE_QUERIES\n",
    "from lib.data import count_words\n",
    "from transformers import DynamicCache\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "N_QUERIES = 200\n",
    "LAYER_CUTOFF = 17          # layers 0-16 inclusive\n",
    "LAYER_CUTOFF_LOW = 9       # layers 0-8 inclusive (for oracle_vel_low)\n",
    "CHECKPOINT_EVERY = 5\n",
    "QVI_ALPHAS = [0.10, 0.25, 0.50]\n",
    "INTERP_ALPHA = 0.25\n",
    "MAX_PASSAGE_WORDS = 300\n",
    "\n",
    "# 5 intent prefixes\n",
    "INTENT_PREFIXES = {\n",
    "    'fact': STATIC_SURROGATE_QUERIES['static_factual']['query'],\n",
    "    'def': STATIC_SURROGATE_QUERIES['static_definitional']['query'],\n",
    "    'proc': STATIC_SURROGATE_QUERIES['static_procedural']['query'],\n",
    "    'quant': STATIC_SURROGATE_QUERIES['static_quantitative']['query'],\n",
    "    'prob': STATIC_SURROGATE_QUERIES['static_problem']['query'],\n",
    "}\n",
    "\n",
    "# Scoring templates \u2014 3 targets\n",
    "ANSWER_QP = \"\\nQuery: {query}\\nAnswer:\"\n",
    "ANSWER_AT = \" {answer}\"\n",
    "QDOC_QP = \"\\nThis document is about:\"\n",
    "QDOC_AT = \" {query}\"\n",
    "RELEVANCE_QP = \"\\nQuery: {query}\\nIs this document relevant?\"\n",
    "RELEVANCE_AT = \" Yes, this document is relevant to the query\"\n",
    "\n",
    "# All 13 condition names (for consistent ordering)\n",
    "CONDITION_NAMES = [\n",
    "    'bare', 'fact_vel', 'def_vel', 'proc_vel', 'quant_vel', 'prob_vel',\n",
    "    'oracle_vel', 'oracle_vel_low', 'oracle_interp', 'oracle_full',\n",
    "    'qvi_010', 'qvi_025', 'qvi_050',\n",
    "]\n",
    "TARGET_NAMES = ['answer', 'qdoc', 'relevance']\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  N_QUERIES: {N_QUERIES}\")\n",
    "print(f\"  LAYER_CUTOFF: {LAYER_CUTOFF} (layers 0-{LAYER_CUTOFF-1})\")\n",
    "print(f\"  LAYER_CUTOFF_LOW: {LAYER_CUTOFF_LOW} (layers 0-{LAYER_CUTOFF_LOW-1})\")\n",
    "print(f\"  CHECKPOINT_EVERY: {CHECKPOINT_EVERY}\")\n",
    "print(f\"  QVI_ALPHAS: {QVI_ALPHAS}\")\n",
    "print(f\"  INTERP_ALPHA: {INTERP_ALPHA}\")\n",
    "print(f\"  MAX_PASSAGE_WORDS: {MAX_PASSAGE_WORDS}\")\n",
    "print(f\"\\nIntent prefixes:\")\n",
    "for name, query in INTENT_PREFIXES.items():\n",
    "    print(f\"  {name}: '{query}'\")\n",
    "print(f\"\\nScoring targets:\")\n",
    "print(f\"  answer:    QP='{ANSWER_QP}'  AT='{ANSWER_AT}'\")\n",
    "print(f\"  qdoc:      QP='{QDOC_QP}'  AT='{QDOC_AT}'\")\n",
    "print(f\"  relevance: QP='{RELEVANCE_QP}'  AT='{RELEVANCE_AT}'\")\n",
    "print(f\"\\n13 conditions \u00d7 3 targets = {len(CONDITION_NAMES) * len(TARGET_NAMES)} scores per passage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T23:26:07.151186Z",
     "iopub.status.busy": "2026-02-15T23:26:07.150730Z",
     "iopub.status.idle": "2026-02-15T23:26:08.219903Z",
     "shell.execute_reply": "2026-02-15T23:26:08.218849Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`trust_remote_code` is not supported anymore.\n",
      "Please check that the Hugging Face dataset 'microsoft/ms_marco' isn't based on a loading script and remove `trust_remote_code`.\n",
      "If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "LOADING MS MARCO v1.1 \u2014 ALL PASSAGES PER QUERY\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total items in validation: 10047\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30263886684047c1b6f8594397e4666c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filtering:   0%|          | 0/10047 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Selected 200 queries (1692 total passages)\n",
      "Passages per query: mean=8.5, min=5, max=10\n",
      "Word counts: mean=71\n",
      "Relevant passages: 221 (13.1%)\n",
      "Irrelevant passages: 1471 (86.9%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 3: Load MS MARCO Data\n",
    "from datasets import load_dataset\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"LOADING MS MARCO v1.1 \u2014 ALL PASSAGES PER QUERY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "dataset = load_dataset(\"microsoft/ms_marco\", \"v1.1\", split=\"validation\",\n",
    "                        trust_remote_code=True)\n",
    "print(f\"Total items in validation: {len(dataset)}\")\n",
    "\n",
    "queries = []\n",
    "np.random.seed(SEED)\n",
    "\n",
    "for item in tqdm(dataset, desc=\"Filtering\"):\n",
    "    passages_info = item.get('passages', {})\n",
    "    passage_texts = passages_info.get('passage_text', [])\n",
    "    is_selected = passages_info.get('is_selected', [])\n",
    "    query = item.get('query', '')\n",
    "    answers = item.get('answers', [])\n",
    "    well_formed = item.get('wellFormedAnswers', [])\n",
    "\n",
    "    if not passage_texts or not query:\n",
    "        continue\n",
    "    # Require at least 1 relevant passage\n",
    "    if not is_selected or sum(is_selected) == 0:\n",
    "        continue\n",
    "\n",
    "    # All passages must be <= MAX_PASSAGE_WORDS\n",
    "    word_counts = [count_words(p) for p in passage_texts]\n",
    "    if any(wc > MAX_PASSAGE_WORDS for wc in word_counts):\n",
    "        continue\n",
    "\n",
    "    # Require valid answer\n",
    "    answer = None\n",
    "    if well_formed and len(well_formed) > 0 and well_formed[0] != '[]':\n",
    "        answer = well_formed[0]\n",
    "    elif answers and len(answers) > 0 and answers[0] != 'No Answer Present.':\n",
    "        answer = answers[0]\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "    passage_list = []\n",
    "    for i, (ptext, sel) in enumerate(zip(passage_texts, is_selected)):\n",
    "        passage_list.append({\n",
    "            'passage': ptext,\n",
    "            'is_relevant': bool(sel == 1),\n",
    "            'word_count': word_counts[i],\n",
    "            'passage_idx': i,\n",
    "        })\n",
    "\n",
    "    queries.append({\n",
    "        'query': query,\n",
    "        'answer': answer,\n",
    "        'passages': passage_list,\n",
    "        'n_passages': len(passage_list),\n",
    "        'n_relevant': sum(1 for p in passage_list if p['is_relevant']),\n",
    "    })\n",
    "\n",
    "    if len(queries) >= N_QUERIES * 3:\n",
    "        break\n",
    "\n",
    "np.random.shuffle(queries)\n",
    "queries = queries[:N_QUERIES]\n",
    "N = len(queries)\n",
    "\n",
    "n_passages_list = [q['n_passages'] for q in queries]\n",
    "total_passages = sum(n_passages_list)\n",
    "total_relevant = sum(q['n_relevant'] for q in queries)\n",
    "total_irrelevant = total_passages - total_relevant\n",
    "\n",
    "print(f\"\\nSelected {N} queries ({total_passages} total passages)\")\n",
    "print(f\"Passages per query: mean={np.mean(n_passages_list):.1f}, \"\n",
    "      f\"min={min(n_passages_list)}, max={max(n_passages_list)}\")\n",
    "print(f\"Word counts: mean={np.mean([p['word_count'] for q in queries for p in q['passages']]):.0f}\")\n",
    "print(f\"Relevant passages: {total_relevant} ({100*total_relevant/total_passages:.1f}%)\")\n",
    "print(f\"Irrelevant passages: {total_irrelevant} ({100*total_irrelevant/total_passages:.1f}%)\")\n",
    "\n",
    "del dataset\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T23:26:08.224556Z",
     "iopub.status.busy": "2026-02-15T23:26:08.224281Z",
     "iopub.status.idle": "2026-02-15T23:26:08.233146Z",
     "shell.execute_reply": "2026-02-15T23:26:08.232377Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "EXPERIMENTAL CONDITIONS EXPLAINED\n",
      "======================================================================\n",
      "\n",
      "Example query:   \"what is the bride entrance called\"\n",
      "Example answer:  \"Procession...\"\n",
      "Example passage: \"Check out our ultimate guide below, beginning with Mom and ending with the bride...\"\n",
      "\n",
      "=== 13 CACHE CONDITIONS ===\n",
      "\n",
      "### 1. bare ###\n",
      "  Cache: model([BOS][doc_ids])\n",
      "  No prefix. Baseline cache.\n",
      "\n",
      "### 2-6. {intent}_vel (5 intent-specialized VEL caches) ###\n",
      "  For each intent in [fact, def, proc, quant, prob]:\n",
      "    Step 1: Forward pass [BOS][intent_prefix][doc_ids]\n",
      "    Step 2: Truncate to [BOS][last doc_len positions]\n",
      "    Step 3: RoPE correct (offset = # prefix tokens)\n",
      "    Step 4: replace_values_at_layers(bare, corrected, layers 0-16)\n",
      "  Prefixes:\n",
      "    fact:  \"What are the key facts I need to know?\"\n",
      "    def:   \"What is this and what does it mean?\"\n",
      "    proc:  \"How do I do this step by step?\"\n",
      "    quant: \"How much does this cost or how long does it take?\"\n",
      "    prob:  \"What problem does this solve?\"\n",
      "\n",
      "### 7. oracle_vel ###\n",
      "  Same as intent VEL, but prefix = actual query text.\n",
      "  Layers 0-16. Upper bound for query-aware VEL.\n",
      "\n",
      "### 8. oracle_vel_low ###\n",
      "  Same as oracle_vel, but only layers 0-8.\n",
      "  Reuses oracle's forward pass.\n",
      "\n",
      "### 9. oracle_interp ###\n",
      "  interpolate_values(bare, oracle_corrected, alpha=0.25)\n",
      "  v_blended = 0.25 * v_oracle + 0.75 * v_bare (ALL layers).\n",
      "  Keys always from bare.\n",
      "\n",
      "### 10. oracle_full ###\n",
      "  Full [BOS][query][doc] cache, not truncated.\n",
      "  context_len = 1 + prefix_len + doc_len.\n",
      "  Gold standard: query is visible to scoring.\n",
      "\n",
      "### 11-13. qvi_{alpha} (Query-Time Value Injection) ###\n",
      "  For each alpha in [0.1, 0.25, 0.5]:\n",
      "    1. Build query cache: model([BOS][query_ids])\n",
      "    2. Compute mean query values per layer: v_query_mean[layer]\n",
      "    3. For layers 0-16: v_new = (1-alpha) * v_bare + alpha * v_query_mean\n",
      "    4. Keys from bare.\n",
      "  Formula: v_new = (1-\u03b1) * v_doc + \u03b1 * mean(v_query)\n",
      "  No extra forward pass \u2014 blends existing caches.\n",
      "\n",
      "=== 3 SCORING TARGETS ===\n",
      "\n",
      "### answer ###\n",
      "  query_prompt: \"\n",
      "Query: {query}\n",
      "Answer:\"\n",
      "  answer_text:  \" {answer}\"\n",
      "  Standard: can the cache predict the actual answer?\n",
      "\n",
      "### qdoc ###\n",
      "  query_prompt: \"\n",
      "This document is about:\"\n",
      "  answer_text:  \" {query}\"\n",
      "  Bidirectional: does the document predict the query?\n",
      "\n",
      "### relevance ###\n",
      "  query_prompt: \"\n",
      "Query: {query}\n",
      "Is this document relevant?\"\n",
      "  answer_text:  \" Yes, this document is relevant to the query\"\n",
      "  Template: explicit relevance judgment (~8 tokens).\n",
      "\n",
      "=== ANALYSIS-TIME DERIVED METHODS (no extra data) ===\n",
      "\n",
      "1. PMI: score - baseline for all 39 condition \u00d7 target pairs\n",
      "2. Best-intent routing: min(fact, def, proc, quant, prob) per passage\n",
      "3. Two-stage pipeline: bare PMI rank \u2192 re-rank top-k with oracle_vel PMI\n",
      "4. Per-query intent routing: pick best intent per query\n",
      "\n",
      "Relevance answer tokens (9 tokens): [8438, 236764, 672, 2354, 563, 7798, 531, 506, 7609]\n",
      "Decoded: [' Yes', ',', ' this', ' document', ' is', ' relevant', ' to', ' the', ' query']\n",
      "\n",
      "Verification passed: relevance answer = 9 tokens (>= 2).\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Explain Experimental Conditions\n",
    "print(\"=\" * 70)\n",
    "print(\"EXPERIMENTAL CONDITIONS EXPLAINED\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "example_query = queries[0]['query']\n",
    "example_answer = queries[0]['answer']\n",
    "example_passage = queries[0]['passages'][0]['passage'][:80] + \"...\"\n",
    "\n",
    "print(f\"\"\"\n",
    "Example query:   \\\"{example_query}\\\"\n",
    "Example answer:  \\\"{example_answer[:60]}...\\\"\n",
    "Example passage: \\\"{example_passage}\\\"\n",
    "\n",
    "=== 13 CACHE CONDITIONS ===\n",
    "\n",
    "### 1. bare ###\n",
    "  Cache: model([BOS][doc_ids])\n",
    "  No prefix. Baseline cache.\n",
    "\n",
    "### 2-6. {{intent}}_vel (5 intent-specialized VEL caches) ###\n",
    "  For each intent in [fact, def, proc, quant, prob]:\n",
    "    Step 1: Forward pass [BOS][intent_prefix][doc_ids]\n",
    "    Step 2: Truncate to [BOS][last doc_len positions]\n",
    "    Step 3: RoPE correct (offset = # prefix tokens)\n",
    "    Step 4: replace_values_at_layers(bare, corrected, layers 0-{LAYER_CUTOFF-1})\n",
    "  Prefixes:\n",
    "    fact:  \\\"{INTENT_PREFIXES['fact']}\\\"\n",
    "    def:   \\\"{INTENT_PREFIXES['def']}\\\"\n",
    "    proc:  \\\"{INTENT_PREFIXES['proc']}\\\"\n",
    "    quant: \\\"{INTENT_PREFIXES['quant']}\\\"\n",
    "    prob:  \\\"{INTENT_PREFIXES['prob']}\\\"\n",
    "\n",
    "### 7. oracle_vel ###\n",
    "  Same as intent VEL, but prefix = actual query text.\n",
    "  Layers 0-{LAYER_CUTOFF-1}. Upper bound for query-aware VEL.\n",
    "\n",
    "### 8. oracle_vel_low ###\n",
    "  Same as oracle_vel, but only layers 0-{LAYER_CUTOFF_LOW-1}.\n",
    "  Reuses oracle's forward pass.\n",
    "\n",
    "### 9. oracle_interp ###\n",
    "  interpolate_values(bare, oracle_corrected, alpha={INTERP_ALPHA})\n",
    "  v_blended = {INTERP_ALPHA} * v_oracle + {1-INTERP_ALPHA} * v_bare (ALL layers).\n",
    "  Keys always from bare.\n",
    "\n",
    "### 10. oracle_full ###\n",
    "  Full [BOS][query][doc] cache, not truncated.\n",
    "  context_len = 1 + prefix_len + doc_len.\n",
    "  Gold standard: query is visible to scoring.\n",
    "\n",
    "### 11-13. qvi_{{alpha}} (Query-Time Value Injection) ###\n",
    "  For each alpha in {QVI_ALPHAS}:\n",
    "    1. Build query cache: model([BOS][query_ids])\n",
    "    2. Compute mean query values per layer: v_query_mean[layer]\n",
    "    3. For layers 0-{LAYER_CUTOFF-1}: v_new = (1-alpha) * v_bare + alpha * v_query_mean\n",
    "    4. Keys from bare.\n",
    "  Formula: v_new = (1-\u03b1) * v_doc + \u03b1 * mean(v_query)\n",
    "  No extra forward pass \u2014 blends existing caches.\n",
    "\n",
    "=== 3 SCORING TARGETS ===\n",
    "\n",
    "### answer ###\n",
    "  query_prompt: \"\\nQuery: {{query}}\\nAnswer:\"\n",
    "  answer_text:  \" {{answer}}\"\n",
    "  Standard: can the cache predict the actual answer?\n",
    "\n",
    "### qdoc ###\n",
    "  query_prompt: \"\\nThis document is about:\"\n",
    "  answer_text:  \" {{query}}\"\n",
    "  Bidirectional: does the document predict the query?\n",
    "\n",
    "### relevance ###\n",
    "  query_prompt: \"\\nQuery: {{query}}\\nIs this document relevant?\"\n",
    "  answer_text:  \" Yes, this document is relevant to the query\"\n",
    "  Template: explicit relevance judgment (~8 tokens).\n",
    "\n",
    "=== ANALYSIS-TIME DERIVED METHODS (no extra data) ===\n",
    "\n",
    "1. PMI: score - baseline for all 39 condition \u00d7 target pairs\n",
    "2. Best-intent routing: min(fact, def, proc, quant, prob) per passage\n",
    "3. Two-stage pipeline: bare PMI rank \u2192 re-rank top-k with oracle_vel PMI\n",
    "4. Per-query intent routing: pick best intent per query\n",
    "\"\"\")\n",
    "\n",
    "# Verify relevance answer tokenizes to >= 2 tokens\n",
    "rel_tokens = tokenizer(RELEVANCE_AT, add_special_tokens=False)['input_ids']\n",
    "print(f\"Relevance answer tokens ({len(rel_tokens)} tokens): {rel_tokens}\")\n",
    "print(f\"Decoded: {[tokenizer.decode([t]) for t in rel_tokens]}\")\n",
    "assert len(rel_tokens) >= 2, f\"Relevance answer must be >= 2 tokens, got {len(rel_tokens)}\"\n",
    "print(f\"\\nVerification passed: relevance answer = {len(rel_tokens)} tokens (>= 2).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T23:26:08.237027Z",
     "iopub.status.busy": "2026-02-15T23:26:08.236652Z",
     "iopub.status.idle": "2026-02-15T23:26:22.440149Z",
     "shell.execute_reply": "2026-02-15T23:26:22.439459Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smoke-testing helper functions...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Baselines: {'answer': 4.75, 'qdoc': 8.45, 'relevance': 1.3359375}\n",
      "  Query cache built: 7 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Doc len: 151\n",
      "  Conditions scored: 13\n",
      "  All 39 NLL values are finite.\n",
      "\n",
      "  Condition                answer       qdoc  relevance\n",
      "  ----------------------------------------------------\n",
      "  bare                     8.1250     5.1000     1.8750\n",
      "  fact_vel                 9.6250     4.8000     1.2891\n",
      "  def_vel                  8.7500     4.9000     1.2656\n",
      "  proc_vel                 8.8750     4.9250     1.2656\n",
      "  quant_vel                8.0000     4.8500     1.2344\n",
      "  prob_vel                 8.6250     4.8750     1.1172\n",
      "  oracle_vel               8.2500     4.6500     1.2578\n",
      "  oracle_vel_low           8.5000     5.1000     1.6719\n",
      "  oracle_interp            8.5000     4.9500     1.7266\n",
      "  oracle_full              9.2500     0.0106     1.1328\n",
      "  qvi_010                  6.8750     5.2500     1.5312\n",
      "  qvi_025                  5.3750     5.6500     1.1250\n",
      "  qvi_050                  3.1719     5.9500     1.2500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Smoke test passed: 13 conditions \u00d7 3 targets = 39 scores, all finite.\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Helper Functions\n",
    "\n",
    "def build_query_cache(query_text, model, tokenizer, config):\n",
    "    \"\"\"Forward pass [BOS][query] \u2192 return (cache, len).\"\"\"\n",
    "    enc = tokenizer(query_text, return_tensors=\"pt\", add_special_tokens=True)\n",
    "    ids = enc['input_ids'].to(config.device)\n",
    "    with torch.no_grad():\n",
    "        out = model(input_ids=ids, attention_mask=torch.ones_like(ids),\n",
    "                    use_cache=True, return_dict=True)\n",
    "    cache = _ensure_dynamic_cache(out.past_key_values)\n",
    "    q_len = ids.shape[1]\n",
    "    del out, ids, enc\n",
    "    return cache, q_len\n",
    "\n",
    "\n",
    "def build_qvi_cache(bare_cache, query_cache, layer_cutoff, alpha):\n",
    "    \"\"\"Blend mean query values into early layers of bare doc cache.\n",
    "\n",
    "    Formula: v_new = (1-alpha) * v_bare + alpha * mean(v_query)\n",
    "    Keys always from bare. Only affects layers < layer_cutoff.\n",
    "    \"\"\"\n",
    "    new_cache = DynamicCache()\n",
    "    for layer_idx in range(len(bare_cache)):\n",
    "        k = _get_cache_keys(bare_cache, layer_idx).clone()\n",
    "        v_bare = _get_cache_values(bare_cache, layer_idx)\n",
    "        if layer_idx < layer_cutoff:\n",
    "            v_query = _get_cache_values(query_cache, layer_idx)\n",
    "            v_query_mean = v_query.mean(dim=2, keepdim=True)  # [1, heads, 1, dim]\n",
    "            v_new = (1 - alpha) * v_bare + alpha * v_query_mean\n",
    "            new_cache.update(k, v_new.clone(), layer_idx)\n",
    "        else:\n",
    "            new_cache.update(k, v_bare.clone(), layer_idx)\n",
    "    return new_cache\n",
    "\n",
    "\n",
    "def score_all_targets(cache, context_len, query, answer, model, tokenizer, config):\n",
    "    \"\"\"Score all 3 targets against one cache. Last target consumes cache (no deepcopy).\n",
    "\n",
    "    Returns: {'answer': float, 'qdoc': float, 'relevance': float}\n",
    "    \"\"\"\n",
    "    answer_qp = ANSWER_QP.format(query=query)\n",
    "    answer_at = ANSWER_AT.format(answer=answer)\n",
    "    qdoc_qp = QDOC_QP\n",
    "    qdoc_at = QDOC_AT.format(query=query)\n",
    "    rel_qp = RELEVANCE_QP.format(query=query)\n",
    "    rel_at = RELEVANCE_AT\n",
    "\n",
    "    nll_answer = score_answer_with_cache(\n",
    "        deepcopy_cache(cache), context_len, answer_qp, answer_at,\n",
    "        model, tokenizer, config)\n",
    "    nll_qdoc = score_answer_with_cache(\n",
    "        deepcopy_cache(cache), context_len, qdoc_qp, qdoc_at,\n",
    "        model, tokenizer, config)\n",
    "    nll_relevance = score_answer_with_cache(\n",
    "        cache, context_len, rel_qp, rel_at,\n",
    "        model, tokenizer, config)  # consumes cache \u2014 last use\n",
    "    return {'answer': nll_answer, 'qdoc': nll_qdoc, 'relevance': nll_relevance}\n",
    "\n",
    "\n",
    "def score_baselines(query, answer, model, tokenizer, config):\n",
    "    \"\"\"BOS-only cache \u2192 score all 3 targets. Returns dict of 3 baseline NLLs.\"\"\"\n",
    "    bos_id = torch.tensor([[tokenizer.bos_token_id]], device=config.device)\n",
    "    with torch.no_grad():\n",
    "        bos_out = model(input_ids=bos_id, attention_mask=torch.ones_like(bos_id),\n",
    "                        use_cache=True, return_dict=True)\n",
    "    bos_cache = _ensure_dynamic_cache(bos_out.past_key_values)\n",
    "    del bos_out\n",
    "    context_len = 1  # just BOS\n",
    "    return score_all_targets(bos_cache, context_len, query, answer,\n",
    "                             model, tokenizer, config)\n",
    "\n",
    "\n",
    "def build_vel_cache(bare_cache, doc_len, prefix_str, passage_text,\n",
    "                    model, tokenizer, config, layer_cutoff):\n",
    "    \"\"\"Build a VEL cache for a given prefix.\n",
    "\n",
    "    Matched tokenization: tokenize prefix+passage together, split by prefix length.\n",
    "    Returns VEL cache (bare keys, primed values in early layers).\n",
    "    \"\"\"\n",
    "    full_text = prefix_str + passage_text\n",
    "    full_enc = tokenizer(full_text, return_tensors=\"pt\",\n",
    "                          add_special_tokens=True, padding=False, truncation=False)\n",
    "    full_ids = full_enc['input_ids'].to(config.device)\n",
    "\n",
    "    sf_prefix_enc = tokenizer(prefix_str, return_tensors=\"pt\",\n",
    "                               add_special_tokens=True, padding=False, truncation=False)\n",
    "    sf_prefix_len_matched = sf_prefix_enc['input_ids'].shape[1]\n",
    "    del sf_prefix_enc\n",
    "\n",
    "    # Forward pass: [BOS][prefix][doc]\n",
    "    with torch.no_grad():\n",
    "        primed_out = model(input_ids=full_ids,\n",
    "                           attention_mask=torch.ones_like(full_ids),\n",
    "                           use_cache=True, return_dict=True)\n",
    "    primed_full = _ensure_dynamic_cache(primed_out.past_key_values)\n",
    "    prefix_offset = sf_prefix_len_matched - 1  # tokens without BOS\n",
    "    del primed_out, full_ids, full_enc\n",
    "\n",
    "    # Truncate + RoPE correct\n",
    "    trunc_cache = extract_and_truncate_cache_with_bos(primed_full, doc_len)\n",
    "    del primed_full\n",
    "    correct_rope_positions_with_bos(trunc_cache, prefix_offset, model)\n",
    "\n",
    "    # Build VEL: bare keys, primed values in early layers\n",
    "    vel_cache = replace_values_at_layers(\n",
    "        bare_cache, trunc_cache, list(range(layer_cutoff)))\n",
    "    del trunc_cache\n",
    "    return vel_cache\n",
    "\n",
    "\n",
    "def build_oracle_caches(bare_cache, doc_len, query, passage_text,\n",
    "                        model, tokenizer, config):\n",
    "    \"\"\"Build all oracle-derived caches from one forward pass.\n",
    "\n",
    "    Returns dict with keys: oracle_vel, oracle_vel_low, oracle_interp, oracle_full,\n",
    "    plus oracle_full_context_len.\n",
    "    \"\"\"\n",
    "    prefix_str = query + \"\\n\"\n",
    "    full_text = prefix_str + passage_text\n",
    "    full_enc = tokenizer(full_text, return_tensors=\"pt\",\n",
    "                          add_special_tokens=True, padding=False, truncation=False)\n",
    "    full_ids = full_enc['input_ids'].to(config.device)\n",
    "\n",
    "    sf_prefix_enc = tokenizer(prefix_str, return_tensors=\"pt\",\n",
    "                               add_special_tokens=True, padding=False, truncation=False)\n",
    "    sf_prefix_len_matched = sf_prefix_enc['input_ids'].shape[1]\n",
    "    prefix_offset = sf_prefix_len_matched - 1  # tokens without BOS\n",
    "    del sf_prefix_enc\n",
    "\n",
    "    # Forward pass: [BOS][query][doc]\n",
    "    with torch.no_grad():\n",
    "        primed_out = model(input_ids=full_ids,\n",
    "                           attention_mask=torch.ones_like(full_ids),\n",
    "                           use_cache=True, return_dict=True)\n",
    "    oracle_primed = _ensure_dynamic_cache(primed_out.past_key_values)\n",
    "    oracle_full_context_len = full_ids.shape[1]  # 1 + prefix_len + doc_len\n",
    "    del primed_out, full_ids, full_enc\n",
    "\n",
    "    # --- oracle_full: score directly from primed cache (no truncation) ---\n",
    "    oracle_full = deepcopy_cache(oracle_primed)\n",
    "\n",
    "    # --- Truncate + RoPE correct for VEL variants ---\n",
    "    oracle_trunc = extract_and_truncate_cache_with_bos(oracle_primed, doc_len)\n",
    "    del oracle_primed\n",
    "    correct_rope_positions_with_bos(oracle_trunc, prefix_offset, model)\n",
    "\n",
    "    # oracle_vel: layers 0-16\n",
    "    oracle_vel = replace_values_at_layers(\n",
    "        bare_cache, oracle_trunc, list(range(LAYER_CUTOFF)))\n",
    "\n",
    "    # oracle_vel_low: layers 0-8\n",
    "    oracle_vel_low = replace_values_at_layers(\n",
    "        bare_cache, oracle_trunc, list(range(LAYER_CUTOFF_LOW)))\n",
    "\n",
    "    # oracle_interp: interpolate ALL layers\n",
    "    oracle_interp = interpolate_values(bare_cache, oracle_trunc, INTERP_ALPHA)\n",
    "\n",
    "    del oracle_trunc\n",
    "\n",
    "    return {\n",
    "        'oracle_vel': oracle_vel,\n",
    "        'oracle_vel_low': oracle_vel_low,\n",
    "        'oracle_interp': oracle_interp,\n",
    "        'oracle_full': oracle_full,\n",
    "        'oracle_full_context_len': oracle_full_context_len,\n",
    "    }\n",
    "\n",
    "\n",
    "def build_and_score_all_conditions(passage_text, query, answer, query_cache,\n",
    "                                    model, tokenizer, config):\n",
    "    \"\"\"Build all 13 caches and score all 3 targets for one passage.\n",
    "\n",
    "    Memory-safe processing order: build, score, delete sequentially.\n",
    "    Returns: dict of {condition_name: {target: nll}} plus doc_len.\n",
    "    \"\"\"\n",
    "    scores = {}\n",
    "\n",
    "    # === 1. Build bare cache (KEEP alive throughout) ===\n",
    "    bare_enc = tokenizer(passage_text, return_tensors=\"pt\",\n",
    "                          add_special_tokens=True, padding=False, truncation=False)\n",
    "    bare_ids = bare_enc['input_ids'].to(config.device)\n",
    "    doc_len = bare_ids.shape[1] - 1  # exclude BOS\n",
    "    context_len = 1 + doc_len  # BOS + doc\n",
    "\n",
    "    with torch.no_grad():\n",
    "        bare_out = model(input_ids=bare_ids,\n",
    "                         attention_mask=torch.ones_like(bare_ids),\n",
    "                         use_cache=True, return_dict=True)\n",
    "    bare_cache = _ensure_dynamic_cache(bare_out.past_key_values)\n",
    "    del bare_out, bare_ids, bare_enc\n",
    "\n",
    "    # === 2. Score bare ===\n",
    "    scores['bare'] = score_all_targets(\n",
    "        deepcopy_cache(bare_cache), context_len, query, answer,\n",
    "        model, tokenizer, config)\n",
    "\n",
    "    # === 3. Five intent VEL caches ===\n",
    "    intent_to_cond = {\n",
    "        'fact': 'fact_vel', 'def': 'def_vel', 'proc': 'proc_vel',\n",
    "        'quant': 'quant_vel', 'prob': 'prob_vel',\n",
    "    }\n",
    "    for intent_name, cond_name in intent_to_cond.items():\n",
    "        prefix_str = INTENT_PREFIXES[intent_name] + \"\\n\"\n",
    "        vel = build_vel_cache(bare_cache, doc_len, prefix_str, passage_text,\n",
    "                              model, tokenizer, config, LAYER_CUTOFF)\n",
    "        scores[cond_name] = score_all_targets(\n",
    "            vel, context_len, query, answer, model, tokenizer, config)\n",
    "        del vel\n",
    "\n",
    "    # === 4. Oracle caches (one FP, four conditions) ===\n",
    "    oracle = build_oracle_caches(bare_cache, doc_len, query, passage_text,\n",
    "                                  model, tokenizer, config)\n",
    "\n",
    "    # oracle_vel (L0-16)\n",
    "    scores['oracle_vel'] = score_all_targets(\n",
    "        oracle['oracle_vel'], context_len, query, answer,\n",
    "        model, tokenizer, config)\n",
    "    del oracle['oracle_vel']\n",
    "\n",
    "    # oracle_vel_low (L0-8)\n",
    "    scores['oracle_vel_low'] = score_all_targets(\n",
    "        oracle['oracle_vel_low'], context_len, query, answer,\n",
    "        model, tokenizer, config)\n",
    "    del oracle['oracle_vel_low']\n",
    "\n",
    "    # oracle_interp\n",
    "    scores['oracle_interp'] = score_all_targets(\n",
    "        oracle['oracle_interp'], context_len, query, answer,\n",
    "        model, tokenizer, config)\n",
    "    del oracle['oracle_interp']\n",
    "\n",
    "    # oracle_full (different context_len!)\n",
    "    scores['oracle_full'] = score_all_targets(\n",
    "        oracle['oracle_full'], oracle['oracle_full_context_len'],\n",
    "        query, answer, model, tokenizer, config)\n",
    "    del oracle\n",
    "\n",
    "    # === 5. QVI caches ===\n",
    "    for alpha in QVI_ALPHAS:\n",
    "        cond_name = f'qvi_{int(alpha*100):03d}'\n",
    "        qvi = build_qvi_cache(bare_cache, query_cache, LAYER_CUTOFF, alpha)\n",
    "        scores[cond_name] = score_all_targets(\n",
    "            qvi, context_len, query, answer, model, tokenizer, config)\n",
    "        del qvi\n",
    "\n",
    "    # === 6. Cleanup ===\n",
    "    del bare_cache\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    return scores, doc_len\n",
    "\n",
    "\n",
    "# === Smoke test ===\n",
    "print(\"Smoke-testing helper functions...\")\n",
    "test_q = queries[0]\n",
    "test_query = test_q['query']\n",
    "test_answer = test_q['answer']\n",
    "test_passage = test_q['passages'][0]['passage']\n",
    "\n",
    "# Test baselines (3 targets)\n",
    "test_baselines = score_baselines(test_query, test_answer, model, tokenizer, exp_config)\n",
    "print(f\"  Baselines: {test_baselines}\")\n",
    "for tgt, nll in test_baselines.items():\n",
    "    assert np.isfinite(nll), f\"Baseline {tgt} NLL not finite: {nll}\"\n",
    "\n",
    "# Test query cache\n",
    "test_qcache, test_qlen = build_query_cache(test_query, model, tokenizer, exp_config)\n",
    "print(f\"  Query cache built: {test_qlen} tokens\")\n",
    "\n",
    "# Test full passage scoring (13 conditions \u00d7 3 targets)\n",
    "test_scores, test_doc_len = build_and_score_all_conditions(\n",
    "    test_passage, test_query, test_answer, test_qcache,\n",
    "    model, tokenizer, exp_config)\n",
    "print(f\"  Doc len: {test_doc_len}\")\n",
    "print(f\"  Conditions scored: {len(test_scores)}\")\n",
    "n_finite = 0\n",
    "for cond, targets in test_scores.items():\n",
    "    for tgt, nll in targets.items():\n",
    "        assert np.isfinite(nll), f\"{cond}/{tgt} NLL not finite: {nll}\"\n",
    "        n_finite += 1\n",
    "print(f\"  All {n_finite} NLL values are finite.\")\n",
    "\n",
    "# Print a summary\n",
    "print(f\"\\n  {'Condition':<20} {'answer':>10} {'qdoc':>10} {'relevance':>10}\")\n",
    "print(\"  \" + \"-\" * 52)\n",
    "for cond in CONDITION_NAMES:\n",
    "    t = test_scores[cond]\n",
    "    print(f\"  {cond:<20} {t['answer']:>10.4f} {t['qdoc']:>10.4f} {t['relevance']:>10.4f}\")\n",
    "\n",
    "del test_qcache, test_scores\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "print(f\"\\nSmoke test passed: 13 conditions \u00d7 3 targets = {n_finite} scores, all finite.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T23:26:22.443830Z",
     "iopub.status.busy": "2026-02-15T23:26:22.443550Z",
     "iopub.status.idle": "2026-02-16T05:23:47.839258Z",
     "shell.execute_reply": "2026-02-16T05:23:47.838558Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "MAIN EVALUATION (200 queries, ~1692 passages)\n",
      "Model: Gemma 3 4B | 13 conditions \u00d7 3 targets = 39 scores/passage\n",
      "======================================================================\n",
      "No checkpoint found. Starting fresh.\n",
      "Evaluating queries 0 to 199\n",
      "Per passage: 7 forward passes + 39 scoring calls\n",
      "Per query: +1 query FP + 3 baseline scores\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ff74aca9b8842d3a0d390a84b2c7d56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Queries:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 5/200 | 5 done in 8.7m | ETA: 339.6 min | GPU mem: 3.25 GB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 10/200 | 10 done in 17.5m | ETA: 333.4 min | GPU mem: 3.25 GB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 15/200 | 15 done in 26.2m | ETA: 323.3 min | GPU mem: 3.25 GB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 20/200 | 20 done in 36.0m | ETA: 324.2 min | GPU mem: 3.25 GB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 25/200 | 25 done in 45.4m | ETA: 317.6 min | GPU mem: 3.25 GB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 30/200 | 30 done in 53.6m | ETA: 303.9 min | GPU mem: 3.25 GB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 35/200 | 35 done in 62.7m | ETA: 295.7 min | GPU mem: 3.25 GB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 40/200 | 40 done in 72.0m | ETA: 288.1 min | GPU mem: 3.25 GB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 45/200 | 45 done in 80.5m | ETA: 277.2 min | GPU mem: 3.25 GB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 50/200 | 50 done in 89.1m | ETA: 267.4 min | GPU mem: 3.25 GB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 55/200 | 55 done in 98.3m | ETA: 259.2 min | GPU mem: 3.25 GB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 60/200 | 60 done in 107.7m | ETA: 251.3 min | GPU mem: 3.25 GB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 65/200 | 65 done in 116.4m | ETA: 241.8 min | GPU mem: 3.25 GB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 70/200 | 70 done in 125.7m | ETA: 233.4 min | GPU mem: 3.25 GB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 75/200 | 75 done in 133.5m | ETA: 222.5 min | GPU mem: 3.25 GB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 80/200 | 80 done in 143.2m | ETA: 214.9 min | GPU mem: 3.25 GB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 85/200 | 85 done in 152.0m | ETA: 205.7 min | GPU mem: 3.25 GB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 90/200 | 90 done in 160.9m | ETA: 196.7 min | GPU mem: 3.25 GB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 95/200 | 95 done in 169.6m | ETA: 187.4 min | GPU mem: 3.25 GB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 100/200 | 100 done in 178.0m | ETA: 178.0 min | GPU mem: 3.25 GB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 105/200 | 105 done in 187.2m | ETA: 169.4 min | GPU mem: 3.25 GB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 110/200 | 110 done in 196.7m | ETA: 160.9 min | GPU mem: 3.25 GB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 115/200 | 115 done in 205.4m | ETA: 151.8 min | GPU mem: 3.25 GB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 120/200 | 120 done in 214.8m | ETA: 143.2 min | GPU mem: 3.25 GB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 125/200 | 125 done in 224.1m | ETA: 134.5 min | GPU mem: 3.25 GB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 130/200 | 130 done in 233.1m | ETA: 125.5 min | GPU mem: 3.25 GB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 135/200 | 135 done in 242.0m | ETA: 116.5 min | GPU mem: 3.25 GB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 140/200 | 140 done in 250.6m | ETA: 107.4 min | GPU mem: 3.25 GB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 145/200 | 145 done in 258.8m | ETA: 98.2 min | GPU mem: 3.25 GB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 150/200 | 150 done in 267.7m | ETA: 89.2 min | GPU mem: 3.25 GB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 155/200 | 155 done in 276.3m | ETA: 80.2 min | GPU mem: 3.25 GB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 160/200 | 160 done in 285.5m | ETA: 71.4 min | GPU mem: 3.25 GB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 165/200 | 165 done in 293.5m | ETA: 62.3 min | GPU mem: 3.25 GB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 170/200 | 170 done in 303.2m | ETA: 53.5 min | GPU mem: 3.25 GB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 175/200 | 175 done in 312.1m | ETA: 44.6 min | GPU mem: 3.25 GB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 180/200 | 180 done in 321.1m | ETA: 35.7 min | GPU mem: 3.25 GB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 185/200 | 185 done in 330.3m | ETA: 26.8 min | GPU mem: 3.25 GB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 190/200 | 190 done in 338.8m | ETA: 17.8 min | GPU mem: 3.25 GB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 195/200 | 195 done in 348.4m | ETA: 8.9 min | GPU mem: 3.25 GB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 200/200 | 200 done in 357.4m | ETA: 0.0 min | GPU mem: 3.25 GB\n",
      "\n",
      "Evaluation complete: 200 queries in 357.4 min\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Evaluation Loop\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"MAIN EVALUATION ({N} queries, ~{total_passages} passages)\")\n",
    "print(\"Model: Gemma 3 4B | 13 conditions \u00d7 3 targets = 39 scores/passage\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Checkpoint resume\n",
    "all_results = []\n",
    "start_idx = 0\n",
    "\n",
    "if CHECKPOINT_PATH.exists():\n",
    "    with open(CHECKPOINT_PATH, 'r') as f:\n",
    "        ckpt = json.load(f)\n",
    "    ckpt_queries = ckpt.get('query_texts', [])\n",
    "    current_queries = [q['query'] for q in queries]\n",
    "    if ckpt_queries == current_queries:\n",
    "        all_results = ckpt['results']\n",
    "        start_idx = len(all_results)\n",
    "        print(f\"Resuming from checkpoint: {start_idx}/{N}\")\n",
    "    else:\n",
    "        print(\"Checkpoint query mismatch. Starting fresh.\")\n",
    "else:\n",
    "    print(\"No checkpoint found. Starting fresh.\")\n",
    "\n",
    "print(f\"Evaluating queries {start_idx} to {N-1}\")\n",
    "print(f\"Per passage: 7 forward passes + 39 scoring calls\")\n",
    "print(f\"Per query: +1 query FP + 3 baseline scores\")\n",
    "\n",
    "t_start = time.time()\n",
    "\n",
    "for qidx in tqdm(range(start_idx, N), initial=start_idx, total=N, desc=\"Queries\"):\n",
    "    query_data = queries[qidx]\n",
    "    query = query_data['query']\n",
    "    answer = query_data['answer']\n",
    "\n",
    "    # 1. Build query cache for QVI (once per query)\n",
    "    query_cache, query_len = build_query_cache(query, model, tokenizer, exp_config)\n",
    "\n",
    "    # 2. Compute baselines for all 3 targets (once per query)\n",
    "    baselines = score_baselines(query, answer, model, tokenizer, exp_config)\n",
    "\n",
    "    # 3. Score each passage (all 13 conditions \u00d7 3 targets)\n",
    "    passage_results = []\n",
    "    for pidx, pinfo in enumerate(query_data['passages']):\n",
    "        scores, doc_len = build_and_score_all_conditions(\n",
    "            pinfo['passage'], query, answer, query_cache,\n",
    "            model, tokenizer, exp_config)\n",
    "\n",
    "        # Flatten scores into per-passage dict\n",
    "        passage_row = {\n",
    "            'passage_idx': pinfo['passage_idx'],\n",
    "            'is_relevant': pinfo['is_relevant'],\n",
    "            'word_count': pinfo['word_count'],\n",
    "            'doc_len': doc_len,\n",
    "        }\n",
    "        for cond in CONDITION_NAMES:\n",
    "            for tgt in TARGET_NAMES:\n",
    "                passage_row[f'{cond}_{tgt}'] = scores[cond][tgt]\n",
    "        passage_results.append(passage_row)\n",
    "\n",
    "    # 4. Store query result\n",
    "    all_results.append({\n",
    "        'query_idx': qidx,\n",
    "        'query': query,\n",
    "        'answer': answer,\n",
    "        'n_passages': len(passage_results),\n",
    "        'n_relevant': query_data['n_relevant'],\n",
    "        'baselines': baselines,\n",
    "        'passage_data': passage_results,\n",
    "    })\n",
    "\n",
    "    # 5. Cleanup query cache\n",
    "    del query_cache\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    # 6. Checkpoint\n",
    "    if (qidx + 1) % CHECKPOINT_EVERY == 0 or qidx == N - 1:\n",
    "        ckpt_data = {\n",
    "            'results': all_results,\n",
    "            'query_texts': [q['query'] for q in queries],\n",
    "            'completed': len(all_results),\n",
    "            'total': N,\n",
    "            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        }\n",
    "        with open(CHECKPOINT_PATH, 'w') as f:\n",
    "            json.dump(ckpt_data, f)\n",
    "        elapsed = time.time() - t_start\n",
    "        n_done = qidx - start_idx + 1\n",
    "        rate = n_done / elapsed if elapsed > 0 else 0\n",
    "        remaining = (N - qidx - 1) / rate if rate > 0 else 0\n",
    "        gpu_mem = torch.cuda.memory_allocated() / 1e9 if torch.cuda.is_available() else 0\n",
    "        tqdm.write(f\"  Checkpoint {qidx+1}/{N} | {n_done} done in {elapsed/60:.1f}m | \"\n",
    "                   f\"ETA: {remaining/60:.1f} min | GPU mem: {gpu_mem:.2f} GB\")\n",
    "\n",
    "elapsed_total = time.time() - t_start\n",
    "print(f\"\\nEvaluation complete: {len(all_results)} queries in {elapsed_total/60:.1f} min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T05:23:47.843485Z",
     "iopub.status.busy": "2026-02-16T05:23:47.843212Z",
     "iopub.status.idle": "2026-02-16T05:23:48.299519Z",
     "shell.execute_reply": "2026-02-16T05:23:48.298850Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ANALYSIS \u2014 MULTI-SIGNAL RANKING\n",
      "======================================================================\n",
      "\n",
      "Total passages: 1692\n",
      "Relevant: 221 (13.1%)\n",
      "Irrelevant: 1471 (86.9%)\n",
      "\n",
      "======================================================================\n",
      "AUC-ROC: RAW NLL (lower = more relevant \u2192 negate for AUC)\n",
      "======================================================================\n",
      "\n",
      "Condition              Raw answer   PMI answer     Raw qdoc     PMI qdoc Raw relevance PMI relevance\n",
      "--------------------------------------------------------------------------------------------\n",
      "bare                        0.828        0.841        0.570        0.574        0.448        0.451\n",
      "fact_vel                    0.829        0.832        0.569        0.570        0.455        0.454\n",
      "def_vel                     0.827        0.835        0.567        0.568        0.455        0.456\n",
      "proc_vel                    0.830        0.834        0.567        0.568        0.457        0.462\n",
      "quant_vel                   0.831        0.830        0.566        0.565        0.462        0.466\n",
      "prob_vel                    0.829        0.830        0.569        0.569        0.456        0.459\n",
      "oracle_vel                  0.828        0.831        0.569        0.568        0.445        0.447\n",
      "oracle_vel_low              0.827        0.841        0.570        0.573        0.435        0.438\n",
      "oracle_interp               0.828        0.842        0.572        0.574        0.440        0.442\n",
      "oracle_full                 0.823        0.837"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        0.502        0.497        0.457        0.455\n",
      "qvi_010                     0.822        0.836        0.560        0.563        0.469        0.471\n",
      "qvi_025                     0.794        0.811        0.553        0.558        0.488        0.488\n",
      "qvi_050                     0.733        0.767        0.550        0.555"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        0.500        0.498\n",
      "\n",
      "======================================================================\n",
      "MRR@10\n",
      "======================================================================\n",
      "\n",
      "Condition              Raw answer   PMI answer     Raw qdoc     PMI qdoc Raw relevance PMI relevance\n",
      "--------------------------------------------------------------------------------------------\n",
      "bare                        0.860        0.860        0.436        0.436        0.298        0.298\n",
      "fact_vel                    0.853        0.853        0.462        0.462        0.306        0.306\n",
      "def_vel                     0.853        0.853        0.462        0.462        0.311        0.311\n",
      "proc_vel                    0.862        0.862        0.453        0.453        0.307        0.307\n",
      "quant_vel                   0.863        0.863        0.458        0.458        0.310        0.310\n",
      "prob_vel                    0.859        0.859        0.447        0.447        0.313        0.313\n",
      "oracle_vel                  0.865        0.865        0.459        0.459        0.286        0.286\n",
      "oracle_vel_low              0.864        0.864        0.447        0.447        0.286        0.286\n",
      "oracle_interp               0.861        0.861        0.447        0.447        0.292        0.292\n",
      "oracle_full                 0.850        0.850        0.326        0.326        0.291        0.291\n",
      "qvi_010                     0.847        0.847        0.458        0.458        0.321        0.321\n",
      "qvi_025                     0.840        0.840        0.471        0.471        0.348        0.348\n",
      "qvi_050                     0.835        0.835        0.475        0.475        0.330        0.330\n",
      "\n",
      "======================================================================\n",
      "DERIVED METHODS (analysis-time, no extra data)\n",
      "======================================================================\n",
      "\n",
      "--- Best-Intent Routing (min across 5 intents per passage) ---\n",
      "Target          AUC Raw    AUC PMI    MRR Raw    MRR PMI\n",
      "-------------------------------------------------------\n",
      "answer            0.831      0.825      0.855      0.855\n",
      "qdoc              0.569      0.567      0.465      0.465\n",
      "relevance         0.451      0.457      0.299      0.299\n",
      "\n",
      "--- Per-Query Intent Routing (pick best intent per query) ---\n",
      "Target          AUC Raw    AUC PMI    MRR Raw    MRR PMI\n",
      "-------------------------------------------------------\n",
      "answer            0.830      0.828      0.858      0.858\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qdoc              0.567      0.567      0.465      0.465\n",
      "relevance         0.447      0.453      0.295      0.295\n",
      "\n",
      "--- Two-Stage Pipeline: Bare PMI \u2192 Oracle VEL Re-rank ---\n",
      "Target          k     MRR@10\n",
      "------------------------------\n",
      "answer          3      0.866\n",
      "answer          5      0.865\n",
      "qdoc            3      0.443\n",
      "qdoc            5      0.460\n",
      "relevance       3      0.298\n",
      "relevance       5      0.292\n",
      "\n",
      "======================================================================\n",
      "BEST METHODS PER TARGET (PMI AUC)\n",
      "======================================================================\n",
      "\n",
      "  answer: best = oracle_interp (AUC = 0.842)\n",
      "    oracle_interp        0.842\n",
      "    bare                 0.841\n",
      "    oracle_vel_low       0.841\n",
      "    oracle_full          0.837\n",
      "    qvi_010              0.836\n",
      "\n",
      "  qdoc: best = oracle_interp (AUC = 0.574)\n",
      "    oracle_interp        0.574\n",
      "    bare                 0.574\n",
      "    oracle_vel_low       0.573\n",
      "    fact_vel             0.570\n",
      "    prob_vel             0.569\n",
      "\n",
      "  relevance: best = qvi_050 (AUC = 0.498)\n",
      "    qvi_050              0.498\n",
      "    qvi_025              0.488\n",
      "    qvi_010              0.471\n",
      "    quant_vel            0.466\n",
      "    proc_vel             0.462\n",
      "\n",
      "======================================================================\n",
      "HEAD-TO-HEAD vs BARE (per-query MRR wins/ties/losses, PMI answer target)\n",
      "======================================================================\n",
      "\n",
      "Condition              Wins   Ties Losses\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fact_vel                  6    185      9\n",
      "def_vel                   6    184     10\n",
      "proc_vel                  6    189      5\n",
      "quant_vel                 6    188      6\n",
      "prob_vel                  7    186      7\n",
      "oracle_vel                8    187      5\n",
      "oracle_vel_low            8    189      3\n",
      "oracle_interp             2    197      1\n",
      "oracle_full               7    185      8\n",
      "qvi_010                   6    183     11\n",
      "qvi_025                   7    176     17\n",
      "qvi_050                   9    172     19\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Analysis\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from scipy import stats\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ANALYSIS \u2014 MULTI-SIGNAL RANKING\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# --- Flatten passage-level data ---\n",
    "is_relevant_all = []\n",
    "nll_data = {f'{c}_{t}': [] for c in CONDITION_NAMES for t in TARGET_NAMES}\n",
    "baseline_data = {t: [] for t in TARGET_NAMES}\n",
    "\n",
    "for r in all_results:\n",
    "    bl = r['baselines']\n",
    "    for p in r['passage_data']:\n",
    "        is_relevant_all.append(int(p['is_relevant']))\n",
    "        for c in CONDITION_NAMES:\n",
    "            for t in TARGET_NAMES:\n",
    "                nll_data[f'{c}_{t}'].append(p[f'{c}_{t}'])\n",
    "        for t in TARGET_NAMES:\n",
    "            baseline_data[t].append(bl[t])\n",
    "\n",
    "is_relevant = np.array(is_relevant_all)\n",
    "n_total = len(is_relevant)\n",
    "n_rel = int(is_relevant.sum())\n",
    "n_irr = n_total - n_rel\n",
    "\n",
    "# Convert to numpy\n",
    "nll_arrays = {k: np.array(v) for k, v in nll_data.items()}\n",
    "baseline_arrays = {k: np.array(v) for k, v in baseline_data.items()}\n",
    "\n",
    "print(f\"\\nTotal passages: {n_total}\")\n",
    "print(f\"Relevant: {n_rel} ({100*n_rel/n_total:.1f}%)\")\n",
    "print(f\"Irrelevant: {n_irr} ({100*n_irr/n_total:.1f}%)\")\n",
    "\n",
    "# === Compute AUC for all 13\u00d73 raw + PMI ===\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"AUC-ROC: RAW NLL (lower = more relevant \u2192 negate for AUC)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "auc_raw = {}\n",
    "auc_pmi = {}\n",
    "\n",
    "print(f\"\\n{'Condition':<20}\", end=\"\")\n",
    "for t in TARGET_NAMES:\n",
    "    print(f\" {'Raw '+t:>12} {'PMI '+t:>12}\", end=\"\")\n",
    "print()\n",
    "print(\"-\" * 92)\n",
    "\n",
    "for c in CONDITION_NAMES:\n",
    "    print(f\"{c:<20}\", end=\"\")\n",
    "    for t in TARGET_NAMES:\n",
    "        key = f'{c}_{t}'\n",
    "        raw_scores = nll_arrays[key]\n",
    "        pmi_scores = raw_scores - baseline_arrays[t]\n",
    "        auc_r = roc_auc_score(is_relevant, -raw_scores)\n",
    "        auc_p = roc_auc_score(is_relevant, -pmi_scores)\n",
    "        auc_raw[key] = auc_r\n",
    "        auc_pmi[key] = auc_p\n",
    "        print(f\" {auc_r:>12.3f} {auc_p:>12.3f}\", end=\"\")\n",
    "    print()\n",
    "\n",
    "# === MRR@10 ===\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"MRR@10\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "def compute_mrr_at_k(all_results, score_key, baseline_key=None, k=10):\n",
    "    \"\"\"Compute MRR@k. If baseline_key, use PMI (score - baseline).\"\"\"\n",
    "    rr_list = []\n",
    "    for r in all_results:\n",
    "        passages = r['passage_data']\n",
    "        bl = r['baselines']\n",
    "        scored = []\n",
    "        for p in passages:\n",
    "            s = p[score_key]\n",
    "            if baseline_key:\n",
    "                s = s - bl[baseline_key]\n",
    "            scored.append((s, p['is_relevant']))\n",
    "        scored.sort(key=lambda x: x[0])  # lower = more relevant\n",
    "        rr = 0.0\n",
    "        for rank, (score, rel) in enumerate(scored[:k], 1):\n",
    "            if rel:\n",
    "                rr = 1.0 / rank\n",
    "                break\n",
    "        rr_list.append(rr)\n",
    "    return np.mean(rr_list), rr_list\n",
    "\n",
    "mrr_raw = {}\n",
    "mrr_pmi = {}\n",
    "\n",
    "print(f\"\\n{'Condition':<20}\", end=\"\")\n",
    "for t in TARGET_NAMES:\n",
    "    print(f\" {'Raw '+t:>12} {'PMI '+t:>12}\", end=\"\")\n",
    "print()\n",
    "print(\"-\" * 92)\n",
    "\n",
    "for c in CONDITION_NAMES:\n",
    "    print(f\"{c:<20}\", end=\"\")\n",
    "    for t in TARGET_NAMES:\n",
    "        key = f'{c}_{t}'\n",
    "        mrr_r, _ = compute_mrr_at_k(all_results, key, k=10)\n",
    "        mrr_p, _ = compute_mrr_at_k(all_results, key, baseline_key=t, k=10)\n",
    "        mrr_raw[key] = mrr_r\n",
    "        mrr_pmi[key] = mrr_p\n",
    "        print(f\" {mrr_r:>12.3f} {mrr_p:>12.3f}\", end=\"\")\n",
    "    print()\n",
    "\n",
    "# === Derived methods ===\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"DERIVED METHODS (analysis-time, no extra data)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "intent_conds = ['fact_vel', 'def_vel', 'proc_vel', 'quant_vel', 'prob_vel']\n",
    "\n",
    "# 1. Best-intent routing: min across 5 intents per passage\n",
    "print(\"\\n--- Best-Intent Routing (min across 5 intents per passage) ---\")\n",
    "print(f\"{'Target':<12} {'AUC Raw':>10} {'AUC PMI':>10} {'MRR Raw':>10} {'MRR PMI':>10}\")\n",
    "print(\"-\" * 55)\n",
    "\n",
    "derived_auc = {}\n",
    "derived_mrr = {}\n",
    "\n",
    "for t in TARGET_NAMES:\n",
    "    # Best-intent routing (raw): min NLL across intents per passage\n",
    "    intent_scores = np.stack([nll_arrays[f'{c}_{t}'] for c in intent_conds], axis=1)\n",
    "    best_raw = intent_scores.min(axis=1)\n",
    "    best_pmi = (intent_scores - baseline_arrays[t][:, None]).min(axis=1)\n",
    "    auc_r = roc_auc_score(is_relevant, -best_raw)\n",
    "    auc_p = roc_auc_score(is_relevant, -best_pmi)\n",
    "    derived_auc[f'best_intent_{t}_raw'] = auc_r\n",
    "    derived_auc[f'best_intent_{t}_pmi'] = auc_p\n",
    "\n",
    "    # MRR for best-intent: compute per-query\n",
    "    rr_raw_list, rr_pmi_list = [], []\n",
    "    for r in all_results:\n",
    "        bl = r['baselines'][t]\n",
    "        scored_raw, scored_pmi = [], []\n",
    "        for p in r['passage_data']:\n",
    "            vals = [p[f'{c}_{t}'] for c in intent_conds]\n",
    "            scored_raw.append((min(vals), p['is_relevant']))\n",
    "            scored_pmi.append((min(v - bl for v in vals), p['is_relevant']))\n",
    "        for scored, rr_list in [(scored_raw, rr_raw_list), (scored_pmi, rr_pmi_list)]:\n",
    "            scored.sort(key=lambda x: x[0])\n",
    "            rr = 0.0\n",
    "            for rank, (s, rel) in enumerate(scored[:10], 1):\n",
    "                if rel:\n",
    "                    rr = 1.0 / rank\n",
    "                    break\n",
    "            rr_list.append(rr)\n",
    "    mrr_r = np.mean(rr_raw_list)\n",
    "    mrr_p = np.mean(rr_pmi_list)\n",
    "    derived_mrr[f'best_intent_{t}_raw'] = mrr_r\n",
    "    derived_mrr[f'best_intent_{t}_pmi'] = mrr_p\n",
    "    print(f\"{t:<12} {auc_r:>10.3f} {auc_p:>10.3f} {mrr_r:>10.3f} {mrr_p:>10.3f}\")\n",
    "\n",
    "# 2. Per-query intent routing: pick best intent per query (by mean NLL)\n",
    "print(\"\\n--- Per-Query Intent Routing (pick best intent per query) ---\")\n",
    "print(f\"{'Target':<12} {'AUC Raw':>10} {'AUC PMI':>10} {'MRR Raw':>10} {'MRR PMI':>10}\")\n",
    "print(\"-\" * 55)\n",
    "\n",
    "for t in TARGET_NAMES:\n",
    "    # For each query, find which intent has lowest mean NLL\n",
    "    routed_raw = []\n",
    "    routed_pmi = []\n",
    "    rr_raw_list, rr_pmi_list = [], []\n",
    "\n",
    "    for r in all_results:\n",
    "        bl = r['baselines'][t]\n",
    "        # Mean NLL per intent for this query\n",
    "        mean_nlls = {}\n",
    "        for c in intent_conds:\n",
    "            mean_nlls[c] = np.mean([p[f'{c}_{t}'] for p in r['passage_data']])\n",
    "        best_intent = min(mean_nlls, key=mean_nlls.get)\n",
    "\n",
    "        scored_raw, scored_pmi = [], []\n",
    "        for p in r['passage_data']:\n",
    "            s = p[f'{best_intent}_{t}']\n",
    "            routed_raw.append(s)\n",
    "            routed_pmi.append(s - bl)\n",
    "            scored_raw.append((s, p['is_relevant']))\n",
    "            scored_pmi.append((s - bl, p['is_relevant']))\n",
    "\n",
    "        for scored, rr_list in [(scored_raw, rr_raw_list), (scored_pmi, rr_pmi_list)]:\n",
    "            scored.sort(key=lambda x: x[0])\n",
    "            rr = 0.0\n",
    "            for rank, (s, rel) in enumerate(scored[:10], 1):\n",
    "                if rel:\n",
    "                    rr = 1.0 / rank\n",
    "                    break\n",
    "            rr_list.append(rr)\n",
    "\n",
    "    routed_raw = np.array(routed_raw)\n",
    "    routed_pmi = np.array(routed_pmi)\n",
    "    auc_r = roc_auc_score(is_relevant, -routed_raw)\n",
    "    auc_p = roc_auc_score(is_relevant, -routed_pmi)\n",
    "    mrr_r = np.mean(rr_raw_list)\n",
    "    mrr_p = np.mean(rr_pmi_list)\n",
    "    derived_auc[f'query_route_{t}_raw'] = auc_r\n",
    "    derived_auc[f'query_route_{t}_pmi'] = auc_p\n",
    "    derived_mrr[f'query_route_{t}_raw'] = mrr_r\n",
    "    derived_mrr[f'query_route_{t}_pmi'] = mrr_p\n",
    "    print(f\"{t:<12} {auc_r:>10.3f} {auc_p:>10.3f} {mrr_r:>10.3f} {mrr_p:>10.3f}\")\n",
    "\n",
    "# 3. Two-stage pipeline: bare PMI \u2192 re-rank top-k with oracle_vel PMI\n",
    "print(\"\\n--- Two-Stage Pipeline: Bare PMI \u2192 Oracle VEL Re-rank ---\")\n",
    "print(f\"{'Target':<12} {'k':>4} {'MRR@10':>10}\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "two_stage_mrr = {}\n",
    "for t in TARGET_NAMES:\n",
    "    for k in [3, 5]:\n",
    "        rr_list = []\n",
    "        for r in all_results:\n",
    "            bl = r['baselines'][t]\n",
    "            # Stage 1: rank by bare PMI\n",
    "            bare_scored = []\n",
    "            for p in r['passage_data']:\n",
    "                bare_pmi = p[f'bare_{t}'] - bl\n",
    "                oracle_pmi = p[f'oracle_vel_{t}'] - bl\n",
    "                bare_scored.append((bare_pmi, oracle_pmi, p['is_relevant']))\n",
    "            bare_scored.sort(key=lambda x: x[0])  # lower = more relevant\n",
    "\n",
    "            # Stage 2: re-rank top-k by oracle_vel PMI\n",
    "            top_k = bare_scored[:k]\n",
    "            rest = bare_scored[k:]\n",
    "            top_k.sort(key=lambda x: x[1])  # re-rank by oracle PMI\n",
    "            reranked = top_k + rest\n",
    "\n",
    "            rr = 0.0\n",
    "            for rank, (_, _, rel) in enumerate(reranked[:10], 1):\n",
    "                if rel:\n",
    "                    rr = 1.0 / rank\n",
    "                    break\n",
    "            rr_list.append(rr)\n",
    "        mrr = np.mean(rr_list)\n",
    "        two_stage_mrr[f'{t}_k{k}'] = mrr\n",
    "        print(f\"{t:<12} {k:>4} {mrr:>10.3f}\")\n",
    "\n",
    "# === Summary: Best method per target ===\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"BEST METHODS PER TARGET (PMI AUC)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for t in TARGET_NAMES:\n",
    "    # Gather all AUC PMI values for this target\n",
    "    target_aucs = {c: auc_pmi[f'{c}_{t}'] for c in CONDITION_NAMES}\n",
    "    # Add derived\n",
    "    for key in ['best_intent', 'query_route']:\n",
    "        if f'{key}_{t}_pmi' in derived_auc:\n",
    "            target_aucs[key] = derived_auc[f'{key}_{t}_pmi']\n",
    "    best_name = max(target_aucs, key=target_aucs.get)\n",
    "    print(f\"\\n  {t}: best = {best_name} (AUC = {target_aucs[best_name]:.3f})\")\n",
    "    sorted_aucs = sorted(target_aucs.items(), key=lambda x: x[1], reverse=True)\n",
    "    for name, auc in sorted_aucs[:5]:\n",
    "        print(f\"    {name:<20} {auc:.3f}\")\n",
    "\n",
    "# === Head-to-head: each condition vs bare ===\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"HEAD-TO-HEAD vs BARE (per-query MRR wins/ties/losses, PMI answer target)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\n{'Condition':<20} {'Wins':>6} {'Ties':>6} {'Losses':>6}\")\n",
    "print(\"-\" * 42)\n",
    "\n",
    "t = 'answer'  # primary target\n",
    "bare_rr = []\n",
    "cond_rr = {c: [] for c in CONDITION_NAMES if c != 'bare'}\n",
    "\n",
    "for r in all_results:\n",
    "    bl = r['baselines'][t]\n",
    "    # Bare RR\n",
    "    scored = [(p[f'bare_{t}'] - bl, p['is_relevant']) for p in r['passage_data']]\n",
    "    scored.sort(key=lambda x: x[0])\n",
    "    rr = 0.0\n",
    "    for rank, (s, rel) in enumerate(scored[:10], 1):\n",
    "        if rel:\n",
    "            rr = 1.0 / rank\n",
    "            break\n",
    "    bare_rr.append(rr)\n",
    "\n",
    "    for c in cond_rr:\n",
    "        scored = [(p[f'{c}_{t}'] - bl, p['is_relevant']) for p in r['passage_data']]\n",
    "        scored.sort(key=lambda x: x[0])\n",
    "        rr = 0.0\n",
    "        for rank, (s, rel) in enumerate(scored[:10], 1):\n",
    "            if rel:\n",
    "                rr = 1.0 / rank\n",
    "                break\n",
    "        cond_rr[c].append(rr)\n",
    "\n",
    "bare_rr = np.array(bare_rr)\n",
    "for c in CONDITION_NAMES:\n",
    "    if c == 'bare':\n",
    "        continue\n",
    "    c_rr = np.array(cond_rr[c])\n",
    "    wins = int(np.sum(c_rr > bare_rr))\n",
    "    ties = int(np.sum(c_rr == bare_rr))\n",
    "    losses = int(np.sum(c_rr < bare_rr))\n",
    "    print(f\"{c:<20} {wins:>6} {ties:>6} {losses:>6}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T05:23:48.302652Z",
     "iopub.status.busy": "2026-02-16T05:23:48.302370Z",
     "iopub.status.idle": "2026-02-16T05:23:50.671963Z",
     "shell.execute_reply": "2026-02-16T05:23:50.671226Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved heatmap.png\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved auc_answer.png\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved qvi_alpha.png\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved two_stage.png\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved score_distributions.png\n",
      "\n",
      "All figures saved to results/exp23/figures\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Figures\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig_dir = RESULTS_DIR / 'figures'\n",
    "\n",
    "# === Figure 1: Heatmap \u2014 13 conditions \u00d7 3 targets (AUC PMI) ===\n",
    "fig1, ax1 = plt.subplots(figsize=(8, 7))\n",
    "heatmap_data = np.zeros((len(CONDITION_NAMES), len(TARGET_NAMES)))\n",
    "for i, c in enumerate(CONDITION_NAMES):\n",
    "    for j, t in enumerate(TARGET_NAMES):\n",
    "        heatmap_data[i, j] = auc_pmi[f'{c}_{t}']\n",
    "\n",
    "im = ax1.imshow(heatmap_data, cmap='YlOrRd', aspect='auto', vmin=0.5, vmax=1.0)\n",
    "ax1.set_xticks(range(len(TARGET_NAMES)))\n",
    "ax1.set_xticklabels(TARGET_NAMES, fontsize=10)\n",
    "ax1.set_yticks(range(len(CONDITION_NAMES)))\n",
    "ax1.set_yticklabels(CONDITION_NAMES, fontsize=9)\n",
    "ax1.set_title('AUC-ROC (PMI) \u2014 13 Conditions \u00d7 3 Targets', fontsize=12)\n",
    "for i in range(len(CONDITION_NAMES)):\n",
    "    for j in range(len(TARGET_NAMES)):\n",
    "        ax1.text(j, i, f'{heatmap_data[i, j]:.3f}', ha='center', va='center',\n",
    "                fontsize=8, color='black' if heatmap_data[i, j] < 0.85 else 'white')\n",
    "plt.colorbar(im, ax=ax1, label='AUC')\n",
    "fig1.tight_layout()\n",
    "fig1.savefig(fig_dir / 'heatmap.png', dpi=150, bbox_inches='tight')\n",
    "plt.close(fig1)\n",
    "print(\"Saved heatmap.png\")\n",
    "\n",
    "# === Figure 2: Bar chart \u2014 AUC for all conditions under `answer` target ===\n",
    "fig2, ax2 = plt.subplots(figsize=(12, 5))\n",
    "t = 'answer'\n",
    "aucs_answer = [auc_pmi[f'{c}_{t}'] for c in CONDITION_NAMES]\n",
    "colors_bar = ['#1f77b4'] + ['#2ca02c']*5 + ['#ff7f0e']*4 + ['#d62728']*3\n",
    "bars = ax2.bar(range(len(CONDITION_NAMES)), aucs_answer, color=colors_bar, alpha=0.8)\n",
    "ax2.set_xticks(range(len(CONDITION_NAMES)))\n",
    "ax2.set_xticklabels(CONDITION_NAMES, rotation=45, ha='right', fontsize=9)\n",
    "ax2.set_ylabel('AUC (PMI)')\n",
    "ax2.set_title('AUC-ROC (PMI, answer target) by Condition')\n",
    "ax2.axhline(y=auc_pmi[f'bare_{t}'], color='gray', linestyle='--', alpha=0.7, label='bare')\n",
    "ax2.legend(fontsize=9)\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "for bar, val in zip(bars, aucs_answer):\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.002,\n",
    "            f'{val:.3f}', ha='center', va='bottom', fontsize=7)\n",
    "fig2.tight_layout()\n",
    "fig2.savefig(fig_dir / 'auc_answer.png', dpi=150, bbox_inches='tight')\n",
    "plt.close(fig2)\n",
    "print(\"Saved auc_answer.png\")\n",
    "\n",
    "# === Figure 3: QVI alpha sensitivity curve ===\n",
    "fig3, ax3 = plt.subplots(figsize=(7, 5))\n",
    "for t in TARGET_NAMES:\n",
    "    alphas = QVI_ALPHAS\n",
    "    aucs_qvi = [auc_pmi[f'qvi_{int(a*100):03d}_{t}'] for a in alphas]\n",
    "    ax3.plot(alphas, aucs_qvi, 'o-', label=f'{t}', linewidth=2, markersize=8)\n",
    "    # Add bare baseline\n",
    "    ax3.axhline(y=auc_pmi[f'bare_{t}'], linestyle='--', alpha=0.3)\n",
    "ax3.set_xlabel('QVI Alpha')\n",
    "ax3.set_ylabel('AUC (PMI)')\n",
    "ax3.set_title('QVI: AUC vs Blending Alpha')\n",
    "ax3.legend(fontsize=9)\n",
    "ax3.grid(alpha=0.3)\n",
    "fig3.tight_layout()\n",
    "fig3.savefig(fig_dir / 'qvi_alpha.png', dpi=150, bbox_inches='tight')\n",
    "plt.close(fig3)\n",
    "print(\"Saved qvi_alpha.png\")\n",
    "\n",
    "# === Figure 4: Two-stage pipeline MRR ===\n",
    "fig4, ax4 = plt.subplots(figsize=(7, 5))\n",
    "for t in TARGET_NAMES:\n",
    "    ks = [3, 5]\n",
    "    mrrs_ts = [two_stage_mrr[f'{t}_k{k}'] for k in ks]\n",
    "    ax4.plot(ks, mrrs_ts, 'o-', label=f'{t}', linewidth=2, markersize=8)\n",
    "    # Bare PMI MRR baseline\n",
    "    bare_mrr_val = mrr_pmi[f'bare_{t}']\n",
    "    ax4.axhline(y=bare_mrr_val, linestyle='--', alpha=0.3)\n",
    "ax4.set_xlabel('Top-k cutoff')\n",
    "ax4.set_ylabel('MRR@10')\n",
    "ax4.set_title('Two-Stage Pipeline: Bare PMI \u2192 Oracle VEL Re-rank')\n",
    "ax4.legend(fontsize=9)\n",
    "ax4.grid(alpha=0.3)\n",
    "ax4.set_xticks(ks)\n",
    "fig4.tight_layout()\n",
    "fig4.savefig(fig_dir / 'two_stage.png', dpi=150, bbox_inches='tight')\n",
    "plt.close(fig4)\n",
    "print(\"Saved two_stage.png\")\n",
    "\n",
    "# === Figure 5: Score distributions \u2014 best condition vs bare (violin plots) ===\n",
    "fig5, axes5 = plt.subplots(1, 3, figsize=(15, 5))\n",
    "for idx, t in enumerate(TARGET_NAMES):\n",
    "    ax = axes5[idx]\n",
    "    # Find best non-bare condition for this target (PMI AUC)\n",
    "    best_cond = max(\n",
    "        [c for c in CONDITION_NAMES if c != 'bare'],\n",
    "        key=lambda c: auc_pmi[f'{c}_{t}']\n",
    "    )\n",
    "    bare_pmi = nll_arrays[f'bare_{t}'] - baseline_arrays[t]\n",
    "    best_pmi = nll_arrays[f'{best_cond}_{t}'] - baseline_arrays[t]\n",
    "\n",
    "    data = [\n",
    "        bare_pmi[is_relevant == 1], bare_pmi[is_relevant == 0],\n",
    "        best_pmi[is_relevant == 1], best_pmi[is_relevant == 0],\n",
    "    ]\n",
    "    labels = ['bare\\nRel', 'bare\\nIrr', f'{best_cond}\\nRel', f'{best_cond}\\nIrr']\n",
    "    vp = ax.violinplot(data, positions=[1, 2, 3, 4], showmeans=True, showmedians=True)\n",
    "    ax.set_xticks([1, 2, 3, 4])\n",
    "    ax.set_xticklabels(labels, fontsize=8)\n",
    "    ax.set_ylabel('PMI Score')\n",
    "    ax.set_title(f'{t} (best: {best_cond})', fontsize=10)\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "fig5.suptitle('PMI Score Distributions: Best Condition vs Bare', fontsize=12)\n",
    "fig5.tight_layout()\n",
    "fig5.savefig(fig_dir / 'score_distributions.png', dpi=150, bbox_inches='tight')\n",
    "plt.close(fig5)\n",
    "print(\"Saved score_distributions.png\")\n",
    "\n",
    "print(f\"\\nAll figures saved to {fig_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T05:23:50.675601Z",
     "iopub.status.busy": "2026-02-16T05:23:50.675048Z",
     "iopub.status.idle": "2026-02-16T05:23:50.969674Z",
     "shell.execute_reply": "2026-02-16T05:23:50.968944Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV saved to results/exp23/passage_scores.csv (1692 rows)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON saved to results/exp23/results.json (3048.7 KB)\n",
      "\n",
      "======================================================================\n",
      "FINAL SUMMARY \u2014 Exp 23\n",
      "======================================================================\n",
      "Model: Gemma 3 4B | 13 conditions \u00d7 3 targets = 39 scores/passage\n",
      "Dataset: MS MARCO v1.1 (200 queries, 1692 passages)\n",
      "\n",
      "Best AUC (PMI) per target:\n",
      "  answer: bare=0.841 \u2192 best=oracle_interp (0.842)\n",
      "  qdoc: bare=0.574 \u2192 best=oracle_interp (0.574)\n",
      "  relevance: bare=0.451 \u2192 best=qvi_050 (0.498)\n",
      "\n",
      "Two-stage pipeline MRR@10 (answer target):\n",
      "  k=3: 0.866\n",
      "  k=5: 0.865\n",
      "  bare PMI: 0.860\n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Save CSV + JSON\n",
    "import csv\n",
    "\n",
    "# --- CSV: one row per passage with all 39 NLL scores + 3 baselines ---\n",
    "csv_rows = []\n",
    "for r in all_results:\n",
    "    bl = r['baselines']\n",
    "    for p in r['passage_data']:\n",
    "        row = {\n",
    "            'query_idx': r['query_idx'],\n",
    "            'passage_idx': p['passage_idx'],\n",
    "            'is_relevant': int(p['is_relevant']),\n",
    "            'word_count': p['word_count'],\n",
    "            'doc_len': p['doc_len'],\n",
    "        }\n",
    "        # 3 baselines\n",
    "        for t in TARGET_NAMES:\n",
    "            row[f'baseline_{t}'] = bl[t]\n",
    "        # 39 NLL scores\n",
    "        for c in CONDITION_NAMES:\n",
    "            for t in TARGET_NAMES:\n",
    "                row[f'{c}_{t}'] = p[f'{c}_{t}']\n",
    "        csv_rows.append(row)\n",
    "\n",
    "with open(CSV_PATH, 'w', newline='') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=csv_rows[0].keys())\n",
    "    writer.writeheader()\n",
    "    writer.writerows(csv_rows)\n",
    "print(f\"CSV saved to {CSV_PATH} ({len(csv_rows)} rows)\")\n",
    "\n",
    "# --- JSON: full analysis + per-query results ---\n",
    "final = {\n",
    "    'experiment': 'exp23_multi_signal_ranking',\n",
    "    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'config': {\n",
    "        'model_name': MODEL_NAME,\n",
    "        'model_type': 'gemma3',\n",
    "        'seed': SEED,\n",
    "        'n_queries': N,\n",
    "        'total_passages': n_total,\n",
    "        'n_relevant': n_rel,\n",
    "        'n_irrelevant': n_irr,\n",
    "        'layer_cutoff': LAYER_CUTOFF,\n",
    "        'layer_cutoff_low': LAYER_CUTOFF_LOW,\n",
    "        'qvi_alphas': QVI_ALPHAS,\n",
    "        'interp_alpha': INTERP_ALPHA,\n",
    "        'max_passage_words': MAX_PASSAGE_WORDS,\n",
    "        'dataset': 'MS MARCO v1.1 validation',\n",
    "        'conditions': CONDITION_NAMES,\n",
    "        'targets': TARGET_NAMES,\n",
    "        'intent_prefixes': INTENT_PREFIXES,\n",
    "    },\n",
    "    'analysis': {\n",
    "        'auc_raw': {k: float(v) for k, v in auc_raw.items()},\n",
    "        'auc_pmi': {k: float(v) for k, v in auc_pmi.items()},\n",
    "        'mrr_raw': {k: float(v) for k, v in mrr_raw.items()},\n",
    "        'mrr_pmi': {k: float(v) for k, v in mrr_pmi.items()},\n",
    "        'derived_auc': {k: float(v) for k, v in derived_auc.items()},\n",
    "        'derived_mrr': {k: float(v) for k, v in derived_mrr.items()},\n",
    "        'two_stage_mrr': {k: float(v) for k, v in two_stage_mrr.items()},\n",
    "    },\n",
    "    'per_query_results': all_results,\n",
    "}\n",
    "\n",
    "with open(FINAL_RESULTS_PATH, 'w') as f:\n",
    "    json.dump(final, f, indent=2)\n",
    "print(f\"JSON saved to {FINAL_RESULTS_PATH} ({FINAL_RESULTS_PATH.stat().st_size / 1024:.1f} KB)\")\n",
    "\n",
    "# --- Final Summary ---\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"FINAL SUMMARY \u2014 Exp 23\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Model: Gemma 3 4B | 13 conditions \u00d7 3 targets = 39 scores/passage\")\n",
    "print(f\"Dataset: MS MARCO v1.1 ({N} queries, {n_total} passages)\")\n",
    "print(f\"\")\n",
    "print(f\"Best AUC (PMI) per target:\")\n",
    "for t in TARGET_NAMES:\n",
    "    target_aucs = {c: auc_pmi[f'{c}_{t}'] for c in CONDITION_NAMES}\n",
    "    best = max(target_aucs, key=target_aucs.get)\n",
    "    bare_auc = auc_pmi[f'bare_{t}']\n",
    "    print(f\"  {t}: bare={bare_auc:.3f} \u2192 best={best} ({target_aucs[best]:.3f})\")\n",
    "print(f\"\")\n",
    "print(f\"Two-stage pipeline MRR@10 (answer target):\")\n",
    "for k in [3, 5]:\n",
    "    print(f\"  k={k}: {two_stage_mrr[f'answer_k{k}']:.3f}\")\n",
    "print(f\"  bare PMI: {mrr_pmi['bare_answer']:.3f}\")\n",
    "print(\"\\nDone!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "10356b014d10436ea00f0ba7749fb438": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "137906774420476fb504234f13655f41": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "1bf7af304cd7446c9c7d42b44f55f55c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "22a1fa2340d841d18521a1e53b2406a7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_4d169235639f4476adbe544a9f5756eb",
       "placeholder": "\u200b",
       "style": "IPY_MODEL_e2bb2fc2e5cb45108f58df8d6bd3df0e",
       "tabbable": null,
       "tooltip": null,
       "value": "\u2007618/10047\u2007[00:00&lt;00:01,\u20075883.64it/s]"
      }
     },
     "25ae87ebf7f947fdbcf15094f1efaa4d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "288b239575864607aa01401394f1bac1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "30263886684047c1b6f8594397e4666c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_df8ecdab01dd4a289809372d2aa46ba4",
        "IPY_MODEL_fbf1916e1aaf47d28cb99c6248a0231e",
        "IPY_MODEL_22a1fa2340d841d18521a1e53b2406a7"
       ],
       "layout": "IPY_MODEL_288b239575864607aa01401394f1bac1",
       "tabbable": null,
       "tooltip": null
      }
     },
     "30670c9e21f34344828ec4733df0f78d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "40254a7aef7a40888fb4a6e143db887a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_6f8d11e5c67e4be983061dc0f2c82f98",
       "max": 883.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_10356b014d10436ea00f0ba7749fb438",
       "tabbable": null,
       "tooltip": null,
       "value": 883.0
      }
     },
     "4593920f7b9e496e862bd864488551fa": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "45e556d733854180ba7daf5115f2e369": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_aaa3d9d6b62d470b98bf1218f1a04e5f",
        "IPY_MODEL_40254a7aef7a40888fb4a6e143db887a",
        "IPY_MODEL_54f58fa255ad4c979dd079113d79e0b2"
       ],
       "layout": "IPY_MODEL_30670c9e21f34344828ec4733df0f78d",
       "tabbable": null,
       "tooltip": null
      }
     },
     "46b2de968b3f436694f83e3c87a8ec12": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "4d169235639f4476adbe544a9f5756eb": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "54f58fa255ad4c979dd079113d79e0b2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_c64ce698dcdf4d4b984a1c1bf68e3c78",
       "placeholder": "\u200b",
       "style": "IPY_MODEL_9c191f80a92a4fac86c23f2b74c483d4",
       "tabbable": null,
       "tooltip": null,
       "value": "\u2007883/883\u2007[00:03&lt;00:00,\u2007795.68it/s,\u2007Materializing\u2007param=model.vision_tower.vision_model.post_layernorm.weight]"
      }
     },
     "570771bf5b5c4f519872b8ee3069f89a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6f8d11e5c67e4be983061dc0f2c82f98": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "77185fd261744cc5b39962465c9906bc": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "77f6e87f61c54a08ba54dc88bbdf0362": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "86726c52602a4dfbb3587d384c94df0a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_77185fd261744cc5b39962465c9906bc",
       "max": 200.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_b7f5d16f679441e1b7de4b5787e4f3e6",
       "tabbable": null,
       "tooltip": null,
       "value": 200.0
      }
     },
     "94ecdebecfbf42cd94cbb1869ce4344b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "9c191f80a92a4fac86c23f2b74c483d4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "9ff74aca9b8842d3a0d390a84b2c7d56": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_b29664146734471a9ca7d5aeda80be3b",
        "IPY_MODEL_86726c52602a4dfbb3587d384c94df0a",
        "IPY_MODEL_a47ce628dd4c4379a15bb711d093b171"
       ],
       "layout": "IPY_MODEL_a14f70859ce84277b57ced52f9c49b45",
       "tabbable": null,
       "tooltip": null
      }
     },
     "a14f70859ce84277b57ced52f9c49b45": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a47ce628dd4c4379a15bb711d093b171": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_137906774420476fb504234f13655f41",
       "placeholder": "\u200b",
       "style": "IPY_MODEL_fcdb5b98de264814ab1cd61d7089b044",
       "tabbable": null,
       "tooltip": null,
       "value": "\u2007200/200\u2007[5:57:25&lt;00:00,\u2007105.58s/it]"
      }
     },
     "aaa3d9d6b62d470b98bf1218f1a04e5f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_94ecdebecfbf42cd94cbb1869ce4344b",
       "placeholder": "\u200b",
       "style": "IPY_MODEL_77f6e87f61c54a08ba54dc88bbdf0362",
       "tabbable": null,
       "tooltip": null,
       "value": "Loading\u2007weights:\u2007100%"
      }
     },
     "b29664146734471a9ca7d5aeda80be3b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_570771bf5b5c4f519872b8ee3069f89a",
       "placeholder": "\u200b",
       "style": "IPY_MODEL_4593920f7b9e496e862bd864488551fa",
       "tabbable": null,
       "tooltip": null,
       "value": "Queries:\u2007100%"
      }
     },
     "b7f5d16f679441e1b7de4b5787e4f3e6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "bf0647decb424bdc84d44d5105f1838e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c64ce698dcdf4d4b984a1c1bf68e3c78": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "df8ecdab01dd4a289809372d2aa46ba4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_bf0647decb424bdc84d44d5105f1838e",
       "placeholder": "\u200b",
       "style": "IPY_MODEL_46b2de968b3f436694f83e3c87a8ec12",
       "tabbable": null,
       "tooltip": null,
       "value": "Filtering:\u2007\u2007\u20076%"
      }
     },
     "e2bb2fc2e5cb45108f58df8d6bd3df0e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "fbf1916e1aaf47d28cb99c6248a0231e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "danger",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_1bf7af304cd7446c9c7d42b44f55f55c",
       "max": 10047.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_25ae87ebf7f947fdbcf15094f1efaa4d",
       "tabbable": null,
       "tooltip": null,
       "value": 618.0
      }
     },
     "fcdb5b98de264814ab1cd61d7089b044": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}