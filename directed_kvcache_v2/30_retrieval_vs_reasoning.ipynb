{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exp 30: Retrieval vs Reasoning Task-Type Dissociation (Gemma 3 4B)\n",
    "\n",
    "## Motivation\n",
    "\n",
    "Exp 29 showed hero layers significantly **hurt** on DROP (d=-0.152, p=0.009) but were neutral on\n",
    "AdversarialQA/CoQA. The hypothesis is that priming helps *retrieval* but not *reasoning/computation*.\n",
    "However, this conclusion rests on a single dataset per type.\n",
    "\n",
    "This experiment provides a clean test: **does task type predict hero layer effect beyond difficulty?**\n",
    "\n",
    "## Dataset Taxonomy\n",
    "\n",
    "| Dataset | Task Type | Why Chosen | N |\n",
    "|---------|-----------|-----------|---|\n",
    "| **NQ** | Retrieval (factoid) | Known positive control \u2014 hero d=+0.213 in Exp 27b | 300 |\n",
    "| **DROP** | Mixed (computation + extraction) | Known negative \u2014 hero d=-0.152 in Exp 29 | 300 |\n",
    "| **BoolQ** | Retrieval (binary judgment) | New \u2014 pure passage-based yes/no, second retrieval data point | 300 |\n",
    "\n",
    "DROP gets split by answer type (number vs span) for within-dataset comparison:\n",
    "- **DROP-number**: Computational answers (counting, arithmetic) \u2192 tagged \"computation\"\n",
    "- **DROP-span**: Extractive answers \u2192 tagged \"retrieval\"\n",
    "\n",
    "## Conditions (reduced to 4)\n",
    "\n",
    "| # | Condition | Description |\n",
    "|---|-----------|-------------|\n",
    "| 1 | bare | Baseline |\n",
    "| 2 | sf_trunc | Standard priming (truncate + RoPE correct) |\n",
    "| 3 | values_early | Bare keys + primed values layers 0-15 |\n",
    "| 4 | values_hero | Bare keys + primed values at hero layers {10,12,14,15,20} |\n",
    "\n",
    "Dropped sf_trunc_bias2 (always hurts on these datasets) and values_only (dominated by values_early).\n",
    "\n",
    "## Key Question\n",
    "\n",
    "Does task type predict hero layer effect **beyond** difficulty?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 1: Setup\n",
    "import os\n",
    "os.umask(0o000)\n",
    "\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "import gc\n",
    "import csv\n",
    "import numpy as np\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "RESULTS_DIR = Path(\"results/exp30\")\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CHECKPOINT_PATH = RESULTS_DIR / \"checkpoint.json\"\n",
    "FINAL_RESULTS_PATH = RESULTS_DIR / \"results.json\"\n",
    "CSV_PATH = RESULTS_DIR / \"results.csv\"\n",
    "\n",
    "print(f\"SEED: {SEED}\")\n",
    "print(f\"Results directory: {RESULTS_DIR}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 2: Load Gemma 3 4B via load_model()\n",
    "sys.path.insert(0, \".\")\n",
    "\n",
    "from lib.config import ExperimentConfig\n",
    "from lib.model_utils import load_model\n",
    "\n",
    "MODEL_NAME = \"google/gemma-3-4b-it\"\n",
    "\n",
    "exp_config = ExperimentConfig(\n",
    "    model_name=MODEL_NAME,\n",
    "    model_type=\"gemma3\",\n",
    "    compute_dtype=\"auto\",  # resolves to bfloat16 for Gemma\n",
    "    use_4bit=True,\n",
    "    num_samples=2000,\n",
    "    seed=SEED,\n",
    ")\n",
    "\n",
    "print(f\"Loading {MODEL_NAME} (4-bit, bfloat16)...\")\n",
    "model, tokenizer = load_model(exp_config)\n",
    "\n",
    "from lib.kv_cache import _get_text_config, _get_head_dim, _ensure_dynamic_cache, _get_cache_keys\n",
    "\n",
    "text_config = _get_text_config(model.config)\n",
    "N_LAYERS = text_config.num_hidden_layers\n",
    "print(f\"\\nModel loaded successfully.\")\n",
    "print(f\"  Num layers: {N_LAYERS}\")\n",
    "print(f\"  Head dim: {_get_head_dim(model.config)}\")\n",
    "print(f\"  Model dtype: {model.dtype}\")\n",
    "print(f\"  Sliding window: {getattr(text_config, 'sliding_window', 'N/A')}\")\n",
    "\n",
    "# Verify with test forward pass\n",
    "sample_ids = tokenizer(\"test\", return_tensors=\"pt\")['input_ids'].to(exp_config.device)\n",
    "with torch.no_grad():\n",
    "    out = model(sample_ids, use_cache=True)\n",
    "    cache_check = _ensure_dynamic_cache(out.past_key_values)\n",
    "    k0 = _get_cache_keys(cache_check, 0)\n",
    "    print(f\"  Cache key dtype: {k0.dtype}\")\n",
    "    print(f\"  Cache key shape: {k0.shape}\")\n",
    "del out, sample_ids, cache_check\n",
    "torch.cuda.empty_cache()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 3: Lib imports + constants\n",
    "from lib.kv_cache import (\n",
    "    _get_cache_keys,\n",
    "    _get_cache_values,\n",
    "    _set_cache_keys,\n",
    "    _set_cache_values,\n",
    "    _ensure_dynamic_cache,\n",
    "    extract_and_truncate_cache_with_bos,\n",
    "    correct_rope_positions_with_bos,\n",
    "    score_answer_with_cache,\n",
    "    deepcopy_cache,\n",
    "    replace_values_at_layers,\n",
    ")\n",
    "from lib.analysis import cohens_d\n",
    "from lib.data import count_words\n",
    "from scipy import stats\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Templates -- bare text, no \"Document:\\n\" framing\n",
    "SURROGATE_PREFIX_TEMPLATE = \"{surrogate}\\n\"\n",
    "DOCUMENT_TEMPLATE = \"{document}\"\n",
    "QUERY_TEMPLATE = \"\\nQuestion: {question}\\nAnswer:\"\n",
    "ANSWER_TEMPLATE = \" {answer}\"\n",
    "\n",
    "# Prefix text\n",
    "from lib.surrogate import STATIC_SURROGATE_QUERIES\n",
    "STATIC_FACT = STATIC_SURROGATE_QUERIES['static_factual']['query']\n",
    "\n",
    "# Experiment parameters\n",
    "N_PER_DATASET = 300\n",
    "MAX_DOC_TOKENS = 900\n",
    "CHECKPOINT_EVERY = 25\n",
    "\n",
    "# Conditions (reduced to 4 -- dropped sf_trunc_bias2, values_only)\n",
    "CONDITION_NAMES = ['bare', 'sf_trunc', 'values_early', 'values_hero']\n",
    "\n",
    "# Layer-selective conditions from Exps 19/21/24\n",
    "EARLY_LAYER_CUTOFF = 16  # layers 0-15\n",
    "HERO_LAYERS = [10, 12, 14, 15, 20]  # from Exp 24 single-layer scan\n",
    "\n",
    "print(\"Config ready\")\n",
    "print(f\"  Model: {MODEL_NAME}\")\n",
    "print(f\"  N per dataset: {N_PER_DATASET}\")\n",
    "print(f\"  MAX_DOC_TOKENS: {MAX_DOC_TOKENS} (sliding window constraint)\")\n",
    "print(f\"  N_LAYERS: {N_LAYERS}\")\n",
    "print(f\"  EARLY_LAYER_CUTOFF: {EARLY_LAYER_CUTOFF}\")\n",
    "print(f\"  HERO_LAYERS: {HERO_LAYERS}\")\n",
    "print(f\"  Conditions: {CONDITION_NAMES}\")\n",
    "print(f\"  Static fact prefix: '{STATIC_FACT}'\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 4: Load Natural Questions (streaming, same approach as Exp 27b)\n",
    "from datasets import load_dataset\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"LOADING NATURAL QUESTIONS (validation, streaming)\")\n",
    "print(\"=\" * 70)\n",
    "print(\"Factoid retrieval QA. Known positive control for hero layers (d=+0.213 in Exp 27b).\")\n",
    "\n",
    "NQ_CACHE = RESULTS_DIR / \"nq_samples.json\"\n",
    "\n",
    "if NQ_CACHE.exists():\n",
    "    with open(NQ_CACHE, 'r') as f:\n",
    "        nq_samples = json.load(f)\n",
    "    print(f\"Loaded {len(nq_samples)} cached NQ samples\")\n",
    "else:\n",
    "    nq_ds = load_dataset(\n",
    "        \"google-research-datasets/natural_questions\",\n",
    "        split=\"validation\",\n",
    "        streaming=True,\n",
    "    )\n",
    "\n",
    "    nq_samples = []\n",
    "    n_processed = 0\n",
    "\n",
    "    for example in tqdm(nq_ds, desc=\"Processing NQ\"):\n",
    "        n_processed += 1\n",
    "\n",
    "        doc_tokens = example['document']['tokens']\n",
    "        if isinstance(doc_tokens, dict):\n",
    "            token_strs = doc_tokens['token']\n",
    "            is_html_flags = doc_tokens['is_html']\n",
    "            clean_tokens = [t for t, h in zip(token_strs, is_html_flags) if not h]\n",
    "        else:\n",
    "            clean_tokens = [t['token'] for t in doc_tokens if not t['is_html']]\n",
    "\n",
    "        doc_text = ' '.join(clean_tokens)\n",
    "        wc = count_words(doc_text)\n",
    "\n",
    "        if wc < 50 or wc > 4000:\n",
    "            continue\n",
    "\n",
    "        annotations = example['annotations']\n",
    "        short_answers_list = annotations['short_answers']\n",
    "\n",
    "        answer_text = None\n",
    "        for annotator_sa in short_answers_list:\n",
    "            if not annotator_sa:\n",
    "                continue\n",
    "            texts = annotator_sa.get('text', [])\n",
    "            if texts:\n",
    "                answer_text = texts[0]\n",
    "                break\n",
    "            starts = annotator_sa.get('start_token', [])\n",
    "            ends = annotator_sa.get('end_token', [])\n",
    "            if not starts or not ends:\n",
    "                continue\n",
    "            start_tok = starts[0] if isinstance(starts, list) else starts\n",
    "            end_tok = ends[0] if isinstance(ends, list) else ends\n",
    "            if start_tok >= 0 and end_tok > start_tok:\n",
    "                if isinstance(doc_tokens, dict):\n",
    "                    ans_tokens = [\n",
    "                        doc_tokens['token'][i]\n",
    "                        for i in range(start_tok, min(end_tok, len(doc_tokens['token'])))\n",
    "                        if not doc_tokens['is_html'][i]\n",
    "                    ]\n",
    "                else:\n",
    "                    ans_tokens = [\n",
    "                        doc_tokens[i]['token']\n",
    "                        for i in range(start_tok, min(end_tok, len(doc_tokens)))\n",
    "                        if not doc_tokens[i]['is_html']\n",
    "                    ]\n",
    "                if ans_tokens:\n",
    "                    answer_text = ' '.join(ans_tokens)\n",
    "                    break\n",
    "\n",
    "        if not answer_text or len(answer_text.strip()) == 0:\n",
    "            continue\n",
    "        if len(answer_text.split()) > 20:\n",
    "            continue\n",
    "\n",
    "        question = example['question']\n",
    "        if isinstance(question, dict):\n",
    "            query = question.get('text', '')\n",
    "        else:\n",
    "            query = str(question)\n",
    "        if not query.strip():\n",
    "            continue\n",
    "\n",
    "        nq_samples.append({\n",
    "            'passage': doc_text,\n",
    "            'query': query,\n",
    "            'answer': answer_text,\n",
    "            'word_count': wc,\n",
    "            'dataset': 'nq',\n",
    "        })\n",
    "\n",
    "        if len(nq_samples) >= N_PER_DATASET * 3:\n",
    "            break\n",
    "\n",
    "    np.random.seed(SEED)\n",
    "    np.random.shuffle(nq_samples)\n",
    "    nq_samples = nq_samples[:N_PER_DATASET]\n",
    "\n",
    "    with open(NQ_CACHE, 'w') as f:\n",
    "        json.dump(nq_samples, f)\n",
    "    print(f\"Cached {len(nq_samples)} samples (processed {n_processed})\")\n",
    "\n",
    "print(f\"NQ samples: {len(nq_samples)}\")\n",
    "wcs = [s['word_count'] for s in nq_samples]\n",
    "print(f\"  Word counts: mean={np.mean(wcs):.0f}, min={min(wcs)}, max={max(wcs)}\")\n",
    "if nq_samples:\n",
    "    for i in range(min(3, len(nq_samples))):\n",
    "        print(f\"  Example {i+1}:\")\n",
    "        print(f\"    Q: {nq_samples[i]['query']}\")\n",
    "        print(f\"    A: {nq_samples[i]['answer']}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 5: Load DROP dataset (numerical/discrete reasoning + extraction)\n",
    "print(\"=\" * 70)\n",
    "print(\"LOADING DROP (validation)\")\n",
    "print(\"=\" * 70)\n",
    "print(\"Mixed computation + extraction. Known negative for hero layers (d=-0.152 in Exp 29).\")\n",
    "print(\"Each sample tagged with answer_type: 'number' or 'span'.\")\n",
    "\n",
    "DROP_CACHE = RESULTS_DIR / \"drop_samples.json\"\n",
    "\n",
    "# Regex for number answers: integers, decimals, comma-separated numbers\n",
    "NUMBER_PATTERN = re.compile(r'^\\d[\\d,\\.]*$')\n",
    "\n",
    "if DROP_CACHE.exists():\n",
    "    with open(DROP_CACHE, 'r') as f:\n",
    "        drop_samples = json.load(f)\n",
    "    print(f\"Loaded {len(drop_samples)} cached DROP samples\")\n",
    "else:\n",
    "    drop_ds = load_dataset(\"drop\", split=\"validation\")\n",
    "    print(f\"DROP validation size: {len(drop_ds)}\")\n",
    "\n",
    "    drop_samples = []\n",
    "    np.random.seed(SEED)\n",
    "\n",
    "    for item in tqdm(drop_ds, desc=\"Filtering DROP\"):\n",
    "        passage = item.get('passage', '')\n",
    "        question = item.get('question', '')\n",
    "        answers_info = item.get('answers_spans', {})\n",
    "\n",
    "        spans = answers_info.get('spans', [])\n",
    "        if not spans:\n",
    "            continue\n",
    "        answer_text = spans[0]\n",
    "\n",
    "        if not question or not answer_text or not passage:\n",
    "            continue\n",
    "        if len(answer_text.strip()) == 0:\n",
    "            continue\n",
    "\n",
    "        wc = count_words(passage)\n",
    "        if wc < 30 or wc > 2000:\n",
    "            continue\n",
    "\n",
    "        # Tag answer type\n",
    "        answer_type = 'number' if NUMBER_PATTERN.match(answer_text.strip()) else 'span'\n",
    "\n",
    "        drop_samples.append({\n",
    "            'passage': passage,\n",
    "            'query': question,\n",
    "            'answer': answer_text,\n",
    "            'word_count': wc,\n",
    "            'dataset': 'drop',\n",
    "            'answer_type': answer_type,\n",
    "            'all_answers': spans,\n",
    "        })\n",
    "\n",
    "        if len(drop_samples) >= N_PER_DATASET * 3:\n",
    "            break\n",
    "\n",
    "    np.random.shuffle(drop_samples)\n",
    "    drop_samples = drop_samples[:N_PER_DATASET]\n",
    "\n",
    "    with open(DROP_CACHE, 'w') as f:\n",
    "        json.dump(drop_samples, f)\n",
    "    print(f\"Cached {len(drop_samples)} samples\")\n",
    "\n",
    "    del drop_ds\n",
    "    gc.collect()\n",
    "\n",
    "# Print answer type distribution\n",
    "n_number = sum(1 for s in drop_samples if s.get('answer_type') == 'number')\n",
    "n_span = sum(1 for s in drop_samples if s.get('answer_type') == 'span')\n",
    "print(f\"DROP samples: {len(drop_samples)}\")\n",
    "print(f\"  Answer type distribution: number={n_number}, span={n_span}\")\n",
    "wcs = [s['word_count'] for s in drop_samples]\n",
    "ans_lens = [len(s['answer'].split()) for s in drop_samples]\n",
    "print(f\"  Word counts: mean={np.mean(wcs):.0f}, min={min(wcs)}, max={max(wcs)}\")\n",
    "print(f\"  Answer word lengths: mean={np.mean(ans_lens):.1f}, min={min(ans_lens)}, max={max(ans_lens)}\")\n",
    "\n",
    "# Show borderline cases (answers that look numeric but don't match regex)\n",
    "borderline = [s for s in drop_samples\n",
    "              if s.get('answer_type') == 'span' and any(c.isdigit() for c in s['answer'])]\n",
    "if borderline:\n",
    "    print(f\"\\n  Borderline cases (span with digits): {len(borderline)}\")\n",
    "    for b in borderline[:5]:\n",
    "        print(f\"    '{b['answer']}' -> tagged as '{b['answer_type']}'\")\n",
    "\n",
    "if drop_samples:\n",
    "    for i in range(min(3, len(drop_samples))):\n",
    "        print(f\"  Example {i+1} ({drop_samples[i].get('answer_type', '?')}):\")\n",
    "        print(f\"    Q: {drop_samples[i]['query']}\")\n",
    "        print(f\"    A: {drop_samples[i]['answer']}\")\n",
    "        print(f\"    Passage (first 120 chars): {drop_samples[i]['passage'][:120]}...\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 6: Load BoolQ dataset (binary judgment retrieval)\n",
    "print(\"=\" * 70)\n",
    "print(\"LOADING BOOLQ (validation)\")\n",
    "print(\"=\" * 70)\n",
    "print(\"Pure passage-based yes/no questions. New retrieval data point.\")\n",
    "\n",
    "BOOLQ_CACHE = RESULTS_DIR / \"boolq_samples.json\"\n",
    "\n",
    "if BOOLQ_CACHE.exists():\n",
    "    with open(BOOLQ_CACHE, 'r') as f:\n",
    "        boolq_samples = json.load(f)\n",
    "    print(f\"Loaded {len(boolq_samples)} cached BoolQ samples\")\n",
    "else:\n",
    "    boolq_ds = load_dataset(\"google/boolq\", split=\"validation\")\n",
    "    print(f\"BoolQ validation size: {len(boolq_ds)}\")\n",
    "\n",
    "    boolq_samples = []\n",
    "    np.random.seed(SEED)\n",
    "\n",
    "    for item in tqdm(boolq_ds, desc=\"Filtering BoolQ\"):\n",
    "        passage = item.get('passage', '')\n",
    "        question = item.get('question', '')\n",
    "        answer_bool = item.get('answer', None)\n",
    "\n",
    "        if not question or not passage or answer_bool is None:\n",
    "            continue\n",
    "\n",
    "        # Map boolean to text answer\n",
    "        answer_text = \"Yes\" if answer_bool else \"No\"\n",
    "\n",
    "        wc = count_words(passage)\n",
    "        if wc < 30 or wc > 2000:\n",
    "            continue\n",
    "\n",
    "        boolq_samples.append({\n",
    "            'passage': passage,\n",
    "            'query': question,\n",
    "            'answer': answer_text,\n",
    "            'word_count': wc,\n",
    "            'dataset': 'boolq',\n",
    "        })\n",
    "\n",
    "    np.random.shuffle(boolq_samples)\n",
    "    boolq_samples = boolq_samples[:N_PER_DATASET]\n",
    "\n",
    "    with open(BOOLQ_CACHE, 'w') as f:\n",
    "        json.dump(boolq_samples, f)\n",
    "    print(f\"Cached {len(boolq_samples)} samples\")\n",
    "\n",
    "    del boolq_ds\n",
    "    gc.collect()\n",
    "\n",
    "print(f\"BoolQ samples: {len(boolq_samples)}\")\n",
    "wcs = [s['word_count'] for s in boolq_samples]\n",
    "n_yes = sum(1 for s in boolq_samples if s['answer'] == 'Yes')\n",
    "n_no = sum(1 for s in boolq_samples if s['answer'] == 'No')\n",
    "print(f\"  Answer distribution: Yes={n_yes}, No={n_no}\")\n",
    "print(f\"  Word counts: mean={np.mean(wcs):.0f}, min={min(wcs)}, max={max(wcs)}\")\n",
    "if boolq_samples:\n",
    "    for i in range(min(3, len(boolq_samples))):\n",
    "        print(f\"  Example {i+1} ({boolq_samples[i]['answer']}):\")\n",
    "        print(f\"    Q: {boolq_samples[i]['query']}\")\n",
    "        print(f\"    Passage (first 120 chars): {boolq_samples[i]['passage'][:120]}...\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 7: Unified sample pool + tokenization + pre-screening\n",
    "print(\"=\" * 70)\n",
    "print(\"UNIFIED SAMPLE POOL\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "all_samples = []\n",
    "for ds_name, ds_samples in [(\"nq\", nq_samples),\n",
    "                              (\"drop\", drop_samples),\n",
    "                              (\"boolq\", boolq_samples)]:\n",
    "    for sample in ds_samples:\n",
    "        sample['dataset'] = ds_name\n",
    "    all_samples.extend(ds_samples)\n",
    "\n",
    "print(f\"Total samples: {len(all_samples)}\")\n",
    "for ds_name in ['nq', 'drop', 'boolq']:\n",
    "    ds_s = [s for s in all_samples if s['dataset'] == ds_name]\n",
    "    wcs = [s['word_count'] for s in ds_s]\n",
    "    print(f\"  {ds_name}: n={len(ds_s)}, mean_words={np.mean(wcs):.0f}, \"\n",
    "          f\"range=[{min(wcs)}, {max(wcs)}]\")\n",
    "\n",
    "# Tokenize prefix\n",
    "sf_str = SURROGATE_PREFIX_TEMPLATE.format(surrogate=STATIC_FACT)\n",
    "sf_ids = tokenizer(sf_str, return_tensors=\"pt\",\n",
    "                    add_special_tokens=False)['input_ids'].to(exp_config.device)\n",
    "PREFIX_TOKEN_LEN = sf_ids.shape[1]\n",
    "\n",
    "print(f\"\\nPrefix: '{STATIC_FACT}'\")\n",
    "print(f\"  Token length (no BOS): {PREFIX_TOKEN_LEN}\")\n",
    "\n",
    "# Verify sliding window safety\n",
    "max_primed_seq = 1 + PREFIX_TOKEN_LEN + MAX_DOC_TOKENS\n",
    "print(f\"  Max primed sequence: 1 + {PREFIX_TOKEN_LEN} + {MAX_DOC_TOKENS} = {max_primed_seq}\")\n",
    "print(f\"  Sliding window: 1024\")\n",
    "assert max_primed_seq < 1024, f\"UNSAFE: {max_primed_seq} >= 1024\"\n",
    "print(f\"  SAFE: {max_primed_seq} < 1024\")\n",
    "\n",
    "# Tokenize doc lengths\n",
    "print(f\"\\nTokenizing documents to measure token lengths...\")\n",
    "n_truncated = 0\n",
    "for sample in tqdm(all_samples, desc=\"Tokenizing\"):\n",
    "    tok_len = len(tokenizer.encode(sample['passage'], add_special_tokens=False))\n",
    "    if tok_len > MAX_DOC_TOKENS:\n",
    "        n_truncated += 1\n",
    "    sample['doc_token_len'] = min(tok_len, MAX_DOC_TOKENS)\n",
    "    sample['answer_token_len'] = len(tokenizer.encode(sample['answer'], add_special_tokens=False))\n",
    "\n",
    "print(f\"  Documents truncated to {MAX_DOC_TOKENS}: {n_truncated}/{len(all_samples)} \"\n",
    "      f\"({100*n_truncated/len(all_samples):.0f}%)\")\n",
    "\n",
    "for ds_name in ['nq', 'drop', 'boolq']:\n",
    "    ds_s = [s for s in all_samples if s['dataset'] == ds_name]\n",
    "    tls = [s['doc_token_len'] for s in ds_s]\n",
    "    atls = [s['answer_token_len'] for s in ds_s]\n",
    "    n_trunc = sum(1 for s in ds_s if s['doc_token_len'] == MAX_DOC_TOKENS)\n",
    "    print(f\"  {ds_name}: mean_tok={np.mean(tls):.0f}, median={np.median(tls):.0f}, \"\n",
    "          f\"truncated={n_trunc}/{len(ds_s)} ({100*n_trunc/len(ds_s):.0f}%), \"\n",
    "          f\"mean_ans_tok={np.mean(atls):.1f}\")\n",
    "\n",
    "# === PRE-SCREENING: Bare NLL check ===\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"PRE-SCREENING: Bare NLL distribution check (20 samples/dataset)\")\n",
    "print(\"If median bare NLL < 0.05, ceiling effects may dominate.\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for ds_name in ['nq', 'drop', 'boolq']:\n",
    "    ds_s = [s for s in all_samples if s['dataset'] == ds_name][:20]\n",
    "    bare_nlls = []\n",
    "    for sample in ds_s:\n",
    "        passage = sample['passage']\n",
    "        question = sample['query']\n",
    "        answer = sample['answer']\n",
    "\n",
    "        document_text = DOCUMENT_TEMPLATE.format(document=passage)\n",
    "        query_prompt = QUERY_TEMPLATE.format(question=question)\n",
    "        answer_text = ANSWER_TEMPLATE.format(answer=answer)\n",
    "\n",
    "        doc_ids = tokenizer(document_text, return_tensors=\"pt\",\n",
    "                            add_special_tokens=False)['input_ids'].to(exp_config.device)\n",
    "        if doc_ids.shape[1] > MAX_DOC_TOKENS:\n",
    "            doc_ids = doc_ids[:, :MAX_DOC_TOKENS]\n",
    "\n",
    "        bos_id = tokenizer.bos_token_id\n",
    "        if bos_id is None:\n",
    "            bos_id = tokenizer.encode(\"\", add_special_tokens=True)[0]\n",
    "        bos_tensor = torch.tensor([[bos_id]], device=exp_config.device)\n",
    "        bare_input = torch.cat([bos_tensor, doc_ids], dim=1)\n",
    "        context_len = bare_input.shape[1]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            bare_out = model(input_ids=bare_input,\n",
    "                             attention_mask=torch.ones_like(bare_input),\n",
    "                             use_cache=True, return_dict=True)\n",
    "        bare_cache = _ensure_dynamic_cache(bare_out.past_key_values)\n",
    "        del bare_out\n",
    "\n",
    "        nll = score_answer_with_cache(\n",
    "            deepcopy_cache(bare_cache), context_len,\n",
    "            query_prompt, answer_text, model, tokenizer, exp_config)\n",
    "        bare_nlls.append(nll)\n",
    "        del bare_cache, bare_input, doc_ids\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    bare_arr = np.array(bare_nlls)\n",
    "    pct_floor = 100 * np.mean(bare_arr < 0.01)\n",
    "    median = np.median(bare_arr)\n",
    "    mean = np.mean(bare_arr)\n",
    "    status = \"WARNING: CEILING\" if pct_floor > 50 else \"OK\" if pct_floor < 30 else \"MARGINAL\"\n",
    "    print(f\"  {ds_name:15s}: median={median:.3f}, mean={mean:.3f}, \"\n",
    "          f\"pct_floor(<0.01)={pct_floor:.0f}% -> {status}\")\n",
    "\n",
    "print(\"\\nPre-screening complete. Proceeding with full experiment.\")\n",
    "\n",
    "# Condition explanation\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"EXPERIMENTAL CONDITIONS (Gemma 3 4B) -- 4 conditions\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\n### 1. bare ###\")\n",
    "print(\"  Forward: [BOS][doc]\")\n",
    "print(\"  Baseline. Standard causal attention.\")\n",
    "\n",
    "print(\"\\n### 2. sf_trunc (standard priming) ###\")\n",
    "print(f\"  Forward: [BOS][prefix_{PREFIX_TOKEN_LEN}][doc]\")\n",
    "print(\"  Standard causal, truncate + RoPE. Keys carry negative interference on Gemma.\")\n",
    "\n",
    "print(\"\\n### 3. values_early (layers 0-15 only) ###\")\n",
    "print(\"  Bare keys + primed values from layers 0-15 only.\")\n",
    "print(\"  Expected: d ~ +0.211 (Exp 19 on MARCO). Late layers carry interference.\")\n",
    "\n",
    "print(\"\\n### 4. values_hero (layers {10,12,14,15,20}) ###\")\n",
    "print(\"  Bare keys + primed values from 5 hero layers identified in Exp 24.\")\n",
    "print(\"  NQ: d=+0.213 (Exp 27b). DROP: d=-0.152 (Exp 29).\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 8: Helper function \u2014 run_single_sample_4cond()\n",
    "\n",
    "def run_single_sample_4cond(sample, model, tokenizer, exp_config, sf_ids, sf_str,\n",
    "                             PREFIX_TOKEN_LEN, N_LAYERS, EARLY_LAYER_CUTOFF, HERO_LAYERS):\n",
    "    \"\"\"Run 4 conditions for a single sample. Returns dict of NLLs + metadata.\n",
    "\n",
    "    Conditions:\n",
    "      1. bare: [BOS][doc] standard causal\n",
    "      2. sf_trunc: [BOS][prefix][doc] truncate + RoPE correct\n",
    "      3. values_early: bare keys + primed values layers 0-15\n",
    "      4. values_hero: bare keys + primed values at hero layers\n",
    "    \"\"\"\n",
    "    passage = sample['passage']\n",
    "    query = sample['query']\n",
    "    answer = sample['answer']\n",
    "    ds_name = sample['dataset']\n",
    "\n",
    "    query_prompt = QUERY_TEMPLATE.format(question=query)\n",
    "    answer_text = ANSWER_TEMPLATE.format(answer=answer)\n",
    "    document_text = DOCUMENT_TEMPLATE.format(document=passage)\n",
    "\n",
    "    # === Matched tokenization ===\n",
    "    full_text = sf_str + document_text\n",
    "    full_enc = tokenizer(full_text, return_tensors=\"pt\",\n",
    "                          add_special_tokens=True, padding=False, truncation=False)\n",
    "    full_ids = full_enc['input_ids'].to(exp_config.device)\n",
    "\n",
    "    sf_prefix_enc = tokenizer(sf_str, return_tensors=\"pt\",\n",
    "                               add_special_tokens=True, padding=False, truncation=False)\n",
    "    sf_prefix_len_with_bos = sf_prefix_enc['input_ids'].shape[1]\n",
    "\n",
    "    bos_id = full_ids[:, :1]\n",
    "    doc_ids = full_ids[:, sf_prefix_len_with_bos:]\n",
    "\n",
    "    # Truncate long docs\n",
    "    if doc_ids.shape[1] > MAX_DOC_TOKENS:\n",
    "        doc_ids = doc_ids[:, :MAX_DOC_TOKENS]\n",
    "\n",
    "    doc_len = doc_ids.shape[1]\n",
    "    context_len = 1 + doc_len  # BOS + doc\n",
    "\n",
    "    del full_enc, full_ids, sf_prefix_enc\n",
    "\n",
    "    # === 1. BARE ===\n",
    "    bare_input = torch.cat([bos_id, doc_ids], dim=1)\n",
    "    with torch.no_grad():\n",
    "        bare_out = model(input_ids=bare_input,\n",
    "                         attention_mask=torch.ones_like(bare_input),\n",
    "                         use_cache=True, return_dict=True)\n",
    "    bare_cache = _ensure_dynamic_cache(bare_out.past_key_values)\n",
    "    del bare_out\n",
    "\n",
    "    bare_nll = score_answer_with_cache(\n",
    "        deepcopy_cache(bare_cache), context_len,\n",
    "        query_prompt, answer_text, model, tokenizer, exp_config)\n",
    "\n",
    "    # === 2. sf_trunc (standard priming) ===\n",
    "    primed_input = torch.cat([bos_id, sf_ids, doc_ids], dim=1)\n",
    "    prefix_offset = sf_ids.shape[1]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        primed_out = model(input_ids=primed_input,\n",
    "                           attention_mask=torch.ones_like(primed_input),\n",
    "                           use_cache=True, return_dict=True)\n",
    "    primed_full_std = _ensure_dynamic_cache(primed_out.past_key_values)\n",
    "    del primed_out\n",
    "\n",
    "    trunc_raw = extract_and_truncate_cache_with_bos(primed_full_std, doc_len)\n",
    "    del primed_full_std\n",
    "\n",
    "    sf_trunc_cache = deepcopy_cache(trunc_raw)\n",
    "    correct_rope_positions_with_bos(sf_trunc_cache, prefix_offset, model)\n",
    "    del trunc_raw\n",
    "\n",
    "    sf_trunc_nll = score_answer_with_cache(\n",
    "        deepcopy_cache(sf_trunc_cache), context_len,\n",
    "        query_prompt, answer_text, model, tokenizer, exp_config)\n",
    "\n",
    "    # === 3. values_early (layers 0 to EARLY_LAYER_CUTOFF-1) ===\n",
    "    early_layers = list(range(EARLY_LAYER_CUTOFF))\n",
    "    values_early_cache = replace_values_at_layers(bare_cache, sf_trunc_cache, early_layers)\n",
    "\n",
    "    values_early_nll = score_answer_with_cache(\n",
    "        deepcopy_cache(values_early_cache), context_len,\n",
    "        query_prompt, answer_text, model, tokenizer, exp_config)\n",
    "    del values_early_cache\n",
    "\n",
    "    # === 4. values_hero (hero layers only) ===\n",
    "    values_hero_cache = replace_values_at_layers(bare_cache, sf_trunc_cache, HERO_LAYERS)\n",
    "\n",
    "    values_hero_nll = score_answer_with_cache(\n",
    "        deepcopy_cache(values_hero_cache), context_len,\n",
    "        query_prompt, answer_text, model, tokenizer, exp_config)\n",
    "    del values_hero_cache\n",
    "\n",
    "    del bare_cache, sf_trunc_cache, bare_input, primed_input\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    result = {\n",
    "        'dataset': ds_name,\n",
    "        'query': query,\n",
    "        'answer': answer,\n",
    "        'word_count': sample['word_count'],\n",
    "        'doc_token_len': doc_len,\n",
    "        'answer_token_len': sample.get('answer_token_len', 0),\n",
    "        'bare': bare_nll,\n",
    "        'sf_trunc': sf_trunc_nll,\n",
    "        'values_early': values_early_nll,\n",
    "        'values_hero': values_hero_nll,\n",
    "    }\n",
    "    # Carry forward answer_type for DROP\n",
    "    if 'answer_type' in sample:\n",
    "        result['answer_type'] = sample['answer_type']\n",
    "    return result\n",
    "\n",
    "\n",
    "print(\"Helper function defined: run_single_sample_4cond()\")\n",
    "print(\"  Conditions: bare, sf_trunc, values_early, values_hero\")\n",
    "print(\"  No bias mask needed (sf_trunc_bias2 dropped)\")\n",
    "print(\"  No values_only (dominated by values_early)\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 9: Main experiment loop\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"EXPERIMENT 30: {len(all_samples)} samples, {len(CONDITION_NAMES)} conditions\")\n",
    "print(f\"Model: Gemma 3 4B, MAX_DOC_TOKENS: {MAX_DOC_TOKENS}\")\n",
    "print(f\"Datasets: NQ, DROP, BoolQ\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Checkpoint resume\n",
    "all_results = []\n",
    "start_idx = 0\n",
    "\n",
    "if CHECKPOINT_PATH.exists():\n",
    "    with open(CHECKPOINT_PATH, 'r') as f:\n",
    "        ckpt = json.load(f)\n",
    "    ckpt_queries = ckpt.get('sample_queries', [])\n",
    "    current_queries = [s['query'] for s in all_samples]\n",
    "    if ckpt_queries == current_queries:\n",
    "        all_results = ckpt['results']\n",
    "        start_idx = len(all_results)\n",
    "        print(f\"Resuming from checkpoint: {start_idx}/{len(all_samples)}\")\n",
    "    else:\n",
    "        print(\"Checkpoint query mismatch. Starting fresh.\")\n",
    "else:\n",
    "    print(\"No checkpoint found. Starting fresh.\")\n",
    "\n",
    "t_start = time.time()\n",
    "N_TOTAL = len(all_samples)\n",
    "\n",
    "for qidx in tqdm(range(start_idx, N_TOTAL), initial=start_idx, total=N_TOTAL,\n",
    "                  desc=\"Exp 30\"):\n",
    "    sample = all_samples[qidx]\n",
    "\n",
    "    result = run_single_sample_4cond(\n",
    "        sample, model, tokenizer, exp_config,\n",
    "        sf_ids, sf_str, PREFIX_TOKEN_LEN, N_LAYERS,\n",
    "        EARLY_LAYER_CUTOFF, HERO_LAYERS)\n",
    "    result['query_idx'] = qidx\n",
    "    all_results.append(result)\n",
    "\n",
    "    # Checkpoint\n",
    "    if (qidx + 1) % CHECKPOINT_EVERY == 0 or qidx == N_TOTAL - 1:\n",
    "        ckpt_data = {\n",
    "            'results': all_results,\n",
    "            'sample_queries': [s['query'] for s in all_samples],\n",
    "            'completed': len(all_results),\n",
    "            'total': N_TOTAL,\n",
    "            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        }\n",
    "        with open(CHECKPOINT_PATH, 'w') as f:\n",
    "            json.dump(ckpt_data, f)\n",
    "        elapsed = time.time() - t_start\n",
    "        n_done = qidx - start_idx + 1\n",
    "        rate = n_done / elapsed if elapsed > 0 else 0\n",
    "        remaining = (N_TOTAL - qidx - 1) / rate if rate > 0 else 0\n",
    "        tqdm.write(f\"  Checkpoint {qidx+1}/{N_TOTAL} | {n_done} done in {elapsed/60:.1f}m | \"\n",
    "                   f\"ETA: {remaining/60:.1f} min\")\n",
    "\n",
    "elapsed_total = time.time() - t_start\n",
    "print(f\"\\nExperiment complete: {len(all_results)} samples in {elapsed_total/60:.1f} min\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 10: Per-dataset results table\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ANALYSIS: PER-DATASET RESULTS (Gemma 3 4B)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "dataset_names = ['nq', 'drop', 'boolq']\n",
    "analysis = {}\n",
    "\n",
    "for ds_name in dataset_names:\n",
    "    ds_results = [r for r in all_results if r['dataset'] == ds_name]\n",
    "    n_ds = len(ds_results)\n",
    "    if n_ds == 0:\n",
    "        continue\n",
    "\n",
    "    bare_arr = np.array([r['bare'] for r in ds_results])\n",
    "\n",
    "    # Filter invalid (keep zeros -- valid for some datasets)\n",
    "    valid = np.isfinite(bare_arr)\n",
    "    for cname in CONDITION_NAMES:\n",
    "        if cname == 'bare':\n",
    "            continue\n",
    "        c_arr = np.array([r[cname] for r in ds_results])\n",
    "        valid &= np.isfinite(c_arr)\n",
    "\n",
    "    n_valid = int(np.sum(valid))\n",
    "\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    pct_floor = 100 * np.mean(bare_arr < 0.01)\n",
    "    print(f\"DATASET: {ds_name.upper()} (n={n_valid}/{n_ds}, \"\n",
    "          f\"median bare NLL={np.median(bare_arr):.3f}, \"\n",
    "          f\"pct_floor={pct_floor:.0f}%)\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "    print(f\"\\n{'Condition':<20} {'Mean Bare':>10} {'Mean Cond':>10} \"\n",
    "          f\"{'Mean D':>10} {'d':>8} {'Win%':>7} {'p':>12} {'sig':>5}\")\n",
    "    print(\"-\" * 90)\n",
    "\n",
    "    ds_analysis = {}\n",
    "    for cname in CONDITION_NAMES:\n",
    "        if cname == 'bare':\n",
    "            continue\n",
    "        c_arr = np.array([r[cname] for r in ds_results])\n",
    "        delta = bare_arr[valid] - c_arr[valid]\n",
    "        d = cohens_d(delta)\n",
    "        win = np.mean(delta > 0) * 100\n",
    "        t_stat, p_val = stats.ttest_1samp(delta, 0)\n",
    "        sig = '***' if p_val < 0.001 else '**' if p_val < 0.01 else '*' if p_val < 0.05 else 'ns'\n",
    "        print(f\"{cname:<20} {np.mean(bare_arr[valid]):>10.4f} {np.mean(c_arr[valid]):>10.4f} \"\n",
    "              f\"{np.mean(delta):>+10.4f} {d:>+8.3f} {win:>6.1f}% {p_val:>12.2e} {sig:>5}\")\n",
    "        ds_analysis[cname] = {\n",
    "            'n_valid': n_valid,\n",
    "            'mean_bare': float(np.mean(bare_arr[valid])),\n",
    "            'mean_cond': float(np.mean(c_arr[valid])),\n",
    "            'mean_delta': float(np.mean(delta)),\n",
    "            'cohens_d': float(d),\n",
    "            'win_pct': float(win),\n",
    "            't_stat': float(t_stat),\n",
    "            'p_value': float(p_val),\n",
    "        }\n",
    "\n",
    "    analysis[ds_name] = ds_analysis\n",
    "\n",
    "# Cross-dataset summary table\n",
    "print(f\"\\n\\n{'='*90}\")\n",
    "print(\"CROSS-DATASET SUMMARY: Cohen's d vs bare (Gemma 3 4B)\")\n",
    "print(f\"{'='*90}\")\n",
    "print(f\"\\n{'Condition':<20}\", end='')\n",
    "for ds in dataset_names:\n",
    "    print(f\"{'  ' + ds:>16}\", end='')\n",
    "print()\n",
    "print(\"-\" * 68)\n",
    "for cname in CONDITION_NAMES:\n",
    "    if cname == 'bare':\n",
    "        continue\n",
    "    print(f\"{cname:<20}\", end='')\n",
    "    for ds in dataset_names:\n",
    "        if ds in analysis and cname in analysis[ds]:\n",
    "            d = analysis[ds][cname]['cohens_d']\n",
    "            p = analysis[ds][cname]['p_value']\n",
    "            sig = '***' if p < 0.001 else '**' if p < 0.01 else '*' if p < 0.05 else ''\n",
    "            print(f\"{d:>+12.3f}{sig:>4}\", end='')\n",
    "        else:\n",
    "            print(f\"{'n/a':>16}\", end='')\n",
    "    print()\n",
    "\n",
    "# Bare NLL distributions\n",
    "print(f\"\\n\\nBARE NLL DISTRIBUTIONS (ceiling effect check):\")\n",
    "for ds in dataset_names:\n",
    "    ds_r = [r for r in all_results if r['dataset'] == ds]\n",
    "    bare = [r['bare'] for r in ds_r]\n",
    "    pct_zero = 100 * np.mean(np.array(bare) < 0.01)\n",
    "    iqr = np.percentile(bare, 75) - np.percentile(bare, 25)\n",
    "    print(f\"  {ds:15s}: mean={np.mean(bare):.3f}, median={np.median(bare):.3f}, \"\n",
    "          f\"IQR={iqr:.3f}, pct_floor={pct_zero:.0f}%\")\n",
    "\n",
    "# Reference from prior exps\n",
    "print(f\"\\n\\n{'='*90}\")\n",
    "print(\"COMPARISON WITH PRIOR EXPERIMENTS\")\n",
    "print(f\"{'='*90}\")\n",
    "print(\"\\nExp 27b (Gemma, NQ/TriviaQA/HotpotQA):\")\n",
    "print(\"  NQ:       values_hero d=+0.213***\")\n",
    "print(\"  TriviaQA: values_hero d=+0.000 (77% at floor)\")\n",
    "print(\"  HotpotQA: values_hero d=-0.069 (56% at floor)\")\n",
    "print(\"\\nExp 29 (Gemma, DROP/AdvQA/CoQA):\")\n",
    "print(\"  DROP:     values_hero d=-0.152**\")\n",
    "print(\"  AdvQA:    values_hero d=+0.026 (72% at floor)\")\n",
    "print(\"  CoQA:     values_hero d=+0.070 (65% at floor)\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 11: Within-DROP split by answer type (THE KEY TEST)\n",
    "print(\"=\" * 70)\n",
    "print(\"WITHIN-DROP SPLIT: Number vs Span Answer Types\")\n",
    "print(\"=\" * 70)\n",
    "print(\"This is the most important analysis: same dataset, same passages,\")\n",
    "print(\"different answer types. Number = computation, Span = retrieval.\")\n",
    "\n",
    "drop_results = [r for r in all_results if r['dataset'] == 'drop']\n",
    "\n",
    "# Split by answer_type\n",
    "drop_number = [r for r in drop_results if r.get('answer_type') == 'number']\n",
    "drop_span = [r for r in drop_results if r.get('answer_type') == 'span']\n",
    "\n",
    "print(f\"\\nDROP-number: n={len(drop_number)}\")\n",
    "print(f\"DROP-span:   n={len(drop_span)}\")\n",
    "\n",
    "drop_split_analysis = {}\n",
    "\n",
    "for split_name, split_results in [('drop_number', drop_number), ('drop_span', drop_span)]:\n",
    "    if len(split_results) < 20:\n",
    "        print(f\"\\n  {split_name}: too few samples ({len(split_results)}), skipping\")\n",
    "        drop_split_analysis[split_name] = {'n': len(split_results), 'skipped': True}\n",
    "        continue\n",
    "\n",
    "    bare_arr = np.array([r['bare'] for r in split_results])\n",
    "    pct_floor = 100 * np.mean(bare_arr < 0.01)\n",
    "\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"{split_name.upper()} (n={len(split_results)}, \"\n",
    "          f\"median bare NLL={np.median(bare_arr):.3f}, pct_floor={pct_floor:.0f}%)\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "    print(f\"\\n{'Condition':<20} {'d':>8} {'Win%':>7} {'p':>12} {'sig':>5}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    split_data = {'n': len(split_results), 'pct_floor': float(pct_floor)}\n",
    "    for cname in ['sf_trunc', 'values_early', 'values_hero']:\n",
    "        c_arr = np.array([r[cname] for r in split_results])\n",
    "        delta = bare_arr - c_arr\n",
    "        valid = np.isfinite(delta)\n",
    "        delta = delta[valid]\n",
    "        if len(delta) < 10:\n",
    "            print(f\"{cname:<20} {'n/a (too few valid)':>40}\")\n",
    "            continue\n",
    "        d = cohens_d(delta)\n",
    "        win = np.mean(delta > 0) * 100\n",
    "        t_stat, p_val = stats.ttest_1samp(delta, 0)\n",
    "        sig = '***' if p_val < 0.001 else '**' if p_val < 0.01 else '*' if p_val < 0.05 else 'ns'\n",
    "        print(f\"{cname:<20} {d:>+8.3f} {win:>6.1f}% {p_val:>12.2e} {sig:>5}\")\n",
    "        split_data[cname] = {\n",
    "            'cohens_d': float(d),\n",
    "            'win_pct': float(win),\n",
    "            'p_value': float(p_val),\n",
    "        }\n",
    "\n",
    "    drop_split_analysis[split_name] = split_data\n",
    "\n",
    "# Bare NLL distributions by type\n",
    "print(f\"\\n\\nBARE NLL DISTRIBUTIONS BY ANSWER TYPE:\")\n",
    "for split_name, split_results in [('drop_number', drop_number), ('drop_span', drop_span)]:\n",
    "    if not split_results:\n",
    "        continue\n",
    "    bare = np.array([r['bare'] for r in split_results])\n",
    "    pct_floor = 100 * np.mean(bare < 0.01)\n",
    "    print(f\"  {split_name:15s}: mean={np.mean(bare):.3f}, median={np.median(bare):.3f}, \"\n",
    "          f\"IQR={np.percentile(bare,75)-np.percentile(bare,25):.3f}, \"\n",
    "          f\"pct_floor={pct_floor:.0f}%\")\n",
    "\n",
    "# Difficulty-matched within DROP\n",
    "print(f\"\\n\\nDIFFICULTY-MATCHED WITHIN DROP (bare > 0.5):\")\n",
    "for split_name, split_results in [('drop_number', drop_number), ('drop_span', drop_span)]:\n",
    "    hard = [r for r in split_results if r['bare'] > 0.5]\n",
    "    if len(hard) < 10:\n",
    "        print(f\"  {split_name}: n_hard={len(hard)} (too few)\")\n",
    "        continue\n",
    "    bare_h = np.array([r['bare'] for r in hard])\n",
    "    hero_h = np.array([r['values_hero'] for r in hard])\n",
    "    delta_h = bare_h - hero_h\n",
    "    d_h = cohens_d(delta_h)\n",
    "    win_h = np.mean(delta_h > 0) * 100\n",
    "    _, p_h = stats.ttest_1samp(delta_h, 0)\n",
    "    sig_h = '***' if p_h < 0.001 else '**' if p_h < 0.01 else '*' if p_h < 0.05 else 'ns'\n",
    "    print(f\"  {split_name} (n_hard={len(hard)}): hero d={d_h:+.3f}, \"\n",
    "          f\"win={win_h:.0f}%, p={p_h:.2e} {sig_h}\")\n",
    "\n",
    "# Key interpretation\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"INTERPRETATION\")\n",
    "print(f\"{'='*70}\")\n",
    "hero_num = drop_split_analysis.get('drop_number', {}).get('values_hero', {})\n",
    "hero_span = drop_split_analysis.get('drop_span', {}).get('values_hero', {})\n",
    "d_num = hero_num.get('cohens_d', float('nan'))\n",
    "d_span = hero_span.get('cohens_d', float('nan'))\n",
    "print(f\"  DROP-number hero d: {d_num:+.3f}\")\n",
    "print(f\"  DROP-span hero d:   {d_span:+.3f}\")\n",
    "if d_span > d_num + 0.05:\n",
    "    print(\"  -> CONSISTENT with task-type hypothesis: span (retrieval) > number (computation)\")\n",
    "elif abs(d_span - d_num) < 0.05:\n",
    "    print(\"  -> INCONCLUSIVE: number and span show similar effects\")\n",
    "else:\n",
    "    print(\"  -> INCONSISTENT: number benefits MORE than span (unexpected)\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 12: Difficulty-matched cross-dataset comparison\n",
    "print(\"=\" * 70)\n",
    "print(\"DIFFICULTY-MATCHED CROSS-DATASET COMPARISON\")\n",
    "print(\"=\" * 70)\n",
    "print(\"Filter to hard samples (bare > 0.5) across all datasets.\")\n",
    "print(\"Compare hero d for retrieval vs computation subsets.\")\n",
    "\n",
    "# Collect hard samples by subset\n",
    "hard_subsets = {}\n",
    "for ds_name in ['nq', 'drop', 'boolq']:\n",
    "    ds_r = [r for r in all_results if r['dataset'] == ds_name]\n",
    "    hard = [r for r in ds_r if r['bare'] > 0.5]\n",
    "    hard_subsets[ds_name] = hard\n",
    "    print(f\"  {ds_name}: {len(hard)}/{len(ds_r)} hard samples (bare > 0.5)\")\n",
    "\n",
    "# Also split DROP hard by type\n",
    "drop_hard = hard_subsets.get('drop', [])\n",
    "drop_hard_number = [r for r in drop_hard if r.get('answer_type') == 'number']\n",
    "drop_hard_span = [r for r in drop_hard if r.get('answer_type') == 'span']\n",
    "hard_subsets['drop_number'] = drop_hard_number\n",
    "hard_subsets['drop_span'] = drop_hard_span\n",
    "print(f\"  drop_number: {len(drop_hard_number)} hard\")\n",
    "print(f\"  drop_span:   {len(drop_hard_span)} hard\")\n",
    "\n",
    "# Hero d for each hard subset\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"HERO d ON HARD SAMPLES (bare > 0.5)\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"\\n{'Subset':<20} {'N':>5} {'hero d':>8} {'95% CI':>20} {'p':>12} {'sig':>5}\")\n",
    "print(\"-\" * 75)\n",
    "\n",
    "hard_hero_ds = {}\n",
    "for subset_name in ['nq', 'drop_number', 'drop_span', 'boolq']:\n",
    "    subset = hard_subsets.get(subset_name, [])\n",
    "    if len(subset) < 20:\n",
    "        print(f\"{subset_name:<20} {len(subset):>5} {'(n<20, skip)':>40}\")\n",
    "        hard_hero_ds[subset_name] = {'n': len(subset), 'skipped': True}\n",
    "        continue\n",
    "\n",
    "    bare_h = np.array([r['bare'] for r in subset])\n",
    "    hero_h = np.array([r['values_hero'] for r in subset])\n",
    "    delta_h = bare_h - hero_h\n",
    "\n",
    "    d_h = cohens_d(delta_h)\n",
    "    _, p_h = stats.ttest_1samp(delta_h, 0)\n",
    "    sig_h = '***' if p_h < 0.001 else '**' if p_h < 0.01 else '*' if p_h < 0.05 else 'ns'\n",
    "\n",
    "    # Bootstrap 95% CI for hero d\n",
    "    np.random.seed(SEED)\n",
    "    boot_ds = []\n",
    "    for _ in range(2000):\n",
    "        idx = np.random.choice(len(delta_h), len(delta_h), replace=True)\n",
    "        boot_ds.append(cohens_d(delta_h[idx]))\n",
    "    ci_lo = np.percentile(boot_ds, 2.5)\n",
    "    ci_hi = np.percentile(boot_ds, 97.5)\n",
    "\n",
    "    print(f\"{subset_name:<20} {len(subset):>5} {d_h:>+8.3f} \"\n",
    "          f\"[{ci_lo:>+8.3f}, {ci_hi:>+8.3f}] {p_h:>12.2e} {sig_h:>5}\")\n",
    "    hard_hero_ds[subset_name] = {\n",
    "        'n': len(subset),\n",
    "        'cohens_d': float(d_h),\n",
    "        'ci_lo': float(ci_lo),\n",
    "        'ci_hi': float(ci_hi),\n",
    "        'p_value': float(p_h),\n",
    "    }\n",
    "\n",
    "# Welch's t-test: retrieval vs computation hard-subset deltas\n",
    "print(f\"\\n\\nWELCH'S T-TEST: Retrieval vs Computation (hard samples)\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "retrieval_deltas = []\n",
    "computation_deltas = []\n",
    "\n",
    "# Retrieval: NQ-hard + DROP-span-hard + BoolQ-hard\n",
    "for subset_name in ['nq', 'drop_span', 'boolq']:\n",
    "    subset = hard_subsets.get(subset_name, [])\n",
    "    for r in subset:\n",
    "        retrieval_deltas.append(r['bare'] - r['values_hero'])\n",
    "\n",
    "# Computation: DROP-number-hard\n",
    "for r in hard_subsets.get('drop_number', []):\n",
    "    computation_deltas.append(r['bare'] - r['values_hero'])\n",
    "\n",
    "retrieval_deltas = np.array(retrieval_deltas)\n",
    "computation_deltas = np.array(computation_deltas)\n",
    "\n",
    "print(f\"  Retrieval hard samples: n={len(retrieval_deltas)}\")\n",
    "print(f\"  Computation hard samples: n={len(computation_deltas)}\")\n",
    "\n",
    "if len(retrieval_deltas) >= 10 and len(computation_deltas) >= 10:\n",
    "    t_stat, p_welch = stats.ttest_ind(retrieval_deltas, computation_deltas, equal_var=False)\n",
    "    sig_w = '***' if p_welch < 0.001 else '**' if p_welch < 0.01 else '*' if p_welch < 0.05 else 'ns'\n",
    "    print(f\"  Retrieval mean delta: {np.mean(retrieval_deltas):+.4f}\")\n",
    "    print(f\"  Computation mean delta: {np.mean(computation_deltas):+.4f}\")\n",
    "    print(f\"  Welch's t={t_stat:.3f}, p={p_welch:.2e} {sig_w}\")\n",
    "    if p_welch < 0.05:\n",
    "        print(\"  -> SIGNIFICANT difference between retrieval and computation hard-sample hero effects\")\n",
    "    else:\n",
    "        print(\"  -> No significant difference (may be underpowered or confounded)\")\n",
    "else:\n",
    "    p_welch = float('nan')\n",
    "    print(\"  -> Too few samples for Welch's t-test\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 13: Task-type regression\n",
    "print(\"=\" * 70)\n",
    "print(\"TASK-TYPE REGRESSION\")\n",
    "print(\"=\" * 70)\n",
    "print(\"Linear regression: delta_i = b0 + b1*bare_i + b2*is_retrieval_i\")\n",
    "print(\"Tests whether task type predicts hero effect BEYOND difficulty.\")\n",
    "\n",
    "# Tag all samples by task type\n",
    "# Retrieval: NQ, BoolQ, DROP-span\n",
    "# Computation: DROP-number\n",
    "task_type_map = {}\n",
    "for r in all_results:\n",
    "    if r['dataset'] == 'nq':\n",
    "        task_type_map[r['query_idx']] = 'retrieval'\n",
    "    elif r['dataset'] == 'boolq':\n",
    "        task_type_map[r['query_idx']] = 'retrieval'\n",
    "    elif r['dataset'] == 'drop':\n",
    "        if r.get('answer_type') == 'number':\n",
    "            task_type_map[r['query_idx']] = 'computation'\n",
    "        else:\n",
    "            task_type_map[r['query_idx']] = 'retrieval'\n",
    "\n",
    "# Build regression data\n",
    "bare_vals = []\n",
    "delta_vals = []\n",
    "is_retrieval = []\n",
    "for r in all_results:\n",
    "    if r['query_idx'] not in task_type_map:\n",
    "        continue\n",
    "    b = r['bare']\n",
    "    h = r['values_hero']\n",
    "    if not (np.isfinite(b) and np.isfinite(h)):\n",
    "        continue\n",
    "    bare_vals.append(b)\n",
    "    delta_vals.append(b - h)\n",
    "    is_retrieval.append(1 if task_type_map[r['query_idx']] == 'retrieval' else 0)\n",
    "\n",
    "bare_vals = np.array(bare_vals)\n",
    "delta_vals = np.array(delta_vals)\n",
    "is_retrieval = np.array(is_retrieval)\n",
    "\n",
    "print(f\"\\nRegression samples: {len(bare_vals)}\")\n",
    "print(f\"  Retrieval: {np.sum(is_retrieval)}\")\n",
    "print(f\"  Computation: {np.sum(1 - is_retrieval)}\")\n",
    "\n",
    "# OLS regression: delta = b0 + b1*bare + b2*is_retrieval\n",
    "from numpy.linalg import lstsq\n",
    "\n",
    "X = np.column_stack([np.ones(len(bare_vals)), bare_vals, is_retrieval])\n",
    "beta, residuals, rank, sv = lstsq(X, delta_vals, rcond=None)\n",
    "\n",
    "b0, b1, b2 = beta\n",
    "y_hat = X @ beta\n",
    "ss_res = np.sum((delta_vals - y_hat) ** 2)\n",
    "ss_tot = np.sum((delta_vals - np.mean(delta_vals)) ** 2)\n",
    "r_squared = 1 - ss_res / ss_tot if ss_tot > 0 else 0\n",
    "\n",
    "# Standard errors\n",
    "n = len(delta_vals)\n",
    "p_params = X.shape[1]\n",
    "mse = ss_res / (n - p_params)\n",
    "var_beta = mse * np.linalg.inv(X.T @ X)\n",
    "se_beta = np.sqrt(np.diag(var_beta))\n",
    "t_stats = beta / se_beta\n",
    "p_values = 2 * (1 - stats.t.cdf(np.abs(t_stats), df=n - p_params))\n",
    "\n",
    "print(f\"\\nOLS Regression: delta_hero = b0 + b1*bare + b2*is_retrieval\")\n",
    "print(f\"  R-squared: {r_squared:.4f}\")\n",
    "print(f\"\\n{'Parameter':<15} {'Estimate':>10} {'SE':>10} {'t':>8} {'p':>12} {'sig':>5}\")\n",
    "print(\"-\" * 65)\n",
    "param_names = ['intercept', 'bare_nll', 'is_retrieval']\n",
    "for i, pname in enumerate(param_names):\n",
    "    sig = '***' if p_values[i] < 0.001 else '**' if p_values[i] < 0.01 else '*' if p_values[i] < 0.05 else 'ns'\n",
    "    print(f\"{pname:<15} {beta[i]:>+10.4f} {se_beta[i]:>10.4f} \"\n",
    "          f\"{t_stats[i]:>8.3f} {p_values[i]:>12.2e} {sig:>5}\")\n",
    "\n",
    "print(f\"\\nKEY RESULT: beta_2 (is_retrieval) = {b2:+.4f}, p = {p_values[2]:.2e}\")\n",
    "if p_values[2] < 0.05:\n",
    "    direction = \"MORE\" if b2 > 0 else \"LESS\"\n",
    "    print(f\"  -> SIGNIFICANT: Retrieval tasks benefit {direction} from hero layers, \"\n",
    "          f\"controlling for difficulty\")\n",
    "else:\n",
    "    print(\"  -> NOT SIGNIFICANT: Task type does not predict hero effect beyond difficulty\")\n",
    "\n",
    "# Spearman correlation at dataset-subset level\n",
    "print(f\"\\n\\nSPEARMAN CORRELATION: hero_d vs pct_floor (dataset level)\")\n",
    "subset_ds = []\n",
    "subset_floors = []\n",
    "for subset_name in ['nq', 'drop_number', 'drop_span', 'boolq']:\n",
    "    if subset_name == 'drop_number':\n",
    "        sr = [r for r in all_results if r['dataset'] == 'drop' and r.get('answer_type') == 'number']\n",
    "    elif subset_name == 'drop_span':\n",
    "        sr = [r for r in all_results if r['dataset'] == 'drop' and r.get('answer_type') == 'span']\n",
    "    else:\n",
    "        sr = [r for r in all_results if r['dataset'] == subset_name]\n",
    "    if len(sr) < 20:\n",
    "        continue\n",
    "    bare_s = np.array([r['bare'] for r in sr])\n",
    "    hero_s = np.array([r['values_hero'] for r in sr])\n",
    "    delta_s = bare_s - hero_s\n",
    "    d_s = cohens_d(delta_s)\n",
    "    floor_s = float(np.mean(bare_s < 0.01) * 100)\n",
    "    subset_ds.append(d_s)\n",
    "    subset_floors.append(floor_s)\n",
    "    print(f\"  {subset_name:15s}: hero d={d_s:+.3f}, pct_floor={floor_s:.0f}%\")\n",
    "\n",
    "if len(subset_ds) >= 3:\n",
    "    rho, p_spear = stats.spearmanr(subset_floors, subset_ds)\n",
    "    print(f\"  Spearman rho={rho:+.3f}, p={p_spear:.3f} (n={len(subset_ds)} subsets)\")\n",
    "    if p_spear < 0.05:\n",
    "        print(\"  -> Ceiling effects significantly predict hero d at dataset level\")\n",
    "    else:\n",
    "        print(\"  -> Ceiling effects do NOT significantly predict hero d (but n is small)\")\n",
    "else:\n",
    "    rho, p_spear = float('nan'), float('nan')\n",
    "    print(\"  -> Too few subsets for Spearman\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 14: Multi-panel figure (2x2)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 14))\n",
    "\n",
    "colors_task = {'retrieval': '#2ca02c', 'computation': '#d62728', 'mixed': '#7f7f7f'}\n",
    "\n",
    "# ---- Panel (a): Hero d by dataset-subset, colored by task type ----\n",
    "ax = axes[0, 0]\n",
    "\n",
    "subset_info = [\n",
    "    ('NQ', 'nq', 'retrieval'),\n",
    "    ('DROP-num', 'drop_number', 'computation'),\n",
    "    ('DROP-span', 'drop_span', 'retrieval'),\n",
    "    ('BoolQ', 'boolq', 'retrieval'),\n",
    "]\n",
    "\n",
    "x_pos = np.arange(len(subset_info))\n",
    "bar_ds = []\n",
    "bar_colors = []\n",
    "bar_labels = []\n",
    "\n",
    "for label, subset_key, task_type in subset_info:\n",
    "    if subset_key == 'drop_number':\n",
    "        sr = [r for r in all_results if r['dataset'] == 'drop' and r.get('answer_type') == 'number']\n",
    "    elif subset_key == 'drop_span':\n",
    "        sr = [r for r in all_results if r['dataset'] == 'drop' and r.get('answer_type') == 'span']\n",
    "    else:\n",
    "        sr = [r for r in all_results if r['dataset'] == subset_key]\n",
    "\n",
    "    if len(sr) >= 10:\n",
    "        bare_s = np.array([r['bare'] for r in sr])\n",
    "        hero_s = np.array([r['values_hero'] for r in sr])\n",
    "        delta_s = bare_s - hero_s\n",
    "        d_s = cohens_d(delta_s)\n",
    "        _, p_s = stats.ttest_1samp(delta_s, 0)\n",
    "        sig_s = '***' if p_s < 0.001 else '**' if p_s < 0.01 else '*' if p_s < 0.05 else ''\n",
    "    else:\n",
    "        d_s = 0\n",
    "        sig_s = 'n/a'\n",
    "    bar_ds.append(d_s)\n",
    "    bar_colors.append(colors_task[task_type])\n",
    "    bar_labels.append(f\"{label}\\n({task_type})\")\n",
    "\n",
    "bars = ax.bar(x_pos, bar_ds, color=bar_colors, edgecolor='black', linewidth=0.5)\n",
    "for i, (d_val, sig_val) in enumerate(zip(bar_ds, [\n",
    "    '***' if subset_info[j][1] in ['nq'] else sig_s for j in range(len(subset_info))\n",
    "])):\n",
    "    ax.text(i, d_val + (0.01 if d_val >= 0 else -0.03),\n",
    "            f\"{d_val:+.3f}\", ha='center',\n",
    "            va='bottom' if d_val >= 0 else 'top', fontsize=9, fontweight='bold')\n",
    "\n",
    "# Re-compute significance for each bar\n",
    "for i, (label, subset_key, task_type) in enumerate(subset_info):\n",
    "    if subset_key == 'drop_number':\n",
    "        sr = [r for r in all_results if r['dataset'] == 'drop' and r.get('answer_type') == 'number']\n",
    "    elif subset_key == 'drop_span':\n",
    "        sr = [r for r in all_results if r['dataset'] == 'drop' and r.get('answer_type') == 'span']\n",
    "    else:\n",
    "        sr = [r for r in all_results if r['dataset'] == subset_key]\n",
    "    if len(sr) >= 10:\n",
    "        bare_s = np.array([r['bare'] for r in sr])\n",
    "        hero_s = np.array([r['values_hero'] for r in sr])\n",
    "        delta_s = bare_s - hero_s\n",
    "        _, p_s = stats.ttest_1samp(delta_s, 0)\n",
    "        sig_s = '***' if p_s < 0.001 else '**' if p_s < 0.01 else '*' if p_s < 0.05 else ''\n",
    "        if sig_s:\n",
    "            ax.text(i, bar_ds[i] + (0.035 if bar_ds[i] >= 0 else -0.055),\n",
    "                    sig_s, ha='center', va='bottom' if bar_ds[i] >= 0 else 'top', fontsize=10)\n",
    "\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels(bar_labels, fontsize=9)\n",
    "ax.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "ax.set_ylabel(\"Cohen's d (positive = helps)\")\n",
    "ax.set_title(\"(a) Hero Layer Effect by Dataset-Subset\\n(colored by task type)\")\n",
    "\n",
    "# Legend for task types\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [Patch(facecolor=colors_task['retrieval'], label='Retrieval'),\n",
    "                   Patch(facecolor=colors_task['computation'], label='Computation')]\n",
    "ax.legend(handles=legend_elements, fontsize=9)\n",
    "\n",
    "# ---- Panel (b): Hard-sample delta distributions (violin/box) ----\n",
    "ax = axes[0, 1]\n",
    "\n",
    "hard_data = []\n",
    "hard_labels = []\n",
    "hard_colors_list = []\n",
    "for label, subset_key, task_type in subset_info:\n",
    "    if subset_key == 'drop_number':\n",
    "        sr = [r for r in all_results if r['dataset'] == 'drop' and r.get('answer_type') == 'number']\n",
    "    elif subset_key == 'drop_span':\n",
    "        sr = [r for r in all_results if r['dataset'] == 'drop' and r.get('answer_type') == 'span']\n",
    "    else:\n",
    "        sr = [r for r in all_results if r['dataset'] == subset_key]\n",
    "    hard = [r['bare'] - r['values_hero'] for r in sr if r['bare'] > 0.5]\n",
    "    if len(hard) >= 10:\n",
    "        hard_data.append(hard)\n",
    "        hard_labels.append(f\"{label}\\n(n={len(hard)})\")\n",
    "        hard_colors_list.append(colors_task[task_type])\n",
    "\n",
    "if hard_data:\n",
    "    bp = ax.boxplot(hard_data, labels=hard_labels, showfliers=True, patch_artist=True,\n",
    "                    medianprops={'color': 'black', 'linewidth': 2},\n",
    "                    flierprops={'markersize': 3, 'alpha': 0.5})\n",
    "    for patch, color in zip(bp['boxes'], hard_colors_list):\n",
    "        patch.set_facecolor(color)\n",
    "        patch.set_alpha(0.7)\n",
    "\n",
    "ax.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "ax.set_ylabel(\"Delta (bare - hero, positive = hero helps)\")\n",
    "ax.set_title(\"(b) Hard-Sample Delta Distributions\\n(bare > 0.5)\")\n",
    "\n",
    "# ---- Panel (c): Hardness gradient: hero d by quintile for NQ vs DROP-number ----\n",
    "ax = axes[1, 0]\n",
    "quintile_labels = ['Q1\\n(easy)', 'Q2', 'Q3', 'Q4', 'Q5\\n(hard)']\n",
    "\n",
    "for ds_label, ds_filter, color, marker in [\n",
    "    ('NQ', lambda r: r['dataset'] == 'nq', '#2ca02c', 'o'),\n",
    "    ('DROP-number', lambda r: r['dataset'] == 'drop' and r.get('answer_type') == 'number', '#d62728', 's'),\n",
    "    ('DROP-span', lambda r: r['dataset'] == 'drop' and r.get('answer_type') == 'span', '#ff7f0e', '^'),\n",
    "    ('BoolQ', lambda r: r['dataset'] == 'boolq', '#1f77b4', 'D'),\n",
    "]:\n",
    "    ds_r = [r for r in all_results if ds_filter(r)]\n",
    "    if len(ds_r) < 50:\n",
    "        continue\n",
    "    bare_arr = np.array([r['bare'] for r in ds_r])\n",
    "    hero_arr = np.array([r['values_hero'] for r in ds_r])\n",
    "    delta_arr = bare_arr - hero_arr\n",
    "    quintile_boundaries = np.percentile(bare_arr, [20, 40, 60, 80])\n",
    "\n",
    "    q_ds = []\n",
    "    q_ns = []\n",
    "    for q in range(5):\n",
    "        if q < 4:\n",
    "            lo = quintile_boundaries[q-1] if q > 0 else -np.inf\n",
    "            hi = quintile_boundaries[q]\n",
    "        else:\n",
    "            lo = quintile_boundaries[3]\n",
    "            hi = np.inf\n",
    "        mask = (bare_arr > lo) & (bare_arr <= hi)\n",
    "        if q == 0:\n",
    "            mask = bare_arr <= quintile_boundaries[0]\n",
    "        n_q = int(np.sum(mask))\n",
    "        if n_q >= 5:\n",
    "            q_ds.append(cohens_d(delta_arr[mask]))\n",
    "        else:\n",
    "            q_ds.append(np.nan)\n",
    "        q_ns.append(n_q)\n",
    "\n",
    "    valid_q = [(i, d) for i, d in enumerate(q_ds) if not np.isnan(d)]\n",
    "    if valid_q:\n",
    "        xs, ys = zip(*valid_q)\n",
    "        ax.plot(xs, ys, marker=marker, linewidth=2, markersize=7, label=ds_label, color=color)\n",
    "\n",
    "ax.set_xticks(range(5))\n",
    "ax.set_xticklabels(quintile_labels, fontsize=8)\n",
    "ax.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "ax.set_ylabel(\"Cohen's d (hero vs bare)\")\n",
    "ax.set_xlabel(\"Bare NLL Quintile\")\n",
    "ax.set_title(\"(c) Hardness Gradient: Hero Effect by Quintile\")\n",
    "ax.legend(fontsize=8)\n",
    "\n",
    "# ---- Panel (d): Bare NLL distributions by dataset (ceiling check) ----\n",
    "ax = axes[1, 1]\n",
    "bare_by_subset = []\n",
    "subset_labels_plot = []\n",
    "for label, subset_key, task_type in subset_info:\n",
    "    if subset_key == 'drop_number':\n",
    "        sr = [r for r in all_results if r['dataset'] == 'drop' and r.get('answer_type') == 'number']\n",
    "    elif subset_key == 'drop_span':\n",
    "        sr = [r for r in all_results if r['dataset'] == 'drop' and r.get('answer_type') == 'span']\n",
    "    else:\n",
    "        sr = [r for r in all_results if r['dataset'] == subset_key]\n",
    "    if sr:\n",
    "        bare_by_subset.append([r['bare'] for r in sr])\n",
    "        pct_f = 100 * np.mean(np.array([r['bare'] for r in sr]) < 0.01)\n",
    "        subset_labels_plot.append(f\"{label}\\n({pct_f:.0f}% floor)\")\n",
    "\n",
    "if bare_by_subset:\n",
    "    bp = ax.boxplot(bare_by_subset, labels=subset_labels_plot, showfliers=False, patch_artist=True,\n",
    "                    medianprops={'color': 'red', 'linewidth': 2})\n",
    "    for patch in bp['boxes']:\n",
    "        patch.set_facecolor('#8ecae6')\n",
    "        patch.set_alpha(0.7)\n",
    "\n",
    "ax.axhline(y=0.01, color='red', linestyle='--', alpha=0.3, label='Floor threshold (0.01)')\n",
    "ax.set_ylabel(\"Bare NLL\")\n",
    "ax.set_title(\"(d) Bare NLL Distributions (ceiling check)\")\n",
    "ax.legend(fontsize=7)\n",
    "\n",
    "plt.suptitle('Exp 30: Retrieval vs Reasoning Task-Type Dissociation (Gemma 3 4B)',\n",
    "             fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'analysis_plots.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"Plots saved to {RESULTS_DIR / 'analysis_plots.png'}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 15: Save results.json + CSV + verdict\n",
    "\n",
    "# --- CSV ---\n",
    "with open(CSV_PATH, 'w', newline='') as f:\n",
    "    fieldnames = ['query_idx', 'dataset', 'query', 'answer', 'word_count',\n",
    "                  'doc_token_len', 'answer_token_len', 'answer_type',\n",
    "                  'bare', 'sf_trunc', 'values_early', 'values_hero']\n",
    "    writer = csv.DictWriter(f, fieldnames=fieldnames, extrasaction='ignore')\n",
    "    writer.writeheader()\n",
    "    for r in all_results:\n",
    "        writer.writerow({k: r.get(k, '') for k in fieldnames})\n",
    "print(f\"CSV saved: {CSV_PATH}\")\n",
    "\n",
    "# --- Compute verdict inputs ---\n",
    "\n",
    "# Hero d on hard retrieval samples\n",
    "retrieval_hard = []\n",
    "for r in all_results:\n",
    "    task_type = task_type_map.get(r['query_idx'])\n",
    "    if task_type == 'retrieval' and r['bare'] > 0.5:\n",
    "        retrieval_hard.append(r['bare'] - r['values_hero'])\n",
    "retrieval_hard_d = cohens_d(np.array(retrieval_hard)) if len(retrieval_hard) >= 10 else float('nan')\n",
    "\n",
    "# Hero d on hard computation samples\n",
    "computation_hard = []\n",
    "for r in all_results:\n",
    "    task_type = task_type_map.get(r['query_idx'])\n",
    "    if task_type == 'computation' and r['bare'] > 0.5:\n",
    "        computation_hard.append(r['bare'] - r['values_hero'])\n",
    "computation_hard_d = cohens_d(np.array(computation_hard)) if len(computation_hard) >= 10 else float('nan')\n",
    "\n",
    "# Regression significance\n",
    "regression_sig = p_values[2] < 0.05 if len(p_values) > 2 else False\n",
    "\n",
    "print(f\"\\nVerdict inputs:\")\n",
    "print(f\"  Retrieval hard d: {retrieval_hard_d:+.3f} (n={len(retrieval_hard)})\")\n",
    "print(f\"  Computation hard d: {computation_hard_d:+.3f} (n={len(computation_hard)})\")\n",
    "print(f\"  Regression beta_2 p: {p_values[2]:.2e}\")\n",
    "print(f\"  Regression significant: {regression_sig}\")\n",
    "\n",
    "# --- Verdict ---\n",
    "if retrieval_hard_d > 0.15 and computation_hard_d < 0 and regression_sig:\n",
    "    verdict = (\"SUPPORTED: Hero layers selectively help retrieval. \"\n",
    "               f\"Retrieval hard d={retrieval_hard_d:+.3f}, \"\n",
    "               f\"computation hard d={computation_hard_d:+.3f}, \"\n",
    "               f\"beta_2 p={p_values[2]:.2e}\")\n",
    "elif np.mean(np.array([r['bare'] for r in all_results]) < 0.01) > 0.5 and not regression_sig:\n",
    "    verdict = (\"CONFOUNDED: Cannot separate task type from ceiling effects. \"\n",
    "               f\"Overall {100*np.mean(np.array([r['bare'] for r in all_results]) < 0.01):.0f}% at floor.\")\n",
    "else:\n",
    "    verdict = (\"INCONCLUSIVE: \" +\n",
    "               f\"Retrieval hard d={retrieval_hard_d:+.3f}, \"\n",
    "               f\"computation hard d={computation_hard_d:+.3f}, \"\n",
    "               f\"regression p={p_values[2]:.2e}\")\n",
    "\n",
    "# Hero scorecard\n",
    "hero_scorecard = {}\n",
    "for ds_name in dataset_names:\n",
    "    if ds_name in analysis and 'values_hero' in analysis[ds_name]:\n",
    "        hero_scorecard[ds_name] = analysis[ds_name]['values_hero']['cohens_d']\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"VERDICT: {verdict}\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"\\nHero scorecard (this experiment):\")\n",
    "for ds, d in hero_scorecard.items():\n",
    "    p = analysis[ds]['values_hero']['p_value']\n",
    "    sig = '***' if p < 0.001 else '**' if p < 0.01 else '*' if p < 0.05 else 'ns'\n",
    "    print(f\"  {ds:15s}: d={d:+.3f} {sig}\")\n",
    "\n",
    "# Reference table\n",
    "print(f\"\\nUpdated hero scorecard (all experiments):\")\n",
    "print(\"  MARCO:    d=+0.472*** (Exp 07, Mistral)\")\n",
    "print(\"  NQ:       d=+0.213*** (Exp 27b, Gemma)\")\n",
    "hero_nq = hero_scorecard.get('nq', '?')\n",
    "if isinstance(hero_nq, float):\n",
    "    print(f\"  NQ (30):  d={hero_nq:+.3f} (this experiment)\")\n",
    "print(\"  TriviaQA: d=+0.000 (Exp 27b, ceiling)\")\n",
    "print(\"  HotpotQA: d=-0.069 (Exp 27b)\")\n",
    "print(\"  AdvQA:    d=+0.026 (Exp 29, ceiling)\")\n",
    "print(\"  CoQA:     d=+0.070 (Exp 29, ceiling)\")\n",
    "print(\"  DROP:     d=-0.152** (Exp 29)\")\n",
    "hero_drop = hero_scorecard.get('drop', '?')\n",
    "if isinstance(hero_drop, float):\n",
    "    print(f\"  DROP (30): d={hero_drop:+.3f} (this experiment)\")\n",
    "hero_boolq = hero_scorecard.get('boolq', '?')\n",
    "if isinstance(hero_boolq, float):\n",
    "    print(f\"  BoolQ:    d={hero_boolq:+.3f} (this experiment, NEW)\")\n",
    "\n",
    "# --- results.json ---\n",
    "final = {\n",
    "    'experiment': 'exp30_retrieval_vs_reasoning',\n",
    "    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'config': {\n",
    "        'model_name': MODEL_NAME,\n",
    "        'model_type': 'gemma3',\n",
    "        'seed': SEED,\n",
    "        'n_per_dataset': N_PER_DATASET,\n",
    "        'max_doc_tokens': MAX_DOC_TOKENS,\n",
    "        'conditions': CONDITION_NAMES,\n",
    "        'early_layer_cutoff': EARLY_LAYER_CUTOFF,\n",
    "        'hero_layers': HERO_LAYERS,\n",
    "        'prefix': STATIC_FACT,\n",
    "        'prefix_token_len': PREFIX_TOKEN_LEN,\n",
    "        'datasets': dataset_names,\n",
    "    },\n",
    "    'per_dataset_analysis': analysis,\n",
    "    'drop_split_analysis': drop_split_analysis,\n",
    "    'hard_hero_analysis': hard_hero_ds,\n",
    "    'regression': {\n",
    "        'beta_0': float(b0),\n",
    "        'beta_1_bare': float(b1),\n",
    "        'beta_2_is_retrieval': float(b2),\n",
    "        'se_beta': [float(s) for s in se_beta],\n",
    "        'p_values': [float(p) for p in p_values],\n",
    "        'r_squared': float(r_squared),\n",
    "    },\n",
    "    'ceiling_status': {\n",
    "        ds: float(np.mean(np.array([r['bare'] for r in all_results if r['dataset'] == ds]) < 0.01) * 100)\n",
    "        for ds in dataset_names\n",
    "    },\n",
    "    'verdict': verdict,\n",
    "    'hero_scorecard': hero_scorecard,\n",
    "    'per_sample_results': all_results,\n",
    "}\n",
    "\n",
    "with open(FINAL_RESULTS_PATH, 'w') as f:\n",
    "    json.dump(final, f, indent=2)\n",
    "\n",
    "print(f\"\\nResults saved to {FINAL_RESULTS_PATH}\")\n",
    "print(f\"File size: {FINAL_RESULTS_PATH.stat().st_size / 1024:.1f} KB\")\n",
    "print(f\"\\nDone!\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 16: GPU cleanup\n",
    "import gc\n",
    "\n",
    "print(\"Cleaning up GPU memory...\")\n",
    "mem_before = torch.cuda.memory_allocated() / 1e9\n",
    "\n",
    "del model\n",
    "del tokenizer\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "mem_after = torch.cuda.memory_allocated() / 1e9\n",
    "print(f\"GPU memory: {mem_before:.2f} GB -> {mem_after:.2f} GB\")\n",
    "print(\"Cleanup complete.\")"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}