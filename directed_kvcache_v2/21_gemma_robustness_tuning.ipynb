{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exp 21: Gemma Mechanism Robustness & Tuning\n",
    "\n",
    "## Motivation\n",
    "\n",
    "Exp 19 discovered that **layer-selective value contamination** (layers 0-16 only)\n",
    "amplifies Gemma's priming signal from d=+0.056 to d=**+0.211** (p=3.7e-15). This works\n",
    "because late-layer values (17-33) carry interference that cancels the early-layer benefit.\n",
    "\n",
    "Two critical questions remain:\n",
    "\n",
    "### Q1: Does this survive document length scaling?\n",
    "Exp 20 showed Mistral's full priming vanishes by ~256 tokens. Does Gemma's layer-selective\n",
    "approach also fail at long lengths, or does removing late-layer interference make it more robust?\n",
    "\n",
    "### Q2: Is cutoff=17 optimal?\n",
    "Exp 19 only tested one boundary (layers 0-16). Sweeping cutoffs may find a better split point.\n",
    "\n",
    "## Design\n",
    "\n",
    "### Part 1: Length Generalization \"Kryptonite\" Test\n",
    "- **N=500 queries** from MS MARCO v1.1 (positive passages only)\n",
    "- **Pad lengths**: `[None, 512, 1024, 2048]`\n",
    "- **Fixed cutoff**: layers 0-15 (`list(range(16))`)\n",
    "- **Method**: `replace_values_at_layers(bare_cache, sf_trunc_cache, list(range(16)))`\n",
    "\n",
    "### Part 2: Layer Boundary Sweep\n",
    "- **N=200 queries** (first 200 from Part 1)\n",
    "- **Cutoffs**: `[8, 12, 16, 20, 24]`\n",
    "- **No padding** (original passage length)\n",
    "\n",
    "## Reference Values\n",
    "\n",
    "| Source | Condition | d |\n",
    "|--------|-----------|---|\n",
    "| Exp 19 (Gemma) | values_only (all layers) | +0.056 |\n",
    "| Exp 19 (Gemma) | values_early_layers (0-16) | +0.211 |\n",
    "| Exp 20 (Mistral) | full priming @ original | +0.303 |\n",
    "| Exp 20 (Mistral) | full priming @ 256 tok | +0.114 (ns) |\n",
    "| Exp 20 (Mistral) | full priming @ 512 tok | +0.034 (ns) |"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 1: Setup\n",
    "import os\n",
    "os.umask(0o000)\n",
    "\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import gc\n",
    "from pathlib import Path\n",
    "\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "RESULTS_DIR = Path(\"results/exp21\")\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CHECKPOINT_P1_PATH = RESULTS_DIR / \"checkpoint_part1.json\"\n",
    "CHECKPOINT_P2_PATH = RESULTS_DIR / \"checkpoint_part2.json\"\n",
    "FINAL_RESULTS_PATH = RESULTS_DIR / \"results.json\"\n",
    "CSV_P1_PATH = RESULTS_DIR / \"part1_results.csv\"\n",
    "CSV_P2_PATH = RESULTS_DIR / \"part2_results.csv\"\n",
    "\n",
    "print(f\"SEED: {SEED}\")\n",
    "print(f\"Results directory: {RESULTS_DIR}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 2: Load Gemma 3 4B via load_model()\n",
    "sys.path.insert(0, \".\")\n",
    "\n",
    "from lib.config import ExperimentConfig\n",
    "from lib.model_utils import load_model\n",
    "\n",
    "MODEL_NAME = \"google/gemma-3-4b-it\"\n",
    "\n",
    "exp_config = ExperimentConfig(\n",
    "    model_name=MODEL_NAME,\n",
    "    model_type=\"gemma3\",\n",
    "    compute_dtype=\"auto\",  # resolves to bfloat16 for gemma3\n",
    "    use_4bit=True,\n",
    "    num_samples=2000,\n",
    "    seed=SEED,\n",
    ")\n",
    "\n",
    "print(f\"Loading {MODEL_NAME} (4-bit, bfloat16)...\")\n",
    "model, tokenizer = load_model(exp_config)\n",
    "\n",
    "# Architecture diagnostics\n",
    "from lib.kv_cache import _get_text_config, _get_head_dim, _get_rope_theta_for_layer, _get_cache_keys, _ensure_dynamic_cache\n",
    "\n",
    "text_config = _get_text_config(model.config)\n",
    "print(f\"\\nModel loaded successfully.\")\n",
    "print(f\"  Model class: {type(model).__name__}\")\n",
    "print(f\"  Text config class: {type(text_config).__name__}\")\n",
    "print(f\"  Hidden size: {text_config.hidden_size}\")\n",
    "print(f\"  Num layers: {text_config.num_hidden_layers}\")\n",
    "print(f\"  Num attention heads: {text_config.num_attention_heads}\")\n",
    "print(f\"  Num KV heads: {text_config.num_key_value_heads}\")\n",
    "print(f\"  Head dim: {_get_head_dim(model.config)}\")\n",
    "print(f\"  BOS token ID: {tokenizer.bos_token_id}\")\n",
    "\n",
    "# Per-layer RoPE diagnostics\n",
    "thetas = set()\n",
    "for layer_idx in range(text_config.num_hidden_layers):\n",
    "    thetas.add(_get_rope_theta_for_layer(model.config, layer_idx))\n",
    "print(f\"  Unique RoPE thetas: {sorted(thetas)}\")\n",
    "\n",
    "# Verify dtype\n",
    "sample_ids = tokenizer(\"test\", return_tensors=\"pt\")['input_ids'].to(exp_config.device)\n",
    "with torch.no_grad():\n",
    "    out = model(sample_ids, use_cache=True)\n",
    "    cache_check = _ensure_dynamic_cache(out.past_key_values)\n",
    "    k0 = _get_cache_keys(cache_check, 0)\n",
    "    print(f\"  Cache key dtype: {k0.dtype}\")\n",
    "    print(f\"  Cache key shape: {k0.shape}  (batch, kv_heads, seq, head_dim)\")\n",
    "del out, sample_ids\n",
    "torch.cuda.empty_cache()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 3: Lib imports + templates + constants\n",
    "from lib.kv_cache import (\n",
    "    _get_cache_keys,\n",
    "    _get_cache_values,\n",
    "    _set_cache_keys,\n",
    "    _set_cache_values,\n",
    "    _ensure_dynamic_cache,\n",
    "    extract_and_truncate_cache_with_bos,\n",
    "    correct_rope_positions_with_bos,\n",
    "    score_answer_with_cache,\n",
    "    deepcopy_cache,\n",
    "    replace_values_at_layers,\n",
    "    _get_text_config,\n",
    ")\n",
    "from lib.analysis import cohens_d\n",
    "from lib.data import count_words\n",
    "from scipy import stats\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Templates \u2014 bare text, no \"Document:\\n\" framing\n",
    "SURROGATE_PREFIX_TEMPLATE = \"{surrogate}\\n\"\n",
    "DOCUMENT_TEMPLATE = \"{document}\"\n",
    "QUERY_TEMPLATE = \"\\nQuery: {query}\\nAnswer:\"\n",
    "ANSWER_TEMPLATE = \" {answer}\"\n",
    "\n",
    "# Prefix text\n",
    "from lib.surrogate import STATIC_SURROGATE_QUERIES\n",
    "STATIC_FACT = STATIC_SURROGATE_QUERIES['static_factual']['query']\n",
    "\n",
    "# Experiment parameters\n",
    "N_PART1 = 500\n",
    "N_PART2 = 200  # first 200 from Part 1\n",
    "MAX_PASSAGE_WORDS = 300\n",
    "CHECKPOINT_EVERY = 25\n",
    "\n",
    "# Part 1: Length generalization\n",
    "# Gemma 3 sliding_window=1024 \u2192 max safe cache = 1023 positions (incl. BOS).\n",
    "# Beyond this, sliding attention layers truncate their cache, breaking our\n",
    "# cross-cache value substitution. Cap at 900 tokens (+ BOS = 901 < 1024).\n",
    "PAD_LENGTHS = [None, 256, 512, 900]\n",
    "\n",
    "# Part 2: Layer boundary sweep\n",
    "CUTOFFS = [8, 12, 16, 20, 24]\n",
    "\n",
    "# Fixed cutoff for Part 1 (layers 0-15 = 16 layers)\n",
    "FIXED_CUTOFF = 16\n",
    "\n",
    "# Reference values\n",
    "EXP19_REF = {\n",
    "    'values_only_d': 0.056,\n",
    "    'values_early_layers_d': 0.211,  # layers 0-16 (17 layers)\n",
    "}\n",
    "EXP20_REF = {\n",
    "    'original_d': 0.303,\n",
    "    '256_d': 0.114,\n",
    "    '512_d': 0.034,\n",
    "}\n",
    "\n",
    "print(\"Config ready\")\n",
    "print(f\"  Model: {MODEL_NAME}\")\n",
    "print(f\"  Part 1: N={N_PART1}, pad_lengths={PAD_LENGTHS}, cutoff={FIXED_CUTOFF}\")\n",
    "print(f\"  Part 2: N={N_PART2}, cutoffs={CUTOFFS}, no padding\")\n",
    "print(f\"  Static fact prefix: '{STATIC_FACT}'\")\n",
    "print(f\"\\nExp 19 Gemma reference:\")\n",
    "for k, v in EXP19_REF.items():\n",
    "    print(f\"    {k}: {v:+.3f}\")\n",
    "print(f\"\\nExp 20 Mistral reference:\")\n",
    "for k, v in EXP20_REF.items():\n",
    "    print(f\"    {k}: {v:+.3f}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 4: Load MS MARCO v1.1, filter positive passages, build padding pool\n",
    "from datasets import load_dataset\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"LOADING MS MARCO v1.1 \u2014 POSITIVE PASSAGES ONLY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "dataset = load_dataset(\"microsoft/ms_marco\", \"v1.1\", split=\"validation\",\n",
    "                        trust_remote_code=True)\n",
    "print(f\"Total items in validation: {len(dataset)}\")\n",
    "\n",
    "queries = []\n",
    "padding_passages = []\n",
    "eval_passage_set = set()\n",
    "np.random.seed(SEED)\n",
    "\n",
    "for item in tqdm(dataset, desc=\"Filtering\"):\n",
    "    passages_info = item.get('passages', {})\n",
    "    passage_texts = passages_info.get('passage_text', [])\n",
    "    is_selected = passages_info.get('is_selected', [])\n",
    "    query = item.get('query', '')\n",
    "    answers = item.get('answers', [])\n",
    "    well_formed = item.get('wellFormedAnswers', [])\n",
    "\n",
    "    if not passage_texts or not query:\n",
    "        continue\n",
    "\n",
    "    # Get best answer\n",
    "    answer = None\n",
    "    if well_formed and len(well_formed) > 0 and well_formed[0] != '[]':\n",
    "        answer = well_formed[0]\n",
    "    elif answers and len(answers) > 0 and answers[0] != 'No Answer Present.':\n",
    "        answer = answers[0]\n",
    "    else:\n",
    "        # Still collect non-eval passages for padding pool\n",
    "        for p in passage_texts:\n",
    "            if count_words(p) <= MAX_PASSAGE_WORDS:\n",
    "                padding_passages.append(p)\n",
    "        continue\n",
    "\n",
    "    # Find positive passage(s)\n",
    "    found_positive = False\n",
    "    for i, (ptext, sel) in enumerate(zip(passage_texts, is_selected)):\n",
    "        if sel == 1 and count_words(ptext) <= MAX_PASSAGE_WORDS:\n",
    "            if len(queries) < N_PART1 * 3:  # collect 3x for shuffling\n",
    "                queries.append({\n",
    "                    'query': query,\n",
    "                    'answer': answer,\n",
    "                    'passage': ptext,\n",
    "                    'word_count': count_words(ptext),\n",
    "                })\n",
    "                eval_passage_set.add(ptext)\n",
    "                found_positive = True\n",
    "                break\n",
    "\n",
    "    # Collect non-eval passages for padding pool\n",
    "    for p in passage_texts:\n",
    "        if p not in eval_passage_set and count_words(p) <= MAX_PASSAGE_WORDS:\n",
    "            padding_passages.append(p)\n",
    "\n",
    "np.random.shuffle(queries)\n",
    "queries = queries[:N_PART1]\n",
    "N = len(queries)\n",
    "\n",
    "print(f\"\\nSelected {N} queries with positive passages\")\n",
    "print(f\"Word counts: mean={np.mean([q['word_count'] for q in queries]):.0f}, \"\n",
    "      f\"min={min(q['word_count'] for q in queries)}, \"\n",
    "      f\"max={max(q['word_count'] for q in queries)}\")\n",
    "\n",
    "# Build padding pool (pre-tokenize)\n",
    "print(f\"\\nPadding pool passages: {len(padding_passages):,}\")\n",
    "padding_text = ' '.join(padding_passages)\n",
    "padding_ids = tokenizer.encode(padding_text, add_special_tokens=False)\n",
    "print(f\"Padding pool tokens: {len(padding_ids):,}\")\n",
    "\n",
    "max_needed = max(tl for tl in PAD_LENGTHS if tl is not None) * N_PART1\n",
    "print(f\"Max tokens needed: {max_needed:,}\")\n",
    "assert len(padding_ids) > max_needed, (\n",
    "    f\"Padding pool too small: {len(padding_ids):,} < {max_needed:,}\"\n",
    ")\n",
    "print(f\"Pool is {len(padding_ids) / max_needed:.1f}x the max needed. OK.\")\n",
    "\n",
    "del dataset\n",
    "gc.collect()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 5: Tokenize prefix and verify BPE boundaries\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PREFIX TOKENIZATION \u2014 GEMMA 3 4B\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "sf_str = SURROGATE_PREFIX_TEMPLATE.format(surrogate=STATIC_FACT)\n",
    "\n",
    "sf_ids = tokenizer(sf_str, return_tensors=\"pt\",\n",
    "                    add_special_tokens=False)['input_ids'].to(exp_config.device)\n",
    "\n",
    "print(f\"\\nStatic fact prefix: '{STATIC_FACT}'\")\n",
    "print(f\"  Formatted: '{sf_str.strip()}'\")\n",
    "print(f\"  Token length: {sf_ids.shape[1]}\")\n",
    "\n",
    "# Verify BPE boundary consistency\n",
    "print(\"\\nBPE BOUNDARY CHECK (first passage):\")\n",
    "example_doc = queries[0]['passage']\n",
    "concat = sf_str + DOCUMENT_TEMPLATE.format(document=example_doc)\n",
    "concat_enc = tokenizer(concat, add_special_tokens=True)['input_ids']\n",
    "prefix_enc = tokenizer(sf_str, add_special_tokens=True)['input_ids']\n",
    "doc_ids_from_concat = concat_enc[len(prefix_enc):]\n",
    "\n",
    "bare_doc_enc = tokenizer(DOCUMENT_TEMPLATE.format(document=example_doc),\n",
    "                          add_special_tokens=False)['input_ids']\n",
    "match = sum(1 for a, b in zip(doc_ids_from_concat, bare_doc_enc) if a == b)\n",
    "total = max(len(bare_doc_enc), 1)\n",
    "print(f\"  static_fact: {match}/{total} tokens match ({100*match/total:.1f}%)\")\n",
    "\n",
    "# Condition explanations\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"EXPERIMENTAL CONDITIONS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\n### Part 1: Length Generalization ###\")\n",
    "print(f\"  Fixed cutoff: {FIXED_CUTOFF} layers (layers 0-{FIXED_CUTOFF-1})\")\n",
    "print(f\"  Pad lengths: {PAD_LENGTHS}\")\n",
    "print(\"  Per sample per length:\")\n",
    "print(\"    1. Pad doc_ids to target length from pre-tokenized padding pool\")\n",
    "print(\"    2. Forward pass 1 (bare):    [BOS][padded_doc_ids]\")\n",
    "print(\"    3. Forward pass 2 (primed):  [BOS][sf_prefix_ids][padded_doc_ids]\")\n",
    "print(\"       -> truncate prefix -> RoPE correct\")\n",
    "print(f\"    4. values_early_layers = replace_values_at_layers(bare, sf_trunc, range({FIXED_CUTOFF}))\")\n",
    "print(\"    5. Score bare + values_early_layers with deepcopy_cache()\")\n",
    "\n",
    "print(\"\\n### Part 2: Layer Boundary Sweep ###\")\n",
    "print(f\"  Cutoffs: {CUTOFFS}\")\n",
    "print(\"  No padding (original passage length)\")\n",
    "print(\"  Per sample:\")\n",
    "print(\"    1. Forward pass (bare):    [BOS][doc_ids]\")\n",
    "print(\"    2. Forward pass (primed):  [BOS][sf_prefix_ids][doc_ids]\")\n",
    "print(\"       -> truncate prefix -> RoPE correct\")\n",
    "print(\"    3. Score bare once\")\n",
    "print(f\"    4. For each cutoff in {CUTOFFS}:\")\n",
    "print(\"       vel_cache = replace_values_at_layers(bare, sf_trunc, range(cutoff))\")\n",
    "print(\"       Score vel_cache\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 6: Part 1 \u2014 Length Generalization (N=500, 4 pad lengths, cutoff=16)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"PART 1: LENGTH GENERALIZATION ({N_PART1} queries, {len(PAD_LENGTHS)} lengths)\")\n",
    "print(f\"Fixed cutoff: {FIXED_CUTOFF} layers\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Checkpoint resume\n",
    "p1_results = []\n",
    "p1_start_idx = 0\n",
    "\n",
    "if CHECKPOINT_P1_PATH.exists():\n",
    "    with open(CHECKPOINT_P1_PATH, 'r') as f:\n",
    "        ckpt = json.load(f)\n",
    "    ckpt_queries = ckpt.get('query_texts', [])\n",
    "    current_queries = [q['query'] for q in queries[:N_PART1]]\n",
    "    if ckpt_queries == current_queries:\n",
    "        p1_results = ckpt['results']\n",
    "        p1_start_idx = len(p1_results)\n",
    "        print(f\"Resuming from checkpoint: {p1_start_idx}/{N_PART1}\")\n",
    "    else:\n",
    "        print(\"Checkpoint query mismatch. Starting fresh.\")\n",
    "else:\n",
    "    print(\"No checkpoint found. Starting fresh.\")\n",
    "\n",
    "layer_indices = list(range(FIXED_CUTOFF))\n",
    "t_start = time.time()\n",
    "\n",
    "for qidx in tqdm(range(p1_start_idx, N_PART1), initial=p1_start_idx, total=N_PART1,\n",
    "                  desc=\"Part 1\"):\n",
    "    qdata = queries[qidx]\n",
    "    query_prompt = QUERY_TEMPLATE.format(query=qdata['query'])\n",
    "    answer_text = ANSWER_TEMPLATE.format(answer=qdata['answer'])\n",
    "    passage = qdata['passage']\n",
    "    document_text = DOCUMENT_TEMPLATE.format(document=passage)\n",
    "\n",
    "    # Matched tokenization: tokenize concatenated then split\n",
    "    full_text = sf_str + document_text\n",
    "    full_enc = tokenizer(full_text, return_tensors=\"pt\",\n",
    "                          add_special_tokens=True, padding=False, truncation=False)\n",
    "    full_ids = full_enc['input_ids'].to(exp_config.device)\n",
    "\n",
    "    sf_prefix_enc = tokenizer(sf_str, return_tensors=\"pt\",\n",
    "                               add_special_tokens=True, padding=False, truncation=False)\n",
    "    sf_prefix_len_matched = sf_prefix_enc['input_ids'].shape[1]\n",
    "\n",
    "    bos_id = full_ids[:, :1]\n",
    "    base_doc_ids = full_ids[:, sf_prefix_len_matched:]\n",
    "    base_doc_len = base_doc_ids.shape[1]\n",
    "\n",
    "    del full_enc, full_ids, sf_prefix_enc\n",
    "\n",
    "    query_rows = []\n",
    "\n",
    "    for pad_length in PAD_LENGTHS:\n",
    "        # Pad doc_ids at token level if needed\n",
    "        if pad_length is not None and base_doc_len < pad_length:\n",
    "            pad_needed = pad_length - base_doc_len\n",
    "            max_start = len(padding_ids) - pad_needed\n",
    "            start = np.random.randint(0, max_start)\n",
    "            pad_tensor = torch.tensor([padding_ids[start:start + pad_needed]],\n",
    "                                       device=exp_config.device)\n",
    "            doc_ids = torch.cat([base_doc_ids, pad_tensor], dim=1)\n",
    "        else:\n",
    "            doc_ids = base_doc_ids\n",
    "\n",
    "        doc_len = doc_ids.shape[1]\n",
    "        context_len = 1 + doc_len  # BOS + doc\n",
    "\n",
    "        # Forward pass 1: BARE\n",
    "        bare_input = torch.cat([bos_id, doc_ids], dim=1)\n",
    "        with torch.no_grad():\n",
    "            bare_out = model(input_ids=bare_input,\n",
    "                             attention_mask=torch.ones_like(bare_input),\n",
    "                             use_cache=True, return_dict=True)\n",
    "        bare_cache = _ensure_dynamic_cache(bare_out.past_key_values)\n",
    "        del bare_out\n",
    "\n",
    "        # Forward pass 2: PRIMED (static_fact prefix)\n",
    "        primed_input = torch.cat([bos_id, sf_ids, doc_ids], dim=1)\n",
    "        with torch.no_grad():\n",
    "            primed_out = model(input_ids=primed_input,\n",
    "                               attention_mask=torch.ones_like(primed_input),\n",
    "                               use_cache=True, return_dict=True)\n",
    "        primed_full = _ensure_dynamic_cache(primed_out.past_key_values)\n",
    "        del primed_out\n",
    "\n",
    "        # Truncate: keep [BOS] + [last doc_len positions]\n",
    "        trunc_raw = extract_and_truncate_cache_with_bos(primed_full, doc_len)\n",
    "        prefix_offset = sf_ids.shape[1]\n",
    "        del primed_full\n",
    "\n",
    "        # RoPE correct\n",
    "        sf_trunc_cache = deepcopy_cache(trunc_raw)\n",
    "        correct_rope_positions_with_bos(sf_trunc_cache, prefix_offset, model)\n",
    "        del trunc_raw\n",
    "\n",
    "        # Build values_early_layers: bare keys + primed values at layers 0-15\n",
    "        vel_cache = replace_values_at_layers(bare_cache, sf_trunc_cache, layer_indices)\n",
    "\n",
    "        # Score bare\n",
    "        bare_nll = score_answer_with_cache(\n",
    "            deepcopy_cache(bare_cache), context_len,\n",
    "            query_prompt, answer_text, model, tokenizer, exp_config)\n",
    "\n",
    "        # Score values_early_layers\n",
    "        vel_nll = score_answer_with_cache(\n",
    "            deepcopy_cache(vel_cache), context_len,\n",
    "            query_prompt, answer_text, model, tokenizer, exp_config)\n",
    "\n",
    "        del bare_cache, sf_trunc_cache, vel_cache, bare_input, primed_input\n",
    "        if pad_length is not None and base_doc_len < pad_length:\n",
    "            del pad_tensor\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        pad_label = \"original\" if pad_length is None else str(pad_length)\n",
    "        query_rows.append({\n",
    "            'query_idx': qidx,\n",
    "            'layer_cutoff': FIXED_CUTOFF,\n",
    "            'pad_length': pad_label,\n",
    "            'actual_doc_len': doc_len,\n",
    "            'unprimed_nll': bare_nll,\n",
    "            'primed_nll': vel_nll,\n",
    "            'delta_nll': bare_nll - vel_nll,\n",
    "        })\n",
    "\n",
    "    p1_results.append({\n",
    "        'query_idx': qidx,\n",
    "        'query': qdata['query'],\n",
    "        'base_doc_len': base_doc_len,\n",
    "        'rows': query_rows,\n",
    "    })\n",
    "\n",
    "    # Checkpoint\n",
    "    if (qidx + 1) % CHECKPOINT_EVERY == 0 or qidx == N_PART1 - 1:\n",
    "        ckpt_data = {\n",
    "            'results': p1_results,\n",
    "            'query_texts': [q['query'] for q in queries[:N_PART1]],\n",
    "            'completed': len(p1_results),\n",
    "            'total': N_PART1,\n",
    "            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        }\n",
    "        with open(CHECKPOINT_P1_PATH, 'w') as f:\n",
    "            json.dump(ckpt_data, f)\n",
    "        elapsed = time.time() - t_start\n",
    "        n_done = qidx - p1_start_idx + 1\n",
    "        rate = n_done / elapsed if elapsed > 0 else 0\n",
    "        remaining = (N_PART1 - qidx - 1) / rate if rate > 0 else 0\n",
    "        tqdm.write(f\"  Checkpoint {qidx+1}/{N_PART1} | {n_done} done in {elapsed/60:.1f}m | \"\n",
    "                   f\"ETA: {remaining/60:.1f} min\")\n",
    "\n",
    "elapsed_total = time.time() - t_start\n",
    "print(f\"\\nPart 1 complete: {len(p1_results)} queries in {elapsed_total/60:.1f} min\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 7: Part 2 \u2014 Layer Boundary Sweep (N=200, 5 cutoffs, no padding)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"PART 2: LAYER BOUNDARY SWEEP ({N_PART2} queries, cutoffs={CUTOFFS})\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Checkpoint resume\n",
    "p2_results = []\n",
    "p2_start_idx = 0\n",
    "\n",
    "if CHECKPOINT_P2_PATH.exists():\n",
    "    with open(CHECKPOINT_P2_PATH, 'r') as f:\n",
    "        ckpt = json.load(f)\n",
    "    ckpt_queries = ckpt.get('query_texts', [])\n",
    "    current_queries = [q['query'] for q in queries[:N_PART2]]\n",
    "    if ckpt_queries == current_queries:\n",
    "        p2_results = ckpt['results']\n",
    "        p2_start_idx = len(p2_results)\n",
    "        print(f\"Resuming from checkpoint: {p2_start_idx}/{N_PART2}\")\n",
    "    else:\n",
    "        print(\"Checkpoint query mismatch. Starting fresh.\")\n",
    "else:\n",
    "    print(\"No checkpoint found. Starting fresh.\")\n",
    "\n",
    "t_start = time.time()\n",
    "\n",
    "for qidx in tqdm(range(p2_start_idx, N_PART2), initial=p2_start_idx, total=N_PART2,\n",
    "                  desc=\"Part 2\"):\n",
    "    qdata = queries[qidx]\n",
    "    query_prompt = QUERY_TEMPLATE.format(query=qdata['query'])\n",
    "    answer_text = ANSWER_TEMPLATE.format(answer=qdata['answer'])\n",
    "    passage = qdata['passage']\n",
    "    document_text = DOCUMENT_TEMPLATE.format(document=passage)\n",
    "\n",
    "    # Matched tokenization\n",
    "    full_text = sf_str + document_text\n",
    "    full_enc = tokenizer(full_text, return_tensors=\"pt\",\n",
    "                          add_special_tokens=True, padding=False, truncation=False)\n",
    "    full_ids = full_enc['input_ids'].to(exp_config.device)\n",
    "\n",
    "    sf_prefix_enc = tokenizer(sf_str, return_tensors=\"pt\",\n",
    "                               add_special_tokens=True, padding=False, truncation=False)\n",
    "    sf_prefix_len_matched = sf_prefix_enc['input_ids'].shape[1]\n",
    "\n",
    "    bos_id = full_ids[:, :1]\n",
    "    doc_ids = full_ids[:, sf_prefix_len_matched:]\n",
    "    doc_len = doc_ids.shape[1]\n",
    "    context_len = 1 + doc_len\n",
    "\n",
    "    del full_enc, full_ids, sf_prefix_enc\n",
    "\n",
    "    # Forward pass 1: BARE\n",
    "    bare_input = torch.cat([bos_id, doc_ids], dim=1)\n",
    "    with torch.no_grad():\n",
    "        bare_out = model(input_ids=bare_input,\n",
    "                         attention_mask=torch.ones_like(bare_input),\n",
    "                         use_cache=True, return_dict=True)\n",
    "    bare_cache = _ensure_dynamic_cache(bare_out.past_key_values)\n",
    "    del bare_out\n",
    "\n",
    "    # Forward pass 2: PRIMED\n",
    "    primed_input = torch.cat([bos_id, sf_ids, doc_ids], dim=1)\n",
    "    with torch.no_grad():\n",
    "        primed_out = model(input_ids=primed_input,\n",
    "                           attention_mask=torch.ones_like(primed_input),\n",
    "                           use_cache=True, return_dict=True)\n",
    "    primed_full = _ensure_dynamic_cache(primed_out.past_key_values)\n",
    "    del primed_out\n",
    "\n",
    "    # Truncate + RoPE correct\n",
    "    trunc_raw = extract_and_truncate_cache_with_bos(primed_full, doc_len)\n",
    "    prefix_offset = sf_ids.shape[1]\n",
    "    del primed_full\n",
    "\n",
    "    sf_trunc_cache = deepcopy_cache(trunc_raw)\n",
    "    correct_rope_positions_with_bos(sf_trunc_cache, prefix_offset, model)\n",
    "    del trunc_raw\n",
    "\n",
    "    # Score bare once\n",
    "    bare_nll = score_answer_with_cache(\n",
    "        deepcopy_cache(bare_cache), context_len,\n",
    "        query_prompt, answer_text, model, tokenizer, exp_config)\n",
    "\n",
    "    query_rows = []\n",
    "\n",
    "    # Score each cutoff\n",
    "    for cutoff in CUTOFFS:\n",
    "        vel_cache = replace_values_at_layers(\n",
    "            bare_cache, sf_trunc_cache, list(range(cutoff)))\n",
    "        vel_nll = score_answer_with_cache(\n",
    "            deepcopy_cache(vel_cache), context_len,\n",
    "            query_prompt, answer_text, model, tokenizer, exp_config)\n",
    "        del vel_cache\n",
    "\n",
    "        query_rows.append({\n",
    "            'query_idx': qidx,\n",
    "            'layer_cutoff': cutoff,\n",
    "            'pad_length': 'none',\n",
    "            'actual_doc_len': doc_len,\n",
    "            'unprimed_nll': bare_nll,\n",
    "            'primed_nll': vel_nll,\n",
    "            'delta_nll': bare_nll - vel_nll,\n",
    "        })\n",
    "\n",
    "    del bare_cache, sf_trunc_cache, bare_input, primed_input\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    p2_results.append({\n",
    "        'query_idx': qidx,\n",
    "        'query': qdata['query'],\n",
    "        'doc_len': doc_len,\n",
    "        'bare_nll': bare_nll,\n",
    "        'rows': query_rows,\n",
    "    })\n",
    "\n",
    "    # Checkpoint\n",
    "    if (qidx + 1) % CHECKPOINT_EVERY == 0 or qidx == N_PART2 - 1:\n",
    "        ckpt_data = {\n",
    "            'results': p2_results,\n",
    "            'query_texts': [q['query'] for q in queries[:N_PART2]],\n",
    "            'completed': len(p2_results),\n",
    "            'total': N_PART2,\n",
    "            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        }\n",
    "        with open(CHECKPOINT_P2_PATH, 'w') as f:\n",
    "            json.dump(ckpt_data, f)\n",
    "        elapsed = time.time() - t_start\n",
    "        n_done = qidx - p2_start_idx + 1\n",
    "        rate = n_done / elapsed if elapsed > 0 else 0\n",
    "        remaining = (N_PART2 - qidx - 1) / rate if rate > 0 else 0\n",
    "        tqdm.write(f\"  Checkpoint {qidx+1}/{N_PART2} | {n_done} done in {elapsed/60:.1f}m | \"\n",
    "                   f\"ETA: {remaining/60:.1f} min\")\n",
    "\n",
    "elapsed_total = time.time() - t_start\n",
    "print(f\"\\nPart 2 complete: {len(p2_results)} queries in {elapsed_total/60:.1f} min\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 8: Part 1 Analysis \u2014 d vs length, statistical tests\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PART 1 ANALYSIS: LENGTH GENERALIZATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Collect per-sample deltas by pad_length\n",
    "p1_deltas = {}\n",
    "p1_bare = {}\n",
    "p1_primed = {}\n",
    "for pl in ['original', '256', '512', '900']:\n",
    "    p1_deltas[pl] = []\n",
    "    p1_bare[pl] = []\n",
    "    p1_primed[pl] = []\n",
    "\n",
    "for r in p1_results:\n",
    "    for row in r['rows']:\n",
    "        pl = row['pad_length']\n",
    "        if pl in p1_deltas:\n",
    "            p1_deltas[pl].append(row['delta_nll'])\n",
    "            p1_bare[pl].append(row['unprimed_nll'])\n",
    "            p1_primed[pl].append(row['primed_nll'])\n",
    "\n",
    "# Convert to arrays and filter zeros\n",
    "p1_arrays = {}\n",
    "for pl in p1_deltas:\n",
    "    bare = np.array(p1_bare[pl])\n",
    "    primed = np.array(p1_primed[pl])\n",
    "    delta = np.array(p1_deltas[pl])\n",
    "    valid = (bare != 0) & (primed != 0) & np.isfinite(bare) & np.isfinite(primed)\n",
    "    p1_arrays[pl] = {\n",
    "        'bare': bare[valid],\n",
    "        'primed': primed[valid],\n",
    "        'delta': delta[valid],\n",
    "        'n_valid': int(np.sum(valid)),\n",
    "    }\n",
    "\n",
    "# Summary table\n",
    "print(f\"\\n{'Pad Length':<12} {'N':>5} {'Mean Bare':>10} {'Mean Primed':>12} \"\n",
    "      f\"{'Mean D':>10} {'d':>8} {'Win%':>7} {'p':>12} {'sig':>5}\")\n",
    "print(\"-\" * 90)\n",
    "\n",
    "p1_analysis = {}\n",
    "for pl in ['original', '256', '512', '900']:\n",
    "    a = p1_arrays[pl]\n",
    "    d = cohens_d(a['delta'])\n",
    "    win = np.mean(a['delta'] > 0) * 100\n",
    "    t_stat, p_val = stats.ttest_1samp(a['delta'], 0)\n",
    "    sig = '***' if p_val < 0.001 else '**' if p_val < 0.01 else '*' if p_val < 0.05 else 'ns'\n",
    "    print(f\"{pl:<12} {a['n_valid']:>5} {np.mean(a['bare']):>10.4f} \"\n",
    "          f\"{np.mean(a['primed']):>12.4f} {np.mean(a['delta']):>+10.4f} \"\n",
    "          f\"{d:>+8.3f} {win:>6.1f}% {p_val:>12.2e} {sig:>5}\")\n",
    "    p1_analysis[pl] = {\n",
    "        'n_valid': a['n_valid'],\n",
    "        'mean_bare': float(np.mean(a['bare'])),\n",
    "        'mean_primed': float(np.mean(a['primed'])),\n",
    "        'mean_delta': float(np.mean(a['delta'])),\n",
    "        'cohens_d': float(d),\n",
    "        'win_pct': float(win),\n",
    "        't_stat': float(t_stat),\n",
    "        'p_value': float(p_val),\n",
    "    }\n",
    "\n",
    "# Exp 20 comparison\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"COMPARISON: Gemma layer-selective vs Mistral full priming (Exp 20)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "exp20_lengths = ['original', '256', '512']\n",
    "exp20_ds = [EXP20_REF.get(f'{pl}_d', EXP20_REF.get('original_d', 0))\n",
    "            for pl in exp20_lengths]\n",
    "\n",
    "print(f\"\\n{'Length':<12} {'Gemma VEL d':>12} {'Mistral d':>10} {'Diff':>8}\")\n",
    "print(\"-\" * 48)\n",
    "for pl in exp20_lengths:\n",
    "    if pl in p1_analysis:\n",
    "        gemma_d = p1_analysis[pl]['cohens_d']\n",
    "        mistral_d = EXP20_REF.get(f'{pl}_d', EXP20_REF.get('original_d', 0))\n",
    "        print(f\"{pl:<12} {gemma_d:>+12.3f} {mistral_d:>+10.3f} {gemma_d - mistral_d:>+8.3f}\")\n",
    "\n",
    "# Verdict (compare original to 900, the max safe length for Gemma's sliding window)\n",
    "d_orig = p1_analysis['original']['cohens_d']\n",
    "d_max = p1_analysis['900']['cohens_d']\n",
    "if d_orig > 0.1 and d_max < 0.05:\n",
    "    p1_verdict = (\"Layer-selective values ARE NOT robust to length. \"\n",
    "                  f\"d decays from {d_orig:+.3f} at original to {d_max:+.3f} at 900 tokens.\")\n",
    "elif d_max > 0.1:\n",
    "    p1_verdict = (\"Layer-selective values ARE robust to length! \"\n",
    "                  f\"d remains {d_max:+.3f} even at 900 tokens (vs {d_orig:+.3f} at original).\")\n",
    "else:\n",
    "    p1_verdict = (f\"Mixed: d goes from {d_orig:+.3f} (original) to {d_max:+.3f} (900). \"\n",
    "                  \"See detailed numbers above.\")\n",
    "\n",
    "print(f\"\\nVERDICT: {p1_verdict}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 9: Part 2 Analysis \u2014 d vs cutoff, optimal cutoff\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PART 2 ANALYSIS: LAYER BOUNDARY SWEEP\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Collect per-sample deltas by cutoff\n",
    "p2_deltas = {}\n",
    "p2_bare_all = []\n",
    "for cutoff in CUTOFFS:\n",
    "    p2_deltas[cutoff] = []\n",
    "\n",
    "for r in p2_results:\n",
    "    p2_bare_all.append(r['bare_nll'])\n",
    "    for row in r['rows']:\n",
    "        cutoff = row['layer_cutoff']\n",
    "        if cutoff in p2_deltas:\n",
    "            p2_deltas[cutoff].append(row['delta_nll'])\n",
    "\n",
    "p2_bare_arr = np.array(p2_bare_all)\n",
    "\n",
    "# Summary table\n",
    "print(f\"\\n{'Cutoff':<10} {'Layers':<15} {'N':>5} {'Mean D':>10} \"\n",
    "      f\"{'d':>8} {'Win%':>7} {'p':>12} {'sig':>5}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "p2_analysis = {}\n",
    "best_cutoff = None\n",
    "best_d = -999\n",
    "\n",
    "for cutoff in CUTOFFS:\n",
    "    delta = np.array(p2_deltas[cutoff])\n",
    "    valid = np.isfinite(delta)\n",
    "    delta = delta[valid]\n",
    "    n_valid = len(delta)\n",
    "    d = cohens_d(delta)\n",
    "    win = np.mean(delta > 0) * 100\n",
    "    t_stat, p_val = stats.ttest_1samp(delta, 0)\n",
    "    sig = '***' if p_val < 0.001 else '**' if p_val < 0.01 else '*' if p_val < 0.05 else 'ns'\n",
    "    layer_desc = f\"0-{cutoff-1}\"\n",
    "    print(f\"{cutoff:<10} {layer_desc:<15} {n_valid:>5} {np.mean(delta):>+10.4f} \"\n",
    "          f\"{d:>+8.3f} {win:>6.1f}% {p_val:>12.2e} {sig:>5}\")\n",
    "    p2_analysis[cutoff] = {\n",
    "        'n_valid': n_valid,\n",
    "        'layers': layer_desc,\n",
    "        'mean_delta': float(np.mean(delta)),\n",
    "        'cohens_d': float(d),\n",
    "        'win_pct': float(win),\n",
    "        't_stat': float(t_stat),\n",
    "        'p_value': float(p_val),\n",
    "    }\n",
    "    if d > best_d:\n",
    "        best_d = d\n",
    "        best_cutoff = cutoff\n",
    "\n",
    "# Exp 19 comparison\n",
    "print(f\"\\nOptimal cutoff: {best_cutoff} layers (d={best_d:+.3f})\")\n",
    "print(f\"Exp 19 reference: layers 0-16 (17 layers), d={EXP19_REF['values_early_layers_d']:+.3f}\")\n",
    "print(f\"Exp 19 values_only (all layers): d={EXP19_REF['values_only_d']:+.3f}\")\n",
    "\n",
    "# Hardness interaction for Part 2\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"HARDNESS \u00d7 CUTOFF INTERACTION (Part 2)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Compute quintile boundaries from bare NLL\n",
    "quintile_boundaries = np.percentile(p2_bare_arr, [20, 40, 60, 80])\n",
    "quintile_labels = ['Q1 (easy)', 'Q2', 'Q3', 'Q4', 'Q5 (hard)']\n",
    "\n",
    "def get_quintile(nll, boundaries):\n",
    "    for i, b in enumerate(boundaries):\n",
    "        if nll <= b:\n",
    "            return i\n",
    "    return len(boundaries)\n",
    "\n",
    "quintiles = np.array([get_quintile(nll, quintile_boundaries) for nll in p2_bare_arr])\n",
    "\n",
    "header = f\"{'Cutoff':<10}\" + \"\".join(f\"{ql:>14}\" for ql in quintile_labels) + f\"{'Overall':>14}\"\n",
    "print(f\"\\n{header}\")\n",
    "print(\"-\" * (10 + 14 * 6))\n",
    "\n",
    "hardness_data = {}\n",
    "for cutoff in CUTOFFS:\n",
    "    delta = np.array(p2_deltas[cutoff])\n",
    "    row_str = f\"{cutoff:<10}\"\n",
    "    quintile_ds = []\n",
    "    for q in range(5):\n",
    "        mask_q = quintiles == q\n",
    "        if np.sum(mask_q) < 5:\n",
    "            row_str += f\"{'n/a':>14}\"\n",
    "            quintile_ds.append(None)\n",
    "        else:\n",
    "            d_q = cohens_d(delta[mask_q])\n",
    "            row_str += f\"{d_q:>+14.3f}\"\n",
    "            quintile_ds.append(float(d_q))\n",
    "    d_all = cohens_d(delta)\n",
    "    row_str += f\"{d_all:>+14.3f}\"\n",
    "    print(row_str)\n",
    "    hardness_data[cutoff] = quintile_ds"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 10: Plots \u2014 4-panel figure\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# ---- Panel 1 (top-left): Part 1 \u2014 d vs Document Length ----\n",
    "ax = axes[0, 0]\n",
    "\n",
    "length_labels = ['original', '256', '512', '900']\n",
    "gemma_ds = [p1_analysis[pl]['cohens_d'] for pl in length_labels]\n",
    "x_lengths = []\n",
    "for pl in length_labels:\n",
    "    if pl == 'original':\n",
    "        # Mean original doc length\n",
    "        x_lengths.append(np.mean([r['base_doc_len'] for r in p1_results]))\n",
    "    else:\n",
    "        x_lengths.append(int(pl))\n",
    "\n",
    "# Bootstrap 95% CI\n",
    "np.random.seed(SEED)\n",
    "ci_lo, ci_hi = [], []\n",
    "for pl in length_labels:\n",
    "    delta = p1_arrays[pl]['delta']\n",
    "    boot_ds = []\n",
    "    for _ in range(2000):\n",
    "        idx_boot = np.random.randint(0, len(delta), size=len(delta))\n",
    "        boot_ds.append(cohens_d(delta[idx_boot]))\n",
    "    boot_ds = np.array(boot_ds)\n",
    "    ci_lo.append(np.percentile(boot_ds, 2.5))\n",
    "    ci_hi.append(np.percentile(boot_ds, 97.5))\n",
    "ci_lo = np.array(ci_lo)\n",
    "ci_hi = np.array(ci_hi)\n",
    "\n",
    "ax.errorbar(x_lengths, gemma_ds,\n",
    "            yerr=[np.array(gemma_ds) - ci_lo, ci_hi - np.array(gemma_ds)],\n",
    "            marker='o', markersize=8, linewidth=2, capsize=5,\n",
    "            color='#9467bd', ecolor='#c5b0d5', label='Gemma VEL (this exp)')\n",
    "\n",
    "# Exp 20 Mistral reference (only lengths we overlap with)\n",
    "exp20_x = [130, 256, 512]\n",
    "exp20_y = [EXP20_REF['original_d'], EXP20_REF['256_d'], EXP20_REF['512_d']]\n",
    "ax.plot(exp20_x, exp20_y, marker='s', markersize=6, linewidth=1.5, linestyle='--',\n",
    "        color='#d62728', alpha=0.7, label='Mistral full priming (Exp 20)')\n",
    "\n",
    "ax.axhline(y=0, color='gray', linestyle='-', alpha=0.3)\n",
    "ax.set_xscale('log')\n",
    "ax.set_xlabel('Document Token Length')\n",
    "ax.set_ylabel(\"Cohen's d (positive = helps)\")\n",
    "ax.set_title(\"Part 1: d vs Document Length\")\n",
    "ax.legend(fontsize=8)\n",
    "\n",
    "for i, pl in enumerate(length_labels):\n",
    "    p_val = p1_analysis[pl]['p_value']\n",
    "    sig = '***' if p_val < 0.001 else '**' if p_val < 0.01 else '*' if p_val < 0.05 else 'ns'\n",
    "    ax.annotate(f'{pl}\\nd={gemma_ds[i]:+.3f} {sig}',\n",
    "                (x_lengths[i], gemma_ds[i]),\n",
    "                textcoords='offset points', xytext=(0, 18),\n",
    "                ha='center', fontsize=7)\n",
    "\n",
    "# ---- Panel 2 (top-right): Part 1 \u2014 Win Rate vs Length ----\n",
    "ax = axes[0, 1]\n",
    "\n",
    "wins = [p1_analysis[pl]['win_pct'] for pl in length_labels]\n",
    "ax.plot(x_lengths, wins, marker='s', markersize=8, linewidth=2, color='#2ca02c')\n",
    "ax.axhline(y=50, color='black', linestyle='--', linewidth=0.8, label='Chance (50%)')\n",
    "\n",
    "for i, pl in enumerate(length_labels):\n",
    "    ax.annotate(f'{pl}\\n{wins[i]:.1f}%',\n",
    "                (x_lengths[i], wins[i]),\n",
    "                textcoords='offset points', xytext=(0, 12),\n",
    "                ha='center', fontsize=8)\n",
    "\n",
    "ax.set_xscale('log')\n",
    "ax.set_xlabel('Document Token Length')\n",
    "ax.set_ylabel('Win Rate (%)')\n",
    "ax.set_title(\"Part 1: Win Rate vs Length\")\n",
    "ax.legend(fontsize=8)\n",
    "\n",
    "# ---- Panel 3 (bottom-left): Part 2 \u2014 d vs Layer Cutoff ----\n",
    "ax = axes[1, 0]\n",
    "\n",
    "cutoff_ds = [p2_analysis[c]['cohens_d'] for c in CUTOFFS]\n",
    "x_pos = range(len(CUTOFFS))\n",
    "colors_bar = ['#1f77b4' if c != best_cutoff else '#ff7f0e' for c in CUTOFFS]\n",
    "bars = ax.bar(x_pos, cutoff_ds, color=colors_bar, edgecolor='black', linewidth=0.5)\n",
    "\n",
    "# Exp 19 reference\n",
    "ax.axhline(y=EXP19_REF['values_early_layers_d'], color='#9467bd', linestyle='--',\n",
    "           linewidth=1.5, label=f\"Exp 19 (17 layers) d={EXP19_REF['values_early_layers_d']:+.3f}\")\n",
    "ax.axhline(y=EXP19_REF['values_only_d'], color='#7f7f7f', linestyle=':',\n",
    "           linewidth=1.5, label=f\"Exp 19 values_only d={EXP19_REF['values_only_d']:+.3f}\")\n",
    "ax.axhline(y=0, color='gray', linestyle='-', alpha=0.3)\n",
    "\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels([str(c) for c in CUTOFFS])\n",
    "ax.set_xlabel('Layer Cutoff (layers 0 to cutoff-1)')\n",
    "ax.set_ylabel(\"Cohen's d vs Bare\")\n",
    "ax.set_title(f\"Part 2: d vs Layer Cutoff (best={best_cutoff})\")\n",
    "ax.legend(fontsize=7)\n",
    "\n",
    "for i, d_val in enumerate(cutoff_ds):\n",
    "    ax.text(i, d_val + 0.003 if d_val >= 0 else d_val - 0.012,\n",
    "            f\"{d_val:+.3f}\", ha='center',\n",
    "            va='bottom' if d_val >= 0 else 'top', fontsize=9)\n",
    "\n",
    "# ---- Panel 4 (bottom-right): Part 2 \u2014 Hardness x Cutoff heatmap ----\n",
    "ax = axes[1, 1]\n",
    "\n",
    "heatmap_data = np.zeros((len(CUTOFFS), 5))\n",
    "for i, cutoff in enumerate(CUTOFFS):\n",
    "    for q in range(5):\n",
    "        val = hardness_data[cutoff][q]\n",
    "        heatmap_data[i, q] = val if val is not None else np.nan\n",
    "\n",
    "im = ax.imshow(heatmap_data, cmap='RdBu', aspect='auto',\n",
    "               vmin=-0.5, vmax=0.5)\n",
    "ax.set_xticks(range(5))\n",
    "ax.set_xticklabels(quintile_labels, fontsize=8)\n",
    "ax.set_yticks(range(len(CUTOFFS)))\n",
    "ax.set_yticklabels([str(c) for c in CUTOFFS])\n",
    "ax.set_xlabel('Difficulty Quintile')\n",
    "ax.set_ylabel('Layer Cutoff')\n",
    "ax.set_title(\"Part 2: Hardness x Cutoff (Cohen's d)\")\n",
    "\n",
    "# Add text annotations\n",
    "for i in range(len(CUTOFFS)):\n",
    "    for j in range(5):\n",
    "        val = heatmap_data[i, j]\n",
    "        if not np.isnan(val):\n",
    "            ax.text(j, i, f\"{val:+.2f}\", ha='center', va='center',\n",
    "                    fontsize=7, color='white' if abs(val) > 0.25 else 'black')\n",
    "\n",
    "fig.colorbar(im, ax=ax, shrink=0.8, label=\"Cohen's d\")\n",
    "\n",
    "plt.suptitle('Exp 21: Gemma Mechanism Robustness & Tuning', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'analysis_plots.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"Plots saved to {RESULTS_DIR / 'analysis_plots.png'}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 11: Save results.json + CSVs\n",
    "import csv\n",
    "\n",
    "# --- Part 1 CSV ---\n",
    "with open(CSV_P1_PATH, 'w', newline='') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=[\n",
    "        'query_idx', 'layer_cutoff', 'pad_length', 'actual_doc_len',\n",
    "        'unprimed_nll', 'primed_nll', 'delta_nll'])\n",
    "    writer.writeheader()\n",
    "    for r in p1_results:\n",
    "        for row in r['rows']:\n",
    "            writer.writerow(row)\n",
    "print(f\"Part 1 CSV saved: {CSV_P1_PATH}\")\n",
    "\n",
    "# --- Part 2 CSV ---\n",
    "with open(CSV_P2_PATH, 'w', newline='') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=[\n",
    "        'query_idx', 'layer_cutoff', 'pad_length', 'actual_doc_len',\n",
    "        'unprimed_nll', 'primed_nll', 'delta_nll'])\n",
    "    writer.writeheader()\n",
    "    for r in p2_results:\n",
    "        for row in r['rows']:\n",
    "            writer.writerow(row)\n",
    "print(f\"Part 2 CSV saved: {CSV_P2_PATH}\")\n",
    "\n",
    "# --- Combined results.json ---\n",
    "final = {\n",
    "    'experiment': 'exp21_gemma_robustness_tuning',\n",
    "    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'config': {\n",
    "        'model_name': MODEL_NAME,\n",
    "        'model_type': 'gemma3',\n",
    "        'seed': SEED,\n",
    "        'dataset': 'MS MARCO v1.1 validation',\n",
    "        'max_passage_words': MAX_PASSAGE_WORDS,\n",
    "        'part1': {\n",
    "            'n_queries': N_PART1,\n",
    "            'pad_lengths': [str(pl) if pl else 'original' for pl in PAD_LENGTHS],\n",
    "            'fixed_cutoff': FIXED_CUTOFF,\n",
    "        },\n",
    "        'part2': {\n",
    "            'n_queries': N_PART2,\n",
    "            'cutoffs': CUTOFFS,\n",
    "        },\n",
    "    },\n",
    "    'gemma_architecture': {\n",
    "        'hidden_size': text_config.hidden_size,\n",
    "        'num_layers': text_config.num_hidden_layers,\n",
    "        'num_attention_heads': text_config.num_attention_heads,\n",
    "        'num_kv_heads': text_config.num_key_value_heads,\n",
    "        'head_dim': _get_head_dim(model.config),\n",
    "        'rope_thetas': sorted(list(thetas)),\n",
    "    },\n",
    "    'part1_analysis': p1_analysis,\n",
    "    'part1_verdict': p1_verdict,\n",
    "    'part2_analysis': {str(k): v for k, v in p2_analysis.items()},\n",
    "    'part2_best_cutoff': best_cutoff,\n",
    "    'part2_best_d': float(best_d),\n",
    "    'part2_hardness_data': {str(k): v for k, v in hardness_data.items()},\n",
    "    'reference_values': {\n",
    "        'exp19_gemma': EXP19_REF,\n",
    "        'exp20_mistral': EXP20_REF,\n",
    "    },\n",
    "    'part1_per_query': p1_results,\n",
    "    'part2_per_query': p2_results,\n",
    "}\n",
    "\n",
    "with open(FINAL_RESULTS_PATH, 'w') as f:\n",
    "    json.dump(final, f, indent=2)\n",
    "\n",
    "print(f\"\\nResults saved to {FINAL_RESULTS_PATH}\")\n",
    "print(f\"File size: {FINAL_RESULTS_PATH.stat().st_size / 1024:.1f} KB\")\n",
    "\n",
    "# Final summary\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"SUMMARY \u2014 Exp 21: Gemma Mechanism Robustness & Tuning\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Model: Gemma 3 4B (34 layers, head_dim=256, bfloat16)\")\n",
    "print(f\"\\nPart 1: Length Generalization (cutoff={FIXED_CUTOFF})\")\n",
    "for pl in ['original', '256', '512', '900']:\n",
    "    a = p1_analysis[pl]\n",
    "    sig = '***' if a['p_value'] < 0.001 else '**' if a['p_value'] < 0.01 else '*' if a['p_value'] < 0.05 else 'ns'\n",
    "    print(f\"  {pl:<12} d={a['cohens_d']:>+.3f}  win={a['win_pct']:.0f}%  {sig}\")\n",
    "print(f\"  Verdict: {p1_verdict}\")\n",
    "print(f\"\\nPart 2: Layer Boundary Sweep\")\n",
    "for cutoff in CUTOFFS:\n",
    "    a = p2_analysis[cutoff]\n",
    "    sig = '***' if a['p_value'] < 0.001 else '**' if a['p_value'] < 0.01 else '*' if a['p_value'] < 0.05 else 'ns'\n",
    "    marker = \" <-- BEST\" if cutoff == best_cutoff else \"\"\n",
    "    print(f\"  cutoff={cutoff:<4} d={a['cohens_d']:>+.3f}  win={a['win_pct']:.0f}%  {sig}{marker}\")\n",
    "print(f\"  Optimal cutoff: {best_cutoff} layers (d={best_d:+.3f})\")\n",
    "print(f\"\\nDone!\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 12: GPU cleanup\n",
    "import gc\n",
    "\n",
    "print(\"Cleaning up GPU memory...\")\n",
    "mem_before = torch.cuda.memory_allocated() / 1e9\n",
    "\n",
    "del model\n",
    "del tokenizer\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "mem_after = torch.cuda.memory_allocated() / 1e9\n",
    "print(f\"GPU memory: {mem_before:.2f} GB -> {mem_after:.2f} GB\")\n",
    "print(\"Cleanup complete.\")"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}