{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exp 08: Mechanism Isolation + Prefix Amplification\n",
    "\n",
    "## Motivation\n",
    "\n",
    "Exp 06 found that LLM surrogates improve KV cache quality via value contamination (truncated\n",
    "prefix alters document value vectors) and suffix attention (separator provides new attention\n",
    "targets). But which component of the KV cache carries the signal \u2014 keys, values, or both?\n",
    "And does repeating the prefix amplify the effect?\n",
    "\n",
    "## Core Questions\n",
    "\n",
    "1. **Key vs Value isolation**: Is the priming effect carried by keys, values, or both?\n",
    "2. **Prefix amplification**: Does repeating the surrogate prefix K times amplify the effect?\n",
    "\n",
    "## Self-contained. N=1000 samples (smaller, focused probe).\n",
    "\n",
    "## 10 Conditions\n",
    "\n",
    "| # | Condition | Construction | Tests |\n",
    "|---|-----------|-------------|-------|\n",
    "| 1 | Bare | `[BOS][doc]` | Baseline |\n",
    "| 2 | LLM-keyword-trunc | Standard truncated + RoPE | Reference |\n",
    "| 3 | LLM-keyword-suffix | Suffix mode | Reference |\n",
    "| 4 | LLM-keyword+sep | Trunc + suffix | Best method reference |\n",
    "| 5 | Primed-values-only | Keys from bare, values from LLM-trunc | Key vs value |\n",
    "| 6 | Primed-keys-only | Values from bare, keys from LLM-trunc | Key vs value |\n",
    "| 7 | Prefix-1x | `[BOS][kw\\n][doc]` \u2192 truncate | Baseline repetition |\n",
    "| 8 | Prefix-3x | `[BOS][kw\\n kw\\n kw\\n][doc]` \u2192 truncate | 3\u00d7 amplification |\n",
    "| 9 | Prefix-5x | `[BOS][kw\\n kw\\n kw\\n kw\\n kw\\n][doc]` \u2192 truncate | 5\u00d7 amplification |\n",
    "| 10 | Separator-only | `[BOS][doc][\\n\\nRelated question: ]` | Control |\n",
    "\n",
    "## 6 Primary Comparisons (Bonferroni alpha = 0.0083)\n",
    "\n",
    "| # | Comparison | Question |\n",
    "|---|-----------|----------|\n",
    "| C1 | Primed-values-only vs Bare | Is the effect in values? |\n",
    "| C2 | Primed-keys-only vs Bare | Is the effect in keys? |\n",
    "| C3 | Primed-values-only vs LLM-keyword-trunc | How much do values capture? |\n",
    "| C4 | Prefix-3x vs Prefix-1x | Does 3\u00d7 amplify? |\n",
    "| C5 | Prefix-5x vs Prefix-1x | Does 5\u00d7 amplify? |\n",
    "| C6 | Prefix-5x vs Prefix-3x | Diminishing returns? |"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 1: Setup \u2014 permissions, seeds, results directory\n",
    "import os\n",
    "os.umask(0o000)\n",
    "\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "RESULTS_DIR = Path(\"results/exp08\")\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "SURROGATES_DIR = RESULTS_DIR / \"surrogates\"\n",
    "SURROGATES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CHECKPOINT_PATH = RESULTS_DIR / \"checkpoint.json\"\n",
    "FINAL_RESULTS_PATH = RESULTS_DIR / \"results.json\"\n",
    "\n",
    "print(f\"SEED: {SEED}\")\n",
    "print(f\"Results directory: {RESULTS_DIR}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 2: Load model (Mistral-7B 4-bit)\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "print(f\"Loading {MODEL_NAME} (4-bit)...\")\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "print(f\"Model loaded. dtype={model.dtype}, device={model.device}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 3: Imports + config + templates + helpers\n",
    "sys.path.insert(0, \".\")\n",
    "\n",
    "from lib.config import ExperimentConfig\n",
    "from lib.kv_cache import (\n",
    "    build_kv_cache,\n",
    "    build_suffix_kv_cache,\n",
    "    score_answer_with_cache,\n",
    "    deepcopy_cache,\n",
    "    extract_and_truncate_cache_with_bos,\n",
    "    correct_rope_positions_with_bos,\n",
    "    build_hybrid_cache,\n",
    "    _get_cache_keys,\n",
    "    _get_cache_values,\n",
    ")\n",
    "from lib.data import load_ms_marco, load_evaluation_samples\n",
    "from lib.analysis import cohens_d\n",
    "from lib.surrogate import generate_all_5_surrogates, TOP_5_SURROGATE_TEMPLATES\n",
    "from scipy import stats\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "config = ExperimentConfig(\n",
    "    model_name=MODEL_NAME,\n",
    "    num_samples=2000,\n",
    "    min_passage_words=20,\n",
    "    max_passage_words=500,\n",
    "    seed=SEED,\n",
    ")\n",
    "\n",
    "# Templates \u2014 bare text, no \"Document:\\n\" framing\n",
    "SURROGATE_PREFIX_TEMPLATE = \"{surrogate}\\n\"\n",
    "DOCUMENT_TEMPLATE = \"{document}\"\n",
    "QUERY_TEMPLATE = \"\\nQuery: {query}\\nAnswer:\"\n",
    "ANSWER_TEMPLATE = \" {answer}\"\n",
    "SUFFIX_SEPARATOR = \"\\n\\nRelated question: \"\n",
    "CHECKPOINT_EVERY = 50\n",
    "N_COMPARISONS = 6\n",
    "BONFERRONI_ALPHA = 0.05 / N_COMPARISONS\n",
    "N_EVAL = 1000  # Smaller focused probe\n",
    "\n",
    "print(\"Config ready\")\n",
    "print(f\"  num_samples pool: {config.num_samples}\")\n",
    "print(f\"  eval samples: {N_EVAL}\")\n",
    "print(f\"  bonferroni_alpha: {BONFERRONI_ALPHA:.4f} ({N_COMPARISONS} comparisons)\")\n",
    "print(f\"  conditions: 10\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 4: Load MS MARCO (1000 samples)\n",
    "dataset = load_ms_marco(config)\n",
    "\n",
    "np.random.seed(SEED)\n",
    "all_samples = load_evaluation_samples(dataset, config, require_answer=True)\n",
    "\n",
    "samples = all_samples[:N_EVAL]\n",
    "N = len(samples)\n",
    "print(f\"Loaded {len(all_samples)} candidates, using first {N} for evaluation\")\n",
    "print(f\"Example passage ({len(samples[0]['passage'].split())} words): {samples[0]['passage'][:100]}...\")\n",
    "print(f\"Example query: {samples[0]['query']}\")\n",
    "print(f\"Example answer: {samples[0]['answer']}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 5: Generate LLM keyword surrogates (only keyword_query template needed)\n",
    "# Using generate_all_5_surrogates for consistency but only using keyword_query\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PHASE 1: LLM SURROGATE GENERATION (keyword only)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "surrogates_path = SURROGATES_DIR / \"keyword_surrogates.json\"\n",
    "\n",
    "if surrogates_path.exists():\n",
    "    with open(surrogates_path, 'r') as f:\n",
    "        surrogates_data = json.load(f)\n",
    "    keyword_surrogates = surrogates_data['surrogates']\n",
    "    print(f\"Loaded {len(keyword_surrogates)} keyword surrogates from cache\")\n",
    "else:\n",
    "    keyword_surrogates = []\n",
    "\n",
    "start_idx_gen = len(keyword_surrogates)\n",
    "if start_idx_gen < N:\n",
    "    print(f\"Generating keyword surrogates for samples {start_idx_gen} to {N-1}...\")\n",
    "    t_start = time.time()\n",
    "    for idx in tqdm(range(start_idx_gen, N), initial=start_idx_gen, total=N,\n",
    "                     desc=\"Keyword surrogates\"):\n",
    "        passage = samples[idx]['passage']\n",
    "        try:\n",
    "            s5 = generate_all_5_surrogates(passage, model, tokenizer, config)\n",
    "            kw = s5.get('keyword_query', '')\n",
    "        except Exception as e:\n",
    "            print(f\"  WARNING: Generation failed for sample {idx}: {e}\")\n",
    "            kw = \"\"\n",
    "        keyword_surrogates.append(kw)\n",
    "\n",
    "        if (idx + 1) % 100 == 0 or idx == N - 1:\n",
    "            with open(surrogates_path, 'w') as f:\n",
    "                json.dump({'surrogates': keyword_surrogates}, f)\n",
    "            elapsed = time.time() - t_start\n",
    "            rate = (idx - start_idx_gen + 1) / elapsed if elapsed > 0 else 0\n",
    "            remaining = (N - idx - 1) / rate if rate > 0 else 0\n",
    "            tqdm.write(f\"  Saved {idx+1}/{N} | {rate:.2f} s/s | ETA: {remaining/60:.1f} min\")\n",
    "\n",
    "    with open(surrogates_path, 'w') as f:\n",
    "        json.dump({'surrogates': keyword_surrogates}, f)\n",
    "    print(f\"Keyword surrogates complete: {len(keyword_surrogates)} samples\")\n",
    "else:\n",
    "    print(f\"All keyword surrogates already cached ({len(keyword_surrogates)} samples)\")\n",
    "\n",
    "n_empty = sum(1 for s in keyword_surrogates if not s.strip())\n",
    "print(f\"Empty surrogates: {n_empty}/{N}\")\n",
    "print(f\"Example: '{keyword_surrogates[0]}'\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 6: Condition explanation with concrete examples\n",
    "print(\"=\" * 70)\n",
    "print(\"EXPERIMENTAL CONDITIONS EXPLAINED\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "ex_kw = keyword_surrogates[0]\n",
    "\n",
    "conditions_explained = [\n",
    "    (\"1. Bare\",\n",
    "     \"[BOS][doc]\",\n",
    "     \"No prefix \u2014 baseline\"),\n",
    "    (\"2. LLM-keyword-trunc\",\n",
    "     \"[BOS][llm_kw\\\\n][doc] \u2192 truncate + RoPE\",\n",
    "     f\"Standard truncated prefix: '{ex_kw}'\"),\n",
    "    (\"3. LLM-keyword-suffix\",\n",
    "     \"[BOS][doc][\\\\n\\\\nRelated question: llm_kw]\",\n",
    "     f\"Suffix mode: '{ex_kw}'\"),\n",
    "    (\"4. LLM-keyword+sep\",\n",
    "     \"[BOS][llm_kw\\\\n][doc][\\\\n\\\\nRelated question: ] (prefix+suffix)\",\n",
    "     \"Stacking: truncated prefix + suffix separator\"),\n",
    "    (\"5. Primed-values-only\",\n",
    "     \"Keys from bare cache, values from LLM-trunc cache\",\n",
    "     \"Tests: is the effect carried by values?\"),\n",
    "    (\"6. Primed-keys-only\",\n",
    "     \"Values from bare cache, keys from LLM-trunc cache\",\n",
    "     \"Tests: is the effect carried by keys?\"),\n",
    "    (\"7. Prefix-1x\",\n",
    "     \"[BOS][kw\\\\n][doc] \u2192 truncate (same as #2)\",\n",
    "     \"1\u00d7 prefix \u2014 baseline for repetition\"),\n",
    "    (\"8. Prefix-3x\",\n",
    "     \"[BOS][kw\\\\n kw\\\\n kw\\\\n][doc] \u2192 truncate + RoPE(3\u00d7offset)\",\n",
    "     \"3\u00d7 repeated prefix \u2014 amplification test\"),\n",
    "    (\"9. Prefix-5x\",\n",
    "     \"[BOS][kw\\\\n kw\\\\n kw\\\\n kw\\\\n kw\\\\n][doc] \u2192 truncate + RoPE(5\u00d7offset)\",\n",
    "     \"5\u00d7 repeated prefix \u2014 maximum amplification\"),\n",
    "    (\"10. Separator-only\",\n",
    "     \"[BOS][doc][\\\\n\\\\nRelated question: ]\",\n",
    "     \"Suffix framing only \u2014 structural control\"),\n",
    "]\n",
    "\n",
    "for name, pattern, detail in conditions_explained:\n",
    "    print(f\"\\n### {name} ###\")\n",
    "    print(f\"  Cache: {pattern}\")\n",
    "    print(f\"  Detail: {detail}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 7: Main eval loop \u2014 10 conditions \u00d7 1000 samples\n",
    "print(\"=\" * 70)\n",
    "print(\"PHASE 2: MAIN EVALUATION (10 conditions \u00d7 1000 samples)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "CONDITION_NAMES = [\n",
    "    'bare', 'llm_kw_trunc', 'llm_kw_suffix', 'llm_kw_sep',\n",
    "    'primed_values_only', 'primed_keys_only',\n",
    "    'prefix_1x', 'prefix_3x', 'prefix_5x',\n",
    "    'separator_only',\n",
    "]\n",
    "\n",
    "results = []\n",
    "start_idx = 0\n",
    "\n",
    "if CHECKPOINT_PATH.exists():\n",
    "    with open(CHECKPOINT_PATH, 'r') as f:\n",
    "        ckpt = json.load(f)\n",
    "    ckpt_queries = ckpt.get('sample_queries', [])\n",
    "    current_queries = [s['query'] for s in samples]\n",
    "    if ckpt_queries == current_queries:\n",
    "        results = ckpt['results']\n",
    "        start_idx = len(results)\n",
    "        print(f\"Resuming from checkpoint: {start_idx}/{N}\")\n",
    "    else:\n",
    "        print(\"Checkpoint sample mismatch. Starting fresh.\")\n",
    "else:\n",
    "    print(\"No checkpoint found. Starting fresh.\")\n",
    "\n",
    "print(f\"Evaluating samples {start_idx} to {N-1}\")\n",
    "print(f\"Conditions: {len(CONDITION_NAMES)}\")\n",
    "\n",
    "t_start = time.time()\n",
    "\n",
    "for idx in tqdm(range(start_idx, N), initial=start_idx, total=N, desc=\"Evaluating\"):\n",
    "    sample = samples[idx]\n",
    "    passage = sample['passage']\n",
    "    query = sample['query']\n",
    "    answer = sample['answer']\n",
    "\n",
    "    query_prompt = QUERY_TEMPLATE.format(query=query)\n",
    "    answer_text = ANSWER_TEMPLATE.format(answer=answer)\n",
    "    llm_kw_text = keyword_surrogates[idx]\n",
    "\n",
    "    # --- Matched tokenization ---\n",
    "    oracle_prefix = SURROGATE_PREFIX_TEMPLATE.format(surrogate=query)\n",
    "    document_text = DOCUMENT_TEMPLATE.format(document=passage)\n",
    "    full_oracle_text = oracle_prefix + document_text\n",
    "\n",
    "    full_oracle_enc = tokenizer(full_oracle_text, return_tensors=\"pt\",\n",
    "                                add_special_tokens=True, padding=False, truncation=False)\n",
    "    full_oracle_ids = full_oracle_enc['input_ids'].to(config.device)\n",
    "\n",
    "    oracle_prefix_enc = tokenizer(oracle_prefix, return_tensors=\"pt\",\n",
    "                                  add_special_tokens=True, padding=False, truncation=False)\n",
    "    oracle_prefix_len = oracle_prefix_enc['input_ids'].shape[1]\n",
    "\n",
    "    bos_id = full_oracle_ids[:, :1]\n",
    "    doc_ids = full_oracle_ids[:, oracle_prefix_len:]\n",
    "    doc_len = doc_ids.shape[1]\n",
    "\n",
    "    # Prefix IDs for LLM keyword (used for all truncated conditions)\n",
    "    kw_prefix_str = SURROGATE_PREFIX_TEMPLATE.format(surrogate=llm_kw_text)\n",
    "    kw_prefix_enc = tokenizer(kw_prefix_str, return_tensors=\"pt\",\n",
    "                              add_special_tokens=False, padding=False, truncation=False)\n",
    "    kw_prefix_ids = kw_prefix_enc['input_ids'].to(config.device)\n",
    "    kw_prefix_token_len_1x = kw_prefix_ids.shape[1]  # without BOS\n",
    "\n",
    "    # === Condition 1: BARE ===\n",
    "    bare_ids = torch.cat([bos_id, doc_ids], dim=1)\n",
    "    with torch.no_grad():\n",
    "        bare_out = model(input_ids=bare_ids, attention_mask=torch.ones_like(bare_ids),\n",
    "                         use_cache=True, return_dict=True)\n",
    "    bare_cache = bare_out.past_key_values\n",
    "    nll_bare = score_answer_with_cache(\n",
    "        deepcopy_cache(bare_cache), bare_ids.shape[1],\n",
    "        query_prompt, answer_text, model, tokenizer, config)\n",
    "\n",
    "    # === Condition 2: LLM-KEYWORD-TRUNC (also prefix-1x) ===\n",
    "    full_1x_ids = torch.cat([bos_id, kw_prefix_ids, doc_ids], dim=1)\n",
    "    prefix_token_len_1x = 1 + kw_prefix_token_len_1x\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out_1x = model(input_ids=full_1x_ids,\n",
    "                       attention_mask=torch.ones_like(full_1x_ids),\n",
    "                       use_cache=True, return_dict=True)\n",
    "    trunc_cache_1x = extract_and_truncate_cache_with_bos(out_1x.past_key_values, doc_len)\n",
    "    correct_rope_positions_with_bos(trunc_cache_1x, prefix_token_len_1x - 1, model)\n",
    "    nll_llm_kw_trunc = score_answer_with_cache(\n",
    "        deepcopy_cache(trunc_cache_1x), 1 + doc_len,\n",
    "        query_prompt, answer_text, model, tokenizer, config)\n",
    "    del out_1x\n",
    "\n",
    "    # prefix_1x is same as llm_kw_trunc\n",
    "    nll_prefix_1x = nll_llm_kw_trunc\n",
    "\n",
    "    # === Condition 3: LLM-KEYWORD-SUFFIX ===\n",
    "    suf_len, suf_cache = build_suffix_kv_cache(\n",
    "        passage, llm_kw_text, model, tokenizer, config, separator=SUFFIX_SEPARATOR)\n",
    "    nll_llm_kw_suffix = score_answer_with_cache(\n",
    "        deepcopy_cache(suf_cache), suf_len,\n",
    "        query_prompt, answer_text, model, tokenizer, config)\n",
    "    del suf_cache\n",
    "\n",
    "    # === Condition 4: LLM-KEYWORD+SEP (stacking) ===\n",
    "    suffix_enc = tokenizer(SUFFIX_SEPARATOR, return_tensors=\"pt\",\n",
    "                           add_special_tokens=False, padding=False, truncation=False)\n",
    "    suffix_ids = suffix_enc['input_ids'].to(config.device)\n",
    "    suffix_len_tok = suffix_ids.shape[1]\n",
    "    cache_len_before_suffix = 1 + doc_len\n",
    "\n",
    "    suffix_position_ids = torch.arange(\n",
    "        cache_len_before_suffix, cache_len_before_suffix + suffix_len_tok,\n",
    "        device=config.device\n",
    "    ).unsqueeze(0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        suffix_out = model(\n",
    "            input_ids=suffix_ids,\n",
    "            attention_mask=torch.ones(1, cache_len_before_suffix + suffix_len_tok,\n",
    "                                      device=config.device, dtype=torch.long),\n",
    "            position_ids=suffix_position_ids,\n",
    "            past_key_values=deepcopy_cache(trunc_cache_1x),\n",
    "            use_cache=True,\n",
    "            return_dict=True,\n",
    "        )\n",
    "    combo_cache = suffix_out.past_key_values\n",
    "    combo_len = cache_len_before_suffix + suffix_len_tok\n",
    "    nll_llm_kw_sep = score_answer_with_cache(\n",
    "        deepcopy_cache(combo_cache), combo_len,\n",
    "        query_prompt, answer_text, model, tokenizer, config)\n",
    "    del suffix_out, combo_cache\n",
    "\n",
    "    # === Condition 5: PRIMED-VALUES-ONLY ===\n",
    "    # Keys from bare cache, values from LLM-trunc cache\n",
    "    hybrid_values = build_hybrid_cache(\n",
    "        keys_source=bare_cache,\n",
    "        values_source=trunc_cache_1x,\n",
    "    )\n",
    "    nll_primed_values = score_answer_with_cache(\n",
    "        deepcopy_cache(hybrid_values), 1 + doc_len,\n",
    "        query_prompt, answer_text, model, tokenizer, config)\n",
    "    del hybrid_values\n",
    "\n",
    "    # === Condition 6: PRIMED-KEYS-ONLY ===\n",
    "    # Values from bare cache, keys from LLM-trunc cache\n",
    "    hybrid_keys = build_hybrid_cache(\n",
    "        keys_source=trunc_cache_1x,\n",
    "        values_source=bare_cache,\n",
    "    )\n",
    "    nll_primed_keys = score_answer_with_cache(\n",
    "        deepcopy_cache(hybrid_keys), 1 + doc_len,\n",
    "        query_prompt, answer_text, model, tokenizer, config)\n",
    "    del hybrid_keys\n",
    "\n",
    "    del bare_cache, trunc_cache_1x  # Free memory\n",
    "\n",
    "    # === Condition 8: PREFIX-3x ===\n",
    "    # [BOS][kw\\n kw\\n kw\\n][doc] \u2192 truncate + RoPE\n",
    "    prefix_3x_ids = kw_prefix_ids.repeat(1, 3)\n",
    "    full_3x_ids = torch.cat([bos_id, prefix_3x_ids, doc_ids], dim=1)\n",
    "    prefix_token_len_3x = 1 + prefix_3x_ids.shape[1]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out_3x = model(input_ids=full_3x_ids,\n",
    "                       attention_mask=torch.ones_like(full_3x_ids),\n",
    "                       use_cache=True, return_dict=True)\n",
    "    trunc_cache_3x = extract_and_truncate_cache_with_bos(out_3x.past_key_values, doc_len)\n",
    "    correct_rope_positions_with_bos(trunc_cache_3x, prefix_token_len_3x - 1, model)\n",
    "    nll_prefix_3x = score_answer_with_cache(\n",
    "        deepcopy_cache(trunc_cache_3x), 1 + doc_len,\n",
    "        query_prompt, answer_text, model, tokenizer, config)\n",
    "    del out_3x, trunc_cache_3x\n",
    "\n",
    "    # === Condition 9: PREFIX-5x ===\n",
    "    prefix_5x_ids = kw_prefix_ids.repeat(1, 5)\n",
    "    full_5x_ids = torch.cat([bos_id, prefix_5x_ids, doc_ids], dim=1)\n",
    "    prefix_token_len_5x = 1 + prefix_5x_ids.shape[1]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out_5x = model(input_ids=full_5x_ids,\n",
    "                       attention_mask=torch.ones_like(full_5x_ids),\n",
    "                       use_cache=True, return_dict=True)\n",
    "    trunc_cache_5x = extract_and_truncate_cache_with_bos(out_5x.past_key_values, doc_len)\n",
    "    correct_rope_positions_with_bos(trunc_cache_5x, prefix_token_len_5x - 1, model)\n",
    "    nll_prefix_5x = score_answer_with_cache(\n",
    "        deepcopy_cache(trunc_cache_5x), 1 + doc_len,\n",
    "        query_prompt, answer_text, model, tokenizer, config)\n",
    "    del out_5x, trunc_cache_5x\n",
    "\n",
    "    # === Condition 10: SEPARATOR-ONLY ===\n",
    "    sep_len, sep_cache = build_suffix_kv_cache(\n",
    "        passage, \"\", model, tokenizer, config, separator=SUFFIX_SEPARATOR)\n",
    "    nll_separator = score_answer_with_cache(\n",
    "        deepcopy_cache(sep_cache), sep_len,\n",
    "        query_prompt, answer_text, model, tokenizer, config)\n",
    "    del sep_cache\n",
    "\n",
    "    # --- Store result ---\n",
    "    result = {\n",
    "        'idx': idx,\n",
    "        'doc_len': doc_len,\n",
    "        'passage_word_count': len(passage.split()),\n",
    "        'bare': nll_bare,\n",
    "        'llm_kw_trunc': nll_llm_kw_trunc,\n",
    "        'llm_kw_suffix': nll_llm_kw_suffix,\n",
    "        'llm_kw_sep': nll_llm_kw_sep,\n",
    "        'primed_values_only': nll_primed_values,\n",
    "        'primed_keys_only': nll_primed_keys,\n",
    "        'prefix_1x': nll_prefix_1x,\n",
    "        'prefix_3x': nll_prefix_3x,\n",
    "        'prefix_5x': nll_prefix_5x,\n",
    "        'separator_only': nll_separator,\n",
    "    }\n",
    "    results.append(result)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    if (idx + 1) % CHECKPOINT_EVERY == 0 or idx == N - 1:\n",
    "        ckpt_data = {\n",
    "            'results': results,\n",
    "            'sample_queries': [s['query'] for s in samples],\n",
    "            'completed': len(results),\n",
    "            'total': N,\n",
    "            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        }\n",
    "        with open(CHECKPOINT_PATH, 'w') as f:\n",
    "            json.dump(ckpt_data, f)\n",
    "        elapsed = time.time() - t_start\n",
    "        rate = (idx - start_idx + 1) / elapsed if elapsed > 0 else 0\n",
    "        remaining = (N - idx - 1) / rate if rate > 0 else 0\n",
    "        tqdm.write(f\"  Checkpoint {idx+1}/{N} | {rate:.2f} s/s | ETA: {remaining/60:.1f} min\")\n",
    "\n",
    "elapsed_total = time.time() - t_start\n",
    "print(f\"\\nEvaluation complete: {len(results)} samples in {elapsed_total/60:.1f} min\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 8: Primary analysis \u2014 6 comparisons + NLL summary\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ANALYSIS \u2014 MECHANISM ISOLATION + AMPLIFICATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Extract arrays and filter zero NLLs\n",
    "cond_arrays = {}\n",
    "for cname in CONDITION_NAMES:\n",
    "    cond_arrays[cname] = np.array([r[cname] for r in results])\n",
    "\n",
    "valid = np.ones(len(results), dtype=bool)\n",
    "for cname in CONDITION_NAMES:\n",
    "    valid &= (cond_arrays[cname] != 0)\n",
    "n_valid = int(np.sum(valid))\n",
    "n_excluded = int(np.sum(~valid))\n",
    "print(f\"Total: {len(results)}, Valid: {n_valid}, Excluded: {n_excluded}\")\n",
    "\n",
    "c = {}\n",
    "for cname in CONDITION_NAMES:\n",
    "    c[cname] = cond_arrays[cname][valid]\n",
    "\n",
    "# NLL summary table\n",
    "print(f\"\\n{'Condition':<25} {'Mean NLL':>10} {'Std':>10} {'d vs Bare':>10}\")\n",
    "print(\"-\" * 60)\n",
    "for cname in CONDITION_NAMES:\n",
    "    mean_nll = np.mean(c[cname])\n",
    "    std_nll = np.std(c[cname])\n",
    "    if cname == 'bare':\n",
    "        d_str = \"\u2014\"\n",
    "    else:\n",
    "        d = cohens_d(c['bare'] - c[cname])\n",
    "        d_str = f\"{d:+.3f}\"\n",
    "    print(f\"{cname:<25} {mean_nll:>10.4f} {std_nll:>10.4f} {d_str:>10}\")\n",
    "\n",
    "# 6 primary comparisons\n",
    "print(f\"\\n{'='*85}\")\n",
    "print(f\"6 PRIMARY COMPARISONS (Bonferroni alpha = {BONFERRONI_ALPHA:.4f})\")\n",
    "print(f\"{'='*85}\")\n",
    "\n",
    "comparisons = [\n",
    "    ('C1: Values-only vs Bare',\n",
    "     c['bare'] - c['primed_values_only'],\n",
    "     'Is the effect in values?'),\n",
    "    ('C2: Keys-only vs Bare',\n",
    "     c['bare'] - c['primed_keys_only'],\n",
    "     'Is the effect in keys?'),\n",
    "    ('C3: Values-only vs LLM-trunc',\n",
    "     c['llm_kw_trunc'] - c['primed_values_only'],\n",
    "     'How much do values capture?'),\n",
    "    ('C4: Prefix-3x vs Prefix-1x',\n",
    "     c['prefix_1x'] - c['prefix_3x'],\n",
    "     'Does 3x amplify?'),\n",
    "    ('C5: Prefix-5x vs Prefix-1x',\n",
    "     c['prefix_1x'] - c['prefix_5x'],\n",
    "     'Does 5x amplify?'),\n",
    "    ('C6: Prefix-5x vs Prefix-3x',\n",
    "     c['prefix_3x'] - c['prefix_5x'],\n",
    "     'Diminishing returns?'),\n",
    "]\n",
    "\n",
    "print(f\"\\n{'Comparison':<35} {'Mean \u0394':>8} {'d':>8} {'Win%':>7} {'t':>8} {'p':>12} {'Sig':>5}\")\n",
    "print(\"-\" * 90)\n",
    "\n",
    "comparison_results = {}\n",
    "for name, delta, question in comparisons:\n",
    "    d = cohens_d(delta)\n",
    "    win = np.mean(delta > 0) * 100\n",
    "    t_stat, p_val = stats.ttest_1samp(delta, 0)\n",
    "    sig = \"***\" if p_val < 0.001 else \"**\" if p_val < BONFERRONI_ALPHA else \"*\" if p_val < 0.05 else \"ns\"\n",
    "    print(f\"{name:<35} {np.mean(delta):>8.4f} {d:>8.3f} {win:>6.1f}% {t_stat:>8.2f} {p_val:>11.2e} {sig:>5}\")\n",
    "    comparison_results[name] = {\n",
    "        'mean_delta': float(np.mean(delta)),\n",
    "        'cohens_d': float(d),\n",
    "        'win_rate': float(win / 100),\n",
    "        't_stat': float(t_stat),\n",
    "        'p_value': float(p_val),\n",
    "        'bonferroni_significant': bool(p_val < BONFERRONI_ALPHA),\n",
    "        'question': question,\n",
    "    }\n",
    "\n",
    "# All vs Bare\n",
    "print(f\"\\n{'='*85}\")\n",
    "print(\"ALL CONDITIONS vs BARE\")\n",
    "print(f\"{'='*85}\")\n",
    "print(f\"\\n{'Condition':<25} {'d vs Bare':>10} {'Win%':>7} {'p':>12}\")\n",
    "print(\"-\" * 60)\n",
    "all_vs_bare = {}\n",
    "for cname in CONDITION_NAMES:\n",
    "    if cname == 'bare':\n",
    "        continue\n",
    "    delta = c['bare'] - c[cname]\n",
    "    d = cohens_d(delta)\n",
    "    win = np.mean(delta > 0) * 100\n",
    "    _, p_val = stats.ttest_1samp(delta, 0)\n",
    "    sig = \"***\" if p_val < 0.001 else \"**\" if p_val < BONFERRONI_ALPHA else \"*\" if p_val < 0.05 else \"ns\"\n",
    "    print(f\"{cname:<25} {d:>10.3f} {win:>6.1f}% {p_val:>11.2e} {sig:>5}\")\n",
    "    all_vs_bare[cname] = {'cohens_d': float(d), 'win_rate': float(win/100), 'p_value': float(p_val)}"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 9: Mechanism isolation deep-dive + amplification analysis\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"MECHANISM ISOLATION DEEP-DIVE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "d_values_only = cohens_d(c['bare'] - c['primed_values_only'])\n",
    "d_keys_only = cohens_d(c['bare'] - c['primed_keys_only'])\n",
    "d_full_trunc = cohens_d(c['bare'] - c['llm_kw_trunc'])\n",
    "\n",
    "print(f\"\\nKey/Value Decomposition:\")\n",
    "print(f\"  Full LLM-trunc (keys+values): d = {d_full_trunc:+.3f}\")\n",
    "print(f\"  Values-only (bare keys):      d = {d_values_only:+.3f} ({d_values_only/d_full_trunc*100:.0f}% of full)\")\n",
    "print(f\"  Keys-only (bare values):       d = {d_keys_only:+.3f} ({d_keys_only/d_full_trunc*100:.0f}% of full)\")\n",
    "print(f\"  Sum of parts:                  d = {d_values_only + d_keys_only:+.3f} (vs full: {d_full_trunc:+.3f})\")\n",
    "\n",
    "if d_values_only > d_keys_only and d_values_only > 0:\n",
    "    if d_keys_only < 0.05:\n",
    "        print(f\"\\n\u2192 VALUES carry the priming signal. Keys contribute negligibly.\")\n",
    "    else:\n",
    "        print(f\"\\n\u2192 BOTH contribute, but values carry more ({d_values_only/d_full_trunc*100:.0f}% vs {d_keys_only/d_full_trunc*100:.0f}%).\")\n",
    "elif d_keys_only > d_values_only and d_keys_only > 0:\n",
    "    print(f\"\\n\u2192 KEYS carry the priming signal (unexpected \u2014 RoPE correction may be key).\")\n",
    "else:\n",
    "    print(f\"\\n\u2192 NEITHER component alone matches the combined effect \u2014 synergy required.\")\n",
    "\n",
    "# Amplification curve\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"PREFIX AMPLIFICATION CURVE\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "d_1x = cohens_d(c['bare'] - c['prefix_1x'])\n",
    "d_3x = cohens_d(c['bare'] - c['prefix_3x'])\n",
    "d_5x = cohens_d(c['bare'] - c['prefix_5x'])\n",
    "\n",
    "print(f\"\\nAmplification curve (d vs bare):\")\n",
    "print(f\"  1\u00d7 prefix: d = {d_1x:+.3f}\")\n",
    "print(f\"  3\u00d7 prefix: d = {d_3x:+.3f} (\u0394d from 1x = {d_3x - d_1x:+.3f})\")\n",
    "print(f\"  5\u00d7 prefix: d = {d_5x:+.3f} (\u0394d from 1x = {d_5x - d_1x:+.3f})\")\n",
    "print(f\"  3x\u21925x marginal: \u0394d = {d_5x - d_3x:+.3f}\")\n",
    "\n",
    "if d_5x > d_3x > d_1x:\n",
    "    if (d_5x - d_3x) < (d_3x - d_1x) * 0.5:\n",
    "        print(f\"\\n\u2192 Amplification works but with DIMINISHING RETURNS.\")\n",
    "    else:\n",
    "        print(f\"\\n\u2192 Amplification works with roughly LINEAR scaling.\")\n",
    "elif d_3x > d_1x and d_5x <= d_3x:\n",
    "    print(f\"\\n\u2192 Amplification helps up to 3\u00d7 but SATURATES at 5\u00d7.\")\n",
    "elif d_3x <= d_1x:\n",
    "    print(f\"\\n\u2192 No amplification effect \u2014 repetition does NOT help.\")\n",
    "\n",
    "# Hardness quintile for mechanism conditions\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"HARDNESS QUINTILE BREAKDOWN\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "bare_valid = c['bare']\n",
    "quintile_boundaries = np.percentile(bare_valid, [20, 40, 60, 80])\n",
    "quintile_labels = ['Q1 (easy)', 'Q2', 'Q3', 'Q4', 'Q5 (hard)']\n",
    "\n",
    "def get_quintile(nll, boundaries):\n",
    "    for i, b in enumerate(boundaries):\n",
    "        if nll <= b:\n",
    "            return i\n",
    "    return len(boundaries)\n",
    "\n",
    "quintiles = np.array([get_quintile(nll, quintile_boundaries) for nll in bare_valid])\n",
    "\n",
    "mechanism_conds = ['llm_kw_trunc', 'primed_values_only', 'primed_keys_only',\n",
    "                   'prefix_1x', 'prefix_3x', 'prefix_5x', 'llm_kw_sep', 'separator_only']\n",
    "\n",
    "header = f\"{'Condition':<25}\" + \"\".join(f\"{ql:>14}\" for ql in quintile_labels) + f\"{'Overall':>14}\"\n",
    "print(f\"\\n{header}\")\n",
    "print(\"-\" * (25 + 14 * 6))\n",
    "\n",
    "hardness_breakdown = {}\n",
    "for cname in mechanism_conds:\n",
    "    row = f\"{cname:<25}\"\n",
    "    quintile_ds = []\n",
    "    for q in range(5):\n",
    "        mask_q = quintiles == q\n",
    "        if np.sum(mask_q) < 10:\n",
    "            row += f\"{'n/a':>14}\"\n",
    "            quintile_ds.append(None)\n",
    "        else:\n",
    "            delta = bare_valid[mask_q] - c[cname][mask_q]\n",
    "            d = cohens_d(delta)\n",
    "            row += f\"{d:>+14.3f}\"\n",
    "            quintile_ds.append(float(d))\n",
    "    d_all = cohens_d(bare_valid - c[cname])\n",
    "    row += f\"{d_all:>+14.3f}\"\n",
    "    print(row)\n",
    "    hardness_breakdown[cname] = {'quintile_ds': quintile_ds, 'overall_d': float(d_all)}"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 10: Plots\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "\n",
    "# --- Plot 1: All conditions bar chart ---\n",
    "ax = axes[0, 0]\n",
    "cnames_sorted = sorted(\n",
    "    [cn for cn in CONDITION_NAMES if cn != 'bare'],\n",
    "    key=lambda cn: cohens_d(c['bare'] - c[cn]),\n",
    "    reverse=True\n",
    ")\n",
    "ds_bar = [cohens_d(c['bare'] - c[cn]) for cn in cnames_sorted]\n",
    "color_map = {\n",
    "    'llm_kw_trunc': 'forestgreen', 'llm_kw_suffix': 'limegreen',\n",
    "    'llm_kw_sep': 'gold', 'primed_values_only': 'steelblue',\n",
    "    'primed_keys_only': 'cornflowerblue', 'prefix_1x': 'mediumpurple',\n",
    "    'prefix_3x': 'darkorchid', 'prefix_5x': 'purple',\n",
    "    'separator_only': 'salmon',\n",
    "}\n",
    "colors_bar = [color_map.get(cn, 'lightgray') for cn in cnames_sorted]\n",
    "ax.barh(range(len(cnames_sorted)), ds_bar, color=colors_bar, edgecolor='black', linewidth=0.5)\n",
    "ax.set_yticks(range(len(cnames_sorted)))\n",
    "ax.set_yticklabels(cnames_sorted, fontsize=8)\n",
    "ax.axvline(x=0, color='gray', linestyle='--')\n",
    "ax.set_xlabel(\"Cohen's d vs Bare\")\n",
    "ax.set_title('All Conditions vs Bare')\n",
    "ax.invert_yaxis()\n",
    "\n",
    "# --- Plot 2: Key/Value decomposition ---\n",
    "ax = axes[0, 1]\n",
    "decomp_labels = ['Full\\n(K+V)', 'Values\\nonly', 'Keys\\nonly', 'Sum\\n(V+K)']\n",
    "decomp_vals = [d_full_trunc, d_values_only, d_keys_only, d_values_only + d_keys_only]\n",
    "decomp_colors = ['forestgreen', 'steelblue', 'cornflowerblue', 'lightgray']\n",
    "bars = ax.bar(range(len(decomp_labels)), decomp_vals, color=decomp_colors,\n",
    "              edgecolor='black', linewidth=0.5)\n",
    "ax.set_xticks(range(len(decomp_labels)))\n",
    "ax.set_xticklabels(decomp_labels, fontsize=9)\n",
    "ax.axhline(y=0, color='gray', linestyle='--')\n",
    "ax.set_ylabel(\"Cohen's d vs Bare\")\n",
    "ax.set_title('Key vs Value Decomposition')\n",
    "for i, v in enumerate(decomp_vals):\n",
    "    ax.text(i, v + 0.005, f\"{v:+.3f}\", ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# --- Plot 3: Amplification curve ---\n",
    "ax = axes[1, 0]\n",
    "reps = [1, 3, 5]\n",
    "amp_ds = [d_1x, d_3x, d_5x]\n",
    "ax.plot(reps, amp_ds, 'o-', color='purple', linewidth=2, markersize=8)\n",
    "ax.set_xlabel('Number of Prefix Repetitions')\n",
    "ax.set_ylabel(\"Cohen's d vs Bare\")\n",
    "ax.set_title('Prefix Amplification Curve')\n",
    "ax.set_xticks(reps)\n",
    "ax.grid(True, alpha=0.3)\n",
    "for x, y in zip(reps, amp_ds):\n",
    "    ax.annotate(f\"d={y:+.3f}\", (x, y), textcoords=\"offset points\",\n",
    "                xytext=(0, 10), ha='center', fontsize=9)\n",
    "\n",
    "# --- Plot 4: Hardness \u00d7 mechanism heatmap ---\n",
    "ax = axes[1, 1]\n",
    "hm_data = []\n",
    "for cname in mechanism_conds:\n",
    "    row = []\n",
    "    for q in range(5):\n",
    "        mask_q = quintiles == q\n",
    "        if np.sum(mask_q) < 10:\n",
    "            row.append(0)\n",
    "        else:\n",
    "            delta = bare_valid[mask_q] - c[cname][mask_q]\n",
    "            row.append(cohens_d(delta))\n",
    "    hm_data.append(row)\n",
    "hm_data = np.array(hm_data)\n",
    "im = ax.imshow(hm_data, cmap='RdBu_r', vmin=-0.5, vmax=0.7, aspect='auto')\n",
    "ax.set_xticks(range(5))\n",
    "ax.set_xticklabels(quintile_labels, fontsize=7)\n",
    "ax.set_yticks(range(len(mechanism_conds)))\n",
    "ax.set_yticklabels(mechanism_conds, fontsize=7)\n",
    "for i in range(len(mechanism_conds)):\n",
    "    for j in range(5):\n",
    "        ax.text(j, i, f\"{hm_data[i,j]:+.2f}\", ha='center', va='center', fontsize=6)\n",
    "plt.colorbar(im, ax=ax, label=\"Cohen's d vs bare\")\n",
    "ax.set_title('Hardness \u00d7 Mechanism')\n",
    "\n",
    "plt.suptitle('Exp 08: Mechanism Isolation + Prefix Amplification', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'analysis_plots.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"Plots saved to {RESULTS_DIR / 'analysis_plots.png'}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 11: Save comprehensive results JSON\n",
    "\n",
    "final = {\n",
    "    'experiment': 'exp08_mechanism_and_amplification',\n",
    "    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'config': {\n",
    "        'model_name': config.model_name,\n",
    "        'seed': SEED,\n",
    "        'n_eval': N,\n",
    "        'n_valid': n_valid,\n",
    "        'n_excluded': n_excluded,\n",
    "        'min_passage_words': config.min_passage_words,\n",
    "        'max_passage_words': config.max_passage_words,\n",
    "        'n_conditions': len(CONDITION_NAMES),\n",
    "        'n_comparisons': N_COMPARISONS,\n",
    "        'bonferroni_alpha': BONFERRONI_ALPHA,\n",
    "    },\n",
    "    'condition_names': CONDITION_NAMES,\n",
    "    'nll_summary': {\n",
    "        cname: {\n",
    "            'mean': float(np.mean(c[cname])),\n",
    "            'std': float(np.std(c[cname])),\n",
    "            'cohens_d_vs_bare': float(cohens_d(c['bare'] - c[cname])) if cname != 'bare' else 0.0,\n",
    "        }\n",
    "        for cname in CONDITION_NAMES\n",
    "    },\n",
    "    'mechanism_decomposition': {\n",
    "        'd_full_trunc': float(d_full_trunc),\n",
    "        'd_values_only': float(d_values_only),\n",
    "        'd_keys_only': float(d_keys_only),\n",
    "        'd_sum_parts': float(d_values_only + d_keys_only),\n",
    "        'values_fraction': float(d_values_only / d_full_trunc) if d_full_trunc != 0 else 0,\n",
    "        'keys_fraction': float(d_keys_only / d_full_trunc) if d_full_trunc != 0 else 0,\n",
    "    },\n",
    "    'amplification': {\n",
    "        'd_1x': float(d_1x),\n",
    "        'd_3x': float(d_3x),\n",
    "        'd_5x': float(d_5x),\n",
    "        'delta_1x_to_3x': float(d_3x - d_1x),\n",
    "        'delta_3x_to_5x': float(d_5x - d_3x),\n",
    "    },\n",
    "    'primary_comparisons': comparison_results,\n",
    "    'all_vs_bare': all_vs_bare,\n",
    "    'hardness_breakdown': hardness_breakdown,\n",
    "    'per_sample_results': results,\n",
    "}\n",
    "\n",
    "with open(FINAL_RESULTS_PATH, 'w') as f:\n",
    "    json.dump(final, f, indent=2)\n",
    "\n",
    "print(f\"Results saved to {FINAL_RESULTS_PATH}\")\n",
    "print(f\"File size: {FINAL_RESULTS_PATH.stat().st_size / 1024:.1f} KB\")\n",
    "print(\"\\nDone!\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 12: GPU cleanup\n",
    "import gc\n",
    "\n",
    "print(\"Cleaning up GPU memory...\")\n",
    "mem_before = torch.cuda.memory_allocated() / 1e9\n",
    "\n",
    "del model\n",
    "del tokenizer\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "mem_after = torch.cuda.memory_allocated() / 1e9\n",
    "print(f\"GPU memory: {mem_before:.2f} GB -> {mem_after:.2f} GB\")\n",
    "print(\"Cleanup complete.\")"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}