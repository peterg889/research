{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fc242d5",
   "metadata": {},
   "source": [
    "# Exp 29: Cross-Dataset Generalization on Hard QA Datasets (Gemma 3 4B)\n",
    "\n",
    "## Motivation\n",
    "\n",
    "Exp 27b established that Gemma hero layers generalize to NQ (d=+0.213, p<0.001), but\n",
    "TriviaQA and HotpotQA were dominated by ceiling effects (77% and 56% of samples with\n",
    "bare NLL < 0.01). This experiment tests three datasets **specifically chosen to avoid\n",
    "ceiling effects**:\n",
    "\n",
    "| Dataset | Why No Ceiling | Passage Length | Answer Type |\n",
    "|---------|---------------|----------------|-------------|\n",
    "| **DROP** | Requires counting/arithmetic — model must compute, not extract | ~150-300 tok | Numbers, dates, short spans |\n",
    "| **AdversarialQA** | Questions designed to fool RoBERTa — exploits model blind spots | ~100-300 tok | Extracted spans (but adversarial) |\n",
    "| **CoQA** | Abstractive answers — many valid phrasings spread probability mass | ~100-400 tok | Free-form natural language |\n",
    "\n",
    "## Conditions (same as Exp 27b)\n",
    "\n",
    "| # | Condition | Description |\n",
    "|---|-----------|-------------|\n",
    "| 1 | bare | Baseline |\n",
    "| 2 | sf_trunc | Standard priming |\n",
    "| 3 | sf_trunc_bias2 | +2.0 attention forcing |\n",
    "| 4 | values_only | All-layer value swap |\n",
    "| 5 | values_early | Layers 0-15 value swap |\n",
    "| 6 | values_hero | Layers {10,12,14,15,20} value swap |\n",
    "\n",
    "## Key Question\n",
    "\n",
    "Do hero layers (the best Gemma intervention) generalize to datasets with meaningful\n",
    "NLL spread? If so, this confirms the mechanism is broadly useful. If not, it may be\n",
    "NQ-specific."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73f1bd6a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T22:14:05.528340Z",
     "iopub.status.busy": "2026-02-16T22:14:05.527537Z",
     "iopub.status.idle": "2026-02-16T22:14:08.693718Z",
     "shell.execute_reply": "2026-02-16T22:14:08.691973Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEED: 42\n",
      "Results directory: results/exp29\n",
      "CUDA available: True\n",
      "GPU: NVIDIA L4\n",
      "GPU memory: 23.6 GB\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Setup\n",
    "import os\n",
    "os.umask(0o000)\n",
    "\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import gc\n",
    "import csv\n",
    "import numpy as np\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "RESULTS_DIR = Path(\"results/exp29\")\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CHECKPOINT_PATH = RESULTS_DIR / \"checkpoint.json\"\n",
    "FINAL_RESULTS_PATH = RESULTS_DIR / \"results.json\"\n",
    "CSV_PATH = RESULTS_DIR / \"results.csv\"\n",
    "\n",
    "print(f\"SEED: {SEED}\")\n",
    "print(f\"Results directory: {RESULTS_DIR}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a239a5d4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T22:14:08.698593Z",
     "iopub.status.busy": "2026-02-16T22:14:08.698127Z",
     "iopub.status.idle": "2026-02-16T22:14:25.079829Z",
     "shell.execute_reply": "2026-02-16T22:14:25.078827Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading google/gemma-3-4b-it (4-bit, bfloat16)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1db23364ad54146909f40f57802f3bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/883 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model loaded successfully.\n",
      "  Num layers: 34\n",
      "  Head dim: 256\n",
      "  Model dtype: torch.bfloat16\n",
      "  Sliding window: 1024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Cache key dtype: torch.bfloat16\n",
      "  Cache key shape: torch.Size([1, 4, 2, 256])\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Load Gemma 3 4B via load_model()\n",
    "sys.path.insert(0, \".\")\n",
    "\n",
    "from lib.config import ExperimentConfig\n",
    "from lib.model_utils import load_model\n",
    "\n",
    "MODEL_NAME = \"google/gemma-3-4b-it\"\n",
    "\n",
    "exp_config = ExperimentConfig(\n",
    "    model_name=MODEL_NAME,\n",
    "    model_type=\"gemma3\",\n",
    "    compute_dtype=\"auto\",  # resolves to bfloat16 for Gemma\n",
    "    use_4bit=True,\n",
    "    num_samples=2000,\n",
    "    seed=SEED,\n",
    ")\n",
    "\n",
    "print(f\"Loading {MODEL_NAME} (4-bit, bfloat16)...\")\n",
    "model, tokenizer = load_model(exp_config)\n",
    "\n",
    "from lib.kv_cache import _get_text_config, _get_head_dim, _ensure_dynamic_cache, _get_cache_keys\n",
    "\n",
    "text_config = _get_text_config(model.config)\n",
    "N_LAYERS = text_config.num_hidden_layers\n",
    "print(f\"\\nModel loaded successfully.\")\n",
    "print(f\"  Num layers: {N_LAYERS}\")\n",
    "print(f\"  Head dim: {_get_head_dim(model.config)}\")\n",
    "print(f\"  Model dtype: {model.dtype}\")\n",
    "print(f\"  Sliding window: {getattr(text_config, 'sliding_window', 'N/A')}\")\n",
    "\n",
    "# Verify with test forward pass\n",
    "sample_ids = tokenizer(\"test\", return_tensors=\"pt\")['input_ids'].to(exp_config.device)\n",
    "with torch.no_grad():\n",
    "    out = model(sample_ids, use_cache=True)\n",
    "    cache_check = _ensure_dynamic_cache(out.past_key_values)\n",
    "    k0 = _get_cache_keys(cache_check, 0)\n",
    "    print(f\"  Cache key dtype: {k0.dtype}\")\n",
    "    print(f\"  Cache key shape: {k0.shape}\")\n",
    "del out, sample_ids, cache_check\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1e1e8c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T22:14:25.084097Z",
     "iopub.status.busy": "2026-02-16T22:14:25.083028Z",
     "iopub.status.idle": "2026-02-16T22:14:25.092180Z",
     "shell.execute_reply": "2026-02-16T22:14:25.091233Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config ready\n",
      "  Model: google/gemma-3-4b-it\n",
      "  N per dataset: 300\n",
      "  MAX_DOC_TOKENS: 900 (sliding window constraint)\n",
      "  N_LAYERS: 34\n",
      "  EARLY_LAYER_CUTOFF: 16\n",
      "  HERO_LAYERS: [10, 12, 14, 15, 20]\n",
      "  Conditions: ['bare', 'sf_trunc', 'sf_trunc_bias2', 'values_only', 'values_early', 'values_hero']\n",
      "  Static fact prefix: 'What are the key facts I need to know?'\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Lib imports + constants\n",
    "from lib.kv_cache import (\n",
    "    _get_cache_keys,\n",
    "    _get_cache_values,\n",
    "    _set_cache_keys,\n",
    "    _set_cache_values,\n",
    "    _ensure_dynamic_cache,\n",
    "    extract_and_truncate_cache_with_bos,\n",
    "    correct_rope_positions_with_bos,\n",
    "    score_answer_with_cache,\n",
    "    deepcopy_cache,\n",
    "    replace_values_at_layers,\n",
    ")\n",
    "from lib.analysis import cohens_d\n",
    "from lib.data import count_words\n",
    "from scipy import stats\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Templates -- bare text, no \"Document:\\n\" framing\n",
    "SURROGATE_PREFIX_TEMPLATE = \"{surrogate}\\n\"\n",
    "DOCUMENT_TEMPLATE = \"{document}\"\n",
    "QUERY_TEMPLATE = \"\\nQuestion: {question}\\nAnswer:\"\n",
    "ANSWER_TEMPLATE = \" {answer}\"\n",
    "\n",
    "# Prefix text\n",
    "from lib.surrogate import STATIC_SURROGATE_QUERIES\n",
    "STATIC_FACT = STATIC_SURROGATE_QUERIES['static_factual']['query']\n",
    "\n",
    "# Experiment parameters\n",
    "N_PER_DATASET = 300\n",
    "# Gemma sliding window = 1024: total seq must be < 1024\n",
    "# Primed pass: 1(BOS) + ~10(prefix) + doc_len < 1024 -> doc_len < ~1013\n",
    "# Cap at 900 for safety (matching Exp 21/27b)\n",
    "MAX_DOC_TOKENS = 900\n",
    "CHECKPOINT_EVERY = 25\n",
    "\n",
    "# Conditions\n",
    "CONDITION_NAMES = ['bare', 'sf_trunc', 'sf_trunc_bias2', 'values_only',\n",
    "                   'values_early', 'values_hero']\n",
    "\n",
    "# Layer-selective conditions from Exps 19/21/24\n",
    "EARLY_LAYER_CUTOFF = 16  # layers 0-15\n",
    "HERO_LAYERS = [10, 12, 14, 15, 20]  # from Exp 24 single-layer scan\n",
    "\n",
    "# Length bins for stratified analysis (token count)\n",
    "LENGTH_BINS = [\n",
    "    ('<128', 0, 128),\n",
    "    ('128-256', 128, 256),\n",
    "    ('256-512', 256, 512),\n",
    "    ('512-900', 512, 901),\n",
    "]\n",
    "\n",
    "print(\"Config ready\")\n",
    "print(f\"  Model: {MODEL_NAME}\")\n",
    "print(f\"  N per dataset: {N_PER_DATASET}\")\n",
    "print(f\"  MAX_DOC_TOKENS: {MAX_DOC_TOKENS} (sliding window constraint)\")\n",
    "print(f\"  N_LAYERS: {N_LAYERS}\")\n",
    "print(f\"  EARLY_LAYER_CUTOFF: {EARLY_LAYER_CUTOFF}\")\n",
    "print(f\"  HERO_LAYERS: {HERO_LAYERS}\")\n",
    "print(f\"  Conditions: {CONDITION_NAMES}\")\n",
    "print(f\"  Static fact prefix: '{STATIC_FACT}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81faf753",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T22:14:25.095574Z",
     "iopub.status.busy": "2026-02-16T22:14:25.095009Z",
     "iopub.status.idle": "2026-02-16T22:14:26.381746Z",
     "shell.execute_reply": "2026-02-16T22:14:26.380934Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "LOADING DROP (validation)\n",
      "======================================================================\n",
      "DROP requires counting, sorting, arithmetic over passage content.\n",
      "Answers are numbers, dates, or short spans that the model must COMPUTE.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DROP validation size: 9535\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48ffc956f586482884a454c079f49958",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filtering DROP:   0%|          | 0/9535 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cached 300 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DROP samples: 300\n",
      "  Word counts: mean=174, min=67, max=488\n",
      "  Answer word lengths: mean=1.5, min=1, max=8\n",
      "  Example 1:\n",
      "    Q: How many years after the last Battle IN Guadalajara did king Fernando III give a new fuero to the city?\n",
      "    A: 7\n",
      "    Passage (first 120 chars): In 1085, Guadalajara was retaken by the Christian forces of Alfonso VI . The chronicles say that the Christian army was ...\n",
      "  Example 2:\n",
      "    Q: Which group from the census in Skopje is larger: Macedonians or Serbs?\n",
      "    A: Macedonians\n",
      "    Passage (first 120 chars): Skopje, as the Republic of Macedonia as a whole, is characterised by a large ethnic diversity. The city is located in a ...\n",
      "  Example 3:\n",
      "    Q: How many points were scored in the first quarter?\n",
      "    A: 0\n",
      "    Passage (first 120 chars):  Coming off their impressive road win over the 49ers, the Falcons went home for a Week 6 Sunday night duel with the Chic...\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Load DROP dataset (numerical/discrete reasoning)\n",
    "from datasets import load_dataset\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"LOADING DROP (validation)\")\n",
    "print(\"=\" * 70)\n",
    "print(\"DROP requires counting, sorting, arithmetic over passage content.\")\n",
    "print(\"Answers are numbers, dates, or short spans that the model must COMPUTE.\")\n",
    "\n",
    "DROP_CACHE = RESULTS_DIR / \"drop_samples.json\"\n",
    "\n",
    "if DROP_CACHE.exists():\n",
    "    with open(DROP_CACHE, 'r') as f:\n",
    "        drop_samples = json.load(f)\n",
    "    print(f\"Loaded {len(drop_samples)} cached DROP samples\")\n",
    "else:\n",
    "    drop_ds = load_dataset(\"drop\", split=\"validation\")\n",
    "    print(f\"DROP validation size: {len(drop_ds)}\")\n",
    "\n",
    "    drop_samples = []\n",
    "    np.random.seed(SEED)\n",
    "\n",
    "    for item in tqdm(drop_ds, desc=\"Filtering DROP\"):\n",
    "        passage = item.get('passage', '')\n",
    "        question = item.get('question', '')\n",
    "        answers_info = item.get('answers_spans', {})\n",
    "\n",
    "        # Extract answer spans\n",
    "        spans = answers_info.get('spans', [])\n",
    "        if not spans:\n",
    "            continue\n",
    "        answer_text = spans[0]  # use first valid answer\n",
    "\n",
    "        if not question or not answer_text or not passage:\n",
    "            continue\n",
    "        if len(answer_text.strip()) == 0:\n",
    "            continue\n",
    "\n",
    "        wc = count_words(passage)\n",
    "        if wc < 30 or wc > 2000:\n",
    "            continue\n",
    "\n",
    "        drop_samples.append({\n",
    "            'passage': passage,\n",
    "            'query': question,\n",
    "            'answer': answer_text,\n",
    "            'word_count': wc,\n",
    "            'dataset': 'drop',\n",
    "            'all_answers': spans,  # keep all valid answers for reference\n",
    "        })\n",
    "\n",
    "        if len(drop_samples) >= N_PER_DATASET * 3:\n",
    "            break\n",
    "\n",
    "    np.random.shuffle(drop_samples)\n",
    "    drop_samples = drop_samples[:N_PER_DATASET]\n",
    "\n",
    "    with open(DROP_CACHE, 'w') as f:\n",
    "        json.dump(drop_samples, f)\n",
    "    print(f\"Cached {len(drop_samples)} samples\")\n",
    "\n",
    "    del drop_ds\n",
    "    gc.collect()\n",
    "\n",
    "print(f\"DROP samples: {len(drop_samples)}\")\n",
    "wcs = [s['word_count'] for s in drop_samples]\n",
    "ans_lens = [len(s['answer'].split()) for s in drop_samples]\n",
    "print(f\"  Word counts: mean={np.mean(wcs):.0f}, min={min(wcs)}, max={max(wcs)}\")\n",
    "print(f\"  Answer word lengths: mean={np.mean(ans_lens):.1f}, min={min(ans_lens)}, max={max(ans_lens)}\")\n",
    "if drop_samples:\n",
    "    for i in range(min(3, len(drop_samples))):\n",
    "        print(f\"  Example {i+1}:\")\n",
    "        print(f\"    Q: {drop_samples[i]['query']}\")\n",
    "        print(f\"    A: {drop_samples[i]['answer']}\")\n",
    "        print(f\"    Passage (first 120 chars): {drop_samples[i]['passage'][:120]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b93628e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T22:14:26.385558Z",
     "iopub.status.busy": "2026-02-16T22:14:26.384838Z",
     "iopub.status.idle": "2026-02-16T22:14:27.728429Z",
     "shell.execute_reply": "2026-02-16T22:14:27.727572Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "LOADING ADVERSARIALQA (droberta, validation)\n",
      "======================================================================\n",
      "Questions written by humans specifically to fool RoBERTa.\n",
      "Same extractive format as SQuAD, but adversarially chosen hard cases.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdversarialQA droberta validation size: 1000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81332e3558644c99a304dfb48cb7e422",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filtering AdversarialQA:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cached 300 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdversarialQA samples: 300\n",
      "  Word counts: mean=115, min=31, max=327\n",
      "  Answer word lengths: mean=3.2, min=1, max=39\n",
      "  Example 1:\n",
      "    Q: without luther's backing\n",
      "    A: many rebels laid down their weapons\n",
      "    Passage (first 120 chars): Without Luther's backing for the uprising, many rebels laid down their weapons; others felt betrayed. Their defeat by th...\n",
      "  Example 2:\n",
      "    Q: What organelle is only found in plants which they use to photosynthesize?\n",
      "    A: chloroplast\n",
      "    Passage (first 120 chars): Plastoglobuli (singular plastoglobulus, sometimes spelled plastoglobule(s)), are spherical bubbles of lipids and protein...\n",
      "  Example 3:\n",
      "    Q: what bus station start with the letter S?\n",
      "    A: Stagecoach\n",
      "    Passage (first 120 chars): There are 3 main bus companies providing services in the city; Arriva North East, Go North East and Stagecoach North Eas...\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Load AdversarialQA dataset (adversarially hard extractive QA)\n",
    "print(\"=\" * 70)\n",
    "print(\"LOADING ADVERSARIALQA (droberta, validation)\")\n",
    "print(\"=\" * 70)\n",
    "print(\"Questions written by humans specifically to fool RoBERTa.\")\n",
    "print(\"Same extractive format as SQuAD, but adversarially chosen hard cases.\")\n",
    "\n",
    "AQA_CACHE = RESULTS_DIR / \"aqa_samples.json\"\n",
    "\n",
    "if AQA_CACHE.exists():\n",
    "    with open(AQA_CACHE, 'r') as f:\n",
    "        aqa_samples = json.load(f)\n",
    "    print(f\"Loaded {len(aqa_samples)} cached AdversarialQA samples\")\n",
    "else:\n",
    "    aqa_ds = load_dataset(\"adversarial_qa\", \"droberta\", split=\"validation\")\n",
    "    print(f\"AdversarialQA droberta validation size: {len(aqa_ds)}\")\n",
    "\n",
    "    aqa_samples = []\n",
    "    np.random.seed(SEED)\n",
    "\n",
    "    for item in tqdm(aqa_ds, desc=\"Filtering AdversarialQA\"):\n",
    "        context = item.get('context', '')\n",
    "        question = item.get('question', '')\n",
    "        answers_info = item.get('answers', {})\n",
    "\n",
    "        answer_texts = answers_info.get('text', [])\n",
    "        if not answer_texts:\n",
    "            continue\n",
    "        answer_text = answer_texts[0]\n",
    "\n",
    "        if not question or not answer_text or not context:\n",
    "            continue\n",
    "\n",
    "        wc = count_words(context)\n",
    "        if wc < 30 or wc > 2000:\n",
    "            continue\n",
    "\n",
    "        aqa_samples.append({\n",
    "            'passage': context,\n",
    "            'query': question,\n",
    "            'answer': answer_text,\n",
    "            'word_count': wc,\n",
    "            'dataset': 'adversarialqa',\n",
    "        })\n",
    "\n",
    "    np.random.shuffle(aqa_samples)\n",
    "    aqa_samples = aqa_samples[:N_PER_DATASET]\n",
    "\n",
    "    with open(AQA_CACHE, 'w') as f:\n",
    "        json.dump(aqa_samples, f)\n",
    "    print(f\"Cached {len(aqa_samples)} samples\")\n",
    "\n",
    "    del aqa_ds\n",
    "    gc.collect()\n",
    "\n",
    "print(f\"AdversarialQA samples: {len(aqa_samples)}\")\n",
    "wcs = [s['word_count'] for s in aqa_samples]\n",
    "ans_lens = [len(s['answer'].split()) for s in aqa_samples]\n",
    "print(f\"  Word counts: mean={np.mean(wcs):.0f}, min={min(wcs)}, max={max(wcs)}\")\n",
    "print(f\"  Answer word lengths: mean={np.mean(ans_lens):.1f}, min={min(ans_lens)}, max={max(ans_lens)}\")\n",
    "if aqa_samples:\n",
    "    for i in range(min(3, len(aqa_samples))):\n",
    "        print(f\"  Example {i+1}:\")\n",
    "        print(f\"    Q: {aqa_samples[i]['query']}\")\n",
    "        print(f\"    A: {aqa_samples[i]['answer']}\")\n",
    "        print(f\"    Passage (first 120 chars): {aqa_samples[i]['passage'][:120]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7f85f8d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T22:14:27.731891Z",
     "iopub.status.busy": "2026-02-16T22:14:27.731608Z",
     "iopub.status.idle": "2026-02-16T22:14:28.763015Z",
     "shell.execute_reply": "2026-02-16T22:14:28.762106Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "LOADING COQA (validation)\n",
      "======================================================================\n",
      "Free-form abstractive answers across 7 domains.\n",
      "Using FIRST question per story only (no conversational dependency).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CoQA validation stories: 500\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "925b606372074a9d9ad4cde5e9382810",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing CoQA:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cached 300 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CoQA samples: 300\n",
      "  Word counts: mean=259, min=125, max=804\n",
      "  Answer word lengths: mean=2.3, min=1, max=11\n",
      "  Domain distribution: {'gutenberg': 53, 'mctest': 61, 'cnn': 66, 'wikipedia': 60, 'race': 60}\n",
      "  Example 1 (domain=gutenberg):\n",
      "    Q: Who is Nat's father?\n",
      "    A: the money-lender\n",
      "    Passage (first 120 chars): CHAPTER XXII \n",
      "\n",
      "AFTER THE RUNAWAYS \n",
      "\n",
      "\"Why, Nat, what do you mean?\" demanded Dave. \n",
      "\n",
      "\"I mean just what I say!\" declared th...\n",
      "  Example 2 (domain=mctest):\n",
      "    Q: What season was it?\n",
      "    A: winter\n",
      "    Passage (first 120 chars): Luna the hawk wanted some socks. It was the middle of winter and sitting on tree branches made her feet very cold. Luna ...\n",
      "  Example 3 (domain=cnn):\n",
      "    Q: Who created Do the Right Thing?\n",
      "    A: Spike Lee\n",
      "    Passage (first 120 chars): ATLANTA, Georgia (CNN) -- In 1989, the warnings were dire. The Spike Lee film \"Do the Right Thing,\" critics and columnis...\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Load CoQA dataset (abstractive conversational QA)\n",
    "print(\"=\" * 70)\n",
    "print(\"LOADING COQA (validation)\")\n",
    "print(\"=\" * 70)\n",
    "print(\"Free-form abstractive answers across 7 domains.\")\n",
    "print(\"Using FIRST question per story only (no conversational dependency).\")\n",
    "\n",
    "COQA_CACHE = RESULTS_DIR / \"coqa_samples.json\"\n",
    "\n",
    "if COQA_CACHE.exists():\n",
    "    with open(COQA_CACHE, 'r') as f:\n",
    "        coqa_samples = json.load(f)\n",
    "    print(f\"Loaded {len(coqa_samples)} cached CoQA samples\")\n",
    "else:\n",
    "    coqa_ds = load_dataset(\"stanfordnlp/coqa\", split=\"validation\")\n",
    "    print(f\"CoQA validation stories: {len(coqa_ds)}\")\n",
    "\n",
    "    coqa_samples = []\n",
    "    np.random.seed(SEED)\n",
    "\n",
    "    for item in tqdm(coqa_ds, desc=\"Processing CoQA\"):\n",
    "        story = item.get('story', '')\n",
    "        questions = item.get('questions', [])\n",
    "        answers_info = item.get('answers', {})\n",
    "        answer_texts = answers_info.get('input_text', [])\n",
    "\n",
    "        if not story or not questions or not answer_texts:\n",
    "            continue\n",
    "\n",
    "        # Use first question only (no conversational dependency)\n",
    "        question = questions[0]\n",
    "        answer_text = answer_texts[0]\n",
    "\n",
    "        if not question or not answer_text:\n",
    "            continue\n",
    "        if answer_text.strip().lower() in ('unknown', 'n/a', ''):\n",
    "            continue\n",
    "\n",
    "        wc = count_words(story)\n",
    "        if wc < 30 or wc > 2000:\n",
    "            continue\n",
    "\n",
    "        coqa_samples.append({\n",
    "            'passage': story,\n",
    "            'query': question,\n",
    "            'answer': answer_text,\n",
    "            'word_count': wc,\n",
    "            'dataset': 'coqa',\n",
    "            'source_domain': item.get('source', 'unknown'),\n",
    "        })\n",
    "\n",
    "    # CoQA has 500 stories; if we don't get 300 from first questions,\n",
    "    # add second questions from remaining stories\n",
    "    if len(coqa_samples) < N_PER_DATASET:\n",
    "        print(f\"  Only {len(coqa_samples)} first-question samples. Adding second questions...\")\n",
    "        used_stories = {s['passage'][:50] for s in coqa_samples}\n",
    "        for item in coqa_ds:\n",
    "            story = item.get('story', '')\n",
    "            questions = item.get('questions', [])\n",
    "            answers_info = item.get('answers', {})\n",
    "            answer_texts = answers_info.get('input_text', [])\n",
    "\n",
    "            if len(questions) < 2 or len(answer_texts) < 2:\n",
    "                continue\n",
    "            if story[:50] in used_stories:\n",
    "                # Second question from a story we already used\n",
    "                question = questions[1]\n",
    "                answer_text = answer_texts[1]\n",
    "\n",
    "                if not question or not answer_text:\n",
    "                    continue\n",
    "                if answer_text.strip().lower() in ('unknown', 'n/a', ''):\n",
    "                    continue\n",
    "\n",
    "                wc = count_words(story)\n",
    "                if wc < 30 or wc > 2000:\n",
    "                    continue\n",
    "\n",
    "                coqa_samples.append({\n",
    "                    'passage': story,\n",
    "                    'query': question,\n",
    "                    'answer': answer_text,\n",
    "                    'word_count': wc,\n",
    "                    'dataset': 'coqa',\n",
    "                    'source_domain': item.get('source', 'unknown'),\n",
    "                })\n",
    "\n",
    "            if len(coqa_samples) >= N_PER_DATASET * 2:\n",
    "                break\n",
    "\n",
    "    np.random.shuffle(coqa_samples)\n",
    "    coqa_samples = coqa_samples[:N_PER_DATASET]\n",
    "\n",
    "    with open(COQA_CACHE, 'w') as f:\n",
    "        json.dump(coqa_samples, f)\n",
    "    print(f\"Cached {len(coqa_samples)} samples\")\n",
    "\n",
    "    del coqa_ds\n",
    "    gc.collect()\n",
    "\n",
    "print(f\"CoQA samples: {len(coqa_samples)}\")\n",
    "wcs = [s['word_count'] for s in coqa_samples]\n",
    "ans_lens = [len(s['answer'].split()) for s in coqa_samples]\n",
    "# Domain distribution\n",
    "domains = {}\n",
    "for s_ in coqa_samples:\n",
    "    d = s_.get('source_domain', 'unknown')\n",
    "    domains[d] = domains.get(d, 0) + 1\n",
    "print(f\"  Word counts: mean={np.mean(wcs):.0f}, min={min(wcs)}, max={max(wcs)}\")\n",
    "print(f\"  Answer word lengths: mean={np.mean(ans_lens):.1f}, min={min(ans_lens)}, max={max(ans_lens)}\")\n",
    "print(f\"  Domain distribution: {domains}\")\n",
    "if coqa_samples:\n",
    "    for i in range(min(3, len(coqa_samples))):\n",
    "        print(f\"  Example {i+1} (domain={coqa_samples[i].get('source_domain', '?')}):\")\n",
    "        print(f\"    Q: {coqa_samples[i]['query']}\")\n",
    "        print(f\"    A: {coqa_samples[i]['answer']}\")\n",
    "        print(f\"    Passage (first 120 chars): {coqa_samples[i]['passage'][:120]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e00fc5d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T22:14:28.766826Z",
     "iopub.status.busy": "2026-02-16T22:14:28.766535Z",
     "iopub.status.idle": "2026-02-16T22:14:57.619637Z",
     "shell.execute_reply": "2026-02-16T22:14:57.618740Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "UNIFIED SAMPLE POOL\n",
      "======================================================================\n",
      "Total samples: 900\n",
      "  drop: n=300, mean_words=174, range=[67, 488]\n",
      "  adversarialqa: n=300, mean_words=115, range=[31, 327]\n",
      "  coqa: n=300, mean_words=259, range=[125, 804]\n",
      "\n",
      "Prefix: 'What are the key facts I need to know?'\n",
      "  Token length (no BOS): 11\n",
      "  Max primed sequence: 1 + 11 + 900 = 912\n",
      "  Sliding window: 1024\n",
      "  SAFE: 912 < 1024\n",
      "\n",
      "Tokenizing documents to measure token lengths...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f473727643df4789b94dc3e8d642d693",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing:   0%|          | 0/900 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Documents truncated to 900: 1/900 (0%)\n",
      "  drop: mean_tok=268, median=256, truncated=0/300 (0%), mean_ans_tok=2.7\n",
      "  adversarialqa: mean_tok=153, median=148, truncated=0/300 (0%), mean_ans_tok=4.5\n",
      "  coqa: mean_tok=342, median=346, truncated=1/300 (0%), mean_ans_tok=3.1\n",
      "\n",
      "======================================================================\n",
      "PRE-SCREENING: Bare NLL distribution check (20 samples/dataset)\n",
      "If median bare NLL < 0.05, ceiling effects may dominate.\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  drop           : median=0.018, mean=1.427, pct_floor(<0.01)=35% -> MARGINAL\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adversarialqa  : median=0.003, mean=0.412, pct_floor(<0.01)=60% -> WARNING: CEILING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  coqa           : median=0.000, mean=0.449, pct_floor(<0.01)=70% -> WARNING: CEILING\n",
      "\n",
      "Pre-screening complete. Proceeding with full experiment.\n",
      "\n",
      "======================================================================\n",
      "EXPERIMENTAL CONDITIONS (Gemma 3 4B)\n",
      "======================================================================\n",
      "\n",
      "### 1. bare ###\n",
      "  Forward: [BOS][doc]\n",
      "  Baseline. Standard causal attention.\n",
      "\n",
      "### 2. sf_trunc (standard priming) ###\n",
      "  Forward: [BOS][prefix_11][doc]\n",
      "  Standard causal, truncate + RoPE. Keys carry negative interference on Gemma.\n",
      "\n",
      "### 3. sf_trunc_bias2 (attention forcing, bias=+2.0) ###\n",
      "  Forward: [BOS][prefix_11][doc] with +2.0 bias\n",
      "  Novel: amplifies doc->prefix attention during cache building.\n",
      "\n",
      "### 4. values_only (all layers) ###\n",
      "  Bare keys + all primed values from sf_trunc cache.\n",
      "  Expected d ~ +0.056 (Exp 16 on MARCO). Bypasses key interference.\n",
      "\n",
      "### 5. values_early (layers 0-15 only) ###\n",
      "  Bare keys + primed values from layers 0-15 only.\n",
      "  Expected best: d ~ +0.211 (Exp 19 on MARCO). Late layers carry interference.\n",
      "\n",
      "### 6. values_hero (layers {10,12,14,15,20}) ###\n",
      "  Bare keys + primed values from 5 hero layers identified in Exp 24.\n",
      "  NQ generalization: d=+0.213 (Exp 27b).\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Unified sample pool + tokenization + pre-screening\n",
    "print(\"=\" * 70)\n",
    "print(\"UNIFIED SAMPLE POOL\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "all_samples = []\n",
    "for ds_name, ds_samples in [(\"drop\", drop_samples),\n",
    "                              (\"adversarialqa\", aqa_samples),\n",
    "                              (\"coqa\", coqa_samples)]:\n",
    "    for sample in ds_samples:\n",
    "        sample['dataset'] = ds_name\n",
    "    all_samples.extend(ds_samples)\n",
    "\n",
    "print(f\"Total samples: {len(all_samples)}\")\n",
    "for ds_name in ['drop', 'adversarialqa', 'coqa']:\n",
    "    ds_s = [s for s in all_samples if s['dataset'] == ds_name]\n",
    "    wcs = [s['word_count'] for s in ds_s]\n",
    "    print(f\"  {ds_name}: n={len(ds_s)}, mean_words={np.mean(wcs):.0f}, \"\n",
    "          f\"range=[{min(wcs)}, {max(wcs)}]\")\n",
    "\n",
    "# Tokenize prefix\n",
    "sf_str = SURROGATE_PREFIX_TEMPLATE.format(surrogate=STATIC_FACT)\n",
    "sf_ids = tokenizer(sf_str, return_tensors=\"pt\",\n",
    "                    add_special_tokens=False)['input_ids'].to(exp_config.device)\n",
    "PREFIX_TOKEN_LEN = sf_ids.shape[1]\n",
    "\n",
    "print(f\"\\nPrefix: '{STATIC_FACT}'\")\n",
    "print(f\"  Token length (no BOS): {PREFIX_TOKEN_LEN}\")\n",
    "\n",
    "# Verify sliding window safety\n",
    "max_primed_seq = 1 + PREFIX_TOKEN_LEN + MAX_DOC_TOKENS\n",
    "print(f\"  Max primed sequence: 1 + {PREFIX_TOKEN_LEN} + {MAX_DOC_TOKENS} = {max_primed_seq}\")\n",
    "print(f\"  Sliding window: 1024\")\n",
    "assert max_primed_seq < 1024, f\"UNSAFE: {max_primed_seq} >= 1024\"\n",
    "print(f\"  SAFE: {max_primed_seq} < 1024\")\n",
    "\n",
    "# Tokenize doc lengths\n",
    "print(f\"\\nTokenizing documents to measure token lengths...\")\n",
    "n_truncated = 0\n",
    "for sample in tqdm(all_samples, desc=\"Tokenizing\"):\n",
    "    tok_len = len(tokenizer.encode(sample['passage'], add_special_tokens=False))\n",
    "    if tok_len > MAX_DOC_TOKENS:\n",
    "        n_truncated += 1\n",
    "    sample['doc_token_len'] = min(tok_len, MAX_DOC_TOKENS)\n",
    "    sample['answer_token_len'] = len(tokenizer.encode(sample['answer'], add_special_tokens=False))\n",
    "\n",
    "print(f\"  Documents truncated to {MAX_DOC_TOKENS}: {n_truncated}/{len(all_samples)} \"\n",
    "      f\"({100*n_truncated/len(all_samples):.0f}%)\")\n",
    "\n",
    "for ds_name in ['drop', 'adversarialqa', 'coqa']:\n",
    "    ds_s = [s for s in all_samples if s['dataset'] == ds_name]\n",
    "    tls = [s['doc_token_len'] for s in ds_s]\n",
    "    atls = [s['answer_token_len'] for s in ds_s]\n",
    "    n_trunc = sum(1 for s in ds_s if s['doc_token_len'] == MAX_DOC_TOKENS)\n",
    "    print(f\"  {ds_name}: mean_tok={np.mean(tls):.0f}, median={np.median(tls):.0f}, \"\n",
    "          f\"truncated={n_trunc}/{len(ds_s)} ({100*n_trunc/len(ds_s):.0f}%), \"\n",
    "          f\"mean_ans_tok={np.mean(atls):.1f}\")\n",
    "\n",
    "# === PRE-SCREENING: Bare NLL check ===\n",
    "# Quick bare NLL on 20 samples per dataset to check for ceiling effects\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"PRE-SCREENING: Bare NLL distribution check (20 samples/dataset)\")\n",
    "print(\"If median bare NLL < 0.05, ceiling effects may dominate.\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for ds_name in ['drop', 'adversarialqa', 'coqa']:\n",
    "    ds_s = [s for s in all_samples if s['dataset'] == ds_name][:20]\n",
    "    bare_nlls = []\n",
    "    for sample in ds_s:\n",
    "        passage = sample['passage']\n",
    "        question = sample['query']\n",
    "        answer = sample['answer']\n",
    "\n",
    "        document_text = DOCUMENT_TEMPLATE.format(document=passage)\n",
    "        query_prompt = QUERY_TEMPLATE.format(question=question)\n",
    "        answer_text = ANSWER_TEMPLATE.format(answer=answer)\n",
    "\n",
    "        doc_ids = tokenizer(document_text, return_tensors=\"pt\",\n",
    "                            add_special_tokens=False)['input_ids'].to(exp_config.device)\n",
    "        if doc_ids.shape[1] > MAX_DOC_TOKENS:\n",
    "            doc_ids = doc_ids[:, :MAX_DOC_TOKENS]\n",
    "\n",
    "        bos_id = tokenizer.bos_token_id\n",
    "        if bos_id is None:\n",
    "            bos_id = tokenizer.encode(\"\", add_special_tokens=True)[0]\n",
    "        bos_tensor = torch.tensor([[bos_id]], device=exp_config.device)\n",
    "        bare_input = torch.cat([bos_tensor, doc_ids], dim=1)\n",
    "        context_len = bare_input.shape[1]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            bare_out = model(input_ids=bare_input,\n",
    "                             attention_mask=torch.ones_like(bare_input),\n",
    "                             use_cache=True, return_dict=True)\n",
    "        bare_cache = _ensure_dynamic_cache(bare_out.past_key_values)\n",
    "        del bare_out\n",
    "\n",
    "        nll = score_answer_with_cache(\n",
    "            deepcopy_cache(bare_cache), context_len,\n",
    "            query_prompt, answer_text, model, tokenizer, exp_config)\n",
    "        bare_nlls.append(nll)\n",
    "        del bare_cache, bare_input, doc_ids\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    bare_arr = np.array(bare_nlls)\n",
    "    pct_floor = 100 * np.mean(bare_arr < 0.01)\n",
    "    median = np.median(bare_arr)\n",
    "    mean = np.mean(bare_arr)\n",
    "    status = \"WARNING: CEILING\" if pct_floor > 50 else \"OK\" if pct_floor < 30 else \"MARGINAL\"\n",
    "    print(f\"  {ds_name:15s}: median={median:.3f}, mean={mean:.3f}, \"\n",
    "          f\"pct_floor(<0.01)={pct_floor:.0f}% -> {status}\")\n",
    "\n",
    "print(\"\\nPre-screening complete. Proceeding with full experiment.\")\n",
    "\n",
    "# Condition explanation\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"EXPERIMENTAL CONDITIONS (Gemma 3 4B)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\n### 1. bare ###\")\n",
    "print(\"  Forward: [BOS][doc]\")\n",
    "print(\"  Baseline. Standard causal attention.\")\n",
    "\n",
    "print(\"\\n### 2. sf_trunc (standard priming) ###\")\n",
    "print(f\"  Forward: [BOS][prefix_{PREFIX_TOKEN_LEN}][doc]\")\n",
    "print(\"  Standard causal, truncate + RoPE. Keys carry negative interference on Gemma.\")\n",
    "\n",
    "print(\"\\n### 3. sf_trunc_bias2 (attention forcing, bias=+2.0) ###\")\n",
    "print(f\"  Forward: [BOS][prefix_{PREFIX_TOKEN_LEN}][doc] with +2.0 bias\")\n",
    "print(\"  Novel: amplifies doc->prefix attention during cache building.\")\n",
    "\n",
    "print(\"\\n### 4. values_only (all layers) ###\")\n",
    "print(\"  Bare keys + all primed values from sf_trunc cache.\")\n",
    "print(\"  Expected d ~ +0.056 (Exp 16 on MARCO). Bypasses key interference.\")\n",
    "\n",
    "print(\"\\n### 5. values_early (layers 0-15 only) ###\")\n",
    "print(\"  Bare keys + primed values from layers 0-15 only.\")\n",
    "print(\"  Expected best: d ~ +0.211 (Exp 19 on MARCO). Late layers carry interference.\")\n",
    "\n",
    "print(\"\\n### 6. values_hero (layers {10,12,14,15,20}) ###\")\n",
    "print(\"  Bare keys + primed values from 5 hero layers identified in Exp 24.\")\n",
    "print(\"  NQ generalization: d=+0.213 (Exp 27b).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ede970b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T22:14:57.623445Z",
     "iopub.status.busy": "2026-02-16T22:14:57.623123Z",
     "iopub.status.idle": "2026-02-16T22:14:57.642613Z",
     "shell.execute_reply": "2026-02-16T22:14:57.641864Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mask verification (toy: BOS + 3 prefix + 5 doc = 9 total):\n",
      "  Shape: torch.Size([1, 1, 9, 9])\n",
      "  Doc->Prefix bias (row 4, col 1): 2.0 (expect +2.0)\n",
      "  Causal mask (row 3, col 5): -inf (expect -inf)\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Helper functions\n",
    "\n",
    "def build_biased_causal_mask(total_len, prefix_start, prefix_end, bias_value, dtype, device):\n",
    "    \"\"\"Build a 4D causal attention mask with logit bias on doc->prefix attention.\"\"\"\n",
    "    mask = torch.zeros((total_len, total_len), dtype=dtype, device=device)\n",
    "    causal = torch.triu(\n",
    "        torch.ones(total_len, total_len, dtype=torch.bool, device=device),\n",
    "        diagonal=1\n",
    "    )\n",
    "    mask.masked_fill_(causal, float('-inf'))\n",
    "\n",
    "    if bias_value != 0.0:\n",
    "        doc_start = prefix_end\n",
    "        mask[doc_start:, prefix_start:prefix_end] += bias_value\n",
    "\n",
    "    return mask.unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "\n",
    "def run_single_sample(sample, model, tokenizer, exp_config, sf_ids, sf_str,\n",
    "                      PREFIX_TOKEN_LEN, N_LAYERS, EARLY_LAYER_CUTOFF, HERO_LAYERS):\n",
    "    \"\"\"Run all 6 conditions for a single sample. Returns dict of NLLs + metadata.\"\"\"\n",
    "    passage = sample['passage']\n",
    "    query = sample['query']\n",
    "    answer = sample['answer']\n",
    "    ds_name = sample['dataset']\n",
    "\n",
    "    query_prompt = QUERY_TEMPLATE.format(question=query)\n",
    "    answer_text = ANSWER_TEMPLATE.format(answer=answer)\n",
    "    document_text = DOCUMENT_TEMPLATE.format(document=passage)\n",
    "\n",
    "    # === Matched tokenization ===\n",
    "    full_text = sf_str + document_text\n",
    "    full_enc = tokenizer(full_text, return_tensors=\"pt\",\n",
    "                          add_special_tokens=True, padding=False, truncation=False)\n",
    "    full_ids = full_enc['input_ids'].to(exp_config.device)\n",
    "\n",
    "    sf_prefix_enc = tokenizer(sf_str, return_tensors=\"pt\",\n",
    "                               add_special_tokens=True, padding=False, truncation=False)\n",
    "    sf_prefix_len_with_bos = sf_prefix_enc['input_ids'].shape[1]\n",
    "\n",
    "    bos_id = full_ids[:, :1]\n",
    "    doc_ids = full_ids[:, sf_prefix_len_with_bos:]\n",
    "\n",
    "    # Truncate long docs\n",
    "    if doc_ids.shape[1] > MAX_DOC_TOKENS:\n",
    "        doc_ids = doc_ids[:, :MAX_DOC_TOKENS]\n",
    "\n",
    "    doc_len = doc_ids.shape[1]\n",
    "    context_len = 1 + doc_len  # BOS + doc\n",
    "\n",
    "    del full_enc, full_ids, sf_prefix_enc\n",
    "\n",
    "    # === 1. BARE ===\n",
    "    bare_input = torch.cat([bos_id, doc_ids], dim=1)\n",
    "    with torch.no_grad():\n",
    "        bare_out = model(input_ids=bare_input,\n",
    "                         attention_mask=torch.ones_like(bare_input),\n",
    "                         use_cache=True, return_dict=True)\n",
    "    bare_cache = _ensure_dynamic_cache(bare_out.past_key_values)\n",
    "    del bare_out\n",
    "\n",
    "    bare_nll = score_answer_with_cache(\n",
    "        deepcopy_cache(bare_cache), context_len,\n",
    "        query_prompt, answer_text, model, tokenizer, exp_config)\n",
    "\n",
    "    # === 2. sf_trunc (standard priming, bias=0) ===\n",
    "    primed_input = torch.cat([bos_id, sf_ids, doc_ids], dim=1)\n",
    "    total_seq_len = primed_input.shape[1]\n",
    "    prefix_start = 1\n",
    "    prefix_end = 1 + sf_ids.shape[1]\n",
    "    prefix_offset = sf_ids.shape[1]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        primed_out = model(input_ids=primed_input,\n",
    "                           attention_mask=torch.ones_like(primed_input),\n",
    "                           use_cache=True, return_dict=True)\n",
    "    primed_full_std = _ensure_dynamic_cache(primed_out.past_key_values)\n",
    "    del primed_out\n",
    "\n",
    "    trunc_raw = extract_and_truncate_cache_with_bos(primed_full_std, doc_len)\n",
    "    del primed_full_std\n",
    "\n",
    "    sf_trunc_cache = deepcopy_cache(trunc_raw)\n",
    "    correct_rope_positions_with_bos(sf_trunc_cache, prefix_offset, model)\n",
    "    del trunc_raw\n",
    "\n",
    "    sf_trunc_nll = score_answer_with_cache(\n",
    "        deepcopy_cache(sf_trunc_cache), context_len,\n",
    "        query_prompt, answer_text, model, tokenizer, exp_config)\n",
    "\n",
    "    # === 3. sf_trunc_bias2 (attention forcing, bias=+2.0) ===\n",
    "    mask_4d = build_biased_causal_mask(\n",
    "        total_seq_len, prefix_start, prefix_end,\n",
    "        2.0, model.dtype, exp_config.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        primed_out = model(input_ids=primed_input,\n",
    "                           attention_mask=mask_4d,\n",
    "                           use_cache=True, return_dict=True)\n",
    "    primed_full_b2 = _ensure_dynamic_cache(primed_out.past_key_values)\n",
    "    del primed_out, mask_4d\n",
    "\n",
    "    trunc_raw = extract_and_truncate_cache_with_bos(primed_full_b2, doc_len)\n",
    "    del primed_full_b2\n",
    "\n",
    "    bias2_cache = deepcopy_cache(trunc_raw)\n",
    "    correct_rope_positions_with_bos(bias2_cache, prefix_offset, model)\n",
    "    del trunc_raw\n",
    "\n",
    "    bias2_nll = score_answer_with_cache(\n",
    "        deepcopy_cache(bias2_cache), context_len,\n",
    "        query_prompt, answer_text, model, tokenizer, exp_config)\n",
    "    del bias2_cache\n",
    "\n",
    "    # === 4. values_only (all layers) ===\n",
    "    values_all_cache = deepcopy_cache(bare_cache)\n",
    "    for layer_idx in range(N_LAYERS):\n",
    "        primed_vals = _get_cache_values(sf_trunc_cache, layer_idx)\n",
    "        _set_cache_values(values_all_cache, layer_idx, primed_vals.clone())\n",
    "\n",
    "    values_only_nll = score_answer_with_cache(\n",
    "        deepcopy_cache(values_all_cache), context_len,\n",
    "        query_prompt, answer_text, model, tokenizer, exp_config)\n",
    "    del values_all_cache\n",
    "\n",
    "    # === 5. values_early (layers 0 to EARLY_LAYER_CUTOFF-1) ===\n",
    "    early_layers = list(range(EARLY_LAYER_CUTOFF))\n",
    "    values_early_cache = replace_values_at_layers(bare_cache, sf_trunc_cache, early_layers)\n",
    "\n",
    "    values_early_nll = score_answer_with_cache(\n",
    "        deepcopy_cache(values_early_cache), context_len,\n",
    "        query_prompt, answer_text, model, tokenizer, exp_config)\n",
    "    del values_early_cache\n",
    "\n",
    "    # === 6. values_hero (hero layers only) ===\n",
    "    values_hero_cache = replace_values_at_layers(bare_cache, sf_trunc_cache, HERO_LAYERS)\n",
    "\n",
    "    values_hero_nll = score_answer_with_cache(\n",
    "        deepcopy_cache(values_hero_cache), context_len,\n",
    "        query_prompt, answer_text, model, tokenizer, exp_config)\n",
    "    del values_hero_cache\n",
    "\n",
    "    del bare_cache, sf_trunc_cache, bare_input, primed_input\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return {\n",
    "        'dataset': ds_name,\n",
    "        'query': query,\n",
    "        'answer': answer,\n",
    "        'word_count': sample['word_count'],\n",
    "        'doc_token_len': doc_len,\n",
    "        'answer_token_len': sample.get('answer_token_len', 0),\n",
    "        'bare': bare_nll,\n",
    "        'sf_trunc': sf_trunc_nll,\n",
    "        'sf_trunc_bias2': bias2_nll,\n",
    "        'values_only': values_only_nll,\n",
    "        'values_early': values_early_nll,\n",
    "        'values_hero': values_hero_nll,\n",
    "    }\n",
    "\n",
    "\n",
    "# Verify mask for a toy example\n",
    "print(\"Mask verification (toy: BOS + 3 prefix + 5 doc = 9 total):\")\n",
    "toy_mask = build_biased_causal_mask(9, 1, 4, 2.0, model.dtype, 'cpu')\n",
    "m = toy_mask.squeeze()\n",
    "print(f\"  Shape: {toy_mask.shape}\")\n",
    "print(f\"  Doc->Prefix bias (row 4, col 1): {m[4, 1].item():.1f} (expect +2.0)\")\n",
    "print(f\"  Causal mask (row 3, col 5): {m[3, 5].item()} (expect -inf)\")\n",
    "del toy_mask, m\n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3fa6784",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T22:14:57.645981Z",
     "iopub.status.busy": "2026-02-16T22:14:57.645441Z",
     "iopub.status.idle": "2026-02-16T22:52:20.497147Z",
     "shell.execute_reply": "2026-02-16T22:52:20.496230Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "EXPERIMENT 29: 900 samples, 6 conditions\n",
      "Model: Gemma 3 4B, MAX_DOC_TOKENS: 900\n",
      "Datasets: DROP, AdversarialQA, CoQA\n",
      "======================================================================\n",
      "No checkpoint found. Starting fresh.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1597315c96ee45a29d495b0932e1bec1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Exp 29:   0%|          | 0/900 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 25/900 | 25 done in 1.1m | ETA: 37.9 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 50/900 | 50 done in 2.1m | ETA: 36.4 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 75/900 | 75 done in 3.2m | ETA: 35.0 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 100/900 | 100 done in 4.2m | ETA: 33.8 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 125/900 | 125 done in 5.3m | ETA: 32.7 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 150/900 | 150 done in 6.3m | ETA: 31.6 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 175/900 | 175 done in 7.4m | ETA: 30.5 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 200/900 | 200 done in 8.4m | ETA: 29.5 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 225/900 | 225 done in 9.5m | ETA: 28.4 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 250/900 | 250 done in 10.5m | ETA: 27.3 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 275/900 | 275 done in 11.5m | ETA: 26.2 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 300/900 | 300 done in 12.6m | ETA: 25.2 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 325/900 | 325 done in 13.6m | ETA: 24.1 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 350/900 | 350 done in 14.6m | ETA: 23.0 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 375/900 | 375 done in 15.7m | ETA: 21.9 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 400/900 | 400 done in 16.7m | ETA: 20.9 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 425/900 | 425 done in 17.7m | ETA: 19.8 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 450/900 | 450 done in 18.7m | ETA: 18.7 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 475/900 | 475 done in 19.8m | ETA: 17.7 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 500/900 | 500 done in 20.8m | ETA: 16.6 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 525/900 | 525 done in 21.8m | ETA: 15.6 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 550/900 | 550 done in 22.8m | ETA: 14.5 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 575/900 | 575 done in 23.9m | ETA: 13.5 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 600/900 | 600 done in 24.9m | ETA: 12.5 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 625/900 | 625 done in 26.0m | ETA: 11.4 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 650/900 | 650 done in 27.0m | ETA: 10.4 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 675/900 | 675 done in 28.0m | ETA: 9.3 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 700/900 | 700 done in 29.1m | ETA: 8.3 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 725/900 | 725 done in 30.1m | ETA: 7.3 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 750/900 | 750 done in 31.2m | ETA: 6.2 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 775/900 | 775 done in 32.2m | ETA: 5.2 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 800/900 | 800 done in 33.3m | ETA: 4.2 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 825/900 | 825 done in 34.3m | ETA: 3.1 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 850/900 | 850 done in 35.3m | ETA: 2.1 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 875/900 | 875 done in 36.4m | ETA: 1.0 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 900/900 | 900 done in 37.4m | ETA: 0.0 min\n",
      "\n",
      "Experiment complete: 900 samples in 37.4 min\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Main experiment loop\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"EXPERIMENT 29: {len(all_samples)} samples, {len(CONDITION_NAMES)} conditions\")\n",
    "print(f\"Model: Gemma 3 4B, MAX_DOC_TOKENS: {MAX_DOC_TOKENS}\")\n",
    "print(f\"Datasets: DROP, AdversarialQA, CoQA\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Checkpoint resume\n",
    "all_results = []\n",
    "start_idx = 0\n",
    "\n",
    "if CHECKPOINT_PATH.exists():\n",
    "    with open(CHECKPOINT_PATH, 'r') as f:\n",
    "        ckpt = json.load(f)\n",
    "    ckpt_queries = ckpt.get('sample_queries', [])\n",
    "    current_queries = [s['query'] for s in all_samples]\n",
    "    if ckpt_queries == current_queries:\n",
    "        all_results = ckpt['results']\n",
    "        start_idx = len(all_results)\n",
    "        print(f\"Resuming from checkpoint: {start_idx}/{len(all_samples)}\")\n",
    "    else:\n",
    "        print(\"Checkpoint query mismatch. Starting fresh.\")\n",
    "else:\n",
    "    print(\"No checkpoint found. Starting fresh.\")\n",
    "\n",
    "t_start = time.time()\n",
    "N_TOTAL = len(all_samples)\n",
    "\n",
    "for qidx in tqdm(range(start_idx, N_TOTAL), initial=start_idx, total=N_TOTAL,\n",
    "                  desc=\"Exp 29\"):\n",
    "    sample = all_samples[qidx]\n",
    "\n",
    "    result = run_single_sample(\n",
    "        sample, model, tokenizer, exp_config,\n",
    "        sf_ids, sf_str, PREFIX_TOKEN_LEN, N_LAYERS,\n",
    "        EARLY_LAYER_CUTOFF, HERO_LAYERS)\n",
    "    result['query_idx'] = qidx\n",
    "    all_results.append(result)\n",
    "\n",
    "    # Checkpoint\n",
    "    if (qidx + 1) % CHECKPOINT_EVERY == 0 or qidx == N_TOTAL - 1:\n",
    "        ckpt_data = {\n",
    "            'results': all_results,\n",
    "            'sample_queries': [s['query'] for s in all_samples],\n",
    "            'completed': len(all_results),\n",
    "            'total': N_TOTAL,\n",
    "            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        }\n",
    "        with open(CHECKPOINT_PATH, 'w') as f:\n",
    "            json.dump(ckpt_data, f)\n",
    "        elapsed = time.time() - t_start\n",
    "        n_done = qidx - start_idx + 1\n",
    "        rate = n_done / elapsed if elapsed > 0 else 0\n",
    "        remaining = (N_TOTAL - qidx - 1) / rate if rate > 0 else 0\n",
    "        tqdm.write(f\"  Checkpoint {qidx+1}/{N_TOTAL} | {n_done} done in {elapsed/60:.1f}m | \"\n",
    "                   f\"ETA: {remaining/60:.1f} min\")\n",
    "\n",
    "elapsed_total = time.time() - t_start\n",
    "print(f\"\\nExperiment complete: {len(all_results)} samples in {elapsed_total/60:.1f} min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "916750c2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T22:52:20.501575Z",
     "iopub.status.busy": "2026-02-16T22:52:20.501295Z",
     "iopub.status.idle": "2026-02-16T22:52:20.594188Z",
     "shell.execute_reply": "2026-02-16T22:52:20.593288Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ANALYSIS: PER-DATASET RESULTS (Gemma 3 4B)\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "DATASET: DROP (n=300/300, median bare NLL=0.028, pct_floor=44%)\n",
      "======================================================================\n",
      "\n",
      "Condition             Mean Bare  Mean Cond     Mean D        d    Win%            p   sig\n",
      "------------------------------------------------------------------------------------------\n",
      "sf_trunc                 1.3794     1.4918    -0.1124   -0.084   32.7%     1.49e-01    ns\n",
      "sf_trunc_bias2           1.3794     1.6124    -0.2330   -0.153   32.0%     8.60e-03    **\n",
      "values_only              1.3794     1.4750    -0.0956   -0.076   33.7%     1.90e-01    ns\n",
      "values_early             1.3794     1.4739    -0.0945   -0.090   34.3%     1.19e-01    ns\n",
      "values_hero              1.3794     1.4837    -0.1043   -0.152   26.3%     9.12e-03    **\n",
      "\n",
      "======================================================================\n",
      "DATASET: ADVERSARIALQA (n=300/300, median bare NLL=0.001, pct_floor=72%)\n",
      "======================================================================\n",
      "\n",
      "Condition             Mean Bare  Mean Cond     Mean D        d    Win%            p   sig\n",
      "------------------------------------------------------------------------------------------\n",
      "sf_trunc                 0.1895     0.2061    -0.0165   -0.078   32.7%     1.77e-01    ns\n",
      "sf_trunc_bias2           0.1895     0.2471    -0.0576   -0.131   18.3%     2.40e-02     *\n",
      "values_only              0.1895     0.2291    -0.0396   -0.142   24.3%     1.43e-02     *\n",
      "values_early             0.1895     0.2129    -0.0234   -0.094   20.3%     1.03e-01    ns\n",
      "values_hero              0.1895     0.1860    +0.0036   +0.026   18.3%     6.51e-01    ns\n",
      "\n",
      "======================================================================\n",
      "DATASET: COQA (n=300/300, median bare NLL=0.001, pct_floor=65%)\n",
      "======================================================================\n",
      "\n",
      "Condition             Mean Bare  Mean Cond     Mean D        d    Win%            p   sig\n",
      "------------------------------------------------------------------------------------------\n",
      "sf_trunc                 0.3469     0.3598    -0.0129   -0.028   36.7%     6.31e-01    ns\n",
      "sf_trunc_bias2           0.3469     0.4405    -0.0936   -0.111   26.3%     5.66e-02    ns\n",
      "values_only              0.3469     0.3791    -0.0322   -0.134   31.0%     2.13e-02     *\n",
      "values_early             0.3469     0.3581    -0.0112   -0.051   24.3%     3.82e-01    ns\n",
      "values_hero              0.3469     0.3379    +0.0090   +0.070   16.0%     2.29e-01    ns\n",
      "\n",
      "\n",
      "==========================================================================================\n",
      "CROSS-DATASET SUMMARY: Cohen's d vs bare (Gemma 3 4B)\n",
      "==========================================================================================\n",
      "\n",
      "Condition                       drop   adversarialqa            coqa\n",
      "--------------------------------------------------------------------\n",
      "sf_trunc                  -0.084          -0.078          -0.028    \n",
      "sf_trunc_bias2            -0.153  **      -0.131   *      -0.111    \n",
      "values_only               -0.076          -0.142   *      -0.134   *\n",
      "values_early              -0.090          -0.094          -0.051    \n",
      "values_hero               -0.152  **      +0.026          +0.070    \n",
      "\n",
      "\n",
      "BARE NLL DISTRIBUTIONS (ceiling effect check):\n",
      "  drop           : mean=1.379, median=0.028, IQR=2.458, pct_floor=44%\n",
      "  adversarialqa  : mean=0.190, median=0.001, IQR=0.021, pct_floor=72%\n",
      "  coqa           : mean=0.347, median=0.001, IQR=0.044, pct_floor=65%\n",
      "\n",
      "\n",
      "==========================================================================================\n",
      "COMPARISON WITH EXP 27b (Gemma: TriviaQA, NQ, HotpotQA)\n",
      "==========================================================================================\n",
      "\n",
      "Exp 27b results (for reference):\n",
      "  TriviaQA: values_hero d=+0.000 (77% at floor)\n",
      "  NQ:       values_hero d=+0.213*** (55% at floor)\n",
      "  HotpotQA: values_hero d=-0.069 (56% at floor)\n"
     ]
    }
   ],
   "source": [
    "# Cell 10: Per-dataset analysis\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ANALYSIS: PER-DATASET RESULTS (Gemma 3 4B)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "dataset_names = ['drop', 'adversarialqa', 'coqa']\n",
    "analysis = {}\n",
    "\n",
    "for ds_name in dataset_names:\n",
    "    ds_results = [r for r in all_results if r['dataset'] == ds_name]\n",
    "    n_ds = len(ds_results)\n",
    "    if n_ds == 0:\n",
    "        continue\n",
    "\n",
    "    bare_arr = np.array([r['bare'] for r in ds_results])\n",
    "\n",
    "    # Filter invalid (but keep zeros — they ARE valid for some datasets)\n",
    "    valid = np.isfinite(bare_arr)\n",
    "    for cname in CONDITION_NAMES:\n",
    "        if cname == 'bare':\n",
    "            continue\n",
    "        c_arr = np.array([r[cname] for r in ds_results])\n",
    "        valid &= np.isfinite(c_arr)\n",
    "\n",
    "    n_valid = int(np.sum(valid))\n",
    "\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    pct_floor = 100 * np.mean(bare_arr < 0.01)\n",
    "    print(f\"DATASET: {ds_name.upper()} (n={n_valid}/{n_ds}, \"\n",
    "          f\"median bare NLL={np.median(bare_arr):.3f}, \"\n",
    "          f\"pct_floor={pct_floor:.0f}%)\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "    print(f\"\\n{'Condition':<20} {'Mean Bare':>10} {'Mean Cond':>10} \"\n",
    "          f\"{'Mean D':>10} {'d':>8} {'Win%':>7} {'p':>12} {'sig':>5}\")\n",
    "    print(\"-\" * 90)\n",
    "\n",
    "    ds_analysis = {}\n",
    "    for cname in CONDITION_NAMES:\n",
    "        if cname == 'bare':\n",
    "            continue\n",
    "        c_arr = np.array([r[cname] for r in ds_results])\n",
    "        delta = bare_arr[valid] - c_arr[valid]\n",
    "        d = cohens_d(delta)\n",
    "        win = np.mean(delta > 0) * 100\n",
    "        t_stat, p_val = stats.ttest_1samp(delta, 0)\n",
    "        sig = '***' if p_val < 0.001 else '**' if p_val < 0.01 else '*' if p_val < 0.05 else 'ns'\n",
    "        print(f\"{cname:<20} {np.mean(bare_arr[valid]):>10.4f} {np.mean(c_arr[valid]):>10.4f} \"\n",
    "              f\"{np.mean(delta):>+10.4f} {d:>+8.3f} {win:>6.1f}% {p_val:>12.2e} {sig:>5}\")\n",
    "        ds_analysis[cname] = {\n",
    "            'n_valid': n_valid,\n",
    "            'mean_bare': float(np.mean(bare_arr[valid])),\n",
    "            'mean_cond': float(np.mean(c_arr[valid])),\n",
    "            'mean_delta': float(np.mean(delta)),\n",
    "            'cohens_d': float(d),\n",
    "            'win_pct': float(win),\n",
    "            't_stat': float(t_stat),\n",
    "            'p_value': float(p_val),\n",
    "        }\n",
    "\n",
    "    analysis[ds_name] = ds_analysis\n",
    "\n",
    "# Cross-dataset summary table\n",
    "print(f\"\\n\\n{'='*90}\")\n",
    "print(\"CROSS-DATASET SUMMARY: Cohen's d vs bare (Gemma 3 4B)\")\n",
    "print(f\"{'='*90}\")\n",
    "print(f\"\\n{'Condition':<20}\", end='')\n",
    "for ds in dataset_names:\n",
    "    print(f\"{'  ' + ds:>16}\", end='')\n",
    "print()\n",
    "print(\"-\" * 68)\n",
    "for cname in CONDITION_NAMES:\n",
    "    if cname == 'bare':\n",
    "        continue\n",
    "    print(f\"{cname:<20}\", end='')\n",
    "    for ds in dataset_names:\n",
    "        if ds in analysis and cname in analysis[ds]:\n",
    "            d = analysis[ds][cname]['cohens_d']\n",
    "            p = analysis[ds][cname]['p_value']\n",
    "            sig = '***' if p < 0.001 else '**' if p < 0.01 else '*' if p < 0.05 else ''\n",
    "            print(f\"{d:>+12.3f}{sig:>4}\", end='')\n",
    "        else:\n",
    "            print(f\"{'n/a':>16}\", end='')\n",
    "    print()\n",
    "\n",
    "# Bare NLL distributions\n",
    "print(f\"\\n\\nBARE NLL DISTRIBUTIONS (ceiling effect check):\")\n",
    "for ds in dataset_names:\n",
    "    ds_r = [r for r in all_results if r['dataset'] == ds]\n",
    "    bare = [r['bare'] for r in ds_r]\n",
    "    pct_zero = 100 * np.mean(np.array(bare) < 0.01)\n",
    "    iqr = np.percentile(bare, 75) - np.percentile(bare, 25)\n",
    "    print(f\"  {ds:15s}: mean={np.mean(bare):.3f}, median={np.median(bare):.3f}, \"\n",
    "          f\"IQR={iqr:.3f}, pct_floor={pct_zero:.0f}%\")\n",
    "\n",
    "# Compare with Exp 27b (Gemma on previous datasets)\n",
    "print(f\"\\n\\n{'='*90}\")\n",
    "print(\"COMPARISON WITH EXP 27b (Gemma: TriviaQA, NQ, HotpotQA)\")\n",
    "print(f\"{'='*90}\")\n",
    "print(\"\\nExp 27b results (for reference):\")\n",
    "print(\"  TriviaQA: values_hero d=+0.000 (77% at floor)\")\n",
    "print(\"  NQ:       values_hero d=+0.213*** (55% at floor)\")\n",
    "print(\"  HotpotQA: values_hero d=-0.069 (56% at floor)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "45b26ad1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T22:52:20.598015Z",
     "iopub.status.busy": "2026-02-16T22:52:20.597483Z",
     "iopub.status.idle": "2026-02-16T22:52:20.652027Z",
     "shell.execute_reply": "2026-02-16T22:52:20.651283Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "LENGTH STRATIFICATION (Gemma 3 4B)\n",
      "======================================================================\n",
      "\n",
      "--- DROP ---\n",
      "  sf_trunc:\n",
      "    <128: n=0 (too few)\n",
      "    128-256: n=145, d=-0.188, p=2.50e-02 *\n",
      "    256-512: n=150, d=-0.023, p=7.78e-01 ns\n",
      "    512-900: n=5 (too few)\n",
      "  sf_trunc_bias2:\n",
      "    <128: n=0 (too few)\n",
      "    128-256: n=145, d=-0.153, p=6.79e-02 ns\n",
      "    256-512: n=150, d=-0.156, p=5.77e-02 ns\n",
      "    512-900: n=5 (too few)\n",
      "  values_only:\n",
      "    <128: n=0 (too few)\n",
      "    128-256: n=145, d=-0.222, p=8.51e-03 **\n",
      "    256-512: n=150, d=+0.007, p=9.30e-01 ns\n",
      "    512-900: n=5 (too few)\n",
      "  values_early:\n",
      "    <128: n=0 (too few)\n",
      "    128-256: n=145, d=-0.135, p=1.06e-01 ns\n",
      "    256-512: n=150, d=-0.069, p=4.01e-01 ns\n",
      "    512-900: n=5 (too few)\n",
      "  values_hero:\n",
      "    <128: n=0 (too few)\n",
      "    128-256: n=145, d=-0.174, p=3.78e-02 *\n",
      "    256-512: n=150, d=-0.138, p=9.29e-02 ns\n",
      "    512-900: n=5 (too few)\n",
      "\n",
      "--- ADVERSARIALQA ---\n",
      "  sf_trunc:\n",
      "    <128: n=89, d=-0.098, p=3.56e-01 ns\n",
      "    128-256: n=198, d=-0.053, p=4.55e-01 ns\n",
      "    256-512: n=13, d=-0.222, p=4.40e-01 ns\n",
      "    512-900: n=0 (too few)\n",
      "  sf_trunc_bias2:\n",
      "    <128: n=89, d=-0.209, p=5.17e-02 ns\n",
      "    128-256: n=198, d=-0.093, p=1.92e-01 ns\n",
      "    256-512: n=13, d=+0.131, p=6.44e-01 ns\n",
      "    512-900: n=0 (too few)\n",
      "  values_only:\n",
      "    <128: n=89, d=-0.144, p=1.79e-01 ns\n",
      "    128-256: n=198, d=-0.186, p=9.72e-03 **\n",
      "    256-512: n=13, d=-0.171, p=5.50e-01 ns\n",
      "    512-900: n=0 (too few)\n",
      "  values_early:\n",
      "    <128: n=89, d=-0.130, p=2.23e-01 ns\n",
      "    128-256: n=198, d=-0.075, p=2.94e-01 ns\n",
      "    256-512: n=13, d=-0.407, p=1.68e-01 ns\n",
      "    512-900: n=0 (too few)\n",
      "  values_hero:\n",
      "    <128: n=89, d=+0.094, p=3.78e-01 ns\n",
      "    128-256: n=198, d=-0.036, p=6.09e-01 ns\n",
      "    256-512: n=13, d=+0.425, p=1.51e-01 ns\n",
      "    512-900: n=0 (too few)\n",
      "\n",
      "--- COQA ---\n",
      "  sf_trunc:\n",
      "    <128: n=0 (too few)\n",
      "    128-256: n=56, d=-0.263, p=5.38e-02 ns\n",
      "    256-512: n=237, d=+0.017, p=7.99e-01 ns\n",
      "    512-900: n=7 (too few)\n",
      "  sf_trunc_bias2:\n",
      "    <128: n=0 (too few)\n",
      "    128-256: n=56, d=-0.249, p=6.75e-02 ns\n",
      "    256-512: n=237, d=-0.106, p=1.04e-01 ns\n",
      "    512-900: n=7 (too few)\n",
      "  values_only:\n",
      "    <128: n=0 (too few)\n",
      "    128-256: n=56, d=-0.303, p=2.72e-02 *\n",
      "    256-512: n=237, d=-0.114, p=8.05e-02 ns\n",
      "    512-900: n=7 (too few)\n",
      "  values_early:\n",
      "    <128: n=0 (too few)\n",
      "    128-256: n=56, d=-0.235, p=8.45e-02 ns\n",
      "    256-512: n=237, d=-0.025, p=6.95e-01 ns\n",
      "    512-900: n=7 (too few)\n",
      "  values_hero:\n",
      "    <128: n=0 (too few)\n",
      "    128-256: n=56, d=-0.082, p=5.44e-01 ns\n",
      "    256-512: n=237, d=+0.085, p=1.92e-01 ns\n",
      "    512-900: n=7 (too few)\n",
      "\n",
      "\n",
      "======================================================================\n",
      "HARDNESS QUINTILE INTERACTION (Gemma 3 4B)\n",
      "======================================================================\n",
      "\n",
      "--- DROP (boundaries: ['0.000', '0.005', '0.184', '3.078']) ---\n",
      "                          Q1(easy)          Q2          Q3          Q4    Q5(hard)     Overall\n",
      "  sf_trunc_bias2            -0.327      -0.195      -0.593      -0.325      +0.312      -0.153\n",
      "                          Q1(easy)          Q2          Q3          Q4    Q5(hard)     Overall\n",
      "  values_only               -0.181      -0.285      -0.388      -0.492      +0.349      -0.076\n",
      "                          Q1(easy)          Q2          Q3          Q4    Q5(hard)     Overall\n",
      "  values_early              -0.189      -0.383      -0.265      -0.397      +0.249      -0.090\n",
      "                          Q1(easy)          Q2          Q3          Q4    Q5(hard)     Overall\n",
      "  values_hero               -0.302      -0.334      -0.145      -0.446      -0.020      -0.152\n",
      "\n",
      "--- ADVERSARIALQA (boundaries: ['0.000', '0.000', '0.002', '0.051']) ---\n",
      "                          Q1(easy)          Q2          Q3          Q4    Q5(hard)     Overall\n",
      "  sf_trunc_bias2            -0.093      -0.468      -0.461      -0.342      -0.262      -0.131\n",
      "                          Q1(easy)          Q2          Q3          Q4    Q5(hard)     Overall\n",
      "  values_only               -0.122      -0.149      -0.309      -0.301      -0.281      -0.142\n",
      "                          Q1(easy)          Q2          Q3          Q4    Q5(hard)     Overall\n",
      "  values_early              -0.093      +0.043      -0.542      -0.284      -0.181      -0.094\n",
      "                          Q1(easy)          Q2          Q3          Q4    Q5(hard)     Overall\n",
      "  values_hero               -0.093      -0.753      -0.706      -0.232      +0.096      +0.026\n",
      "\n",
      "--- COQA (boundaries: ['0.000', '0.000', '0.005', '0.085']) ---\n",
      "                          Q1(easy)          Q2          Q3          Q4    Q5(hard)     Overall\n",
      "  sf_trunc_bias2            -0.107      -0.343      -0.182      -0.193      -0.084      -0.111\n",
      "                          Q1(easy)          Q2          Q3          Q4    Q5(hard)     Overall\n",
      "  values_only               -0.107      -0.186      -0.315      -0.287      -0.220      -0.134\n",
      "                          Q1(easy)          Q2          Q3          Q4    Q5(hard)     Overall\n",
      "  values_early              -0.107      -0.614      -0.507      -0.314      -0.066      -0.051\n",
      "                          Q1(easy)          Q2          Q3          Q4    Q5(hard)     Overall\n",
      "  values_hero               -0.107      -0.582      -0.714      -0.343      +0.174      +0.070\n"
     ]
    }
   ],
   "source": [
    "# Cell 11: Length stratification + hardness interaction\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"LENGTH STRATIFICATION (Gemma 3 4B)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "length_strat = {}\n",
    "\n",
    "for ds_name in dataset_names:\n",
    "    ds_results = [r for r in all_results if r['dataset'] == ds_name]\n",
    "    if not ds_results:\n",
    "        continue\n",
    "\n",
    "    print(f\"\\n--- {ds_name.upper()} ---\")\n",
    "    ds_length_data = {}\n",
    "\n",
    "    for cname in ['sf_trunc', 'sf_trunc_bias2', 'values_only', 'values_early', 'values_hero']:\n",
    "        print(f\"  {cname}:\")\n",
    "        bin_ds = []\n",
    "        for bin_label, bin_min, bin_max in LENGTH_BINS:\n",
    "            bin_results = [r for r in ds_results\n",
    "                          if bin_min <= r['doc_token_len'] < bin_max]\n",
    "            n_bin = len(bin_results)\n",
    "            if n_bin < 10:\n",
    "                print(f\"    {bin_label}: n={n_bin} (too few)\")\n",
    "                bin_ds.append({'label': bin_label, 'n': n_bin, 'd': None})\n",
    "                continue\n",
    "            bare = np.array([r['bare'] for r in bin_results])\n",
    "            cond = np.array([r[cname] for r in bin_results])\n",
    "            delta = bare - cond\n",
    "            d = cohens_d(delta)\n",
    "            _, p_val = stats.ttest_1samp(delta, 0)\n",
    "            sig = '***' if p_val < 0.001 else '**' if p_val < 0.01 else '*' if p_val < 0.05 else 'ns'\n",
    "            print(f\"    {bin_label}: n={n_bin}, d={d:+.3f}, p={p_val:.2e} {sig}\")\n",
    "            bin_ds.append({'label': bin_label, 'n': n_bin, 'd': float(d), 'p': float(p_val)})\n",
    "        ds_length_data[cname] = bin_ds\n",
    "\n",
    "    length_strat[ds_name] = ds_length_data\n",
    "\n",
    "# === HARDNESS QUINTILE INTERACTION ===\n",
    "print(f\"\\n\\n{'='*70}\")\n",
    "print(\"HARDNESS QUINTILE INTERACTION (Gemma 3 4B)\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "hardness_data = {}\n",
    "quintile_labels = ['Q1(easy)', 'Q2', 'Q3', 'Q4', 'Q5(hard)']\n",
    "\n",
    "for ds_name in dataset_names:\n",
    "    ds_results = [r for r in all_results if r['dataset'] == ds_name]\n",
    "    if len(ds_results) < 50:\n",
    "        continue\n",
    "\n",
    "    bare_arr = np.array([r['bare'] for r in ds_results])\n",
    "    quintile_boundaries = np.percentile(bare_arr, [20, 40, 60, 80])\n",
    "    print(f\"\\n--- {ds_name.upper()} (boundaries: {[f'{b:.3f}' for b in quintile_boundaries]}) ---\")\n",
    "\n",
    "    def get_quintile(nll):\n",
    "        for i, b in enumerate(quintile_boundaries):\n",
    "            if nll <= b:\n",
    "                return i\n",
    "        return 4\n",
    "\n",
    "    quintiles = np.array([get_quintile(r['bare']) for r in ds_results])\n",
    "\n",
    "    ds_hardness = {}\n",
    "    for cname in ['sf_trunc_bias2', 'values_only', 'values_early', 'values_hero']:\n",
    "        cond_arr = np.array([r[cname] for r in ds_results])\n",
    "        delta = bare_arr - cond_arr\n",
    "        q_header = \"\".join(f\"{ql:>12}\" for ql in quintile_labels) + f\"{'Overall':>12}\"\n",
    "        row = f\"  {cname:<20}\"\n",
    "        q_ds = []\n",
    "        for q in range(5):\n",
    "            mask_q = quintiles == q\n",
    "            n_q = int(np.sum(mask_q))\n",
    "            if n_q < 5:\n",
    "                row += f\"{'n/a':>12}\"\n",
    "                q_ds.append(None)\n",
    "            else:\n",
    "                d_q = cohens_d(delta[mask_q])\n",
    "                row += f\"{d_q:>+12.3f}\"\n",
    "                q_ds.append(float(d_q))\n",
    "        d_all = cohens_d(delta)\n",
    "        row += f\"{d_all:>+12.3f}\"\n",
    "        print(f\"  {'':20}\" + q_header)\n",
    "        print(row)\n",
    "        ds_hardness[cname] = q_ds\n",
    "\n",
    "    hardness_data[ds_name] = ds_hardness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f9311ed3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T22:52:20.655453Z",
     "iopub.status.busy": "2026-02-16T22:52:20.654959Z",
     "iopub.status.idle": "2026-02-16T22:52:22.722729Z",
     "shell.execute_reply": "2026-02-16T22:52:22.721822Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plots saved to results/exp29/analysis_plots.png\n"
     ]
    }
   ],
   "source": [
    "# Cell 12: Multi-panel figure\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 14))\n",
    "\n",
    "colors = {\n",
    "    'sf_trunc': '#1f77b4',\n",
    "    'sf_trunc_bias2': '#d62728',\n",
    "    'values_only': '#7f7f7f',\n",
    "    'values_early': '#2ca02c',\n",
    "    'values_hero': '#ff7f0e',\n",
    "}\n",
    "\n",
    "# ---- Panel (a): Cohen's d by dataset x condition ----\n",
    "ax = axes[0, 0]\n",
    "x = np.arange(len(dataset_names))\n",
    "width = 0.15\n",
    "conds_plot = ['sf_trunc', 'sf_trunc_bias2', 'values_only', 'values_early', 'values_hero']\n",
    "for i, cname in enumerate(conds_plot):\n",
    "    ds_vals = []\n",
    "    for ds in dataset_names:\n",
    "        if ds in analysis and cname in analysis[ds]:\n",
    "            ds_vals.append(analysis[ds][cname]['cohens_d'])\n",
    "        else:\n",
    "            ds_vals.append(0)\n",
    "    offset = (i - 2) * width\n",
    "    bars = ax.bar(x + offset, ds_vals, width, label=cname, color=colors[cname],\n",
    "                  edgecolor='black', linewidth=0.5)\n",
    "    for j, val in enumerate(ds_vals):\n",
    "        ax.text(x[j] + offset, val + (0.01 if val >= 0 else -0.03),\n",
    "                f\"{val:+.2f}\", ha='center', va='bottom' if val >= 0 else 'top', fontsize=6)\n",
    "\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([ds.upper() for ds in dataset_names])\n",
    "ax.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "ax.set_ylabel(\"Cohen's d (positive = helps)\")\n",
    "ax.set_title(\"(a) Gemma 3 4B: Effect Size by Dataset x Condition\")\n",
    "ax.legend(fontsize=6, loc='best')\n",
    "\n",
    "# ---- Panel (b): Length stratification for values_hero across datasets ----\n",
    "ax = axes[0, 1]\n",
    "for ds_name in dataset_names:\n",
    "    if ds_name not in length_strat:\n",
    "        continue\n",
    "    cname = 'values_hero'\n",
    "    if cname not in length_strat[ds_name]:\n",
    "        continue\n",
    "    bins_data = length_strat[ds_name][cname]\n",
    "    valid_idx = [i for i, b in enumerate(bins_data) if b['d'] is not None]\n",
    "    if valid_idx:\n",
    "        x_vals = valid_idx\n",
    "        y_vals = [bins_data[i]['d'] for i in valid_idx]\n",
    "        ns = [bins_data[i]['n'] for i in valid_idx]\n",
    "        ax.plot(x_vals, y_vals, marker='o', linewidth=2, markersize=6, label=ds_name)\n",
    "        for xv, yv, n in zip(x_vals, y_vals, ns):\n",
    "            ax.annotate(f\"n={n}\", (xv, yv), fontsize=6, textcoords=\"offset points\",\n",
    "                       xytext=(0, 8), ha='center')\n",
    "\n",
    "bin_labels_all = [b[0] for b in LENGTH_BINS]\n",
    "ax.set_xticks(range(len(bin_labels_all)))\n",
    "ax.set_xticklabels(bin_labels_all, rotation=30, ha='right', fontsize=8)\n",
    "ax.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "ax.set_ylabel(\"Cohen's d\")\n",
    "ax.set_xlabel(\"Document Token Length Bin\")\n",
    "ax.set_title(\"(b) values_hero by Length Bin\")\n",
    "ax.legend(fontsize=8)\n",
    "\n",
    "# ---- Panel (c): Hardness heatmap for values_hero ----\n",
    "ax = axes[1, 0]\n",
    "hm_rows = []\n",
    "hm_ylabels = []\n",
    "for ds_name in dataset_names:\n",
    "    if ds_name in hardness_data and 'values_hero' in hardness_data[ds_name]:\n",
    "        row = hardness_data[ds_name]['values_hero']\n",
    "        hm_rows.append([v if v is not None else 0 for v in row])\n",
    "        hm_ylabels.append(ds_name.upper())\n",
    "\n",
    "if hm_rows:\n",
    "    hm_arr = np.array(hm_rows)\n",
    "    im = ax.imshow(hm_arr, cmap='RdBu', aspect='auto', vmin=-0.5, vmax=0.5)\n",
    "    ax.set_xticks(range(5))\n",
    "    ax.set_xticklabels(quintile_labels, fontsize=8)\n",
    "    ax.set_yticks(range(len(hm_ylabels)))\n",
    "    ax.set_yticklabels(hm_ylabels)\n",
    "    for i in range(len(hm_ylabels)):\n",
    "        for j in range(5):\n",
    "            val = hm_arr[i, j]\n",
    "            ax.text(j, i, f\"{val:+.2f}\", ha='center', va='center',\n",
    "                    fontsize=9, color='white' if abs(val) > 0.25 else 'black')\n",
    "    fig.colorbar(im, ax=ax, shrink=0.8, label=\"Cohen's d\")\n",
    "ax.set_title(\"(c) values_hero: Hardness x Dataset\")\n",
    "\n",
    "# ---- Panel (d): Bare NLL distributions (box/violin) ----\n",
    "ax = axes[1, 1]\n",
    "bare_by_ds = []\n",
    "ds_labels_plot = []\n",
    "for ds in dataset_names:\n",
    "    ds_r = [r for r in all_results if r['dataset'] == ds]\n",
    "    bare_by_ds.append([r['bare'] for r in ds_r])\n",
    "    ds_labels_plot.append(ds.upper())\n",
    "\n",
    "# Also add Exp 27b reference datasets for comparison\n",
    "# (hardcoded from prior results)\n",
    "ref_medians = {'TRIVIAQA\\n(27b)': 0.000, 'NQ\\n(27b)': 0.006, 'HOTPOTQA\\n(27b)': 0.003}\n",
    "\n",
    "bp = ax.boxplot(bare_by_ds, labels=ds_labels_plot, showfliers=False, patch_artist=True,\n",
    "                medianprops={'color': 'red', 'linewidth': 2})\n",
    "for patch in bp['boxes']:\n",
    "    patch.set_facecolor('#8ecae6')\n",
    "    patch.set_alpha(0.7)\n",
    "\n",
    "# Add reference lines for 27b medians\n",
    "for i, (label, med) in enumerate(ref_medians.items()):\n",
    "    ax.axhline(y=med, color='gray', linestyle=':', alpha=0.4)\n",
    "ax.axhline(y=0.01, color='red', linestyle='--', alpha=0.3, label='Floor threshold (0.01)')\n",
    "\n",
    "ax.set_ylabel(\"Bare NLL\")\n",
    "ax.set_title(\"(d) Bare NLL Distributions (ceiling check)\")\n",
    "ax.legend(fontsize=7)\n",
    "\n",
    "plt.suptitle('Exp 29: Hard QA Datasets (Gemma 3 4B)', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'analysis_plots.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"Plots saved to {RESULTS_DIR / 'analysis_plots.png'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f79d2b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T22:52:22.726203Z",
     "iopub.status.busy": "2026-02-16T22:52:22.725909Z",
     "iopub.status.idle": "2026-02-16T22:52:22.777772Z",
     "shell.execute_reply": "2026-02-16T22:52:22.776932Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV saved: results/exp29/results.csv\n",
      "\n",
      "VERDICT: PARTIAL: Weak generalization. Best: coqa/values_hero d=+0.070\n",
      "HERO: values_hero results: drop=-0.152, adversarialqa=+0.026, coqa=+0.070\n",
      "CEILING: Ceiling check: drop=44% floor, adversarialqa=72% floor, coqa=65% floor\n",
      "\n",
      "Results saved to results/exp29/results.json\n",
      "File size: 409.7 KB\n",
      "\n",
      "======================================================================\n",
      "SUMMARY -- Exp 29: Hard QA Datasets (Gemma 3 4B)\n",
      "======================================================================\n",
      "\n",
      "  DROP (floor: 44%):\n",
      "    sf_trunc             d=-0.084  win=33%  ns\n",
      "    sf_trunc_bias2       d=-0.153  win=32%  **\n",
      "    values_only          d=-0.076  win=34%  ns\n",
      "    values_early         d=-0.090  win=34%  ns\n",
      "    values_hero          d=-0.152  win=26%  **\n",
      "\n",
      "  ADVERSARIALQA (floor: 72%):\n",
      "    sf_trunc             d=-0.078  win=33%  ns\n",
      "    sf_trunc_bias2       d=-0.131  win=18%  *\n",
      "    values_only          d=-0.142  win=24%  *\n",
      "    values_early         d=-0.094  win=20%  ns\n",
      "    values_hero          d=+0.026  win=18%  ns\n",
      "\n",
      "  COQA (floor: 65%):\n",
      "    sf_trunc             d=-0.028  win=37%  ns\n",
      "    sf_trunc_bias2       d=-0.111  win=26%  ns\n",
      "    values_only          d=-0.134  win=31%  *\n",
      "    values_early         d=-0.051  win=24%  ns\n",
      "    values_hero          d=+0.070  win=16%  ns\n",
      "\n",
      "VERDICT: PARTIAL: Weak generalization. Best: coqa/values_hero d=+0.070\n",
      "HERO: values_hero results: drop=-0.152, adversarialqa=+0.026, coqa=+0.070\n",
      "CEILING: Ceiling check: drop=44% floor, adversarialqa=72% floor, coqa=65% floor\n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Cell 13: Save results.json + CSV\n",
    "\n",
    "# --- CSV ---\n",
    "with open(CSV_PATH, 'w', newline='') as f:\n",
    "    fieldnames = ['query_idx', 'dataset', 'query', 'answer', 'word_count',\n",
    "                  'doc_token_len', 'answer_token_len',\n",
    "                  'bare', 'sf_trunc', 'sf_trunc_bias2',\n",
    "                  'values_only', 'values_early', 'values_hero']\n",
    "    writer = csv.DictWriter(f, fieldnames=fieldnames, extrasaction='ignore')\n",
    "    writer.writeheader()\n",
    "    for r in all_results:\n",
    "        writer.writerow({k: r.get(k, '') for k in fieldnames})\n",
    "print(f\"CSV saved: {CSV_PATH}\")\n",
    "\n",
    "# --- Verdict ---\n",
    "best_ds = None\n",
    "best_cond = None\n",
    "best_d = -999\n",
    "for ds_name in dataset_names:\n",
    "    if ds_name not in analysis:\n",
    "        continue\n",
    "    for cname in CONDITION_NAMES:\n",
    "        if cname == 'bare':\n",
    "            continue\n",
    "        if cname in analysis[ds_name]:\n",
    "            d = analysis[ds_name][cname]['cohens_d']\n",
    "            if d > best_d:\n",
    "                best_d = d\n",
    "                best_ds = ds_name\n",
    "                best_cond = cname\n",
    "\n",
    "if best_d > 0.15:\n",
    "    verdict = (f\"SUCCESS: Gemma toolkit generalizes! Best: {best_ds}/{best_cond} \"\n",
    "               f\"d={best_d:+.3f}\")\n",
    "elif best_d > 0.05:\n",
    "    verdict = (f\"PARTIAL: Weak generalization. Best: {best_ds}/{best_cond} \"\n",
    "               f\"d={best_d:+.3f}\")\n",
    "else:\n",
    "    verdict = (f\"FAILURE: Gemma toolkit does NOT generalize to hard QA datasets. \"\n",
    "               f\"Best: {best_ds}/{best_cond} d={best_d:+.3f}\")\n",
    "\n",
    "# Check values_hero on each dataset\n",
    "hero_results = {}\n",
    "for ds_name in dataset_names:\n",
    "    if ds_name in analysis and 'values_hero' in analysis[ds_name]:\n",
    "        hero_results[ds_name] = analysis[ds_name]['values_hero']['cohens_d']\n",
    "hero_verdict = \"values_hero results: \" + \", \".join(\n",
    "    f\"{ds}={d:+.3f}\" for ds, d in hero_results.items())\n",
    "\n",
    "# Check ceiling status\n",
    "ceiling_status = {}\n",
    "for ds_name in dataset_names:\n",
    "    ds_r = [r for r in all_results if r['dataset'] == ds_name]\n",
    "    pct_floor = 100 * np.mean(np.array([r['bare'] for r in ds_r]) < 0.01)\n",
    "    ceiling_status[ds_name] = pct_floor\n",
    "ceiling_verdict = \"Ceiling check: \" + \", \".join(\n",
    "    f\"{ds}={pct:.0f}% floor\" for ds, pct in ceiling_status.items())\n",
    "\n",
    "print(f\"\\nVERDICT: {verdict}\")\n",
    "print(f\"HERO: {hero_verdict}\")\n",
    "print(f\"CEILING: {ceiling_verdict}\")\n",
    "\n",
    "# --- results.json ---\n",
    "final = {\n",
    "    'experiment': 'exp29_hard_datasets_gemma',\n",
    "    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'config': {\n",
    "        'model_name': MODEL_NAME,\n",
    "        'model_type': 'gemma3',\n",
    "        'seed': SEED,\n",
    "        'n_per_dataset': N_PER_DATASET,\n",
    "        'max_doc_tokens': MAX_DOC_TOKENS,\n",
    "        'conditions': CONDITION_NAMES,\n",
    "        'early_layer_cutoff': EARLY_LAYER_CUTOFF,\n",
    "        'hero_layers': HERO_LAYERS,\n",
    "        'prefix': STATIC_FACT,\n",
    "        'prefix_token_len': PREFIX_TOKEN_LEN,\n",
    "        'datasets': dataset_names,\n",
    "        'length_bins': LENGTH_BINS,\n",
    "    },\n",
    "    'per_dataset_analysis': analysis,\n",
    "    'length_stratification': length_strat,\n",
    "    'hardness_data': hardness_data,\n",
    "    'ceiling_status': ceiling_status,\n",
    "    'verdict': verdict,\n",
    "    'hero_verdict': hero_verdict,\n",
    "    'ceiling_verdict': ceiling_verdict,\n",
    "    'per_sample_results': all_results,\n",
    "}\n",
    "\n",
    "with open(FINAL_RESULTS_PATH, 'w') as f:\n",
    "    json.dump(final, f, indent=2)\n",
    "\n",
    "print(f\"\\nResults saved to {FINAL_RESULTS_PATH}\")\n",
    "print(f\"File size: {FINAL_RESULTS_PATH.stat().st_size / 1024:.1f} KB\")\n",
    "\n",
    "# Final summary\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"SUMMARY -- Exp 29: Hard QA Datasets (Gemma 3 4B)\")\n",
    "print(\"=\" * 70)\n",
    "for ds_name in dataset_names:\n",
    "    if ds_name not in analysis:\n",
    "        continue\n",
    "    ds_r = [r for r in all_results if r['dataset'] == ds_name]\n",
    "    pct_floor = 100 * np.mean(np.array([r['bare'] for r in ds_r]) < 0.01)\n",
    "    print(f\"\\n  {ds_name.upper()} (floor: {pct_floor:.0f}%):\")\n",
    "    for cname in CONDITION_NAMES:\n",
    "        if cname == 'bare':\n",
    "            continue\n",
    "        if cname in analysis[ds_name]:\n",
    "            a = analysis[ds_name][cname]\n",
    "            sig = '***' if a['p_value'] < 0.001 else '**' if a['p_value'] < 0.01 else '*' if a['p_value'] < 0.05 else 'ns'\n",
    "            print(f\"    {cname:<20} d={a['cohens_d']:>+.3f}  win={a['win_pct']:.0f}%  {sig}\")\n",
    "\n",
    "print(f\"\\nVERDICT: {verdict}\")\n",
    "print(f\"HERO: {hero_verdict}\")\n",
    "print(f\"CEILING: {ceiling_verdict}\")\n",
    "print(f\"\\nDone!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4ee2df4e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T22:52:22.781473Z",
     "iopub.status.busy": "2026-02-16T22:52:22.780803Z",
     "iopub.status.idle": "2026-02-16T22:52:23.474497Z",
     "shell.execute_reply": "2026-02-16T22:52:23.473631Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning up GPU memory...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory: 3.24 GB -> 0.01 GB\n",
      "Cleanup complete.\n"
     ]
    }
   ],
   "source": [
    "# Cell 14: GPU cleanup\n",
    "import gc\n",
    "\n",
    "print(\"Cleaning up GPU memory...\")\n",
    "mem_before = torch.cuda.memory_allocated() / 1e9\n",
    "\n",
    "del model\n",
    "del tokenizer\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "mem_after = torch.cuda.memory_allocated() / 1e9\n",
    "print(f\"GPU memory: {mem_before:.2f} GB -> {mem_after:.2f} GB\")\n",
    "print(\"Cleanup complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "03b76df177cf4a71a10a6b0034d57254": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "07114661e70d4d05af576d84ae68d28b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "07a9fed658584c27bcb030b2fed3e3d8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "07abe22039e94635b1f2efe807bda2d3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_9ec208b16e864563a6b4a69574282692",
       "placeholder": "​",
       "style": "IPY_MODEL_c22ec247d0af4daab59fa31e5cee8c75",
       "tabbable": null,
       "tooltip": null,
       "value": "Filtering AdversarialQA: 100%"
      }
     },
     "07bc3d1d5a7745d4abc84f07f52268f5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "0c776ba0bfc24c738852cecc1f22b080": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "11d9fb2376a840cab5806ebf440f0a26": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "127c7e043e7e4c1ebb028674b19c70a5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "1597315c96ee45a29d495b0932e1bec1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_d5636aedfca24c48b8aaa601ba459f3c",
        "IPY_MODEL_cd3291d8be7f445eb5c490d87d2ad68e",
        "IPY_MODEL_1b9e745a422f4c779b3cf0a32daecd9d"
       ],
       "layout": "IPY_MODEL_84f90341d7664992ad6cc0a166690557",
       "tabbable": null,
       "tooltip": null
      }
     },
     "15f03ef7e61d480da0a2fdb5e8f061ed": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "1b9e745a422f4c779b3cf0a32daecd9d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_127c7e043e7e4c1ebb028674b19c70a5",
       "placeholder": "​",
       "style": "IPY_MODEL_9024cd5c3643479b86d2d4d09fb0fc8e",
       "tabbable": null,
       "tooltip": null,
       "value": " 900/900 [37:22&lt;00:00,  2.49s/it]"
      }
     },
     "1d64dbce2ac34e67aeb931cb338994df": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "20386f04b4aa49839b5285f1d526f00c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2127ea470aeb40d8b5df05c5ed8162a3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2526b9f77cad478dbc63ecf315c0f155": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_03b76df177cf4a71a10a6b0034d57254",
       "max": 500.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_bf75119f211b462a9a9afa9a4a881b8a",
       "tabbable": null,
       "tooltip": null,
       "value": 500.0
      }
     },
     "263790201b2345fbbf4cfc24d8c42330": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2795aa565d9b4f1f80fa90fc26db5145": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "307aef1f44aa46689b96deaaf429f07c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "320a231af1c04c49aabceb412cbcbfdb": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3354cb813571438f8ffefbe2df553f4b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_2127ea470aeb40d8b5df05c5ed8162a3",
       "placeholder": "​",
       "style": "IPY_MODEL_4e990663272b4418b01d95902ef6130d",
       "tabbable": null,
       "tooltip": null,
       "value": " 900/900 [00:00&lt;00:00, 1442.60it/s]"
      }
     },
     "47dd79ce73cc486cb8273960f5e5f26a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_1d64dbce2ac34e67aeb931cb338994df",
       "placeholder": "​",
       "style": "IPY_MODEL_ae9fdf92675d4e1da132507ff7c51bfb",
       "tabbable": null,
       "tooltip": null,
       "value": " 500/500 [00:00&lt;00:00, 6035.42it/s]"
      }
     },
     "48ffc956f586482884a454c079f49958": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_9668e1accf7048fb85f9e297400c6458",
        "IPY_MODEL_5afa2c2e46c94d83ad7ee6a5fc3a9b87",
        "IPY_MODEL_fdfca1a13cab4384b81df312ae85890f"
       ],
       "layout": "IPY_MODEL_07bc3d1d5a7745d4abc84f07f52268f5",
       "tabbable": null,
       "tooltip": null
      }
     },
     "4b5552ab7eec4a54945862273a9416f4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "4e990663272b4418b01d95902ef6130d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "53a421f7319e48798c3ac9e37a2f9e07": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "5afa2c2e46c94d83ad7ee6a5fc3a9b87": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "danger",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_07a9fed658584c27bcb030b2fed3e3d8",
       "max": 9535.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_dece74979c7f4cd38f2d66584f611803",
       "tabbable": null,
       "tooltip": null,
       "value": 899.0
      }
     },
     "645db85febdb49e4a0b61393d21b8e50": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "69d86662f1684364a6db6ac1f9d3d0b9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "71446413fe624c859d5f2d11cdea5841": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "744951c7ad2b4cce8f92059c6131d18b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_aabb3aaef18b4adba705633e5768a574",
       "max": 900.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_07114661e70d4d05af576d84ae68d28b",
       "tabbable": null,
       "tooltip": null,
       "value": 900.0
      }
     },
     "7c828fd46bba42d181eb175f36e191fd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_be974a3869fd4f59b1631ce50fcc7719",
       "placeholder": "​",
       "style": "IPY_MODEL_bbe2dcc0f8ec41a0911e3c19418ef6fa",
       "tabbable": null,
       "tooltip": null,
       "value": "Tokenizing: 100%"
      }
     },
     "81332e3558644c99a304dfb48cb7e422": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_07abe22039e94635b1f2efe807bda2d3",
        "IPY_MODEL_85dfe669756d4b1f83b068e499bdd9d9",
        "IPY_MODEL_9fb39116e24e4fc38a9099933beced76"
       ],
       "layout": "IPY_MODEL_263790201b2345fbbf4cfc24d8c42330",
       "tabbable": null,
       "tooltip": null
      }
     },
     "84f90341d7664992ad6cc0a166690557": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "85dfe669756d4b1f83b068e499bdd9d9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_2795aa565d9b4f1f80fa90fc26db5145",
       "max": 1000.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_ce82b3decb80459588b1599fcc95910b",
       "tabbable": null,
       "tooltip": null,
       "value": 1000.0
      }
     },
     "8dc8c81e6dbf420bbe8a37590169e971": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_a7ffde030ee1431282df3492aa26057c",
       "placeholder": "​",
       "style": "IPY_MODEL_9f004a85a49d455d964d216c142424b4",
       "tabbable": null,
       "tooltip": null,
       "value": "Processing CoQA: 100%"
      }
     },
     "9024cd5c3643479b86d2d4d09fb0fc8e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "925b606372074a9d9ad4cde5e9382810": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_8dc8c81e6dbf420bbe8a37590169e971",
        "IPY_MODEL_2526b9f77cad478dbc63ecf315c0f155",
        "IPY_MODEL_47dd79ce73cc486cb8273960f5e5f26a"
       ],
       "layout": "IPY_MODEL_eda8bdc350774e6897e58dac5a408fe9",
       "tabbable": null,
       "tooltip": null
      }
     },
     "9668e1accf7048fb85f9e297400c6458": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_20386f04b4aa49839b5285f1d526f00c",
       "placeholder": "​",
       "style": "IPY_MODEL_11d9fb2376a840cab5806ebf440f0a26",
       "tabbable": null,
       "tooltip": null,
       "value": "Filtering DROP:   9%"
      }
     },
     "986a3370d5d14016a6a5c4e653a7456d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_320a231af1c04c49aabceb412cbcbfdb",
       "placeholder": "​",
       "style": "IPY_MODEL_53a421f7319e48798c3ac9e37a2f9e07",
       "tabbable": null,
       "tooltip": null,
       "value": " 883/883 [00:03&lt;00:00, 650.85it/s, Materializing param=model.vision_tower.vision_model.post_layernorm.weight]"
      }
     },
     "99b76f0ef97a41e493ad03c38fffa3c8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "9cdd9c77eaff4a1c82d190ac4d3e5a40": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "9ec208b16e864563a6b4a69574282692": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "9f004a85a49d455d964d216c142424b4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "9fb39116e24e4fc38a9099933beced76": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_ea42dbaa24d84c31aa0a46c04c42d1b0",
       "placeholder": "​",
       "style": "IPY_MODEL_15f03ef7e61d480da0a2fdb5e8f061ed",
       "tabbable": null,
       "tooltip": null,
       "value": " 1000/1000 [00:00&lt;00:00, 10544.23it/s]"
      }
     },
     "a1acb4ad07084a20ab4895a74863d108": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "a7ffde030ee1431282df3492aa26057c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "aabb3aaef18b4adba705633e5768a574": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ab42095a597c4db4a77302e8e9697ecd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "ae9fdf92675d4e1da132507ff7c51bfb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "b5ccd35fb25b41ea8600cab13f001b2d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "bbe2dcc0f8ec41a0911e3c19418ef6fa": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "be974a3869fd4f59b1631ce50fcc7719": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "bf75119f211b462a9a9afa9a4a881b8a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "c22ec247d0af4daab59fa31e5cee8c75": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "c29f0f2770ca4995b2f802f44055620b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c7f97701cf5546ae9afb3a04b2db42ac": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_b5ccd35fb25b41ea8600cab13f001b2d",
       "placeholder": "​",
       "style": "IPY_MODEL_a1acb4ad07084a20ab4895a74863d108",
       "tabbable": null,
       "tooltip": null,
       "value": "Loading weights: 100%"
      }
     },
     "cd3291d8be7f445eb5c490d87d2ad68e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_99b76f0ef97a41e493ad03c38fffa3c8",
       "max": 900.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_71446413fe624c859d5f2d11cdea5841",
       "tabbable": null,
       "tooltip": null,
       "value": 900.0
      }
     },
     "ce82b3decb80459588b1599fcc95910b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "d5636aedfca24c48b8aaa601ba459f3c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_c29f0f2770ca4995b2f802f44055620b",
       "placeholder": "​",
       "style": "IPY_MODEL_9cdd9c77eaff4a1c82d190ac4d3e5a40",
       "tabbable": null,
       "tooltip": null,
       "value": "Exp 29: 100%"
      }
     },
     "dece74979c7f4cd38f2d66584f611803": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "ea42dbaa24d84c31aa0a46c04c42d1b0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "eda8bdc350774e6897e58dac5a408fe9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f1db23364ad54146909f40f57802f3bb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_c7f97701cf5546ae9afb3a04b2db42ac",
        "IPY_MODEL_ff9f0c7be4144639b8dcfa5c0d8d6906",
        "IPY_MODEL_986a3370d5d14016a6a5c4e653a7456d"
       ],
       "layout": "IPY_MODEL_4b5552ab7eec4a54945862273a9416f4",
       "tabbable": null,
       "tooltip": null
      }
     },
     "f473727643df4789b94dc3e8d642d693": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_7c828fd46bba42d181eb175f36e191fd",
        "IPY_MODEL_744951c7ad2b4cce8f92059c6131d18b",
        "IPY_MODEL_3354cb813571438f8ffefbe2df553f4b"
       ],
       "layout": "IPY_MODEL_0c776ba0bfc24c738852cecc1f22b080",
       "tabbable": null,
       "tooltip": null
      }
     },
     "fdfca1a13cab4384b81df312ae85890f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_69d86662f1684364a6db6ac1f9d3d0b9",
       "placeholder": "​",
       "style": "IPY_MODEL_ab42095a597c4db4a77302e8e9697ecd",
       "tabbable": null,
       "tooltip": null,
       "value": " 899/9535 [00:00&lt;00:00, 11888.14it/s]"
      }
     },
     "ff9f0c7be4144639b8dcfa5c0d8d6906": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_307aef1f44aa46689b96deaaf429f07c",
       "max": 883.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_645db85febdb49e4a0b61393d21b8e50",
       "tabbable": null,
       "tooltip": null,
       "value": 883.0
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
