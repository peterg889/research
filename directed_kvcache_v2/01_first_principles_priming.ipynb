{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 01: First-Principles Surrogate Priming Test\n",
    "\n",
    "**Goal**: Determine whether surrogate priming has an effect on NLL scoring, and whether the effect is structural (any prefix) or semantic (relevant prefix).\n",
    "\n",
    "## Three Conditions\n",
    "\n",
    "| Condition | How cache is built | What it tests |\n",
    "|-----------|-------------------|---------------|\n",
    "| **Bare** | `[BOS] + doc_ids` (no prefix) | Baseline — no prefix at all |\n",
    "| **Random prefix** | `[BOS][random_tokens][doc_ids]` → truncate + RoPE correct | Does *any* prefix alter values in a way that affects scoring? |\n",
    "| **Oracle prefix** | `[BOS][oracle_query][doc_ids]` → truncate + RoPE correct | Does *semantically relevant* prefix content matter? |\n",
    "\n",
    "## Key Comparisons\n",
    "- **Bare vs Random**: Does the truncation/RoPE-correction process itself (with arbitrary content) change NLL?\n",
    "- **Bare vs Oracle**: Does the actual query as prefix help?\n",
    "- **Random vs Oracle**: Is there a semantic signal beyond structural noise?\n",
    "\n",
    "## Critical Design Decisions (avoiding all prior bugs)\n",
    "1. **No template framing**: `surrogate_prefix_template=\"{surrogate}\\n\"`, `document_template=\"{document}\"` — avoids \"Document:\\n\" artifact\n",
    "2. **Matched tokenization**: Doc token IDs are extracted from the oracle concatenated tokenization and reused across all 3 conditions — eliminates BPE boundary mismatch entirely (`\\n` alone gives 0% clean boundaries)\n",
    "3. **`deepcopy_cache()` before every `score_answer_with_cache()` call**: Prevents cache mutation bug\n",
    "4. **Random tokens from vocabulary**: Deterministic seed, length-matched to oracle, with decode→re-encode verification\n",
    "5. **`np.random.seed(SEED)` immediately before `load_evaluation_samples()`**: Deterministic sample selection\n",
    "6. **Checkpoint every 50 samples**: With full sample list saved for resume correctness\n",
    "7. **Query format**: `\"\\nQuery: {query}\\nAnswer:\"` with answer `\" {answer}\"` (leading space for correct BPE)\n",
    "8. **GPU memory management**: `del` caches + `torch.cuda.empty_cache()` after each sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEED: 42\n",
      "Results directory: results/exp01\n",
      "CUDA available: True\n",
      "GPU: NVIDIA L4\n",
      "GPU memory: 23.6 GB\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Setup — permissions, seeds, output directory\n",
    "import os\n",
    "os.umask(0o000)  # Required: two-user environment (jupyter + CLI user)\n",
    "\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# Output directory\n",
    "RESULTS_DIR = Path(\"results/exp01\")\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Paths\n",
    "CHECKPOINT_PATH = RESULTS_DIR / \"checkpoint.json\"\n",
    "FINAL_RESULTS_PATH = RESULTS_DIR / \"results.json\"\n",
    "\n",
    "print(f\"SEED: {SEED}\")\n",
    "print(f\"Results directory: {RESULTS_DIR}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading mistralai/Mistral-7B-Instruct-v0.2 (4-bit)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06284b2d7e2b43e6b29753bf34110163",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/291 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded. dtype=torch.float16, device=cuda:0\n",
      "Vocab size: 32000\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Load model (Mistral-7B 4-bit)\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "print(f\"Loading {MODEL_NAME} (4-bit)...\")\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "print(f\"Model loaded. dtype={model.dtype}, device={model.device}\")\n",
    "print(f\"Vocab size: {len(tokenizer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config:\n",
      "  num_samples: 2500\n",
      "  passage words: 50-300\n",
      "  surrogate_prefix_template: '{surrogate}\\n'\n",
      "  document_template: '{document}'\n",
      "  query_template: '\\nQuery: {query}\\nAnswer:'\n",
      "  answer_template: ' {answer}'\n",
      "  checkpoint_every: 50\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Library imports + config\n",
    "sys.path.insert(0, \".\")\n",
    "\n",
    "from lib.config import ExperimentConfig\n",
    "from lib.kv_cache import (\n",
    "    build_kv_cache,\n",
    "    build_truncated_kv_cache_corrected,\n",
    "    score_answer_with_cache,\n",
    "    deepcopy_cache,\n",
    "    extract_and_truncate_cache_with_bos,\n",
    "    correct_rope_positions_with_bos,\n",
    ")\n",
    "from lib.data import load_ms_marco, load_evaluation_samples\n",
    "from lib.analysis import cohens_d\n",
    "from scipy import stats\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "config = ExperimentConfig(\n",
    "    model_name=MODEL_NAME,\n",
    "    num_samples=2500,\n",
    "    seed=SEED,\n",
    ")\n",
    "\n",
    "# Templates: NO framing to avoid \"Document:\\n\" artifact\n",
    "SURROGATE_PREFIX_TEMPLATE = \"{surrogate}\\n\"\n",
    "DOCUMENT_TEMPLATE = \"{document}\"\n",
    "\n",
    "# Query/answer format\n",
    "QUERY_TEMPLATE = \"\\nQuery: {query}\\nAnswer:\"\n",
    "ANSWER_TEMPLATE = \" {answer}\"  # Leading space for correct BPE of first word\n",
    "\n",
    "# Checkpoint frequency\n",
    "CHECKPOINT_EVERY = 50\n",
    "\n",
    "print(\"Config:\")\n",
    "print(f\"  num_samples: {config.num_samples}\")\n",
    "print(f\"  passage words: {config.min_passage_words}-{config.max_passage_words}\")\n",
    "print(f\"  surrogate_prefix_template: {repr(SURROGATE_PREFIX_TEMPLATE)}\")\n",
    "print(f\"  document_template: {repr(DOCUMENT_TEMPLATE)}\")\n",
    "print(f\"  query_template: {repr(QUERY_TEMPLATE)}\")\n",
    "print(f\"  answer_template: {repr(ANSWER_TEMPLATE)}\")\n",
    "print(f\"  checkpoint_every: {CHECKPOINT_EVERY}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`trust_remote_code` is not supported anymore.\n",
      "Please check that the Hugging Face dataset 'microsoft/ms_marco' isn't based on a loading script and remove `trust_remote_code`.\n",
      "If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading microsoft/ms_marco dataset...\n",
      "Dataset loaded: 10047 samples\n",
      "Filtering samples...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73c445e60e65413ca566543a2903a6bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filtering:   0%|          | 0/10047 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 2500 samples\n",
      "\n",
      "Loaded 2500 samples\n",
      "\n",
      "Example sample:\n",
      "  Query: what temperature should it be to plant grass seeds...\n",
      "  Passage: Usually planted in the early fall, cool-season grass seeds prefer daytime temperatures ranging from ...\n",
      "  Answer: Between 50deg and 65deg F...\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Load dataset (seed immediately before for determinism)\n",
    "dataset = load_ms_marco(config)\n",
    "\n",
    "# CRITICAL: Set seed immediately before load_evaluation_samples\n",
    "# to ensure deterministic sample selection regardless of prior random state\n",
    "np.random.seed(SEED)\n",
    "samples = load_evaluation_samples(dataset, config, require_answer=True)\n",
    "\n",
    "print(f\"\\nLoaded {len(samples)} samples\")\n",
    "print(f\"\\nExample sample:\")\n",
    "print(f\"  Query: {samples[0]['query'][:100]}...\")\n",
    "print(f\"  Passage: {samples[0]['passage'][:100]}...\")\n",
    "print(f\"  Answer: {samples[0]['answer'][:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oracle query: 'what temperature should it be to plant grass seeds'\n",
      "Oracle tokens: 9\n",
      "Random prefix: 'Restaur Mars didova少 DATA luxwalkshine'...\n",
      "Random tokens: 9\n",
      "Length match: True\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: generate_random_prefix_text() function + test\n",
    "\n",
    "def generate_random_prefix_text(target_text, tokenizer, seed):\n",
    "    \"\"\"\n",
    "    Generate random text from vocabulary tokens that is length-matched\n",
    "    (in tokens) to target_text.\n",
    "    \n",
    "    Uses a decode->re-encode verification loop to ensure the random\n",
    "    prefix tokenizes to exactly the expected number of tokens.\n",
    "    \n",
    "    Args:\n",
    "        target_text: Text to match in token length\n",
    "        tokenizer: The tokenizer\n",
    "        seed: Random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "        Random text string with same token count as target_text\n",
    "    \"\"\"\n",
    "    # Get target token count (no special tokens — we want content tokens only)\n",
    "    target_ids = tokenizer.encode(target_text, add_special_tokens=False)\n",
    "    target_len = len(target_ids)\n",
    "    \n",
    "    if target_len == 0:\n",
    "        return \"\"\n",
    "    \n",
    "    rng = np.random.RandomState(seed)\n",
    "    vocab_size = len(tokenizer)\n",
    "    \n",
    "    # Sample random token IDs from the full vocabulary\n",
    "    # Exclude special tokens (typically IDs 0-2 for BOS, EOS, UNK)\n",
    "    min_id = 3  # Skip BOS, EOS, UNK\n",
    "    random_ids = rng.randint(min_id, vocab_size, size=target_len)\n",
    "    \n",
    "    # Decode to text\n",
    "    random_text = tokenizer.decode(random_ids.tolist(), skip_special_tokens=True)\n",
    "    \n",
    "    # Verification: re-encode and check length\n",
    "    reencoded = tokenizer.encode(random_text, add_special_tokens=False)\n",
    "    \n",
    "    # If lengths don't match after round-trip, truncate or pad\n",
    "    if len(reencoded) != target_len:\n",
    "        # Truncate the text and re-decode from exactly target_len tokens\n",
    "        if len(reencoded) > target_len:\n",
    "            random_text = tokenizer.decode(reencoded[:target_len], skip_special_tokens=True)\n",
    "        else:\n",
    "            # Pad with more random tokens\n",
    "            extra_needed = target_len - len(reencoded)\n",
    "            extra_ids = rng.randint(min_id, vocab_size, size=extra_needed)\n",
    "            extra_text = tokenizer.decode(extra_ids.tolist(), skip_special_tokens=True)\n",
    "            random_text = random_text + extra_text\n",
    "            reencoded2 = tokenizer.encode(random_text, add_special_tokens=False)\n",
    "            if len(reencoded2) > target_len:\n",
    "                random_text = tokenizer.decode(reencoded2[:target_len], skip_special_tokens=True)\n",
    "    \n",
    "    return random_text\n",
    "\n",
    "\n",
    "# Test the function\n",
    "test_query = samples[0]['query']\n",
    "test_random = generate_random_prefix_text(test_query, tokenizer, seed=SEED)\n",
    "\n",
    "oracle_tokens = tokenizer.encode(test_query, add_special_tokens=False)\n",
    "random_tokens = tokenizer.encode(test_random, add_special_tokens=False)\n",
    "\n",
    "print(f\"Oracle query: {repr(test_query)}\")\n",
    "print(f\"Oracle tokens: {len(oracle_tokens)}\")\n",
    "print(f\"Random prefix: {repr(test_random[:80])}...\")\n",
    "print(f\"Random tokens: {len(random_tokens)}\")\n",
    "print(f\"Length match: {len(oracle_tokens) == len(random_tokens)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BPE Boundary Diagnostic\n",
      "============================================================\n",
      "Tested 100 samples\n",
      "BPE mismatches: 100/100 (100%)\n",
      "\n",
      "As expected, independent tokenization does NOT match concatenated tokenization.\n",
      "The main loop uses MATCHED tokenization: doc token IDs are extracted from\n",
      "the oracle concatenation, then reused for bare and random conditions.\n",
      "This eliminates BPE mismatch entirely.\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: BPE boundary diagnostic — why matched tokenization is necessary\n",
    "#\n",
    "# SentencePiece adds a leading \"▁\" to the first token of a string. After \"\\n\",\n",
    "# the BPE merge decisions change, so tokenizing \"prefix\\npassage\" and \"passage\"\n",
    "# independently produces DIFFERENT token sequences for the passage.\n",
    "#\n",
    "# Fix: extract doc token IDs from the concatenated tokenization, then reuse\n",
    "# those exact IDs for bare/oracle/random caches. This guarantees all three\n",
    "# conditions operate on identical document tokens.\n",
    "\n",
    "print(\"BPE Boundary Diagnostic\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "n_mismatch = 0\n",
    "n_total = min(100, len(samples))\n",
    "\n",
    "for i in range(n_total):\n",
    "    passage = samples[i]['passage']\n",
    "    query = samples[i]['query']\n",
    "\n",
    "    # Concatenated tokenization (what build_truncated_kv_cache_corrected does)\n",
    "    oracle_prefix = SURROGATE_PREFIX_TEMPLATE.format(surrogate=query)\n",
    "    document_text = DOCUMENT_TEMPLATE.format(document=passage)\n",
    "    full_text = oracle_prefix + document_text\n",
    "    full_ids = tokenizer.encode(full_text, add_special_tokens=True)\n",
    "    prefix_ids = tokenizer.encode(oracle_prefix, add_special_tokens=True)\n",
    "    prefix_len = len(prefix_ids)\n",
    "    doc_from_concat = full_ids[prefix_len:]\n",
    "\n",
    "    # Independent tokenization (what build_kv_cache does)\n",
    "    doc_independent = tokenizer.encode(passage, add_special_tokens=True)[1:]  # strip BOS\n",
    "\n",
    "    if doc_from_concat != doc_independent:\n",
    "        n_mismatch += 1\n",
    "\n",
    "print(f\"Tested {n_total} samples\")\n",
    "print(f\"BPE mismatches: {n_mismatch}/{n_total} ({100*n_mismatch/n_total:.0f}%)\")\n",
    "print()\n",
    "if n_mismatch > 0:\n",
    "    print(\"As expected, independent tokenization does NOT match concatenated tokenization.\")\n",
    "    print(\"The main loop uses MATCHED tokenization: doc token IDs are extracted from\")\n",
    "    print(\"the oracle concatenation, then reused for bare and random conditions.\")\n",
    "    print(\"This eliminates BPE mismatch entirely.\")\n",
    "else:\n",
    "    print(\"Surprisingly, all boundaries are clean. Matched tokenization is still used\")\n",
    "    print(\"for safety, but independent tokenization would also work here.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "EXPERIMENTAL CONDITIONS EXPLAINED\n",
      "======================================================================\n",
      "\n",
      "Example passage: 'Usually planted in the early fall, cool-season grass seeds prefer daytime temper...'\n",
      "Example query:   'what temperature should it be to plant grass seeds'\n",
      "Example answer:  'Between 50deg and 65deg F...'\n",
      "\n",
      "──────────────────────────────────────────────────────────────────────\n",
      "MATCHED TOKENIZATION (avoids BPE boundary mismatch)\n",
      "  Tokenize oracle_prefix + passage together → 134 tokens\n",
      "  Oracle prefix tokens (with BOS): 11\n",
      "  Document tokens (shared by ALL conditions): 123\n",
      "\n",
      "──────────────────────────────────────────────────────────────────────\n",
      "### CONDITION 1: BARE (baseline) ###\n",
      "  Input IDs:  [BOS] + doc_ids (124 tokens)\n",
      "  Key insight: Pure baseline. Same doc tokens, no prefix, no RoPE correction.\n",
      "\n",
      "──────────────────────────────────────────────────────────────────────\n",
      "### CONDITION 2: RANDOM PREFIX (structural control) ###\n",
      "  Random text:  'Restaur Mars didova少 DATA luxwalkshine'...\n",
      "  Input IDs:  [BOS] + random_prefix_ids + doc_ids (134 tokens)\n",
      "  After truncation: [BOS] + doc_ids (124 tokens) + RoPE correction\n",
      "  Key insight: Tests if ANY prefix affects NLL via value contamination.\n",
      "\n",
      "──────────────────────────────────────────────────────────────────────\n",
      "### CONDITION 3: ORACLE PREFIX (semantic signal) ###\n",
      "  Oracle query: 'what temperature should it be to plant grass seeds'\n",
      "  Input IDs:  [BOS] + oracle_prefix_ids + doc_ids (134 tokens)\n",
      "  After truncation: [BOS] + doc_ids (124 tokens) + RoPE correction\n",
      "  Key insight: Tests if RELEVANT prefix adds semantic signal beyond structural noise.\n",
      "\n",
      "──────────────────────────────────────────────────────────────────────\n",
      "ALL conditions use IDENTICAL doc_ids → differences are purely from prefix contamination.\n",
      "CACHE SAFETY: deepcopy_cache() before every score call.\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Condition explanation printout\n",
    "print(\"=\" * 70)\n",
    "print(\"EXPERIMENTAL CONDITIONS EXPLAINED\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "ex = samples[0]\n",
    "ex_query = ex['query']\n",
    "ex_passage = ex['passage'][:80] + \"...\"\n",
    "ex_answer = ex['answer'][:60] + \"...\"\n",
    "ex_random = generate_random_prefix_text(ex_query, tokenizer, seed=SEED)\n",
    "\n",
    "print(f\"\\nExample passage: {repr(ex_passage)}\")\n",
    "print(f\"Example query:   {repr(ex_query)}\")\n",
    "print(f\"Example answer:  {repr(ex_answer)}\")\n",
    "\n",
    "# Show the matched tokenization approach\n",
    "oracle_prefix = SURROGATE_PREFIX_TEMPLATE.format(surrogate=ex_query)\n",
    "doc_text = DOCUMENT_TEMPLATE.format(document=ex['passage'])\n",
    "full_text = oracle_prefix + doc_text\n",
    "full_ids = tokenizer.encode(full_text, add_special_tokens=True)\n",
    "prefix_ids = tokenizer.encode(oracle_prefix, add_special_tokens=True)\n",
    "prefix_len = len(prefix_ids)\n",
    "doc_ids = full_ids[prefix_len:]\n",
    "\n",
    "print(f\"\\n{'─' * 70}\")\n",
    "print(\"MATCHED TOKENIZATION (avoids BPE boundary mismatch)\")\n",
    "print(f\"  Tokenize oracle_prefix + passage together → {len(full_ids)} tokens\")\n",
    "print(f\"  Oracle prefix tokens (with BOS): {prefix_len}\")\n",
    "print(f\"  Document tokens (shared by ALL conditions): {len(doc_ids)}\")\n",
    "\n",
    "print(f\"\\n{'─' * 70}\")\n",
    "print(\"### CONDITION 1: BARE (baseline) ###\")\n",
    "print(f\"  Input IDs:  [BOS] + doc_ids ({1 + len(doc_ids)} tokens)\")\n",
    "print(f\"  Key insight: Pure baseline. Same doc tokens, no prefix, no RoPE correction.\")\n",
    "\n",
    "print(f\"\\n{'─' * 70}\")\n",
    "print(\"### CONDITION 2: RANDOM PREFIX (structural control) ###\")\n",
    "random_prefix = SURROGATE_PREFIX_TEMPLATE.format(surrogate=ex_random)\n",
    "random_prefix_ids = tokenizer.encode(random_prefix, add_special_tokens=False)\n",
    "print(f\"  Random text:  {repr(ex_random[:60])}...\")\n",
    "print(f\"  Input IDs:  [BOS] + random_prefix_ids + doc_ids ({1 + len(random_prefix_ids) + len(doc_ids)} tokens)\")\n",
    "print(f\"  After truncation: [BOS] + doc_ids ({1 + len(doc_ids)} tokens) + RoPE correction\")\n",
    "print(f\"  Key insight: Tests if ANY prefix affects NLL via value contamination.\")\n",
    "\n",
    "print(f\"\\n{'─' * 70}\")\n",
    "print(\"### CONDITION 3: ORACLE PREFIX (semantic signal) ###\")\n",
    "print(f\"  Oracle query: {repr(ex_query)}\")\n",
    "print(f\"  Input IDs:  [BOS] + oracle_prefix_ids + doc_ids ({len(full_ids)} tokens)\")\n",
    "print(f\"  After truncation: [BOS] + doc_ids ({1 + len(doc_ids)} tokens) + RoPE correction\")\n",
    "print(f\"  Key insight: Tests if RELEVANT prefix adds semantic signal beyond structural noise.\")\n",
    "\n",
    "print(f\"\\n{'─' * 70}\")\n",
    "print(\"ALL conditions use IDENTICAL doc_ids → differences are purely from prefix contamination.\")\n",
    "print(\"CACHE SAFETY: deepcopy_cache() before every score call.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 2500\n",
      "No checkpoint found. Starting from scratch.\n",
      "Will evaluate samples 0 to 2499\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Evaluation parameters + checkpoint loading\n",
    "\n",
    "N = len(samples)\n",
    "print(f\"Total samples: {N}\")\n",
    "\n",
    "# Check for existing checkpoint\n",
    "results = []\n",
    "start_idx = 0\n",
    "\n",
    "if CHECKPOINT_PATH.exists():\n",
    "    with open(CHECKPOINT_PATH, 'r') as f:\n",
    "        checkpoint = json.load(f)\n",
    "    \n",
    "    # Verify checkpoint matches current sample list (resume correctness)\n",
    "    ckpt_sample_ids = checkpoint.get('sample_queries', [])\n",
    "    current_sample_ids = [s['query'] for s in samples]\n",
    "    \n",
    "    if ckpt_sample_ids == current_sample_ids:\n",
    "        results = checkpoint['results']\n",
    "        start_idx = len(results)\n",
    "        print(f\"Resuming from checkpoint: {start_idx}/{N} samples completed\")\n",
    "    else:\n",
    "        print(\"WARNING: Checkpoint sample list doesn't match current samples.\")\n",
    "        print(\"Starting from scratch.\")\n",
    "        results = []\n",
    "        start_idx = 0\n",
    "else:\n",
    "    print(\"No checkpoint found. Starting from scratch.\")\n",
    "\n",
    "print(f\"Will evaluate samples {start_idx} to {N-1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aad5b886fc2a49e0b8c944f7c02e1c3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/2500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint at 50/2500 | Rate: 0.8 samples/s | ETA: 53.0 min\n",
      "  Checkpoint at 100/2500 | Rate: 0.8 samples/s | ETA: 52.0 min\n",
      "  Checkpoint at 150/2500 | Rate: 0.8 samples/s | ETA: 51.1 min\n",
      "  Checkpoint at 200/2500 | Rate: 0.8 samples/s | ETA: 50.1 min\n",
      "  Checkpoint at 250/2500 | Rate: 0.8 samples/s | ETA: 49.2 min\n",
      "  Checkpoint at 300/2500 | Rate: 0.8 samples/s | ETA: 48.2 min\n",
      "  Checkpoint at 350/2500 | Rate: 0.8 samples/s | ETA: 47.1 min\n",
      "  Checkpoint at 400/2500 | Rate: 0.8 samples/s | ETA: 46.0 min\n",
      "  Checkpoint at 450/2500 | Rate: 0.8 samples/s | ETA: 45.0 min\n",
      "  Checkpoint at 500/2500 | Rate: 0.8 samples/s | ETA: 43.9 min\n",
      "  Checkpoint at 550/2500 | Rate: 0.8 samples/s | ETA: 42.9 min\n",
      "  Checkpoint at 600/2500 | Rate: 0.8 samples/s | ETA: 41.8 min\n",
      "  Checkpoint at 650/2500 | Rate: 0.8 samples/s | ETA: 40.7 min\n",
      "  Checkpoint at 700/2500 | Rate: 0.8 samples/s | ETA: 39.6 min\n",
      "  Checkpoint at 750/2500 | Rate: 0.8 samples/s | ETA: 38.5 min\n",
      "  Checkpoint at 800/2500 | Rate: 0.8 samples/s | ETA: 37.4 min\n",
      "  Checkpoint at 850/2500 | Rate: 0.8 samples/s | ETA: 36.3 min\n",
      "  Checkpoint at 900/2500 | Rate: 0.8 samples/s | ETA: 35.2 min\n",
      "  Checkpoint at 950/2500 | Rate: 0.8 samples/s | ETA: 34.1 min\n",
      "  Checkpoint at 1000/2500 | Rate: 0.8 samples/s | ETA: 33.0 min\n",
      "  Checkpoint at 1050/2500 | Rate: 0.8 samples/s | ETA: 31.9 min\n",
      "  Checkpoint at 1100/2500 | Rate: 0.8 samples/s | ETA: 30.8 min\n",
      "  Checkpoint at 1150/2500 | Rate: 0.8 samples/s | ETA: 29.7 min\n",
      "  Checkpoint at 1200/2500 | Rate: 0.8 samples/s | ETA: 28.6 min\n",
      "  Checkpoint at 1250/2500 | Rate: 0.8 samples/s | ETA: 27.5 min\n",
      "  Checkpoint at 1300/2500 | Rate: 0.8 samples/s | ETA: 26.5 min\n",
      "  Checkpoint at 1350/2500 | Rate: 0.8 samples/s | ETA: 25.4 min\n",
      "  Checkpoint at 1400/2500 | Rate: 0.8 samples/s | ETA: 24.2 min\n",
      "  Checkpoint at 1450/2500 | Rate: 0.8 samples/s | ETA: 23.1 min\n",
      "  Checkpoint at 1500/2500 | Rate: 0.8 samples/s | ETA: 22.0 min\n",
      "  Checkpoint at 1550/2500 | Rate: 0.8 samples/s | ETA: 20.9 min\n",
      "  Checkpoint at 1600/2500 | Rate: 0.8 samples/s | ETA: 19.8 min\n",
      "  Checkpoint at 1650/2500 | Rate: 0.8 samples/s | ETA: 18.7 min\n",
      "  Checkpoint at 1700/2500 | Rate: 0.8 samples/s | ETA: 17.6 min\n",
      "  Checkpoint at 1750/2500 | Rate: 0.8 samples/s | ETA: 16.5 min\n",
      "  Checkpoint at 1800/2500 | Rate: 0.8 samples/s | ETA: 15.4 min\n",
      "  Checkpoint at 1850/2500 | Rate: 0.8 samples/s | ETA: 14.3 min\n",
      "  Checkpoint at 1900/2500 | Rate: 0.8 samples/s | ETA: 13.2 min\n",
      "  Checkpoint at 1950/2500 | Rate: 0.8 samples/s | ETA: 12.1 min\n",
      "  Checkpoint at 2000/2500 | Rate: 0.8 samples/s | ETA: 11.0 min\n",
      "  Checkpoint at 2050/2500 | Rate: 0.8 samples/s | ETA: 9.9 min\n",
      "  Checkpoint at 2100/2500 | Rate: 0.8 samples/s | ETA: 8.8 min\n",
      "  Checkpoint at 2150/2500 | Rate: 0.8 samples/s | ETA: 7.7 min\n",
      "  Checkpoint at 2200/2500 | Rate: 0.8 samples/s | ETA: 6.6 min\n",
      "  Checkpoint at 2250/2500 | Rate: 0.8 samples/s | ETA: 5.5 min\n",
      "  Checkpoint at 2300/2500 | Rate: 0.8 samples/s | ETA: 4.4 min\n",
      "  Checkpoint at 2350/2500 | Rate: 0.8 samples/s | ETA: 3.3 min\n",
      "  Checkpoint at 2400/2500 | Rate: 0.8 samples/s | ETA: 2.2 min\n",
      "  Checkpoint at 2450/2500 | Rate: 0.8 samples/s | ETA: 1.1 min\n",
      "  Checkpoint at 2500/2500 | Rate: 0.8 samples/s | ETA: 0.0 min\n",
      "\n",
      "Evaluation complete: 2500 samples in 55.1 minutes\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Main evaluation loop (3 conditions × N samples, with checkpointing)\n",
    "#\n",
    "# KEY FIX: Matched tokenization. For each sample we:\n",
    "# 1. Tokenize oracle_prefix + passage together to get the canonical doc_ids\n",
    "# 2. Build bare cache from [BOS] + doc_ids\n",
    "# 3. Build oracle full cache from [BOS][oracle_prefix_ids][doc_ids], truncate + RoPE correct\n",
    "# 4. Build random full cache from [BOS][random_prefix_ids][doc_ids], truncate + RoPE correct\n",
    "# All three caches use IDENTICAL document tokens — differences are purely from prefix.\n",
    "\n",
    "t_start = time.time()\n",
    "\n",
    "for idx in tqdm(range(start_idx, N), initial=start_idx, total=N, desc=\"Evaluating\"):\n",
    "    sample = samples[idx]\n",
    "    passage = sample['passage']\n",
    "    query = sample['query']\n",
    "    answer = sample['answer']\n",
    "\n",
    "    query_prompt = QUERY_TEMPLATE.format(query=query)\n",
    "    answer_text = ANSWER_TEMPLATE.format(answer=answer)\n",
    "\n",
    "    # === Step 1: Determine canonical document token IDs ===\n",
    "    oracle_prefix = SURROGATE_PREFIX_TEMPLATE.format(surrogate=query)\n",
    "    document_text = DOCUMENT_TEMPLATE.format(document=passage)\n",
    "    full_oracle_text = oracle_prefix + document_text\n",
    "\n",
    "    full_oracle_enc = tokenizer(full_oracle_text, return_tensors=\"pt\",\n",
    "                                add_special_tokens=True, padding=False, truncation=False)\n",
    "    full_oracle_ids = full_oracle_enc['input_ids'].to(config.device)\n",
    "\n",
    "    oracle_prefix_enc = tokenizer(oracle_prefix, return_tensors=\"pt\",\n",
    "                                  add_special_tokens=True, padding=False, truncation=False)\n",
    "    oracle_prefix_len = oracle_prefix_enc['input_ids'].shape[1]  # includes BOS\n",
    "\n",
    "    bos_id = full_oracle_ids[:, :1]\n",
    "    doc_ids = full_oracle_ids[:, oracle_prefix_len:]\n",
    "    doc_len = doc_ids.shape[1]\n",
    "\n",
    "    # === Condition 1: BARE — [BOS] + doc_ids ===\n",
    "    bare_ids = torch.cat([bos_id, doc_ids], dim=1)\n",
    "    bare_len = bare_ids.shape[1]  # = 1 + doc_len\n",
    "    with torch.no_grad():\n",
    "        bare_out = model(input_ids=bare_ids, attention_mask=torch.ones_like(bare_ids),\n",
    "                         use_cache=True, return_dict=True)\n",
    "    bare_cache = bare_out.past_key_values\n",
    "    bare_nll = score_answer_with_cache(\n",
    "        deepcopy_cache(bare_cache), bare_len, query_prompt, answer_text,\n",
    "        model, tokenizer, config\n",
    "    )\n",
    "\n",
    "    # === Condition 2: RANDOM PREFIX — [BOS][random_prefix][doc_ids] → truncate ===\n",
    "    random_text = generate_random_prefix_text(query, tokenizer, seed=SEED + idx)\n",
    "    random_prefix = SURROGATE_PREFIX_TEMPLATE.format(surrogate=random_text)\n",
    "    random_prefix_enc = tokenizer(random_prefix, return_tensors=\"pt\",\n",
    "                                  add_special_tokens=False, padding=False, truncation=False)\n",
    "    random_prefix_ids = random_prefix_enc['input_ids'].to(config.device)\n",
    "    random_full_ids = torch.cat([bos_id, random_prefix_ids, doc_ids], dim=1)\n",
    "    random_prefix_len = 1 + random_prefix_ids.shape[1]  # BOS + prefix tokens\n",
    "\n",
    "    with torch.no_grad():\n",
    "        random_out = model(input_ids=random_full_ids,\n",
    "                           attention_mask=torch.ones_like(random_full_ids),\n",
    "                           use_cache=True, return_dict=True)\n",
    "    random_cache = extract_and_truncate_cache_with_bos(random_out.past_key_values, doc_len)\n",
    "    random_len = 1 + doc_len\n",
    "    correct_rope_positions_with_bos(random_cache, random_prefix_len - 1, model)\n",
    "\n",
    "    random_nll = score_answer_with_cache(\n",
    "        deepcopy_cache(random_cache), random_len, query_prompt, answer_text,\n",
    "        model, tokenizer, config\n",
    "    )\n",
    "\n",
    "    # === Condition 3: ORACLE PREFIX — already have full_oracle_ids ===\n",
    "    with torch.no_grad():\n",
    "        oracle_out = model(input_ids=full_oracle_ids,\n",
    "                           attention_mask=torch.ones_like(full_oracle_ids),\n",
    "                           use_cache=True, return_dict=True)\n",
    "    oracle_cache = extract_and_truncate_cache_with_bos(oracle_out.past_key_values, doc_len)\n",
    "    oracle_len = 1 + doc_len\n",
    "    correct_rope_positions_with_bos(oracle_cache, oracle_prefix_len - 1, model)\n",
    "\n",
    "    oracle_nll = score_answer_with_cache(\n",
    "        deepcopy_cache(oracle_cache), oracle_len, query_prompt, answer_text,\n",
    "        model, tokenizer, config\n",
    "    )\n",
    "\n",
    "    # Record result\n",
    "    result = {\n",
    "        'idx': idx,\n",
    "        'bare_nll': bare_nll,\n",
    "        'random_nll': random_nll,\n",
    "        'oracle_nll': oracle_nll,\n",
    "        'bare_len': bare_len,\n",
    "        'random_len': random_len,\n",
    "        'oracle_len': oracle_len,\n",
    "        'doc_len': doc_len,\n",
    "        'delta_random_vs_bare': bare_nll - random_nll,     # positive = random better\n",
    "        'delta_oracle_vs_bare': bare_nll - oracle_nll,     # positive = oracle better\n",
    "        'delta_oracle_vs_random': random_nll - oracle_nll, # positive = oracle better\n",
    "    }\n",
    "    results.append(result)\n",
    "\n",
    "    # GPU memory management\n",
    "    del bare_cache, random_cache, oracle_cache, bare_out, random_out, oracle_out\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # Checkpoint\n",
    "    if (idx + 1) % CHECKPOINT_EVERY == 0 or idx == N - 1:\n",
    "        checkpoint_data = {\n",
    "            'results': results,\n",
    "            'sample_queries': [s['query'] for s in samples],\n",
    "            'completed': len(results),\n",
    "            'total': N,\n",
    "            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        }\n",
    "        with open(CHECKPOINT_PATH, 'w') as f:\n",
    "            json.dump(checkpoint_data, f)\n",
    "\n",
    "        elapsed = time.time() - t_start\n",
    "        samples_done = idx - start_idx + 1\n",
    "        rate = samples_done / elapsed if elapsed > 0 else 0\n",
    "        remaining = (N - idx - 1) / rate if rate > 0 else 0\n",
    "        tqdm.write(f\"  Checkpoint at {idx+1}/{N} | Rate: {rate:.1f} samples/s | ETA: {remaining/60:.1f} min\")\n",
    "\n",
    "elapsed_total = time.time() - t_start\n",
    "print(f\"\\nEvaluation complete: {len(results)} samples in {elapsed_total/60:.1f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache Length Diagnostic\n",
      "==================================================\n",
      "\n",
      "Samples: 2500\n",
      "All cache lengths match (bare == random == oracle == 1+doc): True\n",
      "PASS: Matched tokenization guarantees identical cache lengths.\n"
     ]
    }
   ],
   "source": [
    "# Cell 10: Cache length diagnostic\n",
    "print(\"Cache Length Diagnostic\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "bare_lens = [r['bare_len'] for r in results]\n",
    "random_lens = [r['random_len'] for r in results]\n",
    "oracle_lens = [r['oracle_len'] for r in results]\n",
    "\n",
    "n = len(results)\n",
    "\n",
    "# Check if results have doc_len (new matched code) or not (old code)\n",
    "has_doc_len = 'doc_len' in results[0]\n",
    "if has_doc_len:\n",
    "    doc_lens = [r['doc_len'] for r in results]\n",
    "    all_match = all(b == r == o == 1 + d\n",
    "                    for b, r, o, d in zip(bare_lens, random_lens, oracle_lens, doc_lens))\n",
    "    print(f\"\\nSamples: {n}\")\n",
    "    print(f\"All cache lengths match (bare == random == oracle == 1+doc): {all_match}\")\n",
    "    if all_match:\n",
    "        print(\"PASS: Matched tokenization guarantees identical cache lengths.\")\n",
    "    else:\n",
    "        mismatches = sum(1 for b, r, o in zip(bare_lens, random_lens, oracle_lens) if not (b == r == o))\n",
    "        print(f\"WARNING: {mismatches} samples have mismatched lengths!\")\n",
    "else:\n",
    "    # Old results — check how close they are\n",
    "    exact_bare_rand = sum(1 for b, r in zip(bare_lens, random_lens) if b == r)\n",
    "    exact_bare_orac = sum(1 for b, o in zip(bare_lens, oracle_lens) if b == o)\n",
    "    exact_rand_orac = sum(1 for r, o in zip(random_lens, oracle_lens) if r == o)\n",
    "    max_diff = max(max(abs(b - r) for b, r in zip(bare_lens, random_lens)),\n",
    "                   max(abs(b - o) for b, o in zip(bare_lens, oracle_lens)))\n",
    "\n",
    "    print(f\"\\nSamples: {n}\")\n",
    "    print(f\"NOTE: Results from pre-matched-tokenization run (no doc_len key).\")\n",
    "    print(f\"  Bare vs Random exact match:  {exact_bare_rand}/{n} ({100*exact_bare_rand/n:.1f}%)\")\n",
    "    print(f\"  Bare vs Oracle exact match:  {exact_bare_orac}/{n} ({100*exact_bare_orac/n:.1f}%)\")\n",
    "    print(f\"  Random vs Oracle exact match: {exact_rand_orac}/{n} ({100*exact_rand_orac/n:.1f}%)\")\n",
    "    print(f\"  Max length difference: {max_diff} token(s)\")\n",
    "    print()\n",
    "    print(\"Random and Oracle always match (both from concatenated tokenization).\")\n",
    "    print(\"Bare differs by ±1 token due to BPE boundary mismatch.\")\n",
    "    print(\"To get perfectly matched results, re-run cells 8 → 9 with the new code.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PRIMARY ANALYSIS\n",
      "======================================================================\n",
      "Total samples: 2500\n",
      "Excluded (zero NLL from single-token answers): 197\n",
      "Valid samples for analysis: 2303\n",
      "\n",
      "Condition              Mean NLL        Std     Median\n",
      "-------------------------------------------------------\n",
      "Bare                     1.1455     1.5698     0.6206\n",
      "Random prefix            1.1171     1.5405     0.5923\n",
      "Oracle prefix            1.1369     1.5553     0.6270\n",
      "\n",
      "──────────────────────────────────────────────────────────────────────\n",
      "PAIRED COMPARISONS (positive delta = first condition has HIGHER NLL)\n",
      "──────────────────────────────────────────────────────────────────────\n",
      "\n",
      "Comparison                  Mean Δ        d    Win%        t            p\n",
      "------------------------------------------------------------------------\n",
      "Bare vs Random              0.0285    0.091   59.5%     4.37    1.31e-05 ***\n",
      "Bare vs Oracle              0.0086    0.023   50.0%     1.12    2.63e-01 ns\n",
      "Random vs Oracle           -0.0198   -0.051   44.9%    -2.42    1.54e-02 *\n",
      "\n",
      "Note: Cohen's d interpretation: |d|<0.2 = negligible, 0.2-0.5 = small, 0.5-0.8 = medium, >0.8 = large\n"
     ]
    }
   ],
   "source": [
    "# Cell 11: Primary analysis — NLL means, paired comparisons\n",
    "print(\"=\" * 70)\n",
    "print(\"PRIMARY ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "bare_nlls_raw = np.array([r['bare_nll'] for r in results])\n",
    "random_nlls_raw = np.array([r['random_nll'] for r in results])\n",
    "oracle_nlls_raw = np.array([r['oracle_nll'] for r in results])\n",
    "\n",
    "# Sanity checks\n",
    "assert not np.any(np.isnan(bare_nlls_raw)), \"NaN in bare NLLs!\"\n",
    "assert not np.any(np.isnan(random_nlls_raw)), \"NaN in random NLLs!\"\n",
    "assert not np.any(np.isnan(oracle_nlls_raw)), \"NaN in oracle NLLs!\"\n",
    "\n",
    "# Filter out degenerate samples where any NLL is 0.0\n",
    "# This happens when the answer is a single token (num_scored = answer_len - 1 = 0)\n",
    "valid_mask = (bare_nlls_raw != 0.0) & (random_nlls_raw != 0.0) & (oracle_nlls_raw != 0.0)\n",
    "n_invalid = np.sum(~valid_mask)\n",
    "print(f\"Total samples: {len(results)}\")\n",
    "print(f\"Excluded (zero NLL from single-token answers): {n_invalid}\")\n",
    "print(f\"Valid samples for analysis: {np.sum(valid_mask)}\\n\")\n",
    "\n",
    "bare_nlls = bare_nlls_raw[valid_mask]\n",
    "random_nlls = random_nlls_raw[valid_mask]\n",
    "oracle_nlls = oracle_nlls_raw[valid_mask]\n",
    "\n",
    "# NLL summary\n",
    "print(f\"{'Condition':<20} {'Mean NLL':>10} {'Std':>10} {'Median':>10}\")\n",
    "print(\"-\" * 55)\n",
    "print(f\"{'Bare':<20} {np.mean(bare_nlls):>10.4f} {np.std(bare_nlls):>10.4f} {np.median(bare_nlls):>10.4f}\")\n",
    "print(f\"{'Random prefix':<20} {np.mean(random_nlls):>10.4f} {np.std(random_nlls):>10.4f} {np.median(random_nlls):>10.4f}\")\n",
    "print(f\"{'Oracle prefix':<20} {np.mean(oracle_nlls):>10.4f} {np.std(oracle_nlls):>10.4f} {np.median(oracle_nlls):>10.4f}\")\n",
    "\n",
    "# Paired differences\n",
    "delta_random_bare = bare_nlls - random_nlls   # positive = random better\n",
    "delta_oracle_bare = bare_nlls - oracle_nlls   # positive = oracle better\n",
    "delta_oracle_random = random_nlls - oracle_nlls  # positive = oracle better\n",
    "\n",
    "print(f\"\\n{'─' * 70}\")\n",
    "print(\"PAIRED COMPARISONS (positive delta = first condition has HIGHER NLL)\")\n",
    "print(f\"{'─' * 70}\")\n",
    "\n",
    "comparisons = [\n",
    "    (\"Bare vs Random\", delta_random_bare, \"Random better?\"),\n",
    "    (\"Bare vs Oracle\", delta_oracle_bare, \"Oracle better?\"),\n",
    "    (\"Random vs Oracle\", delta_oracle_random, \"Oracle better than random?\"),\n",
    "]\n",
    "\n",
    "print(f\"\\n{'Comparison':<25} {'Mean Δ':>8} {'d':>8} {'Win%':>7} {'t':>8} {'p':>12}\")\n",
    "print(\"-\" * 72)\n",
    "\n",
    "for name, delta, question in comparisons:\n",
    "    d = cohens_d(delta)\n",
    "    win_rate = np.mean(delta > 0) * 100\n",
    "    t_stat, p_val = stats.ttest_1samp(delta, 0)\n",
    "    sig = \"***\" if p_val < 0.001 else \"**\" if p_val < 0.01 else \"*\" if p_val < 0.05 else \"ns\"\n",
    "    print(f\"{name:<25} {np.mean(delta):>8.4f} {d:>8.3f} {win_rate:>6.1f}% {t_stat:>8.2f} {p_val:>11.2e} {sig}\")\n",
    "\n",
    "print(f\"\\nNote: Cohen's d interpretation: |d|<0.2 = negligible, 0.2-0.5 = small, 0.5-0.8 = medium, >0.8 = large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "SUMMARY TABLE\n",
      "======================================================================\n",
      "\n",
      "N = 2500 samples\n",
      "\n",
      "Q: Does truncation/RoPE process itself change NLL?\n",
      "  Δ = +0.0285, d = +0.091, Win% = 59.5%, p = 1.31e-05 ***\n",
      "  Answer: YES (helps)\n",
      "\n",
      "Q: Does the actual query as prefix help?\n",
      "  Δ = +0.0086, d = +0.023, Win% = 50.0%, p = 2.63e-01 ns\n",
      "  Answer: NO \n",
      "\n",
      "Q: Is there semantic signal beyond structural noise?\n",
      "  Δ = -0.0198, d = -0.051, Win% = 44.9%, p = 1.54e-02 *\n",
      "  Answer: YES (hurts)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 12: Summary table\n",
    "print(\"=\" * 70)\n",
    "print(\"SUMMARY TABLE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\nN = {len(results)} samples\\n\")\n",
    "\n",
    "# Build summary\n",
    "summary_rows = [\n",
    "    {\n",
    "        'comparison': 'Bare vs Random',\n",
    "        'question': 'Does truncation/RoPE process itself change NLL?',\n",
    "        'mean_delta': np.mean(delta_random_bare),\n",
    "        'cohens_d': cohens_d(delta_random_bare),\n",
    "        'win_pct': np.mean(delta_random_bare > 0) * 100,\n",
    "        'p_value': stats.ttest_1samp(delta_random_bare, 0)[1],\n",
    "    },\n",
    "    {\n",
    "        'comparison': 'Bare vs Oracle',\n",
    "        'question': 'Does the actual query as prefix help?',\n",
    "        'mean_delta': np.mean(delta_oracle_bare),\n",
    "        'cohens_d': cohens_d(delta_oracle_bare),\n",
    "        'win_pct': np.mean(delta_oracle_bare > 0) * 100,\n",
    "        'p_value': stats.ttest_1samp(delta_oracle_bare, 0)[1],\n",
    "    },\n",
    "    {\n",
    "        'comparison': 'Random vs Oracle',\n",
    "        'question': 'Is there semantic signal beyond structural noise?',\n",
    "        'mean_delta': np.mean(delta_oracle_random),\n",
    "        'cohens_d': cohens_d(delta_oracle_random),\n",
    "        'win_pct': np.mean(delta_oracle_random > 0) * 100,\n",
    "        'p_value': stats.ttest_1samp(delta_oracle_random, 0)[1],\n",
    "    },\n",
    "]\n",
    "\n",
    "for row in summary_rows:\n",
    "    sig = \"***\" if row['p_value'] < 0.001 else \"**\" if row['p_value'] < 0.01 else \"*\" if row['p_value'] < 0.05 else \"ns\"\n",
    "    verdict = \"YES\" if row['p_value'] < 0.05 else \"NO\"\n",
    "    direction = \"(helps)\" if row['mean_delta'] > 0 else \"(hurts)\"\n",
    "    print(f\"Q: {row['question']}\")\n",
    "    print(f\"  Δ = {row['mean_delta']:+.4f}, d = {row['cohens_d']:+.3f}, Win% = {row['win_pct']:.1f}%, p = {row['p_value']:.2e} {sig}\")\n",
    "    print(f\"  Answer: {verdict} {direction if row['p_value'] < 0.05 else ''}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "HARDNESS INTERACTION ANALYSIS\n",
      "======================================================================\n",
      "\n",
      "Correlation of bare NLL (hardness) with:\n",
      "  Oracle benefit:  r = 0.1569, p = 3.63e-14\n",
      "  Random benefit:  r = 0.1927, p = 1.08e-20\n",
      "\n",
      "Quartile breakdown (by bare NLL hardness):\n",
      "Quartile            N   Bare NLL   Oracle Δ   Oracle d  Oracle Win%   Random Δ\n",
      "--------------------------------------------------------------------------------\n",
      "Q1 (easy)         576      0.086    -0.0331     -0.195        37.8%     0.0014\n",
      "Q2                576      0.417    -0.0213     -0.127        45.7%     0.0132\n",
      "Q3                575      0.943     0.0114      0.044        56.7%     0.0303\n",
      "Q4 (hard)         576      3.136     0.0775      0.121        59.7%     0.0690\n",
      "\n",
      "Plot saved to results/exp01/hardness_interaction.png\n"
     ]
    }
   ],
   "source": [
    "# Cell 13: Hardness interaction analysis\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')  # Non-interactive backend\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"HARDNESS INTERACTION ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Correlation of bare NLL (hardness) with oracle benefit\n",
    "r_oracle, p_r_oracle = stats.pearsonr(bare_nlls, delta_oracle_bare)\n",
    "r_random, p_r_random = stats.pearsonr(bare_nlls, delta_random_bare)\n",
    "\n",
    "print(f\"\\nCorrelation of bare NLL (hardness) with:\")\n",
    "print(f\"  Oracle benefit:  r = {r_oracle:.4f}, p = {p_r_oracle:.2e}\")\n",
    "print(f\"  Random benefit:  r = {r_random:.4f}, p = {p_r_random:.2e}\")\n",
    "\n",
    "# Quartile breakdown\n",
    "quartiles = np.percentile(bare_nlls, [25, 50, 75])\n",
    "q_labels = ['Q1 (easy)', 'Q2', 'Q3', 'Q4 (hard)']\n",
    "q_bounds = [(-np.inf, quartiles[0]), (quartiles[0], quartiles[1]),\n",
    "            (quartiles[1], quartiles[2]), (quartiles[2], np.inf)]\n",
    "\n",
    "print(f\"\\nQuartile breakdown (by bare NLL hardness):\")\n",
    "print(f\"{'Quartile':<15} {'N':>5} {'Bare NLL':>10} {'Oracle Δ':>10} {'Oracle d':>10} {'Oracle Win%':>12} {'Random Δ':>10}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for label, (lo, hi) in zip(q_labels, q_bounds):\n",
    "    mask = (bare_nlls > lo) & (bare_nlls <= hi)\n",
    "    n_q = np.sum(mask)\n",
    "    mean_bare = np.mean(bare_nlls[mask])\n",
    "    oracle_delta_q = delta_oracle_bare[mask]\n",
    "    random_delta_q = delta_random_bare[mask]\n",
    "    d_q = cohens_d(oracle_delta_q) if n_q > 1 else 0\n",
    "    win_q = np.mean(oracle_delta_q > 0) * 100\n",
    "    print(f\"{label:<15} {n_q:>5} {mean_bare:>10.3f} {np.mean(oracle_delta_q):>10.4f} {d_q:>10.3f} {win_q:>11.1f}% {np.mean(random_delta_q):>10.4f}\")\n",
    "\n",
    "# Scatter plot: hardness vs oracle benefit\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].scatter(bare_nlls, delta_oracle_bare, alpha=0.15, s=5, c='steelblue')\n",
    "axes[0].axhline(y=0, color='red', linestyle='--', alpha=0.5)\n",
    "z = np.polyfit(bare_nlls, delta_oracle_bare, 1)\n",
    "p = np.poly1d(z)\n",
    "x_range = np.linspace(bare_nlls.min(), bare_nlls.max(), 100)\n",
    "axes[0].plot(x_range, p(x_range), 'r-', alpha=0.8, label=f'r={r_oracle:.3f}')\n",
    "axes[0].set_xlabel('Bare NLL (hardness)')\n",
    "axes[0].set_ylabel('Oracle benefit (positive = oracle helps)')\n",
    "axes[0].set_title('Hardness vs Oracle Benefit')\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].scatter(bare_nlls, delta_random_bare, alpha=0.15, s=5, c='orange')\n",
    "axes[1].axhline(y=0, color='red', linestyle='--', alpha=0.5)\n",
    "z2 = np.polyfit(bare_nlls, delta_random_bare, 1)\n",
    "p2 = np.poly1d(z2)\n",
    "axes[1].plot(x_range, p2(x_range), 'r-', alpha=0.8, label=f'r={r_random:.3f}')\n",
    "axes[1].set_xlabel('Bare NLL (hardness)')\n",
    "axes[1].set_ylabel('Random benefit (positive = random helps)')\n",
    "axes[1].set_title('Hardness vs Random Prefix Benefit')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'hardness_interaction.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"\\nPlot saved to {RESULTS_DIR / 'hardness_interaction.png'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plot saved to results/exp01/delta_distributions.png\n"
     ]
    }
   ],
   "source": [
    "# Cell 14: Delta distribution plots\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "plot_configs = [\n",
    "    ('Bare - Random (+ = random helps)', delta_random_bare, 'steelblue'),\n",
    "    ('Bare - Oracle (+ = oracle helps)', delta_oracle_bare, 'forestgreen'),\n",
    "    ('Random - Oracle (+ = oracle > random)', delta_oracle_random, 'darkorange'),\n",
    "]\n",
    "\n",
    "for ax, (title, delta, color) in zip(axes, plot_configs):\n",
    "    ax.hist(delta, bins=80, color=color, alpha=0.7, edgecolor='black', linewidth=0.3)\n",
    "    ax.axvline(x=0, color='red', linestyle='--', alpha=0.7)\n",
    "    ax.axvline(x=np.mean(delta), color='black', linestyle='-', alpha=0.8,\n",
    "               label=f'mean={np.mean(delta):.4f}')\n",
    "    ax.set_xlabel('Delta NLL')\n",
    "    ax.set_ylabel('Count')\n",
    "    ax.set_title(title)\n",
    "    ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'delta_distributions.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"Plot saved to {RESULTS_DIR / 'delta_distributions.png'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final results saved to results/exp01/results.json\n",
      "File size: 931.7 KB\n",
      "Total samples: 2500\n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Cell 15: Save final results JSON\n",
    "\n",
    "final_results = {\n",
    "    'experiment': 'exp01_first_principles_priming',\n",
    "    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'config': {\n",
    "        'model_name': config.model_name,\n",
    "        'num_samples': config.num_samples,\n",
    "        'seed': SEED,\n",
    "        'min_passage_words': config.min_passage_words,\n",
    "        'max_passage_words': config.max_passage_words,\n",
    "        'surrogate_prefix_template': SURROGATE_PREFIX_TEMPLATE,\n",
    "        'document_template': DOCUMENT_TEMPLATE,\n",
    "        'query_template': QUERY_TEMPLATE,\n",
    "        'answer_template': ANSWER_TEMPLATE,\n",
    "    },\n",
    "    'summary': {\n",
    "        'n_samples': len(results),\n",
    "        'bare_nll_mean': float(np.mean(bare_nlls)),\n",
    "        'bare_nll_std': float(np.std(bare_nlls)),\n",
    "        'random_nll_mean': float(np.mean(random_nlls)),\n",
    "        'random_nll_std': float(np.std(random_nlls)),\n",
    "        'oracle_nll_mean': float(np.mean(oracle_nlls)),\n",
    "        'oracle_nll_std': float(np.std(oracle_nlls)),\n",
    "        'comparisons': {\n",
    "            'bare_vs_random': {\n",
    "                'mean_delta': float(np.mean(delta_random_bare)),\n",
    "                'cohens_d': float(cohens_d(delta_random_bare)),\n",
    "                'win_rate': float(np.mean(delta_random_bare > 0)),\n",
    "                't_stat': float(stats.ttest_1samp(delta_random_bare, 0)[0]),\n",
    "                'p_value': float(stats.ttest_1samp(delta_random_bare, 0)[1]),\n",
    "            },\n",
    "            'bare_vs_oracle': {\n",
    "                'mean_delta': float(np.mean(delta_oracle_bare)),\n",
    "                'cohens_d': float(cohens_d(delta_oracle_bare)),\n",
    "                'win_rate': float(np.mean(delta_oracle_bare > 0)),\n",
    "                't_stat': float(stats.ttest_1samp(delta_oracle_bare, 0)[0]),\n",
    "                'p_value': float(stats.ttest_1samp(delta_oracle_bare, 0)[1]),\n",
    "            },\n",
    "            'random_vs_oracle': {\n",
    "                'mean_delta': float(np.mean(delta_oracle_random)),\n",
    "                'cohens_d': float(cohens_d(delta_oracle_random)),\n",
    "                'win_rate': float(np.mean(delta_oracle_random > 0)),\n",
    "                't_stat': float(stats.ttest_1samp(delta_oracle_random, 0)[0]),\n",
    "                'p_value': float(stats.ttest_1samp(delta_oracle_random, 0)[1]),\n",
    "            },\n",
    "        },\n",
    "        'hardness_interaction': {\n",
    "            'oracle_r': float(r_oracle),\n",
    "            'oracle_p': float(p_r_oracle),\n",
    "            'random_r': float(r_random),\n",
    "            'random_p': float(p_r_random),\n",
    "        },\n",
    "    },\n",
    "    'per_sample_results': results,\n",
    "}\n",
    "\n",
    "with open(FINAL_RESULTS_PATH, 'w') as f:\n",
    "    json.dump(final_results, f, indent=2)\n",
    "\n",
    "print(f\"Final results saved to {FINAL_RESULTS_PATH}\")\n",
    "print(f\"File size: {FINAL_RESULTS_PATH.stat().st_size / 1024:.1f} KB\")\n",
    "print(f\"Total samples: {len(results)}\")\n",
    "print(f\"\\nDone!\")"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cu124.m127",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/base-cu124:m127"
  },
  "kernelspec": {
   "display_name": "Python 3 (Local)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
