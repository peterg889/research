{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec2af14a",
   "metadata": {},
   "source": [
    "# Exp 21: Gemma Mechanism Robustness & Tuning\n",
    "\n",
    "## Motivation\n",
    "\n",
    "Exp 19 discovered that **layer-selective value contamination** (layers 0-16 only)\n",
    "amplifies Gemma's priming signal from d=+0.056 to d=**+0.211** (p=3.7e-15). This works\n",
    "because late-layer values (17-33) carry interference that cancels the early-layer benefit.\n",
    "\n",
    "Two critical questions remain:\n",
    "\n",
    "### Q1: Does this survive document length scaling?\n",
    "Exp 20 showed Mistral's full priming vanishes by ~256 tokens. Does Gemma's layer-selective\n",
    "approach also fail at long lengths, or does removing late-layer interference make it more robust?\n",
    "\n",
    "### Q2: Is cutoff=17 optimal?\n",
    "Exp 19 only tested one boundary (layers 0-16). Sweeping cutoffs may find a better split point.\n",
    "\n",
    "## Design\n",
    "\n",
    "### Part 1: Length Generalization \"Kryptonite\" Test\n",
    "- **N=500 queries** from MS MARCO v1.1 (positive passages only)\n",
    "- **Pad lengths**: `[None, 512, 1024, 2048]`\n",
    "- **Fixed cutoff**: layers 0-15 (`list(range(16))`)\n",
    "- **Method**: `replace_values_at_layers(bare_cache, sf_trunc_cache, list(range(16)))`\n",
    "\n",
    "### Part 2: Layer Boundary Sweep\n",
    "- **N=200 queries** (first 200 from Part 1)\n",
    "- **Cutoffs**: `[8, 12, 16, 20, 24]`\n",
    "- **No padding** (original passage length)\n",
    "\n",
    "## Reference Values\n",
    "\n",
    "| Source | Condition | d |\n",
    "|--------|-----------|---|\n",
    "| Exp 19 (Gemma) | values_only (all layers) | +0.056 |\n",
    "| Exp 19 (Gemma) | values_early_layers (0-16) | +0.211 |\n",
    "| Exp 20 (Mistral) | full priming @ original | +0.303 |\n",
    "| Exp 20 (Mistral) | full priming @ 256 tok | +0.114 (ns) |\n",
    "| Exp 20 (Mistral) | full priming @ 512 tok | +0.034 (ns) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "883a68b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T23:34:10.515017Z",
     "iopub.status.busy": "2026-02-15T23:34:10.514722Z",
     "iopub.status.idle": "2026-02-15T23:34:14.962962Z",
     "shell.execute_reply": "2026-02-15T23:34:14.961770Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEED: 42\n",
      "Results directory: results/exp21\n",
      "CUDA available: True\n",
      "GPU: NVIDIA L4\n",
      "GPU memory: 23.6 GB\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Setup\n",
    "import os\n",
    "os.umask(0o000)\n",
    "\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import gc\n",
    "from pathlib import Path\n",
    "\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "RESULTS_DIR = Path(\"results/exp21\")\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CHECKPOINT_P1_PATH = RESULTS_DIR / \"checkpoint_part1.json\"\n",
    "CHECKPOINT_P2_PATH = RESULTS_DIR / \"checkpoint_part2.json\"\n",
    "FINAL_RESULTS_PATH = RESULTS_DIR / \"results.json\"\n",
    "CSV_P1_PATH = RESULTS_DIR / \"part1_results.csv\"\n",
    "CSV_P2_PATH = RESULTS_DIR / \"part2_results.csv\"\n",
    "\n",
    "print(f\"SEED: {SEED}\")\n",
    "print(f\"Results directory: {RESULTS_DIR}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d1552fa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T23:34:14.967405Z",
     "iopub.status.busy": "2026-02-15T23:34:14.966397Z",
     "iopub.status.idle": "2026-02-15T23:34:34.213314Z",
     "shell.execute_reply": "2026-02-15T23:34:34.212354Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading google/gemma-3-4b-it (4-bit, bfloat16)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f29e28eae5c44068a744d43b3180795e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/883 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model loaded successfully.\n",
      "  Model class: Gemma3ForConditionalGeneration\n",
      "  Text config class: Gemma3TextConfig\n",
      "  Hidden size: 2560\n",
      "  Num layers: 34\n",
      "  Num attention heads: 8\n",
      "  Num KV heads: 4\n",
      "  Head dim: 256\n",
      "  BOS token ID: 2\n",
      "  Unique RoPE thetas: [10000.0, 1000000.0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Cache key dtype: torch.bfloat16\n",
      "  Cache key shape: torch.Size([1, 4, 2, 256])  (batch, kv_heads, seq, head_dim)\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Load Gemma 3 4B via load_model()\n",
    "sys.path.insert(0, \".\")\n",
    "\n",
    "from lib.config import ExperimentConfig\n",
    "from lib.model_utils import load_model\n",
    "\n",
    "MODEL_NAME = \"google/gemma-3-4b-it\"\n",
    "\n",
    "exp_config = ExperimentConfig(\n",
    "    model_name=MODEL_NAME,\n",
    "    model_type=\"gemma3\",\n",
    "    compute_dtype=\"auto\",  # resolves to bfloat16 for gemma3\n",
    "    use_4bit=True,\n",
    "    num_samples=2000,\n",
    "    seed=SEED,\n",
    ")\n",
    "\n",
    "print(f\"Loading {MODEL_NAME} (4-bit, bfloat16)...\")\n",
    "model, tokenizer = load_model(exp_config)\n",
    "\n",
    "# Architecture diagnostics\n",
    "from lib.kv_cache import _get_text_config, _get_head_dim, _get_rope_theta_for_layer, _get_cache_keys, _ensure_dynamic_cache\n",
    "\n",
    "text_config = _get_text_config(model.config)\n",
    "print(f\"\\nModel loaded successfully.\")\n",
    "print(f\"  Model class: {type(model).__name__}\")\n",
    "print(f\"  Text config class: {type(text_config).__name__}\")\n",
    "print(f\"  Hidden size: {text_config.hidden_size}\")\n",
    "print(f\"  Num layers: {text_config.num_hidden_layers}\")\n",
    "print(f\"  Num attention heads: {text_config.num_attention_heads}\")\n",
    "print(f\"  Num KV heads: {text_config.num_key_value_heads}\")\n",
    "print(f\"  Head dim: {_get_head_dim(model.config)}\")\n",
    "print(f\"  BOS token ID: {tokenizer.bos_token_id}\")\n",
    "\n",
    "# Per-layer RoPE diagnostics\n",
    "thetas = set()\n",
    "for layer_idx in range(text_config.num_hidden_layers):\n",
    "    thetas.add(_get_rope_theta_for_layer(model.config, layer_idx))\n",
    "print(f\"  Unique RoPE thetas: {sorted(thetas)}\")\n",
    "\n",
    "# Verify dtype\n",
    "sample_ids = tokenizer(\"test\", return_tensors=\"pt\")['input_ids'].to(exp_config.device)\n",
    "with torch.no_grad():\n",
    "    out = model(sample_ids, use_cache=True)\n",
    "    cache_check = _ensure_dynamic_cache(out.past_key_values)\n",
    "    k0 = _get_cache_keys(cache_check, 0)\n",
    "    print(f\"  Cache key dtype: {k0.dtype}\")\n",
    "    print(f\"  Cache key shape: {k0.shape}  (batch, kv_heads, seq, head_dim)\")\n",
    "del out, sample_ids\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a7b9b42",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T23:34:34.217615Z",
     "iopub.status.busy": "2026-02-15T23:34:34.216636Z",
     "iopub.status.idle": "2026-02-15T23:34:34.226009Z",
     "shell.execute_reply": "2026-02-15T23:34:34.225193Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config ready\n",
      "  Model: google/gemma-3-4b-it\n",
      "  Part 1: N=500, pad_lengths=[None, 256, 512, 900], cutoff=16\n",
      "  Part 2: N=200, cutoffs=[8, 12, 16, 20, 24], no padding\n",
      "  Static fact prefix: 'What are the key facts I need to know?'\n",
      "\n",
      "Exp 19 Gemma reference:\n",
      "    values_only_d: +0.056\n",
      "    values_early_layers_d: +0.211\n",
      "\n",
      "Exp 20 Mistral reference:\n",
      "    original_d: +0.303\n",
      "    256_d: +0.114\n",
      "    512_d: +0.034\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Lib imports + templates + constants\n",
    "from lib.kv_cache import (\n",
    "    _get_cache_keys,\n",
    "    _get_cache_values,\n",
    "    _set_cache_keys,\n",
    "    _set_cache_values,\n",
    "    _ensure_dynamic_cache,\n",
    "    extract_and_truncate_cache_with_bos,\n",
    "    correct_rope_positions_with_bos,\n",
    "    score_answer_with_cache,\n",
    "    deepcopy_cache,\n",
    "    replace_values_at_layers,\n",
    "    _get_text_config,\n",
    ")\n",
    "from lib.analysis import cohens_d\n",
    "from lib.data import count_words\n",
    "from scipy import stats\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Templates — bare text, no \"Document:\\n\" framing\n",
    "SURROGATE_PREFIX_TEMPLATE = \"{surrogate}\\n\"\n",
    "DOCUMENT_TEMPLATE = \"{document}\"\n",
    "QUERY_TEMPLATE = \"\\nQuery: {query}\\nAnswer:\"\n",
    "ANSWER_TEMPLATE = \" {answer}\"\n",
    "\n",
    "# Prefix text\n",
    "from lib.surrogate import STATIC_SURROGATE_QUERIES\n",
    "STATIC_FACT = STATIC_SURROGATE_QUERIES['static_factual']['query']\n",
    "\n",
    "# Experiment parameters\n",
    "N_PART1 = 500\n",
    "N_PART2 = 200  # first 200 from Part 1\n",
    "MAX_PASSAGE_WORDS = 300\n",
    "CHECKPOINT_EVERY = 25\n",
    "\n",
    "# Part 1: Length generalization\n",
    "# Gemma 3 sliding_window=1024 → max safe cache = 1023 positions (incl. BOS).\n",
    "# Beyond this, sliding attention layers truncate their cache, breaking our\n",
    "# cross-cache value substitution. Cap at 900 tokens (+ BOS = 901 < 1024).\n",
    "PAD_LENGTHS = [None, 256, 512, 900]\n",
    "\n",
    "# Part 2: Layer boundary sweep\n",
    "CUTOFFS = [8, 12, 16, 20, 24]\n",
    "\n",
    "# Fixed cutoff for Part 1 (layers 0-15 = 16 layers)\n",
    "FIXED_CUTOFF = 16\n",
    "\n",
    "# Reference values\n",
    "EXP19_REF = {\n",
    "    'values_only_d': 0.056,\n",
    "    'values_early_layers_d': 0.211,  # layers 0-16 (17 layers)\n",
    "}\n",
    "EXP20_REF = {\n",
    "    'original_d': 0.303,\n",
    "    '256_d': 0.114,\n",
    "    '512_d': 0.034,\n",
    "}\n",
    "\n",
    "print(\"Config ready\")\n",
    "print(f\"  Model: {MODEL_NAME}\")\n",
    "print(f\"  Part 1: N={N_PART1}, pad_lengths={PAD_LENGTHS}, cutoff={FIXED_CUTOFF}\")\n",
    "print(f\"  Part 2: N={N_PART2}, cutoffs={CUTOFFS}, no padding\")\n",
    "print(f\"  Static fact prefix: '{STATIC_FACT}'\")\n",
    "print(f\"\\nExp 19 Gemma reference:\")\n",
    "for k, v in EXP19_REF.items():\n",
    "    print(f\"    {k}: {v:+.3f}\")\n",
    "print(f\"\\nExp 20 Mistral reference:\")\n",
    "for k, v in EXP20_REF.items():\n",
    "    print(f\"    {k}: {v:+.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab125079",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T23:34:34.229342Z",
     "iopub.status.busy": "2026-02-15T23:34:34.228708Z",
     "iopub.status.idle": "2026-02-15T23:35:32.512833Z",
     "shell.execute_reply": "2026-02-15T23:35:32.511950Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`trust_remote_code` is not supported anymore.\n",
      "Please check that the Hugging Face dataset 'microsoft/ms_marco' isn't based on a loading script and remove `trust_remote_code`.\n",
      "If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "LOADING MS MARCO v1.1 — POSITIVE PASSAGES ONLY\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total items in validation: 10047\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4d7aebecb1e47c0bebb602e5f16d23e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filtering:   0%|          | 0/10047 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Selected 500 queries with positive passages\n",
      "Word counts: mean=72, min=15, max=157\n",
      "\n",
      "Padding pool passages: 80,814\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padding pool tokens: 7,643,150\n",
      "Max tokens needed: 450,000\n",
      "Pool is 17.0x the max needed. OK.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "94"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 4: Load MS MARCO v1.1, filter positive passages, build padding pool\n",
    "from datasets import load_dataset\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"LOADING MS MARCO v1.1 — POSITIVE PASSAGES ONLY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "dataset = load_dataset(\"microsoft/ms_marco\", \"v1.1\", split=\"validation\",\n",
    "                        trust_remote_code=True)\n",
    "print(f\"Total items in validation: {len(dataset)}\")\n",
    "\n",
    "queries = []\n",
    "padding_passages = []\n",
    "eval_passage_set = set()\n",
    "np.random.seed(SEED)\n",
    "\n",
    "for item in tqdm(dataset, desc=\"Filtering\"):\n",
    "    passages_info = item.get('passages', {})\n",
    "    passage_texts = passages_info.get('passage_text', [])\n",
    "    is_selected = passages_info.get('is_selected', [])\n",
    "    query = item.get('query', '')\n",
    "    answers = item.get('answers', [])\n",
    "    well_formed = item.get('wellFormedAnswers', [])\n",
    "\n",
    "    if not passage_texts or not query:\n",
    "        continue\n",
    "\n",
    "    # Get best answer\n",
    "    answer = None\n",
    "    if well_formed and len(well_formed) > 0 and well_formed[0] != '[]':\n",
    "        answer = well_formed[0]\n",
    "    elif answers and len(answers) > 0 and answers[0] != 'No Answer Present.':\n",
    "        answer = answers[0]\n",
    "    else:\n",
    "        # Still collect non-eval passages for padding pool\n",
    "        for p in passage_texts:\n",
    "            if count_words(p) <= MAX_PASSAGE_WORDS:\n",
    "                padding_passages.append(p)\n",
    "        continue\n",
    "\n",
    "    # Find positive passage(s)\n",
    "    found_positive = False\n",
    "    for i, (ptext, sel) in enumerate(zip(passage_texts, is_selected)):\n",
    "        if sel == 1 and count_words(ptext) <= MAX_PASSAGE_WORDS:\n",
    "            if len(queries) < N_PART1 * 3:  # collect 3x for shuffling\n",
    "                queries.append({\n",
    "                    'query': query,\n",
    "                    'answer': answer,\n",
    "                    'passage': ptext,\n",
    "                    'word_count': count_words(ptext),\n",
    "                })\n",
    "                eval_passage_set.add(ptext)\n",
    "                found_positive = True\n",
    "                break\n",
    "\n",
    "    # Collect non-eval passages for padding pool\n",
    "    for p in passage_texts:\n",
    "        if p not in eval_passage_set and count_words(p) <= MAX_PASSAGE_WORDS:\n",
    "            padding_passages.append(p)\n",
    "\n",
    "np.random.shuffle(queries)\n",
    "queries = queries[:N_PART1]\n",
    "N = len(queries)\n",
    "\n",
    "print(f\"\\nSelected {N} queries with positive passages\")\n",
    "print(f\"Word counts: mean={np.mean([q['word_count'] for q in queries]):.0f}, \"\n",
    "      f\"min={min(q['word_count'] for q in queries)}, \"\n",
    "      f\"max={max(q['word_count'] for q in queries)}\")\n",
    "\n",
    "# Build padding pool (pre-tokenize)\n",
    "print(f\"\\nPadding pool passages: {len(padding_passages):,}\")\n",
    "padding_text = ' '.join(padding_passages)\n",
    "padding_ids = tokenizer.encode(padding_text, add_special_tokens=False)\n",
    "print(f\"Padding pool tokens: {len(padding_ids):,}\")\n",
    "\n",
    "max_needed = max(tl for tl in PAD_LENGTHS if tl is not None) * N_PART1\n",
    "print(f\"Max tokens needed: {max_needed:,}\")\n",
    "assert len(padding_ids) > max_needed, (\n",
    "    f\"Padding pool too small: {len(padding_ids):,} < {max_needed:,}\"\n",
    ")\n",
    "print(f\"Pool is {len(padding_ids) / max_needed:.1f}x the max needed. OK.\")\n",
    "\n",
    "del dataset\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41512599",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T23:35:32.516368Z",
     "iopub.status.busy": "2026-02-15T23:35:32.515848Z",
     "iopub.status.idle": "2026-02-15T23:35:32.528416Z",
     "shell.execute_reply": "2026-02-15T23:35:32.527534Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PREFIX TOKENIZATION — GEMMA 3 4B\n",
      "======================================================================\n",
      "\n",
      "Static fact prefix: 'What are the key facts I need to know?'\n",
      "  Formatted: 'What are the key facts I need to know?'\n",
      "  Token length: 11\n",
      "\n",
      "BPE BOUNDARY CHECK (first passage):\n",
      "  static_fact: 54/54 tokens match (100.0%)\n",
      "\n",
      "======================================================================\n",
      "EXPERIMENTAL CONDITIONS\n",
      "======================================================================\n",
      "\n",
      "### Part 1: Length Generalization ###\n",
      "  Fixed cutoff: 16 layers (layers 0-15)\n",
      "  Pad lengths: [None, 256, 512, 900]\n",
      "  Per sample per length:\n",
      "    1. Pad doc_ids to target length from pre-tokenized padding pool\n",
      "    2. Forward pass 1 (bare):    [BOS][padded_doc_ids]\n",
      "    3. Forward pass 2 (primed):  [BOS][sf_prefix_ids][padded_doc_ids]\n",
      "       -> truncate prefix -> RoPE correct\n",
      "    4. values_early_layers = replace_values_at_layers(bare, sf_trunc, range(16))\n",
      "    5. Score bare + values_early_layers with deepcopy_cache()\n",
      "\n",
      "### Part 2: Layer Boundary Sweep ###\n",
      "  Cutoffs: [8, 12, 16, 20, 24]\n",
      "  No padding (original passage length)\n",
      "  Per sample:\n",
      "    1. Forward pass (bare):    [BOS][doc_ids]\n",
      "    2. Forward pass (primed):  [BOS][sf_prefix_ids][doc_ids]\n",
      "       -> truncate prefix -> RoPE correct\n",
      "    3. Score bare once\n",
      "    4. For each cutoff in [8, 12, 16, 20, 24]:\n",
      "       vel_cache = replace_values_at_layers(bare, sf_trunc, range(cutoff))\n",
      "       Score vel_cache\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Tokenize prefix and verify BPE boundaries\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PREFIX TOKENIZATION — GEMMA 3 4B\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "sf_str = SURROGATE_PREFIX_TEMPLATE.format(surrogate=STATIC_FACT)\n",
    "\n",
    "sf_ids = tokenizer(sf_str, return_tensors=\"pt\",\n",
    "                    add_special_tokens=False)['input_ids'].to(exp_config.device)\n",
    "\n",
    "print(f\"\\nStatic fact prefix: '{STATIC_FACT}'\")\n",
    "print(f\"  Formatted: '{sf_str.strip()}'\")\n",
    "print(f\"  Token length: {sf_ids.shape[1]}\")\n",
    "\n",
    "# Verify BPE boundary consistency\n",
    "print(\"\\nBPE BOUNDARY CHECK (first passage):\")\n",
    "example_doc = queries[0]['passage']\n",
    "concat = sf_str + DOCUMENT_TEMPLATE.format(document=example_doc)\n",
    "concat_enc = tokenizer(concat, add_special_tokens=True)['input_ids']\n",
    "prefix_enc = tokenizer(sf_str, add_special_tokens=True)['input_ids']\n",
    "doc_ids_from_concat = concat_enc[len(prefix_enc):]\n",
    "\n",
    "bare_doc_enc = tokenizer(DOCUMENT_TEMPLATE.format(document=example_doc),\n",
    "                          add_special_tokens=False)['input_ids']\n",
    "match = sum(1 for a, b in zip(doc_ids_from_concat, bare_doc_enc) if a == b)\n",
    "total = max(len(bare_doc_enc), 1)\n",
    "print(f\"  static_fact: {match}/{total} tokens match ({100*match/total:.1f}%)\")\n",
    "\n",
    "# Condition explanations\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"EXPERIMENTAL CONDITIONS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\n### Part 1: Length Generalization ###\")\n",
    "print(f\"  Fixed cutoff: {FIXED_CUTOFF} layers (layers 0-{FIXED_CUTOFF-1})\")\n",
    "print(f\"  Pad lengths: {PAD_LENGTHS}\")\n",
    "print(\"  Per sample per length:\")\n",
    "print(\"    1. Pad doc_ids to target length from pre-tokenized padding pool\")\n",
    "print(\"    2. Forward pass 1 (bare):    [BOS][padded_doc_ids]\")\n",
    "print(\"    3. Forward pass 2 (primed):  [BOS][sf_prefix_ids][padded_doc_ids]\")\n",
    "print(\"       -> truncate prefix -> RoPE correct\")\n",
    "print(f\"    4. values_early_layers = replace_values_at_layers(bare, sf_trunc, range({FIXED_CUTOFF}))\")\n",
    "print(\"    5. Score bare + values_early_layers with deepcopy_cache()\")\n",
    "\n",
    "print(\"\\n### Part 2: Layer Boundary Sweep ###\")\n",
    "print(f\"  Cutoffs: {CUTOFFS}\")\n",
    "print(\"  No padding (original passage length)\")\n",
    "print(\"  Per sample:\")\n",
    "print(\"    1. Forward pass (bare):    [BOS][doc_ids]\")\n",
    "print(\"    2. Forward pass (primed):  [BOS][sf_prefix_ids][doc_ids]\")\n",
    "print(\"       -> truncate prefix -> RoPE correct\")\n",
    "print(\"    3. Score bare once\")\n",
    "print(f\"    4. For each cutoff in {CUTOFFS}:\")\n",
    "print(\"       vel_cache = replace_values_at_layers(bare, sf_trunc, range(cutoff))\")\n",
    "print(\"       Score vel_cache\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06d656e0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T23:35:32.531754Z",
     "iopub.status.busy": "2026-02-15T23:35:32.531467Z",
     "iopub.status.idle": "2026-02-15T23:35:32.566220Z",
     "shell.execute_reply": "2026-02-15T23:35:32.565354Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PART 1: LENGTH GENERALIZATION (500 queries, 4 lengths)\n",
      "Fixed cutoff: 16 layers\n",
      "======================================================================\n",
      "Resuming from checkpoint: 500/500\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76dc5292439c4ab6a3ba607344fda40a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Part 1: 100%|##########| 500/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Part 1 complete: 500 queries in 0.0 min\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Part 1 — Length Generalization (N=500, 4 pad lengths, cutoff=16)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"PART 1: LENGTH GENERALIZATION ({N_PART1} queries, {len(PAD_LENGTHS)} lengths)\")\n",
    "print(f\"Fixed cutoff: {FIXED_CUTOFF} layers\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Checkpoint resume\n",
    "p1_results = []\n",
    "p1_start_idx = 0\n",
    "\n",
    "if CHECKPOINT_P1_PATH.exists():\n",
    "    with open(CHECKPOINT_P1_PATH, 'r') as f:\n",
    "        ckpt = json.load(f)\n",
    "    ckpt_queries = ckpt.get('query_texts', [])\n",
    "    current_queries = [q['query'] for q in queries[:N_PART1]]\n",
    "    if ckpt_queries == current_queries:\n",
    "        p1_results = ckpt['results']\n",
    "        p1_start_idx = len(p1_results)\n",
    "        print(f\"Resuming from checkpoint: {p1_start_idx}/{N_PART1}\")\n",
    "    else:\n",
    "        print(\"Checkpoint query mismatch. Starting fresh.\")\n",
    "else:\n",
    "    print(\"No checkpoint found. Starting fresh.\")\n",
    "\n",
    "layer_indices = list(range(FIXED_CUTOFF))\n",
    "t_start = time.time()\n",
    "\n",
    "for qidx in tqdm(range(p1_start_idx, N_PART1), initial=p1_start_idx, total=N_PART1,\n",
    "                  desc=\"Part 1\"):\n",
    "    qdata = queries[qidx]\n",
    "    query_prompt = QUERY_TEMPLATE.format(query=qdata['query'])\n",
    "    answer_text = ANSWER_TEMPLATE.format(answer=qdata['answer'])\n",
    "    passage = qdata['passage']\n",
    "    document_text = DOCUMENT_TEMPLATE.format(document=passage)\n",
    "\n",
    "    # Matched tokenization: tokenize concatenated then split\n",
    "    full_text = sf_str + document_text\n",
    "    full_enc = tokenizer(full_text, return_tensors=\"pt\",\n",
    "                          add_special_tokens=True, padding=False, truncation=False)\n",
    "    full_ids = full_enc['input_ids'].to(exp_config.device)\n",
    "\n",
    "    sf_prefix_enc = tokenizer(sf_str, return_tensors=\"pt\",\n",
    "                               add_special_tokens=True, padding=False, truncation=False)\n",
    "    sf_prefix_len_matched = sf_prefix_enc['input_ids'].shape[1]\n",
    "\n",
    "    bos_id = full_ids[:, :1]\n",
    "    base_doc_ids = full_ids[:, sf_prefix_len_matched:]\n",
    "    base_doc_len = base_doc_ids.shape[1]\n",
    "\n",
    "    del full_enc, full_ids, sf_prefix_enc\n",
    "\n",
    "    query_rows = []\n",
    "\n",
    "    for pad_length in PAD_LENGTHS:\n",
    "        # Pad doc_ids at token level if needed\n",
    "        if pad_length is not None and base_doc_len < pad_length:\n",
    "            pad_needed = pad_length - base_doc_len\n",
    "            max_start = len(padding_ids) - pad_needed\n",
    "            start = np.random.randint(0, max_start)\n",
    "            pad_tensor = torch.tensor([padding_ids[start:start + pad_needed]],\n",
    "                                       device=exp_config.device)\n",
    "            doc_ids = torch.cat([base_doc_ids, pad_tensor], dim=1)\n",
    "        else:\n",
    "            doc_ids = base_doc_ids\n",
    "\n",
    "        doc_len = doc_ids.shape[1]\n",
    "        context_len = 1 + doc_len  # BOS + doc\n",
    "\n",
    "        # Forward pass 1: BARE\n",
    "        bare_input = torch.cat([bos_id, doc_ids], dim=1)\n",
    "        with torch.no_grad():\n",
    "            bare_out = model(input_ids=bare_input,\n",
    "                             attention_mask=torch.ones_like(bare_input),\n",
    "                             use_cache=True, return_dict=True)\n",
    "        bare_cache = _ensure_dynamic_cache(bare_out.past_key_values)\n",
    "        del bare_out\n",
    "\n",
    "        # Forward pass 2: PRIMED (static_fact prefix)\n",
    "        primed_input = torch.cat([bos_id, sf_ids, doc_ids], dim=1)\n",
    "        with torch.no_grad():\n",
    "            primed_out = model(input_ids=primed_input,\n",
    "                               attention_mask=torch.ones_like(primed_input),\n",
    "                               use_cache=True, return_dict=True)\n",
    "        primed_full = _ensure_dynamic_cache(primed_out.past_key_values)\n",
    "        del primed_out\n",
    "\n",
    "        # Truncate: keep [BOS] + [last doc_len positions]\n",
    "        trunc_raw = extract_and_truncate_cache_with_bos(primed_full, doc_len)\n",
    "        prefix_offset = sf_ids.shape[1]\n",
    "        del primed_full\n",
    "\n",
    "        # RoPE correct\n",
    "        sf_trunc_cache = deepcopy_cache(trunc_raw)\n",
    "        correct_rope_positions_with_bos(sf_trunc_cache, prefix_offset, model)\n",
    "        del trunc_raw\n",
    "\n",
    "        # Build values_early_layers: bare keys + primed values at layers 0-15\n",
    "        vel_cache = replace_values_at_layers(bare_cache, sf_trunc_cache, layer_indices)\n",
    "\n",
    "        # Score bare\n",
    "        bare_nll = score_answer_with_cache(\n",
    "            deepcopy_cache(bare_cache), context_len,\n",
    "            query_prompt, answer_text, model, tokenizer, exp_config)\n",
    "\n",
    "        # Score values_early_layers\n",
    "        vel_nll = score_answer_with_cache(\n",
    "            deepcopy_cache(vel_cache), context_len,\n",
    "            query_prompt, answer_text, model, tokenizer, exp_config)\n",
    "\n",
    "        del bare_cache, sf_trunc_cache, vel_cache, bare_input, primed_input\n",
    "        if pad_length is not None and base_doc_len < pad_length:\n",
    "            del pad_tensor\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        pad_label = \"original\" if pad_length is None else str(pad_length)\n",
    "        query_rows.append({\n",
    "            'query_idx': qidx,\n",
    "            'layer_cutoff': FIXED_CUTOFF,\n",
    "            'pad_length': pad_label,\n",
    "            'actual_doc_len': doc_len,\n",
    "            'unprimed_nll': bare_nll,\n",
    "            'primed_nll': vel_nll,\n",
    "            'delta_nll': bare_nll - vel_nll,\n",
    "        })\n",
    "\n",
    "    p1_results.append({\n",
    "        'query_idx': qidx,\n",
    "        'query': qdata['query'],\n",
    "        'base_doc_len': base_doc_len,\n",
    "        'rows': query_rows,\n",
    "    })\n",
    "\n",
    "    # Checkpoint\n",
    "    if (qidx + 1) % CHECKPOINT_EVERY == 0 or qidx == N_PART1 - 1:\n",
    "        ckpt_data = {\n",
    "            'results': p1_results,\n",
    "            'query_texts': [q['query'] for q in queries[:N_PART1]],\n",
    "            'completed': len(p1_results),\n",
    "            'total': N_PART1,\n",
    "            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        }\n",
    "        with open(CHECKPOINT_P1_PATH, 'w') as f:\n",
    "            json.dump(ckpt_data, f)\n",
    "        elapsed = time.time() - t_start\n",
    "        n_done = qidx - p1_start_idx + 1\n",
    "        rate = n_done / elapsed if elapsed > 0 else 0\n",
    "        remaining = (N_PART1 - qidx - 1) / rate if rate > 0 else 0\n",
    "        tqdm.write(f\"  Checkpoint {qidx+1}/{N_PART1} | {n_done} done in {elapsed/60:.1f}m | \"\n",
    "                   f\"ETA: {remaining/60:.1f} min\")\n",
    "\n",
    "elapsed_total = time.time() - t_start\n",
    "print(f\"\\nPart 1 complete: {len(p1_results)} queries in {elapsed_total/60:.1f} min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec824955",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T23:35:32.575958Z",
     "iopub.status.busy": "2026-02-15T23:35:32.575695Z",
     "iopub.status.idle": "2026-02-15T23:35:32.605048Z",
     "shell.execute_reply": "2026-02-15T23:35:32.604219Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PART 2: LAYER BOUNDARY SWEEP (200 queries, cutoffs=[8, 12, 16, 20, 24])\n",
      "======================================================================\n",
      "Resuming from checkpoint: 200/200\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "089feeb264264b3a86efa1a013f7a7b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Part 2: 100%|##########| 200/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Part 2 complete: 200 queries in 0.0 min\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Part 2 — Layer Boundary Sweep (N=200, 5 cutoffs, no padding)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"PART 2: LAYER BOUNDARY SWEEP ({N_PART2} queries, cutoffs={CUTOFFS})\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Checkpoint resume\n",
    "p2_results = []\n",
    "p2_start_idx = 0\n",
    "\n",
    "if CHECKPOINT_P2_PATH.exists():\n",
    "    with open(CHECKPOINT_P2_PATH, 'r') as f:\n",
    "        ckpt = json.load(f)\n",
    "    ckpt_queries = ckpt.get('query_texts', [])\n",
    "    current_queries = [q['query'] for q in queries[:N_PART2]]\n",
    "    if ckpt_queries == current_queries:\n",
    "        p2_results = ckpt['results']\n",
    "        p2_start_idx = len(p2_results)\n",
    "        print(f\"Resuming from checkpoint: {p2_start_idx}/{N_PART2}\")\n",
    "    else:\n",
    "        print(\"Checkpoint query mismatch. Starting fresh.\")\n",
    "else:\n",
    "    print(\"No checkpoint found. Starting fresh.\")\n",
    "\n",
    "t_start = time.time()\n",
    "\n",
    "for qidx in tqdm(range(p2_start_idx, N_PART2), initial=p2_start_idx, total=N_PART2,\n",
    "                  desc=\"Part 2\"):\n",
    "    qdata = queries[qidx]\n",
    "    query_prompt = QUERY_TEMPLATE.format(query=qdata['query'])\n",
    "    answer_text = ANSWER_TEMPLATE.format(answer=qdata['answer'])\n",
    "    passage = qdata['passage']\n",
    "    document_text = DOCUMENT_TEMPLATE.format(document=passage)\n",
    "\n",
    "    # Matched tokenization\n",
    "    full_text = sf_str + document_text\n",
    "    full_enc = tokenizer(full_text, return_tensors=\"pt\",\n",
    "                          add_special_tokens=True, padding=False, truncation=False)\n",
    "    full_ids = full_enc['input_ids'].to(exp_config.device)\n",
    "\n",
    "    sf_prefix_enc = tokenizer(sf_str, return_tensors=\"pt\",\n",
    "                               add_special_tokens=True, padding=False, truncation=False)\n",
    "    sf_prefix_len_matched = sf_prefix_enc['input_ids'].shape[1]\n",
    "\n",
    "    bos_id = full_ids[:, :1]\n",
    "    doc_ids = full_ids[:, sf_prefix_len_matched:]\n",
    "    doc_len = doc_ids.shape[1]\n",
    "    context_len = 1 + doc_len\n",
    "\n",
    "    del full_enc, full_ids, sf_prefix_enc\n",
    "\n",
    "    # Forward pass 1: BARE\n",
    "    bare_input = torch.cat([bos_id, doc_ids], dim=1)\n",
    "    with torch.no_grad():\n",
    "        bare_out = model(input_ids=bare_input,\n",
    "                         attention_mask=torch.ones_like(bare_input),\n",
    "                         use_cache=True, return_dict=True)\n",
    "    bare_cache = _ensure_dynamic_cache(bare_out.past_key_values)\n",
    "    del bare_out\n",
    "\n",
    "    # Forward pass 2: PRIMED\n",
    "    primed_input = torch.cat([bos_id, sf_ids, doc_ids], dim=1)\n",
    "    with torch.no_grad():\n",
    "        primed_out = model(input_ids=primed_input,\n",
    "                           attention_mask=torch.ones_like(primed_input),\n",
    "                           use_cache=True, return_dict=True)\n",
    "    primed_full = _ensure_dynamic_cache(primed_out.past_key_values)\n",
    "    del primed_out\n",
    "\n",
    "    # Truncate + RoPE correct\n",
    "    trunc_raw = extract_and_truncate_cache_with_bos(primed_full, doc_len)\n",
    "    prefix_offset = sf_ids.shape[1]\n",
    "    del primed_full\n",
    "\n",
    "    sf_trunc_cache = deepcopy_cache(trunc_raw)\n",
    "    correct_rope_positions_with_bos(sf_trunc_cache, prefix_offset, model)\n",
    "    del trunc_raw\n",
    "\n",
    "    # Score bare once\n",
    "    bare_nll = score_answer_with_cache(\n",
    "        deepcopy_cache(bare_cache), context_len,\n",
    "        query_prompt, answer_text, model, tokenizer, exp_config)\n",
    "\n",
    "    query_rows = []\n",
    "\n",
    "    # Score each cutoff\n",
    "    for cutoff in CUTOFFS:\n",
    "        vel_cache = replace_values_at_layers(\n",
    "            bare_cache, sf_trunc_cache, list(range(cutoff)))\n",
    "        vel_nll = score_answer_with_cache(\n",
    "            deepcopy_cache(vel_cache), context_len,\n",
    "            query_prompt, answer_text, model, tokenizer, exp_config)\n",
    "        del vel_cache\n",
    "\n",
    "        query_rows.append({\n",
    "            'query_idx': qidx,\n",
    "            'layer_cutoff': cutoff,\n",
    "            'pad_length': 'none',\n",
    "            'actual_doc_len': doc_len,\n",
    "            'unprimed_nll': bare_nll,\n",
    "            'primed_nll': vel_nll,\n",
    "            'delta_nll': bare_nll - vel_nll,\n",
    "        })\n",
    "\n",
    "    del bare_cache, sf_trunc_cache, bare_input, primed_input\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    p2_results.append({\n",
    "        'query_idx': qidx,\n",
    "        'query': qdata['query'],\n",
    "        'doc_len': doc_len,\n",
    "        'bare_nll': bare_nll,\n",
    "        'rows': query_rows,\n",
    "    })\n",
    "\n",
    "    # Checkpoint\n",
    "    if (qidx + 1) % CHECKPOINT_EVERY == 0 or qidx == N_PART2 - 1:\n",
    "        ckpt_data = {\n",
    "            'results': p2_results,\n",
    "            'query_texts': [q['query'] for q in queries[:N_PART2]],\n",
    "            'completed': len(p2_results),\n",
    "            'total': N_PART2,\n",
    "            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        }\n",
    "        with open(CHECKPOINT_P2_PATH, 'w') as f:\n",
    "            json.dump(ckpt_data, f)\n",
    "        elapsed = time.time() - t_start\n",
    "        n_done = qidx - p2_start_idx + 1\n",
    "        rate = n_done / elapsed if elapsed > 0 else 0\n",
    "        remaining = (N_PART2 - qidx - 1) / rate if rate > 0 else 0\n",
    "        tqdm.write(f\"  Checkpoint {qidx+1}/{N_PART2} | {n_done} done in {elapsed/60:.1f}m | \"\n",
    "                   f\"ETA: {remaining/60:.1f} min\")\n",
    "\n",
    "elapsed_total = time.time() - t_start\n",
    "print(f\"\\nPart 2 complete: {len(p2_results)} queries in {elapsed_total/60:.1f} min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9d49f62",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T23:35:32.614405Z",
     "iopub.status.busy": "2026-02-15T23:35:32.614161Z",
     "iopub.status.idle": "2026-02-15T23:35:32.692326Z",
     "shell.execute_reply": "2026-02-15T23:35:32.691440Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PART 1 ANALYSIS: LENGTH GENERALIZATION\n",
      "======================================================================\n",
      "\n",
      "Pad Length       N  Mean Bare  Mean Primed     Mean D        d    Win%            p   sig\n",
      "------------------------------------------------------------------------------------------\n",
      "original       455     0.5729       0.5260    +0.0470   +0.227   59.8%     1.76e-06   ***\n",
      "256            455     0.5560       0.5137    +0.0423   +0.227   53.6%     1.77e-06   ***\n",
      "512            455     0.5679       0.5386    +0.0293   +0.173   56.5%     2.51e-04   ***\n",
      "900            455     0.5697       0.5514    +0.0183   +0.085   55.6%     6.94e-02    ns\n",
      "\n",
      "======================================================================\n",
      "COMPARISON: Gemma layer-selective vs Mistral full priming (Exp 20)\n",
      "======================================================================\n",
      "\n",
      "Length        Gemma VEL d  Mistral d     Diff\n",
      "------------------------------------------------\n",
      "original           +0.227     +0.303   -0.076\n",
      "256                +0.227     +0.114   +0.113\n",
      "512                +0.173     +0.034   +0.139\n",
      "\n",
      "VERDICT: Mixed: d goes from +0.227 (original) to +0.085 (900). See detailed numbers above.\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Part 1 Analysis — d vs length, statistical tests\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PART 1 ANALYSIS: LENGTH GENERALIZATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Collect per-sample deltas by pad_length\n",
    "p1_deltas = {}\n",
    "p1_bare = {}\n",
    "p1_primed = {}\n",
    "for pl in ['original', '256', '512', '900']:\n",
    "    p1_deltas[pl] = []\n",
    "    p1_bare[pl] = []\n",
    "    p1_primed[pl] = []\n",
    "\n",
    "for r in p1_results:\n",
    "    for row in r['rows']:\n",
    "        pl = row['pad_length']\n",
    "        if pl in p1_deltas:\n",
    "            p1_deltas[pl].append(row['delta_nll'])\n",
    "            p1_bare[pl].append(row['unprimed_nll'])\n",
    "            p1_primed[pl].append(row['primed_nll'])\n",
    "\n",
    "# Convert to arrays and filter zeros\n",
    "p1_arrays = {}\n",
    "for pl in p1_deltas:\n",
    "    bare = np.array(p1_bare[pl])\n",
    "    primed = np.array(p1_primed[pl])\n",
    "    delta = np.array(p1_deltas[pl])\n",
    "    valid = (bare != 0) & (primed != 0) & np.isfinite(bare) & np.isfinite(primed)\n",
    "    p1_arrays[pl] = {\n",
    "        'bare': bare[valid],\n",
    "        'primed': primed[valid],\n",
    "        'delta': delta[valid],\n",
    "        'n_valid': int(np.sum(valid)),\n",
    "    }\n",
    "\n",
    "# Summary table\n",
    "print(f\"\\n{'Pad Length':<12} {'N':>5} {'Mean Bare':>10} {'Mean Primed':>12} \"\n",
    "      f\"{'Mean D':>10} {'d':>8} {'Win%':>7} {'p':>12} {'sig':>5}\")\n",
    "print(\"-\" * 90)\n",
    "\n",
    "p1_analysis = {}\n",
    "for pl in ['original', '256', '512', '900']:\n",
    "    a = p1_arrays[pl]\n",
    "    d = cohens_d(a['delta'])\n",
    "    win = np.mean(a['delta'] > 0) * 100\n",
    "    t_stat, p_val = stats.ttest_1samp(a['delta'], 0)\n",
    "    sig = '***' if p_val < 0.001 else '**' if p_val < 0.01 else '*' if p_val < 0.05 else 'ns'\n",
    "    print(f\"{pl:<12} {a['n_valid']:>5} {np.mean(a['bare']):>10.4f} \"\n",
    "          f\"{np.mean(a['primed']):>12.4f} {np.mean(a['delta']):>+10.4f} \"\n",
    "          f\"{d:>+8.3f} {win:>6.1f}% {p_val:>12.2e} {sig:>5}\")\n",
    "    p1_analysis[pl] = {\n",
    "        'n_valid': a['n_valid'],\n",
    "        'mean_bare': float(np.mean(a['bare'])),\n",
    "        'mean_primed': float(np.mean(a['primed'])),\n",
    "        'mean_delta': float(np.mean(a['delta'])),\n",
    "        'cohens_d': float(d),\n",
    "        'win_pct': float(win),\n",
    "        't_stat': float(t_stat),\n",
    "        'p_value': float(p_val),\n",
    "    }\n",
    "\n",
    "# Exp 20 comparison\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"COMPARISON: Gemma layer-selective vs Mistral full priming (Exp 20)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "exp20_lengths = ['original', '256', '512']\n",
    "exp20_ds = [EXP20_REF.get(f'{pl}_d', EXP20_REF.get('original_d', 0))\n",
    "            for pl in exp20_lengths]\n",
    "\n",
    "print(f\"\\n{'Length':<12} {'Gemma VEL d':>12} {'Mistral d':>10} {'Diff':>8}\")\n",
    "print(\"-\" * 48)\n",
    "for pl in exp20_lengths:\n",
    "    if pl in p1_analysis:\n",
    "        gemma_d = p1_analysis[pl]['cohens_d']\n",
    "        mistral_d = EXP20_REF.get(f'{pl}_d', EXP20_REF.get('original_d', 0))\n",
    "        print(f\"{pl:<12} {gemma_d:>+12.3f} {mistral_d:>+10.3f} {gemma_d - mistral_d:>+8.3f}\")\n",
    "\n",
    "# Verdict (compare original to 900, the max safe length for Gemma's sliding window)\n",
    "d_orig = p1_analysis['original']['cohens_d']\n",
    "d_max = p1_analysis['900']['cohens_d']\n",
    "if d_orig > 0.1 and d_max < 0.05:\n",
    "    p1_verdict = (\"Layer-selective values ARE NOT robust to length. \"\n",
    "                  f\"d decays from {d_orig:+.3f} at original to {d_max:+.3f} at 900 tokens.\")\n",
    "elif d_max > 0.1:\n",
    "    p1_verdict = (\"Layer-selective values ARE robust to length! \"\n",
    "                  f\"d remains {d_max:+.3f} even at 900 tokens (vs {d_orig:+.3f} at original).\")\n",
    "else:\n",
    "    p1_verdict = (f\"Mixed: d goes from {d_orig:+.3f} (original) to {d_max:+.3f} (900). \"\n",
    "                  \"See detailed numbers above.\")\n",
    "\n",
    "print(f\"\\nVERDICT: {p1_verdict}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d08ea980",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T23:35:32.696050Z",
     "iopub.status.busy": "2026-02-15T23:35:32.695523Z",
     "iopub.status.idle": "2026-02-15T23:35:32.720280Z",
     "shell.execute_reply": "2026-02-15T23:35:32.719424Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PART 2 ANALYSIS: LAYER BOUNDARY SWEEP\n",
      "======================================================================\n",
      "\n",
      "Cutoff     Layers              N     Mean D        d    Win%            p   sig\n",
      "--------------------------------------------------------------------------------\n",
      "8          0-7               200    +0.0124   +0.073   48.0%     3.04e-01    ns\n",
      "12         0-11              200    +0.0209   +0.097   46.5%     1.74e-01    ns\n",
      "16         0-15              200    +0.0387   +0.161   50.5%     2.40e-02     *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20         0-19              200    +0.0096   +0.031   47.5%     6.64e-01    ns\n",
      "24         0-23              200    +0.0092   +0.028   41.5%     6.93e-01    ns\n",
      "\n",
      "Optimal cutoff: 16 layers (d=+0.161)\n",
      "Exp 19 reference: layers 0-16 (17 layers), d=+0.211\n",
      "Exp 19 values_only (all layers): d=+0.056\n",
      "\n",
      "======================================================================\n",
      "HARDNESS × CUTOFF INTERACTION (Part 2)\n",
      "======================================================================\n",
      "\n",
      "Cutoff         Q1 (easy)            Q2            Q3            Q4     Q5 (hard)       Overall\n",
      "----------------------------------------------------------------------------------------------\n",
      "8                 -0.220        +0.317        -0.155        -0.053        +0.230        +0.073\n",
      "12                -0.412        -0.251        -0.165        +0.042        +0.297        +0.097\n",
      "16                -0.480        -0.251        -0.037        +0.178        +0.436        +0.161\n",
      "20                -0.462        -0.272        -0.060        -0.097        +0.276        +0.031\n",
      "24                -0.447        -0.322        -0.229        -0.107        +0.353        +0.028\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Part 2 Analysis — d vs cutoff, optimal cutoff\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PART 2 ANALYSIS: LAYER BOUNDARY SWEEP\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Collect per-sample deltas by cutoff\n",
    "p2_deltas = {}\n",
    "p2_bare_all = []\n",
    "for cutoff in CUTOFFS:\n",
    "    p2_deltas[cutoff] = []\n",
    "\n",
    "for r in p2_results:\n",
    "    p2_bare_all.append(r['bare_nll'])\n",
    "    for row in r['rows']:\n",
    "        cutoff = row['layer_cutoff']\n",
    "        if cutoff in p2_deltas:\n",
    "            p2_deltas[cutoff].append(row['delta_nll'])\n",
    "\n",
    "p2_bare_arr = np.array(p2_bare_all)\n",
    "\n",
    "# Summary table\n",
    "print(f\"\\n{'Cutoff':<10} {'Layers':<15} {'N':>5} {'Mean D':>10} \"\n",
    "      f\"{'d':>8} {'Win%':>7} {'p':>12} {'sig':>5}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "p2_analysis = {}\n",
    "best_cutoff = None\n",
    "best_d = -999\n",
    "\n",
    "for cutoff in CUTOFFS:\n",
    "    delta = np.array(p2_deltas[cutoff])\n",
    "    valid = np.isfinite(delta)\n",
    "    delta = delta[valid]\n",
    "    n_valid = len(delta)\n",
    "    d = cohens_d(delta)\n",
    "    win = np.mean(delta > 0) * 100\n",
    "    t_stat, p_val = stats.ttest_1samp(delta, 0)\n",
    "    sig = '***' if p_val < 0.001 else '**' if p_val < 0.01 else '*' if p_val < 0.05 else 'ns'\n",
    "    layer_desc = f\"0-{cutoff-1}\"\n",
    "    print(f\"{cutoff:<10} {layer_desc:<15} {n_valid:>5} {np.mean(delta):>+10.4f} \"\n",
    "          f\"{d:>+8.3f} {win:>6.1f}% {p_val:>12.2e} {sig:>5}\")\n",
    "    p2_analysis[cutoff] = {\n",
    "        'n_valid': n_valid,\n",
    "        'layers': layer_desc,\n",
    "        'mean_delta': float(np.mean(delta)),\n",
    "        'cohens_d': float(d),\n",
    "        'win_pct': float(win),\n",
    "        't_stat': float(t_stat),\n",
    "        'p_value': float(p_val),\n",
    "    }\n",
    "    if d > best_d:\n",
    "        best_d = d\n",
    "        best_cutoff = cutoff\n",
    "\n",
    "# Exp 19 comparison\n",
    "print(f\"\\nOptimal cutoff: {best_cutoff} layers (d={best_d:+.3f})\")\n",
    "print(f\"Exp 19 reference: layers 0-16 (17 layers), d={EXP19_REF['values_early_layers_d']:+.3f}\")\n",
    "print(f\"Exp 19 values_only (all layers): d={EXP19_REF['values_only_d']:+.3f}\")\n",
    "\n",
    "# Hardness interaction for Part 2\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"HARDNESS × CUTOFF INTERACTION (Part 2)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Compute quintile boundaries from bare NLL\n",
    "quintile_boundaries = np.percentile(p2_bare_arr, [20, 40, 60, 80])\n",
    "quintile_labels = ['Q1 (easy)', 'Q2', 'Q3', 'Q4', 'Q5 (hard)']\n",
    "\n",
    "def get_quintile(nll, boundaries):\n",
    "    for i, b in enumerate(boundaries):\n",
    "        if nll <= b:\n",
    "            return i\n",
    "    return len(boundaries)\n",
    "\n",
    "quintiles = np.array([get_quintile(nll, quintile_boundaries) for nll in p2_bare_arr])\n",
    "\n",
    "header = f\"{'Cutoff':<10}\" + \"\".join(f\"{ql:>14}\" for ql in quintile_labels) + f\"{'Overall':>14}\"\n",
    "print(f\"\\n{header}\")\n",
    "print(\"-\" * (10 + 14 * 6))\n",
    "\n",
    "hardness_data = {}\n",
    "for cutoff in CUTOFFS:\n",
    "    delta = np.array(p2_deltas[cutoff])\n",
    "    row_str = f\"{cutoff:<10}\"\n",
    "    quintile_ds = []\n",
    "    for q in range(5):\n",
    "        mask_q = quintiles == q\n",
    "        if np.sum(mask_q) < 5:\n",
    "            row_str += f\"{'n/a':>14}\"\n",
    "            quintile_ds.append(None)\n",
    "        else:\n",
    "            d_q = cohens_d(delta[mask_q])\n",
    "            row_str += f\"{d_q:>+14.3f}\"\n",
    "            quintile_ds.append(float(d_q))\n",
    "    d_all = cohens_d(delta)\n",
    "    row_str += f\"{d_all:>+14.3f}\"\n",
    "    print(row_str)\n",
    "    hardness_data[cutoff] = quintile_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b0320d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T23:35:32.723838Z",
     "iopub.status.busy": "2026-02-15T23:35:32.723580Z",
     "iopub.status.idle": "2026-02-15T23:35:35.728526Z",
     "shell.execute_reply": "2026-02-15T23:35:35.727521Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plots saved to results/exp21/analysis_plots.png\n"
     ]
    }
   ],
   "source": [
    "# Cell 10: Plots — 4-panel figure\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# ---- Panel 1 (top-left): Part 1 — d vs Document Length ----\n",
    "ax = axes[0, 0]\n",
    "\n",
    "length_labels = ['original', '256', '512', '900']\n",
    "gemma_ds = [p1_analysis[pl]['cohens_d'] for pl in length_labels]\n",
    "x_lengths = []\n",
    "for pl in length_labels:\n",
    "    if pl == 'original':\n",
    "        # Mean original doc length\n",
    "        x_lengths.append(np.mean([r['base_doc_len'] for r in p1_results]))\n",
    "    else:\n",
    "        x_lengths.append(int(pl))\n",
    "\n",
    "# Bootstrap 95% CI\n",
    "np.random.seed(SEED)\n",
    "ci_lo, ci_hi = [], []\n",
    "for pl in length_labels:\n",
    "    delta = p1_arrays[pl]['delta']\n",
    "    boot_ds = []\n",
    "    for _ in range(2000):\n",
    "        idx_boot = np.random.randint(0, len(delta), size=len(delta))\n",
    "        boot_ds.append(cohens_d(delta[idx_boot]))\n",
    "    boot_ds = np.array(boot_ds)\n",
    "    ci_lo.append(np.percentile(boot_ds, 2.5))\n",
    "    ci_hi.append(np.percentile(boot_ds, 97.5))\n",
    "ci_lo = np.array(ci_lo)\n",
    "ci_hi = np.array(ci_hi)\n",
    "\n",
    "ax.errorbar(x_lengths, gemma_ds,\n",
    "            yerr=[np.array(gemma_ds) - ci_lo, ci_hi - np.array(gemma_ds)],\n",
    "            marker='o', markersize=8, linewidth=2, capsize=5,\n",
    "            color='#9467bd', ecolor='#c5b0d5', label='Gemma VEL (this exp)')\n",
    "\n",
    "# Exp 20 Mistral reference (only lengths we overlap with)\n",
    "exp20_x = [130, 256, 512]\n",
    "exp20_y = [EXP20_REF['original_d'], EXP20_REF['256_d'], EXP20_REF['512_d']]\n",
    "ax.plot(exp20_x, exp20_y, marker='s', markersize=6, linewidth=1.5, linestyle='--',\n",
    "        color='#d62728', alpha=0.7, label='Mistral full priming (Exp 20)')\n",
    "\n",
    "ax.axhline(y=0, color='gray', linestyle='-', alpha=0.3)\n",
    "ax.set_xscale('log')\n",
    "ax.set_xlabel('Document Token Length')\n",
    "ax.set_ylabel(\"Cohen's d (positive = helps)\")\n",
    "ax.set_title(\"Part 1: d vs Document Length\")\n",
    "ax.legend(fontsize=8)\n",
    "\n",
    "for i, pl in enumerate(length_labels):\n",
    "    p_val = p1_analysis[pl]['p_value']\n",
    "    sig = '***' if p_val < 0.001 else '**' if p_val < 0.01 else '*' if p_val < 0.05 else 'ns'\n",
    "    ax.annotate(f'{pl}\\nd={gemma_ds[i]:+.3f} {sig}',\n",
    "                (x_lengths[i], gemma_ds[i]),\n",
    "                textcoords='offset points', xytext=(0, 18),\n",
    "                ha='center', fontsize=7)\n",
    "\n",
    "# ---- Panel 2 (top-right): Part 1 — Win Rate vs Length ----\n",
    "ax = axes[0, 1]\n",
    "\n",
    "wins = [p1_analysis[pl]['win_pct'] for pl in length_labels]\n",
    "ax.plot(x_lengths, wins, marker='s', markersize=8, linewidth=2, color='#2ca02c')\n",
    "ax.axhline(y=50, color='black', linestyle='--', linewidth=0.8, label='Chance (50%)')\n",
    "\n",
    "for i, pl in enumerate(length_labels):\n",
    "    ax.annotate(f'{pl}\\n{wins[i]:.1f}%',\n",
    "                (x_lengths[i], wins[i]),\n",
    "                textcoords='offset points', xytext=(0, 12),\n",
    "                ha='center', fontsize=8)\n",
    "\n",
    "ax.set_xscale('log')\n",
    "ax.set_xlabel('Document Token Length')\n",
    "ax.set_ylabel('Win Rate (%)')\n",
    "ax.set_title(\"Part 1: Win Rate vs Length\")\n",
    "ax.legend(fontsize=8)\n",
    "\n",
    "# ---- Panel 3 (bottom-left): Part 2 — d vs Layer Cutoff ----\n",
    "ax = axes[1, 0]\n",
    "\n",
    "cutoff_ds = [p2_analysis[c]['cohens_d'] for c in CUTOFFS]\n",
    "x_pos = range(len(CUTOFFS))\n",
    "colors_bar = ['#1f77b4' if c != best_cutoff else '#ff7f0e' for c in CUTOFFS]\n",
    "bars = ax.bar(x_pos, cutoff_ds, color=colors_bar, edgecolor='black', linewidth=0.5)\n",
    "\n",
    "# Exp 19 reference\n",
    "ax.axhline(y=EXP19_REF['values_early_layers_d'], color='#9467bd', linestyle='--',\n",
    "           linewidth=1.5, label=f\"Exp 19 (17 layers) d={EXP19_REF['values_early_layers_d']:+.3f}\")\n",
    "ax.axhline(y=EXP19_REF['values_only_d'], color='#7f7f7f', linestyle=':',\n",
    "           linewidth=1.5, label=f\"Exp 19 values_only d={EXP19_REF['values_only_d']:+.3f}\")\n",
    "ax.axhline(y=0, color='gray', linestyle='-', alpha=0.3)\n",
    "\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels([str(c) for c in CUTOFFS])\n",
    "ax.set_xlabel('Layer Cutoff (layers 0 to cutoff-1)')\n",
    "ax.set_ylabel(\"Cohen's d vs Bare\")\n",
    "ax.set_title(f\"Part 2: d vs Layer Cutoff (best={best_cutoff})\")\n",
    "ax.legend(fontsize=7)\n",
    "\n",
    "for i, d_val in enumerate(cutoff_ds):\n",
    "    ax.text(i, d_val + 0.003 if d_val >= 0 else d_val - 0.012,\n",
    "            f\"{d_val:+.3f}\", ha='center',\n",
    "            va='bottom' if d_val >= 0 else 'top', fontsize=9)\n",
    "\n",
    "# ---- Panel 4 (bottom-right): Part 2 — Hardness x Cutoff heatmap ----\n",
    "ax = axes[1, 1]\n",
    "\n",
    "heatmap_data = np.zeros((len(CUTOFFS), 5))\n",
    "for i, cutoff in enumerate(CUTOFFS):\n",
    "    for q in range(5):\n",
    "        val = hardness_data[cutoff][q]\n",
    "        heatmap_data[i, q] = val if val is not None else np.nan\n",
    "\n",
    "im = ax.imshow(heatmap_data, cmap='RdBu', aspect='auto',\n",
    "               vmin=-0.5, vmax=0.5)\n",
    "ax.set_xticks(range(5))\n",
    "ax.set_xticklabels(quintile_labels, fontsize=8)\n",
    "ax.set_yticks(range(len(CUTOFFS)))\n",
    "ax.set_yticklabels([str(c) for c in CUTOFFS])\n",
    "ax.set_xlabel('Difficulty Quintile')\n",
    "ax.set_ylabel('Layer Cutoff')\n",
    "ax.set_title(\"Part 2: Hardness x Cutoff (Cohen's d)\")\n",
    "\n",
    "# Add text annotations\n",
    "for i in range(len(CUTOFFS)):\n",
    "    for j in range(5):\n",
    "        val = heatmap_data[i, j]\n",
    "        if not np.isnan(val):\n",
    "            ax.text(j, i, f\"{val:+.2f}\", ha='center', va='center',\n",
    "                    fontsize=7, color='white' if abs(val) > 0.25 else 'black')\n",
    "\n",
    "fig.colorbar(im, ax=ax, shrink=0.8, label=\"Cohen's d\")\n",
    "\n",
    "plt.suptitle('Exp 21: Gemma Mechanism Robustness & Tuning', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'analysis_plots.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"Plots saved to {RESULTS_DIR / 'analysis_plots.png'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c568d9a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T23:35:35.732426Z",
     "iopub.status.busy": "2026-02-15T23:35:35.732117Z",
     "iopub.status.idle": "2026-02-15T23:35:35.819749Z",
     "shell.execute_reply": "2026-02-15T23:35:35.818978Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part 1 CSV saved: results/exp21/part1_results.csv\n",
      "Part 2 CSV saved: results/exp21/part2_results.csv\n",
      "\n",
      "Results saved to results/exp21/results.json\n",
      "File size: 885.2 KB\n",
      "\n",
      "======================================================================\n",
      "SUMMARY — Exp 21: Gemma Mechanism Robustness & Tuning\n",
      "======================================================================\n",
      "Model: Gemma 3 4B (34 layers, head_dim=256, bfloat16)\n",
      "\n",
      "Part 1: Length Generalization (cutoff=16)\n",
      "  original     d=+0.227  win=60%  ***\n",
      "  256          d=+0.227  win=54%  ***\n",
      "  512          d=+0.173  win=56%  ***\n",
      "  900          d=+0.085  win=56%  ns\n",
      "  Verdict: Mixed: d goes from +0.227 (original) to +0.085 (900). See detailed numbers above.\n",
      "\n",
      "Part 2: Layer Boundary Sweep\n",
      "  cutoff=8    d=+0.073  win=48%  ns\n",
      "  cutoff=12   d=+0.097  win=46%  ns\n",
      "  cutoff=16   d=+0.161  win=50%  * <-- BEST\n",
      "  cutoff=20   d=+0.031  win=48%  ns\n",
      "  cutoff=24   d=+0.028  win=42%  ns\n",
      "  Optimal cutoff: 16 layers (d=+0.161)\n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Cell 11: Save results.json + CSVs\n",
    "import csv\n",
    "\n",
    "# --- Part 1 CSV ---\n",
    "with open(CSV_P1_PATH, 'w', newline='') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=[\n",
    "        'query_idx', 'layer_cutoff', 'pad_length', 'actual_doc_len',\n",
    "        'unprimed_nll', 'primed_nll', 'delta_nll'])\n",
    "    writer.writeheader()\n",
    "    for r in p1_results:\n",
    "        for row in r['rows']:\n",
    "            writer.writerow(row)\n",
    "print(f\"Part 1 CSV saved: {CSV_P1_PATH}\")\n",
    "\n",
    "# --- Part 2 CSV ---\n",
    "with open(CSV_P2_PATH, 'w', newline='') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=[\n",
    "        'query_idx', 'layer_cutoff', 'pad_length', 'actual_doc_len',\n",
    "        'unprimed_nll', 'primed_nll', 'delta_nll'])\n",
    "    writer.writeheader()\n",
    "    for r in p2_results:\n",
    "        for row in r['rows']:\n",
    "            writer.writerow(row)\n",
    "print(f\"Part 2 CSV saved: {CSV_P2_PATH}\")\n",
    "\n",
    "# --- Combined results.json ---\n",
    "final = {\n",
    "    'experiment': 'exp21_gemma_robustness_tuning',\n",
    "    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'config': {\n",
    "        'model_name': MODEL_NAME,\n",
    "        'model_type': 'gemma3',\n",
    "        'seed': SEED,\n",
    "        'dataset': 'MS MARCO v1.1 validation',\n",
    "        'max_passage_words': MAX_PASSAGE_WORDS,\n",
    "        'part1': {\n",
    "            'n_queries': N_PART1,\n",
    "            'pad_lengths': [str(pl) if pl else 'original' for pl in PAD_LENGTHS],\n",
    "            'fixed_cutoff': FIXED_CUTOFF,\n",
    "        },\n",
    "        'part2': {\n",
    "            'n_queries': N_PART2,\n",
    "            'cutoffs': CUTOFFS,\n",
    "        },\n",
    "    },\n",
    "    'gemma_architecture': {\n",
    "        'hidden_size': text_config.hidden_size,\n",
    "        'num_layers': text_config.num_hidden_layers,\n",
    "        'num_attention_heads': text_config.num_attention_heads,\n",
    "        'num_kv_heads': text_config.num_key_value_heads,\n",
    "        'head_dim': _get_head_dim(model.config),\n",
    "        'rope_thetas': sorted(list(thetas)),\n",
    "    },\n",
    "    'part1_analysis': p1_analysis,\n",
    "    'part1_verdict': p1_verdict,\n",
    "    'part2_analysis': {str(k): v for k, v in p2_analysis.items()},\n",
    "    'part2_best_cutoff': best_cutoff,\n",
    "    'part2_best_d': float(best_d),\n",
    "    'part2_hardness_data': {str(k): v for k, v in hardness_data.items()},\n",
    "    'reference_values': {\n",
    "        'exp19_gemma': EXP19_REF,\n",
    "        'exp20_mistral': EXP20_REF,\n",
    "    },\n",
    "    'part1_per_query': p1_results,\n",
    "    'part2_per_query': p2_results,\n",
    "}\n",
    "\n",
    "with open(FINAL_RESULTS_PATH, 'w') as f:\n",
    "    json.dump(final, f, indent=2)\n",
    "\n",
    "print(f\"\\nResults saved to {FINAL_RESULTS_PATH}\")\n",
    "print(f\"File size: {FINAL_RESULTS_PATH.stat().st_size / 1024:.1f} KB\")\n",
    "\n",
    "# Final summary\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"SUMMARY — Exp 21: Gemma Mechanism Robustness & Tuning\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Model: Gemma 3 4B (34 layers, head_dim=256, bfloat16)\")\n",
    "print(f\"\\nPart 1: Length Generalization (cutoff={FIXED_CUTOFF})\")\n",
    "for pl in ['original', '256', '512', '900']:\n",
    "    a = p1_analysis[pl]\n",
    "    sig = '***' if a['p_value'] < 0.001 else '**' if a['p_value'] < 0.01 else '*' if a['p_value'] < 0.05 else 'ns'\n",
    "    print(f\"  {pl:<12} d={a['cohens_d']:>+.3f}  win={a['win_pct']:.0f}%  {sig}\")\n",
    "print(f\"  Verdict: {p1_verdict}\")\n",
    "print(f\"\\nPart 2: Layer Boundary Sweep\")\n",
    "for cutoff in CUTOFFS:\n",
    "    a = p2_analysis[cutoff]\n",
    "    sig = '***' if a['p_value'] < 0.001 else '**' if a['p_value'] < 0.01 else '*' if a['p_value'] < 0.05 else 'ns'\n",
    "    marker = \" <-- BEST\" if cutoff == best_cutoff else \"\"\n",
    "    print(f\"  cutoff={cutoff:<4} d={a['cohens_d']:>+.3f}  win={a['win_pct']:.0f}%  {sig}{marker}\")\n",
    "print(f\"  Optimal cutoff: {best_cutoff} layers (d={best_d:+.3f})\")\n",
    "print(f\"\\nDone!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b94473b9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T23:35:35.823051Z",
     "iopub.status.busy": "2026-02-15T23:35:35.822558Z",
     "iopub.status.idle": "2026-02-15T23:35:36.644614Z",
     "shell.execute_reply": "2026-02-15T23:35:36.643661Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning up GPU memory...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory: 3.24 GB -> 0.01 GB\n",
      "Cleanup complete.\n"
     ]
    }
   ],
   "source": [
    "# Cell 12: GPU cleanup\n",
    "import gc\n",
    "\n",
    "print(\"Cleaning up GPU memory...\")\n",
    "mem_before = torch.cuda.memory_allocated() / 1e9\n",
    "\n",
    "del model\n",
    "del tokenizer\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "mem_after = torch.cuda.memory_allocated() / 1e9\n",
    "print(f\"GPU memory: {mem_before:.2f} GB -> {mem_after:.2f} GB\")\n",
    "print(\"Cleanup complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "033bcdf9b84442e085423c34be6495bd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "04d394e4852d46c4974bcfe1382b07eb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_60ffd4ca8d2345eda9d4fa8538814a72",
       "placeholder": "​",
       "style": "IPY_MODEL_bcc23f21ae1849a3a5e018397e1bdfa7",
       "tabbable": null,
       "tooltip": null,
       "value": "Loading weights: 100%"
      }
     },
     "06f4c50e072349488aa204937a8146af": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "089feeb264264b3a86efa1a013f7a7b6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_6b3cb075fc0c44fdbbbbd24c67e4835f",
        "IPY_MODEL_25210a0479dd4d229931377dcd069972",
        "IPY_MODEL_17096127b8c54c42b5dc4e6983e28c55"
       ],
       "layout": "IPY_MODEL_7acc489ef81f4876a132100b59776ac6",
       "tabbable": null,
       "tooltip": null
      }
     },
     "0ffbb08e9df3466ea8dd84e46bf72912": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_1024529d9b504be58ab4678952eb9ccb",
       "max": 500.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_db83cc93b9a04f10af57724446d80310",
       "tabbable": null,
       "tooltip": null,
       "value": 500.0
      }
     },
     "1024529d9b504be58ab4678952eb9ccb": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "17096127b8c54c42b5dc4e6983e28c55": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_c9eb3102549b47dc80f29ede1844d7b6",
       "placeholder": "​",
       "style": "IPY_MODEL_626bc7147bcc42229f8c9f0747bc8349",
       "tabbable": null,
       "tooltip": null,
       "value": " 200/200 [00:00&lt;?, ?it/s]"
      }
     },
     "1db96676d04f4a8e8792386c58d98cec": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_474b53ed58e949c1b9bfd24b582cd2b7",
       "max": 10047.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_42ebd4347a394dc2a82e6d9d5baef877",
       "tabbable": null,
       "tooltip": null,
       "value": 10047.0
      }
     },
     "22bbe2b3753e4f16adafa7f50a3bf67b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "25210a0479dd4d229931377dcd069972": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_30e937fa1e8f4f86866ec75639351242",
       "max": 200.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_3e8dc8b025a3460895893cedc06e9e03",
       "tabbable": null,
       "tooltip": null,
       "value": 200.0
      }
     },
     "30e937fa1e8f4f86866ec75639351242": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3d706aee17074ffeb83f2f999da9c041": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3dc0d0038e1d4333a36e8286956eea7e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3e8dc8b025a3460895893cedc06e9e03": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "42ebd4347a394dc2a82e6d9d5baef877": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "474b53ed58e949c1b9bfd24b582cd2b7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "4fe94cf7411046639dbea57678285d1e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_95c925408e1a41548d1f877ee668ae0b",
       "max": 883.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_b6bba4bb8e1c4ab78ef7871c5127f1af",
       "tabbable": null,
       "tooltip": null,
       "value": 883.0
      }
     },
     "55fd7366dadf4a798c2f3eebace06ab1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "5bb1e1f725c74cf78c0301008c5e4d35": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "60ffd4ca8d2345eda9d4fa8538814a72": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "626bc7147bcc42229f8c9f0747bc8349": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "66d64d2d0c1f4bdfbfb3108d2e99d59b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_d71f0efe6f644c299924c39a7c4c5b53",
       "placeholder": "​",
       "style": "IPY_MODEL_5bb1e1f725c74cf78c0301008c5e4d35",
       "tabbable": null,
       "tooltip": null,
       "value": "Part 1: 100%"
      }
     },
     "6b3cb075fc0c44fdbbbbd24c67e4835f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_3dc0d0038e1d4333a36e8286956eea7e",
       "placeholder": "​",
       "style": "IPY_MODEL_cd2b07f5f5ea4ba68dfde5a7d9a2e688",
       "tabbable": null,
       "tooltip": null,
       "value": "Part 2: 100%"
      }
     },
     "7372bbd0f03e4a3db1538721e880b6a7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_af8eecde02f1430c9f31d23a0b787ac1",
       "placeholder": "​",
       "style": "IPY_MODEL_033bcdf9b84442e085423c34be6495bd",
       "tabbable": null,
       "tooltip": null,
       "value": " 500/500 [00:00&lt;?, ?it/s]"
      }
     },
     "7457264630ae4f95850632eca8b5a64a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "76dc5292439c4ab6a3ba607344fda40a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_66d64d2d0c1f4bdfbfb3108d2e99d59b",
        "IPY_MODEL_0ffbb08e9df3466ea8dd84e46bf72912",
        "IPY_MODEL_7372bbd0f03e4a3db1538721e880b6a7"
       ],
       "layout": "IPY_MODEL_55fd7366dadf4a798c2f3eebace06ab1",
       "tabbable": null,
       "tooltip": null
      }
     },
     "7acc489ef81f4876a132100b59776ac6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "95c925408e1a41548d1f877ee668ae0b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "9d06763470324c82a4a77f909bfd9926": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_db83dc13805d451eae7bb515ee043d16",
       "placeholder": "​",
       "style": "IPY_MODEL_06f4c50e072349488aa204937a8146af",
       "tabbable": null,
       "tooltip": null,
       "value": "Filtering: 100%"
      }
     },
     "a186780edfe94e2b86e9cb74e3d43d7e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "a5c64f1a921749c3b4da67678e92a155": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_cb6853b28666414c8e6ce35d5911497a",
       "placeholder": "​",
       "style": "IPY_MODEL_a186780edfe94e2b86e9cb74e3d43d7e",
       "tabbable": null,
       "tooltip": null,
       "value": " 10047/10047 [00:01&lt;00:00, 6300.33it/s]"
      }
     },
     "af8eecde02f1430c9f31d23a0b787ac1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b4d7aebecb1e47c0bebb602e5f16d23e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_9d06763470324c82a4a77f909bfd9926",
        "IPY_MODEL_1db96676d04f4a8e8792386c58d98cec",
        "IPY_MODEL_a5c64f1a921749c3b4da67678e92a155"
       ],
       "layout": "IPY_MODEL_c31fb72fcb2348bf86795540b4a4265f",
       "tabbable": null,
       "tooltip": null
      }
     },
     "b6bba4bb8e1c4ab78ef7871c5127f1af": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "b9a27b9243904eb4aa9c5fdb0a41b42a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_3d706aee17074ffeb83f2f999da9c041",
       "placeholder": "​",
       "style": "IPY_MODEL_7457264630ae4f95850632eca8b5a64a",
       "tabbable": null,
       "tooltip": null,
       "value": " 883/883 [00:03&lt;00:00, 678.54it/s, Materializing param=model.vision_tower.vision_model.post_layernorm.weight]"
      }
     },
     "bcc23f21ae1849a3a5e018397e1bdfa7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "c31fb72fcb2348bf86795540b4a4265f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c9eb3102549b47dc80f29ede1844d7b6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "cb6853b28666414c8e6ce35d5911497a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "cd2b07f5f5ea4ba68dfde5a7d9a2e688": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "d71f0efe6f644c299924c39a7c4c5b53": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "db83cc93b9a04f10af57724446d80310": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "db83dc13805d451eae7bb515ee043d16": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f29e28eae5c44068a744d43b3180795e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_04d394e4852d46c4974bcfe1382b07eb",
        "IPY_MODEL_4fe94cf7411046639dbea57678285d1e",
        "IPY_MODEL_b9a27b9243904eb4aa9c5fdb0a41b42a"
       ],
       "layout": "IPY_MODEL_22bbe2b3753e4f16adafa7f50a3bf67b",
       "tabbable": null,
       "tooltip": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
