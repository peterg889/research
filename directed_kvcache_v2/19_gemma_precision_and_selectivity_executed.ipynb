{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ae42587",
   "metadata": {},
   "source": [
    "# Exp 19: Gemma Priming — Precision Fix & Selective Value Contamination\n",
    "\n",
    "## Motivation\n",
    "\n",
    "Exp 16 showed priming **FAILS** on Gemma 3 4B (`static_fact_trunc` d=-0.031, ns), but\n",
    "**`values_only` works** (d=+0.056, p=0.009). The gap reveals **-0.087 of key interference**.\n",
    "\n",
    "Two hypotheses explain this:\n",
    "\n",
    "### H1: Precision Hypothesis\n",
    "RoPE correction in bfloat16 (7-bit mantissa) with head_dim=256 introduces ~8.6x more\n",
    "quantization noise than Mistral's float16 (10-bit mantissa) / 128-dim. Computing the\n",
    "correction in float32 may recover the effect.\n",
    "\n",
    "### H2: Selectivity Hypothesis\n",
    "On Mistral (Exp 09), value contamination signal lives in layers 0-15 (88% of effect) and\n",
    "first 25% of positions (dominant). Targeting these on Gemma — and reducing the \"dose\" —\n",
    "may amplify the weak d=+0.056.\n",
    "\n",
    "## Exp 16 Reference Values\n",
    "\n",
    "| Condition | Gemma d | Mistral d | Gap |\n",
    "|-----------|---------|-----------|-----|\n",
    "| static_fact_trunc | -0.031 (ns) | +0.472 | -0.503 |\n",
    "| random_trunc | -0.109 (***) | +0.091 | -0.200 |\n",
    "| values_only | +0.056 (**) | +0.275 | -0.219 |\n",
    "| Key interference | -0.087 | ~0 | — |\n",
    "\n",
    "## Design: 9 Conditions, N=300 queries (MS MARCO)\n",
    "\n",
    "| # | Condition | Description | Tests |\n",
    "|---|-----------|-------------|-------|\n",
    "| 1 | `bare` | Baseline | — |\n",
    "| 2 | `sf_trunc` | Standard truncated + bfloat16 RoPE correction | Replicate Exp 16 reference |\n",
    "| 3 | `sf_trunc_fp32` | Truncated + **float32** RoPE correction | **H1: precision hypothesis** |\n",
    "| 4 | `sf_trunc_nocorr` | Truncated, NO RoPE correction | Is correction worse than mismatch? |\n",
    "| 5 | `values_only` | Bare keys + sf primed values (all layers) | Replicate Exp 16 d=+0.056 |\n",
    "| 6 | `values_early_layers` | Values_only, layers 0-16 only | **H2: layer selectivity** |\n",
    "| 7 | `values_early_pos` | Values_only, first 25% of doc positions only | **H2: position selectivity** |\n",
    "| 8 | `values_alpha_25` | 25% primed / 75% bare value blend | **H2: dose reduction** |\n",
    "| 9 | `rope_roundtrip` | Bare cache + RoPE roundtrip noise on keys | **Control: noise vs content** |\n",
    "\n",
    "## 7 Primary Comparisons (Bonferroni α = 0.05/7 = 0.00714)\n",
    "\n",
    "| # | Comparison | Question |\n",
    "|---|-----------|----------|\n",
    "| C1 | sf_trunc_fp32 vs sf_trunc | Does fp32 precision help? |\n",
    "| C2 | sf_trunc_nocorr vs bare | Does uncorrected truncation hurt? |\n",
    "| C3 | sf_trunc_nocorr vs sf_trunc | Is correction better than no correction? |\n",
    "| C4 | values_only vs bare | Replicate Exp 16 d=+0.056 |\n",
    "| C5 | values_early_layers vs values_only | Does layer selectivity help? |\n",
    "| C6 | values_early_pos vs values_only | Does position selectivity help? |\n",
    "| C7 | values_alpha_25 vs values_only | Does reduced dose help? |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3290458",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T14:53:54.782514Z",
     "iopub.status.busy": "2026-02-15T14:53:54.782217Z",
     "iopub.status.idle": "2026-02-15T14:53:57.997024Z",
     "shell.execute_reply": "2026-02-15T14:53:57.995647Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEED: 42\n",
      "Results directory: results/exp19\n",
      "CUDA available: True\n",
      "GPU: NVIDIA L4\n",
      "GPU memory: 23.6 GB\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Setup\n",
    "import os\n",
    "os.umask(0o000)\n",
    "\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import gc\n",
    "from pathlib import Path\n",
    "\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "RESULTS_DIR = Path(\"results/exp19\")\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CHECKPOINT_PATH = RESULTS_DIR / \"checkpoint.json\"\n",
    "FINAL_RESULTS_PATH = RESULTS_DIR / \"results.json\"\n",
    "\n",
    "print(f\"SEED: {SEED}\")\n",
    "print(f\"Results directory: {RESULTS_DIR}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33f928ac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T14:53:58.001753Z",
     "iopub.status.busy": "2026-02-15T14:53:58.000802Z",
     "iopub.status.idle": "2026-02-15T14:54:14.046583Z",
     "shell.execute_reply": "2026-02-15T14:54:14.045620Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading google/gemma-3-4b-it (4-bit, bfloat16)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3781e01d3554a38999565ae584d7160",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/883 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model loaded successfully.\n",
      "  Model class: Gemma3ForConditionalGeneration\n",
      "  Text config class: Gemma3TextConfig\n",
      "  Hidden size: 2560\n",
      "  Num layers: 34\n",
      "  Num attention heads: 8\n",
      "  Num KV heads: 4\n",
      "  Head dim: 256\n",
      "  BOS token ID: 2\n",
      "  EOS token ID: 1\n",
      "  Unique RoPE thetas: [10000.0, 1000000.0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Cache key dtype: torch.bfloat16\n",
      "  Cache key shape: torch.Size([1, 4, 2, 256])  (batch, kv_heads, seq, head_dim)\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Load Gemma 3 4B via load_model()\n",
    "sys.path.insert(0, \".\")\n",
    "\n",
    "from lib.config import ExperimentConfig\n",
    "from lib.model_utils import load_model\n",
    "\n",
    "MODEL_NAME = \"google/gemma-3-4b-it\"\n",
    "\n",
    "exp_config = ExperimentConfig(\n",
    "    model_name=MODEL_NAME,\n",
    "    model_type=\"gemma3\",\n",
    "    compute_dtype=\"auto\",  # resolves to bfloat16 for gemma3\n",
    "    use_4bit=True,\n",
    "    num_samples=2000,\n",
    "    seed=SEED,\n",
    ")\n",
    "\n",
    "print(f\"Loading {MODEL_NAME} (4-bit, bfloat16)...\")\n",
    "model, tokenizer = load_model(exp_config)\n",
    "\n",
    "# Architecture diagnostics\n",
    "from lib.kv_cache import _get_text_config, _get_head_dim, _get_rope_theta_for_layer, _get_cache_keys, _ensure_dynamic_cache\n",
    "\n",
    "text_config = _get_text_config(model.config)\n",
    "print(f\"\\nModel loaded successfully.\")\n",
    "print(f\"  Model class: {type(model).__name__}\")\n",
    "print(f\"  Text config class: {type(text_config).__name__}\")\n",
    "print(f\"  Hidden size: {text_config.hidden_size}\")\n",
    "print(f\"  Num layers: {text_config.num_hidden_layers}\")\n",
    "print(f\"  Num attention heads: {text_config.num_attention_heads}\")\n",
    "print(f\"  Num KV heads: {text_config.num_key_value_heads}\")\n",
    "print(f\"  Head dim: {_get_head_dim(model.config)}\")\n",
    "print(f\"  BOS token ID: {tokenizer.bos_token_id}\")\n",
    "print(f\"  EOS token ID: {tokenizer.eos_token_id}\")\n",
    "\n",
    "# Per-layer RoPE diagnostics\n",
    "thetas = set()\n",
    "for layer_idx in range(text_config.num_hidden_layers):\n",
    "    thetas.add(_get_rope_theta_for_layer(model.config, layer_idx))\n",
    "print(f\"  Unique RoPE thetas: {sorted(thetas)}\")\n",
    "\n",
    "# Verify dtype\n",
    "sample_ids = tokenizer(\"test\", return_tensors=\"pt\")['input_ids'].to(exp_config.device)\n",
    "with torch.no_grad():\n",
    "    out = model(sample_ids, use_cache=True)\n",
    "    cache_check = _ensure_dynamic_cache(out.past_key_values)\n",
    "    k0 = _get_cache_keys(cache_check, 0)\n",
    "    print(f\"  Cache key dtype: {k0.dtype}\")\n",
    "    print(f\"  Cache key shape: {k0.shape}  (batch, kv_heads, seq, head_dim)\")\n",
    "del out, sample_ids\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3150e545",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T14:54:14.050745Z",
     "iopub.status.busy": "2026-02-15T14:54:14.050164Z",
     "iopub.status.idle": "2026-02-15T14:54:14.270656Z",
     "shell.execute_reply": "2026-02-15T14:54:14.269732Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config ready\n",
      "  Model: google/gemma-3-4b-it\n",
      "  MAX_QUERIES: 300\n",
      "  Conditions: ['bare', 'sf_trunc', 'sf_trunc_fp32', 'sf_trunc_nocorr', 'values_only', 'values_early_layers', 'values_early_pos', 'values_alpha_25', 'rope_roundtrip']\n",
      "  N_COMPARISONS: 7, Bonferroni alpha: 0.0071\n",
      "  Static fact prefix: 'What are the key facts I need to know?'\n",
      "\n",
      "Exp 16 Gemma reference:\n",
      "    sf_trunc_d: -0.031\n",
      "    random_trunc_d: -0.109\n",
      "    values_only_d: +0.056\n",
      "\n",
      "Exp 09 Mistral selectivity reference:\n",
      "    layers_0_7_d: 0.172\n",
      "    layers_8_15_d: 0.069\n",
      "    layers_0_15_pct: 0.88\n",
      "    first_quarter_pct: dominant\n",
      "    alpha_025_pct: 0.86\n",
      "\n",
      "Verifying correct_rope_fp32()...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Mean key difference after fp32 correction (offset=5): 0.402748\n",
      "  BOS preserved: True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  correct_rope_fp32() verified OK\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Lib imports + custom correct_rope_fp32() + templates + constants\n",
    "from lib.kv_cache import (\n",
    "    _get_cache_keys,\n",
    "    _get_cache_values,\n",
    "    _set_cache_keys,\n",
    "    _set_cache_values,\n",
    "    _ensure_dynamic_cache,\n",
    "    extract_and_truncate_cache_with_bos,\n",
    "    correct_rope_positions_with_bos,\n",
    "    score_answer_with_cache,\n",
    "    deepcopy_cache,\n",
    "    build_hybrid_cache,\n",
    "    replace_values_at_layers,\n",
    "    replace_values_at_positions,\n",
    "    interpolate_values,\n",
    "    apply_rope_roundtrip_noise,\n",
    "    _build_rope_correction,\n",
    "    _get_rope_theta_for_layer,\n",
    "    _get_head_dim,\n",
    "    _rotate_half,\n",
    "    _get_text_config,\n",
    ")\n",
    "from lib.analysis import cohens_d\n",
    "from lib.data import count_words\n",
    "from scipy import stats\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "# === Custom function: RoPE correction in float32 ===\n",
    "def correct_rope_fp32(cache, offset, model):\n",
    "    \"\"\"RoPE correction in float32 (not bfloat16) for Gemma precision test.\n",
    "\n",
    "    Upcasts keys to float32 before applying the correction, then downcasts\n",
    "    back to the original dtype. This isolates the precision hypothesis: if\n",
    "    bfloat16 quantization during RoPE correction is the bottleneck, fp32\n",
    "    should recover the priming effect.\n",
    "    \"\"\"\n",
    "    if offset == 0:\n",
    "        return cache\n",
    "    config = model.config\n",
    "    head_dim = _get_head_dim(config)\n",
    "    text_cfg = _get_text_config(config)\n",
    "    n_layers = text_cfg.num_hidden_layers\n",
    "\n",
    "    # Pre-compute corrections per unique theta (in float32)\n",
    "    corrections = {}\n",
    "    for layer_idx in range(n_layers):\n",
    "        theta = _get_rope_theta_for_layer(config, layer_idx)\n",
    "        if theta not in corrections:\n",
    "            corrections[theta] = _build_rope_correction(offset, head_dim, theta)\n",
    "\n",
    "    for layer_idx in range(n_layers):\n",
    "        theta = _get_rope_theta_for_layer(config, layer_idx)\n",
    "        cos_a, sin_a = corrections[theta]\n",
    "        keys = _get_cache_keys(cache, layer_idx)\n",
    "        device = keys.device\n",
    "        orig_dtype = keys.dtype\n",
    "\n",
    "        # Keep cos/sin in float32, upcast keys to float32\n",
    "        c = cos_a.to(device=device)       # stays float32\n",
    "        s_val = sin_a.to(device=device)    # stays float32\n",
    "\n",
    "        # Skip BOS at position 0, correct doc keys at positions 1:\n",
    "        doc_keys = keys[:, :, 1:, :].float()  # upcast bfloat16 -> float32\n",
    "        corrected = doc_keys * c + _rotate_half(doc_keys) * s_val\n",
    "        corrected = corrected.to(orig_dtype)   # downcast back\n",
    "\n",
    "        _set_cache_keys(cache, layer_idx,\n",
    "                       torch.cat([keys[:, :, :1, :], corrected], dim=2))\n",
    "    return cache\n",
    "\n",
    "\n",
    "# Templates — bare text, no \"Document:\\n\" framing\n",
    "SURROGATE_PREFIX_TEMPLATE = \"{surrogate}\\n\"\n",
    "DOCUMENT_TEMPLATE = \"{document}\"\n",
    "QUERY_TEMPLATE = \"\\nQuery: {query}\\nAnswer:\"\n",
    "ANSWER_TEMPLATE = \" {answer}\"\n",
    "\n",
    "# Prefix text\n",
    "from lib.surrogate import STATIC_SURROGATE_QUERIES\n",
    "STATIC_FACT = STATIC_SURROGATE_QUERIES['static_factual']['query']\n",
    "\n",
    "# Experiment parameters\n",
    "MAX_QUERIES = 300\n",
    "MAX_PASSAGE_WORDS = 300\n",
    "MIN_PASSAGES_PER_QUERY = 2\n",
    "CHECKPOINT_EVERY = 25\n",
    "\n",
    "CONDITION_NAMES = [\n",
    "    'bare', 'sf_trunc', 'sf_trunc_fp32', 'sf_trunc_nocorr',\n",
    "    'values_only', 'values_early_layers', 'values_early_pos',\n",
    "    'values_alpha_25', 'rope_roundtrip',\n",
    "]\n",
    "\n",
    "N_COMPARISONS = 7\n",
    "BONFERRONI_ALPHA = 0.05 / N_COMPARISONS\n",
    "\n",
    "# Exp 16 reference values (Gemma)\n",
    "EXP16_REF = {\n",
    "    'sf_trunc_d': -0.031,\n",
    "    'random_trunc_d': -0.109,\n",
    "    'values_only_d': 0.056,\n",
    "}\n",
    "\n",
    "# Exp 09 reference values (Mistral layer/position selectivity)\n",
    "EXP09_REF = {\n",
    "    'layers_0_7_d': 0.172,   # ~63% of values-only d=0.275\n",
    "    'layers_8_15_d': 0.069,  # ~25%\n",
    "    'layers_0_15_pct': 0.88, # 88% of total signal in first 16 layers\n",
    "    'first_quarter_pct': 'dominant',\n",
    "    'alpha_025_pct': 0.86,   # 86% of full values-only effect\n",
    "}\n",
    "\n",
    "# Mistral reference values\n",
    "MISTRAL_REF = {\n",
    "    'static_fact_trunc_d': 0.472,\n",
    "    'values_only_d': 0.275,\n",
    "}\n",
    "\n",
    "print(\"Config ready\")\n",
    "print(f\"  Model: {MODEL_NAME}\")\n",
    "print(f\"  MAX_QUERIES: {MAX_QUERIES}\")\n",
    "print(f\"  Conditions: {CONDITION_NAMES}\")\n",
    "print(f\"  N_COMPARISONS: {N_COMPARISONS}, Bonferroni alpha: {BONFERRONI_ALPHA:.4f}\")\n",
    "print(f\"  Static fact prefix: '{STATIC_FACT}'\")\n",
    "print(f\"\\nExp 16 Gemma reference:\")\n",
    "for k, v in EXP16_REF.items():\n",
    "    print(f\"    {k}: {v:+.3f}\")\n",
    "print(f\"\\nExp 09 Mistral selectivity reference:\")\n",
    "for k, v in EXP09_REF.items():\n",
    "    print(f\"    {k}: {v}\")\n",
    "\n",
    "# Verify correct_rope_fp32 works on a small test\n",
    "print(\"\\nVerifying correct_rope_fp32()...\")\n",
    "test_ids = tokenizer(\"Hello world\", return_tensors=\"pt\", add_special_tokens=True)['input_ids'].to(exp_config.device)\n",
    "with torch.no_grad():\n",
    "    test_out = model(test_ids, use_cache=True, return_dict=True)\n",
    "test_cache = _ensure_dynamic_cache(test_out.past_key_values)\n",
    "test_cache_copy = deepcopy_cache(test_cache)\n",
    "correct_rope_fp32(test_cache_copy, 5, model)\n",
    "k_before = _get_cache_keys(test_cache, 0)[:, :, 1:, :]\n",
    "k_after = _get_cache_keys(test_cache_copy, 0)[:, :, 1:, :]\n",
    "diff = (k_before.float() - k_after.float()).abs().mean().item()\n",
    "print(f\"  Mean key difference after fp32 correction (offset=5): {diff:.6f}\")\n",
    "print(f\"  BOS preserved: {torch.equal(_get_cache_keys(test_cache, 0)[:,:,:1,:], _get_cache_keys(test_cache_copy, 0)[:,:,:1,:])}\")\n",
    "del test_out, test_cache, test_cache_copy, test_ids\n",
    "torch.cuda.empty_cache()\n",
    "print(\"  correct_rope_fp32() verified OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dbabd09a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T14:54:14.274576Z",
     "iopub.status.busy": "2026-02-15T14:54:14.274280Z",
     "iopub.status.idle": "2026-02-15T14:54:15.492630Z",
     "shell.execute_reply": "2026-02-15T14:54:15.491656Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`trust_remote_code` is not supported anymore.\n",
      "Please check that the Hugging Face dataset 'microsoft/ms_marco' isn't based on a loading script and remove `trust_remote_code`.\n",
      "If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "LOADING MS MARCO v1.1 — ALL PASSAGES PER QUERY\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total items in validation: 10047\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adbedae1b9b945f6b9d58bafe55b20af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filtering:   0%|          | 0/10047 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Selected 300 queries (2504 total passages)\n",
      "Passages per query: mean=8.3, min=3, max=10\n",
      "Word counts: mean=71\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 4: Load MS MARCO v1.1, filter ≤300 words, ≥2 passages, limit 300 queries\n",
    "from datasets import load_dataset\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"LOADING MS MARCO v1.1 — ALL PASSAGES PER QUERY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "dataset = load_dataset(\"microsoft/ms_marco\", \"v1.1\", split=\"validation\",\n",
    "                        trust_remote_code=True)\n",
    "print(f\"Total items in validation: {len(dataset)}\")\n",
    "\n",
    "queries = []\n",
    "np.random.seed(SEED)\n",
    "\n",
    "for item in tqdm(dataset, desc=\"Filtering\"):\n",
    "    passages_info = item.get('passages', {})\n",
    "    passage_texts = passages_info.get('passage_text', [])\n",
    "    is_selected = passages_info.get('is_selected', [])\n",
    "    query = item.get('query', '')\n",
    "    answers = item.get('answers', [])\n",
    "    well_formed = item.get('wellFormedAnswers', [])\n",
    "\n",
    "    if not passage_texts or not query:\n",
    "        continue\n",
    "    if len(passage_texts) < MIN_PASSAGES_PER_QUERY:\n",
    "        continue\n",
    "    if not is_selected or sum(is_selected) == 0:\n",
    "        continue\n",
    "\n",
    "    word_counts = [count_words(p) for p in passage_texts]\n",
    "    if any(wc > MAX_PASSAGE_WORDS for wc in word_counts):\n",
    "        continue\n",
    "\n",
    "    answer = None\n",
    "    if well_formed and len(well_formed) > 0 and well_formed[0] != '[]':\n",
    "        answer = well_formed[0]\n",
    "    elif answers and len(answers) > 0 and answers[0] != 'No Answer Present.':\n",
    "        answer = answers[0]\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "    passage_list = []\n",
    "    for i, (ptext, sel) in enumerate(zip(passage_texts, is_selected)):\n",
    "        passage_list.append({\n",
    "            'passage': ptext,\n",
    "            'is_relevant': bool(sel == 1),\n",
    "            'word_count': word_counts[i],\n",
    "            'passage_idx': i,\n",
    "        })\n",
    "\n",
    "    queries.append({\n",
    "        'query': query,\n",
    "        'answer': answer,\n",
    "        'passages': passage_list,\n",
    "        'n_passages': len(passage_list),\n",
    "        'n_relevant': sum(1 for p in passage_list if p['is_relevant']),\n",
    "    })\n",
    "\n",
    "    if len(queries) >= MAX_QUERIES * 3:\n",
    "        break\n",
    "\n",
    "np.random.shuffle(queries)\n",
    "queries = queries[:MAX_QUERIES]\n",
    "N = len(queries)\n",
    "\n",
    "n_passages_list = [q['n_passages'] for q in queries]\n",
    "total_passages = sum(n_passages_list)\n",
    "\n",
    "print(f\"\\nSelected {N} queries ({total_passages} total passages)\")\n",
    "print(f\"Passages per query: mean={np.mean(n_passages_list):.1f}, \"\n",
    "      f\"min={min(n_passages_list)}, max={max(n_passages_list)}\")\n",
    "print(f\"Word counts: mean={np.mean([p['word_count'] for q in queries for p in q['passages']]):.0f}\")\n",
    "\n",
    "del dataset\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e405eb7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T14:54:15.497809Z",
     "iopub.status.busy": "2026-02-15T14:54:15.497469Z",
     "iopub.status.idle": "2026-02-15T14:54:15.509536Z",
     "shell.execute_reply": "2026-02-15T14:54:15.508822Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PREFIX TOKENIZATION — GEMMA 3 4B\n",
      "======================================================================\n",
      "\n",
      "Static fact prefix: 'What are the key facts I need to know?'\n",
      "  Formatted: 'What are the key facts I need to know?'\n",
      "  Token length: 11\n",
      "\n",
      "BPE BOUNDARY CHECK (first passage):\n",
      "  static_fact: 151/151 tokens match (100.0%)\n",
      "\n",
      "======================================================================\n",
      "CONDITION DETAILS\n",
      "======================================================================\n",
      "\n",
      "### 1. bare ###\n",
      "  Cache: [BOS][doc]\n",
      "  Purpose: Baseline. All other conditions compared to this.\n",
      "\n",
      "### 2. sf_trunc ###\n",
      "  Cache: [BOS]['What are the key facts I need to know?'\n",
      "][doc] -> truncate + RoPE correct (bfloat16)\n",
      "  Purpose: Standard Exp 16 condition. Expected d ~ -0.031 (ns).\n",
      "\n",
      "### 3. sf_trunc_fp32 ###\n",
      "  Cache: Same as sf_trunc but RoPE correction computed in float32\n",
      "  Purpose: H1 TEST: Does float32 precision recover the priming effect?\n",
      "\n",
      "### 4. sf_trunc_nocorr ###\n",
      "  Cache: [BOS]['What are the key facts I need to know?'\n",
      "][doc] -> truncate, NO RoPE correction\n",
      "  Purpose: Control: Is correction worse than position mismatch?\n",
      "\n",
      "### 5. values_only ###\n",
      "  Cache: Bare keys + sf primed values (all layers, all positions)\n",
      "  Purpose: Replicate Exp 16 d=+0.056. Isolates value contamination.\n",
      "\n",
      "### 6. values_early_layers ###\n",
      "  Cache: values_only but ONLY layers 0-16 (first 50% of 34 layers)\n",
      "  Purpose: H2 TEST: Layer selectivity. On Mistral layers 0-15 = 88%.\n",
      "\n",
      "### 7. values_early_pos ###\n",
      "  Cache: values_only but ONLY first 25% of document positions\n",
      "  Purpose: H2 TEST: Position selectivity. On Mistral first 25% = dominant.\n",
      "\n",
      "### 8. values_alpha_25 ###\n",
      "  Cache: 25% primed values + 75% bare values (linear blend)\n",
      "  Purpose: H2 TEST: Dose reduction. On Mistral alpha=0.25 retains 86%.\n",
      "\n",
      "### 9. rope_roundtrip ###\n",
      "  Cache: Bare cache with RoPE(+offset) then RoPE(-offset) applied to keys\n",
      "  Purpose: CONTROL: Pure bfloat16 quantization noise, no content signal.\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Tokenize prefix and verify BPE boundaries\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PREFIX TOKENIZATION — GEMMA 3 4B\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "sf_str = SURROGATE_PREFIX_TEMPLATE.format(surrogate=STATIC_FACT)\n",
    "\n",
    "sf_ids = tokenizer(sf_str, return_tensors=\"pt\",\n",
    "                    add_special_tokens=False)['input_ids'].to(exp_config.device)\n",
    "\n",
    "print(f\"\\nStatic fact prefix: '{STATIC_FACT}'\")\n",
    "print(f\"  Formatted: '{sf_str.strip()}'\")\n",
    "print(f\"  Token length: {sf_ids.shape[1]}\")\n",
    "\n",
    "# Verify BPE boundary consistency\n",
    "print(\"\\nBPE BOUNDARY CHECK (first passage):\")\n",
    "example_doc = queries[0]['passages'][0]['passage']\n",
    "concat = sf_str + DOCUMENT_TEMPLATE.format(document=example_doc)\n",
    "concat_enc = tokenizer(concat, add_special_tokens=True)['input_ids']\n",
    "prefix_enc = tokenizer(sf_str, add_special_tokens=True)['input_ids']\n",
    "doc_ids_from_concat = concat_enc[len(prefix_enc):]\n",
    "\n",
    "bare_doc_enc = tokenizer(DOCUMENT_TEMPLATE.format(document=example_doc),\n",
    "                          add_special_tokens=False)['input_ids']\n",
    "match = sum(1 for a, b in zip(doc_ids_from_concat, bare_doc_enc) if a == b)\n",
    "total = max(len(bare_doc_enc), 1)\n",
    "print(f\"  static_fact: {match}/{total} tokens match ({100*match/total:.1f}%)\")\n",
    "\n",
    "# Condition explanation\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"CONDITION DETAILS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "conditions_detail = [\n",
    "    (\"1. bare\",\n",
    "     \"[BOS][doc]\",\n",
    "     \"Baseline. All other conditions compared to this.\"),\n",
    "    (\"2. sf_trunc\",\n",
    "     f\"[BOS]['{STATIC_FACT}'\\n][doc] -> truncate + RoPE correct (bfloat16)\",\n",
    "     \"Standard Exp 16 condition. Expected d ~ -0.031 (ns).\"),\n",
    "    (\"3. sf_trunc_fp32\",\n",
    "     f\"Same as sf_trunc but RoPE correction computed in float32\",\n",
    "     \"H1 TEST: Does float32 precision recover the priming effect?\"),\n",
    "    (\"4. sf_trunc_nocorr\",\n",
    "     f\"[BOS]['{STATIC_FACT}'\\n][doc] -> truncate, NO RoPE correction\",\n",
    "     \"Control: Is correction worse than position mismatch?\"),\n",
    "    (\"5. values_only\",\n",
    "     \"Bare keys + sf primed values (all layers, all positions)\",\n",
    "     \"Replicate Exp 16 d=+0.056. Isolates value contamination.\"),\n",
    "    (\"6. values_early_layers\",\n",
    "     \"values_only but ONLY layers 0-16 (first 50% of 34 layers)\",\n",
    "     \"H2 TEST: Layer selectivity. On Mistral layers 0-15 = 88%.\"),\n",
    "    (\"7. values_early_pos\",\n",
    "     \"values_only but ONLY first 25% of document positions\",\n",
    "     \"H2 TEST: Position selectivity. On Mistral first 25% = dominant.\"),\n",
    "    (\"8. values_alpha_25\",\n",
    "     \"25% primed values + 75% bare values (linear blend)\",\n",
    "     \"H2 TEST: Dose reduction. On Mistral alpha=0.25 retains 86%.\"),\n",
    "    (\"9. rope_roundtrip\",\n",
    "     \"Bare cache with RoPE(+offset) then RoPE(-offset) applied to keys\",\n",
    "     \"CONTROL: Pure bfloat16 quantization noise, no content signal.\"),\n",
    "]\n",
    "\n",
    "for name, detail, purpose in conditions_detail:\n",
    "    print(f\"\\n### {name} ###\")\n",
    "    print(f\"  Cache: {detail}\")\n",
    "    print(f\"  Purpose: {purpose}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb4bf115",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T14:54:15.513094Z",
     "iopub.status.busy": "2026-02-15T14:54:15.512830Z",
     "iopub.status.idle": "2026-02-15T17:12:34.686288Z",
     "shell.execute_reply": "2026-02-15T17:12:34.685160Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "MAIN EVALUATION (300 queries, ~2504 passages)\n",
      "Model: Gemma 3 4B | 9 conditions\n",
      "======================================================================\n",
      "No checkpoint found. Starting fresh.\n",
      "Evaluating queries 0 to 299\n",
      "Per passage: 2 forward passes + 9 scoring passes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb1be520e3ec497dbc767c01190d759a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Queries:   0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 25/300 | 25 done in 11.3m | ETA: 123.8 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 50/300 | 50 done in 22.7m | ETA: 113.6 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 75/300 | 75 done in 34.3m | ETA: 102.9 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 100/300 | 100 done in 46.0m | ETA: 92.0 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 125/300 | 125 done in 58.2m | ETA: 81.5 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 150/300 | 150 done in 69.3m | ETA: 69.3 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 175/300 | 175 done in 81.0m | ETA: 57.9 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 200/300 | 200 done in 92.3m | ETA: 46.1 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 225/300 | 225 done in 104.0m | ETA: 34.7 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 250/300 | 250 done in 115.4m | ETA: 23.1 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 275/300 | 275 done in 126.8m | ETA: 11.5 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 300/300 | 300 done in 138.3m | ETA: 0.0 min\n",
      "\n",
      "Evaluation complete: 300 queries in 138.3 min\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Main loop — 2 fwd passes per passage, build 9 cache variants, score all\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"MAIN EVALUATION ({N} queries, ~{total_passages} passages)\")\n",
    "print(\"Model: Gemma 3 4B | 9 conditions\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Checkpoint resume\n",
    "all_results = []\n",
    "start_idx = 0\n",
    "\n",
    "if CHECKPOINT_PATH.exists():\n",
    "    with open(CHECKPOINT_PATH, 'r') as f:\n",
    "        ckpt = json.load(f)\n",
    "    ckpt_queries = ckpt.get('query_texts', [])\n",
    "    current_queries = [q['query'] for q in queries]\n",
    "    if ckpt_queries == current_queries:\n",
    "        all_results = ckpt['results']\n",
    "        start_idx = len(all_results)\n",
    "        print(f\"Resuming from checkpoint: {start_idx}/{N}\")\n",
    "    else:\n",
    "        print(\"Checkpoint query mismatch. Starting fresh.\")\n",
    "else:\n",
    "    print(\"No checkpoint found. Starting fresh.\")\n",
    "\n",
    "print(f\"Evaluating queries {start_idx} to {N-1}\")\n",
    "print(f\"Per passage: 2 forward passes + 9 scoring passes\")\n",
    "\n",
    "t_start = time.time()\n",
    "\n",
    "for qidx in tqdm(range(start_idx, N), initial=start_idx, total=N, desc=\"Queries\"):\n",
    "    query_data = queries[qidx]\n",
    "    query = query_data['query']\n",
    "    answer = query_data['answer']\n",
    "    query_prompt = QUERY_TEMPLATE.format(query=query)\n",
    "    answer_text = ANSWER_TEMPLATE.format(answer=answer)\n",
    "\n",
    "    passage_results = []\n",
    "\n",
    "    for pidx, pinfo in enumerate(query_data['passages']):\n",
    "        passage = pinfo['passage']\n",
    "        document_text = DOCUMENT_TEMPLATE.format(document=passage)\n",
    "\n",
    "        # --- Matched tokenization (using sf prefix as reference) ---\n",
    "        full_text = sf_str + document_text\n",
    "        full_enc = tokenizer(full_text, return_tensors=\"pt\",\n",
    "                              add_special_tokens=True, padding=False, truncation=False)\n",
    "        full_ids = full_enc['input_ids'].to(exp_config.device)\n",
    "\n",
    "        sf_prefix_enc = tokenizer(sf_str, return_tensors=\"pt\",\n",
    "                                   add_special_tokens=True, padding=False, truncation=False)\n",
    "        sf_prefix_len_matched = sf_prefix_enc['input_ids'].shape[1]\n",
    "\n",
    "        bos_id = full_ids[:, :1]\n",
    "        doc_ids = full_ids[:, sf_prefix_len_matched:]\n",
    "        doc_len = doc_ids.shape[1]\n",
    "        context_len = 1 + doc_len  # BOS + doc\n",
    "\n",
    "        del full_enc, full_ids, sf_prefix_enc\n",
    "\n",
    "        # === Forward pass 1: BARE cache ===\n",
    "        bare_input = torch.cat([bos_id, doc_ids], dim=1)\n",
    "        with torch.no_grad():\n",
    "            bare_out = model(input_ids=bare_input,\n",
    "                             attention_mask=torch.ones_like(bare_input),\n",
    "                             use_cache=True, return_dict=True)\n",
    "        bare_cache = _ensure_dynamic_cache(bare_out.past_key_values)\n",
    "        del bare_out, bare_input\n",
    "\n",
    "        # === Forward pass 2: PRIMED cache (static_fact prefix) ===\n",
    "        primed_input = torch.cat([bos_id, sf_ids, doc_ids], dim=1)\n",
    "        with torch.no_grad():\n",
    "            primed_out = model(input_ids=primed_input,\n",
    "                               attention_mask=torch.ones_like(primed_input),\n",
    "                               use_cache=True, return_dict=True)\n",
    "        primed_full = _ensure_dynamic_cache(primed_out.past_key_values)\n",
    "        del primed_out, primed_input\n",
    "\n",
    "        # === Truncate once: keep [BOS] + [last doc_len positions] ===\n",
    "        trunc_raw = extract_and_truncate_cache_with_bos(primed_full, doc_len)\n",
    "        prefix_offset = sf_ids.shape[1]\n",
    "        del primed_full\n",
    "\n",
    "        # === Build 9 scoring caches ===\n",
    "\n",
    "        # (1) bare — use bare_cache directly\n",
    "        # Score bare LAST since score_answer_with_cache mutates\n",
    "\n",
    "        # (2) sf_trunc — standard bfloat16 correction\n",
    "        sf_trunc_cache = deepcopy_cache(trunc_raw)\n",
    "        correct_rope_positions_with_bos(sf_trunc_cache, prefix_offset, model)\n",
    "\n",
    "        # (3) sf_trunc_fp32 — float32 precision correction\n",
    "        sf_trunc_fp32_cache = deepcopy_cache(trunc_raw)\n",
    "        correct_rope_fp32(sf_trunc_fp32_cache, prefix_offset, model)\n",
    "\n",
    "        # (4) sf_trunc_nocorr — no correction at all\n",
    "        sf_trunc_nocorr_cache = deepcopy_cache(trunc_raw)\n",
    "        # no correction applied\n",
    "\n",
    "        # (5) values_only — bare keys + sf primed values (from corrected cache)\n",
    "        values_only_cache = build_hybrid_cache(\n",
    "            keys_source=bare_cache,\n",
    "            values_source=sf_trunc_cache,\n",
    "        )\n",
    "\n",
    "        # (6) values_early_layers — values from layers 0-16 only\n",
    "        values_early_layers_cache = replace_values_at_layers(\n",
    "            bare_cache, sf_trunc_cache, list(range(17))\n",
    "        )\n",
    "\n",
    "        # (7) values_early_pos — values from first 25% of doc positions only\n",
    "        pos_end = 1 + max(1, doc_len // 4)\n",
    "        values_early_pos_cache = replace_values_at_positions(\n",
    "            bare_cache, sf_trunc_cache, 1, pos_end\n",
    "        )\n",
    "\n",
    "        # (8) values_alpha_25 — 25% primed / 75% bare value blend\n",
    "        values_alpha_25_cache = interpolate_values(\n",
    "            bare_cache, sf_trunc_cache, 0.25\n",
    "        )\n",
    "\n",
    "        # (9) rope_roundtrip — bare cache + RoPE roundtrip noise\n",
    "        rope_roundtrip_cache = deepcopy_cache(bare_cache)\n",
    "        apply_rope_roundtrip_noise(rope_roundtrip_cache, prefix_offset, model)\n",
    "\n",
    "        del trunc_raw\n",
    "\n",
    "        # === Score all 9 conditions (deepcopy before each except bare which is last) ===\n",
    "        nll_sf_trunc = score_answer_with_cache(\n",
    "            deepcopy_cache(sf_trunc_cache), context_len,\n",
    "            query_prompt, answer_text, model, tokenizer, exp_config)\n",
    "        del sf_trunc_cache\n",
    "\n",
    "        nll_sf_trunc_fp32 = score_answer_with_cache(\n",
    "            deepcopy_cache(sf_trunc_fp32_cache), context_len,\n",
    "            query_prompt, answer_text, model, tokenizer, exp_config)\n",
    "        del sf_trunc_fp32_cache\n",
    "\n",
    "        nll_sf_trunc_nocorr = score_answer_with_cache(\n",
    "            deepcopy_cache(sf_trunc_nocorr_cache), context_len,\n",
    "            query_prompt, answer_text, model, tokenizer, exp_config)\n",
    "        del sf_trunc_nocorr_cache\n",
    "\n",
    "        nll_values_only = score_answer_with_cache(\n",
    "            deepcopy_cache(values_only_cache), context_len,\n",
    "            query_prompt, answer_text, model, tokenizer, exp_config)\n",
    "        del values_only_cache\n",
    "\n",
    "        nll_values_early_layers = score_answer_with_cache(\n",
    "            deepcopy_cache(values_early_layers_cache), context_len,\n",
    "            query_prompt, answer_text, model, tokenizer, exp_config)\n",
    "        del values_early_layers_cache\n",
    "\n",
    "        nll_values_early_pos = score_answer_with_cache(\n",
    "            deepcopy_cache(values_early_pos_cache), context_len,\n",
    "            query_prompt, answer_text, model, tokenizer, exp_config)\n",
    "        del values_early_pos_cache\n",
    "\n",
    "        nll_values_alpha_25 = score_answer_with_cache(\n",
    "            deepcopy_cache(values_alpha_25_cache), context_len,\n",
    "            query_prompt, answer_text, model, tokenizer, exp_config)\n",
    "        del values_alpha_25_cache\n",
    "\n",
    "        nll_rope_roundtrip = score_answer_with_cache(\n",
    "            deepcopy_cache(rope_roundtrip_cache), context_len,\n",
    "            query_prompt, answer_text, model, tokenizer, exp_config)\n",
    "        del rope_roundtrip_cache\n",
    "\n",
    "        # Score bare LAST (mutates cache)\n",
    "        nll_bare = score_answer_with_cache(\n",
    "            bare_cache, context_len, query_prompt, answer_text,\n",
    "            model, tokenizer, exp_config)\n",
    "        del bare_cache\n",
    "\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        passage_results.append({\n",
    "            'passage_idx': pinfo['passage_idx'],\n",
    "            'is_relevant': pinfo['is_relevant'],\n",
    "            'word_count': pinfo['word_count'],\n",
    "            'doc_len': doc_len,\n",
    "            'bare_nll': nll_bare,\n",
    "            'sf_trunc_nll': nll_sf_trunc,\n",
    "            'sf_trunc_fp32_nll': nll_sf_trunc_fp32,\n",
    "            'sf_trunc_nocorr_nll': nll_sf_trunc_nocorr,\n",
    "            'values_only_nll': nll_values_only,\n",
    "            'values_early_layers_nll': nll_values_early_layers,\n",
    "            'values_early_pos_nll': nll_values_early_pos,\n",
    "            'values_alpha_25_nll': nll_values_alpha_25,\n",
    "            'rope_roundtrip_nll': nll_rope_roundtrip,\n",
    "        })\n",
    "\n",
    "    all_results.append({\n",
    "        'query_idx': qidx,\n",
    "        'query': query,\n",
    "        'n_passages': len(passage_results),\n",
    "        'n_relevant': query_data['n_relevant'],\n",
    "        'passage_data': passage_results,\n",
    "    })\n",
    "\n",
    "    # Checkpoint\n",
    "    if (qidx + 1) % CHECKPOINT_EVERY == 0 or qidx == N - 1:\n",
    "        ckpt_data = {\n",
    "            'results': all_results,\n",
    "            'query_texts': [q['query'] for q in queries],\n",
    "            'completed': len(all_results),\n",
    "            'total': N,\n",
    "            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        }\n",
    "        with open(CHECKPOINT_PATH, 'w') as f:\n",
    "            json.dump(ckpt_data, f)\n",
    "        elapsed = time.time() - t_start\n",
    "        n_done = qidx - start_idx + 1\n",
    "        rate = n_done / elapsed if elapsed > 0 else 0\n",
    "        remaining = (N - qidx - 1) / rate if rate > 0 else 0\n",
    "        tqdm.write(f\"  Checkpoint {qidx+1}/{N} | {n_done} done in {elapsed/60:.1f}m | \"\n",
    "                   f\"ETA: {remaining/60:.1f} min\")\n",
    "\n",
    "elapsed_total = time.time() - t_start\n",
    "print(f\"\\nEvaluation complete: {len(all_results)} queries in {elapsed_total/60:.1f} min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "055df2e4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T17:12:34.691355Z",
     "iopub.status.busy": "2026-02-15T17:12:34.691006Z",
     "iopub.status.idle": "2026-02-15T17:12:35.122069Z",
     "shell.execute_reply": "2026-02-15T17:12:35.120939Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ANALYSIS — GEMMA PRECISION FIX & SELECTIVE VALUE CONTAMINATION\n",
      "======================================================================\n",
      "Valid queries: 300\n",
      "Total passages: 2504, Valid: 2174, Excluded: 330\n",
      "\n",
      "======================================================================\n",
      "NLL SUMMARY (per-passage, Gemma 3 4B)\n",
      "======================================================================\n",
      "\n",
      "Condition                   Mean NLL        Std  d vs Bare     Win%\n",
      "--------------------------------------------------------------------\n",
      "bare                          2.2966     1.7849        ---      ---\n",
      "sf_trunc                      2.3138     1.8009     -0.031    45.2%\n",
      "sf_trunc_fp32                 2.3143     1.7997     -0.032    45.5%\n",
      "sf_trunc_nocorr               2.3026     1.8093     -0.009    47.1%\n",
      "values_only                   2.2693     1.7790     +0.056    49.2%\n",
      "values_early_layers           2.2189     1.7680     +0.211    60.6%\n",
      "values_early_pos              2.2919     1.7712     +0.010    45.7%\n",
      "values_alpha_25               2.2876     1.7881     +0.081    50.2%\n",
      "rope_roundtrip                2.2972     1.7921     -0.019    35.5%\n",
      "\n",
      "======================================================================\n",
      "7 PRIMARY COMPARISONS (Bonferroni alpha = 0.0071)\n",
      "======================================================================\n",
      "\n",
      "Comparison                            Mean D        d    Win%        t            p   Sig\n",
      "------------------------------------------------------------------------------------------\n",
      "C1: fp32 vs bf16                     -0.0005   -0.016   34.7%    -0.77    4.42e-01    ns\n",
      "C2: nocorr vs bare                   -0.0060   -0.009   47.1%    -0.43    6.69e-01    ns\n",
      "C3: nocorr vs bf16                    0.0112   +0.021   51.7%     0.97    3.30e-01    ns\n",
      "C4: values_only vs bare               0.0273   +0.056   49.2%     2.62    8.73e-03     *\n",
      "C5: early_layers vs values_only       0.0504   +0.170   62.4%     7.92    3.68e-15   ***\n",
      "C6: early_pos vs values_only         -0.0226   -0.124   40.9%    -5.80    7.66e-09   ***\n",
      "C7: alpha_25 vs values_only          -0.0184   -0.043   50.6%    -2.02    4.31e-02     *\n",
      "\n",
      "======================================================================\n",
      "KEY DERIVED METRICS\n",
      "======================================================================\n",
      "\n",
      "  Precision gain (fp32 - bf16):    -0.001\n",
      "    sf_trunc (bf16): d = -0.031\n",
      "    sf_trunc_fp32:   d = -0.032\n",
      "\n",
      "  Key interference (bf16):          +0.088\n",
      "    = d(values_only) - d(sf_trunc)\n",
      "    = +0.056 - (-0.031)\n",
      "\n",
      "  Key interference (fp32):          +0.089\n",
      "    = d(values_only) - d(sf_trunc_fp32)\n",
      "    = +0.056 - (-0.032)\n",
      "    (If smaller than bf16 interference, precision matters)\n",
      "\n",
      "  Noise baseline (rope_roundtrip):  -0.019\n",
      "    (Pure bfloat16 roundtrip noise cost, no content)\n",
      "\n",
      "  Best selective condition:          values_early_layers d=+0.211\n",
      "\n",
      "======================================================================\n",
      "H1 VERDICT: PRECISION HYPOTHESIS\n",
      "======================================================================\n",
      "\n",
      "  >>> H1 REJECTED: fp32 correction DOES NOT RECOVER priming on Gemma (gain = -0.001)\n",
      "\n",
      "  Details:\n",
      "    sf_trunc (bf16):     d = -0.031\n",
      "    sf_trunc_fp32:       d = -0.032\n",
      "    precision gain:      -0.001\n",
      "    C1 test p-value:     4.42e-01\n",
      "    noise baseline:      d = -0.019\n",
      "\n",
      "  Cross-reference:\n",
      "    Mistral static_fact: d = +0.472\n",
      "    Gemma sf_trunc_fp32: d = -0.032\n",
      "    Recovery fraction:   -6.9%\n",
      "\n",
      "======================================================================\n",
      "H2 VERDICT: SELECTIVE CONTAMINATION\n",
      "======================================================================\n",
      "\n",
      "  >>> H2 SUPPORTED: Selective contamination AMPLIFIES the value signal\n",
      "\n",
      "  Details:\n",
      "    values_only (all):    d = +0.056\n",
      "    values_early_layers:  d = +0.211 (*)\n",
      "    values_early_pos:     d = +0.010 (*)\n",
      "    values_alpha_25:      d = +0.081 (ns)\n",
      "\n",
      "  Exp 09 Mistral comparison:\n",
      "    Mistral layers 0-15:  ~88% of full signal\n",
      "    Gemma layers 0-16:    375% of values_only\n",
      "    Mistral alpha=0.25:   ~86% of full signal\n",
      "    Gemma alpha=0.25:     143% of values_only\n",
      "\n",
      "======================================================================\n",
      "KEY INTERFERENCE DECOMPOSITION\n",
      "======================================================================\n",
      "\n",
      "  bf16 pipeline:  values d=+0.056 + keys d=-0.088 = total d=-0.031\n",
      "  fp32 pipeline:  values d=+0.056 + keys d=-0.089 = total d=-0.032\n",
      "\n",
      "  Key interference reduced by fp32: -0.001\n",
      "    bf16 interference: -0.088\n",
      "    fp32 interference: -0.089\n",
      "\n",
      "======================================================================\n",
      "HARDNESS INTERACTION\n",
      "======================================================================\n",
      "\n",
      "Condition                     Q1 (easy)            Q2            Q3            Q4     Q5 (hard)       Overall\n",
      "-------------------------------------------------------------------------------------------------------------\n",
      "sf_trunc                         -0.275        -0.248        -0.134        -0.036        +0.148        -0.031\n",
      "sf_trunc_fp32                    -0.277        -0.248        -0.134        -0.040        +0.145        -0.032\n",
      "values_only                      -0.242        -0.083        +0.020        +0.083        +0.219        +0.056\n",
      "values_early_layers              -0.112        +0.142        +0.311        +0.270        +0.365        +0.211\n",
      "values_alpha_25                  -0.074        +0.031        +0.138        +0.117        +0.144        +0.081\n",
      "\n",
      "Hardness correlation (bare NLL vs delta):\n",
      "  sf_trunc                  r=+0.125  p=5.69e-09  ***\n",
      "  sf_trunc_fp32             r=+0.126  p=3.59e-09  ***\n",
      "  values_only               r=+0.148  p=4.18e-12  ***\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Analysis — H1 verdict, H2 verdict, key interference decomposition\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ANALYSIS — GEMMA PRECISION FIX & SELECTIVE VALUE CONTAMINATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "N_VALID = len(all_results)\n",
    "print(f\"Valid queries: {N_VALID}\")\n",
    "\n",
    "# --- Collect per-passage NLLs ---\n",
    "cond_nlls = {cn: [] for cn in CONDITION_NAMES}\n",
    "for r in all_results:\n",
    "    for p in r['passage_data']:\n",
    "        for cn in CONDITION_NAMES:\n",
    "            cond_nlls[cn].append(p[f'{cn}_nll'])\n",
    "\n",
    "cond_arrays = {cn: np.array(vals) for cn, vals in cond_nlls.items()}\n",
    "\n",
    "# Filter zero NLLs\n",
    "valid = np.ones(len(cond_arrays['bare']), dtype=bool)\n",
    "for cn in CONDITION_NAMES:\n",
    "    valid &= (cond_arrays[cn] != 0)\n",
    "n_passages_valid = int(np.sum(valid))\n",
    "n_excluded = int(np.sum(~valid))\n",
    "print(f\"Total passages: {len(valid)}, Valid: {n_passages_valid}, Excluded: {n_excluded}\")\n",
    "\n",
    "c = {}\n",
    "for cn in CONDITION_NAMES:\n",
    "    c[cn] = cond_arrays[cn][valid]\n",
    "\n",
    "# === 1. NLL Summary Table ===\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"NLL SUMMARY (per-passage, Gemma 3 4B)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\n{'Condition':<25} {'Mean NLL':>10} {'Std':>10} {'d vs Bare':>10} {'Win%':>8}\")\n",
    "print(\"-\" * 68)\n",
    "\n",
    "gemma_ds = {}\n",
    "for cn in CONDITION_NAMES:\n",
    "    mean_nll = np.mean(c[cn])\n",
    "    std_nll = np.std(c[cn])\n",
    "    if cn == 'bare':\n",
    "        print(f\"{cn:<25} {mean_nll:>10.4f} {std_nll:>10.4f} {'---':>10} {'---':>8}\")\n",
    "    else:\n",
    "        delta = c['bare'] - c[cn]\n",
    "        d = cohens_d(delta)\n",
    "        win = np.mean(delta > 0) * 100\n",
    "        gemma_ds[cn] = d\n",
    "        print(f\"{cn:<25} {mean_nll:>10.4f} {std_nll:>10.4f} {d:>+10.3f} {win:>7.1f}%\")\n",
    "\n",
    "# === 2. Statistical Tests (7 primary comparisons) ===\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(f\"7 PRIMARY COMPARISONS (Bonferroni alpha = {BONFERRONI_ALPHA:.4f})\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "comparisons = [\n",
    "    ('C1: fp32 vs bf16',\n",
    "     c['sf_trunc'] - c['sf_trunc_fp32'],\n",
    "     'Does fp32 precision help?'),\n",
    "    ('C2: nocorr vs bare',\n",
    "     c['bare'] - c['sf_trunc_nocorr'],\n",
    "     'Does uncorrected truncation hurt?'),\n",
    "    ('C3: nocorr vs bf16',\n",
    "     c['sf_trunc'] - c['sf_trunc_nocorr'],\n",
    "     'Is correction better than no correction?'),\n",
    "    ('C4: values_only vs bare',\n",
    "     c['bare'] - c['values_only'],\n",
    "     'Replicate Exp 16 d=+0.056'),\n",
    "    ('C5: early_layers vs values_only',\n",
    "     c['values_only'] - c['values_early_layers'],\n",
    "     'Does layer selectivity help?'),\n",
    "    ('C6: early_pos vs values_only',\n",
    "     c['values_only'] - c['values_early_pos'],\n",
    "     'Does position selectivity help?'),\n",
    "    ('C7: alpha_25 vs values_only',\n",
    "     c['values_only'] - c['values_alpha_25'],\n",
    "     'Does reduced dose help?'),\n",
    "]\n",
    "\n",
    "print(f\"\\n{'Comparison':<35} {'Mean D':>8} {'d':>8} {'Win%':>7} {'t':>8} {'p':>12} {'Sig':>5}\")\n",
    "print(\"-\" * 90)\n",
    "\n",
    "comparison_results = {}\n",
    "for name, delta, question in comparisons:\n",
    "    d = cohens_d(delta)\n",
    "    win = np.mean(delta > 0) * 100\n",
    "    t_stat, p_val = stats.ttest_1samp(delta, 0)\n",
    "    sig = \"***\" if p_val < 0.001 else \"**\" if p_val < BONFERRONI_ALPHA else \"*\" if p_val < 0.05 else \"ns\"\n",
    "    bonf_sig = p_val < BONFERRONI_ALPHA\n",
    "    print(f\"{name:<35} {np.mean(delta):>8.4f} {d:>+8.3f} {win:>6.1f}% {t_stat:>8.2f} {p_val:>11.2e} {sig:>5}\")\n",
    "    comparison_results[name] = {\n",
    "        'mean_delta': float(np.mean(delta)),\n",
    "        'cohens_d': float(d),\n",
    "        'win_rate': float(win / 100),\n",
    "        't_stat': float(t_stat),\n",
    "        'p_value': float(p_val),\n",
    "        'bonferroni_significant': bool(bonf_sig),\n",
    "        'question': question,\n",
    "    }\n",
    "\n",
    "# === 3. Key Derived Metrics ===\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"KEY DERIVED METRICS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "d_sf = gemma_ds.get('sf_trunc', 0)\n",
    "d_fp32 = gemma_ds.get('sf_trunc_fp32', 0)\n",
    "d_nocorr = gemma_ds.get('sf_trunc_nocorr', 0)\n",
    "d_vo = gemma_ds.get('values_only', 0)\n",
    "d_el = gemma_ds.get('values_early_layers', 0)\n",
    "d_ep = gemma_ds.get('values_early_pos', 0)\n",
    "d_a25 = gemma_ds.get('values_alpha_25', 0)\n",
    "d_rt = gemma_ds.get('rope_roundtrip', 0)\n",
    "\n",
    "precision_gain = d_fp32 - d_sf\n",
    "key_interference_bf16 = d_vo - d_sf\n",
    "key_interference_fp32 = d_vo - d_fp32\n",
    "noise_baseline = d_rt\n",
    "best_selective = max(d_el, d_ep, d_a25)\n",
    "best_selective_name = ['values_early_layers', 'values_early_pos', 'values_alpha_25'][\n",
    "    [d_el, d_ep, d_a25].index(best_selective)\n",
    "]\n",
    "\n",
    "print(f\"\\n  Precision gain (fp32 - bf16):    {precision_gain:+.3f}\")\n",
    "print(f\"    sf_trunc (bf16): d = {d_sf:+.3f}\")\n",
    "print(f\"    sf_trunc_fp32:   d = {d_fp32:+.3f}\")\n",
    "print(f\"\")\n",
    "print(f\"  Key interference (bf16):          {key_interference_bf16:+.3f}\")\n",
    "print(f\"    = d(values_only) - d(sf_trunc)\")\n",
    "print(f\"    = {d_vo:+.3f} - ({d_sf:+.3f})\")\n",
    "print(f\"\")\n",
    "print(f\"  Key interference (fp32):          {key_interference_fp32:+.3f}\")\n",
    "print(f\"    = d(values_only) - d(sf_trunc_fp32)\")\n",
    "print(f\"    = {d_vo:+.3f} - ({d_fp32:+.3f})\")\n",
    "print(f\"    (If smaller than bf16 interference, precision matters)\")\n",
    "print(f\"\")\n",
    "print(f\"  Noise baseline (rope_roundtrip):  {noise_baseline:+.3f}\")\n",
    "print(f\"    (Pure bfloat16 roundtrip noise cost, no content)\")\n",
    "print(f\"\")\n",
    "print(f\"  Best selective condition:          {best_selective_name} d={best_selective:+.3f}\")\n",
    "\n",
    "# === 4. H1 VERDICT ===\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"H1 VERDICT: PRECISION HYPOTHESIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "c1_result = comparison_results.get('C1: fp32 vs bf16', {})\n",
    "c1_sig = c1_result.get('bonferroni_significant', False)\n",
    "c1_d = c1_result.get('cohens_d', 0)\n",
    "\n",
    "if c1_sig and precision_gain > 0.05:\n",
    "    h1_verdict = \"SUPPORTED\"\n",
    "    h1_msg = f\"fp32 correction RECOVERS priming on Gemma (gain = {precision_gain:+.3f})\"\n",
    "elif precision_gain > 0 and not c1_sig:\n",
    "    h1_verdict = \"TREND\"\n",
    "    h1_msg = f\"fp32 shows trend toward recovery ({precision_gain:+.3f}) but not significant\"\n",
    "else:\n",
    "    h1_verdict = \"REJECTED\"\n",
    "    h1_msg = f\"fp32 correction DOES NOT RECOVER priming on Gemma (gain = {precision_gain:+.3f})\"\n",
    "\n",
    "print(f\"\\n  >>> H1 {h1_verdict}: {h1_msg}\")\n",
    "print(f\"\")\n",
    "print(f\"  Details:\")\n",
    "print(f\"    sf_trunc (bf16):     d = {d_sf:+.3f}\")\n",
    "print(f\"    sf_trunc_fp32:       d = {d_fp32:+.3f}\")\n",
    "print(f\"    precision gain:      {precision_gain:+.3f}\")\n",
    "print(f\"    C1 test p-value:     {c1_result.get('p_value', 1):.2e}\")\n",
    "print(f\"    noise baseline:      d = {d_rt:+.3f}\")\n",
    "print(f\"\")\n",
    "print(f\"  Cross-reference:\")\n",
    "print(f\"    Mistral static_fact: d = {MISTRAL_REF['static_fact_trunc_d']:+.3f}\")\n",
    "print(f\"    Gemma sf_trunc_fp32: d = {d_fp32:+.3f}\")\n",
    "print(f\"    Recovery fraction:   {d_fp32 / MISTRAL_REF['static_fact_trunc_d'] * 100:.1f}%\" if MISTRAL_REF['static_fact_trunc_d'] != 0 else \"\")\n",
    "\n",
    "# === 5. H2 VERDICT ===\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"H2 VERDICT: SELECTIVE CONTAMINATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "any_selective_better = (d_el > d_vo + 0.01) or (d_ep > d_vo + 0.01) or (d_a25 > d_vo + 0.01)\n",
    "c5_sig = comparison_results.get('C5: early_layers vs values_only', {}).get('bonferroni_significant', False)\n",
    "c6_sig = comparison_results.get('C6: early_pos vs values_only', {}).get('bonferroni_significant', False)\n",
    "c7_sig = comparison_results.get('C7: alpha_25 vs values_only', {}).get('bonferroni_significant', False)\n",
    "\n",
    "if any_selective_better and (c5_sig or c6_sig or c7_sig):\n",
    "    h2_verdict = \"SUPPORTED\"\n",
    "    h2_msg = \"Selective contamination AMPLIFIES the value signal\"\n",
    "elif any_selective_better:\n",
    "    h2_verdict = \"TREND\"\n",
    "    h2_msg = f\"Selective conditions show trends but not significant\"\n",
    "else:\n",
    "    h2_verdict = \"REJECTED\"\n",
    "    h2_msg = \"Selective contamination DOES NOT AMPLIFY the value signal\"\n",
    "\n",
    "print(f\"\\n  >>> H2 {h2_verdict}: {h2_msg}\")\n",
    "print(f\"\")\n",
    "print(f\"  Details:\")\n",
    "print(f\"    values_only (all):    d = {d_vo:+.3f}\")\n",
    "print(f\"    values_early_layers:  d = {d_el:+.3f} ({'*' if c5_sig else 'ns'})\")\n",
    "print(f\"    values_early_pos:     d = {d_ep:+.3f} ({'*' if c6_sig else 'ns'})\")\n",
    "print(f\"    values_alpha_25:      d = {d_a25:+.3f} ({'*' if c7_sig else 'ns'})\")\n",
    "print(f\"\")\n",
    "print(f\"  Exp 09 Mistral comparison:\")\n",
    "print(f\"    Mistral layers 0-15:  ~88% of full signal\")\n",
    "print(f\"    Gemma layers 0-16:    {d_el/d_vo*100:.0f}% of values_only\" if d_vo != 0 else \"    Gemma: d_vo = 0\")\n",
    "print(f\"    Mistral alpha=0.25:   ~86% of full signal\")\n",
    "print(f\"    Gemma alpha=0.25:     {d_a25/d_vo*100:.0f}% of values_only\" if d_vo != 0 else \"    Gemma: d_vo = 0\")\n",
    "\n",
    "# === 6. Key Interference Decomposition ===\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"KEY INTERFERENCE DECOMPOSITION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\n  bf16 pipeline:  values d={d_vo:+.3f} + keys d={d_sf - d_vo:+.3f} = total d={d_sf:+.3f}\")\n",
    "print(f\"  fp32 pipeline:  values d={d_vo:+.3f} + keys d={d_fp32 - d_vo:+.3f} = total d={d_fp32:+.3f}\")\n",
    "print(f\"\")\n",
    "print(f\"  Key interference reduced by fp32: {key_interference_bf16 - key_interference_fp32:+.3f}\")\n",
    "print(f\"    bf16 interference: {-key_interference_bf16:+.3f}\")\n",
    "print(f\"    fp32 interference: {-key_interference_fp32:+.3f}\")\n",
    "\n",
    "# === 7. Hardness Interaction ===\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"HARDNESS INTERACTION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "bare_all = c['bare']\n",
    "quintile_boundaries = np.percentile(bare_all, [20, 40, 60, 80])\n",
    "quintile_labels = ['Q1 (easy)', 'Q2', 'Q3', 'Q4', 'Q5 (hard)']\n",
    "\n",
    "def get_quintile(nll, boundaries):\n",
    "    for i, b in enumerate(boundaries):\n",
    "        if nll <= b:\n",
    "            return i\n",
    "    return len(boundaries)\n",
    "\n",
    "quintiles = np.array([get_quintile(nll, quintile_boundaries) for nll in bare_all])\n",
    "\n",
    "hardness_conds = ['sf_trunc', 'sf_trunc_fp32', 'values_only',\n",
    "                  'values_early_layers', 'values_alpha_25']\n",
    "\n",
    "header = f\"{'Condition':<25}\" + \"\".join(f\"{ql:>14}\" for ql in quintile_labels) + f\"{'Overall':>14}\"\n",
    "print(f\"\\n{header}\")\n",
    "print(\"-\" * (25 + 14 * 6))\n",
    "\n",
    "hardness_breakdown = {}\n",
    "for cn in hardness_conds:\n",
    "    row = f\"{cn:<25}\"\n",
    "    quintile_ds = []\n",
    "    for q in range(5):\n",
    "        mask_q = quintiles == q\n",
    "        if np.sum(mask_q) < 10:\n",
    "            row += f\"{'n/a':>14}\"\n",
    "            quintile_ds.append(None)\n",
    "        else:\n",
    "            delta = bare_all[mask_q] - c[cn][mask_q]\n",
    "            d_q = cohens_d(delta)\n",
    "            row += f\"{d_q:>+14.3f}\"\n",
    "            quintile_ds.append(float(d_q))\n",
    "    d_all = cohens_d(bare_all - c[cn])\n",
    "    row += f\"{d_all:>+14.3f}\"\n",
    "    print(row)\n",
    "    hardness_breakdown[cn] = {'quintile_ds': quintile_ds, 'overall_d': float(d_all)}\n",
    "\n",
    "# Hardness correlation\n",
    "print(\"\\nHardness correlation (bare NLL vs delta):\")\n",
    "for cn in ['sf_trunc', 'sf_trunc_fp32', 'values_only']:\n",
    "    delta = bare_all - c[cn]\n",
    "    r, p = stats.pearsonr(bare_all, delta)\n",
    "    sig = \"***\" if p < 0.001 else \"**\" if p < 0.01 else \"*\" if p < 0.05 else \"ns\"\n",
    "    print(f\"  {cn:<25} r={r:+.3f}  p={p:.2e}  {sig}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f0d29bc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T17:12:35.127061Z",
     "iopub.status.busy": "2026-02-15T17:12:35.126075Z",
     "iopub.status.idle": "2026-02-15T17:12:36.794611Z",
     "shell.execute_reply": "2026-02-15T17:12:36.793620Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plots saved to results/exp19/analysis_plots.png\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Plots — 4-panel figure\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Color scheme\n",
    "colors_h1 = {\n",
    "    'sf_trunc': '#d62728',\n",
    "    'sf_trunc_fp32': '#2ca02c',\n",
    "    'sf_trunc_nocorr': '#ff7f0e',\n",
    "    'rope_roundtrip': '#7f7f7f',\n",
    "}\n",
    "colors_h2 = {\n",
    "    'values_only': '#1f77b4',\n",
    "    'values_early_layers': '#9467bd',\n",
    "    'values_early_pos': '#8c564b',\n",
    "    'values_alpha_25': '#e377c2',\n",
    "}\n",
    "\n",
    "# --- Panel 1 (top-left): H1 — RoPE Precision ---\n",
    "ax = axes[0, 0]\n",
    "h1_conds = ['sf_trunc', 'sf_trunc_fp32', 'sf_trunc_nocorr', 'rope_roundtrip']\n",
    "h1_ds = [gemma_ds.get(cn, 0) for cn in h1_conds]\n",
    "h1_colors = [colors_h1[cn] for cn in h1_conds]\n",
    "h1_labels = ['sf_trunc\\n(bf16)', 'sf_trunc\\n(fp32)', 'sf_trunc\\n(no corr)', 'rope\\nroundtrip']\n",
    "\n",
    "bars = ax.bar(range(len(h1_conds)), h1_ds, color=h1_colors, edgecolor='black', linewidth=0.5)\n",
    "# values_only reference line\n",
    "ax.axhline(y=d_vo, color='#1f77b4', linestyle='--', linewidth=1.5,\n",
    "           label=f'values_only (d={d_vo:+.3f})')\n",
    "ax.axhline(y=0, color='gray', linestyle='-', alpha=0.3)\n",
    "ax.set_xticks(range(len(h1_conds)))\n",
    "ax.set_xticklabels(h1_labels, fontsize=8)\n",
    "ax.set_ylabel(\"Cohen's d vs Bare\")\n",
    "ax.set_title(\"H1: RoPE Precision\")\n",
    "ax.legend(fontsize=8)\n",
    "for i, v in enumerate(h1_ds):\n",
    "    ax.text(i, v + 0.003 if v >= 0 else v - 0.012, f\"{v:+.3f}\",\n",
    "            ha='center', va='bottom' if v >= 0 else 'top', fontsize=9)\n",
    "\n",
    "# --- Panel 2 (top-right): H2 — Selective Contamination ---\n",
    "ax = axes[0, 1]\n",
    "h2_conds = ['values_only', 'values_early_layers', 'values_early_pos', 'values_alpha_25']\n",
    "h2_ds = [gemma_ds.get(cn, 0) for cn in h2_conds]\n",
    "h2_colors = [colors_h2[cn] for cn in h2_conds]\n",
    "h2_labels = ['values\\nonly (all)', 'early\\nlayers', 'early\\npositions', 'alpha\\n0.25']\n",
    "\n",
    "bars = ax.bar(range(len(h2_conds)), h2_ds, color=h2_colors, edgecolor='black', linewidth=0.5)\n",
    "ax.axhline(y=0, color='gray', linestyle='-', alpha=0.3)\n",
    "ax.set_xticks(range(len(h2_conds)))\n",
    "ax.set_xticklabels(h2_labels, fontsize=8)\n",
    "ax.set_ylabel(\"Cohen's d vs Bare\")\n",
    "ax.set_title(\"H2: Selective Contamination\")\n",
    "for i, v in enumerate(h2_ds):\n",
    "    ax.text(i, v + 0.003 if v >= 0 else v - 0.012, f\"{v:+.3f}\",\n",
    "            ha='center', va='bottom' if v >= 0 else 'top', fontsize=9)\n",
    "\n",
    "# --- Panel 3 (bottom-left): Key Interference Decomposition (waterfall) ---\n",
    "ax = axes[1, 0]\n",
    "\n",
    "# Waterfall: show values contribution + key interference for bf16 and fp32\n",
    "categories = ['Values\\n(shared)', 'Keys\\n(bf16)', 'Total\\n(bf16)',\n",
    "              'Values\\n(shared)', 'Keys\\n(fp32)', 'Total\\n(fp32)']\n",
    "values_contrib = d_vo\n",
    "keys_bf16 = d_sf - d_vo\n",
    "total_bf16 = d_sf\n",
    "keys_fp32 = d_fp32 - d_vo\n",
    "total_fp32 = d_fp32\n",
    "\n",
    "# Draw as grouped bars\n",
    "x_pos = [0, 1, 2, 3.5, 4.5, 5.5]\n",
    "bar_vals = [values_contrib, keys_bf16, total_bf16, values_contrib, keys_fp32, total_fp32]\n",
    "bar_colors = ['#1f77b4', '#d62728', '#2c2c2c', '#1f77b4', '#2ca02c', '#2c2c2c']\n",
    "\n",
    "bars = ax.bar(x_pos, bar_vals, color=bar_colors, edgecolor='black', linewidth=0.5, width=0.8)\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels(categories, fontsize=7)\n",
    "ax.axhline(y=0, color='gray', linestyle='-', alpha=0.5)\n",
    "ax.set_ylabel(\"Cohen's d component\")\n",
    "ax.set_title(\"Key Interference Decomposition\")\n",
    "\n",
    "# Add labels\n",
    "for i, (xp, v) in enumerate(zip(x_pos, bar_vals)):\n",
    "    ax.text(xp, v + 0.003 if v >= 0 else v - 0.012, f\"{v:+.3f}\",\n",
    "            ha='center', va='bottom' if v >= 0 else 'top', fontsize=8)\n",
    "\n",
    "# Add bf16/fp32 section labels\n",
    "ax.text(1, ax.get_ylim()[1] * 0.9, 'bfloat16', ha='center', fontsize=10, fontstyle='italic')\n",
    "ax.text(4.5, ax.get_ylim()[1] * 0.9, 'float32', ha='center', fontsize=10, fontstyle='italic')\n",
    "\n",
    "# --- Panel 4 (bottom-right): Hardness Interaction scatter ---\n",
    "ax = axes[1, 1]\n",
    "\n",
    "# Scatter for sf_trunc_fp32 and values_only\n",
    "delta_fp32 = c['bare'] - c['sf_trunc_fp32']\n",
    "delta_vo = c['bare'] - c['values_only']\n",
    "\n",
    "ax.scatter(c['bare'], delta_fp32, alpha=0.1, s=6, color='#2ca02c', label='sf_trunc_fp32')\n",
    "ax.scatter(c['bare'], delta_vo, alpha=0.1, s=6, color='#1f77b4', label='values_only')\n",
    "\n",
    "# Fit lines\n",
    "z_fp32 = np.polyfit(c['bare'], delta_fp32, 1)\n",
    "z_vo = np.polyfit(c['bare'], delta_vo, 1)\n",
    "x_fit = np.linspace(c['bare'].min(), c['bare'].max(), 100)\n",
    "ax.plot(x_fit, np.polyval(z_fp32, x_fit), color='#2ca02c', linewidth=2, linestyle='--')\n",
    "ax.plot(x_fit, np.polyval(z_vo, x_fit), color='#1f77b4', linewidth=2, linestyle='--')\n",
    "\n",
    "r_fp32, p_fp32 = stats.pearsonr(c['bare'], delta_fp32)\n",
    "r_vo, p_vo = stats.pearsonr(c['bare'], delta_vo)\n",
    "\n",
    "ax.set_xlabel(\"Bare NLL (difficulty)\")\n",
    "ax.set_ylabel(\"ΔNLL (bare - condition)\")\n",
    "ax.set_title(f\"Hardness: fp32 r={r_fp32:+.3f}, values r={r_vo:+.3f}\")\n",
    "ax.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "ax.legend(fontsize=8, markerscale=3)\n",
    "\n",
    "plt.suptitle('Exp 19: Gemma Precision Fix & Selective Value Contamination', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'analysis_plots.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"Plots saved to {RESULTS_DIR / 'analysis_plots.png'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "40358fdf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T17:12:36.798247Z",
     "iopub.status.busy": "2026-02-15T17:12:36.797922Z",
     "iopub.status.idle": "2026-02-15T17:12:36.895928Z",
     "shell.execute_reply": "2026-02-15T17:12:36.894954Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to results/exp19/results.json\n",
      "File size: 1374.3 KB\n",
      "\n",
      "======================================================================\n",
      "SUMMARY — Exp 19: Gemma Precision & Selectivity\n",
      "======================================================================\n",
      "Model: Gemma 3 4B (34 layers, head_dim=256, bfloat16)\n",
      "Dataset: MS MARCO v1.1 (300 queries, 2174 passages)\n",
      "\n",
      "Effect sizes (Cohen's d vs bare):\n",
      "  sf_trunc                  d=-0.031\n",
      "  sf_trunc_fp32             d=-0.032\n",
      "  sf_trunc_nocorr           d=-0.009\n",
      "  values_only               d=+0.056\n",
      "  values_early_layers       d=+0.211\n",
      "  values_early_pos          d=+0.010\n",
      "  values_alpha_25           d=+0.081\n",
      "  rope_roundtrip            d=-0.019\n",
      "\n",
      "H1 (Precision):    REJECTED — fp32 correction DOES NOT RECOVER priming on Gemma (gain = -0.001)\n",
      "H2 (Selectivity):  SUPPORTED — Selective contamination AMPLIFIES the value signal\n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Save results JSON\n",
    "final = {\n",
    "    'experiment': 'exp19_gemma_precision_and_selectivity',\n",
    "    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'config': {\n",
    "        'model_name': MODEL_NAME,\n",
    "        'model_type': 'gemma3',\n",
    "        'seed': SEED,\n",
    "        'n_queries': N,\n",
    "        'n_valid': N_VALID,\n",
    "        'n_passages_valid': n_passages_valid,\n",
    "        'n_passages_excluded': n_excluded,\n",
    "        'max_passage_words': MAX_PASSAGE_WORDS,\n",
    "        'min_passages_per_query': MIN_PASSAGES_PER_QUERY,\n",
    "        'dataset': 'MS MARCO v1.1 validation',\n",
    "        'n_comparisons': N_COMPARISONS,\n",
    "        'bonferroni_alpha': BONFERRONI_ALPHA,\n",
    "    },\n",
    "    'gemma_architecture': {\n",
    "        'hidden_size': text_config.hidden_size,\n",
    "        'num_layers': text_config.num_hidden_layers,\n",
    "        'num_attention_heads': text_config.num_attention_heads,\n",
    "        'num_kv_heads': text_config.num_key_value_heads,\n",
    "        'head_dim': _get_head_dim(model.config),\n",
    "        'rope_thetas': sorted(list(thetas)),\n",
    "    },\n",
    "    'condition_names': CONDITION_NAMES,\n",
    "    'nll_summary': {\n",
    "        cn: {\n",
    "            'mean': float(np.mean(c[cn])),\n",
    "            'std': float(np.std(c[cn])),\n",
    "            'cohens_d_vs_bare': float(cohens_d(c['bare'] - c[cn])) if cn != 'bare' else 0.0,\n",
    "        }\n",
    "        for cn in CONDITION_NAMES\n",
    "    },\n",
    "    'primary_comparisons': comparison_results,\n",
    "    'derived_metrics': {\n",
    "        'precision_gain': float(precision_gain),\n",
    "        'key_interference_bf16': float(key_interference_bf16),\n",
    "        'key_interference_fp32': float(key_interference_fp32),\n",
    "        'noise_baseline': float(noise_baseline),\n",
    "        'best_selective_condition': best_selective_name,\n",
    "        'best_selective_d': float(best_selective),\n",
    "    },\n",
    "    'verdicts': {\n",
    "        'h1_precision': h1_verdict,\n",
    "        'h1_message': h1_msg,\n",
    "        'h2_selectivity': h2_verdict,\n",
    "        'h2_message': h2_msg,\n",
    "    },\n",
    "    'reference_values': {\n",
    "        'exp16_gemma': EXP16_REF,\n",
    "        'exp09_mistral_selectivity': {k: str(v) for k, v in EXP09_REF.items()},\n",
    "        'mistral': MISTRAL_REF,\n",
    "    },\n",
    "    'hardness_breakdown': hardness_breakdown,\n",
    "    'per_query_results': all_results,\n",
    "}\n",
    "\n",
    "with open(FINAL_RESULTS_PATH, 'w') as f:\n",
    "    json.dump(final, f, indent=2)\n",
    "\n",
    "print(f\"Results saved to {FINAL_RESULTS_PATH}\")\n",
    "print(f\"File size: {FINAL_RESULTS_PATH.stat().st_size / 1024:.1f} KB\")\n",
    "\n",
    "# Print final summary\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"SUMMARY — Exp 19: Gemma Precision & Selectivity\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Model: Gemma 3 4B (34 layers, head_dim=256, bfloat16)\")\n",
    "print(f\"Dataset: MS MARCO v1.1 ({N} queries, {n_passages_valid} passages)\")\n",
    "print(f\"\\nEffect sizes (Cohen's d vs bare):\")\n",
    "for cn in CONDITION_NAMES[1:]:\n",
    "    d_val = gemma_ds.get(cn, 0)\n",
    "    print(f\"  {cn:<25} d={d_val:>+.3f}\")\n",
    "print(f\"\\nH1 (Precision):    {h1_verdict} — {h1_msg}\")\n",
    "print(f\"H2 (Selectivity):  {h2_verdict} — {h2_msg}\")\n",
    "print(f\"\\nDone!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b7c2ed3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T17:12:36.900151Z",
     "iopub.status.busy": "2026-02-15T17:12:36.899837Z",
     "iopub.status.idle": "2026-02-15T17:12:37.653468Z",
     "shell.execute_reply": "2026-02-15T17:12:37.652561Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning up GPU memory...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory: 3.24 GB -> 0.01 GB\n",
      "Cleanup complete.\n"
     ]
    }
   ],
   "source": [
    "# Cell 10: GPU cleanup\n",
    "import gc\n",
    "\n",
    "print(\"Cleaning up GPU memory...\")\n",
    "mem_before = torch.cuda.memory_allocated() / 1e9\n",
    "\n",
    "del model\n",
    "del tokenizer\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "mem_after = torch.cuda.memory_allocated() / 1e9\n",
    "print(f\"GPU memory: {mem_before:.2f} GB -> {mem_after:.2f} GB\")\n",
    "print(\"Cleanup complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "029866d92ff4432694e46fb8b7538736": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "050affbc782149a3a09062a5e6646bf1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_f9870b6595ea4ff1ab87fb128abe6bb3",
       "placeholder": "​",
       "style": "IPY_MODEL_adb8c1796f67454e9bfc3746cb7457a9",
       "tabbable": null,
       "tooltip": null,
       "value": " 300/300 [2:18:19&lt;00:00, 25.17s/it]"
      }
     },
     "09c147d408bd4885ae055d37c7ed4e79": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "0e173118628147708ce5c4af09e441db": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "1d6caf2fd94343a5a34efea9ef775959": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_6730e601d3514795a131f3050c965971",
       "max": 300.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_3aaf2f719ac9440a8cf59abee2f06526",
       "tabbable": null,
       "tooltip": null,
       "value": 300.0
      }
     },
     "1dc2730f5fe6438aac15a5952445fe99": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "2350cd9d6b73433cb5d962c42c312b03": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "27d177bf94ae49e79a89e52983b42961": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2a204b32ca4f47ebbff21ae473b9c5ed": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "3aaf2f719ac9440a8cf59abee2f06526": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "504a0399e0c14e0eb659c169867ae3f7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "56bda748d72f4bd597505d8115d12b86": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_27d177bf94ae49e79a89e52983b42961",
       "placeholder": "​",
       "style": "IPY_MODEL_2a204b32ca4f47ebbff21ae473b9c5ed",
       "tabbable": null,
       "tooltip": null,
       "value": " 883/883 [00:04&lt;00:00, 669.27it/s, Materializing param=model.vision_tower.vision_model.post_layernorm.weight]"
      }
     },
     "5bd9ae7dc7d24b39a0a8aefe80f73fa1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "615b4df6b3f64ac5880b33aa6f391421": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "6609a1772d854563ba5c1d003d238496": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "danger",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_fac91871eec34269be0c46c605c4c7a2",
       "max": 10047.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_09c147d408bd4885ae055d37c7ed4e79",
       "tabbable": null,
       "tooltip": null,
       "value": 925.0
      }
     },
     "6730e601d3514795a131f3050c965971": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "68715492e8b04e57be9d462d8566f989": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6defde7fe4434d49ba1de925e336c2dc": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "8817e7a500f6462588a14e91af4ab5d3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_0e173118628147708ce5c4af09e441db",
       "max": 883.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_1dc2730f5fe6438aac15a5952445fe99",
       "tabbable": null,
       "tooltip": null,
       "value": 883.0
      }
     },
     "9db601de2617476ca4152949247bb95d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a5a2a5209d684cc19f1ba5326291e2b3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_9db601de2617476ca4152949247bb95d",
       "placeholder": "​",
       "style": "IPY_MODEL_029866d92ff4432694e46fb8b7538736",
       "tabbable": null,
       "tooltip": null,
       "value": " 925/10047 [00:00&lt;00:01, 5570.97it/s]"
      }
     },
     "adb8c1796f67454e9bfc3746cb7457a9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "adbedae1b9b945f6b9d58bafe55b20af": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_cc8bbcf31e884268aec37d03fc9feb46",
        "IPY_MODEL_6609a1772d854563ba5c1d003d238496",
        "IPY_MODEL_a5a2a5209d684cc19f1ba5326291e2b3"
       ],
       "layout": "IPY_MODEL_6defde7fe4434d49ba1de925e336c2dc",
       "tabbable": null,
       "tooltip": null
      }
     },
     "cc8bbcf31e884268aec37d03fc9feb46": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_68715492e8b04e57be9d462d8566f989",
       "placeholder": "​",
       "style": "IPY_MODEL_2350cd9d6b73433cb5d962c42c312b03",
       "tabbable": null,
       "tooltip": null,
       "value": "Filtering:   9%"
      }
     },
     "d89c570056724563904d9a90dfc81eda": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "dad51f18d5204ee68dd720851683abe9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "df539e2888914553b1487f039ef5ae78": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_d89c570056724563904d9a90dfc81eda",
       "placeholder": "​",
       "style": "IPY_MODEL_504a0399e0c14e0eb659c169867ae3f7",
       "tabbable": null,
       "tooltip": null,
       "value": "Queries: 100%"
      }
     },
     "f3781e01d3554a38999565ae584d7160": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_f98aaec01d30467fa807a1a22df11971",
        "IPY_MODEL_8817e7a500f6462588a14e91af4ab5d3",
        "IPY_MODEL_56bda748d72f4bd597505d8115d12b86"
       ],
       "layout": "IPY_MODEL_fb45a24484874d788ac2e141a8ef606a",
       "tabbable": null,
       "tooltip": null
      }
     },
     "f9870b6595ea4ff1ab87fb128abe6bb3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f98aaec01d30467fa807a1a22df11971": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_5bd9ae7dc7d24b39a0a8aefe80f73fa1",
       "placeholder": "​",
       "style": "IPY_MODEL_615b4df6b3f64ac5880b33aa6f391421",
       "tabbable": null,
       "tooltip": null,
       "value": "Loading weights: 100%"
      }
     },
     "fac91871eec34269be0c46c605c4c7a2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "fb1be520e3ec497dbc767c01190d759a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_df539e2888914553b1487f039ef5ae78",
        "IPY_MODEL_1d6caf2fd94343a5a34efea9ef775959",
        "IPY_MODEL_050affbc782149a3a09062a5e6646bf1"
       ],
       "layout": "IPY_MODEL_dad51f18d5204ee68dd720851683abe9",
       "tabbable": null,
       "tooltip": null
      }
     },
     "fb45a24484874d788ac2e141a8ef606a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
