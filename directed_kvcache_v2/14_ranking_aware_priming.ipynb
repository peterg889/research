{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exp 14: Ranking-Aware Priming \u2014 Does Priming Improve Ad Ranking?\n",
    "\n",
    "## Background & Motivation\n",
    "\n",
    "All prior experiments (Exps 01-13) measured priming benefit as per-document NLL deltas.\n",
    "But for ad serving, what matters is **ranking**: does priming help you rank the relevant\n",
    "document higher among candidates?\n",
    "\n",
    "Exp 13's per-example analysis found a \"sweet spot\" (short, hard, high-overlap docs) and\n",
    "showed that priming response correlates with query-doc overlap (r=+0.10). This suggests\n",
    "a **differential effect**: relevant passages may benefit more from priming than irrelevant\n",
    "ones, which would improve ranking even if average NLL worsens.\n",
    "\n",
    "MS MARCO v1.1 has ~10 candidate passages per query with `is_selected` relevance labels \u2014\n",
    "a natural ad-ranking simulation.\n",
    "\n",
    "## Key Hypotheses\n",
    "\n",
    "1. **Differential Effect**: Priming reduces NLL more for relevant vs irrelevant passages\n",
    "2. **Ranking Improvement**: Even if average NLL worsens, the differential improves MRR/Hit@k\n",
    "3. **Delta-as-Signal**: The NLL delta itself (bare - primed) is a relevance predictor (AUC > 0.5)\n",
    "4. **Gating Improvement**: Selective priming (only hard passages) beats always-prime\n",
    "\n",
    "## Conditions\n",
    "\n",
    "| # | Condition | Ranking Signal | Tests |\n",
    "|---|-----------|---------------|-------|\n",
    "| 1 | `bare` | bare NLL | Baseline |\n",
    "| 2 | `primed_1x` | static_fact truncated NLL | Does uniform priming help ranking? |\n",
    "| 3 | `primed_amp2x` | L0-15 2x amplified NLL | Does stronger contamination help? |\n",
    "| 4 | `oracle_gated` | primed if bare_nll > median, else bare | Selective priming |\n",
    "| 5 | `delta_signal` | bare_nll - primed_nll (as score) | Is priming response a relevance signal? |\n",
    "| 6 | `combined` | alpha*bare + (1-alpha)*(-delta), alpha tuned | Are NLL and delta complementary? |\n",
    "\n",
    "## Compute Budget\n",
    "~300 queries \u00d7 ~8 passages \u00d7 2 forward passes = ~4800 forward passes + ~4800 scoring passes.\n",
    "Estimated: ~60-90 min on L4."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 1: Setup\n",
    "import os\n",
    "os.umask(0o000)\n",
    "\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import gc\n",
    "from pathlib import Path\n",
    "\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "RESULTS_DIR = Path(\"results/exp14\")\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CHECKPOINT_PATH = RESULTS_DIR / \"checkpoint.json\"\n",
    "FINAL_RESULTS_PATH = RESULTS_DIR / \"results.json\"\n",
    "\n",
    "print(f\"SEED: {SEED}\")\n",
    "print(f\"Results directory: {RESULTS_DIR}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 2: Load model (Mistral-7B 4-bit)\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, DynamicCache\n",
    "\n",
    "MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "print(f\"Loading {MODEL_NAME} (4-bit)...\")\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "print(f\"Model loaded. dtype={model.dtype}, device={model.device}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 3: Config and library imports\n",
    "sys.path.insert(0, \".\")\n",
    "\n",
    "from lib.config import ExperimentConfig\n",
    "from lib.kv_cache import (\n",
    "    _get_cache_keys,\n",
    "    _get_cache_values,\n",
    "    _set_cache_keys,\n",
    "    _set_cache_values,\n",
    "    _ensure_dynamic_cache,\n",
    "    extract_and_truncate_cache_with_bos,\n",
    "    correct_rope_positions_with_bos,\n",
    "    score_answer_with_cache,\n",
    "    deepcopy_cache,\n",
    "    build_hybrid_cache,\n",
    ")\n",
    "from lib.analysis import cohens_d, compute_ranking_metrics, compute_token_overlap\n",
    "from lib.surrogate import STATIC_SURROGATE_QUERIES\n",
    "from lib.data import count_words\n",
    "from scipy import stats\n",
    "from scipy.stats import wilcoxon\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "config = ExperimentConfig(\n",
    "    model_name=MODEL_NAME,\n",
    "    num_samples=2000,\n",
    "    seed=SEED,\n",
    ")\n",
    "\n",
    "SURROGATE_PREFIX_TEMPLATE = \"{surrogate}\\n\"\n",
    "DOCUMENT_TEMPLATE = \"{document}\"\n",
    "QUERY_TEMPLATE = \"\\nQuery: {query}\\nAnswer:\"\n",
    "ANSWER_TEMPLATE = \" {answer}\"\n",
    "\n",
    "STATIC_FACT = STATIC_SURROGATE_QUERIES['static_factual']['query']\n",
    "\n",
    "MAX_QUERIES = 300\n",
    "MAX_PASSAGE_WORDS = 300\n",
    "MIN_PASSAGES_PER_QUERY = 2\n",
    "CHECKPOINT_EVERY = 25\n",
    "\n",
    "print(\"Config ready\")\n",
    "print(f\"  MAX_QUERIES: {MAX_QUERIES}\")\n",
    "print(f\"  MAX_PASSAGE_WORDS: {MAX_PASSAGE_WORDS}\")\n",
    "print(f\"  MIN_PASSAGES_PER_QUERY: {MIN_PASSAGES_PER_QUERY}\")\n",
    "print(f\"  static_fact: '{STATIC_FACT}'\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 4: Load ALL passages per query from MS MARCO v1.1\n",
    "from datasets import load_dataset\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"LOADING MS MARCO v1.1 \u2014 ALL PASSAGES PER QUERY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "dataset = load_dataset(\"microsoft/ms_marco\", \"v1.1\", split=\"validation\",\n",
    "                        trust_remote_code=True)\n",
    "print(f\"Total items in validation: {len(dataset)}\")\n",
    "\n",
    "# Filter: >=2 passages, >=1 selected, valid answer, all passages <300 words\n",
    "queries = []\n",
    "np.random.seed(SEED)\n",
    "\n",
    "for item in tqdm(dataset, desc=\"Filtering\"):\n",
    "    passages_info = item.get('passages', {})\n",
    "    passage_texts = passages_info.get('passage_text', [])\n",
    "    is_selected = passages_info.get('is_selected', [])\n",
    "    query = item.get('query', '')\n",
    "    answers = item.get('answers', [])\n",
    "    well_formed = item.get('wellFormedAnswers', [])\n",
    "\n",
    "    if not passage_texts or not query:\n",
    "        continue\n",
    "\n",
    "    # Need at least MIN_PASSAGES_PER_QUERY passages\n",
    "    if len(passage_texts) < MIN_PASSAGES_PER_QUERY:\n",
    "        continue\n",
    "\n",
    "    # Need at least 1 selected passage\n",
    "    if not is_selected or sum(is_selected) == 0:\n",
    "        continue\n",
    "\n",
    "    # All passages must be <= MAX_PASSAGE_WORDS\n",
    "    word_counts = [count_words(p) for p in passage_texts]\n",
    "    if any(wc > MAX_PASSAGE_WORDS for wc in word_counts):\n",
    "        continue\n",
    "\n",
    "    # Need a valid answer\n",
    "    answer = None\n",
    "    if well_formed and len(well_formed) > 0 and well_formed[0] != '[]':\n",
    "        answer = well_formed[0]\n",
    "    elif answers and len(answers) > 0 and answers[0] != 'No Answer Present.':\n",
    "        answer = answers[0]\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "    # Build passage list with relevance labels\n",
    "    passage_list = []\n",
    "    for i, (ptext, sel) in enumerate(zip(passage_texts, is_selected)):\n",
    "        passage_list.append({\n",
    "            'passage': ptext,\n",
    "            'is_relevant': bool(sel == 1),\n",
    "            'word_count': word_counts[i],\n",
    "            'passage_idx': i,\n",
    "        })\n",
    "\n",
    "    queries.append({\n",
    "        'query': query,\n",
    "        'answer': answer,\n",
    "        'passages': passage_list,\n",
    "        'n_passages': len(passage_list),\n",
    "        'n_relevant': sum(1 for p in passage_list if p['is_relevant']),\n",
    "    })\n",
    "\n",
    "    if len(queries) >= MAX_QUERIES * 3:\n",
    "        break\n",
    "\n",
    "# Shuffle and take MAX_QUERIES\n",
    "np.random.shuffle(queries)\n",
    "queries = queries[:MAX_QUERIES]\n",
    "N = len(queries)\n",
    "\n",
    "# Statistics\n",
    "n_passages_list = [q['n_passages'] for q in queries]\n",
    "n_relevant_list = [q['n_relevant'] for q in queries]\n",
    "total_passages = sum(n_passages_list)\n",
    "\n",
    "print(f\"\\nSelected {N} queries ({total_passages} total passages)\")\n",
    "print(f\"Passages per query: mean={np.mean(n_passages_list):.1f}, \"\n",
    "      f\"min={min(n_passages_list)}, max={max(n_passages_list)}\")\n",
    "print(f\"Relevant per query: mean={np.mean(n_relevant_list):.1f}, \"\n",
    "      f\"min={min(n_relevant_list)}, max={max(n_relevant_list)}\")\n",
    "print(f\"Word counts: mean={np.mean([p['word_count'] for q in queries for p in q['passages']]):.0f}\")\n",
    "\n",
    "del dataset\n",
    "gc.collect()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 5: Explain experimental conditions with concrete example\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"EXPERIMENTAL CONDITIONS \u2014 RANKING-AWARE PRIMING\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "sf_str = SURROGATE_PREFIX_TEMPLATE.format(surrogate=STATIC_FACT)\n",
    "sf_ids = tokenizer(sf_str, add_special_tokens=False)['input_ids']\n",
    "sf_tok_len = len(sf_ids)\n",
    "\n",
    "print(f\"\\nStatic fact prefix: '{STATIC_FACT}' ({sf_tok_len} tokens)\")\n",
    "print(f\"\\nFor each query with N candidate passages, we:\")\n",
    "print(f\"  1. Build bare cache for each passage (N forward passes)\")\n",
    "print(f\"  2. Build primed cache for each passage (N forward passes)\")\n",
    "print(f\"  3. Score answer with each cache (2N scoring passes)\")\n",
    "print(f\"  4. Compute rankings under 6 conditions (no extra passes)\")\n",
    "\n",
    "conditions_explained = [\n",
    "    (\"1. bare\",\n",
    "     \"Rank passages by bare NLL (lower = more relevant). Standard LM ranking baseline.\",\n",
    "     \"Does the model rank relevant passages lower-NLL without any help?\"),\n",
    "    (\"2. primed_1x\",\n",
    "     \"Rank passages by primed NLL using static_fact_trunc prefix. \"\n",
    "     \"Every passage gets the same prefix.\",\n",
    "     \"Does uniform priming improve ranking? If relevant passages benefit more \"\n",
    "     \"from priming than irrelevant ones, ranking improves even without query info.\"),\n",
    "    (\"3. primed_amp2x\",\n",
    "     \"Rank passages by NLL from an amplified priming cache (2x delta at layers 0-15). \"\n",
    "     \"Amplification boosts the contamination signal.\",\n",
    "     \"Does stronger contamination amplify the differential effect?\"),\n",
    "    (\"4. oracle_gated\",\n",
    "     \"Use primed NLL only for 'hard' passages (bare NLL > per-query median). \"\n",
    "     \"For 'easy' passages, use bare NLL. Threshold is per-query.\",\n",
    "     \"Does selective priming (only hard passages) beat always-prime?\"),\n",
    "    (\"5. delta_signal\",\n",
    "     \"Rank by the DELTA itself: bare_nll - primed_nll. Passages that benefit \"\n",
    "     \"more from priming are ranked higher. Ignores absolute NLL entirely.\",\n",
    "     \"Is the priming response itself a relevance signal? If relevant passages \"\n",
    "     \"respond more to priming, delta alone predicts relevance.\"),\n",
    "    (\"6. combined\",\n",
    "     \"Rank by alpha * bare_nll + (1-alpha) * (-delta), with alpha tuned via grid search. \"\n",
    "     \"Tests whether bare NLL and delta carry complementary information.\",\n",
    "     \"Are NLL and delta complementary signals? Best alpha = 1 means delta is useless; \"\n",
    "     \"best alpha = 0 means bare NLL is useless.\"),\n",
    "]\n",
    "\n",
    "for name, detail, test in conditions_explained:\n",
    "    print(f\"\\n### {name} ###\")\n",
    "    print(f\"  {detail}\")\n",
    "    print(f\"  Test: {test}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"CONCRETE EXAMPLE\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nQuery: 'What is the capital of France?'\")\n",
    "print(\"Passage A (relevant): 'Paris is the capital...' -> bare NLL=0.8, primed NLL=0.6\")\n",
    "print(\"Passage B (irrelevant): 'Berlin is in Germany...' -> bare NLL=1.5, primed NLL=1.4\")\n",
    "print()\n",
    "print(\"  bare ranking:      A(0.8) > B(1.5)   => A ranked #1 (correct)\")\n",
    "print(\"  primed ranking:    A(0.6) > B(1.4)   => A ranked #1 (correct)\")\n",
    "print(\"  delta ranking:     A(0.2) > B(0.1)   => A ranked #1 (delta predicts relevance)\")\n",
    "print(\"  If priming helps A more than B, ranking is preserved/improved.\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 6: Helper functions for ranking experiment\n",
    "\n",
    "def compute_delta(bare_cache, primed_cache, layers=None):\n",
    "    \"\"\"Compute per-position value delta between primed and bare caches.\n",
    "    Returns dict mapping layer_idx -> delta tensor.\n",
    "    \"\"\"\n",
    "    bare_cache = _ensure_dynamic_cache(bare_cache)\n",
    "    primed_cache = _ensure_dynamic_cache(primed_cache)\n",
    "    n_layers = len(bare_cache)\n",
    "    deltas = {}\n",
    "    layer_range = layers if layers is not None else range(n_layers)\n",
    "    for li in layer_range:\n",
    "        v_bare = _get_cache_values(bare_cache, li)\n",
    "        v_primed = _get_cache_values(primed_cache, li)\n",
    "        deltas[li] = v_primed - v_bare\n",
    "    return deltas\n",
    "\n",
    "\n",
    "def apply_delta(bare_cache, deltas, scale=1.0):\n",
    "    \"\"\"Apply scaled value delta to bare cache. Returns new DynamicCache.\"\"\"\n",
    "    bare_cache = _ensure_dynamic_cache(bare_cache)\n",
    "    n_layers = len(bare_cache)\n",
    "    new_cache = DynamicCache()\n",
    "    for li in range(n_layers):\n",
    "        k = _get_cache_keys(bare_cache, li)\n",
    "        v = _get_cache_values(bare_cache, li).clone()\n",
    "        if li in deltas:\n",
    "            v = v + deltas[li] * scale\n",
    "        new_cache.update(k, v, li)\n",
    "    return new_cache\n",
    "\n",
    "\n",
    "def score_all_passages_for_query(query_data, sf_prefix_ids, model, tokenizer, config):\n",
    "    \"\"\"Score all passages for a single query under bare and primed conditions.\n",
    "\n",
    "    Returns list of dicts, one per passage, with bare_nll, primed_nll, amp2x_nll.\n",
    "    Memory: builds and scores one passage at a time, freeing between passages.\n",
    "    \"\"\"\n",
    "    query = query_data['query']\n",
    "    answer = query_data['answer']\n",
    "    query_prompt = QUERY_TEMPLATE.format(query=query)\n",
    "    answer_text = ANSWER_TEMPLATE.format(answer=answer)\n",
    "\n",
    "    n_layers = model.config.num_hidden_layers\n",
    "\n",
    "    # Pre-tokenize the oracle prefix (for matched tokenization)\n",
    "    oracle_prefix = SURROGATE_PREFIX_TEMPLATE.format(surrogate=query)\n",
    "\n",
    "    passage_results = []\n",
    "\n",
    "    for pidx, pinfo in enumerate(query_data['passages']):\n",
    "        passage = pinfo['passage']\n",
    "\n",
    "        # --- Matched tokenization ---\n",
    "        document_text = DOCUMENT_TEMPLATE.format(document=passage)\n",
    "        full_oracle_text = oracle_prefix + document_text\n",
    "\n",
    "        full_oracle_enc = tokenizer(full_oracle_text, return_tensors=\"pt\",\n",
    "                                    add_special_tokens=True, padding=False, truncation=False)\n",
    "        full_oracle_ids = full_oracle_enc['input_ids'].to(config.device)\n",
    "\n",
    "        oracle_prefix_enc = tokenizer(oracle_prefix, return_tensors=\"pt\",\n",
    "                                      add_special_tokens=True, padding=False, truncation=False)\n",
    "        oracle_prefix_len = oracle_prefix_enc['input_ids'].shape[1]\n",
    "\n",
    "        bos_id = full_oracle_ids[:, :1]\n",
    "        doc_ids = full_oracle_ids[:, oracle_prefix_len:]\n",
    "        doc_len = doc_ids.shape[1]\n",
    "        context_len = 1 + doc_len  # BOS + doc\n",
    "\n",
    "        # === Build bare cache ===\n",
    "        bare_ids = torch.cat([bos_id, doc_ids], dim=1)\n",
    "        with torch.no_grad():\n",
    "            bare_out = model(input_ids=bare_ids,\n",
    "                             attention_mask=torch.ones_like(bare_ids),\n",
    "                             use_cache=True, return_dict=True)\n",
    "        bare_cache = _ensure_dynamic_cache(bare_out.past_key_values)\n",
    "        del bare_out\n",
    "\n",
    "        # === Build primed cache (static_fact prefix) ===\n",
    "        primed_ids = torch.cat([bos_id, sf_prefix_ids, doc_ids], dim=1)\n",
    "        sf_prefix_len = sf_prefix_ids.shape[1]\n",
    "        with torch.no_grad():\n",
    "            primed_out = model(input_ids=primed_ids,\n",
    "                               attention_mask=torch.ones_like(primed_ids),\n",
    "                               use_cache=True, return_dict=True)\n",
    "        primed_full = _ensure_dynamic_cache(primed_out.past_key_values)\n",
    "        del primed_out\n",
    "\n",
    "        # Truncate + RoPE correct\n",
    "        primed_trunc = extract_and_truncate_cache_with_bos(primed_full, doc_len)\n",
    "        correct_rope_positions_with_bos(primed_trunc, sf_prefix_len, model)\n",
    "        del primed_full\n",
    "\n",
    "        del bare_ids, primed_ids\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # === Score primed_1x first (uses deepcopy) ===\n",
    "        primed_for_score = build_hybrid_cache(bare_cache, primed_trunc)\n",
    "        nll_primed = score_answer_with_cache(\n",
    "            primed_for_score, context_len, query_prompt, answer_text, model, tokenizer, config)\n",
    "        del primed_for_score\n",
    "\n",
    "        # === Build amp2x cache (layers 0-15, 2x delta) ===\n",
    "        deltas_0_15 = compute_delta(bare_cache, primed_trunc, layers=range(16))\n",
    "        amp_deltas = {li: d * 2.0 for li, d in deltas_0_15.items()}\n",
    "        amp_cache = apply_delta(bare_cache, amp_deltas)\n",
    "        nll_amp2x = score_answer_with_cache(\n",
    "            amp_cache, context_len, query_prompt, answer_text, model, tokenizer, config)\n",
    "        del amp_cache, amp_deltas, deltas_0_15\n",
    "\n",
    "        del primed_trunc\n",
    "\n",
    "        # === Score bare LAST (mutates cache) ===\n",
    "        nll_bare = score_answer_with_cache(\n",
    "            bare_cache, context_len, query_prompt, answer_text, model, tokenizer, config)\n",
    "        del bare_cache\n",
    "\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        passage_results.append({\n",
    "            'passage_idx': pinfo['passage_idx'],\n",
    "            'is_relevant': pinfo['is_relevant'],\n",
    "            'word_count': pinfo['word_count'],\n",
    "            'bare_nll': nll_bare,\n",
    "            'primed_nll': nll_primed,\n",
    "            'amp2x_nll': nll_amp2x,\n",
    "            'delta': nll_bare - nll_primed,\n",
    "            'delta_amp2x': nll_bare - nll_amp2x,\n",
    "        })\n",
    "\n",
    "    return passage_results\n",
    "\n",
    "\n",
    "def compute_rankings_for_query(passage_results):\n",
    "    \"\"\"Compute rankings under all 6 conditions for a single query.\n",
    "\n",
    "    Returns dict with MRR, Hit@1, Hit@3 for each condition.\n",
    "    \"\"\"\n",
    "    n = len(passage_results)\n",
    "    if n < 2:\n",
    "        return None\n",
    "\n",
    "    # Find relevant passage indices\n",
    "    relevant_indices = [i for i, p in enumerate(passage_results) if p['is_relevant']]\n",
    "    if not relevant_indices:\n",
    "        return None\n",
    "\n",
    "    bare_nlls = [p['bare_nll'] for p in passage_results]\n",
    "    primed_nlls = [p['primed_nll'] for p in passage_results]\n",
    "    amp2x_nlls = [p['amp2x_nll'] for p in passage_results]\n",
    "    deltas = [p['delta'] for p in passage_results]\n",
    "\n",
    "    # Per-query median of bare NLLs for gating\n",
    "    bare_median = np.median(bare_nlls)\n",
    "\n",
    "    # Gated: use primed if bare > median, else bare\n",
    "    gated_nlls = []\n",
    "    for i in range(n):\n",
    "        if bare_nlls[i] > bare_median:\n",
    "            gated_nlls.append(primed_nlls[i])\n",
    "        else:\n",
    "            gated_nlls.append(bare_nlls[i])\n",
    "\n",
    "    # For each condition, build scores dict and compute metrics\n",
    "    # For all conditions: lower score = ranked higher (like NLL)\n",
    "    # Exception: delta_signal \u2014 higher delta = more relevant, so negate\n",
    "    conditions = {\n",
    "        'bare': {i: bare_nlls[i] for i in range(n)},\n",
    "        'primed_1x': {i: primed_nlls[i] for i in range(n)},\n",
    "        'primed_amp2x': {i: amp2x_nlls[i] for i in range(n)},\n",
    "        'oracle_gated': {i: gated_nlls[i] for i in range(n)},\n",
    "        'delta_signal': {i: -deltas[i] for i in range(n)},  # negate: higher delta = lower score = ranked higher\n",
    "    }\n",
    "\n",
    "    # Use first relevant index for compute_ranking_metrics\n",
    "    rel_idx = relevant_indices[0]\n",
    "\n",
    "    result = {'n_passages': n, 'n_relevant': len(relevant_indices)}\n",
    "\n",
    "    for cond_name, scores in conditions.items():\n",
    "        metrics = compute_ranking_metrics(scores, relevant_idx=rel_idx)\n",
    "        result[cond_name] = metrics\n",
    "\n",
    "    # Combined condition: sweep alpha in [0, 1]\n",
    "    # score = alpha * bare_nll + (1-alpha) * (-delta)\n",
    "    # = alpha * bare_nll - (1-alpha) * delta\n",
    "    best_alpha = None\n",
    "    best_mrr = -1\n",
    "    for alpha in np.arange(0.0, 1.05, 0.05):\n",
    "        combined_scores = {}\n",
    "        for i in range(n):\n",
    "            combined_scores[i] = alpha * bare_nlls[i] + (1 - alpha) * (-deltas[i])\n",
    "        m = compute_ranking_metrics(combined_scores, relevant_idx=rel_idx)\n",
    "        if m['mrr'] > best_mrr:\n",
    "            best_mrr = m['mrr']\n",
    "            best_alpha = alpha\n",
    "\n",
    "    # Compute final combined with best alpha\n",
    "    combined_scores = {i: best_alpha * bare_nlls[i] + (1 - best_alpha) * (-deltas[i])\n",
    "                       for i in range(n)}\n",
    "    result['combined'] = compute_ranking_metrics(combined_scores, relevant_idx=rel_idx)\n",
    "    result['combined_best_alpha'] = best_alpha\n",
    "\n",
    "    # Store per-passage data for differential analysis\n",
    "    result['passage_data'] = passage_results\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "print(\"Helper functions defined:\")\n",
    "print(\"  compute_delta(bare, primed, layers=None) -> dict\")\n",
    "print(\"  apply_delta(bare, deltas, scale) -> cache\")\n",
    "print(\"  score_all_passages_for_query(query_data, sf_prefix_ids, ...) -> list\")\n",
    "print(\"  compute_rankings_for_query(passage_results) -> dict\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 7: Main loop \u2014 iterate queries, score all passages, compute rankings\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"MAIN EVALUATION ({N} queries)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Pre-tokenize prefix\n",
    "sf_prefix_str = SURROGATE_PREFIX_TEMPLATE.format(surrogate=STATIC_FACT)\n",
    "sf_prefix_enc = tokenizer(sf_prefix_str, return_tensors=\"pt\",\n",
    "                           add_special_tokens=False, padding=False, truncation=False)\n",
    "sf_prefix_ids = sf_prefix_enc['input_ids'].to(config.device)\n",
    "sf_prefix_len = sf_prefix_ids.shape[1]\n",
    "\n",
    "print(f\"Static fact prefix: '{STATIC_FACT}' ({sf_prefix_len} tokens)\")\n",
    "\n",
    "# Checkpoint resume\n",
    "all_results = []\n",
    "start_idx = 0\n",
    "\n",
    "if CHECKPOINT_PATH.exists():\n",
    "    with open(CHECKPOINT_PATH, 'r') as f:\n",
    "        ckpt = json.load(f)\n",
    "    ckpt_queries = ckpt.get('query_texts', [])\n",
    "    current_queries = [q['query'] for q in queries]\n",
    "    if ckpt_queries == current_queries:\n",
    "        all_results = ckpt['results']\n",
    "        start_idx = len(all_results)\n",
    "        print(f\"Resuming from checkpoint: {start_idx}/{N}\")\n",
    "    else:\n",
    "        print(\"Checkpoint query mismatch. Starting fresh.\")\n",
    "else:\n",
    "    print(\"No checkpoint found. Starting fresh.\")\n",
    "\n",
    "print(f\"Evaluating queries {start_idx} to {N-1}\")\n",
    "\n",
    "t_start = time.time()\n",
    "\n",
    "for qidx in tqdm(range(start_idx, N), initial=start_idx, total=N, desc=\"Queries\"):\n",
    "    query_data = queries[qidx]\n",
    "\n",
    "    # Score all passages for this query\n",
    "    passage_results = score_all_passages_for_query(\n",
    "        query_data, sf_prefix_ids, model, tokenizer, config)\n",
    "\n",
    "    # Compute rankings under all conditions\n",
    "    rankings = compute_rankings_for_query(passage_results)\n",
    "\n",
    "    if rankings is not None:\n",
    "        rankings['query_idx'] = qidx\n",
    "        rankings['query'] = query_data['query']\n",
    "\n",
    "        # Compute query-doc overlap for each passage\n",
    "        for pr in rankings['passage_data']:\n",
    "            pidx_in_list = pr['passage_idx']\n",
    "            passage_text = query_data['passages'][\n",
    "                next(i for i, p in enumerate(query_data['passages'])\n",
    "                     if p['passage_idx'] == pidx_in_list)\n",
    "            ]['passage']\n",
    "            pr['query_doc_overlap'] = compute_token_overlap(\n",
    "                query_data['query'], passage_text, tokenizer)\n",
    "\n",
    "        all_results.append(rankings)\n",
    "\n",
    "    # Checkpoint\n",
    "    if (qidx + 1) % CHECKPOINT_EVERY == 0 or qidx == N - 1:\n",
    "        ckpt_data = {\n",
    "            'results': all_results,\n",
    "            'query_texts': [q['query'] for q in queries],\n",
    "            'completed': len(all_results),\n",
    "            'total': N,\n",
    "            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        }\n",
    "        with open(CHECKPOINT_PATH, 'w') as f:\n",
    "            json.dump(ckpt_data, f)\n",
    "        elapsed = time.time() - t_start\n",
    "        n_done = qidx - start_idx + 1\n",
    "        rate = n_done / elapsed if elapsed > 0 else 0\n",
    "        remaining = (N - qidx - 1) / rate if rate > 0 else 0\n",
    "        tqdm.write(f\"  Checkpoint {qidx+1}/{N} | {n_done} done in {elapsed/60:.1f}m | \"\n",
    "                   f\"ETA: {remaining/60:.1f} min\")\n",
    "\n",
    "elapsed_total = time.time() - t_start\n",
    "print(f\"\\nEvaluation complete: {len(all_results)} queries in {elapsed_total/60:.1f} min\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 8: Ranking analysis \u2014 aggregate MRR/Hit@k, significance tests\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"RANKING ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "CONDITION_NAMES = ['bare', 'primed_1x', 'primed_amp2x', 'oracle_gated', 'delta_signal', 'combined']\n",
    "N_VALID = len(all_results)\n",
    "print(f\"Valid queries: {N_VALID}\")\n",
    "\n",
    "# Aggregate metrics per condition\n",
    "ranking_summary = {}\n",
    "for cond in CONDITION_NAMES:\n",
    "    mrrs = [r[cond]['mrr'] for r in all_results]\n",
    "    hit1s = [r[cond]['hit_at_1'] for r in all_results]\n",
    "    hit3s = [r[cond]['hit_at_3'] for r in all_results]\n",
    "    ranks = [r[cond]['relevant_rank'] for r in all_results]\n",
    "    ranking_summary[cond] = {\n",
    "        'mrr_mean': float(np.mean(mrrs)),\n",
    "        'mrr_std': float(np.std(mrrs)),\n",
    "        'mrr_values': mrrs,\n",
    "        'hit_at_1': float(np.mean(hit1s)),\n",
    "        'hit_at_3': float(np.mean(hit3s)),\n",
    "        'rank_mean': float(np.mean(ranks)),\n",
    "        'rank_median': float(np.median(ranks)),\n",
    "    }\n",
    "\n",
    "# Print results table\n",
    "print(f\"\\n{'Condition':<18} {'MRR':>8} {'Hit@1':>8} {'Hit@3':>8} {'Rank':>8} {'p vs bare':>12} {'Sig':>5}\")\n",
    "print(\"-\" * 75)\n",
    "\n",
    "significance_results = {}\n",
    "bare_mrrs = np.array(ranking_summary['bare']['mrr_values'])\n",
    "\n",
    "for cond in CONDITION_NAMES:\n",
    "    s_data = ranking_summary[cond]\n",
    "    mrrs_arr = np.array(s_data['mrr_values'])\n",
    "\n",
    "    if cond == 'bare':\n",
    "        print(f\"{cond:<18} {s_data['mrr_mean']:>8.3f} {s_data['hit_at_1']:>8.3f} \"\n",
    "              f\"{s_data['hit_at_3']:>8.3f} {s_data['rank_mean']:>8.2f} {'--':>12} {'--':>5}\")\n",
    "    else:\n",
    "        # Wilcoxon signed-rank test (paired, non-parametric)\n",
    "        diff = mrrs_arr - bare_mrrs\n",
    "        nonzero = np.sum(diff != 0)\n",
    "        if nonzero > 10:\n",
    "            stat, p_val = wilcoxon(mrrs_arr, bare_mrrs)\n",
    "        else:\n",
    "            p_val = 1.0\n",
    "\n",
    "        sig = \"***\" if p_val < 0.001 else \"**\" if p_val < 0.01 else \"*\" if p_val < 0.05 else \"ns\"\n",
    "        delta_mrr = s_data['mrr_mean'] - ranking_summary['bare']['mrr_mean']\n",
    "        print(f\"{cond:<18} {s_data['mrr_mean']:>8.3f} {s_data['hit_at_1']:>8.3f} \"\n",
    "              f\"{s_data['hit_at_3']:>8.3f} {s_data['rank_mean']:>8.2f} {p_val:>11.2e} {sig:>5}\"\n",
    "              f\"  \u0394MRR={delta_mrr:+.3f}\")\n",
    "\n",
    "        significance_results[f'{cond} vs bare'] = {\n",
    "            'delta_mrr': float(delta_mrr),\n",
    "            'p_value': float(p_val),\n",
    "            'significant': bool(p_val < 0.05),\n",
    "        }\n",
    "\n",
    "# Best condition\n",
    "best_cond = max(CONDITION_NAMES, key=lambda c: ranking_summary[c]['mrr_mean'])\n",
    "print(f\"\\nBest condition: {best_cond} (MRR={ranking_summary[best_cond]['mrr_mean']:.3f})\")\n",
    "\n",
    "# Alpha distribution for combined condition\n",
    "alphas = [r['combined_best_alpha'] for r in all_results]\n",
    "print(f\"\\nCombined condition alpha: mean={np.mean(alphas):.2f}, \"\n",
    "      f\"median={np.median(alphas):.2f}, std={np.std(alphas):.2f}\")\n",
    "print(f\"  alpha=1 (bare only): {sum(1 for a in alphas if a >= 0.95)}/{N_VALID}\")\n",
    "print(f\"  alpha=0 (delta only): {sum(1 for a in alphas if a <= 0.05)}/{N_VALID}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 9: Differential effect \u2014 relevant vs irrelevant deltas, AUC, ROC\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"DIFFERENTIAL EFFECT ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Collect all passage-level data\n",
    "all_passages = []\n",
    "for r in all_results:\n",
    "    for p in r['passage_data']:\n",
    "        all_passages.append(p)\n",
    "\n",
    "is_relevant = np.array([p['is_relevant'] for p in all_passages])\n",
    "bare_nlls = np.array([p['bare_nll'] for p in all_passages])\n",
    "primed_nlls = np.array([p['primed_nll'] for p in all_passages])\n",
    "amp2x_nlls = np.array([p['amp2x_nll'] for p in all_passages])\n",
    "deltas = np.array([p['delta'] for p in all_passages])\n",
    "deltas_amp2x = np.array([p['delta_amp2x'] for p in all_passages])\n",
    "overlaps = np.array([p.get('query_doc_overlap', 0) for p in all_passages])\n",
    "\n",
    "n_rel = int(np.sum(is_relevant))\n",
    "n_irr = int(np.sum(~is_relevant))\n",
    "print(f\"\\nTotal passages: {len(all_passages)} (relevant: {n_rel}, irrelevant: {n_irr})\")\n",
    "\n",
    "# === Differential effect: delta for relevant vs irrelevant ===\n",
    "delta_rel = deltas[is_relevant]\n",
    "delta_irr = deltas[~is_relevant]\n",
    "delta_amp_rel = deltas_amp2x[is_relevant]\n",
    "delta_amp_irr = deltas_amp2x[~is_relevant]\n",
    "\n",
    "print(f\"\\n{'Metric':<25} {'Relevant':>12} {'Irrelevant':>12} {'Diff':>10} {'p':>12}\")\n",
    "print(\"-\" * 75)\n",
    "\n",
    "# Delta (primed_1x)\n",
    "t_stat, p_val = stats.ttest_ind(delta_rel, delta_irr)\n",
    "d = cohens_d(np.concatenate([delta_rel - np.mean(delta_irr), np.zeros(0)]))\n",
    "print(f\"{'Delta (1x)':25} {np.mean(delta_rel):>+12.4f} {np.mean(delta_irr):>+12.4f} \"\n",
    "      f\"{np.mean(delta_rel) - np.mean(delta_irr):>+10.4f} {p_val:>11.2e}\")\n",
    "\n",
    "# Delta (amp2x)\n",
    "t_stat2, p_val2 = stats.ttest_ind(delta_amp_rel, delta_amp_irr)\n",
    "print(f\"{'Delta (amp2x)':25} {np.mean(delta_amp_rel):>+12.4f} {np.mean(delta_amp_irr):>+12.4f} \"\n",
    "      f\"{np.mean(delta_amp_rel) - np.mean(delta_amp_irr):>+10.4f} {p_val2:>11.2e}\")\n",
    "\n",
    "# Bare NLL\n",
    "bare_rel = bare_nlls[is_relevant]\n",
    "bare_irr = bare_nlls[~is_relevant]\n",
    "t_stat3, p_val3 = stats.ttest_ind(bare_rel, bare_irr)\n",
    "print(f\"{'Bare NLL':25} {np.mean(bare_rel):>12.4f} {np.mean(bare_irr):>12.4f} \"\n",
    "      f\"{np.mean(bare_rel) - np.mean(bare_irr):>+10.4f} {p_val3:>11.2e}\")\n",
    "\n",
    "# Overlap\n",
    "overlap_rel = overlaps[is_relevant]\n",
    "overlap_irr = overlaps[~is_relevant]\n",
    "t_stat4, p_val4 = stats.ttest_ind(overlap_rel, overlap_irr)\n",
    "print(f\"{'Query-doc overlap':25} {np.mean(overlap_rel):>12.4f} {np.mean(overlap_irr):>12.4f} \"\n",
    "      f\"{np.mean(overlap_rel) - np.mean(overlap_irr):>+10.4f} {p_val4:>11.2e}\")\n",
    "\n",
    "differential_results = {\n",
    "    'delta_1x_relevant_mean': float(np.mean(delta_rel)),\n",
    "    'delta_1x_irrelevant_mean': float(np.mean(delta_irr)),\n",
    "    'delta_1x_diff': float(np.mean(delta_rel) - np.mean(delta_irr)),\n",
    "    'delta_1x_p_value': float(p_val),\n",
    "    'delta_amp2x_relevant_mean': float(np.mean(delta_amp_rel)),\n",
    "    'delta_amp2x_irrelevant_mean': float(np.mean(delta_amp_irr)),\n",
    "    'delta_amp2x_diff': float(np.mean(delta_amp_rel) - np.mean(delta_amp_irr)),\n",
    "    'delta_amp2x_p_value': float(p_val2),\n",
    "    'bare_nll_relevant_mean': float(np.mean(bare_rel)),\n",
    "    'bare_nll_irrelevant_mean': float(np.mean(bare_irr)),\n",
    "}\n",
    "\n",
    "# === AUC: delta as relevance predictor ===\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"DELTA AS RELEVANCE PREDICTOR (AUC)\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "# For AUC: higher score should predict relevant=True\n",
    "# bare NLL: relevant passages should have LOWER NLL -> negate for AUC\n",
    "# delta: relevant passages should have HIGHER delta -> use as-is\n",
    "# amp2x delta: same\n",
    "\n",
    "predictors = {\n",
    "    'bare_nll': -bare_nlls,       # negate: lower NLL = more relevant\n",
    "    'primed_nll': -primed_nlls,\n",
    "    'delta_1x': deltas,            # higher delta = more relevant (hypothesis)\n",
    "    'delta_amp2x': deltas_amp2x,\n",
    "    'overlap': overlaps,\n",
    "}\n",
    "\n",
    "auc_results = {}\n",
    "roc_data = {}\n",
    "print(f\"\\n{'Predictor':<18} {'AUC':>8} {'Direction':>12}\")\n",
    "print(\"-\" * 40)\n",
    "for pred_name, pred_vals in predictors.items():\n",
    "    try:\n",
    "        auc = roc_auc_score(is_relevant.astype(int), pred_vals)\n",
    "        fpr, tpr, thresholds = roc_curve(is_relevant.astype(int), pred_vals)\n",
    "        roc_data[pred_name] = {'fpr': fpr.tolist(), 'tpr': tpr.tolist()}\n",
    "        direction = \"correct\" if auc > 0.5 else \"INVERTED\"\n",
    "        print(f\"{pred_name:<18} {auc:>8.3f} {direction:>12}\")\n",
    "        auc_results[pred_name] = float(auc)\n",
    "    except Exception as e:\n",
    "        print(f\"{pred_name:<18} ERROR: {e}\")\n",
    "        auc_results[pred_name] = None\n",
    "\n",
    "# Point-biserial correlation\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"POINT-BISERIAL CORRELATIONS WITH RELEVANCE\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "corr_results = {}\n",
    "for pred_name, pred_vals in predictors.items():\n",
    "    r, p = stats.pointbiserialr(is_relevant.astype(int), pred_vals)\n",
    "    sig = \"***\" if p < 0.001 else \"**\" if p < 0.01 else \"*\" if p < 0.05 else \"ns\"\n",
    "    print(f\"  {pred_name:<18} r={r:+.4f}  p={p:.2e}  {sig}\")\n",
    "    corr_results[pred_name] = {'r': float(r), 'p': float(p)}"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 10: Stratified analysis and alpha sweep\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"STRATIFIED ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# === By query difficulty ===\n",
    "# Difficulty = mean bare NLL across all passages for the query\n",
    "query_difficulties = []\n",
    "for r in all_results:\n",
    "    bare_mean = np.mean([p['bare_nll'] for p in r['passage_data']])\n",
    "    query_difficulties.append(bare_mean)\n",
    "\n",
    "query_difficulties = np.array(query_difficulties)\n",
    "tercile_edges = np.percentile(query_difficulties, [33.3, 66.7])\n",
    "difficulty_labels = np.array(['easy' if d <= tercile_edges[0] else\n",
    "                               'hard' if d >= tercile_edges[1] else 'medium'\n",
    "                               for d in query_difficulties])\n",
    "\n",
    "print(f\"\\nQuery difficulty terciles:\")\n",
    "print(f\"  Easy (NLL <= {tercile_edges[0]:.2f}): n={np.sum(difficulty_labels=='easy')}\")\n",
    "print(f\"  Medium: n={np.sum(difficulty_labels=='medium')}\")\n",
    "print(f\"  Hard (NLL >= {tercile_edges[1]:.2f}): n={np.sum(difficulty_labels=='hard')}\")\n",
    "\n",
    "stratified_results = {}\n",
    "for stratum in ['easy', 'medium', 'hard']:\n",
    "    mask = difficulty_labels == stratum\n",
    "    n_stratum = int(np.sum(mask))\n",
    "    stratum_results_list = [r for r, m in zip(all_results, mask) if m]\n",
    "    stratified_results[stratum] = {}\n",
    "    print(f\"\\n  {stratum} (n={n_stratum}):\")\n",
    "    for cond in CONDITION_NAMES:\n",
    "        mrrs = [r[cond]['mrr'] for r in stratum_results_list]\n",
    "        stratified_results[stratum][cond] = float(np.mean(mrrs))\n",
    "    print(f\"    {'Condition':<18} {'MRR':>8}\")\n",
    "    print(f\"    {'-'*28}\")\n",
    "    for cond in CONDITION_NAMES:\n",
    "        mrr = stratified_results[stratum][cond]\n",
    "        delta_mrr = mrr - stratified_results[stratum]['bare']\n",
    "        marker = \" <-- best\" if cond == max(CONDITION_NAMES, key=lambda c: stratified_results[stratum][c]) else \"\"\n",
    "        print(f\"    {cond:<18} {mrr:>8.3f}  (\u0394MRR={delta_mrr:+.3f}){marker}\")\n",
    "\n",
    "# === By N candidates ===\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"BY NUMBER OF CANDIDATES\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "n_cands = np.array([r['n_passages'] for r in all_results])\n",
    "for n_bin_lo, n_bin_hi in [(2, 5), (5, 8), (8, 11)]:\n",
    "    mask = (n_cands >= n_bin_lo) & (n_cands < n_bin_hi)\n",
    "    n_in_bin = int(np.sum(mask))\n",
    "    if n_in_bin < 10:\n",
    "        continue\n",
    "    bin_results = [r for r, m in zip(all_results, mask) if m]\n",
    "    print(f\"\\n  {n_bin_lo}-{n_bin_hi-1} candidates (n={n_in_bin}):\")\n",
    "    for cond in ['bare', 'primed_1x', 'delta_signal', 'combined']:\n",
    "        mrrs = [r[cond]['mrr'] for r in bin_results]\n",
    "        print(f\"    {cond:<18} MRR={np.mean(mrrs):.3f}\")\n",
    "\n",
    "# === Global alpha sweep (MRR) ===\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"GLOBAL ALPHA SWEEP: score = alpha * bare_nll + (1-alpha) * (-delta)\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "alpha_sweep_results = {}\n",
    "for alpha in np.arange(0.0, 1.05, 0.05):\n",
    "    alpha_val = round(alpha, 2)\n",
    "    mrrs = []\n",
    "    for r in all_results:\n",
    "        n = len(r['passage_data'])\n",
    "        scores = {}\n",
    "        for i, p in enumerate(r['passage_data']):\n",
    "            scores[i] = alpha_val * p['bare_nll'] + (1 - alpha_val) * (-p['delta'])\n",
    "        rel_idx = next(i for i, p in enumerate(r['passage_data']) if p['is_relevant'])\n",
    "        m = compute_ranking_metrics(scores, relevant_idx=rel_idx)\n",
    "        mrrs.append(m['mrr'])\n",
    "    alpha_sweep_results[alpha_val] = float(np.mean(mrrs))\n",
    "\n",
    "best_global_alpha = max(alpha_sweep_results, key=alpha_sweep_results.get)\n",
    "print(f\"\\n  {'Alpha':>8} {'MRR':>8}\")\n",
    "print(f\"  {'-'*18}\")\n",
    "for alpha_val in sorted(alpha_sweep_results.keys()):\n",
    "    marker = \" <-- best\" if alpha_val == best_global_alpha else \"\"\n",
    "    print(f\"  {alpha_val:>8.2f} {alpha_sweep_results[alpha_val]:>8.3f}{marker}\")\n",
    "\n",
    "print(f\"\\nBest global alpha: {best_global_alpha:.2f} \"\n",
    "      f\"(MRR={alpha_sweep_results[best_global_alpha]:.3f})\")\n",
    "print(f\"Bare-only (alpha=1.0): MRR={alpha_sweep_results.get(1.0, 0):.3f}\")\n",
    "print(f\"Delta-only (alpha=0.0): MRR={alpha_sweep_results.get(0.0, 0):.3f}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 11: Plots (6-panel figure)\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "\n",
    "colors = {\n",
    "    'bare': '#7f7f7f',\n",
    "    'primed_1x': '#d62728',\n",
    "    'primed_amp2x': '#ff7f0e',\n",
    "    'oracle_gated': '#2ca02c',\n",
    "    'delta_signal': '#1f77b4',\n",
    "    'combined': '#9467bd',\n",
    "}\n",
    "\n",
    "# --- Plot 1: MRR comparison bar chart ---\n",
    "ax = axes[0, 0]\n",
    "conds = CONDITION_NAMES\n",
    "mrrs = [ranking_summary[c]['mrr_mean'] for c in conds]\n",
    "bar_colors = [colors[c] for c in conds]\n",
    "bars = ax.bar(range(len(conds)), mrrs, color=bar_colors, edgecolor='black', linewidth=0.5)\n",
    "ax.set_xticks(range(len(conds)))\n",
    "ax.set_xticklabels(conds, rotation=30, ha='right', fontsize=8)\n",
    "for i, (c, m) in enumerate(zip(conds, mrrs)):\n",
    "    ax.text(i, m + 0.005, f\"{m:.3f}\", ha='center', fontsize=8)\n",
    "ax.set_ylabel(\"MRR\")\n",
    "ax.set_title(\"MRR by Condition\")\n",
    "ax.set_ylim(0, max(mrrs) * 1.15)\n",
    "\n",
    "# --- Plot 2: Delta distributions \u2014 relevant vs irrelevant ---\n",
    "ax = axes[0, 1]\n",
    "bins = np.linspace(min(deltas.min(), -1), max(deltas.max(), 1), 50)\n",
    "ax.hist(delta_rel, bins=bins, alpha=0.6, color='green', label=f'Relevant (n={n_rel})', density=True)\n",
    "ax.hist(delta_irr, bins=bins, alpha=0.6, color='red', label=f'Irrelevant (n={n_irr})', density=True)\n",
    "ax.axvline(np.mean(delta_rel), color='green', linestyle='--', linewidth=2)\n",
    "ax.axvline(np.mean(delta_irr), color='red', linestyle='--', linewidth=2)\n",
    "ax.set_xlabel(\"Delta (bare - primed NLL)\")\n",
    "ax.set_ylabel(\"Density\")\n",
    "ax.set_title(\"Delta Distribution: Relevant vs Irrelevant\")\n",
    "ax.legend(fontsize=8)\n",
    "\n",
    "# --- Plot 3: Query difficulty vs \u0394MRR ---\n",
    "ax = axes[0, 2]\n",
    "delta_mrrs_primed = np.array([r['primed_1x']['mrr'] - r['bare']['mrr'] for r in all_results])\n",
    "ax.scatter(query_difficulties, delta_mrrs_primed, alpha=0.3, s=15, color=colors['primed_1x'],\n",
    "           label='primed_1x')\n",
    "# Trend line via binning\n",
    "n_trend_bins = 8\n",
    "trend_edges = np.percentile(query_difficulties, np.linspace(0, 100, n_trend_bins + 1))\n",
    "for k in range(n_trend_bins):\n",
    "    mask_k = (query_difficulties >= trend_edges[k]) & (query_difficulties < trend_edges[k+1])\n",
    "    if np.sum(mask_k) > 5:\n",
    "        center = (trend_edges[k] + trend_edges[k+1]) / 2\n",
    "        ax.scatter(center, np.mean(delta_mrrs_primed[mask_k]), s=80, color=colors['primed_1x'],\n",
    "                  edgecolor='black', linewidth=1, zorder=5)\n",
    "ax.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "ax.set_xlabel(\"Query Difficulty (mean bare NLL)\")\n",
    "ax.set_ylabel(\"\u0394MRR (primed - bare)\")\n",
    "ax.set_title(\"Query Difficulty vs Ranking Improvement\")\n",
    "ax.legend(fontsize=8)\n",
    "\n",
    "# --- Plot 4: ROC curves ---\n",
    "ax = axes[1, 0]\n",
    "roc_colors = {'bare_nll': '#7f7f7f', 'delta_1x': '#1f77b4', 'delta_amp2x': '#ff7f0e',\n",
    "              'overlap': '#2ca02c'}\n",
    "for pred_name in ['bare_nll', 'delta_1x', 'delta_amp2x', 'overlap']:\n",
    "    if pred_name in roc_data:\n",
    "        fpr = roc_data[pred_name]['fpr']\n",
    "        tpr = roc_data[pred_name]['tpr']\n",
    "        auc_val = auc_results.get(pred_name, 0)\n",
    "        ax.plot(fpr, tpr, color=roc_colors.get(pred_name, 'gray'),\n",
    "                label=f\"{pred_name} (AUC={auc_val:.3f})\")\n",
    "ax.plot([0, 1], [0, 1], 'k--', alpha=0.3, label='Random')\n",
    "ax.set_xlabel(\"False Positive Rate\")\n",
    "ax.set_ylabel(\"True Positive Rate\")\n",
    "ax.set_title(\"ROC: Delta as Relevance Predictor\")\n",
    "ax.legend(fontsize=8)\n",
    "\n",
    "# --- Plot 5: Heatmap \u2014 condition \u00d7 difficulty stratum MRR ---\n",
    "ax = axes[1, 1]\n",
    "strata = ['easy', 'medium', 'hard']\n",
    "heatmap_data = np.zeros((len(CONDITION_NAMES), len(strata)))\n",
    "for i, cond in enumerate(CONDITION_NAMES):\n",
    "    for j, stratum in enumerate(strata):\n",
    "        heatmap_data[i, j] = stratified_results.get(stratum, {}).get(cond, 0)\n",
    "im = ax.imshow(heatmap_data, aspect='auto', cmap='YlOrRd')\n",
    "ax.set_xticks(range(len(strata)))\n",
    "ax.set_xticklabels(strata)\n",
    "ax.set_yticks(range(len(CONDITION_NAMES)))\n",
    "ax.set_yticklabels(CONDITION_NAMES, fontsize=8)\n",
    "for i in range(len(CONDITION_NAMES)):\n",
    "    for j in range(len(strata)):\n",
    "        ax.text(j, i, f\"{heatmap_data[i,j]:.3f}\", ha='center', va='center', fontsize=7)\n",
    "plt.colorbar(im, ax=ax, label=\"MRR\")\n",
    "ax.set_title(\"MRR by Condition \u00d7 Difficulty\")\n",
    "\n",
    "# --- Plot 6: Per-query MRR change distribution ---\n",
    "ax = axes[1, 2]\n",
    "for cond in ['primed_1x', 'delta_signal', 'combined']:\n",
    "    delta_mrrs = [r[cond]['mrr'] - r['bare']['mrr'] for r in all_results]\n",
    "    ax.hist(delta_mrrs, bins=30, alpha=0.5, label=cond, color=colors[cond])\n",
    "ax.axvline(x=0, color='gray', linestyle='--', alpha=0.5)\n",
    "ax.set_xlabel(\"\u0394MRR (condition - bare)\")\n",
    "ax.set_ylabel(\"Count\")\n",
    "ax.set_title(\"Per-Query MRR Change Distribution\")\n",
    "ax.legend(fontsize=8)\n",
    "\n",
    "plt.suptitle('Exp 14: Ranking-Aware Priming', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'analysis_plots.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"Plots saved to {RESULTS_DIR / 'analysis_plots.png'}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 12: Save results JSON\n",
    "final = {\n",
    "    'experiment': 'exp14_ranking_aware_priming',\n",
    "    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'config': {\n",
    "        'model_name': config.model_name,\n",
    "        'seed': SEED,\n",
    "        'n_queries': N,\n",
    "        'n_valid': N_VALID,\n",
    "        'max_passage_words': MAX_PASSAGE_WORDS,\n",
    "        'min_passages_per_query': MIN_PASSAGES_PER_QUERY,\n",
    "        'static_fact': STATIC_FACT,\n",
    "        'dataset': 'MS MARCO v1.1 validation',\n",
    "    },\n",
    "    'condition_names': CONDITION_NAMES,\n",
    "    'ranking_summary': {c: {k: v for k, v in d.items() if k != 'mrr_values'}\n",
    "                        for c, d in ranking_summary.items()},\n",
    "    'significance_results': significance_results,\n",
    "    'differential_effect': differential_results,\n",
    "    'auc_results': auc_results,\n",
    "    'correlation_results': corr_results,\n",
    "    'stratified_results': stratified_results,\n",
    "    'alpha_sweep': alpha_sweep_results,\n",
    "    'best_global_alpha': float(best_global_alpha),\n",
    "    'per_query_results': [{k: v for k, v in r.items() if k != 'passage_data'}\n",
    "                          for r in all_results],\n",
    "    'per_passage_summary': {\n",
    "        'n_total': len(all_passages),\n",
    "        'n_relevant': n_rel,\n",
    "        'n_irrelevant': n_irr,\n",
    "        'mean_delta_relevant': float(np.mean(delta_rel)),\n",
    "        'mean_delta_irrelevant': float(np.mean(delta_irr)),\n",
    "    },\n",
    "}\n",
    "\n",
    "with open(FINAL_RESULTS_PATH, 'w') as f:\n",
    "    json.dump(final, f, indent=2)\n",
    "\n",
    "print(f\"Results saved to {FINAL_RESULTS_PATH}\")\n",
    "print(f\"File size: {FINAL_RESULTS_PATH.stat().st_size / 1024:.1f} KB\")\n",
    "print(\"\\nDone!\")"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}