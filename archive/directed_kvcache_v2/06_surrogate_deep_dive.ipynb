{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e881dcf",
   "metadata": {},
   "source": [
    "# Exp 06: LLM Surrogate Deep-Dive — Mechanism Decomposition\n",
    "\n",
    "## Motivation\n",
    "\n",
    "Exp 05 found LLM-generated keyword surrogates (d=0.37 vs bare) massively outperform oracle\n",
    "queries (d=0.13, indistinguishable from random) on hard MS MARCO. But **WHY?** The LLM\n",
    "surrogates look like \"Cumulonimbus clouds height atmosphere\" — keyword-dense topic phrases\n",
    "with high vocabulary overlap with the passage. Oracle queries are natural-language questions\n",
    "with lower overlap.\n",
    "\n",
    "This experiment decomposes the mechanism through controlled ablations on the **full MS MARCO\n",
    "distribution** (2000 samples, not hardness-filtered).\n",
    "\n",
    "## Hypotheses\n",
    "\n",
    "1. **Token Overlap** — LLM surrogates share tokens with the passage, creating coherent value contamination.\n",
    "2. **Coherence** — Token ORDER matters beyond identity.\n",
    "3. **Format** — Question syntax hurts (question words create unhelpful attention patterns).\n",
    "4. **Passage Specificity** — Document-specific content words help more than generic content words.\n",
    "5. **Mechanism Stacking** — Truncated prefix + separator suffix should combine.\n",
    "\n",
    "## 15 Conditions\n",
    "\n",
    "| # | Condition | Type | Tests |\n",
    "|---|-----------|------|-------|\n",
    "| 1 | Bare | Baseline | — |\n",
    "| 2 | Random-truncated | Control | Structural control |\n",
    "| 3 | Separator-only | Control | Suffix framing |\n",
    "| 4 | Oracle-truncated | Oracle | Semantic control |\n",
    "| 5 | Oracle-as-keywords | Oracle | Format (H3) |\n",
    "| 6 | Anti-keywords | Overlap | Specificity (H4) |\n",
    "| 7 | TF-IDF-keywords | Overlap | Cheap surrogate |\n",
    "| 8 | Passage-echo | Overlap | Overlap ceiling |\n",
    "| 9 | Shuffled-LLM | Overlap | Coherence (H2) |\n",
    "| 10 | LLM-keyword | LLM | Keyword template |\n",
    "| 11 | LLM-question | LLM | Question template |\n",
    "| 12 | LLM-symptom | LLM | Symptom template |\n",
    "| 13 | LLM-summary | LLM | Summary template |\n",
    "| 14 | LLM-keyword+sep | Production | Stacking (H5) |\n",
    "| 15 | LLM-messy | Production | Informal style |\n",
    "\n",
    "## 10 Primary Comparisons (Bonferroni alpha = 0.005)\n",
    "\n",
    "- M1: Shuffled-LLM vs LLM-keyword (coherence)\n",
    "- M2: Oracle-as-keywords vs Oracle (format)\n",
    "- M3: LLM-keyword vs LLM-question (keyword vs question for LLM)\n",
    "- M4: TF-IDF vs Anti-keywords (specificity)\n",
    "- M5: Passage-echo vs LLM-keyword (max overlap ceiling)\n",
    "- M6: TF-IDF vs LLM-keyword (is LLM necessary?)\n",
    "- M7: LLM-keyword+sep vs max(LLM-keyword, sep-only) (stacking)\n",
    "- R1: LLM-keyword vs Random (replicates Exp 05)\n",
    "- R2: Oracle vs Random (replicates null)\n",
    "- R3: Template ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a962a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Setup — permissions, seeds, results directory\n",
    "import os\n",
    "os.umask(0o000)\n",
    "\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "import numpy as np\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "RESULTS_DIR = Path(\"results/exp06\")\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "SURROGATES_DIR = RESULTS_DIR / \"surrogates\"\n",
    "SURROGATES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CHECKPOINT_PATH = RESULTS_DIR / \"checkpoint.json\"\n",
    "FINAL_RESULTS_PATH = RESULTS_DIR / \"results.json\"\n",
    "\n",
    "print(f\"SEED: {SEED}\")\n",
    "print(f\"Results directory: {RESULTS_DIR}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5cb753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Load model (Mistral-7B 4-bit)\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "print(f\"Loading {MODEL_NAME} (4-bit)...\")\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "print(f\"Model loaded. dtype={model.dtype}, device={model.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a48b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Imports + config + templates + all helper functions\n",
    "sys.path.insert(0, \".\")\n",
    "\n",
    "from lib.config import ExperimentConfig\n",
    "from lib.kv_cache import (\n",
    "    build_kv_cache,\n",
    "    build_suffix_kv_cache,\n",
    "    score_answer_with_cache,\n",
    "    deepcopy_cache,\n",
    "    extract_and_truncate_cache_with_bos,\n",
    "    correct_rope_positions_with_bos,\n",
    ")\n",
    "from lib.data import load_ms_marco, load_evaluation_samples\n",
    "from lib.analysis import cohens_d, compute_token_overlap\n",
    "from lib.surrogate import (\n",
    "    generate_all_5_surrogates,\n",
    "    generate_summary,\n",
    "    TOP_5_SURROGATE_TEMPLATES,\n",
    ")\n",
    "from scipy import stats\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "config = ExperimentConfig(\n",
    "    model_name=MODEL_NAME,\n",
    "    num_samples=4000,\n",
    "    min_passage_words=20,\n",
    "    max_passage_words=500,\n",
    "    seed=SEED,\n",
    ")\n",
    "\n",
    "# Templates — bare text, no \"Document:\\n\" framing (hurts NLL, d=-0.45)\n",
    "SURROGATE_PREFIX_TEMPLATE = \"{surrogate}\\n\"\n",
    "DOCUMENT_TEMPLATE = \"{document}\"\n",
    "QUERY_TEMPLATE = \"\\nQuery: {query}\\nAnswer:\"\n",
    "ANSWER_TEMPLATE = \" {answer}\"\n",
    "SUFFIX_SEPARATOR = \"\\n\\nRelated question: \"\n",
    "CHECKPOINT_EVERY = 50\n",
    "N_COMPARISONS = 10\n",
    "BONFERRONI_ALPHA = 0.05 / N_COMPARISONS\n",
    "N_EVAL = 2000  # number of samples to evaluate\n",
    "\n",
    "# Stopwords for TF-IDF and oracle-as-keywords\n",
    "STOPWORDS = set([\n",
    "    'the', 'a', 'an', 'is', 'are', 'was', 'were', 'be', 'been', 'being',\n",
    "    'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could',\n",
    "    'should', 'may', 'might', 'shall', 'can', 'need', 'dare', 'ought',\n",
    "    'used', 'to', 'of', 'in', 'for', 'on', 'with', 'at', 'by', 'from',\n",
    "    'as', 'into', 'through', 'during', 'before', 'after', 'above', 'below',\n",
    "    'between', 'out', 'off', 'over', 'under', 'again', 'further', 'then',\n",
    "    'once', 'and', 'but', 'or', 'nor', 'not', 'so', 'yet', 'both', 'either',\n",
    "    'neither', 'each', 'every', 'all', 'any', 'few', 'more', 'most', 'other',\n",
    "    'some', 'such', 'no', 'only', 'own', 'same', 'than', 'too', 'very',\n",
    "    'just', 'because', 'about', 'that', 'this', 'these', 'those', 'it',\n",
    "    'its', 'they', 'them', 'their', 'we', 'our', 'you', 'your', 'he', 'him',\n",
    "    'his', 'she', 'her', 'which', 'who', 'whom', 'there', 'here', 'when',\n",
    "    'where', 'why', 'how', 'what', 'if', 'up', 'also', 'well', 'back',\n",
    "    'even', 'still', 'new', 'now', 'way', 'many', 'much', 'like', 'get',\n",
    "    'got', 'make', 'made', 'take', 'come', 'go', 'see', 'know', 'think',\n",
    "])\n",
    "\n",
    "QUESTION_STOPWORDS = STOPWORDS | set([\n",
    "    'what', 'which', 'who', 'whom', 'whose', 'when', 'where', 'why', 'how',\n",
    "    'does', 'did', 'can', 'could', 'would', 'should', 'will', 'shall',\n",
    "    'may', 'might', 'must', 'isn', 'aren', 'wasn', 'weren', 'don', 'doesn',\n",
    "    'didn', 'won', 'wouldn', 'couldn', 'shouldn',\n",
    "])\n",
    "\n",
    "\n",
    "def generate_random_prefix_text(target_text, tokenizer, seed):\n",
    "    # Generate random token text matching the token length of target_text.\n",
    "    target_ids = tokenizer.encode(target_text, add_special_tokens=False)\n",
    "    target_len = len(target_ids)\n",
    "    if target_len == 0:\n",
    "        return \"\"\n",
    "    rng = np.random.RandomState(seed)\n",
    "    vocab_size = len(tokenizer)\n",
    "    min_id = 3\n",
    "    random_ids = rng.randint(min_id, vocab_size, size=target_len)\n",
    "    random_text = tokenizer.decode(random_ids.tolist(), skip_special_tokens=True)\n",
    "    reencoded = tokenizer.encode(random_text, add_special_tokens=False)\n",
    "    if len(reencoded) != target_len:\n",
    "        if len(reencoded) > target_len:\n",
    "            random_text = tokenizer.decode(reencoded[:target_len], skip_special_tokens=True)\n",
    "        else:\n",
    "            extra_needed = target_len - len(reencoded)\n",
    "            extra_ids = rng.randint(min_id, vocab_size, size=extra_needed)\n",
    "            extra_text = tokenizer.decode(extra_ids.tolist(), skip_special_tokens=True)\n",
    "            random_text = random_text + extra_text\n",
    "            reencoded2 = tokenizer.encode(random_text, add_special_tokens=False)\n",
    "            if len(reencoded2) > target_len:\n",
    "                random_text = tokenizer.decode(reencoded2[:target_len], skip_special_tokens=True)\n",
    "    return random_text\n",
    "\n",
    "\n",
    "def extract_tfidf_keywords(passage, n_keywords=8):\n",
    "    # Extract top content words by frequency (stopwords removed).\n",
    "    words = re.findall(r'\\b[a-zA-Z]+\\b', passage.lower())\n",
    "    content_words = [w for w in words if w not in STOPWORDS and len(w) > 2]\n",
    "    return ' '.join([w for w, _ in Counter(content_words).most_common(n_keywords)])\n",
    "\n",
    "\n",
    "def extract_first_sentence(passage, max_words=30):\n",
    "    # Extract first sentence of passage, up to max_words.\n",
    "    first = passage.split('.')[0].strip()\n",
    "    return ' '.join(first.split()[:max_words])\n",
    "\n",
    "\n",
    "def oracle_to_keywords(query):\n",
    "    # Strip question/function words from oracle query.\n",
    "    words = re.findall(r'\\b[a-zA-Z]+\\b', query)\n",
    "    return ' '.join([w for w in words if w.lower() not in QUESTION_STOPWORDS and len(w) > 2])\n",
    "\n",
    "\n",
    "def shuffle_tokens(text, tokenizer, seed):\n",
    "    # Shuffle token IDs of text while preserving token count.\n",
    "    ids = tokenizer.encode(text, add_special_tokens=False)\n",
    "    rng = np.random.RandomState(seed)\n",
    "    rng.shuffle(ids)\n",
    "    return tokenizer.decode(ids, skip_special_tokens=True)\n",
    "\n",
    "\n",
    "print(\"Config ready\")\n",
    "print(f\"  num_samples pool: {config.num_samples}\")\n",
    "print(f\"  eval samples: {N_EVAL}\")\n",
    "print(f\"  passage words: {config.min_passage_words}-{config.max_passage_words}\")\n",
    "print(f\"  bonferroni_alpha: {BONFERRONI_ALPHA:.4f} ({N_COMPARISONS} comparisons)\")\n",
    "print(f\"  conditions: 15\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a0f860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Load MS MARCO (2000 samples, full distribution)\n",
    "dataset = load_ms_marco(config)\n",
    "\n",
    "np.random.seed(SEED)\n",
    "all_samples = load_evaluation_samples(dataset, config, require_answer=True)\n",
    "\n",
    "# Take first 2000 for manageable runtime\n",
    "samples = all_samples[:N_EVAL]\n",
    "N = len(samples)\n",
    "print(f\"Loaded {len(all_samples)} candidates, using first {N} for evaluation\")\n",
    "print(f\"Example passage ({len(samples[0]['passage'].split())} words): {samples[0]['passage'][:100]}...\")\n",
    "print(f\"Example query: {samples[0]['query']}\")\n",
    "print(f\"Example answer: {samples[0]['answer']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffdd33c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Generate ALL LLM surrogates (5 templates + summary, checkpointed)\n",
    "# generate_all_5_surrogates() gives: keyword_query, target_question, symptom_scenario,\n",
    "#     misconception_negative, messy_realworld\n",
    "# generate_summary() gives: 2-sentence summary\n",
    "# Total: 6 LLM calls per sample\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PHASE 1: LLM SURROGATE GENERATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "surrogates_5_path = SURROGATES_DIR / \"all_5_surrogates.json\"\n",
    "summaries_path = SURROGATES_DIR / \"summaries.json\"\n",
    "\n",
    "# --- Load or generate all_5_surrogates ---\n",
    "if surrogates_5_path.exists():\n",
    "    with open(surrogates_5_path, 'r') as f:\n",
    "        surrogates_5_data = json.load(f)\n",
    "    surrogates_5 = surrogates_5_data['surrogates']\n",
    "    print(f\"Loaded {len(surrogates_5)} sets of 5-template surrogates from cache\")\n",
    "else:\n",
    "    surrogates_5 = []\n",
    "\n",
    "start_5 = len(surrogates_5)\n",
    "if start_5 < N:\n",
    "    print(f\"Generating 5-template surrogates for samples {start_5} to {N-1}...\")\n",
    "    t_start = time.time()\n",
    "    for idx in tqdm(range(start_5, N), initial=start_5, total=N,\n",
    "                     desc=\"5-template surrogates\"):\n",
    "        passage = samples[idx]['passage']\n",
    "        try:\n",
    "            s5 = generate_all_5_surrogates(passage, model, tokenizer, config)\n",
    "        except Exception as e:\n",
    "            print(f\"  WARNING: 5-template generation failed for sample {idx}: {e}\")\n",
    "            s5 = {k: \"\" for k in TOP_5_SURROGATE_TEMPLATES.keys()}\n",
    "        surrogates_5.append(s5)\n",
    "\n",
    "        if (idx + 1) % 100 == 0 or idx == N - 1:\n",
    "            with open(surrogates_5_path, 'w') as f:\n",
    "                json.dump({'surrogates': surrogates_5}, f)\n",
    "            elapsed = time.time() - t_start\n",
    "            rate = (idx - start_5 + 1) / elapsed if elapsed > 0 else 0\n",
    "            remaining = (N - idx - 1) / rate if rate > 0 else 0\n",
    "            tqdm.write(f\"  Saved {idx+1}/{N} | {rate:.2f} s/s | ETA: {remaining/60:.1f} min\")\n",
    "\n",
    "    with open(surrogates_5_path, 'w') as f:\n",
    "        json.dump({'surrogates': surrogates_5}, f)\n",
    "    print(f\"5-template surrogates complete: {len(surrogates_5)} samples\")\n",
    "else:\n",
    "    print(f\"All 5-template surrogates already cached ({len(surrogates_5)} samples)\")\n",
    "\n",
    "# --- Load or generate summaries ---\n",
    "if summaries_path.exists():\n",
    "    with open(summaries_path, 'r') as f:\n",
    "        summaries_data = json.load(f)\n",
    "    summaries = summaries_data['summaries']\n",
    "    print(f\"Loaded {len(summaries)} summaries from cache\")\n",
    "else:\n",
    "    summaries = []\n",
    "\n",
    "start_sum = len(summaries)\n",
    "if start_sum < N:\n",
    "    print(f\"Generating summaries for samples {start_sum} to {N-1}...\")\n",
    "    t_start = time.time()\n",
    "    for idx in tqdm(range(start_sum, N), initial=start_sum, total=N, desc=\"Summaries\"):\n",
    "        passage = samples[idx]['passage']\n",
    "        try:\n",
    "            summary = generate_summary(passage, model, tokenizer, config)\n",
    "        except Exception as e:\n",
    "            print(f\"  WARNING: Summary generation failed for sample {idx}: {e}\")\n",
    "            summary = \"\"\n",
    "        summaries.append(summary)\n",
    "\n",
    "        if (idx + 1) % 100 == 0 or idx == N - 1:\n",
    "            with open(summaries_path, 'w') as f:\n",
    "                json.dump({'summaries': summaries}, f)\n",
    "            elapsed = time.time() - t_start\n",
    "            rate = (idx - start_sum + 1) / elapsed if elapsed > 0 else 0\n",
    "            remaining = (N - idx - 1) / rate if rate > 0 else 0\n",
    "            tqdm.write(f\"  Saved {idx+1}/{N} | {rate:.2f} s/s | ETA: {remaining/60:.1f} min\")\n",
    "\n",
    "    with open(summaries_path, 'w') as f:\n",
    "        json.dump({'summaries': summaries}, f)\n",
    "    print(f\"Summaries complete: {len(summaries)} samples\")\n",
    "else:\n",
    "    print(f\"All summaries already cached ({len(summaries)} samples)\")\n",
    "\n",
    "# Validate\n",
    "n_empty_kw = sum(1 for s in surrogates_5 if not s.get('keyword_query', '').strip())\n",
    "n_empty_q = sum(1 for s in surrogates_5 if not s.get('target_question', '').strip())\n",
    "n_empty_sum = sum(1 for s in summaries if not s.strip())\n",
    "print(f\"\\nValidation:\")\n",
    "print(f\"  Empty keyword surrogates: {n_empty_kw}/{N}\")\n",
    "print(f\"  Empty question surrogates: {n_empty_q}/{N}\")\n",
    "print(f\"  Empty summaries: {n_empty_sum}/{N}\")\n",
    "\n",
    "# Show examples\n",
    "print(f\"\\nExamples (sample 0):\")\n",
    "print(f\"  Passage: {samples[0]['passage'][:80]}...\")\n",
    "for k, v in surrogates_5[0].items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "print(f\"  summary: {summaries[0][:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9cb21a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Compute non-LLM surrogates + token overlap diagnostics\n",
    "print(\"=\" * 70)\n",
    "print(\"PHASE 1b: NON-LLM SURROGATES + TOKEN OVERLAP\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Pre-compute all non-LLM surrogates\n",
    "tfidf_keywords = []\n",
    "anti_keywords = []\n",
    "passage_echos = []\n",
    "oracle_as_kw = []\n",
    "shuffled_llm = []\n",
    "\n",
    "for i in range(N):\n",
    "    passage = samples[i]['passage']\n",
    "    query = samples[i]['query']\n",
    "    llm_kw = surrogates_5[i].get('keyword_query', '')\n",
    "\n",
    "    # TF-IDF keywords from THIS passage\n",
    "    tfidf_keywords.append(extract_tfidf_keywords(passage, n_keywords=8))\n",
    "\n",
    "    # Anti-keywords from WRONG passage (offset by 500)\n",
    "    wrong_passage = samples[(i + 500) % N]['passage']\n",
    "    anti_keywords.append(extract_tfidf_keywords(wrong_passage, n_keywords=8))\n",
    "\n",
    "    # Passage echo: first sentence\n",
    "    passage_echos.append(extract_first_sentence(passage, max_words=30))\n",
    "\n",
    "    # Oracle-as-keywords: strip question/function words\n",
    "    oracle_as_kw.append(oracle_to_keywords(query))\n",
    "\n",
    "    # Shuffled-LLM: shuffle token IDs of LLM keyword surrogate\n",
    "    shuffled_llm.append(shuffle_tokens(llm_kw, tokenizer, seed=SEED + i))\n",
    "\n",
    "# Compute token overlaps for ALL non-bare conditions vs passage\n",
    "print(\"Computing token overlaps...\")\n",
    "overlap_data = {}\n",
    "overlap_labels = [\n",
    "    ('random', 'Random prefix'),\n",
    "    ('oracle', 'Oracle query'),\n",
    "    ('oracle_kw', 'Oracle-as-keywords'),\n",
    "    ('anti_kw', 'Anti-keywords (wrong doc)'),\n",
    "    ('tfidf', 'TF-IDF keywords (right doc)'),\n",
    "    ('echo', 'Passage echo (1st sentence)'),\n",
    "    ('shuffled', 'Shuffled LLM'),\n",
    "    ('llm_keyword', 'LLM keyword'),\n",
    "    ('llm_question', 'LLM question'),\n",
    "    ('llm_symptom', 'LLM symptom'),\n",
    "    ('llm_summary', 'LLM summary'),\n",
    "    ('llm_messy', 'LLM messy'),\n",
    "]\n",
    "\n",
    "for i in tqdm(range(N), desc=\"Token overlap\"):\n",
    "    passage = samples[i]['passage']\n",
    "    query = samples[i]['query']\n",
    "    random_text = generate_random_prefix_text(query, tokenizer, seed=SEED + i)\n",
    "\n",
    "    overlaps_i = {\n",
    "        'random': compute_token_overlap(random_text, passage, tokenizer),\n",
    "        'oracle': compute_token_overlap(query, passage, tokenizer),\n",
    "        'oracle_kw': compute_token_overlap(oracle_as_kw[i], passage, tokenizer),\n",
    "        'anti_kw': compute_token_overlap(anti_keywords[i], passage, tokenizer),\n",
    "        'tfidf': compute_token_overlap(tfidf_keywords[i], passage, tokenizer),\n",
    "        'echo': compute_token_overlap(passage_echos[i], passage, tokenizer),\n",
    "        'shuffled': compute_token_overlap(shuffled_llm[i], passage, tokenizer),\n",
    "        'llm_keyword': compute_token_overlap(surrogates_5[i].get('keyword_query', ''), passage, tokenizer),\n",
    "        'llm_question': compute_token_overlap(surrogates_5[i].get('target_question', ''), passage, tokenizer),\n",
    "        'llm_symptom': compute_token_overlap(surrogates_5[i].get('symptom_scenario', ''), passage, tokenizer),\n",
    "        'llm_summary': compute_token_overlap(summaries[i], passage, tokenizer),\n",
    "        'llm_messy': compute_token_overlap(surrogates_5[i].get('messy_realworld', ''), passage, tokenizer),\n",
    "    }\n",
    "    overlap_data[i] = overlaps_i\n",
    "\n",
    "# Report overlap gradient\n",
    "print(f\"\\n{'Condition':<30} {'Mean Overlap':>12} {'Std':>10}\")\n",
    "print(\"-\" * 55)\n",
    "for key, label in overlap_labels:\n",
    "    vals = [overlap_data[i][key] for i in range(N)]\n",
    "    print(f\"{label:<30} {np.mean(vals):>12.4f} {np.std(vals):>10.4f}\")\n",
    "\n",
    "# Save overlap data\n",
    "with open(RESULTS_DIR / \"overlap_data.json\", 'w') as f:\n",
    "    json.dump({str(k): v for k, v in overlap_data.items()}, f)\n",
    "print(f\"\\nOverlap data saved to {RESULTS_DIR / 'overlap_data.json'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c611d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Condition explanation with concrete examples\n",
    "print(\"=\" * 70)\n",
    "print(\"EXPERIMENTAL CONDITIONS EXPLAINED\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "ex_i = 0\n",
    "ex_passage = samples[ex_i]['passage']\n",
    "ex_query = samples[ex_i]['query']\n",
    "ex_llm_kw = surrogates_5[ex_i].get('keyword_query', '')\n",
    "ex_llm_q = surrogates_5[ex_i].get('target_question', '')\n",
    "ex_llm_symp = surrogates_5[ex_i].get('symptom_scenario', '')\n",
    "ex_llm_messy = surrogates_5[ex_i].get('messy_realworld', '')\n",
    "ex_summary = summaries[ex_i]\n",
    "ex_tfidf = tfidf_keywords[ex_i]\n",
    "ex_anti = anti_keywords[ex_i]\n",
    "ex_echo = passage_echos[ex_i]\n",
    "ex_oracle_kw = oracle_as_kw[ex_i]\n",
    "ex_shuffled = shuffled_llm[ex_i]\n",
    "\n",
    "conditions_explained = [\n",
    "    (\"1. Bare\", \"[BOS][doc]\", \"No prefix — baseline\"),\n",
    "    (\"2. Random-truncated\", \"[BOS][random_tokens\\\\n][doc] → truncate + RoPE\",\n",
    "     f\"Random text: '{generate_random_prefix_text(ex_query, tokenizer, SEED)[:60]}...'\"),\n",
    "    (\"3. Separator-only\", \"[BOS][doc][\\\\n\\\\nRelated question: ]\",\n",
    "     \"Suffix appended after passage — structural framing only\"),\n",
    "    (\"4. Oracle-truncated\", \"[BOS][query\\\\n][doc] → truncate + RoPE\",\n",
    "     f\"Query: '{ex_query[:60]}...'\"),\n",
    "    (\"5. Oracle-as-keywords\", \"[BOS][keywords\\\\n][doc] → truncate + RoPE\",\n",
    "     f\"Keywords from oracle: '{ex_oracle_kw[:60]}'\"),\n",
    "    (\"6. Anti-keywords\", \"[BOS][wrong_tfidf\\\\n][doc] → truncate + RoPE\",\n",
    "     f\"TF-IDF from WRONG passage: '{ex_anti[:60]}'\"),\n",
    "    (\"7. TF-IDF-keywords\", \"[BOS][tfidf\\\\n][doc] → truncate + RoPE\",\n",
    "     f\"TF-IDF from THIS passage: '{ex_tfidf[:60]}'\"),\n",
    "    (\"8. Passage-echo\", \"[BOS][first_sent\\\\n][doc] → truncate + RoPE\",\n",
    "     f\"First sentence: '{ex_echo[:60]}...'\"),\n",
    "    (\"9. Shuffled-LLM\", \"[BOS][shuffled\\\\n][doc] → truncate + RoPE\",\n",
    "     f\"Shuffled LLM keyword tokens: '{ex_shuffled[:60]}'\"),\n",
    "    (\"10. LLM-keyword\", \"[BOS][llm_kw\\\\n][doc] → truncate + RoPE\",\n",
    "     f\"LLM keyword: '{ex_llm_kw[:60]}'\"),\n",
    "    (\"11. LLM-question\", \"[BOS][llm_q\\\\n][doc] → truncate + RoPE\",\n",
    "     f\"LLM question: '{ex_llm_q[:60]}'\"),\n",
    "    (\"12. LLM-symptom\", \"[BOS][llm_symp\\\\n][doc] → truncate + RoPE\",\n",
    "     f\"LLM symptom: '{ex_llm_symp[:60]}'\"),\n",
    "    (\"13. LLM-summary\", \"[BOS][summary\\\\n][doc] → truncate + RoPE\",\n",
    "     f\"Summary: '{ex_summary[:60]}...'\"),\n",
    "    (\"14. LLM-keyword+sep\", \"[BOS][llm_kw\\\\n][doc][\\\\n\\\\nRelated question: ] (prefix+suffix)\",\n",
    "     f\"Stacking: truncated prefix + suffix separator\"),\n",
    "    (\"15. LLM-messy\", \"[BOS][llm_messy\\\\n][doc] → truncate + RoPE\",\n",
    "     f\"LLM messy: '{ex_llm_messy[:60]}'\"),\n",
    "]\n",
    "\n",
    "for name, pattern, detail in conditions_explained:\n",
    "    print(f\"\\n### {name} ###\")\n",
    "    print(f\"  Cache: {pattern}\")\n",
    "    print(f\"  Detail: {detail}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db8429c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Main eval loop — 15 conditions × 2000 samples\n",
    "print(\"=\" * 70)\n",
    "print(\"PHASE 2: MAIN EVALUATION (15 conditions × 2000 samples)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "CONDITION_NAMES = [\n",
    "    'bare', 'random_trunc', 'separator_only',\n",
    "    'oracle_trunc', 'oracle_as_kw',\n",
    "    'anti_keywords', 'tfidf_keywords', 'passage_echo', 'shuffled_llm',\n",
    "    'llm_keyword', 'llm_question', 'llm_symptom', 'llm_summary',\n",
    "    'llm_keyword_sep', 'llm_messy',\n",
    "]\n",
    "\n",
    "results = []\n",
    "start_idx = 0\n",
    "\n",
    "if CHECKPOINT_PATH.exists():\n",
    "    with open(CHECKPOINT_PATH, 'r') as f:\n",
    "        ckpt = json.load(f)\n",
    "    ckpt_queries = ckpt.get('sample_queries', [])\n",
    "    current_queries = [s['query'] for s in samples]\n",
    "    if ckpt_queries == current_queries:\n",
    "        results = ckpt['results']\n",
    "        start_idx = len(results)\n",
    "        print(f\"Resuming from checkpoint: {start_idx}/{N}\")\n",
    "    else:\n",
    "        print(\"Checkpoint sample mismatch. Starting fresh.\")\n",
    "else:\n",
    "    print(\"No checkpoint found. Starting fresh.\")\n",
    "\n",
    "print(f\"Evaluating samples {start_idx} to {N-1}\")\n",
    "print(f\"Conditions: {len(CONDITION_NAMES)}\")\n",
    "\n",
    "t_start = time.time()\n",
    "\n",
    "for idx in tqdm(range(start_idx, N), initial=start_idx, total=N, desc=\"Evaluating\"):\n",
    "    sample = samples[idx]\n",
    "    passage = sample['passage']\n",
    "    query = sample['query']\n",
    "    answer = sample['answer']\n",
    "\n",
    "    query_prompt = QUERY_TEMPLATE.format(query=query)\n",
    "    answer_text = ANSWER_TEMPLATE.format(answer=answer)\n",
    "\n",
    "    # --- Matched tokenization (same doc_ids for ALL truncated conditions) ---\n",
    "    oracle_prefix = SURROGATE_PREFIX_TEMPLATE.format(surrogate=query)\n",
    "    document_text = DOCUMENT_TEMPLATE.format(document=passage)\n",
    "    full_oracle_text = oracle_prefix + document_text\n",
    "\n",
    "    full_oracle_enc = tokenizer(full_oracle_text, return_tensors=\"pt\",\n",
    "                                add_special_tokens=True, padding=False, truncation=False)\n",
    "    full_oracle_ids = full_oracle_enc['input_ids'].to(config.device)\n",
    "\n",
    "    oracle_prefix_enc = tokenizer(oracle_prefix, return_tensors=\"pt\",\n",
    "                                  add_special_tokens=True, padding=False, truncation=False)\n",
    "    oracle_prefix_len = oracle_prefix_enc['input_ids'].shape[1]\n",
    "\n",
    "    bos_id = full_oracle_ids[:, :1]\n",
    "    doc_ids = full_oracle_ids[:, oracle_prefix_len:]\n",
    "    doc_len = doc_ids.shape[1]\n",
    "\n",
    "    # Pre-compute all prefix texts\n",
    "    random_text = generate_random_prefix_text(query, tokenizer, seed=SEED + idx)\n",
    "    llm_kw_text = surrogates_5[idx].get('keyword_query', '')\n",
    "    llm_q_text = surrogates_5[idx].get('target_question', '')\n",
    "    llm_symp_text = surrogates_5[idx].get('symptom_scenario', '')\n",
    "    llm_messy_text = surrogates_5[idx].get('messy_realworld', '')\n",
    "    summary_text = summaries[idx]\n",
    "    tfidf_text = tfidf_keywords[idx]\n",
    "    anti_kw_text = anti_keywords[idx]\n",
    "    echo_text = passage_echos[idx]\n",
    "    oracle_kw_text = oracle_as_kw[idx]\n",
    "    shuffled_text = shuffled_llm[idx]\n",
    "\n",
    "    # Helper: build truncated cache from prefix text\n",
    "    def build_trunc(prefix_text):\n",
    "        prefix_str = SURROGATE_PREFIX_TEMPLATE.format(surrogate=prefix_text)\n",
    "        prefix_enc = tokenizer(prefix_str, return_tensors=\"pt\",\n",
    "                               add_special_tokens=False, padding=False, truncation=False)\n",
    "        prefix_ids = prefix_enc['input_ids'].to(config.device)\n",
    "        full_ids = torch.cat([bos_id, prefix_ids, doc_ids], dim=1)\n",
    "        prefix_token_len = 1 + prefix_ids.shape[1]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            out = model(input_ids=full_ids,\n",
    "                        attention_mask=torch.ones_like(full_ids),\n",
    "                        use_cache=True, return_dict=True)\n",
    "        cache = extract_and_truncate_cache_with_bos(out.past_key_values, doc_len)\n",
    "        correct_rope_positions_with_bos(cache, prefix_token_len - 1, model)\n",
    "        nll = score_answer_with_cache(\n",
    "            deepcopy_cache(cache), 1 + doc_len,\n",
    "            query_prompt, answer_text, model, tokenizer, config)\n",
    "        del out, cache\n",
    "        return nll\n",
    "\n",
    "    # === Condition 1: BARE ===\n",
    "    bare_ids = torch.cat([bos_id, doc_ids], dim=1)\n",
    "    bare_len = bare_ids.shape[1]\n",
    "    with torch.no_grad():\n",
    "        bare_out = model(input_ids=bare_ids, attention_mask=torch.ones_like(bare_ids),\n",
    "                         use_cache=True, return_dict=True)\n",
    "    nll_bare = score_answer_with_cache(\n",
    "        deepcopy_cache(bare_out.past_key_values), bare_len,\n",
    "        query_prompt, answer_text, model, tokenizer, config)\n",
    "    del bare_out\n",
    "\n",
    "    # === Condition 2: RANDOM-TRUNCATED ===\n",
    "    nll_random = build_trunc(random_text)\n",
    "\n",
    "    # === Condition 3: SEPARATOR-ONLY ===\n",
    "    sep_len, sep_cache = build_suffix_kv_cache(\n",
    "        passage, \"\", model, tokenizer, config, separator=SUFFIX_SEPARATOR)\n",
    "    nll_separator = score_answer_with_cache(\n",
    "        deepcopy_cache(sep_cache), sep_len,\n",
    "        query_prompt, answer_text, model, tokenizer, config)\n",
    "    del sep_cache\n",
    "\n",
    "    # === Condition 4: ORACLE-TRUNCATED ===\n",
    "    with torch.no_grad():\n",
    "        oracle_out = model(input_ids=full_oracle_ids,\n",
    "                           attention_mask=torch.ones_like(full_oracle_ids),\n",
    "                           use_cache=True, return_dict=True)\n",
    "    oracle_cache = extract_and_truncate_cache_with_bos(oracle_out.past_key_values, doc_len)\n",
    "    correct_rope_positions_with_bos(oracle_cache, oracle_prefix_len - 1, model)\n",
    "    nll_oracle = score_answer_with_cache(\n",
    "        deepcopy_cache(oracle_cache), 1 + doc_len,\n",
    "        query_prompt, answer_text, model, tokenizer, config)\n",
    "    del oracle_out, oracle_cache\n",
    "\n",
    "    # === Condition 5: ORACLE-AS-KEYWORDS ===\n",
    "    nll_oracle_kw = build_trunc(oracle_kw_text)\n",
    "\n",
    "    # === Condition 6: ANTI-KEYWORDS ===\n",
    "    nll_anti_kw = build_trunc(anti_kw_text)\n",
    "\n",
    "    # === Condition 7: TF-IDF-KEYWORDS ===\n",
    "    nll_tfidf = build_trunc(tfidf_text)\n",
    "\n",
    "    # === Condition 8: PASSAGE-ECHO ===\n",
    "    nll_echo = build_trunc(echo_text)\n",
    "\n",
    "    # === Condition 9: SHUFFLED-LLM ===\n",
    "    nll_shuffled = build_trunc(shuffled_text)\n",
    "\n",
    "    # === Condition 10: LLM-KEYWORD ===\n",
    "    nll_llm_kw = build_trunc(llm_kw_text)\n",
    "\n",
    "    # === Condition 11: LLM-QUESTION ===\n",
    "    nll_llm_q = build_trunc(llm_q_text)\n",
    "\n",
    "    # === Condition 12: LLM-SYMPTOM ===\n",
    "    nll_llm_symp = build_trunc(llm_symp_text)\n",
    "\n",
    "    # === Condition 13: LLM-SUMMARY ===\n",
    "    nll_llm_sum = build_trunc(summary_text)\n",
    "\n",
    "    # === Condition 14: LLM-KEYWORD + SEPARATOR ===\n",
    "    # Build truncated cache with LLM keyword prefix, then use build_suffix on THAT\n",
    "    # Step 1: Build the primed document cache (truncated)\n",
    "    kw_prefix_str = SURROGATE_PREFIX_TEMPLATE.format(surrogate=llm_kw_text)\n",
    "    kw_prefix_enc = tokenizer(kw_prefix_str, return_tensors=\"pt\",\n",
    "                              add_special_tokens=False, padding=False, truncation=False)\n",
    "    kw_prefix_ids = kw_prefix_enc['input_ids'].to(config.device)\n",
    "    kw_full_ids = torch.cat([bos_id, kw_prefix_ids, doc_ids], dim=1)\n",
    "    kw_prefix_token_len = 1 + kw_prefix_ids.shape[1]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        kw_out = model(input_ids=kw_full_ids,\n",
    "                       attention_mask=torch.ones_like(kw_full_ids),\n",
    "                       use_cache=True, return_dict=True)\n",
    "    kw_trunc_cache = extract_and_truncate_cache_with_bos(kw_out.past_key_values, doc_len)\n",
    "    correct_rope_positions_with_bos(kw_trunc_cache, kw_prefix_token_len - 1, model)\n",
    "    del kw_out\n",
    "\n",
    "    # Step 2: Extend with suffix separator (forward pass through separator tokens)\n",
    "    suffix_enc = tokenizer(SUFFIX_SEPARATOR, return_tensors=\"pt\",\n",
    "                           add_special_tokens=False, padding=False, truncation=False)\n",
    "    suffix_ids = suffix_enc['input_ids'].to(config.device)\n",
    "    suffix_len = suffix_ids.shape[1]\n",
    "    cache_len_before_suffix = 1 + doc_len\n",
    "\n",
    "    # Create position_ids continuing from doc end\n",
    "    suffix_position_ids = torch.arange(\n",
    "        cache_len_before_suffix, cache_len_before_suffix + suffix_len,\n",
    "        device=config.device\n",
    "    ).unsqueeze(0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        suffix_out = model(\n",
    "            input_ids=suffix_ids,\n",
    "            attention_mask=torch.ones(1, cache_len_before_suffix + suffix_len,\n",
    "                                      device=config.device, dtype=torch.long),\n",
    "            position_ids=suffix_position_ids,\n",
    "            past_key_values=deepcopy_cache(kw_trunc_cache),\n",
    "            use_cache=True,\n",
    "            return_dict=True,\n",
    "        )\n",
    "    combo_cache = suffix_out.past_key_values\n",
    "    combo_len = cache_len_before_suffix + suffix_len\n",
    "    nll_llm_kw_sep = score_answer_with_cache(\n",
    "        deepcopy_cache(combo_cache), combo_len,\n",
    "        query_prompt, answer_text, model, tokenizer, config)\n",
    "    del suffix_out, combo_cache, kw_trunc_cache\n",
    "\n",
    "    # === Condition 15: LLM-MESSY ===\n",
    "    nll_llm_messy = build_trunc(llm_messy_text)\n",
    "\n",
    "    # --- Store result ---\n",
    "    result = {\n",
    "        'idx': idx,\n",
    "        'doc_len': doc_len,\n",
    "        'passage_word_count': len(passage.split()),\n",
    "        'bare': nll_bare,\n",
    "        'random_trunc': nll_random,\n",
    "        'separator_only': nll_separator,\n",
    "        'oracle_trunc': nll_oracle,\n",
    "        'oracle_as_kw': nll_oracle_kw,\n",
    "        'anti_keywords': nll_anti_kw,\n",
    "        'tfidf_keywords': nll_tfidf,\n",
    "        'passage_echo': nll_echo,\n",
    "        'shuffled_llm': nll_shuffled,\n",
    "        'llm_keyword': nll_llm_kw,\n",
    "        'llm_question': nll_llm_q,\n",
    "        'llm_symptom': nll_llm_symp,\n",
    "        'llm_summary': nll_llm_sum,\n",
    "        'llm_keyword_sep': nll_llm_kw_sep,\n",
    "        'llm_messy': nll_llm_messy,\n",
    "    }\n",
    "    results.append(result)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    if (idx + 1) % CHECKPOINT_EVERY == 0 or idx == N - 1:\n",
    "        ckpt_data = {\n",
    "            'results': results,\n",
    "            'sample_queries': [s['query'] for s in samples],\n",
    "            'completed': len(results),\n",
    "            'total': N,\n",
    "            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        }\n",
    "        with open(CHECKPOINT_PATH, 'w') as f:\n",
    "            json.dump(ckpt_data, f)\n",
    "        elapsed = time.time() - t_start\n",
    "        rate = (idx - start_idx + 1) / elapsed if elapsed > 0 else 0\n",
    "        remaining = (N - idx - 1) / rate if rate > 0 else 0\n",
    "        tqdm.write(f\"  Checkpoint {idx+1}/{N} | {rate:.2f} s/s | ETA: {remaining/60:.1f} min\")\n",
    "\n",
    "elapsed_total = time.time() - t_start\n",
    "print(f\"\\nEvaluation complete: {len(results)} samples in {elapsed_total/60:.1f} min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c310e46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Primary analysis — all 10 comparisons (Bonferroni alpha = 0.005)\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ANALYSIS — MECHANISM DECOMPOSITION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Extract arrays and filter zero NLLs\n",
    "cond_arrays = {}\n",
    "for cname in CONDITION_NAMES:\n",
    "    cond_arrays[cname] = np.array([r[cname] for r in results])\n",
    "\n",
    "# Valid mask: no zero NLLs in any condition\n",
    "valid = np.ones(len(results), dtype=bool)\n",
    "for cname in CONDITION_NAMES:\n",
    "    valid &= (cond_arrays[cname] != 0)\n",
    "n_valid = int(np.sum(valid))\n",
    "n_excluded = int(np.sum(~valid))\n",
    "print(f\"Total: {len(results)}, Valid: {n_valid}, Excluded: {n_excluded}\")\n",
    "\n",
    "# Apply mask\n",
    "c = {}\n",
    "for cname in CONDITION_NAMES:\n",
    "    c[cname] = cond_arrays[cname][valid]\n",
    "\n",
    "# NLL summary table\n",
    "print(f\"\\n{'Condition':<25} {'Mean NLL':>10} {'Std':>10} {'d vs Bare':>10}\")\n",
    "print(\"-\" * 60)\n",
    "for cname in CONDITION_NAMES:\n",
    "    mean_nll = np.mean(c[cname])\n",
    "    std_nll = np.std(c[cname])\n",
    "    if cname == 'bare':\n",
    "        d_str = \"—\"\n",
    "    else:\n",
    "        d = cohens_d(c['bare'] - c[cname])\n",
    "        d_str = f\"{d:+.3f}\"\n",
    "    print(f\"{cname:<25} {mean_nll:>10.4f} {std_nll:>10.4f} {d_str:>10}\")\n",
    "\n",
    "# 10 primary comparisons\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"10 PRIMARY COMPARISONS (Bonferroni alpha = {BONFERRONI_ALPHA:.4f})\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "comparisons = [\n",
    "    # (name, delta_array, question)\n",
    "    # delta > 0 means first condition is better (lower NLL)\n",
    "    ('M1: Shuffled vs LLM-kw', c['shuffled_llm'] - c['llm_keyword'],\n",
    "     'Does coherence matter?'),\n",
    "    ('M2: Oracle-kw vs Oracle', c['oracle_trunc'] - c['oracle_as_kw'],\n",
    "     'Does question format hurt?'),\n",
    "    ('M3: LLM-kw vs LLM-question', c['llm_question'] - c['llm_keyword'],\n",
    "     'Keyword vs question format?'),\n",
    "    ('M4: TF-IDF vs Anti-kw', c['anti_keywords'] - c['tfidf_keywords'],\n",
    "     'Does passage specificity matter?'),\n",
    "    ('M5: Echo vs LLM-kw', c['llm_keyword'] - c['passage_echo'],\n",
    "     'Is max overlap the ceiling?'),\n",
    "    ('M6: TF-IDF vs LLM-kw', c['llm_keyword'] - c['tfidf_keywords'],\n",
    "     'Is LLM necessary beyond TF-IDF?'),\n",
    "    ('M7: LLM-kw+sep vs best single',\n",
    "     np.minimum(c['llm_keyword'], c['separator_only']) - c['llm_keyword_sep'],\n",
    "     'Does stacking help?'),\n",
    "    ('R1: LLM-kw vs Random', c['random_trunc'] - c['llm_keyword'],\n",
    "     'Replicates Exp 05 (d≈0.2)?'),\n",
    "    ('R2: Oracle vs Random', c['random_trunc'] - c['oracle_trunc'],\n",
    "     'Replicates null (d≈0)?'),\n",
    "    ('R3: LLM-kw vs Bare', c['bare'] - c['llm_keyword'],\n",
    "     'Overall LLM benefit?'),\n",
    "]\n",
    "\n",
    "print(f\"\\n{'Comparison':<30} {'Mean Δ':>8} {'d':>8} {'Win%':>7} {'t':>8} {'p':>12} {'Sig':>5}\")\n",
    "print(\"-\" * 85)\n",
    "\n",
    "comparison_results = {}\n",
    "for name, delta, question in comparisons:\n",
    "    d = cohens_d(delta)\n",
    "    win = np.mean(delta > 0) * 100\n",
    "    t_stat, p_val = stats.ttest_1samp(delta, 0)\n",
    "    sig = \"***\" if p_val < 0.001 else \"**\" if p_val < BONFERRONI_ALPHA else \"*\" if p_val < 0.05 else \"ns\"\n",
    "    print(f\"{name:<30} {np.mean(delta):>8.4f} {d:>8.3f} {win:>6.1f}% {t_stat:>8.2f} {p_val:>11.2e} {sig:>5}\")\n",
    "    comparison_results[name] = {\n",
    "        'mean_delta': float(np.mean(delta)),\n",
    "        'cohens_d': float(d),\n",
    "        'win_rate': float(win / 100),\n",
    "        't_stat': float(t_stat),\n",
    "        'p_value': float(p_val),\n",
    "        'bonferroni_significant': bool(p_val < BONFERRONI_ALPHA),\n",
    "        'question': question,\n",
    "    }\n",
    "\n",
    "# All conditions vs Bare\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"ALL CONDITIONS vs BARE\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"\\n{'Condition':<25} {'d vs Bare':>10} {'Win%':>7} {'p':>12}\")\n",
    "print(\"-\" * 60)\n",
    "all_vs_bare = {}\n",
    "for cname in CONDITION_NAMES:\n",
    "    if cname == 'bare':\n",
    "        continue\n",
    "    delta = c['bare'] - c[cname]\n",
    "    d = cohens_d(delta)\n",
    "    win = np.mean(delta > 0) * 100\n",
    "    _, p_val = stats.ttest_1samp(delta, 0)\n",
    "    sig = \"***\" if p_val < 0.001 else \"**\" if p_val < BONFERRONI_ALPHA else \"*\" if p_val < 0.05 else \"ns\"\n",
    "    print(f\"{cname:<25} {d:>10.3f} {win:>6.1f}% {p_val:>11.2e} {sig:>5}\")\n",
    "    all_vs_bare[cname] = {'cohens_d': float(d), 'win_rate': float(win/100), 'p_value': float(p_val)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3103fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Token overlap mechanism analysis + regression\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"TOKEN OVERLAP MECHANISM ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Map condition names to overlap keys\n",
    "cond_to_overlap = {\n",
    "    'random_trunc': 'random',\n",
    "    'oracle_trunc': 'oracle',\n",
    "    'oracle_as_kw': 'oracle_kw',\n",
    "    'anti_keywords': 'anti_kw',\n",
    "    'tfidf_keywords': 'tfidf',\n",
    "    'passage_echo': 'echo',\n",
    "    'shuffled_llm': 'shuffled',\n",
    "    'llm_keyword': 'llm_keyword',\n",
    "    'llm_question': 'llm_question',\n",
    "    'llm_symptom': 'llm_symptom',\n",
    "    'llm_summary': 'llm_summary',\n",
    "    'llm_messy': 'llm_messy',\n",
    "}\n",
    "\n",
    "# 1. Pool all (sample, condition) pairs → overlap vs delta\n",
    "all_overlaps = []\n",
    "all_deltas = []\n",
    "all_cond_labels = []\n",
    "valid_indices = np.where(valid)[0]\n",
    "\n",
    "for cname, okey in cond_to_overlap.items():\n",
    "    for i_valid, i_orig in enumerate(valid_indices):\n",
    "        ov = overlap_data[i_orig][okey]\n",
    "        delta = c['bare'][i_valid] - c[cname][i_valid]\n",
    "        all_overlaps.append(ov)\n",
    "        all_deltas.append(delta)\n",
    "        all_cond_labels.append(cname)\n",
    "\n",
    "all_overlaps = np.array(all_overlaps)\n",
    "all_deltas = np.array(all_deltas)\n",
    "\n",
    "# Universal correlation\n",
    "r_all, p_all = stats.pearsonr(all_overlaps, all_deltas)\n",
    "print(f\"\\nUniversal overlap-delta correlation (pooled across all conditions):\")\n",
    "print(f\"  r = {r_all:.4f}, p = {p_all:.2e}, N = {len(all_overlaps)}\")\n",
    "\n",
    "# 2. Within-condition correlations\n",
    "print(f\"\\n{'Condition':<25} {'r(overlap,delta)':>18} {'p':>12} {'N':>6}\")\n",
    "print(\"-\" * 65)\n",
    "for cname, okey in cond_to_overlap.items():\n",
    "    ovs = []\n",
    "    deltas = []\n",
    "    for i_valid, i_orig in enumerate(valid_indices):\n",
    "        ovs.append(overlap_data[i_orig][okey])\n",
    "        deltas.append(c['bare'][i_valid] - c[cname][i_valid])\n",
    "    r, p = stats.pearsonr(ovs, deltas)\n",
    "    print(f\"{cname:<25} {r:>18.4f} {p:>11.2e} {len(ovs):>6}\")\n",
    "\n",
    "# 3. Cross-condition: median overlap vs mean effect size\n",
    "print(f\"\\nCross-condition: median overlap vs mean Cohen's d\")\n",
    "print(f\"{'Condition':<25} {'Median Overlap':>15} {'Mean d':>10}\")\n",
    "print(\"-\" * 55)\n",
    "cond_median_overlap = []\n",
    "cond_mean_d = []\n",
    "for cname, okey in cond_to_overlap.items():\n",
    "    ovs = [overlap_data[i_orig][okey] for i_orig in valid_indices]\n",
    "    d = cohens_d(c['bare'] - c[cname])\n",
    "    med_ov = np.median(ovs)\n",
    "    cond_median_overlap.append(med_ov)\n",
    "    cond_mean_d.append(d)\n",
    "    print(f\"{cname:<25} {med_ov:>15.4f} {d:>10.3f}\")\n",
    "\n",
    "r_cross, p_cross = stats.pearsonr(cond_median_overlap, cond_mean_d)\n",
    "print(f\"\\nCross-condition correlation: r = {r_cross:.4f}, p = {p_cross:.4f}\")\n",
    "\n",
    "# 4. Regression: delta ~ overlap + hardness + overlap*hardness\n",
    "from numpy.polynomial import polynomial as P\n",
    "\n",
    "print(f\"\\n--- Regression: delta ~ overlap + hardness + overlap*hardness ---\")\n",
    "bare_valid = c['bare']\n",
    "# Standardize\n",
    "ov_std = (all_overlaps - np.mean(all_overlaps)) / (np.std(all_overlaps) + 1e-8)\n",
    "# Hardness: repeat bare NLL for each condition\n",
    "hardness_all = np.tile(bare_valid, len(cond_to_overlap))\n",
    "h_std = (hardness_all - np.mean(hardness_all)) / (np.std(hardness_all) + 1e-8)\n",
    "interaction = ov_std * h_std\n",
    "\n",
    "X = np.column_stack([np.ones(len(all_deltas)), ov_std, h_std, interaction])\n",
    "betas, residuals, rank, sv = np.linalg.lstsq(X, all_deltas, rcond=None)\n",
    "y_pred = X @ betas\n",
    "ss_res = np.sum((all_deltas - y_pred) ** 2)\n",
    "ss_tot = np.sum((all_deltas - np.mean(all_deltas)) ** 2)\n",
    "r_squared = 1 - ss_res / ss_tot\n",
    "\n",
    "print(f\"  beta_0 (intercept):           {betas[0]:+.5f}\")\n",
    "print(f\"  beta_1 (overlap):             {betas[1]:+.5f}\")\n",
    "print(f\"  beta_2 (hardness):            {betas[2]:+.5f}\")\n",
    "print(f\"  beta_3 (overlap × hardness):  {betas[3]:+.5f}\")\n",
    "print(f\"  R² = {r_squared:.4f}\")\n",
    "\n",
    "# Decision criteria\n",
    "print(f\"\\n--- Decision Criteria ---\")\n",
    "if abs(r_all) > 0.3:\n",
    "    print(f\"  ✓ r(overlap, delta) = {r_all:.3f} > 0.3 → overlap IS the mechanism\")\n",
    "elif abs(r_all) > 0.1:\n",
    "    print(f\"  ~ r(overlap, delta) = {r_all:.3f} ∈ (0.1, 0.3) → overlap is PART of the mechanism\")\n",
    "else:\n",
    "    print(f\"  ✗ r(overlap, delta) = {r_all:.3f} < 0.1 → overlap is NOT the main mechanism\")\n",
    "\n",
    "# TF-IDF vs LLM comparison\n",
    "d_tfidf = cohens_d(c['bare'] - c['tfidf_keywords'])\n",
    "d_llm_kw = cohens_d(c['bare'] - c['llm_keyword'])\n",
    "if abs(d_tfidf - d_llm_kw) < 0.05:\n",
    "    print(f\"  ✓ TF-IDF (d={d_tfidf:.3f}) ≈ LLM-kw (d={d_llm_kw:.3f}) → LLM may be unnecessary\")\n",
    "else:\n",
    "    print(f\"  ✗ TF-IDF (d={d_tfidf:.3f}) ≠ LLM-kw (d={d_llm_kw:.3f}) → LLM adds value beyond overlap\")\n",
    "\n",
    "# Shuffled vs ordered\n",
    "d_shuffled = cohens_d(c['bare'] - c['shuffled_llm'])\n",
    "if abs(d_shuffled - d_llm_kw) < 0.05:\n",
    "    print(f\"  ✓ Shuffled (d={d_shuffled:.3f}) ≈ Ordered (d={d_llm_kw:.3f}) → coherence doesn't matter\")\n",
    "else:\n",
    "    print(f\"  ✗ Shuffled (d={d_shuffled:.3f}) ≠ Ordered (d={d_llm_kw:.3f}) → coherence matters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0d25f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Format & coherence ablation results\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"FORMAT & COHERENCE ABLATION RESULTS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# H2: Coherence — shuffled vs ordered\n",
    "delta_coherence = c['shuffled_llm'] - c['llm_keyword']\n",
    "d_coherence = cohens_d(delta_coherence)\n",
    "t_coh, p_coh = stats.ttest_1samp(delta_coherence, 0)\n",
    "print(f\"\\nH2 — COHERENCE: Shuffled-LLM vs LLM-keyword\")\n",
    "print(f\"  d = {d_coherence:+.3f}, win% = {np.mean(delta_coherence > 0)*100:.1f}%, \"\n",
    "      f\"t = {t_coh:.2f}, p = {p_coh:.2e}\")\n",
    "if p_coh < BONFERRONI_ALPHA and d_coherence > 0:\n",
    "    print(f\"  → Coherence MATTERS: ordered tokens outperform shuffled\")\n",
    "elif p_coh < BONFERRONI_ALPHA and d_coherence < 0:\n",
    "    print(f\"  → SURPRISING: shuffled tokens are BETTER than ordered\")\n",
    "else:\n",
    "    print(f\"  → Coherence does NOT matter: order is irrelevant, just token identity\")\n",
    "\n",
    "# H3: Format — question syntax\n",
    "# Oracle\n",
    "delta_fmt_oracle = c['oracle_trunc'] - c['oracle_as_kw']\n",
    "d_fmt_oracle = cohens_d(delta_fmt_oracle)\n",
    "t_fo, p_fo = stats.ttest_1samp(delta_fmt_oracle, 0)\n",
    "print(f\"\\nH3a — FORMAT (Oracle): Oracle-full-question vs Oracle-as-keywords\")\n",
    "print(f\"  d = {d_fmt_oracle:+.3f}, win% = {np.mean(delta_fmt_oracle > 0)*100:.1f}%, \"\n",
    "      f\"t = {t_fo:.2f}, p = {p_fo:.2e}\")\n",
    "if d_fmt_oracle > 0:\n",
    "    print(f\"  → Oracle-as-keywords IMPROVES over full question (question format hurts)\")\n",
    "else:\n",
    "    print(f\"  → Oracle-as-keywords does NOT improve over full question\")\n",
    "\n",
    "# LLM\n",
    "delta_fmt_llm = c['llm_question'] - c['llm_keyword']\n",
    "d_fmt_llm = cohens_d(delta_fmt_llm)\n",
    "t_fl, p_fl = stats.ttest_1samp(delta_fmt_llm, 0)\n",
    "print(f\"\\nH3b — FORMAT (LLM): LLM-keyword vs LLM-question\")\n",
    "print(f\"  d = {d_fmt_llm:+.3f}, win% = {np.mean(delta_fmt_llm > 0)*100:.1f}%, \"\n",
    "      f\"t = {t_fl:.2f}, p = {p_fl:.2e}\")\n",
    "if d_fmt_llm > 0:\n",
    "    print(f\"  → LLM-keyword OUTPERFORMS LLM-question (keyword format is better)\")\n",
    "else:\n",
    "    print(f\"  → No advantage for keyword format in LLM surrogates\")\n",
    "\n",
    "# H4: Passage Specificity\n",
    "delta_spec = c['anti_keywords'] - c['tfidf_keywords']\n",
    "d_spec = cohens_d(delta_spec)\n",
    "t_sp, p_sp = stats.ttest_1samp(delta_spec, 0)\n",
    "print(f\"\\nH4 — SPECIFICITY: TF-IDF (right doc) vs Anti-keywords (wrong doc)\")\n",
    "print(f\"  d = {d_spec:+.3f}, win% = {np.mean(delta_spec > 0)*100:.1f}%, \"\n",
    "      f\"t = {t_sp:.2f}, p = {p_sp:.2e}\")\n",
    "if p_sp < BONFERRONI_ALPHA and d_spec > 0:\n",
    "    print(f\"  → Passage-SPECIFIC keywords are BETTER than wrong-doc keywords\")\n",
    "elif p_sp >= BONFERRONI_ALPHA:\n",
    "    print(f\"  → Specificity does NOT matter: any content words work\")\n",
    "\n",
    "# H5: Stacking\n",
    "delta_stack = np.minimum(c['llm_keyword'], c['separator_only']) - c['llm_keyword_sep']\n",
    "d_stack = cohens_d(delta_stack)\n",
    "t_st, p_st = stats.ttest_1samp(delta_stack, 0)\n",
    "print(f\"\\nH5 — STACKING: LLM-keyword+sep vs best-of(LLM-keyword, sep-only)\")\n",
    "print(f\"  d = {d_stack:+.3f}, win% = {np.mean(delta_stack > 0)*100:.1f}%, \"\n",
    "      f\"t = {t_st:.2f}, p = {p_st:.2e}\")\n",
    "if p_st < BONFERRONI_ALPHA and d_stack > 0:\n",
    "    print(f\"  → Stacking WORKS: combining prefix + suffix exceeds either alone\")\n",
    "elif p_st < 0.05 and d_stack > 0:\n",
    "    print(f\"  → Suggestive stacking benefit (not Bonferroni significant)\")\n",
    "else:\n",
    "    print(f\"  → No stacking benefit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8b04d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Template ranking + production cost-benefit analysis\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"LLM TEMPLATE RANKING + COST-BENEFIT\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "llm_conditions = [\n",
    "    ('llm_keyword', 'Keyword (3-6 words)', 'cheap'),\n",
    "    ('llm_question', 'Question (5-12 words)', 'cheap'),\n",
    "    ('llm_symptom', 'Symptom (4-10 words)', 'cheap'),\n",
    "    ('llm_summary', 'Summary (2 sentences)', 'moderate'),\n",
    "    ('llm_messy', 'Messy/informal (3-8 words)', 'cheap'),\n",
    "]\n",
    "\n",
    "non_llm_conditions = [\n",
    "    ('tfidf_keywords', 'TF-IDF keywords', 'free'),\n",
    "    ('passage_echo', 'First sentence echo', 'free'),\n",
    "    ('oracle_as_kw', 'Oracle-as-keywords', 'oracle'),\n",
    "]\n",
    "\n",
    "print(f\"\\n{'Template':<30} {'d vs Bare':>10} {'d vs Random':>12} {'Win% vs Bare':>13} {'Cost':>8}\")\n",
    "print(\"-\" * 78)\n",
    "\n",
    "# LLM templates\n",
    "for cname, label, cost in llm_conditions:\n",
    "    d_bare = cohens_d(c['bare'] - c[cname])\n",
    "    d_random = cohens_d(c['random_trunc'] - c[cname])\n",
    "    win = np.mean(c['bare'] > c[cname]) * 100\n",
    "    print(f\"{label:<30} {d_bare:>10.3f} {d_random:>12.3f} {win:>12.1f}% {cost:>8}\")\n",
    "\n",
    "print(\"-\" * 78)\n",
    "# Non-LLM alternatives\n",
    "for cname, label, cost in non_llm_conditions:\n",
    "    d_bare = cohens_d(c['bare'] - c[cname])\n",
    "    d_random = cohens_d(c['random_trunc'] - c[cname])\n",
    "    win = np.mean(c['bare'] > c[cname]) * 100\n",
    "    print(f\"{label:<30} {d_bare:>10.3f} {d_random:>12.3f} {win:>12.1f}% {cost:>8}\")\n",
    "\n",
    "# Pairwise template comparisons\n",
    "print(f\"\\n--- Pairwise LLM Template Comparisons ---\")\n",
    "llm_cnames = [cn for cn, _, _ in llm_conditions]\n",
    "for i in range(len(llm_cnames)):\n",
    "    for j in range(i+1, len(llm_cnames)):\n",
    "        cn_a, cn_b = llm_cnames[i], llm_cnames[j]\n",
    "        delta = c[cn_b] - c[cn_a]  # positive means a is better\n",
    "        d = cohens_d(delta)\n",
    "        t, p = stats.ttest_1samp(delta, 0)\n",
    "        sig = \"***\" if p < 0.001 else \"**\" if p < BONFERRONI_ALPHA else \"*\" if p < 0.05 else \"ns\"\n",
    "        print(f\"  {cn_a} vs {cn_b}: d={d:+.3f}, p={p:.2e} {sig}\")\n",
    "\n",
    "# Production recommendation\n",
    "print(f\"\\n--- Production Recommendation ---\")\n",
    "best_d = -999\n",
    "best_name = \"\"\n",
    "for cname in ['llm_keyword', 'tfidf_keywords', 'passage_echo']:\n",
    "    d = cohens_d(c['bare'] - c[cname])\n",
    "    if d > best_d:\n",
    "        best_d = d\n",
    "        best_name = cname\n",
    "print(f\"Best overall: {best_name} (d={best_d:+.3f} vs bare)\")\n",
    "\n",
    "# Is LLM worth it?\n",
    "d_tfidf_bare = cohens_d(c['bare'] - c['tfidf_keywords'])\n",
    "d_llmkw_bare = cohens_d(c['bare'] - c['llm_keyword'])\n",
    "improvement = d_llmkw_bare - d_tfidf_bare\n",
    "print(f\"LLM keyword over TF-IDF: Δd = {improvement:+.3f}\")\n",
    "if improvement > 0.05:\n",
    "    print(f\"→ LLM IS worth the cost (meaningful improvement over free TF-IDF)\")\n",
    "else:\n",
    "    print(f\"→ LLM may NOT be worth the cost (minimal improvement over free TF-IDF)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0ce6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13: Hardness quintile breakdown (all conditions)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"HARDNESS QUINTILE BREAKDOWN\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "bare_valid = c['bare']\n",
    "quintile_boundaries = np.percentile(bare_valid, [20, 40, 60, 80])\n",
    "quintile_labels = ['Q1 (easiest)', 'Q2', 'Q3', 'Q4', 'Q5 (hardest)']\n",
    "\n",
    "def get_quintile(nll, boundaries):\n",
    "    for i, b in enumerate(boundaries):\n",
    "        if nll <= b:\n",
    "            return i\n",
    "    return len(boundaries)\n",
    "\n",
    "quintiles = np.array([get_quintile(nll, quintile_boundaries) for nll in bare_valid])\n",
    "\n",
    "# Header\n",
    "conditions_to_show = [\n",
    "    'random_trunc', 'oracle_trunc', 'oracle_as_kw', 'tfidf_keywords',\n",
    "    'llm_keyword', 'llm_question', 'llm_symptom', 'passage_echo',\n",
    "    'shuffled_llm', 'llm_keyword_sep',\n",
    "]\n",
    "header = f\"{'Condition':<20}\" + \"\".join(f\"{ql:>14}\" for ql in quintile_labels) + f\"{'Overall':>14}\"\n",
    "print(f\"\\n{header}\")\n",
    "print(\"-\" * (20 + 14 * 6))\n",
    "\n",
    "hardness_breakdown = {}\n",
    "for cname in conditions_to_show:\n",
    "    row = f\"{cname:<20}\"\n",
    "    quintile_ds = []\n",
    "    for q in range(5):\n",
    "        mask_q = quintiles == q\n",
    "        if np.sum(mask_q) < 10:\n",
    "            row += f\"{'n/a':>14}\"\n",
    "            quintile_ds.append(None)\n",
    "        else:\n",
    "            delta = bare_valid[mask_q] - c[cname][mask_q]\n",
    "            d = cohens_d(delta)\n",
    "            row += f\"{d:>+14.3f}\"\n",
    "            quintile_ds.append(float(d))\n",
    "    # Overall\n",
    "    d_all = cohens_d(bare_valid - c[cname])\n",
    "    row += f\"{d_all:>+14.3f}\"\n",
    "    print(row)\n",
    "    hardness_breakdown[cname] = {\n",
    "        'quintile_ds': quintile_ds,\n",
    "        'overall_d': float(d_all),\n",
    "    }\n",
    "\n",
    "# Hardness interaction correlations\n",
    "print(f\"\\nHardness interaction (r between bare NLL and benefit):\")\n",
    "for cname in conditions_to_show:\n",
    "    delta = bare_valid - c[cname]\n",
    "    r, p = stats.pearsonr(bare_valid, delta)\n",
    "    sig = \"***\" if p < 0.001 else \"**\" if p < 0.01 else \"*\" if p < 0.05 else \"ns\"\n",
    "    print(f\"  {cname:<20}: r={r:+.3f}, p={p:.2e} {sig}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5948913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 14: Plots (overlap scatter, condition bars, hardness heatmap)\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "\n",
    "# --- Plot 1: Overlap scatter (pooled across all conditions) ---\n",
    "ax = axes[0, 0]\n",
    "# Color by condition type\n",
    "cond_colors = {\n",
    "    'random_trunc': 'gray', 'oracle_trunc': 'royalblue', 'oracle_as_kw': 'cornflowerblue',\n",
    "    'anti_keywords': 'salmon', 'tfidf_keywords': 'orange', 'passage_echo': 'gold',\n",
    "    'shuffled_llm': 'mediumpurple', 'llm_keyword': 'forestgreen', 'llm_question': 'limegreen',\n",
    "    'llm_symptom': 'darkgreen', 'llm_summary': 'olive', 'llm_messy': 'teal',\n",
    "}\n",
    "for cname, okey in cond_to_overlap.items():\n",
    "    ovs = [overlap_data[i_orig][okey] for i_orig in valid_indices]\n",
    "    deltas_plot = c['bare'] - c[cname]\n",
    "    ax.scatter(ovs, deltas_plot, alpha=0.03, s=3, c=cond_colors.get(cname, 'gray'))\n",
    "# Overlay condition means\n",
    "for cname, okey in cond_to_overlap.items():\n",
    "    ovs = [overlap_data[i_orig][okey] for i_orig in valid_indices]\n",
    "    mean_ov = np.mean(ovs)\n",
    "    mean_delta = np.mean(c['bare'] - c[cname])\n",
    "    ax.scatter([mean_ov], [mean_delta], s=80, c=cond_colors.get(cname, 'gray'),\n",
    "               edgecolors='black', linewidths=1, zorder=5)\n",
    "    ax.annotate(cname.replace('_', '\\n'), (mean_ov, mean_delta), fontsize=5,\n",
    "                ha='center', va='bottom')\n",
    "ax.axhline(y=0, color='red', linestyle='--', alpha=0.5)\n",
    "ax.set_xlabel('Token Jaccard Overlap with Passage')\n",
    "ax.set_ylabel('NLL Benefit vs Bare (positive = better)')\n",
    "ax.set_title(f'Overlap vs Benefit (r={r_all:.3f})')\n",
    "\n",
    "# --- Plot 2: All conditions bar chart (Cohen's d vs bare) ---\n",
    "ax = axes[0, 1]\n",
    "cnames_sorted = sorted(\n",
    "    [cn for cn in CONDITION_NAMES if cn != 'bare'],\n",
    "    key=lambda cn: cohens_d(c['bare'] - c[cn]),\n",
    "    reverse=True\n",
    ")\n",
    "ds = [cohens_d(c['bare'] - c[cn]) for cn in cnames_sorted]\n",
    "colors_bar = [cond_colors.get(cn, 'gray') for cn in cnames_sorted]\n",
    "bars = ax.barh(range(len(cnames_sorted)), ds, color=colors_bar, edgecolor='black', linewidth=0.5)\n",
    "ax.set_yticks(range(len(cnames_sorted)))\n",
    "ax.set_yticklabels(cnames_sorted, fontsize=8)\n",
    "ax.axvline(x=0, color='gray', linestyle='--')\n",
    "ax.set_xlabel(\"Cohen's d vs Bare\")\n",
    "ax.set_title('All Conditions vs Bare (d > 0 = better)')\n",
    "ax.invert_yaxis()\n",
    "\n",
    "# --- Plot 3: Mechanism ablation summary ---\n",
    "ax = axes[0, 2]\n",
    "mech_labels = ['M1:\\nCoherence', 'M2:\\nFormat\\n(Oracle)', 'M3:\\nFormat\\n(LLM)',\n",
    "               'M4:\\nSpecificity', 'M5:\\nOverlap\\nCeiling', 'M6:\\nLLM vs\\nTF-IDF',\n",
    "               'M7:\\nStacking']\n",
    "mech_ds = [comparison_results[k]['cohens_d'] for k in\n",
    "           ['M1: Shuffled vs LLM-kw', 'M2: Oracle-kw vs Oracle', 'M3: LLM-kw vs LLM-question',\n",
    "            'M4: TF-IDF vs Anti-kw', 'M5: Echo vs LLM-kw', 'M6: TF-IDF vs LLM-kw',\n",
    "            'M7: LLM-kw+sep vs best single']]\n",
    "mech_sig = [comparison_results[k]['bonferroni_significant'] for k in\n",
    "            ['M1: Shuffled vs LLM-kw', 'M2: Oracle-kw vs Oracle', 'M3: LLM-kw vs LLM-question',\n",
    "             'M4: TF-IDF vs Anti-kw', 'M5: Echo vs LLM-kw', 'M6: TF-IDF vs LLM-kw',\n",
    "             'M7: LLM-kw+sep vs best single']]\n",
    "mech_colors = ['mediumpurple' if s else 'lightgray' for s in mech_sig]\n",
    "ax.bar(range(len(mech_labels)), mech_ds, color=mech_colors, edgecolor='black', linewidth=0.5)\n",
    "ax.set_xticks(range(len(mech_labels)))\n",
    "ax.set_xticklabels(mech_labels, fontsize=7)\n",
    "ax.axhline(y=0, color='gray', linestyle='--')\n",
    "ax.set_ylabel(\"Cohen's d\")\n",
    "ax.set_title('Mechanism Tests (colored = Bonferroni sig)')\n",
    "\n",
    "# --- Plot 4: Hardness × condition heatmap ---\n",
    "ax = axes[1, 0]\n",
    "hm_conditions = conditions_to_show\n",
    "hm_data = []\n",
    "for cname in hm_conditions:\n",
    "    row = []\n",
    "    for q in range(5):\n",
    "        mask_q = quintiles == q\n",
    "        if np.sum(mask_q) < 10:\n",
    "            row.append(0)\n",
    "        else:\n",
    "            delta = bare_valid[mask_q] - c[cname][mask_q]\n",
    "            row.append(cohens_d(delta))\n",
    "    hm_data.append(row)\n",
    "hm_data = np.array(hm_data)\n",
    "im = ax.imshow(hm_data, cmap='RdBu_r', vmin=-0.5, vmax=0.5, aspect='auto')\n",
    "ax.set_xticks(range(5))\n",
    "ax.set_xticklabels(quintile_labels, fontsize=7)\n",
    "ax.set_yticks(range(len(hm_conditions)))\n",
    "ax.set_yticklabels(hm_conditions, fontsize=7)\n",
    "for i in range(len(hm_conditions)):\n",
    "    for j in range(5):\n",
    "        ax.text(j, i, f\"{hm_data[i,j]:+.2f}\", ha='center', va='center', fontsize=6)\n",
    "plt.colorbar(im, ax=ax, label=\"Cohen's d vs bare\")\n",
    "ax.set_title('Hardness × Condition (d vs bare)')\n",
    "\n",
    "# --- Plot 5: Cross-condition median overlap vs mean d ---\n",
    "ax = axes[1, 1]\n",
    "for i, (cname, okey) in enumerate(cond_to_overlap.items()):\n",
    "    ax.scatter([cond_median_overlap[i]], [cond_mean_d[i]],\n",
    "               s=80, c=cond_colors.get(cname, 'gray'),\n",
    "               edgecolors='black', linewidths=1, zorder=5)\n",
    "    ax.annotate(cname.replace('_', ' '), (cond_median_overlap[i], cond_mean_d[i]),\n",
    "                fontsize=7, ha='left', va='bottom')\n",
    "# Fit line\n",
    "z = np.polyfit(cond_median_overlap, cond_mean_d, 1)\n",
    "x_line = np.linspace(min(cond_median_overlap), max(cond_median_overlap), 50)\n",
    "ax.plot(x_line, np.polyval(z, x_line), 'r--', alpha=0.7)\n",
    "ax.set_xlabel('Median Token Overlap')\n",
    "ax.set_ylabel(\"Mean Cohen's d vs Bare\")\n",
    "ax.set_title(f'Cross-Condition Overlap vs Effect (r={r_cross:.3f})')\n",
    "ax.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "\n",
    "# --- Plot 6: LLM template ranking bar chart ---\n",
    "ax = axes[1, 2]\n",
    "template_conds = [\n",
    "    ('llm_keyword', 'Keyword'), ('llm_question', 'Question'),\n",
    "    ('llm_symptom', 'Symptom'), ('llm_summary', 'Summary'),\n",
    "    ('llm_messy', 'Messy'),\n",
    "]\n",
    "tmpl_ds = [cohens_d(c['bare'] - c[cn]) for cn, _ in template_conds]\n",
    "tmpl_names = [label for _, label in template_conds]\n",
    "tmpl_colors = ['forestgreen', 'limegreen', 'darkgreen', 'olive', 'teal']\n",
    "ax.bar(range(len(tmpl_ds)), tmpl_ds, color=tmpl_colors, edgecolor='black', linewidth=0.5)\n",
    "ax.set_xticks(range(len(tmpl_ds)))\n",
    "ax.set_xticklabels(tmpl_names, fontsize=9)\n",
    "ax.axhline(y=0, color='gray', linestyle='--')\n",
    "ax.set_ylabel(\"Cohen's d vs Bare\")\n",
    "ax.set_title('LLM Template Ranking')\n",
    "\n",
    "plt.suptitle('Exp 06: Surrogate Deep-Dive — Mechanism Decomposition', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'analysis_plots.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"Plots saved to {RESULTS_DIR / 'analysis_plots.png'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ecd6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 15: Save comprehensive results JSON\n",
    "\n",
    "final = {\n",
    "    'experiment': 'exp06_surrogate_deep_dive',\n",
    "    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'config': {\n",
    "        'model_name': config.model_name,\n",
    "        'seed': SEED,\n",
    "        'n_eval': N,\n",
    "        'n_valid': n_valid,\n",
    "        'n_excluded': n_excluded,\n",
    "        'min_passage_words': config.min_passage_words,\n",
    "        'max_passage_words': config.max_passage_words,\n",
    "        'n_conditions': len(CONDITION_NAMES),\n",
    "        'n_comparisons': N_COMPARISONS,\n",
    "        'bonferroni_alpha': BONFERRONI_ALPHA,\n",
    "    },\n",
    "    'condition_names': CONDITION_NAMES,\n",
    "    'nll_summary': {\n",
    "        cname: {\n",
    "            'mean': float(np.mean(c[cname])),\n",
    "            'std': float(np.std(c[cname])),\n",
    "            'cohens_d_vs_bare': float(cohens_d(c['bare'] - c[cname])) if cname != 'bare' else 0.0,\n",
    "        }\n",
    "        for cname in CONDITION_NAMES\n",
    "    },\n",
    "    'primary_comparisons': comparison_results,\n",
    "    'all_vs_bare': all_vs_bare,\n",
    "    'overlap_analysis': {\n",
    "        'universal_r': float(r_all),\n",
    "        'universal_p': float(p_all),\n",
    "        'cross_condition_r': float(r_cross),\n",
    "        'cross_condition_p': float(p_cross),\n",
    "        'regression_betas': [float(b) for b in betas],\n",
    "        'regression_r_squared': float(r_squared),\n",
    "    },\n",
    "    'hardness_breakdown': hardness_breakdown,\n",
    "    'per_sample_results': results,\n",
    "}\n",
    "\n",
    "with open(FINAL_RESULTS_PATH, 'w') as f:\n",
    "    json.dump(final, f, indent=2)\n",
    "\n",
    "print(f\"Results saved to {FINAL_RESULTS_PATH}\")\n",
    "print(f\"File size: {FINAL_RESULTS_PATH.stat().st_size / 1024:.1f} KB\")\n",
    "print(\"\\nDone!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42c4c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 16: GPU cleanup — free all VRAM\n",
    "import gc\n",
    "\n",
    "print(\"Cleaning up GPU memory...\")\n",
    "mem_before = torch.cuda.memory_allocated() / 1e9\n",
    "\n",
    "del model\n",
    "del tokenizer\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "mem_after = torch.cuda.memory_allocated() / 1e9\n",
    "print(f\"GPU memory: {mem_before:.2f} GB -> {mem_after:.2f} GB\")\n",
    "print(\"Cleanup complete.\")"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cu124.m127",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/base-cu124:m127"
  },
  "kernelspec": {
   "display_name": "Python 3 (Local)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
