{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exp 27: Cross-Dataset Generalization with Attention Forcing\n",
    "\n",
    "## Motivation\n",
    "\n",
    "All 26 experiments have been primarily tested on MS MARCO (short web snippets, ~130 tokens).\n",
    "The few cross-dataset tests (Exp 11 on NQ, Exp 19v1 on 6 datasets, Exp 24 Part 2 on SQuAD)\n",
    "used only **basic priming** -- no attention forcing, no values-only isolation on other datasets.\n",
    "The Exp 26 breakthrough (attention forcing recovers d=+0.291 at 1024 tok on padded MARCO)\n",
    "has **never been tested on other datasets or on naturally long documents**.\n",
    "\n",
    "## Core Question\n",
    "\n",
    "Does attention forcing generalize beyond MS MARCO? Does the mechanism (value contamination)\n",
    "work on other QA datasets when we apply our full toolkit?\n",
    "\n",
    "## Datasets\n",
    "\n",
    "| Dataset | Doc Length | QA Type | Prior Result (basic priming) |\n",
    "|---------|------------|---------|------------------------------|\n",
    "| **TriviaQA** | 500-5000 tok | Factoid (closest to MARCO) | Never tested |\n",
    "| **Natural Questions** | 150-6000 tok | Factoid, long Wikipedia | d=-0.019 (Exp 11) |\n",
    "| **HotpotQA** | 800-2000 tok | Multi-hop reasoning | d=-0.35 (Exp 19v1) |\n",
    "\n",
    "## Conditions\n",
    "\n",
    "| # | Condition | Description |\n",
    "|---|-----------|-------------|\n",
    "| 1 | `bare` | BOS + doc standard causal (baseline) |\n",
    "| 2 | `sf_trunc` | Standard priming (bias=0) + truncate + RoPE correct |\n",
    "| 3 | `sf_trunc_bias2` | +2.0 logit bias on doc->prefix attention (optimal from Exp 26) |\n",
    "| 4 | `sf_trunc_bias4` | +4.0 logit bias (may help with longer docs) |\n",
    "| 5 | `values_only` | Bare keys + primed values from sf_trunc cache |\n",
    "\n",
    "## Success Criteria\n",
    "\n",
    "1. Does attention forcing (bias=2.0 or 4.0) show d > 0 on any non-MARCO dataset?\n",
    "2. Does values_only show positive signal universally (confirming value contamination mechanism)?\n",
    "3. Does the length x bias interaction hold: optimal bias increases with document length?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 1: Setup\n",
    "import os\n",
    "os.umask(0o000)\n",
    "\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import gc\n",
    "import csv\n",
    "import numpy as np\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "RESULTS_DIR = Path(\"results/exp27\")\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CHECKPOINT_PATH = RESULTS_DIR / \"checkpoint.json\"\n",
    "FINAL_RESULTS_PATH = RESULTS_DIR / \"results.json\"\n",
    "CSV_PATH = RESULTS_DIR / \"results.csv\"\n",
    "\n",
    "print(f\"SEED: {SEED}\")\n",
    "print(f\"Results directory: {RESULTS_DIR}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 2: Load Mistral 7B via load_model()\n",
    "sys.path.insert(0, \".\")\n",
    "\n",
    "from lib.config import ExperimentConfig\n",
    "from lib.model_utils import load_model\n",
    "\n",
    "MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "exp_config = ExperimentConfig(\n",
    "    model_name=MODEL_NAME,\n",
    "    model_type=\"mistral\",\n",
    "    compute_dtype=\"auto\",\n",
    "    use_4bit=True,\n",
    "    num_samples=2000,\n",
    "    seed=SEED,\n",
    ")\n",
    "\n",
    "print(f\"Loading {MODEL_NAME} (4-bit, float16)...\")\n",
    "model, tokenizer = load_model(exp_config)\n",
    "\n",
    "from lib.kv_cache import _get_text_config, _get_head_dim, _ensure_dynamic_cache, _get_cache_keys\n",
    "\n",
    "text_config = _get_text_config(model.config)\n",
    "N_LAYERS = text_config.num_hidden_layers\n",
    "print(f\"\\nModel loaded successfully.\")\n",
    "print(f\"  Num layers: {N_LAYERS}\")\n",
    "print(f\"  Head dim: {_get_head_dim(model.config)}\")\n",
    "print(f\"  Model dtype: {model.dtype}\")\n",
    "\n",
    "# Verify with test forward pass\n",
    "sample_ids = tokenizer(\"test\", return_tensors=\"pt\")['input_ids'].to(exp_config.device)\n",
    "with torch.no_grad():\n",
    "    out = model(sample_ids, use_cache=True)\n",
    "    cache_check = _ensure_dynamic_cache(out.past_key_values)\n",
    "    k0 = _get_cache_keys(cache_check, 0)\n",
    "    print(f\"  Cache key dtype: {k0.dtype}\")\n",
    "    print(f\"  Cache key shape: {k0.shape}\")\n",
    "del out, sample_ids, cache_check\n",
    "torch.cuda.empty_cache()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 3: Lib imports + constants\n",
    "from lib.kv_cache import (\n",
    "    _get_cache_keys,\n",
    "    _get_cache_values,\n",
    "    _set_cache_keys,\n",
    "    _set_cache_values,\n",
    "    _ensure_dynamic_cache,\n",
    "    extract_and_truncate_cache_with_bos,\n",
    "    correct_rope_positions_with_bos,\n",
    "    score_answer_with_cache,\n",
    "    deepcopy_cache,\n",
    ")\n",
    "from lib.analysis import cohens_d\n",
    "from lib.data import count_words\n",
    "from scipy import stats\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Templates -- bare text, no \"Document:\\n\" framing\n",
    "SURROGATE_PREFIX_TEMPLATE = \"{surrogate}\\n\"\n",
    "DOCUMENT_TEMPLATE = \"{document}\"\n",
    "QUERY_TEMPLATE = \"\\nQuestion: {question}\\nAnswer:\"\n",
    "ANSWER_TEMPLATE = \" {answer}\"\n",
    "\n",
    "# Prefix text\n",
    "from lib.surrogate import STATIC_SURROGATE_QUERIES\n",
    "STATIC_FACT = STATIC_SURROGATE_QUERIES['static_factual']['query']\n",
    "\n",
    "# Experiment parameters\n",
    "N_PER_DATASET = 300\n",
    "MAX_DOC_TOKENS = 4096\n",
    "CHECKPOINT_EVERY = 25\n",
    "\n",
    "# Conditions\n",
    "CONDITION_NAMES = ['bare', 'sf_trunc', 'sf_trunc_bias2', 'sf_trunc_bias4', 'values_only']\n",
    "BIAS_MAP = {\n",
    "    'sf_trunc': 0.0,\n",
    "    'sf_trunc_bias2': 2.0,\n",
    "    'sf_trunc_bias4': 4.0,\n",
    "}\n",
    "\n",
    "# Length bins for stratified analysis (token count)\n",
    "LENGTH_BINS = [\n",
    "    ('<256', 0, 256),\n",
    "    ('256-512', 256, 512),\n",
    "    ('512-1024', 512, 1024),\n",
    "    ('1024-2048', 1024, 2048),\n",
    "    ('>2048', 2048, 999999),\n",
    "]\n",
    "\n",
    "print(\"Config ready\")\n",
    "print(f\"  Model: {MODEL_NAME}\")\n",
    "print(f\"  N per dataset: {N_PER_DATASET}\")\n",
    "print(f\"  MAX_DOC_TOKENS: {MAX_DOC_TOKENS}\")\n",
    "print(f\"  Conditions: {CONDITION_NAMES}\")\n",
    "print(f\"  Static fact prefix: '{STATIC_FACT}'\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 4: Load TriviaQA dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"LOADING TRIVIAQA (rc.wikipedia, validation)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "TQA_CACHE = RESULTS_DIR / \"tqa_samples.json\"\n",
    "\n",
    "if TQA_CACHE.exists():\n",
    "    with open(TQA_CACHE, 'r') as f:\n",
    "        tqa_samples = json.load(f)\n",
    "    print(f\"Loaded {len(tqa_samples)} cached TriviaQA samples\")\n",
    "else:\n",
    "    tqa_ds = load_dataset(\"trivia_qa\", \"rc.wikipedia\", split=\"validation\",\n",
    "                           trust_remote_code=True)\n",
    "    print(f\"TriviaQA validation size: {len(tqa_ds)}\")\n",
    "\n",
    "    tqa_samples = []\n",
    "    np.random.seed(SEED)\n",
    "\n",
    "    for item in tqdm(tqa_ds, desc=\"Filtering TriviaQA\"):\n",
    "        # Extract document from entity_pages\n",
    "        entity_pages = item.get('entity_pages', {})\n",
    "        wiki_contexts = entity_pages.get('wiki_context', [])\n",
    "        if not wiki_contexts:\n",
    "            continue\n",
    "        doc_text = wiki_contexts[0]  # first Wikipedia evidence article\n",
    "\n",
    "        # Get question and answer\n",
    "        question = item.get('question', '')\n",
    "        answer_data = item.get('answer', {})\n",
    "        answer_text = answer_data.get('value', '') if isinstance(answer_data, dict) else str(answer_data)\n",
    "\n",
    "        if not question or not answer_text or not doc_text:\n",
    "            continue\n",
    "\n",
    "        # Filter: answer must appear in document, doc 200-5000 words\n",
    "        wc = count_words(doc_text)\n",
    "        if wc < 200 or wc > 5000:\n",
    "            continue\n",
    "        if answer_text.lower() not in doc_text.lower():\n",
    "            continue\n",
    "\n",
    "        tqa_samples.append({\n",
    "            'passage': doc_text,\n",
    "            'query': question,\n",
    "            'answer': answer_text,\n",
    "            'word_count': wc,\n",
    "            'dataset': 'triviaqa',\n",
    "        })\n",
    "\n",
    "        if len(tqa_samples) >= N_PER_DATASET * 3:\n",
    "            break\n",
    "\n",
    "    np.random.shuffle(tqa_samples)\n",
    "    tqa_samples = tqa_samples[:N_PER_DATASET]\n",
    "\n",
    "    with open(TQA_CACHE, 'w') as f:\n",
    "        json.dump(tqa_samples, f)\n",
    "    print(f\"Cached {len(tqa_samples)} samples\")\n",
    "\n",
    "    del tqa_ds\n",
    "    gc.collect()\n",
    "\n",
    "print(f\"TriviaQA samples: {len(tqa_samples)}\")\n",
    "wcs = [s['word_count'] for s in tqa_samples]\n",
    "print(f\"  Word counts: mean={np.mean(wcs):.0f}, min={min(wcs)}, max={max(wcs)}\")\n",
    "if tqa_samples:\n",
    "    print(f\"  Example Q: {tqa_samples[0]['query']}\")\n",
    "    print(f\"  Example A: {tqa_samples[0]['answer']}\")\n",
    "    print(f\"  Doc preview: {tqa_samples[0]['passage'][:150]}...\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 5: Load Natural Questions dataset (streaming)\n",
    "print(\"=\" * 70)\n",
    "print(\"LOADING NATURAL QUESTIONS (validation, streaming)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "NQ_CACHE = RESULTS_DIR / \"nq_samples.json\"\n",
    "\n",
    "if NQ_CACHE.exists():\n",
    "    with open(NQ_CACHE, 'r') as f:\n",
    "        nq_samples = json.load(f)\n",
    "    print(f\"Loaded {len(nq_samples)} cached NQ samples\")\n",
    "else:\n",
    "    nq_ds = load_dataset(\n",
    "        \"google-research-datasets/natural_questions\",\n",
    "        split=\"validation\",\n",
    "        streaming=True,\n",
    "    )\n",
    "\n",
    "    nq_samples = []\n",
    "    n_processed = 0\n",
    "\n",
    "    for example in tqdm(nq_ds, desc=\"Processing NQ\"):\n",
    "        n_processed += 1\n",
    "\n",
    "        # Extract clean document text (non-HTML tokens)\n",
    "        doc_tokens = example['document']['tokens']\n",
    "        if isinstance(doc_tokens, dict):\n",
    "            token_strs = doc_tokens['token']\n",
    "            is_html_flags = doc_tokens['is_html']\n",
    "            clean_tokens = [t for t, h in zip(token_strs, is_html_flags) if not h]\n",
    "        else:\n",
    "            clean_tokens = [t['token'] for t in doc_tokens if not t['is_html']]\n",
    "\n",
    "        doc_text = ' '.join(clean_tokens)\n",
    "        wc = count_words(doc_text)\n",
    "\n",
    "        if wc < 100 or wc > 4000:\n",
    "            continue\n",
    "\n",
    "        # Extract short answer\n",
    "        annotations = example['annotations']\n",
    "        short_answers_list = annotations['short_answers']\n",
    "\n",
    "        answer_text = None\n",
    "        for annotator_sa in short_answers_list:\n",
    "            if not annotator_sa:\n",
    "                continue\n",
    "            texts = annotator_sa.get('text', [])\n",
    "            if texts:\n",
    "                answer_text = texts[0]\n",
    "                break\n",
    "            starts = annotator_sa.get('start_token', [])\n",
    "            ends = annotator_sa.get('end_token', [])\n",
    "            if not starts or not ends:\n",
    "                continue\n",
    "            start_tok = starts[0] if isinstance(starts, list) else starts\n",
    "            end_tok = ends[0] if isinstance(ends, list) else ends\n",
    "            if start_tok >= 0 and end_tok > start_tok:\n",
    "                if isinstance(doc_tokens, dict):\n",
    "                    ans_tokens = [\n",
    "                        doc_tokens['token'][i]\n",
    "                        for i in range(start_tok, min(end_tok, len(doc_tokens['token'])))\n",
    "                        if not doc_tokens['is_html'][i]\n",
    "                    ]\n",
    "                else:\n",
    "                    ans_tokens = [\n",
    "                        doc_tokens[i]['token']\n",
    "                        for i in range(start_tok, min(end_tok, len(doc_tokens)))\n",
    "                        if not doc_tokens[i]['is_html']\n",
    "                    ]\n",
    "                if ans_tokens:\n",
    "                    answer_text = ' '.join(ans_tokens)\n",
    "                    break\n",
    "\n",
    "        if not answer_text or len(answer_text.strip()) == 0:\n",
    "            continue\n",
    "        if len(answer_text.split()) > 20:\n",
    "            continue\n",
    "\n",
    "        # Extract query\n",
    "        question = example['question']\n",
    "        if isinstance(question, dict):\n",
    "            query = question.get('text', '')\n",
    "        else:\n",
    "            query = str(question)\n",
    "        if not query.strip():\n",
    "            continue\n",
    "\n",
    "        nq_samples.append({\n",
    "            'passage': doc_text,\n",
    "            'query': query,\n",
    "            'answer': answer_text,\n",
    "            'word_count': wc,\n",
    "            'dataset': 'nq',\n",
    "        })\n",
    "\n",
    "        if len(nq_samples) >= N_PER_DATASET * 3:\n",
    "            break\n",
    "\n",
    "    np.random.seed(SEED)\n",
    "    np.random.shuffle(nq_samples)\n",
    "    nq_samples = nq_samples[:N_PER_DATASET]\n",
    "\n",
    "    with open(NQ_CACHE, 'w') as f:\n",
    "        json.dump(nq_samples, f)\n",
    "    print(f\"Cached {len(nq_samples)} samples (processed {n_processed})\")\n",
    "\n",
    "print(f\"NQ samples: {len(nq_samples)}\")\n",
    "wcs = [s['word_count'] for s in nq_samples]\n",
    "print(f\"  Word counts: mean={np.mean(wcs):.0f}, min={min(wcs)}, max={max(wcs)}\")\n",
    "if nq_samples:\n",
    "    print(f\"  Example Q: {nq_samples[0]['query']}\")\n",
    "    print(f\"  Example A: {nq_samples[0]['answer']}\")\n",
    "    print(f\"  Doc preview: {nq_samples[0]['passage'][:150]}...\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 6: Load HotpotQA dataset\n",
    "print(\"=\" * 70)\n",
    "print(\"LOADING HOTPOTQA (distractor, validation)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "HQA_CACHE = RESULTS_DIR / \"hqa_samples.json\"\n",
    "\n",
    "if HQA_CACHE.exists():\n",
    "    with open(HQA_CACHE, 'r') as f:\n",
    "        hqa_samples = json.load(f)\n",
    "    print(f\"Loaded {len(hqa_samples)} cached HotpotQA samples\")\n",
    "else:\n",
    "    hqa_ds = load_dataset(\"hotpot_qa\", \"distractor\", split=\"validation\",\n",
    "                           trust_remote_code=True)\n",
    "    print(f\"HotpotQA validation size: {len(hqa_ds)}\")\n",
    "\n",
    "    hqa_samples = []\n",
    "    np.random.seed(SEED)\n",
    "\n",
    "    for item in tqdm(hqa_ds, desc=\"Filtering HotpotQA\"):\n",
    "        question = item.get('question', '')\n",
    "        answer_text = item.get('answer', '')\n",
    "\n",
    "        if not question or not answer_text:\n",
    "            continue\n",
    "\n",
    "        # Concatenate context paragraphs (including distractors)\n",
    "        context = item.get('context', {})\n",
    "        titles = context.get('title', [])\n",
    "        sentences_list = context.get('sentences', [])\n",
    "\n",
    "        paragraphs = []\n",
    "        for title, sents in zip(titles, sentences_list):\n",
    "            para_text = ''.join(sents)\n",
    "            paragraphs.append(para_text)\n",
    "\n",
    "        doc_text = '\\n\\n'.join(paragraphs)\n",
    "        wc = count_words(doc_text)\n",
    "\n",
    "        if wc < 200 or wc > 3000:\n",
    "            continue\n",
    "\n",
    "        hqa_samples.append({\n",
    "            'passage': doc_text,\n",
    "            'query': question,\n",
    "            'answer': answer_text,\n",
    "            'word_count': wc,\n",
    "            'dataset': 'hotpotqa',\n",
    "        })\n",
    "\n",
    "        if len(hqa_samples) >= N_PER_DATASET * 3:\n",
    "            break\n",
    "\n",
    "    np.random.shuffle(hqa_samples)\n",
    "    hqa_samples = hqa_samples[:N_PER_DATASET]\n",
    "\n",
    "    with open(HQA_CACHE, 'w') as f:\n",
    "        json.dump(hqa_samples, f)\n",
    "    print(f\"Cached {len(hqa_samples)} samples\")\n",
    "\n",
    "    del hqa_ds\n",
    "    gc.collect()\n",
    "\n",
    "print(f\"HotpotQA samples: {len(hqa_samples)}\")\n",
    "wcs = [s['word_count'] for s in hqa_samples]\n",
    "print(f\"  Word counts: mean={np.mean(wcs):.0f}, min={min(wcs)}, max={max(wcs)}\")\n",
    "if hqa_samples:\n",
    "    print(f\"  Example Q: {hqa_samples[0]['query']}\")\n",
    "    print(f\"  Example A: {hqa_samples[0]['answer']}\")\n",
    "    print(f\"  Doc preview: {hqa_samples[0]['passage'][:150]}...\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 7: Unified sample pool + tokenize prefix + condition explanation\n",
    "print(\"=\" * 70)\n",
    "print(\"UNIFIED SAMPLE POOL\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "all_samples = []\n",
    "for ds_name, ds_samples in [(\"triviaqa\", tqa_samples), (\"nq\", nq_samples), (\"hotpotqa\", hqa_samples)]:\n",
    "    for sample in ds_samples:\n",
    "        sample['dataset'] = ds_name\n",
    "    all_samples.extend(ds_samples)\n",
    "\n",
    "print(f\"Total samples: {len(all_samples)}\")\n",
    "for ds_name in ['triviaqa', 'nq', 'hotpotqa']:\n",
    "    ds_s = [s for s in all_samples if s['dataset'] == ds_name]\n",
    "    wcs = [s['word_count'] for s in ds_s]\n",
    "    print(f\"  {ds_name}: n={len(ds_s)}, mean_words={np.mean(wcs):.0f}, \"\n",
    "          f\"range=[{min(wcs)}, {max(wcs)}]\")\n",
    "\n",
    "# Tokenize prefix\n",
    "sf_str = SURROGATE_PREFIX_TEMPLATE.format(surrogate=STATIC_FACT)\n",
    "sf_ids = tokenizer(sf_str, return_tensors=\"pt\",\n",
    "                    add_special_tokens=False)['input_ids'].to(exp_config.device)\n",
    "PREFIX_TOKEN_LEN = sf_ids.shape[1]\n",
    "\n",
    "print(f\"\\nPrefix: '{STATIC_FACT}'\")\n",
    "print(f\"  Token length (no BOS): {PREFIX_TOKEN_LEN}\")\n",
    "\n",
    "# Tokenize doc lengths for all samples\n",
    "print(f\"\\nTokenizing documents to measure token lengths...\")\n",
    "for sample in tqdm(all_samples, desc=\"Tokenizing\"):\n",
    "    tok_len = len(tokenizer.encode(sample['passage'], add_special_tokens=False))\n",
    "    sample['doc_token_len'] = min(tok_len, MAX_DOC_TOKENS)\n",
    "    sample['answer_token_len'] = len(tokenizer.encode(sample['answer'], add_special_tokens=False))\n",
    "\n",
    "# Token length summary\n",
    "for ds_name in ['triviaqa', 'nq', 'hotpotqa']:\n",
    "    ds_s = [s for s in all_samples if s['dataset'] == ds_name]\n",
    "    tls = [s['doc_token_len'] for s in ds_s]\n",
    "    print(f\"  {ds_name} tokens: mean={np.mean(tls):.0f}, median={np.median(tls):.0f}, \"\n",
    "          f\"range=[{min(tls)}, {max(tls)}]\")\n",
    "\n",
    "# Condition explanation\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"EXPERIMENTAL CONDITIONS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\n### 1. bare ###\")\n",
    "print(\"  Forward: [BOS][doc]\")\n",
    "print(\"  Standard causal attention, no prefix. Baseline for all comparisons.\")\n",
    "\n",
    "print(\"\\n### 2. sf_trunc (bias=0.0) ###\")\n",
    "print(f\"  Forward: [BOS][prefix_{PREFIX_TOKEN_LEN}][doc]\")\n",
    "print(\"  Standard priming: standard causal mask, then truncate prefix + RoPE correct.\")\n",
    "print(\"  This is the 'classic' priming that works on short MARCO but fails on long docs.\")\n",
    "\n",
    "print(\"\\n### 3. sf_trunc_bias2 (bias=+2.0) ###\")\n",
    "print(f\"  Forward: [BOS][prefix_{PREFIX_TOKEN_LEN}][doc] with +2.0 logit bias\")\n",
    "print(\"  Every doc token gets +2.0 added to pre-softmax attention for prefix positions.\")\n",
    "print(\"  Optimal bias from Exp 26 (d=+0.291 at 1024 tok on padded MARCO).\")\n",
    "\n",
    "print(\"\\n### 4. sf_trunc_bias4 (bias=+4.0) ###\")\n",
    "print(f\"  Forward: [BOS][prefix_{PREFIX_TOKEN_LEN}][doc] with +4.0 logit bias\")\n",
    "print(\"  Stronger forcing. May be needed for docs longer than 1024 tokens.\")\n",
    "\n",
    "print(\"\\n### 5. values_only ###\")\n",
    "print(\"  Bare cache keys + primed cache values (from sf_trunc, bias=0).\")\n",
    "print(\"  Tests pure value contamination without key interference.\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 8: Helper functions\n",
    "\n",
    "def build_biased_causal_mask(total_len, prefix_start, prefix_end, bias_value, dtype, device):\n",
    "    \"\"\"Build a 4D causal attention mask with logit bias on doc->prefix attention.\n",
    "\n",
    "    Args:\n",
    "        total_len: Total sequence length [BOS + prefix + doc]\n",
    "        prefix_start: Start index of prefix tokens (typically 1, after BOS)\n",
    "        prefix_end: End index of prefix tokens (exclusive)\n",
    "        bias_value: Positive float to add to doc->prefix attention scores.\n",
    "        dtype: Model dtype\n",
    "        device: Model device\n",
    "\n",
    "    Returns:\n",
    "        Tensor of shape (1, 1, total_len, total_len)\n",
    "    \"\"\"\n",
    "    mask = torch.zeros((total_len, total_len), dtype=dtype, device=device)\n",
    "    causal = torch.triu(\n",
    "        torch.ones(total_len, total_len, dtype=torch.bool, device=device),\n",
    "        diagonal=1\n",
    "    )\n",
    "    mask.masked_fill_(causal, float('-inf'))\n",
    "\n",
    "    if bias_value != 0.0:\n",
    "        doc_start = prefix_end\n",
    "        mask[doc_start:, prefix_start:prefix_end] += bias_value\n",
    "\n",
    "    return mask.unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "\n",
    "def run_single_sample(sample, model, tokenizer, exp_config, sf_ids, sf_str, PREFIX_TOKEN_LEN, N_LAYERS):\n",
    "    \"\"\"Run all 5 conditions for a single sample. Returns dict of NLLs + metadata.\"\"\"\n",
    "    passage = sample['passage']\n",
    "    query = sample['query']\n",
    "    answer = sample['answer']\n",
    "    ds_name = sample['dataset']\n",
    "\n",
    "    query_prompt = QUERY_TEMPLATE.format(question=query)\n",
    "    answer_text = ANSWER_TEMPLATE.format(answer=answer)\n",
    "    document_text = DOCUMENT_TEMPLATE.format(document=passage)\n",
    "\n",
    "    # === Matched tokenization ===\n",
    "    full_text = sf_str + document_text\n",
    "    full_enc = tokenizer(full_text, return_tensors=\"pt\",\n",
    "                          add_special_tokens=True, padding=False, truncation=False)\n",
    "    full_ids = full_enc['input_ids'].to(exp_config.device)\n",
    "\n",
    "    sf_prefix_enc = tokenizer(sf_str, return_tensors=\"pt\",\n",
    "                               add_special_tokens=True, padding=False, truncation=False)\n",
    "    sf_prefix_len_with_bos = sf_prefix_enc['input_ids'].shape[1]\n",
    "\n",
    "    bos_id = full_ids[:, :1]\n",
    "    doc_ids = full_ids[:, sf_prefix_len_with_bos:]\n",
    "\n",
    "    # Truncate long docs\n",
    "    if doc_ids.shape[1] > MAX_DOC_TOKENS:\n",
    "        doc_ids = doc_ids[:, :MAX_DOC_TOKENS]\n",
    "\n",
    "    doc_len = doc_ids.shape[1]\n",
    "    context_len = 1 + doc_len  # BOS + doc\n",
    "\n",
    "    del full_enc, full_ids, sf_prefix_enc\n",
    "\n",
    "    # === 1. BARE ===\n",
    "    bare_input = torch.cat([bos_id, doc_ids], dim=1)\n",
    "    with torch.no_grad():\n",
    "        bare_out = model(input_ids=bare_input,\n",
    "                         attention_mask=torch.ones_like(bare_input),\n",
    "                         use_cache=True, return_dict=True)\n",
    "    bare_cache = _ensure_dynamic_cache(bare_out.past_key_values)\n",
    "    del bare_out\n",
    "\n",
    "    bare_nll = score_answer_with_cache(\n",
    "        deepcopy_cache(bare_cache), context_len,\n",
    "        query_prompt, answer_text, model, tokenizer, exp_config)\n",
    "\n",
    "    # === 2-4. PRIMED with bias=0, 2.0, 4.0 ===\n",
    "    primed_input = torch.cat([bos_id, sf_ids, doc_ids], dim=1)\n",
    "    total_seq_len = primed_input.shape[1]\n",
    "    prefix_start = 1\n",
    "    prefix_end = 1 + sf_ids.shape[1]\n",
    "    prefix_offset = sf_ids.shape[1]\n",
    "\n",
    "    primed_nlls = {}\n",
    "    sf_trunc_cache_for_values = None  # save bias=0 cache for values_only\n",
    "\n",
    "    for cond_name, bias_value in BIAS_MAP.items():\n",
    "        mask_4d = build_biased_causal_mask(\n",
    "            total_seq_len, prefix_start, prefix_end,\n",
    "            bias_value, model.dtype, exp_config.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            primed_out = model(input_ids=primed_input,\n",
    "                               attention_mask=mask_4d,\n",
    "                               use_cache=True, return_dict=True)\n",
    "        primed_full = _ensure_dynamic_cache(primed_out.past_key_values)\n",
    "        del primed_out, mask_4d\n",
    "\n",
    "        trunc_raw = extract_and_truncate_cache_with_bos(primed_full, doc_len)\n",
    "        del primed_full\n",
    "\n",
    "        sf_trunc_cache = deepcopy_cache(trunc_raw)\n",
    "        correct_rope_positions_with_bos(sf_trunc_cache, prefix_offset, model)\n",
    "        del trunc_raw\n",
    "\n",
    "        nll = score_answer_with_cache(\n",
    "            deepcopy_cache(sf_trunc_cache), context_len,\n",
    "            query_prompt, answer_text, model, tokenizer, exp_config)\n",
    "        primed_nlls[cond_name] = nll\n",
    "\n",
    "        if cond_name == 'sf_trunc':\n",
    "            sf_trunc_cache_for_values = sf_trunc_cache\n",
    "        else:\n",
    "            del sf_trunc_cache\n",
    "\n",
    "    # === 5. VALUES_ONLY ===\n",
    "    # Bare keys + primed values from sf_trunc (bias=0) cache\n",
    "    values_cache = deepcopy_cache(bare_cache)\n",
    "    for layer_idx in range(N_LAYERS):\n",
    "        primed_vals = _get_cache_values(sf_trunc_cache_for_values, layer_idx)\n",
    "        _set_cache_values(values_cache, layer_idx, primed_vals.clone())\n",
    "\n",
    "    values_nll = score_answer_with_cache(\n",
    "        deepcopy_cache(values_cache), context_len,\n",
    "        query_prompt, answer_text, model, tokenizer, exp_config)\n",
    "\n",
    "    del bare_cache, values_cache, sf_trunc_cache_for_values\n",
    "    del bare_input, primed_input\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return {\n",
    "        'dataset': ds_name,\n",
    "        'query': query,\n",
    "        'answer': answer,\n",
    "        'word_count': sample['word_count'],\n",
    "        'doc_token_len': doc_len,\n",
    "        'answer_token_len': sample.get('answer_token_len', 0),\n",
    "        'bare': bare_nll,\n",
    "        'sf_trunc': primed_nlls['sf_trunc'],\n",
    "        'sf_trunc_bias2': primed_nlls['sf_trunc_bias2'],\n",
    "        'sf_trunc_bias4': primed_nlls['sf_trunc_bias4'],\n",
    "        'values_only': values_nll,\n",
    "    }\n",
    "\n",
    "\n",
    "# Verify mask for a toy example\n",
    "print(\"Mask verification (toy: BOS + 3 prefix + 5 doc = 9 total):\")\n",
    "toy_mask = build_biased_causal_mask(9, 1, 4, 2.0, model.dtype, 'cpu')\n",
    "m = toy_mask.squeeze()\n",
    "print(f\"  Shape: {toy_mask.shape}\")\n",
    "print(f\"  Doc->Prefix bias (row 4, col 1): {m[4, 1].item():.1f} (expect +2.0)\")\n",
    "print(f\"  Doc->Doc (row 5, col 4): {m[5, 4].item():.1f} (expect 0.0)\")\n",
    "print(f\"  Causal mask (row 3, col 5): {m[3, 5].item()} (expect -inf)\")\n",
    "del toy_mask, m\n",
    "print(\"OK\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 9: Main experiment loop\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"EXPERIMENT: {len(all_samples)} samples, {len(CONDITION_NAMES)} conditions\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Checkpoint resume\n",
    "all_results = []\n",
    "start_idx = 0\n",
    "\n",
    "if CHECKPOINT_PATH.exists():\n",
    "    with open(CHECKPOINT_PATH, 'r') as f:\n",
    "        ckpt = json.load(f)\n",
    "    ckpt_queries = ckpt.get('sample_queries', [])\n",
    "    current_queries = [s['query'] for s in all_samples]\n",
    "    if ckpt_queries == current_queries:\n",
    "        all_results = ckpt['results']\n",
    "        start_idx = len(all_results)\n",
    "        print(f\"Resuming from checkpoint: {start_idx}/{len(all_samples)}\")\n",
    "    else:\n",
    "        print(\"Checkpoint query mismatch. Starting fresh.\")\n",
    "else:\n",
    "    print(\"No checkpoint found. Starting fresh.\")\n",
    "\n",
    "t_start = time.time()\n",
    "N_TOTAL = len(all_samples)\n",
    "\n",
    "for qidx in tqdm(range(start_idx, N_TOTAL), initial=start_idx, total=N_TOTAL,\n",
    "                  desc=\"Exp 27\"):\n",
    "    sample = all_samples[qidx]\n",
    "\n",
    "    result = run_single_sample(\n",
    "        sample, model, tokenizer, exp_config,\n",
    "        sf_ids, sf_str, PREFIX_TOKEN_LEN, N_LAYERS)\n",
    "    result['query_idx'] = qidx\n",
    "    all_results.append(result)\n",
    "\n",
    "    # Checkpoint\n",
    "    if (qidx + 1) % CHECKPOINT_EVERY == 0 or qidx == N_TOTAL - 1:\n",
    "        ckpt_data = {\n",
    "            'results': all_results,\n",
    "            'sample_queries': [s['query'] for s in all_samples],\n",
    "            'completed': len(all_results),\n",
    "            'total': N_TOTAL,\n",
    "            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        }\n",
    "        with open(CHECKPOINT_PATH, 'w') as f:\n",
    "            json.dump(ckpt_data, f)\n",
    "        elapsed = time.time() - t_start\n",
    "        n_done = qidx - start_idx + 1\n",
    "        rate = n_done / elapsed if elapsed > 0 else 0\n",
    "        remaining = (N_TOTAL - qidx - 1) / rate if rate > 0 else 0\n",
    "        tqdm.write(f\"  Checkpoint {qidx+1}/{N_TOTAL} | {n_done} done in {elapsed/60:.1f}m | \"\n",
    "                   f\"ETA: {remaining/60:.1f} min\")\n",
    "\n",
    "elapsed_total = time.time() - t_start\n",
    "print(f\"\\nExperiment complete: {len(all_results)} samples in {elapsed_total/60:.1f} min\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 10: Per-dataset analysis \u2014 Cohen's d, win rates, p-values\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ANALYSIS: PER-DATASET RESULTS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "dataset_names = ['triviaqa', 'nq', 'hotpotqa']\n",
    "analysis = {}\n",
    "\n",
    "for ds_name in dataset_names:\n",
    "    ds_results = [r for r in all_results if r['dataset'] == ds_name]\n",
    "    n_ds = len(ds_results)\n",
    "    if n_ds == 0:\n",
    "        print(f\"\\n{ds_name}: NO RESULTS\")\n",
    "        continue\n",
    "\n",
    "    bare_arr = np.array([r['bare'] for r in ds_results])\n",
    "\n",
    "    # Filter invalid\n",
    "    valid = np.isfinite(bare_arr) & (bare_arr != 0)\n",
    "    for cname in CONDITION_NAMES:\n",
    "        if cname == 'bare':\n",
    "            continue\n",
    "        c_arr = np.array([r[cname] for r in ds_results])\n",
    "        valid &= np.isfinite(c_arr) & (c_arr != 0)\n",
    "\n",
    "    n_valid = int(np.sum(valid))\n",
    "\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"DATASET: {ds_name.upper()} (n={n_valid}/{n_ds})\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "    print(f\"\\n{'Condition':<20} {'Mean Bare':>10} {'Mean Cond':>10} \"\n",
    "          f\"{'Mean D':>10} {'d':>8} {'Win%':>7} {'p':>12} {'sig':>5}\")\n",
    "    print(\"-\" * 90)\n",
    "\n",
    "    ds_analysis = {}\n",
    "    for cname in CONDITION_NAMES:\n",
    "        if cname == 'bare':\n",
    "            continue\n",
    "        c_arr = np.array([r[cname] for r in ds_results])\n",
    "        delta = bare_arr[valid] - c_arr[valid]\n",
    "        d = cohens_d(delta)\n",
    "        win = np.mean(delta > 0) * 100\n",
    "        t_stat, p_val = stats.ttest_1samp(delta, 0)\n",
    "        sig = '***' if p_val < 0.001 else '**' if p_val < 0.01 else '*' if p_val < 0.05 else 'ns'\n",
    "        print(f\"{cname:<20} {np.mean(bare_arr[valid]):>10.4f} {np.mean(c_arr[valid]):>10.4f} \"\n",
    "              f\"{np.mean(delta):>+10.4f} {d:>+8.3f} {win:>6.1f}% {p_val:>12.2e} {sig:>5}\")\n",
    "        ds_analysis[cname] = {\n",
    "            'n_valid': n_valid,\n",
    "            'mean_bare': float(np.mean(bare_arr[valid])),\n",
    "            'mean_cond': float(np.mean(c_arr[valid])),\n",
    "            'mean_delta': float(np.mean(delta)),\n",
    "            'cohens_d': float(d),\n",
    "            'win_pct': float(win),\n",
    "            't_stat': float(t_stat),\n",
    "            'p_value': float(p_val),\n",
    "        }\n",
    "\n",
    "    analysis[ds_name] = ds_analysis\n",
    "\n",
    "# Cross-dataset summary\n",
    "print(f\"\\n\\n{'='*70}\")\n",
    "print(\"CROSS-DATASET SUMMARY (Cohen's d vs bare)\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"\\n{'Condition':<20}\", end='')\n",
    "for ds in dataset_names:\n",
    "    print(f\"{'  ' + ds:>14}\", end='')\n",
    "print()\n",
    "print(\"-\" * 62)\n",
    "for cname in CONDITION_NAMES:\n",
    "    if cname == 'bare':\n",
    "        continue\n",
    "    print(f\"{cname:<20}\", end='')\n",
    "    for ds in dataset_names:\n",
    "        if ds in analysis and cname in analysis[ds]:\n",
    "            d = analysis[ds][cname]['cohens_d']\n",
    "            p = analysis[ds][cname]['p_value']\n",
    "            sig = '***' if p < 0.001 else '**' if p < 0.01 else '*' if p < 0.05 else ''\n",
    "            print(f\"{d:>+10.3f}{sig:>4}\", end='')\n",
    "        else:\n",
    "            print(f\"{'n/a':>14}\", end='')\n",
    "    print()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 11: Length stratification + hardness + answer length interaction\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"LENGTH STRATIFICATION ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "length_strat = {}\n",
    "\n",
    "for ds_name in dataset_names:\n",
    "    ds_results = [r for r in all_results if r['dataset'] == ds_name]\n",
    "    if not ds_results:\n",
    "        continue\n",
    "\n",
    "    print(f\"\\n--- {ds_name.upper()} ---\")\n",
    "    ds_length_data = {}\n",
    "\n",
    "    for cname in ['sf_trunc', 'sf_trunc_bias2', 'sf_trunc_bias4', 'values_only']:\n",
    "        print(f\"  {cname}:\")\n",
    "        bin_ds = []\n",
    "        for bin_label, bin_min, bin_max in LENGTH_BINS:\n",
    "            bin_results = [r for r in ds_results\n",
    "                          if bin_min <= r['doc_token_len'] < bin_max]\n",
    "            n_bin = len(bin_results)\n",
    "            if n_bin < 10:\n",
    "                print(f\"    {bin_label}: n={n_bin} (too few)\")\n",
    "                bin_ds.append({'label': bin_label, 'n': n_bin, 'd': None})\n",
    "                continue\n",
    "            bare = np.array([r['bare'] for r in bin_results])\n",
    "            cond = np.array([r[cname] for r in bin_results])\n",
    "            delta = bare - cond\n",
    "            d = cohens_d(delta)\n",
    "            _, p_val = stats.ttest_1samp(delta, 0)\n",
    "            sig = '***' if p_val < 0.001 else '**' if p_val < 0.01 else '*' if p_val < 0.05 else 'ns'\n",
    "            print(f\"    {bin_label}: n={n_bin}, d={d:+.3f}, p={p_val:.2e} {sig}\")\n",
    "            bin_ds.append({'label': bin_label, 'n': n_bin, 'd': float(d), 'p': float(p_val)})\n",
    "        ds_length_data[cname] = bin_ds\n",
    "\n",
    "    length_strat[ds_name] = ds_length_data\n",
    "\n",
    "# === HARDNESS QUINTILE INTERACTION ===\n",
    "print(f\"\\n\\n{'='*70}\")\n",
    "print(\"HARDNESS QUINTILE INTERACTION\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "hardness_data = {}\n",
    "\n",
    "for ds_name in dataset_names:\n",
    "    ds_results = [r for r in all_results if r['dataset'] == ds_name]\n",
    "    if len(ds_results) < 50:\n",
    "        continue\n",
    "\n",
    "    bare_arr = np.array([r['bare'] for r in ds_results])\n",
    "    quintile_boundaries = np.percentile(bare_arr, [20, 40, 60, 80])\n",
    "    quintile_labels = ['Q1(easy)', 'Q2', 'Q3', 'Q4', 'Q5(hard)']\n",
    "\n",
    "    def get_quintile(nll):\n",
    "        for i, b in enumerate(quintile_boundaries):\n",
    "            if nll <= b:\n",
    "                return i\n",
    "        return 4\n",
    "\n",
    "    quintiles = np.array([get_quintile(r['bare']) for r in ds_results])\n",
    "\n",
    "    print(f\"\\n--- {ds_name.upper()} ---\")\n",
    "    ds_hardness = {}\n",
    "\n",
    "    for cname in ['sf_trunc', 'sf_trunc_bias2', 'values_only']:\n",
    "        cond_arr = np.array([r[cname] for r in ds_results])\n",
    "        delta = bare_arr - cond_arr\n",
    "        row = f\"  {cname:<20}\"\n",
    "        q_ds = []\n",
    "        for q in range(5):\n",
    "            mask_q = quintiles == q\n",
    "            n_q = int(np.sum(mask_q))\n",
    "            if n_q < 5:\n",
    "                row += f\"{'n/a':>12}\"\n",
    "                q_ds.append(None)\n",
    "            else:\n",
    "                d_q = cohens_d(delta[mask_q])\n",
    "                row += f\"{d_q:>+12.3f}\"\n",
    "                q_ds.append(float(d_q))\n",
    "        d_all = cohens_d(delta)\n",
    "        row += f\"{d_all:>+12.3f}\"\n",
    "        print(f\"  {'':20}\" + \"\".join(f\"{ql:>12}\" for ql in quintile_labels) + f\"{'Overall':>12}\")\n",
    "        print(row)\n",
    "        ds_hardness[cname] = q_ds\n",
    "\n",
    "    hardness_data[ds_name] = ds_hardness\n",
    "\n",
    "# === ANSWER LENGTH INTERACTION ===\n",
    "print(f\"\\n\\n{'='*70}\")\n",
    "print(\"ANSWER LENGTH INTERACTION\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "answer_len_data = {}\n",
    "answer_bins = [('short(<5)', 0, 5), ('medium(5-15)', 5, 15), ('long(>15)', 15, 9999)]\n",
    "\n",
    "for ds_name in dataset_names:\n",
    "    ds_results = [r for r in all_results if r['dataset'] == ds_name]\n",
    "    if len(ds_results) < 50:\n",
    "        continue\n",
    "\n",
    "    print(f\"\\n--- {ds_name.upper()} ---\")\n",
    "    ds_ans_data = {}\n",
    "\n",
    "    for cname in ['sf_trunc_bias2', 'values_only']:\n",
    "        bare_arr = np.array([r['bare'] for r in ds_results])\n",
    "        cond_arr = np.array([r[cname] for r in ds_results])\n",
    "        delta = bare_arr - cond_arr\n",
    "        a_lens = np.array([r['answer_token_len'] for r in ds_results])\n",
    "\n",
    "        row_parts = []\n",
    "        for a_label, a_min, a_max in answer_bins:\n",
    "            mask_a = (a_lens >= a_min) & (a_lens < a_max)\n",
    "            n_a = int(np.sum(mask_a))\n",
    "            if n_a < 10:\n",
    "                row_parts.append(f\"  {a_label}: n={n_a} (too few)\")\n",
    "            else:\n",
    "                d_a = cohens_d(delta[mask_a])\n",
    "                row_parts.append(f\"  {a_label}: n={n_a}, d={d_a:+.3f}\")\n",
    "        print(f\"  {cname}: \" + \", \".join(row_parts))\n",
    "        ds_ans_data[cname] = row_parts\n",
    "\n",
    "    answer_len_data[ds_name] = ds_ans_data"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 12: Multi-panel figure\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 14))\n",
    "\n",
    "colors = {\n",
    "    'sf_trunc': '#1f77b4',\n",
    "    'sf_trunc_bias2': '#d62728',\n",
    "    'sf_trunc_bias4': '#ff7f0e',\n",
    "    'values_only': '#2ca02c',\n",
    "}\n",
    "\n",
    "# ---- Panel (a): Cohen's d by dataset x condition ----\n",
    "ax = axes[0, 0]\n",
    "x = np.arange(len(dataset_names))\n",
    "width = 0.18\n",
    "for i, cname in enumerate(['sf_trunc', 'sf_trunc_bias2', 'sf_trunc_bias4', 'values_only']):\n",
    "    ds_vals = []\n",
    "    for ds in dataset_names:\n",
    "        if ds in analysis and cname in analysis[ds]:\n",
    "            ds_vals.append(analysis[ds][cname]['cohens_d'])\n",
    "        else:\n",
    "            ds_vals.append(0)\n",
    "    offset = (i - 1.5) * width\n",
    "    bars = ax.bar(x + offset, ds_vals, width, label=cname, color=colors[cname],\n",
    "                  edgecolor='black', linewidth=0.5)\n",
    "    for j, val in enumerate(ds_vals):\n",
    "        ax.text(x[j] + offset, val + (0.01 if val >= 0 else -0.03),\n",
    "                f\"{val:+.2f}\", ha='center', va='bottom' if val >= 0 else 'top', fontsize=7)\n",
    "\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([ds.upper() for ds in dataset_names])\n",
    "ax.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "ax.set_ylabel(\"Cohen's d (positive = helps)\")\n",
    "ax.set_title(\"(a) Effect Size by Dataset x Condition\")\n",
    "ax.legend(fontsize=7, loc='best')\n",
    "\n",
    "# ---- Panel (b): Length stratification for bias2 across datasets ----\n",
    "ax = axes[0, 1]\n",
    "for ds_idx, ds_name in enumerate(dataset_names):\n",
    "    if ds_name not in length_strat:\n",
    "        continue\n",
    "    cname = 'sf_trunc_bias2'\n",
    "    if cname not in length_strat[ds_name]:\n",
    "        continue\n",
    "    bins_data = length_strat[ds_name][cname]\n",
    "    bin_labels = [b['label'] for b in bins_data]\n",
    "    bin_ds = [b['d'] if b['d'] is not None else 0 for b in bins_data]\n",
    "    bin_ns = [b['n'] for b in bins_data]\n",
    "    # Only plot bins with enough data\n",
    "    valid_idx = [i for i, b in enumerate(bins_data) if b['d'] is not None]\n",
    "    if valid_idx:\n",
    "        x_vals = [i for i in valid_idx]\n",
    "        y_vals = [bin_ds[i] for i in valid_idx]\n",
    "        ax.plot(x_vals, y_vals, marker='o', linewidth=2, markersize=6,\n",
    "                label=ds_name)\n",
    "\n",
    "bin_labels_all = [b[0] for b in LENGTH_BINS]\n",
    "ax.set_xticks(range(len(bin_labels_all)))\n",
    "ax.set_xticklabels(bin_labels_all, rotation=30, ha='right', fontsize=8)\n",
    "ax.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "ax.set_ylabel(\"Cohen's d\")\n",
    "ax.set_xlabel(\"Document Token Length Bin\")\n",
    "ax.set_title(\"(b) Attention Forcing (bias=2.0) by Length\")\n",
    "ax.legend(fontsize=8)\n",
    "\n",
    "# ---- Panel (c): Hardness heatmap for bias2 ----\n",
    "ax = axes[1, 0]\n",
    "quintile_labels = ['Q1\\n(easy)', 'Q2', 'Q3', 'Q4', 'Q5\\n(hard)']\n",
    "hm_rows = []\n",
    "hm_ylabels = []\n",
    "for ds_name in dataset_names:\n",
    "    if ds_name in hardness_data and 'sf_trunc_bias2' in hardness_data[ds_name]:\n",
    "        row = hardness_data[ds_name]['sf_trunc_bias2']\n",
    "        hm_rows.append([v if v is not None else 0 for v in row])\n",
    "        hm_ylabels.append(ds_name.upper())\n",
    "\n",
    "if hm_rows:\n",
    "    hm_arr = np.array(hm_rows)\n",
    "    im = ax.imshow(hm_arr, cmap='RdBu', aspect='auto', vmin=-0.5, vmax=0.5)\n",
    "    ax.set_xticks(range(5))\n",
    "    ax.set_xticklabels(quintile_labels, fontsize=8)\n",
    "    ax.set_yticks(range(len(hm_ylabels)))\n",
    "    ax.set_yticklabels(hm_ylabels)\n",
    "    for i in range(len(hm_ylabels)):\n",
    "        for j in range(5):\n",
    "            val = hm_arr[i, j]\n",
    "            ax.text(j, i, f\"{val:+.2f}\", ha='center', va='center',\n",
    "                    fontsize=9, color='white' if abs(val) > 0.25 else 'black')\n",
    "    fig.colorbar(im, ax=ax, shrink=0.8, label=\"Cohen's d\")\n",
    "ax.set_title(\"(c) Hardness x Dataset (bias=2.0)\")\n",
    "\n",
    "# ---- Panel (d): Bias tuning per dataset ----\n",
    "ax = axes[1, 1]\n",
    "bias_vals_plot = [0.0, 2.0, 4.0]\n",
    "bias_labels_plot = ['bias=0', 'bias=2', 'bias=4']\n",
    "cond_for_bias = ['sf_trunc', 'sf_trunc_bias2', 'sf_trunc_bias4']\n",
    "\n",
    "for ds_name in dataset_names:\n",
    "    if ds_name not in analysis:\n",
    "        continue\n",
    "    d_vals = []\n",
    "    for cname in cond_for_bias:\n",
    "        if cname in analysis[ds_name]:\n",
    "            d_vals.append(analysis[ds_name][cname]['cohens_d'])\n",
    "        else:\n",
    "            d_vals.append(0)\n",
    "    ax.plot(bias_vals_plot, d_vals, marker='o', linewidth=2, markersize=8,\n",
    "            label=ds_name)\n",
    "    for i, (bv, dv) in enumerate(zip(bias_vals_plot, d_vals)):\n",
    "        ax.annotate(f\"{dv:+.2f}\", (bv, dv), textcoords='offset points',\n",
    "                    xytext=(5, 8), fontsize=7)\n",
    "\n",
    "ax.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "ax.set_xlabel('Attention Bias Value')\n",
    "ax.set_ylabel(\"Cohen's d\")\n",
    "ax.set_title(\"(d) Bias Tuning by Dataset\")\n",
    "ax.legend(fontsize=8)\n",
    "\n",
    "plt.suptitle('Exp 27: Cross-Dataset Generalization with Attention Forcing', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'analysis_plots.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"Plots saved to {RESULTS_DIR / 'analysis_plots.png'}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 13: Save results.json + CSV\n",
    "\n",
    "# --- CSV ---\n",
    "with open(CSV_PATH, 'w', newline='') as f:\n",
    "    fieldnames = ['query_idx', 'dataset', 'query', 'answer', 'word_count',\n",
    "                  'doc_token_len', 'answer_token_len',\n",
    "                  'bare', 'sf_trunc', 'sf_trunc_bias2', 'sf_trunc_bias4', 'values_only']\n",
    "    writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    for r in all_results:\n",
    "        writer.writerow({k: r.get(k, '') for k in fieldnames})\n",
    "print(f\"CSV saved: {CSV_PATH}\")\n",
    "\n",
    "# --- Verdict ---\n",
    "best_ds = None\n",
    "best_cond = None\n",
    "best_d = -999\n",
    "for ds_name in dataset_names:\n",
    "    if ds_name not in analysis:\n",
    "        continue\n",
    "    for cname in ['sf_trunc_bias2', 'sf_trunc_bias4']:\n",
    "        if cname in analysis[ds_name]:\n",
    "            d = analysis[ds_name][cname]['cohens_d']\n",
    "            if d > best_d:\n",
    "                best_d = d\n",
    "                best_ds = ds_name\n",
    "                best_cond = cname\n",
    "\n",
    "if best_d > 0.15:\n",
    "    verdict = (f\"SUCCESS: Attention forcing generalizes! Best: {best_ds}/{best_cond} \"\n",
    "               f\"d={best_d:+.3f}\")\n",
    "elif best_d > 0.05:\n",
    "    verdict = (f\"PARTIAL: Weak generalization. Best: {best_ds}/{best_cond} \"\n",
    "               f\"d={best_d:+.3f}\")\n",
    "else:\n",
    "    verdict = (f\"FAILURE: Attention forcing does NOT generalize beyond MS MARCO. \"\n",
    "               f\"Best: {best_ds}/{best_cond} d={best_d:+.3f}\")\n",
    "\n",
    "# Check values_only universality\n",
    "values_positive = []\n",
    "for ds_name in dataset_names:\n",
    "    if ds_name in analysis and 'values_only' in analysis[ds_name]:\n",
    "        d = analysis[ds_name]['values_only']['cohens_d']\n",
    "        values_positive.append(d > 0)\n",
    "values_verdict = (\"VALUES_ONLY universally positive\" if all(values_positive)\n",
    "                   else \"VALUES_ONLY NOT universally positive\")\n",
    "\n",
    "print(f\"\\nVERDICT: {verdict}\")\n",
    "print(f\"VALUES: {values_verdict}\")\n",
    "\n",
    "# --- results.json ---\n",
    "final = {\n",
    "    'experiment': 'exp27_cross_dataset_attention_forcing',\n",
    "    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'config': {\n",
    "        'model_name': MODEL_NAME,\n",
    "        'model_type': 'mistral',\n",
    "        'seed': SEED,\n",
    "        'n_per_dataset': N_PER_DATASET,\n",
    "        'max_doc_tokens': MAX_DOC_TOKENS,\n",
    "        'conditions': CONDITION_NAMES,\n",
    "        'bias_map': {k: float(v) for k, v in BIAS_MAP.items()},\n",
    "        'prefix': STATIC_FACT,\n",
    "        'prefix_token_len': PREFIX_TOKEN_LEN,\n",
    "        'datasets': dataset_names,\n",
    "        'length_bins': LENGTH_BINS,\n",
    "    },\n",
    "    'per_dataset_analysis': analysis,\n",
    "    'length_stratification': length_strat,\n",
    "    'hardness_data': hardness_data,\n",
    "    'verdict': verdict,\n",
    "    'values_verdict': values_verdict,\n",
    "    'per_sample_results': all_results,\n",
    "}\n",
    "\n",
    "with open(FINAL_RESULTS_PATH, 'w') as f:\n",
    "    json.dump(final, f, indent=2)\n",
    "\n",
    "print(f\"\\nResults saved to {FINAL_RESULTS_PATH}\")\n",
    "print(f\"File size: {FINAL_RESULTS_PATH.stat().st_size / 1024:.1f} KB\")\n",
    "\n",
    "# Final summary\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"SUMMARY -- Exp 27: Cross-Dataset Generalization\")\n",
    "print(\"=\" * 70)\n",
    "for ds_name in dataset_names:\n",
    "    if ds_name not in analysis:\n",
    "        continue\n",
    "    print(f\"\\n  {ds_name.upper()}:\")\n",
    "    for cname in CONDITION_NAMES:\n",
    "        if cname == 'bare':\n",
    "            continue\n",
    "        if cname in analysis[ds_name]:\n",
    "            a = analysis[ds_name][cname]\n",
    "            sig = '***' if a['p_value'] < 0.001 else '**' if a['p_value'] < 0.01 else '*' if a['p_value'] < 0.05 else 'ns'\n",
    "            print(f\"    {cname:<20} d={a['cohens_d']:>+.3f}  win={a['win_pct']:.0f}%  {sig}\")\n",
    "\n",
    "print(f\"\\nVERDICT: {verdict}\")\n",
    "print(f\"VALUES: {values_verdict}\")\n",
    "print(f\"\\nDone!\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 14: GPU cleanup\n",
    "import gc\n",
    "\n",
    "print(\"Cleaning up GPU memory...\")\n",
    "mem_before = torch.cuda.memory_allocated() / 1e9\n",
    "\n",
    "del model\n",
    "del tokenizer\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "mem_after = torch.cuda.memory_allocated() / 1e9\n",
    "print(f\"GPU memory: {mem_before:.2f} GB -> {mem_after:.2f} GB\")\n",
    "print(\"Cleanup complete.\")"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}