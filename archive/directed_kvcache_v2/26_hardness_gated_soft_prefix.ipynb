{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Exp 26: Hardness-Gated Soft Prefix\n",
    "\n",
    "## Motivation\n",
    "\n",
    "Exp 25 showed soft prefix optimization beats discrete prefix (d=+0.288 vs +0.195) but has a\n",
    "problem: **it hurts easy queries** (Q1: d=-0.43) while massively helping hard ones (Q4: d=+0.85).\n",
    "This suggests a simple gating strategy: only apply the soft prefix when the query is hard enough\n",
    "to benefit.\n",
    "\n",
    "## Design\n",
    "\n",
    "| Part | Data | Description |\n",
    "|------|------|-------------|\n",
    "| 1 | Exp 25 eval CSV (275 valid samples) | Threshold sweep on existing data |\n",
    "| 2 | 300 fresh validation queries (seed=44) | Test generalization of optimal threshold |\n",
    "| 3 | Cross-validation of threshold | Does Part 1 threshold hold on Part 2 data? |\n",
    "\n",
    "### Gating Rule\n",
    "\n",
    "```\n",
    "gated_nll = soft_fact_nll  if bare_nll >= threshold\n",
    "            bare_nll       otherwise\n",
    "```\n",
    "\n",
    "### Success Criteria\n",
    "\n",
    "- **Primary**: Gated d > ungated d (+0.288) on fresh validation data\n",
    "- **Secondary**: Q1 harm eliminated (d >= 0 for easiest quintile)\n",
    "- **Tertiary**: Optimal threshold generalizes from Exp 25 data to fresh data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": "# Cell 1: Setup\nimport os\nos.umask(0o000)\n\nimport sys\nimport json\nimport time\nimport csv\nimport numpy as np\nimport torch\nimport gc\nfrom pathlib import Path\nfrom scipy import stats\n\nSEED = 42\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed_all(SEED)\nnp.random.seed(SEED)\n\nRESULTS_DIR = Path(\"results/exp26\")\nRESULTS_DIR.mkdir(parents=True, exist_ok=True)\n\nEXP25_CSV = Path(\"results/exp25/eval_results.csv\")\nEXP25_SOFT_FACT = Path(\"results/exp25/soft_prefix_fact.pt\")\nCHECKPOINT_EVAL_PATH = RESULTS_DIR / \"checkpoint_eval.json\"\nFINAL_RESULTS_PATH = RESULTS_DIR / \"results.json\"\nCSV_EVAL_PATH = RESULTS_DIR / \"eval_results.csv\"\n\nprint(f\"SEED: {SEED}\")\nprint(f\"Results directory: {RESULTS_DIR}\")\nprint(f\"Exp 25 CSV: {EXP25_CSV} (exists: {EXP25_CSV.exists()})\")\nprint(f\"Exp 25 soft_prefix_fact: {EXP25_SOFT_FACT} (exists: {EXP25_SOFT_FACT.exists()})\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Load Exp 25 evaluation data\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PART 1: THRESHOLD SIMULATION ON EXP 25 DATA\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Load CSV\n",
    "exp25_data = []\n",
    "with open(EXP25_CSV, 'r') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        exp25_data.append({\n",
    "            'query_idx': int(row['query_idx']),\n",
    "            'doc_len': int(row['doc_len']),\n",
    "            'bare_nll': float(row['bare_nll']),\n",
    "            'vel_soft_fact_nll': float(row['vel_soft_fact_nll']),\n",
    "        })\n",
    "\n",
    "print(f\"Loaded {len(exp25_data)} rows from Exp 25 CSV\")\n",
    "\n",
    "bare_arr = np.array([r['bare_nll'] for r in exp25_data])\n",
    "soft_arr = np.array([r['vel_soft_fact_nll'] for r in exp25_data])\n",
    "\n",
    "# Filter valid samples (same as Exp 25)\n",
    "valid = (\n",
    "    (bare_arr != 0) & np.isfinite(bare_arr) &\n",
    "    (soft_arr != 0) & np.isfinite(soft_arr)\n",
    ")\n",
    "bare = bare_arr[valid]\n",
    "soft = soft_arr[valid]\n",
    "n_valid = int(np.sum(valid))\n",
    "\n",
    "print(f\"Valid samples: {n_valid}/{len(exp25_data)}\")\n",
    "print(f\"Bare NLL: mean={np.mean(bare):.4f}, std={np.std(bare):.4f}\")\n",
    "print(f\"Soft fact NLL: mean={np.mean(soft):.4f}, std={np.std(soft):.4f}\")\n",
    "\n",
    "# Ungated reference\n",
    "from lib.analysis import cohens_d\n",
    "\n",
    "ungated_delta = bare - soft\n",
    "ungated_d = cohens_d(ungated_delta)\n",
    "ungated_win = np.mean(ungated_delta > 0) * 100\n",
    "_, ungated_p = stats.ttest_1samp(ungated_delta, 0)\n",
    "\n",
    "print(f\"\\nUngated soft_fact: d={ungated_d:+.3f}, win={ungated_win:.1f}%, p={ungated_p:.2e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Threshold simulation sweep\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"THRESHOLD SWEEP\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "def compute_gated_metrics(bare, soft, threshold):\n",
    "    \"\"\"Apply hardness gating and compute metrics.\n",
    "    \n",
    "    Gated NLL = soft_nll if bare_nll >= threshold, else bare_nll.\n",
    "    Returns dict with d, win%, p, n_gated, fraction_gated.\n",
    "    \"\"\"\n",
    "    gated = np.where(bare >= threshold, soft, bare)\n",
    "    delta = bare - gated\n",
    "    n_gated = int(np.sum(bare >= threshold))\n",
    "    \n",
    "    d = cohens_d(delta)\n",
    "    win_pct = np.mean(delta > 0) * 100\n",
    "    _, p_val = stats.ttest_1samp(delta, 0)\n",
    "    \n",
    "    return {\n",
    "        'threshold': float(threshold),\n",
    "        'cohens_d': float(d),\n",
    "        'win_pct': float(win_pct),\n",
    "        'p_value': float(p_val),\n",
    "        'n_gated': n_gated,\n",
    "        'frac_gated': n_gated / len(bare),\n",
    "        'mean_delta': float(np.mean(delta)),\n",
    "    }\n",
    "\n",
    "# Sweep: percentile-based + fine grid\n",
    "percentiles = [0, 10, 20, 30, 40, 50, 60, 70, 75, 80, 85, 90, 95]\n",
    "percentile_thresholds = np.percentile(bare, percentiles)\n",
    "\n",
    "# Also sweep a fine grid of absolute thresholds\n",
    "fine_thresholds = np.linspace(0, np.percentile(bare, 95), 100)\n",
    "\n",
    "# Combine and deduplicate\n",
    "all_thresholds = np.unique(np.concatenate([percentile_thresholds, fine_thresholds]))\n",
    "all_thresholds.sort()\n",
    "\n",
    "sweep_results = []\n",
    "for t in all_thresholds:\n",
    "    metrics = compute_gated_metrics(bare, soft, t)\n",
    "    sweep_results.append(metrics)\n",
    "\n",
    "# Find optimal threshold\n",
    "best_idx = np.argmax([r['cohens_d'] for r in sweep_results])\n",
    "best = sweep_results[best_idx]\n",
    "\n",
    "print(f\"\\nSweep: {len(all_thresholds)} thresholds tested\")\n",
    "print(f\"\\nOptimal threshold: {best['threshold']:.4f}\")\n",
    "print(f\"  Cohen's d: {best['cohens_d']:+.3f} (ungated: {ungated_d:+.3f})\")\n",
    "print(f\"  Win%: {best['win_pct']:.1f}% (ungated: {ungated_win:.1f}%)\")\n",
    "print(f\"  p-value: {best['p_value']:.2e}\")\n",
    "print(f\"  N gated: {best['n_gated']}/{n_valid} ({best['frac_gated']*100:.1f}%)\")\n",
    "\n",
    "# Show percentile-based results\n",
    "print(f\"\\n{'Percentile':<12} {'Threshold':>10} {'d':>8} {'Win%':>7} {'N gated':>8} {'% gated':>8}\")\n",
    "print(\"-\" * 58)\n",
    "for pct in percentiles:\n",
    "    t = np.percentile(bare, pct)\n",
    "    m = compute_gated_metrics(bare, soft, t)\n",
    "    marker = \" <<<\" if abs(t - best['threshold']) < 1e-6 else \"\"\n",
    "    print(f\"P{pct:<11} {t:>10.4f} {m['cohens_d']:>+8.3f} {m['win_pct']:>6.1f}% \"\n",
    "          f\"{m['n_gated']:>8} {m['frac_gated']*100:>7.1f}%{marker}\")\n",
    "\n",
    "# Select top-5 candidate thresholds for fresh validation\n",
    "# Percentile-based: P50, P60, P70, P75, P80 + absolute optimal\n",
    "candidate_percentiles = [50, 60, 70, 75, 80]\n",
    "candidate_thresholds = {f'P{p}': float(np.percentile(bare, p)) for p in candidate_percentiles}\n",
    "candidate_thresholds['optimal'] = best['threshold']\n",
    "\n",
    "print(f\"\\nCandidate thresholds for fresh validation:\")\n",
    "for name, t in candidate_thresholds.items():\n",
    "    m = compute_gated_metrics(bare, soft, t)\n",
    "    print(f\"  {name}: threshold={t:.4f}, d={m['cohens_d']:+.3f}, \"\n",
    "          f\"win={m['win_pct']:.1f}%, gated={m['frac_gated']*100:.0f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Bootstrap CI + oracle gating upper bound\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"BOOTSTRAP CONFIDENCE INTERVALS & ORACLE GATING\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "N_BOOTSTRAP = 1000\n",
    "\n",
    "def bootstrap_d(bare, soft, threshold, n_boot=N_BOOTSTRAP, seed=42):\n",
    "    \"\"\"Bootstrap 95% CI for gated Cohen's d.\"\"\"\n",
    "    rng = np.random.RandomState(seed)\n",
    "    ds = []\n",
    "    n = len(bare)\n",
    "    for _ in range(n_boot):\n",
    "        idx = rng.choice(n, size=n, replace=True)\n",
    "        b, s = bare[idx], soft[idx]\n",
    "        gated = np.where(b >= threshold, s, b)\n",
    "        delta = b - gated\n",
    "        d = cohens_d(delta)\n",
    "        ds.append(d)\n",
    "    ds = np.array(ds)\n",
    "    return {\n",
    "        'mean': float(np.mean(ds)),\n",
    "        'ci_low': float(np.percentile(ds, 2.5)),\n",
    "        'ci_high': float(np.percentile(ds, 97.5)),\n",
    "        'std': float(np.std(ds)),\n",
    "    }\n",
    "\n",
    "# Bootstrap for ungated\n",
    "boot_ungated = bootstrap_d(bare, soft, 0.0)  # threshold=0 means all gated\n",
    "print(f\"\\nUngated d: {ungated_d:+.3f} (95% CI: [{boot_ungated['ci_low']:+.3f}, {boot_ungated['ci_high']:+.3f}])\")\n",
    "\n",
    "# Bootstrap for optimal threshold\n",
    "boot_optimal = bootstrap_d(bare, soft, best['threshold'])\n",
    "print(f\"Optimal gated d: {best['cohens_d']:+.3f} (95% CI: [{boot_optimal['ci_low']:+.3f}, {boot_optimal['ci_high']:+.3f}])\")\n",
    "\n",
    "# Bootstrap for each candidate\n",
    "print(f\"\\n{'Candidate':<12} {'d':>8} {'CI low':>8} {'CI high':>8}\")\n",
    "print(\"-\" * 40)\n",
    "boot_candidates = {}\n",
    "for name, t in candidate_thresholds.items():\n",
    "    m = compute_gated_metrics(bare, soft, t)\n",
    "    boot = bootstrap_d(bare, soft, t)\n",
    "    boot_candidates[name] = boot\n",
    "    print(f\"{name:<12} {m['cohens_d']:>+8.3f} {boot['ci_low']:>+8.3f} {boot['ci_high']:>+8.3f}\")\n",
    "\n",
    "# Oracle gating: apply soft only when it actually helps\n",
    "oracle_gated = np.where(soft < bare, soft, bare)\n",
    "oracle_delta = bare - oracle_gated\n",
    "oracle_d = cohens_d(oracle_delta)\n",
    "oracle_win = np.mean(oracle_delta > 0) * 100\n",
    "\n",
    "print(f\"\\nOracle gating (perfect knowledge):\")\n",
    "print(f\"  d={oracle_d:+.3f}, win={oracle_win:.1f}%\")\n",
    "print(f\"  This is the UPPER BOUND — no threshold can beat this.\")\n",
    "print(f\"  Fraction where soft helps: {np.mean(soft < bare)*100:.1f}%\")\n",
    "print(f\"  Fraction where soft hurts: {np.mean(soft > bare)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Part 1 plots\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# --- Panel 1: d vs threshold curve ---\n",
    "ax = axes[0]\n",
    "thresholds = [r['threshold'] for r in sweep_results]\n",
    "ds = [r['cohens_d'] for r in sweep_results]\n",
    "\n",
    "ax.plot(thresholds, ds, linewidth=2, color='#1f77b4')\n",
    "ax.axhline(y=ungated_d, color='red', linestyle='--', linewidth=1.5,\n",
    "           label=f'Ungated d={ungated_d:+.3f}')\n",
    "ax.axhline(y=oracle_d, color='green', linestyle=':', linewidth=1.5,\n",
    "           label=f'Oracle d={oracle_d:+.3f}')\n",
    "ax.axvline(x=best['threshold'], color='orange', linestyle='--', alpha=0.7,\n",
    "           label=f'Optimal t={best[\"threshold\"]:.3f}')\n",
    "ax.set_xlabel('Bare NLL Threshold')\n",
    "ax.set_ylabel(\"Cohen's d (gated)\")\n",
    "ax.set_title('Gated d vs Threshold')\n",
    "ax.legend(fontsize=8)\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "# --- Panel 2: Per-quintile heatmap (gated vs ungated) ---\n",
    "ax = axes[1]\n",
    "\n",
    "quintile_bounds = np.percentile(bare, [20, 40, 60, 80])\n",
    "qlabels = ['Q1\\neasy', 'Q2', 'Q3', 'Q4', 'Q5\\nhard']\n",
    "quintiles = np.digitize(bare, quintile_bounds)\n",
    "\n",
    "# Compare ungated vs optimal gated per quintile\n",
    "gated_optimal = np.where(bare >= best['threshold'], soft, bare)\n",
    "conditions = {\n",
    "    'ungated': soft,\n",
    "    'gated_optimal': gated_optimal,\n",
    "}\n",
    "\n",
    "heatmap = np.zeros((len(conditions), 5))\n",
    "for i, (cname, carr) in enumerate(conditions.items()):\n",
    "    delta = bare - carr\n",
    "    for q in range(5):\n",
    "        mask = quintiles == q\n",
    "        if np.sum(mask) >= 5:\n",
    "            heatmap[i, q] = cohens_d(delta[mask])\n",
    "\n",
    "im = ax.imshow(heatmap, cmap='RdBu', aspect='auto', vmin=-0.5, vmax=1.0)\n",
    "ax.set_xticks(range(5))\n",
    "ax.set_xticklabels(qlabels, fontsize=8)\n",
    "ax.set_yticks(range(len(conditions)))\n",
    "ax.set_yticklabels(['ungated', 'gated'])\n",
    "ax.set_title('Quintile d: Ungated vs Gated')\n",
    "\n",
    "for i in range(len(conditions)):\n",
    "    for j in range(5):\n",
    "        val = heatmap[i, j]\n",
    "        ax.text(j, i, f\"{val:+.2f}\", ha='center', va='center',\n",
    "                fontsize=9, color='white' if abs(val) > 0.3 else 'black')\n",
    "fig.colorbar(im, ax=ax, shrink=0.8)\n",
    "\n",
    "# --- Panel 3: Scatter (bare NLL vs delta) with threshold ---\n",
    "ax = axes[2]\n",
    "delta_ungated = bare - soft\n",
    "ax.scatter(bare, delta_ungated, alpha=0.4, s=20, c='#1f77b4', edgecolors='none',\n",
    "           label='ungated')\n",
    "ax.axhline(y=0, color='gray', linestyle='-', alpha=0.5)\n",
    "ax.axvline(x=best['threshold'], color='orange', linestyle='--', linewidth=2,\n",
    "           label=f'threshold={best[\"threshold\"]:.3f}')\n",
    "ax.set_xlabel('Bare NLL (difficulty)')\n",
    "ax.set_ylabel('Delta (bare - soft, positive = helps)')\n",
    "ax.set_title('Per-Sample: Difficulty vs Benefit')\n",
    "ax.legend(fontsize=8)\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "# Annotate regions\n",
    "ax.text(best['threshold'] * 0.4, ax.get_ylim()[0] * 0.7,\n",
    "        'SKIP\\n(easy)', ha='center', fontsize=10, color='red', alpha=0.7)\n",
    "ax.text(best['threshold'] * 1.5 if best['threshold'] > 0 else 0.5,\n",
    "        ax.get_ylim()[1] * 0.7,\n",
    "        'APPLY\\n(hard)', ha='center', fontsize=10, color='green', alpha=0.7)\n",
    "\n",
    "plt.suptitle('Exp 26 Part 1: Threshold Simulation on Exp 25 Data', fontsize=13)\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'part1_threshold_sweep.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"Saved to {RESULTS_DIR / 'part1_threshold_sweep.png'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Load Gemma 3 4B + Exp 25 soft_prefix_fact.pt\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PART 2: FRESH VALIDATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "sys.path.insert(0, \".\")\n",
    "\n",
    "from lib.config import ExperimentConfig\n",
    "from lib.model_utils import load_model\n",
    "\n",
    "MODEL_NAME = \"google/gemma-3-4b-it\"\n",
    "\n",
    "exp_config = ExperimentConfig(\n",
    "    model_name=MODEL_NAME,\n",
    "    model_type=\"gemma3\",\n",
    "    compute_dtype=\"auto\",\n",
    "    use_4bit=True,\n",
    "    num_samples=300,\n",
    "    seed=SEED,\n",
    ")\n",
    "\n",
    "print(f\"Loading {MODEL_NAME} (4-bit, bfloat16)...\")\n",
    "model, tokenizer = load_model(exp_config)\n",
    "\n",
    "from lib.kv_cache import (\n",
    "    _get_text_config, _get_head_dim,\n",
    "    _get_cache_keys, _get_cache_values,\n",
    "    _set_cache_keys, _set_cache_values,\n",
    "    _ensure_dynamic_cache,\n",
    "    extract_and_truncate_cache_with_bos,\n",
    "    correct_rope_positions_with_bos,\n",
    "    score_answer_with_cache,\n",
    "    deepcopy_cache,\n",
    "    replace_values_at_layers,\n",
    ")\n",
    "\n",
    "text_config = _get_text_config(model.config)\n",
    "NUM_LAYERS = text_config.num_hidden_layers\n",
    "HIDDEN_SIZE = text_config.hidden_size\n",
    "\n",
    "print(f\"Model loaded. Layers={NUM_LAYERS}, hidden={HIDDEN_SIZE}\")\n",
    "\n",
    "# Load soft prefix\n",
    "soft_prefix_fact = torch.load(EXP25_SOFT_FACT).to(exp_config.device)\n",
    "print(f\"Loaded soft_prefix_fact: shape={soft_prefix_fact.shape}\")\n",
    "\n",
    "# Get embedding function\n",
    "embed_fn = model.get_input_embeddings()\n",
    "print(f\"Embedding layer: {type(embed_fn).__name__}, shape={embed_fn.weight.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Load 300 fresh validation queries (seed=44, non-overlapping with Exp 25's seed=43)\n",
    "\n",
    "from lib.data import count_words\n",
    "from lib.surrogate import STATIC_SURROGATE_QUERIES\n",
    "from datasets import load_dataset\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "CUTOFF = 16\n",
    "MAX_PASSAGE_WORDS = 300\n",
    "N_FRESH = 300\n",
    "FRESH_SEED = 44  # Different from Exp 25 train (42) and eval (43)\n",
    "CHECKPOINT_EVERY = 50\n",
    "\n",
    "STATIC_FACT = STATIC_SURROGATE_QUERIES['static_factual']['query']\n",
    "SURROGATE_PREFIX_TEMPLATE = \"{surrogate}\\n\"\n",
    "DOCUMENT_TEMPLATE = \"{document}\"\n",
    "QUERY_TEMPLATE = \"\\nQuery: {query}\\nAnswer:\"\n",
    "ANSWER_TEMPLATE = \" {answer}\"\n",
    "\n",
    "sf_str = SURROGATE_PREFIX_TEMPLATE.format(surrogate=STATIC_FACT)\n",
    "sf_ids = tokenizer(sf_str, return_tensors=\"pt\",\n",
    "                    add_special_tokens=False)['input_ids'].to(exp_config.device)\n",
    "PREFIX_LEN = sf_ids.shape[1]\n",
    "\n",
    "print(f\"Loading MS MARCO validation with seed={FRESH_SEED}...\")\n",
    "\n",
    "dataset = load_dataset(\"microsoft/ms_marco\", \"v1.1\", split=\"validation\")\n",
    "print(f\"Total items: {len(dataset)}\")\n",
    "\n",
    "fresh_samples = []\n",
    "np.random.seed(FRESH_SEED)\n",
    "\n",
    "for item in tqdm(dataset, desc=\"Filtering\"):\n",
    "    passages_info = item.get('passages', {})\n",
    "    passage_texts = passages_info.get('passage_text', [])\n",
    "    is_selected = passages_info.get('is_selected', [])\n",
    "    query = item.get('query', '')\n",
    "    answers = item.get('answers', [])\n",
    "    well_formed = item.get('wellFormedAnswers', [])\n",
    "\n",
    "    if not passage_texts or not query:\n",
    "        continue\n",
    "\n",
    "    answer = None\n",
    "    if well_formed and len(well_formed) > 0 and well_formed[0] != '[]':\n",
    "        answer = well_formed[0]\n",
    "    elif answers and len(answers) > 0 and answers[0] != 'No Answer Present.':\n",
    "        answer = answers[0]\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "    for ptext, sel in zip(passage_texts, is_selected):\n",
    "        if sel == 1 and count_words(ptext) <= MAX_PASSAGE_WORDS:\n",
    "            fresh_samples.append({\n",
    "                'query': query,\n",
    "                'answer': answer,\n",
    "                'passage': ptext,\n",
    "                'word_count': count_words(ptext),\n",
    "            })\n",
    "            break\n",
    "\n",
    "    if len(fresh_samples) >= N_FRESH * 3:\n",
    "        break\n",
    "\n",
    "np.random.shuffle(fresh_samples)\n",
    "fresh_samples = fresh_samples[:N_FRESH]\n",
    "\n",
    "del dataset\n",
    "gc.collect()\n",
    "\n",
    "print(f\"\\nSelected {len(fresh_samples)} fresh validation samples (seed={FRESH_SEED})\")\n",
    "print(f\"Word counts: mean={np.mean([q['word_count'] for q in fresh_samples]):.0f}, \"\n",
    "      f\"min={min(q['word_count'] for q in fresh_samples)}, \"\n",
    "      f\"max={max(q['word_count'] for q in fresh_samples)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Score bare + soft_fact for all 300 fresh queries\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"SCORING {N_FRESH} FRESH QUERIES (bare + soft_fact)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "layer_indices = list(range(CUTOFF))\n",
    "\n",
    "# Checkpoint resume\n",
    "fresh_results = []\n",
    "fresh_start_idx = 0\n",
    "\n",
    "if CHECKPOINT_EVAL_PATH.exists():\n",
    "    with open(CHECKPOINT_EVAL_PATH, 'r') as f:\n",
    "        ckpt = json.load(f)\n",
    "    ckpt_queries = ckpt.get('query_texts', [])\n",
    "    current_queries = [q['query'] for q in fresh_samples[:N_FRESH]]\n",
    "    if ckpt_queries == current_queries:\n",
    "        fresh_results = ckpt['results']\n",
    "        fresh_start_idx = len(fresh_results)\n",
    "        print(f\"Resuming from checkpoint: {fresh_start_idx}/{N_FRESH}\")\n",
    "    else:\n",
    "        print(\"Checkpoint query mismatch. Starting fresh.\")\n",
    "else:\n",
    "    print(\"No checkpoint found. Starting fresh.\")\n",
    "\n",
    "t_start = time.time()\n",
    "\n",
    "for qidx in tqdm(range(fresh_start_idx, N_FRESH), initial=fresh_start_idx,\n",
    "                  total=N_FRESH, desc=\"Scoring\"):\n",
    "    qdata = fresh_samples[qidx]\n",
    "    query_prompt = QUERY_TEMPLATE.format(query=qdata['query'])\n",
    "    answer_text = ANSWER_TEMPLATE.format(answer=qdata['answer'])\n",
    "    document_text = DOCUMENT_TEMPLATE.format(document=qdata['passage'])\n",
    "\n",
    "    # Matched tokenization\n",
    "    full_text = sf_str + document_text\n",
    "    full_enc = tokenizer(full_text, return_tensors=\"pt\",\n",
    "                          add_special_tokens=True, padding=False, truncation=False)\n",
    "    full_ids = full_enc['input_ids'].to(exp_config.device)\n",
    "    sf_prefix_enc = tokenizer(sf_str, return_tensors=\"pt\",\n",
    "                               add_special_tokens=True, padding=False, truncation=False)\n",
    "    sf_prefix_len_matched = sf_prefix_enc['input_ids'].shape[1]\n",
    "    bos_id = full_ids[:, :1]\n",
    "    doc_ids = full_ids[:, sf_prefix_len_matched:]\n",
    "    doc_len = doc_ids.shape[1]\n",
    "    context_len = 1 + doc_len\n",
    "\n",
    "    del full_enc, full_ids, sf_prefix_enc\n",
    "\n",
    "    # --- Bare NLL ---\n",
    "    bare_input = torch.cat([bos_id, doc_ids], dim=1)\n",
    "    with torch.no_grad():\n",
    "        bare_out = model(input_ids=bare_input,\n",
    "                         attention_mask=torch.ones_like(bare_input),\n",
    "                         use_cache=True, return_dict=True)\n",
    "    bare_cache = _ensure_dynamic_cache(bare_out.past_key_values)\n",
    "    del bare_out\n",
    "\n",
    "    bare_nll = score_answer_with_cache(\n",
    "        deepcopy_cache(bare_cache), context_len,\n",
    "        query_prompt, answer_text, model, tokenizer, exp_config)\n",
    "\n",
    "    # --- Soft fact NLL ---\n",
    "    with torch.no_grad():\n",
    "        bos_emb = embed_fn(bos_id)\n",
    "        doc_emb = embed_fn(doc_ids)\n",
    "        soft_cast = soft_prefix_fact.to(device=exp_config.device, dtype=bos_emb.dtype)\n",
    "\n",
    "        inputs_embeds = torch.cat([bos_emb, soft_cast, doc_emb], dim=1)\n",
    "        total_len = inputs_embeds.shape[1]\n",
    "        attn_mask = torch.ones((1, total_len), device=exp_config.device, dtype=torch.long)\n",
    "\n",
    "        soft_out = model(inputs_embeds=inputs_embeds,\n",
    "                        attention_mask=attn_mask,\n",
    "                        use_cache=True, return_dict=True)\n",
    "        soft_cache = _ensure_dynamic_cache(soft_out.past_key_values)\n",
    "        del soft_out\n",
    "\n",
    "        soft_trunc = extract_and_truncate_cache_with_bos(soft_cache, doc_len)\n",
    "        del soft_cache\n",
    "\n",
    "        vel_soft_cache = replace_values_at_layers(bare_cache, soft_trunc, layer_indices)\n",
    "        del soft_trunc\n",
    "\n",
    "        soft_fact_nll = score_answer_with_cache(\n",
    "            deepcopy_cache(vel_soft_cache), context_len,\n",
    "            query_prompt, answer_text, model, tokenizer, exp_config)\n",
    "        del vel_soft_cache\n",
    "\n",
    "    del bare_cache, bare_input\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    fresh_results.append({\n",
    "        'query_idx': qidx,\n",
    "        'doc_len': doc_len,\n",
    "        'bare_nll': float(bare_nll),\n",
    "        'soft_fact_nll': float(soft_fact_nll),\n",
    "    })\n",
    "\n",
    "    # Checkpoint\n",
    "    if (qidx + 1) % CHECKPOINT_EVERY == 0 or qidx == N_FRESH - 1:\n",
    "        ckpt_data = {\n",
    "            'results': fresh_results,\n",
    "            'query_texts': [q['query'] for q in fresh_samples[:N_FRESH]],\n",
    "            'completed': len(fresh_results),\n",
    "            'total': N_FRESH,\n",
    "            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        }\n",
    "        with open(CHECKPOINT_EVAL_PATH, 'w') as f:\n",
    "            json.dump(ckpt_data, f)\n",
    "        elapsed = time.time() - t_start\n",
    "        n_done = qidx - fresh_start_idx + 1\n",
    "        rate = n_done / elapsed if elapsed > 0 else 0\n",
    "        remaining = (N_FRESH - qidx - 1) / rate if rate > 0 else 0\n",
    "        tqdm.write(f\"  Checkpoint {qidx+1}/{N_FRESH} | {n_done} done in {elapsed/60:.1f}m | \"\n",
    "                   f\"ETA: {remaining/60:.1f} min\")\n",
    "\n",
    "elapsed_total = time.time() - t_start\n",
    "print(f\"\\nScoring complete: {len(fresh_results)} queries in {elapsed_total/60:.1f} min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Apply threshold candidates to fresh data\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"GATED METRICS ON FRESH DATA\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "fresh_bare = np.array([r['bare_nll'] for r in fresh_results])\n",
    "fresh_soft = np.array([r['soft_fact_nll'] for r in fresh_results])\n",
    "\n",
    "# Filter valid\n",
    "fresh_valid = (\n",
    "    (fresh_bare != 0) & np.isfinite(fresh_bare) &\n",
    "    (fresh_soft != 0) & np.isfinite(fresh_soft)\n",
    ")\n",
    "fb = fresh_bare[fresh_valid]\n",
    "fs = fresh_soft[fresh_valid]\n",
    "n_fresh_valid = int(np.sum(fresh_valid))\n",
    "\n",
    "print(f\"Fresh valid samples: {n_fresh_valid}/{len(fresh_results)}\")\n",
    "\n",
    "# Ungated on fresh data\n",
    "fresh_ungated_delta = fb - fs\n",
    "fresh_ungated_d = cohens_d(fresh_ungated_delta)\n",
    "fresh_ungated_win = np.mean(fresh_ungated_delta > 0) * 100\n",
    "_, fresh_ungated_p = stats.ttest_1samp(fresh_ungated_delta, 0)\n",
    "\n",
    "print(f\"\\nFresh ungated: d={fresh_ungated_d:+.3f}, win={fresh_ungated_win:.1f}%, p={fresh_ungated_p:.2e}\")\n",
    "\n",
    "# Apply each candidate threshold\n",
    "print(f\"\\n{'Candidate':<12} {'Threshold':>10} {'d':>8} {'Win%':>7} {'p':>12} {'N gated':>8} {'Improve?':>10}\")\n",
    "print(\"-\" * 75)\n",
    "\n",
    "fresh_gated_results = {}\n",
    "for name, t in candidate_thresholds.items():\n",
    "    m = compute_gated_metrics(fb, fs, t)\n",
    "    improve = \"YES\" if m['cohens_d'] > fresh_ungated_d else \"no\"\n",
    "    sig = '***' if m['p_value'] < 0.001 else '**' if m['p_value'] < 0.01 else '*' if m['p_value'] < 0.05 else 'ns'\n",
    "    print(f\"{name:<12} {t:>10.4f} {m['cohens_d']:>+8.3f} {m['win_pct']:>6.1f}% \"\n",
    "          f\"{m['p_value']:>12.2e} {m['n_gated']:>8} {improve:>10}\")\n",
    "    fresh_gated_results[name] = m\n",
    "\n",
    "# Oracle gating on fresh data\n",
    "fresh_oracle_gated = np.where(fs < fb, fs, fb)\n",
    "fresh_oracle_delta = fb - fresh_oracle_gated\n",
    "fresh_oracle_d = cohens_d(fresh_oracle_delta)\n",
    "print(f\"\\nOracle gating on fresh data: d={fresh_oracle_d:+.3f}\")\n",
    "\n",
    "# Also sweep fresh data for its own optimal\n",
    "fresh_sweep = []\n",
    "for t in np.linspace(0, np.percentile(fb, 95), 100):\n",
    "    m = compute_gated_metrics(fb, fs, t)\n",
    "    fresh_sweep.append(m)\n",
    "\n",
    "fresh_best_idx = np.argmax([r['cohens_d'] for r in fresh_sweep])\n",
    "fresh_best = fresh_sweep[fresh_best_idx]\n",
    "print(f\"Fresh-optimal threshold: {fresh_best['threshold']:.4f}, d={fresh_best['cohens_d']:+.3f}\")\n",
    "print(f\"Exp25-optimal threshold: {best['threshold']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Generalization analysis\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PART 3: GENERALIZATION ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Does the Exp 25-optimal threshold generalize to fresh data?\n",
    "exp25_optimal_on_fresh = compute_gated_metrics(fb, fs, best['threshold'])\n",
    "fresh_optimal_on_fresh = fresh_best\n",
    "\n",
    "print(f\"\\nThreshold transfer analysis:\")\n",
    "print(f\"  Exp 25 optimal threshold: {best['threshold']:.4f}\")\n",
    "print(f\"  Fresh optimal threshold:  {fresh_best['threshold']:.4f}\")\n",
    "print(f\"  Difference: {abs(best['threshold'] - fresh_best['threshold']):.4f}\")\n",
    "print(f\"\")\n",
    "print(f\"  Exp25 threshold on Exp25 data: d={best['cohens_d']:+.3f}\")\n",
    "print(f\"  Exp25 threshold on fresh data: d={exp25_optimal_on_fresh['cohens_d']:+.3f}\")\n",
    "print(f\"  Fresh threshold on fresh data: d={fresh_optimal_on_fresh['cohens_d']:+.3f}\")\n",
    "print(f\"  Ungated on fresh data:         d={fresh_ungated_d:+.3f}\")\n",
    "\n",
    "# Per-quintile analysis on fresh data (gated vs ungated)\n",
    "fresh_quintile_bounds = np.percentile(fb, [20, 40, 60, 80])\n",
    "fresh_quintiles = np.digitize(fb, fresh_quintile_bounds)\n",
    "fresh_qlabels = ['Q1 easy', 'Q2', 'Q3', 'Q4', 'Q5 hard']\n",
    "\n",
    "# Use the best performing threshold from fresh_gated_results\n",
    "best_fresh_name = max(fresh_gated_results, key=lambda k: fresh_gated_results[k]['cohens_d'])\n",
    "best_fresh_t = candidate_thresholds[best_fresh_name]\n",
    "fresh_gated_best = np.where(fb >= best_fresh_t, fs, fb)\n",
    "\n",
    "print(f\"\\nBest candidate on fresh data: {best_fresh_name} (threshold={best_fresh_t:.4f})\")\n",
    "\n",
    "print(f\"\\n{'Quintile':<12} {'Ungated d':>10} {'Gated d':>10} {'Improve?':>10}\")\n",
    "print(\"-\" * 46)\n",
    "for q in range(5):\n",
    "    mask = fresh_quintiles == q\n",
    "    n_q = int(np.sum(mask))\n",
    "    if n_q < 5:\n",
    "        print(f\"{fresh_qlabels[q]:<12} {'n/a':>10} {'n/a':>10}\")\n",
    "        continue\n",
    "    \n",
    "    d_ungated = cohens_d(fb[mask] - fs[mask])\n",
    "    d_gated = cohens_d(fb[mask] - fresh_gated_best[mask])\n",
    "    improve = \"YES\" if d_gated > d_ungated else \"no\"\n",
    "    print(f\"{fresh_qlabels[q]:<12} {d_ungated:>+10.3f} {d_gated:>+10.3f} {improve:>10}\")\n",
    "\n",
    "# Bootstrap CI on fresh data\n",
    "boot_fresh_ungated = bootstrap_d(fb, fs, 0.0, seed=44)\n",
    "boot_fresh_gated = bootstrap_d(fb, fs, best_fresh_t, seed=44)\n",
    "print(f\"\\nFresh ungated d: {fresh_ungated_d:+.3f} (95% CI: [{boot_fresh_ungated['ci_low']:+.3f}, {boot_fresh_ungated['ci_high']:+.3f}])\")\n",
    "print(f\"Fresh gated d:   {fresh_gated_results[best_fresh_name]['cohens_d']:+.3f} (95% CI: [{boot_fresh_gated['ci_low']:+.3f}, {boot_fresh_gated['ci_high']:+.3f}])\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Comprehensive 6-panel visualization\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "\n",
    "# --- Panel 1: d vs threshold (Exp 25 data) ---\n",
    "ax = axes[0, 0]\n",
    "thresholds_p1 = [r['threshold'] for r in sweep_results]\n",
    "ds_p1 = [r['cohens_d'] for r in sweep_results]\n",
    "ax.plot(thresholds_p1, ds_p1, linewidth=2, color='#1f77b4', label='Exp 25 data')\n",
    "ax.axhline(y=ungated_d, color='red', linestyle='--', linewidth=1.5,\n",
    "           label=f'Ungated d={ungated_d:+.3f}')\n",
    "ax.axvline(x=best['threshold'], color='orange', linestyle='--', alpha=0.7,\n",
    "           label=f'Exp25 optimal')\n",
    "ax.set_xlabel('Bare NLL Threshold')\n",
    "ax.set_ylabel(\"Cohen's d\")\n",
    "ax.set_title('Part 1: Threshold Sweep (Exp 25)')\n",
    "ax.legend(fontsize=7)\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "# --- Panel 2: d vs threshold (fresh data) ---\n",
    "ax = axes[0, 1]\n",
    "fresh_ts = [r['threshold'] for r in fresh_sweep]\n",
    "fresh_ds = [r['cohens_d'] for r in fresh_sweep]\n",
    "ax.plot(fresh_ts, fresh_ds, linewidth=2, color='#ff7f0e', label='Fresh data')\n",
    "ax.axhline(y=fresh_ungated_d, color='red', linestyle='--', linewidth=1.5,\n",
    "           label=f'Ungated d={fresh_ungated_d:+.3f}')\n",
    "ax.axvline(x=best['threshold'], color='blue', linestyle='--', alpha=0.7,\n",
    "           label=f'Exp25 optimal t')\n",
    "ax.axvline(x=fresh_best['threshold'], color='orange', linestyle=':', alpha=0.7,\n",
    "           label=f'Fresh optimal t')\n",
    "ax.set_xlabel('Bare NLL Threshold')\n",
    "ax.set_ylabel(\"Cohen's d\")\n",
    "ax.set_title('Part 2: Threshold Sweep (Fresh)')\n",
    "ax.legend(fontsize=7)\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "# --- Panel 3: Bar chart comparing conditions ---\n",
    "ax = axes[0, 2]\n",
    "bar_names = ['Ungated\\n(Exp25)', f'Gated\\n(Exp25)', 'Ungated\\n(Fresh)', f'Gated\\n(Fresh)']\n",
    "bar_ds = [ungated_d, best['cohens_d'], fresh_ungated_d,\n",
    "          fresh_gated_results[best_fresh_name]['cohens_d']]\n",
    "bar_colors = ['#1f77b4', '#2ca02c', '#ff7f0e', '#d62728']\n",
    "\n",
    "bars = ax.bar(range(4), bar_ds, color=bar_colors, edgecolor='black', linewidth=0.5)\n",
    "ax.axhline(y=0, color='gray', linestyle='-', alpha=0.3)\n",
    "ax.set_xticks(range(4))\n",
    "ax.set_xticklabels(bar_names, fontsize=8)\n",
    "ax.set_ylabel(\"Cohen's d\")\n",
    "ax.set_title('Summary: Ungated vs Gated')\n",
    "for i, d_val in enumerate(bar_ds):\n",
    "    ax.text(i, d_val + 0.01, f\"{d_val:+.3f}\", ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# --- Panel 4: Per-quintile comparison (fresh data) ---\n",
    "ax = axes[1, 0]\n",
    "\n",
    "q_ungated_ds = []\n",
    "q_gated_ds = []\n",
    "for q in range(5):\n",
    "    mask = fresh_quintiles == q\n",
    "    if np.sum(mask) >= 5:\n",
    "        q_ungated_ds.append(cohens_d(fb[mask] - fs[mask]))\n",
    "        q_gated_ds.append(cohens_d(fb[mask] - fresh_gated_best[mask]))\n",
    "    else:\n",
    "        q_ungated_ds.append(0)\n",
    "        q_gated_ds.append(0)\n",
    "\n",
    "x = np.arange(5)\n",
    "width = 0.35\n",
    "ax.bar(x - width/2, q_ungated_ds, width, label='Ungated', color='#1f77b4', alpha=0.7)\n",
    "ax.bar(x + width/2, q_gated_ds, width, label='Gated', color='#2ca02c', alpha=0.7)\n",
    "ax.axhline(y=0, color='gray', linestyle='-', alpha=0.5)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(fresh_qlabels, fontsize=8)\n",
    "ax.set_ylabel(\"Cohen's d\")\n",
    "ax.set_title('Per-Quintile: Ungated vs Gated (Fresh)')\n",
    "ax.legend(fontsize=8)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# --- Panel 5: Scatter bare NLL vs delta (fresh data) ---\n",
    "ax = axes[1, 1]\n",
    "delta_fresh = fb - fs\n",
    "ax.scatter(fb, delta_fresh, alpha=0.4, s=20, c='#ff7f0e', edgecolors='none')\n",
    "ax.axhline(y=0, color='gray', linestyle='-', alpha=0.5)\n",
    "ax.axvline(x=best_fresh_t, color='green', linestyle='--', linewidth=2,\n",
    "           label=f'threshold={best_fresh_t:.3f}')\n",
    "ax.set_xlabel('Bare NLL (difficulty)')\n",
    "ax.set_ylabel('Delta (bare - soft, positive = helps)')\n",
    "ax.set_title('Fresh Data: Difficulty vs Benefit')\n",
    "ax.legend(fontsize=8)\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "# --- Panel 6: NLL distribution (fresh) ---\n",
    "ax = axes[1, 2]\n",
    "ax.hist(fb, bins=40, alpha=0.5, color='#1f77b4', label='Bare', density=True)\n",
    "ax.hist(fs, bins=40, alpha=0.5, color='#ff7f0e', label='Soft fact', density=True)\n",
    "ax.axvline(x=best_fresh_t, color='green', linestyle='--', linewidth=2,\n",
    "           label=f'Gate threshold')\n",
    "ax.set_xlabel('NLL')\n",
    "ax.set_ylabel('Density')\n",
    "ax.set_title('Fresh: NLL Distributions')\n",
    "ax.legend(fontsize=8)\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "plt.suptitle('Exp 26: Hardness-Gated Soft Prefix', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'comprehensive_plots.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"Saved to {RESULTS_DIR / 'comprehensive_plots.png'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Save results.json + CSV + final verdict\n",
    "\n",
    "# --- CSV ---\n",
    "with open(CSV_EVAL_PATH, 'w', newline='') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=[\n",
    "        'query_idx', 'doc_len', 'bare_nll', 'soft_fact_nll'])\n",
    "    writer.writeheader()\n",
    "    for r in fresh_results:\n",
    "        writer.writerow(r)\n",
    "print(f\"CSV saved: {CSV_EVAL_PATH}\")\n",
    "\n",
    "# --- Results JSON ---\n",
    "final = {\n",
    "    'experiment': 'exp26_hardness_gated_soft_prefix',\n",
    "    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'config': {\n",
    "        'model_name': MODEL_NAME,\n",
    "        'cutoff': CUTOFF,\n",
    "        'fresh_seed': FRESH_SEED,\n",
    "        'n_fresh': N_FRESH,\n",
    "        'n_fresh_valid': n_fresh_valid,\n",
    "        'soft_prefix_source': str(EXP25_SOFT_FACT),\n",
    "    },\n",
    "    'part1_exp25_data': {\n",
    "        'n_valid': n_valid,\n",
    "        'ungated_d': ungated_d,\n",
    "        'ungated_win_pct': ungated_win,\n",
    "        'ungated_p': float(ungated_p),\n",
    "        'optimal_threshold': best['threshold'],\n",
    "        'optimal_d': best['cohens_d'],\n",
    "        'oracle_d': oracle_d,\n",
    "        'candidate_thresholds': candidate_thresholds,\n",
    "        'bootstrap_ungated': boot_ungated,\n",
    "        'bootstrap_optimal': boot_optimal,\n",
    "    },\n",
    "    'part2_fresh_data': {\n",
    "        'n_valid': n_fresh_valid,\n",
    "        'ungated_d': fresh_ungated_d,\n",
    "        'ungated_win_pct': fresh_ungated_win,\n",
    "        'ungated_p': float(fresh_ungated_p),\n",
    "        'fresh_optimal_threshold': fresh_best['threshold'],\n",
    "        'fresh_optimal_d': fresh_best['cohens_d'],\n",
    "        'oracle_d': float(fresh_oracle_d),\n",
    "        'gated_results': fresh_gated_results,\n",
    "        'bootstrap_ungated': boot_fresh_ungated,\n",
    "        'bootstrap_gated': boot_fresh_gated,\n",
    "    },\n",
    "    'part3_generalization': {\n",
    "        'exp25_threshold': best['threshold'],\n",
    "        'fresh_threshold': fresh_best['threshold'],\n",
    "        'exp25_threshold_on_exp25': best['cohens_d'],\n",
    "        'exp25_threshold_on_fresh': exp25_optimal_on_fresh['cohens_d'],\n",
    "        'fresh_threshold_on_fresh': fresh_best['cohens_d'],\n",
    "        'best_candidate_name': best_fresh_name,\n",
    "        'best_candidate_threshold': candidate_thresholds[best_fresh_name],\n",
    "        'best_candidate_d': fresh_gated_results[best_fresh_name]['cohens_d'],\n",
    "    },\n",
    "    'fresh_per_query': fresh_results,\n",
    "}\n",
    "\n",
    "with open(FINAL_RESULTS_PATH, 'w') as f:\n",
    "    json.dump(final, f, indent=2)\n",
    "print(f\"Results saved: {FINAL_RESULTS_PATH} ({FINAL_RESULTS_PATH.stat().st_size / 1024:.1f} KB)\")\n",
    "\n",
    "# --- Final verdict ---\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"FINAL VERDICT — Exp 26: Hardness-Gated Soft Prefix\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nPart 1 (Exp 25 data, {n_valid} samples):\")\n",
    "print(f\"  Ungated d: {ungated_d:+.3f}\")\n",
    "print(f\"  Best gated d: {best['cohens_d']:+.3f} (threshold={best['threshold']:.4f})\")\n",
    "print(f\"  Oracle d: {oracle_d:+.3f}\")\n",
    "print(f\"\\nPart 2 (Fresh data, {n_fresh_valid} samples):\")\n",
    "print(f\"  Ungated d: {fresh_ungated_d:+.3f}\")\n",
    "print(f\"  Best gated d: {fresh_gated_results[best_fresh_name]['cohens_d']:+.3f} \"\n",
    "      f\"(threshold={candidate_thresholds[best_fresh_name]:.4f}, {best_fresh_name})\")\n",
    "print(f\"  Oracle d: {fresh_oracle_d:+.3f}\")\n",
    "\n",
    "gated_d_fresh = fresh_gated_results[best_fresh_name]['cohens_d']\n",
    "if gated_d_fresh > fresh_ungated_d + 0.02:\n",
    "    print(f\"\\nVERDICT: Hardness gating IMPROVES over ungated soft prefix \")\n",
    "    print(f\"  ({gated_d_fresh:+.3f} vs {fresh_ungated_d:+.3f} on fresh data).\")\n",
    "    print(f\"  Gating avoids easy-query harm while preserving hard-query benefit.\")\n",
    "elif gated_d_fresh > fresh_ungated_d - 0.02:\n",
    "    print(f\"\\nVERDICT: Hardness gating MATCHES ungated soft prefix \")\n",
    "    print(f\"  ({gated_d_fresh:+.3f} vs {fresh_ungated_d:+.3f} on fresh data).\")\n",
    "    print(f\"  Gating does not significantly change overall effect.\")\n",
    "else:\n",
    "    print(f\"\\nVERDICT: Hardness gating HURTS vs ungated soft prefix \")\n",
    "    print(f\"  ({gated_d_fresh:+.3f} vs {fresh_ungated_d:+.3f} on fresh data).\")\n",
    "    print(f\"  Excluding easy queries reduces total positive signal.\")\n",
    "\n",
    "exp25_transfers = abs(exp25_optimal_on_fresh['cohens_d'] - fresh_best['cohens_d']) < 0.05\n",
    "print(f\"\\nThreshold generalizes: {'YES' if exp25_transfers else 'NO'}\")\n",
    "print(f\"  (Exp25 optimal on fresh: d={exp25_optimal_on_fresh['cohens_d']:+.3f}, \"\n",
    "      f\"Fresh optimal: d={fresh_best['cohens_d']:+.3f})\")\n",
    "\n",
    "print(f\"\\nDone!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13: GPU cleanup\n",
    "\n",
    "print(\"Cleaning up GPU memory...\")\n",
    "mem_before = torch.cuda.memory_allocated() / 1e9\n",
    "\n",
    "del model\n",
    "del tokenizer\n",
    "if 'soft_prefix_fact' in dir():\n",
    "    del soft_prefix_fact\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "mem_after = torch.cuda.memory_allocated() / 1e9\n",
    "print(f\"GPU memory: {mem_before:.2f} GB -> {mem_after:.2f} GB\")\n",
    "print(\"Cleanup complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}