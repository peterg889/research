{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 32: Ad-Serving Pipeline",
    "## Signal Fusion, Generation Quality, and Commercial Domain Evaluation",
    "",
    "Three-part experiment combining our strongest findings into an ad-serving pipeline.",
    "",
    "### Part A -- AL+QL Signal Fusion for Ranking",
    "Exp 31 showed answer-likelihood (AL) and query-likelihood (QL) are nearly uncorrelated",
    "(r=0.111). We test whether combining these orthogonal signals improves ranking beyond",
    "either alone. **Uses existing Exp 31 data -- no new model inference needed.**",
    "",
    "### Part B -- Generation Quality with Primed Caches",
    "We've measured NLL improvement obsessively but never measured what it means for generated",
    "text. Does d=+0.35 NLL improvement translate to measurably better generated answers?",
    "If priming improves generation quality, the research has immediate ad-serving value.",
    "",
    "### Part C -- Commercial Domain (Amazon ESCI / MS MARCO Commercial)",
    "MS MARCO is an information retrieval dataset. Real ad serving involves commercial queries",
    "and product descriptions. We test whether our findings transfer to commercial domains",
    "where product descriptions (~30-150 words) fall squarely in the priming sweet spot.",
    "",
    "### Success Criteria",
    "- **Part A**: Fusion AUC > 0.841 (PMI-AL alone). Even +0.005 is meaningful.",
    "- **Part B**: Primed generation has higher Token F1 / contains-answer rate than bare.",
    "- **Part C**: QL performs better on diverse commercial pools than on MS MARCO (AUC > 0.60).",
    ""
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 2: Imports and setup\n",
    "import os\n",
    "os.umask(0o000)\n",
    "\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "import gc\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from scipy import stats\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "sys.path.insert(0, \".\")\n",
    "\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "RESULTS_DIR = Path(\"results/exp32\")\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Results dir: {RESULTS_DIR}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    gpu = torch.cuda.get_device_properties(0)\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {gpu.total_memory / 1e9:.1f} GB\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 3: Load Gemma 3 4B\n",
    "from lib.config import ExperimentConfig\n",
    "from lib.model_utils import load_model\n",
    "from lib.kv_cache import (\n",
    "    _get_text_config, _get_head_dim,\n",
    "    _get_cache_keys, _get_cache_values,\n",
    "    _set_cache_keys, _set_cache_values,\n",
    "    _ensure_dynamic_cache, deepcopy_cache,\n",
    "    extract_and_truncate_cache_with_bos,\n",
    "    correct_rope_positions_with_bos,\n",
    "    replace_values_at_layers,\n",
    "    score_answer_with_cache,\n",
    ")\n",
    "from lib.analysis import cohens_d\n",
    "from lib.data import count_words\n",
    "\n",
    "MODEL_NAME = \"google/gemma-3-4b-it\"\n",
    "config = ExperimentConfig(\n",
    "    model_name=MODEL_NAME, model_type=\"gemma3\",\n",
    "    compute_dtype=\"auto\", use_4bit=True, seed=SEED,\n",
    ")\n",
    "\n",
    "print(f\"\\nLoading Gemma 3 4B...\")\n",
    "model, tokenizer = load_model(config)\n",
    "\n",
    "text_config = _get_text_config(model.config)\n",
    "NUM_LAYERS = text_config.num_hidden_layers\n",
    "HIDDEN_SIZE = text_config.hidden_size\n",
    "DEVICE = config.device\n",
    "\n",
    "print(f\"Model loaded. dtype={next(model.parameters()).dtype}\")\n",
    "print(f\"GPU memory: {torch.cuda.memory_allocated()/1e9:.2f} GB\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 4: Load soft prefix and define helper functions\n",
    "CUTOFF = 16  # layers 0-15\n",
    "QUERY_TEMPLATE = \"\\nQuery: {query}\\nAnswer:\"\n",
    "ANSWER_TEMPLATE = \" {answer}\"\n",
    "QL_NEWLINE_SEP = \"\\n\"\n",
    "QL_SEARCH_SEP = \"\\nSearch query: \"\n",
    "\n",
    "# --- Load soft prefix from Exp 25 ---\n",
    "SOFT_PREFIX_PATH = Path(\"results/exp25/soft_prefix_fact.pt\")\n",
    "USE_SOFT = False\n",
    "\n",
    "if SOFT_PREFIX_PATH.exists():\n",
    "    soft_prefix_raw = torch.load(SOFT_PREFIX_PATH, map_location=DEVICE, weights_only=True)\n",
    "    if soft_prefix_raw.dim() == 3:\n",
    "        soft_prefix_embeds = soft_prefix_raw.squeeze(0)  # (11, 2560)\n",
    "    else:\n",
    "        soft_prefix_embeds = soft_prefix_raw\n",
    "    PREFIX_LEN = soft_prefix_embeds.shape[0]\n",
    "    USE_SOFT = True\n",
    "    print(f\"Loaded soft prefix: shape={soft_prefix_embeds.shape}, dtype={soft_prefix_embeds.dtype}\")\n",
    "else:\n",
    "    print(\"Soft prefix not found -- using discrete 'static_fact' prefix\")\n",
    "    PREFIX_TEXT = \"What are the key facts I need to know?\\n\"\n",
    "    prefix_ids = tokenizer(PREFIX_TEXT, return_tensors=\"pt\",\n",
    "                           add_special_tokens=False).input_ids.to(DEVICE)\n",
    "    embed_fn = model.get_input_embeddings()\n",
    "    with torch.no_grad():\n",
    "        soft_prefix_embeds = embed_fn(prefix_ids).squeeze(0).float()\n",
    "    PREFIX_LEN = soft_prefix_embeds.shape[0]\n",
    "    print(f\"Discrete prefix: '{PREFIX_TEXT.strip()}' -> {PREFIX_LEN} tokens\")\n",
    "\n",
    "print(f\"Prefix length: {PREFIX_LEN}, Cutoff: {CUTOFF} (layers 0-{CUTOFF-1})\")\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# Helper: Build bare cache\n",
    "# ================================================================\n",
    "def build_bare_cache(passage_text):\n",
    "    '''Build bare KV cache from passage text.'''\n",
    "    ids = tokenizer(passage_text, return_tensors=\"pt\",\n",
    "                    add_special_tokens=True, padding=False, truncation=False\n",
    "                    ).input_ids.to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        out = model(input_ids=ids, attention_mask=torch.ones_like(ids),\n",
    "                    use_cache=True, return_dict=True)\n",
    "    cache = _ensure_dynamic_cache(out.past_key_values)\n",
    "    length = ids.shape[1]\n",
    "    del out, ids\n",
    "    return cache, length\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# Helper: Build primed cache (layer-selective values)\n",
    "# ================================================================\n",
    "def build_primed_cache(passage_text, prefix_embeds=None):\n",
    "    '''Build hybrid cache: bare keys + primed values at layers 0-CUTOFF.\n",
    "\n",
    "    Works for both soft (learned) and discrete prefixes by operating in\n",
    "    embedding space, avoiding BPE boundary issues.\n",
    "    '''\n",
    "    if prefix_embeds is None:\n",
    "        prefix_embeds = soft_prefix_embeds\n",
    "\n",
    "    # Passage embeddings\n",
    "    passage_ids = tokenizer(passage_text, return_tensors=\"pt\",\n",
    "                            add_special_tokens=True, padding=False, truncation=False\n",
    "                            ).input_ids.to(DEVICE)\n",
    "    passage_len = passage_ids.shape[1]\n",
    "    embed_layer = model.get_input_embeddings()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        passage_embs = embed_layer(passage_ids)\n",
    "        pf = prefix_embeds.unsqueeze(0).to(dtype=passage_embs.dtype, device=DEVICE)\n",
    "\n",
    "        # [prefix_embeds, BOS + passage_embeds]\n",
    "        full_embeds = torch.cat([pf, passage_embs], dim=1)\n",
    "        full_mask = torch.ones(1, full_embeds.shape[1], device=DEVICE)\n",
    "\n",
    "        primed_out = model(inputs_embeds=full_embeds, attention_mask=full_mask,\n",
    "                           use_cache=True, return_dict=True)\n",
    "        primed_cache = _ensure_dynamic_cache(primed_out.past_key_values)\n",
    "\n",
    "        # Bare cache\n",
    "        bare_out = model(input_ids=passage_ids, attention_mask=torch.ones_like(passage_ids),\n",
    "                         use_cache=True, return_dict=True)\n",
    "        bare_cache = _ensure_dynamic_cache(bare_out.past_key_values)\n",
    "\n",
    "        pf_len = prefix_embeds.shape[0]\n",
    "        # Splice: bare keys + primed values (passage portion) at early layers\n",
    "        for layer_idx in range(min(CUTOFF, NUM_LAYERS)):\n",
    "            primed_v = _get_cache_values(primed_cache, layer_idx)\n",
    "            # Extract passage portion: positions pf_len to pf_len+passage_len\n",
    "            _set_cache_values(bare_cache, layer_idx,\n",
    "                              primed_v[:, :, pf_len:pf_len + passage_len, :])\n",
    "\n",
    "    del primed_cache, primed_out, bare_out, full_embeds, passage_embs, pf\n",
    "    return bare_cache, passage_len\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# Helper: Generate text with a pre-built cache\n",
    "# ================================================================\n",
    "def generate_with_cache(cache, context_len, prompt, max_new_tokens=64):\n",
    "    '''Greedy decode from a pre-built cache + prompt.'''\n",
    "    cache = deepcopy_cache(cache)\n",
    "    prompt_ids = tokenizer(prompt, return_tensors=\"pt\",\n",
    "                           add_special_tokens=False).input_ids.to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = model(input_ids=prompt_ids, past_key_values=cache,\n",
    "                    use_cache=True, return_dict=True)\n",
    "\n",
    "    generated = []\n",
    "    next_token = out.logits[:, -1, :].argmax(dim=-1, keepdim=True)\n",
    "    cache = out.past_key_values\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        tok_id = next_token.item()\n",
    "        if tok_id == tokenizer.eos_token_id:\n",
    "            break\n",
    "        generated.append(tok_id)\n",
    "        with torch.no_grad():\n",
    "            out = model(input_ids=next_token, past_key_values=cache,\n",
    "                        use_cache=True, return_dict=True)\n",
    "        next_token = out.logits[:, -1, :].argmax(dim=-1, keepdim=True)\n",
    "        cache = out.past_key_values\n",
    "\n",
    "    text = tokenizer.decode(generated, skip_special_tokens=True)\n",
    "    del cache\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# Quality metrics for generation\n",
    "# ================================================================\n",
    "def normalize_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    return ' '.join(text.split())\n",
    "\n",
    "def token_f1(prediction, ground_truth):\n",
    "    pred_tokens = normalize_text(prediction).split()\n",
    "    truth_tokens = normalize_text(ground_truth).split()\n",
    "    if not pred_tokens or not truth_tokens:\n",
    "        return 0.0\n",
    "    common = Counter(pred_tokens) & Counter(truth_tokens)\n",
    "    n_common = sum(common.values())\n",
    "    if n_common == 0:\n",
    "        return 0.0\n",
    "    precision = n_common / len(pred_tokens)\n",
    "    recall = n_common / len(truth_tokens)\n",
    "    return 2 * precision * recall / (precision + recall)\n",
    "\n",
    "def exact_match(prediction, ground_truth):\n",
    "    return float(normalize_text(prediction) == normalize_text(ground_truth))\n",
    "\n",
    "def contains_answer(prediction, ground_truth):\n",
    "    return float(normalize_text(ground_truth) in normalize_text(prediction))\n",
    "\n",
    "def passage_grounding(prediction, passage):\n",
    "    '''Fraction of prediction tokens found in the passage.'''\n",
    "    pred_tokens = set(normalize_text(prediction).split())\n",
    "    pass_tokens = set(normalize_text(passage).split())\n",
    "    if not pred_tokens:\n",
    "        return 0.0\n",
    "    return len(pred_tokens & pass_tokens) / len(pred_tokens)\n",
    "\n",
    "print(\"All helpers defined.\")\n",
    "print(f\"  build_bare_cache(), build_primed_cache()\")\n",
    "print(f\"  generate_with_cache(), token_f1(), contains_answer(), passage_grounding()\")\n",
    "print(f\"  Using {'SOFT' if USE_SOFT else 'DISCRETE'} prefix ({PREFIX_LEN} vectors)\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 5: Explain experimental conditions\n",
    "print(\"=\" * 70)\n",
    "print(\"EXPERIMENTAL CONDITIONS EXPLAINED\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print('''\n",
    "======================================================================\n",
    "PART A -- AL+QL SIGNAL FUSION (analysis of Exp 31 data)\n",
    "======================================================================\n",
    "Exp 31 scored 1692 passages across 200 queries with 6 methods:\n",
    "  - Raw AL:  NLL(answer | passage + query_template)     AUC=0.828\n",
    "  - PMI AL:  Raw AL - NLL(answer | BOS + query_template) AUC=0.841\n",
    "  - Raw QL:  NLL(query | passage + \"\\\\n\")                 AUC=0.578\n",
    "  - PMI QL:  Raw QL - NLL(query | BOS + \"\\\\n\")            AUC=0.561\n",
    "  - Raw QL-s: NLL(query | passage + \"\\\\nSearch query: \")  AUC=0.593\n",
    "  - PMI QL-s: Raw QL-s - baseline                        AUC=0.568\n",
    "\n",
    "Key finding: AL-QL correlation r=0.111 (nearly orthogonal).\n",
    "Hypothesis: Combining orthogonal signals should beat either alone.\n",
    "\n",
    "Methods:\n",
    "  1. Linear: score = alpha * PMI_AL + (1-alpha) * PMI_QL, grid-search alpha\n",
    "  2. LogReg: sklearn LogisticRegression on all 6 features, 5-fold CV by query\n",
    "  3. Rank fusion: average rank positions from AL and QL rankings\n",
    "\n",
    "======================================================================\n",
    "PART B -- GENERATION QUALITY (MS MARCO, N=100 queries)\n",
    "======================================================================\n",
    "For each query with a relevant passage:\n",
    "  Bare cache:   [BOS][passage]  -> score NLL + generate answer\n",
    "  Primed cache: [soft_prefix values at L0-15][passage] -> score NLL + generate answer\n",
    "\n",
    "Compare generated text quality:\n",
    "  - Token F1 vs ground-truth answer\n",
    "  - Contains-answer rate (ground truth substring in generation)\n",
    "  - Passage grounding (fraction of generated tokens from passage)\n",
    "  - NLL of ground-truth answer (reference)\n",
    "\n",
    "Key hypothesis: NLL improvement -> better passage-grounded generation.\n",
    "\n",
    "======================================================================\n",
    "PART C -- COMMERCIAL DOMAIN\n",
    "======================================================================\n",
    "Test 1: MS MARCO commercial vs informational query split\n",
    "  - Filter queries by commercial-intent keywords\n",
    "  - Compare scoring/ranking performance on each subset\n",
    "  - Tests whether the commercial domain is easier/harder for our methods\n",
    "\n",
    "Test 2: Amazon ESCI (Shopping Queries Dataset) if available\n",
    "  - Product descriptions as passages, search queries as queries\n",
    "  - Score: NLL(query | product_desc) [QL] and PMI versions\n",
    "  - Hypothesis: QL works better on diverse pools than MS MARCOs\n",
    "    topically homogeneous retrieval set (Exp 31: AUC=0.59)\n",
    "''')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 6: PART A -- AL+QL Signal Fusion\n",
    "print(\"=\" * 70)\n",
    "print(\"PART A: AL+QL SIGNAL FUSION FOR RANKING\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Load Exp 31 data\n",
    "exp31_csv = Path(\"results/exp31/passage_scores.csv\")\n",
    "if not exp31_csv.exists():\n",
    "    print(\"ERROR: Exp 31 passage_scores.csv not found. Skipping Part A.\")\n",
    "    fusion_results = None\n",
    "else:\n",
    "    df = pd.read_csv(exp31_csv)\n",
    "    print(f\"Loaded {len(df)} passages from Exp 31\")\n",
    "    print(f\"Columns: {list(df.columns)}\")\n",
    "    print(f\"Queries: {df['query_idx'].nunique()}\")\n",
    "    print(f\"Relevant: {df['is_relevant'].sum()} ({100*df['is_relevant'].mean():.1f}%)\")\n",
    "\n",
    "    # --- Method 1: Linear combination grid search ---\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "\n",
    "    query_ids = df['query_idx'].unique()\n",
    "    n_queries = len(query_ids)\n",
    "\n",
    "    # Cross-validated AUC for a given scoring function\n",
    "    def cv_auc(score_fn, n_folds=5):\n",
    "        '''5-fold CV by query, return mean AUC.'''\n",
    "        np.random.seed(SEED)\n",
    "        fold_ids = np.random.permutation(n_queries) % n_folds\n",
    "        aucs = []\n",
    "        for fold in range(n_folds):\n",
    "            test_queries = query_ids[fold_ids == fold]\n",
    "            test_mask = df['query_idx'].isin(test_queries)\n",
    "            test_df = df[test_mask]\n",
    "            if test_df['is_relevant'].sum() == 0 or test_df['is_relevant'].sum() == len(test_df):\n",
    "                continue\n",
    "            scores = score_fn(test_df)\n",
    "            # Lower score = more relevant for NLL-based scoring\n",
    "            aucs.append(roc_auc_score(test_df['is_relevant'], -scores))\n",
    "        return np.mean(aucs) if aucs else 0.0\n",
    "\n",
    "    # Baselines\n",
    "    auc_pmi_al = cv_auc(lambda d: d['pmi_al'].values)\n",
    "    auc_pmi_ql = cv_auc(lambda d: d['pmi_ql'].values)\n",
    "    auc_raw_al = cv_auc(lambda d: d['nll_al'].values)\n",
    "    auc_raw_ql = cv_auc(lambda d: d['nll_ql'].values)\n",
    "    auc_pmi_qls = cv_auc(lambda d: d['pmi_ql_search'].values)\n",
    "\n",
    "    print(f\"\\n--- Baseline AUCs (5-fold CV) ---\")\n",
    "    print(f\"  PMI AL:         {auc_pmi_al:.4f}\")\n",
    "    print(f\"  Raw AL:         {auc_raw_al:.4f}\")\n",
    "    print(f\"  PMI QL:         {auc_pmi_ql:.4f}\")\n",
    "    print(f\"  Raw QL:         {auc_raw_ql:.4f}\")\n",
    "    print(f\"  PMI QL-search:  {auc_pmi_qls:.4f}\")\n",
    "\n",
    "    # Grid search: alpha * PMI_AL + (1-alpha) * PMI_QL\n",
    "    alphas = np.arange(0.0, 1.01, 0.05)\n",
    "    fusion_aucs = []\n",
    "    for alpha in alphas:\n",
    "        auc = cv_auc(lambda d, a=alpha: a * d['pmi_al'].values + (1-a) * d['pmi_ql'].values)\n",
    "        fusion_aucs.append(auc)\n",
    "\n",
    "    best_alpha_idx = np.argmax(fusion_aucs)\n",
    "    best_alpha = alphas[best_alpha_idx]\n",
    "    best_fusion_auc = fusion_aucs[best_alpha_idx]\n",
    "\n",
    "    print(f\"\\n--- Linear Fusion: alpha * PMI_AL + (1-alpha) * PMI_QL ---\")\n",
    "    print(f\"  Best alpha: {best_alpha:.2f}\")\n",
    "    print(f\"  Best AUC:   {best_fusion_auc:.4f} (vs PMI_AL alone: {auc_pmi_al:.4f})\")\n",
    "    print(f\"  Delta:      {best_fusion_auc - auc_pmi_al:+.4f}\")\n",
    "\n",
    "    # Also try with QL-search\n",
    "    fusion_aucs_s = []\n",
    "    for alpha in alphas:\n",
    "        auc = cv_auc(lambda d, a=alpha: a * d['pmi_al'].values + (1-a) * d['pmi_ql_search'].values)\n",
    "        fusion_aucs_s.append(auc)\n",
    "    best_alpha_s = alphas[np.argmax(fusion_aucs_s)]\n",
    "    best_fusion_auc_s = max(fusion_aucs_s)\n",
    "\n",
    "    print(f\"\\n--- Linear Fusion: alpha * PMI_AL + (1-alpha) * PMI_QL_search ---\")\n",
    "    print(f\"  Best alpha: {best_alpha_s:.2f}\")\n",
    "    print(f\"  Best AUC:   {best_fusion_auc_s:.4f} (vs PMI_AL alone: {auc_pmi_al:.4f})\")\n",
    "    print(f\"  Delta:      {best_fusion_auc_s - auc_pmi_al:+.4f}\")\n",
    "\n",
    "    # --- Method 2: Logistic Regression (all 6 features) ---\n",
    "    try:\n",
    "        from sklearn.linear_model import LogisticRegression\n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "        from sklearn.model_selection import GroupKFold\n",
    "\n",
    "        feature_cols = ['nll_al', 'nll_ql', 'nll_ql_search', 'pmi_al', 'pmi_ql', 'pmi_ql_search']\n",
    "        X = df[feature_cols].values\n",
    "        y = df['is_relevant'].values\n",
    "        groups = df['query_idx'].values\n",
    "\n",
    "        gkf = GroupKFold(n_splits=5)\n",
    "        logreg_aucs = []\n",
    "        for train_idx, test_idx in gkf.split(X, y, groups):\n",
    "            X_train, X_test = X[train_idx], X[test_idx]\n",
    "            y_train, y_test = y[train_idx], y[test_idx]\n",
    "            if y_test.sum() == 0 or y_test.sum() == len(y_test):\n",
    "                continue\n",
    "            scaler = StandardScaler()\n",
    "            X_train_s = scaler.fit_transform(X_train)\n",
    "            X_test_s = scaler.transform(X_test)\n",
    "            clf = LogisticRegression(max_iter=1000, random_state=SEED)\n",
    "            clf.fit(X_train_s, y_train)\n",
    "            proba = clf.predict_proba(X_test_s)[:, 1]\n",
    "            logreg_aucs.append(roc_auc_score(y_test, proba))\n",
    "\n",
    "        logreg_auc = np.mean(logreg_aucs)\n",
    "\n",
    "        # Feature importance from full model\n",
    "        scaler = StandardScaler()\n",
    "        X_s = scaler.fit_transform(X)\n",
    "        clf_full = LogisticRegression(max_iter=1000, random_state=SEED)\n",
    "        clf_full.fit(X_s, y)\n",
    "        coefs = dict(zip(feature_cols, clf_full.coef_[0]))\n",
    "\n",
    "        print(f\"\\n--- Logistic Regression (6 features, 5-fold GroupKFold) ---\")\n",
    "        print(f\"  AUC: {logreg_auc:.4f} (vs PMI_AL alone: {auc_pmi_al:.4f})\")\n",
    "        print(f\"  Delta: {logreg_auc - auc_pmi_al:+.4f}\")\n",
    "        print(f\"  Feature coefficients (standardized):\")\n",
    "        for feat, coef in sorted(coefs.items(), key=lambda x: abs(x[1]), reverse=True):\n",
    "            print(f\"    {feat:<18} {coef:+.4f}\")\n",
    "    except ImportError:\n",
    "        logreg_auc = None\n",
    "        print(\"\\nsklearn not available -- skipping logistic regression\")\n",
    "\n",
    "    # --- Method 3: Rank fusion ---\n",
    "    def rank_fusion_auc():\n",
    "        '''Average rank from AL and QL within each query.'''\n",
    "        aucs = []\n",
    "        for qid in query_ids:\n",
    "            qdf = df[df['query_idx'] == qid].copy()\n",
    "            if qdf['is_relevant'].sum() == 0 or qdf['is_relevant'].sum() == len(qdf):\n",
    "                continue\n",
    "            qdf['rank_al'] = qdf['pmi_al'].rank()\n",
    "            qdf['rank_ql'] = qdf['pmi_ql'].rank()\n",
    "            qdf['rank_fusion'] = (qdf['rank_al'] + qdf['rank_ql']) / 2\n",
    "            aucs.append(roc_auc_score(qdf['is_relevant'], -qdf['rank_fusion']))\n",
    "        return np.mean(aucs) if aucs else 0.0\n",
    "\n",
    "    rank_fusion = rank_fusion_auc()\n",
    "    print(f\"\\n--- Rank Fusion (avg rank from PMI_AL + PMI_QL) ---\")\n",
    "    print(f\"  AUC: {rank_fusion:.4f} (vs PMI_AL alone: {auc_pmi_al:.4f})\")\n",
    "    print(f\"  Delta: {rank_fusion - auc_pmi_al:+.4f}\")\n",
    "\n",
    "    # --- MRR@10 comparison ---\n",
    "    def compute_mrr(score_col, negate=True):\n",
    "        '''Compute MRR@10 across queries.'''\n",
    "        mrrs = []\n",
    "        for qid in query_ids:\n",
    "            qdf = df[df['query_idx'] == qid].copy()\n",
    "            scores = qdf[score_col].values if isinstance(score_col, str) else score_col(qdf)\n",
    "            if negate:\n",
    "                order = np.argsort(scores)  # lower NLL = better\n",
    "            else:\n",
    "                order = np.argsort(-scores)  # higher = better\n",
    "            relevant = qdf['is_relevant'].values\n",
    "            for rank, idx in enumerate(order[:10], 1):\n",
    "                if relevant[idx]:\n",
    "                    mrrs.append(1.0 / rank)\n",
    "                    break\n",
    "            else:\n",
    "                mrrs.append(0.0)\n",
    "        return np.mean(mrrs)\n",
    "\n",
    "    mrr_pmi_al = compute_mrr('pmi_al')\n",
    "    mrr_pmi_ql = compute_mrr('pmi_ql')\n",
    "    mrr_fusion = compute_mrr(\n",
    "        lambda d: best_alpha * d['pmi_al'].values + (1-best_alpha) * d['pmi_ql'].values,\n",
    "        negate=True)\n",
    "\n",
    "    print(f\"\\n--- MRR@10 ---\")\n",
    "    print(f\"  PMI AL:               {mrr_pmi_al:.4f}\")\n",
    "    print(f\"  PMI QL:               {mrr_pmi_ql:.4f}\")\n",
    "    print(f\"  Linear fusion (a={best_alpha:.2f}): {mrr_fusion:.4f}\")\n",
    "\n",
    "    # Summary table\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"PART A SUMMARY -- Signal Fusion Results\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"{'Method':<40} {'AUC':>8} {'vs PMI-AL':>10}\")\n",
    "    print(f\"{'-'*58}\")\n",
    "    print(f\"{'PMI AL (baseline)':<40} {auc_pmi_al:>8.4f} {'--':>10}\")\n",
    "    print(f\"{'Raw AL':<40} {auc_raw_al:>8.4f} {auc_raw_al-auc_pmi_al:>+10.4f}\")\n",
    "    print(f\"{'PMI QL':<40} {auc_pmi_ql:>8.4f} {auc_pmi_ql-auc_pmi_al:>+10.4f}\")\n",
    "    print(f\"{'Linear fusion (AL+QL, a={:.2f})'.format(best_alpha):<40} {best_fusion_auc:>8.4f} {best_fusion_auc-auc_pmi_al:>+10.4f}\")\n",
    "    print(f\"{'Linear fusion (AL+QL-s, a={:.2f})'.format(best_alpha_s):<40} {best_fusion_auc_s:>8.4f} {best_fusion_auc_s-auc_pmi_al:>+10.4f}\")\n",
    "    if logreg_auc is not None:\n",
    "        print(f\"{'Logistic regression (6 features)':<40} {logreg_auc:>8.4f} {logreg_auc-auc_pmi_al:>+10.4f}\")\n",
    "    print(f\"{'Rank fusion (AL+QL avg rank)':<40} {rank_fusion:>8.4f} {rank_fusion-auc_pmi_al:>+10.4f}\")\n",
    "\n",
    "    fusion_results = {\n",
    "        'auc_pmi_al': auc_pmi_al, 'auc_raw_al': auc_raw_al,\n",
    "        'auc_pmi_ql': auc_pmi_ql, 'auc_raw_ql': auc_raw_ql,\n",
    "        'best_linear_alpha': float(best_alpha),\n",
    "        'best_linear_auc': float(best_fusion_auc),\n",
    "        'best_linear_s_alpha': float(best_alpha_s),\n",
    "        'best_linear_s_auc': float(best_fusion_auc_s),\n",
    "        'logreg_auc': float(logreg_auc) if logreg_auc is not None else None,\n",
    "        'rank_fusion_auc': float(rank_fusion),\n",
    "        'mrr_pmi_al': mrr_pmi_al, 'mrr_pmi_ql': mrr_pmi_ql, 'mrr_fusion': mrr_fusion,\n",
    "        'fusion_curve': [{'alpha': float(a), 'auc': float(v)} for a, v in zip(alphas, fusion_aucs)],\n",
    "    }\n",
    "    print(f\"\\nPrimary: Fusion AUC > 0.841?  {'YES' if best_fusion_auc > 0.841 else 'NO'} (best={best_fusion_auc:.4f})\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 7: PART B -- Generation Quality\n",
    "print(\"=\" * 70)\n",
    "print(\"PART B: GENERATION QUALITY WITH PRIMED CACHES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Load MS MARCO validation (multi-passage format for relevant passages with answers)\n",
    "from datasets import load_dataset\n",
    "\n",
    "N_GEN = 100  # queries for generation eval\n",
    "MAX_NEW_TOKENS = 64\n",
    "CHECKPOINT_GEN = RESULTS_DIR / \"checkpoint_gen.json\"\n",
    "\n",
    "print(f\"Loading MS MARCO v1.1 validation (single-passage, relevant only)...\")\n",
    "ds = load_dataset(\"microsoft/ms_marco\", \"v1.1\", split=\"validation\")\n",
    "gen_samples = []\n",
    "\n",
    "for item in ds:\n",
    "    if len(gen_samples) >= N_GEN * 3:\n",
    "        break\n",
    "    passages = item.get('passages', {})\n",
    "    ptexts = passages.get('passage_text', [])\n",
    "    is_sel = passages.get('is_selected', [])\n",
    "    query = item.get('query', '')\n",
    "    answers = item.get('answers', [])\n",
    "    well_formed = item.get('wellFormedAnswers', [])\n",
    "\n",
    "    answer = None\n",
    "    if well_formed and len(well_formed) > 0 and well_formed[0] not in ('[]', ''):\n",
    "        answer = well_formed[0]\n",
    "    elif answers and len(answers) > 0 and answers[0] != 'No Answer Present.':\n",
    "        answer = answers[0]\n",
    "    if not answer:\n",
    "        continue\n",
    "\n",
    "    for pt, sel in zip(ptexts, is_sel):\n",
    "        wc = count_words(pt)\n",
    "        if sel == 1 and 30 <= wc <= 300:\n",
    "            gen_samples.append({'passage': pt, 'query': query, 'answer': answer,\n",
    "                                'word_count': wc})\n",
    "            break\n",
    "\n",
    "np.random.seed(SEED + 100)\n",
    "np.random.shuffle(gen_samples)\n",
    "gen_samples = gen_samples[:N_GEN]\n",
    "del ds\n",
    "gc.collect()\n",
    "\n",
    "print(f\"Selected {len(gen_samples)} queries with relevant passages\")\n",
    "print(f\"Word counts: mean={np.mean([s['word_count'] for s in gen_samples]):.0f}\")\n",
    "\n",
    "# Resume from checkpoint\n",
    "gen_results = []\n",
    "gen_start = 0\n",
    "if CHECKPOINT_GEN.exists():\n",
    "    ckpt = json.loads(CHECKPOINT_GEN.read_text())\n",
    "    if ckpt.get('n_total') == N_GEN and len(ckpt.get('results', [])) > 0:\n",
    "        # Verify query match\n",
    "        saved_queries = [r['query'][:50] for r in ckpt['results']]\n",
    "        current_queries = [s['query'][:50] for s in gen_samples[:len(saved_queries)]]\n",
    "        if saved_queries == current_queries:\n",
    "            gen_results = ckpt['results']\n",
    "            gen_start = len(gen_results)\n",
    "            print(f\"Resuming from checkpoint: {gen_start}/{N_GEN}\")\n",
    "\n",
    "print(f\"\\nRunning generation evaluation ({gen_start}/{N_GEN} done)...\")\n",
    "t0 = time.time()\n",
    "\n",
    "for i in tqdm(range(gen_start, N_GEN), initial=gen_start, total=N_GEN, desc=\"GenQual\"):\n",
    "    sample = gen_samples[i]\n",
    "    passage, query, answer = sample['passage'], sample['query'], sample['answer']\n",
    "    query_prompt = QUERY_TEMPLATE.format(query=query)\n",
    "    answer_text = ANSWER_TEMPLATE.format(answer=answer)\n",
    "\n",
    "    # Build caches\n",
    "    bare_cache, bare_len = build_bare_cache(passage)\n",
    "    primed_cache, primed_len = build_primed_cache(passage)\n",
    "\n",
    "    # Score NLL\n",
    "    bare_nll = score_answer_with_cache(\n",
    "        deepcopy_cache(bare_cache), bare_len,\n",
    "        query_prompt, answer_text, model, tokenizer, config)\n",
    "    primed_nll = score_answer_with_cache(\n",
    "        deepcopy_cache(primed_cache), primed_len,\n",
    "        query_prompt, answer_text, model, tokenizer, config)\n",
    "\n",
    "    # Generate answers\n",
    "    bare_gen = generate_with_cache(bare_cache, bare_len, query_prompt, MAX_NEW_TOKENS)\n",
    "    primed_gen = generate_with_cache(primed_cache, primed_len, query_prompt, MAX_NEW_TOKENS)\n",
    "\n",
    "    # Quality metrics\n",
    "    result = {\n",
    "        'query': query, 'answer': answer, 'passage_words': sample['word_count'],\n",
    "        'bare_nll': bare_nll, 'primed_nll': primed_nll,\n",
    "        'bare_gen': bare_gen, 'primed_gen': primed_gen,\n",
    "        'bare_f1': token_f1(bare_gen, answer),\n",
    "        'primed_f1': token_f1(primed_gen, answer),\n",
    "        'bare_em': exact_match(bare_gen, answer),\n",
    "        'primed_em': exact_match(primed_gen, answer),\n",
    "        'bare_contains': contains_answer(bare_gen, answer),\n",
    "        'primed_contains': contains_answer(primed_gen, answer),\n",
    "        'bare_grounding': passage_grounding(bare_gen, passage),\n",
    "        'primed_grounding': passage_grounding(primed_gen, passage),\n",
    "        'bare_gen_len': len(bare_gen.split()),\n",
    "        'primed_gen_len': len(primed_gen.split()),\n",
    "    }\n",
    "    gen_results.append(result)\n",
    "\n",
    "    del bare_cache, primed_cache\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # Checkpoint every 10\n",
    "    if (i + 1) % 10 == 0 or i == N_GEN - 1:\n",
    "        ckpt = {'n_total': N_GEN, 'results': gen_results,\n",
    "                'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')}\n",
    "        CHECKPOINT_GEN.write_text(json.dumps(ckpt))\n",
    "        elapsed = time.time() - t0\n",
    "        done = i - gen_start + 1\n",
    "        eta = (N_GEN - i - 1) * elapsed / done if done > 0 else 0\n",
    "        tqdm.write(f\"  Checkpoint {i+1}/{N_GEN} | {elapsed/60:.1f}m elapsed | ETA {eta/60:.1f}m\")\n",
    "\n",
    "elapsed = time.time() - t0\n",
    "print(f\"\\nGeneration eval complete: {len(gen_results)} queries in {elapsed/60:.1f} min\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 8: PART B -- Generation Quality Results\n",
    "print(\"=\" * 70)\n",
    "print(f\"PART B RESULTS -- Generation Quality (N={len(gen_results)})\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Aggregate metrics\n",
    "metrics = {\n",
    "    'NLL': ('bare_nll', 'primed_nll'),\n",
    "    'Token F1': ('bare_f1', 'primed_f1'),\n",
    "    'Exact Match': ('bare_em', 'primed_em'),\n",
    "    'Contains Answer': ('bare_contains', 'primed_contains'),\n",
    "    'Passage Grounding': ('bare_grounding', 'primed_grounding'),\n",
    "    'Gen Length (words)': ('bare_gen_len', 'primed_gen_len'),\n",
    "}\n",
    "\n",
    "print(f\"\\n{'Metric':<22} {'Bare':>8} {'Primed':>8} {'Delta':>8} {'p':>12} {'sig':>5}\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "gen_analysis = {}\n",
    "for metric_name, (bare_key, primed_key) in metrics.items():\n",
    "    bare_vals = np.array([r[bare_key] for r in gen_results])\n",
    "    primed_vals = np.array([r[primed_key] for r in gen_results])\n",
    "\n",
    "    bare_mean = np.mean(bare_vals)\n",
    "    primed_mean = np.mean(primed_vals)\n",
    "    delta = primed_mean - bare_mean\n",
    "\n",
    "    # For NLL, lower is better; for others, higher is better\n",
    "    if metric_name == 'NLL':\n",
    "        diff = bare_vals - primed_vals  # positive = primed better\n",
    "    elif metric_name == 'Gen Length (words)':\n",
    "        diff = primed_vals - bare_vals  # just show direction\n",
    "    else:\n",
    "        diff = primed_vals - bare_vals  # positive = primed better\n",
    "\n",
    "    if np.std(diff) > 0:\n",
    "        t_stat, p_val = stats.ttest_1samp(diff, 0)\n",
    "    else:\n",
    "        t_stat, p_val = 0.0, 1.0\n",
    "    sig = '***' if p_val < 0.001 else '**' if p_val < 0.01 else '*' if p_val < 0.05 else 'ns'\n",
    "\n",
    "    print(f\"{metric_name:<22} {bare_mean:>8.4f} {primed_mean:>8.4f} {delta:>+8.4f} {p_val:>12.2e} {sig:>5}\")\n",
    "    gen_analysis[metric_name] = {\n",
    "        'bare_mean': float(bare_mean), 'primed_mean': float(primed_mean),\n",
    "        'delta': float(delta), 'p_value': float(p_val),\n",
    "    }\n",
    "\n",
    "# Hardness interaction\n",
    "bare_nlls = np.array([r['bare_nll'] for r in gen_results])\n",
    "quintile_bounds = np.percentile(bare_nlls, [20, 40, 60, 80])\n",
    "quintiles = np.digitize(bare_nlls, quintile_bounds)\n",
    "\n",
    "print(f\"\\n--- Hardness Gradient (by bare NLL quintile) ---\")\n",
    "print(f\"{'Quintile':<12} {'N':>4} {'Bare F1':>10} {'Primed F1':>10} {'Delta F1':>10} {'NLL Delta':>10}\")\n",
    "print(\"-\" * 58)\n",
    "\n",
    "for q in range(5):\n",
    "    mask = quintiles == q\n",
    "    n_q = mask.sum()\n",
    "    if n_q < 3:\n",
    "        continue\n",
    "    bf1 = np.mean([gen_results[j]['bare_f1'] for j in range(len(gen_results)) if mask[j]])\n",
    "    pf1 = np.mean([gen_results[j]['primed_f1'] for j in range(len(gen_results)) if mask[j]])\n",
    "    bnll = np.mean([gen_results[j]['bare_nll'] for j in range(len(gen_results)) if mask[j]])\n",
    "    pnll = np.mean([gen_results[j]['primed_nll'] for j in range(len(gen_results)) if mask[j]])\n",
    "    qlabel = ['Q1 easy', 'Q2', 'Q3', 'Q4', 'Q5 hard'][q]\n",
    "    print(f\"{qlabel:<12} {n_q:>4} {bf1:>10.4f} {pf1:>10.4f} {pf1-bf1:>+10.4f} {bnll-pnll:>+10.4f}\")\n",
    "\n",
    "# Show example generations\n",
    "print(f\"\\n--- Example Generations (3 samples) ---\")\n",
    "for i in [0, len(gen_results)//2, len(gen_results)-1]:\n",
    "    r = gen_results[i]\n",
    "    print(f\"\\nQuery: {r['query'][:80]}\")\n",
    "    print(f\"Truth: {r['answer'][:80]}\")\n",
    "    print(f\"Bare:  {r['bare_gen'][:80]}  (F1={r['bare_f1']:.3f})\")\n",
    "    print(f\"Prime: {r['primed_gen'][:80]}  (F1={r['primed_f1']:.3f})\")\n",
    "    print(f\"NLL:   bare={r['bare_nll']:.3f}, primed={r['primed_nll']:.3f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 9: PART C -- Load Commercial Data\n",
    "print(\"=\" * 70)\n",
    "print(\"PART C: COMMERCIAL DOMAIN EVALUATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "N_COMMERCIAL = 100  # queries for commercial eval\n",
    "CHECKPOINT_COM = RESULTS_DIR / \"checkpoint_commercial.json\"\n",
    "\n",
    "# ================================================================\n",
    "# Test 1: MS MARCO commercial vs informational split\n",
    "# ================================================================\n",
    "COMMERCIAL_KEYWORDS = {\n",
    "    'buy', 'price', 'cost', 'product', 'review', 'best', 'shop', 'order',\n",
    "    'cheap', 'deal', 'discount', 'sale', 'recommend', 'brand', 'store',\n",
    "    'purchase', 'compare', 'worth', 'quality', 'rating', 'how much',\n",
    "}\n",
    "\n",
    "def is_commercial(query):\n",
    "    q = query.lower()\n",
    "    return any(kw in q for kw in COMMERCIAL_KEYWORDS)\n",
    "\n",
    "# Load multi-passage format (like Exp 31)\n",
    "print(\"Loading MS MARCO v1.1 validation (multi-passage format)...\")\n",
    "ds = load_dataset(\"microsoft/ms_marco\", \"v1.1\", split=\"validation\")\n",
    "\n",
    "commercial_queries = []\n",
    "informational_queries = []\n",
    "\n",
    "for item in ds:\n",
    "    if len(commercial_queries) >= N_COMMERCIAL and len(informational_queries) >= N_COMMERCIAL:\n",
    "        break\n",
    "    passages = item.get('passages', {})\n",
    "    ptexts = passages.get('passage_text', [])\n",
    "    is_sel = passages.get('is_selected', [])\n",
    "    query = item.get('query', '')\n",
    "\n",
    "    if not ptexts or len(ptexts) < 3:\n",
    "        continue\n",
    "\n",
    "    # Filter passages by length\n",
    "    valid_passages = []\n",
    "    for pt, sel in zip(ptexts, is_sel):\n",
    "        wc = count_words(pt)\n",
    "        if 20 <= wc <= 300:\n",
    "            valid_passages.append({'text': pt, 'is_relevant': int(sel == 1), 'words': wc})\n",
    "\n",
    "    if len(valid_passages) < 3 or not any(p['is_relevant'] for p in valid_passages):\n",
    "        continue\n",
    "\n",
    "    entry = {'query': query, 'passages': valid_passages}\n",
    "\n",
    "    if is_commercial(query) and len(commercial_queries) < N_COMMERCIAL:\n",
    "        commercial_queries.append(entry)\n",
    "    elif not is_commercial(query) and len(informational_queries) < N_COMMERCIAL:\n",
    "        informational_queries.append(entry)\n",
    "\n",
    "del ds\n",
    "gc.collect()\n",
    "\n",
    "print(f\"Commercial queries: {len(commercial_queries)}\")\n",
    "print(f\"Informational queries: {len(informational_queries)}\")\n",
    "print(f\"Example commercial: {commercial_queries[0]['query'][:60] if commercial_queries else 'N/A'}\")\n",
    "print(f\"Example informational: {informational_queries[0]['query'][:60] if informational_queries else 'N/A'}\")\n",
    "\n",
    "# ================================================================\n",
    "# Test 2: Try loading Amazon ESCI\n",
    "# ================================================================\n",
    "esci_queries = []\n",
    "esci_loaded = False\n",
    "\n",
    "for source in [\"amazon-science/esci-data\", \"smhavens/esci-s\", \"tasksource/esci\"]:\n",
    "    try:\n",
    "        print(f\"\\nTrying to load ESCI from '{source}'...\")\n",
    "        esci_ds = load_dataset(source, trust_remote_code=True)\n",
    "        # Inspect structure\n",
    "        if isinstance(esci_ds, dict):\n",
    "            print(f\"  Splits: {list(esci_ds.keys())}\")\n",
    "            split_name = list(esci_ds.keys())[0]\n",
    "            esci_split = esci_ds[split_name]\n",
    "        else:\n",
    "            esci_split = esci_ds\n",
    "\n",
    "        print(f\"  Columns: {esci_split.column_names}\")\n",
    "        print(f\"  Rows: {len(esci_split)}\")\n",
    "\n",
    "        # Try to extract query-product pairs\n",
    "        if 'query' in esci_split.column_names and 'product_title' in esci_split.column_names:\n",
    "            # Filter for US locale if available\n",
    "            if 'product_locale' in esci_split.column_names:\n",
    "                esci_split = esci_split.filter(lambda x: x.get('product_locale') == 'us')\n",
    "                print(f\"  US-only rows: {len(esci_split)}\")\n",
    "\n",
    "            # Group by query\n",
    "            from collections import defaultdict\n",
    "            query_products = defaultdict(list)\n",
    "            for row in esci_split:\n",
    "                q = row['query']\n",
    "                label = row.get('esci_label', 'I')\n",
    "                title = row.get('product_title', '')\n",
    "                desc = row.get('product_description', '') or ''\n",
    "                bullets = row.get('product_bullet_point', '') or ''\n",
    "\n",
    "                # Build passage\n",
    "                passage = title\n",
    "                if bullets:\n",
    "                    passage += \"\\n\" + bullets[:500]\n",
    "                elif desc:\n",
    "                    passage += \"\\n\" + desc[:500]\n",
    "\n",
    "                wc = count_words(passage)\n",
    "                if 5 <= wc <= 300 and title:\n",
    "                    is_rel = 1 if label in ('E', 'S') else 0\n",
    "                    query_products[q].append({\n",
    "                        'text': passage, 'is_relevant': is_rel,\n",
    "                        'words': wc, 'label': label, 'title': title\n",
    "                    })\n",
    "\n",
    "            # Filter queries with enough products and at least one relevant\n",
    "            for q, prods in query_products.items():\n",
    "                if len(prods) >= 4 and any(p['is_relevant'] for p in prods):\n",
    "                    esci_queries.append({'query': q, 'passages': prods[:15]})\n",
    "                if len(esci_queries) >= N_COMMERCIAL:\n",
    "                    break\n",
    "\n",
    "            if esci_queries:\n",
    "                esci_loaded = True\n",
    "                print(f\"  ESCI loaded: {len(esci_queries)} queries\")\n",
    "                print(f\"  Total products: {sum(len(q['passages']) for q in esci_queries)}\")\n",
    "                print(f\"  Relevant: {sum(sum(p['is_relevant'] for p in q['passages']) for q in esci_queries)}\")\n",
    "                break\n",
    "\n",
    "        del esci_ds\n",
    "    except Exception as e:\n",
    "        print(f\"  Failed: {e}\")\n",
    "        continue\n",
    "\n",
    "if not esci_loaded:\n",
    "    print(\"\\nESCI not available -- Part C will use MS MARCO commercial/informational split only.\")\n",
    "\n",
    "gc.collect()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 10: PART C -- Score commercial data\n",
    "print(\"=\" * 70)\n",
    "print(\"PART C: SCORING\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "def score_query_set(query_set, set_name, do_priming=False, max_queries=None):\n",
    "    '''Score a set of queries with their passages using QL and AL scoring.'''\n",
    "    if max_queries:\n",
    "        query_set = query_set[:max_queries]\n",
    "\n",
    "    all_scores = []\n",
    "    t0 = time.time()\n",
    "\n",
    "    for qi, qdata in enumerate(tqdm(query_set, desc=set_name)):\n",
    "        query = qdata['query']\n",
    "\n",
    "        # Baselines (once per query)\n",
    "        # BL for QL: NLL(query | BOS + \"\\n\")\n",
    "        bos_cache, bos_len = build_bare_cache(\"\")\n",
    "        bl_ql = score_answer_with_cache(\n",
    "            deepcopy_cache(bos_cache), bos_len,\n",
    "            QL_NEWLINE_SEP, query, model, tokenizer, config)\n",
    "        bl_ql_s = score_answer_with_cache(\n",
    "            deepcopy_cache(bos_cache), bos_len,\n",
    "            QL_SEARCH_SEP, query, model, tokenizer, config)\n",
    "        del bos_cache\n",
    "\n",
    "        for pi, pdata in enumerate(qdata['passages']):\n",
    "            passage = pdata['text']\n",
    "\n",
    "            # Bare cache\n",
    "            bare_cache, bare_len = build_bare_cache(passage)\n",
    "\n",
    "            # QL scores\n",
    "            nll_ql = score_answer_with_cache(\n",
    "                deepcopy_cache(bare_cache), bare_len,\n",
    "                QL_NEWLINE_SEP, query, model, tokenizer, config)\n",
    "            nll_ql_s = score_answer_with_cache(\n",
    "                deepcopy_cache(bare_cache), bare_len,\n",
    "                QL_SEARCH_SEP, query, model, tokenizer, config)\n",
    "\n",
    "            result = {\n",
    "                'query_idx': qi, 'passage_idx': pi,\n",
    "                'is_relevant': pdata['is_relevant'],\n",
    "                'word_count': pdata.get('words', count_words(passage)),\n",
    "                'nll_ql': nll_ql, 'nll_ql_search': nll_ql_s,\n",
    "                'bl_ql': bl_ql, 'bl_ql_search': bl_ql_s,\n",
    "                'pmi_ql': nll_ql - bl_ql, 'pmi_ql_search': nll_ql_s - bl_ql_s,\n",
    "            }\n",
    "\n",
    "            # Priming comparison (subset only)\n",
    "            if do_priming and qi < 50:\n",
    "                primed_cache, primed_len = build_primed_cache(passage)\n",
    "                primed_ql = score_answer_with_cache(\n",
    "                    deepcopy_cache(primed_cache), primed_len,\n",
    "                    QL_NEWLINE_SEP, query, model, tokenizer, config)\n",
    "                result['primed_ql'] = primed_ql\n",
    "                result['primed_pmi_ql'] = primed_ql - bl_ql\n",
    "                del primed_cache\n",
    "\n",
    "            del bare_cache\n",
    "            all_scores.append(result)\n",
    "\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        if (qi + 1) % 20 == 0:\n",
    "            elapsed = time.time() - t0\n",
    "            eta = (len(query_set) - qi - 1) * elapsed / (qi + 1)\n",
    "            tqdm.write(f\"  {set_name} {qi+1}/{len(query_set)} | {elapsed/60:.1f}m | ETA {eta/60:.1f}m\")\n",
    "\n",
    "    elapsed = time.time() - t0\n",
    "    print(f\"  {set_name}: {len(all_scores)} passages scored in {elapsed/60:.1f} min\")\n",
    "    return all_scores\n",
    "\n",
    "\n",
    "# Score MS MARCO commercial queries\n",
    "print(\"\\n--- Scoring MS MARCO Commercial Queries ---\")\n",
    "com_scores = score_query_set(commercial_queries, \"Commercial\", do_priming=True)\n",
    "\n",
    "# Score MS MARCO informational queries\n",
    "print(\"\\n--- Scoring MS MARCO Informational Queries ---\")\n",
    "info_scores = score_query_set(informational_queries, \"Informational\", do_priming=True)\n",
    "\n",
    "# Score ESCI (if available)\n",
    "esci_scores = None\n",
    "if esci_loaded and esci_queries:\n",
    "    print(\"\\n--- Scoring Amazon ESCI ---\")\n",
    "    esci_scores = score_query_set(esci_queries, \"ESCI\", do_priming=True)\n",
    "\n",
    "# Save checkpoint\n",
    "com_checkpoint = {\n",
    "    'commercial_scores': com_scores,\n",
    "    'informational_scores': info_scores,\n",
    "    'esci_scores': esci_scores,\n",
    "    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "}\n",
    "CHECKPOINT_COM.write_text(json.dumps(com_checkpoint))\n",
    "print(f\"\\nCheckpoint saved to {CHECKPOINT_COM}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 11: PART C -- Commercial Domain Results\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PART C RESULTS -- Commercial Domain\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "def analyze_scores(scores, set_name):\n",
    "    '''Compute AUC and MRR for a set of passage scores.'''\n",
    "    df = pd.DataFrame(scores)\n",
    "    n_passages = len(df)\n",
    "    n_queries = df['query_idx'].nunique()\n",
    "    n_relevant = df['is_relevant'].sum()\n",
    "\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"{set_name}: {n_queries} queries, {n_passages} passages, {n_relevant} relevant ({100*n_relevant/n_passages:.1f}%)\")\n",
    "    print(f\"{'='*50}\")\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    # AUC for each scoring method\n",
    "    score_cols = [c for c in ['nll_ql', 'nll_ql_search', 'pmi_ql', 'pmi_ql_search'] if c in df.columns]\n",
    "\n",
    "    print(f\"\\n{'Method':<25} {'AUC':>8} {'MRR@10':>8}\")\n",
    "    print(\"-\" * 43)\n",
    "\n",
    "    for col in score_cols:\n",
    "        try:\n",
    "            # Lower NLL = more relevant, so negate for AUC\n",
    "            auc = roc_auc_score(df['is_relevant'], -df[col])\n",
    "        except ValueError:\n",
    "            auc = 0.5\n",
    "\n",
    "        # MRR@10\n",
    "        mrrs = []\n",
    "        for qid in df['query_idx'].unique():\n",
    "            qdf = df[df['query_idx'] == qid]\n",
    "            order = np.argsort(qdf[col].values)  # lower = better\n",
    "            relevant = qdf['is_relevant'].values\n",
    "            for rank, idx in enumerate(order[:10], 1):\n",
    "                if relevant[idx]:\n",
    "                    mrrs.append(1.0 / rank)\n",
    "                    break\n",
    "            else:\n",
    "                mrrs.append(0.0)\n",
    "        mrr = np.mean(mrrs)\n",
    "\n",
    "        print(f\"{col:<25} {auc:>8.4f} {mrr:>8.4f}\")\n",
    "        results[col] = {'auc': float(auc), 'mrr': float(mrr)}\n",
    "\n",
    "    # Priming effect (if available)\n",
    "    if 'primed_ql' in df.columns:\n",
    "        primed_df = df.dropna(subset=['primed_ql'])\n",
    "        if len(primed_df) > 0:\n",
    "            bare_ql = primed_df['nll_ql'].values\n",
    "            primed_ql = primed_df['primed_ql'].values\n",
    "            delta = bare_ql - primed_ql\n",
    "            d = cohens_d(delta) if np.std(delta) > 0 else 0.0\n",
    "            win_pct = 100 * np.mean(delta > 0)\n",
    "            t_stat, p_val = stats.ttest_1samp(delta, 0) if np.std(delta) > 0 else (0, 1)\n",
    "            sig = '***' if p_val < 0.001 else '**' if p_val < 0.01 else '*' if p_val < 0.05 else 'ns'\n",
    "            print(f\"\\n  Priming effect (NLL QL): d={d:+.3f}, win%={win_pct:.0f}%, p={p_val:.2e} {sig}\")\n",
    "            print(f\"  Mean bare QL:   {np.mean(bare_ql):.4f}\")\n",
    "            print(f\"  Mean primed QL: {np.mean(primed_ql):.4f}\")\n",
    "            results['priming'] = {'d': float(d), 'win_pct': float(win_pct),\n",
    "                                  'p_value': float(p_val)}\n",
    "\n",
    "    # Differential NLL\n",
    "    rel_mask = df['is_relevant'] == 1\n",
    "    irr_mask = df['is_relevant'] == 0\n",
    "    for col in ['nll_ql', 'pmi_ql']:\n",
    "        if col in df.columns:\n",
    "            rel_mean = df.loc[rel_mask, col].mean()\n",
    "            irr_mean = df.loc[irr_mask, col].mean()\n",
    "            gap = irr_mean - rel_mean  # positive = relevant has lower NLL = good\n",
    "            print(f\"  {col} gap: relevant={rel_mean:.3f}, irrelevant={irr_mean:.3f}, gap={gap:+.3f}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# Analyze each set\n",
    "com_analysis = analyze_scores(com_scores, \"MS MARCO -- Commercial\")\n",
    "info_analysis = analyze_scores(info_scores, \"MS MARCO -- Informational\")\n",
    "\n",
    "esci_analysis = None\n",
    "if esci_scores:\n",
    "    esci_analysis = analyze_scores(esci_scores, \"Amazon ESCI\")\n",
    "\n",
    "# Comparison table\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"PART C SUMMARY -- Cross-Domain Comparison\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"{'Dataset':<30} {'QL AUC':>8} {'PMI QL AUC':>10} {'Exp31 ref':>10}\")\n",
    "print(f\"{'-'*60}\")\n",
    "\n",
    "ql_ref = 0.578  # from Exp 31\n",
    "pmi_ql_ref = 0.561\n",
    "\n",
    "for name, analysis in [(\"MS MARCO Commercial\", com_analysis),\n",
    "                        (\"MS MARCO Informational\", info_analysis),\n",
    "                        (\"Amazon ESCI\", esci_analysis)]:\n",
    "    if analysis is None:\n",
    "        continue\n",
    "    ql_auc = analysis.get('nll_ql', {}).get('auc', 0)\n",
    "    pmi_auc = analysis.get('pmi_ql', {}).get('auc', 0)\n",
    "    print(f\"{name:<30} {ql_auc:>8.4f} {pmi_auc:>10.4f} {ql_ref:>10.3f}\")\n",
    "\n",
    "print(f\"\\nExp 31 references (full MS MARCO):\")\n",
    "print(f\"  Raw QL AUC:  {ql_ref}\")\n",
    "print(f\"  PMI QL AUC:  {pmi_ql_ref}\")\n",
    "print(f\"  PMI AL AUC:  0.841\")\n",
    "\n",
    "commercial_results = {\n",
    "    'commercial': com_analysis, 'informational': info_analysis,\n",
    "    'esci': esci_analysis, 'esci_loaded': esci_loaded,\n",
    "}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 12: Final Verdict and Save\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Save plots ---\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Panel 1: Fusion alpha curve\n",
    "if fusion_results and 'fusion_curve' in fusion_results:\n",
    "    ax = axes[0, 0]\n",
    "    alphas_plot = [p['alpha'] for p in fusion_results['fusion_curve']]\n",
    "    aucs_plot = [p['auc'] for p in fusion_results['fusion_curve']]\n",
    "    ax.plot(alphas_plot, aucs_plot, 'b-o', markersize=3, label='Linear fusion')\n",
    "    ax.axhline(y=fusion_results['auc_pmi_al'], color='r', linestyle='--',\n",
    "               label=f\"PMI-AL alone ({fusion_results['auc_pmi_al']:.4f})\")\n",
    "    ax.set_xlabel('Alpha (weight on PMI-AL)')\n",
    "    ax.set_ylabel('AUC-ROC (5-fold CV)')\n",
    "    ax.set_title('Part A: Signal Fusion')\n",
    "    ax.legend(fontsize=8)\n",
    "\n",
    "# Panel 2: Generation quality comparison\n",
    "ax = axes[0, 1]\n",
    "if gen_results:\n",
    "    bare_f1s = [r['bare_f1'] for r in gen_results]\n",
    "    primed_f1s = [r['primed_f1'] for r in gen_results]\n",
    "    ax.scatter(bare_f1s, primed_f1s, alpha=0.4, s=20, edgecolors='none')\n",
    "    ax.plot([0, 1], [0, 1], 'k--', alpha=0.5)\n",
    "    ax.set_xlabel('Bare Token F1')\n",
    "    ax.set_ylabel('Primed Token F1')\n",
    "    ax.set_title(f'Part B: Generation Quality (N={len(gen_results)})')\n",
    "    wins = sum(1 for b, p in zip(bare_f1s, primed_f1s) if p > b)\n",
    "    losses = sum(1 for b, p in zip(bare_f1s, primed_f1s) if p < b)\n",
    "    ax.text(0.05, 0.95, f\"Primed wins: {wins}\\nBare wins: {losses}\",\n",
    "            transform=ax.transAxes, va='top', fontsize=9,\n",
    "            bbox=dict(facecolor='wheat', alpha=0.5))\n",
    "\n",
    "# Panel 3: QL AUC across domains\n",
    "ax = axes[1, 0]\n",
    "domains = []\n",
    "ql_aucs = []\n",
    "for name, analysis in [(\"MARCO\\n(Exp31)\", {'nll_ql': {'auc': 0.578}}),\n",
    "                        (\"Commercial\", com_analysis),\n",
    "                        (\"Informational\", info_analysis)]:\n",
    "    if analysis:\n",
    "        domains.append(name)\n",
    "        ql_aucs.append(analysis.get('nll_ql', {}).get('auc', 0))\n",
    "if esci_analysis:\n",
    "    domains.append(\"ESCI\")\n",
    "    ql_aucs.append(esci_analysis.get('nll_ql', {}).get('auc', 0))\n",
    "\n",
    "colors = ['gray'] + ['#2ca02c' if a > 0.65 else '#ff7f0e' if a > 0.55 else '#d62728'\n",
    "                      for a in ql_aucs[1:]]\n",
    "ax.bar(range(len(domains)), ql_aucs, color=colors, edgecolor='black', linewidth=0.5)\n",
    "ax.set_xticks(range(len(domains)))\n",
    "ax.set_xticklabels(domains, fontsize=9)\n",
    "ax.set_ylabel('AUC-ROC')\n",
    "ax.set_title('Part C: QL Ranking Across Domains')\n",
    "ax.axhline(y=0.5, color='gray', linestyle=':', alpha=0.5, label='chance')\n",
    "ax.legend(fontsize=8)\n",
    "\n",
    "# Panel 4: NLL improvement from priming (generation)\n",
    "ax = axes[1, 1]\n",
    "if gen_results:\n",
    "    bare_nlls = [r['bare_nll'] for r in gen_results]\n",
    "    primed_nlls = [r['primed_nll'] for r in gen_results]\n",
    "    deltas = [b - p for b, p in zip(bare_nlls, primed_nlls)]\n",
    "    ax.hist(deltas, bins=30, color='#1f77b4', alpha=0.7, edgecolor='black', linewidth=0.5)\n",
    "    ax.axvline(x=0, color='red', linestyle='--')\n",
    "    ax.axvline(x=np.mean(deltas), color='green', linestyle='-',\n",
    "               label=f'Mean={np.mean(deltas):.3f}')\n",
    "    ax.set_xlabel('NLL Delta (bare - primed, positive = primed helps)')\n",
    "    ax.set_ylabel('Count')\n",
    "    ax.set_title('Part B: NLL Improvement Distribution')\n",
    "    ax.legend(fontsize=8)\n",
    "\n",
    "plt.suptitle('Experiment 32: Ad-Serving Pipeline', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'exp32_plots.png', dpi=150, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(f\"Saved plots to {RESULTS_DIR / 'exp32_plots.png'}\")\n",
    "\n",
    "# --- Save all results ---\n",
    "final_results = {\n",
    "    'experiment': 'exp32_ad_serving_pipeline',\n",
    "    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'model': MODEL_NAME,\n",
    "    'part_a_fusion': fusion_results,\n",
    "    'part_b_generation': {\n",
    "        'n_queries': len(gen_results),\n",
    "        'analysis': gen_analysis,\n",
    "        'use_soft_prefix': USE_SOFT,\n",
    "        'prefix_len': PREFIX_LEN,\n",
    "        'cutoff': CUTOFF,\n",
    "    },\n",
    "    'part_c_commercial': commercial_results,\n",
    "}\n",
    "\n",
    "with open(RESULTS_DIR / 'results.json', 'w') as f:\n",
    "    json.dump(final_results, f, indent=2)\n",
    "print(f\"Results saved to {RESULTS_DIR / 'results.json'}\")\n",
    "\n",
    "# --- CSV for generation results ---\n",
    "gen_csv = RESULTS_DIR / 'generation_scores.csv'\n",
    "with open(gen_csv, 'w', newline='') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=[\n",
    "        'query', 'bare_nll', 'primed_nll', 'bare_f1', 'primed_f1',\n",
    "        'bare_contains', 'primed_contains', 'bare_grounding', 'primed_grounding'])\n",
    "    writer.writeheader()\n",
    "    for r in gen_results:\n",
    "        writer.writerow({k: r[k] for k in writer.fieldnames})\n",
    "print(f\"Generation CSV saved: {gen_csv}\")\n",
    "\n",
    "# ================================================================\n",
    "# FINAL VERDICT\n",
    "# ================================================================\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"FINAL VERDICT -- Exp 32: Ad-Serving Pipeline\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\nModel: Gemma 3 4B | Prefix: {'soft (Exp25)' if USE_SOFT else 'discrete'} ({PREFIX_LEN} vectors)\")\n",
    "\n",
    "# Part A\n",
    "print(f\"\\n--- Part A: Signal Fusion ---\")\n",
    "if fusion_results:\n",
    "    best_auc = max(fusion_results.get('best_linear_auc', 0),\n",
    "                   fusion_results.get('logreg_auc', 0) or 0,\n",
    "                   fusion_results.get('rank_fusion_auc', 0))\n",
    "    ref_auc = fusion_results['auc_pmi_al']\n",
    "    verdict_a = \"YES\" if best_auc > ref_auc + 0.003 else \"MARGINAL\" if best_auc > ref_auc else \"NO\"\n",
    "    print(f\"  Fusion beats PMI-AL?  {verdict_a} (best={best_auc:.4f} vs ref={ref_auc:.4f})\")\n",
    "else:\n",
    "    print(f\"  SKIPPED (Exp 31 data not available)\")\n",
    "\n",
    "# Part B\n",
    "print(f\"\\n--- Part B: Generation Quality ---\")\n",
    "if gen_analysis:\n",
    "    f1_delta = gen_analysis.get('Token F1', {}).get('delta', 0)\n",
    "    contains_delta = gen_analysis.get('Contains Answer', {}).get('delta', 0)\n",
    "    nll_delta = gen_analysis.get('NLL', {}).get('delta', 0)\n",
    "    f1_p = gen_analysis.get('Token F1', {}).get('p_value', 1)\n",
    "    verdict_b = \"YES\" if f1_delta > 0 and f1_p < 0.05 else \"TRENDING\" if f1_delta > 0 else \"NO\"\n",
    "    print(f\"  Priming improves generation?  {verdict_b}\")\n",
    "    print(f\"  Token F1 delta: {f1_delta:+.4f} (p={f1_p:.3e})\")\n",
    "    print(f\"  Contains-answer delta: {contains_delta:+.4f}\")\n",
    "    print(f\"  NLL delta: {nll_delta:+.4f}\")\n",
    "\n",
    "# Part C\n",
    "print(f\"\\n--- Part C: Commercial Domain ---\")\n",
    "if com_analysis:\n",
    "    com_ql = com_analysis.get('nll_ql', {}).get('auc', 0)\n",
    "    info_ql = info_analysis.get('nll_ql', {}).get('auc', 0)\n",
    "    print(f\"  Commercial QL AUC:     {com_ql:.4f}\")\n",
    "    print(f\"  Informational QL AUC:  {info_ql:.4f}\")\n",
    "    print(f\"  Exp 31 full QL AUC:    0.578\")\n",
    "    if esci_analysis:\n",
    "        esci_ql = esci_analysis.get('nll_ql', {}).get('auc', 0)\n",
    "        print(f\"  ESCI QL AUC:           {esci_ql:.4f}\")\n",
    "        print(f\"  QL better on diverse pool? {'YES' if esci_ql > 0.65 else 'NO'} (ESCI={esci_ql:.3f} vs MARCO=0.578)\")\n",
    "\n",
    "print(f\"\\nDone!\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 13: Cleanup\n",
    "print(\"Cleaning up GPU memory...\")\n",
    "mem_before = torch.cuda.memory_allocated() / 1e9\n",
    "\n",
    "del model, tokenizer\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "mem_after = torch.cuda.memory_allocated() / 1e9\n",
    "print(f\"\\nGPU memory: {mem_before:.2f} GB -> {mem_after:.2f} GB\")\n",
    "print(\"Cleanup complete.\")"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}