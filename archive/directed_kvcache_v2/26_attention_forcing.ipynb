{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exp 26: Attention Forcing for Long Documents\n",
    "\n",
    "## Motivation\n",
    "\n",
    "Experiments 11 and 20 proved that **value contamination fails on documents longer than 256 tokens**.\n",
    "The prefix signal is diluted because attention weights for the prefix become infinitesimally small\n",
    "as document length grows. Experiment 12 attempted to fix this by repeating the prefix 20 times,\n",
    "but it failed \u2014 the model's attention mechanism naturally learns to ignore redundant tokens.\n",
    "\n",
    "## Core Question\n",
    "\n",
    "If we bypass the model's natural attention decay and **mathematically force** document tokens to\n",
    "pay attention to the prefix during cache generation, can we recover the priming benefit?\n",
    "\n",
    "## Theoretical Mechanism\n",
    "\n",
    "In SDPA, the attention scores (pre-softmax) determine how much information flows from previous tokens\n",
    "into the current token's representation. Normally, we pass a boolean causal mask (0 for visible,\n",
    "-inf for masked). Instead, we pass a **float-based attention mask** with a positive logit bias\n",
    "(e.g., +5.0) exclusively at the intersection of document token queries and prefix token keys.\n",
    "\n",
    "This forces every document token to aggressively mix prefix semantics into its value vector,\n",
    "artificially counteracting long-document dilution.\n",
    "\n",
    "## Conditions (on 1024-token padded MS MARCO)\n",
    "\n",
    "| Condition | Description |\n",
    "|-----------|-------------|\n",
    "| `bare` | BOS + doc cache (control) |\n",
    "| `bias_0.0` | Standard priming, no bias (failure baseline, expected d \u2248 0) |\n",
    "| `bias_2.0` | +2.0 logit bias on doc\u2192prefix attention |\n",
    "| `bias_5.0` | +5.0 logit bias on doc\u2192prefix attention |\n",
    "| `bias_10.0` | +10.0 logit bias on doc\u2192prefix attention |\n",
    "\n",
    "## Reference Values\n",
    "\n",
    "| Source | Condition | d |\n",
    "|--------|-----------|---|\n",
    "| Exp 20 (Mistral) | full priming @ original (~130 tok) | +0.303 |\n",
    "| Exp 20 (Mistral) | full priming @ 256 tok | +0.114 (ns) |\n",
    "| Exp 20 (Mistral) | full priming @ 1024 tok | -0.043 (ns) |\n",
    "\n",
    "## Success Criteria\n",
    "\n",
    "1. Does any bias level recover a positive Cohen's d (> +0.20) at 1024 tokens?\n",
    "2. At what bias does the document representation corrupt (NLL explodes)?\n",
    "3. What is the optimal bias on the tuning curve?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 1: Setup\n",
    "import os\n",
    "os.umask(0o000)\n",
    "\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import gc\n",
    "from pathlib import Path\n",
    "\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "RESULTS_DIR = Path(\"results/exp26\")\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CHECKPOINT_PATH = RESULTS_DIR / \"checkpoint.json\"\n",
    "FINAL_RESULTS_PATH = RESULTS_DIR / \"results.json\"\n",
    "CSV_PATH = RESULTS_DIR / \"results.csv\"\n",
    "\n",
    "print(f\"SEED: {SEED}\")\n",
    "print(f\"Results directory: {RESULTS_DIR}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 2: Load Mistral 7B via load_model()\n",
    "sys.path.insert(0, \".\")\n",
    "\n",
    "from lib.config import ExperimentConfig\n",
    "from lib.model_utils import load_model\n",
    "\n",
    "MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "exp_config = ExperimentConfig(\n",
    "    model_name=MODEL_NAME,\n",
    "    model_type=\"mistral\",\n",
    "    compute_dtype=\"auto\",  # resolves to float16 for Mistral\n",
    "    use_4bit=True,\n",
    "    num_samples=2000,\n",
    "    seed=SEED,\n",
    ")\n",
    "\n",
    "print(f\"Loading {MODEL_NAME} (4-bit, float16)...\")\n",
    "model, tokenizer = load_model(exp_config)\n",
    "\n",
    "from lib.kv_cache import _get_text_config, _get_head_dim, _ensure_dynamic_cache, _get_cache_keys\n",
    "\n",
    "text_config = _get_text_config(model.config)\n",
    "print(f\"\\nModel loaded successfully.\")\n",
    "print(f\"  Model class: {type(model).__name__}\")\n",
    "print(f\"  Hidden size: {text_config.hidden_size}\")\n",
    "print(f\"  Num layers: {text_config.num_hidden_layers}\")\n",
    "print(f\"  Num attention heads: {text_config.num_attention_heads}\")\n",
    "print(f\"  Num KV heads: {text_config.num_key_value_heads}\")\n",
    "print(f\"  Head dim: {_get_head_dim(model.config)}\")\n",
    "print(f\"  Model dtype: {model.dtype}\")\n",
    "\n",
    "# Verify dtype with a test forward pass\n",
    "sample_ids = tokenizer(\"test\", return_tensors=\"pt\")['input_ids'].to(exp_config.device)\n",
    "with torch.no_grad():\n",
    "    out = model(sample_ids, use_cache=True)\n",
    "    cache_check = _ensure_dynamic_cache(out.past_key_values)\n",
    "    k0 = _get_cache_keys(cache_check, 0)\n",
    "    print(f\"  Cache key dtype: {k0.dtype}\")\n",
    "    print(f\"  Cache key shape: {k0.shape}  (batch, kv_heads, seq, head_dim)\")\n",
    "del out, sample_ids, cache_check\n",
    "torch.cuda.empty_cache()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 3: Lib imports + templates + constants\n",
    "from lib.kv_cache import (\n",
    "    _get_cache_keys,\n",
    "    _get_cache_values,\n",
    "    _set_cache_keys,\n",
    "    _set_cache_values,\n",
    "    _ensure_dynamic_cache,\n",
    "    extract_and_truncate_cache_with_bos,\n",
    "    correct_rope_positions_with_bos,\n",
    "    score_answer_with_cache,\n",
    "    deepcopy_cache,\n",
    ")\n",
    "from lib.analysis import cohens_d\n",
    "from lib.data import count_words\n",
    "from scipy import stats\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Templates -- bare text, no \"Document:\\n\" framing\n",
    "SURROGATE_PREFIX_TEMPLATE = \"{surrogate}\\n\"\n",
    "DOCUMENT_TEMPLATE = \"{document}\"\n",
    "QUERY_TEMPLATE = \"\\nQuery: {query}\\nAnswer:\"\n",
    "ANSWER_TEMPLATE = \" {answer}\"\n",
    "\n",
    "# Prefix text\n",
    "from lib.surrogate import STATIC_SURROGATE_QUERIES\n",
    "STATIC_FACT = STATIC_SURROGATE_QUERIES['static_factual']['query']\n",
    "\n",
    "# Experiment parameters\n",
    "N_QUERIES = 500\n",
    "MAX_PASSAGE_WORDS = 300\n",
    "PAD_TARGET = 1024  # pad all documents to this token length\n",
    "CHECKPOINT_EVERY = 25\n",
    "\n",
    "# Bias values to sweep (0.0 = standard priming, no bias)\n",
    "BIAS_VALUES = [0.0, 2.0, 5.0, 10.0]\n",
    "\n",
    "# Reference values from Exp 20 (Mistral)\n",
    "EXP20_REF = {\n",
    "    'original_d': 0.303,\n",
    "    '256_d': 0.114,\n",
    "    '1024_d': -0.043,\n",
    "}\n",
    "\n",
    "print(\"Config ready\")\n",
    "print(f\"  Model: {MODEL_NAME}\")\n",
    "print(f\"  N_QUERIES: {N_QUERIES}\")\n",
    "print(f\"  PAD_TARGET: {PAD_TARGET} tokens\")\n",
    "print(f\"  BIAS_VALUES: {BIAS_VALUES}\")\n",
    "print(f\"  Static fact prefix: '{STATIC_FACT}'\")\n",
    "print(f\"\\nExp 20 reference (Mistral, standard priming):\")\n",
    "for k, v in EXP20_REF.items():\n",
    "    print(f\"    {k}: {v:+.3f}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 4: Load MS MARCO v1.1, filter positive passages, build padding pool\n",
    "from datasets import load_dataset\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"LOADING MS MARCO v1.1 -- POSITIVE PASSAGES ONLY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "dataset = load_dataset(\"microsoft/ms_marco\", \"v1.1\", split=\"validation\",\n",
    "                        trust_remote_code=True)\n",
    "print(f\"Total items in validation: {len(dataset)}\")\n",
    "\n",
    "queries = []\n",
    "padding_passages = []\n",
    "eval_passage_set = set()\n",
    "np.random.seed(SEED)\n",
    "\n",
    "for item in tqdm(dataset, desc=\"Filtering\"):\n",
    "    passages_info = item.get('passages', {})\n",
    "    passage_texts = passages_info.get('passage_text', [])\n",
    "    is_selected = passages_info.get('is_selected', [])\n",
    "    query = item.get('query', '')\n",
    "    answers = item.get('answers', [])\n",
    "    well_formed = item.get('wellFormedAnswers', [])\n",
    "\n",
    "    if not passage_texts or not query:\n",
    "        continue\n",
    "\n",
    "    # Get best answer\n",
    "    answer = None\n",
    "    if well_formed and len(well_formed) > 0 and well_formed[0] != '[]':\n",
    "        answer = well_formed[0]\n",
    "    elif answers and len(answers) > 0 and answers[0] != 'No Answer Present.':\n",
    "        answer = answers[0]\n",
    "    else:\n",
    "        # Still collect non-eval passages for padding pool\n",
    "        for p in passage_texts:\n",
    "            if count_words(p) <= MAX_PASSAGE_WORDS:\n",
    "                padding_passages.append(p)\n",
    "        continue\n",
    "\n",
    "    # Find positive passage(s)\n",
    "    for i, (ptext, sel) in enumerate(zip(passage_texts, is_selected)):\n",
    "        if sel == 1 and count_words(ptext) <= MAX_PASSAGE_WORDS:\n",
    "            if len(queries) < N_QUERIES * 3:  # collect 3x for shuffling\n",
    "                queries.append({\n",
    "                    'query': query,\n",
    "                    'answer': answer,\n",
    "                    'passage': ptext,\n",
    "                    'word_count': count_words(ptext),\n",
    "                })\n",
    "                eval_passage_set.add(ptext)\n",
    "                break\n",
    "\n",
    "    # Collect non-eval passages for padding pool\n",
    "    for p in passage_texts:\n",
    "        if p not in eval_passage_set and count_words(p) <= MAX_PASSAGE_WORDS:\n",
    "            padding_passages.append(p)\n",
    "\n",
    "np.random.shuffle(queries)\n",
    "queries = queries[:N_QUERIES]\n",
    "N = len(queries)\n",
    "\n",
    "print(f\"\\nSelected {N} queries with positive passages\")\n",
    "print(f\"Word counts: mean={np.mean([q['word_count'] for q in queries]):.0f}, \"\n",
    "      f\"min={min(q['word_count'] for q in queries)}, \"\n",
    "      f\"max={max(q['word_count'] for q in queries)}\")\n",
    "\n",
    "# Build padding pool (pre-tokenize)\n",
    "print(f\"\\nPadding pool passages: {len(padding_passages):,}\")\n",
    "padding_text = ' '.join(padding_passages)\n",
    "padding_ids = tokenizer.encode(padding_text, add_special_tokens=False)\n",
    "print(f\"Padding pool tokens: {len(padding_ids):,}\")\n",
    "\n",
    "max_needed = PAD_TARGET * N_QUERIES\n",
    "print(f\"Max tokens needed: {max_needed:,}\")\n",
    "assert len(padding_ids) > max_needed, (\n",
    "    f\"Padding pool too small: {len(padding_ids):,} < {max_needed:,}\"\n",
    ")\n",
    "print(f\"Pool is {len(padding_ids) / max_needed:.1f}x the max needed. OK.\")\n",
    "\n",
    "del dataset\n",
    "gc.collect()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 5: Tokenize prefix and explain experimental conditions\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PREFIX TOKENIZATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "sf_str = SURROGATE_PREFIX_TEMPLATE.format(surrogate=STATIC_FACT)\n",
    "\n",
    "sf_ids = tokenizer(sf_str, return_tensors=\"pt\",\n",
    "                    add_special_tokens=False)['input_ids'].to(exp_config.device)\n",
    "PREFIX_TOKEN_LEN = sf_ids.shape[1]\n",
    "\n",
    "print(f\"\\nStatic fact prefix: '{STATIC_FACT}'\")\n",
    "print(f\"  Formatted: '{sf_str.strip()}'\")\n",
    "print(f\"  Token length (no BOS): {PREFIX_TOKEN_LEN}\")\n",
    "\n",
    "# Verify BPE boundary consistency\n",
    "print(\"\\nBPE BOUNDARY CHECK (first passage):\")\n",
    "example_doc = queries[0]['passage']\n",
    "concat = sf_str + DOCUMENT_TEMPLATE.format(document=example_doc)\n",
    "concat_enc = tokenizer(concat, add_special_tokens=True)['input_ids']\n",
    "prefix_enc = tokenizer(sf_str, add_special_tokens=True)['input_ids']\n",
    "doc_ids_from_concat = concat_enc[len(prefix_enc):]\n",
    "\n",
    "bare_doc_enc = tokenizer(DOCUMENT_TEMPLATE.format(document=example_doc),\n",
    "                          add_special_tokens=False)['input_ids']\n",
    "match = sum(1 for a, b in zip(doc_ids_from_concat, bare_doc_enc) if a == b)\n",
    "total = max(len(bare_doc_enc), 1)\n",
    "print(f\"  Token match: {match}/{total} ({100*match/total:.1f}%)\")\n",
    "\n",
    "# Condition explanations\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"EXPERIMENTAL CONDITIONS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nAll conditions use 1024-token padded documents from MS MARCO.\")\n",
    "print(\"Prefix: static_fact_trunc ('What are the key facts I need to know?')\")\n",
    "print(f\"Prefix tokens: {PREFIX_TOKEN_LEN} (plus BOS = {PREFIX_TOKEN_LEN + 1} total prefix positions)\")\n",
    "print(f\"Document tokens: {PAD_TARGET}\")\n",
    "print(f\"Total sequence for primed passes: 1 + {PREFIX_TOKEN_LEN} + {PAD_TARGET} = {1 + PREFIX_TOKEN_LEN + PAD_TARGET}\")\n",
    "\n",
    "print(\"\\n### bare ###\")\n",
    "print(\"  Forward: [BOS][doc_1024]\")\n",
    "print(\"  Cache:   Standard causal attention, no prefix\")\n",
    "print(\"  Score:   Standard scoring against bare cache\")\n",
    "\n",
    "for bias in BIAS_VALUES:\n",
    "    label = f\"bias_{bias:.1f}\"\n",
    "    print(f\"\\n### {label} ###\")\n",
    "    print(f\"  Forward: [BOS][prefix_{PREFIX_TOKEN_LEN}][doc_{PAD_TARGET}]\")\n",
    "    if bias == 0.0:\n",
    "        print(\"  Mask:    Standard causal (no bias)\")\n",
    "        print(\"  Note:    This is standard priming -- the failure baseline at 1024 tokens\")\n",
    "    else:\n",
    "        print(f\"  Mask:    Causal + {bias:+.1f} logit boost on doc->prefix attention\")\n",
    "        print(f\"           Every doc token gets +{bias:.1f} added to its pre-softmax\")\n",
    "        print(f\"           attention scores for the {PREFIX_TOKEN_LEN} prefix positions\")\n",
    "    print(\"  Post:    Truncate prefix -> RoPE correct -> score\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 6: Helper function for building biased attention masks\n",
    "\n",
    "def build_biased_causal_mask(total_len, prefix_start, prefix_end, bias_value, dtype, device):\n",
    "    \"\"\"Build a 4D causal attention mask with logit bias on doc->prefix attention.\n",
    "\n",
    "    Creates a standard causal mask (lower-triangular = 0, upper-triangular = -inf),\n",
    "    then adds a positive bias to the attention scores at the intersection of\n",
    "    document token queries (rows) and prefix token keys (columns).\n",
    "\n",
    "    Args:\n",
    "        total_len: Total sequence length [BOS + prefix + doc]\n",
    "        prefix_start: Start index of prefix tokens (typically 1, after BOS)\n",
    "        prefix_end: End index of prefix tokens (exclusive)\n",
    "        bias_value: Positive float to add to doc->prefix attention scores.\n",
    "            0.0 = standard causal mask (no bias).\n",
    "        dtype: Model dtype (e.g., torch.float16)\n",
    "        device: Model device\n",
    "\n",
    "    Returns:\n",
    "        Tensor of shape (1, 1, total_len, total_len)\n",
    "    \"\"\"\n",
    "    # Standard causal mask: 0 for attend, -inf for future positions\n",
    "    mask = torch.zeros((total_len, total_len), dtype=dtype, device=device)\n",
    "    causal = torch.triu(\n",
    "        torch.ones(total_len, total_len, dtype=torch.bool, device=device),\n",
    "        diagonal=1\n",
    "    )\n",
    "    mask.masked_fill_(causal, float('-inf'))\n",
    "\n",
    "    # Apply positive bias to doc->prefix attention\n",
    "    # Doc tokens start at prefix_end, prefix tokens at [prefix_start, prefix_end)\n",
    "    if bias_value != 0.0:\n",
    "        doc_start = prefix_end\n",
    "        mask[doc_start:, prefix_start:prefix_end] += bias_value\n",
    "\n",
    "    return mask.unsqueeze(0).unsqueeze(0)  # (1, 1, total_len, total_len)\n",
    "\n",
    "\n",
    "# Verify mask shape and values for a toy example\n",
    "print(\"=\" * 70)\n",
    "print(\"MASK VERIFICATION (toy example)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Toy: BOS + 3 prefix tokens + 5 doc tokens = 9 total\n",
    "toy_mask = build_biased_causal_mask(\n",
    "    total_len=9, prefix_start=1, prefix_end=4,\n",
    "    bias_value=5.0, dtype=model.dtype, device='cpu'\n",
    ")\n",
    "m = toy_mask.squeeze()  # (9, 9)\n",
    "\n",
    "print(f\"\\nMask shape: {toy_mask.shape}\")\n",
    "print(f\"Dtype: {toy_mask.dtype}\")\n",
    "print(\"\\nPositions: [BOS=0, P1=1, P2=2, P3=3, D1=4, D2=5, D3=6, D4=7, D5=8]\")\n",
    "print(\"\\nMask values (rows=queries, cols=keys):\")\n",
    "print(\"         BOS   P1    P2    P3    D1    D2    D3    D4    D5\")\n",
    "labels = ['BOS', 'P1 ', 'P2 ', 'P3 ', 'D1 ', 'D2 ', 'D3 ', 'D4 ', 'D5 ']\n",
    "for i, label in enumerate(labels):\n",
    "    row_vals = []\n",
    "    for j in range(9):\n",
    "        v = m[i, j].item()\n",
    "        if v == float('-inf'):\n",
    "            row_vals.append(' -inf')\n",
    "        elif v == 0.0:\n",
    "            row_vals.append('  0.0')\n",
    "        else:\n",
    "            row_vals.append(f'{v:+5.1f}')\n",
    "    print(f\"  {label}: {'  '.join(row_vals)}\")\n",
    "\n",
    "print(\"\\nKey observations:\")\n",
    "print(\"  - BOS (row 0) can only attend to itself (0.0)\")\n",
    "print(\"  - Prefix tokens (rows 1-3) attend causally to BOS + prior prefix (0.0)\")\n",
    "print(\"  - Doc tokens (rows 4-8) attend to prefix with +5.0 bias\")\n",
    "print(\"  - Doc tokens attend to BOS and other doc tokens normally (0.0)\")\n",
    "print(\"  - Upper triangle is -inf (causal masking)\")\n",
    "\n",
    "del toy_mask, m"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 7: Main experiment loop\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"EXPERIMENT: {N_QUERIES} queries, {len(BIAS_VALUES)} bias levels + bare\")\n",
    "print(f\"Document length: {PAD_TARGET} tokens (padded)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Checkpoint resume\n",
    "all_results = []\n",
    "start_idx = 0\n",
    "\n",
    "if CHECKPOINT_PATH.exists():\n",
    "    with open(CHECKPOINT_PATH, 'r') as f:\n",
    "        ckpt = json.load(f)\n",
    "    ckpt_queries = ckpt.get('query_texts', [])\n",
    "    current_queries = [q['query'] for q in queries[:N_QUERIES]]\n",
    "    if ckpt_queries == current_queries:\n",
    "        all_results = ckpt['results']\n",
    "        start_idx = len(all_results)\n",
    "        print(f\"Resuming from checkpoint: {start_idx}/{N_QUERIES}\")\n",
    "    else:\n",
    "        print(\"Checkpoint query mismatch. Starting fresh.\")\n",
    "else:\n",
    "    print(\"No checkpoint found. Starting fresh.\")\n",
    "\n",
    "t_start = time.time()\n",
    "\n",
    "for qidx in tqdm(range(start_idx, N_QUERIES), initial=start_idx, total=N_QUERIES,\n",
    "                  desc=\"Exp 26\"):\n",
    "    qdata = queries[qidx]\n",
    "    query_prompt = QUERY_TEMPLATE.format(query=qdata['query'])\n",
    "    answer_text = ANSWER_TEMPLATE.format(answer=qdata['answer'])\n",
    "    passage = qdata['passage']\n",
    "    document_text = DOCUMENT_TEMPLATE.format(document=passage)\n",
    "\n",
    "    # === Matched tokenization ===\n",
    "    # Tokenize concatenated prefix+doc to get matched BPE boundaries\n",
    "    full_text = sf_str + document_text\n",
    "    full_enc = tokenizer(full_text, return_tensors=\"pt\",\n",
    "                          add_special_tokens=True, padding=False, truncation=False)\n",
    "    full_ids = full_enc['input_ids'].to(exp_config.device)\n",
    "\n",
    "    sf_prefix_enc = tokenizer(sf_str, return_tensors=\"pt\",\n",
    "                               add_special_tokens=True, padding=False, truncation=False)\n",
    "    sf_prefix_len_with_bos = sf_prefix_enc['input_ids'].shape[1]\n",
    "\n",
    "    bos_id = full_ids[:, :1]\n",
    "    base_doc_ids = full_ids[:, sf_prefix_len_with_bos:]\n",
    "    base_doc_len = base_doc_ids.shape[1]\n",
    "\n",
    "    del full_enc, full_ids, sf_prefix_enc\n",
    "\n",
    "    # === Pad doc to PAD_TARGET tokens ===\n",
    "    if base_doc_len < PAD_TARGET:\n",
    "        pad_needed = PAD_TARGET - base_doc_len\n",
    "        max_start = len(padding_ids) - pad_needed\n",
    "        start = np.random.randint(0, max_start)\n",
    "        pad_tensor = torch.tensor([padding_ids[start:start + pad_needed]],\n",
    "                                   device=exp_config.device)\n",
    "        doc_ids = torch.cat([base_doc_ids, pad_tensor], dim=1)\n",
    "    else:\n",
    "        doc_ids = base_doc_ids[:, :PAD_TARGET]\n",
    "\n",
    "    doc_len = doc_ids.shape[1]\n",
    "    context_len = 1 + doc_len  # BOS + doc\n",
    "\n",
    "    # === Forward pass: BARE ===\n",
    "    bare_input = torch.cat([bos_id, doc_ids], dim=1)\n",
    "    with torch.no_grad():\n",
    "        bare_out = model(input_ids=bare_input,\n",
    "                         attention_mask=torch.ones_like(bare_input),\n",
    "                         use_cache=True, return_dict=True)\n",
    "    bare_cache = _ensure_dynamic_cache(bare_out.past_key_values)\n",
    "    del bare_out\n",
    "\n",
    "    # Score bare\n",
    "    bare_nll = score_answer_with_cache(\n",
    "        deepcopy_cache(bare_cache), context_len,\n",
    "        query_prompt, answer_text, model, tokenizer, exp_config)\n",
    "\n",
    "    # === Forward passes: BIASED (one per bias level) ===\n",
    "    primed_input = torch.cat([bos_id, sf_ids, doc_ids], dim=1)\n",
    "    total_seq_len = primed_input.shape[1]\n",
    "    prefix_start = 1  # prefix starts after BOS\n",
    "    prefix_end = 1 + sf_ids.shape[1]  # prefix ends before doc\n",
    "    prefix_offset = sf_ids.shape[1]  # for RoPE correction\n",
    "\n",
    "    query_rows = []\n",
    "\n",
    "    for bias_value in BIAS_VALUES:\n",
    "        # Build 4D attention mask with bias\n",
    "        mask_4d = build_biased_causal_mask(\n",
    "            total_seq_len, prefix_start, prefix_end,\n",
    "            bias_value, model.dtype, exp_config.device)\n",
    "\n",
    "        # Forward pass with custom mask\n",
    "        with torch.no_grad():\n",
    "            primed_out = model(input_ids=primed_input,\n",
    "                               attention_mask=mask_4d,\n",
    "                               use_cache=True, return_dict=True)\n",
    "        primed_full = _ensure_dynamic_cache(primed_out.past_key_values)\n",
    "        del primed_out, mask_4d\n",
    "\n",
    "        # Truncate: keep [BOS] + [last doc_len positions]\n",
    "        trunc_raw = extract_and_truncate_cache_with_bos(primed_full, doc_len)\n",
    "        del primed_full\n",
    "\n",
    "        # RoPE correct\n",
    "        sf_trunc_cache = deepcopy_cache(trunc_raw)\n",
    "        correct_rope_positions_with_bos(sf_trunc_cache, prefix_offset, model)\n",
    "        del trunc_raw\n",
    "\n",
    "        # Score\n",
    "        biased_nll = score_answer_with_cache(\n",
    "            deepcopy_cache(sf_trunc_cache), context_len,\n",
    "            query_prompt, answer_text, model, tokenizer, exp_config)\n",
    "        del sf_trunc_cache\n",
    "\n",
    "        query_rows.append({\n",
    "            'query_idx': qidx,\n",
    "            'bias_value': bias_value,\n",
    "            'bias_label': f\"bias_{bias_value:.1f}\",\n",
    "            'actual_doc_len': doc_len,\n",
    "            'bare_nll': bare_nll,\n",
    "            'primed_nll': biased_nll,\n",
    "            'delta_nll': bare_nll - biased_nll,\n",
    "        })\n",
    "\n",
    "    del bare_cache, bare_input, primed_input\n",
    "    if base_doc_len < PAD_TARGET:\n",
    "        del pad_tensor\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    all_results.append({\n",
    "        'query_idx': qidx,\n",
    "        'query': qdata['query'],\n",
    "        'base_doc_len': base_doc_len,\n",
    "        'padded_doc_len': doc_len,\n",
    "        'bare_nll': bare_nll,\n",
    "        'rows': query_rows,\n",
    "    })\n",
    "\n",
    "    # Checkpoint\n",
    "    if (qidx + 1) % CHECKPOINT_EVERY == 0 or qidx == N_QUERIES - 1:\n",
    "        ckpt_data = {\n",
    "            'results': all_results,\n",
    "            'query_texts': [q['query'] for q in queries[:N_QUERIES]],\n",
    "            'completed': len(all_results),\n",
    "            'total': N_QUERIES,\n",
    "            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        }\n",
    "        with open(CHECKPOINT_PATH, 'w') as f:\n",
    "            json.dump(ckpt_data, f)\n",
    "        elapsed = time.time() - t_start\n",
    "        n_done = qidx - start_idx + 1\n",
    "        rate = n_done / elapsed if elapsed > 0 else 0\n",
    "        remaining = (N_QUERIES - qidx - 1) / rate if rate > 0 else 0\n",
    "        tqdm.write(f\"  Checkpoint {qidx+1}/{N_QUERIES} | {n_done} done in {elapsed/60:.1f}m | \"\n",
    "                   f\"ETA: {remaining/60:.1f} min\")\n",
    "\n",
    "elapsed_total = time.time() - t_start\n",
    "print(f\"\\nExperiment complete: {len(all_results)} queries in {elapsed_total/60:.1f} min\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 8: Analysis -- Cohen's d, statistical tests, perplexity check\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ANALYSIS: ATTENTION FORCING RESULTS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Collect per-sample deltas by bias level\n",
    "bias_deltas = {}\n",
    "bias_bare = {}\n",
    "bias_primed = {}\n",
    "for bv in BIAS_VALUES:\n",
    "    label = f\"bias_{bv:.1f}\"\n",
    "    bias_deltas[label] = []\n",
    "    bias_bare[label] = []\n",
    "    bias_primed[label] = []\n",
    "\n",
    "for r in all_results:\n",
    "    for row in r['rows']:\n",
    "        label = row['bias_label']\n",
    "        if label in bias_deltas:\n",
    "            bias_deltas[label].append(row['delta_nll'])\n",
    "            bias_bare[label].append(row['bare_nll'])\n",
    "            bias_primed[label].append(row['primed_nll'])\n",
    "\n",
    "# Convert to arrays and filter invalid values\n",
    "bias_arrays = {}\n",
    "for label in bias_deltas:\n",
    "    bare = np.array(bias_bare[label])\n",
    "    primed = np.array(bias_primed[label])\n",
    "    delta = np.array(bias_deltas[label])\n",
    "    valid = (bare != 0) & (primed != 0) & np.isfinite(bare) & np.isfinite(primed)\n",
    "    bias_arrays[label] = {\n",
    "        'bare': bare[valid],\n",
    "        'primed': primed[valid],\n",
    "        'delta': delta[valid],\n",
    "        'n_valid': int(np.sum(valid)),\n",
    "    }\n",
    "\n",
    "# Summary table\n",
    "print(f\"\\n{'Condition':<14} {'N':>5} {'Mean Bare':>10} {'Mean Primed':>12} \"\n",
    "      f\"{'Mean D':>10} {'d':>8} {'Win%':>7} {'p':>12} {'sig':>5}\")\n",
    "print(\"-\" * 95)\n",
    "\n",
    "analysis = {}\n",
    "for bv in BIAS_VALUES:\n",
    "    label = f\"bias_{bv:.1f}\"\n",
    "    a = bias_arrays[label]\n",
    "    d = cohens_d(a['delta'])\n",
    "    win = np.mean(a['delta'] > 0) * 100\n",
    "    t_stat, p_val = stats.ttest_1samp(a['delta'], 0)\n",
    "    sig = '***' if p_val < 0.001 else '**' if p_val < 0.01 else '*' if p_val < 0.05 else 'ns'\n",
    "    print(f\"{label:<14} {a['n_valid']:>5} {np.mean(a['bare']):>10.4f} \"\n",
    "          f\"{np.mean(a['primed']):>12.4f} {np.mean(a['delta']):>+10.4f} \"\n",
    "          f\"{d:>+8.3f} {win:>6.1f}% {p_val:>12.2e} {sig:>5}\")\n",
    "    analysis[label] = {\n",
    "        'bias_value': bv,\n",
    "        'n_valid': a['n_valid'],\n",
    "        'mean_bare': float(np.mean(a['bare'])),\n",
    "        'mean_primed': float(np.mean(a['primed'])),\n",
    "        'mean_delta': float(np.mean(a['delta'])),\n",
    "        'std_delta': float(np.std(a['delta'])),\n",
    "        'cohens_d': float(d),\n",
    "        'win_pct': float(win),\n",
    "        't_stat': float(t_stat),\n",
    "        'p_value': float(p_val),\n",
    "    }\n",
    "\n",
    "# Perplexity check\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"PERPLEXITY CHECK\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nDoes attention forcing corrupt the document representation?\")\n",
    "print(\"If mean primed NLL is much higher than mean bare NLL, the bias is too aggressive.\")\n",
    "print()\n",
    "\n",
    "bare_mean = np.mean(bias_arrays['bias_0.0']['bare'])\n",
    "for bv in BIAS_VALUES:\n",
    "    label = f\"bias_{bv:.1f}\"\n",
    "    a = bias_arrays[label]\n",
    "    primed_mean = np.mean(a['primed'])\n",
    "    ratio = primed_mean / bare_mean if bare_mean > 0 else float('inf')\n",
    "    corruption = \"OK\" if ratio < 1.5 else \"WARNING\" if ratio < 3.0 else \"CORRUPTED\"\n",
    "    print(f\"  {label}: bare={bare_mean:.4f}, primed={primed_mean:.4f}, \"\n",
    "          f\"ratio={ratio:.2f}x  [{corruption}]\")\n",
    "\n",
    "# Exp 20 comparison\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"COMPARISON WITH EXP 20 (standard priming)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "d_standard = analysis['bias_0.0']['cohens_d']\n",
    "print(f\"\\nExp 20 standard priming @ 1024 tok: d={EXP20_REF['1024_d']:+.3f}\")\n",
    "print(f\"This exp bias_0.0 (standard priming):  d={d_standard:+.3f}\")\n",
    "\n",
    "best_label = max(analysis, key=lambda k: analysis[k]['cohens_d'])\n",
    "best_d = analysis[best_label]['cohens_d']\n",
    "best_bias = analysis[best_label]['bias_value']\n",
    "\n",
    "print(f\"\\nBest bias: {best_label} (d={best_d:+.3f})\")\n",
    "if best_d > 0.20:\n",
    "    verdict = (f\"SUCCESS: Attention forcing recovers a meaningful effect (d={best_d:+.3f}) \"\n",
    "               f\"at 1024 tokens with bias={best_bias:.1f}\")\n",
    "elif best_d > 0.05:\n",
    "    verdict = (f\"PARTIAL: Some recovery (d={best_d:+.3f}) but below the +0.20 target. \"\n",
    "               f\"Best bias={best_bias:.1f}\")\n",
    "elif best_d > d_standard + 0.05:\n",
    "    verdict = (f\"MARGINAL: Bias helps vs standard (d={best_d:+.3f} vs {d_standard:+.3f}) \"\n",
    "               f\"but effect is small\")\n",
    "else:\n",
    "    verdict = (f\"FAILURE: Attention forcing does not recover priming at 1024 tokens. \"\n",
    "               f\"Best d={best_d:+.3f}, standard d={d_standard:+.3f}\")\n",
    "\n",
    "print(f\"\\nVERDICT: {verdict}\")\n",
    "\n",
    "# Hardness interaction\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"HARDNESS INTERACTION (quintiles by bare NLL)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "all_bare_nlls = np.array([r['bare_nll'] for r in all_results])\n",
    "quintile_boundaries = np.percentile(all_bare_nlls, [20, 40, 60, 80])\n",
    "quintile_labels = ['Q1 (easy)', 'Q2', 'Q3', 'Q4', 'Q5 (hard)']\n",
    "\n",
    "def get_quintile(nll, boundaries):\n",
    "    for i, b in enumerate(boundaries):\n",
    "        if nll <= b:\n",
    "            return i\n",
    "    return len(boundaries)\n",
    "\n",
    "quintiles = np.array([get_quintile(nll, quintile_boundaries) for nll in all_bare_nlls])\n",
    "\n",
    "header = f\"{'Condition':<14}\" + \"\".join(f\"{ql:>14}\" for ql in quintile_labels) + f\"{'Overall':>14}\"\n",
    "print(f\"\\n{header}\")\n",
    "print(\"-\" * (14 + 14 * 6))\n",
    "\n",
    "hardness_data = {}\n",
    "for bv in BIAS_VALUES:\n",
    "    label = f\"bias_{bv:.1f}\"\n",
    "    delta = np.array(bias_deltas[label])\n",
    "    row_str = f\"{label:<14}\"\n",
    "    quintile_ds = []\n",
    "    for q in range(5):\n",
    "        mask_q = quintiles == q\n",
    "        if np.sum(mask_q) < 5:\n",
    "            row_str += f\"{'n/a':>14}\"\n",
    "            quintile_ds.append(None)\n",
    "        else:\n",
    "            d_q = cohens_d(delta[mask_q])\n",
    "            row_str += f\"{d_q:>+14.3f}\"\n",
    "            quintile_ds.append(float(d_q))\n",
    "    d_all = cohens_d(delta)\n",
    "    row_str += f\"{d_all:>+14.3f}\"\n",
    "    print(row_str)\n",
    "    hardness_data[label] = quintile_ds"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 9: Plots -- 4-panel figure\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# ---- Panel 1 (top-left): Bias Tuning Curve (d vs bias) ----\n",
    "ax = axes[0, 0]\n",
    "\n",
    "bias_vals_plot = BIAS_VALUES\n",
    "d_vals = [analysis[f\"bias_{bv:.1f}\"]['cohens_d'] for bv in bias_vals_plot]\n",
    "\n",
    "# Bootstrap 95% CI\n",
    "np.random.seed(SEED)\n",
    "ci_lo, ci_hi = [], []\n",
    "for bv in bias_vals_plot:\n",
    "    label = f\"bias_{bv:.1f}\"\n",
    "    delta = bias_arrays[label]['delta']\n",
    "    boot_ds = []\n",
    "    for _ in range(2000):\n",
    "        idx_boot = np.random.randint(0, len(delta), size=len(delta))\n",
    "        boot_ds.append(cohens_d(delta[idx_boot]))\n",
    "    boot_ds = np.array(boot_ds)\n",
    "    ci_lo.append(np.percentile(boot_ds, 2.5))\n",
    "    ci_hi.append(np.percentile(boot_ds, 97.5))\n",
    "ci_lo = np.array(ci_lo)\n",
    "ci_hi = np.array(ci_hi)\n",
    "\n",
    "ax.errorbar(bias_vals_plot, d_vals,\n",
    "            yerr=[np.array(d_vals) - ci_lo, ci_hi - np.array(d_vals)],\n",
    "            marker='o', markersize=8, linewidth=2, capsize=5,\n",
    "            color='#1f77b4', ecolor='#aec7e8')\n",
    "\n",
    "# Reference lines\n",
    "ax.axhline(y=0, color='gray', linestyle='-', alpha=0.3)\n",
    "ax.axhline(y=EXP20_REF['1024_d'], color='#d62728', linestyle='--', linewidth=1.5,\n",
    "           label=f\"Exp 20 standard @ 1024tok (d={EXP20_REF['1024_d']:+.3f})\")\n",
    "ax.axhline(y=EXP20_REF['original_d'], color='#2ca02c', linestyle=':', linewidth=1.5,\n",
    "           label=f\"Exp 20 standard @ original (d={EXP20_REF['original_d']:+.3f})\")\n",
    "\n",
    "for i, bv in enumerate(bias_vals_plot):\n",
    "    label = f\"bias_{bv:.1f}\"\n",
    "    p_val = analysis[label]['p_value']\n",
    "    sig = '***' if p_val < 0.001 else '**' if p_val < 0.01 else '*' if p_val < 0.05 else 'ns'\n",
    "    ax.annotate(f'd={d_vals[i]:+.3f} {sig}',\n",
    "                (bv, d_vals[i]),\n",
    "                textcoords='offset points', xytext=(0, 18),\n",
    "                ha='center', fontsize=8)\n",
    "\n",
    "ax.set_xlabel('Attention Bias Value')\n",
    "ax.set_ylabel(\"Cohen's d (positive = helps)\")\n",
    "ax.set_title(\"Bias Tuning Curve: d vs Attention Bias\")\n",
    "ax.legend(fontsize=7)\n",
    "\n",
    "# ---- Panel 2 (top-right): Mean NLL by condition (perplexity check) ----\n",
    "ax = axes[0, 1]\n",
    "\n",
    "cond_labels = ['bare'] + [f\"bias_{bv:.1f}\" for bv in BIAS_VALUES]\n",
    "bare_mean_nll = float(np.mean(all_bare_nlls))\n",
    "mean_nlls = [bare_mean_nll] + [analysis[f\"bias_{bv:.1f}\"]['mean_primed'] for bv in BIAS_VALUES]\n",
    "\n",
    "colors_bar = ['#7f7f7f'] + ['#1f77b4' if bv < 10 else '#ff7f0e' for bv in BIAS_VALUES]\n",
    "bars = ax.bar(range(len(cond_labels)), mean_nlls, color=colors_bar, edgecolor='black', linewidth=0.5)\n",
    "ax.set_xticks(range(len(cond_labels)))\n",
    "ax.set_xticklabels(cond_labels, rotation=30, ha='right', fontsize=8)\n",
    "ax.set_ylabel('Mean NLL')\n",
    "ax.set_title('Perplexity Check: Mean NLL by Condition')\n",
    "\n",
    "for i, v in enumerate(mean_nlls):\n",
    "    ax.text(i, v + 0.01, f'{v:.3f}', ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "# ---- Panel 3 (bottom-left): Win Rate vs Bias ----\n",
    "ax = axes[1, 0]\n",
    "\n",
    "win_rates = [analysis[f\"bias_{bv:.1f}\"]['win_pct'] for bv in BIAS_VALUES]\n",
    "ax.plot(BIAS_VALUES, win_rates, marker='s', markersize=8, linewidth=2, color='#2ca02c')\n",
    "ax.axhline(y=50, color='black', linestyle='--', linewidth=0.8, label='Chance (50%)')\n",
    "\n",
    "for i, bv in enumerate(BIAS_VALUES):\n",
    "    ax.annotate(f'{win_rates[i]:.1f}%',\n",
    "                (bv, win_rates[i]),\n",
    "                textcoords='offset points', xytext=(0, 12),\n",
    "                ha='center', fontsize=9)\n",
    "\n",
    "ax.set_xlabel('Attention Bias Value')\n",
    "ax.set_ylabel('Win Rate (% where primed < bare)')\n",
    "ax.set_title('Win Rate vs Bias Level')\n",
    "ax.legend(fontsize=8)\n",
    "\n",
    "# ---- Panel 4 (bottom-right): Hardness x Bias heatmap ----\n",
    "ax = axes[1, 1]\n",
    "\n",
    "heatmap_data = np.zeros((len(BIAS_VALUES), 5))\n",
    "for i, bv in enumerate(BIAS_VALUES):\n",
    "    label = f\"bias_{bv:.1f}\"\n",
    "    for q in range(5):\n",
    "        val = hardness_data[label][q]\n",
    "        heatmap_data[i, q] = val if val is not None else np.nan\n",
    "\n",
    "im = ax.imshow(heatmap_data, cmap='RdBu', aspect='auto',\n",
    "               vmin=-0.5, vmax=0.5)\n",
    "ax.set_xticks(range(5))\n",
    "ax.set_xticklabels(quintile_labels, fontsize=8)\n",
    "ax.set_yticks(range(len(BIAS_VALUES)))\n",
    "ax.set_yticklabels([f\"bias_{bv:.1f}\" for bv in BIAS_VALUES])\n",
    "ax.set_xlabel('Difficulty Quintile')\n",
    "ax.set_ylabel('Bias Level')\n",
    "ax.set_title(\"Hardness x Bias Interaction (Cohen's d)\")\n",
    "\n",
    "for i in range(len(BIAS_VALUES)):\n",
    "    for j in range(5):\n",
    "        val = heatmap_data[i, j]\n",
    "        if not np.isnan(val):\n",
    "            ax.text(j, i, f\"{val:+.2f}\", ha='center', va='center',\n",
    "                    fontsize=8, color='white' if abs(val) > 0.25 else 'black')\n",
    "\n",
    "fig.colorbar(im, ax=ax, shrink=0.8, label=\"Cohen's d\")\n",
    "\n",
    "plt.suptitle('Exp 26: Attention Forcing for Long Documents (1024 tok)', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'analysis_plots.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"Plots saved to {RESULTS_DIR / 'analysis_plots.png'}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 10: Save results.json + CSV\n",
    "import csv\n",
    "\n",
    "# --- CSV ---\n",
    "with open(CSV_PATH, 'w', newline='') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=[\n",
    "        'query_idx', 'bias_value', 'bias_label', 'actual_doc_len',\n",
    "        'bare_nll', 'primed_nll', 'delta_nll'])\n",
    "    writer.writeheader()\n",
    "    for r in all_results:\n",
    "        for row in r['rows']:\n",
    "            writer.writerow(row)\n",
    "print(f\"CSV saved: {CSV_PATH}\")\n",
    "\n",
    "# --- results.json ---\n",
    "final = {\n",
    "    'experiment': 'exp26_attention_forcing',\n",
    "    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'config': {\n",
    "        'model_name': MODEL_NAME,\n",
    "        'model_type': 'mistral',\n",
    "        'seed': SEED,\n",
    "        'dataset': 'MS MARCO v1.1 validation',\n",
    "        'max_passage_words': MAX_PASSAGE_WORDS,\n",
    "        'n_queries': N_QUERIES,\n",
    "        'pad_target': PAD_TARGET,\n",
    "        'bias_values': BIAS_VALUES,\n",
    "        'prefix': STATIC_FACT,\n",
    "        'prefix_token_len': PREFIX_TOKEN_LEN,\n",
    "    },\n",
    "    'analysis': analysis,\n",
    "    'verdict': verdict,\n",
    "    'hardness_data': hardness_data,\n",
    "    'reference_values': {\n",
    "        'exp20_mistral': EXP20_REF,\n",
    "    },\n",
    "    'per_query': all_results,\n",
    "}\n",
    "\n",
    "with open(FINAL_RESULTS_PATH, 'w') as f:\n",
    "    json.dump(final, f, indent=2)\n",
    "\n",
    "print(f\"\\nResults saved to {FINAL_RESULTS_PATH}\")\n",
    "print(f\"File size: {FINAL_RESULTS_PATH.stat().st_size / 1024:.1f} KB\")\n",
    "\n",
    "# Final summary\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"SUMMARY -- Exp 26: Attention Forcing for Long Documents\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Model: Mistral 7B (4-bit, float16)\")\n",
    "print(f\"Document length: {PAD_TARGET} tokens (padded)\")\n",
    "print(f\"Prefix: static_fact_trunc ({PREFIX_TOKEN_LEN} tokens)\")\n",
    "print()\n",
    "for bv in BIAS_VALUES:\n",
    "    label = f\"bias_{bv:.1f}\"\n",
    "    a = analysis[label]\n",
    "    sig = '***' if a['p_value'] < 0.001 else '**' if a['p_value'] < 0.01 else '*' if a['p_value'] < 0.05 else 'ns'\n",
    "    marker = \" <-- BEST\" if label == best_label else \"\"\n",
    "    print(f\"  {label:<14} d={a['cohens_d']:>+.3f}  win={a['win_pct']:.0f}%  \"\n",
    "          f\"NLL={a['mean_primed']:.4f}  {sig}{marker}\")\n",
    "print(f\"\\nExp 20 standard @ 1024 tok: d={EXP20_REF['1024_d']:+.3f}\")\n",
    "print(f\"Exp 20 standard @ original:  d={EXP20_REF['original_d']:+.3f}\")\n",
    "print(f\"\\nVERDICT: {verdict}\")\n",
    "print(f\"\\nDone!\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 11: GPU cleanup\n",
    "import gc\n",
    "\n",
    "print(\"Cleaning up GPU memory...\")\n",
    "mem_before = torch.cuda.memory_allocated() / 1e9\n",
    "\n",
    "del model\n",
    "del tokenizer\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "mem_after = torch.cuda.memory_allocated() / 1e9\n",
    "print(f\"GPU memory: {mem_before:.2f} GB -> {mem_after:.2f} GB\")\n",
    "print(\"Cleanup complete.\")"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}