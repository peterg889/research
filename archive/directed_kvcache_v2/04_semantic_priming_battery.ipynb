{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Exp 04: Cross-Dataset Semantic Priming Battery\n\n## Goal\n\nTest whether semantic priming (oracle query > random prefix) emerges with longer documents\nacross multiple datasets. Exp 01-03 used MS MARCO (~80 words) and found random beats oracle.\nHypothesis: passages were too short for semantic signal to matter.\n\n## Datasets (length gradient)\n\n| Dataset | Avg doc words | N samples | Source |\n|---------|--------------|-----------|--------|\n| MS MARCO multi-passage | ~550 | 1500 | 10 passages concatenated per query |\n| SQuAD 2.0 | ~150 | 1500 | Wikipedia paragraphs |\n| Natural Questions | ~800 | 800 | Wikipedia article windows |\n| TriviaQA | ~800 | 800 | Wikipedia/web evidence windows |\n\n## Conditions (4 per dataset)\n\n| # | Condition | Cache | Tests |\n|---|-----------|-------|-------|\n| 1 | Bare | `[BOS][doc]` (matched tokenization) | Baseline |\n| 2 | Oracle-truncated | `[BOS][query\\n][doc]` → truncate + RoPE | Semantic signal |\n| 3 | Random-truncated | `[BOS][random\\n][doc]` → truncate + RoPE | Structural control |\n| 4 | Separator-only | `[BOS][doc][\\n\\nRelated question: ]` | Framing effect |\n\n## Key comparison: Oracle vs Random (positive d = semantic priming works)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Setup\n",
    "import os\n",
    "os.umask(0o000)\n",
    "\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "RESULTS_DIR = Path(\"results/exp04\")\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"SEED: {SEED}\")\n",
    "print(f\"Results directory: {RESULTS_DIR}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Load model (Mistral-7B 4-bit)\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "print(f\"Loading {MODEL_NAME} (4-bit)...\")\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "print(f\"Model loaded. dtype={model.dtype}, device={model.device}\")\n",
    "print(f\"Vocab size: {len(tokenizer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Imports + config + templates + shared functions\n",
    "sys.path.insert(0, \".\")\n",
    "\n",
    "from lib.config import ExperimentConfig\n",
    "from lib.kv_cache import (\n",
    "    build_kv_cache,\n",
    "    build_suffix_kv_cache,\n",
    "    score_answer_with_cache,\n",
    "    deepcopy_cache,\n",
    "    extract_and_truncate_cache_with_bos,\n",
    "    correct_rope_positions_with_bos,\n",
    ")\n",
    "from lib.analysis import cohens_d\n",
    "from scipy import stats\n",
    "from tqdm.auto import tqdm\n",
    "import traceback\n",
    "\n",
    "config = ExperimentConfig(\n",
    "    model_name=MODEL_NAME,\n",
    "    num_samples=5000,\n",
    "    min_passage_words=20,\n",
    "    max_passage_words=5000,\n",
    "    seed=SEED,\n",
    ")\n",
    "\n",
    "SURROGATE_PREFIX_TEMPLATE = \"{surrogate}\\n\"\n",
    "DOCUMENT_TEMPLATE = \"{document}\"\n",
    "QUERY_TEMPLATE = \"\\nQuery: {query}\\nAnswer:\"\n",
    "ANSWER_TEMPLATE = \" {answer}\"\n",
    "SUFFIX_SEPARATOR = \"\\n\\nRelated question: \"\n",
    "CHECKPOINT_EVERY = 50\n",
    "\n",
    "print(\"Config ready\")\n",
    "print(f\"  device: {config.device}\")\n",
    "print(f\"  templates: prefix={repr(SURROGATE_PREFIX_TEMPLATE)}, doc={repr(DOCUMENT_TEMPLATE)}\")\n",
    "print(f\"  suffix_separator: {repr(SUFFIX_SEPARATOR)}\")\n",
    "\n",
    "\n",
    "# ===================== SHARED UTILITY FUNCTIONS =====================\n",
    "\n",
    "def generate_random_prefix_text(target_text, tokenizer, seed):\n",
    "    target_ids = tokenizer.encode(target_text, add_special_tokens=False)\n",
    "    target_len = len(target_ids)\n",
    "    if target_len == 0:\n",
    "        return \"\"\n",
    "    rng = np.random.RandomState(seed)\n",
    "    vocab_size = len(tokenizer)\n",
    "    min_id = 3\n",
    "    random_ids = rng.randint(min_id, vocab_size, size=target_len)\n",
    "    random_text = tokenizer.decode(random_ids.tolist(), skip_special_tokens=True)\n",
    "    reencoded = tokenizer.encode(random_text, add_special_tokens=False)\n",
    "    if len(reencoded) != target_len:\n",
    "        if len(reencoded) > target_len:\n",
    "            random_text = tokenizer.decode(reencoded[:target_len], skip_special_tokens=True)\n",
    "        else:\n",
    "            extra_needed = target_len - len(reencoded)\n",
    "            extra_ids = rng.randint(min_id, vocab_size, size=extra_needed)\n",
    "            extra_text = tokenizer.decode(extra_ids.tolist(), skip_special_tokens=True)\n",
    "            random_text = random_text + extra_text\n",
    "            reencoded2 = tokenizer.encode(random_text, add_special_tokens=False)\n",
    "            if len(reencoded2) > target_len:\n",
    "                random_text = tokenizer.decode(reencoded2[:target_len], skip_special_tokens=True)\n",
    "    return random_text\n",
    "\n",
    "\n",
    "def evaluate_sample(sample, idx, model, tokenizer, device, seed, n_total):\n",
    "    passage = sample['passage']\n",
    "    query = sample['query']\n",
    "    answer = sample['answer']\n",
    "    query_prompt = QUERY_TEMPLATE.format(query=query)\n",
    "    answer_text = ANSWER_TEMPLATE.format(answer=answer)\n",
    "\n",
    "    # Matched tokenization\n",
    "    oracle_prefix = SURROGATE_PREFIX_TEMPLATE.format(surrogate=query)\n",
    "    document_text = DOCUMENT_TEMPLATE.format(document=passage)\n",
    "    full_oracle_text = oracle_prefix + document_text\n",
    "\n",
    "    full_oracle_enc = tokenizer(full_oracle_text, return_tensors=\"pt\",\n",
    "                                add_special_tokens=True, padding=False, truncation=False)\n",
    "    full_oracle_ids = full_oracle_enc['input_ids'].to(device)\n",
    "\n",
    "    oracle_prefix_enc = tokenizer(oracle_prefix, return_tensors=\"pt\",\n",
    "                                  add_special_tokens=True, padding=False, truncation=False)\n",
    "    oracle_prefix_len = oracle_prefix_enc['input_ids'].shape[1]\n",
    "\n",
    "    bos_id = full_oracle_ids[:, :1]\n",
    "    doc_ids = full_oracle_ids[:, oracle_prefix_len:]\n",
    "    doc_len = doc_ids.shape[1]\n",
    "\n",
    "    random_text = generate_random_prefix_text(query, tokenizer, seed=seed + idx)\n",
    "\n",
    "    # Condition 1: BARE\n",
    "    bare_ids = torch.cat([bos_id, doc_ids], dim=1)\n",
    "    bare_len = bare_ids.shape[1]\n",
    "    with torch.no_grad():\n",
    "        bare_out = model(input_ids=bare_ids, attention_mask=torch.ones_like(bare_ids),\n",
    "                         use_cache=True, return_dict=True)\n",
    "    bare_nll = score_answer_with_cache(\n",
    "        deepcopy_cache(bare_out.past_key_values), bare_len,\n",
    "        query_prompt, answer_text, model, tokenizer, config)\n",
    "\n",
    "    # Condition 2: ORACLE-TRUNCATED\n",
    "    with torch.no_grad():\n",
    "        oracle_out = model(input_ids=full_oracle_ids,\n",
    "                           attention_mask=torch.ones_like(full_oracle_ids),\n",
    "                           use_cache=True, return_dict=True)\n",
    "    oracle_cache = extract_and_truncate_cache_with_bos(oracle_out.past_key_values, doc_len)\n",
    "    oracle_trunc_len = 1 + doc_len\n",
    "    correct_rope_positions_with_bos(oracle_cache, oracle_prefix_len - 1, model)\n",
    "    oracle_trunc_nll = score_answer_with_cache(\n",
    "        deepcopy_cache(oracle_cache), oracle_trunc_len,\n",
    "        query_prompt, answer_text, model, tokenizer, config)\n",
    "\n",
    "    # Condition 3: RANDOM-TRUNCATED\n",
    "    random_prefix = SURROGATE_PREFIX_TEMPLATE.format(surrogate=random_text)\n",
    "    random_prefix_enc = tokenizer(random_prefix, return_tensors=\"pt\",\n",
    "                                  add_special_tokens=False, padding=False, truncation=False)\n",
    "    random_prefix_ids = random_prefix_enc['input_ids'].to(device)\n",
    "    random_full_ids = torch.cat([bos_id, random_prefix_ids, doc_ids], dim=1)\n",
    "    random_prefix_len = 1 + random_prefix_ids.shape[1]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        random_out = model(input_ids=random_full_ids,\n",
    "                           attention_mask=torch.ones_like(random_full_ids),\n",
    "                           use_cache=True, return_dict=True)\n",
    "    random_cache = extract_and_truncate_cache_with_bos(random_out.past_key_values, doc_len)\n",
    "    random_trunc_len = 1 + doc_len\n",
    "    correct_rope_positions_with_bos(random_cache, random_prefix_len - 1, model)\n",
    "    random_trunc_nll = score_answer_with_cache(\n",
    "        deepcopy_cache(random_cache), random_trunc_len,\n",
    "        query_prompt, answer_text, model, tokenizer, config)\n",
    "\n",
    "    # Condition 4: SEPARATOR-ONLY\n",
    "    sep_only_len, sep_only_cache = build_suffix_kv_cache(\n",
    "        passage, \"\", model, tokenizer, config, separator=SUFFIX_SEPARATOR)\n",
    "    separator_only_nll = score_answer_with_cache(\n",
    "        deepcopy_cache(sep_only_cache), sep_only_len,\n",
    "        query_prompt, answer_text, model, tokenizer, config)\n",
    "\n",
    "    del bare_out, oracle_out, oracle_cache, random_out, random_cache, sep_only_cache\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return {\n",
    "        'idx': idx,\n",
    "        'bare_nll': bare_nll,\n",
    "        'oracle_trunc_nll': oracle_trunc_nll,\n",
    "        'random_trunc_nll': random_trunc_nll,\n",
    "        'separator_only_nll': separator_only_nll,\n",
    "        'bare_len': bare_len,\n",
    "        'oracle_trunc_len': oracle_trunc_len,\n",
    "        'random_trunc_len': random_trunc_len,\n",
    "        'separator_only_len': sep_only_len,\n",
    "        'doc_len': doc_len,\n",
    "        'passage_word_count': len(passage.split()),\n",
    "        'delta_oracle_vs_bare': bare_nll - oracle_trunc_nll,\n",
    "        'delta_random_vs_bare': bare_nll - random_trunc_nll,\n",
    "        'delta_oracle_vs_random': random_trunc_nll - oracle_trunc_nll,\n",
    "        'delta_seponly_vs_bare': bare_nll - separator_only_nll,\n",
    "    }\n",
    "\n",
    "\n",
    "def run_experiment(samples, dataset_name, model, tokenizer, config, results_dir,\n",
    "                   seed=42, checkpoint_every=50):\n",
    "    dataset_dir = results_dir / dataset_name\n",
    "    dataset_dir.mkdir(parents=True, exist_ok=True)\n",
    "    checkpoint_path = dataset_dir / \"checkpoint.json\"\n",
    "    results_path = dataset_dir / \"results.json\"\n",
    "\n",
    "    N = len(samples)\n",
    "    results = []\n",
    "    start_idx = 0\n",
    "\n",
    "    if checkpoint_path.exists():\n",
    "        with open(checkpoint_path, 'r') as f:\n",
    "            ckpt = json.load(f)\n",
    "        ckpt_queries = ckpt.get('sample_queries', [])\n",
    "        current_queries = [s['query'] for s in samples]\n",
    "        if ckpt_queries == current_queries:\n",
    "            results = ckpt['results']\n",
    "            start_idx = len(results)\n",
    "            print(f\"  Resuming from checkpoint: {start_idx}/{N}\")\n",
    "        else:\n",
    "            print(\"  Checkpoint mismatch. Starting fresh.\")\n",
    "\n",
    "    t_start = time.time()\n",
    "    for idx in tqdm(range(start_idx, N), initial=start_idx, total=N,\n",
    "                     desc=dataset_name):\n",
    "        result = evaluate_sample(samples[idx], idx, model, tokenizer,\n",
    "                                 config.device, seed, N)\n",
    "        results.append(result)\n",
    "\n",
    "        if (idx + 1) % checkpoint_every == 0 or idx == N - 1:\n",
    "            ckpt_data = {\n",
    "                'results': results,\n",
    "                'sample_queries': [s['query'] for s in samples],\n",
    "                'completed': len(results),\n",
    "                'total': N,\n",
    "                'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            }\n",
    "            with open(checkpoint_path, 'w') as f:\n",
    "                json.dump(ckpt_data, f)\n",
    "            elapsed = time.time() - t_start\n",
    "            rate = (idx - start_idx + 1) / elapsed if elapsed > 0 else 0\n",
    "            remaining = (N - idx - 1) / rate if rate > 0 else 0\n",
    "            tqdm.write(f\"  [{dataset_name}] {idx+1}/{N} | \"\n",
    "                       f\"{rate:.2f} s/s | ETA: {remaining/60:.1f} min\")\n",
    "\n",
    "    elapsed = time.time() - t_start\n",
    "    print(f\"  {dataset_name}: {len(results)} samples in {elapsed/60:.1f} min\")\n",
    "\n",
    "    analysis = analyze_experiment(results, dataset_name)\n",
    "\n",
    "    final = {\n",
    "        'dataset': dataset_name,\n",
    "        'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'config': {'num_samples': N, 'seed': seed},\n",
    "        'summary': analysis,\n",
    "        'per_sample_results': results,\n",
    "    }\n",
    "    with open(results_path, 'w') as f:\n",
    "        json.dump(final, f, indent=2)\n",
    "    print(f\"  Saved to {results_path}\")\n",
    "\n",
    "    return results, analysis\n",
    "\n",
    "\n",
    "def analyze_experiment(results, dataset_name):\n",
    "    bare = np.array([r['bare_nll'] for r in results])\n",
    "    oracle = np.array([r['oracle_trunc_nll'] for r in results])\n",
    "    random = np.array([r['random_trunc_nll'] for r in results])\n",
    "    seponly = np.array([r['separator_only_nll'] for r in results])\n",
    "\n",
    "    valid = (bare != 0) & (oracle != 0) & (random != 0) & (seponly != 0)\n",
    "    n_valid = int(np.sum(valid))\n",
    "    n_excluded = int(np.sum(~valid))\n",
    "\n",
    "    b, o, r, s = bare[valid], oracle[valid], random[valid], seponly[valid]\n",
    "\n",
    "    comparisons = [\n",
    "        ('Oracle vs Bare', b - o, 'Does oracle prefix help?'),\n",
    "        ('Random vs Bare', b - r, 'Does any prefix help?'),\n",
    "        ('Oracle vs Random', r - o, 'THE KEY TEST: semantic signal?'),\n",
    "        ('Sep-only vs Bare', b - s, 'Separator framing alone?'),\n",
    "    ]\n",
    "\n",
    "    print(f\"\\n  [{dataset_name}] N={n_valid} valid ({n_excluded} excluded)\")\n",
    "    print(f\"  {'Comparison':<25} {'d':>8} {'Win%':>7} {'p':>12} {'Sig':>5}\")\n",
    "    print(f\"  \" + \"-\" * 60)\n",
    "\n",
    "    comp_results = {}\n",
    "    for name, delta, question in comparisons:\n",
    "        d = cohens_d(delta)\n",
    "        win = np.mean(delta > 0) * 100\n",
    "        t_stat, p_val = stats.ttest_1samp(delta, 0)\n",
    "        sig = \"***\" if p_val < 0.001 else \"**\" if p_val < 0.01 else \"*\" if p_val < 0.05 else \"ns\"\n",
    "        print(f\"  {name:<25} {d:>8.3f} {win:>6.1f}% {p_val:>12.2e} {sig:>5}\")\n",
    "        comp_results[name] = {\n",
    "            'mean_delta': float(np.mean(delta)),\n",
    "            'cohens_d': float(d),\n",
    "            'win_rate': float(win / 100),\n",
    "            't_stat': float(t_stat),\n",
    "            'p_value': float(p_val),\n",
    "            'question': question,\n",
    "        }\n",
    "\n",
    "    # Hardness interaction for oracle\n",
    "    delta_oracle = b - o\n",
    "    r_hard, p_hard = stats.pearsonr(b, delta_oracle)\n",
    "    print(f\"  Hardness x Oracle: r={r_hard:.3f}, p={p_hard:.2e}\")\n",
    "\n",
    "    # Length stats\n",
    "    doc_lens = np.array([r['doc_len'] for r in results])[valid]\n",
    "    word_counts = np.array([r['passage_word_count'] for r in results])[valid]\n",
    "\n",
    "    return {\n",
    "        'n_total': len(results),\n",
    "        'n_valid': n_valid,\n",
    "        'n_excluded': n_excluded,\n",
    "        'comparisons': comp_results,\n",
    "        'nll_means': {\n",
    "            'bare': float(np.mean(b)), 'oracle_trunc': float(np.mean(o)),\n",
    "            'random_trunc': float(np.mean(r)), 'separator_only': float(np.mean(s)),\n",
    "        },\n",
    "        'hardness_oracle_r': float(r_hard),\n",
    "        'hardness_oracle_p': float(p_hard),\n",
    "        'doc_len_mean': float(np.mean(doc_lens)),\n",
    "        'doc_len_median': float(np.median(doc_lens)),\n",
    "        'word_count_mean': float(np.mean(word_counts)),\n",
    "        'word_count_median': float(np.median(word_counts)),\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"All shared functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Sub-experiment A — MS MARCO Multi-Passage\n",
    "# Concatenate all 10 passages per query into one ~550-word document.\n",
    "# Oracle query must find the answer in ONE passage among 10.\n",
    "\n",
    "try:\n",
    "    print(\"=\" * 70)\n",
    "    print(\"SUB-EXPERIMENT A: MS MARCO MULTI-PASSAGE\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    from datasets import load_dataset as hf_load_dataset\n",
    "\n",
    "    print(\"  Loading MS MARCO v2.1 (all passages)...\")\n",
    "    ds_marco = hf_load_dataset('microsoft/ms_marco', 'v2.1', split='validation')\n",
    "\n",
    "    msmarco_mp_candidates = []\n",
    "    for item in ds_marco:\n",
    "        answers = item['answers']\n",
    "        if not answers or not answers[0] or answers[0] == 'No Answer Present.':\n",
    "            continue\n",
    "        passages = item['passages']['passage_text']\n",
    "        full_doc = '\\n\\n'.join(passages)\n",
    "        word_count = len(full_doc.split())\n",
    "        if word_count < 100:\n",
    "            continue\n",
    "        query_text = item['query'].strip('. ')\n",
    "        msmarco_mp_candidates.append({\n",
    "            'passage': full_doc,\n",
    "            'query': query_text,\n",
    "            'answer': answers[0],\n",
    "        })\n",
    "\n",
    "    np.random.seed(SEED)\n",
    "    np.random.shuffle(msmarco_mp_candidates)\n",
    "    msmarco_mp_samples = msmarco_mp_candidates[:1500]\n",
    "\n",
    "    wc = np.array([len(s['passage'].split()) for s in msmarco_mp_samples])\n",
    "    print(f\"  Loaded {len(msmarco_mp_samples)} samples\")\n",
    "    print(f\"  Word counts: mean={wc.mean():.0f}, median={np.median(wc):.0f}, \"\n",
    "          f\"min={wc.min()}, max={wc.max()}\")\n",
    "    print(f\"  Example query: {msmarco_mp_samples[0]['query'][:80]}...\")\n",
    "    print(f\"  Example answer: {msmarco_mp_samples[0]['answer'][:80]}...\")\n",
    "\n",
    "    msmarco_mp_results, msmarco_mp_analysis = run_experiment(\n",
    "        msmarco_mp_samples, \"msmarco_mp\", model, tokenizer, config,\n",
    "        RESULTS_DIR, seed=SEED)\n",
    "\n",
    "    del ds_marco, msmarco_mp_candidates\n",
    "except Exception as e:\n",
    "    print(f\"MS MARCO multi-passage FAILED: {e}\")\n",
    "    traceback.print_exc()\n",
    "    msmarco_mp_results, msmarco_mp_analysis = None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Sub-experiment B — SQuAD 2.0\n",
    "# Wikipedia paragraphs (~150 words). Intermediate length.\n",
    "\n",
    "try:\n",
    "    print(\"=\" * 70)\n",
    "    print(\"SUB-EXPERIMENT B: SQuAD 2.0\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    from datasets import load_dataset as hf_load_dataset\n",
    "\n",
    "    print(\"  Loading SQuAD 2.0...\")\n",
    "    ds_squad = hf_load_dataset('rajpurkar/squad_v2', split='validation')\n",
    "\n",
    "    squad_candidates = []\n",
    "    for item in ds_squad:\n",
    "        answers = item['answers']\n",
    "        if not answers['text']:\n",
    "            continue  # unanswerable\n",
    "        answer_text = answers['text'][0]\n",
    "        if len(answer_text.split()) < 2:\n",
    "            continue  # skip single-word (likely single-token NLL=0)\n",
    "        squad_candidates.append({\n",
    "            'passage': item['context'],\n",
    "            'query': item['question'],\n",
    "            'answer': answer_text,\n",
    "        })\n",
    "\n",
    "    np.random.seed(SEED + 1)\n",
    "    np.random.shuffle(squad_candidates)\n",
    "    squad_samples = squad_candidates[:1500]\n",
    "\n",
    "    wc = np.array([len(s['passage'].split()) for s in squad_samples])\n",
    "    print(f\"  Loaded {len(squad_samples)} samples (from {len(squad_candidates)} candidates)\")\n",
    "    print(f\"  Word counts: mean={wc.mean():.0f}, median={np.median(wc):.0f}, \"\n",
    "          f\"min={wc.min()}, max={wc.max()}\")\n",
    "    print(f\"  Example query: {squad_samples[0]['query'][:80]}...\")\n",
    "    print(f\"  Example answer: {squad_samples[0]['answer'][:80]}...\")\n",
    "\n",
    "    squad_results, squad_analysis = run_experiment(\n",
    "        squad_samples, \"squad\", model, tokenizer, config,\n",
    "        RESULTS_DIR, seed=SEED)\n",
    "\n",
    "    del ds_squad, squad_candidates\n",
    "except Exception as e:\n",
    "    print(f\"SQuAD FAILED: {e}\")\n",
    "    traceback.print_exc()\n",
    "    squad_results, squad_analysis = None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Sub-experiment C — Natural Questions\n",
    "# Wikipedia articles windowed to ~800 words around the short answer.\n",
    "\n",
    "try:\n",
    "    print(\"=\" * 70)\n",
    "    print(\"SUB-EXPERIMENT C: NATURAL QUESTIONS\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    from datasets import load_dataset as hf_load_dataset\n",
    "\n",
    "    print(\"  Loading Natural Questions (streaming)...\")\n",
    "    ds_nq = hf_load_dataset('google-research-datasets/natural_questions',\n",
    "                             'default', split='validation', streaming=True)\n",
    "\n",
    "    WINDOW_WORDS = 800\n",
    "    nq_candidates = []\n",
    "    n_scanned = 0\n",
    "\n",
    "    for item in ds_nq:\n",
    "        n_scanned += 1\n",
    "        if len(nq_candidates) >= 2400:\n",
    "            break\n",
    "\n",
    "        # Find short answer\n",
    "        short_answers = item['annotations']['short_answers']\n",
    "        answer_text = None\n",
    "        for sa in short_answers:\n",
    "            if sa['text'] and len(sa['text']) > 0:\n",
    "                candidate = sa['text'][0]\n",
    "                if len(candidate.split()) >= 2:\n",
    "                    answer_text = candidate\n",
    "                    break\n",
    "        if answer_text is None:\n",
    "            continue\n",
    "\n",
    "        # Extract plain text from document tokens\n",
    "        tokens = item['document']['tokens']\n",
    "        plain_tokens = [t for t, h in zip(tokens['token'], tokens['is_html']) if not h]\n",
    "        full_text = ' '.join(plain_tokens)\n",
    "\n",
    "        # Find answer in text\n",
    "        answer_pos = full_text.lower().find(answer_text.lower())\n",
    "        if answer_pos == -1:\n",
    "            continue\n",
    "\n",
    "        # Window around answer\n",
    "        words = full_text.split()\n",
    "        char_count = 0\n",
    "        answer_word_pos = 0\n",
    "        for wi, w in enumerate(words):\n",
    "            if char_count >= answer_pos:\n",
    "                answer_word_pos = wi\n",
    "                break\n",
    "            char_count += len(w) + 1\n",
    "\n",
    "        half = WINDOW_WORDS // 2\n",
    "        start = max(0, answer_word_pos - half)\n",
    "        end = min(len(words), start + WINDOW_WORDS)\n",
    "        start = max(0, end - WINDOW_WORDS)\n",
    "\n",
    "        passage = ' '.join(words[start:end])\n",
    "        if len(passage.split()) < 100:\n",
    "            continue\n",
    "\n",
    "        question = item['question']\n",
    "        if isinstance(question, dict):\n",
    "            question = question.get('text', str(question))\n",
    "\n",
    "        nq_candidates.append({\n",
    "            'passage': passage,\n",
    "            'query': question,\n",
    "            'answer': answer_text,\n",
    "        })\n",
    "\n",
    "    print(f\"  Scanned {n_scanned} items, found {len(nq_candidates)} candidates\")\n",
    "\n",
    "    np.random.seed(SEED + 2)\n",
    "    np.random.shuffle(nq_candidates)\n",
    "    nq_samples = nq_candidates[:800]\n",
    "\n",
    "    wc = np.array([len(s['passage'].split()) for s in nq_samples])\n",
    "    print(f\"  Selected {len(nq_samples)} samples\")\n",
    "    print(f\"  Word counts: mean={wc.mean():.0f}, median={np.median(wc):.0f}, \"\n",
    "          f\"min={wc.min()}, max={wc.max()}\")\n",
    "    print(f\"  Example query: {nq_samples[0]['query'][:80]}...\")\n",
    "    print(f\"  Example answer: {nq_samples[0]['answer'][:80]}...\")\n",
    "\n",
    "    nq_results, nq_analysis = run_experiment(\n",
    "        nq_samples, \"nq\", model, tokenizer, config,\n",
    "        RESULTS_DIR, seed=SEED)\n",
    "\n",
    "    del nq_candidates\n",
    "except Exception as e:\n",
    "    print(f\"Natural Questions FAILED: {e}\")\n",
    "    traceback.print_exc()\n",
    "    nq_results, nq_analysis = None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Sub-experiment D — TriviaQA\n",
    "# Wikipedia/web evidence documents windowed to ~800 words around the answer.\n",
    "\n",
    "try:\n",
    "    print(\"=\" * 70)\n",
    "    print(\"SUB-EXPERIMENT D: TriviaQA\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    from datasets import load_dataset as hf_load_dataset\n",
    "\n",
    "    print(\"  Loading TriviaQA RC (streaming)...\")\n",
    "    ds_tqa = hf_load_dataset('trivia_qa', 'rc', split='validation', streaming=True)\n",
    "\n",
    "    WINDOW_WORDS = 800\n",
    "    tqa_candidates = []\n",
    "    n_scanned = 0\n",
    "\n",
    "    for item in ds_tqa:\n",
    "        n_scanned += 1\n",
    "        if len(tqa_candidates) >= 2400:\n",
    "            break\n",
    "\n",
    "        answer_text = item['answer']['value']\n",
    "        if len(answer_text.split()) < 2:\n",
    "            continue\n",
    "\n",
    "        # Find evidence containing the answer\n",
    "        wiki_contexts = item['entity_pages'].get('wiki_context', [])\n",
    "        search_contexts = item['search_results'].get('search_context', [])\n",
    "\n",
    "        context = None\n",
    "        for ctx in wiki_contexts + search_contexts:\n",
    "            if answer_text.lower() in ctx.lower():\n",
    "                context = ctx\n",
    "                break\n",
    "        if context is None:\n",
    "            continue\n",
    "\n",
    "        # Window around answer\n",
    "        answer_pos = context.lower().find(answer_text.lower())\n",
    "        words = context.split()\n",
    "        char_count = 0\n",
    "        answer_word_pos = 0\n",
    "        for wi, w in enumerate(words):\n",
    "            if char_count >= answer_pos:\n",
    "                answer_word_pos = wi\n",
    "                break\n",
    "            char_count += len(w) + 1\n",
    "\n",
    "        half = WINDOW_WORDS // 2\n",
    "        start = max(0, answer_word_pos - half)\n",
    "        end = min(len(words), start + WINDOW_WORDS)\n",
    "        start = max(0, end - WINDOW_WORDS)\n",
    "\n",
    "        passage = ' '.join(words[start:end])\n",
    "        if len(passage.split()) < 100:\n",
    "            continue\n",
    "\n",
    "        tqa_candidates.append({\n",
    "            'passage': passage,\n",
    "            'query': item['question'],\n",
    "            'answer': answer_text,\n",
    "        })\n",
    "\n",
    "    print(f\"  Scanned {n_scanned} items, found {len(tqa_candidates)} candidates\")\n",
    "\n",
    "    np.random.seed(SEED + 3)\n",
    "    np.random.shuffle(tqa_candidates)\n",
    "    tqa_samples = tqa_candidates[:800]\n",
    "\n",
    "    wc = np.array([len(s['passage'].split()) for s in tqa_samples])\n",
    "    print(f\"  Selected {len(tqa_samples)} samples\")\n",
    "    print(f\"  Word counts: mean={wc.mean():.0f}, median={np.median(wc):.0f}, \"\n",
    "          f\"min={wc.min()}, max={wc.max()}\")\n",
    "    print(f\"  Example query: {tqa_samples[0]['query'][:80]}...\")\n",
    "    print(f\"  Example answer: {tqa_samples[0]['answer'][:80]}...\")\n",
    "\n",
    "    tqa_results, tqa_analysis = run_experiment(\n",
    "        tqa_samples, \"triviaqa\", model, tokenizer, config,\n",
    "        RESULTS_DIR, seed=SEED)\n",
    "\n",
    "    del tqa_candidates\n",
    "except Exception as e:\n",
    "    print(f\"TriviaQA FAILED: {e}\")\n",
    "    traceback.print_exc()\n",
    "    tqa_results, tqa_analysis = None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Cross-dataset comparison table\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"CROSS-DATASET COMPARISON\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "all_analyses = {}\n",
    "for name, analysis in [('msmarco_mp', msmarco_mp_analysis),\n",
    "                        ('squad', squad_analysis),\n",
    "                        ('nq', nq_analysis),\n",
    "                        ('triviaqa', tqa_analysis)]:\n",
    "    if analysis is not None:\n",
    "        all_analyses[name] = analysis\n",
    "\n",
    "if not all_analyses:\n",
    "    print(\"No experiments completed successfully!\")\n",
    "else:\n",
    "    print(f\"\\n{'Dataset':<18} {'N':>6} {'Words':>8} {'Tokens':>8} \"\n",
    "          f\"{'d(Orc-Bare)':>12} {'d(Rnd-Bare)':>12} {'d(Orc-Rnd)':>12} {'d(Sep-Bare)':>12} \"\n",
    "          f\"{'Hard r':>8}\")\n",
    "    print(\"-\" * 110)\n",
    "\n",
    "    for name, a in all_analyses.items():\n",
    "        c = a['comparisons']\n",
    "        d_ob = c['Oracle vs Bare']['cohens_d']\n",
    "        d_rb = c['Random vs Bare']['cohens_d']\n",
    "        d_or = c['Oracle vs Random']['cohens_d']\n",
    "        d_sb = c['Sep-only vs Bare']['cohens_d']\n",
    "        p_or = c['Oracle vs Random']['p_value']\n",
    "        sig = \"***\" if p_or < 0.001 else \"**\" if p_or < 0.01 else \"*\" if p_or < 0.05 else \"ns\"\n",
    "\n",
    "        print(f\"{name:<18} {a['n_valid']:>6} {a['word_count_mean']:>7.0f} \"\n",
    "              f\"{a['doc_len_mean']:>7.0f} \"\n",
    "              f\"{d_ob:>+12.3f} {d_rb:>+12.3f} {d_or:>+12.3f}{sig:<3} \"\n",
    "              f\"{d_sb:>+12.3f} {a['hardness_oracle_r']:>8.3f}\")\n",
    "\n",
    "    print(f\"\\n{'_' * 110}\")\n",
    "    print(\"KEY: d(Orc-Rnd) is THE critical test. Positive = semantic priming works.\")\n",
    "    print(\"     Negative = oracle interferes (same as Exp 01).\")\n",
    "    print(f\"\\nInterpretation:\")\n",
    "    for name, a in all_analyses.items():\n",
    "        d_or = a['comparisons']['Oracle vs Random']['cohens_d']\n",
    "        p_or = a['comparisons']['Oracle vs Random']['p_value']\n",
    "        if p_or < 0.05 and d_or > 0:\n",
    "            print(f\"  {name}: SEMANTIC PRIMING DETECTED (d={d_or:+.3f}, p={p_or:.2e})\")\n",
    "        elif p_or < 0.05 and d_or < 0:\n",
    "            print(f\"  {name}: Oracle INTERFERES (d={d_or:+.3f}, p={p_or:.2e})\")\n",
    "        else:\n",
    "            print(f\"  {name}: No semantic signal (d={d_or:+.3f}, p={p_or:.2e}, ns)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Cross-dataset plots\n",
    "\n",
    "if all_analyses:\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    datasets = list(all_analyses.keys())\n",
    "    x = np.arange(len(datasets))\n",
    "\n",
    "    # Plot 1: Oracle vs Random d (THE KEY TEST)\n",
    "    d_vals = [all_analyses[d]['comparisons']['Oracle vs Random']['cohens_d'] for d in datasets]\n",
    "    colors = ['green' if d > 0 else 'red' for d in d_vals]\n",
    "    axes[0].bar(x, d_vals, color=colors, alpha=0.7, edgecolor='black')\n",
    "    axes[0].axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "    axes[0].set_xticks(x)\n",
    "    axes[0].set_xticklabels(datasets, rotation=15)\n",
    "    axes[0].set_ylabel(\"Cohen's d\")\n",
    "    axes[0].set_title(\"Oracle vs Random (+ = semantic priming works)\")\n",
    "\n",
    "    # Plot 2: All conditions vs Bare\n",
    "    width = 0.2\n",
    "    for ci, (cname, key) in enumerate([\n",
    "        ('Oracle', 'Oracle vs Bare'), ('Random', 'Random vs Bare'),\n",
    "        ('Sep-only', 'Sep-only vs Bare')]):\n",
    "        d_vals = [all_analyses[d]['comparisons'][key]['cohens_d'] for d in datasets]\n",
    "        axes[1].bar(x + ci * width, d_vals, width, label=cname, alpha=0.8)\n",
    "    axes[1].axhline(y=0, color='gray', linestyle='--')\n",
    "    axes[1].set_xticks(x + width)\n",
    "    axes[1].set_xticklabels(datasets, rotation=15)\n",
    "    axes[1].set_ylabel(\"Cohen's d vs Bare\")\n",
    "    axes[1].set_title(\"All Conditions vs Bare\")\n",
    "    axes[1].legend()\n",
    "\n",
    "    # Plot 3: Effect size vs document length\n",
    "    doc_lens = [all_analyses[d]['doc_len_mean'] for d in datasets]\n",
    "    d_or = [all_analyses[d]['comparisons']['Oracle vs Random']['cohens_d'] for d in datasets]\n",
    "    d_rb = [all_analyses[d]['comparisons']['Random vs Bare']['cohens_d'] for d in datasets]\n",
    "    axes[2].scatter(doc_lens, d_or, s=100, c='steelblue', label='Oracle vs Random', zorder=3)\n",
    "    axes[2].scatter(doc_lens, d_rb, s=100, c='darkorange', marker='^', label='Random vs Bare', zorder=3)\n",
    "    axes[2].axhline(y=0, color='gray', linestyle='--')\n",
    "    for i, name in enumerate(datasets):\n",
    "        axes[2].annotate(name, (doc_lens[i], d_or[i]), textcoords=\"offset points\",\n",
    "                          xytext=(5, 5), fontsize=8)\n",
    "    axes[2].set_xlabel('Mean document token length')\n",
    "    axes[2].set_ylabel(\"Cohen's d\")\n",
    "    axes[2].set_title(\"Effect Size vs Document Length\")\n",
    "    axes[2].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(RESULTS_DIR / 'cross_dataset_comparison.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f\"Plot saved to {RESULTS_DIR / 'cross_dataset_comparison.png'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Save combined results\n",
    "\n",
    "combined = {\n",
    "    'experiment': 'exp04_semantic_priming_battery',\n",
    "    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'datasets': {},\n",
    "}\n",
    "\n",
    "for name, analysis in all_analyses.items():\n",
    "    combined['datasets'][name] = analysis\n",
    "\n",
    "combined_path = RESULTS_DIR / 'combined_results.json'\n",
    "with open(combined_path, 'w') as f:\n",
    "    json.dump(combined, f, indent=2)\n",
    "\n",
    "print(f\"Combined results saved to {combined_path}\")\n",
    "print(f\"Individual dataset results in {RESULTS_DIR}/{{dataset_name}}/results.json\")\n",
    "print(f\"\\nDone! Total datasets completed: {len(all_analyses)}\")"
   ]
  },
  {
   "cell_type": "code",
   "source": "# Cell 11: GPU cleanup — free all VRAM before next notebook\nimport gc\n\nprint(\"Cleaning up GPU memory...\")\nmem_before = torch.cuda.memory_allocated() / 1e9\n\n# Delete model and tokenizer\ndel model\ndel tokenizer\n\n# Clear all remaining tensors\ngc.collect()\ntorch.cuda.empty_cache()\ngc.collect()\n\nmem_after = torch.cuda.memory_allocated() / 1e9\nprint(f\"GPU memory: {mem_before:.2f} GB -> {mem_after:.2f} GB\")\nprint(\"Cleanup complete. Safe to start next notebook.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}