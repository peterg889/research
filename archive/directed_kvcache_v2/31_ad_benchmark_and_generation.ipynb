{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exp 31: Ad-Content Benchmark & Generation Quality (Gemma 3 4B)\n",
    "\n",
    "## Motivation\n",
    "\n",
    "Prior experiments showed hero layer value contamination helps on short factoid QA\n",
    "(MS MARCO d=+0.472, NQ d=+0.213) but fails on long documents and non-retrieval tasks.\n",
    "For ad-serving, the content is typically short (product descriptions, review snippets)\n",
    "and the task is generation (producing relevant responses), not just NLL scoring.\n",
    "\n",
    "This experiment tests two questions:\n",
    "1. **Does hero layer priming help on ad-like short content?** (NLL benchmark)\n",
    "2. **Does lower NLL translate to better generated answers?** (Generation quality)\n",
    "\n",
    "## Datasets\n",
    "\n",
    "| Dataset | Content Type | Why Chosen | N |\n",
    "|---------|-------------|-----------|---|\n",
    "| **Amazon SubjQA** | Product reviews (electronics + grocery) | Commercial content closest to ads | 300 |\n",
    "| **MS MARCO** | Web passages (<250 words) | Known positive control for priming | 300 |\n",
    "| **SQuAD** | Wikipedia paragraphs (<250 words) | Clean extractive QA, new short-passage data point | 300 |\n",
    "\n",
    "All passages filtered to 30-250 words to stay in the \"short content\" regime.\n",
    "\n",
    "## Two Phases\n",
    "\n",
    "**Phase 1: NLL Evaluation** (900 samples x 4 conditions)\n",
    "- bare, sf_trunc, values_early, values_hero\n",
    "- Same methodology as Exp 30\n",
    "\n",
    "**Phase 2: Generation Quality** (hard subset x 2 conditions)\n",
    "- Greedy generation with bare vs hero caches\n",
    "- Metrics: Exact Match (substring), Token F1, ROUGE-L, generation confidence"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 1: Setup\n",
    "import os\n",
    "os.umask(0o000)\n",
    "\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "import gc\n",
    "import csv\n",
    "import string\n",
    "import numpy as np\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "RESULTS_DIR = Path(\"results/exp31\")\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CHECKPOINT_PATH = RESULTS_DIR / \"checkpoint.json\"\n",
    "GEN_CHECKPOINT_PATH = RESULTS_DIR / \"gen_checkpoint.json\"\n",
    "FINAL_RESULTS_PATH = RESULTS_DIR / \"results.json\"\n",
    "CSV_PATH = RESULTS_DIR / \"results.csv\"\n",
    "GEN_CSV_PATH = RESULTS_DIR / \"gen_results.csv\"\n",
    "\n",
    "print(f\"SEED: {SEED}\")\n",
    "print(f\"Results directory: {RESULTS_DIR}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 2: Load Gemma 3 4B via load_model()\n",
    "sys.path.insert(0, \".\")\n",
    "\n",
    "from lib.config import ExperimentConfig\n",
    "from lib.model_utils import load_model\n",
    "\n",
    "MODEL_NAME = \"google/gemma-3-4b-it\"\n",
    "\n",
    "exp_config = ExperimentConfig(\n",
    "    model_name=MODEL_NAME,\n",
    "    model_type=\"gemma3\",\n",
    "    compute_dtype=\"auto\",  # resolves to bfloat16 for Gemma\n",
    "    use_4bit=True,\n",
    "    num_samples=2000,\n",
    "    seed=SEED,\n",
    ")\n",
    "\n",
    "print(f\"Loading {MODEL_NAME} (4-bit, bfloat16)...\")\n",
    "model, tokenizer = load_model(exp_config)\n",
    "\n",
    "from lib.kv_cache import _get_text_config, _get_head_dim, _ensure_dynamic_cache, _get_cache_keys\n",
    "\n",
    "text_config = _get_text_config(model.config)\n",
    "N_LAYERS = text_config.num_hidden_layers\n",
    "print(f\"\\nModel loaded successfully.\")\n",
    "print(f\"  Num layers: {N_LAYERS}\")\n",
    "print(f\"  Head dim: {_get_head_dim(model.config)}\")\n",
    "print(f\"  Model dtype: {model.dtype}\")\n",
    "print(f\"  Sliding window: {getattr(text_config, 'sliding_window', 'N/A')}\")\n",
    "\n",
    "# Verify with test forward pass\n",
    "sample_ids = tokenizer(\"test\", return_tensors=\"pt\")['input_ids'].to(exp_config.device)\n",
    "with torch.no_grad():\n",
    "    out = model(sample_ids, use_cache=True)\n",
    "    cache_check = _ensure_dynamic_cache(out.past_key_values)\n",
    "    k0 = _get_cache_keys(cache_check, 0)\n",
    "    print(f\"  Cache key dtype: {k0.dtype}\")\n",
    "    print(f\"  Cache key shape: {k0.shape}\")\n",
    "del out, sample_ids, cache_check\n",
    "torch.cuda.empty_cache()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 3: Lib imports + constants\n",
    "from lib.kv_cache import (\n",
    "    _get_cache_keys,\n",
    "    _get_cache_values,\n",
    "    _set_cache_keys,\n",
    "    _set_cache_values,\n",
    "    _ensure_dynamic_cache,\n",
    "    extract_and_truncate_cache_with_bos,\n",
    "    correct_rope_positions_with_bos,\n",
    "    score_answer_with_cache,\n",
    "    deepcopy_cache,\n",
    "    replace_values_at_layers,\n",
    ")\n",
    "from lib.analysis import cohens_d\n",
    "from lib.data import count_words\n",
    "from scipy import stats\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Templates -- bare text, no \"Document:\\n\" framing\n",
    "SURROGATE_PREFIX_TEMPLATE = \"{surrogate}\\n\"\n",
    "DOCUMENT_TEMPLATE = \"{document}\"\n",
    "QUERY_TEMPLATE = \"\\nQuestion: {question}\\nAnswer:\"\n",
    "ANSWER_TEMPLATE = \" {answer}\"\n",
    "\n",
    "# Prefix text\n",
    "from lib.surrogate import STATIC_SURROGATE_QUERIES\n",
    "STATIC_FACT = STATIC_SURROGATE_QUERIES['static_factual']['query']\n",
    "\n",
    "# Experiment parameters\n",
    "N_PER_DATASET = 300\n",
    "MAX_DOC_TOKENS = 900\n",
    "MAX_PASSAGE_WORDS = 250\n",
    "MIN_PASSAGE_WORDS = 30\n",
    "CHECKPOINT_EVERY = 25\n",
    "MAX_GEN_TOKENS = 50\n",
    "\n",
    "# Conditions (same 4 as Exp 30)\n",
    "CONDITION_NAMES = ['bare', 'sf_trunc', 'values_early', 'values_hero']\n",
    "\n",
    "# Layer-selective conditions from Exps 19/21/24\n",
    "EARLY_LAYER_CUTOFF = 16  # layers 0-15\n",
    "HERO_LAYERS = [10, 12, 14, 15, 20]  # from Exp 24 single-layer scan\n",
    "\n",
    "print(\"Config ready\")\n",
    "print(f\"  Model: {MODEL_NAME}\")\n",
    "print(f\"  N per dataset: {N_PER_DATASET}\")\n",
    "print(f\"  MAX_DOC_TOKENS: {MAX_DOC_TOKENS} (sliding window constraint)\")\n",
    "print(f\"  MAX_PASSAGE_WORDS: {MAX_PASSAGE_WORDS}\")\n",
    "print(f\"  MAX_GEN_TOKENS: {MAX_GEN_TOKENS}\")\n",
    "print(f\"  N_LAYERS: {N_LAYERS}\")\n",
    "print(f\"  EARLY_LAYER_CUTOFF: {EARLY_LAYER_CUTOFF}\")\n",
    "print(f\"  HERO_LAYERS: {HERO_LAYERS}\")\n",
    "print(f\"  Conditions: {CONDITION_NAMES}\")\n",
    "print(f\"  Static fact prefix: '{STATIC_FACT}'\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 4: Load Amazon SubjQA (electronics + grocery)\n",
    "from datasets import load_dataset\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"LOADING AMAZON SubjQA (electronics + grocery)\")\n",
    "print(\"=\" * 70)\n",
    "print(\"Product review QA. Commercial content closest to ad-serving use case.\")\n",
    "\n",
    "SUBJQA_CACHE = RESULTS_DIR / \"subjqa_samples.json\"\n",
    "\n",
    "if SUBJQA_CACHE.exists():\n",
    "    with open(SUBJQA_CACHE, 'r') as f:\n",
    "        subjqa_samples = json.load(f)\n",
    "    print(f\"Loaded {len(subjqa_samples)} cached SubjQA samples\")\n",
    "else:\n",
    "    subjqa_samples = []\n",
    "    try:\n",
    "        # datasets v4.5+ dropped loading scripts; use parquet branch\n",
    "        ds = load_dataset(\"megagonlabs/subjqa\", revision=\"refs/convert/parquet\")\n",
    "        print(f\"  Loaded SubjQA from parquet branch ({list(ds.keys())} splits)\")\n",
    "    except Exception as e:\n",
    "        print(f\"  FAILED to load SubjQA: {e}\")\n",
    "        ds = None\n",
    "\n",
    "    if ds is not None:\n",
    "        for domain in ['electronics', 'grocery']:\n",
    "            n_domain = 0\n",
    "            for split_name in ds:\n",
    "                for item in ds[split_name]:\n",
    "                    if item.get('domain', '') != domain:\n",
    "                        continue\n",
    "                    context = item.get('context', '')\n",
    "                    question = item.get('question', '')\n",
    "                    answers = item.get('answers', {})\n",
    "                    answer_texts = answers.get('text', [])\n",
    "                    if not answer_texts or not question or not context:\n",
    "                        continue\n",
    "                    answer_text = answer_texts[0]\n",
    "                    if not answer_text.strip():\n",
    "                        continue\n",
    "                    wc = count_words(context)\n",
    "                    if MIN_PASSAGE_WORDS <= wc <= MAX_PASSAGE_WORDS:\n",
    "                        subjqa_samples.append({\n",
    "                            'passage': context,\n",
    "                            'query': question,\n",
    "                            'answer': answer_text,\n",
    "                            'word_count': wc,\n",
    "                            'dataset': 'subjqa',\n",
    "                            'domain': domain,\n",
    "                        })\n",
    "                        n_domain += 1\n",
    "            print(f\"  {domain}: {n_domain} samples\")\n",
    "\n",
    "    # Deduplicate by question text\n",
    "    seen_queries = set()\n",
    "    unique_samples = []\n",
    "    for sample in subjqa_samples:\n",
    "        if sample['query'] not in seen_queries:\n",
    "            seen_queries.add(sample['query'])\n",
    "            unique_samples.append(sample)\n",
    "    subjqa_samples = unique_samples\n",
    "    print(f\"  After dedup: {len(subjqa_samples)} unique samples\")\n",
    "\n",
    "    np.random.seed(SEED)\n",
    "    np.random.shuffle(subjqa_samples)\n",
    "    subjqa_samples = subjqa_samples[:N_PER_DATASET]\n",
    "\n",
    "    with open(SUBJQA_CACHE, 'w') as f:\n",
    "        json.dump(subjqa_samples, f)\n",
    "    print(f\"Cached {len(subjqa_samples)} samples\")\n",
    "\n",
    "print(f\"\\nSubjQA samples: {len(subjqa_samples)}\")\n",
    "if subjqa_samples:\n",
    "    wcs = [s['word_count'] for s in subjqa_samples]\n",
    "    domains = [s.get('domain', '?') for s in subjqa_samples]\n",
    "    domain_counts = Counter(domains)\n",
    "    print(f\"  Word counts: mean={np.mean(wcs):.0f}, min={min(wcs)}, max={max(wcs)}\")\n",
    "    print(f\"  Domain distribution: {dict(domain_counts)}\")\n",
    "    ans_lens = [len(s['answer'].split()) for s in subjqa_samples]\n",
    "    print(f\"  Answer word lengths: mean={np.mean(ans_lens):.1f}, max={max(ans_lens)}\")\n",
    "    for i in range(min(3, len(subjqa_samples))):\n",
    "        print(f\"  Example {i+1}:\")\n",
    "        print(f\"    Q: {subjqa_samples[i]['query']}\")\n",
    "        print(f\"    A: {subjqa_samples[i]['answer']}\")\n",
    "        print(f\"    Context (first 120 chars): {subjqa_samples[i]['passage'][:120]}...\")\n",
    "else:\n",
    "    print(\"  WARNING: No SubjQA samples loaded! Check dataset availability.\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 5: Load MS MARCO short passages (<250 words)\n",
    "print(\"=\" * 70)\n",
    "print(\"LOADING MS MARCO (validation, short passages only)\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Short web passages (<{MAX_PASSAGE_WORDS} words). Known positive control for priming.\")\n",
    "\n",
    "MARCO_CACHE = RESULTS_DIR / \"marco_samples.json\"\n",
    "\n",
    "if MARCO_CACHE.exists():\n",
    "    with open(MARCO_CACHE, 'r') as f:\n",
    "        marco_samples = json.load(f)\n",
    "    print(f\"Loaded {len(marco_samples)} cached MS MARCO samples\")\n",
    "else:\n",
    "    marco_ds = load_dataset(\"ms_marco\", \"v1.1\", split=\"validation\", trust_remote_code=True)\n",
    "    print(f\"MS MARCO validation size: {len(marco_ds)}\")\n",
    "\n",
    "    marco_samples = []\n",
    "    np.random.seed(SEED)\n",
    "\n",
    "    for item in tqdm(marco_ds, desc=\"Filtering MARCO\"):\n",
    "        passages = item.get('passages', {})\n",
    "        passage_texts = passages.get('passage_text', [])\n",
    "        is_selected = passages.get('is_selected', [])\n",
    "        query = item.get('query', '')\n",
    "        answers = item.get('answers', [])\n",
    "        well_formed = item.get('wellFormedAnswers', [])\n",
    "\n",
    "        if not passage_texts or not query:\n",
    "            continue\n",
    "\n",
    "        # Get best answer\n",
    "        answer = None\n",
    "        if well_formed and len(well_formed) > 0 and well_formed[0] != '[]':\n",
    "            answer = well_formed[0]\n",
    "        elif answers and len(answers) > 0 and answers[0] != 'No Answer Present.':\n",
    "            answer = answers[0]\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        # Find short selected passage\n",
    "        for i, passage in enumerate(passage_texts):\n",
    "            wc = count_words(passage)\n",
    "            if MIN_PASSAGE_WORDS <= wc <= MAX_PASSAGE_WORDS:\n",
    "                if is_selected and i < len(is_selected) and is_selected[i] == 1:\n",
    "                    marco_samples.append({\n",
    "                        'passage': passage,\n",
    "                        'query': query,\n",
    "                        'answer': answer,\n",
    "                        'word_count': wc,\n",
    "                        'dataset': 'marco',\n",
    "                    })\n",
    "                    break\n",
    "\n",
    "        if len(marco_samples) >= N_PER_DATASET * 3:\n",
    "            break\n",
    "\n",
    "    np.random.shuffle(marco_samples)\n",
    "    marco_samples = marco_samples[:N_PER_DATASET]\n",
    "\n",
    "    with open(MARCO_CACHE, 'w') as f:\n",
    "        json.dump(marco_samples, f)\n",
    "    print(f\"Cached {len(marco_samples)} samples\")\n",
    "\n",
    "    del marco_ds\n",
    "    gc.collect()\n",
    "\n",
    "print(f\"\\nMS MARCO samples: {len(marco_samples)}\")\n",
    "if marco_samples:\n",
    "    wcs = [s['word_count'] for s in marco_samples]\n",
    "    ans_lens = [len(s['answer'].split()) for s in marco_samples]\n",
    "    print(f\"  Word counts: mean={np.mean(wcs):.0f}, min={min(wcs)}, max={max(wcs)}\")\n",
    "    print(f\"  Answer word lengths: mean={np.mean(ans_lens):.1f}, max={max(ans_lens)}\")\n",
    "    for i in range(min(3, len(marco_samples))):\n",
    "        print(f\"  Example {i+1}:\")\n",
    "        print(f\"    Q: {marco_samples[i]['query']}\")\n",
    "        print(f\"    A: {marco_samples[i]['answer']}\")\n",
    "        print(f\"    Passage (first 120 chars): {marco_samples[i]['passage'][:120]}...\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 6: Load SQuAD short passages (<250 words)\n",
    "print(\"=\" * 70)\n",
    "print(\"LOADING SQuAD (validation, short passages only)\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Short Wikipedia paragraphs (<{MAX_PASSAGE_WORDS} words). Clean extractive QA baseline.\")\n",
    "\n",
    "SQUAD_CACHE = RESULTS_DIR / \"squad_samples.json\"\n",
    "\n",
    "if SQUAD_CACHE.exists():\n",
    "    with open(SQUAD_CACHE, 'r') as f:\n",
    "        squad_samples = json.load(f)\n",
    "    print(f\"Loaded {len(squad_samples)} cached SQuAD samples\")\n",
    "else:\n",
    "    squad_ds = load_dataset(\"squad\", split=\"validation\")\n",
    "    print(f\"SQuAD validation size: {len(squad_ds)}\")\n",
    "\n",
    "    squad_samples = []\n",
    "    np.random.seed(SEED)\n",
    "\n",
    "    for item in tqdm(squad_ds, desc=\"Filtering SQuAD\"):\n",
    "        context = item.get('context', '')\n",
    "        question = item.get('question', '')\n",
    "        answers = item.get('answers', {})\n",
    "        answer_texts = answers.get('text', [])\n",
    "\n",
    "        if not answer_texts or not question or not context:\n",
    "            continue\n",
    "\n",
    "        answer_text = answer_texts[0]\n",
    "        if not answer_text.strip():\n",
    "            continue\n",
    "\n",
    "        wc = count_words(context)\n",
    "        if MIN_PASSAGE_WORDS <= wc <= MAX_PASSAGE_WORDS:\n",
    "            squad_samples.append({\n",
    "                'passage': context,\n",
    "                'query': question,\n",
    "                'answer': answer_text,\n",
    "                'word_count': wc,\n",
    "                'dataset': 'squad',\n",
    "            })\n",
    "\n",
    "    np.random.shuffle(squad_samples)\n",
    "    squad_samples = squad_samples[:N_PER_DATASET]\n",
    "\n",
    "    with open(SQUAD_CACHE, 'w') as f:\n",
    "        json.dump(squad_samples, f)\n",
    "    print(f\"Cached {len(squad_samples)} samples\")\n",
    "\n",
    "    del squad_ds\n",
    "    gc.collect()\n",
    "\n",
    "print(f\"\\nSQuAD samples: {len(squad_samples)}\")\n",
    "if squad_samples:\n",
    "    wcs = [s['word_count'] for s in squad_samples]\n",
    "    ans_lens = [len(s['answer'].split()) for s in squad_samples]\n",
    "    print(f\"  Word counts: mean={np.mean(wcs):.0f}, min={min(wcs)}, max={max(wcs)}\")\n",
    "    print(f\"  Answer word lengths: mean={np.mean(ans_lens):.1f}, max={max(ans_lens)}\")\n",
    "    for i in range(min(3, len(squad_samples))):\n",
    "        print(f\"  Example {i+1}:\")\n",
    "        print(f\"    Q: {squad_samples[i]['query']}\")\n",
    "        print(f\"    A: {squad_samples[i]['answer']}\")\n",
    "        print(f\"    Passage (first 120 chars): {squad_samples[i]['passage'][:120]}...\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 7: Unified sample pool + tokenization + pre-screening\n",
    "print(\"=\" * 70)\n",
    "print(\"UNIFIED SAMPLE POOL\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "all_samples = []\n",
    "for ds_name, ds_samples in [(\"subjqa\", subjqa_samples),\n",
    "                              (\"marco\", marco_samples),\n",
    "                              (\"squad\", squad_samples)]:\n",
    "    for sample in ds_samples:\n",
    "        sample['dataset'] = ds_name\n",
    "    all_samples.extend(ds_samples)\n",
    "\n",
    "print(f\"Total samples: {len(all_samples)}\")\n",
    "for ds_name in ['subjqa', 'marco', 'squad']:\n",
    "    ds_s = [s for s in all_samples if s['dataset'] == ds_name]\n",
    "    if not ds_s:\n",
    "        print(f\"  {ds_name}: n=0 (MISSING)\")\n",
    "        continue\n",
    "    wcs = [s['word_count'] for s in ds_s]\n",
    "    print(f\"  {ds_name}: n={len(ds_s)}, mean_words={np.mean(wcs):.0f}, \"\n",
    "          f\"range=[{min(wcs)}, {max(wcs)}]\")\n",
    "\n",
    "# Tokenize prefix\n",
    "sf_str = SURROGATE_PREFIX_TEMPLATE.format(surrogate=STATIC_FACT)\n",
    "sf_ids = tokenizer(sf_str, return_tensors=\"pt\",\n",
    "                    add_special_tokens=False)['input_ids'].to(exp_config.device)\n",
    "PREFIX_TOKEN_LEN = sf_ids.shape[1]\n",
    "\n",
    "print(f\"\\nPrefix: '{STATIC_FACT}'\")\n",
    "print(f\"  Token length (no BOS): {PREFIX_TOKEN_LEN}\")\n",
    "\n",
    "# Verify sliding window safety\n",
    "max_primed_seq = 1 + PREFIX_TOKEN_LEN + MAX_DOC_TOKENS\n",
    "print(f\"  Max primed sequence: 1 + {PREFIX_TOKEN_LEN} + {MAX_DOC_TOKENS} = {max_primed_seq}\")\n",
    "print(f\"  Sliding window: 1024\")\n",
    "assert max_primed_seq < 1024, f\"UNSAFE: {max_primed_seq} >= 1024\"\n",
    "print(f\"  SAFE: {max_primed_seq} < 1024\")\n",
    "\n",
    "# Tokenize doc lengths\n",
    "print(f\"\\nTokenizing documents to measure token lengths...\")\n",
    "n_truncated = 0\n",
    "for sample in tqdm(all_samples, desc=\"Tokenizing\"):\n",
    "    tok_len = len(tokenizer.encode(sample['passage'], add_special_tokens=False))\n",
    "    if tok_len > MAX_DOC_TOKENS:\n",
    "        n_truncated += 1\n",
    "    sample['doc_token_len'] = min(tok_len, MAX_DOC_TOKENS)\n",
    "    sample['answer_token_len'] = len(tokenizer.encode(sample['answer'], add_special_tokens=False))\n",
    "\n",
    "print(f\"  Documents truncated to {MAX_DOC_TOKENS}: {n_truncated}/{len(all_samples)} \"\n",
    "      f\"({100*n_truncated/len(all_samples):.0f}%)\")\n",
    "\n",
    "for ds_name in ['subjqa', 'marco', 'squad']:\n",
    "    ds_s = [s for s in all_samples if s['dataset'] == ds_name]\n",
    "    if not ds_s:\n",
    "        continue\n",
    "    tls = [s['doc_token_len'] for s in ds_s]\n",
    "    atls = [s['answer_token_len'] for s in ds_s]\n",
    "    n_trunc = sum(1 for s in ds_s if s['doc_token_len'] == MAX_DOC_TOKENS)\n",
    "    print(f\"  {ds_name}: mean_tok={np.mean(tls):.0f}, median={np.median(tls):.0f}, \"\n",
    "          f\"truncated={n_trunc}/{len(ds_s)} ({100*n_trunc/len(ds_s):.0f}%), \"\n",
    "          f\"mean_ans_tok={np.mean(atls):.1f}\")\n",
    "\n",
    "# === PRE-SCREENING: Bare NLL check ===\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"PRE-SCREENING: Bare NLL distribution check (20 samples/dataset)\")\n",
    "print(\"If median bare NLL < 0.05, ceiling effects may dominate.\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for ds_name in ['subjqa', 'marco', 'squad']:\n",
    "    ds_s = [s for s in all_samples if s['dataset'] == ds_name][:20]\n",
    "    if not ds_s:\n",
    "        continue\n",
    "    bare_nlls = []\n",
    "    for sample in ds_s:\n",
    "        passage = sample['passage']\n",
    "        question = sample['query']\n",
    "        answer = sample['answer']\n",
    "\n",
    "        document_text = DOCUMENT_TEMPLATE.format(document=passage)\n",
    "        query_prompt = QUERY_TEMPLATE.format(question=question)\n",
    "        answer_text = ANSWER_TEMPLATE.format(answer=answer)\n",
    "\n",
    "        doc_ids = tokenizer(document_text, return_tensors=\"pt\",\n",
    "                            add_special_tokens=False)['input_ids'].to(exp_config.device)\n",
    "        if doc_ids.shape[1] > MAX_DOC_TOKENS:\n",
    "            doc_ids = doc_ids[:, :MAX_DOC_TOKENS]\n",
    "\n",
    "        bos_id = tokenizer.bos_token_id\n",
    "        if bos_id is None:\n",
    "            bos_id = tokenizer.encode(\"\", add_special_tokens=True)[0]\n",
    "        bos_tensor = torch.tensor([[bos_id]], device=exp_config.device)\n",
    "        bare_input = torch.cat([bos_tensor, doc_ids], dim=1)\n",
    "        context_len = bare_input.shape[1]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            bare_out = model(input_ids=bare_input,\n",
    "                             attention_mask=torch.ones_like(bare_input),\n",
    "                             use_cache=True, return_dict=True)\n",
    "        bare_cache = _ensure_dynamic_cache(bare_out.past_key_values)\n",
    "        del bare_out\n",
    "\n",
    "        nll = score_answer_with_cache(\n",
    "            deepcopy_cache(bare_cache), context_len,\n",
    "            query_prompt, answer_text, model, tokenizer, exp_config)\n",
    "        bare_nlls.append(nll)\n",
    "        del bare_cache, bare_input, doc_ids\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    bare_arr = np.array(bare_nlls)\n",
    "    pct_floor = 100 * np.mean(bare_arr < 0.01)\n",
    "    median = np.median(bare_arr)\n",
    "    mean = np.mean(bare_arr)\n",
    "    status = \"WARNING: CEILING\" if pct_floor > 50 else \"OK\" if pct_floor < 30 else \"MARGINAL\"\n",
    "    print(f\"  {ds_name:15s}: median={median:.3f}, mean={mean:.3f}, \"\n",
    "          f\"pct_floor(<0.01)={pct_floor:.0f}% -> {status}\")\n",
    "\n",
    "print(\"\\nPre-screening complete. Proceeding with full experiment.\")\n",
    "\n",
    "# Condition explanation\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"EXPERIMENTAL CONDITIONS (Gemma 3 4B) -- 4 conditions\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\n### 1. bare ###\")\n",
    "print(\"  Forward: [BOS][doc]\")\n",
    "print(\"  Baseline. Standard causal attention.\")\n",
    "\n",
    "print(\"\\n### 2. sf_trunc (standard priming) ###\")\n",
    "print(f\"  Forward: [BOS][prefix_{PREFIX_TOKEN_LEN}][doc]\")\n",
    "print(\"  Standard causal, truncate + RoPE. Keys carry negative interference on Gemma.\")\n",
    "\n",
    "print(\"\\n### 3. values_early (layers 0-15 only) ###\")\n",
    "print(\"  Bare keys + primed values from layers 0-15 only.\")\n",
    "print(\"  Expected: d ~ +0.211 (Exp 19 on MARCO). Late layers carry interference.\")\n",
    "\n",
    "print(\"\\n### 4. values_hero (layers {10,12,14,15,20}) ###\")\n",
    "print(\"  Bare keys + primed values from 5 hero layers identified in Exp 24.\")\n",
    "print(\"  NQ: d=+0.213 (Exp 27b). DROP: d=-0.152 (Exp 29).\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 8: Helper function -- run_single_sample_4cond()\n",
    "\n",
    "def run_single_sample_4cond(sample, model, tokenizer, exp_config, sf_ids, sf_str,\n",
    "                             PREFIX_TOKEN_LEN, N_LAYERS, EARLY_LAYER_CUTOFF, HERO_LAYERS):\n",
    "    \"\"\"Run 4 conditions for a single sample. Returns dict of NLLs + metadata.\n",
    "\n",
    "    Conditions:\n",
    "      1. bare: [BOS][doc] standard causal\n",
    "      2. sf_trunc: [BOS][prefix][doc] truncate + RoPE correct\n",
    "      3. values_early: bare keys + primed values layers 0-15\n",
    "      4. values_hero: bare keys + primed values at hero layers\n",
    "    \"\"\"\n",
    "    passage = sample['passage']\n",
    "    query = sample['query']\n",
    "    answer = sample['answer']\n",
    "    ds_name = sample['dataset']\n",
    "\n",
    "    query_prompt = QUERY_TEMPLATE.format(question=query)\n",
    "    answer_text = ANSWER_TEMPLATE.format(answer=answer)\n",
    "    document_text = DOCUMENT_TEMPLATE.format(document=passage)\n",
    "\n",
    "    # === Matched tokenization ===\n",
    "    full_text = sf_str + document_text\n",
    "    full_enc = tokenizer(full_text, return_tensors=\"pt\",\n",
    "                          add_special_tokens=True, padding=False, truncation=False)\n",
    "    full_ids = full_enc['input_ids'].to(exp_config.device)\n",
    "\n",
    "    sf_prefix_enc = tokenizer(sf_str, return_tensors=\"pt\",\n",
    "                               add_special_tokens=True, padding=False, truncation=False)\n",
    "    sf_prefix_len_with_bos = sf_prefix_enc['input_ids'].shape[1]\n",
    "\n",
    "    bos_id = full_ids[:, :1]\n",
    "    doc_ids = full_ids[:, sf_prefix_len_with_bos:]\n",
    "\n",
    "    # Truncate long docs\n",
    "    if doc_ids.shape[1] > MAX_DOC_TOKENS:\n",
    "        doc_ids = doc_ids[:, :MAX_DOC_TOKENS]\n",
    "\n",
    "    doc_len = doc_ids.shape[1]\n",
    "    context_len = 1 + doc_len  # BOS + doc\n",
    "\n",
    "    del full_enc, full_ids, sf_prefix_enc\n",
    "\n",
    "    # === 1. BARE ===\n",
    "    bare_input = torch.cat([bos_id, doc_ids], dim=1)\n",
    "    with torch.no_grad():\n",
    "        bare_out = model(input_ids=bare_input,\n",
    "                         attention_mask=torch.ones_like(bare_input),\n",
    "                         use_cache=True, return_dict=True)\n",
    "    bare_cache = _ensure_dynamic_cache(bare_out.past_key_values)\n",
    "    del bare_out\n",
    "\n",
    "    bare_nll = score_answer_with_cache(\n",
    "        deepcopy_cache(bare_cache), context_len,\n",
    "        query_prompt, answer_text, model, tokenizer, exp_config)\n",
    "\n",
    "    # === 2. sf_trunc (standard priming) ===\n",
    "    primed_input = torch.cat([bos_id, sf_ids, doc_ids], dim=1)\n",
    "    prefix_offset = sf_ids.shape[1]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        primed_out = model(input_ids=primed_input,\n",
    "                           attention_mask=torch.ones_like(primed_input),\n",
    "                           use_cache=True, return_dict=True)\n",
    "    primed_full_std = _ensure_dynamic_cache(primed_out.past_key_values)\n",
    "    del primed_out\n",
    "\n",
    "    trunc_raw = extract_and_truncate_cache_with_bos(primed_full_std, doc_len)\n",
    "    del primed_full_std\n",
    "\n",
    "    sf_trunc_cache = deepcopy_cache(trunc_raw)\n",
    "    correct_rope_positions_with_bos(sf_trunc_cache, prefix_offset, model)\n",
    "    del trunc_raw\n",
    "\n",
    "    sf_trunc_nll = score_answer_with_cache(\n",
    "        deepcopy_cache(sf_trunc_cache), context_len,\n",
    "        query_prompt, answer_text, model, tokenizer, exp_config)\n",
    "\n",
    "    # === 3. values_early (layers 0 to EARLY_LAYER_CUTOFF-1) ===\n",
    "    early_layers = list(range(EARLY_LAYER_CUTOFF))\n",
    "    values_early_cache = replace_values_at_layers(bare_cache, sf_trunc_cache, early_layers)\n",
    "\n",
    "    values_early_nll = score_answer_with_cache(\n",
    "        deepcopy_cache(values_early_cache), context_len,\n",
    "        query_prompt, answer_text, model, tokenizer, exp_config)\n",
    "    del values_early_cache\n",
    "\n",
    "    # === 4. values_hero (hero layers only) ===\n",
    "    values_hero_cache = replace_values_at_layers(bare_cache, sf_trunc_cache, HERO_LAYERS)\n",
    "\n",
    "    values_hero_nll = score_answer_with_cache(\n",
    "        deepcopy_cache(values_hero_cache), context_len,\n",
    "        query_prompt, answer_text, model, tokenizer, exp_config)\n",
    "    del values_hero_cache\n",
    "\n",
    "    del bare_cache, sf_trunc_cache, bare_input, primed_input\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    result = {\n",
    "        'dataset': ds_name,\n",
    "        'query': query,\n",
    "        'answer': answer,\n",
    "        'word_count': sample['word_count'],\n",
    "        'doc_token_len': doc_len,\n",
    "        'answer_token_len': sample.get('answer_token_len', 0),\n",
    "        'bare': bare_nll,\n",
    "        'sf_trunc': sf_trunc_nll,\n",
    "        'values_early': values_early_nll,\n",
    "        'values_hero': values_hero_nll,\n",
    "    }\n",
    "    if 'domain' in sample:\n",
    "        result['domain'] = sample['domain']\n",
    "    return result\n",
    "\n",
    "\n",
    "print(\"Helper function defined: run_single_sample_4cond()\")\n",
    "print(\"  Conditions: bare, sf_trunc, values_early, values_hero\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 9: Main NLL experiment loop (Phase 1)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"PHASE 1: NLL EVALUATION -- {len(all_samples)} samples, {len(CONDITION_NAMES)} conditions\")\n",
    "print(f\"Model: Gemma 3 4B, MAX_DOC_TOKENS: {MAX_DOC_TOKENS}\")\n",
    "print(f\"Datasets: SubjQA, MS MARCO, SQuAD\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Checkpoint resume\n",
    "all_results = []\n",
    "start_idx = 0\n",
    "\n",
    "if CHECKPOINT_PATH.exists():\n",
    "    with open(CHECKPOINT_PATH, 'r') as f:\n",
    "        ckpt = json.load(f)\n",
    "    ckpt_queries = ckpt.get('sample_queries', [])\n",
    "    current_queries = [s['query'] for s in all_samples]\n",
    "    if ckpt_queries == current_queries:\n",
    "        all_results = ckpt['results']\n",
    "        start_idx = len(all_results)\n",
    "        print(f\"Resuming from checkpoint: {start_idx}/{len(all_samples)}\")\n",
    "    else:\n",
    "        print(\"Checkpoint query mismatch. Starting fresh.\")\n",
    "else:\n",
    "    print(\"No checkpoint found. Starting fresh.\")\n",
    "\n",
    "t_start = time.time()\n",
    "N_TOTAL = len(all_samples)\n",
    "\n",
    "for qidx in tqdm(range(start_idx, N_TOTAL), initial=start_idx, total=N_TOTAL,\n",
    "                  desc=\"Exp 31 Phase 1\"):\n",
    "    sample = all_samples[qidx]\n",
    "\n",
    "    result = run_single_sample_4cond(\n",
    "        sample, model, tokenizer, exp_config,\n",
    "        sf_ids, sf_str, PREFIX_TOKEN_LEN, N_LAYERS,\n",
    "        EARLY_LAYER_CUTOFF, HERO_LAYERS)\n",
    "    result['query_idx'] = qidx\n",
    "    all_results.append(result)\n",
    "\n",
    "    # Checkpoint\n",
    "    if (qidx + 1) % CHECKPOINT_EVERY == 0 or qidx == N_TOTAL - 1:\n",
    "        ckpt_data = {\n",
    "            'results': all_results,\n",
    "            'sample_queries': [s['query'] for s in all_samples],\n",
    "            'completed': len(all_results),\n",
    "            'total': N_TOTAL,\n",
    "            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        }\n",
    "        with open(CHECKPOINT_PATH, 'w') as f:\n",
    "            json.dump(ckpt_data, f)\n",
    "        elapsed = time.time() - t_start\n",
    "        n_done = qidx - start_idx + 1\n",
    "        rate = n_done / elapsed if elapsed > 0 else 0\n",
    "        remaining = (N_TOTAL - qidx - 1) / rate if rate > 0 else 0\n",
    "        tqdm.write(f\"  Checkpoint {qidx+1}/{N_TOTAL} | {n_done} done in {elapsed/60:.1f}m | \"\n",
    "                   f\"ETA: {remaining/60:.1f} min\")\n",
    "\n",
    "elapsed_total = time.time() - t_start\n",
    "print(f\"\\nPhase 1 complete: {len(all_results)} samples in {elapsed_total/60:.1f} min\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 10: Per-dataset NLL results table (Phase 1 analysis)\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PHASE 1 ANALYSIS: PER-DATASET NLL RESULTS (Gemma 3 4B)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "dataset_names = ['subjqa', 'marco', 'squad']\n",
    "analysis = {}\n",
    "\n",
    "for ds_name in dataset_names:\n",
    "    ds_results = [r for r in all_results if r['dataset'] == ds_name]\n",
    "    n_ds = len(ds_results)\n",
    "    if n_ds == 0:\n",
    "        print(f\"\\n  {ds_name}: NO RESULTS\")\n",
    "        continue\n",
    "\n",
    "    bare_arr = np.array([r['bare'] for r in ds_results])\n",
    "\n",
    "    # Filter invalid\n",
    "    valid = np.isfinite(bare_arr)\n",
    "    for cname in CONDITION_NAMES:\n",
    "        if cname == 'bare':\n",
    "            continue\n",
    "        c_arr = np.array([r[cname] for r in ds_results])\n",
    "        valid &= np.isfinite(c_arr)\n",
    "\n",
    "    n_valid = int(np.sum(valid))\n",
    "\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    pct_floor = 100 * np.mean(bare_arr < 0.01)\n",
    "    print(f\"DATASET: {ds_name.upper()} (n={n_valid}/{n_ds}, \"\n",
    "          f\"median bare NLL={np.median(bare_arr):.3f}, \"\n",
    "          f\"pct_floor={pct_floor:.0f}%)\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "    print(f\"\\n{'Condition':<20} {'Mean Bare':>10} {'Mean Cond':>10} \"\n",
    "          f\"{'Mean D':>10} {'d':>8} {'Win%':>7} {'p':>12} {'sig':>5}\")\n",
    "    print(\"-\" * 90)\n",
    "\n",
    "    ds_analysis = {}\n",
    "    for cname in CONDITION_NAMES:\n",
    "        if cname == 'bare':\n",
    "            continue\n",
    "        c_arr = np.array([r[cname] for r in ds_results])\n",
    "        delta = bare_arr[valid] - c_arr[valid]\n",
    "        d = cohens_d(delta)\n",
    "        win = np.mean(delta > 0) * 100\n",
    "        t_stat, p_val = stats.ttest_1samp(delta, 0)\n",
    "        sig = '***' if p_val < 0.001 else '**' if p_val < 0.01 else '*' if p_val < 0.05 else 'ns'\n",
    "        print(f\"{cname:<20} {np.mean(bare_arr[valid]):>10.4f} {np.mean(c_arr[valid]):>10.4f} \"\n",
    "              f\"{np.mean(delta):>+10.4f} {d:>+8.3f} {win:>6.1f}% {p_val:>12.2e} {sig:>5}\")\n",
    "        ds_analysis[cname] = {\n",
    "            'n_valid': n_valid,\n",
    "            'mean_bare': float(np.mean(bare_arr[valid])),\n",
    "            'mean_cond': float(np.mean(c_arr[valid])),\n",
    "            'mean_delta': float(np.mean(delta)),\n",
    "            'cohens_d': float(d),\n",
    "            'win_pct': float(win),\n",
    "            't_stat': float(t_stat),\n",
    "            'p_value': float(p_val),\n",
    "        }\n",
    "\n",
    "    analysis[ds_name] = ds_analysis\n",
    "\n",
    "# Cross-dataset summary table\n",
    "print(f\"\\n\\n{'='*90}\")\n",
    "print(\"CROSS-DATASET SUMMARY: Cohen's d vs bare (Gemma 3 4B)\")\n",
    "print(f\"{'='*90}\")\n",
    "print(f\"\\n{'Condition':<20}\", end='')\n",
    "for ds in dataset_names:\n",
    "    print(f\"{'  ' + ds:>16}\", end='')\n",
    "print()\n",
    "print(\"-\" * 68)\n",
    "for cname in CONDITION_NAMES:\n",
    "    if cname == 'bare':\n",
    "        continue\n",
    "    print(f\"{cname:<20}\", end='')\n",
    "    for ds in dataset_names:\n",
    "        if ds in analysis and cname in analysis[ds]:\n",
    "            d = analysis[ds][cname]['cohens_d']\n",
    "            p = analysis[ds][cname]['p_value']\n",
    "            sig = '***' if p < 0.001 else '**' if p < 0.01 else '*' if p < 0.05 else ''\n",
    "            print(f\"{d:>+12.3f}{sig:>4}\", end='')\n",
    "        else:\n",
    "            print(f\"{'n/a':>16}\", end='')\n",
    "    print()\n",
    "\n",
    "# Bare NLL distributions\n",
    "print(f\"\\n\\nBARE NLL DISTRIBUTIONS (ceiling effect check):\")\n",
    "for ds in dataset_names:\n",
    "    ds_r = [r for r in all_results if r['dataset'] == ds]\n",
    "    if not ds_r:\n",
    "        continue\n",
    "    bare = [r['bare'] for r in ds_r]\n",
    "    pct_zero = 100 * np.mean(np.array(bare) < 0.01)\n",
    "    iqr = np.percentile(bare, 75) - np.percentile(bare, 25)\n",
    "    print(f\"  {ds:15s}: mean={np.mean(bare):.3f}, median={np.median(bare):.3f}, \"\n",
    "          f\"IQR={iqr:.3f}, pct_floor={pct_zero:.0f}%\")\n",
    "\n",
    "# Comparison with prior experiments\n",
    "print(f\"\\n\\n{'='*90}\")\n",
    "print(\"COMPARISON WITH PRIOR EXPERIMENTS\")\n",
    "print(f\"{'='*90}\")\n",
    "print(\"\\nExp 07 (Mistral, MS MARCO): static_fact_trunc d=+0.472***\")\n",
    "print(\"Exp 27b (Gemma, NQ): values_hero d=+0.213***\")\n",
    "print(\"Exp 30 (Gemma, NQ): values_hero d=+0.213***\")\n",
    "print(\"Exp 29 (Gemma, DROP): values_hero d=-0.152**\")\n",
    "print(\"Exp 24 (Gemma, MARCO): values_cutoff_16 d=+0.211***\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 11: Generation helper functions + evaluation metrics\n",
    "\n",
    "def normalize_answer(s):\n",
    "    \"\"\"Lower text, remove punctuation, articles, and extra whitespace.\"\"\"\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "    def remove_punc(text):\n",
    "        return ''.join(ch for ch in text if ch not in string.punctuation)\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "\n",
    "def compute_exact_match(prediction, gold):\n",
    "    \"\"\"Check if normalized gold appears in normalized prediction (substring match).\"\"\"\n",
    "    return normalize_answer(gold) in normalize_answer(prediction)\n",
    "\n",
    "\n",
    "def compute_token_f1(prediction, gold):\n",
    "    \"\"\"Token-level F1 between prediction and gold (SQuAD-style).\"\"\"\n",
    "    pred_tokens = normalize_answer(prediction).split()\n",
    "    gold_tokens = normalize_answer(gold).split()\n",
    "    if not pred_tokens or not gold_tokens:\n",
    "        return 0.0\n",
    "    common = Counter(pred_tokens) & Counter(gold_tokens)\n",
    "    num_common = sum(common.values())\n",
    "    if num_common == 0:\n",
    "        return 0.0\n",
    "    precision = num_common / len(pred_tokens)\n",
    "    recall = num_common / len(gold_tokens)\n",
    "    return 2 * precision * recall / (precision + recall)\n",
    "\n",
    "\n",
    "def compute_rouge_l(prediction, gold):\n",
    "    \"\"\"ROUGE-L F-score based on longest common subsequence.\"\"\"\n",
    "    pred_tokens = normalize_answer(prediction).split()\n",
    "    gold_tokens = normalize_answer(gold).split()\n",
    "    if not pred_tokens or not gold_tokens:\n",
    "        return 0.0\n",
    "    m, n = len(pred_tokens), len(gold_tokens)\n",
    "    dp = [[0] * (n + 1) for _ in range(m + 1)]\n",
    "    for i in range(1, m + 1):\n",
    "        for j in range(1, n + 1):\n",
    "            if pred_tokens[i-1] == gold_tokens[j-1]:\n",
    "                dp[i][j] = dp[i-1][j-1] + 1\n",
    "            else:\n",
    "                dp[i][j] = max(dp[i-1][j], dp[i][j-1])\n",
    "    lcs_len = dp[m][n]\n",
    "    if lcs_len == 0:\n",
    "        return 0.0\n",
    "    precision = lcs_len / m\n",
    "    recall = lcs_len / n\n",
    "    return 2 * precision * recall / (precision + recall)\n",
    "\n",
    "\n",
    "def generate_with_cache(cache, context_len, query_prompt, model, tokenizer, config,\n",
    "                         max_new_tokens=50):\n",
    "    \"\"\"Generate text autoregressively using a pre-built KV cache.\n",
    "\n",
    "    WARNING: Mutates the cache. Always deepcopy before calling.\n",
    "\n",
    "    Args:\n",
    "        cache: Pre-built KV cache (DynamicCache)\n",
    "        context_len: Number of tokens in the cache\n",
    "        query_prompt: Query string to feed before generating\n",
    "        model: The language model\n",
    "        tokenizer: The tokenizer\n",
    "        config: ExperimentConfig\n",
    "        max_new_tokens: Max tokens to generate\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (generated_text, mean_log_prob)\n",
    "    \"\"\"\n",
    "    query_ids = tokenizer(query_prompt, return_tensors=\"pt\",\n",
    "                          add_special_tokens=False)['input_ids'].to(config.device)\n",
    "    query_len = query_ids.shape[1]\n",
    "\n",
    "    # Feed query through model to extend cache\n",
    "    combined_len = context_len + query_len\n",
    "    attention_mask = torch.ones((1, combined_len), device=config.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        query_out = model(input_ids=query_ids,\n",
    "                          attention_mask=attention_mask,\n",
    "                          past_key_values=cache,\n",
    "                          use_cache=True, return_dict=True)\n",
    "    gen_cache = query_out.past_key_values\n",
    "    next_logits = query_out.logits[:, -1:, :]  # (1, 1, vocab)\n",
    "    cur_len = combined_len\n",
    "\n",
    "    generated_ids = []\n",
    "    log_probs = []\n",
    "\n",
    "    for step in range(max_new_tokens):\n",
    "        log_p = torch.log_softmax(next_logits[:, 0, :], dim=-1)\n",
    "        next_id = torch.argmax(log_p, dim=-1)  # (1,)\n",
    "        token_log_prob = log_p[0, next_id[0]].item()\n",
    "\n",
    "        generated_ids.append(next_id[0].item())\n",
    "        log_probs.append(token_log_prob)\n",
    "\n",
    "        # Stop at EOS\n",
    "        if next_id[0].item() == tokenizer.eos_token_id:\n",
    "            break\n",
    "\n",
    "        cur_len += 1\n",
    "        attention_mask = torch.ones((1, cur_len), device=config.device)\n",
    "        next_token = next_id.unsqueeze(0)  # (1, 1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            out = model(input_ids=next_token,\n",
    "                        attention_mask=attention_mask,\n",
    "                        past_key_values=gen_cache,\n",
    "                        use_cache=True, return_dict=True)\n",
    "        gen_cache = out.past_key_values\n",
    "        next_logits = out.logits\n",
    "\n",
    "    del gen_cache\n",
    "    generated_text = tokenizer.decode(generated_ids, skip_special_tokens=True).strip()\n",
    "    mean_log_prob = float(np.mean(log_probs)) if log_probs else float('-inf')\n",
    "\n",
    "    return generated_text, mean_log_prob\n",
    "\n",
    "\n",
    "# Quick test\n",
    "print(\"Generation helpers defined:\")\n",
    "print(\"  normalize_answer(), compute_exact_match(), compute_token_f1(), compute_rouge_l()\")\n",
    "print(\"  generate_with_cache(cache, context_len, query, model, tokenizer, config)\")\n",
    "print()\n",
    "print(\"Test metrics:\")\n",
    "print(f\"  EM('Paris is the capital', 'Paris'): {compute_exact_match('Paris is the capital', 'Paris')}\")\n",
    "print(f\"  F1('paris is capital city', 'paris capital'): {compute_token_f1('paris is capital city', 'paris capital'):.3f}\")\n",
    "print(f\"  ROUGE-L('the cat sat', 'the cat'): {compute_rouge_l('the cat sat', 'the cat'):.3f}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 12: Generation experiment on hard subset (Phase 2)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PHASE 2: GENERATION QUALITY EVALUATION\")\n",
    "print(\"=\" * 70)\n",
    "print(\"Generate with bare and hero caches on hard samples (top 50% by bare NLL).\")\n",
    "print(f\"Max generation tokens: {MAX_GEN_TOKENS}\")\n",
    "\n",
    "# Select hard samples: top 50% by bare NLL per dataset\n",
    "gen_sample_indices = []\n",
    "for ds_name in dataset_names:\n",
    "    ds_results = [r for r in all_results if r['dataset'] == ds_name]\n",
    "    if not ds_results:\n",
    "        continue\n",
    "    bare_nlls = np.array([r['bare'] for r in ds_results])\n",
    "    threshold = np.median(bare_nlls)\n",
    "    hard_indices = [r['query_idx'] for r in ds_results if r['bare'] >= threshold]\n",
    "    gen_sample_indices.extend(hard_indices)\n",
    "    print(f\"  {ds_name}: {len(hard_indices)} hard samples (threshold={threshold:.3f})\")\n",
    "\n",
    "# Map query_idx -> Phase 1 result for later analysis\n",
    "results_by_idx = {r['query_idx']: r for r in all_results}\n",
    "\n",
    "print(f\"\\nTotal generation samples: {len(gen_sample_indices)}\")\n",
    "\n",
    "# Checkpoint resume\n",
    "gen_results = []\n",
    "gen_start_idx = 0\n",
    "\n",
    "if GEN_CHECKPOINT_PATH.exists():\n",
    "    with open(GEN_CHECKPOINT_PATH, 'r') as f:\n",
    "        gen_ckpt = json.load(f)\n",
    "    if gen_ckpt.get('gen_indices') == gen_sample_indices:\n",
    "        gen_results = gen_ckpt['gen_results']\n",
    "        gen_start_idx = len(gen_results)\n",
    "        print(f\"Resuming generation from checkpoint: {gen_start_idx}/{len(gen_sample_indices)}\")\n",
    "    else:\n",
    "        print(\"Generation checkpoint mismatch. Starting fresh.\")\n",
    "\n",
    "t_gen_start = time.time()\n",
    "\n",
    "for gidx in tqdm(range(gen_start_idx, len(gen_sample_indices)),\n",
    "                  initial=gen_start_idx, total=len(gen_sample_indices),\n",
    "                  desc=\"Exp 31 Phase 2\"):\n",
    "    qidx = gen_sample_indices[gidx]\n",
    "    sample = all_samples[qidx]\n",
    "    phase1 = results_by_idx[qidx]\n",
    "\n",
    "    passage = sample['passage']\n",
    "    query = sample['query']\n",
    "    answer = sample['answer']\n",
    "    query_prompt = QUERY_TEMPLATE.format(question=query)\n",
    "    document_text = DOCUMENT_TEMPLATE.format(document=passage)\n",
    "\n",
    "    # Matched tokenization (same as Phase 1)\n",
    "    full_text = sf_str + document_text\n",
    "    full_enc = tokenizer(full_text, return_tensors=\"pt\",\n",
    "                          add_special_tokens=True, padding=False, truncation=False)\n",
    "    full_ids = full_enc['input_ids'].to(exp_config.device)\n",
    "\n",
    "    sf_prefix_enc = tokenizer(sf_str, return_tensors=\"pt\",\n",
    "                               add_special_tokens=True, padding=False, truncation=False)\n",
    "    sf_prefix_len_with_bos = sf_prefix_enc['input_ids'].shape[1]\n",
    "\n",
    "    bos_id = full_ids[:, :1]\n",
    "    doc_ids = full_ids[:, sf_prefix_len_with_bos:]\n",
    "    if doc_ids.shape[1] > MAX_DOC_TOKENS:\n",
    "        doc_ids = doc_ids[:, :MAX_DOC_TOKENS]\n",
    "    doc_len = doc_ids.shape[1]\n",
    "    context_len = 1 + doc_len\n",
    "\n",
    "    del full_enc, full_ids, sf_prefix_enc\n",
    "\n",
    "    # Build bare cache\n",
    "    bare_input = torch.cat([bos_id, doc_ids], dim=1)\n",
    "    with torch.no_grad():\n",
    "        bare_out = model(input_ids=bare_input,\n",
    "                         attention_mask=torch.ones_like(bare_input),\n",
    "                         use_cache=True, return_dict=True)\n",
    "    bare_cache = _ensure_dynamic_cache(bare_out.past_key_values)\n",
    "    del bare_out\n",
    "\n",
    "    # Build primed cache -> truncate -> RoPE -> hero\n",
    "    primed_input = torch.cat([bos_id, sf_ids, doc_ids], dim=1)\n",
    "    prefix_offset = sf_ids.shape[1]\n",
    "    with torch.no_grad():\n",
    "        primed_out = model(input_ids=primed_input,\n",
    "                           attention_mask=torch.ones_like(primed_input),\n",
    "                           use_cache=True, return_dict=True)\n",
    "    primed_cache = _ensure_dynamic_cache(primed_out.past_key_values)\n",
    "    del primed_out\n",
    "\n",
    "    trunc_raw = extract_and_truncate_cache_with_bos(primed_cache, doc_len)\n",
    "    del primed_cache\n",
    "    sf_trunc_cache = deepcopy_cache(trunc_raw)\n",
    "    correct_rope_positions_with_bos(sf_trunc_cache, prefix_offset, model)\n",
    "    del trunc_raw\n",
    "\n",
    "    hero_cache = replace_values_at_layers(bare_cache, sf_trunc_cache, HERO_LAYERS)\n",
    "    del sf_trunc_cache\n",
    "\n",
    "    # Generate with bare\n",
    "    bare_text, bare_lp = generate_with_cache(\n",
    "        deepcopy_cache(bare_cache), context_len, query_prompt,\n",
    "        model, tokenizer, exp_config, MAX_GEN_TOKENS)\n",
    "\n",
    "    # Generate with hero\n",
    "    hero_text, hero_lp = generate_with_cache(\n",
    "        deepcopy_cache(hero_cache), context_len, query_prompt,\n",
    "        model, tokenizer, exp_config, MAX_GEN_TOKENS)\n",
    "\n",
    "    del bare_cache, hero_cache, bare_input, primed_input\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # Evaluate\n",
    "    gen_result = {\n",
    "        'query_idx': qidx,\n",
    "        'dataset': sample['dataset'],\n",
    "        'query': query,\n",
    "        'answer': answer,\n",
    "        'word_count': sample['word_count'],\n",
    "        'bare_nll': phase1['bare'],\n",
    "        'hero_nll': phase1['values_hero'],\n",
    "        'bare_text': bare_text,\n",
    "        'hero_text': hero_text,\n",
    "        'bare_log_prob': bare_lp,\n",
    "        'hero_log_prob': hero_lp,\n",
    "        'bare_em': compute_exact_match(bare_text, answer),\n",
    "        'hero_em': compute_exact_match(hero_text, answer),\n",
    "        'bare_f1': compute_token_f1(bare_text, answer),\n",
    "        'hero_f1': compute_token_f1(hero_text, answer),\n",
    "        'bare_rouge_l': compute_rouge_l(bare_text, answer),\n",
    "        'hero_rouge_l': compute_rouge_l(hero_text, answer),\n",
    "        'texts_differ': bare_text != hero_text,\n",
    "    }\n",
    "    gen_results.append(gen_result)\n",
    "\n",
    "    # Checkpoint every 25\n",
    "    if (gidx + 1) % CHECKPOINT_EVERY == 0 or gidx == len(gen_sample_indices) - 1:\n",
    "        gen_ckpt_data = {\n",
    "            'gen_results': gen_results,\n",
    "            'gen_indices': gen_sample_indices,\n",
    "            'completed': len(gen_results),\n",
    "            'total': len(gen_sample_indices),\n",
    "            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        }\n",
    "        with open(GEN_CHECKPOINT_PATH, 'w') as f:\n",
    "            json.dump(gen_ckpt_data, f)\n",
    "        elapsed = time.time() - t_gen_start\n",
    "        n_done = gidx - gen_start_idx + 1\n",
    "        rate = n_done / elapsed if elapsed > 0 else 0\n",
    "        remaining = (len(gen_sample_indices) - gidx - 1) / rate if rate > 0 else 0\n",
    "        tqdm.write(f\"  Gen checkpoint {gidx+1}/{len(gen_sample_indices)} | \"\n",
    "                   f\"{n_done} done in {elapsed/60:.1f}m | ETA: {remaining/60:.1f} min\")\n",
    "\n",
    "elapsed_gen = time.time() - t_gen_start\n",
    "print(f\"\\nPhase 2 complete: {len(gen_results)} generations in {elapsed_gen/60:.1f} min\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 13: Generation quality analysis\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PHASE 2 ANALYSIS: GENERATION QUALITY (bare vs hero)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Overall metrics\n",
    "n_gen = len(gen_results)\n",
    "n_differ = sum(1 for r in gen_results if r['texts_differ'])\n",
    "print(f\"\\nGeneration samples: {n_gen}\")\n",
    "print(f\"Texts differ (bare vs hero): {n_differ}/{n_gen} ({100*n_differ/n_gen:.0f}%)\")\n",
    "\n",
    "# Overall comparison\n",
    "print(f\"\\n{'Metric':<20} {'Bare':>10} {'Hero':>10} {'Delta':>10} {'p':>12} {'sig':>5}\")\n",
    "print(\"-\" * 72)\n",
    "\n",
    "gen_analysis = {}\n",
    "\n",
    "for metric_name, bare_key, hero_key in [\n",
    "    ('Exact Match', 'bare_em', 'hero_em'),\n",
    "    ('Token F1', 'bare_f1', 'hero_f1'),\n",
    "    ('ROUGE-L', 'bare_rouge_l', 'hero_rouge_l'),\n",
    "    ('Log Prob', 'bare_log_prob', 'hero_log_prob'),\n",
    "]:\n",
    "    bare_vals = np.array([float(r[bare_key]) for r in gen_results])\n",
    "    hero_vals = np.array([float(r[hero_key]) for r in gen_results])\n",
    "    delta = hero_vals - bare_vals  # positive = hero better\n",
    "\n",
    "    # Filter inf for log_prob\n",
    "    valid = np.isfinite(bare_vals) & np.isfinite(hero_vals)\n",
    "    if np.sum(valid) < 10:\n",
    "        print(f\"{metric_name:<20} {'n/a (too few valid)':>40}\")\n",
    "        continue\n",
    "\n",
    "    bare_mean = np.mean(bare_vals[valid])\n",
    "    hero_mean = np.mean(hero_vals[valid])\n",
    "    delta_mean = np.mean(delta[valid])\n",
    "\n",
    "    if np.std(delta[valid]) > 0:\n",
    "        t_stat, p_val = stats.ttest_1samp(delta[valid], 0)\n",
    "    else:\n",
    "        t_stat, p_val = 0.0, 1.0\n",
    "\n",
    "    sig = '***' if p_val < 0.001 else '**' if p_val < 0.01 else '*' if p_val < 0.05 else 'ns'\n",
    "    print(f\"{metric_name:<20} {bare_mean:>10.3f} {hero_mean:>10.3f} \"\n",
    "          f\"{delta_mean:>+10.3f} {p_val:>12.2e} {sig:>5}\")\n",
    "\n",
    "    gen_analysis[metric_name] = {\n",
    "        'bare_mean': float(bare_mean),\n",
    "        'hero_mean': float(hero_mean),\n",
    "        'delta_mean': float(delta_mean),\n",
    "        'p_value': float(p_val),\n",
    "        'n_valid': int(np.sum(valid)),\n",
    "    }\n",
    "\n",
    "# Per-dataset generation metrics\n",
    "print(f\"\\n\\n{'='*90}\")\n",
    "print(\"PER-DATASET GENERATION QUALITY\")\n",
    "print(f\"{'='*90}\")\n",
    "\n",
    "gen_per_dataset = {}\n",
    "for ds_name in dataset_names:\n",
    "    ds_gen = [r for r in gen_results if r['dataset'] == ds_name]\n",
    "    if not ds_gen:\n",
    "        continue\n",
    "    n = len(ds_gen)\n",
    "    n_diff = sum(1 for r in ds_gen if r['texts_differ'])\n",
    "\n",
    "    print(f\"\\n--- {ds_name.upper()} (n={n}, {n_diff} differ) ---\")\n",
    "    print(f\"{'Metric':<20} {'Bare':>10} {'Hero':>10} {'Delta':>10} {'p':>12}\")\n",
    "    print(\"-\" * 65)\n",
    "\n",
    "    ds_gen_analysis = {}\n",
    "    for metric_name, bare_key, hero_key in [\n",
    "        ('Exact Match', 'bare_em', 'hero_em'),\n",
    "        ('Token F1', 'bare_f1', 'hero_f1'),\n",
    "        ('ROUGE-L', 'bare_rouge_l', 'hero_rouge_l'),\n",
    "    ]:\n",
    "        bare_vals = np.array([float(r[bare_key]) for r in ds_gen])\n",
    "        hero_vals = np.array([float(r[hero_key]) for r in ds_gen])\n",
    "        delta = hero_vals - bare_vals\n",
    "        valid = np.isfinite(delta)\n",
    "\n",
    "        if np.sum(valid) < 5:\n",
    "            continue\n",
    "\n",
    "        bare_m = np.mean(bare_vals[valid])\n",
    "        hero_m = np.mean(hero_vals[valid])\n",
    "        delta_m = np.mean(delta[valid])\n",
    "        if np.std(delta[valid]) > 0:\n",
    "            _, p = stats.ttest_1samp(delta[valid], 0)\n",
    "        else:\n",
    "            p = 1.0\n",
    "        sig = '***' if p < 0.001 else '**' if p < 0.01 else '*' if p < 0.05 else 'ns'\n",
    "        print(f\"{metric_name:<20} {bare_m:>10.3f} {hero_m:>10.3f} \"\n",
    "              f\"{delta_m:>+10.3f} {p:>12.2e} {sig}\")\n",
    "\n",
    "        ds_gen_analysis[metric_name] = {\n",
    "            'bare_mean': float(bare_m),\n",
    "            'hero_mean': float(hero_m),\n",
    "            'delta_mean': float(delta_m),\n",
    "            'p_value': float(p),\n",
    "        }\n",
    "    gen_per_dataset[ds_name] = ds_gen_analysis\n",
    "\n",
    "# NLL vs Generation quality correlation\n",
    "print(f\"\\n\\n{'='*90}\")\n",
    "print(\"NLL IMPROVEMENT vs GENERATION QUALITY IMPROVEMENT\")\n",
    "print(f\"{'='*90}\")\n",
    "print(\"Does lower hero NLL predict better hero generation?\")\n",
    "\n",
    "nll_deltas = []\n",
    "f1_deltas = []\n",
    "for r in gen_results:\n",
    "    nll_delta = r['bare_nll'] - r['hero_nll']  # positive = hero lower NLL\n",
    "    f1_delta = r['hero_f1'] - r['bare_f1']  # positive = hero better F1\n",
    "    if np.isfinite(nll_delta) and np.isfinite(f1_delta):\n",
    "        nll_deltas.append(nll_delta)\n",
    "        f1_deltas.append(f1_delta)\n",
    "\n",
    "if len(nll_deltas) >= 10:\n",
    "    rho, p_rho = stats.spearmanr(nll_deltas, f1_deltas)\n",
    "    print(f\"  Spearman rho (NLL delta vs F1 delta): {rho:+.3f}, p={p_rho:.3f}\")\n",
    "    if p_rho < 0.05:\n",
    "        print(\"  -> SIGNIFICANT: NLL improvement predicts generation quality improvement\")\n",
    "    else:\n",
    "        print(\"  -> Not significant: NLL improvement does NOT predict generation quality\")\n",
    "else:\n",
    "    rho = float('nan')\n",
    "    print(\"  Too few valid samples for correlation\")\n",
    "\n",
    "# Example outputs: 10 most improved samples\n",
    "print(f\"\\n\\n{'='*90}\")\n",
    "print(\"EXAMPLE GENERATIONS: Top 10 Most Improved (hero F1 > bare F1)\")\n",
    "print(f\"{'='*90}\")\n",
    "improved = sorted(gen_results,\n",
    "                  key=lambda r: r['hero_f1'] - r['bare_f1'],\n",
    "                  reverse=True)[:10]\n",
    "\n",
    "for i, r in enumerate(improved):\n",
    "    f1_gain = r['hero_f1'] - r['bare_f1']\n",
    "    print(f\"\\n--- Example {i+1} ({r['dataset']}, F1 gain: {f1_gain:+.3f}) ---\")\n",
    "    print(f\"  Q: {r['query']}\")\n",
    "    print(f\"  Gold: {r['answer']}\")\n",
    "    print(f\"  Bare: {r['bare_text'][:100]}{'...' if len(r['bare_text']) > 100 else ''}\")\n",
    "    print(f\"  Hero: {r['hero_text'][:100]}{'...' if len(r['hero_text']) > 100 else ''}\")\n",
    "    print(f\"  Bare EM={r['bare_em']}, F1={r['bare_f1']:.3f} | \"\n",
    "          f\"Hero EM={r['hero_em']}, F1={r['hero_f1']:.3f}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 14: Multi-panel figure (2x3)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "\n",
    "colors_ds = {'subjqa': '#ff7f0e', 'marco': '#2ca02c', 'squad': '#1f77b4'}\n",
    "\n",
    "# ---- Panel (a): Hero d by dataset (NLL) ----\n",
    "ax = axes[0, 0]\n",
    "ds_labels = []\n",
    "ds_hero_ds = []\n",
    "ds_colors = []\n",
    "for ds in dataset_names:\n",
    "    if ds not in analysis or 'values_hero' not in analysis[ds]:\n",
    "        continue\n",
    "    d_val = analysis[ds]['values_hero']['cohens_d']\n",
    "    p_val = analysis[ds]['values_hero']['p_value']\n",
    "    sig = '***' if p_val < 0.001 else '**' if p_val < 0.01 else '*' if p_val < 0.05 else ''\n",
    "    ds_labels.append(f\"{ds.upper()}\\n{sig}\")\n",
    "    ds_hero_ds.append(d_val)\n",
    "    ds_colors.append(colors_ds.get(ds, '#7f7f7f'))\n",
    "\n",
    "if ds_hero_ds:\n",
    "    bars = ax.bar(range(len(ds_hero_ds)), ds_hero_ds, color=ds_colors,\n",
    "                  edgecolor='black', linewidth=0.5)\n",
    "    for i, d_val in enumerate(ds_hero_ds):\n",
    "        ax.text(i, d_val + (0.01 if d_val >= 0 else -0.03),\n",
    "                f\"{d_val:+.3f}\", ha='center',\n",
    "                va='bottom' if d_val >= 0 else 'top', fontsize=9, fontweight='bold')\n",
    "    ax.set_xticks(range(len(ds_labels)))\n",
    "    ax.set_xticklabels(ds_labels)\n",
    "ax.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "ax.set_ylabel(\"Cohen's d (positive = helps)\")\n",
    "ax.set_title(\"(a) Hero Layer NLL Effect by Dataset\")\n",
    "\n",
    "# ---- Panel (b): All conditions comparison ----\n",
    "ax = axes[0, 1]\n",
    "cond_labels = ['sf_trunc', 'values_early', 'values_hero']\n",
    "x_pos = np.arange(len(dataset_names))\n",
    "width = 0.25\n",
    "for ci, cname in enumerate(cond_labels):\n",
    "    ds_vals = []\n",
    "    for ds in dataset_names:\n",
    "        if ds in analysis and cname in analysis[ds]:\n",
    "            ds_vals.append(analysis[ds][cname]['cohens_d'])\n",
    "        else:\n",
    "            ds_vals.append(0)\n",
    "    ax.bar(x_pos + ci * width, ds_vals, width, label=cname, alpha=0.8)\n",
    "ax.set_xticks(x_pos + width)\n",
    "ax.set_xticklabels([ds.upper() for ds in dataset_names])\n",
    "ax.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "ax.set_ylabel(\"Cohen's d\")\n",
    "ax.set_title(\"(b) All Conditions by Dataset\")\n",
    "ax.legend(fontsize=8)\n",
    "\n",
    "# ---- Panel (c): Bare NLL distributions ----\n",
    "ax = axes[0, 2]\n",
    "bare_by_ds = []\n",
    "ds_labels_box = []\n",
    "for ds in dataset_names:\n",
    "    ds_r = [r for r in all_results if r['dataset'] == ds]\n",
    "    if ds_r:\n",
    "        bare_by_ds.append([r['bare'] for r in ds_r])\n",
    "        pct_f = 100 * np.mean(np.array([r['bare'] for r in ds_r]) < 0.01)\n",
    "        ds_labels_box.append(f\"{ds.upper()}\\n({pct_f:.0f}% floor)\")\n",
    "if bare_by_ds:\n",
    "    bp = ax.boxplot(bare_by_ds, labels=ds_labels_box, showfliers=False, patch_artist=True,\n",
    "                    medianprops={'color': 'red', 'linewidth': 2})\n",
    "    for patch, ds in zip(bp['boxes'], dataset_names):\n",
    "        patch.set_facecolor(colors_ds.get(ds, '#8ecae6'))\n",
    "        patch.set_alpha(0.7)\n",
    "ax.axhline(y=0.01, color='red', linestyle='--', alpha=0.3, label='Floor (0.01)')\n",
    "ax.set_ylabel(\"Bare NLL\")\n",
    "ax.set_title(\"(c) Bare NLL Distributions\")\n",
    "ax.legend(fontsize=7)\n",
    "\n",
    "# ---- Panel (d): Generation EM by dataset ----\n",
    "ax = axes[1, 0]\n",
    "if gen_results:\n",
    "    gen_ds_labels = []\n",
    "    bare_ems = []\n",
    "    hero_ems = []\n",
    "    for ds in dataset_names:\n",
    "        ds_gen = [r for r in gen_results if r['dataset'] == ds]\n",
    "        if ds_gen:\n",
    "            gen_ds_labels.append(ds.upper())\n",
    "            bare_ems.append(100 * np.mean([r['bare_em'] for r in ds_gen]))\n",
    "            hero_ems.append(100 * np.mean([r['hero_em'] for r in ds_gen]))\n",
    "\n",
    "    x = np.arange(len(gen_ds_labels))\n",
    "    ax.bar(x - 0.15, bare_ems, 0.3, label='Bare', color='#7f7f7f', alpha=0.8)\n",
    "    ax.bar(x + 0.15, hero_ems, 0.3, label='Hero', color='#d62728', alpha=0.8)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(gen_ds_labels)\n",
    "    ax.legend()\n",
    "ax.set_ylabel(\"Exact Match (%)\")\n",
    "ax.set_title(\"(d) Generation Exact Match Rate\")\n",
    "\n",
    "# ---- Panel (e): Generation F1 by dataset ----\n",
    "ax = axes[1, 1]\n",
    "if gen_results:\n",
    "    gen_ds_labels = []\n",
    "    bare_f1s = []\n",
    "    hero_f1s = []\n",
    "    for ds in dataset_names:\n",
    "        ds_gen = [r for r in gen_results if r['dataset'] == ds]\n",
    "        if ds_gen:\n",
    "            gen_ds_labels.append(ds.upper())\n",
    "            bare_f1s.append(np.mean([r['bare_f1'] for r in ds_gen]))\n",
    "            hero_f1s.append(np.mean([r['hero_f1'] for r in ds_gen]))\n",
    "\n",
    "    x = np.arange(len(gen_ds_labels))\n",
    "    ax.bar(x - 0.15, bare_f1s, 0.3, label='Bare', color='#7f7f7f', alpha=0.8)\n",
    "    ax.bar(x + 0.15, hero_f1s, 0.3, label='Hero', color='#d62728', alpha=0.8)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(gen_ds_labels)\n",
    "    ax.legend()\n",
    "ax.set_ylabel(\"Token F1\")\n",
    "ax.set_title(\"(e) Generation Token F1\")\n",
    "\n",
    "# ---- Panel (f): NLL delta vs F1 delta scatter ----\n",
    "ax = axes[1, 2]\n",
    "if gen_results:\n",
    "    for ds in dataset_names:\n",
    "        ds_gen = [r for r in gen_results if r['dataset'] == ds]\n",
    "        if ds_gen:\n",
    "            nll_d = [r['bare_nll'] - r['hero_nll'] for r in ds_gen]\n",
    "            f1_d = [r['hero_f1'] - r['bare_f1'] for r in ds_gen]\n",
    "            ax.scatter(nll_d, f1_d, alpha=0.3, s=15, label=ds.upper(),\n",
    "                       color=colors_ds.get(ds, '#7f7f7f'))\n",
    "    ax.axhline(y=0, color='gray', linestyle='--', alpha=0.3)\n",
    "    ax.axvline(x=0, color='gray', linestyle='--', alpha=0.3)\n",
    "    ax.set_xlabel(\"NLL Delta (bare - hero, positive = hero lower)\")\n",
    "    ax.set_ylabel(\"F1 Delta (hero - bare, positive = hero better)\")\n",
    "    ax.set_title(\"(f) NLL Improvement vs F1 Improvement\")\n",
    "    ax.legend(fontsize=8)\n",
    "\n",
    "plt.suptitle('Exp 31: Ad-Content Benchmark & Generation Quality (Gemma 3 4B)',\n",
    "             fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'analysis_plots.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"Plots saved to {RESULTS_DIR / 'analysis_plots.png'}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 15: Save results.json + CSV + verdict\n",
    "\n",
    "# --- CSV (Phase 1) ---\n",
    "with open(CSV_PATH, 'w', newline='') as f:\n",
    "    fieldnames = ['query_idx', 'dataset', 'query', 'answer', 'word_count',\n",
    "                  'doc_token_len', 'answer_token_len', 'domain',\n",
    "                  'bare', 'sf_trunc', 'values_early', 'values_hero']\n",
    "    writer = csv.DictWriter(f, fieldnames=fieldnames, extrasaction='ignore')\n",
    "    writer.writeheader()\n",
    "    for r in all_results:\n",
    "        writer.writerow({k: r.get(k, '') for k in fieldnames})\n",
    "print(f\"NLL CSV saved: {CSV_PATH}\")\n",
    "\n",
    "# --- CSV (Phase 2: Generation) ---\n",
    "with open(GEN_CSV_PATH, 'w', newline='') as f:\n",
    "    fieldnames = ['query_idx', 'dataset', 'query', 'answer', 'word_count',\n",
    "                  'bare_nll', 'hero_nll', 'bare_text', 'hero_text',\n",
    "                  'bare_em', 'hero_em', 'bare_f1', 'hero_f1',\n",
    "                  'bare_rouge_l', 'hero_rouge_l', 'bare_log_prob', 'hero_log_prob',\n",
    "                  'texts_differ']\n",
    "    writer = csv.DictWriter(f, fieldnames=fieldnames, extrasaction='ignore')\n",
    "    writer.writeheader()\n",
    "    for r in gen_results:\n",
    "        writer.writerow({k: r.get(k, '') for k in fieldnames})\n",
    "print(f\"Generation CSV saved: {GEN_CSV_PATH}\")\n",
    "\n",
    "# --- Compute verdict inputs ---\n",
    "\n",
    "# Hero d per dataset (Phase 1)\n",
    "hero_ds = {}\n",
    "for ds in dataset_names:\n",
    "    if ds in analysis and 'values_hero' in analysis[ds]:\n",
    "        hero_ds[ds] = analysis[ds]['values_hero']['cohens_d']\n",
    "\n",
    "# Generation metrics (Phase 2)\n",
    "gen_f1_delta = None\n",
    "gen_em_delta = None\n",
    "if gen_results:\n",
    "    bare_f1s = [r['bare_f1'] for r in gen_results]\n",
    "    hero_f1s = [r['hero_f1'] for r in gen_results]\n",
    "    gen_f1_delta = float(np.mean(hero_f1s) - np.mean(bare_f1s))\n",
    "\n",
    "    bare_ems = [float(r['bare_em']) for r in gen_results]\n",
    "    hero_ems = [float(r['hero_em']) for r in gen_results]\n",
    "    gen_em_delta = float(np.mean(hero_ems) - np.mean(bare_ems))\n",
    "\n",
    "print(f\"\\nVerdict inputs:\")\n",
    "for ds, d in hero_ds.items():\n",
    "    p = analysis[ds]['values_hero']['p_value']\n",
    "    sig = '***' if p < 0.001 else '**' if p < 0.01 else '*' if p < 0.05 else 'ns'\n",
    "    print(f\"  {ds} hero d: {d:+.3f} {sig}\")\n",
    "if gen_f1_delta is not None:\n",
    "    print(f\"  Generation F1 delta (hero - bare): {gen_f1_delta:+.3f}\")\n",
    "    print(f\"  Generation EM delta (hero - bare): {gen_em_delta:+.3f}\")\n",
    "\n",
    "# --- Verdict ---\n",
    "n_positive = sum(1 for d in hero_ds.values() if d > 0.1)\n",
    "n_significant = sum(1 for ds, d in hero_ds.items()\n",
    "                    if analysis[ds]['values_hero']['p_value'] < 0.05 and d > 0)\n",
    "\n",
    "if n_positive >= 2 and gen_f1_delta is not None and gen_f1_delta > 0:\n",
    "    verdict = (f\"AD-CONTENT BENEFIT: Hero layers help NLL on {n_positive}/3 datasets \"\n",
    "               f\"and generation F1 improves by {gen_f1_delta:+.3f}. \"\n",
    "               f\"Operationalization for short ad-content is viable.\")\n",
    "elif n_positive >= 1 and gen_f1_delta is not None and gen_f1_delta > 0:\n",
    "    verdict = (f\"PARTIAL BENEFIT: Hero layers help NLL on {n_positive}/3 datasets \"\n",
    "               f\"and generation F1 improves by {gen_f1_delta:+.3f}. \"\n",
    "               f\"Benefits are dataset-specific.\")\n",
    "elif n_positive >= 1 and (gen_f1_delta is None or gen_f1_delta <= 0):\n",
    "    verdict = (f\"NLL-ONLY BENEFIT: Hero layers help NLL on {n_positive}/3 datasets \"\n",
    "               f\"but generation quality does NOT improve \"\n",
    "               f\"(F1 delta: {gen_f1_delta:+.3f if gen_f1_delta is not None else 'n/a'}). \"\n",
    "               f\"NLL improvement does not translate to better answers.\")\n",
    "else:\n",
    "    verdict = (f\"NO BENEFIT: Hero layers do not help NLL on ad-content \"\n",
    "               f\"({n_positive}/3 datasets with d>0.1). \"\n",
    "               f\"Priming is not viable for this content type.\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"VERDICT: {verdict}\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "# Updated hero scorecard\n",
    "print(f\"\\nUpdated hero scorecard (all experiments + this one):\")\n",
    "print(\"  MARCO (Mistral): d=+0.472*** (Exp 07)\")\n",
    "for ds, d in hero_ds.items():\n",
    "    p = analysis[ds]['values_hero']['p_value']\n",
    "    sig = '***' if p < 0.001 else '**' if p < 0.01 else '*' if p < 0.05 else 'ns'\n",
    "    print(f\"  {ds} (Exp 31): d={d:+.3f} {sig}\")\n",
    "print(\"  NQ (Exp 27b/30): d=+0.213***\")\n",
    "print(\"  TriviaQA (Exp 27b): d=+0.000 (ceiling)\")\n",
    "print(\"  DROP (Exp 29/30): d=-0.152** (hurts)\")\n",
    "\n",
    "# --- results.json ---\n",
    "final = {\n",
    "    'experiment': 'exp31_ad_benchmark_and_generation',\n",
    "    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'config': {\n",
    "        'model_name': MODEL_NAME,\n",
    "        'model_type': 'gemma3',\n",
    "        'seed': SEED,\n",
    "        'n_per_dataset': N_PER_DATASET,\n",
    "        'max_doc_tokens': MAX_DOC_TOKENS,\n",
    "        'max_passage_words': MAX_PASSAGE_WORDS,\n",
    "        'max_gen_tokens': MAX_GEN_TOKENS,\n",
    "        'conditions': CONDITION_NAMES,\n",
    "        'early_layer_cutoff': EARLY_LAYER_CUTOFF,\n",
    "        'hero_layers': HERO_LAYERS,\n",
    "        'prefix': STATIC_FACT,\n",
    "        'prefix_token_len': PREFIX_TOKEN_LEN,\n",
    "        'datasets': dataset_names,\n",
    "    },\n",
    "    'phase1_nll': {\n",
    "        'per_dataset_analysis': analysis,\n",
    "        'hero_ds': hero_ds,\n",
    "    },\n",
    "    'phase2_generation': {\n",
    "        'overall': gen_analysis,\n",
    "        'per_dataset': gen_per_dataset,\n",
    "        'n_gen_samples': len(gen_results),\n",
    "        'n_texts_differ': sum(1 for r in gen_results if r['texts_differ']),\n",
    "        'f1_delta': gen_f1_delta,\n",
    "        'em_delta': gen_em_delta,\n",
    "        'nll_f1_correlation': {\n",
    "            'spearman_rho': float(rho) if np.isfinite(rho) else None,\n",
    "        },\n",
    "    },\n",
    "    'ceiling_status': {\n",
    "        ds: float(np.mean(np.array([r['bare'] for r in all_results if r['dataset'] == ds]) < 0.01) * 100)\n",
    "        for ds in dataset_names if any(r['dataset'] == ds for r in all_results)\n",
    "    },\n",
    "    'verdict': verdict,\n",
    "    'per_sample_nll_results': all_results,\n",
    "    'per_sample_gen_results': gen_results,\n",
    "}\n",
    "\n",
    "with open(FINAL_RESULTS_PATH, 'w') as f:\n",
    "    json.dump(final, f, indent=2)\n",
    "\n",
    "print(f\"\\nResults saved to {FINAL_RESULTS_PATH}\")\n",
    "print(f\"File size: {FINAL_RESULTS_PATH.stat().st_size / 1024:.1f} KB\")\n",
    "print(f\"\\nDone!\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 16: GPU cleanup\n",
    "import gc\n",
    "\n",
    "print(\"Cleaning up GPU memory...\")\n",
    "mem_before = torch.cuda.memory_allocated() / 1e9\n",
    "\n",
    "del model\n",
    "del tokenizer\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "mem_after = torch.cuda.memory_allocated() / 1e9\n",
    "print(f\"GPU memory: {mem_before:.2f} GB -> {mem_after:.2f} GB\")\n",
    "print(\"Cleanup complete.\")"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}