{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exp 16: Cross-Model Priming Replication (Gemma 3 4B)\n",
    "\n",
    "## Motivation\n",
    "\n",
    "All 15 prior experiments used Mistral-7B-Instruct-v0.2 exclusively. The critical open\n",
    "question: **is value contamination via priming a universal transformer mechanism, or\n",
    "Mistral-specific?**\n",
    "\n",
    "Gemma 3 4B is an ideal comparison model:\n",
    "- Different architecture: 34 layers (vs 32), head_dim=256 (vs 128), 4 KV heads (vs 8)\n",
    "- Per-layer RoPE: sliding_attention=10k theta, full_attention=1M theta (vs uniform 1M)\n",
    "- GQA with different grouping ratio\n",
    "- bfloat16 required (float16 produces garbage)\n",
    "- All lib functions already support it\n",
    "\n",
    "## Architecture Comparison\n",
    "\n",
    "| Property | Mistral-7B | Gemma 3 4B |\n",
    "|----------|-----------|------------|\n",
    "| Parameters | 7B | 4B |\n",
    "| Layers | 32 | 34 |\n",
    "| Hidden size | 4096 | 2560 |\n",
    "| Attention heads | 32 | 8 |\n",
    "| KV heads | 8 | 4 |\n",
    "| Head dim | 128 | 256 |\n",
    "| RoPE theta | 1M (uniform) | 10k/1M (per-layer) |\n",
    "| Dtype | float16 | bfloat16 |\n",
    "\n",
    "## Design\n",
    "\n",
    "**5 conditions on MS MARCO v1.1 (where priming works on Mistral), N=300 queries:**\n",
    "\n",
    "| # | Condition | Description | Tests |\n",
    "|---|-----------|-------------|-------|\n",
    "| 1 | bare | Baseline \u2014 no prefix | \u2014 |\n",
    "| 2 | static_fact_trunc | \"What are the key facts?\" prefix, truncated+RoPE | Best Mistral condition on Gemma? |\n",
    "| 3 | random_trunc | Random text prefix, truncated+RoPE | Does any prefix help? |\n",
    "| 4 | oracle_trunc | Actual query as prefix, truncated+RoPE | Is random > oracle on Gemma too? |\n",
    "| 5 | values_only | Bare keys + sf primed values (hybrid cache) | Is it value contamination on Gemma? |\n",
    "\n",
    "## Mistral Reference Values (from Exp 01 + 08)\n",
    "\n",
    "| Metric | Mistral Value | Source |\n",
    "|--------|--------------|--------|\n",
    "| static_fact_trunc d | +0.472 | Exp 07 |\n",
    "| random_trunc d | +0.091 | Exp 01 |\n",
    "| oracle_trunc d | +0.023 (ns) | Exp 01 |\n",
    "| values_only fraction | 108% | Exp 08 |\n",
    "| keys_only fraction | -4% | Exp 08 |"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 1: Setup\n",
    "import os\n",
    "os.umask(0o000)\n",
    "\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import gc\n",
    "from pathlib import Path\n",
    "\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "RESULTS_DIR = Path(\"results/exp16\")\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CHECKPOINT_PATH = RESULTS_DIR / \"checkpoint.json\"\n",
    "FINAL_RESULTS_PATH = RESULTS_DIR / \"results.json\"\n",
    "\n",
    "print(f\"SEED: {SEED}\")\n",
    "print(f\"Results directory: {RESULTS_DIR}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 2: Load Gemma 3 4B via load_model()\n",
    "sys.path.insert(0, \".\")\n",
    "\n",
    "from lib.config import ExperimentConfig\n",
    "from lib.model_utils import load_model\n",
    "\n",
    "MODEL_NAME = \"google/gemma-3-4b-it\"\n",
    "\n",
    "exp_config = ExperimentConfig(\n",
    "    model_name=MODEL_NAME,\n",
    "    model_type=\"gemma3\",\n",
    "    compute_dtype=\"auto\",  # resolves to bfloat16 for gemma3\n",
    "    use_4bit=True,\n",
    "    num_samples=2000,\n",
    "    seed=SEED,\n",
    ")\n",
    "\n",
    "print(f\"Loading {MODEL_NAME} (4-bit, bfloat16)...\")\n",
    "model, tokenizer = load_model(exp_config)\n",
    "\n",
    "# Architecture diagnostics\n",
    "from lib.kv_cache import _get_text_config, _get_head_dim, _get_rope_theta_for_layer, _get_cache_keys, _ensure_dynamic_cache\n",
    "\n",
    "text_config = _get_text_config(model.config)\n",
    "print(f\"\\nModel loaded successfully.\")\n",
    "print(f\"  Model class: {type(model).__name__}\")\n",
    "print(f\"  Text config class: {type(text_config).__name__}\")\n",
    "print(f\"  Hidden size: {text_config.hidden_size}\")\n",
    "print(f\"  Num layers: {text_config.num_hidden_layers}\")\n",
    "print(f\"  Num attention heads: {text_config.num_attention_heads}\")\n",
    "print(f\"  Num KV heads: {text_config.num_key_value_heads}\")\n",
    "print(f\"  Head dim: {_get_head_dim(model.config)}\")\n",
    "print(f\"  BOS token ID: {tokenizer.bos_token_id}\")\n",
    "print(f\"  EOS token ID: {tokenizer.eos_token_id}\")\n",
    "\n",
    "# Per-layer RoPE diagnostics\n",
    "thetas = set()\n",
    "for layer_idx in range(text_config.num_hidden_layers):\n",
    "    thetas.add(_get_rope_theta_for_layer(model.config, layer_idx))\n",
    "print(f\"  Unique RoPE thetas: {sorted(thetas)}\")\n",
    "\n",
    "# Verify dtype\n",
    "sample_ids = tokenizer(\"test\", return_tensors=\"pt\")['input_ids'].to(exp_config.device)\n",
    "with torch.no_grad():\n",
    "    out = model(sample_ids, use_cache=True)\n",
    "    cache_check = _ensure_dynamic_cache(out.past_key_values)\n",
    "    k0 = _get_cache_keys(cache_check, 0)\n",
    "    print(f\"  Cache key dtype: {k0.dtype}\")\n",
    "    print(f\"  Cache key shape: {k0.shape}  (batch, kv_heads, seq, head_dim)\")\n",
    "del out, sample_ids\n",
    "torch.cuda.empty_cache()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 3: Config and library imports\n",
    "from lib.kv_cache import (\n",
    "    _get_cache_keys,\n",
    "    _get_cache_values,\n",
    "    _set_cache_keys,\n",
    "    _set_cache_values,\n",
    "    _ensure_dynamic_cache,\n",
    "    extract_and_truncate_cache_with_bos,\n",
    "    correct_rope_positions_with_bos,\n",
    "    score_answer_with_cache,\n",
    "    deepcopy_cache,\n",
    "    build_hybrid_cache,\n",
    ")\n",
    "from lib.analysis import cohens_d\n",
    "from lib.data import count_words\n",
    "from scipy import stats\n",
    "from scipy.stats import wilcoxon\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Templates \u2014 bare text, no \"Document:\\n\" framing\n",
    "SURROGATE_PREFIX_TEMPLATE = \"{surrogate}\\n\"\n",
    "DOCUMENT_TEMPLATE = \"{document}\"\n",
    "QUERY_TEMPLATE = \"\\nQuery: {query}\\nAnswer:\"\n",
    "ANSWER_TEMPLATE = \" {answer}\"\n",
    "\n",
    "# Prefix texts\n",
    "from lib.surrogate import STATIC_SURROGATE_QUERIES\n",
    "STATIC_FACT = STATIC_SURROGATE_QUERIES['static_factual']['query']\n",
    "RANDOM_PREFIX_TEXT = \"The purple elephant danced gracefully on the frozen lake during twilight\"\n",
    "\n",
    "# Experiment parameters\n",
    "MAX_QUERIES = 300\n",
    "MAX_PASSAGE_WORDS = 300\n",
    "MIN_PASSAGES_PER_QUERY = 2\n",
    "CHECKPOINT_EVERY = 25\n",
    "\n",
    "CONDITION_NAMES = ['bare', 'static_fact_trunc', 'random_trunc', 'oracle_trunc', 'values_only']\n",
    "\n",
    "# Mistral reference values (from Exp 01 + 08)\n",
    "MISTRAL_REF = {\n",
    "    'random_trunc_d': 0.091,\n",
    "    'oracle_trunc_d': 0.023,\n",
    "    'static_fact_trunc_d': 0.472,\n",
    "    'values_fraction': 1.083,\n",
    "    'keys_fraction': -0.036,\n",
    "    'd_full_trunc': 0.254,\n",
    "    'd_values_only': 0.275,\n",
    "    'd_keys_only': -0.009,\n",
    "}\n",
    "\n",
    "print(\"Config ready\")\n",
    "print(f\"  Model: {MODEL_NAME}\")\n",
    "print(f\"  MAX_QUERIES: {MAX_QUERIES}\")\n",
    "print(f\"  Conditions: {CONDITION_NAMES}\")\n",
    "print(f\"  Prefixes:\")\n",
    "print(f\"    static_fact: '{STATIC_FACT}'\")\n",
    "print(f\"    random:      '{RANDOM_PREFIX_TEXT}'\")\n",
    "print(f\"    oracle:      (actual query text per sample)\")\n",
    "print(f\"  Mistral reference d values:\")\n",
    "for k, v in MISTRAL_REF.items():\n",
    "    print(f\"    {k}: {v:+.3f}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 4: Load MS MARCO v1.1 (same filtering as previous ranking experiments)\n",
    "from datasets import load_dataset\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"LOADING MS MARCO v1.1 \u2014 ALL PASSAGES PER QUERY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "dataset = load_dataset(\"microsoft/ms_marco\", \"v1.1\", split=\"validation\",\n",
    "                        trust_remote_code=True)\n",
    "print(f\"Total items in validation: {len(dataset)}\")\n",
    "\n",
    "queries = []\n",
    "np.random.seed(SEED)\n",
    "\n",
    "for item in tqdm(dataset, desc=\"Filtering\"):\n",
    "    passages_info = item.get('passages', {})\n",
    "    passage_texts = passages_info.get('passage_text', [])\n",
    "    is_selected = passages_info.get('is_selected', [])\n",
    "    query = item.get('query', '')\n",
    "    answers = item.get('answers', [])\n",
    "    well_formed = item.get('wellFormedAnswers', [])\n",
    "\n",
    "    if not passage_texts or not query:\n",
    "        continue\n",
    "    if len(passage_texts) < MIN_PASSAGES_PER_QUERY:\n",
    "        continue\n",
    "    if not is_selected or sum(is_selected) == 0:\n",
    "        continue\n",
    "\n",
    "    word_counts = [count_words(p) for p in passage_texts]\n",
    "    if any(wc > MAX_PASSAGE_WORDS for wc in word_counts):\n",
    "        continue\n",
    "\n",
    "    answer = None\n",
    "    if well_formed and len(well_formed) > 0 and well_formed[0] != '[]':\n",
    "        answer = well_formed[0]\n",
    "    elif answers and len(answers) > 0 and answers[0] != 'No Answer Present.':\n",
    "        answer = answers[0]\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "    passage_list = []\n",
    "    for i, (ptext, sel) in enumerate(zip(passage_texts, is_selected)):\n",
    "        passage_list.append({\n",
    "            'passage': ptext,\n",
    "            'is_relevant': bool(sel == 1),\n",
    "            'word_count': word_counts[i],\n",
    "            'passage_idx': i,\n",
    "        })\n",
    "\n",
    "    queries.append({\n",
    "        'query': query,\n",
    "        'answer': answer,\n",
    "        'passages': passage_list,\n",
    "        'n_passages': len(passage_list),\n",
    "        'n_relevant': sum(1 for p in passage_list if p['is_relevant']),\n",
    "    })\n",
    "\n",
    "    if len(queries) >= MAX_QUERIES * 3:\n",
    "        break\n",
    "\n",
    "np.random.shuffle(queries)\n",
    "queries = queries[:MAX_QUERIES]\n",
    "N = len(queries)\n",
    "\n",
    "n_passages_list = [q['n_passages'] for q in queries]\n",
    "total_passages = sum(n_passages_list)\n",
    "\n",
    "print(f\"\\nSelected {N} queries ({total_passages} total passages)\")\n",
    "print(f\"Passages per query: mean={np.mean(n_passages_list):.1f}, \"\n",
    "      f\"min={min(n_passages_list)}, max={max(n_passages_list)}\")\n",
    "print(f\"Word counts: mean={np.mean([p['word_count'] for q in queries for p in q['passages']]):.0f}\")\n",
    "\n",
    "del dataset\n",
    "gc.collect()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 5: Tokenize prefixes and verify BPE boundaries\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PREFIX TOKENIZATION \u2014 GEMMA 3 4B\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Tokenize each prefix\n",
    "sf_str = SURROGATE_PREFIX_TEMPLATE.format(surrogate=STATIC_FACT)\n",
    "rand_str = SURROGATE_PREFIX_TEMPLATE.format(surrogate=RANDOM_PREFIX_TEXT)\n",
    "\n",
    "sf_ids = tokenizer(sf_str, return_tensors=\"pt\",\n",
    "                    add_special_tokens=False)['input_ids'].to(exp_config.device)\n",
    "rand_ids = tokenizer(rand_str, return_tensors=\"pt\",\n",
    "                      add_special_tokens=False)['input_ids'].to(exp_config.device)\n",
    "\n",
    "# Example oracle prefix\n",
    "example_oracle_str = SURROGATE_PREFIX_TEMPLATE.format(surrogate=queries[0]['query'])\n",
    "oracle_example_ids = tokenizer(example_oracle_str, return_tensors=\"pt\",\n",
    "                                add_special_tokens=False)['input_ids'].to(exp_config.device)\n",
    "\n",
    "PREFIX_CONFIGS = [\n",
    "    ('static_fact', STATIC_FACT, sf_str, sf_ids),\n",
    "    ('random', RANDOM_PREFIX_TEXT, rand_str, rand_ids),\n",
    "    ('oracle (ex)', queries[0]['query'], example_oracle_str, oracle_example_ids),\n",
    "]\n",
    "\n",
    "print(\"\\nPREFIX TOKEN LENGTHS (Gemma tokenizer):\")\n",
    "for name, text, full_str, ids in PREFIX_CONFIGS:\n",
    "    print(f\"  {name:<15} {ids.shape[1]:>3} tokens | '{text}'\")\n",
    "\n",
    "# Verify BPE boundary consistency across prefixes\n",
    "print(\"\\nBPE BOUNDARY CHECK (first passage):\")\n",
    "example_doc = queries[0]['passages'][0]['passage']\n",
    "for name, text, full_str, ids in PREFIX_CONFIGS:\n",
    "    concat = full_str + DOCUMENT_TEMPLATE.format(document=example_doc)\n",
    "    concat_enc = tokenizer(concat, add_special_tokens=True)['input_ids']\n",
    "    prefix_enc = tokenizer(full_str, add_special_tokens=True)['input_ids']\n",
    "    doc_ids_from_concat = concat_enc[len(prefix_enc):]\n",
    "\n",
    "    bare_doc_enc = tokenizer(DOCUMENT_TEMPLATE.format(document=example_doc),\n",
    "                              add_special_tokens=False)['input_ids']\n",
    "    match = sum(1 for a, b in zip(doc_ids_from_concat, bare_doc_enc) if a == b)\n",
    "    total = max(len(bare_doc_enc), 1)\n",
    "    print(f\"  {name:<15}: {match}/{total} tokens match ({100*match/total:.1f}%)\")\n",
    "\n",
    "# Condition summary\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"CONDITION DETAILS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "conditions_detail = [\n",
    "    (\"bare\",\n",
    "     \"Standard bare cache: [BOS][doc]\",\n",
    "     \"Baseline. All other conditions compared to this.\"),\n",
    "    (\"static_fact_trunc\",\n",
    "     f\"Prefix: '{STATIC_FACT}' \u2192 truncate + RoPE correct\",\n",
    "     \"Best single Mistral condition (d=+0.472). Does it replicate on Gemma?\"),\n",
    "    (\"random_trunc\",\n",
    "     f\"Prefix: '{RANDOM_PREFIX_TEXT}' \u2192 truncate + RoPE correct\",\n",
    "     \"Random prefix (d=+0.091 on Mistral). Does ANY prefix help on Gemma?\"),\n",
    "    (\"oracle_trunc\",\n",
    "     \"Prefix: actual query text \u2192 truncate + RoPE correct\",\n",
    "     \"Oracle prefix (d=+0.023 ns on Mistral). Is random > oracle on Gemma too?\"),\n",
    "    (\"values_only\",\n",
    "     \"Bare keys + static_fact primed values (hybrid cache)\",\n",
    "     \"Tests value contamination mechanism. On Mistral: values=108%, keys=-4%.\"),\n",
    "]\n",
    "\n",
    "for name, detail, purpose in conditions_detail:\n",
    "    print(f\"\\n### {name} ###\")\n",
    "    print(f\"  {detail}\")\n",
    "    print(f\"  Purpose: {purpose}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 6: Main loop \u2014 score all passages under all 5 conditions\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"MAIN EVALUATION ({N} queries, ~{total_passages} passages)\")\n",
    "print(\"Model: Gemma 3 4B\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Checkpoint resume\n",
    "all_results = []\n",
    "start_idx = 0\n",
    "\n",
    "if CHECKPOINT_PATH.exists():\n",
    "    with open(CHECKPOINT_PATH, 'r') as f:\n",
    "        ckpt = json.load(f)\n",
    "    ckpt_queries = ckpt.get('query_texts', [])\n",
    "    current_queries = [q['query'] for q in queries]\n",
    "    if ckpt_queries == current_queries:\n",
    "        all_results = ckpt['results']\n",
    "        start_idx = len(all_results)\n",
    "        print(f\"Resuming from checkpoint: {start_idx}/{N}\")\n",
    "    else:\n",
    "        print(\"Checkpoint query mismatch. Starting fresh.\")\n",
    "else:\n",
    "    print(\"No checkpoint found. Starting fresh.\")\n",
    "\n",
    "print(f\"Evaluating queries {start_idx} to {N-1}\")\n",
    "print(f\"Per passage: 4 forward passes (bare + 3 primed) + 5 scoring passes\")\n",
    "\n",
    "t_start = time.time()\n",
    "\n",
    "for qidx in tqdm(range(start_idx, N), initial=start_idx, total=N, desc=\"Queries\"):\n",
    "    query_data = queries[qidx]\n",
    "    query = query_data['query']\n",
    "    answer = query_data['answer']\n",
    "    query_prompt = QUERY_TEMPLATE.format(query=query)\n",
    "    answer_text = ANSWER_TEMPLATE.format(answer=answer)\n",
    "\n",
    "    passage_results = []\n",
    "\n",
    "    for pidx, pinfo in enumerate(query_data['passages']):\n",
    "        passage = pinfo['passage']\n",
    "        document_text = DOCUMENT_TEMPLATE.format(document=passage)\n",
    "\n",
    "        # --- Matched tokenization (using sf prefix as reference) ---\n",
    "        full_text = sf_str + document_text\n",
    "        full_enc = tokenizer(full_text, return_tensors=\"pt\",\n",
    "                              add_special_tokens=True, padding=False, truncation=False)\n",
    "        full_ids = full_enc['input_ids'].to(exp_config.device)\n",
    "\n",
    "        sf_prefix_enc = tokenizer(sf_str, return_tensors=\"pt\",\n",
    "                                   add_special_tokens=True, padding=False, truncation=False)\n",
    "        sf_prefix_len_matched = sf_prefix_enc['input_ids'].shape[1]\n",
    "\n",
    "        bos_id = full_ids[:, :1]\n",
    "        doc_ids = full_ids[:, sf_prefix_len_matched:]\n",
    "        doc_len = doc_ids.shape[1]\n",
    "        context_len = 1 + doc_len  # BOS + doc\n",
    "\n",
    "        del full_enc, full_ids, sf_prefix_enc\n",
    "\n",
    "        # === 1. Build bare cache ===\n",
    "        bare_input = torch.cat([bos_id, doc_ids], dim=1)\n",
    "        with torch.no_grad():\n",
    "            bare_out = model(input_ids=bare_input,\n",
    "                             attention_mask=torch.ones_like(bare_input),\n",
    "                             use_cache=True, return_dict=True)\n",
    "        bare_cache = _ensure_dynamic_cache(bare_out.past_key_values)\n",
    "        del bare_out, bare_input\n",
    "\n",
    "        # === 2-4. For each priming prefix: build primed cache, truncate + RoPE ===\n",
    "        # Prefix configs for this passage\n",
    "        oracle_str = SURROGATE_PREFIX_TEMPLATE.format(surrogate=query)\n",
    "        oracle_ids_local = tokenizer(oracle_str, return_tensors=\"pt\",\n",
    "                                      add_special_tokens=False)['input_ids'].to(exp_config.device)\n",
    "\n",
    "        prefix_configs_local = [\n",
    "            ('static_fact_trunc', sf_ids),\n",
    "            ('random_trunc', rand_ids),\n",
    "            ('oracle_trunc', oracle_ids_local),\n",
    "        ]\n",
    "\n",
    "        primed_caches = {}  # Store truncated+corrected caches\n",
    "\n",
    "        for p_name, p_ids in prefix_configs_local:\n",
    "            primed_input = torch.cat([bos_id, p_ids, doc_ids], dim=1)\n",
    "            with torch.no_grad():\n",
    "                primed_out = model(input_ids=primed_input,\n",
    "                                   attention_mask=torch.ones_like(primed_input),\n",
    "                                   use_cache=True, return_dict=True)\n",
    "            primed_full = _ensure_dynamic_cache(primed_out.past_key_values)\n",
    "            del primed_out, primed_input\n",
    "\n",
    "            # Truncate: keep [BOS] + [last doc_len positions]\n",
    "            primed_trunc = extract_and_truncate_cache_with_bos(primed_full, doc_len)\n",
    "            # RoPE correct: shift doc positions back by prefix_len\n",
    "            correct_rope_positions_with_bos(primed_trunc, p_ids.shape[1], model)\n",
    "            del primed_full\n",
    "\n",
    "            primed_caches[p_name] = primed_trunc\n",
    "\n",
    "        del oracle_ids_local\n",
    "\n",
    "        # === 5. Build values_only hybrid cache (bare keys + sf primed values) ===\n",
    "        hybrid_values = build_hybrid_cache(\n",
    "            keys_source=bare_cache,\n",
    "            values_source=primed_caches['static_fact_trunc'],\n",
    "        )\n",
    "\n",
    "        # === Score all conditions ===\n",
    "        # Score hybrid and primed conditions first (need deepcopy for each)\n",
    "        nll_values_only = score_answer_with_cache(\n",
    "            deepcopy_cache(hybrid_values), context_len, query_prompt, answer_text,\n",
    "            model, tokenizer, exp_config)\n",
    "        del hybrid_values\n",
    "\n",
    "        nll_sf = score_answer_with_cache(\n",
    "            deepcopy_cache(primed_caches['static_fact_trunc']), context_len,\n",
    "            query_prompt, answer_text, model, tokenizer, exp_config)\n",
    "\n",
    "        nll_rand = score_answer_with_cache(\n",
    "            deepcopy_cache(primed_caches['random_trunc']), context_len,\n",
    "            query_prompt, answer_text, model, tokenizer, exp_config)\n",
    "\n",
    "        nll_oracle = score_answer_with_cache(\n",
    "            deepcopy_cache(primed_caches['oracle_trunc']), context_len,\n",
    "            query_prompt, answer_text, model, tokenizer, exp_config)\n",
    "\n",
    "        del primed_caches\n",
    "\n",
    "        # Score bare LAST (mutates cache)\n",
    "        nll_bare = score_answer_with_cache(\n",
    "            bare_cache, context_len, query_prompt, answer_text,\n",
    "            model, tokenizer, exp_config)\n",
    "        del bare_cache\n",
    "\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        passage_results.append({\n",
    "            'passage_idx': pinfo['passage_idx'],\n",
    "            'is_relevant': pinfo['is_relevant'],\n",
    "            'word_count': pinfo['word_count'],\n",
    "            'bare_nll': nll_bare,\n",
    "            'static_fact_trunc_nll': nll_sf,\n",
    "            'random_trunc_nll': nll_rand,\n",
    "            'oracle_trunc_nll': nll_oracle,\n",
    "            'values_only_nll': nll_values_only,\n",
    "        })\n",
    "\n",
    "    all_results.append({\n",
    "        'query_idx': qidx,\n",
    "        'query': query,\n",
    "        'n_passages': len(passage_results),\n",
    "        'n_relevant': query_data['n_relevant'],\n",
    "        'passage_data': passage_results,\n",
    "    })\n",
    "\n",
    "    # Checkpoint\n",
    "    if (qidx + 1) % CHECKPOINT_EVERY == 0 or qidx == N - 1:\n",
    "        ckpt_data = {\n",
    "            'results': all_results,\n",
    "            'query_texts': [q['query'] for q in queries],\n",
    "            'completed': len(all_results),\n",
    "            'total': N,\n",
    "            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        }\n",
    "        with open(CHECKPOINT_PATH, 'w') as f:\n",
    "            json.dump(ckpt_data, f)\n",
    "        elapsed = time.time() - t_start\n",
    "        n_done = qidx - start_idx + 1\n",
    "        rate = n_done / elapsed if elapsed > 0 else 0\n",
    "        remaining = (N - qidx - 1) / rate if rate > 0 else 0\n",
    "        tqdm.write(f\"  Checkpoint {qidx+1}/{N} | {n_done} done in {elapsed/60:.1f}m | \"\n",
    "                   f\"ETA: {remaining/60:.1f} min\")\n",
    "\n",
    "elapsed_total = time.time() - t_start\n",
    "print(f\"\\nEvaluation complete: {len(all_results)} queries in {elapsed_total/60:.1f} min\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 7: Analysis \u2014 per-condition stats, Cohen's d, Wilcoxon, cross-model comparison\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ANALYSIS \u2014 CROSS-MODEL PRIMING REPLICATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "N_VALID = len(all_results)\n",
    "print(f\"Valid queries: {N_VALID}\")\n",
    "\n",
    "# --- Collect per-passage NLLs ---\n",
    "cond_nlls = {cn: [] for cn in CONDITION_NAMES}\n",
    "for r in all_results:\n",
    "    for p in r['passage_data']:\n",
    "        for cn in CONDITION_NAMES:\n",
    "            cond_nlls[cn].append(p[f'{cn}_nll'])\n",
    "\n",
    "cond_arrays = {cn: np.array(vals) for cn, vals in cond_nlls.items()}\n",
    "\n",
    "# Filter zero NLLs\n",
    "valid = np.ones(len(cond_arrays['bare']), dtype=bool)\n",
    "for cn in CONDITION_NAMES:\n",
    "    valid &= (cond_arrays[cn] != 0)\n",
    "n_passages_valid = int(np.sum(valid))\n",
    "n_excluded = int(np.sum(~valid))\n",
    "print(f\"Total passages: {len(valid)}, Valid: {n_passages_valid}, Excluded: {n_excluded}\")\n",
    "\n",
    "c = {}\n",
    "for cn in CONDITION_NAMES:\n",
    "    c[cn] = cond_arrays[cn][valid]\n",
    "\n",
    "# === 1. NLL Summary Table ===\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"NLL SUMMARY (per-passage, Gemma 3 4B)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\n{'Condition':<25} {'Mean NLL':>10} {'Std':>10} {'d vs Bare':>10} {'Win%':>8}\")\n",
    "print(\"-\" * 68)\n",
    "\n",
    "gemma_ds = {}\n",
    "for cn in CONDITION_NAMES:\n",
    "    mean_nll = np.mean(c[cn])\n",
    "    std_nll = np.std(c[cn])\n",
    "    if cn == 'bare':\n",
    "        print(f\"{cn:<25} {mean_nll:>10.4f} {std_nll:>10.4f} {'\u2014':>10} {'\u2014':>8}\")\n",
    "    else:\n",
    "        delta = c['bare'] - c[cn]\n",
    "        d = cohens_d(delta)\n",
    "        win = np.mean(delta > 0) * 100\n",
    "        gemma_ds[cn] = d\n",
    "        print(f\"{cn:<25} {mean_nll:>10.4f} {std_nll:>10.4f} {d:>+10.3f} {win:>7.1f}%\")\n",
    "\n",
    "# === 2. Statistical Tests ===\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"STATISTICAL TESTS (paired t-test, per-passage)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\n{'Condition':<25} {'Mean \u0394NLL':>10} {'d':>8} {'t':>8} {'p':>12} {'Sig':>5}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "stat_results = {}\n",
    "for cn in CONDITION_NAMES:\n",
    "    if cn == 'bare':\n",
    "        continue\n",
    "    delta = c['bare'] - c[cn]\n",
    "    d = cohens_d(delta)\n",
    "    t_stat, p_val = stats.ttest_1samp(delta, 0)\n",
    "    sig = \"***\" if p_val < 0.001 else \"**\" if p_val < 0.01 else \"*\" if p_val < 0.05 else \"ns\"\n",
    "    print(f\"{cn:<25} {np.mean(delta):>10.4f} {d:>+8.3f} {t_stat:>8.2f} {p_val:>11.2e} {sig:>5}\")\n",
    "    stat_results[cn] = {\n",
    "        'mean_delta': float(np.mean(delta)),\n",
    "        'cohens_d': float(d),\n",
    "        't_stat': float(t_stat),\n",
    "        'p_value': float(p_val),\n",
    "        'significant': bool(p_val < 0.05),\n",
    "        'win_rate': float(np.mean(delta > 0)),\n",
    "    }\n",
    "\n",
    "# === 3. Mechanism Decomposition ===\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"MECHANISM DECOMPOSITION (Gemma 3 4B)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "d_sf = gemma_ds.get('static_fact_trunc', 0)\n",
    "d_vo = gemma_ds.get('values_only', 0)\n",
    "\n",
    "values_fraction = d_vo / d_sf if d_sf != 0 else float('nan')\n",
    "\n",
    "print(f\"\\n  Full static_fact_trunc (keys+values): d = {d_sf:+.3f}\")\n",
    "print(f\"  Values-only (bare keys + sf values):   d = {d_vo:+.3f}\")\n",
    "if d_sf != 0:\n",
    "    print(f\"  Values fraction of full effect:        {values_fraction:.1%}\")\n",
    "\n",
    "if d_vo > 0 and values_fraction > 0.8:\n",
    "    print(f\"\\n  => VALUE CONTAMINATION confirmed on Gemma 3 4B\")\n",
    "    print(f\"     (values carry {values_fraction:.0%} of the effect, same as Mistral's {MISTRAL_REF['values_fraction']:.0%})\")\n",
    "elif d_vo > 0:\n",
    "    print(f\"\\n  => Values contribute ({values_fraction:.0%}) but keys also matter on Gemma\")\n",
    "else:\n",
    "    print(f\"\\n  => Values-only does NOT help on Gemma \u2014 different mechanism?\")\n",
    "\n",
    "# === 4. Cross-Model Comparison Table ===\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"CROSS-MODEL COMPARISON: Gemma 3 4B vs Mistral 7B\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "mistral_ref_ds = {\n",
    "    'static_fact_trunc': MISTRAL_REF['static_fact_trunc_d'],\n",
    "    'random_trunc': MISTRAL_REF['random_trunc_d'],\n",
    "    'oracle_trunc': MISTRAL_REF['oracle_trunc_d'],\n",
    "}\n",
    "\n",
    "print(f\"\\n{'Condition':<25} {'Mistral d':>10} {'Gemma d':>10} {'Ratio':>8} {'Same Sign?':>12}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for cn in ['static_fact_trunc', 'random_trunc', 'oracle_trunc']:\n",
    "    m_d = mistral_ref_ds.get(cn, 0)\n",
    "    g_d = gemma_ds.get(cn, 0)\n",
    "    ratio = g_d / m_d if m_d != 0 else float('nan')\n",
    "    same_sign = \"Yes\" if (m_d > 0 and g_d > 0) or (m_d < 0 and g_d < 0) or (m_d == 0 and g_d == 0) else \"NO\"\n",
    "    print(f\"{cn:<25} {m_d:>+10.3f} {g_d:>+10.3f} {ratio:>8.2f} {same_sign:>12}\")\n",
    "\n",
    "print(f\"\\n{'Mechanism':<25} {'Mistral':>10} {'Gemma':>10}\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"{'Values fraction':<25} {MISTRAL_REF['values_fraction']:>10.1%} {values_fraction:>10.1%}\")\n",
    "\n",
    "# Verdict\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"VERDICT\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "sf_replicates = gemma_ds.get('static_fact_trunc', 0) > 0 and stat_results.get('static_fact_trunc', {}).get('significant', False)\n",
    "rand_helps = gemma_ds.get('random_trunc', 0) > 0\n",
    "oracle_worse_than_rand = gemma_ds.get('random_trunc', 0) > gemma_ds.get('oracle_trunc', 0)\n",
    "values_mechanism = d_vo > 0 and values_fraction > 0.5\n",
    "\n",
    "if sf_replicates and values_mechanism:\n",
    "    print(\"  UNIVERSAL MECHANISM: Priming via value contamination replicates on Gemma 3 4B.\")\n",
    "    print(f\"  static_fact_trunc: d={gemma_ds.get('static_fact_trunc', 0):+.3f} (Mistral: d={MISTRAL_REF['static_fact_trunc_d']:+.3f})\")\n",
    "    print(f\"  Value contamination: {values_fraction:.0%} (Mistral: {MISTRAL_REF['values_fraction']:.0%})\")\n",
    "elif sf_replicates:\n",
    "    print(\"  PARTIAL REPLICATION: Priming helps but mechanism may differ.\")\n",
    "else:\n",
    "    print(\"  MISTRAL-SPECIFIC: Priming does NOT replicate on Gemma 3 4B.\")\n",
    "\n",
    "if rand_helps:\n",
    "    print(f\"  Random prefix helps: d={gemma_ds.get('random_trunc', 0):+.3f}\")\n",
    "if oracle_worse_than_rand:\n",
    "    print(f\"  Random > Oracle replicates: rand d={gemma_ds.get('random_trunc', 0):+.3f} > oracle d={gemma_ds.get('oracle_trunc', 0):+.3f}\")\n",
    "\n",
    "# === 5. Hardness Interaction ===\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"HARDNESS INTERACTION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "bare_all = c['bare']\n",
    "quintile_boundaries = np.percentile(bare_all, [20, 40, 60, 80])\n",
    "quintile_labels = ['Q1 (easy)', 'Q2', 'Q3', 'Q4', 'Q5 (hard)']\n",
    "\n",
    "def get_quintile(nll, boundaries):\n",
    "    for i, b in enumerate(boundaries):\n",
    "        if nll <= b:\n",
    "            return i\n",
    "    return len(boundaries)\n",
    "\n",
    "quintiles = np.array([get_quintile(nll, quintile_boundaries) for nll in bare_all])\n",
    "\n",
    "hardness_conds = ['static_fact_trunc', 'random_trunc', 'oracle_trunc', 'values_only']\n",
    "\n",
    "header = f\"{'Condition':<25}\" + \"\".join(f\"{ql:>14}\" for ql in quintile_labels) + f\"{'Overall':>14}\"\n",
    "print(f\"\\n{header}\")\n",
    "print(\"-\" * (25 + 14 * 6))\n",
    "\n",
    "hardness_breakdown = {}\n",
    "for cn in hardness_conds:\n",
    "    row = f\"{cn:<25}\"\n",
    "    quintile_ds = []\n",
    "    for q in range(5):\n",
    "        mask_q = quintiles == q\n",
    "        if np.sum(mask_q) < 10:\n",
    "            row += f\"{'n/a':>14}\"\n",
    "            quintile_ds.append(None)\n",
    "        else:\n",
    "            delta = bare_all[mask_q] - c[cn][mask_q]\n",
    "            d = cohens_d(delta)\n",
    "            row += f\"{d:>+14.3f}\"\n",
    "            quintile_ds.append(float(d))\n",
    "    d_all = cohens_d(bare_all - c[cn])\n",
    "    row += f\"{d_all:>+14.3f}\"\n",
    "    print(row)\n",
    "    hardness_breakdown[cn] = {'quintile_ds': quintile_ds, 'overall_d': float(d_all)}\n",
    "\n",
    "# Hardness correlation\n",
    "print(\"\\nHardness correlation (bare NLL vs delta):\")\n",
    "for cn in hardness_conds:\n",
    "    delta = bare_all - c[cn]\n",
    "    r, p = stats.pearsonr(bare_all, delta)\n",
    "    sig = \"***\" if p < 0.001 else \"**\" if p < 0.01 else \"*\" if p < 0.05 else \"ns\"\n",
    "    print(f\"  {cn:<25} r={r:+.3f}  p={p:.2e}  {sig}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 8: Plots \u2014 4-panel figure\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Color scheme\n",
    "colors = {\n",
    "    'bare': '#7f7f7f',\n",
    "    'static_fact_trunc': '#d62728',\n",
    "    'random_trunc': '#ff7f0e',\n",
    "    'oracle_trunc': '#2ca02c',\n",
    "    'values_only': '#1f77b4',\n",
    "}\n",
    "\n",
    "# --- Plot 1: Cohen's d bar chart with Mistral reference ---\n",
    "ax = axes[0, 0]\n",
    "conds_plot = ['static_fact_trunc', 'random_trunc', 'oracle_trunc', 'values_only']\n",
    "gemma_d_vals = [gemma_ds.get(cn, 0) for cn in conds_plot]\n",
    "mistral_d_vals = [mistral_ref_ds.get(cn, MISTRAL_REF.get(f'd_{cn}', 0)) for cn in conds_plot]\n",
    "\n",
    "# For values_only, use Mistral d_values_only\n",
    "mistral_d_vals[3] = MISTRAL_REF['d_values_only']\n",
    "\n",
    "x = np.arange(len(conds_plot))\n",
    "width = 0.35\n",
    "bars_g = ax.bar(x - width/2, gemma_d_vals, width, label='Gemma 3 4B',\n",
    "                color=[colors[cn] for cn in conds_plot], edgecolor='black', linewidth=0.5)\n",
    "bars_m = ax.bar(x + width/2, mistral_d_vals, width, label='Mistral 7B (ref)',\n",
    "                color=[colors[cn] for cn in conds_plot], edgecolor='black', linewidth=0.5,\n",
    "                alpha=0.4, hatch='//')\n",
    "\n",
    "for i, (gd, md) in enumerate(zip(gemma_d_vals, mistral_d_vals)):\n",
    "    ax.text(i - width/2, gd + 0.01, f\"{gd:+.3f}\", ha='center', fontsize=7, rotation=45)\n",
    "    ax.text(i + width/2, md + 0.01, f\"{md:+.3f}\", ha='center', fontsize=7, rotation=45, alpha=0.6)\n",
    "\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([cn.replace('_trunc', '').replace('_', '\\n') for cn in conds_plot],\n",
    "                    fontsize=8)\n",
    "ax.set_ylabel(\"Cohen's d vs Bare\")\n",
    "ax.set_title(\"Cross-Model: Gemma vs Mistral Effect Sizes\")\n",
    "ax.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "ax.legend(fontsize=8)\n",
    "\n",
    "# --- Plot 2: Hardness scatter (static_fact_trunc) ---\n",
    "ax = axes[0, 1]\n",
    "delta_sf = c['bare'] - c['static_fact_trunc']\n",
    "ax.scatter(c['bare'], delta_sf, alpha=0.15, s=8, color=colors['static_fact_trunc'])\n",
    "# Fit line\n",
    "z = np.polyfit(c['bare'], delta_sf, 1)\n",
    "x_fit = np.linspace(c['bare'].min(), c['bare'].max(), 100)\n",
    "ax.plot(x_fit, np.polyval(z, x_fit), color='black', linewidth=2, linestyle='--')\n",
    "r_sf, p_sf = stats.pearsonr(c['bare'], delta_sf)\n",
    "ax.set_xlabel(\"Bare NLL (difficulty)\")\n",
    "ax.set_ylabel(\"\u0394NLL (bare - static_fact)\")\n",
    "ax.set_title(f\"Hardness Interaction (r={r_sf:+.3f}, p={p_sf:.1e})\")\n",
    "ax.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "\n",
    "# --- Plot 3: Mechanism decomposition ---\n",
    "ax = axes[1, 0]\n",
    "decomp_labels = ['Full\\n(K+V)', 'Values\\nonly']\n",
    "gemma_decomp = [d_sf, d_vo]\n",
    "mistral_decomp = [MISTRAL_REF['d_full_trunc'], MISTRAL_REF['d_values_only']]\n",
    "\n",
    "x_dec = np.arange(len(decomp_labels))\n",
    "width_dec = 0.35\n",
    "ax.bar(x_dec - width_dec/2, gemma_decomp, width_dec, label='Gemma 3 4B',\n",
    "       color=['#d62728', '#1f77b4'], edgecolor='black', linewidth=0.5)\n",
    "ax.bar(x_dec + width_dec/2, mistral_decomp, width_dec, label='Mistral 7B (ref)',\n",
    "       color=['#d62728', '#1f77b4'], edgecolor='black', linewidth=0.5,\n",
    "       alpha=0.4, hatch='//')\n",
    "\n",
    "for i, (gd, md) in enumerate(zip(gemma_decomp, mistral_decomp)):\n",
    "    ax.text(i - width_dec/2, gd + 0.005, f\"{gd:+.3f}\", ha='center', va='bottom', fontsize=9)\n",
    "    ax.text(i + width_dec/2, md + 0.005, f\"{md:+.3f}\", ha='center', va='bottom', fontsize=9, alpha=0.6)\n",
    "\n",
    "ax.set_xticks(x_dec)\n",
    "ax.set_xticklabels(decomp_labels, fontsize=9)\n",
    "ax.axhline(y=0, color='gray', linestyle='--')\n",
    "ax.set_ylabel(\"Cohen's d vs Bare\")\n",
    "ax.set_title(\"Mechanism: Value Contamination Decomposition\")\n",
    "ax.legend(fontsize=8)\n",
    "\n",
    "# --- Plot 4: NLL distributions (all conditions) ---\n",
    "ax = axes[1, 1]\n",
    "for cn in CONDITION_NAMES:\n",
    "    ax.hist(c[cn], bins=50, alpha=0.4, label=cn, color=colors.get(cn, 'gray'))\n",
    "ax.set_xlabel(\"NLL\")\n",
    "ax.set_ylabel(\"Count\")\n",
    "ax.set_title(\"NLL Distributions (all conditions)\")\n",
    "ax.legend(fontsize=7)\n",
    "\n",
    "plt.suptitle('Exp 16: Cross-Model Priming \u2014 Gemma 3 4B vs Mistral 7B', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'analysis_plots.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"Plots saved to {RESULTS_DIR / 'analysis_plots.png'}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 9: Save results JSON\n",
    "final = {\n",
    "    'experiment': 'exp16_cross_model_gemma3',\n",
    "    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'config': {\n",
    "        'model_name': MODEL_NAME,\n",
    "        'model_type': 'gemma3',\n",
    "        'seed': SEED,\n",
    "        'n_queries': N,\n",
    "        'n_valid': N_VALID,\n",
    "        'n_passages_valid': n_passages_valid,\n",
    "        'n_passages_excluded': n_excluded,\n",
    "        'max_passage_words': MAX_PASSAGE_WORDS,\n",
    "        'min_passages_per_query': MIN_PASSAGES_PER_QUERY,\n",
    "        'dataset': 'MS MARCO v1.1 validation',\n",
    "        'prefixes': {\n",
    "            'static_fact': STATIC_FACT,\n",
    "            'random': RANDOM_PREFIX_TEXT,\n",
    "            'oracle': '(actual query per sample)',\n",
    "        },\n",
    "    },\n",
    "    'gemma_architecture': {\n",
    "        'hidden_size': text_config.hidden_size,\n",
    "        'num_layers': text_config.num_hidden_layers,\n",
    "        'num_attention_heads': text_config.num_attention_heads,\n",
    "        'num_kv_heads': text_config.num_key_value_heads,\n",
    "        'head_dim': _get_head_dim(model.config),\n",
    "        'rope_thetas': sorted(list(thetas)),\n",
    "    },\n",
    "    'condition_names': CONDITION_NAMES,\n",
    "    'nll_summary': {\n",
    "        cn: {\n",
    "            'mean': float(np.mean(c[cn])),\n",
    "            'std': float(np.std(c[cn])),\n",
    "            'cohens_d_vs_bare': float(cohens_d(c['bare'] - c[cn])) if cn != 'bare' else 0.0,\n",
    "        }\n",
    "        for cn in CONDITION_NAMES\n",
    "    },\n",
    "    'statistical_tests': stat_results,\n",
    "    'mechanism_decomposition': {\n",
    "        'd_full_sf': float(d_sf),\n",
    "        'd_values_only': float(d_vo),\n",
    "        'values_fraction': float(values_fraction) if not np.isnan(values_fraction) else None,\n",
    "    },\n",
    "    'cross_model_comparison': {\n",
    "        'mistral_reference': MISTRAL_REF,\n",
    "        'gemma_ds': {cn: float(gemma_ds[cn]) for cn in gemma_ds},\n",
    "    },\n",
    "    'hardness_breakdown': hardness_breakdown,\n",
    "    'per_query_results': all_results,\n",
    "}\n",
    "\n",
    "with open(FINAL_RESULTS_PATH, 'w') as f:\n",
    "    json.dump(final, f, indent=2)\n",
    "\n",
    "print(f\"Results saved to {FINAL_RESULTS_PATH}\")\n",
    "print(f\"File size: {FINAL_RESULTS_PATH.stat().st_size / 1024:.1f} KB\")\n",
    "\n",
    "# Print summary\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"SUMMARY \u2014 Exp 16: Cross-Model Priming Replication\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Model: Gemma 3 4B (34 layers, head_dim=256, bfloat16)\")\n",
    "print(f\"Dataset: MS MARCO v1.1 ({N} queries, {n_passages_valid} passages)\")\n",
    "print(f\"\\nEffect sizes (Cohen's d vs bare):\")\n",
    "for cn in ['static_fact_trunc', 'random_trunc', 'oracle_trunc', 'values_only']:\n",
    "    g_d = gemma_ds.get(cn, 0)\n",
    "    sig = stat_results.get(cn, {}).get('significant', False)\n",
    "    sig_str = \"(sig)\" if sig else \"(ns)\"\n",
    "    print(f\"  {cn:<25} d={g_d:>+.3f}  {sig_str}\")\n",
    "print(f\"\\nMechanism: values carry {values_fraction:.0%} of the effect\")\n",
    "print(\"\\nDone!\")"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}