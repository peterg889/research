{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "550139a7",
   "metadata": {},
   "source": [
    "# Exp 09: Values Deep Dive\n",
    "\n",
    "## Motivation\n",
    "\n",
    "Exp 08 showed value vectors carry 100% of the priming effect (d=+0.275) while keys\n",
    "contribute nothing (d=-0.009). This experiment investigates the values-only mechanism\n",
    "across 5 directions:\n",
    "\n",
    "1. **Layer-wise isolation** — Which layers carry the signal?\n",
    "2. **Prefix type variation** — Does the prefix content matter in values-only mode?\n",
    "3. **Interpolation** — How does blending bare/primed values at different ratios affect quality?\n",
    "4. **Cross-document transfer** — Do primed values generalize across documents?\n",
    "5. **Positional isolation** — Which token positions carry the signal?\n",
    "\n",
    "## 17 Experimental Conditions\n",
    "\n",
    "| # | Name | Category | Construction |\n",
    "|---|------|----------|-------------|\n",
    "| 1 | `bare` | Baseline | `[BOS][doc]` |\n",
    "| 2 | `full_llm_kw` | Baseline | LLM-kw truncated (keys+values) |\n",
    "| 3 | `values_only_llm_kw` | Baseline | Values from LLM-kw, keys from bare |\n",
    "| 4 | `values_layers_0_7` | Layer-wise | Primed values in layers 0-7 only |\n",
    "| 5 | `values_layers_8_15` | Layer-wise | Primed values in layers 8-15 only |\n",
    "| 6 | `values_layers_16_23` | Layer-wise | Primed values in layers 16-23 only |\n",
    "| 7 | `values_layers_24_31` | Layer-wise | Primed values in layers 24-31 only |\n",
    "| 8 | `values_only_static_fact` | Prefix type | Values from \"What are the key facts?\" prefix |\n",
    "| 9 | `values_only_oracle` | Prefix type | Values from actual query prefix |\n",
    "| 10 | `values_only_random` | Prefix type | Values from random token prefix |\n",
    "| 11 | `values_interp_025` | Interpolation | 25% primed + 75% bare values |\n",
    "| 12 | `values_interp_050` | Interpolation | 50/50 blend |\n",
    "| 13 | `values_interp_075` | Interpolation | 75% primed + 25% bare values |\n",
    "| 14 | `values_cross_doc` | Cross-doc | Primed values from previous sample's document |\n",
    "| 15 | `values_first_quarter` | Positional | Primed values in first 25% of doc positions |\n",
    "| 16 | `values_last_quarter` | Positional | Primed values in last 25% of doc positions |\n",
    "| 17 | `values_middle_half` | Positional | Primed values in middle 50% of doc positions |\n",
    "\n",
    "## 10 Primary Comparisons (Bonferroni alpha = 0.005)\n",
    "\n",
    "| # | Comparison | Question |\n",
    "|---|-----------|----------|\n",
    "| C1 | values_layers_0_7 vs bare | Early layers carry signal? |\n",
    "| C2 | values_layers_8_15 vs bare | Early-mid layers? |\n",
    "| C3 | values_layers_16_23 vs bare | Mid-late layers? |\n",
    "| C4 | values_layers_24_31 vs bare | Late layers? |\n",
    "| C5 | values_only_static_fact vs values_only_llm_kw | Static advantage in values? |\n",
    "| C6 | values_only_random vs bare | Random-primed values help? |\n",
    "| C7 | values_interp_050 vs bare | 50% blend helps? |\n",
    "| C8 | values_cross_doc vs bare | Wrong-doc values help? |\n",
    "| C9 | values_cross_doc vs values_only_llm_kw | Same-doc vs cross-doc? |\n",
    "| C10 | values_first_quarter vs values_last_quarter | Beginning vs end positions? |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f9cfdc9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-10T13:26:55.425199Z",
     "iopub.status.busy": "2026-02-10T13:26:55.424909Z",
     "iopub.status.idle": "2026-02-10T13:26:58.453401Z",
     "shell.execute_reply": "2026-02-10T13:26:58.452372Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEED: 42\n",
      "Results directory: results/exp09\n",
      "CUDA available: True\n",
      "GPU: NVIDIA L4\n",
      "GPU memory: 23.6 GB\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Setup — permissions, seeds, results directory\n",
    "import os\n",
    "os.umask(0o000)\n",
    "\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "RESULTS_DIR = Path(\"results/exp09\")\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "SURROGATES_DIR = RESULTS_DIR / \"surrogates\"\n",
    "SURROGATES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CHECKPOINT_PATH = RESULTS_DIR / \"checkpoint.json\"\n",
    "FINAL_RESULTS_PATH = RESULTS_DIR / \"results.json\"\n",
    "\n",
    "print(f\"SEED: {SEED}\")\n",
    "print(f\"Results directory: {RESULTS_DIR}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a9637c3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-10T13:26:58.457807Z",
     "iopub.status.busy": "2026-02-10T13:26:58.456911Z",
     "iopub.status.idle": "2026-02-10T13:28:01.092990Z",
     "shell.execute_reply": "2026-02-10T13:28:01.092073Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading mistralai/Mistral-7B-Instruct-v0.2 (4-bit)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "632cb2e7e28c480ab50cf0768cd54299",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/291 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded. dtype=torch.float16, device=cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Load model (Mistral-7B 4-bit)\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "print(f\"Loading {MODEL_NAME} (4-bit)...\")\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "print(f\"Model loaded. dtype={model.dtype}, device={model.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8115838",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-10T13:28:01.098247Z",
     "iopub.status.busy": "2026-02-10T13:28:01.097376Z",
     "iopub.status.idle": "2026-02-10T13:28:01.714061Z",
     "shell.execute_reply": "2026-02-10T13:28:01.713248Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config ready\n",
      "  num_samples pool: 2000\n",
      "  eval samples: 1000\n",
      "  bonferroni_alpha: 0.0050 (10 comparisons)\n",
      "  conditions: 17\n",
      "  static_factual_phrase: 'What are the key facts I need to know?'\n",
      "  layer_groups: ['layers_0_7', 'layers_8_15', 'layers_16_23', 'layers_24_31']\n",
      "  interp_alphas: [0.25, 0.5, 0.75]\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Imports + config + constants\n",
    "sys.path.insert(0, \".\")\n",
    "\n",
    "from lib.config import ExperimentConfig\n",
    "from lib.kv_cache import (\n",
    "    build_hybrid_cache,\n",
    "    replace_values_at_layers,\n",
    "    interpolate_values,\n",
    "    replace_values_at_positions,\n",
    "    build_cross_doc_cache,\n",
    "    deepcopy_cache,\n",
    "    extract_and_truncate_cache_with_bos,\n",
    "    correct_rope_positions_with_bos,\n",
    "    score_answer_with_cache,\n",
    "    _get_cache_keys,\n",
    "    _get_cache_values,\n",
    "    _set_cache_values,\n",
    "    _ensure_dynamic_cache,\n",
    ")\n",
    "from lib.data import load_ms_marco, load_evaluation_samples\n",
    "from lib.analysis import cohens_d\n",
    "from lib.surrogate import generate_all_5_surrogates, STATIC_SURROGATE_QUERIES\n",
    "from scipy import stats\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "config = ExperimentConfig(\n",
    "    model_name=MODEL_NAME,\n",
    "    num_samples=2000,\n",
    "    min_passage_words=20,\n",
    "    max_passage_words=500,\n",
    "    seed=SEED,\n",
    ")\n",
    "\n",
    "# Templates — bare text, no \"Document:\\n\" framing\n",
    "SURROGATE_PREFIX_TEMPLATE = \"{surrogate}\\n\"\n",
    "DOCUMENT_TEMPLATE = \"{document}\"\n",
    "QUERY_TEMPLATE = \"\\nQuery: {query}\\nAnswer:\"\n",
    "ANSWER_TEMPLATE = \" {answer}\"\n",
    "\n",
    "N_EVAL = 1000\n",
    "N_COMPARISONS = 10\n",
    "BONFERRONI_ALPHA = 0.05 / N_COMPARISONS\n",
    "CHECKPOINT_EVERY = 50\n",
    "\n",
    "STATIC_FACTUAL_PHRASE = STATIC_SURROGATE_QUERIES['static_factual']['query']\n",
    "\n",
    "LAYER_GROUPS = {\n",
    "    'layers_0_7': list(range(0, 8)),\n",
    "    'layers_8_15': list(range(8, 16)),\n",
    "    'layers_16_23': list(range(16, 24)),\n",
    "    'layers_24_31': list(range(24, 32)),\n",
    "}\n",
    "\n",
    "INTERP_ALPHAS = [0.25, 0.50, 0.75]\n",
    "\n",
    "CONDITION_NAMES = [\n",
    "    'bare', 'full_llm_kw', 'values_only_llm_kw',\n",
    "    'values_layers_0_7', 'values_layers_8_15', 'values_layers_16_23', 'values_layers_24_31',\n",
    "    'values_only_static_fact', 'values_only_oracle', 'values_only_random',\n",
    "    'values_interp_025', 'values_interp_050', 'values_interp_075',\n",
    "    'values_cross_doc',\n",
    "    'values_first_quarter', 'values_last_quarter', 'values_middle_half',\n",
    "]\n",
    "\n",
    "print(\"Config ready\")\n",
    "print(f\"  num_samples pool: {config.num_samples}\")\n",
    "print(f\"  eval samples: {N_EVAL}\")\n",
    "print(f\"  bonferroni_alpha: {BONFERRONI_ALPHA:.4f} ({N_COMPARISONS} comparisons)\")\n",
    "print(f\"  conditions: {len(CONDITION_NAMES)}\")\n",
    "print(f\"  static_factual_phrase: '{STATIC_FACTUAL_PHRASE}'\")\n",
    "print(f\"  layer_groups: {list(LAYER_GROUPS.keys())}\")\n",
    "print(f\"  interp_alphas: {INTERP_ALPHAS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24896a69",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-10T13:28:01.717905Z",
     "iopub.status.busy": "2026-02-10T13:28:01.717064Z",
     "iopub.status.idle": "2026-02-10T13:28:03.013402Z",
     "shell.execute_reply": "2026-02-10T13:28:03.012249Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`trust_remote_code` is not supported anymore.\n",
      "Please check that the Hugging Face dataset 'microsoft/ms_marco' isn't based on a loading script and remove `trust_remote_code`.\n",
      "If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading microsoft/ms_marco dataset...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded: 10047 samples\n",
      "Filtering samples...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18a3db0cc9984b929c070fcd26ec56df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filtering:   0%|          | 0/10047 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 2000 samples\n",
      "Loaded 2000 candidates, using first 1000 for evaluation\n",
      "Example passage (92 words): The word totem derives from the Algonquian (most likely Ojibwe) word odoodem [ oˈtuːtɛm ], his kinsh...\n",
      "Example query: what do the carvings on a totem pole mean\n",
      "Example answer: Represent characters or events in a story.\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Load MS MARCO (1000 samples)\n",
    "dataset = load_ms_marco(config)\n",
    "\n",
    "np.random.seed(SEED)\n",
    "all_samples = load_evaluation_samples(dataset, config, require_answer=True)\n",
    "\n",
    "samples = all_samples[:N_EVAL]\n",
    "N = len(samples)\n",
    "print(f\"Loaded {len(all_samples)} candidates, using first {N} for evaluation\")\n",
    "print(f\"Example passage ({len(samples[0]['passage'].split())} words): {samples[0]['passage'][:100]}...\")\n",
    "print(f\"Example query: {samples[0]['query']}\")\n",
    "print(f\"Example answer: {samples[0]['answer']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5bc0d768",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-10T13:28:03.017002Z",
     "iopub.status.busy": "2026-02-10T13:28:03.016706Z",
     "iopub.status.idle": "2026-02-10T16:59:53.448459Z",
     "shell.execute_reply": "2026-02-10T16:59:53.447533Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PHASE 1: LLM SURROGATE GENERATION (keyword only)\n",
      "======================================================================\n",
      "Generating keyword surrogates for samples 0 to 999...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2d03cadb46f4ae78d6582e5d5bd64ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Keyword surrogates:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saved 100/1000 | 0.07 s/s | ETA: 202.9 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saved 200/1000 | 0.08 s/s | ETA: 176.6 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saved 300/1000 | 0.08 s/s | ETA: 152.1 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saved 400/1000 | 0.08 s/s | ETA: 129.7 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saved 500/1000 | 0.08 s/s | ETA: 107.4 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saved 600/1000 | 0.08 s/s | ETA: 85.3 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saved 700/1000 | 0.08 s/s | ETA: 63.9 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saved 800/1000 | 0.08 s/s | ETA: 42.5 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saved 900/1000 | 0.08 s/s | ETA: 21.2 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saved 1000/1000 | 0.08 s/s | ETA: 0.0 min\n",
      "Keyword surrogates complete: 1000 samples\n",
      "Empty surrogates: 0/1000\n",
      "Example: 'totem, algonquian, odoodem, carvings, animals, characters, story, family legends, coastal Pacific Northwest, native culture, european explorers, history, decorative car'\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Generate LLM keyword surrogates (fresh, independent from Exp 08)\n",
    "print(\"=\" * 70)\n",
    "print(\"PHASE 1: LLM SURROGATE GENERATION (keyword only)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "surrogates_path = SURROGATES_DIR / \"keyword_surrogates.json\"\n",
    "\n",
    "if surrogates_path.exists():\n",
    "    with open(surrogates_path, 'r') as f:\n",
    "        surrogates_data = json.load(f)\n",
    "    keyword_surrogates = surrogates_data['surrogates']\n",
    "    print(f\"Loaded {len(keyword_surrogates)} keyword surrogates from cache\")\n",
    "else:\n",
    "    keyword_surrogates = []\n",
    "\n",
    "start_idx_gen = len(keyword_surrogates)\n",
    "if start_idx_gen < N:\n",
    "    print(f\"Generating keyword surrogates for samples {start_idx_gen} to {N-1}...\")\n",
    "    t_start = time.time()\n",
    "    for idx in tqdm(range(start_idx_gen, N), initial=start_idx_gen, total=N,\n",
    "                     desc=\"Keyword surrogates\"):\n",
    "        passage = samples[idx]['passage']\n",
    "        try:\n",
    "            s5 = generate_all_5_surrogates(passage, model, tokenizer, config)\n",
    "            kw = s5.get('keyword_query', '')\n",
    "        except Exception as e:\n",
    "            print(f\"  WARNING: Generation failed for sample {idx}: {e}\")\n",
    "            kw = \"\"\n",
    "        keyword_surrogates.append(kw)\n",
    "\n",
    "        if (idx + 1) % 100 == 0 or idx == N - 1:\n",
    "            with open(surrogates_path, 'w') as f:\n",
    "                json.dump({'surrogates': keyword_surrogates}, f)\n",
    "            elapsed = time.time() - t_start\n",
    "            rate = (idx - start_idx_gen + 1) / elapsed if elapsed > 0 else 0\n",
    "            remaining = (N - idx - 1) / rate if rate > 0 else 0\n",
    "            tqdm.write(f\"  Saved {idx+1}/{N} | {rate:.2f} s/s | ETA: {remaining/60:.1f} min\")\n",
    "\n",
    "    with open(surrogates_path, 'w') as f:\n",
    "        json.dump({'surrogates': keyword_surrogates}, f)\n",
    "    print(f\"Keyword surrogates complete: {len(keyword_surrogates)} samples\")\n",
    "else:\n",
    "    print(f\"All keyword surrogates already cached ({len(keyword_surrogates)} samples)\")\n",
    "\n",
    "n_empty = sum(1 for s in keyword_surrogates if not s.strip())\n",
    "print(f\"Empty surrogates: {n_empty}/{N}\")\n",
    "print(f\"Example: '{keyword_surrogates[0]}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "886b6b76",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-10T16:59:53.452283Z",
     "iopub.status.busy": "2026-02-10T16:59:53.451991Z",
     "iopub.status.idle": "2026-02-10T16:59:53.459993Z",
     "shell.execute_reply": "2026-02-10T16:59:53.459154Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "EXPERIMENTAL CONDITIONS EXPLAINED\n",
      "======================================================================\n",
      "\n",
      "### 1. bare ###\n",
      "  Cache: [BOS][doc]\n",
      "  Detail: No prefix — baseline\n",
      "\n",
      "### 2. full_llm_kw ###\n",
      "  Cache: [BOS][llm_kw\\n][doc] → truncate + RoPE\n",
      "  Detail: Standard truncated prefix (keys+values): 'totem, algonquian, odoodem, carvings, animals, characters, story, family legends, coastal Pacific Northwest, native culture, european explorers, history, decorative car'\n",
      "\n",
      "### 3. values_only_llm_kw ###\n",
      "  Cache: Keys from bare, values from full_llm_kw cache\n",
      "  Detail: Values-only baseline (replicates Exp 08 finding)\n",
      "\n",
      "### 4-7. values_layers_X_Y ###\n",
      "  Cache: Keys from bare, values from primed cache at specified layers only\n",
      "  Detail: Layer-wise isolation: which layer groups carry the signal?\n",
      "\n",
      "### 8. values_only_static_fact ###\n",
      "  Cache: Prefix: 'What are the key facts I need to know?' → truncate → values-only\n",
      "  Detail: Static factual prefix — same for every document\n",
      "\n",
      "### 9. values_only_oracle ###\n",
      "  Cache: Prefix: 'what do the carvings on a totem pole mean' → truncate → values-only\n",
      "  Detail: Oracle (actual query) prefix — best possible semantic prefix\n",
      "\n",
      "### 10. values_only_random ###\n",
      "  Cache: Prefix: random tokens → truncate → values-only\n",
      "  Detail: Random token prefix — structural control for values\n",
      "\n",
      "### 11-13. values_interp_XXX ###\n",
      "  Cache: v = alpha * v_primed + (1-alpha) * v_bare\n",
      "  Detail: Value interpolation at alpha = 0.25, 0.50, 0.75\n",
      "\n",
      "### 14. values_cross_doc ###\n",
      "  Cache: Values from PREVIOUS sample's primed cache\n",
      "  Detail: Cross-document transfer — wrong-doc values\n",
      "\n",
      "### 15-17. values_first/last/middle ###\n",
      "  Cache: Primed values at specific token positions only\n",
      "  Detail: Positional isolation: first 25%, last 25%, middle 50%\n",
      "\n",
      "======================================================================\n",
      "5 FORWARD PASSES PER SAMPLE:\n",
      "  1. Bare: [BOS][doc] → bare_cache\n",
      "  2. LLM-kw: [BOS][kw\\n][doc] → truncate+RoPE → primed_llm_cache\n",
      "  3. Static fact: [BOS][static_fact\\n][doc] → truncate+RoPE\n",
      "  4. Oracle: [BOS][query\\n][doc] → truncate+RoPE\n",
      "  5. Random: [BOS][random_tokens\\n][doc] → truncate+RoPE\n",
      "  Cross-doc reuses previous sample's primed_llm_cache — no extra pass\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Condition explanation with concrete examples\n",
    "print(\"=\" * 70)\n",
    "print(\"EXPERIMENTAL CONDITIONS EXPLAINED\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "ex_kw = keyword_surrogates[0]\n",
    "ex_query = samples[0]['query']\n",
    "\n",
    "conditions_explained = [\n",
    "    (\"1. bare\",\n",
    "     \"[BOS][doc]\",\n",
    "     \"No prefix — baseline\"),\n",
    "    (\"2. full_llm_kw\",\n",
    "     \"[BOS][llm_kw\\\\n][doc] → truncate + RoPE\",\n",
    "     f\"Standard truncated prefix (keys+values): '{ex_kw}'\"),\n",
    "    (\"3. values_only_llm_kw\",\n",
    "     \"Keys from bare, values from full_llm_kw cache\",\n",
    "     \"Values-only baseline (replicates Exp 08 finding)\"),\n",
    "    (\"4-7. values_layers_X_Y\",\n",
    "     \"Keys from bare, values from primed cache at specified layers only\",\n",
    "     \"Layer-wise isolation: which layer groups carry the signal?\"),\n",
    "    (\"8. values_only_static_fact\",\n",
    "     f\"Prefix: '{STATIC_FACTUAL_PHRASE}' → truncate → values-only\",\n",
    "     \"Static factual prefix — same for every document\"),\n",
    "    (\"9. values_only_oracle\",\n",
    "     f\"Prefix: '{ex_query}' → truncate → values-only\",\n",
    "     \"Oracle (actual query) prefix — best possible semantic prefix\"),\n",
    "    (\"10. values_only_random\",\n",
    "     \"Prefix: random tokens → truncate → values-only\",\n",
    "     \"Random token prefix — structural control for values\"),\n",
    "    (\"11-13. values_interp_XXX\",\n",
    "     \"v = alpha * v_primed + (1-alpha) * v_bare\",\n",
    "     \"Value interpolation at alpha = 0.25, 0.50, 0.75\"),\n",
    "    (\"14. values_cross_doc\",\n",
    "     \"Values from PREVIOUS sample's primed cache\",\n",
    "     \"Cross-document transfer — wrong-doc values\"),\n",
    "    (\"15-17. values_first/last/middle\",\n",
    "     \"Primed values at specific token positions only\",\n",
    "     \"Positional isolation: first 25%, last 25%, middle 50%\"),\n",
    "]\n",
    "\n",
    "for name, pattern, detail in conditions_explained:\n",
    "    print(f\"\\n### {name} ###\")\n",
    "    print(f\"  Cache: {pattern}\")\n",
    "    print(f\"  Detail: {detail}\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"5 FORWARD PASSES PER SAMPLE:\")\n",
    "print(\"  1. Bare: [BOS][doc] → bare_cache\")\n",
    "print(\"  2. LLM-kw: [BOS][kw\\\\n][doc] → truncate+RoPE → primed_llm_cache\")\n",
    "print(\"  3. Static fact: [BOS][static_fact\\\\n][doc] → truncate+RoPE\")\n",
    "print(\"  4. Oracle: [BOS][query\\\\n][doc] → truncate+RoPE\")\n",
    "print(\"  5. Random: [BOS][random_tokens\\\\n][doc] → truncate+RoPE\")\n",
    "print(\"  Cross-doc reuses previous sample's primed_llm_cache — no extra pass\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54e52c93",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-10T16:59:53.463255Z",
     "iopub.status.busy": "2026-02-10T16:59:53.462595Z",
     "iopub.status.idle": "2026-02-10T16:59:53.469361Z",
     "shell.execute_reply": "2026-02-10T16:59:53.468572Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper defined: build_primed_and_truncated()\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Convenience wrapper for building primed+truncated caches\n",
    "\n",
    "def build_primed_and_truncated(prefix_text, bos_id, doc_ids, doc_len, model, tokenizer, config):\n",
    "    \"\"\"Build a primed cache: tokenize prefix, concat [BOS][prefix][doc], forward, truncate+RoPE.\n",
    "\n",
    "    Args:\n",
    "        prefix_text: Raw prefix text (will be formatted with SURROGATE_PREFIX_TEMPLATE)\n",
    "        bos_id: BOS token id tensor (1, 1)\n",
    "        doc_ids: Document token ids (1, doc_len)\n",
    "        doc_len: Number of document tokens\n",
    "        model: The language model\n",
    "        tokenizer: The tokenizer\n",
    "        config: ExperimentConfig\n",
    "\n",
    "    Returns:\n",
    "        (trunc_cache, prefix_token_len) where prefix_token_len includes BOS\n",
    "    \"\"\"\n",
    "    prefix_str = SURROGATE_PREFIX_TEMPLATE.format(surrogate=prefix_text)\n",
    "    prefix_enc = tokenizer(prefix_str, return_tensors=\"pt\",\n",
    "                           add_special_tokens=False, padding=False, truncation=False)\n",
    "    prefix_ids = prefix_enc['input_ids'].to(config.device)\n",
    "    prefix_token_len = 1 + prefix_ids.shape[1]  # BOS + prefix tokens\n",
    "\n",
    "    full_ids = torch.cat([bos_id, prefix_ids, doc_ids], dim=1)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = model(input_ids=full_ids,\n",
    "                    attention_mask=torch.ones_like(full_ids),\n",
    "                    use_cache=True, return_dict=True)\n",
    "\n",
    "    trunc_cache = extract_and_truncate_cache_with_bos(out.past_key_values, doc_len)\n",
    "    correct_rope_positions_with_bos(trunc_cache, prefix_token_len - 1, model)\n",
    "\n",
    "    del out\n",
    "    return trunc_cache, prefix_token_len\n",
    "\n",
    "print(\"Helper defined: build_primed_and_truncated()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c6494b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-10T16:59:53.472808Z",
     "iopub.status.busy": "2026-02-10T16:59:53.472514Z",
     "iopub.status.idle": "2026-02-10T18:30:20.848165Z",
     "shell.execute_reply": "2026-02-10T18:30:20.847263Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PHASE 2: MAIN EVALUATION (17 conditions × 1000 samples)\n",
      "======================================================================\n",
      "No checkpoint found. Starting fresh.\n",
      "Evaluating samples 0 to 999\n",
      "Conditions: 17\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74faddda45834e09b9723363761610c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 50/1000 | 0.19 s/s | ETA: 85.4 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 100/1000 | 0.18 s/s | ETA: 81.4 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 150/1000 | 0.18 s/s | ETA: 76.9 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 200/1000 | 0.18 s/s | ETA: 72.4 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 250/1000 | 0.18 s/s | ETA: 68.0 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 300/1000 | 0.18 s/s | ETA: 63.4 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 350/1000 | 0.18 s/s | ETA: 58.9 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 400/1000 | 0.18 s/s | ETA: 54.4 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 450/1000 | 0.18 s/s | ETA: 49.8 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 500/1000 | 0.18 s/s | ETA: 45.3 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 550/1000 | 0.18 s/s | ETA: 40.8 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 600/1000 | 0.18 s/s | ETA: 36.2 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 650/1000 | 0.18 s/s | ETA: 31.7 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 700/1000 | 0.18 s/s | ETA: 27.2 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 750/1000 | 0.18 s/s | ETA: 22.7 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 800/1000 | 0.18 s/s | ETA: 18.1 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 850/1000 | 0.18 s/s | ETA: 13.6 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 900/1000 | 0.18 s/s | ETA: 9.0 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 950/1000 | 0.18 s/s | ETA: 4.5 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 1000/1000 | 0.18 s/s | ETA: 0.0 min\n",
      "\n",
      "Evaluation complete: 1000 samples in 90.5 min\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Main eval loop — 17 conditions × 1000 samples\n",
    "print(\"=\" * 70)\n",
    "print(\"PHASE 2: MAIN EVALUATION (17 conditions × 1000 samples)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "results = []\n",
    "start_idx = 0\n",
    "prev_primed_cache = None\n",
    "prev_doc_len = None\n",
    "\n",
    "if CHECKPOINT_PATH.exists():\n",
    "    with open(CHECKPOINT_PATH, 'r') as f:\n",
    "        ckpt = json.load(f)\n",
    "    ckpt_queries = ckpt.get('sample_queries', [])\n",
    "    current_queries = [s['query'] for s in samples]\n",
    "    if ckpt_queries == current_queries:\n",
    "        results = ckpt['results']\n",
    "        start_idx = len(results)\n",
    "        print(f\"Resuming from checkpoint: {start_idx}/{N}\")\n",
    "\n",
    "        # Rebuild prev_primed_cache from sample start_idx - 1\n",
    "        if start_idx > 0:\n",
    "            print(f\"Rebuilding prev_primed_cache from sample {start_idx - 1}...\")\n",
    "            prev_sample = samples[start_idx - 1]\n",
    "            prev_passage = prev_sample['passage']\n",
    "            prev_query = prev_sample['query']\n",
    "            prev_kw = keyword_surrogates[start_idx - 1]\n",
    "\n",
    "            oracle_prefix = SURROGATE_PREFIX_TEMPLATE.format(surrogate=prev_query)\n",
    "            document_text = DOCUMENT_TEMPLATE.format(document=prev_passage)\n",
    "            full_text = oracle_prefix + document_text\n",
    "\n",
    "            full_enc = tokenizer(full_text, return_tensors=\"pt\",\n",
    "                                 add_special_tokens=True, padding=False, truncation=False)\n",
    "            full_ids_prev = full_enc['input_ids'].to(config.device)\n",
    "\n",
    "            oracle_prefix_enc = tokenizer(oracle_prefix, return_tensors=\"pt\",\n",
    "                                          add_special_tokens=True, padding=False, truncation=False)\n",
    "            prev_prefix_len = oracle_prefix_enc['input_ids'].shape[1]\n",
    "\n",
    "            prev_bos_id = full_ids_prev[:, :1]\n",
    "            prev_doc_ids = full_ids_prev[:, prev_prefix_len:]\n",
    "            prev_doc_len_rebuild = prev_doc_ids.shape[1]\n",
    "\n",
    "            prev_primed_cache, _ = build_primed_and_truncated(\n",
    "                prev_kw, prev_bos_id, prev_doc_ids, prev_doc_len_rebuild,\n",
    "                model, tokenizer, config)\n",
    "            prev_doc_len = prev_doc_len_rebuild\n",
    "            print(f\"Rebuilt prev_primed_cache (doc_len={prev_doc_len})\")\n",
    "            del full_ids_prev\n",
    "    else:\n",
    "        print(\"Checkpoint sample mismatch. Starting fresh.\")\n",
    "else:\n",
    "    print(\"No checkpoint found. Starting fresh.\")\n",
    "\n",
    "print(f\"Evaluating samples {start_idx} to {N-1}\")\n",
    "print(f\"Conditions: {len(CONDITION_NAMES)}\")\n",
    "\n",
    "t_start = time.time()\n",
    "\n",
    "for idx in tqdm(range(start_idx, N), initial=start_idx, total=N, desc=\"Evaluating\"):\n",
    "    sample = samples[idx]\n",
    "    passage = sample['passage']\n",
    "    query = sample['query']\n",
    "    answer = sample['answer']\n",
    "\n",
    "    query_prompt = QUERY_TEMPLATE.format(query=query)\n",
    "    answer_text = ANSWER_TEMPLATE.format(answer=answer)\n",
    "    llm_kw_text = keyword_surrogates[idx]\n",
    "\n",
    "    # --- Matched tokenization ---\n",
    "    oracle_prefix = SURROGATE_PREFIX_TEMPLATE.format(surrogate=query)\n",
    "    document_text = DOCUMENT_TEMPLATE.format(document=passage)\n",
    "    full_oracle_text = oracle_prefix + document_text\n",
    "\n",
    "    full_oracle_enc = tokenizer(full_oracle_text, return_tensors=\"pt\",\n",
    "                                add_special_tokens=True, padding=False, truncation=False)\n",
    "    full_oracle_ids = full_oracle_enc['input_ids'].to(config.device)\n",
    "\n",
    "    oracle_prefix_enc = tokenizer(oracle_prefix, return_tensors=\"pt\",\n",
    "                                  add_special_tokens=True, padding=False, truncation=False)\n",
    "    oracle_prefix_len = oracle_prefix_enc['input_ids'].shape[1]\n",
    "\n",
    "    bos_id = full_oracle_ids[:, :1]\n",
    "    doc_ids = full_oracle_ids[:, oracle_prefix_len:]\n",
    "    doc_len = doc_ids.shape[1]\n",
    "\n",
    "    # === Forward pass 1: BARE ===\n",
    "    bare_ids = torch.cat([bos_id, doc_ids], dim=1)\n",
    "    with torch.no_grad():\n",
    "        bare_out = model(input_ids=bare_ids, attention_mask=torch.ones_like(bare_ids),\n",
    "                         use_cache=True, return_dict=True)\n",
    "    bare_cache = bare_out.past_key_values\n",
    "    del bare_out\n",
    "\n",
    "    nll_bare = score_answer_with_cache(\n",
    "        deepcopy_cache(bare_cache), bare_ids.shape[1],\n",
    "        query_prompt, answer_text, model, tokenizer, config)\n",
    "\n",
    "    # === Forward pass 2: LLM-KW (primed) ===\n",
    "    primed_llm_cache, llm_prefix_token_len = build_primed_and_truncated(\n",
    "        llm_kw_text, bos_id, doc_ids, doc_len, model, tokenizer, config)\n",
    "\n",
    "    nll_full_llm_kw = score_answer_with_cache(\n",
    "        deepcopy_cache(primed_llm_cache), 1 + doc_len,\n",
    "        query_prompt, answer_text, model, tokenizer, config)\n",
    "\n",
    "    # === Condition 3: VALUES-ONLY LLM-KW ===\n",
    "    hybrid_vals = build_hybrid_cache(bare_cache, primed_llm_cache)\n",
    "    nll_values_only_llm_kw = score_answer_with_cache(\n",
    "        deepcopy_cache(hybrid_vals), 1 + doc_len,\n",
    "        query_prompt, answer_text, model, tokenizer, config)\n",
    "    del hybrid_vals\n",
    "\n",
    "    # === Conditions 4-7: LAYER-WISE ===\n",
    "    nll_layers = {}\n",
    "    for group_name, layer_indices in LAYER_GROUPS.items():\n",
    "        layer_cache = replace_values_at_layers(bare_cache, primed_llm_cache, layer_indices)\n",
    "        nll_layers[f'values_{group_name}'] = score_answer_with_cache(\n",
    "            deepcopy_cache(layer_cache), 1 + doc_len,\n",
    "            query_prompt, answer_text, model, tokenizer, config)\n",
    "        del layer_cache\n",
    "\n",
    "    # === Conditions 11-13: INTERPOLATION ===\n",
    "    nll_interp = {}\n",
    "    for alpha in INTERP_ALPHAS:\n",
    "        interp_cache = interpolate_values(bare_cache, primed_llm_cache, alpha)\n",
    "        key_name = f'values_interp_{int(alpha*100):03d}'\n",
    "        nll_interp[key_name] = score_answer_with_cache(\n",
    "            deepcopy_cache(interp_cache), 1 + doc_len,\n",
    "            query_prompt, answer_text, model, tokenizer, config)\n",
    "        del interp_cache\n",
    "\n",
    "    # === Conditions 15-17: POSITIONAL ===\n",
    "    # Positions are within the cache: BOS=0, doc starts at 1\n",
    "    first_q_end = 1 + max(1, doc_len // 4)\n",
    "    last_q_start = 1 + doc_len - max(1, doc_len // 4)\n",
    "    mid_start = 1 + max(1, doc_len // 4)\n",
    "    mid_end = 1 + doc_len - max(1, doc_len // 4)\n",
    "\n",
    "    pos_first = replace_values_at_positions(bare_cache, primed_llm_cache, 1, first_q_end)\n",
    "    nll_first_quarter = score_answer_with_cache(\n",
    "        deepcopy_cache(pos_first), 1 + doc_len,\n",
    "        query_prompt, answer_text, model, tokenizer, config)\n",
    "    del pos_first\n",
    "\n",
    "    pos_last = replace_values_at_positions(bare_cache, primed_llm_cache, last_q_start, 1 + doc_len)\n",
    "    nll_last_quarter = score_answer_with_cache(\n",
    "        deepcopy_cache(pos_last), 1 + doc_len,\n",
    "        query_prompt, answer_text, model, tokenizer, config)\n",
    "    del pos_last\n",
    "\n",
    "    pos_mid = replace_values_at_positions(bare_cache, primed_llm_cache, mid_start, mid_end)\n",
    "    nll_middle_half = score_answer_with_cache(\n",
    "        deepcopy_cache(pos_mid), 1 + doc_len,\n",
    "        query_prompt, answer_text, model, tokenizer, config)\n",
    "    del pos_mid\n",
    "\n",
    "    # === Condition 14: CROSS-DOC ===\n",
    "    if prev_primed_cache is not None and prev_doc_len is not None:\n",
    "        cross_cache = build_cross_doc_cache(bare_cache, prev_primed_cache, doc_len)\n",
    "        nll_cross_doc = score_answer_with_cache(\n",
    "            deepcopy_cache(cross_cache), 1 + doc_len,\n",
    "            query_prompt, answer_text, model, tokenizer, config)\n",
    "        del cross_cache\n",
    "    else:\n",
    "        nll_cross_doc = 0.0  # First sample — no previous cache\n",
    "\n",
    "    # Save prev_primed_cache for next sample\n",
    "    prev_primed_cache = deepcopy_cache(primed_llm_cache)\n",
    "    prev_doc_len = doc_len\n",
    "\n",
    "    # === Forward pass 3: STATIC FACT ===\n",
    "    static_cache, _ = build_primed_and_truncated(\n",
    "        STATIC_FACTUAL_PHRASE, bos_id, doc_ids, doc_len, model, tokenizer, config)\n",
    "    hybrid_static = build_hybrid_cache(bare_cache, static_cache)\n",
    "    nll_static_fact = score_answer_with_cache(\n",
    "        deepcopy_cache(hybrid_static), 1 + doc_len,\n",
    "        query_prompt, answer_text, model, tokenizer, config)\n",
    "    del static_cache, hybrid_static\n",
    "\n",
    "    # === Forward pass 4: ORACLE ===\n",
    "    oracle_cache, _ = build_primed_and_truncated(\n",
    "        query, bos_id, doc_ids, doc_len, model, tokenizer, config)\n",
    "    hybrid_oracle = build_hybrid_cache(bare_cache, oracle_cache)\n",
    "    nll_oracle = score_answer_with_cache(\n",
    "        deepcopy_cache(hybrid_oracle), 1 + doc_len,\n",
    "        query_prompt, answer_text, model, tokenizer, config)\n",
    "    del oracle_cache, hybrid_oracle\n",
    "\n",
    "    # === Forward pass 5: RANDOM ===\n",
    "    # Generate random token prefix of similar length to keyword surrogate\n",
    "    n_random_tokens = max(5, len(tokenizer.encode(llm_kw_text, add_special_tokens=False)))\n",
    "    random_ids = torch.randint(100, tokenizer.vocab_size - 100,\n",
    "                               (n_random_tokens,), device='cpu')\n",
    "    random_text = tokenizer.decode(random_ids, skip_special_tokens=True)\n",
    "    random_cache, _ = build_primed_and_truncated(\n",
    "        random_text, bos_id, doc_ids, doc_len, model, tokenizer, config)\n",
    "    hybrid_random = build_hybrid_cache(bare_cache, random_cache)\n",
    "    nll_random = score_answer_with_cache(\n",
    "        deepcopy_cache(hybrid_random), 1 + doc_len,\n",
    "        query_prompt, answer_text, model, tokenizer, config)\n",
    "    del random_cache, hybrid_random\n",
    "\n",
    "    # --- Cleanup ---\n",
    "    del bare_cache, primed_llm_cache\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # --- Store result ---\n",
    "    result = {\n",
    "        'idx': idx,\n",
    "        'doc_len': doc_len,\n",
    "        'prev_doc_len': int(prev_doc_len) if prev_doc_len is not None else None,\n",
    "        'passage_word_count': len(passage.split()),\n",
    "        'bare': nll_bare,\n",
    "        'full_llm_kw': nll_full_llm_kw,\n",
    "        'values_only_llm_kw': nll_values_only_llm_kw,\n",
    "        **nll_layers,\n",
    "        'values_only_static_fact': nll_static_fact,\n",
    "        'values_only_oracle': nll_oracle,\n",
    "        'values_only_random': nll_random,\n",
    "        **nll_interp,\n",
    "        'values_cross_doc': nll_cross_doc,\n",
    "        'values_first_quarter': nll_first_quarter,\n",
    "        'values_last_quarter': nll_last_quarter,\n",
    "        'values_middle_half': nll_middle_half,\n",
    "    }\n",
    "    results.append(result)\n",
    "\n",
    "    if (idx + 1) % CHECKPOINT_EVERY == 0 or idx == N - 1:\n",
    "        ckpt_data = {\n",
    "            'results': results,\n",
    "            'sample_queries': [s['query'] for s in samples],\n",
    "            'completed': len(results),\n",
    "            'total': N,\n",
    "            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        }\n",
    "        with open(CHECKPOINT_PATH, 'w') as f:\n",
    "            json.dump(ckpt_data, f)\n",
    "        elapsed = time.time() - t_start\n",
    "        rate = (idx - start_idx + 1) / elapsed if elapsed > 0 else 0\n",
    "        remaining = (N - idx - 1) / rate if rate > 0 else 0\n",
    "        tqdm.write(f\"  Checkpoint {idx+1}/{N} | {rate:.2f} s/s | ETA: {remaining/60:.1f} min\")\n",
    "\n",
    "elapsed_total = time.time() - t_start\n",
    "print(f\"\\nEvaluation complete: {len(results)} samples in {elapsed_total/60:.1f} min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe72841c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-10T18:30:20.852268Z",
     "iopub.status.busy": "2026-02-10T18:30:20.851823Z",
     "iopub.status.idle": "2026-02-10T18:30:21.204717Z",
     "shell.execute_reply": "2026-02-10T18:30:21.203844Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ANALYSIS — VALUES DEEP DIVE\n",
      "======================================================================\n",
      "Total: 1000, Valid: 929, Excluded (cross-doc sample 0): 71\n",
      "\n",
      "Condition                        Mean NLL        Std  d vs Bare\n",
      "-----------------------------------------------------------------\n",
      "bare                               1.1629     1.7280          —\n",
      "full_llm_kw                        1.0761     1.6449     +0.253\n",
      "values_only_llm_kw                 1.0862     1.6347     +0.275\n",
      "values_layers_0_7                  1.1368     1.7007     +0.233\n",
      "values_layers_8_15                 1.1060     1.6566     +0.267\n",
      "values_layers_16_23                1.1641     1.7323     -0.017\n",
      "values_layers_24_31                1.1659     1.7312     -0.068\n",
      "values_only_static_fact            1.0151     1.5552     +0.466\n",
      "values_only_oracle                 1.0817     1.6376     +0.309\n",
      "values_only_random                 1.0841     1.6518     +0.310\n",
      "values_interp_025                  1.1452     1.7127     +0.236\n",
      "values_interp_050                  1.1265     1.6929     +0.244\n",
      "values_interp_075                  1.1059     1.6661     +0.262\n",
      "values_cross_doc                   3.1959     2.3115     -1.070\n",
      "values_first_quarter               1.0961     1.6397     +0.333\n",
      "values_last_quarter                1.1608     1.7340     +0.030\n",
      "values_middle_half                 1.1564     1.7200     +0.044\n",
      "\n",
      "==========================================================================================\n",
      "10 PRIMARY COMPARISONS (Bonferroni alpha = 0.0050)\n",
      "==========================================================================================\n",
      "\n",
      "Comparison                            Mean Δ        d    Win%        t            p   Sig\n",
      "------------------------------------------------------------------------------------------\n",
      "C1: layers_0_7 vs bare                0.0261    0.233   66.1%     7.11    2.30e-12   ***\n",
      "C2: layers_8_15 vs bare               0.0569    0.267   67.1%     8.13    1.41e-15   ***\n",
      "C3: layers_16_23 vs bare             -0.0012   -0.017   48.4%    -0.53    5.95e-01    ns\n",
      "C4: layers_24_31 vs bare             -0.0029   -0.068   45.3%    -2.06    3.97e-02     *\n",
      "C5: static_fact vs llm_kw             0.0711    0.252   63.2%     7.68    3.94e-14   ***\n",
      "C6: random vs bare                    0.0788    0.310   66.7%     9.44    2.98e-20   ***\n",
      "C7: interp_050 vs bare                0.0365    0.244   68.2%     7.45    2.20e-13   ***\n",
      "C8: cross_doc vs bare                -2.0330   -1.070    6.8%   -32.61   4.95e-156   ***\n",
      "C9: cross_doc vs llm_kw              -2.1097   -1.103    5.3%   -33.61   1.14e-162   ***\n",
      "C10: first_q vs last_q                0.0647    0.323   66.4%     9.83    8.93e-22   ***\n",
      "\n",
      "==========================================================================================\n",
      "ALL CONDITIONS vs BARE\n",
      "==========================================================================================\n",
      "\n",
      "Condition                       d vs Bare    Win%            p\n",
      "-----------------------------------------------------------------\n",
      "full_llm_kw                         0.253   69.2%    3.11e-14   ***\n",
      "values_only_llm_kw                  0.275   67.8%    2.13e-16   ***\n",
      "values_layers_0_7                   0.233   66.1%    2.30e-12   ***\n",
      "values_layers_8_15                  0.267   67.1%    1.41e-15   ***\n",
      "values_layers_16_23                -0.017   48.4%    5.95e-01    ns\n",
      "values_layers_24_31                -0.068   45.3%    3.97e-02     *\n",
      "values_only_static_fact             0.466   81.5%    1.22e-41   ***\n",
      "values_only_oracle                  0.309   66.8%    3.57e-20   ***\n",
      "values_only_random                  0.310   66.7%    2.98e-20   ***\n",
      "values_interp_025                   0.236   67.8%    1.43e-12   ***\n",
      "values_interp_050                   0.244   68.2%    2.20e-13   ***\n",
      "values_interp_075                   0.262   68.5%    3.87e-15   ***\n",
      "values_cross_doc                   -1.070    6.8%   4.95e-156   ***\n",
      "values_first_quarter                0.333   69.9%    4.69e-23   ***\n",
      "values_last_quarter                 0.030   53.8%    3.63e-01    ns\n",
      "values_middle_half                  0.044   55.8%    1.85e-01    ns\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Primary analysis — NLL summary + 10 comparisons + all vs bare\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ANALYSIS — VALUES DEEP DIVE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Extract arrays and filter zero NLLs (cross-doc sample 0)\n",
    "cond_arrays = {}\n",
    "for cname in CONDITION_NAMES:\n",
    "    cond_arrays[cname] = np.array([r[cname] for r in results])\n",
    "\n",
    "valid = np.ones(len(results), dtype=bool)\n",
    "for cname in CONDITION_NAMES:\n",
    "    valid &= (cond_arrays[cname] != 0)\n",
    "n_valid = int(np.sum(valid))\n",
    "n_excluded = int(np.sum(~valid))\n",
    "print(f\"Total: {len(results)}, Valid: {n_valid}, Excluded (cross-doc sample 0): {n_excluded}\")\n",
    "\n",
    "c = {}\n",
    "for cname in CONDITION_NAMES:\n",
    "    c[cname] = cond_arrays[cname][valid]\n",
    "\n",
    "# NLL summary table\n",
    "print(f\"\\n{'Condition':<30} {'Mean NLL':>10} {'Std':>10} {'d vs Bare':>10}\")\n",
    "print(\"-\" * 65)\n",
    "for cname in CONDITION_NAMES:\n",
    "    mean_nll = np.mean(c[cname])\n",
    "    std_nll = np.std(c[cname])\n",
    "    if cname == 'bare':\n",
    "        d_str = \"—\"\n",
    "    else:\n",
    "        d = cohens_d(c['bare'] - c[cname])\n",
    "        d_str = f\"{d:+.3f}\"\n",
    "    print(f\"{cname:<30} {mean_nll:>10.4f} {std_nll:>10.4f} {d_str:>10}\")\n",
    "\n",
    "# 10 primary comparisons\n",
    "print(f\"\\n{'='*90}\")\n",
    "print(f\"10 PRIMARY COMPARISONS (Bonferroni alpha = {BONFERRONI_ALPHA:.4f})\")\n",
    "print(f\"{'='*90}\")\n",
    "\n",
    "comparisons = [\n",
    "    ('C1: layers_0_7 vs bare',\n",
    "     c['bare'] - c['values_layers_0_7'],\n",
    "     'Early layers carry signal?'),\n",
    "    ('C2: layers_8_15 vs bare',\n",
    "     c['bare'] - c['values_layers_8_15'],\n",
    "     'Early-mid layers?'),\n",
    "    ('C3: layers_16_23 vs bare',\n",
    "     c['bare'] - c['values_layers_16_23'],\n",
    "     'Mid-late layers?'),\n",
    "    ('C4: layers_24_31 vs bare',\n",
    "     c['bare'] - c['values_layers_24_31'],\n",
    "     'Late layers?'),\n",
    "    ('C5: static_fact vs llm_kw',\n",
    "     c['values_only_llm_kw'] - c['values_only_static_fact'],\n",
    "     'Static advantage in values?'),\n",
    "    ('C6: random vs bare',\n",
    "     c['bare'] - c['values_only_random'],\n",
    "     'Random-primed values help?'),\n",
    "    ('C7: interp_050 vs bare',\n",
    "     c['bare'] - c['values_interp_050'],\n",
    "     '50% blend helps?'),\n",
    "    ('C8: cross_doc vs bare',\n",
    "     c['bare'] - c['values_cross_doc'],\n",
    "     'Wrong-doc values help?'),\n",
    "    ('C9: cross_doc vs llm_kw',\n",
    "     c['values_only_llm_kw'] - c['values_cross_doc'],\n",
    "     'Same-doc vs cross-doc?'),\n",
    "    ('C10: first_q vs last_q',\n",
    "     c['values_last_quarter'] - c['values_first_quarter'],\n",
    "     'Beginning vs end positions?'),\n",
    "]\n",
    "\n",
    "print(f\"\\n{'Comparison':<35} {'Mean Δ':>8} {'d':>8} {'Win%':>7} {'t':>8} {'p':>12} {'Sig':>5}\")\n",
    "print(\"-\" * 90)\n",
    "\n",
    "comparison_results = {}\n",
    "for name, delta, question in comparisons:\n",
    "    d = cohens_d(delta)\n",
    "    win = np.mean(delta > 0) * 100\n",
    "    t_stat, p_val = stats.ttest_1samp(delta, 0)\n",
    "    sig = \"***\" if p_val < 0.001 else \"**\" if p_val < BONFERRONI_ALPHA else \"*\" if p_val < 0.05 else \"ns\"\n",
    "    print(f\"{name:<35} {np.mean(delta):>8.4f} {d:>8.3f} {win:>6.1f}% {t_stat:>8.2f} {p_val:>11.2e} {sig:>5}\")\n",
    "    comparison_results[name] = {\n",
    "        'mean_delta': float(np.mean(delta)),\n",
    "        'cohens_d': float(d),\n",
    "        'win_rate': float(win / 100),\n",
    "        't_stat': float(t_stat),\n",
    "        'p_value': float(p_val),\n",
    "        'bonferroni_significant': bool(p_val < BONFERRONI_ALPHA),\n",
    "        'question': question,\n",
    "    }\n",
    "\n",
    "# All vs Bare\n",
    "print(f\"\\n{'='*90}\")\n",
    "print(\"ALL CONDITIONS vs BARE\")\n",
    "print(f\"{'='*90}\")\n",
    "print(f\"\\n{'Condition':<30} {'d vs Bare':>10} {'Win%':>7} {'p':>12}\")\n",
    "print(\"-\" * 65)\n",
    "all_vs_bare = {}\n",
    "for cname in CONDITION_NAMES:\n",
    "    if cname == 'bare':\n",
    "        continue\n",
    "    delta = c['bare'] - c[cname]\n",
    "    d = cohens_d(delta)\n",
    "    win = np.mean(delta > 0) * 100\n",
    "    _, p_val = stats.ttest_1samp(delta, 0)\n",
    "    sig = \"***\" if p_val < 0.001 else \"**\" if p_val < BONFERRONI_ALPHA else \"*\" if p_val < 0.05 else \"ns\"\n",
    "    print(f\"{cname:<30} {d:>10.3f} {win:>6.1f}% {p_val:>11.2e} {sig:>5}\")\n",
    "    all_vs_bare[cname] = {'cohens_d': float(d), 'win_rate': float(win/100), 'p_value': float(p_val)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d0f28b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-10T18:30:21.208799Z",
     "iopub.status.busy": "2026-02-10T18:30:21.208155Z",
     "iopub.status.idle": "2026-02-10T18:30:21.250940Z",
     "shell.execute_reply": "2026-02-10T18:30:21.250074Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "DIRECTION 1: LAYER-WISE DECOMPOSITION\n",
      "======================================================================\n",
      "  layers_0_7: d = +0.233\n",
      "  layers_8_15: d = +0.267\n",
      "  layers_16_23: d = -0.017\n",
      "  layers_24_31: d = -0.068\n",
      "\n",
      "  Sum of 4 groups: d = +0.415\n",
      "  All layers (values_only_llm_kw): d = +0.275\n",
      "  Additivity ratio: 1.51\n",
      "  Ranked: layers_8_15(+0.267) > layers_0_7(+0.233) > layers_16_23(-0.017) > layers_24_31(-0.068)\n",
      "\n",
      "======================================================================\n",
      "DIRECTION 2: PREFIX TYPE COMPARISON (VALUES-ONLY MODE)\n",
      "======================================================================\n",
      "  LLM keyword               d = +0.275\n",
      "  Static factual            d = +0.466\n",
      "  Oracle (actual query)     d = +0.309\n",
      "  Random tokens             d = +0.310\n",
      "\n",
      "======================================================================\n",
      "DIRECTION 3: INTERPOLATION CURVE\n",
      "======================================================================\n",
      "  alpha=0.00: d = +0.000\n",
      "  alpha=0.25: d = +0.236\n",
      "  alpha=0.50: d = +0.244\n",
      "  alpha=0.75: d = +0.262\n",
      "  alpha=1.00: d = +0.275\n",
      "\n",
      "  Linear fit: d = 0.053 * alpha + 0.221, R² = 0.9613\n",
      "\n",
      "======================================================================\n",
      "DIRECTION 4: CROSS-DOCUMENT TRANSFER\n",
      "======================================================================\n",
      "  Cross-doc vs bare: d = -1.070\n",
      "  Same-doc vs bare:  d = +0.275\n",
      "  Cross-doc retains -390% of same-doc effect\n",
      "  Length-matched pairs (929): d = -1.070\n",
      "\n",
      "======================================================================\n",
      "DIRECTION 5: POSITIONAL ISOLATION\n",
      "======================================================================\n",
      "  First 25%       d = +0.333 (d per fraction = +1.333)\n",
      "  Middle 50%      d = +0.044 (d per fraction = +0.087)\n",
      "  Last 25%        d = +0.030 (d per fraction = +0.119)\n",
      "  All positions: d = +0.275\n",
      "\n",
      "======================================================================\n",
      "HARDNESS QUINTILE BREAKDOWN (key conditions)\n",
      "======================================================================\n",
      "\n",
      "Condition                          Q1 (easy)            Q2            Q3            Q4     Q5 (hard)       Overall\n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "values_only_llm_kw                    -0.026        +0.172        +0.327        +0.618        +0.376        +0.275\n",
      "values_layers_0_7                     +0.114        +0.230        +0.399        +0.420        +0.307        +0.233\n",
      "values_layers_24_31                   +0.027        -0.084        -0.227        -0.060        -0.078        -0.068\n",
      "values_only_oracle                    -0.165        +0.237        +0.335        +0.519        +0.483        +0.309\n",
      "values_only_random                    +0.208        +0.297        +0.427        +0.582        +0.404        +0.310\n",
      "values_interp_050                     +0.149        +0.294        +0.331        +0.603        +0.265        +0.244\n",
      "values_cross_doc                      -0.964        -1.470        -1.421        -1.268        -0.623        -1.070\n",
      "values_first_quarter                  -0.011        +0.411        +0.422        +0.613        +0.493        +0.333\n",
      "values_last_quarter                   +0.141        +0.191        +0.073        +0.139        -0.055        +0.030\n"
     ]
    }
   ],
   "source": [
    "# Cell 10: Direction-specific deep-dive\n",
    "\n",
    "# --- LAYERS ---\n",
    "print(\"=\" * 70)\n",
    "print(\"DIRECTION 1: LAYER-WISE DECOMPOSITION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "layer_ds = {}\n",
    "for group_name in LAYER_GROUPS:\n",
    "    cname = f'values_{group_name}'\n",
    "    d = cohens_d(c['bare'] - c[cname])\n",
    "    layer_ds[group_name] = d\n",
    "    print(f\"  {group_name}: d = {d:+.3f}\")\n",
    "\n",
    "d_all_values = cohens_d(c['bare'] - c['values_only_llm_kw'])\n",
    "d_sum = sum(layer_ds.values())\n",
    "print(f\"\\n  Sum of 4 groups: d = {d_sum:+.3f}\")\n",
    "print(f\"  All layers (values_only_llm_kw): d = {d_all_values:+.3f}\")\n",
    "print(f\"  Additivity ratio: {d_sum/d_all_values:.2f}\" if d_all_values != 0 else \"  All-layers d = 0\")\n",
    "\n",
    "ranked = sorted(layer_ds.items(), key=lambda x: x[1], reverse=True)\n",
    "print(f\"  Ranked: {' > '.join(f'{n}({d:+.3f})' for n, d in ranked)}\")\n",
    "\n",
    "# --- PREFIX TYPES ---\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"DIRECTION 2: PREFIX TYPE COMPARISON (VALUES-ONLY MODE)\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "prefix_conds = {\n",
    "    'values_only_llm_kw': 'LLM keyword',\n",
    "    'values_only_static_fact': 'Static factual',\n",
    "    'values_only_oracle': 'Oracle (actual query)',\n",
    "    'values_only_random': 'Random tokens',\n",
    "}\n",
    "for cname, label in prefix_conds.items():\n",
    "    d = cohens_d(c['bare'] - c[cname])\n",
    "    print(f\"  {label:<25} d = {d:+.3f}\")\n",
    "\n",
    "# --- INTERPOLATION ---\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"DIRECTION 3: INTERPOLATION CURVE\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "alphas = [0.0] + INTERP_ALPHAS + [1.0]\n",
    "interp_ds = []\n",
    "for alpha in alphas:\n",
    "    if alpha == 0.0:\n",
    "        d = 0.0  # bare vs bare\n",
    "    elif alpha == 1.0:\n",
    "        d = cohens_d(c['bare'] - c['values_only_llm_kw'])\n",
    "    else:\n",
    "        cname = f'values_interp_{int(alpha*100):03d}'\n",
    "        d = cohens_d(c['bare'] - c[cname])\n",
    "    interp_ds.append(d)\n",
    "    print(f\"  alpha={alpha:.2f}: d = {d:+.3f}\")\n",
    "\n",
    "# Fit linear to 0.25, 0.50, 0.75\n",
    "from numpy.polynomial import polynomial as P\n",
    "xs = np.array(INTERP_ALPHAS)\n",
    "ys = np.array([interp_ds[1], interp_ds[2], interp_ds[3]])\n",
    "slope, intercept = np.polyfit(xs, ys, 1)\n",
    "y_pred_lin = slope * xs + intercept\n",
    "ss_res = np.sum((ys - y_pred_lin) ** 2)\n",
    "ss_tot = np.sum((ys - np.mean(ys)) ** 2)\n",
    "r2_lin = 1 - ss_res / ss_tot if ss_tot > 0 else 0\n",
    "print(f\"\\n  Linear fit: d = {slope:.3f} * alpha + {intercept:.3f}, R² = {r2_lin:.4f}\")\n",
    "\n",
    "# --- CROSS-DOC ---\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"DIRECTION 4: CROSS-DOCUMENT TRANSFER\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "d_cross = cohens_d(c['bare'] - c['values_cross_doc'])\n",
    "d_same = cohens_d(c['bare'] - c['values_only_llm_kw'])\n",
    "print(f\"  Cross-doc vs bare: d = {d_cross:+.3f}\")\n",
    "print(f\"  Same-doc vs bare:  d = {d_same:+.3f}\")\n",
    "print(f\"  Cross-doc retains {d_cross/d_same*100:.0f}% of same-doc effect\" if d_same != 0 else \"\")\n",
    "\n",
    "# Length-match robustness: filter pairs where doc lengths are within 20%\n",
    "doc_lens = np.array([r['doc_len'] for r in results])[valid]\n",
    "prev_doc_lens = np.array([r.get('prev_doc_len', 0) or 0 for r in results])[valid]\n",
    "length_ratio = np.where(prev_doc_lens > 0, doc_lens / prev_doc_lens, 0)\n",
    "length_matched = (length_ratio > 0.8) & (length_ratio < 1.2)\n",
    "n_matched = int(np.sum(length_matched))\n",
    "if n_matched > 30:\n",
    "    d_cross_matched = cohens_d(c['bare'][length_matched] - c['values_cross_doc'][length_matched])\n",
    "    print(f\"  Length-matched pairs ({n_matched}): d = {d_cross_matched:+.3f}\")\n",
    "\n",
    "# --- POSITIONAL ---\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"DIRECTION 5: POSITIONAL ISOLATION\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "pos_conds = {\n",
    "    'values_first_quarter': ('First 25%', 0.25),\n",
    "    'values_middle_half': ('Middle 50%', 0.50),\n",
    "    'values_last_quarter': ('Last 25%', 0.25),\n",
    "}\n",
    "for cname, (label, frac) in pos_conds.items():\n",
    "    d = cohens_d(c['bare'] - c[cname])\n",
    "    d_per_frac = d / frac if frac > 0 else 0\n",
    "    print(f\"  {label:<15} d = {d:+.3f} (d per fraction = {d_per_frac:+.3f})\")\n",
    "\n",
    "d_all = cohens_d(c['bare'] - c['values_only_llm_kw'])\n",
    "print(f\"  All positions: d = {d_all:+.3f}\")\n",
    "\n",
    "# --- HARDNESS QUINTILE BREAKDOWN ---\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"HARDNESS QUINTILE BREAKDOWN (key conditions)\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "bare_valid = c['bare']\n",
    "quintile_boundaries = np.percentile(bare_valid, [20, 40, 60, 80])\n",
    "quintile_labels = ['Q1 (easy)', 'Q2', 'Q3', 'Q4', 'Q5 (hard)']\n",
    "\n",
    "def get_quintile(nll, boundaries):\n",
    "    for i, b in enumerate(boundaries):\n",
    "        if nll <= b:\n",
    "            return i\n",
    "    return len(boundaries)\n",
    "\n",
    "quintiles = np.array([get_quintile(nll, quintile_boundaries) for nll in bare_valid])\n",
    "\n",
    "key_conds = ['values_only_llm_kw', 'values_layers_0_7', 'values_layers_24_31',\n",
    "             'values_only_oracle', 'values_only_random', 'values_interp_050',\n",
    "             'values_cross_doc', 'values_first_quarter', 'values_last_quarter']\n",
    "\n",
    "header = f\"{'Condition':<30}\" + \"\".join(f\"{ql:>14}\" for ql in quintile_labels) + f\"{'Overall':>14}\"\n",
    "print(f\"\\n{header}\")\n",
    "print(\"-\" * (30 + 14 * 6))\n",
    "\n",
    "hardness_breakdown = {}\n",
    "for cname in key_conds:\n",
    "    row = f\"{cname:<30}\"\n",
    "    quintile_ds = []\n",
    "    for q in range(5):\n",
    "        mask_q = quintiles == q\n",
    "        if np.sum(mask_q) < 10:\n",
    "            row += f\"{'n/a':>14}\"\n",
    "            quintile_ds.append(None)\n",
    "        else:\n",
    "            delta = bare_valid[mask_q] - c[cname][mask_q]\n",
    "            d = cohens_d(delta)\n",
    "            row += f\"{d:>+14.3f}\"\n",
    "            quintile_ds.append(float(d))\n",
    "    d_all = cohens_d(bare_valid - c[cname])\n",
    "    row += f\"{d_all:>+14.3f}\"\n",
    "    print(row)\n",
    "    hardness_breakdown[cname] = {'quintile_ds': quintile_ds, 'overall_d': float(d_all)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61ddd8d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-10T18:30:21.254398Z",
     "iopub.status.busy": "2026-02-10T18:30:21.254118Z",
     "iopub.status.idle": "2026-02-10T18:30:23.626181Z",
     "shell.execute_reply": "2026-02-10T18:30:23.625243Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plots saved to results/exp09/analysis_plots.png\n"
     ]
    }
   ],
   "source": [
    "# Cell 11: Plots (2x3 grid)\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "# Color-coding by direction\n",
    "direction_colors = {\n",
    "    'full_llm_kw': 'forestgreen', 'values_only_llm_kw': 'forestgreen',\n",
    "    'values_layers_0_7': '#1f77b4', 'values_layers_8_15': '#2ca02c',\n",
    "    'values_layers_16_23': '#ff7f0e', 'values_layers_24_31': '#d62728',\n",
    "    'values_only_static_fact': 'goldenrod', 'values_only_oracle': 'gold',\n",
    "    'values_only_random': 'khaki',\n",
    "    'values_interp_025': 'mediumpurple', 'values_interp_050': 'darkorchid',\n",
    "    'values_interp_075': 'purple',\n",
    "    'values_cross_doc': 'coral',\n",
    "    'values_first_quarter': 'lightblue', 'values_last_quarter': 'steelblue',\n",
    "    'values_middle_half': 'cornflowerblue',\n",
    "}\n",
    "\n",
    "# --- Plot 1: All conditions bar chart ---\n",
    "ax = axes[0, 0]\n",
    "cnames_sorted = sorted(\n",
    "    [cn for cn in CONDITION_NAMES if cn != 'bare'],\n",
    "    key=lambda cn: cohens_d(c['bare'] - c[cn]),\n",
    "    reverse=True\n",
    ")\n",
    "ds_bar = [cohens_d(c['bare'] - c[cn]) for cn in cnames_sorted]\n",
    "colors_bar = [direction_colors.get(cn, 'lightgray') for cn in cnames_sorted]\n",
    "ax.barh(range(len(cnames_sorted)), ds_bar, color=colors_bar, edgecolor='black', linewidth=0.5)\n",
    "ax.set_yticks(range(len(cnames_sorted)))\n",
    "ax.set_yticklabels(cnames_sorted, fontsize=7)\n",
    "ax.axvline(x=0, color='gray', linestyle='--')\n",
    "ax.set_xlabel(\"Cohen's d vs Bare\")\n",
    "ax.set_title('All Conditions vs Bare')\n",
    "ax.invert_yaxis()\n",
    "\n",
    "# --- Plot 2: Layer-wise decomposition ---\n",
    "ax = axes[0, 1]\n",
    "layer_names = list(LAYER_GROUPS.keys())\n",
    "layer_d_vals = [cohens_d(c['bare'] - c[f'values_{gn}']) for gn in layer_names]\n",
    "layer_colors = ['#1f77b4', '#2ca02c', '#ff7f0e', '#d62728']\n",
    "bars = ax.bar(range(len(layer_names)), layer_d_vals, color=layer_colors,\n",
    "              edgecolor='black', linewidth=0.5)\n",
    "# Add all-layers reference\n",
    "ax.axhline(y=d_all_values, color='forestgreen', linestyle='--', label=f'All layers (d={d_all_values:+.3f})')\n",
    "ax.set_xticks(range(len(layer_names)))\n",
    "ax.set_xticklabels([n.replace('layers_', 'L') for n in layer_names], fontsize=9)\n",
    "ax.axhline(y=0, color='gray', linestyle='-', alpha=0.3)\n",
    "ax.set_ylabel(\"Cohen's d vs Bare\")\n",
    "ax.set_title('Layer-wise Decomposition')\n",
    "ax.legend(fontsize=8)\n",
    "for i, v in enumerate(layer_d_vals):\n",
    "    ax.text(i, v + 0.005, f\"{v:+.3f}\", ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "# --- Plot 3: Prefix type comparison ---\n",
    "ax = axes[0, 2]\n",
    "prefix_names = list(prefix_conds.keys())\n",
    "prefix_labels = list(prefix_conds.values())\n",
    "prefix_ds = [cohens_d(c['bare'] - c[cn]) for cn in prefix_names]\n",
    "prefix_bar_colors = ['forestgreen', 'goldenrod', 'gold', 'khaki']\n",
    "bars = ax.bar(range(len(prefix_names)), prefix_ds, color=prefix_bar_colors,\n",
    "              edgecolor='black', linewidth=0.5)\n",
    "ax.set_xticks(range(len(prefix_names)))\n",
    "ax.set_xticklabels(prefix_labels, fontsize=8, rotation=15)\n",
    "ax.axhline(y=0, color='gray', linestyle='--')\n",
    "ax.set_ylabel(\"Cohen's d vs Bare\")\n",
    "ax.set_title('Prefix Type (Values-Only)')\n",
    "for i, v in enumerate(prefix_ds):\n",
    "    ax.text(i, v + 0.005, f\"{v:+.3f}\", ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "# --- Plot 4: Interpolation curve ---\n",
    "ax = axes[1, 0]\n",
    "ax.plot(alphas, interp_ds, 'o-', color='purple', linewidth=2, markersize=8)\n",
    "# Linear fit overlay\n",
    "xs_fit = np.linspace(0, 1, 50)\n",
    "ys_fit = slope * xs_fit + intercept\n",
    "ax.plot(xs_fit, ys_fit, '--', color='gray', alpha=0.5, label=f'Linear fit (R²={r2_lin:.3f})')\n",
    "ax.set_xlabel('Alpha (primed fraction)')\n",
    "ax.set_ylabel(\"Cohen's d vs Bare\")\n",
    "ax.set_title('Value Interpolation Curve')\n",
    "ax.set_xticks(alphas)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.legend(fontsize=8)\n",
    "for x, y in zip(alphas, interp_ds):\n",
    "    ax.annotate(f\"d={y:+.3f}\", (x, y), textcoords=\"offset points\",\n",
    "                xytext=(0, 10), ha='center', fontsize=8)\n",
    "\n",
    "# --- Plot 5: Positional breakdown ---\n",
    "ax = axes[1, 1]\n",
    "pos_names = ['first_quarter', 'middle_half', 'last_quarter']\n",
    "pos_labels = ['First 25%', 'Middle 50%', 'Last 25%']\n",
    "pos_ds = [cohens_d(c['bare'] - c[f'values_{pn}']) for pn in pos_names]\n",
    "pos_colors = ['lightblue', 'cornflowerblue', 'steelblue']\n",
    "bars = ax.bar(range(len(pos_names)), pos_ds, color=pos_colors,\n",
    "              edgecolor='black', linewidth=0.5)\n",
    "# All-positions reference\n",
    "ax.axhline(y=d_all_values, color='forestgreen', linestyle='--',\n",
    "           label=f'All positions (d={d_all_values:+.3f})')\n",
    "ax.set_xticks(range(len(pos_names)))\n",
    "ax.set_xticklabels(pos_labels, fontsize=9)\n",
    "ax.axhline(y=0, color='gray', linestyle='-', alpha=0.3)\n",
    "ax.set_ylabel(\"Cohen's d vs Bare\")\n",
    "ax.set_title('Positional Breakdown')\n",
    "ax.legend(fontsize=8)\n",
    "for i, v in enumerate(pos_ds):\n",
    "    ax.text(i, v + 0.005, f\"{v:+.3f}\", ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "# --- Plot 6: Hardness × condition heatmap ---\n",
    "ax = axes[1, 2]\n",
    "hm_data = []\n",
    "for cname in key_conds:\n",
    "    row = []\n",
    "    for q in range(5):\n",
    "        mask_q = quintiles == q\n",
    "        if np.sum(mask_q) < 10:\n",
    "            row.append(0)\n",
    "        else:\n",
    "            delta = bare_valid[mask_q] - c[cname][mask_q]\n",
    "            row.append(cohens_d(delta))\n",
    "    hm_data.append(row)\n",
    "hm_data = np.array(hm_data)\n",
    "im = ax.imshow(hm_data, cmap='RdBu_r', vmin=-0.5, vmax=0.7, aspect='auto')\n",
    "ax.set_xticks(range(5))\n",
    "ax.set_xticklabels(quintile_labels, fontsize=7)\n",
    "ax.set_yticks(range(len(key_conds)))\n",
    "ax.set_yticklabels([cn.replace('values_', '') for cn in key_conds], fontsize=7)\n",
    "for i in range(len(key_conds)):\n",
    "    for j in range(5):\n",
    "        ax.text(j, i, f\"{hm_data[i,j]:+.2f}\", ha='center', va='center', fontsize=6)\n",
    "plt.colorbar(im, ax=ax, label=\"Cohen's d vs bare\")\n",
    "ax.set_title('Hardness × Condition')\n",
    "\n",
    "plt.suptitle('Exp 09: Values Deep Dive', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'analysis_plots.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"Plots saved to {RESULTS_DIR / 'analysis_plots.png'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "53321581",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-10T18:30:23.629634Z",
     "iopub.status.busy": "2026-02-10T18:30:23.629338Z",
     "iopub.status.idle": "2026-02-10T18:30:23.688754Z",
     "shell.execute_reply": "2026-02-10T18:30:23.687810Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to results/exp09/results.json\n",
      "File size: 847.0 KB\n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Cell 12: Save comprehensive results JSON\n",
    "\n",
    "# Direction-specific summaries\n",
    "layer_summary = {}\n",
    "for group_name in LAYER_GROUPS:\n",
    "    cname = f'values_{group_name}'\n",
    "    layer_summary[group_name] = {\n",
    "        'cohens_d': float(cohens_d(c['bare'] - c[cname])),\n",
    "        'layers': LAYER_GROUPS[group_name],\n",
    "    }\n",
    "\n",
    "prefix_summary = {}\n",
    "for cname, label in prefix_conds.items():\n",
    "    prefix_summary[label] = {\n",
    "        'cohens_d': float(cohens_d(c['bare'] - c[cname])),\n",
    "    }\n",
    "\n",
    "interp_summary = {}\n",
    "for alpha, d_val in zip(alphas, interp_ds):\n",
    "    interp_summary[f'alpha_{alpha:.2f}'] = float(d_val)\n",
    "interp_summary['linear_slope'] = float(slope)\n",
    "interp_summary['linear_intercept'] = float(intercept)\n",
    "interp_summary['linear_r2'] = float(r2_lin)\n",
    "\n",
    "cross_doc_summary = {\n",
    "    'd_cross_doc_vs_bare': float(d_cross),\n",
    "    'd_same_doc_vs_bare': float(d_same),\n",
    "    'retention_fraction': float(d_cross / d_same) if d_same != 0 else 0,\n",
    "}\n",
    "\n",
    "positional_summary = {}\n",
    "for cname, (label, frac) in pos_conds.items():\n",
    "    d = cohens_d(c['bare'] - c[cname])\n",
    "    positional_summary[label] = {\n",
    "        'cohens_d': float(d),\n",
    "        'fraction': frac,\n",
    "        'd_per_fraction': float(d / frac) if frac > 0 else 0,\n",
    "    }\n",
    "\n",
    "final = {\n",
    "    'experiment': 'exp09_values_deep_dive',\n",
    "    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'config': {\n",
    "        'model_name': config.model_name,\n",
    "        'seed': SEED,\n",
    "        'n_eval': N,\n",
    "        'n_valid': n_valid,\n",
    "        'n_excluded': n_excluded,\n",
    "        'min_passage_words': config.min_passage_words,\n",
    "        'max_passage_words': config.max_passage_words,\n",
    "        'n_conditions': len(CONDITION_NAMES),\n",
    "        'n_comparisons': N_COMPARISONS,\n",
    "        'bonferroni_alpha': BONFERRONI_ALPHA,\n",
    "    },\n",
    "    'condition_names': CONDITION_NAMES,\n",
    "    'nll_summary': {\n",
    "        cname: {\n",
    "            'mean': float(np.mean(c[cname])),\n",
    "            'std': float(np.std(c[cname])),\n",
    "            'cohens_d_vs_bare': float(cohens_d(c['bare'] - c[cname])) if cname != 'bare' else 0.0,\n",
    "        }\n",
    "        for cname in CONDITION_NAMES\n",
    "    },\n",
    "    'direction_summaries': {\n",
    "        'layer_wise': layer_summary,\n",
    "        'prefix_types': prefix_summary,\n",
    "        'interpolation': interp_summary,\n",
    "        'cross_document': cross_doc_summary,\n",
    "        'positional': positional_summary,\n",
    "    },\n",
    "    'primary_comparisons': comparison_results,\n",
    "    'all_vs_bare': all_vs_bare,\n",
    "    'hardness_breakdown': hardness_breakdown,\n",
    "    'per_sample_results': results,\n",
    "}\n",
    "\n",
    "with open(FINAL_RESULTS_PATH, 'w') as f:\n",
    "    json.dump(final, f, indent=2)\n",
    "\n",
    "print(f\"Results saved to {FINAL_RESULTS_PATH}\")\n",
    "print(f\"File size: {FINAL_RESULTS_PATH.stat().st_size / 1024:.1f} KB\")\n",
    "print(\"\\nDone!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a95dba2a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-10T18:30:23.692281Z",
     "iopub.status.busy": "2026-02-10T18:30:23.691623Z",
     "iopub.status.idle": "2026-02-10T18:30:24.254686Z",
     "shell.execute_reply": "2026-02-10T18:30:24.253779Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning up GPU memory...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory: 4.15 GB -> 0.02 GB\n",
      "Cleanup complete.\n"
     ]
    }
   ],
   "source": [
    "# Cell 13: GPU cleanup\n",
    "import gc\n",
    "\n",
    "print(\"Cleaning up GPU memory...\")\n",
    "mem_before = torch.cuda.memory_allocated() / 1e9\n",
    "\n",
    "del model\n",
    "del tokenizer\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "mem_after = torch.cuda.memory_allocated() / 1e9\n",
    "print(f\"GPU memory: {mem_before:.2f} GB -> {mem_after:.2f} GB\")\n",
    "print(\"Cleanup complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "031f1053b2114af18caf090a2f5f071a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "18a3db0cc9984b929c070fcd26ec56df": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_c793feba35b24ec3824a7c3477b293dc",
        "IPY_MODEL_75c1292fe1b64b18bd8ee1563ec50ce8",
        "IPY_MODEL_94220012b91d41868b51de1b10df4165"
       ],
       "layout": "IPY_MODEL_198f1d743c014b7e8069e49454e1275b",
       "tabbable": null,
       "tooltip": null
      }
     },
     "198f1d743c014b7e8069e49454e1275b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "1c233bfab19941a9b7b77751cea3a88d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_4450aa2d3468490f9b6ca79d8739d1d4",
       "placeholder": "​",
       "style": "IPY_MODEL_031f1053b2114af18caf090a2f5f071a",
       "tabbable": null,
       "tooltip": null,
       "value": "Loading weights: 100%"
      }
     },
     "1c776a9a9b3945379c839730841d5986": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "21fe405685a64ef3b0c51f9af86fc6f9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_5853b0efc524484aaed06e0e0a15430f",
       "max": 291.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_61bf27b78f6c4afc952085cfba386ca5",
       "tabbable": null,
       "tooltip": null,
       "value": 291.0
      }
     },
     "2592e7e6f2e140baa128b72c3ab801ee": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3352ac50b98b4639afc99b202a286094": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "3c93ada3a63c49b18f25935ba933434a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "4450aa2d3468490f9b6ca79d8739d1d4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "4cf4fefb107f4ec4a6f546722cc66f8b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "511bc37f274d43fb8f0bdc685a17102f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_b4972c29882645048c3791adde44190e",
       "placeholder": "​",
       "style": "IPY_MODEL_5da4ad4dd2a249e988c864425c8ec98b",
       "tabbable": null,
       "tooltip": null,
       "value": " 1000/1000 [1:30:27&lt;00:00,  5.44s/it]"
      }
     },
     "52c14b88154e40cc82875b2ee52b7b36": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_af2f25c5866d46f2a233d9ab8237c2af",
       "max": 1000.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_3c93ada3a63c49b18f25935ba933434a",
       "tabbable": null,
       "tooltip": null,
       "value": 1000.0
      }
     },
     "5853b0efc524484aaed06e0e0a15430f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "5da4ad4dd2a249e988c864425c8ec98b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "61bf27b78f6c4afc952085cfba386ca5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "632cb2e7e28c480ab50cf0768cd54299": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_1c233bfab19941a9b7b77751cea3a88d",
        "IPY_MODEL_21fe405685a64ef3b0c51f9af86fc6f9",
        "IPY_MODEL_bdb6d35111854ff3871216d6e74102b8"
       ],
       "layout": "IPY_MODEL_2592e7e6f2e140baa128b72c3ab801ee",
       "tabbable": null,
       "tooltip": null
      }
     },
     "7206b690253f4273b06d06677b78722c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "74faddda45834e09b9723363761610c8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_cf5f4711f49f4a488c8cf80b01038022",
        "IPY_MODEL_52c14b88154e40cc82875b2ee52b7b36",
        "IPY_MODEL_511bc37f274d43fb8f0bdc685a17102f"
       ],
       "layout": "IPY_MODEL_ce7d3471b16c4a448dba8f7221454064",
       "tabbable": null,
       "tooltip": null
      }
     },
     "75c1292fe1b64b18bd8ee1563ec50ce8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "danger",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_fb0a2f71b94c46558b6b88e2e87e06a3",
       "max": 10047.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_f4fd1e67264b423e9b1b4c78528fe469",
       "tabbable": null,
       "tooltip": null,
       "value": 4153.0
      }
     },
     "885c534033eb4f4f8498dcc50690a4fd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "88efe6c27dc94cdb9e38897c1680b038": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "8c5ee0e2187a4751ad372c7a5a3d9b32": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_eb3a9a746de246aa8980884eabbd16cb",
       "max": 1000.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_1c776a9a9b3945379c839730841d5986",
       "tabbable": null,
       "tooltip": null,
       "value": 1000.0
      }
     },
     "94220012b91d41868b51de1b10df4165": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_88efe6c27dc94cdb9e38897c1680b038",
       "placeholder": "​",
       "style": "IPY_MODEL_885c534033eb4f4f8498dcc50690a4fd",
       "tabbable": null,
       "tooltip": null,
       "value": " 4153/10047 [00:00&lt;00:00, 6925.01it/s]"
      }
     },
     "9b02c54d1d0348708434941ae0f11b34": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "9ec8ecabb3b94c77a5e8722ab01dbc48": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a391d34685414fe696a6ea9784770c5a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_e379197ee5244fedbbe17a29c34d5210",
       "placeholder": "​",
       "style": "IPY_MODEL_b91ecc08a3b84ac3ac041b11a9e3f9d3",
       "tabbable": null,
       "tooltip": null,
       "value": "Keyword surrogates: 100%"
      }
     },
     "af2f25c5866d46f2a233d9ab8237c2af": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b2d03cadb46f4ae78d6582e5d5bd64ef": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_a391d34685414fe696a6ea9784770c5a",
        "IPY_MODEL_8c5ee0e2187a4751ad372c7a5a3d9b32",
        "IPY_MODEL_d0694fa01b534537ba7957008de01691"
       ],
       "layout": "IPY_MODEL_9ec8ecabb3b94c77a5e8722ab01dbc48",
       "tabbable": null,
       "tooltip": null
      }
     },
     "b4972c29882645048c3791adde44190e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b91ecc08a3b84ac3ac041b11a9e3f9d3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "bdb6d35111854ff3871216d6e74102b8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_4cf4fefb107f4ec4a6f546722cc66f8b",
       "placeholder": "​",
       "style": "IPY_MODEL_f1aae18883184344a673825015b7f075",
       "tabbable": null,
       "tooltip": null,
       "value": " 291/291 [00:55&lt;00:00,  5.33it/s, Materializing param=model.norm.weight]"
      }
     },
     "c793feba35b24ec3824a7c3477b293dc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_7206b690253f4273b06d06677b78722c",
       "placeholder": "​",
       "style": "IPY_MODEL_f261b9f925bc40468319efa667c4887a",
       "tabbable": null,
       "tooltip": null,
       "value": "Filtering:  41%"
      }
     },
     "ce7d3471b16c4a448dba8f7221454064": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "cf5f4711f49f4a488c8cf80b01038022": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_e978c243bde44ea6b8aa29b2d724bbbc",
       "placeholder": "​",
       "style": "IPY_MODEL_3352ac50b98b4639afc99b202a286094",
       "tabbable": null,
       "tooltip": null,
       "value": "Evaluating: 100%"
      }
     },
     "d0694fa01b534537ba7957008de01691": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_9b02c54d1d0348708434941ae0f11b34",
       "placeholder": "​",
       "style": "IPY_MODEL_de78c005c05443abba1fd89499d2e838",
       "tabbable": null,
       "tooltip": null,
       "value": " 1000/1000 [3:31:50&lt;00:00, 11.67s/it]"
      }
     },
     "de78c005c05443abba1fd89499d2e838": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "e379197ee5244fedbbe17a29c34d5210": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e978c243bde44ea6b8aa29b2d724bbbc": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "eb3a9a746de246aa8980884eabbd16cb": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f1aae18883184344a673825015b7f075": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "f261b9f925bc40468319efa667c4887a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "f4fd1e67264b423e9b1b4c78528fe469": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "fb0a2f71b94c46558b6b88e2e87e06a3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
