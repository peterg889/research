{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd0538cc",
   "metadata": {},
   "source": [
    "# Exp 08: Mechanism Isolation + Prefix Amplification\n",
    "\n",
    "## Motivation\n",
    "\n",
    "Exp 06 found that LLM surrogates improve KV cache quality via value contamination (truncated\n",
    "prefix alters document value vectors) and suffix attention (separator provides new attention\n",
    "targets). But which component of the KV cache carries the signal — keys, values, or both?\n",
    "And does repeating the prefix amplify the effect?\n",
    "\n",
    "## Core Questions\n",
    "\n",
    "1. **Key vs Value isolation**: Is the priming effect carried by keys, values, or both?\n",
    "2. **Prefix amplification**: Does repeating the surrogate prefix K times amplify the effect?\n",
    "\n",
    "## Self-contained. N=1000 samples (smaller, focused probe).\n",
    "\n",
    "## 10 Conditions\n",
    "\n",
    "| # | Condition | Construction | Tests |\n",
    "|---|-----------|-------------|-------|\n",
    "| 1 | Bare | `[BOS][doc]` | Baseline |\n",
    "| 2 | LLM-keyword-trunc | Standard truncated + RoPE | Reference |\n",
    "| 3 | LLM-keyword-suffix | Suffix mode | Reference |\n",
    "| 4 | LLM-keyword+sep | Trunc + suffix | Best method reference |\n",
    "| 5 | Primed-values-only | Keys from bare, values from LLM-trunc | Key vs value |\n",
    "| 6 | Primed-keys-only | Values from bare, keys from LLM-trunc | Key vs value |\n",
    "| 7 | Prefix-1x | `[BOS][kw\\n][doc]` → truncate | Baseline repetition |\n",
    "| 8 | Prefix-3x | `[BOS][kw\\n kw\\n kw\\n][doc]` → truncate | 3× amplification |\n",
    "| 9 | Prefix-5x | `[BOS][kw\\n kw\\n kw\\n kw\\n kw\\n][doc]` → truncate | 5× amplification |\n",
    "| 10 | Separator-only | `[BOS][doc][\\n\\nRelated question: ]` | Control |\n",
    "\n",
    "## 6 Primary Comparisons (Bonferroni alpha = 0.0083)\n",
    "\n",
    "| # | Comparison | Question |\n",
    "|---|-----------|----------|\n",
    "| C1 | Primed-values-only vs Bare | Is the effect in values? |\n",
    "| C2 | Primed-keys-only vs Bare | Is the effect in keys? |\n",
    "| C3 | Primed-values-only vs LLM-keyword-trunc | How much do values capture? |\n",
    "| C4 | Prefix-3x vs Prefix-1x | Does 3× amplify? |\n",
    "| C5 | Prefix-5x vs Prefix-1x | Does 5× amplify? |\n",
    "| C6 | Prefix-5x vs Prefix-3x | Diminishing returns? |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4af9520c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-10T01:22:07.516160Z",
     "iopub.status.busy": "2026-02-10T01:22:07.515840Z",
     "iopub.status.idle": "2026-02-10T01:22:10.524031Z",
     "shell.execute_reply": "2026-02-10T01:22:10.522839Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEED: 42\n",
      "Results directory: results/exp08\n",
      "CUDA available: True\n",
      "GPU: NVIDIA L4\n",
      "GPU memory: 23.6 GB\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Setup — permissions, seeds, results directory\n",
    "import os\n",
    "os.umask(0o000)\n",
    "\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "RESULTS_DIR = Path(\"results/exp08\")\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "SURROGATES_DIR = RESULTS_DIR / \"surrogates\"\n",
    "SURROGATES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CHECKPOINT_PATH = RESULTS_DIR / \"checkpoint.json\"\n",
    "FINAL_RESULTS_PATH = RESULTS_DIR / \"results.json\"\n",
    "\n",
    "print(f\"SEED: {SEED}\")\n",
    "print(f\"Results directory: {RESULTS_DIR}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c69b6cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-10T01:22:10.528442Z",
     "iopub.status.busy": "2026-02-10T01:22:10.527452Z",
     "iopub.status.idle": "2026-02-10T01:23:14.108189Z",
     "shell.execute_reply": "2026-02-10T01:23:14.106963Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading mistralai/Mistral-7B-Instruct-v0.2 (4-bit)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "614602ff3cc54a2b9696b93ed63b75cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/291 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded. dtype=torch.float16, device=cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Load model (Mistral-7B 4-bit)\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "print(f\"Loading {MODEL_NAME} (4-bit)...\")\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "print(f\"Model loaded. dtype={model.dtype}, device={model.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a47656e2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-10T01:23:14.112885Z",
     "iopub.status.busy": "2026-02-10T01:23:14.112191Z",
     "iopub.status.idle": "2026-02-10T01:23:14.701189Z",
     "shell.execute_reply": "2026-02-10T01:23:14.700129Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config ready\n",
      "  num_samples pool: 2000\n",
      "  eval samples: 1000\n",
      "  bonferroni_alpha: 0.0083 (6 comparisons)\n",
      "  conditions: 10\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Imports + config + templates + helpers\n",
    "sys.path.insert(0, \".\")\n",
    "\n",
    "from lib.config import ExperimentConfig\n",
    "from lib.kv_cache import (\n",
    "    build_kv_cache,\n",
    "    build_suffix_kv_cache,\n",
    "    score_answer_with_cache,\n",
    "    deepcopy_cache,\n",
    "    extract_and_truncate_cache_with_bos,\n",
    "    correct_rope_positions_with_bos,\n",
    "    build_hybrid_cache,\n",
    "    _get_cache_keys,\n",
    "    _get_cache_values,\n",
    ")\n",
    "from lib.data import load_ms_marco, load_evaluation_samples\n",
    "from lib.analysis import cohens_d\n",
    "from lib.surrogate import generate_all_5_surrogates, TOP_5_SURROGATE_TEMPLATES\n",
    "from scipy import stats\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "config = ExperimentConfig(\n",
    "    model_name=MODEL_NAME,\n",
    "    num_samples=2000,\n",
    "    min_passage_words=20,\n",
    "    max_passage_words=500,\n",
    "    seed=SEED,\n",
    ")\n",
    "\n",
    "# Templates — bare text, no \"Document:\\n\" framing\n",
    "SURROGATE_PREFIX_TEMPLATE = \"{surrogate}\\n\"\n",
    "DOCUMENT_TEMPLATE = \"{document}\"\n",
    "QUERY_TEMPLATE = \"\\nQuery: {query}\\nAnswer:\"\n",
    "ANSWER_TEMPLATE = \" {answer}\"\n",
    "SUFFIX_SEPARATOR = \"\\n\\nRelated question: \"\n",
    "CHECKPOINT_EVERY = 50\n",
    "N_COMPARISONS = 6\n",
    "BONFERRONI_ALPHA = 0.05 / N_COMPARISONS\n",
    "N_EVAL = 1000  # Smaller focused probe\n",
    "\n",
    "print(\"Config ready\")\n",
    "print(f\"  num_samples pool: {config.num_samples}\")\n",
    "print(f\"  eval samples: {N_EVAL}\")\n",
    "print(f\"  bonferroni_alpha: {BONFERRONI_ALPHA:.4f} ({N_COMPARISONS} comparisons)\")\n",
    "print(f\"  conditions: 10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "863a6e43",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-10T01:23:14.705727Z",
     "iopub.status.busy": "2026-02-10T01:23:14.705175Z",
     "iopub.status.idle": "2026-02-10T01:23:15.990070Z",
     "shell.execute_reply": "2026-02-10T01:23:15.989155Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`trust_remote_code` is not supported anymore.\n",
      "Please check that the Hugging Face dataset 'microsoft/ms_marco' isn't based on a loading script and remove `trust_remote_code`.\n",
      "If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading microsoft/ms_marco dataset...\n",
      "Dataset loaded: 10047 samples\n",
      "Filtering samples...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ec919f2e1ed4c0584b829d0f168d8c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filtering:   0%|          | 0/10047 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 2000 samples\n",
      "Loaded 2000 candidates, using first 1000 for evaluation\n",
      "Example passage (92 words): The word totem derives from the Algonquian (most likely Ojibwe) word odoodem [ oˈtuːtɛm ], his kinsh...\n",
      "Example query: what do the carvings on a totem pole mean\n",
      "Example answer: Represent characters or events in a story.\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Load MS MARCO (1000 samples)\n",
    "dataset = load_ms_marco(config)\n",
    "\n",
    "np.random.seed(SEED)\n",
    "all_samples = load_evaluation_samples(dataset, config, require_answer=True)\n",
    "\n",
    "samples = all_samples[:N_EVAL]\n",
    "N = len(samples)\n",
    "print(f\"Loaded {len(all_samples)} candidates, using first {N} for evaluation\")\n",
    "print(f\"Example passage ({len(samples[0]['passage'].split())} words): {samples[0]['passage'][:100]}...\")\n",
    "print(f\"Example query: {samples[0]['query']}\")\n",
    "print(f\"Example answer: {samples[0]['answer']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b94dd949",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-10T01:23:15.994604Z",
     "iopub.status.busy": "2026-02-10T01:23:15.993908Z",
     "iopub.status.idle": "2026-02-10T04:53:37.820437Z",
     "shell.execute_reply": "2026-02-10T04:53:37.819623Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PHASE 1: LLM SURROGATE GENERATION (keyword only)\n",
      "======================================================================\n",
      "Generating keyword surrogates for samples 0 to 999...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2267c79847ff45dda11f6bd90fd5b9ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Keyword surrogates:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saved 100/1000 | 0.08 s/s | ETA: 192.6 min\n",
      "  Saved 200/1000 | 0.08 s/s | ETA: 169.9 min\n",
      "  Saved 300/1000 | 0.08 s/s | ETA: 147.9 min\n",
      "  Saved 400/1000 | 0.08 s/s | ETA: 126.9 min\n",
      "  Saved 500/1000 | 0.08 s/s | ETA: 105.3 min\n",
      "  Saved 600/1000 | 0.08 s/s | ETA: 83.9 min\n",
      "  Saved 700/1000 | 0.08 s/s | ETA: 63.2 min\n",
      "  Saved 800/1000 | 0.08 s/s | ETA: 42.5 min\n",
      "  Saved 900/1000 | 0.08 s/s | ETA: 21.2 min\n",
      "  Saved 1000/1000 | 0.08 s/s | ETA: 0.0 min\n",
      "Keyword surrogates complete: 1000 samples\n",
      "Empty surrogates: 0/1000\n",
      "Example: 'totem, algonquian, odoodem, carvings, animals, characters, story, family legends, coastal Pacific Northwest, native culture, european explorers, history, decorative car'\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Generate LLM keyword surrogates (only keyword_query template needed)\n",
    "# Using generate_all_5_surrogates for consistency but only using keyword_query\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PHASE 1: LLM SURROGATE GENERATION (keyword only)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "surrogates_path = SURROGATES_DIR / \"keyword_surrogates.json\"\n",
    "\n",
    "if surrogates_path.exists():\n",
    "    with open(surrogates_path, 'r') as f:\n",
    "        surrogates_data = json.load(f)\n",
    "    keyword_surrogates = surrogates_data['surrogates']\n",
    "    print(f\"Loaded {len(keyword_surrogates)} keyword surrogates from cache\")\n",
    "else:\n",
    "    keyword_surrogates = []\n",
    "\n",
    "start_idx_gen = len(keyword_surrogates)\n",
    "if start_idx_gen < N:\n",
    "    print(f\"Generating keyword surrogates for samples {start_idx_gen} to {N-1}...\")\n",
    "    t_start = time.time()\n",
    "    for idx in tqdm(range(start_idx_gen, N), initial=start_idx_gen, total=N,\n",
    "                     desc=\"Keyword surrogates\"):\n",
    "        passage = samples[idx]['passage']\n",
    "        try:\n",
    "            s5 = generate_all_5_surrogates(passage, model, tokenizer, config)\n",
    "            kw = s5.get('keyword_query', '')\n",
    "        except Exception as e:\n",
    "            print(f\"  WARNING: Generation failed for sample {idx}: {e}\")\n",
    "            kw = \"\"\n",
    "        keyword_surrogates.append(kw)\n",
    "\n",
    "        if (idx + 1) % 100 == 0 or idx == N - 1:\n",
    "            with open(surrogates_path, 'w') as f:\n",
    "                json.dump({'surrogates': keyword_surrogates}, f)\n",
    "            elapsed = time.time() - t_start\n",
    "            rate = (idx - start_idx_gen + 1) / elapsed if elapsed > 0 else 0\n",
    "            remaining = (N - idx - 1) / rate if rate > 0 else 0\n",
    "            tqdm.write(f\"  Saved {idx+1}/{N} | {rate:.2f} s/s | ETA: {remaining/60:.1f} min\")\n",
    "\n",
    "    with open(surrogates_path, 'w') as f:\n",
    "        json.dump({'surrogates': keyword_surrogates}, f)\n",
    "    print(f\"Keyword surrogates complete: {len(keyword_surrogates)} samples\")\n",
    "else:\n",
    "    print(f\"All keyword surrogates already cached ({len(keyword_surrogates)} samples)\")\n",
    "\n",
    "n_empty = sum(1 for s in keyword_surrogates if not s.strip())\n",
    "print(f\"Empty surrogates: {n_empty}/{N}\")\n",
    "print(f\"Example: '{keyword_surrogates[0]}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e225ef4e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-10T04:53:37.824129Z",
     "iopub.status.busy": "2026-02-10T04:53:37.823471Z",
     "iopub.status.idle": "2026-02-10T04:53:37.830491Z",
     "shell.execute_reply": "2026-02-10T04:53:37.829714Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "EXPERIMENTAL CONDITIONS EXPLAINED\n",
      "======================================================================\n",
      "\n",
      "### 1. Bare ###\n",
      "  Cache: [BOS][doc]\n",
      "  Detail: No prefix — baseline\n",
      "\n",
      "### 2. LLM-keyword-trunc ###\n",
      "  Cache: [BOS][llm_kw\\n][doc] → truncate + RoPE\n",
      "  Detail: Standard truncated prefix: 'totem, algonquian, odoodem, carvings, animals, characters, story, family legends, coastal Pacific Northwest, native culture, european explorers, history, decorative car'\n",
      "\n",
      "### 3. LLM-keyword-suffix ###\n",
      "  Cache: [BOS][doc][\\n\\nRelated question: llm_kw]\n",
      "  Detail: Suffix mode: 'totem, algonquian, odoodem, carvings, animals, characters, story, family legends, coastal Pacific Northwest, native culture, european explorers, history, decorative car'\n",
      "\n",
      "### 4. LLM-keyword+sep ###\n",
      "  Cache: [BOS][llm_kw\\n][doc][\\n\\nRelated question: ] (prefix+suffix)\n",
      "  Detail: Stacking: truncated prefix + suffix separator\n",
      "\n",
      "### 5. Primed-values-only ###\n",
      "  Cache: Keys from bare cache, values from LLM-trunc cache\n",
      "  Detail: Tests: is the effect carried by values?\n",
      "\n",
      "### 6. Primed-keys-only ###\n",
      "  Cache: Values from bare cache, keys from LLM-trunc cache\n",
      "  Detail: Tests: is the effect carried by keys?\n",
      "\n",
      "### 7. Prefix-1x ###\n",
      "  Cache: [BOS][kw\\n][doc] → truncate (same as #2)\n",
      "  Detail: 1× prefix — baseline for repetition\n",
      "\n",
      "### 8. Prefix-3x ###\n",
      "  Cache: [BOS][kw\\n kw\\n kw\\n][doc] → truncate + RoPE(3×offset)\n",
      "  Detail: 3× repeated prefix — amplification test\n",
      "\n",
      "### 9. Prefix-5x ###\n",
      "  Cache: [BOS][kw\\n kw\\n kw\\n kw\\n kw\\n][doc] → truncate + RoPE(5×offset)\n",
      "  Detail: 5× repeated prefix — maximum amplification\n",
      "\n",
      "### 10. Separator-only ###\n",
      "  Cache: [BOS][doc][\\n\\nRelated question: ]\n",
      "  Detail: Suffix framing only — structural control\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Condition explanation with concrete examples\n",
    "print(\"=\" * 70)\n",
    "print(\"EXPERIMENTAL CONDITIONS EXPLAINED\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "ex_kw = keyword_surrogates[0]\n",
    "\n",
    "conditions_explained = [\n",
    "    (\"1. Bare\",\n",
    "     \"[BOS][doc]\",\n",
    "     \"No prefix — baseline\"),\n",
    "    (\"2. LLM-keyword-trunc\",\n",
    "     \"[BOS][llm_kw\\\\n][doc] → truncate + RoPE\",\n",
    "     f\"Standard truncated prefix: '{ex_kw}'\"),\n",
    "    (\"3. LLM-keyword-suffix\",\n",
    "     \"[BOS][doc][\\\\n\\\\nRelated question: llm_kw]\",\n",
    "     f\"Suffix mode: '{ex_kw}'\"),\n",
    "    (\"4. LLM-keyword+sep\",\n",
    "     \"[BOS][llm_kw\\\\n][doc][\\\\n\\\\nRelated question: ] (prefix+suffix)\",\n",
    "     \"Stacking: truncated prefix + suffix separator\"),\n",
    "    (\"5. Primed-values-only\",\n",
    "     \"Keys from bare cache, values from LLM-trunc cache\",\n",
    "     \"Tests: is the effect carried by values?\"),\n",
    "    (\"6. Primed-keys-only\",\n",
    "     \"Values from bare cache, keys from LLM-trunc cache\",\n",
    "     \"Tests: is the effect carried by keys?\"),\n",
    "    (\"7. Prefix-1x\",\n",
    "     \"[BOS][kw\\\\n][doc] → truncate (same as #2)\",\n",
    "     \"1× prefix — baseline for repetition\"),\n",
    "    (\"8. Prefix-3x\",\n",
    "     \"[BOS][kw\\\\n kw\\\\n kw\\\\n][doc] → truncate + RoPE(3×offset)\",\n",
    "     \"3× repeated prefix — amplification test\"),\n",
    "    (\"9. Prefix-5x\",\n",
    "     \"[BOS][kw\\\\n kw\\\\n kw\\\\n kw\\\\n kw\\\\n][doc] → truncate + RoPE(5×offset)\",\n",
    "     \"5× repeated prefix — maximum amplification\"),\n",
    "    (\"10. Separator-only\",\n",
    "     \"[BOS][doc][\\\\n\\\\nRelated question: ]\",\n",
    "     \"Suffix framing only — structural control\"),\n",
    "]\n",
    "\n",
    "for name, pattern, detail in conditions_explained:\n",
    "    print(f\"\\n### {name} ###\")\n",
    "    print(f\"  Cache: {pattern}\")\n",
    "    print(f\"  Detail: {detail}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3cb85d7e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-10T04:53:37.833822Z",
     "iopub.status.busy": "2026-02-10T04:53:37.833484Z",
     "iopub.status.idle": "2026-02-10T05:53:44.267137Z",
     "shell.execute_reply": "2026-02-10T05:53:44.266126Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PHASE 2: MAIN EVALUATION (10 conditions × 1000 samples)\n",
      "======================================================================\n",
      "No checkpoint found. Starting fresh.\n",
      "Evaluating samples 0 to 999\n",
      "Conditions: 10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d93b5f11256443488138112080896f7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 50/1000 | 0.28 s/s | ETA: 56.9 min\n",
      "  Checkpoint 100/1000 | 0.28 s/s | ETA: 54.2 min\n",
      "  Checkpoint 150/1000 | 0.28 s/s | ETA: 51.1 min\n",
      "  Checkpoint 200/1000 | 0.28 s/s | ETA: 48.1 min\n",
      "  Checkpoint 250/1000 | 0.28 s/s | ETA: 45.2 min\n",
      "  Checkpoint 300/1000 | 0.28 s/s | ETA: 42.1 min\n",
      "  Checkpoint 350/1000 | 0.28 s/s | ETA: 39.1 min\n",
      "  Checkpoint 400/1000 | 0.28 s/s | ETA: 36.1 min\n",
      "  Checkpoint 450/1000 | 0.28 s/s | ETA: 33.1 min\n",
      "  Checkpoint 500/1000 | 0.28 s/s | ETA: 30.1 min\n",
      "  Checkpoint 550/1000 | 0.28 s/s | ETA: 27.1 min\n",
      "  Checkpoint 600/1000 | 0.28 s/s | ETA: 24.1 min\n",
      "  Checkpoint 650/1000 | 0.28 s/s | ETA: 21.1 min\n",
      "  Checkpoint 700/1000 | 0.28 s/s | ETA: 18.1 min\n",
      "  Checkpoint 750/1000 | 0.28 s/s | ETA: 15.1 min\n",
      "  Checkpoint 800/1000 | 0.28 s/s | ETA: 12.0 min\n",
      "  Checkpoint 850/1000 | 0.28 s/s | ETA: 9.0 min\n",
      "  Checkpoint 900/1000 | 0.28 s/s | ETA: 6.0 min\n",
      "  Checkpoint 950/1000 | 0.28 s/s | ETA: 3.0 min\n",
      "  Checkpoint 1000/1000 | 0.28 s/s | ETA: 0.0 min\n",
      "\n",
      "Evaluation complete: 1000 samples in 60.1 min\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Main eval loop — 10 conditions × 1000 samples\n",
    "print(\"=\" * 70)\n",
    "print(\"PHASE 2: MAIN EVALUATION (10 conditions × 1000 samples)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "CONDITION_NAMES = [\n",
    "    'bare', 'llm_kw_trunc', 'llm_kw_suffix', 'llm_kw_sep',\n",
    "    'primed_values_only', 'primed_keys_only',\n",
    "    'prefix_1x', 'prefix_3x', 'prefix_5x',\n",
    "    'separator_only',\n",
    "]\n",
    "\n",
    "results = []\n",
    "start_idx = 0\n",
    "\n",
    "if CHECKPOINT_PATH.exists():\n",
    "    with open(CHECKPOINT_PATH, 'r') as f:\n",
    "        ckpt = json.load(f)\n",
    "    ckpt_queries = ckpt.get('sample_queries', [])\n",
    "    current_queries = [s['query'] for s in samples]\n",
    "    if ckpt_queries == current_queries:\n",
    "        results = ckpt['results']\n",
    "        start_idx = len(results)\n",
    "        print(f\"Resuming from checkpoint: {start_idx}/{N}\")\n",
    "    else:\n",
    "        print(\"Checkpoint sample mismatch. Starting fresh.\")\n",
    "else:\n",
    "    print(\"No checkpoint found. Starting fresh.\")\n",
    "\n",
    "print(f\"Evaluating samples {start_idx} to {N-1}\")\n",
    "print(f\"Conditions: {len(CONDITION_NAMES)}\")\n",
    "\n",
    "t_start = time.time()\n",
    "\n",
    "for idx in tqdm(range(start_idx, N), initial=start_idx, total=N, desc=\"Evaluating\"):\n",
    "    sample = samples[idx]\n",
    "    passage = sample['passage']\n",
    "    query = sample['query']\n",
    "    answer = sample['answer']\n",
    "\n",
    "    query_prompt = QUERY_TEMPLATE.format(query=query)\n",
    "    answer_text = ANSWER_TEMPLATE.format(answer=answer)\n",
    "    llm_kw_text = keyword_surrogates[idx]\n",
    "\n",
    "    # --- Matched tokenization ---\n",
    "    oracle_prefix = SURROGATE_PREFIX_TEMPLATE.format(surrogate=query)\n",
    "    document_text = DOCUMENT_TEMPLATE.format(document=passage)\n",
    "    full_oracle_text = oracle_prefix + document_text\n",
    "\n",
    "    full_oracle_enc = tokenizer(full_oracle_text, return_tensors=\"pt\",\n",
    "                                add_special_tokens=True, padding=False, truncation=False)\n",
    "    full_oracle_ids = full_oracle_enc['input_ids'].to(config.device)\n",
    "\n",
    "    oracle_prefix_enc = tokenizer(oracle_prefix, return_tensors=\"pt\",\n",
    "                                  add_special_tokens=True, padding=False, truncation=False)\n",
    "    oracle_prefix_len = oracle_prefix_enc['input_ids'].shape[1]\n",
    "\n",
    "    bos_id = full_oracle_ids[:, :1]\n",
    "    doc_ids = full_oracle_ids[:, oracle_prefix_len:]\n",
    "    doc_len = doc_ids.shape[1]\n",
    "\n",
    "    # Prefix IDs for LLM keyword (used for all truncated conditions)\n",
    "    kw_prefix_str = SURROGATE_PREFIX_TEMPLATE.format(surrogate=llm_kw_text)\n",
    "    kw_prefix_enc = tokenizer(kw_prefix_str, return_tensors=\"pt\",\n",
    "                              add_special_tokens=False, padding=False, truncation=False)\n",
    "    kw_prefix_ids = kw_prefix_enc['input_ids'].to(config.device)\n",
    "    kw_prefix_token_len_1x = kw_prefix_ids.shape[1]  # without BOS\n",
    "\n",
    "    # === Condition 1: BARE ===\n",
    "    bare_ids = torch.cat([bos_id, doc_ids], dim=1)\n",
    "    with torch.no_grad():\n",
    "        bare_out = model(input_ids=bare_ids, attention_mask=torch.ones_like(bare_ids),\n",
    "                         use_cache=True, return_dict=True)\n",
    "    bare_cache = bare_out.past_key_values\n",
    "    nll_bare = score_answer_with_cache(\n",
    "        deepcopy_cache(bare_cache), bare_ids.shape[1],\n",
    "        query_prompt, answer_text, model, tokenizer, config)\n",
    "\n",
    "    # === Condition 2: LLM-KEYWORD-TRUNC (also prefix-1x) ===\n",
    "    full_1x_ids = torch.cat([bos_id, kw_prefix_ids, doc_ids], dim=1)\n",
    "    prefix_token_len_1x = 1 + kw_prefix_token_len_1x\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out_1x = model(input_ids=full_1x_ids,\n",
    "                       attention_mask=torch.ones_like(full_1x_ids),\n",
    "                       use_cache=True, return_dict=True)\n",
    "    trunc_cache_1x = extract_and_truncate_cache_with_bos(out_1x.past_key_values, doc_len)\n",
    "    correct_rope_positions_with_bos(trunc_cache_1x, prefix_token_len_1x - 1, model)\n",
    "    nll_llm_kw_trunc = score_answer_with_cache(\n",
    "        deepcopy_cache(trunc_cache_1x), 1 + doc_len,\n",
    "        query_prompt, answer_text, model, tokenizer, config)\n",
    "    del out_1x\n",
    "\n",
    "    # prefix_1x is same as llm_kw_trunc\n",
    "    nll_prefix_1x = nll_llm_kw_trunc\n",
    "\n",
    "    # === Condition 3: LLM-KEYWORD-SUFFIX ===\n",
    "    suf_len, suf_cache = build_suffix_kv_cache(\n",
    "        passage, llm_kw_text, model, tokenizer, config, separator=SUFFIX_SEPARATOR)\n",
    "    nll_llm_kw_suffix = score_answer_with_cache(\n",
    "        deepcopy_cache(suf_cache), suf_len,\n",
    "        query_prompt, answer_text, model, tokenizer, config)\n",
    "    del suf_cache\n",
    "\n",
    "    # === Condition 4: LLM-KEYWORD+SEP (stacking) ===\n",
    "    suffix_enc = tokenizer(SUFFIX_SEPARATOR, return_tensors=\"pt\",\n",
    "                           add_special_tokens=False, padding=False, truncation=False)\n",
    "    suffix_ids = suffix_enc['input_ids'].to(config.device)\n",
    "    suffix_len_tok = suffix_ids.shape[1]\n",
    "    cache_len_before_suffix = 1 + doc_len\n",
    "\n",
    "    suffix_position_ids = torch.arange(\n",
    "        cache_len_before_suffix, cache_len_before_suffix + suffix_len_tok,\n",
    "        device=config.device\n",
    "    ).unsqueeze(0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        suffix_out = model(\n",
    "            input_ids=suffix_ids,\n",
    "            attention_mask=torch.ones(1, cache_len_before_suffix + suffix_len_tok,\n",
    "                                      device=config.device, dtype=torch.long),\n",
    "            position_ids=suffix_position_ids,\n",
    "            past_key_values=deepcopy_cache(trunc_cache_1x),\n",
    "            use_cache=True,\n",
    "            return_dict=True,\n",
    "        )\n",
    "    combo_cache = suffix_out.past_key_values\n",
    "    combo_len = cache_len_before_suffix + suffix_len_tok\n",
    "    nll_llm_kw_sep = score_answer_with_cache(\n",
    "        deepcopy_cache(combo_cache), combo_len,\n",
    "        query_prompt, answer_text, model, tokenizer, config)\n",
    "    del suffix_out, combo_cache\n",
    "\n",
    "    # === Condition 5: PRIMED-VALUES-ONLY ===\n",
    "    # Keys from bare cache, values from LLM-trunc cache\n",
    "    hybrid_values = build_hybrid_cache(\n",
    "        keys_source=bare_cache,\n",
    "        values_source=trunc_cache_1x,\n",
    "    )\n",
    "    nll_primed_values = score_answer_with_cache(\n",
    "        deepcopy_cache(hybrid_values), 1 + doc_len,\n",
    "        query_prompt, answer_text, model, tokenizer, config)\n",
    "    del hybrid_values\n",
    "\n",
    "    # === Condition 6: PRIMED-KEYS-ONLY ===\n",
    "    # Values from bare cache, keys from LLM-trunc cache\n",
    "    hybrid_keys = build_hybrid_cache(\n",
    "        keys_source=trunc_cache_1x,\n",
    "        values_source=bare_cache,\n",
    "    )\n",
    "    nll_primed_keys = score_answer_with_cache(\n",
    "        deepcopy_cache(hybrid_keys), 1 + doc_len,\n",
    "        query_prompt, answer_text, model, tokenizer, config)\n",
    "    del hybrid_keys\n",
    "\n",
    "    del bare_cache, trunc_cache_1x  # Free memory\n",
    "\n",
    "    # === Condition 8: PREFIX-3x ===\n",
    "    # [BOS][kw\\n kw\\n kw\\n][doc] → truncate + RoPE\n",
    "    prefix_3x_ids = kw_prefix_ids.repeat(1, 3)\n",
    "    full_3x_ids = torch.cat([bos_id, prefix_3x_ids, doc_ids], dim=1)\n",
    "    prefix_token_len_3x = 1 + prefix_3x_ids.shape[1]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out_3x = model(input_ids=full_3x_ids,\n",
    "                       attention_mask=torch.ones_like(full_3x_ids),\n",
    "                       use_cache=True, return_dict=True)\n",
    "    trunc_cache_3x = extract_and_truncate_cache_with_bos(out_3x.past_key_values, doc_len)\n",
    "    correct_rope_positions_with_bos(trunc_cache_3x, prefix_token_len_3x - 1, model)\n",
    "    nll_prefix_3x = score_answer_with_cache(\n",
    "        deepcopy_cache(trunc_cache_3x), 1 + doc_len,\n",
    "        query_prompt, answer_text, model, tokenizer, config)\n",
    "    del out_3x, trunc_cache_3x\n",
    "\n",
    "    # === Condition 9: PREFIX-5x ===\n",
    "    prefix_5x_ids = kw_prefix_ids.repeat(1, 5)\n",
    "    full_5x_ids = torch.cat([bos_id, prefix_5x_ids, doc_ids], dim=1)\n",
    "    prefix_token_len_5x = 1 + prefix_5x_ids.shape[1]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out_5x = model(input_ids=full_5x_ids,\n",
    "                       attention_mask=torch.ones_like(full_5x_ids),\n",
    "                       use_cache=True, return_dict=True)\n",
    "    trunc_cache_5x = extract_and_truncate_cache_with_bos(out_5x.past_key_values, doc_len)\n",
    "    correct_rope_positions_with_bos(trunc_cache_5x, prefix_token_len_5x - 1, model)\n",
    "    nll_prefix_5x = score_answer_with_cache(\n",
    "        deepcopy_cache(trunc_cache_5x), 1 + doc_len,\n",
    "        query_prompt, answer_text, model, tokenizer, config)\n",
    "    del out_5x, trunc_cache_5x\n",
    "\n",
    "    # === Condition 10: SEPARATOR-ONLY ===\n",
    "    sep_len, sep_cache = build_suffix_kv_cache(\n",
    "        passage, \"\", model, tokenizer, config, separator=SUFFIX_SEPARATOR)\n",
    "    nll_separator = score_answer_with_cache(\n",
    "        deepcopy_cache(sep_cache), sep_len,\n",
    "        query_prompt, answer_text, model, tokenizer, config)\n",
    "    del sep_cache\n",
    "\n",
    "    # --- Store result ---\n",
    "    result = {\n",
    "        'idx': idx,\n",
    "        'doc_len': doc_len,\n",
    "        'passage_word_count': len(passage.split()),\n",
    "        'bare': nll_bare,\n",
    "        'llm_kw_trunc': nll_llm_kw_trunc,\n",
    "        'llm_kw_suffix': nll_llm_kw_suffix,\n",
    "        'llm_kw_sep': nll_llm_kw_sep,\n",
    "        'primed_values_only': nll_primed_values,\n",
    "        'primed_keys_only': nll_primed_keys,\n",
    "        'prefix_1x': nll_prefix_1x,\n",
    "        'prefix_3x': nll_prefix_3x,\n",
    "        'prefix_5x': nll_prefix_5x,\n",
    "        'separator_only': nll_separator,\n",
    "    }\n",
    "    results.append(result)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    if (idx + 1) % CHECKPOINT_EVERY == 0 or idx == N - 1:\n",
    "        ckpt_data = {\n",
    "            'results': results,\n",
    "            'sample_queries': [s['query'] for s in samples],\n",
    "            'completed': len(results),\n",
    "            'total': N,\n",
    "            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        }\n",
    "        with open(CHECKPOINT_PATH, 'w') as f:\n",
    "            json.dump(ckpt_data, f)\n",
    "        elapsed = time.time() - t_start\n",
    "        rate = (idx - start_idx + 1) / elapsed if elapsed > 0 else 0\n",
    "        remaining = (N - idx - 1) / rate if rate > 0 else 0\n",
    "        tqdm.write(f\"  Checkpoint {idx+1}/{N} | {rate:.2f} s/s | ETA: {remaining/60:.1f} min\")\n",
    "\n",
    "elapsed_total = time.time() - t_start\n",
    "print(f\"\\nEvaluation complete: {len(results)} samples in {elapsed_total/60:.1f} min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "111e0abe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-10T05:53:44.271368Z",
     "iopub.status.busy": "2026-02-10T05:53:44.271091Z",
     "iopub.status.idle": "2026-02-10T05:53:44.606957Z",
     "shell.execute_reply": "2026-02-10T05:53:44.606128Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ANALYSIS — MECHANISM ISOLATION + AMPLIFICATION\n",
      "======================================================================\n",
      "Total: 1000, Valid: 930, Excluded: 70\n",
      "\n",
      "Condition                   Mean NLL        Std  d vs Bare\n",
      "------------------------------------------------------------\n",
      "bare                          1.1628     1.7270          —\n",
      "llm_kw_trunc                  1.0757     1.6441     +0.254\n",
      "llm_kw_suffix                 1.0866     1.7106     +0.179\n",
      "llm_kw_sep                    1.0010     1.4590     +0.288\n",
      "primed_values_only            1.0860     1.6338     +0.275\n",
      "primed_keys_only              1.1651     1.7095     -0.009\n",
      "prefix_1x                     1.0757     1.6441     +0.254\n",
      "prefix_3x                     1.0871     1.6625     +0.245\n",
      "prefix_5x                     1.0991     1.6783     +0.208\n",
      "separator_only                1.0665     1.5606     +0.214\n",
      "\n",
      "=====================================================================================\n",
      "6 PRIMARY COMPARISONS (Bonferroni alpha = 0.0083)\n",
      "=====================================================================================\n",
      "\n",
      "Comparison                            Mean Δ        d    Win%        t            p   Sig\n",
      "------------------------------------------------------------------------------------------\n",
      "C1: Values-only vs Bare               0.0769    0.275   67.8%     8.39    1.74e-16   ***\n",
      "C2: Keys-only vs Bare                -0.0023   -0.009   46.0%    -0.28    7.80e-01    ns\n",
      "C3: Values-only vs LLM-trunc         -0.0102   -0.045   45.2%    -1.37    1.70e-01    ns\n",
      "C4: Prefix-3x vs Prefix-1x           -0.0114   -0.069   51.1%    -2.10    3.61e-02     *\n",
      "C5: Prefix-5x vs Prefix-1x           -0.0234   -0.129   46.9%    -3.94    8.93e-05   ***\n",
      "C6: Prefix-5x vs Prefix-3x           -0.0120   -0.220   35.2%    -6.72    3.10e-11   ***\n",
      "\n",
      "=====================================================================================\n",
      "ALL CONDITIONS vs BARE\n",
      "=====================================================================================\n",
      "\n",
      "Condition                  d vs Bare    Win%            p\n",
      "------------------------------------------------------------\n",
      "llm_kw_trunc                   0.254   69.2%    2.46e-14   ***\n",
      "llm_kw_suffix                  0.179   60.8%    6.44e-08   ***\n",
      "llm_kw_sep                     0.288   67.4%    8.30e-18   ***\n",
      "primed_values_only             0.275   67.8%    1.74e-16   ***\n",
      "primed_keys_only              -0.009   46.0%    7.80e-01    ns\n",
      "prefix_1x                      0.254   69.2%    2.46e-14   ***\n",
      "prefix_3x                      0.245   67.4%    1.99e-13   ***\n",
      "prefix_5x                      0.208   66.5%    3.57e-10   ***\n",
      "separator_only                 0.214   65.1%    1.19e-10   ***\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Primary analysis — 6 comparisons + NLL summary\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ANALYSIS — MECHANISM ISOLATION + AMPLIFICATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Extract arrays and filter zero NLLs\n",
    "cond_arrays = {}\n",
    "for cname in CONDITION_NAMES:\n",
    "    cond_arrays[cname] = np.array([r[cname] for r in results])\n",
    "\n",
    "valid = np.ones(len(results), dtype=bool)\n",
    "for cname in CONDITION_NAMES:\n",
    "    valid &= (cond_arrays[cname] != 0)\n",
    "n_valid = int(np.sum(valid))\n",
    "n_excluded = int(np.sum(~valid))\n",
    "print(f\"Total: {len(results)}, Valid: {n_valid}, Excluded: {n_excluded}\")\n",
    "\n",
    "c = {}\n",
    "for cname in CONDITION_NAMES:\n",
    "    c[cname] = cond_arrays[cname][valid]\n",
    "\n",
    "# NLL summary table\n",
    "print(f\"\\n{'Condition':<25} {'Mean NLL':>10} {'Std':>10} {'d vs Bare':>10}\")\n",
    "print(\"-\" * 60)\n",
    "for cname in CONDITION_NAMES:\n",
    "    mean_nll = np.mean(c[cname])\n",
    "    std_nll = np.std(c[cname])\n",
    "    if cname == 'bare':\n",
    "        d_str = \"—\"\n",
    "    else:\n",
    "        d = cohens_d(c['bare'] - c[cname])\n",
    "        d_str = f\"{d:+.3f}\"\n",
    "    print(f\"{cname:<25} {mean_nll:>10.4f} {std_nll:>10.4f} {d_str:>10}\")\n",
    "\n",
    "# 6 primary comparisons\n",
    "print(f\"\\n{'='*85}\")\n",
    "print(f\"6 PRIMARY COMPARISONS (Bonferroni alpha = {BONFERRONI_ALPHA:.4f})\")\n",
    "print(f\"{'='*85}\")\n",
    "\n",
    "comparisons = [\n",
    "    ('C1: Values-only vs Bare',\n",
    "     c['bare'] - c['primed_values_only'],\n",
    "     'Is the effect in values?'),\n",
    "    ('C2: Keys-only vs Bare',\n",
    "     c['bare'] - c['primed_keys_only'],\n",
    "     'Is the effect in keys?'),\n",
    "    ('C3: Values-only vs LLM-trunc',\n",
    "     c['llm_kw_trunc'] - c['primed_values_only'],\n",
    "     'How much do values capture?'),\n",
    "    ('C4: Prefix-3x vs Prefix-1x',\n",
    "     c['prefix_1x'] - c['prefix_3x'],\n",
    "     'Does 3x amplify?'),\n",
    "    ('C5: Prefix-5x vs Prefix-1x',\n",
    "     c['prefix_1x'] - c['prefix_5x'],\n",
    "     'Does 5x amplify?'),\n",
    "    ('C6: Prefix-5x vs Prefix-3x',\n",
    "     c['prefix_3x'] - c['prefix_5x'],\n",
    "     'Diminishing returns?'),\n",
    "]\n",
    "\n",
    "print(f\"\\n{'Comparison':<35} {'Mean Δ':>8} {'d':>8} {'Win%':>7} {'t':>8} {'p':>12} {'Sig':>5}\")\n",
    "print(\"-\" * 90)\n",
    "\n",
    "comparison_results = {}\n",
    "for name, delta, question in comparisons:\n",
    "    d = cohens_d(delta)\n",
    "    win = np.mean(delta > 0) * 100\n",
    "    t_stat, p_val = stats.ttest_1samp(delta, 0)\n",
    "    sig = \"***\" if p_val < 0.001 else \"**\" if p_val < BONFERRONI_ALPHA else \"*\" if p_val < 0.05 else \"ns\"\n",
    "    print(f\"{name:<35} {np.mean(delta):>8.4f} {d:>8.3f} {win:>6.1f}% {t_stat:>8.2f} {p_val:>11.2e} {sig:>5}\")\n",
    "    comparison_results[name] = {\n",
    "        'mean_delta': float(np.mean(delta)),\n",
    "        'cohens_d': float(d),\n",
    "        'win_rate': float(win / 100),\n",
    "        't_stat': float(t_stat),\n",
    "        'p_value': float(p_val),\n",
    "        'bonferroni_significant': bool(p_val < BONFERRONI_ALPHA),\n",
    "        'question': question,\n",
    "    }\n",
    "\n",
    "# All vs Bare\n",
    "print(f\"\\n{'='*85}\")\n",
    "print(\"ALL CONDITIONS vs BARE\")\n",
    "print(f\"{'='*85}\")\n",
    "print(f\"\\n{'Condition':<25} {'d vs Bare':>10} {'Win%':>7} {'p':>12}\")\n",
    "print(\"-\" * 60)\n",
    "all_vs_bare = {}\n",
    "for cname in CONDITION_NAMES:\n",
    "    if cname == 'bare':\n",
    "        continue\n",
    "    delta = c['bare'] - c[cname]\n",
    "    d = cohens_d(delta)\n",
    "    win = np.mean(delta > 0) * 100\n",
    "    _, p_val = stats.ttest_1samp(delta, 0)\n",
    "    sig = \"***\" if p_val < 0.001 else \"**\" if p_val < BONFERRONI_ALPHA else \"*\" if p_val < 0.05 else \"ns\"\n",
    "    print(f\"{cname:<25} {d:>10.3f} {win:>6.1f}% {p_val:>11.2e} {sig:>5}\")\n",
    "    all_vs_bare[cname] = {'cohens_d': float(d), 'win_rate': float(win/100), 'p_value': float(p_val)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e54d2b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-10T05:53:44.610785Z",
     "iopub.status.busy": "2026-02-10T05:53:44.610050Z",
     "iopub.status.idle": "2026-02-10T05:53:44.632758Z",
     "shell.execute_reply": "2026-02-10T05:53:44.632079Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "MECHANISM ISOLATION DEEP-DIVE\n",
      "======================================================================\n",
      "\n",
      "Key/Value Decomposition:\n",
      "  Full LLM-trunc (keys+values): d = +0.254\n",
      "  Values-only (bare keys):      d = +0.275 (108% of full)\n",
      "  Keys-only (bare values):       d = -0.009 (-4% of full)\n",
      "  Sum of parts:                  d = +0.266 (vs full: +0.254)\n",
      "\n",
      "→ VALUES carry the priming signal. Keys contribute negligibly.\n",
      "\n",
      "======================================================================\n",
      "PREFIX AMPLIFICATION CURVE\n",
      "======================================================================\n",
      "\n",
      "Amplification curve (d vs bare):\n",
      "  1× prefix: d = +0.254\n",
      "  3× prefix: d = +0.245 (Δd from 1x = -0.009)\n",
      "  5× prefix: d = +0.208 (Δd from 1x = -0.046)\n",
      "  3x→5x marginal: Δd = -0.037\n",
      "\n",
      "→ No amplification effect — repetition does NOT help.\n",
      "\n",
      "======================================================================\n",
      "HARDNESS QUINTILE BREAKDOWN\n",
      "======================================================================\n",
      "\n",
      "Condition                     Q1 (easy)            Q2            Q3            Q4     Q5 (hard)       Overall\n",
      "-------------------------------------------------------------------------------------------------------------\n",
      "llm_kw_trunc                     -0.030        +0.192        +0.280        +0.530        +0.342        +0.254\n",
      "primed_values_only               -0.026        +0.172        +0.328        +0.623        +0.376        +0.275\n",
      "primed_keys_only                 -0.271        -0.287        -0.100        +0.095        +0.050        -0.009\n",
      "prefix_1x                        -0.030        +0.192        +0.280        +0.530        +0.342        +0.254\n",
      "prefix_3x                        -0.046        +0.184        +0.297        +0.554        +0.270        +0.245\n",
      "prefix_5x                        -0.069        +0.181        +0.224        +0.484        +0.223        +0.208\n",
      "llm_kw_sep                       -0.349        +0.008        +0.152        +0.524        +0.595        +0.288\n",
      "separator_only                   -0.382        -0.070        +0.134        +0.271        +0.498        +0.214\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Mechanism isolation deep-dive + amplification analysis\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"MECHANISM ISOLATION DEEP-DIVE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "d_values_only = cohens_d(c['bare'] - c['primed_values_only'])\n",
    "d_keys_only = cohens_d(c['bare'] - c['primed_keys_only'])\n",
    "d_full_trunc = cohens_d(c['bare'] - c['llm_kw_trunc'])\n",
    "\n",
    "print(f\"\\nKey/Value Decomposition:\")\n",
    "print(f\"  Full LLM-trunc (keys+values): d = {d_full_trunc:+.3f}\")\n",
    "print(f\"  Values-only (bare keys):      d = {d_values_only:+.3f} ({d_values_only/d_full_trunc*100:.0f}% of full)\")\n",
    "print(f\"  Keys-only (bare values):       d = {d_keys_only:+.3f} ({d_keys_only/d_full_trunc*100:.0f}% of full)\")\n",
    "print(f\"  Sum of parts:                  d = {d_values_only + d_keys_only:+.3f} (vs full: {d_full_trunc:+.3f})\")\n",
    "\n",
    "if d_values_only > d_keys_only and d_values_only > 0:\n",
    "    if d_keys_only < 0.05:\n",
    "        print(f\"\\n→ VALUES carry the priming signal. Keys contribute negligibly.\")\n",
    "    else:\n",
    "        print(f\"\\n→ BOTH contribute, but values carry more ({d_values_only/d_full_trunc*100:.0f}% vs {d_keys_only/d_full_trunc*100:.0f}%).\")\n",
    "elif d_keys_only > d_values_only and d_keys_only > 0:\n",
    "    print(f\"\\n→ KEYS carry the priming signal (unexpected — RoPE correction may be key).\")\n",
    "else:\n",
    "    print(f\"\\n→ NEITHER component alone matches the combined effect — synergy required.\")\n",
    "\n",
    "# Amplification curve\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"PREFIX AMPLIFICATION CURVE\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "d_1x = cohens_d(c['bare'] - c['prefix_1x'])\n",
    "d_3x = cohens_d(c['bare'] - c['prefix_3x'])\n",
    "d_5x = cohens_d(c['bare'] - c['prefix_5x'])\n",
    "\n",
    "print(f\"\\nAmplification curve (d vs bare):\")\n",
    "print(f\"  1× prefix: d = {d_1x:+.3f}\")\n",
    "print(f\"  3× prefix: d = {d_3x:+.3f} (Δd from 1x = {d_3x - d_1x:+.3f})\")\n",
    "print(f\"  5× prefix: d = {d_5x:+.3f} (Δd from 1x = {d_5x - d_1x:+.3f})\")\n",
    "print(f\"  3x→5x marginal: Δd = {d_5x - d_3x:+.3f}\")\n",
    "\n",
    "if d_5x > d_3x > d_1x:\n",
    "    if (d_5x - d_3x) < (d_3x - d_1x) * 0.5:\n",
    "        print(f\"\\n→ Amplification works but with DIMINISHING RETURNS.\")\n",
    "    else:\n",
    "        print(f\"\\n→ Amplification works with roughly LINEAR scaling.\")\n",
    "elif d_3x > d_1x and d_5x <= d_3x:\n",
    "    print(f\"\\n→ Amplification helps up to 3× but SATURATES at 5×.\")\n",
    "elif d_3x <= d_1x:\n",
    "    print(f\"\\n→ No amplification effect — repetition does NOT help.\")\n",
    "\n",
    "# Hardness quintile for mechanism conditions\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"HARDNESS QUINTILE BREAKDOWN\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "bare_valid = c['bare']\n",
    "quintile_boundaries = np.percentile(bare_valid, [20, 40, 60, 80])\n",
    "quintile_labels = ['Q1 (easy)', 'Q2', 'Q3', 'Q4', 'Q5 (hard)']\n",
    "\n",
    "def get_quintile(nll, boundaries):\n",
    "    for i, b in enumerate(boundaries):\n",
    "        if nll <= b:\n",
    "            return i\n",
    "    return len(boundaries)\n",
    "\n",
    "quintiles = np.array([get_quintile(nll, quintile_boundaries) for nll in bare_valid])\n",
    "\n",
    "mechanism_conds = ['llm_kw_trunc', 'primed_values_only', 'primed_keys_only',\n",
    "                   'prefix_1x', 'prefix_3x', 'prefix_5x', 'llm_kw_sep', 'separator_only']\n",
    "\n",
    "header = f\"{'Condition':<25}\" + \"\".join(f\"{ql:>14}\" for ql in quintile_labels) + f\"{'Overall':>14}\"\n",
    "print(f\"\\n{header}\")\n",
    "print(\"-\" * (25 + 14 * 6))\n",
    "\n",
    "hardness_breakdown = {}\n",
    "for cname in mechanism_conds:\n",
    "    row = f\"{cname:<25}\"\n",
    "    quintile_ds = []\n",
    "    for q in range(5):\n",
    "        mask_q = quintiles == q\n",
    "        if np.sum(mask_q) < 10:\n",
    "            row += f\"{'n/a':>14}\"\n",
    "            quintile_ds.append(None)\n",
    "        else:\n",
    "            delta = bare_valid[mask_q] - c[cname][mask_q]\n",
    "            d = cohens_d(delta)\n",
    "            row += f\"{d:>+14.3f}\"\n",
    "            quintile_ds.append(float(d))\n",
    "    d_all = cohens_d(bare_valid - c[cname])\n",
    "    row += f\"{d_all:>+14.3f}\"\n",
    "    print(row)\n",
    "    hardness_breakdown[cname] = {'quintile_ds': quintile_ds, 'overall_d': float(d_all)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f73fd42a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-10T05:53:44.635841Z",
     "iopub.status.busy": "2026-02-10T05:53:44.635473Z",
     "iopub.status.idle": "2026-02-10T05:53:46.278428Z",
     "shell.execute_reply": "2026-02-10T05:53:46.277558Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plots saved to results/exp08/analysis_plots.png\n"
     ]
    }
   ],
   "source": [
    "# Cell 10: Plots\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "\n",
    "# --- Plot 1: All conditions bar chart ---\n",
    "ax = axes[0, 0]\n",
    "cnames_sorted = sorted(\n",
    "    [cn for cn in CONDITION_NAMES if cn != 'bare'],\n",
    "    key=lambda cn: cohens_d(c['bare'] - c[cn]),\n",
    "    reverse=True\n",
    ")\n",
    "ds_bar = [cohens_d(c['bare'] - c[cn]) for cn in cnames_sorted]\n",
    "color_map = {\n",
    "    'llm_kw_trunc': 'forestgreen', 'llm_kw_suffix': 'limegreen',\n",
    "    'llm_kw_sep': 'gold', 'primed_values_only': 'steelblue',\n",
    "    'primed_keys_only': 'cornflowerblue', 'prefix_1x': 'mediumpurple',\n",
    "    'prefix_3x': 'darkorchid', 'prefix_5x': 'purple',\n",
    "    'separator_only': 'salmon',\n",
    "}\n",
    "colors_bar = [color_map.get(cn, 'lightgray') for cn in cnames_sorted]\n",
    "ax.barh(range(len(cnames_sorted)), ds_bar, color=colors_bar, edgecolor='black', linewidth=0.5)\n",
    "ax.set_yticks(range(len(cnames_sorted)))\n",
    "ax.set_yticklabels(cnames_sorted, fontsize=8)\n",
    "ax.axvline(x=0, color='gray', linestyle='--')\n",
    "ax.set_xlabel(\"Cohen's d vs Bare\")\n",
    "ax.set_title('All Conditions vs Bare')\n",
    "ax.invert_yaxis()\n",
    "\n",
    "# --- Plot 2: Key/Value decomposition ---\n",
    "ax = axes[0, 1]\n",
    "decomp_labels = ['Full\\n(K+V)', 'Values\\nonly', 'Keys\\nonly', 'Sum\\n(V+K)']\n",
    "decomp_vals = [d_full_trunc, d_values_only, d_keys_only, d_values_only + d_keys_only]\n",
    "decomp_colors = ['forestgreen', 'steelblue', 'cornflowerblue', 'lightgray']\n",
    "bars = ax.bar(range(len(decomp_labels)), decomp_vals, color=decomp_colors,\n",
    "              edgecolor='black', linewidth=0.5)\n",
    "ax.set_xticks(range(len(decomp_labels)))\n",
    "ax.set_xticklabels(decomp_labels, fontsize=9)\n",
    "ax.axhline(y=0, color='gray', linestyle='--')\n",
    "ax.set_ylabel(\"Cohen's d vs Bare\")\n",
    "ax.set_title('Key vs Value Decomposition')\n",
    "for i, v in enumerate(decomp_vals):\n",
    "    ax.text(i, v + 0.005, f\"{v:+.3f}\", ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# --- Plot 3: Amplification curve ---\n",
    "ax = axes[1, 0]\n",
    "reps = [1, 3, 5]\n",
    "amp_ds = [d_1x, d_3x, d_5x]\n",
    "ax.plot(reps, amp_ds, 'o-', color='purple', linewidth=2, markersize=8)\n",
    "ax.set_xlabel('Number of Prefix Repetitions')\n",
    "ax.set_ylabel(\"Cohen's d vs Bare\")\n",
    "ax.set_title('Prefix Amplification Curve')\n",
    "ax.set_xticks(reps)\n",
    "ax.grid(True, alpha=0.3)\n",
    "for x, y in zip(reps, amp_ds):\n",
    "    ax.annotate(f\"d={y:+.3f}\", (x, y), textcoords=\"offset points\",\n",
    "                xytext=(0, 10), ha='center', fontsize=9)\n",
    "\n",
    "# --- Plot 4: Hardness × mechanism heatmap ---\n",
    "ax = axes[1, 1]\n",
    "hm_data = []\n",
    "for cname in mechanism_conds:\n",
    "    row = []\n",
    "    for q in range(5):\n",
    "        mask_q = quintiles == q\n",
    "        if np.sum(mask_q) < 10:\n",
    "            row.append(0)\n",
    "        else:\n",
    "            delta = bare_valid[mask_q] - c[cname][mask_q]\n",
    "            row.append(cohens_d(delta))\n",
    "    hm_data.append(row)\n",
    "hm_data = np.array(hm_data)\n",
    "im = ax.imshow(hm_data, cmap='RdBu_r', vmin=-0.5, vmax=0.7, aspect='auto')\n",
    "ax.set_xticks(range(5))\n",
    "ax.set_xticklabels(quintile_labels, fontsize=7)\n",
    "ax.set_yticks(range(len(mechanism_conds)))\n",
    "ax.set_yticklabels(mechanism_conds, fontsize=7)\n",
    "for i in range(len(mechanism_conds)):\n",
    "    for j in range(5):\n",
    "        ax.text(j, i, f\"{hm_data[i,j]:+.2f}\", ha='center', va='center', fontsize=6)\n",
    "plt.colorbar(im, ax=ax, label=\"Cohen's d vs bare\")\n",
    "ax.set_title('Hardness × Mechanism')\n",
    "\n",
    "plt.suptitle('Exp 08: Mechanism Isolation + Prefix Amplification', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'analysis_plots.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"Plots saved to {RESULTS_DIR / 'analysis_plots.png'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d643695b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-10T05:53:46.282079Z",
     "iopub.status.busy": "2026-02-10T05:53:46.281426Z",
     "iopub.status.idle": "2026-02-10T05:53:46.320018Z",
     "shell.execute_reply": "2026-02-10T05:53:46.319181Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to results/exp08/results.json\n",
      "File size: 462.9 KB\n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Cell 11: Save comprehensive results JSON\n",
    "\n",
    "final = {\n",
    "    'experiment': 'exp08_mechanism_and_amplification',\n",
    "    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'config': {\n",
    "        'model_name': config.model_name,\n",
    "        'seed': SEED,\n",
    "        'n_eval': N,\n",
    "        'n_valid': n_valid,\n",
    "        'n_excluded': n_excluded,\n",
    "        'min_passage_words': config.min_passage_words,\n",
    "        'max_passage_words': config.max_passage_words,\n",
    "        'n_conditions': len(CONDITION_NAMES),\n",
    "        'n_comparisons': N_COMPARISONS,\n",
    "        'bonferroni_alpha': BONFERRONI_ALPHA,\n",
    "    },\n",
    "    'condition_names': CONDITION_NAMES,\n",
    "    'nll_summary': {\n",
    "        cname: {\n",
    "            'mean': float(np.mean(c[cname])),\n",
    "            'std': float(np.std(c[cname])),\n",
    "            'cohens_d_vs_bare': float(cohens_d(c['bare'] - c[cname])) if cname != 'bare' else 0.0,\n",
    "        }\n",
    "        for cname in CONDITION_NAMES\n",
    "    },\n",
    "    'mechanism_decomposition': {\n",
    "        'd_full_trunc': float(d_full_trunc),\n",
    "        'd_values_only': float(d_values_only),\n",
    "        'd_keys_only': float(d_keys_only),\n",
    "        'd_sum_parts': float(d_values_only + d_keys_only),\n",
    "        'values_fraction': float(d_values_only / d_full_trunc) if d_full_trunc != 0 else 0,\n",
    "        'keys_fraction': float(d_keys_only / d_full_trunc) if d_full_trunc != 0 else 0,\n",
    "    },\n",
    "    'amplification': {\n",
    "        'd_1x': float(d_1x),\n",
    "        'd_3x': float(d_3x),\n",
    "        'd_5x': float(d_5x),\n",
    "        'delta_1x_to_3x': float(d_3x - d_1x),\n",
    "        'delta_3x_to_5x': float(d_5x - d_3x),\n",
    "    },\n",
    "    'primary_comparisons': comparison_results,\n",
    "    'all_vs_bare': all_vs_bare,\n",
    "    'hardness_breakdown': hardness_breakdown,\n",
    "    'per_sample_results': results,\n",
    "}\n",
    "\n",
    "with open(FINAL_RESULTS_PATH, 'w') as f:\n",
    "    json.dump(final, f, indent=2)\n",
    "\n",
    "print(f\"Results saved to {FINAL_RESULTS_PATH}\")\n",
    "print(f\"File size: {FINAL_RESULTS_PATH.stat().st_size / 1024:.1f} KB\")\n",
    "print(\"\\nDone!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca3d0054",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-10T05:53:46.323195Z",
     "iopub.status.busy": "2026-02-10T05:53:46.322568Z",
     "iopub.status.idle": "2026-02-10T05:53:46.861864Z",
     "shell.execute_reply": "2026-02-10T05:53:46.861011Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning up GPU memory...\n",
      "GPU memory: 4.16 GB -> 0.02 GB\n",
      "Cleanup complete.\n"
     ]
    }
   ],
   "source": [
    "# Cell 12: GPU cleanup\n",
    "import gc\n",
    "\n",
    "print(\"Cleaning up GPU memory...\")\n",
    "mem_before = torch.cuda.memory_allocated() / 1e9\n",
    "\n",
    "del model\n",
    "del tokenizer\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "mem_after = torch.cuda.memory_allocated() / 1e9\n",
    "print(f\"GPU memory: {mem_before:.2f} GB -> {mem_after:.2f} GB\")\n",
    "print(\"Cleanup complete.\")"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cu124.m127",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/base-cu124:m127"
  },
  "kernelspec": {
   "display_name": "Python 3 (Local)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "003af582d9504aab80048293dc6b99e4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "065178c53c0041ca8899914b28b70a80": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_85e515fc05ae4feabdbfc58355881863",
       "max": 1000,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_78dea523615a42c7a6a442f6ab61a79f",
       "tabbable": null,
       "tooltip": null,
       "value": 1000
      }
     },
     "130550b53ea94dd094c52e742f121915": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "18dce96e20284e9cb38114ec25133379": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "2267c79847ff45dda11f6bd90fd5b9ff": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_4a57f4d3253b4afba4dfcc6d56115d66",
        "IPY_MODEL_9be9ff12dac54ed7aae216489b04fa87",
        "IPY_MODEL_34105779ff28452c9492f57c4ab5f643"
       ],
       "layout": "IPY_MODEL_a76bdb7f8bf341a999d16afb9dad24e3",
       "tabbable": null,
       "tooltip": null
      }
     },
     "2ec919f2e1ed4c0584b829d0f168d8c4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_5144c97d20944630bb38c52ad0dd1b34",
        "IPY_MODEL_b06e29c9c4024eb0ae40d49ddeabb2d0",
        "IPY_MODEL_dd73878c883d4a5584cefdacbc418e64"
       ],
       "layout": "IPY_MODEL_9024a77a98a9455eaa142761791a4f24",
       "tabbable": null,
       "tooltip": null
      }
     },
     "34105779ff28452c9492f57c4ab5f643": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_91ad1ddd499242849eddc6ad9582ff32",
       "placeholder": "​",
       "style": "IPY_MODEL_6286946320054431ba45cd24db3fe81d",
       "tabbable": null,
       "tooltip": null,
       "value": " 1000/1000 [3:30:21&lt;00:00, 11.28s/it]"
      }
     },
     "36aea55aaa6348c285a1299323af75a8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_6743b1c96ac249b89453d8c4148d7705",
       "placeholder": "​",
       "style": "IPY_MODEL_e63c449c5741496897ecde4c341ec9f7",
       "tabbable": null,
       "tooltip": null,
       "value": " 1000/1000 [1:00:06&lt;00:00,  3.61s/it]"
      }
     },
     "402973dde59a4b81b47e43251a1df991": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "4a57f4d3253b4afba4dfcc6d56115d66": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_4d55c141ccee4fb0aaf8e3924d1e5383",
       "placeholder": "​",
       "style": "IPY_MODEL_d00d781186c546ae99d1a7812f746cd5",
       "tabbable": null,
       "tooltip": null,
       "value": "Keyword surrogates: 100%"
      }
     },
     "4d55c141ccee4fb0aaf8e3924d1e5383": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "5091f27edbf44fd29e29056f12ea9d52": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "5144c97d20944630bb38c52ad0dd1b34": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_6c4fd11aba62461484bbe07693f1293c",
       "placeholder": "​",
       "style": "IPY_MODEL_e1dfeefbfc8e4f139c60d5c484f92b1a",
       "tabbable": null,
       "tooltip": null,
       "value": "Filtering:  41%"
      }
     },
     "614602ff3cc54a2b9696b93ed63b75cc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_869643a838b744ef9004d36b96b1c3fc",
        "IPY_MODEL_93acba94f4474241ac554db428f2097d",
        "IPY_MODEL_b69507abc25541caa3e10de1cea72314"
       ],
       "layout": "IPY_MODEL_f32a64b3de6a4e8f9d0f8ab647ac2d5a",
       "tabbable": null,
       "tooltip": null
      }
     },
     "6286946320054431ba45cd24db3fe81d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "63ea4507eb9e4754b536e6efa82ee41e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6743b1c96ac249b89453d8c4148d7705": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6c4fd11aba62461484bbe07693f1293c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "71e019bc61d64357801e09c7de1182ba": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_a76e6eb903a847b9882fbfb4c38089fb",
       "placeholder": "​",
       "style": "IPY_MODEL_18dce96e20284e9cb38114ec25133379",
       "tabbable": null,
       "tooltip": null,
       "value": "Evaluating: 100%"
      }
     },
     "78dea523615a42c7a6a442f6ab61a79f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "85e515fc05ae4feabdbfc58355881863": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "869643a838b744ef9004d36b96b1c3fc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_130550b53ea94dd094c52e742f121915",
       "placeholder": "​",
       "style": "IPY_MODEL_d4475bf16d6c42b985cd1d73f2fc324f",
       "tabbable": null,
       "tooltip": null,
       "value": "Loading weights: 100%"
      }
     },
     "9024a77a98a9455eaa142761791a4f24": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "91ad1ddd499242849eddc6ad9582ff32": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "93acba94f4474241ac554db428f2097d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_c602a2f86b604001b105fddcddfbe7cd",
       "max": 291,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_a88b4d1665ad4b0cbc1c7426580e7b36",
       "tabbable": null,
       "tooltip": null,
       "value": 291
      }
     },
     "9be9ff12dac54ed7aae216489b04fa87": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_003af582d9504aab80048293dc6b99e4",
       "max": 1000,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_5091f27edbf44fd29e29056f12ea9d52",
       "tabbable": null,
       "tooltip": null,
       "value": 1000
      }
     },
     "a6bb1c97e5614cd6b9afe99973cd095e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "a73d7f7a2ae942b3930e4da88b98f6d3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a76bdb7f8bf341a999d16afb9dad24e3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a76e6eb903a847b9882fbfb4c38089fb": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a88b4d1665ad4b0cbc1c7426580e7b36": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "b06e29c9c4024eb0ae40d49ddeabb2d0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "danger",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_a73d7f7a2ae942b3930e4da88b98f6d3",
       "max": 10047,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_402973dde59a4b81b47e43251a1df991",
       "tabbable": null,
       "tooltip": null,
       "value": 4153
      }
     },
     "b69507abc25541caa3e10de1cea72314": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_63ea4507eb9e4754b536e6efa82ee41e",
       "placeholder": "​",
       "style": "IPY_MODEL_a6bb1c97e5614cd6b9afe99973cd095e",
       "tabbable": null,
       "tooltip": null,
       "value": " 291/291 [00:54&lt;00:00,  5.35it/s, Materializing param=model.norm.weight]"
      }
     },
     "b6c9cfc46477455084986a223a618681": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c18ac818f22a4718b0b1682079433cfe": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "c602a2f86b604001b105fddcddfbe7cd": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d00d781186c546ae99d1a7812f746cd5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "d4475bf16d6c42b985cd1d73f2fc324f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "d93b5f11256443488138112080896f7f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_71e019bc61d64357801e09c7de1182ba",
        "IPY_MODEL_065178c53c0041ca8899914b28b70a80",
        "IPY_MODEL_36aea55aaa6348c285a1299323af75a8"
       ],
       "layout": "IPY_MODEL_b6c9cfc46477455084986a223a618681",
       "tabbable": null,
       "tooltip": null
      }
     },
     "dd73878c883d4a5584cefdacbc418e64": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_fe47770e15d7481e86249337983db51c",
       "placeholder": "​",
       "style": "IPY_MODEL_c18ac818f22a4718b0b1682079433cfe",
       "tabbable": null,
       "tooltip": null,
       "value": " 4153/10047 [00:00&lt;00:00, 7155.71it/s]"
      }
     },
     "e1dfeefbfc8e4f139c60d5c484f92b1a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "e63c449c5741496897ecde4c341ec9f7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "f32a64b3de6a4e8f9d0f8ab647ac2d5a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "fe47770e15d7481e86249337983db51c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
