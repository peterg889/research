{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecce3217",
   "metadata": {},
   "source": [
    "# Exp 13: Position-Aware Value Contamination for Long Documents\n",
    "\n",
    "## Background & Motivation\n",
    "\n",
    "Exp 12 tested three hypotheses for why priming fails on long documents:\n",
    "- Signal dilution (repetition) → REFUTED (more reps = worse)\n",
    "- Amplification → Partially promising (amplify_2x d=+0.090, best overall)\n",
    "- RoPE interference → REFUTED (suffix/no_rope both hurt)\n",
    "- Layer targeting (0-15) → Second best (d=+0.083)\n",
    "\n",
    "**But diagnostic analysis of the per-sample data revealed something deeper:**\n",
    "\n",
    "### The Answer Position Finding\n",
    "\n",
    "| Answer Location | % of Samples | static_fact d | Priming Effect |\n",
    "|-----------------|-------------|---------------|----------------|\n",
    "| First 25% of doc | **80%** | **-0.052** | **HURTS** |\n",
    "| Later 75% of doc | 20% | **+0.145** | **HELPS** |\n",
    "\n",
    "The contamination mechanism IS working — it helps when answers are far from\n",
    "the heavily-contaminated early positions. The problem is that **80% of NQ answers\n",
    "sit in the first 25% of the document, exactly where contamination is strongest.**\n",
    "\n",
    "### The Asymmetry Finding\n",
    "\n",
    "The delta distribution has extreme kurtosis (73-227):\n",
    "- Most samples: tiny positive or negative delta (near zero)\n",
    "- A few outliers: catastrophically harmed (Δ = -0.5 to -1.5)\n",
    "- Win rate is 65% but mean is negative because hurt magnitude is 2.16x help magnitude\n",
    "\n",
    "For `layers_0_15`: hurt magnitude is only 0.42x of help magnitude (best ratio).\n",
    "For `amplify_2x`: hurt magnitude is 0.55x of help magnitude.\n",
    "\n",
    "### The Correlation Finding\n",
    "\n",
    "`layers_0_15` and `amplify_2x` have r=0.982 per-sample correlation — they are\n",
    "essentially doing the same thing. This makes sense because the delta at layers\n",
    "16-31 is ~0 (Exp 09), so both operations effectively \"keep delta at layers 0-15.\"\n",
    "\n",
    "## This Experiment: Position-Selective Contamination\n",
    "\n",
    "**Core idea:** Instead of uniform contamination, modulate the contamination delta\n",
    "by position — reduce or eliminate it at early positions (where answers live) and\n",
    "optionally boost it at later positions (where it helps).\n",
    "\n",
    "10 conditions, all derived from just 2 forward passes (bare + primed) per sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "548174b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T16:14:11.630406Z",
     "iopub.status.busy": "2026-02-14T16:14:11.630112Z",
     "iopub.status.idle": "2026-02-14T16:14:16.026809Z",
     "shell.execute_reply": "2026-02-14T16:14:16.025607Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEED: 42\n",
      "Results directory: results/exp13\n",
      "CUDA available: True\n",
      "GPU: NVIDIA L4\n",
      "GPU memory: 23.6 GB\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Setup\n",
    "import os\n",
    "os.umask(0o000)\n",
    "\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import gc\n",
    "from pathlib import Path\n",
    "\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "RESULTS_DIR = Path(\"results/exp13\")\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CHECKPOINT_PATH = RESULTS_DIR / \"checkpoint.json\"\n",
    "FINAL_RESULTS_PATH = RESULTS_DIR / \"results.json\"\n",
    "\n",
    "print(f\"SEED: {SEED}\")\n",
    "print(f\"Results directory: {RESULTS_DIR}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "470f4fca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T16:14:16.031077Z",
     "iopub.status.busy": "2026-02-14T16:14:16.030299Z",
     "iopub.status.idle": "2026-02-14T16:14:57.224867Z",
     "shell.execute_reply": "2026-02-14T16:14:57.223939Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading mistralai/Mistral-7B-Instruct-v0.2 (4-bit)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e569ab8bba8419eaccd9f8ee7d4b7b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/291 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded. dtype=torch.float16, device=cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Load model (Mistral-7B 4-bit)\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, DynamicCache\n",
    "\n",
    "MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "print(f\"Loading {MODEL_NAME} (4-bit)...\")\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "print(f\"Model loaded. dtype={model.dtype}, device={model.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3f679a9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T16:14:57.228417Z",
     "iopub.status.busy": "2026-02-14T16:14:57.227913Z",
     "iopub.status.idle": "2026-02-14T16:14:57.952282Z",
     "shell.execute_reply": "2026-02-14T16:14:57.951397Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config ready\n",
      "  N_CONDITIONS: 10\n",
      "  bonferroni_alpha: 0.0056 (9 comparisons)\n",
      "  static_fact: 'What are the key facts I need to know?'\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Config and library imports\n",
    "sys.path.insert(0, \".\")\n",
    "\n",
    "from lib.config import ExperimentConfig\n",
    "from lib.kv_cache import (\n",
    "    _get_cache_keys,\n",
    "    _get_cache_values,\n",
    "    _set_cache_keys,\n",
    "    _set_cache_values,\n",
    "    _ensure_dynamic_cache,\n",
    "    extract_and_truncate_cache_with_bos,\n",
    "    correct_rope_positions_with_bos,\n",
    "    score_answer_with_cache,\n",
    "    deepcopy_cache,\n",
    "    replace_values_at_layers,\n",
    "    replace_values_at_positions,\n",
    "    interpolate_values,\n",
    "    build_hybrid_cache,\n",
    ")\n",
    "from lib.analysis import cohens_d\n",
    "from lib.surrogate import STATIC_SURROGATE_QUERIES\n",
    "from scipy import stats\n",
    "from scipy.stats import spearmanr\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "config = ExperimentConfig(\n",
    "    model_name=MODEL_NAME,\n",
    "    num_samples=2000,\n",
    "    seed=SEED,\n",
    ")\n",
    "\n",
    "SURROGATE_PREFIX_TEMPLATE = \"{surrogate}\\n\"\n",
    "DOCUMENT_TEMPLATE = \"{document}\"\n",
    "QUERY_TEMPLATE = \"\\nQuery: {query}\\nAnswer:\"\n",
    "ANSWER_TEMPLATE = \" {answer}\"\n",
    "\n",
    "N_CONDITIONS = 10\n",
    "N_COMPARISONS = 9  # each non-bare vs bare\n",
    "BONFERRONI_ALPHA = 0.05 / N_COMPARISONS\n",
    "CHECKPOINT_EVERY = 25\n",
    "DELTA_FORENSICS_EVERY = 50  # log delta diagnostics for every Nth sample\n",
    "\n",
    "STATIC_FACT = STATIC_SURROGATE_QUERIES['static_factual']['query']\n",
    "\n",
    "LENGTH_BINS = [\n",
    "    ('short',     100,  300),\n",
    "    ('medium',    300,  800),\n",
    "    ('long',      800,  2000),\n",
    "    ('very_long', 2000, 4000),\n",
    "]\n",
    "\n",
    "CONDITION_NAMES = [\n",
    "    'bare',\n",
    "    'standard_1x',\n",
    "    'layers_0_15_amp2x',\n",
    "    'layers_0_15_amp3x',\n",
    "    'pos_normalized',\n",
    "    'attenuate_first_25',\n",
    "    'skip_first_25',\n",
    "    'last_50_only',\n",
    "    'window_25_75',\n",
    "    'pos_norm_L0_15',\n",
    "]\n",
    "\n",
    "print(\"Config ready\")\n",
    "print(f\"  N_CONDITIONS: {N_CONDITIONS}\")\n",
    "print(f\"  bonferroni_alpha: {BONFERRONI_ALPHA:.4f} ({N_COMPARISONS} comparisons)\")\n",
    "print(f\"  static_fact: '{STATIC_FACT}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb854a05",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T16:14:57.955975Z",
     "iopub.status.busy": "2026-02-14T16:14:57.955400Z",
     "iopub.status.idle": "2026-02-14T16:14:57.979628Z",
     "shell.execute_reply": "2026-02-14T16:14:57.978631Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "LOADING NATURAL QUESTIONS SAMPLES\n",
      "======================================================================\n",
      "Loaded 315 NQ samples from results/exp12/nq_samples.json\n",
      "\n",
      "Sample distribution:\n",
      "  short (100-300w): n=15, mean=212w\n",
      "  medium (300-800w): n=100, mean=589w\n",
      "  long (800-2000w): n=100, mean=1464w\n",
      "  very_long (2000-4000w): n=100, mean=2895w\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Load NQ samples (reuse exp 12's cached samples)\n",
    "print(\"=\" * 70)\n",
    "print(\"LOADING NATURAL QUESTIONS SAMPLES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "EXP12_SAMPLES_PATH = Path(\"results/exp12/nq_samples.json\")\n",
    "\n",
    "if not EXP12_SAMPLES_PATH.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Exp 12 samples not found at {EXP12_SAMPLES_PATH}. \"\n",
    "        \"Run exp 12 first.\"\n",
    "    )\n",
    "\n",
    "with open(EXP12_SAMPLES_PATH, 'r') as f:\n",
    "    cached = json.load(f)\n",
    "samples = cached['samples']\n",
    "N = len(samples)\n",
    "\n",
    "print(f\"Loaded {N} NQ samples from {EXP12_SAMPLES_PATH}\")\n",
    "print(f\"\\nSample distribution:\")\n",
    "for bin_name, bin_min, bin_max in LENGTH_BINS:\n",
    "    bin_s = [s for s in samples if s['length_bin'] == bin_name]\n",
    "    if bin_s:\n",
    "        wcs = [s['word_count'] for s in bin_s]\n",
    "        print(f\"  {bin_name} ({bin_min}-{bin_max}w): n={len(bin_s)}, \"\n",
    "              f\"mean={np.mean(wcs):.0f}w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7fe91573",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T16:14:57.983642Z",
     "iopub.status.busy": "2026-02-14T16:14:57.983313Z",
     "iopub.status.idle": "2026-02-14T16:14:58.001460Z",
     "shell.execute_reply": "2026-02-14T16:14:58.000546Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "EXPERIMENTAL CONDITIONS — POSITION-AWARE CONTAMINATION\n",
      "======================================================================\n",
      "\n",
      "All conditions use static_fact prefix (11 tokens).\n",
      "All use bare keys. Only VALUES are modified (Exp 08: values carry 100% of signal).\n",
      "All are derived from 2 forward passes: bare + standard 1x primed.\n",
      "\n",
      "Diagram of a 1000-word NQ document (~1500 tokens):\n",
      "\n",
      "Position:     0     25%      50%      75%    100%\n",
      "              |------|--------|--------|-------|\n",
      "standard_1x:  ██████████░░░░░░░░░░░░░░░░░░░░░░  (decay from left)\n",
      "pos_norm:     ████████████████████████████████████ (uniform)\n",
      "atten_25:     ░░████████░░░░░░░░░░░░░░░░░░░░░░  (reduce first 25%)\n",
      "skip_25:      ··████████░░░░░░░░░░░░░░░░░░░░░░  (zero first 25%)\n",
      "last_50:      ··········████████████████████████  (only last 50%)\n",
      "window_25_75: ··████████████████████████··      (middle 50%)\n",
      "\n",
      "█ = full delta  ░ = reduced delta  · = zero delta\n",
      "\n",
      "### 1. bare ###\n",
      "  Baseline — no prefix\n",
      "  Test: —\n",
      "\n",
      "### 2. standard_1x ###\n",
      "  Standard static_fact_trunc. Natural decay: first tokens get ~85% contamination from prefix attention, last tokens get <1%. Replicates exp 12 prefix_1x.\n",
      "  Test: Reference for position-modulated conditions\n",
      "\n",
      "### 3. layers_0_15_amp2x ###\n",
      "  Amplify delta by 2x, but ONLY at layers 0-15 (layers 16-31 get bare values). Combines the two best Exp 12 approaches.\n",
      "  Test: Tests: Do amplification + layer targeting have independent benefits? (r=0.982 in Exp 12 suggests they're nearly identical)\n",
      "\n",
      "### 4. layers_0_15_amp3x ###\n",
      "  Same as above but 3x amplification. Pushes further to find the sweet spot.\n",
      "  Test: Tests: Is 2x undershoot? Does 3x over-amplify?\n",
      "\n",
      "### 5. pos_normalized ###\n",
      "  Normalize the delta at each position to the MEDIAN per-position L2 norm. Early positions (over-contaminated) get reduced. Late positions (under-contaminated) get amplified. Net contamination 'dose' stays the same but distributed evenly.\n",
      "  Test: KEY TEST: If contamination DIRECTION is correct everywhere but MAGNITUDE is wrong, normalization should fix it. This is the cleanest test of 'position-dependent dose.'\n",
      "\n",
      "### 6. attenuate_first_25 ###\n",
      "  Scale delta by 0.25 at first 25% of positions. Keep full delta elsewhere. A gentler version of 'skip' — we know the first 25% has the answer.\n",
      "  Test: Tests: Does REDUCING (not eliminating) early contamination help?\n",
      "\n",
      "### 7. skip_first_25 ###\n",
      "  Set delta to ZERO at first 25% of positions. Full delta elsewhere. Since 80% of answers are in the first 25%, this protects the answer region.\n",
      "  Test: KEY TEST: If early contamination hurts, zeroing it should flip the effect positive.\n",
      "\n",
      "### 8. last_50_only ###\n",
      "  Delta only at positions 50-100% (last half). First half gets bare values. Most aggressive answer-region protection.\n",
      "  Test: Tests: Can contamination help even with zero dose in the answer region?\n",
      "\n",
      "### 9. window_25_75 ###\n",
      "  Delta only at positions 25-75% (middle half). Protects BOTH early (answer) and late (possibly low-quality at extremes) positions.\n",
      "  Test: Tests: Is there an optimal position window for contamination?\n",
      "\n",
      "### 10. pos_norm_L0_15 ###\n",
      "  Position-normalized + layers 0-15 only. The 'kitchen sink' — combines the best layer targeting with position normalization.\n",
      "  Test: Tests: Do position + layer targeting compound?\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Explain experimental conditions\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"EXPERIMENTAL CONDITIONS — POSITION-AWARE CONTAMINATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "sf_str = SURROGATE_PREFIX_TEMPLATE.format(surrogate=STATIC_FACT)\n",
    "sf_ids = tokenizer(sf_str, add_special_tokens=False)['input_ids']\n",
    "sf_tok_len = len(sf_ids)\n",
    "\n",
    "print(f\"\\nAll conditions use static_fact prefix ({sf_tok_len} tokens).\")\n",
    "print(\"All use bare keys. Only VALUES are modified (Exp 08: values carry 100% of signal).\")\n",
    "print(\"All are derived from 2 forward passes: bare + standard 1x primed.\")\n",
    "print(f\"\\nDiagram of a 1000-word NQ document (~1500 tokens):\")\n",
    "print()\n",
    "print(\"Position:     0     25%      50%      75%    100%\")\n",
    "print(\"              |------|--------|--------|-------|\")\n",
    "print(\"standard_1x:  ██████████░░░░░░░░░░░░░░░░░░░░░░  (decay from left)\")\n",
    "print(\"pos_norm:     ████████████████████████████████████ (uniform)\")\n",
    "print(\"atten_25:     ░░████████░░░░░░░░░░░░░░░░░░░░░░  (reduce first 25%)\")\n",
    "print(\"skip_25:      ··████████░░░░░░░░░░░░░░░░░░░░░░  (zero first 25%)\")\n",
    "print(\"last_50:      ··········████████████████████████  (only last 50%)\")\n",
    "print(\"window_25_75: ··████████████████████████··      (middle 50%)\")\n",
    "print()\n",
    "print(\"█ = full delta  ░ = reduced delta  · = zero delta\")\n",
    "\n",
    "conditions_explained = [\n",
    "    (\"1. bare\",\n",
    "     \"Baseline — no prefix\",\n",
    "     \"—\"),\n",
    "    (\"2. standard_1x\",\n",
    "     \"Standard static_fact_trunc. Natural decay: first tokens get ~85% contamination \"\n",
    "     \"from prefix attention, last tokens get <1%. Replicates exp 12 prefix_1x.\",\n",
    "     \"Reference for position-modulated conditions\"),\n",
    "    (\"3. layers_0_15_amp2x\",\n",
    "     \"Amplify delta by 2x, but ONLY at layers 0-15 (layers 16-31 get bare values). \"\n",
    "     \"Combines the two best Exp 12 approaches.\",\n",
    "     \"Tests: Do amplification + layer targeting have independent benefits? \"\n",
    "     \"(r=0.982 in Exp 12 suggests they're nearly identical)\"),\n",
    "    (\"4. layers_0_15_amp3x\",\n",
    "     \"Same as above but 3x amplification. Pushes further to find the sweet spot.\",\n",
    "     \"Tests: Is 2x undershoot? Does 3x over-amplify?\"),\n",
    "    (\"5. pos_normalized\",\n",
    "     \"Normalize the delta at each position to the MEDIAN per-position L2 norm. \"\n",
    "     \"Early positions (over-contaminated) get reduced. Late positions (under-contaminated) \"\n",
    "     \"get amplified. Net contamination 'dose' stays the same but distributed evenly.\",\n",
    "     \"KEY TEST: If contamination DIRECTION is correct everywhere but MAGNITUDE is wrong, \"\n",
    "     \"normalization should fix it. This is the cleanest test of 'position-dependent dose.'\"),\n",
    "    (\"6. attenuate_first_25\",\n",
    "     \"Scale delta by 0.25 at first 25% of positions. Keep full delta elsewhere. \"\n",
    "     \"A gentler version of 'skip' — we know the first 25% has the answer.\",\n",
    "     \"Tests: Does REDUCING (not eliminating) early contamination help?\"),\n",
    "    (\"7. skip_first_25\",\n",
    "     \"Set delta to ZERO at first 25% of positions. Full delta elsewhere. \"\n",
    "     \"Since 80% of answers are in the first 25%, this protects the answer region.\",\n",
    "     \"KEY TEST: If early contamination hurts, zeroing it should flip the effect positive.\"),\n",
    "    (\"8. last_50_only\",\n",
    "     \"Delta only at positions 50-100% (last half). First half gets bare values. \"\n",
    "     \"Most aggressive answer-region protection.\",\n",
    "     \"Tests: Can contamination help even with zero dose in the answer region?\"),\n",
    "    (\"9. window_25_75\",\n",
    "     \"Delta only at positions 25-75% (middle half). Protects BOTH early (answer) \"\n",
    "     \"and late (possibly low-quality at extremes) positions.\",\n",
    "     \"Tests: Is there an optimal position window for contamination?\"),\n",
    "    (\"10. pos_norm_L0_15\",\n",
    "     \"Position-normalized + layers 0-15 only. The 'kitchen sink' — combines the \"\n",
    "     \"best layer targeting with position normalization.\",\n",
    "     \"Tests: Do position + layer targeting compound?\"),\n",
    "]\n",
    "\n",
    "for name, detail, test in conditions_explained:\n",
    "    print(f\"\\n### {name} ###\")\n",
    "    print(f\"  {detail}\")\n",
    "    print(f\"  Test: {test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68266dd7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T16:14:58.004787Z",
     "iopub.status.busy": "2026-02-14T16:14:58.004524Z",
     "iopub.status.idle": "2026-02-14T16:14:58.023317Z",
     "shell.execute_reply": "2026-02-14T16:14:58.022392Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper functions defined:\n",
      "  compute_delta(bare, primed, layers=None) -> dict\n",
      "  apply_delta(bare, deltas, scale, pos_start, pos_end) -> cache\n",
      "  compute_position_norms(deltas) -> tensor\n",
      "  position_normalize_delta(deltas, target_norm) -> (norm_deltas, target)\n",
      "  build_position_scaled_cache(bare, deltas, scale_fn, n_layers) -> cache\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Position-aware cache manipulation functions\n",
    "\n",
    "def compute_delta(bare_cache, primed_cache, layers=None):\n",
    "    \"\"\"Compute per-position value delta between primed and bare caches.\n",
    "\n",
    "    Returns dict mapping layer_idx -> delta tensor (same shape as values).\n",
    "    If layers is specified, only compute for those layers.\n",
    "    \"\"\"\n",
    "    bare_cache = _ensure_dynamic_cache(bare_cache)\n",
    "    primed_cache = _ensure_dynamic_cache(primed_cache)\n",
    "    n_layers = len(bare_cache)\n",
    "\n",
    "    deltas = {}\n",
    "    layer_range = layers if layers is not None else range(n_layers)\n",
    "    for li in layer_range:\n",
    "        v_bare = _get_cache_values(bare_cache, li)\n",
    "        v_primed = _get_cache_values(primed_cache, li)\n",
    "        deltas[li] = v_primed - v_bare\n",
    "\n",
    "    return deltas\n",
    "\n",
    "\n",
    "def apply_delta(bare_cache, deltas, scale=1.0, pos_start=None, pos_end=None):\n",
    "    \"\"\"Apply value delta to bare cache with optional scaling and position range.\n",
    "\n",
    "    Args:\n",
    "        bare_cache: Cache with correct keys and uncontaminated values\n",
    "        deltas: Dict mapping layer_idx -> delta tensor\n",
    "        scale: Scalar or per-position tensor to multiply delta by\n",
    "        pos_start: First position to modify (inclusive, 0-indexed in BOS+doc)\n",
    "        pos_end: Last position to modify (exclusive)\n",
    "\n",
    "    Returns:\n",
    "        New DynamicCache with bare keys and modified values\n",
    "    \"\"\"\n",
    "    bare_cache = _ensure_dynamic_cache(bare_cache)\n",
    "    n_layers = len(bare_cache)\n",
    "    new_cache = DynamicCache()\n",
    "\n",
    "    for li in range(n_layers):\n",
    "        k = _get_cache_keys(bare_cache, li)  # keys never modified — share, don't clone\n",
    "        v = _get_cache_values(bare_cache, li).clone()\n",
    "\n",
    "        if li in deltas:\n",
    "            delta = deltas[li]\n",
    "            if isinstance(scale, torch.Tensor):\n",
    "                # Per-position scaling: scale shape should be (1, 1, seq_len, 1)\n",
    "                # or broadcastable to delta shape\n",
    "                scaled_delta = delta * scale\n",
    "            else:\n",
    "                scaled_delta = delta * scale\n",
    "\n",
    "            if pos_start is not None or pos_end is not None:\n",
    "                ps = pos_start if pos_start is not None else 0\n",
    "                pe = pos_end if pos_end is not None else v.shape[2]\n",
    "                v[:, :, ps:pe, :] += scaled_delta[:, :, ps:pe, :]\n",
    "            else:\n",
    "                v += scaled_delta\n",
    "\n",
    "        new_cache.update(k, v, li)\n",
    "\n",
    "    return new_cache\n",
    "\n",
    "\n",
    "def compute_position_norms(deltas, start_pos=1):\n",
    "    \"\"\"Compute L2 norm of delta at each position (averaged across layers).\n",
    "\n",
    "    Args:\n",
    "        deltas: Dict mapping layer_idx -> delta tensor (1, n_heads, seq_len, head_dim)\n",
    "        start_pos: Skip BOS (position 0)\n",
    "\n",
    "    Returns:\n",
    "        Tensor of shape (seq_len,) with per-position L2 norms averaged across layers\n",
    "    \"\"\"\n",
    "    norms_list = []\n",
    "    for li, delta in deltas.items():\n",
    "        # L2 norm across head_dim, averaged across heads\n",
    "        # delta shape: (1, n_heads, seq_len, head_dim)\n",
    "        pos_norms = torch.norm(delta[0], dim=-1).mean(dim=0)  # (seq_len,)\n",
    "        norms_list.append(pos_norms)\n",
    "\n",
    "    avg_norms = torch.stack(norms_list).mean(dim=0)  # (seq_len,)\n",
    "    return avg_norms\n",
    "\n",
    "\n",
    "def position_normalize_delta(deltas, target_norm=None, start_pos=1, eps=1e-8):\n",
    "    \"\"\"Normalize delta to constant per-position L2 norm.\n",
    "\n",
    "    For each position, scale the delta so its L2 norm equals target_norm.\n",
    "    Default target_norm = median of non-BOS position norms.\n",
    "\n",
    "    Args:\n",
    "        deltas: Dict mapping layer_idx -> delta tensor\n",
    "        target_norm: Target L2 norm per position (default: median)\n",
    "        start_pos: Position to start normalization (skip BOS)\n",
    "        eps: Small constant to avoid division by zero\n",
    "\n",
    "    Returns:\n",
    "        New deltas dict with normalized values, plus the target_norm used\n",
    "    \"\"\"\n",
    "    # Compute per-position norms for each layer\n",
    "    normalized = {}\n",
    "\n",
    "    # First pass: compute all position norms per layer\n",
    "    layer_norms = {}\n",
    "    for li, delta in deltas.items():\n",
    "        # (1, n_heads, seq_len, head_dim) -> per-position norm\n",
    "        # Norm across head_dim for each head, then average across heads\n",
    "        pos_norms = torch.norm(delta[0], dim=-1).mean(dim=0)  # (seq_len,)\n",
    "        layer_norms[li] = pos_norms\n",
    "\n",
    "    # Compute target from average across layers\n",
    "    avg_norms = torch.stack(list(layer_norms.values())).mean(dim=0)\n",
    "    if target_norm is None:\n",
    "        # Use median of doc positions (skip BOS at 0)\n",
    "        target_norm = float(torch.median(avg_norms[start_pos:]).item())\n",
    "\n",
    "    # Second pass: normalize each layer's delta\n",
    "    for li, delta in deltas.items():\n",
    "        pos_norms = layer_norms[li]  # (seq_len,)\n",
    "        # Scale factor per position: target / current_norm\n",
    "        scale = torch.ones_like(pos_norms)\n",
    "        # Only normalize doc positions (not BOS)\n",
    "        for p in range(start_pos, len(pos_norms)):\n",
    "            if pos_norms[p] > eps:\n",
    "                scale[p] = target_norm / pos_norms[p]\n",
    "            else:\n",
    "                scale[p] = 0.0  # Zero delta stays zero\n",
    "\n",
    "        # Reshape for broadcasting: (1, 1, seq_len, 1)\n",
    "        scale_4d = scale.unsqueeze(0).unsqueeze(0).unsqueeze(-1)\n",
    "        normalized[li] = delta * scale_4d.to(delta.device, dtype=delta.dtype)\n",
    "\n",
    "    return normalized, target_norm\n",
    "\n",
    "\n",
    "def build_position_scaled_cache(bare_cache, deltas, scale_fn, n_layers_model):\n",
    "    \"\"\"Build cache with position-dependent scaling applied to deltas.\n",
    "\n",
    "    Args:\n",
    "        bare_cache: Uncontaminated cache\n",
    "        deltas: Dict mapping layer_idx -> delta tensor\n",
    "        scale_fn: Function(position_idx, seq_len) -> scale factor\n",
    "        n_layers_model: Total number of model layers\n",
    "\n",
    "    Returns:\n",
    "        New DynamicCache\n",
    "    \"\"\"\n",
    "    bare_cache = _ensure_dynamic_cache(bare_cache)\n",
    "    new_cache = DynamicCache()\n",
    "\n",
    "    # Pre-compute scale factors\n",
    "    sample_delta = next(iter(deltas.values()))\n",
    "    seq_len = sample_delta.shape[2]\n",
    "    scales = torch.tensor([scale_fn(p, seq_len) for p in range(seq_len)],\n",
    "                          dtype=sample_delta.dtype, device=sample_delta.device)\n",
    "    scales_4d = scales.unsqueeze(0).unsqueeze(0).unsqueeze(-1)  # (1, 1, seq_len, 1)\n",
    "\n",
    "    for li in range(n_layers_model):\n",
    "        k = _get_cache_keys(bare_cache, li)  # keys never modified — share, don't clone\n",
    "        v = _get_cache_values(bare_cache, li).clone()\n",
    "\n",
    "        if li in deltas:\n",
    "            v = v + deltas[li] * scales_4d\n",
    "\n",
    "        new_cache.update(k, v, li)\n",
    "\n",
    "    return new_cache\n",
    "\n",
    "\n",
    "print(\"Helper functions defined:\")\n",
    "print(\"  compute_delta(bare, primed, layers=None) -> dict\")\n",
    "print(\"  apply_delta(bare, deltas, scale, pos_start, pos_end) -> cache\")\n",
    "print(\"  compute_position_norms(deltas) -> tensor\")\n",
    "print(\"  position_normalize_delta(deltas, target_norm) -> (norm_deltas, target)\")\n",
    "print(\"  build_position_scaled_cache(bare, deltas, scale_fn, n_layers) -> cache\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a219ace",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T16:14:58.026746Z",
     "iopub.status.busy": "2026-02-14T16:14:58.026481Z",
     "iopub.status.idle": "2026-02-14T16:21:19.072051Z",
     "shell.execute_reply": "2026-02-14T16:21:19.071013Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "MAIN EVALUATION (10 conditions x 315 samples)\n",
      "======================================================================\n",
      "Static fact prefix: 'What are the key facts I need to know?' (11 tokens)\n",
      "Forward passes per sample: 2 (bare + primed)\n",
      "All other conditions derived via delta manipulation (no extra forward passes)\n",
      "Resuming from checkpoint: 300/315\n",
      "Evaluating samples 300 to 314\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50bc8823c2d6498f879602d1fcf7da3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:  95%|#########5| 300/315 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 315/315 | 0.04 s/s | ETA: 0.0 min\n",
      "\n",
      "Evaluation complete: 315 samples in 6.4 min\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Main evaluation loop — 10 conditions, 2 forward passes per sample\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"MAIN EVALUATION ({N_CONDITIONS} conditions x {N} samples)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Pre-tokenize prefix\n",
    "sf_prefix_str = SURROGATE_PREFIX_TEMPLATE.format(surrogate=STATIC_FACT)\n",
    "sf_prefix_enc = tokenizer(sf_prefix_str, return_tensors=\"pt\",\n",
    "                           add_special_tokens=False, padding=False, truncation=False)\n",
    "sf_prefix_ids = sf_prefix_enc['input_ids'].to(config.device)\n",
    "sf_prefix_len = sf_prefix_ids.shape[1]\n",
    "\n",
    "print(f\"Static fact prefix: '{STATIC_FACT}' ({sf_prefix_len} tokens)\")\n",
    "print(f\"Forward passes per sample: 2 (bare + primed)\")\n",
    "print(f\"All other conditions derived via delta manipulation (no extra forward passes)\")\n",
    "\n",
    "# Checkpoint resume\n",
    "results = []\n",
    "delta_forensics = []\n",
    "start_idx = 0\n",
    "\n",
    "if CHECKPOINT_PATH.exists():\n",
    "    with open(CHECKPOINT_PATH, 'r') as f:\n",
    "        ckpt = json.load(f)\n",
    "    ckpt_queries = ckpt.get('sample_queries', [])\n",
    "    current_queries = [s['query'] for s in samples]\n",
    "    if ckpt_queries == current_queries:\n",
    "        results = ckpt['results']\n",
    "        delta_forensics = ckpt.get('delta_forensics', [])\n",
    "        start_idx = len(results)\n",
    "        print(f\"Resuming from checkpoint: {start_idx}/{N}\")\n",
    "    else:\n",
    "        print(\"Checkpoint sample mismatch. Starting fresh.\")\n",
    "else:\n",
    "    print(\"No checkpoint found. Starting fresh.\")\n",
    "\n",
    "print(f\"Evaluating samples {start_idx} to {N-1}\")\n",
    "n_layers = model.config.num_hidden_layers\n",
    "\n",
    "t_start = time.time()\n",
    "\n",
    "for idx in tqdm(range(start_idx, N), initial=start_idx, total=N, desc=\"Evaluating\"):\n",
    "    sample = samples[idx]\n",
    "    passage = sample['passage']\n",
    "    query = sample['query']\n",
    "    answer = sample['answer']\n",
    "    word_count = sample['word_count']\n",
    "    length_bin = sample['length_bin']\n",
    "\n",
    "    query_prompt = QUERY_TEMPLATE.format(query=query)\n",
    "    answer_text = ANSWER_TEMPLATE.format(answer=answer)\n",
    "\n",
    "    # --- Matched tokenization ---\n",
    "    oracle_prefix = SURROGATE_PREFIX_TEMPLATE.format(surrogate=query)\n",
    "    document_text = DOCUMENT_TEMPLATE.format(document=passage)\n",
    "    full_oracle_text = oracle_prefix + document_text\n",
    "\n",
    "    full_oracle_enc = tokenizer(full_oracle_text, return_tensors=\"pt\",\n",
    "                                add_special_tokens=True, padding=False, truncation=False)\n",
    "    full_oracle_ids = full_oracle_enc['input_ids'].to(config.device)\n",
    "\n",
    "    oracle_prefix_enc = tokenizer(oracle_prefix, return_tensors=\"pt\",\n",
    "                                  add_special_tokens=True, padding=False, truncation=False)\n",
    "    oracle_prefix_len = oracle_prefix_enc['input_ids'].shape[1]\n",
    "\n",
    "    bos_id = full_oracle_ids[:, :1]\n",
    "    doc_ids = full_oracle_ids[:, oracle_prefix_len:]\n",
    "    doc_len = doc_ids.shape[1]\n",
    "    context_len = 1 + doc_len  # BOS + doc\n",
    "\n",
    "    # =================================================================\n",
    "    # PHASE 1: Build bare and primed caches (2 forward passes)\n",
    "    # =================================================================\n",
    "    bare_ids = torch.cat([bos_id, doc_ids], dim=1)\n",
    "    with torch.no_grad():\n",
    "        bare_out = model(input_ids=bare_ids, attention_mask=torch.ones_like(bare_ids),\n",
    "                         use_cache=True, return_dict=True)\n",
    "    bare_cache = _ensure_dynamic_cache(bare_out.past_key_values)\n",
    "    del bare_out\n",
    "\n",
    "    # Use static_fact prefix (matched tokenization: build from sf_prefix + same doc_ids)\n",
    "    primed_ids = torch.cat([bos_id, sf_prefix_ids, doc_ids], dim=1)\n",
    "    with torch.no_grad():\n",
    "        primed_out = model(input_ids=primed_ids,\n",
    "                           attention_mask=torch.ones_like(primed_ids),\n",
    "                           use_cache=True, return_dict=True)\n",
    "    primed_full = _ensure_dynamic_cache(primed_out.past_key_values)\n",
    "    del primed_out\n",
    "\n",
    "    # Truncate + RoPE correct -> standard primed cache\n",
    "    primed_trunc = extract_and_truncate_cache_with_bos(primed_full, doc_len)\n",
    "    correct_rope_positions_with_bos(primed_trunc, sf_prefix_len, model)\n",
    "    del primed_full\n",
    "\n",
    "    del bare_ids, primed_ids\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # =================================================================\n",
    "    # PHASE 2: Compute deltas (all layers, then layer-specific)\n",
    "    # =================================================================\n",
    "    deltas_all = compute_delta(bare_cache, primed_trunc)\n",
    "    deltas_0_15 = {li: deltas_all[li] for li in range(16)}\n",
    "\n",
    "    # =================================================================\n",
    "    # PHASE 3: Build all position-variant caches from deltas\n",
    "    # =================================================================\n",
    "    # Positions: BOS is at 0, doc tokens at 1..doc_len\n",
    "    # \"first 25%\" = positions 1..floor(doc_len*0.25)\n",
    "    p25 = 1 + max(1, int(doc_len * 0.25))  # position index for 25% boundary\n",
    "    p50 = 1 + max(1, int(doc_len * 0.50))\n",
    "    p75 = 1 + max(1, int(doc_len * 0.75))\n",
    "    p_end = 1 + doc_len\n",
    "\n",
    "    # =================================================================\n",
    "    # PHASE 3+4: Build, score, and free each condition one at a time\n",
    "    # (Avoids holding all 9 derived caches in GPU memory simultaneously)\n",
    "    # bare_cache must stay unmutated until all derived caches are scored,\n",
    "    # so we score it LAST.\n",
    "    # =================================================================\n",
    "\n",
    "    # 2. standard_1x\n",
    "    cache = build_hybrid_cache(bare_cache, primed_trunc)\n",
    "    nll_standard = score_answer_with_cache(\n",
    "        cache, context_len, query_prompt, answer_text, model, tokenizer, config)\n",
    "    del cache, primed_trunc\n",
    "\n",
    "    # 3. layers_0_15_amp2x\n",
    "    amp_deltas = {li: d * 2.0 for li, d in deltas_0_15.items()}\n",
    "    cache = apply_delta(bare_cache, amp_deltas)\n",
    "    nll_l15_a2 = score_answer_with_cache(\n",
    "        cache, context_len, query_prompt, answer_text, model, tokenizer, config)\n",
    "    del cache, amp_deltas\n",
    "\n",
    "    # 4. layers_0_15_amp3x\n",
    "    amp_deltas = {li: d * 3.0 for li, d in deltas_0_15.items()}\n",
    "    cache = apply_delta(bare_cache, amp_deltas)\n",
    "    nll_l15_a3 = score_answer_with_cache(\n",
    "        cache, context_len, query_prompt, answer_text, model, tokenizer, config)\n",
    "    del cache, amp_deltas\n",
    "\n",
    "    # 5. pos_normalized (compute norm_deltas just-in-time, free immediately after)\n",
    "    norm_deltas, target_norm = position_normalize_delta(deltas_all)\n",
    "    cache = apply_delta(bare_cache, norm_deltas)\n",
    "    nll_pos_norm = score_answer_with_cache(\n",
    "        cache, context_len, query_prompt, answer_text, model, tokenizer, config)\n",
    "    del cache, norm_deltas\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # 6. attenuate_first_25\n",
    "    def atten_25_scale(pos, seq_len):\n",
    "        if pos == 0:\n",
    "            return 0.0\n",
    "        if pos < p25:\n",
    "            return 0.25\n",
    "        return 1.0\n",
    "    cache = build_position_scaled_cache(bare_cache, deltas_all, atten_25_scale, n_layers)\n",
    "    nll_atten_25 = score_answer_with_cache(\n",
    "        cache, context_len, query_prompt, answer_text, model, tokenizer, config)\n",
    "    del cache\n",
    "\n",
    "    # 7. skip_first_25\n",
    "    def skip_25_scale(pos, seq_len):\n",
    "        if pos < p25:\n",
    "            return 0.0\n",
    "        return 1.0\n",
    "    cache = build_position_scaled_cache(bare_cache, deltas_all, skip_25_scale, n_layers)\n",
    "    nll_skip_25 = score_answer_with_cache(\n",
    "        cache, context_len, query_prompt, answer_text, model, tokenizer, config)\n",
    "    del cache\n",
    "\n",
    "    # 8. last_50_only\n",
    "    def last_50_scale(pos, seq_len):\n",
    "        if pos < p50:\n",
    "            return 0.0\n",
    "        return 1.0\n",
    "    cache = build_position_scaled_cache(bare_cache, deltas_all, last_50_scale, n_layers)\n",
    "    nll_last_50 = score_answer_with_cache(\n",
    "        cache, context_len, query_prompt, answer_text, model, tokenizer, config)\n",
    "    del cache\n",
    "\n",
    "    # 9. window_25_75\n",
    "    def window_scale(pos, seq_len):\n",
    "        if pos < p25 or pos >= p75:\n",
    "            return 0.0\n",
    "        return 1.0\n",
    "    cache = build_position_scaled_cache(bare_cache, deltas_all, window_scale, n_layers)\n",
    "    nll_window = score_answer_with_cache(\n",
    "        cache, context_len, query_prompt, answer_text, model, tokenizer, config)\n",
    "    del cache\n",
    "\n",
    "    # 10. pos_norm_L0_15\n",
    "    norm_deltas_0_15, _ = position_normalize_delta(deltas_0_15, target_norm=target_norm)\n",
    "    cache = apply_delta(bare_cache, norm_deltas_0_15)\n",
    "    nll_pnl15 = score_answer_with_cache(\n",
    "        cache, context_len, query_prompt, answer_text, model, tokenizer, config)\n",
    "    del cache, norm_deltas_0_15\n",
    "\n",
    "    # =================================================================\n",
    "    # Delta forensics (every Nth sample) — reuse existing deltas_all\n",
    "    # =================================================================\n",
    "    if idx % DELTA_FORENSICS_EVERY == 0:\n",
    "        pos_norms = compute_position_norms(deltas_all, start_pos=1)\n",
    "\n",
    "        n_positions = min(20, doc_len)\n",
    "        sample_positions = [1 + int(i * doc_len / n_positions) for i in range(n_positions)]\n",
    "        sample_norms = [float(pos_norms[p].item()) for p in sample_positions if p < len(pos_norms)]\n",
    "        pct_positions = [float((p-1)/doc_len) for p in sample_positions if p < len(pos_norms)]\n",
    "\n",
    "        layer_group_norms = {}\n",
    "        for group_name, layers in [(\"L0-7\", range(8)), (\"L8-15\", range(8,16)),\n",
    "                                    (\"L16-23\", range(16,24)), (\"L24-31\", range(24,32))]:\n",
    "            group_deltas = {li: deltas_all[li] for li in layers if li in deltas_all}\n",
    "            if group_deltas:\n",
    "                gn = compute_position_norms(group_deltas, start_pos=1)\n",
    "                layer_group_norms[group_name] = {\n",
    "                    'mean_norm': float(gn[1:].mean().item()),\n",
    "                    'first_25_norm': float(gn[1:p25].mean().item()) if p25 > 1 else 0,\n",
    "                    'last_25_norm': float(gn[p75:].mean().item()) if p75 < len(gn) else 0,\n",
    "                }\n",
    "\n",
    "        forensic_entry = {\n",
    "            'idx': idx,\n",
    "            'doc_len': doc_len,\n",
    "            'word_count': word_count,\n",
    "            'length_bin': length_bin,\n",
    "            'target_norm': float(target_norm),\n",
    "            'position_norms': sample_norms,\n",
    "            'position_pcts': pct_positions,\n",
    "            'decay_ratio': float(sample_norms[-1] / max(sample_norms[0], 1e-10))\n",
    "                if sample_norms else 0,\n",
    "            'layer_group_norms': layer_group_norms,\n",
    "        }\n",
    "        delta_forensics.append(forensic_entry)\n",
    "\n",
    "    # Clean up deltas, then score bare last (scoring mutates the cache)\n",
    "    del deltas_all, deltas_0_15\n",
    "    nll_bare = score_answer_with_cache(\n",
    "        bare_cache, context_len, query_prompt, answer_text, model, tokenizer, config)\n",
    "    del bare_cache\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # --- Store result ---\n",
    "    # Find answer position for per-sample analysis\n",
    "    ans_pos = None\n",
    "    ans_lower = answer.lower()\n",
    "    pass_lower = passage.lower()\n",
    "    char_idx = pass_lower.find(ans_lower)\n",
    "    if char_idx >= 0:\n",
    "        ans_pos = char_idx / max(len(passage), 1)\n",
    "\n",
    "    result = {\n",
    "        'idx': idx,\n",
    "        'doc_len_tokens': doc_len,\n",
    "        'word_count': word_count,\n",
    "        'length_bin': length_bin,\n",
    "        'answer_position': ans_pos,\n",
    "        'bare': nll_bare,\n",
    "        'standard_1x': nll_standard,\n",
    "        'layers_0_15_amp2x': nll_l15_a2,\n",
    "        'layers_0_15_amp3x': nll_l15_a3,\n",
    "        'pos_normalized': nll_pos_norm,\n",
    "        'attenuate_first_25': nll_atten_25,\n",
    "        'skip_first_25': nll_skip_25,\n",
    "        'last_50_only': nll_last_50,\n",
    "        'window_25_75': nll_window,\n",
    "        'pos_norm_L0_15': nll_pnl15,\n",
    "    }\n",
    "    results.append(result)\n",
    "\n",
    "    if (idx + 1) % CHECKPOINT_EVERY == 0 or idx == N - 1:\n",
    "        ckpt_data = {\n",
    "            'results': results,\n",
    "            'delta_forensics': delta_forensics,\n",
    "            'sample_queries': [s['query'] for s in samples],\n",
    "            'completed': len(results),\n",
    "            'total': N,\n",
    "            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        }\n",
    "        with open(CHECKPOINT_PATH, 'w') as f:\n",
    "            json.dump(ckpt_data, f)\n",
    "        elapsed = time.time() - t_start\n",
    "        rate = (idx - start_idx + 1) / elapsed if elapsed > 0 else 0\n",
    "        remaining = (N - idx - 1) / rate if rate > 0 else 0\n",
    "        tqdm.write(f\"  Checkpoint {idx+1}/{N} | {rate:.2f} s/s | ETA: {remaining/60:.1f} min\")\n",
    "\n",
    "elapsed_total = time.time() - t_start\n",
    "print(f\"\\nEvaluation complete: {len(results)} samples in {elapsed_total/60:.1f} min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8483d3bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T16:21:19.076504Z",
     "iopub.status.busy": "2026-02-14T16:21:19.076228Z",
     "iopub.status.idle": "2026-02-14T16:21:19.593101Z",
     "shell.execute_reply": "2026-02-14T16:21:19.592077Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ANALYSIS — POSITION-AWARE VALUE CONTAMINATION\n",
      "======================================================================\n",
      "Total: 315, Valid: 300, Excluded: 15\n",
      "\n",
      "==========================================================================================\n",
      "OVERALL RESULTS (N=300)\n",
      "==========================================================================================\n",
      "\n",
      "Condition                Mean NLL  d vs Bare    Win%            p   Sig\n",
      "----------------------------------------------------------------------\n",
      "bare                       0.3596         --      --           --\n",
      "standard_1x                0.3477     +0.073  64.3%    2.10e-01    ns\n",
      "layers_0_15_amp2x          0.3301     +0.101  51.3%    8.21e-02    ns\n",
      "layers_0_15_amp3x          0.3212     +0.103  45.7%    7.58e-02    ns\n",
      "pos_normalized             0.3896     -0.051  62.0%    3.78e-01    ns\n",
      "attenuate_first_25         0.3620     -0.030  66.7%    6.04e-01    ns\n",
      "skip_first_25              0.3665     -0.133  64.3%    2.24e-02     *\n",
      "last_50_only               0.3646     -0.128  61.7%    2.76e-02     *\n",
      "window_25_75               0.3628     -0.078  61.7%    1.79e-01    ns\n",
      "pos_norm_L0_15             0.3885     -0.049  60.0%    3.94e-01    ns\n",
      "\n",
      "Best condition: layers_0_15_amp3x vs bare (d=+0.103)\n",
      "\n",
      "==========================================================================================\n",
      "PER LENGTH BIN\n",
      "==========================================================================================\n",
      "\n",
      "  standard_1x:\n",
      "    short: n=14, d=+0.370, win=78.6%, p=1.89e-01 ns\n",
      "    medium: n=92, d=-0.014, win=64.1%, p=8.93e-01 ns\n",
      "    long: n=97, d=+0.061, win=64.9%, p=5.49e-01 ns\n",
      "    very_long: n=97, d=-0.108, win=61.9%, p=2.88e-01 ns\n",
      "\n",
      "  layers_0_15_amp2x:\n",
      "    short: n=14, d=+0.387, win=64.3%, p=1.71e-01 ns\n",
      "    medium: n=92, d=+0.060, win=52.2%, p=5.65e-01 ns\n",
      "    long: n=97, d=+0.120, win=56.7%, p=2.40e-01 ns\n",
      "    very_long: n=97, d=-0.041, win=43.3%, p=6.91e-01 ns\n",
      "\n",
      "  layers_0_15_amp3x:\n",
      "    short: n=14, d=+0.400, win=50.0%, p=1.58e-01 ns\n",
      "    medium: n=92, d=+0.037, win=45.7%, p=7.26e-01 ns\n",
      "    long: n=97, d=+0.130, win=50.5%, p=2.05e-01 ns\n",
      "    very_long: n=97, d=-0.044, win=40.2%, p=6.69e-01 ns\n",
      "\n",
      "  pos_normalized:\n",
      "    short: n=14, d=+0.235, win=50.0%, p=3.96e-01 ns\n",
      "    medium: n=92, d=-0.120, win=65.2%, p=2.54e-01 ns\n",
      "    long: n=97, d=-0.099, win=60.8%, p=3.31e-01 ns\n",
      "    very_long: n=97, d=+0.014, win=61.9%, p=8.92e-01 ns\n",
      "\n",
      "  attenuate_first_25:\n",
      "    short: n=14, d=+0.377, win=92.9%, p=1.81e-01 ns\n",
      "    medium: n=92, d=-0.251, win=59.8%, p=1.81e-02 *\n",
      "    long: n=97, d=-0.276, win=67.0%, p=7.86e-03 *\n",
      "    very_long: n=97, d=-0.183, win=69.1%, p=7.43e-02 ns\n",
      "\n",
      "  skip_first_25:\n",
      "    short: n=14, d=+0.394, win=92.9%, p=1.65e-01 ns\n",
      "    medium: n=92, d=-0.364, win=55.4%, p=7.39e-04 ***\n",
      "    long: n=97, d=-0.288, win=68.0%, p=5.49e-03 **\n",
      "    very_long: n=97, d=-0.204, win=64.9%, p=4.71e-02 *\n",
      "\n",
      "  last_50_only:\n",
      "    short: n=14, d=+0.162, win=71.4%, p=5.55e-01 ns\n",
      "    medium: n=92, d=-0.321, win=53.3%, p=2.74e-03 **\n",
      "    long: n=97, d=-0.279, win=66.0%, p=7.09e-03 *\n",
      "    very_long: n=97, d=-0.113, win=63.9%, p=2.69e-01 ns\n",
      "\n",
      "  window_25_75:\n",
      "    short: n=14, d=+0.410, win=92.9%, p=1.49e-01 ns\n",
      "    medium: n=92, d=-0.261, win=54.3%, p=1.39e-02 *\n",
      "    long: n=97, d=-0.268, win=63.9%, p=9.69e-03 *\n",
      "    very_long: n=97, d=-0.250, win=61.9%, p=1.55e-02 *\n",
      "\n",
      "  pos_norm_L0_15:\n",
      "    short: n=14, d=+0.229, win=42.9%, p=4.07e-01 ns\n",
      "    medium: n=92, d=-0.114, win=63.0%, p=2.78e-01 ns\n",
      "    long: n=97, d=-0.097, win=62.9%, p=3.40e-01 ns\n",
      "    very_long: n=97, d=+0.021, win=56.7%, p=8.34e-01 ns\n",
      "\n",
      "==========================================================================================\n",
      "ANSWER POSITION INTERACTION\n",
      "==========================================================================================\n",
      "\n",
      "Samples with answer position: 233\n",
      "  Early answers (<25%): 185\n",
      "  Late answers (>=25%): 48\n",
      "\n",
      "Condition                 Early d     Late d       Diff\n",
      "-------------------------------------------------------\n",
      "standard_1x                +0.000     +0.177     +0.177\n",
      "layers_0_15_amp2x          +0.032     +0.204     +0.172\n",
      "layers_0_15_amp3x          +0.033     +0.216     +0.183\n",
      "pos_normalized             -0.084     +0.117     +0.201\n",
      "attenuate_first_25         -0.177     +0.136     +0.312\n",
      "skip_first_25              -0.226     +0.086     +0.312\n",
      "last_50_only               -0.164     -0.018     +0.146\n",
      "window_25_75               -0.223     +0.138     +0.361\n",
      "pos_norm_L0_15             -0.083     +0.118     +0.202\n",
      "\n",
      "==========================================================================================\n",
      "ASYMMETRY ANALYSIS — Hurt:Help Magnitude Ratio\n",
      "==========================================================================================\n",
      "\n",
      "Condition                Helped     Hurt   Help mag   Hurt mag   Ratio\n",
      "----------------------------------------------------------------------\n",
      "standard_1x                 193      104    +0.0318    -0.0247   0.78x\n",
      "layers_0_15_amp2x           154      144    +0.0810    -0.0251   0.31x\n",
      "layers_0_15_amp3x           137      159    +0.1250    -0.0352   0.28x\n",
      "pos_normalized              186      106    +0.0276    -0.1332   4.83x\n",
      "attenuate_first_25          200       94    +0.0112    -0.0313   2.79x\n",
      "skip_first_25               193      103    +0.0072    -0.0334   4.65x\n",
      "last_50_only                185      106    +0.0051    -0.0229   4.45x\n",
      "window_25_75                185      109    +0.0066    -0.0199   3.01x\n",
      "pos_norm_L0_15              180      115    +0.0285    -0.1199   4.21x\n",
      "\n",
      "==========================================================================================\n",
      "KEY COMPARISONS: Position-aware vs Standard\n",
      "==========================================================================================\n",
      "\n",
      "Comparison                                 d    Win%            p\n",
      "-----------------------------------------------------------------\n",
      "pos_normalized vs standard_1x         -0.072  50.7%    2.11e-01 ns\n",
      "attenuate_first_25 vs standard_1x     -0.148  47.3%    1.07e-02 *\n",
      "skip_first_25 vs standard_1x          -0.144  45.7%    1.33e-02 *\n",
      "last_50_only vs standard_1x           -0.120  41.3%    3.81e-02 *\n",
      "window_25_75 vs standard_1x           -0.112  37.7%    5.44e-02 ns\n",
      "pos_norm_L0_15 vs standard_1x         -0.071  45.7%    2.22e-01 ns\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Analysis — overall + per-bin + answer position interaction\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ANALYSIS — POSITION-AWARE VALUE CONTAMINATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Arrays\n",
    "cond_arrays = {}\n",
    "for cname in CONDITION_NAMES:\n",
    "    cond_arrays[cname] = np.array([r[cname] for r in results])\n",
    "\n",
    "# Filter zero NLLs\n",
    "valid = np.ones(len(results), dtype=bool)\n",
    "for cname in CONDITION_NAMES:\n",
    "    valid &= (cond_arrays[cname] != 0)\n",
    "n_valid = int(np.sum(valid))\n",
    "n_excluded = int(np.sum(~valid))\n",
    "print(f\"Total: {len(results)}, Valid: {n_valid}, Excluded: {n_excluded}\")\n",
    "\n",
    "c = {}\n",
    "for cname in CONDITION_NAMES:\n",
    "    c[cname] = cond_arrays[cname][valid]\n",
    "\n",
    "length_bins_arr = np.array([r['length_bin'] for r in results])[valid]\n",
    "word_counts_arr = np.array([r['word_count'] for r in results])[valid]\n",
    "answer_pos_arr = np.array([r.get('answer_position', None) for r in results], dtype=object)[valid]\n",
    "\n",
    "# ===== OVERALL RESULTS =====\n",
    "print(f\"\\n{'='*90}\")\n",
    "print(f\"OVERALL RESULTS (N={n_valid})\")\n",
    "print(f\"{'='*90}\")\n",
    "print(f\"\\n{'Condition':<22} {'Mean NLL':>10} {'d vs Bare':>10} {'Win%':>7} {'p':>12} {'Sig':>5}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "nll_summary = {}\n",
    "comparison_results = {}\n",
    "\n",
    "for cname in CONDITION_NAMES:\n",
    "    mean_nll = np.mean(c[cname])\n",
    "    std_nll = np.std(c[cname])\n",
    "\n",
    "    if cname == 'bare':\n",
    "        print(f\"{cname:<22} {mean_nll:>10.4f} {'--':>10} {'--':>7} {'--':>12}\")\n",
    "        nll_summary[cname] = {'mean': float(mean_nll), 'std': float(std_nll), 'cohens_d': 0.0}\n",
    "    else:\n",
    "        delta = c['bare'] - c[cname]\n",
    "        d = cohens_d(delta)\n",
    "        win = np.mean(delta > 0) * 100\n",
    "        _, p_val = stats.ttest_1samp(delta, 0)\n",
    "        sig = \"***\" if p_val < 0.001 else \"**\" if p_val < BONFERRONI_ALPHA else \"*\" if p_val < 0.05 else \"ns\"\n",
    "        print(f\"{cname:<22} {mean_nll:>10.4f} {d:>+10.3f} {win:>5.1f}% {p_val:>11.2e} {sig:>5}\")\n",
    "        nll_summary[cname] = {'mean': float(mean_nll), 'std': float(std_nll), 'cohens_d': float(d)}\n",
    "        comparison_results[f\"{cname} vs bare\"] = {\n",
    "            'mean_delta': float(np.mean(delta)),\n",
    "            'cohens_d': float(d),\n",
    "            'win_rate': float(win / 100),\n",
    "            'p_value': float(p_val),\n",
    "            'bonferroni_significant': bool(p_val < BONFERRONI_ALPHA),\n",
    "        }\n",
    "\n",
    "# Highlight best\n",
    "best_cond = max(comparison_results.items(), key=lambda x: x[1]['cohens_d'])\n",
    "print(f\"\\nBest condition: {best_cond[0]} (d={best_cond[1]['cohens_d']:+.3f})\")\n",
    "\n",
    "# ===== PER LENGTH BIN =====\n",
    "print(f\"\\n{'='*90}\")\n",
    "print(\"PER LENGTH BIN\")\n",
    "print(f\"{'='*90}\")\n",
    "\n",
    "bin_names_ordered = [name for name, _, _ in LENGTH_BINS]\n",
    "per_bin_results = {}\n",
    "\n",
    "for cname in CONDITION_NAMES:\n",
    "    if cname == 'bare':\n",
    "        continue\n",
    "    print(f\"\\n  {cname}:\")\n",
    "    bin_ds = []\n",
    "    bin_wins = []\n",
    "    bin_ns = []\n",
    "    for bin_name in bin_names_ordered:\n",
    "        mask = length_bins_arr == bin_name\n",
    "        n_bin = int(np.sum(mask))\n",
    "        if n_bin < 5:\n",
    "            bin_ds.append(None)\n",
    "            bin_wins.append(None)\n",
    "            bin_ns.append(n_bin)\n",
    "            continue\n",
    "        delta = c['bare'][mask] - c[cname][mask]\n",
    "        d = cohens_d(delta)\n",
    "        win = np.mean(delta > 0) * 100\n",
    "        _, p_val = stats.ttest_1samp(delta, 0)\n",
    "        sig = \"***\" if p_val < 0.001 else \"**\" if p_val < BONFERRONI_ALPHA else \"*\" if p_val < 0.05 else \"ns\"\n",
    "        print(f\"    {bin_name}: n={n_bin}, d={d:+.3f}, win={win:.1f}%, p={p_val:.2e} {sig}\")\n",
    "        bin_ds.append(float(d))\n",
    "        bin_wins.append(float(win))\n",
    "        bin_ns.append(n_bin)\n",
    "    per_bin_results[cname] = {\n",
    "        'bin_names': bin_names_ordered, 'bin_ds': bin_ds, 'bin_wins': bin_wins, 'bin_ns': bin_ns\n",
    "    }\n",
    "\n",
    "# ===== ANSWER POSITION INTERACTION =====\n",
    "print(f\"\\n{'='*90}\")\n",
    "print(\"ANSWER POSITION INTERACTION\")\n",
    "print(f\"{'='*90}\")\n",
    "\n",
    "# Split by answer position\n",
    "ans_pos_valid = np.array([\n",
    "    float(ap) if ap is not None else np.nan for ap in answer_pos_arr\n",
    "])\n",
    "has_ans_pos = ~np.isnan(ans_pos_valid)\n",
    "\n",
    "if np.sum(has_ans_pos) > 20:\n",
    "    print(f\"\\nSamples with answer position: {np.sum(has_ans_pos)}\")\n",
    "    early_mask = has_ans_pos & (ans_pos_valid < 0.25)\n",
    "    late_mask = has_ans_pos & (ans_pos_valid >= 0.25)\n",
    "    print(f\"  Early answers (<25%): {np.sum(early_mask)}\")\n",
    "    print(f\"  Late answers (>=25%): {np.sum(late_mask)}\")\n",
    "\n",
    "    answer_pos_results = {}\n",
    "    print(f\"\\n{'Condition':<22} {'Early d':>10} {'Late d':>10} {'Diff':>10}\")\n",
    "    print(\"-\" * 55)\n",
    "    for cname in CONDITION_NAMES:\n",
    "        if cname == 'bare':\n",
    "            continue\n",
    "        delta = c['bare'] - c[cname]\n",
    "        early_d = cohens_d(delta[early_mask]) if np.sum(early_mask) > 5 else float('nan')\n",
    "        late_d = cohens_d(delta[late_mask]) if np.sum(late_mask) > 5 else float('nan')\n",
    "        diff = late_d - early_d if not (np.isnan(early_d) or np.isnan(late_d)) else float('nan')\n",
    "        if not np.isnan(early_d):\n",
    "            print(f\"{cname:<22} {early_d:>+10.3f} {late_d:>+10.3f} {diff:>+10.3f}\")\n",
    "        answer_pos_results[cname] = {\n",
    "            'early_d': float(early_d) if not np.isnan(early_d) else None,\n",
    "            'late_d': float(late_d) if not np.isnan(late_d) else None,\n",
    "        }\n",
    "\n",
    "# ===== ASYMMETRY ANALYSIS =====\n",
    "print(f\"\\n{'='*90}\")\n",
    "print(\"ASYMMETRY ANALYSIS — Hurt:Help Magnitude Ratio\")\n",
    "print(f\"{'='*90}\")\n",
    "print(f\"\\n{'Condition':<22} {'Helped':>8} {'Hurt':>8} {'Help mag':>10} {'Hurt mag':>10} {'Ratio':>7}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "asymmetry_results = {}\n",
    "for cname in CONDITION_NAMES:\n",
    "    if cname == 'bare':\n",
    "        continue\n",
    "    delta = c['bare'] - c[cname]\n",
    "    helped = delta > 0\n",
    "    hurt = delta < 0\n",
    "    help_mag = np.mean(delta[helped]) if np.any(helped) else 0\n",
    "    hurt_mag = np.mean(delta[hurt]) if np.any(hurt) else 0\n",
    "    ratio = abs(hurt_mag / help_mag) if help_mag != 0 else float('inf')\n",
    "    print(f\"{cname:<22} {np.sum(helped):>8d} {np.sum(hurt):>8d} \"\n",
    "          f\"{help_mag:>+10.4f} {hurt_mag:>+10.4f} {ratio:>6.2f}x\")\n",
    "    asymmetry_results[cname] = {\n",
    "        'n_helped': int(np.sum(helped)),\n",
    "        'n_hurt': int(np.sum(hurt)),\n",
    "        'help_magnitude': float(help_mag),\n",
    "        'hurt_magnitude': float(hurt_mag),\n",
    "        'hurt_help_ratio': float(ratio),\n",
    "    }\n",
    "\n",
    "# ===== KEY COMPARISONS: Position conditions vs standard_1x =====\n",
    "print(f\"\\n{'='*90}\")\n",
    "print(\"KEY COMPARISONS: Position-aware vs Standard\")\n",
    "print(f\"{'='*90}\")\n",
    "\n",
    "position_conditions = ['pos_normalized', 'attenuate_first_25', 'skip_first_25',\n",
    "                        'last_50_only', 'window_25_75', 'pos_norm_L0_15']\n",
    "\n",
    "position_comparisons = {}\n",
    "print(f\"\\n{'Comparison':<35} {'d':>8} {'Win%':>7} {'p':>12}\")\n",
    "print(\"-\" * 65)\n",
    "for cname in position_conditions:\n",
    "    delta = c[cname] - c['standard_1x']  # negative = position version better\n",
    "    d = cohens_d(-delta)  # positive = position version wins\n",
    "    win = np.mean(delta < 0) * 100\n",
    "    _, p_val = stats.ttest_1samp(delta, 0)\n",
    "    sig = \"***\" if p_val < 0.001 else \"**\" if p_val < BONFERRONI_ALPHA else \"*\" if p_val < 0.05 else \"ns\"\n",
    "    print(f\"{cname + ' vs standard_1x':<35} {d:>+8.3f} {win:>5.1f}% {p_val:>11.2e} {sig}\")\n",
    "    position_comparisons[f\"{cname} vs standard_1x\"] = {\n",
    "        'cohens_d': float(d), 'win_rate': float(win/100), 'p_value': float(p_val),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c2ed70c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T16:21:19.597018Z",
     "iopub.status.busy": "2026-02-14T16:21:19.596193Z",
     "iopub.status.idle": "2026-02-14T16:21:19.610384Z",
     "shell.execute_reply": "2026-02-14T16:21:19.609554Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "DELTA FORENSICS — Contamination Profile\n",
      "======================================================================\n",
      "\n",
      "  medium (n=2):\n",
      "    Decay ratio (last/first norm): 0.0921 (range: 0.0718 - 0.1124)\n",
      "    Target norm (median position): 0.469238\n",
      "    L0-7: mean=0.119049, first_25%=0.169922, last_25%=0.101959, ratio=0.6000\n",
      "    L8-15: mean=0.515991, first_25%=0.721924, last_25%=0.420898, ratio=0.5830\n",
      "    L16-23: mean=0.634766, first_25%=0.869385, last_25%=0.523438, ratio=0.6021\n",
      "    L24-31: mean=0.999023, first_25%=1.370117, last_25%=0.879150, ratio=0.6417\n",
      "\n",
      "  long (n=2):\n",
      "    Decay ratio (last/first norm): 0.0697 (range: 0.0582 - 0.0813)\n",
      "    Target norm (median position): 0.388428\n",
      "    L0-7: mean=0.110352, first_25%=0.141235, last_25%=0.102234, ratio=0.7239\n",
      "    L8-15: mean=0.444092, first_25%=0.571533, last_25%=0.387817, ratio=0.6786\n",
      "    L16-23: mean=0.526245, first_25%=0.648438, last_25%=0.462769, ratio=0.7137\n",
      "    L24-31: mean=0.808105, first_25%=1.005127, last_25%=0.761719, ratio=0.7578\n",
      "\n",
      "  very_long (n=2):\n",
      "    Decay ratio (last/first norm): 0.1037 (range: 0.0745 - 0.1329)\n",
      "    Target norm (median position): 0.365356\n",
      "    L0-7: mean=0.108643, first_25%=0.119965, last_25%=0.107117, ratio=0.8929\n",
      "    L8-15: mean=0.409546, first_25%=0.487183, last_25%=0.384277, ratio=0.7888\n",
      "    L16-23: mean=0.468628, first_25%=0.583252, last_25%=0.429932, ratio=0.7371\n",
      "    L24-31: mean=0.764648, first_25%=0.941406, last_25%=0.730713, ratio=0.7762\n",
      "\n",
      "  Average contamination profile (all samples, n=7):\n",
      "        0-  5%: 2.369681  ████████████████████████████████████████\n",
      "        5- 10%: 0.555420  █████████\n",
      "       10- 15%: 0.526611  ████████\n",
      "       15- 20%: 0.685832  ███████████\n",
      "       20- 25%: 0.616414  ██████████\n",
      "       25- 30%: 0.500895  ████████\n",
      "       30- 35%: 0.577218  █████████\n",
      "       35- 40%: 0.526123  ████████\n",
      "       40- 45%: 0.391876  ██████\n",
      "       45- 50%: 0.365560  ██████\n",
      "       50- 55%: 0.417503  ███████\n",
      "       55- 60%: 0.590495  █████████\n",
      "       60- 65%: 0.403259  ██████\n",
      "       65- 70%: 0.470459  ███████\n",
      "       70- 75%: 0.391602  ██████\n",
      "       75- 80%: 0.369415  ██████\n",
      "       80- 85%: 0.431732  ███████\n",
      "       85- 90%: 0.420410  ███████\n",
      "       90- 95%: 0.396694  ██████\n",
      "       95-100%: 0.000000  \n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Delta Forensics — How does contamination decay with position?\n",
    "\n",
    "if delta_forensics:\n",
    "    print(\"=\" * 70)\n",
    "    print(\"DELTA FORENSICS — Contamination Profile\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    # Group by length bin\n",
    "    for bin_name in bin_names_ordered:\n",
    "        bin_forensics = [f for f in delta_forensics if f['length_bin'] == bin_name]\n",
    "        if len(bin_forensics) < 2:\n",
    "            continue\n",
    "\n",
    "        print(f\"\\n  {bin_name} (n={len(bin_forensics)}):\")\n",
    "\n",
    "        # Average decay ratio\n",
    "        decay_ratios = [f['decay_ratio'] for f in bin_forensics]\n",
    "        print(f\"    Decay ratio (last/first norm): {np.mean(decay_ratios):.4f} \"\n",
    "              f\"(range: {min(decay_ratios):.4f} - {max(decay_ratios):.4f})\")\n",
    "\n",
    "        # Average target norm\n",
    "        target_norms = [f['target_norm'] for f in bin_forensics]\n",
    "        print(f\"    Target norm (median position): {np.mean(target_norms):.6f}\")\n",
    "\n",
    "        # Layer group norms\n",
    "        for group in [\"L0-7\", \"L8-15\", \"L16-23\", \"L24-31\"]:\n",
    "            group_means = [f['layer_group_norms'].get(group, {}).get('mean_norm', 0)\n",
    "                          for f in bin_forensics]\n",
    "            first_25_means = [f['layer_group_norms'].get(group, {}).get('first_25_norm', 0)\n",
    "                             for f in bin_forensics]\n",
    "            last_25_means = [f['layer_group_norms'].get(group, {}).get('last_25_norm', 0)\n",
    "                            for f in bin_forensics]\n",
    "            if any(g > 0 for g in group_means):\n",
    "                print(f\"    {group}: mean={np.mean(group_means):.6f}, \"\n",
    "                      f\"first_25%={np.mean(first_25_means):.6f}, \"\n",
    "                      f\"last_25%={np.mean(last_25_means):.6f}, \"\n",
    "                      f\"ratio={np.mean(last_25_means)/max(np.mean(first_25_means), 1e-10):.4f}\")\n",
    "\n",
    "    # Average contamination profile across all samples\n",
    "    print(f\"\\n  Average contamination profile (all samples, n={len(delta_forensics)}):\")\n",
    "    # Normalize to percentage positions\n",
    "    n_bins = 20\n",
    "    pct_bins = np.linspace(0, 1, n_bins + 1)\n",
    "    avg_profile = np.zeros(n_bins)\n",
    "    counts = np.zeros(n_bins)\n",
    "\n",
    "    for f in delta_forensics:\n",
    "        for pct, norm in zip(f['position_pcts'], f['position_norms']):\n",
    "            bin_idx = min(int(pct * n_bins), n_bins - 1)\n",
    "            avg_profile[bin_idx] += norm\n",
    "            counts[bin_idx] += 1\n",
    "\n",
    "    avg_profile = np.divide(avg_profile, counts, where=counts > 0)\n",
    "    for i in range(n_bins):\n",
    "        pct_lo = pct_bins[i] * 100\n",
    "        pct_hi = pct_bins[i+1] * 100\n",
    "        bar = \"█\" * int(avg_profile[i] / max(avg_profile.max(), 1e-10) * 40)\n",
    "        print(f\"    {pct_lo:5.0f}-{pct_hi:3.0f}%: {avg_profile[i]:.6f}  {bar}\")\n",
    "else:\n",
    "    print(\"No delta forensics data collected.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3cfd58af",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T16:21:19.613828Z",
     "iopub.status.busy": "2026-02-14T16:21:19.613521Z",
     "iopub.status.idle": "2026-02-14T16:21:22.956318Z",
     "shell.execute_reply": "2026-02-14T16:21:22.955396Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plots saved to results/exp13/analysis_plots.png\n"
     ]
    }
   ],
   "source": [
    "# Cell 10: Plots\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "\n",
    "colors = {\n",
    "    'standard_1x': '#d62728',\n",
    "    'layers_0_15_amp2x': '#ff7f0e',\n",
    "    'layers_0_15_amp3x': '#e377c2',\n",
    "    'pos_normalized': '#2ca02c',\n",
    "    'attenuate_first_25': '#17becf',\n",
    "    'skip_first_25': '#1f77b4',\n",
    "    'last_50_only': '#9467bd',\n",
    "    'window_25_75': '#8c564b',\n",
    "    'pos_norm_L0_15': '#bcbd22',\n",
    "}\n",
    "\n",
    "# --- Plot 1: Overall bar chart sorted by d ---\n",
    "ax = axes[0, 0]\n",
    "conds_sorted = sorted(\n",
    "    [(cn, cohens_d(c['bare'] - c[cn])) for cn in CONDITION_NAMES if cn != 'bare'],\n",
    "    key=lambda x: x[1], reverse=True\n",
    ")\n",
    "names_s = [x[0] for x in conds_sorted]\n",
    "ds_s = [x[1] for x in conds_sorted]\n",
    "bar_colors = [colors.get(cn, 'gray') for cn in names_s]\n",
    "bars = ax.barh(range(len(names_s)), ds_s, color=bar_colors, edgecolor='black', linewidth=0.5)\n",
    "ax.set_yticks(range(len(names_s)))\n",
    "ax.set_yticklabels(names_s, fontsize=8)\n",
    "for i, (name, dv) in enumerate(conds_sorted):\n",
    "    ax.text(dv + 0.003, i, f\"d={dv:+.3f}\", va='center', fontsize=7)\n",
    "ax.axvline(x=0, color='gray', linestyle='--')\n",
    "ax.axvline(x=0.472, color='red', linestyle=':', alpha=0.4, label='MARCO static_fact')\n",
    "ax.set_xlabel(\"Cohen's d vs Bare\")\n",
    "ax.set_title(\"Overall Effect (All Bins)\")\n",
    "ax.invert_yaxis()\n",
    "ax.legend(fontsize=7)\n",
    "\n",
    "# --- Plot 2: Per-bin for position conditions ---\n",
    "ax = axes[0, 1]\n",
    "x = np.arange(len(bin_names_ordered))\n",
    "width = 0.12\n",
    "plot_conds = ['standard_1x', 'pos_normalized', 'skip_first_25', 'last_50_only', 'pos_norm_L0_15']\n",
    "for i, cname in enumerate(plot_conds):\n",
    "    ds = per_bin_results[cname]['bin_ds']\n",
    "    ds_clean = [d if d is not None else 0 for d in ds]\n",
    "    offset = (i - len(plot_conds)/2 + 0.5) * width\n",
    "    ax.bar(x + offset, ds_clean, width, label=cname, color=colors.get(cname, 'gray'),\n",
    "           edgecolor='black', linewidth=0.3, alpha=0.85)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(bin_names_ordered)\n",
    "ax.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "ax.set_ylabel(\"Cohen's d vs Bare\")\n",
    "ax.set_title(\"Position Conditions by Length Bin\")\n",
    "ax.legend(fontsize=6)\n",
    "\n",
    "# --- Plot 3: Answer position interaction ---\n",
    "ax = axes[0, 2]\n",
    "if np.sum(has_ans_pos) > 20:\n",
    "    for cname in ['standard_1x', 'pos_normalized', 'skip_first_25', 'layers_0_15_amp2x']:\n",
    "        delta = c['bare'] - c[cname]\n",
    "        valid_ap = has_ans_pos\n",
    "        ax.scatter(ans_pos_valid[valid_ap], delta[valid_ap],\n",
    "                  alpha=0.15, s=8, color=colors.get(cname, 'gray'), label=cname)\n",
    "        # Trend line\n",
    "        bins_ap = np.linspace(0, 1, 8)\n",
    "        for k in range(len(bins_ap)-1):\n",
    "            mask_k = valid_ap & (ans_pos_valid >= bins_ap[k]) & (ans_pos_valid < bins_ap[k+1])\n",
    "            if np.sum(mask_k) > 5:\n",
    "                ax.scatter((bins_ap[k]+bins_ap[k+1])/2, np.mean(delta[mask_k]),\n",
    "                          s=50, color=colors.get(cname, 'gray'), edgecolor='black', linewidth=0.5, zorder=5)\n",
    "    ax.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "    ax.set_xlabel(\"Answer Position (0=start, 1=end)\")\n",
    "    ax.set_ylabel(\"NLL Reduction (positive = helps)\")\n",
    "    ax.set_title(\"Answer Position vs Priming Benefit\")\n",
    "    ax.legend(fontsize=7)\n",
    "else:\n",
    "    ax.text(0.5, 0.5, 'Insufficient answer position data', ha='center', transform=ax.transAxes)\n",
    "\n",
    "# --- Plot 4: Asymmetry comparison ---\n",
    "ax = axes[1, 0]\n",
    "asymmetry_conds = sorted(\n",
    "    [(cn, asymmetry_results[cn]['hurt_help_ratio']) for cn in asymmetry_results],\n",
    "    key=lambda x: x[1]\n",
    ")\n",
    "a_names = [x[0] for x in asymmetry_conds]\n",
    "a_ratios = [x[1] for x in asymmetry_conds]\n",
    "a_colors = [colors.get(cn, 'gray') for cn in a_names]\n",
    "ax.barh(range(len(a_names)), a_ratios, color=a_colors, edgecolor='black', linewidth=0.5)\n",
    "ax.set_yticks(range(len(a_names)))\n",
    "ax.set_yticklabels(a_names, fontsize=8)\n",
    "for i, (name, ratio) in enumerate(asymmetry_conds):\n",
    "    ax.text(ratio + 0.02, i, f\"{ratio:.2f}x\", va='center', fontsize=7)\n",
    "ax.axvline(x=1.0, color='red', linestyle='--', alpha=0.5, label='Equal hurt/help')\n",
    "ax.set_xlabel(\"Hurt:Help Magnitude Ratio (lower = better)\")\n",
    "ax.set_title(\"Asymmetry: How Bad is the Hurt Tail?\")\n",
    "ax.invert_yaxis()\n",
    "ax.legend(fontsize=7)\n",
    "\n",
    "# --- Plot 5: Contamination decay profile ---\n",
    "ax = axes[1, 1]\n",
    "if delta_forensics:\n",
    "    for bin_name in bin_names_ordered:\n",
    "        bin_f = [f for f in delta_forensics if f['length_bin'] == bin_name]\n",
    "        if len(bin_f) < 2:\n",
    "            continue\n",
    "        # Average profile\n",
    "        all_pcts = []\n",
    "        all_norms = []\n",
    "        for f in bin_f:\n",
    "            all_pcts.extend(f['position_pcts'])\n",
    "            all_norms.extend(f['position_norms'])\n",
    "        if all_pcts:\n",
    "            # Bin into 20 segments\n",
    "            n_seg = 20\n",
    "            seg_edges = np.linspace(0, 1, n_seg + 1)\n",
    "            seg_means = []\n",
    "            seg_centers = []\n",
    "            for k in range(n_seg):\n",
    "                mask_k = [(p >= seg_edges[k]) and (p < seg_edges[k+1])\n",
    "                         for p in all_pcts]\n",
    "                vals = [n for n, m in zip(all_norms, mask_k) if m]\n",
    "                if vals:\n",
    "                    seg_means.append(np.mean(vals))\n",
    "                    seg_centers.append((seg_edges[k] + seg_edges[k+1])/2)\n",
    "            ax.plot(seg_centers, seg_means, marker='o', markersize=3, label=bin_name)\n",
    "    ax.set_xlabel(\"Position in Document (%)\")\n",
    "    ax.set_ylabel(\"Delta L2 Norm (contamination strength)\")\n",
    "    ax.set_title(\"Contamination Decay by Document Length\")\n",
    "    ax.legend(fontsize=8)\n",
    "    ax.set_yscale('log')\n",
    "else:\n",
    "    ax.text(0.5, 0.5, 'No forensics data', ha='center', transform=ax.transAxes)\n",
    "\n",
    "# --- Plot 6: Key condition comparison per bin (heatmap style) ---\n",
    "ax = axes[1, 2]\n",
    "all_conds = [cn for cn in CONDITION_NAMES if cn != 'bare']\n",
    "heatmap_data = []\n",
    "for cname in all_conds:\n",
    "    row = []\n",
    "    for bin_name in bin_names_ordered:\n",
    "        d = None\n",
    "        if cname in per_bin_results:\n",
    "            bin_idx = bin_names_ordered.index(bin_name)\n",
    "            d = per_bin_results[cname]['bin_ds'][bin_idx]\n",
    "        row.append(d if d is not None else 0)\n",
    "    heatmap_data.append(row)\n",
    "\n",
    "heatmap_data = np.array(heatmap_data)\n",
    "im = ax.imshow(heatmap_data, aspect='auto', cmap='RdYlGn', vmin=-0.3, vmax=0.3)\n",
    "ax.set_xticks(range(len(bin_names_ordered)))\n",
    "ax.set_xticklabels(bin_names_ordered)\n",
    "ax.set_yticks(range(len(all_conds)))\n",
    "ax.set_yticklabels(all_conds, fontsize=8)\n",
    "for i in range(len(all_conds)):\n",
    "    for j in range(len(bin_names_ordered)):\n",
    "        ax.text(j, i, f\"{heatmap_data[i,j]:+.2f}\", ha='center', va='center', fontsize=7)\n",
    "plt.colorbar(im, ax=ax, label=\"Cohen's d vs Bare\")\n",
    "ax.set_title(\"Effect by Condition × Length Bin\")\n",
    "\n",
    "plt.suptitle('Exp 13: Position-Aware Value Contamination', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'analysis_plots.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"Plots saved to {RESULTS_DIR / 'analysis_plots.png'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f170c29b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T16:21:22.960012Z",
     "iopub.status.busy": "2026-02-14T16:21:22.959307Z",
     "iopub.status.idle": "2026-02-14T16:21:22.979726Z",
     "shell.execute_reply": "2026-02-14T16:21:22.978956Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to results/exp13/results.json\n",
      "File size: 205.5 KB\n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Cell 11: Save results\n",
    "final = {\n",
    "    'experiment': 'exp13_position_aware_priming',\n",
    "    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'config': {\n",
    "        'model_name': config.model_name,\n",
    "        'seed': SEED,\n",
    "        'n_eval': N,\n",
    "        'n_valid': n_valid,\n",
    "        'n_excluded': n_excluded,\n",
    "        'n_conditions': N_CONDITIONS,\n",
    "        'n_comparisons': N_COMPARISONS,\n",
    "        'bonferroni_alpha': BONFERRONI_ALPHA,\n",
    "        'dataset': 'Natural Questions (from exp 12 samples)',\n",
    "        'length_bins': LENGTH_BINS,\n",
    "        'static_fact': STATIC_FACT,\n",
    "    },\n",
    "    'condition_names': CONDITION_NAMES,\n",
    "    'nll_summary': nll_summary,\n",
    "    'primary_comparisons': comparison_results,\n",
    "    'position_comparisons': position_comparisons,\n",
    "    'per_bin_results': per_bin_results,\n",
    "    'answer_position_results': answer_pos_results if 'answer_pos_results' in dir() else {},\n",
    "    'asymmetry_results': asymmetry_results,\n",
    "    'delta_forensics_summary': {\n",
    "        'n_samples': len(delta_forensics),\n",
    "        'samples': delta_forensics,\n",
    "    },\n",
    "    'per_sample_results': results,\n",
    "}\n",
    "\n",
    "with open(FINAL_RESULTS_PATH, 'w') as f:\n",
    "    json.dump(final, f, indent=2)\n",
    "\n",
    "print(f\"Results saved to {FINAL_RESULTS_PATH}\")\n",
    "print(f\"File size: {FINAL_RESULTS_PATH.stat().st_size / 1024:.1f} KB\")\n",
    "print(\"\\nDone!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a937766",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T16:21:22.983100Z",
     "iopub.status.busy": "2026-02-14T16:21:22.982820Z",
     "iopub.status.idle": "2026-02-14T16:21:23.634331Z",
     "shell.execute_reply": "2026-02-14T16:21:23.633370Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning up GPU memory...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory: 4.19 GB -> 0.06 GB\n"
     ]
    }
   ],
   "source": [
    "# Cell 12: Cleanup\n",
    "import gc\n",
    "\n",
    "print(\"Cleaning up GPU memory...\")\n",
    "mem_before = torch.cuda.memory_allocated() / 1e9\n",
    "\n",
    "del model\n",
    "del tokenizer\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "mem_after = torch.cuda.memory_allocated() / 1e9\n",
    "print(f\"GPU memory: {mem_before:.2f} GB -> {mem_after:.2f} GB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "01b4ed6d571c4596a8a0794cd52214c9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "07c970ffad7549ec806df641d1c9180e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_44f1be6ab66148efb8d1f62d55f05cf4",
       "placeholder": "​",
       "style": "IPY_MODEL_9eb9df1608c9415bacfd056cfed21799",
       "tabbable": null,
       "tooltip": null,
       "value": " 315/315 [06:21&lt;00:00, 36.40s/it]"
      }
     },
     "0bd0935935cb4b02bca30647c0ec9bc1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_01b4ed6d571c4596a8a0794cd52214c9",
       "placeholder": "​",
       "style": "IPY_MODEL_0e996eaf60de4b0587001eb637b1bd6b",
       "tabbable": null,
       "tooltip": null,
       "value": "Loading weights: 100%"
      }
     },
     "0e996eaf60de4b0587001eb637b1bd6b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "0f2904c9bdb34968947deffba5cd759d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "1a4721c4d0ff4b2ea2a27a08348e10ba": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "1e4b718e63a04d29bbffaca4e63d68f4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "44f1be6ab66148efb8d1f62d55f05cf4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "456c3a02e7ba41c099a1511febc81f70": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "50bc8823c2d6498f879602d1fcf7da3b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_c897d5f059b54bf9a863d3ca1814472f",
        "IPY_MODEL_8e91d96d7e3c4306b5828874d785abc0",
        "IPY_MODEL_07c970ffad7549ec806df641d1c9180e"
       ],
       "layout": "IPY_MODEL_8bba900a74904c00a310cbefe25ea21b",
       "tabbable": null,
       "tooltip": null
      }
     },
     "5e569ab8bba8419eaccd9f8ee7d4b7b9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_0bd0935935cb4b02bca30647c0ec9bc1",
        "IPY_MODEL_7405221f74c44b34ba44caad6986402c",
        "IPY_MODEL_f43b3c35ae374f01b3395b7f6f7a0039"
       ],
       "layout": "IPY_MODEL_1e4b718e63a04d29bbffaca4e63d68f4",
       "tabbable": null,
       "tooltip": null
      }
     },
     "65760f1fcc4845b487a20b1a78aa7195": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6993a93f397b41758f5cc35ade389dd6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "6ccae63616ff459493406cddc023347d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "7405221f74c44b34ba44caad6986402c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_456c3a02e7ba41c099a1511febc81f70",
       "max": 291.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_6993a93f397b41758f5cc35ade389dd6",
       "tabbable": null,
       "tooltip": null,
       "value": 291.0
      }
     },
     "7f04862051a84caaaac1bc81fa07e9f7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "8bba900a74904c00a310cbefe25ea21b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "8e91d96d7e3c4306b5828874d785abc0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_6ccae63616ff459493406cddc023347d",
       "max": 315.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_7f04862051a84caaaac1bc81fa07e9f7",
       "tabbable": null,
       "tooltip": null,
       "value": 315.0
      }
     },
     "9eb9df1608c9415bacfd056cfed21799": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "ad6778744a04496dbc0a2cbd842cfd20": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "c897d5f059b54bf9a863d3ca1814472f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_0f2904c9bdb34968947deffba5cd759d",
       "placeholder": "​",
       "style": "IPY_MODEL_1a4721c4d0ff4b2ea2a27a08348e10ba",
       "tabbable": null,
       "tooltip": null,
       "value": "Evaluating: 100%"
      }
     },
     "f43b3c35ae374f01b3395b7f6f7a0039": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_65760f1fcc4845b487a20b1a78aa7195",
       "placeholder": "​",
       "style": "IPY_MODEL_ad6778744a04496dbc0a2cbd842cfd20",
       "tabbable": null,
       "tooltip": null,
       "value": " 291/291 [00:31&lt;00:00, 13.15it/s, Materializing param=model.norm.weight]"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
