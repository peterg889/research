{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9e36652",
   "metadata": {},
   "source": [
    "# Exp 06: LLM Surrogate Deep-Dive — Mechanism Decomposition\n",
    "\n",
    "## Motivation\n",
    "\n",
    "Exp 05 found LLM-generated keyword surrogates (d=0.37 vs bare) massively outperform oracle\n",
    "queries (d=0.13, indistinguishable from random) on hard MS MARCO. But **WHY?** The LLM\n",
    "surrogates look like \"Cumulonimbus clouds height atmosphere\" — keyword-dense topic phrases\n",
    "with high vocabulary overlap with the passage. Oracle queries are natural-language questions\n",
    "with lower overlap.\n",
    "\n",
    "This experiment decomposes the mechanism through controlled ablations on the **full MS MARCO\n",
    "distribution** (2000 samples, not hardness-filtered).\n",
    "\n",
    "## Hypotheses\n",
    "\n",
    "1. **Token Overlap** — LLM surrogates share tokens with the passage, creating coherent value contamination.\n",
    "2. **Coherence** — Token ORDER matters beyond identity.\n",
    "3. **Format** — Question syntax hurts (question words create unhelpful attention patterns).\n",
    "4. **Passage Specificity** — Document-specific content words help more than generic content words.\n",
    "5. **Mechanism Stacking** — Truncated prefix + separator suffix should combine.\n",
    "\n",
    "## 15 Conditions\n",
    "\n",
    "| # | Condition | Type | Tests |\n",
    "|---|-----------|------|-------|\n",
    "| 1 | Bare | Baseline | — |\n",
    "| 2 | Random-truncated | Control | Structural control |\n",
    "| 3 | Separator-only | Control | Suffix framing |\n",
    "| 4 | Oracle-truncated | Oracle | Semantic control |\n",
    "| 5 | Oracle-as-keywords | Oracle | Format (H3) |\n",
    "| 6 | Anti-keywords | Overlap | Specificity (H4) |\n",
    "| 7 | TF-IDF-keywords | Overlap | Cheap surrogate |\n",
    "| 8 | Passage-echo | Overlap | Overlap ceiling |\n",
    "| 9 | Shuffled-LLM | Overlap | Coherence (H2) |\n",
    "| 10 | LLM-keyword | LLM | Keyword template |\n",
    "| 11 | LLM-question | LLM | Question template |\n",
    "| 12 | LLM-symptom | LLM | Symptom template |\n",
    "| 13 | LLM-summary | LLM | Summary template |\n",
    "| 14 | LLM-keyword+sep | Production | Stacking (H5) |\n",
    "| 15 | LLM-messy | Production | Informal style |\n",
    "\n",
    "## 10 Primary Comparisons (Bonferroni alpha = 0.005)\n",
    "\n",
    "- M1: Shuffled-LLM vs LLM-keyword (coherence)\n",
    "- M2: Oracle-as-keywords vs Oracle (format)\n",
    "- M3: LLM-keyword vs LLM-question (keyword vs question for LLM)\n",
    "- M4: TF-IDF vs Anti-keywords (specificity)\n",
    "- M5: Passage-echo vs LLM-keyword (max overlap ceiling)\n",
    "- M6: TF-IDF vs LLM-keyword (is LLM necessary?)\n",
    "- M7: LLM-keyword+sep vs max(LLM-keyword, sep-only) (stacking)\n",
    "- R1: LLM-keyword vs Random (replicates Exp 05)\n",
    "- R2: Oracle vs Random (replicates null)\n",
    "- R3: Template ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0aedbbfd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-08T15:45:31.363832Z",
     "iopub.status.busy": "2026-02-08T15:45:31.363533Z",
     "iopub.status.idle": "2026-02-08T15:45:34.312393Z",
     "shell.execute_reply": "2026-02-08T15:45:34.311290Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEED: 42\n",
      "Results directory: results/exp06\n",
      "CUDA available: True\n",
      "GPU: NVIDIA L4\n",
      "GPU memory: 23.6 GB\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Setup — permissions, seeds, results directory\n",
    "import os\n",
    "os.umask(0o000)\n",
    "\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "import numpy as np\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "RESULTS_DIR = Path(\"results/exp06\")\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "SURROGATES_DIR = RESULTS_DIR / \"surrogates\"\n",
    "SURROGATES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CHECKPOINT_PATH = RESULTS_DIR / \"checkpoint.json\"\n",
    "FINAL_RESULTS_PATH = RESULTS_DIR / \"results.json\"\n",
    "\n",
    "print(f\"SEED: {SEED}\")\n",
    "print(f\"Results directory: {RESULTS_DIR}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf248122",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-08T15:45:34.316812Z",
     "iopub.status.busy": "2026-02-08T15:45:34.315952Z",
     "iopub.status.idle": "2026-02-08T15:46:38.028188Z",
     "shell.execute_reply": "2026-02-08T15:46:38.027230Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading mistralai/Mistral-7B-Instruct-v0.2 (4-bit)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "469a754b1681482e84b5107edeb889f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/291 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded. dtype=torch.float16, device=cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Load model (Mistral-7B 4-bit)\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "print(f\"Loading {MODEL_NAME} (4-bit)...\")\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "print(f\"Model loaded. dtype={model.dtype}, device={model.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f6ab5e7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-08T15:46:38.032785Z",
     "iopub.status.busy": "2026-02-08T15:46:38.032324Z",
     "iopub.status.idle": "2026-02-08T15:46:38.611498Z",
     "shell.execute_reply": "2026-02-08T15:46:38.610553Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config ready\n",
      "  num_samples pool: 4000\n",
      "  eval samples: 2000\n",
      "  passage words: 20-500\n",
      "  bonferroni_alpha: 0.0050 (10 comparisons)\n",
      "  conditions: 15\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Imports + config + templates + all helper functions\n",
    "sys.path.insert(0, \".\")\n",
    "\n",
    "from lib.config import ExperimentConfig\n",
    "from lib.kv_cache import (\n",
    "    build_kv_cache,\n",
    "    build_suffix_kv_cache,\n",
    "    score_answer_with_cache,\n",
    "    deepcopy_cache,\n",
    "    extract_and_truncate_cache_with_bos,\n",
    "    correct_rope_positions_with_bos,\n",
    ")\n",
    "from lib.data import load_ms_marco, load_evaluation_samples\n",
    "from lib.analysis import cohens_d, compute_token_overlap\n",
    "from lib.surrogate import (\n",
    "    generate_all_5_surrogates,\n",
    "    generate_summary,\n",
    "    TOP_5_SURROGATE_TEMPLATES,\n",
    ")\n",
    "from scipy import stats\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "config = ExperimentConfig(\n",
    "    model_name=MODEL_NAME,\n",
    "    num_samples=4000,\n",
    "    min_passage_words=20,\n",
    "    max_passage_words=500,\n",
    "    seed=SEED,\n",
    ")\n",
    "\n",
    "# Templates — bare text, no \"Document:\\n\" framing (hurts NLL, d=-0.45)\n",
    "SURROGATE_PREFIX_TEMPLATE = \"{surrogate}\\n\"\n",
    "DOCUMENT_TEMPLATE = \"{document}\"\n",
    "QUERY_TEMPLATE = \"\\nQuery: {query}\\nAnswer:\"\n",
    "ANSWER_TEMPLATE = \" {answer}\"\n",
    "SUFFIX_SEPARATOR = \"\\n\\nRelated question: \"\n",
    "CHECKPOINT_EVERY = 50\n",
    "N_COMPARISONS = 10\n",
    "BONFERRONI_ALPHA = 0.05 / N_COMPARISONS\n",
    "N_EVAL = 2000  # number of samples to evaluate\n",
    "\n",
    "# Stopwords for TF-IDF and oracle-as-keywords\n",
    "STOPWORDS = set([\n",
    "    'the', 'a', 'an', 'is', 'are', 'was', 'were', 'be', 'been', 'being',\n",
    "    'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could',\n",
    "    'should', 'may', 'might', 'shall', 'can', 'need', 'dare', 'ought',\n",
    "    'used', 'to', 'of', 'in', 'for', 'on', 'with', 'at', 'by', 'from',\n",
    "    'as', 'into', 'through', 'during', 'before', 'after', 'above', 'below',\n",
    "    'between', 'out', 'off', 'over', 'under', 'again', 'further', 'then',\n",
    "    'once', 'and', 'but', 'or', 'nor', 'not', 'so', 'yet', 'both', 'either',\n",
    "    'neither', 'each', 'every', 'all', 'any', 'few', 'more', 'most', 'other',\n",
    "    'some', 'such', 'no', 'only', 'own', 'same', 'than', 'too', 'very',\n",
    "    'just', 'because', 'about', 'that', 'this', 'these', 'those', 'it',\n",
    "    'its', 'they', 'them', 'their', 'we', 'our', 'you', 'your', 'he', 'him',\n",
    "    'his', 'she', 'her', 'which', 'who', 'whom', 'there', 'here', 'when',\n",
    "    'where', 'why', 'how', 'what', 'if', 'up', 'also', 'well', 'back',\n",
    "    'even', 'still', 'new', 'now', 'way', 'many', 'much', 'like', 'get',\n",
    "    'got', 'make', 'made', 'take', 'come', 'go', 'see', 'know', 'think',\n",
    "])\n",
    "\n",
    "QUESTION_STOPWORDS = STOPWORDS | set([\n",
    "    'what', 'which', 'who', 'whom', 'whose', 'when', 'where', 'why', 'how',\n",
    "    'does', 'did', 'can', 'could', 'would', 'should', 'will', 'shall',\n",
    "    'may', 'might', 'must', 'isn', 'aren', 'wasn', 'weren', 'don', 'doesn',\n",
    "    'didn', 'won', 'wouldn', 'couldn', 'shouldn',\n",
    "])\n",
    "\n",
    "\n",
    "def generate_random_prefix_text(target_text, tokenizer, seed):\n",
    "    # Generate random token text matching the token length of target_text.\n",
    "    target_ids = tokenizer.encode(target_text, add_special_tokens=False)\n",
    "    target_len = len(target_ids)\n",
    "    if target_len == 0:\n",
    "        return \"\"\n",
    "    rng = np.random.RandomState(seed)\n",
    "    vocab_size = len(tokenizer)\n",
    "    min_id = 3\n",
    "    random_ids = rng.randint(min_id, vocab_size, size=target_len)\n",
    "    random_text = tokenizer.decode(random_ids.tolist(), skip_special_tokens=True)\n",
    "    reencoded = tokenizer.encode(random_text, add_special_tokens=False)\n",
    "    if len(reencoded) != target_len:\n",
    "        if len(reencoded) > target_len:\n",
    "            random_text = tokenizer.decode(reencoded[:target_len], skip_special_tokens=True)\n",
    "        else:\n",
    "            extra_needed = target_len - len(reencoded)\n",
    "            extra_ids = rng.randint(min_id, vocab_size, size=extra_needed)\n",
    "            extra_text = tokenizer.decode(extra_ids.tolist(), skip_special_tokens=True)\n",
    "            random_text = random_text + extra_text\n",
    "            reencoded2 = tokenizer.encode(random_text, add_special_tokens=False)\n",
    "            if len(reencoded2) > target_len:\n",
    "                random_text = tokenizer.decode(reencoded2[:target_len], skip_special_tokens=True)\n",
    "    return random_text\n",
    "\n",
    "\n",
    "def extract_tfidf_keywords(passage, n_keywords=8):\n",
    "    # Extract top content words by frequency (stopwords removed).\n",
    "    words = re.findall(r'\\b[a-zA-Z]+\\b', passage.lower())\n",
    "    content_words = [w for w in words if w not in STOPWORDS and len(w) > 2]\n",
    "    return ' '.join([w for w, _ in Counter(content_words).most_common(n_keywords)])\n",
    "\n",
    "\n",
    "def extract_first_sentence(passage, max_words=30):\n",
    "    # Extract first sentence of passage, up to max_words.\n",
    "    first = passage.split('.')[0].strip()\n",
    "    return ' '.join(first.split()[:max_words])\n",
    "\n",
    "\n",
    "def oracle_to_keywords(query):\n",
    "    # Strip question/function words from oracle query.\n",
    "    words = re.findall(r'\\b[a-zA-Z]+\\b', query)\n",
    "    return ' '.join([w for w in words if w.lower() not in QUESTION_STOPWORDS and len(w) > 2])\n",
    "\n",
    "\n",
    "def shuffle_tokens(text, tokenizer, seed):\n",
    "    # Shuffle token IDs of text while preserving token count.\n",
    "    ids = tokenizer.encode(text, add_special_tokens=False)\n",
    "    rng = np.random.RandomState(seed)\n",
    "    rng.shuffle(ids)\n",
    "    return tokenizer.decode(ids, skip_special_tokens=True)\n",
    "\n",
    "\n",
    "print(\"Config ready\")\n",
    "print(f\"  num_samples pool: {config.num_samples}\")\n",
    "print(f\"  eval samples: {N_EVAL}\")\n",
    "print(f\"  passage words: {config.min_passage_words}-{config.max_passage_words}\")\n",
    "print(f\"  bonferroni_alpha: {BONFERRONI_ALPHA:.4f} ({N_COMPARISONS} comparisons)\")\n",
    "print(f\"  conditions: 15\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a853cad3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-08T15:46:38.615362Z",
     "iopub.status.busy": "2026-02-08T15:46:38.614502Z",
     "iopub.status.idle": "2026-02-08T15:46:40.376694Z",
     "shell.execute_reply": "2026-02-08T15:46:40.375731Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`trust_remote_code` is not supported anymore.\n",
      "Please check that the Hugging Face dataset 'microsoft/ms_marco' isn't based on a loading script and remove `trust_remote_code`.\n",
      "If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading microsoft/ms_marco dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded: 10047 samples\n",
      "Filtering samples...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d55381a61eeb4a648e9d3b3b64ffffa1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filtering:   0%|          | 0/10047 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 4000 samples\n",
      "Loaded 4000 candidates, using first 2000 for evaluation\n",
      "Example passage (57 words): Provider Based Billing refers to the billing process for services rendered in a hospital department ...\n",
      "Example query: what is provider based billing mean\n",
      "Example answer: The billing process for services rendered in a hospital department or location.\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Load MS MARCO (2000 samples, full distribution)\n",
    "dataset = load_ms_marco(config)\n",
    "\n",
    "np.random.seed(SEED)\n",
    "all_samples = load_evaluation_samples(dataset, config, require_answer=True)\n",
    "\n",
    "# Take first 2000 for manageable runtime\n",
    "samples = all_samples[:N_EVAL]\n",
    "N = len(samples)\n",
    "print(f\"Loaded {len(all_samples)} candidates, using first {N} for evaluation\")\n",
    "print(f\"Example passage ({len(samples[0]['passage'].split())} words): {samples[0]['passage'][:100]}...\")\n",
    "print(f\"Example query: {samples[0]['query']}\")\n",
    "print(f\"Example answer: {samples[0]['answer']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8f52e54",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-08T15:46:40.380979Z",
     "iopub.status.busy": "2026-02-08T15:46:40.380481Z",
     "iopub.status.idle": "2026-02-09T00:59:47.366300Z",
     "shell.execute_reply": "2026-02-09T00:59:47.365466Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PHASE 1: LLM SURROGATE GENERATION\n",
      "======================================================================\n",
      "Generating 5-template surrogates for samples 0 to 1999...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9fe1a60a73e4455ab83151f6e86dfd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "5-template surrogates:   0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saved 100/2000 | 0.08 s/s | ETA: 402.3 min\n",
      "  Saved 200/2000 | 0.08 s/s | ETA: 381.1 min\n",
      "  Saved 300/2000 | 0.08 s/s | ETA: 359.1 min\n",
      "  Saved 400/2000 | 0.08 s/s | ETA: 337.5 min\n",
      "  Saved 500/2000 | 0.08 s/s | ETA: 317.2 min\n",
      "  Saved 600/2000 | 0.08 s/s | ETA: 295.8 min\n",
      "  Saved 700/2000 | 0.08 s/s | ETA: 274.9 min\n",
      "  Saved 800/2000 | 0.08 s/s | ETA: 253.7 min\n",
      "  Saved 900/2000 | 0.08 s/s | ETA: 232.3 min\n",
      "  Saved 1000/2000 | 0.08 s/s | ETA: 211.3 min\n",
      "  Saved 1100/2000 | 0.08 s/s | ETA: 190.1 min\n",
      "  Saved 1200/2000 | 0.08 s/s | ETA: 168.6 min\n",
      "  Saved 1300/2000 | 0.08 s/s | ETA: 147.5 min\n",
      "  Saved 1400/2000 | 0.08 s/s | ETA: 126.1 min\n",
      "  Saved 1500/2000 | 0.08 s/s | ETA: 104.9 min\n",
      "  Saved 1600/2000 | 0.08 s/s | ETA: 83.8 min\n",
      "  Saved 1700/2000 | 0.08 s/s | ETA: 62.8 min\n",
      "  Saved 1800/2000 | 0.08 s/s | ETA: 41.8 min\n",
      "  Saved 1900/2000 | 0.08 s/s | ETA: 20.9 min\n",
      "  Saved 2000/2000 | 0.08 s/s | ETA: 0.0 min\n",
      "5-template surrogates complete: 2000 samples\n",
      "Generating summaries for samples 0 to 1999...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d19f3af876234d76b4e0967f9c02628b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Summaries:   0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saved 100/2000 | 0.25 s/s | ETA: 125.8 min\n",
      "  Saved 200/2000 | 0.25 s/s | ETA: 119.9 min\n",
      "  Saved 300/2000 | 0.25 s/s | ETA: 113.1 min\n",
      "  Saved 400/2000 | 0.25 s/s | ETA: 107.8 min\n",
      "  Saved 500/2000 | 0.25 s/s | ETA: 101.7 min\n",
      "  Saved 600/2000 | 0.24 s/s | ETA: 95.3 min\n",
      "  Saved 700/2000 | 0.24 s/s | ETA: 88.7 min\n",
      "  Saved 800/2000 | 0.25 s/s | ETA: 81.6 min\n",
      "  Saved 900/2000 | 0.25 s/s | ETA: 74.7 min\n",
      "  Saved 1000/2000 | 0.25 s/s | ETA: 67.9 min\n",
      "  Saved 1100/2000 | 0.24 s/s | ETA: 61.3 min\n",
      "  Saved 1200/2000 | 0.24 s/s | ETA: 54.5 min\n",
      "  Saved 1300/2000 | 0.25 s/s | ETA: 47.6 min\n",
      "  Saved 1400/2000 | 0.24 s/s | ETA: 40.8 min\n",
      "  Saved 1500/2000 | 0.24 s/s | ETA: 34.1 min\n",
      "  Saved 1600/2000 | 0.24 s/s | ETA: 27.3 min\n",
      "  Saved 1700/2000 | 0.24 s/s | ETA: 20.4 min\n",
      "  Saved 1800/2000 | 0.24 s/s | ETA: 13.6 min\n",
      "  Saved 1900/2000 | 0.24 s/s | ETA: 6.8 min\n",
      "  Saved 2000/2000 | 0.24 s/s | ETA: 0.0 min\n",
      "Summaries complete: 2000 samples\n",
      "\n",
      "Validation:\n",
      "  Empty keyword surrogates: 0/2000\n",
      "  Empty question surrogates: 0/2000\n",
      "  Empty summaries: 0/2000\n",
      "\n",
      "Examples (sample 0):\n",
      "  Passage: Provider Based Billing refers to the billing process for services rendered in a ...\n",
      "  target_question: What is Provider Based Billing and how does it work in a hospital setting under Medicare and Medicaid rules?\n",
      "  keyword_query: Provider Based Billing rules, Medicare Medicaid requirements, Hospital billing process, Physician services, Patient care, Space ownership, Support personnel.\n",
      "  symptom_scenario: Hospital-based healthcare services billed by hospital, not insurance company. (Medicare/Medicaid rules)\n",
      "  misconception_negative: Is Provider Based Billing a scam for overcharging patients?\"\n",
      "  messy_realworld: Help me find provider based billing rules medicare medicaid asap plz specific process hospital docs support personnel patient care regulations thanks!\n",
      "  summary: Provider Based Billing is a hospital billing process for services rendered in owned or leased facili...\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Generate ALL LLM surrogates (5 templates + summary, checkpointed)\n",
    "# generate_all_5_surrogates() gives: keyword_query, target_question, symptom_scenario,\n",
    "#     misconception_negative, messy_realworld\n",
    "# generate_summary() gives: 2-sentence summary\n",
    "# Total: 6 LLM calls per sample\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PHASE 1: LLM SURROGATE GENERATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "surrogates_5_path = SURROGATES_DIR / \"all_5_surrogates.json\"\n",
    "summaries_path = SURROGATES_DIR / \"summaries.json\"\n",
    "\n",
    "# --- Load or generate all_5_surrogates ---\n",
    "if surrogates_5_path.exists():\n",
    "    with open(surrogates_5_path, 'r') as f:\n",
    "        surrogates_5_data = json.load(f)\n",
    "    surrogates_5 = surrogates_5_data['surrogates']\n",
    "    print(f\"Loaded {len(surrogates_5)} sets of 5-template surrogates from cache\")\n",
    "else:\n",
    "    surrogates_5 = []\n",
    "\n",
    "start_5 = len(surrogates_5)\n",
    "if start_5 < N:\n",
    "    print(f\"Generating 5-template surrogates for samples {start_5} to {N-1}...\")\n",
    "    t_start = time.time()\n",
    "    for idx in tqdm(range(start_5, N), initial=start_5, total=N,\n",
    "                     desc=\"5-template surrogates\"):\n",
    "        passage = samples[idx]['passage']\n",
    "        try:\n",
    "            s5 = generate_all_5_surrogates(passage, model, tokenizer, config)\n",
    "        except Exception as e:\n",
    "            print(f\"  WARNING: 5-template generation failed for sample {idx}: {e}\")\n",
    "            s5 = {k: \"\" for k in TOP_5_SURROGATE_TEMPLATES.keys()}\n",
    "        surrogates_5.append(s5)\n",
    "\n",
    "        if (idx + 1) % 100 == 0 or idx == N - 1:\n",
    "            with open(surrogates_5_path, 'w') as f:\n",
    "                json.dump({'surrogates': surrogates_5}, f)\n",
    "            elapsed = time.time() - t_start\n",
    "            rate = (idx - start_5 + 1) / elapsed if elapsed > 0 else 0\n",
    "            remaining = (N - idx - 1) / rate if rate > 0 else 0\n",
    "            tqdm.write(f\"  Saved {idx+1}/{N} | {rate:.2f} s/s | ETA: {remaining/60:.1f} min\")\n",
    "\n",
    "    with open(surrogates_5_path, 'w') as f:\n",
    "        json.dump({'surrogates': surrogates_5}, f)\n",
    "    print(f\"5-template surrogates complete: {len(surrogates_5)} samples\")\n",
    "else:\n",
    "    print(f\"All 5-template surrogates already cached ({len(surrogates_5)} samples)\")\n",
    "\n",
    "# --- Load or generate summaries ---\n",
    "if summaries_path.exists():\n",
    "    with open(summaries_path, 'r') as f:\n",
    "        summaries_data = json.load(f)\n",
    "    summaries = summaries_data['summaries']\n",
    "    print(f\"Loaded {len(summaries)} summaries from cache\")\n",
    "else:\n",
    "    summaries = []\n",
    "\n",
    "start_sum = len(summaries)\n",
    "if start_sum < N:\n",
    "    print(f\"Generating summaries for samples {start_sum} to {N-1}...\")\n",
    "    t_start = time.time()\n",
    "    for idx in tqdm(range(start_sum, N), initial=start_sum, total=N, desc=\"Summaries\"):\n",
    "        passage = samples[idx]['passage']\n",
    "        try:\n",
    "            summary = generate_summary(passage, model, tokenizer, config)\n",
    "        except Exception as e:\n",
    "            print(f\"  WARNING: Summary generation failed for sample {idx}: {e}\")\n",
    "            summary = \"\"\n",
    "        summaries.append(summary)\n",
    "\n",
    "        if (idx + 1) % 100 == 0 or idx == N - 1:\n",
    "            with open(summaries_path, 'w') as f:\n",
    "                json.dump({'summaries': summaries}, f)\n",
    "            elapsed = time.time() - t_start\n",
    "            rate = (idx - start_sum + 1) / elapsed if elapsed > 0 else 0\n",
    "            remaining = (N - idx - 1) / rate if rate > 0 else 0\n",
    "            tqdm.write(f\"  Saved {idx+1}/{N} | {rate:.2f} s/s | ETA: {remaining/60:.1f} min\")\n",
    "\n",
    "    with open(summaries_path, 'w') as f:\n",
    "        json.dump({'summaries': summaries}, f)\n",
    "    print(f\"Summaries complete: {len(summaries)} samples\")\n",
    "else:\n",
    "    print(f\"All summaries already cached ({len(summaries)} samples)\")\n",
    "\n",
    "# Validate\n",
    "n_empty_kw = sum(1 for s in surrogates_5 if not s.get('keyword_query', '').strip())\n",
    "n_empty_q = sum(1 for s in surrogates_5 if not s.get('target_question', '').strip())\n",
    "n_empty_sum = sum(1 for s in summaries if not s.strip())\n",
    "print(f\"\\nValidation:\")\n",
    "print(f\"  Empty keyword surrogates: {n_empty_kw}/{N}\")\n",
    "print(f\"  Empty question surrogates: {n_empty_q}/{N}\")\n",
    "print(f\"  Empty summaries: {n_empty_sum}/{N}\")\n",
    "\n",
    "# Show examples\n",
    "print(f\"\\nExamples (sample 0):\")\n",
    "print(f\"  Passage: {samples[0]['passage'][:80]}...\")\n",
    "for k, v in surrogates_5[0].items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "print(f\"  summary: {summaries[0][:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7bff0076",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-09T00:59:47.369960Z",
     "iopub.status.busy": "2026-02-09T00:59:47.369607Z",
     "iopub.status.idle": "2026-02-09T01:00:08.237570Z",
     "shell.execute_reply": "2026-02-09T01:00:08.236716Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PHASE 1b: NON-LLM SURROGATES + TOKEN OVERLAP\n",
      "======================================================================\n",
      "Computing token overlaps...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86693f84ba524134b2560c88c57e63d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Token overlap:   0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Condition                      Mean Overlap        Std\n",
      "-------------------------------------------------------\n",
      "Random prefix                        0.0024     0.0060\n",
      "Oracle query                         0.0847     0.0417\n",
      "Oracle-as-keywords                   0.0619     0.0344\n",
      "Anti-keywords (wrong doc)            0.0037     0.0078\n",
      "TF-IDF keywords (right doc)          0.1861     0.0700\n",
      "Passage echo (1st sentence)          0.2784     0.1934\n",
      "Shuffled LLM                         0.2357     0.1017\n",
      "LLM keyword                          0.2715     0.1180\n",
      "LLM question                         0.2149     0.0850\n",
      "LLM symptom                          0.1558     0.0708\n",
      "LLM summary                          0.4329     0.1148\n",
      "LLM messy                            0.1963     0.0852\n",
      "\n",
      "Overlap data saved to results/exp06/overlap_data.json\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Compute non-LLM surrogates + token overlap diagnostics\n",
    "print(\"=\" * 70)\n",
    "print(\"PHASE 1b: NON-LLM SURROGATES + TOKEN OVERLAP\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Pre-compute all non-LLM surrogates\n",
    "tfidf_keywords = []\n",
    "anti_keywords = []\n",
    "passage_echos = []\n",
    "oracle_as_kw = []\n",
    "shuffled_llm = []\n",
    "\n",
    "for i in range(N):\n",
    "    passage = samples[i]['passage']\n",
    "    query = samples[i]['query']\n",
    "    llm_kw = surrogates_5[i].get('keyword_query', '')\n",
    "\n",
    "    # TF-IDF keywords from THIS passage\n",
    "    tfidf_keywords.append(extract_tfidf_keywords(passage, n_keywords=8))\n",
    "\n",
    "    # Anti-keywords from WRONG passage (offset by 500)\n",
    "    wrong_passage = samples[(i + 500) % N]['passage']\n",
    "    anti_keywords.append(extract_tfidf_keywords(wrong_passage, n_keywords=8))\n",
    "\n",
    "    # Passage echo: first sentence\n",
    "    passage_echos.append(extract_first_sentence(passage, max_words=30))\n",
    "\n",
    "    # Oracle-as-keywords: strip question/function words\n",
    "    oracle_as_kw.append(oracle_to_keywords(query))\n",
    "\n",
    "    # Shuffled-LLM: shuffle token IDs of LLM keyword surrogate\n",
    "    shuffled_llm.append(shuffle_tokens(llm_kw, tokenizer, seed=SEED + i))\n",
    "\n",
    "# Compute token overlaps for ALL non-bare conditions vs passage\n",
    "print(\"Computing token overlaps...\")\n",
    "overlap_data = {}\n",
    "overlap_labels = [\n",
    "    ('random', 'Random prefix'),\n",
    "    ('oracle', 'Oracle query'),\n",
    "    ('oracle_kw', 'Oracle-as-keywords'),\n",
    "    ('anti_kw', 'Anti-keywords (wrong doc)'),\n",
    "    ('tfidf', 'TF-IDF keywords (right doc)'),\n",
    "    ('echo', 'Passage echo (1st sentence)'),\n",
    "    ('shuffled', 'Shuffled LLM'),\n",
    "    ('llm_keyword', 'LLM keyword'),\n",
    "    ('llm_question', 'LLM question'),\n",
    "    ('llm_symptom', 'LLM symptom'),\n",
    "    ('llm_summary', 'LLM summary'),\n",
    "    ('llm_messy', 'LLM messy'),\n",
    "]\n",
    "\n",
    "for i in tqdm(range(N), desc=\"Token overlap\"):\n",
    "    passage = samples[i]['passage']\n",
    "    query = samples[i]['query']\n",
    "    random_text = generate_random_prefix_text(query, tokenizer, seed=SEED + i)\n",
    "\n",
    "    overlaps_i = {\n",
    "        'random': compute_token_overlap(random_text, passage, tokenizer),\n",
    "        'oracle': compute_token_overlap(query, passage, tokenizer),\n",
    "        'oracle_kw': compute_token_overlap(oracle_as_kw[i], passage, tokenizer),\n",
    "        'anti_kw': compute_token_overlap(anti_keywords[i], passage, tokenizer),\n",
    "        'tfidf': compute_token_overlap(tfidf_keywords[i], passage, tokenizer),\n",
    "        'echo': compute_token_overlap(passage_echos[i], passage, tokenizer),\n",
    "        'shuffled': compute_token_overlap(shuffled_llm[i], passage, tokenizer),\n",
    "        'llm_keyword': compute_token_overlap(surrogates_5[i].get('keyword_query', ''), passage, tokenizer),\n",
    "        'llm_question': compute_token_overlap(surrogates_5[i].get('target_question', ''), passage, tokenizer),\n",
    "        'llm_symptom': compute_token_overlap(surrogates_5[i].get('symptom_scenario', ''), passage, tokenizer),\n",
    "        'llm_summary': compute_token_overlap(summaries[i], passage, tokenizer),\n",
    "        'llm_messy': compute_token_overlap(surrogates_5[i].get('messy_realworld', ''), passage, tokenizer),\n",
    "    }\n",
    "    overlap_data[i] = overlaps_i\n",
    "\n",
    "# Report overlap gradient\n",
    "print(f\"\\n{'Condition':<30} {'Mean Overlap':>12} {'Std':>10}\")\n",
    "print(\"-\" * 55)\n",
    "for key, label in overlap_labels:\n",
    "    vals = [overlap_data[i][key] for i in range(N)]\n",
    "    print(f\"{label:<30} {np.mean(vals):>12.4f} {np.std(vals):>10.4f}\")\n",
    "\n",
    "# Save overlap data\n",
    "with open(RESULTS_DIR / \"overlap_data.json\", 'w') as f:\n",
    "    json.dump({str(k): v for k, v in overlap_data.items()}, f)\n",
    "print(f\"\\nOverlap data saved to {RESULTS_DIR / 'overlap_data.json'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0bf6a2c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-09T01:00:08.241237Z",
     "iopub.status.busy": "2026-02-09T01:00:08.240958Z",
     "iopub.status.idle": "2026-02-09T01:00:08.259116Z",
     "shell.execute_reply": "2026-02-09T01:00:08.258434Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "EXPERIMENTAL CONDITIONS EXPLAINED\n",
      "======================================================================\n",
      "\n",
      "### 1. Bare ###\n",
      "  Cache: [BOS][doc]\n",
      "  Detail: No prefix — baseline\n",
      "\n",
      "### 2. Random-truncated ###\n",
      "  Cache: [BOS][random_tokens\\n][doc] → truncate + RoPE\n",
      "  Detail: Random text: 'Restaur Mars didova少 DATA lux...'\n",
      "\n",
      "### 3. Separator-only ###\n",
      "  Cache: [BOS][doc][\\n\\nRelated question: ]\n",
      "  Detail: Suffix appended after passage — structural framing only\n",
      "\n",
      "### 4. Oracle-truncated ###\n",
      "  Cache: [BOS][query\\n][doc] → truncate + RoPE\n",
      "  Detail: Query: 'what is provider based billing mean...'\n",
      "\n",
      "### 5. Oracle-as-keywords ###\n",
      "  Cache: [BOS][keywords\\n][doc] → truncate + RoPE\n",
      "  Detail: Keywords from oracle: 'provider based billing mean'\n",
      "\n",
      "### 6. Anti-keywords ###\n",
      "  Cache: [BOS][wrong_tfidf\\n][doc] → truncate + RoPE\n",
      "  Detail: TF-IDF from WRONG passage: 'decomposition reaction chemical gas example potassium analys'\n",
      "\n",
      "### 7. TF-IDF-keywords ###\n",
      "  Cache: [BOS][tfidf\\n][doc] → truncate + RoPE\n",
      "  Detail: TF-IDF from THIS passage: 'billing provider based process hospital refers services rend'\n",
      "\n",
      "### 8. Passage-echo ###\n",
      "  Cache: [BOS][first_sent\\n][doc] → truncate + RoPE\n",
      "  Detail: First sentence: 'Provider Based Billing refers to the billing process for ser...'\n",
      "\n",
      "### 9. Shuffled-LLM ###\n",
      "  Cache: [BOS][shuffled\\n][doc] → truncate + RoPE\n",
      "  Detail: Shuffled LLM keyword tokens: 'personnel bill, processare Med Phys services requirements Pr'\n",
      "\n",
      "### 10. LLM-keyword ###\n",
      "  Cache: [BOS][llm_kw\\n][doc] → truncate + RoPE\n",
      "  Detail: LLM keyword: 'Provider Based Billing rules, Medicare Medicaid requirements'\n",
      "\n",
      "### 11. LLM-question ###\n",
      "  Cache: [BOS][llm_q\\n][doc] → truncate + RoPE\n",
      "  Detail: LLM question: 'What is Provider Based Billing and how does it work in a hos'\n",
      "\n",
      "### 12. LLM-symptom ###\n",
      "  Cache: [BOS][llm_symp\\n][doc] → truncate + RoPE\n",
      "  Detail: LLM symptom: 'Hospital-based healthcare services billed by hospital, not i'\n",
      "\n",
      "### 13. LLM-summary ###\n",
      "  Cache: [BOS][summary\\n][doc] → truncate + RoPE\n",
      "  Detail: Summary: 'Provider Based Billing is a hospital billing process for ser...'\n",
      "\n",
      "### 14. LLM-keyword+sep ###\n",
      "  Cache: [BOS][llm_kw\\n][doc][\\n\\nRelated question: ] (prefix+suffix)\n",
      "  Detail: Stacking: truncated prefix + suffix separator\n",
      "\n",
      "### 15. LLM-messy ###\n",
      "  Cache: [BOS][llm_messy\\n][doc] → truncate + RoPE\n",
      "  Detail: LLM messy: 'Help me find provider based billing rules medicare medicaid '\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Condition explanation with concrete examples\n",
    "print(\"=\" * 70)\n",
    "print(\"EXPERIMENTAL CONDITIONS EXPLAINED\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "ex_i = 0\n",
    "ex_passage = samples[ex_i]['passage']\n",
    "ex_query = samples[ex_i]['query']\n",
    "ex_llm_kw = surrogates_5[ex_i].get('keyword_query', '')\n",
    "ex_llm_q = surrogates_5[ex_i].get('target_question', '')\n",
    "ex_llm_symp = surrogates_5[ex_i].get('symptom_scenario', '')\n",
    "ex_llm_messy = surrogates_5[ex_i].get('messy_realworld', '')\n",
    "ex_summary = summaries[ex_i]\n",
    "ex_tfidf = tfidf_keywords[ex_i]\n",
    "ex_anti = anti_keywords[ex_i]\n",
    "ex_echo = passage_echos[ex_i]\n",
    "ex_oracle_kw = oracle_as_kw[ex_i]\n",
    "ex_shuffled = shuffled_llm[ex_i]\n",
    "\n",
    "conditions_explained = [\n",
    "    (\"1. Bare\", \"[BOS][doc]\", \"No prefix — baseline\"),\n",
    "    (\"2. Random-truncated\", \"[BOS][random_tokens\\\\n][doc] → truncate + RoPE\",\n",
    "     f\"Random text: '{generate_random_prefix_text(ex_query, tokenizer, SEED)[:60]}...'\"),\n",
    "    (\"3. Separator-only\", \"[BOS][doc][\\\\n\\\\nRelated question: ]\",\n",
    "     \"Suffix appended after passage — structural framing only\"),\n",
    "    (\"4. Oracle-truncated\", \"[BOS][query\\\\n][doc] → truncate + RoPE\",\n",
    "     f\"Query: '{ex_query[:60]}...'\"),\n",
    "    (\"5. Oracle-as-keywords\", \"[BOS][keywords\\\\n][doc] → truncate + RoPE\",\n",
    "     f\"Keywords from oracle: '{ex_oracle_kw[:60]}'\"),\n",
    "    (\"6. Anti-keywords\", \"[BOS][wrong_tfidf\\\\n][doc] → truncate + RoPE\",\n",
    "     f\"TF-IDF from WRONG passage: '{ex_anti[:60]}'\"),\n",
    "    (\"7. TF-IDF-keywords\", \"[BOS][tfidf\\\\n][doc] → truncate + RoPE\",\n",
    "     f\"TF-IDF from THIS passage: '{ex_tfidf[:60]}'\"),\n",
    "    (\"8. Passage-echo\", \"[BOS][first_sent\\\\n][doc] → truncate + RoPE\",\n",
    "     f\"First sentence: '{ex_echo[:60]}...'\"),\n",
    "    (\"9. Shuffled-LLM\", \"[BOS][shuffled\\\\n][doc] → truncate + RoPE\",\n",
    "     f\"Shuffled LLM keyword tokens: '{ex_shuffled[:60]}'\"),\n",
    "    (\"10. LLM-keyword\", \"[BOS][llm_kw\\\\n][doc] → truncate + RoPE\",\n",
    "     f\"LLM keyword: '{ex_llm_kw[:60]}'\"),\n",
    "    (\"11. LLM-question\", \"[BOS][llm_q\\\\n][doc] → truncate + RoPE\",\n",
    "     f\"LLM question: '{ex_llm_q[:60]}'\"),\n",
    "    (\"12. LLM-symptom\", \"[BOS][llm_symp\\\\n][doc] → truncate + RoPE\",\n",
    "     f\"LLM symptom: '{ex_llm_symp[:60]}'\"),\n",
    "    (\"13. LLM-summary\", \"[BOS][summary\\\\n][doc] → truncate + RoPE\",\n",
    "     f\"Summary: '{ex_summary[:60]}...'\"),\n",
    "    (\"14. LLM-keyword+sep\", \"[BOS][llm_kw\\\\n][doc][\\\\n\\\\nRelated question: ] (prefix+suffix)\",\n",
    "     f\"Stacking: truncated prefix + suffix separator\"),\n",
    "    (\"15. LLM-messy\", \"[BOS][llm_messy\\\\n][doc] → truncate + RoPE\",\n",
    "     f\"LLM messy: '{ex_llm_messy[:60]}'\"),\n",
    "]\n",
    "\n",
    "for name, pattern, detail in conditions_explained:\n",
    "    print(f\"\\n### {name} ###\")\n",
    "    print(f\"  Cache: {pattern}\")\n",
    "    print(f\"  Detail: {detail}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea18f215",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-09T01:00:08.262475Z",
     "iopub.status.busy": "2026-02-09T01:00:08.262189Z",
     "iopub.status.idle": "2026-02-09T04:42:16.091692Z",
     "shell.execute_reply": "2026-02-09T04:42:16.090645Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PHASE 2: MAIN EVALUATION (15 conditions × 2000 samples)\n",
      "======================================================================\n",
      "No checkpoint found. Starting fresh.\n",
      "Evaluating samples 0 to 1999\n",
      "Conditions: 15\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a5c80278e394b80802b38c6ead58544",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 50/2000 | 0.15 s/s | ETA: 213.4 min\n",
      "  Checkpoint 100/2000 | 0.15 s/s | ETA: 210.2 min\n",
      "  Checkpoint 150/2000 | 0.15 s/s | ETA: 204.5 min\n",
      "  Checkpoint 200/2000 | 0.15 s/s | ETA: 199.3 min\n",
      "  Checkpoint 250/2000 | 0.15 s/s | ETA: 193.8 min\n",
      "  Checkpoint 300/2000 | 0.15 s/s | ETA: 188.2 min\n",
      "  Checkpoint 350/2000 | 0.15 s/s | ETA: 182.9 min\n",
      "  Checkpoint 400/2000 | 0.15 s/s | ETA: 177.8 min\n",
      "  Checkpoint 450/2000 | 0.15 s/s | ETA: 172.4 min\n",
      "  Checkpoint 500/2000 | 0.15 s/s | ETA: 166.9 min\n",
      "  Checkpoint 550/2000 | 0.15 s/s | ETA: 161.3 min\n",
      "  Checkpoint 600/2000 | 0.15 s/s | ETA: 155.7 min\n",
      "  Checkpoint 650/2000 | 0.15 s/s | ETA: 150.1 min\n",
      "  Checkpoint 700/2000 | 0.15 s/s | ETA: 144.5 min\n",
      "  Checkpoint 750/2000 | 0.15 s/s | ETA: 138.9 min\n",
      "  Checkpoint 800/2000 | 0.15 s/s | ETA: 133.5 min\n",
      "  Checkpoint 850/2000 | 0.15 s/s | ETA: 128.1 min\n",
      "  Checkpoint 900/2000 | 0.15 s/s | ETA: 122.5 min\n",
      "  Checkpoint 950/2000 | 0.15 s/s | ETA: 116.8 min\n",
      "  Checkpoint 1000/2000 | 0.15 s/s | ETA: 111.3 min\n",
      "  Checkpoint 1050/2000 | 0.15 s/s | ETA: 105.7 min\n",
      "  Checkpoint 1100/2000 | 0.15 s/s | ETA: 100.1 min\n",
      "  Checkpoint 1150/2000 | 0.15 s/s | ETA: 94.5 min\n",
      "  Checkpoint 1200/2000 | 0.15 s/s | ETA: 88.9 min\n",
      "  Checkpoint 1250/2000 | 0.15 s/s | ETA: 83.4 min\n",
      "  Checkpoint 1300/2000 | 0.15 s/s | ETA: 77.8 min\n",
      "  Checkpoint 1350/2000 | 0.15 s/s | ETA: 72.2 min\n",
      "  Checkpoint 1400/2000 | 0.15 s/s | ETA: 66.6 min\n",
      "  Checkpoint 1450/2000 | 0.15 s/s | ETA: 61.1 min\n",
      "  Checkpoint 1500/2000 | 0.15 s/s | ETA: 55.5 min\n",
      "  Checkpoint 1550/2000 | 0.15 s/s | ETA: 50.0 min\n",
      "  Checkpoint 1600/2000 | 0.15 s/s | ETA: 44.4 min\n",
      "  Checkpoint 1650/2000 | 0.15 s/s | ETA: 38.9 min\n",
      "  Checkpoint 1700/2000 | 0.15 s/s | ETA: 33.3 min\n",
      "  Checkpoint 1750/2000 | 0.15 s/s | ETA: 27.8 min\n",
      "  Checkpoint 1800/2000 | 0.15 s/s | ETA: 22.2 min\n",
      "  Checkpoint 1850/2000 | 0.15 s/s | ETA: 16.7 min\n",
      "  Checkpoint 1900/2000 | 0.15 s/s | ETA: 11.1 min\n",
      "  Checkpoint 1950/2000 | 0.15 s/s | ETA: 5.6 min\n",
      "  Checkpoint 2000/2000 | 0.15 s/s | ETA: 0.0 min\n",
      "\n",
      "Evaluation complete: 2000 samples in 222.1 min\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Main eval loop — 15 conditions × 2000 samples\n",
    "print(\"=\" * 70)\n",
    "print(\"PHASE 2: MAIN EVALUATION (15 conditions × 2000 samples)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "CONDITION_NAMES = [\n",
    "    'bare', 'random_trunc', 'separator_only',\n",
    "    'oracle_trunc', 'oracle_as_kw',\n",
    "    'anti_keywords', 'tfidf_keywords', 'passage_echo', 'shuffled_llm',\n",
    "    'llm_keyword', 'llm_question', 'llm_symptom', 'llm_summary',\n",
    "    'llm_keyword_sep', 'llm_messy',\n",
    "]\n",
    "\n",
    "results = []\n",
    "start_idx = 0\n",
    "\n",
    "if CHECKPOINT_PATH.exists():\n",
    "    with open(CHECKPOINT_PATH, 'r') as f:\n",
    "        ckpt = json.load(f)\n",
    "    ckpt_queries = ckpt.get('sample_queries', [])\n",
    "    current_queries = [s['query'] for s in samples]\n",
    "    if ckpt_queries == current_queries:\n",
    "        results = ckpt['results']\n",
    "        start_idx = len(results)\n",
    "        print(f\"Resuming from checkpoint: {start_idx}/{N}\")\n",
    "    else:\n",
    "        print(\"Checkpoint sample mismatch. Starting fresh.\")\n",
    "else:\n",
    "    print(\"No checkpoint found. Starting fresh.\")\n",
    "\n",
    "print(f\"Evaluating samples {start_idx} to {N-1}\")\n",
    "print(f\"Conditions: {len(CONDITION_NAMES)}\")\n",
    "\n",
    "t_start = time.time()\n",
    "\n",
    "for idx in tqdm(range(start_idx, N), initial=start_idx, total=N, desc=\"Evaluating\"):\n",
    "    sample = samples[idx]\n",
    "    passage = sample['passage']\n",
    "    query = sample['query']\n",
    "    answer = sample['answer']\n",
    "\n",
    "    query_prompt = QUERY_TEMPLATE.format(query=query)\n",
    "    answer_text = ANSWER_TEMPLATE.format(answer=answer)\n",
    "\n",
    "    # --- Matched tokenization (same doc_ids for ALL truncated conditions) ---\n",
    "    oracle_prefix = SURROGATE_PREFIX_TEMPLATE.format(surrogate=query)\n",
    "    document_text = DOCUMENT_TEMPLATE.format(document=passage)\n",
    "    full_oracle_text = oracle_prefix + document_text\n",
    "\n",
    "    full_oracle_enc = tokenizer(full_oracle_text, return_tensors=\"pt\",\n",
    "                                add_special_tokens=True, padding=False, truncation=False)\n",
    "    full_oracle_ids = full_oracle_enc['input_ids'].to(config.device)\n",
    "\n",
    "    oracle_prefix_enc = tokenizer(oracle_prefix, return_tensors=\"pt\",\n",
    "                                  add_special_tokens=True, padding=False, truncation=False)\n",
    "    oracle_prefix_len = oracle_prefix_enc['input_ids'].shape[1]\n",
    "\n",
    "    bos_id = full_oracle_ids[:, :1]\n",
    "    doc_ids = full_oracle_ids[:, oracle_prefix_len:]\n",
    "    doc_len = doc_ids.shape[1]\n",
    "\n",
    "    # Pre-compute all prefix texts\n",
    "    random_text = generate_random_prefix_text(query, tokenizer, seed=SEED + idx)\n",
    "    llm_kw_text = surrogates_5[idx].get('keyword_query', '')\n",
    "    llm_q_text = surrogates_5[idx].get('target_question', '')\n",
    "    llm_symp_text = surrogates_5[idx].get('symptom_scenario', '')\n",
    "    llm_messy_text = surrogates_5[idx].get('messy_realworld', '')\n",
    "    summary_text = summaries[idx]\n",
    "    tfidf_text = tfidf_keywords[idx]\n",
    "    anti_kw_text = anti_keywords[idx]\n",
    "    echo_text = passage_echos[idx]\n",
    "    oracle_kw_text = oracle_as_kw[idx]\n",
    "    shuffled_text = shuffled_llm[idx]\n",
    "\n",
    "    # Helper: build truncated cache from prefix text\n",
    "    def build_trunc(prefix_text):\n",
    "        prefix_str = SURROGATE_PREFIX_TEMPLATE.format(surrogate=prefix_text)\n",
    "        prefix_enc = tokenizer(prefix_str, return_tensors=\"pt\",\n",
    "                               add_special_tokens=False, padding=False, truncation=False)\n",
    "        prefix_ids = prefix_enc['input_ids'].to(config.device)\n",
    "        full_ids = torch.cat([bos_id, prefix_ids, doc_ids], dim=1)\n",
    "        prefix_token_len = 1 + prefix_ids.shape[1]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            out = model(input_ids=full_ids,\n",
    "                        attention_mask=torch.ones_like(full_ids),\n",
    "                        use_cache=True, return_dict=True)\n",
    "        cache = extract_and_truncate_cache_with_bos(out.past_key_values, doc_len)\n",
    "        correct_rope_positions_with_bos(cache, prefix_token_len - 1, model)\n",
    "        nll = score_answer_with_cache(\n",
    "            deepcopy_cache(cache), 1 + doc_len,\n",
    "            query_prompt, answer_text, model, tokenizer, config)\n",
    "        del out, cache\n",
    "        return nll\n",
    "\n",
    "    # === Condition 1: BARE ===\n",
    "    bare_ids = torch.cat([bos_id, doc_ids], dim=1)\n",
    "    bare_len = bare_ids.shape[1]\n",
    "    with torch.no_grad():\n",
    "        bare_out = model(input_ids=bare_ids, attention_mask=torch.ones_like(bare_ids),\n",
    "                         use_cache=True, return_dict=True)\n",
    "    nll_bare = score_answer_with_cache(\n",
    "        deepcopy_cache(bare_out.past_key_values), bare_len,\n",
    "        query_prompt, answer_text, model, tokenizer, config)\n",
    "    del bare_out\n",
    "\n",
    "    # === Condition 2: RANDOM-TRUNCATED ===\n",
    "    nll_random = build_trunc(random_text)\n",
    "\n",
    "    # === Condition 3: SEPARATOR-ONLY ===\n",
    "    sep_len, sep_cache = build_suffix_kv_cache(\n",
    "        passage, \"\", model, tokenizer, config, separator=SUFFIX_SEPARATOR)\n",
    "    nll_separator = score_answer_with_cache(\n",
    "        deepcopy_cache(sep_cache), sep_len,\n",
    "        query_prompt, answer_text, model, tokenizer, config)\n",
    "    del sep_cache\n",
    "\n",
    "    # === Condition 4: ORACLE-TRUNCATED ===\n",
    "    with torch.no_grad():\n",
    "        oracle_out = model(input_ids=full_oracle_ids,\n",
    "                           attention_mask=torch.ones_like(full_oracle_ids),\n",
    "                           use_cache=True, return_dict=True)\n",
    "    oracle_cache = extract_and_truncate_cache_with_bos(oracle_out.past_key_values, doc_len)\n",
    "    correct_rope_positions_with_bos(oracle_cache, oracle_prefix_len - 1, model)\n",
    "    nll_oracle = score_answer_with_cache(\n",
    "        deepcopy_cache(oracle_cache), 1 + doc_len,\n",
    "        query_prompt, answer_text, model, tokenizer, config)\n",
    "    del oracle_out, oracle_cache\n",
    "\n",
    "    # === Condition 5: ORACLE-AS-KEYWORDS ===\n",
    "    nll_oracle_kw = build_trunc(oracle_kw_text)\n",
    "\n",
    "    # === Condition 6: ANTI-KEYWORDS ===\n",
    "    nll_anti_kw = build_trunc(anti_kw_text)\n",
    "\n",
    "    # === Condition 7: TF-IDF-KEYWORDS ===\n",
    "    nll_tfidf = build_trunc(tfidf_text)\n",
    "\n",
    "    # === Condition 8: PASSAGE-ECHO ===\n",
    "    nll_echo = build_trunc(echo_text)\n",
    "\n",
    "    # === Condition 9: SHUFFLED-LLM ===\n",
    "    nll_shuffled = build_trunc(shuffled_text)\n",
    "\n",
    "    # === Condition 10: LLM-KEYWORD ===\n",
    "    nll_llm_kw = build_trunc(llm_kw_text)\n",
    "\n",
    "    # === Condition 11: LLM-QUESTION ===\n",
    "    nll_llm_q = build_trunc(llm_q_text)\n",
    "\n",
    "    # === Condition 12: LLM-SYMPTOM ===\n",
    "    nll_llm_symp = build_trunc(llm_symp_text)\n",
    "\n",
    "    # === Condition 13: LLM-SUMMARY ===\n",
    "    nll_llm_sum = build_trunc(summary_text)\n",
    "\n",
    "    # === Condition 14: LLM-KEYWORD + SEPARATOR ===\n",
    "    # Build truncated cache with LLM keyword prefix, then use build_suffix on THAT\n",
    "    # Step 1: Build the primed document cache (truncated)\n",
    "    kw_prefix_str = SURROGATE_PREFIX_TEMPLATE.format(surrogate=llm_kw_text)\n",
    "    kw_prefix_enc = tokenizer(kw_prefix_str, return_tensors=\"pt\",\n",
    "                              add_special_tokens=False, padding=False, truncation=False)\n",
    "    kw_prefix_ids = kw_prefix_enc['input_ids'].to(config.device)\n",
    "    kw_full_ids = torch.cat([bos_id, kw_prefix_ids, doc_ids], dim=1)\n",
    "    kw_prefix_token_len = 1 + kw_prefix_ids.shape[1]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        kw_out = model(input_ids=kw_full_ids,\n",
    "                       attention_mask=torch.ones_like(kw_full_ids),\n",
    "                       use_cache=True, return_dict=True)\n",
    "    kw_trunc_cache = extract_and_truncate_cache_with_bos(kw_out.past_key_values, doc_len)\n",
    "    correct_rope_positions_with_bos(kw_trunc_cache, kw_prefix_token_len - 1, model)\n",
    "    del kw_out\n",
    "\n",
    "    # Step 2: Extend with suffix separator (forward pass through separator tokens)\n",
    "    suffix_enc = tokenizer(SUFFIX_SEPARATOR, return_tensors=\"pt\",\n",
    "                           add_special_tokens=False, padding=False, truncation=False)\n",
    "    suffix_ids = suffix_enc['input_ids'].to(config.device)\n",
    "    suffix_len = suffix_ids.shape[1]\n",
    "    cache_len_before_suffix = 1 + doc_len\n",
    "\n",
    "    # Create position_ids continuing from doc end\n",
    "    suffix_position_ids = torch.arange(\n",
    "        cache_len_before_suffix, cache_len_before_suffix + suffix_len,\n",
    "        device=config.device\n",
    "    ).unsqueeze(0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        suffix_out = model(\n",
    "            input_ids=suffix_ids,\n",
    "            attention_mask=torch.ones(1, cache_len_before_suffix + suffix_len,\n",
    "                                      device=config.device, dtype=torch.long),\n",
    "            position_ids=suffix_position_ids,\n",
    "            past_key_values=deepcopy_cache(kw_trunc_cache),\n",
    "            use_cache=True,\n",
    "            return_dict=True,\n",
    "        )\n",
    "    combo_cache = suffix_out.past_key_values\n",
    "    combo_len = cache_len_before_suffix + suffix_len\n",
    "    nll_llm_kw_sep = score_answer_with_cache(\n",
    "        deepcopy_cache(combo_cache), combo_len,\n",
    "        query_prompt, answer_text, model, tokenizer, config)\n",
    "    del suffix_out, combo_cache, kw_trunc_cache\n",
    "\n",
    "    # === Condition 15: LLM-MESSY ===\n",
    "    nll_llm_messy = build_trunc(llm_messy_text)\n",
    "\n",
    "    # --- Store result ---\n",
    "    result = {\n",
    "        'idx': idx,\n",
    "        'doc_len': doc_len,\n",
    "        'passage_word_count': len(passage.split()),\n",
    "        'bare': nll_bare,\n",
    "        'random_trunc': nll_random,\n",
    "        'separator_only': nll_separator,\n",
    "        'oracle_trunc': nll_oracle,\n",
    "        'oracle_as_kw': nll_oracle_kw,\n",
    "        'anti_keywords': nll_anti_kw,\n",
    "        'tfidf_keywords': nll_tfidf,\n",
    "        'passage_echo': nll_echo,\n",
    "        'shuffled_llm': nll_shuffled,\n",
    "        'llm_keyword': nll_llm_kw,\n",
    "        'llm_question': nll_llm_q,\n",
    "        'llm_symptom': nll_llm_symp,\n",
    "        'llm_summary': nll_llm_sum,\n",
    "        'llm_keyword_sep': nll_llm_kw_sep,\n",
    "        'llm_messy': nll_llm_messy,\n",
    "    }\n",
    "    results.append(result)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    if (idx + 1) % CHECKPOINT_EVERY == 0 or idx == N - 1:\n",
    "        ckpt_data = {\n",
    "            'results': results,\n",
    "            'sample_queries': [s['query'] for s in samples],\n",
    "            'completed': len(results),\n",
    "            'total': N,\n",
    "            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        }\n",
    "        with open(CHECKPOINT_PATH, 'w') as f:\n",
    "            json.dump(ckpt_data, f)\n",
    "        elapsed = time.time() - t_start\n",
    "        rate = (idx - start_idx + 1) / elapsed if elapsed > 0 else 0\n",
    "        remaining = (N - idx - 1) / rate if rate > 0 else 0\n",
    "        tqdm.write(f\"  Checkpoint {idx+1}/{N} | {rate:.2f} s/s | ETA: {remaining/60:.1f} min\")\n",
    "\n",
    "elapsed_total = time.time() - t_start\n",
    "print(f\"\\nEvaluation complete: {len(results)} samples in {elapsed_total/60:.1f} min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d7140c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-09T04:42:16.096569Z",
     "iopub.status.busy": "2026-02-09T04:42:16.096249Z",
     "iopub.status.idle": "2026-02-09T04:42:16.557615Z",
     "shell.execute_reply": "2026-02-09T04:42:16.556753Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ANALYSIS — MECHANISM DECOMPOSITION\n",
      "======================================================================\n",
      "Total: 2000, Valid: 1834, Excluded: 166\n",
      "\n",
      "Condition                   Mean NLL        Std  d vs Bare\n",
      "------------------------------------------------------------\n",
      "bare                          1.1158     1.5137          —\n",
      "random_trunc                  1.0725     1.5088     +0.125\n",
      "separator_only                1.0163     1.3690     +0.231\n",
      "oracle_trunc                  1.1027     1.5134     +0.034\n",
      "oracle_as_kw                  1.0828     1.5026     +0.098\n",
      "anti_keywords                 1.0684     1.4826     +0.142\n",
      "tfidf_keywords                1.0777     1.4918     +0.101\n",
      "passage_echo                  1.1127     1.4700     +0.007\n",
      "shuffled_llm                  1.0749     1.5051     +0.115\n",
      "llm_keyword                   1.0351     1.4539     +0.234\n",
      "llm_question                  1.0284     1.4120     +0.200\n",
      "llm_symptom                   1.0254     1.4200     +0.252\n",
      "llm_summary                   1.1273     1.5857     -0.023\n",
      "llm_keyword_sep               0.9559     1.2825     +0.297\n",
      "llm_messy                     1.0501     1.4593     +0.143\n",
      "\n",
      "================================================================================\n",
      "10 PRIMARY COMPARISONS (Bonferroni alpha = 0.0050)\n",
      "================================================================================\n",
      "\n",
      "Comparison                       Mean Δ        d    Win%        t            p   Sig\n",
      "-------------------------------------------------------------------------------------\n",
      "M1: Shuffled vs LLM-kw           0.0398    0.150   60.2%     6.42    1.77e-10   ***\n",
      "M2: Oracle-kw vs Oracle          0.0199    0.092   54.0%     3.94    8.52e-05   ***\n",
      "M3: LLM-kw vs LLM-question      -0.0067   -0.018   53.4%    -0.78    4.38e-01    ns\n",
      "M4: TF-IDF vs Anti-kw           -0.0093   -0.026   49.3%    -1.13    2.59e-01    ns\n",
      "M5: Echo vs LLM-kw              -0.0776   -0.173   38.0%    -7.41    1.95e-13   ***\n",
      "M6: TF-IDF vs LLM-kw            -0.0426   -0.137   41.1%    -5.89    4.70e-09   ***\n",
      "M7: LLM-kw+sep vs best single   -0.0410   -0.119   40.8%    -5.11    3.48e-07   ***\n",
      "R1: LLM-kw vs Random             0.0374    0.099   55.6%     4.23    2.50e-05   ***\n",
      "R2: Oracle vs Random            -0.0302   -0.077   42.6%    -3.29    1.02e-03    **\n",
      "R3: LLM-kw vs Bare               0.0807    0.234   67.6%    10.01    5.24e-23   ***\n",
      "\n",
      "================================================================================\n",
      "ALL CONDITIONS vs BARE\n",
      "================================================================================\n",
      "\n",
      "Condition                  d vs Bare    Win%            p\n",
      "------------------------------------------------------------\n",
      "random_trunc                   0.125   62.3%    9.08e-08   ***\n",
      "separator_only                 0.231   66.6%    1.35e-22   ***\n",
      "oracle_trunc                   0.034   50.8%    1.41e-01    ns\n",
      "oracle_as_kw                   0.098   60.3%    2.74e-05   ***\n",
      "anti_keywords                  0.142   61.5%    1.41e-09   ***\n",
      "tfidf_keywords                 0.101   60.5%    1.74e-05   ***\n",
      "passage_echo                   0.007   55.1%    7.49e-01    ns\n",
      "shuffled_llm                   0.115   60.0%    9.48e-07   ***\n",
      "llm_keyword                    0.234   67.6%    5.24e-23   ***\n",
      "llm_question                   0.200   61.2%    2.21e-17   ***\n",
      "llm_symptom                    0.252   66.7%    2.04e-26   ***\n",
      "llm_summary                   -0.023   53.4%    3.21e-01    ns\n",
      "llm_keyword_sep                0.297   69.0%    1.41e-35   ***\n",
      "llm_messy                      0.143   61.4%    1.01e-09   ***\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Primary analysis — all 10 comparisons (Bonferroni alpha = 0.005)\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ANALYSIS — MECHANISM DECOMPOSITION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Extract arrays and filter zero NLLs\n",
    "cond_arrays = {}\n",
    "for cname in CONDITION_NAMES:\n",
    "    cond_arrays[cname] = np.array([r[cname] for r in results])\n",
    "\n",
    "# Valid mask: no zero NLLs in any condition\n",
    "valid = np.ones(len(results), dtype=bool)\n",
    "for cname in CONDITION_NAMES:\n",
    "    valid &= (cond_arrays[cname] != 0)\n",
    "n_valid = int(np.sum(valid))\n",
    "n_excluded = int(np.sum(~valid))\n",
    "print(f\"Total: {len(results)}, Valid: {n_valid}, Excluded: {n_excluded}\")\n",
    "\n",
    "# Apply mask\n",
    "c = {}\n",
    "for cname in CONDITION_NAMES:\n",
    "    c[cname] = cond_arrays[cname][valid]\n",
    "\n",
    "# NLL summary table\n",
    "print(f\"\\n{'Condition':<25} {'Mean NLL':>10} {'Std':>10} {'d vs Bare':>10}\")\n",
    "print(\"-\" * 60)\n",
    "for cname in CONDITION_NAMES:\n",
    "    mean_nll = np.mean(c[cname])\n",
    "    std_nll = np.std(c[cname])\n",
    "    if cname == 'bare':\n",
    "        d_str = \"—\"\n",
    "    else:\n",
    "        d = cohens_d(c['bare'] - c[cname])\n",
    "        d_str = f\"{d:+.3f}\"\n",
    "    print(f\"{cname:<25} {mean_nll:>10.4f} {std_nll:>10.4f} {d_str:>10}\")\n",
    "\n",
    "# 10 primary comparisons\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"10 PRIMARY COMPARISONS (Bonferroni alpha = {BONFERRONI_ALPHA:.4f})\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "comparisons = [\n",
    "    # (name, delta_array, question)\n",
    "    # delta > 0 means first condition is better (lower NLL)\n",
    "    ('M1: Shuffled vs LLM-kw', c['shuffled_llm'] - c['llm_keyword'],\n",
    "     'Does coherence matter?'),\n",
    "    ('M2: Oracle-kw vs Oracle', c['oracle_trunc'] - c['oracle_as_kw'],\n",
    "     'Does question format hurt?'),\n",
    "    ('M3: LLM-kw vs LLM-question', c['llm_question'] - c['llm_keyword'],\n",
    "     'Keyword vs question format?'),\n",
    "    ('M4: TF-IDF vs Anti-kw', c['anti_keywords'] - c['tfidf_keywords'],\n",
    "     'Does passage specificity matter?'),\n",
    "    ('M5: Echo vs LLM-kw', c['llm_keyword'] - c['passage_echo'],\n",
    "     'Is max overlap the ceiling?'),\n",
    "    ('M6: TF-IDF vs LLM-kw', c['llm_keyword'] - c['tfidf_keywords'],\n",
    "     'Is LLM necessary beyond TF-IDF?'),\n",
    "    ('M7: LLM-kw+sep vs best single',\n",
    "     np.minimum(c['llm_keyword'], c['separator_only']) - c['llm_keyword_sep'],\n",
    "     'Does stacking help?'),\n",
    "    ('R1: LLM-kw vs Random', c['random_trunc'] - c['llm_keyword'],\n",
    "     'Replicates Exp 05 (d≈0.2)?'),\n",
    "    ('R2: Oracle vs Random', c['random_trunc'] - c['oracle_trunc'],\n",
    "     'Replicates null (d≈0)?'),\n",
    "    ('R3: LLM-kw vs Bare', c['bare'] - c['llm_keyword'],\n",
    "     'Overall LLM benefit?'),\n",
    "]\n",
    "\n",
    "print(f\"\\n{'Comparison':<30} {'Mean Δ':>8} {'d':>8} {'Win%':>7} {'t':>8} {'p':>12} {'Sig':>5}\")\n",
    "print(\"-\" * 85)\n",
    "\n",
    "comparison_results = {}\n",
    "for name, delta, question in comparisons:\n",
    "    d = cohens_d(delta)\n",
    "    win = np.mean(delta > 0) * 100\n",
    "    t_stat, p_val = stats.ttest_1samp(delta, 0)\n",
    "    sig = \"***\" if p_val < 0.001 else \"**\" if p_val < BONFERRONI_ALPHA else \"*\" if p_val < 0.05 else \"ns\"\n",
    "    print(f\"{name:<30} {np.mean(delta):>8.4f} {d:>8.3f} {win:>6.1f}% {t_stat:>8.2f} {p_val:>11.2e} {sig:>5}\")\n",
    "    comparison_results[name] = {\n",
    "        'mean_delta': float(np.mean(delta)),\n",
    "        'cohens_d': float(d),\n",
    "        'win_rate': float(win / 100),\n",
    "        't_stat': float(t_stat),\n",
    "        'p_value': float(p_val),\n",
    "        'bonferroni_significant': bool(p_val < BONFERRONI_ALPHA),\n",
    "        'question': question,\n",
    "    }\n",
    "\n",
    "# All conditions vs Bare\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"ALL CONDITIONS vs BARE\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"\\n{'Condition':<25} {'d vs Bare':>10} {'Win%':>7} {'p':>12}\")\n",
    "print(\"-\" * 60)\n",
    "all_vs_bare = {}\n",
    "for cname in CONDITION_NAMES:\n",
    "    if cname == 'bare':\n",
    "        continue\n",
    "    delta = c['bare'] - c[cname]\n",
    "    d = cohens_d(delta)\n",
    "    win = np.mean(delta > 0) * 100\n",
    "    _, p_val = stats.ttest_1samp(delta, 0)\n",
    "    sig = \"***\" if p_val < 0.001 else \"**\" if p_val < BONFERRONI_ALPHA else \"*\" if p_val < 0.05 else \"ns\"\n",
    "    print(f\"{cname:<25} {d:>10.3f} {win:>6.1f}% {p_val:>11.2e} {sig:>5}\")\n",
    "    all_vs_bare[cname] = {'cohens_d': float(d), 'win_rate': float(win/100), 'p_value': float(p_val)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eada9cce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-09T04:42:16.561555Z",
     "iopub.status.busy": "2026-02-09T04:42:16.561059Z",
     "iopub.status.idle": "2026-02-09T04:42:16.753337Z",
     "shell.execute_reply": "2026-02-09T04:42:16.751840Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "TOKEN OVERLAP MECHANISM ANALYSIS\n",
      "======================================================================\n",
      "\n",
      "Universal overlap-delta correlation (pooled across all conditions):\n",
      "  r = -0.0240, p = 3.79e-04, N = 22008\n",
      "\n",
      "Condition                   r(overlap,delta)            p      N\n",
      "-----------------------------------------------------------------\n",
      "random_trunc                          0.0072    7.57e-01   1834\n",
      "oracle_trunc                          0.0462    4.81e-02   1834\n",
      "oracle_as_kw                          0.0422    7.10e-02   1834\n",
      "anti_keywords                         0.0380    1.04e-01   1834\n",
      "tfidf_keywords                        0.0555    1.74e-02   1834\n",
      "passage_echo                         -0.1675    5.18e-13   1834\n",
      "shuffled_llm                          0.0687    3.25e-03   1834\n",
      "llm_keyword                           0.0500    3.24e-02   1834\n",
      "llm_question                          0.0024    9.18e-01   1834\n",
      "llm_symptom                           0.0062    7.91e-01   1834\n",
      "llm_summary                          -0.0103    6.60e-01   1834\n",
      "llm_messy                            -0.0006    9.79e-01   1834\n",
      "\n",
      "Cross-condition: median overlap vs mean Cohen's d\n",
      "Condition                  Median Overlap     Mean d\n",
      "-------------------------------------------------------\n",
      "random_trunc                       0.0000      0.125\n",
      "oracle_trunc                       0.0761      0.034\n",
      "oracle_as_kw                       0.0548      0.098\n",
      "anti_keywords                      0.0000      0.142\n",
      "tfidf_keywords                     0.1702      0.101\n",
      "passage_echo                       0.2553      0.007\n",
      "shuffled_llm                       0.2292      0.115\n",
      "llm_keyword                        0.2667      0.234\n",
      "llm_question                       0.2000      0.200\n",
      "llm_symptom                        0.1453      0.252\n",
      "llm_summary                        0.4246     -0.023\n",
      "llm_messy                          0.1899      0.143\n",
      "\n",
      "Cross-condition correlation: r = -0.2601, p = 0.4142\n",
      "\n",
      "--- Regression: delta ~ overlap + hardness + overlap*hardness ---\n",
      "  beta_0 (intercept):           +0.04431\n",
      "  beta_1 (overlap):             -0.01008\n",
      "  beta_2 (hardness):            +0.07939\n",
      "  beta_3 (overlap × hardness):  +0.00172\n",
      "  R² = 0.0417\n",
      "\n",
      "--- Decision Criteria ---\n",
      "  ✗ r(overlap, delta) = -0.024 < 0.1 → overlap is NOT the main mechanism\n",
      "  ✗ TF-IDF (d=0.101) ≠ LLM-kw (d=0.234) → LLM adds value beyond overlap\n",
      "  ✗ Shuffled (d=0.115) ≠ Ordered (d=0.234) → coherence matters\n"
     ]
    }
   ],
   "source": [
    "# Cell 10: Token overlap mechanism analysis + regression\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"TOKEN OVERLAP MECHANISM ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Map condition names to overlap keys\n",
    "cond_to_overlap = {\n",
    "    'random_trunc': 'random',\n",
    "    'oracle_trunc': 'oracle',\n",
    "    'oracle_as_kw': 'oracle_kw',\n",
    "    'anti_keywords': 'anti_kw',\n",
    "    'tfidf_keywords': 'tfidf',\n",
    "    'passage_echo': 'echo',\n",
    "    'shuffled_llm': 'shuffled',\n",
    "    'llm_keyword': 'llm_keyword',\n",
    "    'llm_question': 'llm_question',\n",
    "    'llm_symptom': 'llm_symptom',\n",
    "    'llm_summary': 'llm_summary',\n",
    "    'llm_messy': 'llm_messy',\n",
    "}\n",
    "\n",
    "# 1. Pool all (sample, condition) pairs → overlap vs delta\n",
    "all_overlaps = []\n",
    "all_deltas = []\n",
    "all_cond_labels = []\n",
    "valid_indices = np.where(valid)[0]\n",
    "\n",
    "for cname, okey in cond_to_overlap.items():\n",
    "    for i_valid, i_orig in enumerate(valid_indices):\n",
    "        ov = overlap_data[i_orig][okey]\n",
    "        delta = c['bare'][i_valid] - c[cname][i_valid]\n",
    "        all_overlaps.append(ov)\n",
    "        all_deltas.append(delta)\n",
    "        all_cond_labels.append(cname)\n",
    "\n",
    "all_overlaps = np.array(all_overlaps)\n",
    "all_deltas = np.array(all_deltas)\n",
    "\n",
    "# Universal correlation\n",
    "r_all, p_all = stats.pearsonr(all_overlaps, all_deltas)\n",
    "print(f\"\\nUniversal overlap-delta correlation (pooled across all conditions):\")\n",
    "print(f\"  r = {r_all:.4f}, p = {p_all:.2e}, N = {len(all_overlaps)}\")\n",
    "\n",
    "# 2. Within-condition correlations\n",
    "print(f\"\\n{'Condition':<25} {'r(overlap,delta)':>18} {'p':>12} {'N':>6}\")\n",
    "print(\"-\" * 65)\n",
    "for cname, okey in cond_to_overlap.items():\n",
    "    ovs = []\n",
    "    deltas = []\n",
    "    for i_valid, i_orig in enumerate(valid_indices):\n",
    "        ovs.append(overlap_data[i_orig][okey])\n",
    "        deltas.append(c['bare'][i_valid] - c[cname][i_valid])\n",
    "    r, p = stats.pearsonr(ovs, deltas)\n",
    "    print(f\"{cname:<25} {r:>18.4f} {p:>11.2e} {len(ovs):>6}\")\n",
    "\n",
    "# 3. Cross-condition: median overlap vs mean effect size\n",
    "print(f\"\\nCross-condition: median overlap vs mean Cohen's d\")\n",
    "print(f\"{'Condition':<25} {'Median Overlap':>15} {'Mean d':>10}\")\n",
    "print(\"-\" * 55)\n",
    "cond_median_overlap = []\n",
    "cond_mean_d = []\n",
    "for cname, okey in cond_to_overlap.items():\n",
    "    ovs = [overlap_data[i_orig][okey] for i_orig in valid_indices]\n",
    "    d = cohens_d(c['bare'] - c[cname])\n",
    "    med_ov = np.median(ovs)\n",
    "    cond_median_overlap.append(med_ov)\n",
    "    cond_mean_d.append(d)\n",
    "    print(f\"{cname:<25} {med_ov:>15.4f} {d:>10.3f}\")\n",
    "\n",
    "r_cross, p_cross = stats.pearsonr(cond_median_overlap, cond_mean_d)\n",
    "print(f\"\\nCross-condition correlation: r = {r_cross:.4f}, p = {p_cross:.4f}\")\n",
    "\n",
    "# 4. Regression: delta ~ overlap + hardness + overlap*hardness\n",
    "from numpy.polynomial import polynomial as P\n",
    "\n",
    "print(f\"\\n--- Regression: delta ~ overlap + hardness + overlap*hardness ---\")\n",
    "bare_valid = c['bare']\n",
    "# Standardize\n",
    "ov_std = (all_overlaps - np.mean(all_overlaps)) / (np.std(all_overlaps) + 1e-8)\n",
    "# Hardness: repeat bare NLL for each condition\n",
    "hardness_all = np.tile(bare_valid, len(cond_to_overlap))\n",
    "h_std = (hardness_all - np.mean(hardness_all)) / (np.std(hardness_all) + 1e-8)\n",
    "interaction = ov_std * h_std\n",
    "\n",
    "X = np.column_stack([np.ones(len(all_deltas)), ov_std, h_std, interaction])\n",
    "betas, residuals, rank, sv = np.linalg.lstsq(X, all_deltas, rcond=None)\n",
    "y_pred = X @ betas\n",
    "ss_res = np.sum((all_deltas - y_pred) ** 2)\n",
    "ss_tot = np.sum((all_deltas - np.mean(all_deltas)) ** 2)\n",
    "r_squared = 1 - ss_res / ss_tot\n",
    "\n",
    "print(f\"  beta_0 (intercept):           {betas[0]:+.5f}\")\n",
    "print(f\"  beta_1 (overlap):             {betas[1]:+.5f}\")\n",
    "print(f\"  beta_2 (hardness):            {betas[2]:+.5f}\")\n",
    "print(f\"  beta_3 (overlap × hardness):  {betas[3]:+.5f}\")\n",
    "print(f\"  R² = {r_squared:.4f}\")\n",
    "\n",
    "# Decision criteria\n",
    "print(f\"\\n--- Decision Criteria ---\")\n",
    "if abs(r_all) > 0.3:\n",
    "    print(f\"  ✓ r(overlap, delta) = {r_all:.3f} > 0.3 → overlap IS the mechanism\")\n",
    "elif abs(r_all) > 0.1:\n",
    "    print(f\"  ~ r(overlap, delta) = {r_all:.3f} ∈ (0.1, 0.3) → overlap is PART of the mechanism\")\n",
    "else:\n",
    "    print(f\"  ✗ r(overlap, delta) = {r_all:.3f} < 0.1 → overlap is NOT the main mechanism\")\n",
    "\n",
    "# TF-IDF vs LLM comparison\n",
    "d_tfidf = cohens_d(c['bare'] - c['tfidf_keywords'])\n",
    "d_llm_kw = cohens_d(c['bare'] - c['llm_keyword'])\n",
    "if abs(d_tfidf - d_llm_kw) < 0.05:\n",
    "    print(f\"  ✓ TF-IDF (d={d_tfidf:.3f}) ≈ LLM-kw (d={d_llm_kw:.3f}) → LLM may be unnecessary\")\n",
    "else:\n",
    "    print(f\"  ✗ TF-IDF (d={d_tfidf:.3f}) ≠ LLM-kw (d={d_llm_kw:.3f}) → LLM adds value beyond overlap\")\n",
    "\n",
    "# Shuffled vs ordered\n",
    "d_shuffled = cohens_d(c['bare'] - c['shuffled_llm'])\n",
    "if abs(d_shuffled - d_llm_kw) < 0.05:\n",
    "    print(f\"  ✓ Shuffled (d={d_shuffled:.3f}) ≈ Ordered (d={d_llm_kw:.3f}) → coherence doesn't matter\")\n",
    "else:\n",
    "    print(f\"  ✗ Shuffled (d={d_shuffled:.3f}) ≠ Ordered (d={d_llm_kw:.3f}) → coherence matters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "609df7bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-09T04:42:16.761027Z",
     "iopub.status.busy": "2026-02-09T04:42:16.760334Z",
     "iopub.status.idle": "2026-02-09T04:42:16.797736Z",
     "shell.execute_reply": "2026-02-09T04:42:16.796626Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "FORMAT & COHERENCE ABLATION RESULTS\n",
      "======================================================================\n",
      "\n",
      "H2 — COHERENCE: Shuffled-LLM vs LLM-keyword\n",
      "  d = +0.150, win% = 60.2%, t = 6.42, p = 1.77e-10\n",
      "  → Coherence MATTERS: ordered tokens outperform shuffled\n",
      "\n",
      "H3a — FORMAT (Oracle): Oracle-full-question vs Oracle-as-keywords\n",
      "  d = +0.092, win% = 54.0%, t = 3.94, p = 8.52e-05\n",
      "  → Oracle-as-keywords IMPROVES over full question (question format hurts)\n",
      "\n",
      "H3b — FORMAT (LLM): LLM-keyword vs LLM-question\n",
      "  d = -0.018, win% = 53.4%, t = -0.78, p = 4.38e-01\n",
      "  → No advantage for keyword format in LLM surrogates\n",
      "\n",
      "H4 — SPECIFICITY: TF-IDF (right doc) vs Anti-keywords (wrong doc)\n",
      "  d = -0.026, win% = 49.3%, t = -1.13, p = 2.59e-01\n",
      "  → Specificity does NOT matter: any content words work\n",
      "\n",
      "H5 — STACKING: LLM-keyword+sep vs best-of(LLM-keyword, sep-only)\n",
      "  d = -0.119, win% = 40.8%, t = -5.11, p = 3.48e-07\n",
      "  → No stacking benefit\n"
     ]
    }
   ],
   "source": [
    "# Cell 11: Format & coherence ablation results\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"FORMAT & COHERENCE ABLATION RESULTS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# H2: Coherence — shuffled vs ordered\n",
    "delta_coherence = c['shuffled_llm'] - c['llm_keyword']\n",
    "d_coherence = cohens_d(delta_coherence)\n",
    "t_coh, p_coh = stats.ttest_1samp(delta_coherence, 0)\n",
    "print(f\"\\nH2 — COHERENCE: Shuffled-LLM vs LLM-keyword\")\n",
    "print(f\"  d = {d_coherence:+.3f}, win% = {np.mean(delta_coherence > 0)*100:.1f}%, \"\n",
    "      f\"t = {t_coh:.2f}, p = {p_coh:.2e}\")\n",
    "if p_coh < BONFERRONI_ALPHA and d_coherence > 0:\n",
    "    print(f\"  → Coherence MATTERS: ordered tokens outperform shuffled\")\n",
    "elif p_coh < BONFERRONI_ALPHA and d_coherence < 0:\n",
    "    print(f\"  → SURPRISING: shuffled tokens are BETTER than ordered\")\n",
    "else:\n",
    "    print(f\"  → Coherence does NOT matter: order is irrelevant, just token identity\")\n",
    "\n",
    "# H3: Format — question syntax\n",
    "# Oracle\n",
    "delta_fmt_oracle = c['oracle_trunc'] - c['oracle_as_kw']\n",
    "d_fmt_oracle = cohens_d(delta_fmt_oracle)\n",
    "t_fo, p_fo = stats.ttest_1samp(delta_fmt_oracle, 0)\n",
    "print(f\"\\nH3a — FORMAT (Oracle): Oracle-full-question vs Oracle-as-keywords\")\n",
    "print(f\"  d = {d_fmt_oracle:+.3f}, win% = {np.mean(delta_fmt_oracle > 0)*100:.1f}%, \"\n",
    "      f\"t = {t_fo:.2f}, p = {p_fo:.2e}\")\n",
    "if d_fmt_oracle > 0:\n",
    "    print(f\"  → Oracle-as-keywords IMPROVES over full question (question format hurts)\")\n",
    "else:\n",
    "    print(f\"  → Oracle-as-keywords does NOT improve over full question\")\n",
    "\n",
    "# LLM\n",
    "delta_fmt_llm = c['llm_question'] - c['llm_keyword']\n",
    "d_fmt_llm = cohens_d(delta_fmt_llm)\n",
    "t_fl, p_fl = stats.ttest_1samp(delta_fmt_llm, 0)\n",
    "print(f\"\\nH3b — FORMAT (LLM): LLM-keyword vs LLM-question\")\n",
    "print(f\"  d = {d_fmt_llm:+.3f}, win% = {np.mean(delta_fmt_llm > 0)*100:.1f}%, \"\n",
    "      f\"t = {t_fl:.2f}, p = {p_fl:.2e}\")\n",
    "if d_fmt_llm > 0:\n",
    "    print(f\"  → LLM-keyword OUTPERFORMS LLM-question (keyword format is better)\")\n",
    "else:\n",
    "    print(f\"  → No advantage for keyword format in LLM surrogates\")\n",
    "\n",
    "# H4: Passage Specificity\n",
    "delta_spec = c['anti_keywords'] - c['tfidf_keywords']\n",
    "d_spec = cohens_d(delta_spec)\n",
    "t_sp, p_sp = stats.ttest_1samp(delta_spec, 0)\n",
    "print(f\"\\nH4 — SPECIFICITY: TF-IDF (right doc) vs Anti-keywords (wrong doc)\")\n",
    "print(f\"  d = {d_spec:+.3f}, win% = {np.mean(delta_spec > 0)*100:.1f}%, \"\n",
    "      f\"t = {t_sp:.2f}, p = {p_sp:.2e}\")\n",
    "if p_sp < BONFERRONI_ALPHA and d_spec > 0:\n",
    "    print(f\"  → Passage-SPECIFIC keywords are BETTER than wrong-doc keywords\")\n",
    "elif p_sp >= BONFERRONI_ALPHA:\n",
    "    print(f\"  → Specificity does NOT matter: any content words work\")\n",
    "\n",
    "# H5: Stacking\n",
    "delta_stack = np.minimum(c['llm_keyword'], c['separator_only']) - c['llm_keyword_sep']\n",
    "d_stack = cohens_d(delta_stack)\n",
    "t_st, p_st = stats.ttest_1samp(delta_stack, 0)\n",
    "print(f\"\\nH5 — STACKING: LLM-keyword+sep vs best-of(LLM-keyword, sep-only)\")\n",
    "print(f\"  d = {d_stack:+.3f}, win% = {np.mean(delta_stack > 0)*100:.1f}%, \"\n",
    "      f\"t = {t_st:.2f}, p = {p_st:.2e}\")\n",
    "if p_st < BONFERRONI_ALPHA and d_stack > 0:\n",
    "    print(f\"  → Stacking WORKS: combining prefix + suffix exceeds either alone\")\n",
    "elif p_st < 0.05 and d_stack > 0:\n",
    "    print(f\"  → Suggestive stacking benefit (not Bonferroni significant)\")\n",
    "else:\n",
    "    print(f\"  → No stacking benefit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f88bf198",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-09T04:42:16.802390Z",
     "iopub.status.busy": "2026-02-09T04:42:16.801505Z",
     "iopub.status.idle": "2026-02-09T04:42:16.850175Z",
     "shell.execute_reply": "2026-02-09T04:42:16.849125Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "LLM TEMPLATE RANKING + COST-BENEFIT\n",
      "======================================================================\n",
      "\n",
      "Template                        d vs Bare  d vs Random  Win% vs Bare     Cost\n",
      "------------------------------------------------------------------------------\n",
      "Keyword (3-6 words)                 0.234        0.099         67.6%    cheap\n",
      "Question (5-12 words)               0.200        0.102         61.2%    cheap\n",
      "Symptom (4-10 words)                0.252        0.119         66.7%    cheap\n",
      "Summary (2 sentences)              -0.023       -0.104         53.4% moderate\n",
      "Messy/informal (3-8 words)          0.143        0.050         61.4%    cheap\n",
      "------------------------------------------------------------------------------\n",
      "TF-IDF keywords                     0.101       -0.014         60.5%     free\n",
      "First sentence echo                 0.007       -0.089         55.1%     free\n",
      "Oracle-as-keywords                  0.098       -0.031         60.3%   oracle\n",
      "\n",
      "--- Pairwise LLM Template Comparisons ---\n",
      "  llm_keyword vs llm_question: d=-0.018, p=4.38e-01 ns\n",
      "  llm_keyword vs llm_symptom: d=-0.031, p=1.87e-01 ns\n",
      "  llm_keyword vs llm_summary: d=+0.195, p=1.37e-16 ***\n",
      "  llm_keyword vs llm_messy: d=+0.042, p=7.35e-02 ns\n",
      "  llm_question vs llm_symptom: d=-0.008, p=7.18e-01 ns\n",
      "  llm_question vs llm_summary: d=+0.217, p=3.50e-20 ***\n",
      "  llm_question vs llm_messy: d=+0.064, p=5.94e-03 *\n",
      "  llm_symptom vs llm_summary: d=+0.238, p=8.69e-24 ***\n",
      "  llm_symptom vs llm_messy: d=+0.071, p=2.33e-03 **\n",
      "  llm_summary vs llm_messy: d=-0.175, p=1.01e-13 ***\n",
      "\n",
      "--- Production Recommendation ---\n",
      "Best overall: llm_keyword (d=+0.234 vs bare)\n",
      "LLM keyword over TF-IDF: Δd = +0.133\n",
      "→ LLM IS worth the cost (meaningful improvement over free TF-IDF)\n"
     ]
    }
   ],
   "source": [
    "# Cell 12: Template ranking + production cost-benefit analysis\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"LLM TEMPLATE RANKING + COST-BENEFIT\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "llm_conditions = [\n",
    "    ('llm_keyword', 'Keyword (3-6 words)', 'cheap'),\n",
    "    ('llm_question', 'Question (5-12 words)', 'cheap'),\n",
    "    ('llm_symptom', 'Symptom (4-10 words)', 'cheap'),\n",
    "    ('llm_summary', 'Summary (2 sentences)', 'moderate'),\n",
    "    ('llm_messy', 'Messy/informal (3-8 words)', 'cheap'),\n",
    "]\n",
    "\n",
    "non_llm_conditions = [\n",
    "    ('tfidf_keywords', 'TF-IDF keywords', 'free'),\n",
    "    ('passage_echo', 'First sentence echo', 'free'),\n",
    "    ('oracle_as_kw', 'Oracle-as-keywords', 'oracle'),\n",
    "]\n",
    "\n",
    "print(f\"\\n{'Template':<30} {'d vs Bare':>10} {'d vs Random':>12} {'Win% vs Bare':>13} {'Cost':>8}\")\n",
    "print(\"-\" * 78)\n",
    "\n",
    "# LLM templates\n",
    "for cname, label, cost in llm_conditions:\n",
    "    d_bare = cohens_d(c['bare'] - c[cname])\n",
    "    d_random = cohens_d(c['random_trunc'] - c[cname])\n",
    "    win = np.mean(c['bare'] > c[cname]) * 100\n",
    "    print(f\"{label:<30} {d_bare:>10.3f} {d_random:>12.3f} {win:>12.1f}% {cost:>8}\")\n",
    "\n",
    "print(\"-\" * 78)\n",
    "# Non-LLM alternatives\n",
    "for cname, label, cost in non_llm_conditions:\n",
    "    d_bare = cohens_d(c['bare'] - c[cname])\n",
    "    d_random = cohens_d(c['random_trunc'] - c[cname])\n",
    "    win = np.mean(c['bare'] > c[cname]) * 100\n",
    "    print(f\"{label:<30} {d_bare:>10.3f} {d_random:>12.3f} {win:>12.1f}% {cost:>8}\")\n",
    "\n",
    "# Pairwise template comparisons\n",
    "print(f\"\\n--- Pairwise LLM Template Comparisons ---\")\n",
    "llm_cnames = [cn for cn, _, _ in llm_conditions]\n",
    "for i in range(len(llm_cnames)):\n",
    "    for j in range(i+1, len(llm_cnames)):\n",
    "        cn_a, cn_b = llm_cnames[i], llm_cnames[j]\n",
    "        delta = c[cn_b] - c[cn_a]  # positive means a is better\n",
    "        d = cohens_d(delta)\n",
    "        t, p = stats.ttest_1samp(delta, 0)\n",
    "        sig = \"***\" if p < 0.001 else \"**\" if p < BONFERRONI_ALPHA else \"*\" if p < 0.05 else \"ns\"\n",
    "        print(f\"  {cn_a} vs {cn_b}: d={d:+.3f}, p={p:.2e} {sig}\")\n",
    "\n",
    "# Production recommendation\n",
    "print(f\"\\n--- Production Recommendation ---\")\n",
    "best_d = -999\n",
    "best_name = \"\"\n",
    "for cname in ['llm_keyword', 'tfidf_keywords', 'passage_echo']:\n",
    "    d = cohens_d(c['bare'] - c[cname])\n",
    "    if d > best_d:\n",
    "        best_d = d\n",
    "        best_name = cname\n",
    "print(f\"Best overall: {best_name} (d={best_d:+.3f} vs bare)\")\n",
    "\n",
    "# Is LLM worth it?\n",
    "d_tfidf_bare = cohens_d(c['bare'] - c['tfidf_keywords'])\n",
    "d_llmkw_bare = cohens_d(c['bare'] - c['llm_keyword'])\n",
    "improvement = d_llmkw_bare - d_tfidf_bare\n",
    "print(f\"LLM keyword over TF-IDF: Δd = {improvement:+.3f}\")\n",
    "if improvement > 0.05:\n",
    "    print(f\"→ LLM IS worth the cost (meaningful improvement over free TF-IDF)\")\n",
    "else:\n",
    "    print(f\"→ LLM may NOT be worth the cost (minimal improvement over free TF-IDF)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f3396a34",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-09T04:42:16.854766Z",
     "iopub.status.busy": "2026-02-09T04:42:16.853921Z",
     "iopub.status.idle": "2026-02-09T04:42:16.886918Z",
     "shell.execute_reply": "2026-02-09T04:42:16.886221Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "HARDNESS QUINTILE BREAKDOWN\n",
      "======================================================================\n",
      "\n",
      "Condition             Q1 (easiest)            Q2            Q3            Q4  Q5 (hardest)       Overall\n",
      "--------------------------------------------------------------------------------------------------------\n",
      "random_trunc                +0.075        +0.244        +0.221        +0.246        +0.117        +0.125\n",
      "oracle_trunc                -0.259        -0.151        -0.067        +0.160        +0.124        +0.034\n",
      "oracle_as_kw                -0.134        +0.113        +0.074        +0.282        +0.117        +0.098\n",
      "tfidf_keywords              -0.130        +0.040        +0.169        +0.277        +0.119        +0.101\n",
      "llm_keyword                 -0.082        +0.222        +0.349        +0.355        +0.340        +0.234\n",
      "llm_question                -0.212        -0.046        +0.256        +0.368        +0.378        +0.200\n",
      "llm_symptom                 -0.110        +0.243        +0.420        +0.427        +0.379        +0.252\n",
      "passage_echo                -0.160        -0.257        -0.151        +0.176        +0.146        +0.007\n",
      "shuffled_llm                -0.060        +0.185        +0.204        +0.227        +0.128        +0.115\n",
      "llm_keyword_sep             -0.226        -0.036        +0.258        +0.475        +0.630        +0.297\n",
      "\n",
      "Hardness interaction (r between bare NLL and benefit):\n",
      "  random_trunc        : r=+0.128, p=3.60e-08 ***\n",
      "  oracle_trunc        : r=+0.127, p=4.71e-08 ***\n",
      "  oracle_as_kw        : r=+0.144, p=5.93e-10 ***\n",
      "  tfidf_keywords      : r=+0.182, p=3.45e-15 ***\n",
      "  llm_keyword         : r=+0.284, p=2.51e-35 ***\n",
      "  llm_question        : r=+0.369, p=2.65e-60 ***\n",
      "  llm_symptom         : r=+0.372, p=3.44e-61 ***\n",
      "  passage_echo        : r=+0.241, p=1.03e-25 ***\n",
      "  shuffled_llm        : r=+0.142, p=1.11e-09 ***\n",
      "  llm_keyword_sep     : r=+0.574, p=1.58e-161 ***\n"
     ]
    }
   ],
   "source": [
    "# Cell 13: Hardness quintile breakdown (all conditions)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"HARDNESS QUINTILE BREAKDOWN\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "bare_valid = c['bare']\n",
    "quintile_boundaries = np.percentile(bare_valid, [20, 40, 60, 80])\n",
    "quintile_labels = ['Q1 (easiest)', 'Q2', 'Q3', 'Q4', 'Q5 (hardest)']\n",
    "\n",
    "def get_quintile(nll, boundaries):\n",
    "    for i, b in enumerate(boundaries):\n",
    "        if nll <= b:\n",
    "            return i\n",
    "    return len(boundaries)\n",
    "\n",
    "quintiles = np.array([get_quintile(nll, quintile_boundaries) for nll in bare_valid])\n",
    "\n",
    "# Header\n",
    "conditions_to_show = [\n",
    "    'random_trunc', 'oracle_trunc', 'oracle_as_kw', 'tfidf_keywords',\n",
    "    'llm_keyword', 'llm_question', 'llm_symptom', 'passage_echo',\n",
    "    'shuffled_llm', 'llm_keyword_sep',\n",
    "]\n",
    "header = f\"{'Condition':<20}\" + \"\".join(f\"{ql:>14}\" for ql in quintile_labels) + f\"{'Overall':>14}\"\n",
    "print(f\"\\n{header}\")\n",
    "print(\"-\" * (20 + 14 * 6))\n",
    "\n",
    "hardness_breakdown = {}\n",
    "for cname in conditions_to_show:\n",
    "    row = f\"{cname:<20}\"\n",
    "    quintile_ds = []\n",
    "    for q in range(5):\n",
    "        mask_q = quintiles == q\n",
    "        if np.sum(mask_q) < 10:\n",
    "            row += f\"{'n/a':>14}\"\n",
    "            quintile_ds.append(None)\n",
    "        else:\n",
    "            delta = bare_valid[mask_q] - c[cname][mask_q]\n",
    "            d = cohens_d(delta)\n",
    "            row += f\"{d:>+14.3f}\"\n",
    "            quintile_ds.append(float(d))\n",
    "    # Overall\n",
    "    d_all = cohens_d(bare_valid - c[cname])\n",
    "    row += f\"{d_all:>+14.3f}\"\n",
    "    print(row)\n",
    "    hardness_breakdown[cname] = {\n",
    "        'quintile_ds': quintile_ds,\n",
    "        'overall_d': float(d_all),\n",
    "    }\n",
    "\n",
    "# Hardness interaction correlations\n",
    "print(f\"\\nHardness interaction (r between bare NLL and benefit):\")\n",
    "for cname in conditions_to_show:\n",
    "    delta = bare_valid - c[cname]\n",
    "    r, p = stats.pearsonr(bare_valid, delta)\n",
    "    sig = \"***\" if p < 0.001 else \"**\" if p < 0.01 else \"*\" if p < 0.05 else \"ns\"\n",
    "    print(f\"  {cname:<20}: r={r:+.3f}, p={p:.2e} {sig}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f30ecae6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-09T04:42:16.890137Z",
     "iopub.status.busy": "2026-02-09T04:42:16.889892Z",
     "iopub.status.idle": "2026-02-09T04:42:20.271902Z",
     "shell.execute_reply": "2026-02-09T04:42:20.271018Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plots saved to results/exp06/analysis_plots.png\n"
     ]
    }
   ],
   "source": [
    "# Cell 14: Plots (overlap scatter, condition bars, hardness heatmap)\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "\n",
    "# --- Plot 1: Overlap scatter (pooled across all conditions) ---\n",
    "ax = axes[0, 0]\n",
    "# Color by condition type\n",
    "cond_colors = {\n",
    "    'random_trunc': 'gray', 'oracle_trunc': 'royalblue', 'oracle_as_kw': 'cornflowerblue',\n",
    "    'anti_keywords': 'salmon', 'tfidf_keywords': 'orange', 'passage_echo': 'gold',\n",
    "    'shuffled_llm': 'mediumpurple', 'llm_keyword': 'forestgreen', 'llm_question': 'limegreen',\n",
    "    'llm_symptom': 'darkgreen', 'llm_summary': 'olive', 'llm_messy': 'teal',\n",
    "}\n",
    "for cname, okey in cond_to_overlap.items():\n",
    "    ovs = [overlap_data[i_orig][okey] for i_orig in valid_indices]\n",
    "    deltas_plot = c['bare'] - c[cname]\n",
    "    ax.scatter(ovs, deltas_plot, alpha=0.03, s=3, c=cond_colors.get(cname, 'gray'))\n",
    "# Overlay condition means\n",
    "for cname, okey in cond_to_overlap.items():\n",
    "    ovs = [overlap_data[i_orig][okey] for i_orig in valid_indices]\n",
    "    mean_ov = np.mean(ovs)\n",
    "    mean_delta = np.mean(c['bare'] - c[cname])\n",
    "    ax.scatter([mean_ov], [mean_delta], s=80, c=cond_colors.get(cname, 'gray'),\n",
    "               edgecolors='black', linewidths=1, zorder=5)\n",
    "    ax.annotate(cname.replace('_', '\\n'), (mean_ov, mean_delta), fontsize=5,\n",
    "                ha='center', va='bottom')\n",
    "ax.axhline(y=0, color='red', linestyle='--', alpha=0.5)\n",
    "ax.set_xlabel('Token Jaccard Overlap with Passage')\n",
    "ax.set_ylabel('NLL Benefit vs Bare (positive = better)')\n",
    "ax.set_title(f'Overlap vs Benefit (r={r_all:.3f})')\n",
    "\n",
    "# --- Plot 2: All conditions bar chart (Cohen's d vs bare) ---\n",
    "ax = axes[0, 1]\n",
    "cnames_sorted = sorted(\n",
    "    [cn for cn in CONDITION_NAMES if cn != 'bare'],\n",
    "    key=lambda cn: cohens_d(c['bare'] - c[cn]),\n",
    "    reverse=True\n",
    ")\n",
    "ds = [cohens_d(c['bare'] - c[cn]) for cn in cnames_sorted]\n",
    "colors_bar = [cond_colors.get(cn, 'gray') for cn in cnames_sorted]\n",
    "bars = ax.barh(range(len(cnames_sorted)), ds, color=colors_bar, edgecolor='black', linewidth=0.5)\n",
    "ax.set_yticks(range(len(cnames_sorted)))\n",
    "ax.set_yticklabels(cnames_sorted, fontsize=8)\n",
    "ax.axvline(x=0, color='gray', linestyle='--')\n",
    "ax.set_xlabel(\"Cohen's d vs Bare\")\n",
    "ax.set_title('All Conditions vs Bare (d > 0 = better)')\n",
    "ax.invert_yaxis()\n",
    "\n",
    "# --- Plot 3: Mechanism ablation summary ---\n",
    "ax = axes[0, 2]\n",
    "mech_labels = ['M1:\\nCoherence', 'M2:\\nFormat\\n(Oracle)', 'M3:\\nFormat\\n(LLM)',\n",
    "               'M4:\\nSpecificity', 'M5:\\nOverlap\\nCeiling', 'M6:\\nLLM vs\\nTF-IDF',\n",
    "               'M7:\\nStacking']\n",
    "mech_ds = [comparison_results[k]['cohens_d'] for k in\n",
    "           ['M1: Shuffled vs LLM-kw', 'M2: Oracle-kw vs Oracle', 'M3: LLM-kw vs LLM-question',\n",
    "            'M4: TF-IDF vs Anti-kw', 'M5: Echo vs LLM-kw', 'M6: TF-IDF vs LLM-kw',\n",
    "            'M7: LLM-kw+sep vs best single']]\n",
    "mech_sig = [comparison_results[k]['bonferroni_significant'] for k in\n",
    "            ['M1: Shuffled vs LLM-kw', 'M2: Oracle-kw vs Oracle', 'M3: LLM-kw vs LLM-question',\n",
    "             'M4: TF-IDF vs Anti-kw', 'M5: Echo vs LLM-kw', 'M6: TF-IDF vs LLM-kw',\n",
    "             'M7: LLM-kw+sep vs best single']]\n",
    "mech_colors = ['mediumpurple' if s else 'lightgray' for s in mech_sig]\n",
    "ax.bar(range(len(mech_labels)), mech_ds, color=mech_colors, edgecolor='black', linewidth=0.5)\n",
    "ax.set_xticks(range(len(mech_labels)))\n",
    "ax.set_xticklabels(mech_labels, fontsize=7)\n",
    "ax.axhline(y=0, color='gray', linestyle='--')\n",
    "ax.set_ylabel(\"Cohen's d\")\n",
    "ax.set_title('Mechanism Tests (colored = Bonferroni sig)')\n",
    "\n",
    "# --- Plot 4: Hardness × condition heatmap ---\n",
    "ax = axes[1, 0]\n",
    "hm_conditions = conditions_to_show\n",
    "hm_data = []\n",
    "for cname in hm_conditions:\n",
    "    row = []\n",
    "    for q in range(5):\n",
    "        mask_q = quintiles == q\n",
    "        if np.sum(mask_q) < 10:\n",
    "            row.append(0)\n",
    "        else:\n",
    "            delta = bare_valid[mask_q] - c[cname][mask_q]\n",
    "            row.append(cohens_d(delta))\n",
    "    hm_data.append(row)\n",
    "hm_data = np.array(hm_data)\n",
    "im = ax.imshow(hm_data, cmap='RdBu_r', vmin=-0.5, vmax=0.5, aspect='auto')\n",
    "ax.set_xticks(range(5))\n",
    "ax.set_xticklabels(quintile_labels, fontsize=7)\n",
    "ax.set_yticks(range(len(hm_conditions)))\n",
    "ax.set_yticklabels(hm_conditions, fontsize=7)\n",
    "for i in range(len(hm_conditions)):\n",
    "    for j in range(5):\n",
    "        ax.text(j, i, f\"{hm_data[i,j]:+.2f}\", ha='center', va='center', fontsize=6)\n",
    "plt.colorbar(im, ax=ax, label=\"Cohen's d vs bare\")\n",
    "ax.set_title('Hardness × Condition (d vs bare)')\n",
    "\n",
    "# --- Plot 5: Cross-condition median overlap vs mean d ---\n",
    "ax = axes[1, 1]\n",
    "for i, (cname, okey) in enumerate(cond_to_overlap.items()):\n",
    "    ax.scatter([cond_median_overlap[i]], [cond_mean_d[i]],\n",
    "               s=80, c=cond_colors.get(cname, 'gray'),\n",
    "               edgecolors='black', linewidths=1, zorder=5)\n",
    "    ax.annotate(cname.replace('_', ' '), (cond_median_overlap[i], cond_mean_d[i]),\n",
    "                fontsize=7, ha='left', va='bottom')\n",
    "# Fit line\n",
    "z = np.polyfit(cond_median_overlap, cond_mean_d, 1)\n",
    "x_line = np.linspace(min(cond_median_overlap), max(cond_median_overlap), 50)\n",
    "ax.plot(x_line, np.polyval(z, x_line), 'r--', alpha=0.7)\n",
    "ax.set_xlabel('Median Token Overlap')\n",
    "ax.set_ylabel(\"Mean Cohen's d vs Bare\")\n",
    "ax.set_title(f'Cross-Condition Overlap vs Effect (r={r_cross:.3f})')\n",
    "ax.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "\n",
    "# --- Plot 6: LLM template ranking bar chart ---\n",
    "ax = axes[1, 2]\n",
    "template_conds = [\n",
    "    ('llm_keyword', 'Keyword'), ('llm_question', 'Question'),\n",
    "    ('llm_symptom', 'Symptom'), ('llm_summary', 'Summary'),\n",
    "    ('llm_messy', 'Messy'),\n",
    "]\n",
    "tmpl_ds = [cohens_d(c['bare'] - c[cn]) for cn, _ in template_conds]\n",
    "tmpl_names = [label for _, label in template_conds]\n",
    "tmpl_colors = ['forestgreen', 'limegreen', 'darkgreen', 'olive', 'teal']\n",
    "ax.bar(range(len(tmpl_ds)), tmpl_ds, color=tmpl_colors, edgecolor='black', linewidth=0.5)\n",
    "ax.set_xticks(range(len(tmpl_ds)))\n",
    "ax.set_xticklabels(tmpl_names, fontsize=9)\n",
    "ax.axhline(y=0, color='gray', linestyle='--')\n",
    "ax.set_ylabel(\"Cohen's d vs Bare\")\n",
    "ax.set_title('LLM Template Ranking')\n",
    "\n",
    "plt.suptitle('Exp 06: Surrogate Deep-Dive — Mechanism Decomposition', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'analysis_plots.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"Plots saved to {RESULTS_DIR / 'analysis_plots.png'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a56732a2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-09T04:42:20.275493Z",
     "iopub.status.busy": "2026-02-09T04:42:20.274826Z",
     "iopub.status.idle": "2026-02-09T04:42:20.360911Z",
     "shell.execute_reply": "2026-02-09T04:42:20.360131Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to results/exp06/results.json\n",
      "File size: 1298.5 KB\n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Cell 15: Save comprehensive results JSON\n",
    "\n",
    "final = {\n",
    "    'experiment': 'exp06_surrogate_deep_dive',\n",
    "    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'config': {\n",
    "        'model_name': config.model_name,\n",
    "        'seed': SEED,\n",
    "        'n_eval': N,\n",
    "        'n_valid': n_valid,\n",
    "        'n_excluded': n_excluded,\n",
    "        'min_passage_words': config.min_passage_words,\n",
    "        'max_passage_words': config.max_passage_words,\n",
    "        'n_conditions': len(CONDITION_NAMES),\n",
    "        'n_comparisons': N_COMPARISONS,\n",
    "        'bonferroni_alpha': BONFERRONI_ALPHA,\n",
    "    },\n",
    "    'condition_names': CONDITION_NAMES,\n",
    "    'nll_summary': {\n",
    "        cname: {\n",
    "            'mean': float(np.mean(c[cname])),\n",
    "            'std': float(np.std(c[cname])),\n",
    "            'cohens_d_vs_bare': float(cohens_d(c['bare'] - c[cname])) if cname != 'bare' else 0.0,\n",
    "        }\n",
    "        for cname in CONDITION_NAMES\n",
    "    },\n",
    "    'primary_comparisons': comparison_results,\n",
    "    'all_vs_bare': all_vs_bare,\n",
    "    'overlap_analysis': {\n",
    "        'universal_r': float(r_all),\n",
    "        'universal_p': float(p_all),\n",
    "        'cross_condition_r': float(r_cross),\n",
    "        'cross_condition_p': float(p_cross),\n",
    "        'regression_betas': [float(b) for b in betas],\n",
    "        'regression_r_squared': float(r_squared),\n",
    "    },\n",
    "    'hardness_breakdown': hardness_breakdown,\n",
    "    'per_sample_results': results,\n",
    "}\n",
    "\n",
    "with open(FINAL_RESULTS_PATH, 'w') as f:\n",
    "    json.dump(final, f, indent=2)\n",
    "\n",
    "print(f\"Results saved to {FINAL_RESULTS_PATH}\")\n",
    "print(f\"File size: {FINAL_RESULTS_PATH.stat().st_size / 1024:.1f} KB\")\n",
    "print(\"\\nDone!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "afb70e27",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-09T04:42:20.364258Z",
     "iopub.status.busy": "2026-02-09T04:42:20.363726Z",
     "iopub.status.idle": "2026-02-09T04:42:20.878033Z",
     "shell.execute_reply": "2026-02-09T04:42:20.877188Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning up GPU memory...\n",
      "GPU memory: 4.14 GB -> 0.01 GB\n",
      "Cleanup complete.\n"
     ]
    }
   ],
   "source": [
    "# Cell 16: GPU cleanup — free all VRAM\n",
    "import gc\n",
    "\n",
    "print(\"Cleaning up GPU memory...\")\n",
    "mem_before = torch.cuda.memory_allocated() / 1e9\n",
    "\n",
    "del model\n",
    "del tokenizer\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "mem_after = torch.cuda.memory_allocated() / 1e9\n",
    "print(f\"GPU memory: {mem_before:.2f} GB -> {mem_after:.2f} GB\")\n",
    "print(\"Cleanup complete.\")"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cu124.m127",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/base-cu124:m127"
  },
  "kernelspec": {
   "display_name": "Python 3 (Local)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "00b5e1e33f624efb8d1f656398fbd611": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "0ceea85e75094e6a851858ae0dcba211": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "0db8fd4d816b4dfc84c9e5b0ad66ffe6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "0f2dcb489f6141c0a95f0327a3cf6bff": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "13dd7b317bca40d7b8b015dbf51cb7d4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_0ceea85e75094e6a851858ae0dcba211",
       "placeholder": "​",
       "style": "IPY_MODEL_948c74430d994b0facf579145d2a1fe4",
       "tabbable": null,
       "tooltip": null,
       "value": " 8322/10047 [00:01&lt;00:00, 7684.08it/s]"
      }
     },
     "1490ddbb180b42d8a1cac6fa0f8d9ecd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_5b0b093a3620432e95bcf2399771f14d",
       "placeholder": "​",
       "style": "IPY_MODEL_5fb7967fcaca43319eabb90a6b882e71",
       "tabbable": null,
       "tooltip": null,
       "value": " 291/291 [00:55&lt;00:00,  5.40it/s, Materializing param=model.norm.weight]"
      }
     },
     "15e8bcaf207047bd860ec44fd95e032e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_00b5e1e33f624efb8d1f656398fbd611",
       "max": 291,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_9000526f8e59454ab4388440cf658ea1",
       "tabbable": null,
       "tooltip": null,
       "value": 291
      }
     },
     "1701bbfaec5b45d896a4601df4f9fc18": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "19afbad7cc104d278b8e59fa96d89dde": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "207566129f6a4916960bc9ab9a9ea3a1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2560b7cc8f694475be3ba5712dde614a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "2a9656dc2d004f168c431b4be31bfe1c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "38274e0330c746dd90863af1cfacc2a0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_72c1ebbd154d4494adf69166524d43dd",
       "placeholder": "​",
       "style": "IPY_MODEL_d5bb2b836a114d94bf9a3305289c7953",
       "tabbable": null,
       "tooltip": null,
       "value": " 2000/2000 [3:42:07&lt;00:00,  6.56s/it]"
      }
     },
     "3cd8bfebc49146158a7515ea137c5da0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3ce187cc768649fd88fcc3ad1bb7de6e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "4116bdfe94ee4abc8dee1facc8391932": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "419c4ec8d9d74e3a8a64b8f364ebc91f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_9ea36acee1ce423c9af3936268a887ba",
       "placeholder": "​",
       "style": "IPY_MODEL_682030f895944144a35443fbfc621336",
       "tabbable": null,
       "tooltip": null,
       "value": " 2000/2000 [6:56:37&lt;00:00, 11.18s/it]"
      }
     },
     "448bce86bf03435cad725c81af7562a4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_207566129f6a4916960bc9ab9a9ea3a1",
       "max": 2000,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_615c2ff9b02b4dbba77beda90031447d",
       "tabbable": null,
       "tooltip": null,
       "value": 2000
      }
     },
     "4550a6f236284d1d863ae13674ce9c48": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_0f2dcb489f6141c0a95f0327a3cf6bff",
       "placeholder": "​",
       "style": "IPY_MODEL_d0968bc3226a4f2e85f999edc8428db8",
       "tabbable": null,
       "tooltip": null,
       "value": "Filtering:  83%"
      }
     },
     "469a754b1681482e84b5107edeb889f8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_c7e4a534ca004edb956ffacf3aaefee5",
        "IPY_MODEL_15e8bcaf207047bd860ec44fd95e032e",
        "IPY_MODEL_1490ddbb180b42d8a1cac6fa0f8d9ecd"
       ],
       "layout": "IPY_MODEL_4116bdfe94ee4abc8dee1facc8391932",
       "tabbable": null,
       "tooltip": null
      }
     },
     "4a5c80278e394b80802b38c6ead58544": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_b323abfa60c54c2080e4d41b39eb0eaf",
        "IPY_MODEL_62147dc195354d2e876861dacea084fb",
        "IPY_MODEL_38274e0330c746dd90863af1cfacc2a0"
       ],
       "layout": "IPY_MODEL_1701bbfaec5b45d896a4601df4f9fc18",
       "tabbable": null,
       "tooltip": null
      }
     },
     "4d60a4c9b9fe4b7e9f8819d187720ad5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "4e79edb2e0074ba9a87e66efa81da865": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_9b369b6201824cf1b8f184e30ed32429",
       "max": 2000,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_2a9656dc2d004f168c431b4be31bfe1c",
       "tabbable": null,
       "tooltip": null,
       "value": 2000
      }
     },
     "4f2ddd7bbee742f1a1f0660180bcd572": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_b6a7149a2f79463bb2ff3885f58e5e3b",
       "placeholder": "​",
       "style": "IPY_MODEL_4d60a4c9b9fe4b7e9f8819d187720ad5",
       "tabbable": null,
       "tooltip": null,
       "value": "Summaries: 100%"
      }
     },
     "5b0b093a3620432e95bcf2399771f14d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "5fb7967fcaca43319eabb90a6b882e71": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "615c2ff9b02b4dbba77beda90031447d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "62147dc195354d2e876861dacea084fb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_eea469d989de411998f367c805b0467f",
       "max": 2000,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_6f5cba4c304247f0a34ea5b15ae938da",
       "tabbable": null,
       "tooltip": null,
       "value": 2000
      }
     },
     "629b7199d8d0422ebce760d1bb5245da": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "682030f895944144a35443fbfc621336": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "6d2e8292c46542a6a27526fefa7b0577": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "danger",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_f45e6355383947e48f8536333b7dd925",
       "max": 10047,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_629b7199d8d0422ebce760d1bb5245da",
       "tabbable": null,
       "tooltip": null,
       "value": 8322
      }
     },
     "6f5cba4c304247f0a34ea5b15ae938da": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "72c1ebbd154d4494adf69166524d43dd": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "773ed23ed2de49299d02b3e7d9e04bb2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_aa629362cce14f72b1e0f92ab2124933",
       "max": 2000,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_9cf76cb251fc4a95bdbc96b1af6f80a4",
       "tabbable": null,
       "tooltip": null,
       "value": 2000
      }
     },
     "77da81c390d04859b7860d9cccae1280": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_ae116a3d55e9486696b1ce0bb750ac62",
       "placeholder": "​",
       "style": "IPY_MODEL_d160752d339944b3854089ffa1d5df53",
       "tabbable": null,
       "tooltip": null,
       "value": "5-template surrogates: 100%"
      }
     },
     "8524350bf26a41ce9e5ee740cdebc3c2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "86693f84ba524134b2560c88c57e63d2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_faa1aae53b444ec5803d7293badc54f3",
        "IPY_MODEL_4e79edb2e0074ba9a87e66efa81da865",
        "IPY_MODEL_cc84c25c0b7a429d9d455185db4427f9"
       ],
       "layout": "IPY_MODEL_a1b70b7ab4ef4cb9955558e3f2a1d219",
       "tabbable": null,
       "tooltip": null
      }
     },
     "9000526f8e59454ab4388440cf658ea1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "948c74430d994b0facf579145d2a1fe4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "959550bcc7664364a397fa151d561e99": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "988fa1fa6ae0490f970dd77b0210df09": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "9af03d7605e645baa7adabe4828d61d0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "9b369b6201824cf1b8f184e30ed32429": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "9cf76cb251fc4a95bdbc96b1af6f80a4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "9ea36acee1ce423c9af3936268a887ba": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a1b70b7ab4ef4cb9955558e3f2a1d219": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "aa629362cce14f72b1e0f92ab2124933": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ad581e105c4b46e6b17d19a6a17e2f17": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "ae116a3d55e9486696b1ce0bb750ac62": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b323abfa60c54c2080e4d41b39eb0eaf": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_8524350bf26a41ce9e5ee740cdebc3c2",
       "placeholder": "​",
       "style": "IPY_MODEL_d22a66c4ec214475906907b6a695db3d",
       "tabbable": null,
       "tooltip": null,
       "value": "Evaluating: 100%"
      }
     },
     "b6a7149a2f79463bb2ff3885f58e5e3b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c7e4a534ca004edb956ffacf3aaefee5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_959550bcc7664364a397fa151d561e99",
       "placeholder": "​",
       "style": "IPY_MODEL_9af03d7605e645baa7adabe4828d61d0",
       "tabbable": null,
       "tooltip": null,
       "value": "Loading weights: 100%"
      }
     },
     "cc84c25c0b7a429d9d455185db4427f9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_19afbad7cc104d278b8e59fa96d89dde",
       "placeholder": "​",
       "style": "IPY_MODEL_3ce187cc768649fd88fcc3ad1bb7de6e",
       "tabbable": null,
       "tooltip": null,
       "value": " 2000/2000 [00:19&lt;00:00, 104.45it/s]"
      }
     },
     "d0968bc3226a4f2e85f999edc8428db8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "d160752d339944b3854089ffa1d5df53": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "d19f3af876234d76b4e0967f9c02628b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_4f2ddd7bbee742f1a1f0660180bcd572",
        "IPY_MODEL_773ed23ed2de49299d02b3e7d9e04bb2",
        "IPY_MODEL_ff071bd15d25489fa4e1d8ad6055a0f5"
       ],
       "layout": "IPY_MODEL_ee598ae8d3f5492ab3fa2766fd26a4d6",
       "tabbable": null,
       "tooltip": null
      }
     },
     "d22a66c4ec214475906907b6a695db3d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "d265c26139484af18be99b1952f5571f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d55381a61eeb4a648e9d3b3b64ffffa1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_4550a6f236284d1d863ae13674ce9c48",
        "IPY_MODEL_6d2e8292c46542a6a27526fefa7b0577",
        "IPY_MODEL_13dd7b317bca40d7b8b015dbf51cb7d4"
       ],
       "layout": "IPY_MODEL_d265c26139484af18be99b1952f5571f",
       "tabbable": null,
       "tooltip": null
      }
     },
     "d5bb2b836a114d94bf9a3305289c7953": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "d9fe1a60a73e4455ab83151f6e86dfd2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_77da81c390d04859b7860d9cccae1280",
        "IPY_MODEL_448bce86bf03435cad725c81af7562a4",
        "IPY_MODEL_419c4ec8d9d74e3a8a64b8f364ebc91f"
       ],
       "layout": "IPY_MODEL_3cd8bfebc49146158a7515ea137c5da0",
       "tabbable": null,
       "tooltip": null
      }
     },
     "ee598ae8d3f5492ab3fa2766fd26a4d6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "eea469d989de411998f367c805b0467f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f45e6355383947e48f8536333b7dd925": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "faa1aae53b444ec5803d7293badc54f3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_988fa1fa6ae0490f970dd77b0210df09",
       "placeholder": "​",
       "style": "IPY_MODEL_2560b7cc8f694475be3ba5712dde614a",
       "tabbable": null,
       "tooltip": null,
       "value": "Token overlap: 100%"
      }
     },
     "ff071bd15d25489fa4e1d8ad6055a0f5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_0db8fd4d816b4dfc84c9e5b0ad66ffe6",
       "placeholder": "​",
       "style": "IPY_MODEL_ad581e105c4b46e6b17d19a6a17e2f17",
       "tabbable": null,
       "tooltip": null,
       "value": " 2000/2000 [2:16:28&lt;00:00,  3.69s/it]"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
