{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05beea65",
   "metadata": {},
   "source": [
    "# Exp 12: Why Does Priming Fail on Long Documents? — Diagnostic Battery\n",
    "\n",
    "## Background\n",
    "\n",
    "Exp 11 showed that static_fact_trunc (d=+0.472 on MS MARCO) collapses to d=-0.019\n",
    "on Natural Questions (100-4000 word documents). Oracle priming actively HURTS\n",
    "(d=-0.188, p<0.001). The failure is a step function at ~300 words, not gradual.\n",
    "\n",
    "## Three Hypotheses\n",
    "\n",
    "### A. Signal Dilution\n",
    "On MARCO (~90 tokens), 7 prefix tokens contaminate ~83 doc values — an 8% \"dose.\"\n",
    "On NQ (~4000 tokens), the same 7 tokens contaminate ~4000 values — a 0.2% dose.\n",
    "The contamination signal gets drowned by sheer volume.\n",
    "\n",
    "**Prediction:** Proportionally increasing the prefix (via repetition) should restore\n",
    "the effect. Value amplification (boosting the contamination delta) should also help.\n",
    "\n",
    "### B. Attention Redistribution\n",
    "On long docs, query attention is spread across thousands of positions. Even if values\n",
    "are contaminated, each contributes a tiny fraction of the output.\n",
    "\n",
    "**Prediction:** Amplifying the contamination delta (alpha > 1) should help, since\n",
    "the direction is correct but the magnitude is too small.\n",
    "\n",
    "### C. Positional Interference\n",
    "RoPE correction shifts positions by ~7 on a 4000-token sequence. While the relative\n",
    "error is tiny, the absolute correction interacts differently at position 3000+ than\n",
    "at position 50+. The correction may introduce phase noise.\n",
    "\n",
    "**Prediction:** Suffix mode (no RoPE correction needed) should work better than\n",
    "truncated prefix on long docs. Removing RoPE correction should be informative.\n",
    "\n",
    "## 9 Experimental Conditions\n",
    "\n",
    "| # | Condition | Build | Tests |\n",
    "|---|-----------|-------|-------|\n",
    "| 1 | bare | [BOS][doc] | Baseline |\n",
    "| 2 | prefix_1x | [BOS][sf\\n][doc] → trunc+RoPE | Confirms exp 11 failure |\n",
    "| 3 | prefix_5x | [BOS][sf\\n ×5][doc] → trunc+RoPE | Hyp A: 5x dose |\n",
    "| 4 | prefix_20x | [BOS][sf\\n ×20][doc] → trunc+RoPE | Hyp A: 20x dose |\n",
    "| 5 | amplify_2x | bare keys + 2x boosted values | Hyp A+B: amplify delta |\n",
    "| 6 | amplify_5x | bare keys + 5x boosted values | Hyp A+B: stronger boost |\n",
    "| 7 | layers_0_15 | primed values only at layers 0-15 | Signal localization |\n",
    "| 8 | suffix | [BOS][doc][sep][sf] (full context) | Hyp C: no RoPE needed |\n",
    "| 9 | no_rope | [BOS][sf\\n][doc] → trunc, NO RoPE | Hyp C: direct test |\n",
    "\n",
    "Where sf = \"What are the key facts?\" (best static surrogate from exp 07).\n",
    "\n",
    "## Key Analysis\n",
    "\n",
    "For each condition, compute Cohen's d vs bare across 4 length bins.\n",
    "The critical question: **which condition recovers the effect on long docs?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f1688f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-13T16:07:04.064826Z",
     "iopub.status.busy": "2026-02-13T16:07:04.063812Z",
     "iopub.status.idle": "2026-02-13T16:07:07.323479Z",
     "shell.execute_reply": "2026-02-13T16:07:07.322290Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEED: 42\n",
      "Results directory: results/exp12\n",
      "CUDA available: True\n",
      "GPU: NVIDIA L4\n",
      "GPU memory: 23.6 GB\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Setup — permissions, seeds, results directory\n",
    "import os\n",
    "os.umask(0o000)\n",
    "\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "RESULTS_DIR = Path(\"results/exp12\")\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CHECKPOINT_PATH = RESULTS_DIR / \"checkpoint.json\"\n",
    "FINAL_RESULTS_PATH = RESULTS_DIR / \"results.json\"\n",
    "\n",
    "print(f\"SEED: {SEED}\")\n",
    "print(f\"Results directory: {RESULTS_DIR}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3cf5811",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-13T16:07:07.327715Z",
     "iopub.status.busy": "2026-02-13T16:07:07.326889Z",
     "iopub.status.idle": "2026-02-13T16:07:45.267164Z",
     "shell.execute_reply": "2026-02-13T16:07:45.266172Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading mistralai/Mistral-7B-Instruct-v0.2 (4-bit)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b8e865ba71b4102ab57a9a2e950d5e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/291 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded. dtype=torch.float16, device=cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Load model (Mistral-7B 4-bit)\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, DynamicCache\n",
    "\n",
    "MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "print(f\"Loading {MODEL_NAME} (4-bit)...\")\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "print(f\"Model loaded. dtype={model.dtype}, device={model.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47b021dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-13T16:07:45.273555Z",
     "iopub.status.busy": "2026-02-13T16:07:45.272691Z",
     "iopub.status.idle": "2026-02-13T16:07:46.032372Z",
     "shell.execute_reply": "2026-02-13T16:07:46.031327Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config ready\n",
      "  N_EVAL: 400\n",
      "  SAMPLES_PER_BIN: 100\n",
      "  bonferroni_alpha: 0.0063 (8 comparisons)\n",
      "  conditions: 9\n",
      "  static_fact: 'What are the key facts I need to know?'\n",
      "  length_bins: [('short', 100, 300), ('medium', 300, 800), ('long', 800, 2000), ('very_long', 2000, 4000)]\n",
      "  max_doc_words: 4000\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Config, constants, and library imports\n",
    "sys.path.insert(0, \".\")\n",
    "\n",
    "from lib.config import ExperimentConfig\n",
    "from lib.kv_cache import (\n",
    "    _get_cache_keys,\n",
    "    _get_cache_values,\n",
    "    _set_cache_keys,\n",
    "    _set_cache_values,\n",
    "    _ensure_dynamic_cache,\n",
    "    extract_and_truncate_cache_with_bos,\n",
    "    correct_rope_positions_with_bos,\n",
    "    score_answer_with_cache,\n",
    "    replace_values_at_layers,\n",
    ")\n",
    "from lib.analysis import cohens_d\n",
    "from lib.surrogate import STATIC_SURROGATE_QUERIES\n",
    "from scipy import stats\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "config = ExperimentConfig(\n",
    "    model_name=MODEL_NAME,\n",
    "    num_samples=2000,\n",
    "    seed=SEED,\n",
    ")\n",
    "\n",
    "# Templates — bare text, no \"Document:\\n\" framing\n",
    "SURROGATE_PREFIX_TEMPLATE = \"{surrogate}\\n\"\n",
    "DOCUMENT_TEMPLATE = \"{document}\"\n",
    "QUERY_TEMPLATE = \"\\nQuery: {query}\\nAnswer:\"\n",
    "ANSWER_TEMPLATE = \" {answer}\"\n",
    "\n",
    "N_EVAL = 400  # total target (100 per bin)\n",
    "N_CONDITIONS = 9\n",
    "N_COMPARISONS = 8  # each non-bare condition vs bare\n",
    "BONFERRONI_ALPHA = 0.05 / N_COMPARISONS\n",
    "CHECKPOINT_EVERY = 25\n",
    "\n",
    "STATIC_FACT = STATIC_SURROGATE_QUERIES['static_factual']['query']\n",
    "\n",
    "# Length bins (word count)\n",
    "LENGTH_BINS = [\n",
    "    ('short',     100,  300),\n",
    "    ('medium',    300,  800),\n",
    "    ('long',      800,  2000),\n",
    "    ('very_long', 2000, 4000),\n",
    "]\n",
    "SAMPLES_PER_BIN = 100\n",
    "MAX_DOC_WORDS = 4000\n",
    "\n",
    "CONDITION_NAMES = [\n",
    "    'bare',\n",
    "    'prefix_1x',\n",
    "    'prefix_5x',\n",
    "    'prefix_20x',\n",
    "    'amplify_2x',\n",
    "    'amplify_5x',\n",
    "    'layers_0_15',\n",
    "    'suffix',\n",
    "    'no_rope',\n",
    "]\n",
    "\n",
    "# Suffix separator\n",
    "SUFFIX_SEPARATOR = \"\\n\\nRelated question: \"\n",
    "\n",
    "# Repetition counts\n",
    "REP_COUNTS = {'prefix_5x': 5, 'prefix_20x': 20}\n",
    "\n",
    "# Amplification alphas (extrapolation beyond 1.0)\n",
    "AMP_ALPHAS = {'amplify_2x': 2.0, 'amplify_5x': 5.0}\n",
    "\n",
    "print(\"Config ready\")\n",
    "print(f\"  N_EVAL: {N_EVAL}\")\n",
    "print(f\"  SAMPLES_PER_BIN: {SAMPLES_PER_BIN}\")\n",
    "print(f\"  bonferroni_alpha: {BONFERRONI_ALPHA:.4f} ({N_COMPARISONS} comparisons)\")\n",
    "print(f\"  conditions: {len(CONDITION_NAMES)}\")\n",
    "print(f\"  static_fact: '{STATIC_FACT}'\")\n",
    "print(f\"  length_bins: {LENGTH_BINS}\")\n",
    "print(f\"  max_doc_words: {MAX_DOC_WORDS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4994bb1d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-13T16:07:46.036246Z",
     "iopub.status.busy": "2026-02-13T16:07:46.035692Z",
     "iopub.status.idle": "2026-02-13T16:07:46.077423Z",
     "shell.execute_reply": "2026-02-13T16:07:46.076551Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "LOADING NATURAL QUESTIONS (validation split)\n",
      "======================================================================\n",
      "Loaded 315 cached NQ samples from results/exp12/nq_samples.json\n",
      "\n",
      "======================================================================\n",
      "SAMPLE SUMMARY\n",
      "======================================================================\n",
      "  short (100-300w): n=15, mean=212w, range=[130, 284]\n",
      "  medium (300-800w): n=100, mean=589w, range=[321, 796]\n",
      "  long (800-2000w): n=100, mean=1464w, range=[811, 1990]\n",
      "  very_long (2000-4000w): n=100, mean=2895w, range=[2037, 3980]\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Load Natural Questions — stratified by document length\n",
    "from datasets import load_dataset\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"LOADING NATURAL QUESTIONS (validation split)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Check for cached samples (can reuse exp 11 samples with subsampling)\n",
    "SAMPLES_CACHE_PATH = RESULTS_DIR / \"nq_samples.json\"\n",
    "EXP11_SAMPLES_PATH = Path(\"results/exp11/nq_samples.json\")\n",
    "\n",
    "if SAMPLES_CACHE_PATH.exists():\n",
    "    with open(SAMPLES_CACHE_PATH, 'r') as f:\n",
    "        cached = json.load(f)\n",
    "    samples = cached['samples']\n",
    "    print(f\"Loaded {len(samples)} cached NQ samples from {SAMPLES_CACHE_PATH}\")\n",
    "elif EXP11_SAMPLES_PATH.exists():\n",
    "    # Reuse exp 11 samples — subsample to 100 per bin\n",
    "    print(\"Reusing exp 11 NQ samples (subsampling to 100 per bin)...\")\n",
    "    with open(EXP11_SAMPLES_PATH, 'r') as f:\n",
    "        exp11_data = json.load(f)\n",
    "    all_exp11 = exp11_data['samples']\n",
    "\n",
    "    samples = []\n",
    "    for bin_name, _, _ in LENGTH_BINS:\n",
    "        bin_s = [s for s in all_exp11 if s['length_bin'] == bin_name]\n",
    "        samples.extend(bin_s[:SAMPLES_PER_BIN])\n",
    "        print(f\"  {bin_name}: {min(len(bin_s), SAMPLES_PER_BIN)} samples (from {len(bin_s)} available)\")\n",
    "\n",
    "    with open(SAMPLES_CACHE_PATH, 'w') as f:\n",
    "        json.dump({'samples': samples, 'source': 'exp11_subsampled'}, f)\n",
    "    print(f\"Cached {len(samples)} samples to {SAMPLES_CACHE_PATH}\")\n",
    "else:\n",
    "    print(\"Loading NQ dataset from scratch (streaming mode)...\")\n",
    "    nq = load_dataset(\n",
    "        \"google-research-datasets/natural_questions\",\n",
    "        split=\"validation\",\n",
    "        streaming=True,\n",
    "    )\n",
    "\n",
    "    bin_samples = {name: [] for name, _, _ in LENGTH_BINS}\n",
    "    n_processed = 0\n",
    "\n",
    "    for example in tqdm(nq, desc=\"Processing NQ\"):\n",
    "        n_processed += 1\n",
    "\n",
    "        doc_tokens = example['document']['tokens']\n",
    "        if isinstance(doc_tokens, dict):\n",
    "            token_strs = doc_tokens['token']\n",
    "            is_html_flags = doc_tokens['is_html']\n",
    "            clean_tokens = [t for t, h in zip(token_strs, is_html_flags) if not h]\n",
    "        else:\n",
    "            clean_tokens = [t['token'] for t in doc_tokens if not t['is_html']]\n",
    "\n",
    "        doc_text = ' '.join(clean_tokens)\n",
    "        word_count = len(doc_text.split())\n",
    "\n",
    "        if word_count < LENGTH_BINS[0][1]:\n",
    "            continue\n",
    "        if word_count > MAX_DOC_WORDS:\n",
    "            words = doc_text.split()\n",
    "            doc_text = ' '.join(words[:MAX_DOC_WORDS])\n",
    "            word_count = MAX_DOC_WORDS\n",
    "\n",
    "        annotations = example['annotations']\n",
    "        short_answers_list = annotations['short_answers']\n",
    "\n",
    "        answer_text = None\n",
    "        for annotator_sa in short_answers_list:\n",
    "            if not annotator_sa:\n",
    "                continue\n",
    "            texts = annotator_sa.get('text', [])\n",
    "            if texts:\n",
    "                answer_text = texts[0]\n",
    "                break\n",
    "            starts = annotator_sa.get('start_token', [])\n",
    "            ends = annotator_sa.get('end_token', [])\n",
    "            if not starts or not ends:\n",
    "                continue\n",
    "            start_tok = starts[0] if isinstance(starts, list) else starts\n",
    "            end_tok = ends[0] if isinstance(ends, list) else ends\n",
    "            if start_tok >= 0 and end_tok > start_tok:\n",
    "                if isinstance(doc_tokens, dict):\n",
    "                    ans_tokens = [\n",
    "                        doc_tokens['token'][i]\n",
    "                        for i in range(start_tok, min(end_tok, len(doc_tokens['token'])))\n",
    "                        if not doc_tokens['is_html'][i]\n",
    "                    ]\n",
    "                else:\n",
    "                    ans_tokens = [\n",
    "                        doc_tokens[i]['token']\n",
    "                        for i in range(start_tok, min(end_tok, len(doc_tokens)))\n",
    "                        if not doc_tokens[i]['is_html']\n",
    "                    ]\n",
    "                if ans_tokens:\n",
    "                    answer_text = ' '.join(ans_tokens)\n",
    "                    break\n",
    "\n",
    "        if not answer_text or len(answer_text.strip()) == 0:\n",
    "            continue\n",
    "        if len(answer_text.split()) > 20:\n",
    "            continue\n",
    "\n",
    "        question = example['question']\n",
    "        if isinstance(question, dict):\n",
    "            query = question.get('text', '')\n",
    "        else:\n",
    "            query = str(question)\n",
    "        if not query.strip():\n",
    "            continue\n",
    "\n",
    "        for bin_name, bin_min, bin_max in LENGTH_BINS:\n",
    "            if bin_min <= word_count < bin_max:\n",
    "                if len(bin_samples[bin_name]) < SAMPLES_PER_BIN:\n",
    "                    bin_samples[bin_name].append({\n",
    "                        'passage': doc_text,\n",
    "                        'query': query,\n",
    "                        'answer': answer_text,\n",
    "                        'word_count': word_count,\n",
    "                        'length_bin': bin_name,\n",
    "                    })\n",
    "                break\n",
    "\n",
    "        all_full = all(len(bin_samples[name]) >= SAMPLES_PER_BIN for name, _, _ in LENGTH_BINS)\n",
    "        if all_full:\n",
    "            print(f\"All bins full after processing {n_processed} examples.\")\n",
    "            break\n",
    "\n",
    "    samples = []\n",
    "    for bin_name, _, _ in LENGTH_BINS:\n",
    "        bin_s = bin_samples[bin_name]\n",
    "        np.random.seed(SEED)\n",
    "        np.random.shuffle(bin_s)\n",
    "        samples.extend(bin_s)\n",
    "        print(f\"  {bin_name}: {len(bin_s)} samples\")\n",
    "\n",
    "    with open(SAMPLES_CACHE_PATH, 'w') as f:\n",
    "        json.dump({'samples': samples, 'n_processed': n_processed}, f)\n",
    "    print(f\"Cached to {SAMPLES_CACHE_PATH}\")\n",
    "\n",
    "N = len(samples)\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"SAMPLE SUMMARY\")\n",
    "print(f\"{'='*70}\")\n",
    "for bin_name, bin_min, bin_max in LENGTH_BINS:\n",
    "    bin_s = [s for s in samples if s['length_bin'] == bin_name]\n",
    "    if bin_s:\n",
    "        wcs = [s['word_count'] for s in bin_s]\n",
    "        print(f\"  {bin_name} ({bin_min}-{bin_max}w): n={len(bin_s)}, \"\n",
    "              f\"mean={np.mean(wcs):.0f}w, range=[{min(wcs)}, {max(wcs)}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e14a6935",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-13T16:07:46.080831Z",
     "iopub.status.busy": "2026-02-13T16:07:46.080538Z",
     "iopub.status.idle": "2026-02-13T16:07:46.096447Z",
     "shell.execute_reply": "2026-02-13T16:07:46.095721Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "EXPERIMENTAL CONDITIONS — DIAGNOSTIC BATTERY\n",
      "======================================================================\n",
      "\n",
      "### 1. bare ###\n",
      "  Cache: [BOS][doc]\n",
      "  Detail: Baseline — no prefix. Document encoded in isolation.\n",
      "  Tests: —\n",
      "\n",
      "### 2. prefix_1x ###\n",
      "  Cache: [BOS][sf\\n][doc] → trunc + RoPE\n",
      "  Detail: Standard 1x static_fact prefix (11 tokens). Should replicate exp 11 failure.\n",
      "  Tests: Hypothesis: NONE (baseline failure)\n",
      "\n",
      "### 3. prefix_5x ###\n",
      "  Cache: [BOS][sf\\n ×5][doc] → trunc + RoPE  (55 prefix tokens)\n",
      "  Detail: 5x repeated prefix with block-diagonal attention mask (reps can't see each other).\n",
      "  Tests: Hypothesis A: If d(5x) > d(1x), dilution is the issue\n",
      "\n",
      "### 4. prefix_20x ###\n",
      "  Cache: [BOS][sf\\n ×20][doc] → trunc + RoPE  (220 prefix tokens)\n",
      "  Detail: 20x repeated prefix (strongest dilution test). ~2.7% dose on 4000w doc.\n",
      "  Tests: Hypothesis A: If d(20x) >> d(5x), more dose = more signal\n",
      "\n",
      "### 5. amplify_2x ###\n",
      "  Cache: bare keys + v_bare + 2.0 * (v_primed - v_bare)\n",
      "  Detail: Value amplification: double the contamination delta. Keys from bare (correct positions).\n",
      "  Tests: Hypothesis A+B: Tests if contamination direction is correct but too weak\n",
      "\n",
      "### 6. amplify_5x ###\n",
      "  Cache: bare keys + v_bare + 5.0 * (v_primed - v_bare)\n",
      "  Detail: 5x value amplification. If 2x helps but 5x hurts, contamination is partially noise.\n",
      "  Tests: Hypothesis A+B: Finds the signal-vs-noise boundary\n",
      "\n",
      "### 7. layers_0_15 ###\n",
      "  Cache: bare everywhere + primed values at layers 0-15 only\n",
      "  Detail: Exp 09 found signal in layers 0-15 on MARCO. Late layers may add noise on long docs.\n",
      "  Tests: Signal localization: If d(L0-15) > d(1x), late layers hurt\n",
      "\n",
      "### 8. suffix ###\n",
      "  Cache: [BOS][doc][sep][sf] (full context, no truncation)\n",
      "  Detail: Suffix mode: doc can't attend backward to suffix. No RoPE correction needed. Sep='Related question:'\n",
      "  Tests: Hypothesis C: If d(suffix) > d(1x), RoPE correction is the problem\n",
      "\n",
      "### 9. no_rope ###\n",
      "  Cache: [BOS][sf\\n][doc] → truncate only, NO RoPE correction\n",
      "  Detail: Same as prefix_1x but without RoPE position correction. Keys keep original positions.\n",
      "  Tests: Hypothesis C: Direct test — is RoPE correction helping or hurting on long docs?\n",
      "\n",
      "======================================================================\n",
      "PREFIX-TO-DOCUMENT DOSE RATIOS\n",
      "======================================================================\n",
      "  MS MARCO (exp 07):  11 prefix / ~90 doc tokens = 12.2%\n",
      "  NQ short (1x): 11 prefix / ~300 doc tokens = 3.7%\n",
      "  NQ short (5x): 55 prefix / ~300 doc tokens = 18.3%\n",
      "  NQ short (20x): 220 prefix / ~300 doc tokens = 73.3%\n",
      "  NQ medium (1x): 11 prefix / ~825 doc tokens = 1.3%\n",
      "  NQ medium (5x): 55 prefix / ~825 doc tokens = 6.7%\n",
      "  NQ medium (20x): 220 prefix / ~825 doc tokens = 26.7%\n",
      "  NQ long (1x): 11 prefix / ~2100 doc tokens = 0.5%\n",
      "  NQ long (5x): 55 prefix / ~2100 doc tokens = 2.6%\n",
      "  NQ long (20x): 220 prefix / ~2100 doc tokens = 10.5%\n",
      "  NQ very_long (1x): 11 prefix / ~4500 doc tokens = 0.2%\n",
      "  NQ very_long (5x): 55 prefix / ~4500 doc tokens = 1.2%\n",
      "  NQ very_long (20x): 220 prefix / ~4500 doc tokens = 4.9%\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Explain experimental conditions with concrete examples\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"EXPERIMENTAL CONDITIONS — DIAGNOSTIC BATTERY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Pre-tokenize the static fact prefix to show token counts\n",
    "sf_str = SURROGATE_PREFIX_TEMPLATE.format(surrogate=STATIC_FACT)\n",
    "sf_ids = tokenizer(sf_str, add_special_tokens=False)['input_ids']\n",
    "sf_tok_len = len(sf_ids)\n",
    "\n",
    "conditions_explained = [\n",
    "    (\"1. bare\",\n",
    "     \"[BOS][doc]\",\n",
    "     \"Baseline — no prefix. Document encoded in isolation.\",\n",
    "     \"—\"),\n",
    "    (\"2. prefix_1x\",\n",
    "     f\"[BOS][sf\\\\n][doc] → trunc + RoPE\",\n",
    "     f\"Standard 1x static_fact prefix ({sf_tok_len} tokens). Should replicate exp 11 failure.\",\n",
    "     \"Hypothesis: NONE (baseline failure)\"),\n",
    "    (\"3. prefix_5x\",\n",
    "     f\"[BOS][sf\\\\n ×5][doc] → trunc + RoPE  ({5*sf_tok_len} prefix tokens)\",\n",
    "     \"5x repeated prefix with block-diagonal attention mask (reps can't see each other).\",\n",
    "     \"Hypothesis A: If d(5x) > d(1x), dilution is the issue\"),\n",
    "    (\"4. prefix_20x\",\n",
    "     f\"[BOS][sf\\\\n ×20][doc] → trunc + RoPE  ({20*sf_tok_len} prefix tokens)\",\n",
    "     \"20x repeated prefix (strongest dilution test). ~2.7% dose on 4000w doc.\",\n",
    "     \"Hypothesis A: If d(20x) >> d(5x), more dose = more signal\"),\n",
    "    (\"5. amplify_2x\",\n",
    "     \"bare keys + v_bare + 2.0 * (v_primed - v_bare)\",\n",
    "     \"Value amplification: double the contamination delta. Keys from bare (correct positions).\",\n",
    "     \"Hypothesis A+B: Tests if contamination direction is correct but too weak\"),\n",
    "    (\"6. amplify_5x\",\n",
    "     \"bare keys + v_bare + 5.0 * (v_primed - v_bare)\",\n",
    "     \"5x value amplification. If 2x helps but 5x hurts, contamination is partially noise.\",\n",
    "     \"Hypothesis A+B: Finds the signal-vs-noise boundary\"),\n",
    "    (\"7. layers_0_15\",\n",
    "     \"bare everywhere + primed values at layers 0-15 only\",\n",
    "     \"Exp 09 found signal in layers 0-15 on MARCO. Late layers may add noise on long docs.\",\n",
    "     \"Signal localization: If d(L0-15) > d(1x), late layers hurt\"),\n",
    "    (\"8. suffix\",\n",
    "     \"[BOS][doc][sep][sf] (full context, no truncation)\",\n",
    "     f\"Suffix mode: doc can't attend backward to suffix. No RoPE correction needed. Sep='{SUFFIX_SEPARATOR.strip()}'\",\n",
    "     \"Hypothesis C: If d(suffix) > d(1x), RoPE correction is the problem\"),\n",
    "    (\"9. no_rope\",\n",
    "     \"[BOS][sf\\\\n][doc] → truncate only, NO RoPE correction\",\n",
    "     \"Same as prefix_1x but without RoPE position correction. Keys keep original positions.\",\n",
    "     \"Hypothesis C: Direct test — is RoPE correction helping or hurting on long docs?\"),\n",
    "]\n",
    "\n",
    "for name, pattern, detail, hypothesis in conditions_explained:\n",
    "    print(f\"\\n### {name} ###\")\n",
    "    print(f\"  Cache: {pattern}\")\n",
    "    print(f\"  Detail: {detail}\")\n",
    "    print(f\"  Tests: {hypothesis}\")\n",
    "\n",
    "# Show dose ratios\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"PREFIX-TO-DOCUMENT DOSE RATIOS\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"  MS MARCO (exp 07):  {sf_tok_len} prefix / ~90 doc tokens = {sf_tok_len/90*100:.1f}%\")\n",
    "for bin_name, bin_min, bin_max in LENGTH_BINS:\n",
    "    mid_tokens = int((bin_min + bin_max) / 2 * 1.5)\n",
    "    for rep_name, n_reps in [('1x', 1), ('5x', 5), ('20x', 20)]:\n",
    "        dose = n_reps * sf_tok_len / mid_tokens * 100\n",
    "        print(f\"  NQ {bin_name} ({rep_name}): {n_reps*sf_tok_len} prefix / ~{mid_tokens} doc tokens = {dose:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e2e1d10",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-13T16:07:46.099590Z",
     "iopub.status.busy": "2026-02-13T16:07:46.099329Z",
     "iopub.status.idle": "2026-02-13T16:07:46.121106Z",
     "shell.execute_reply": "2026-02-13T16:07:46.120313Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating block-diagonal mask...\n",
      "  Block-diagonal mask validated OK\n",
      "\n",
      "Helper functions ready.\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Helper functions for repeated prefix and value amplification\n",
    "\n",
    "\n",
    "def build_repeated_prefix_mask(prefix_len_single, n_reps, doc_len, dtype, device):\n",
    "    \"\"\"Build block-diagonal attention mask for [BOS][rep1][rep2]...[repN][doc].\n",
    "\n",
    "    Pattern:\n",
    "    - BOS (pos 0): visible to everything, sees only itself\n",
    "    - Each rep block: causal within block, can see BOS, CANNOT see other reps\n",
    "    - Doc tokens: causal, can see BOS and all reps (standard)\n",
    "\n",
    "    Returns: (1, 1, total_len, total_len) additive mask (0=attend, -inf=block)\n",
    "    \"\"\"\n",
    "    total_prefix = n_reps * prefix_len_single\n",
    "    total_len = 1 + total_prefix + doc_len  # BOS + reps + doc\n",
    "\n",
    "    # Start with standard causal mask (lower triangle = 0, upper = -inf)\n",
    "    mask = torch.full((total_len, total_len), float('-inf'), dtype=dtype, device=device)\n",
    "    mask = torch.tril(mask, diagonal=0)  # This gives -inf everywhere; wrong approach\n",
    "    # Correct: start fresh\n",
    "    mask = torch.zeros(total_len, total_len, dtype=dtype, device=device)\n",
    "    # Upper triangle = -inf (no future attention)\n",
    "    for i in range(total_len):\n",
    "        for j in range(i + 1, total_len):\n",
    "            mask[i, j] = float('-inf')\n",
    "\n",
    "    # Block cross-repetition attention (rep i can't see rep j for j != i)\n",
    "    for rep_i in range(n_reps):\n",
    "        start_i = 1 + rep_i * prefix_len_single\n",
    "        end_i = start_i + prefix_len_single\n",
    "        for rep_j in range(n_reps):\n",
    "            if rep_j == rep_i:\n",
    "                continue\n",
    "            start_j = 1 + rep_j * prefix_len_single\n",
    "            end_j = start_j + prefix_len_single\n",
    "            # Block rep_i rows from seeing rep_j columns\n",
    "            # Only block where rep_i could causally see rep_j (j < i)\n",
    "            if rep_j < rep_i:\n",
    "                mask[start_i:end_i, start_j:end_j] = float('-inf')\n",
    "\n",
    "    return mask.unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "\n",
    "def build_repeated_prefix_mask_fast(prefix_len_single, n_reps, doc_len, dtype, device):\n",
    "    \"\"\"Vectorized version of build_repeated_prefix_mask for large sequences.\n",
    "\n",
    "    Same semantics as build_repeated_prefix_mask but uses tensor ops\n",
    "    instead of Python loops for the causal structure.\n",
    "    \"\"\"\n",
    "    total_prefix = n_reps * prefix_len_single\n",
    "    total_len = 1 + total_prefix + doc_len\n",
    "\n",
    "    # Start with causal mask using triu\n",
    "    mask = torch.zeros(total_len, total_len, dtype=dtype, device=device)\n",
    "    mask += torch.triu(\n",
    "        torch.full((total_len, total_len), float('-inf'), dtype=dtype, device=device),\n",
    "        diagonal=1,\n",
    "    )\n",
    "\n",
    "    # Block cross-rep attention: rep i rows cannot see rep j columns (j < i)\n",
    "    for rep_i in range(1, n_reps):\n",
    "        start_i = 1 + rep_i * prefix_len_single\n",
    "        end_i = start_i + prefix_len_single\n",
    "        for rep_j in range(rep_i):\n",
    "            start_j = 1 + rep_j * prefix_len_single\n",
    "            end_j = start_j + prefix_len_single\n",
    "            mask[start_i:end_i, start_j:end_j] = float('-inf')\n",
    "\n",
    "    return mask.unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "\n",
    "def amplify_values(bare_cache, primed_cache, alpha):\n",
    "    \"\"\"Create cache with amplified value contamination.\n",
    "\n",
    "    v_amplified = v_bare + alpha * (v_primed - v_bare)\n",
    "               = (1 - alpha) * v_bare + alpha * v_primed\n",
    "\n",
    "    When alpha > 1.0, this EXTRAPOLATES beyond the primed values,\n",
    "    amplifying the contamination signal.\n",
    "\n",
    "    Keys are taken from bare_cache (correct position encoding).\n",
    "\n",
    "    Args:\n",
    "        bare_cache: Cache with uncontaminated values and correct keys\n",
    "        primed_cache: Cache with contaminated values (from truncated prefix)\n",
    "        alpha: Amplification factor (1.0 = primed, 2.0 = 2x amplification)\n",
    "\n",
    "    Returns:\n",
    "        New DynamicCache with bare keys and amplified values\n",
    "    \"\"\"\n",
    "    bare_cache = _ensure_dynamic_cache(bare_cache)\n",
    "    primed_cache = _ensure_dynamic_cache(primed_cache)\n",
    "\n",
    "    n_layers = len(bare_cache)\n",
    "    new_cache = DynamicCache()\n",
    "\n",
    "    for li in range(n_layers):\n",
    "        k = _get_cache_keys(bare_cache, li).clone()\n",
    "        v_bare = _get_cache_values(bare_cache, li)\n",
    "        v_primed = _get_cache_values(primed_cache, li)\n",
    "        v_amp = v_bare + alpha * (v_primed - v_bare)\n",
    "        new_cache.update(k, v_amp.clone(), li)\n",
    "\n",
    "    return new_cache\n",
    "\n",
    "\n",
    "# Quick validation of the mask\n",
    "print(\"Validating block-diagonal mask...\")\n",
    "test_mask = build_repeated_prefix_mask_fast(3, 2, 2, torch.float32, 'cpu')\n",
    "test_mask_2d = test_mask.squeeze()\n",
    "# Shape should be (1+6+2, 1+6+2) = (9, 9)\n",
    "# BOS=0, R1=[1,2,3], R2=[4,5,6], Doc=[7,8]\n",
    "assert test_mask_2d.shape == (9, 9), f\"Expected (9,9), got {test_mask_2d.shape}\"\n",
    "# R2 (row 4) should NOT see R1 (cols 1,2,3)\n",
    "assert test_mask_2d[4, 1] == float('-inf'), \"R2 should not see R1\"\n",
    "assert test_mask_2d[4, 2] == float('-inf'), \"R2 should not see R1\"\n",
    "# R2 (row 4) SHOULD see BOS (col 0) and itself (col 4)\n",
    "assert test_mask_2d[4, 0] == 0.0, \"R2 should see BOS\"\n",
    "assert test_mask_2d[4, 4] == 0.0, \"R2 should see itself\"\n",
    "# Doc (row 7) should see everything before it\n",
    "assert test_mask_2d[7, 0] == 0.0, \"Doc should see BOS\"\n",
    "assert test_mask_2d[7, 1] == 0.0, \"Doc should see R1\"\n",
    "assert test_mask_2d[7, 4] == 0.0, \"Doc should see R2\"\n",
    "assert test_mask_2d[7, 7] == 0.0, \"Doc should see itself\"\n",
    "print(\"  Block-diagonal mask validated OK\")\n",
    "\n",
    "print(\"\\nHelper functions ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "abb1f2d2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-13T16:07:46.124203Z",
     "iopub.status.busy": "2026-02-13T16:07:46.123945Z",
     "iopub.status.idle": "2026-02-13T16:55:35.526454Z",
     "shell.execute_reply": "2026-02-13T16:55:35.525523Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PHASE: MAIN EVALUATION (9 conditions x 315 samples)\n",
      "======================================================================\n",
      "Static fact prefix: 'What are the key facts I need to know?' (11 tokens per rep)\n",
      "5x prefix: 55 tokens, 20x prefix: 220 tokens\n",
      "Suffix separator: 8 tokens\n",
      "Suffix text: 10 tokens\n",
      "No checkpoint found. Starting fresh.\n",
      "Evaluating samples 0 to 314\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb957a6f9109409f8b68daa47f0025a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/315 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 25/315 | 0.20 s/s | ETA: 24.0 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 50/315 | 0.19 s/s | ETA: 23.2 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 75/315 | 0.19 s/s | ETA: 21.3 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 100/315 | 0.19 s/s | ETA: 19.3 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 125/315 | 0.18 s/s | ETA: 18.0 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 150/315 | 0.17 s/s | ETA: 16.6 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 175/315 | 0.16 s/s | ETA: 14.9 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 200/315 | 0.15 s/s | ETA: 12.9 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 225/315 | 0.14 s/s | ETA: 10.6 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 250/315 | 0.13 s/s | ETA: 8.3 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 275/315 | 0.12 s/s | ETA: 5.5 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 300/315 | 0.11 s/s | ETA: 2.2 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 315/315 | 0.11 s/s | ETA: 0.0 min\n",
      "\n",
      "Evaluation complete: 315 samples in 47.8 min\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Main eval loop — 9 conditions x N samples\n",
    "import gc\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"PHASE: MAIN EVALUATION ({N_CONDITIONS} conditions x {N} samples)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Pre-tokenize fixed strings (reused for every sample)\n",
    "sf_prefix_str = SURROGATE_PREFIX_TEMPLATE.format(surrogate=STATIC_FACT)\n",
    "sf_prefix_enc = tokenizer(sf_prefix_str, return_tensors=\"pt\",\n",
    "                           add_special_tokens=False, padding=False, truncation=False)\n",
    "sf_prefix_ids = sf_prefix_enc['input_ids'].to(config.device)\n",
    "sf_prefix_len = sf_prefix_ids.shape[1]  # tokens per single prefix rep\n",
    "\n",
    "suffix_sep_enc = tokenizer(SUFFIX_SEPARATOR, return_tensors=\"pt\",\n",
    "                            add_special_tokens=False, padding=False, truncation=False)\n",
    "suffix_sep_ids = suffix_sep_enc['input_ids'].to(config.device)\n",
    "\n",
    "suffix_text_enc = tokenizer(STATIC_FACT, return_tensors=\"pt\",\n",
    "                             add_special_tokens=False, padding=False, truncation=False)\n",
    "suffix_text_ids = suffix_text_enc['input_ids'].to(config.device)\n",
    "\n",
    "print(f\"Static fact prefix: '{STATIC_FACT}' ({sf_prefix_len} tokens per rep)\")\n",
    "print(f\"5x prefix: {5*sf_prefix_len} tokens, 20x prefix: {20*sf_prefix_len} tokens\")\n",
    "print(f\"Suffix separator: {suffix_sep_ids.shape[1]} tokens\")\n",
    "print(f\"Suffix text: {suffix_text_ids.shape[1]} tokens\")\n",
    "\n",
    "# Checkpoint resume\n",
    "results = []\n",
    "start_idx = 0\n",
    "\n",
    "if CHECKPOINT_PATH.exists():\n",
    "    with open(CHECKPOINT_PATH, 'r') as f:\n",
    "        ckpt = json.load(f)\n",
    "    ckpt_queries = ckpt.get('sample_queries', [])\n",
    "    current_queries = [s['query'] for s in samples]\n",
    "    if ckpt_queries == current_queries:\n",
    "        results = ckpt['results']\n",
    "        start_idx = len(results)\n",
    "        print(f\"Resuming from checkpoint: {start_idx}/{N}\")\n",
    "    else:\n",
    "        print(\"Checkpoint sample mismatch. Starting fresh.\")\n",
    "else:\n",
    "    print(\"No checkpoint found. Starting fresh.\")\n",
    "\n",
    "print(f\"Evaluating samples {start_idx} to {N-1}\")\n",
    "n_layers = model.config.num_hidden_layers\n",
    "\n",
    "t_start = time.time()\n",
    "\n",
    "for idx in tqdm(range(start_idx, N), initial=start_idx, total=N, desc=\"Evaluating\"):\n",
    "    sample = samples[idx]\n",
    "    passage = sample['passage']\n",
    "    query = sample['query']\n",
    "    answer = sample['answer']\n",
    "    word_count = sample['word_count']\n",
    "    length_bin = sample['length_bin']\n",
    "\n",
    "    query_prompt = QUERY_TEMPLATE.format(query=query)\n",
    "    answer_text = ANSWER_TEMPLATE.format(answer=answer)\n",
    "\n",
    "    # --- Matched tokenization (from exp 11) ---\n",
    "    oracle_prefix = SURROGATE_PREFIX_TEMPLATE.format(surrogate=query)\n",
    "    document_text = DOCUMENT_TEMPLATE.format(document=passage)\n",
    "    full_oracle_text = oracle_prefix + document_text\n",
    "\n",
    "    full_oracle_enc = tokenizer(full_oracle_text, return_tensors=\"pt\",\n",
    "                                add_special_tokens=True, padding=False, truncation=False)\n",
    "    full_oracle_ids = full_oracle_enc['input_ids'].to(config.device)\n",
    "\n",
    "    oracle_prefix_enc = tokenizer(oracle_prefix, return_tensors=\"pt\",\n",
    "                                  add_special_tokens=True, padding=False, truncation=False)\n",
    "    oracle_prefix_len = oracle_prefix_enc['input_ids'].shape[1]\n",
    "\n",
    "    bos_id = full_oracle_ids[:, :1]\n",
    "    doc_ids = full_oracle_ids[:, oracle_prefix_len:]\n",
    "    doc_len = doc_ids.shape[1]\n",
    "    context_len = 1 + doc_len  # BOS + doc\n",
    "\n",
    "    # ===================================================================\n",
    "    # PHASE 1: BUILD bare + 1x PREFIX (shared forward passes)\n",
    "    # ===================================================================\n",
    "    bare_ids = torch.cat([bos_id, doc_ids], dim=1)\n",
    "    with torch.no_grad():\n",
    "        bare_out = model(input_ids=bare_ids, attention_mask=torch.ones_like(bare_ids),\n",
    "                         use_cache=True, return_dict=True)\n",
    "    bare_cache = bare_out.past_key_values\n",
    "    del bare_out\n",
    "\n",
    "    primed_1x_ids = torch.cat([bos_id, sf_prefix_ids, doc_ids], dim=1)\n",
    "    with torch.no_grad():\n",
    "        out_1x = model(input_ids=primed_1x_ids,\n",
    "                       attention_mask=torch.ones_like(primed_1x_ids),\n",
    "                       use_cache=True, return_dict=True)\n",
    "    trunc_1x = extract_and_truncate_cache_with_bos(out_1x.past_key_values, doc_len)\n",
    "    correct_rope_positions_with_bos(trunc_1x, sf_prefix_len, model)\n",
    "    trunc_no_rope = extract_and_truncate_cache_with_bos(out_1x.past_key_values, doc_len)\n",
    "    del out_1x\n",
    "\n",
    "    # ===================================================================\n",
    "    # PHASE 2: DERIVED CACHES (need bare + 1x, no forward passes)\n",
    "    # ===================================================================\n",
    "    amp_2x_cache = amplify_values(bare_cache, trunc_1x, 2.0)\n",
    "    amp_5x_cache = amplify_values(bare_cache, trunc_1x, 5.0)\n",
    "    layers_cache = replace_values_at_layers(bare_cache, trunc_1x, list(range(16)))\n",
    "\n",
    "    # ===================================================================\n",
    "    # PHASE 3: SCORE + FREE all caches built so far\n",
    "    # No deepcopy needed — each cache is scored exactly once then freed.\n",
    "    # ===================================================================\n",
    "    nll_bare = score_answer_with_cache(\n",
    "        bare_cache, context_len,\n",
    "        query_prompt, answer_text, model, tokenizer, config)\n",
    "    del bare_cache\n",
    "\n",
    "    nll_1x = score_answer_with_cache(\n",
    "        trunc_1x, context_len,\n",
    "        query_prompt, answer_text, model, tokenizer, config)\n",
    "    del trunc_1x\n",
    "\n",
    "    nll_no_rope = score_answer_with_cache(\n",
    "        trunc_no_rope, context_len,\n",
    "        query_prompt, answer_text, model, tokenizer, config)\n",
    "    del trunc_no_rope\n",
    "\n",
    "    nll_amp2 = score_answer_with_cache(\n",
    "        amp_2x_cache, context_len,\n",
    "        query_prompt, answer_text, model, tokenizer, config)\n",
    "    del amp_2x_cache\n",
    "\n",
    "    nll_amp5 = score_answer_with_cache(\n",
    "        amp_5x_cache, context_len,\n",
    "        query_prompt, answer_text, model, tokenizer, config)\n",
    "    del amp_5x_cache\n",
    "\n",
    "    nll_layers = score_answer_with_cache(\n",
    "        layers_cache, context_len,\n",
    "        query_prompt, answer_text, model, tokenizer, config)\n",
    "    del layers_cache\n",
    "\n",
    "    # Free memory before heavy build passes\n",
    "    del bare_ids, primed_1x_ids\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # ===================================================================\n",
    "    # PHASE 4: BUILD + SCORE 5x PREFIX (isolated to limit peak memory)\n",
    "    # ===================================================================\n",
    "    rep5_ids = sf_prefix_ids.repeat(1, 5)\n",
    "    rep5_full_ids = torch.cat([bos_id, rep5_ids, doc_ids], dim=1)\n",
    "    mask_5x = build_repeated_prefix_mask_fast(\n",
    "        sf_prefix_len, 5, doc_len, model.dtype, model.device)\n",
    "    with torch.no_grad():\n",
    "        out_5x = model(input_ids=rep5_full_ids, attention_mask=mask_5x,\n",
    "                       use_cache=True, return_dict=True)\n",
    "    trunc_5x = extract_and_truncate_cache_with_bos(out_5x.past_key_values, doc_len)\n",
    "    correct_rope_positions_with_bos(trunc_5x, 5 * sf_prefix_len, model)\n",
    "    del out_5x, mask_5x, rep5_ids, rep5_full_ids\n",
    "\n",
    "    nll_5x = score_answer_with_cache(\n",
    "        trunc_5x, context_len,\n",
    "        query_prompt, answer_text, model, tokenizer, config)\n",
    "    del trunc_5x\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # ===================================================================\n",
    "    # PHASE 5: BUILD + SCORE 20x PREFIX (heaviest — isolated)\n",
    "    # ===================================================================\n",
    "    rep20_ids = sf_prefix_ids.repeat(1, 20)\n",
    "    rep20_full_ids = torch.cat([bos_id, rep20_ids, doc_ids], dim=1)\n",
    "    mask_20x = build_repeated_prefix_mask_fast(\n",
    "        sf_prefix_len, 20, doc_len, model.dtype, model.device)\n",
    "    with torch.no_grad():\n",
    "        out_20x = model(input_ids=rep20_full_ids, attention_mask=mask_20x,\n",
    "                        use_cache=True, return_dict=True)\n",
    "    trunc_20x = extract_and_truncate_cache_with_bos(out_20x.past_key_values, doc_len)\n",
    "    correct_rope_positions_with_bos(trunc_20x, 20 * sf_prefix_len, model)\n",
    "    del out_20x, mask_20x, rep20_ids, rep20_full_ids\n",
    "\n",
    "    nll_20x = score_answer_with_cache(\n",
    "        trunc_20x, context_len,\n",
    "        query_prompt, answer_text, model, tokenizer, config)\n",
    "    del trunc_20x\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # ===================================================================\n",
    "    # PHASE 6: BUILD + SCORE SUFFIX\n",
    "    # ===================================================================\n",
    "    suffix_full_ids = torch.cat([bos_id, doc_ids, suffix_sep_ids, suffix_text_ids], dim=1)\n",
    "    with torch.no_grad():\n",
    "        out_suffix = model(input_ids=suffix_full_ids,\n",
    "                           attention_mask=torch.ones_like(suffix_full_ids),\n",
    "                           use_cache=True, return_dict=True)\n",
    "    suffix_cache = out_suffix.past_key_values\n",
    "    suffix_context_len = suffix_full_ids.shape[1]\n",
    "    del out_suffix, suffix_full_ids\n",
    "\n",
    "    nll_suffix = score_answer_with_cache(\n",
    "        suffix_cache, suffix_context_len,\n",
    "        query_prompt, answer_text, model, tokenizer, config)\n",
    "    del suffix_cache\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # --- Store result ---\n",
    "    result = {\n",
    "        'idx': idx,\n",
    "        'doc_len_tokens': doc_len,\n",
    "        'word_count': word_count,\n",
    "        'length_bin': length_bin,\n",
    "        'bare': nll_bare,\n",
    "        'prefix_1x': nll_1x,\n",
    "        'prefix_5x': nll_5x,\n",
    "        'prefix_20x': nll_20x,\n",
    "        'amplify_2x': nll_amp2,\n",
    "        'amplify_5x': nll_amp5,\n",
    "        'layers_0_15': nll_layers,\n",
    "        'suffix': nll_suffix,\n",
    "        'no_rope': nll_no_rope,\n",
    "    }\n",
    "    results.append(result)\n",
    "\n",
    "    if (idx + 1) % CHECKPOINT_EVERY == 0 or idx == N - 1:\n",
    "        ckpt_data = {\n",
    "            'results': results,\n",
    "            'sample_queries': [s['query'] for s in samples],\n",
    "            'completed': len(results),\n",
    "            'total': N,\n",
    "            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        }\n",
    "        with open(CHECKPOINT_PATH, 'w') as f:\n",
    "            json.dump(ckpt_data, f)\n",
    "        elapsed = time.time() - t_start\n",
    "        rate = (idx - start_idx + 1) / elapsed if elapsed > 0 else 0\n",
    "        remaining = (N - idx - 1) / rate if rate > 0 else 0\n",
    "        tqdm.write(f\"  Checkpoint {idx+1}/{N} | {rate:.2f} s/s | ETA: {remaining/60:.1f} min\")\n",
    "\n",
    "elapsed_total = time.time() - t_start\n",
    "print(f\"\\nEvaluation complete: {len(results)} samples in {elapsed_total/60:.1f} min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5889dbf9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-13T16:55:35.531373Z",
     "iopub.status.busy": "2026-02-13T16:55:35.531026Z",
     "iopub.status.idle": "2026-02-13T16:55:36.071847Z",
     "shell.execute_reply": "2026-02-13T16:55:36.070881Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ANALYSIS — LONG-DOCUMENT PRIMING DIAGNOSTIC\n",
      "======================================================================\n",
      "Total: 315, Valid: 299, Excluded: 16\n",
      "\n",
      "Condition              Mean NLL        Std  d vs Bare    Win%\n",
      "--------------------------------------------------------------\n",
      "bare                     0.3608     0.9367         --      --\n",
      "prefix_1x                0.3626     0.9206     -0.016  65.2% ns\n",
      "prefix_5x                0.3651     0.9063     -0.017  65.2% ns\n",
      "prefix_20x               0.3710     0.8848     -0.026  59.2% ns\n",
      "amplify_2x               0.3360     0.8161     +0.090  57.9% ns\n",
      "amplify_5x               0.3338     0.7361     +0.060  46.8% ns\n",
      "layers_0_15              0.3465     0.8628     +0.083  56.2% ns\n",
      "suffix                   0.4594     1.1455     -0.196  35.8% ***\n",
      "no_rope                  0.4584     1.1286     -0.205  56.5% ***\n",
      "\n",
      "==========================================================================================\n",
      "8 PRIMARY COMPARISONS (Bonferroni alpha = 0.0063)\n",
      "==========================================================================================\n",
      "\n",
      "Comparison                Mean delta        d    Win%        t            p   Sig\n",
      "--------------------------------------------------------------------------------\n",
      "prefix_1x vs bare            -0.0018   -0.016   65.2%    -0.28    7.78e-01    ns\n",
      "prefix_5x vs bare            -0.0043   -0.017   65.2%    -0.29    7.70e-01    ns\n",
      "prefix_20x vs bare           -0.0102   -0.026   59.2%    -0.45    6.51e-01    ns\n",
      "amplify_2x vs bare            0.0248    0.090   57.9%     1.55    1.22e-01    ns\n",
      "amplify_5x vs bare            0.0271    0.060   46.8%     1.04    3.01e-01    ns\n",
      "layers_0_15 vs bare           0.0144    0.083   56.2%     1.44    1.52e-01    ns\n",
      "suffix vs bare               -0.0985   -0.196   35.8%    -3.39    7.86e-04   ***\n",
      "no_rope vs bare              -0.0975   -0.205   56.5%    -3.54    4.60e-04   ***\n",
      "\n",
      "==========================================================================================\n",
      "HYPOTHESIS TESTS — Between-condition comparisons\n",
      "==========================================================================================\n",
      "\n",
      "Comparison                               Mean delta        d    Win%            p\n",
      "----------------------------------------------------------------------------------\n",
      "Hyp A: 5x vs 1x (repetition helps?)         -0.0025   -0.015   56.2%    7.94e-01 ns\n",
      "Hyp A: 20x vs 1x (strong repetition?)       -0.0084   -0.026   51.8%    6.57e-01 ns\n",
      "Hyp A: 20x vs 5x (more = better?)           -0.0059   -0.023   47.2%    6.90e-01 ns\n",
      "Hyp A+B: amplify_2x vs 1x                    0.0266    0.131   41.8%    2.38e-02 *\n",
      "Hyp A+B: amplify_5x vs 1x                    0.0289    0.072   39.1%    2.15e-01 ns\n",
      "Hyp C: suffix vs 1x (RoPE issue?)           -0.0968   -0.191   27.1%    1.08e-03 **\n",
      "Hyp C: no_rope vs 1x (correction helps?)    -0.0958   -0.192   46.8%    1.01e-03 **\n",
      "Signal: layers_0_15 vs 1x                    0.0162    0.148   36.1%    1.12e-02 *\n",
      "\n",
      "==========================================================================================\n",
      "PER LENGTH BIN — Does any condition recover the effect on long docs?\n",
      "==========================================================================================\n",
      "\n",
      "  prefix_1x:\n",
      "    short: n=14, d=+0.378, win=78.6%, p=1.81e-01 ns\n",
      "    medium: n=92, d=-0.074, win=62.0%, p=4.80e-01 ns\n",
      "    long: n=96, d=-0.146, win=67.7%, p=1.56e-01 ns\n",
      "    very_long: n=97, d=-0.115, win=63.9%, p=2.62e-01 ns\n",
      "\n",
      "  prefix_5x:\n",
      "    short: n=14, d=+0.319, win=78.6%, p=2.54e-01 ns\n",
      "    medium: n=92, d=-0.161, win=58.7%, p=1.26e-01 ns\n",
      "    long: n=96, d=-0.050, win=68.8%, p=6.26e-01 ns\n",
      "    very_long: n=97, d=-0.224, win=66.0%, p=2.99e-02 *\n",
      "\n",
      "  prefix_20x:\n",
      "    short: n=14, d=+0.400, win=71.4%, p=1.58e-01 ns\n",
      "    medium: n=92, d=-0.232, win=55.4%, p=2.82e-02 *\n",
      "    long: n=96, d=-0.170, win=61.5%, p=9.95e-02 ns\n",
      "    very_long: n=97, d=-0.241, win=58.8%, p=1.98e-02 *\n",
      "\n",
      "  amplify_2x:\n",
      "    short: n=14, d=+0.399, win=64.3%, p=1.59e-01 ns\n",
      "    medium: n=92, d=+0.045, win=58.7%, p=6.66e-01 ns\n",
      "    long: n=96, d=+0.091, win=57.3%, p=3.76e-01 ns\n",
      "    very_long: n=97, d=-0.153, win=56.7%, p=1.35e-01 ns\n",
      "\n",
      "  amplify_5x:\n",
      "    short: n=14, d=+0.483, win=64.3%, p=9.39e-02 ns\n",
      "    medium: n=92, d=-0.065, win=43.5%, p=5.33e-01 ns\n",
      "    long: n=96, d=+0.089, win=50.0%, p=3.86e-01 ns\n",
      "    very_long: n=97, d=-0.255, win=44.3%, p=1.37e-02 *\n",
      "\n",
      "  layers_0_15:\n",
      "    short: n=14, d=+0.358, win=71.4%, p=2.03e-01 ns\n",
      "    medium: n=92, d=+0.007, win=57.6%, p=9.50e-01 ns\n",
      "    long: n=96, d=+0.110, win=59.4%, p=2.85e-01 ns\n",
      "    very_long: n=97, d=-0.032, win=49.5%, p=7.53e-01 ns\n",
      "\n",
      "  suffix:\n",
      "    short: n=14, d=-0.112, win=35.7%, p=6.82e-01 ns\n",
      "    medium: n=92, d=-0.180, win=35.9%, p=8.75e-02 ns\n",
      "    long: n=96, d=-0.251, win=34.4%, p=1.59e-02 *\n",
      "    very_long: n=97, d=-0.287, win=37.1%, p=5.71e-03 **\n",
      "\n",
      "  no_rope:\n",
      "    short: n=14, d=-0.259, win=50.0%, p=3.51e-01 ns\n",
      "    medium: n=92, d=-0.226, win=57.6%, p=3.27e-02 *\n",
      "    long: n=96, d=-0.176, win=57.3%, p=8.75e-02 ns\n",
      "    very_long: n=97, d=-0.211, win=55.7%, p=4.02e-02 *\n",
      "\n",
      "==========================================================================================\n",
      "DOSE-RESPONSE: Does increasing prefix repetitions help progressively?\n",
      "==========================================================================================\n",
      "  short (n=14): 1x d=+0.378, 5x d=+0.319, 20x d=+0.400\n",
      "    Trend: NON-MONOTONIC\n",
      "  medium (n=92): 1x d=-0.074, 5x d=-0.161, 20x d=-0.232\n",
      "    Trend: DECREASING\n",
      "  long (n=96): 1x d=-0.146, 5x d=-0.050, 20x d=-0.170\n",
      "    Trend: DECREASING\n",
      "  very_long (n=97): 1x d=-0.115, 5x d=-0.224, 20x d=-0.241\n",
      "    Trend: DECREASING\n",
      "\n",
      "==========================================================================================\n",
      "AMPLIFICATION RESPONSE: Does boosting the value delta help?\n",
      "==========================================================================================\n",
      "  short (n=14): 1x d=+0.378, amp2x d=+0.399, amp5x d=+0.483\n",
      "  medium (n=92): 1x d=-0.074, amp2x d=+0.045, amp5x d=-0.065\n",
      "  long (n=96): 1x d=-0.146, amp2x d=+0.091, amp5x d=+0.089\n",
      "  very_long (n=97): 1x d=-0.115, amp2x d=-0.153, amp5x d=-0.255\n",
      "\n",
      "==========================================================================================\n",
      "LENGTH INTERACTION — Correlation between doc length and benefit\n",
      "==========================================================================================\n",
      "  prefix_1x: Spearman r=-0.035 (p=0.546)\n",
      "  prefix_5x: Spearman r=-0.020 (p=0.735)\n",
      "  prefix_20x: Spearman r=-0.005 (p=0.928)\n",
      "  amplify_2x: Spearman r=-0.090 (p=0.119)\n",
      "  amplify_5x: Spearman r=-0.081 (p=0.161)\n",
      "  layers_0_15: Spearman r=-0.091 (p=0.116)\n",
      "  suffix: Spearman r=+0.021 (p=0.713)\n",
      "  no_rope: Spearman r=+0.012 (p=0.830)\n",
      "\n",
      "==========================================================================================\n",
      "HYPOTHESIS VERDICT\n",
      "==========================================================================================\n",
      "\n",
      "  Hypothesis A (Signal Dilution):\n",
      "    1x d=-0.016 → 5x d=-0.017 → 20x d=-0.026\n",
      "    INCONCLUSIVE: Repetition has negligible effect\n",
      "\n",
      "  Hypothesis A+B (Amplification):\n",
      "    1x d=-0.016 → amp2x d=+0.090 → amp5x d=+0.060\n",
      "    SUPPORTED: Amplification helps (direction is correct, magnitude too small)\n",
      "\n",
      "  Hypothesis C (RoPE Interference):\n",
      "    1x d=-0.016 vs suffix d=-0.196 vs no_rope d=-0.205\n",
      "    REFUTED: RoPE is not the issue\n",
      "\n",
      "  Signal Localization:\n",
      "    1x d=-0.016 vs layers_0_15 d=+0.083\n",
      "    SUPPORTED: Late layers add noise on long docs\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Analysis — overall + per length bin + hypothesis testing\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ANALYSIS — LONG-DOCUMENT PRIMING DIAGNOSTIC\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Extract arrays and filter zero NLLs\n",
    "cond_arrays = {}\n",
    "for cname in CONDITION_NAMES:\n",
    "    cond_arrays[cname] = np.array([r[cname] for r in results])\n",
    "\n",
    "valid = np.ones(len(results), dtype=bool)\n",
    "for cname in CONDITION_NAMES:\n",
    "    valid &= (cond_arrays[cname] != 0)\n",
    "n_valid = int(np.sum(valid))\n",
    "n_excluded = int(np.sum(~valid))\n",
    "print(f\"Total: {len(results)}, Valid: {n_valid}, Excluded: {n_excluded}\")\n",
    "\n",
    "c = {}\n",
    "for cname in CONDITION_NAMES:\n",
    "    c[cname] = cond_arrays[cname][valid]\n",
    "\n",
    "length_bins_arr = np.array([r['length_bin'] for r in results])[valid]\n",
    "word_counts_arr = np.array([r['word_count'] for r in results])[valid]\n",
    "doc_lens_arr = np.array([r['doc_len_tokens'] for r in results])[valid]\n",
    "\n",
    "# ===== OVERALL NLL SUMMARY =====\n",
    "print(f\"\\n{'Condition':<20} {'Mean NLL':>10} {'Std':>10} {'d vs Bare':>10} {'Win%':>7}\")\n",
    "print(\"-\" * 62)\n",
    "for cname in CONDITION_NAMES:\n",
    "    mean_nll = np.mean(c[cname])\n",
    "    std_nll = np.std(c[cname])\n",
    "    if cname == 'bare':\n",
    "        print(f\"{cname:<20} {mean_nll:>10.4f} {std_nll:>10.4f} {'--':>10} {'--':>7}\")\n",
    "    else:\n",
    "        delta = c['bare'] - c[cname]\n",
    "        d = cohens_d(delta)\n",
    "        win = np.mean(delta > 0) * 100\n",
    "        _, p_val = stats.ttest_1samp(delta, 0)\n",
    "        sig = \"***\" if p_val < 0.001 else \"**\" if p_val < BONFERRONI_ALPHA else \"*\" if p_val < 0.05 else \"ns\"\n",
    "        print(f\"{cname:<20} {mean_nll:>10.4f} {std_nll:>10.4f} {d:>+10.3f} {win:>5.1f}% {sig}\")\n",
    "\n",
    "# ===== 8 PRIMARY COMPARISONS =====\n",
    "print(f\"\\n{'='*90}\")\n",
    "print(f\"8 PRIMARY COMPARISONS (Bonferroni alpha = {BONFERRONI_ALPHA:.4f})\")\n",
    "print(f\"{'='*90}\")\n",
    "\n",
    "primary_conditions = [cn for cn in CONDITION_NAMES if cn != 'bare']\n",
    "comparisons = []\n",
    "for cname in primary_conditions:\n",
    "    delta = c['bare'] - c[cname]\n",
    "    comparisons.append((f\"{cname} vs bare\", delta, cname))\n",
    "\n",
    "print(f\"\\n{'Comparison':<25} {'Mean delta':>10} {'d':>8} {'Win%':>7} {'t':>8} {'p':>12} {'Sig':>5}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "comparison_results = {}\n",
    "for name, delta, cname in comparisons:\n",
    "    d = cohens_d(delta)\n",
    "    win = np.mean(delta > 0) * 100\n",
    "    t_stat, p_val = stats.ttest_1samp(delta, 0)\n",
    "    sig = \"***\" if p_val < 0.001 else \"**\" if p_val < BONFERRONI_ALPHA else \"*\" if p_val < 0.05 else \"ns\"\n",
    "    print(f\"{name:<25} {np.mean(delta):>10.4f} {d:>8.3f} {win:>6.1f}% {t_stat:>8.2f} {p_val:>11.2e} {sig:>5}\")\n",
    "    comparison_results[name] = {\n",
    "        'mean_delta': float(np.mean(delta)),\n",
    "        'cohens_d': float(d),\n",
    "        'win_rate': float(win / 100),\n",
    "        't_stat': float(t_stat),\n",
    "        'p_value': float(p_val),\n",
    "        'bonferroni_significant': bool(p_val < BONFERRONI_ALPHA),\n",
    "    }\n",
    "\n",
    "# ===== HYPOTHESIS-SPECIFIC COMPARISONS =====\n",
    "print(f\"\\n{'='*90}\")\n",
    "print(\"HYPOTHESIS TESTS — Between-condition comparisons\")\n",
    "print(f\"{'='*90}\")\n",
    "\n",
    "hyp_comparisons = [\n",
    "    (\"Hyp A: 5x vs 1x (repetition helps?)\",\n",
    "     c['prefix_1x'] - c['prefix_5x']),\n",
    "    (\"Hyp A: 20x vs 1x (strong repetition?)\",\n",
    "     c['prefix_1x'] - c['prefix_20x']),\n",
    "    (\"Hyp A: 20x vs 5x (more = better?)\",\n",
    "     c['prefix_5x'] - c['prefix_20x']),\n",
    "    (\"Hyp A+B: amplify_2x vs 1x\",\n",
    "     c['prefix_1x'] - c['amplify_2x']),\n",
    "    (\"Hyp A+B: amplify_5x vs 1x\",\n",
    "     c['prefix_1x'] - c['amplify_5x']),\n",
    "    (\"Hyp C: suffix vs 1x (RoPE issue?)\",\n",
    "     c['prefix_1x'] - c['suffix']),\n",
    "    (\"Hyp C: no_rope vs 1x (correction helps?)\",\n",
    "     c['prefix_1x'] - c['no_rope']),\n",
    "    (\"Signal: layers_0_15 vs 1x\",\n",
    "     c['prefix_1x'] - c['layers_0_15']),\n",
    "]\n",
    "\n",
    "print(f\"\\n{'Comparison':<40} {'Mean delta':>10} {'d':>8} {'Win%':>7} {'p':>12}\")\n",
    "print(\"-\" * 82)\n",
    "hyp_results = {}\n",
    "for name, delta in hyp_comparisons:\n",
    "    d = cohens_d(delta)\n",
    "    win = np.mean(delta > 0) * 100\n",
    "    _, p_val = stats.ttest_1samp(delta, 0)\n",
    "    sig = \"***\" if p_val < 0.001 else \"**\" if p_val < 0.01 else \"*\" if p_val < 0.05 else \"ns\"\n",
    "    print(f\"{name:<40} {np.mean(delta):>10.4f} {d:>8.3f} {win:>6.1f}% {p_val:>11.2e} {sig}\")\n",
    "    hyp_results[name] = {\n",
    "        'mean_delta': float(np.mean(delta)),\n",
    "        'cohens_d': float(d),\n",
    "        'win_rate': float(win / 100),\n",
    "        'p_value': float(p_val),\n",
    "    }\n",
    "\n",
    "# ===== PER LENGTH BIN ANALYSIS =====\n",
    "print(f\"\\n{'='*90}\")\n",
    "print(\"PER LENGTH BIN — Does any condition recover the effect on long docs?\")\n",
    "print(f\"{'='*90}\")\n",
    "\n",
    "bin_names_ordered = [name for name, _, _ in LENGTH_BINS]\n",
    "per_bin_results = {}\n",
    "\n",
    "for cname in CONDITION_NAMES:\n",
    "    if cname == 'bare':\n",
    "        continue\n",
    "    print(f\"\\n  {cname}:\")\n",
    "    bin_ds = []\n",
    "    bin_wins = []\n",
    "    bin_ns = []\n",
    "    for bin_name in bin_names_ordered:\n",
    "        mask = length_bins_arr == bin_name\n",
    "        n_bin = int(np.sum(mask))\n",
    "        if n_bin < 10:\n",
    "            print(f\"    {bin_name}: n={n_bin} (too few)\")\n",
    "            bin_ds.append(None)\n",
    "            bin_wins.append(None)\n",
    "            bin_ns.append(n_bin)\n",
    "            continue\n",
    "        delta = c['bare'][mask] - c[cname][mask]\n",
    "        d = cohens_d(delta)\n",
    "        win = np.mean(delta > 0) * 100\n",
    "        _, p_val = stats.ttest_1samp(delta, 0)\n",
    "        sig = \"***\" if p_val < 0.001 else \"**\" if p_val < BONFERRONI_ALPHA else \"*\" if p_val < 0.05 else \"ns\"\n",
    "        print(f\"    {bin_name}: n={n_bin}, d={d:+.3f}, win={win:.1f}%, p={p_val:.2e} {sig}\")\n",
    "        bin_ds.append(float(d))\n",
    "        bin_wins.append(float(win))\n",
    "        bin_ns.append(n_bin)\n",
    "\n",
    "    per_bin_results[cname] = {\n",
    "        'bin_names': bin_names_ordered,\n",
    "        'bin_ds': bin_ds,\n",
    "        'bin_wins': bin_wins,\n",
    "        'bin_ns': bin_ns,\n",
    "    }\n",
    "\n",
    "# ===== DOSE-RESPONSE: 1x vs 5x vs 20x across bins =====\n",
    "print(f\"\\n{'='*90}\")\n",
    "print(\"DOSE-RESPONSE: Does increasing prefix repetitions help progressively?\")\n",
    "print(f\"{'='*90}\")\n",
    "\n",
    "dose_response = {}\n",
    "for bin_name in bin_names_ordered:\n",
    "    mask = length_bins_arr == bin_name\n",
    "    n_bin = int(np.sum(mask))\n",
    "    if n_bin < 10:\n",
    "        continue\n",
    "    d_1x = cohens_d(c['bare'][mask] - c['prefix_1x'][mask])\n",
    "    d_5x = cohens_d(c['bare'][mask] - c['prefix_5x'][mask])\n",
    "    d_20x = cohens_d(c['bare'][mask] - c['prefix_20x'][mask])\n",
    "    print(f\"  {bin_name} (n={n_bin}): 1x d={d_1x:+.3f}, 5x d={d_5x:+.3f}, 20x d={d_20x:+.3f}\")\n",
    "    trend = \"INCREASING\" if d_20x > d_5x > d_1x else \"NON-MONOTONIC\" if d_20x > d_1x else \"DECREASING\"\n",
    "    print(f\"    Trend: {trend}\")\n",
    "    dose_response[bin_name] = {'d_1x': d_1x, 'd_5x': d_5x, 'd_20x': d_20x, 'trend': trend}\n",
    "\n",
    "# ===== AMPLIFICATION RESPONSE =====\n",
    "print(f\"\\n{'='*90}\")\n",
    "print(\"AMPLIFICATION RESPONSE: Does boosting the value delta help?\")\n",
    "print(f\"{'='*90}\")\n",
    "\n",
    "amp_response = {}\n",
    "for bin_name in bin_names_ordered:\n",
    "    mask = length_bins_arr == bin_name\n",
    "    n_bin = int(np.sum(mask))\n",
    "    if n_bin < 10:\n",
    "        continue\n",
    "    d_1x = cohens_d(c['bare'][mask] - c['prefix_1x'][mask])\n",
    "    d_a2 = cohens_d(c['bare'][mask] - c['amplify_2x'][mask])\n",
    "    d_a5 = cohens_d(c['bare'][mask] - c['amplify_5x'][mask])\n",
    "    print(f\"  {bin_name} (n={n_bin}): 1x d={d_1x:+.3f}, amp2x d={d_a2:+.3f}, amp5x d={d_a5:+.3f}\")\n",
    "    amp_response[bin_name] = {'d_1x': d_1x, 'd_amp2x': d_a2, 'd_amp5x': d_a5}\n",
    "\n",
    "# ===== LENGTH INTERACTION CORRELATIONS =====\n",
    "print(f\"\\n{'='*90}\")\n",
    "print(\"LENGTH INTERACTION — Correlation between doc length and benefit\")\n",
    "print(f\"{'='*90}\")\n",
    "\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "interaction_results = {}\n",
    "for cname in CONDITION_NAMES:\n",
    "    if cname == 'bare':\n",
    "        continue\n",
    "    delta = c['bare'] - c[cname]\n",
    "    r_spear, p_spear = spearmanr(word_counts_arr, delta)\n",
    "    print(f\"  {cname}: Spearman r={r_spear:+.3f} (p={p_spear:.3f})\")\n",
    "    interaction_results[cname] = {\n",
    "        'spearman_r': float(r_spear), 'spearman_p': float(p_spear),\n",
    "    }\n",
    "\n",
    "# ===== HYPOTHESIS VERDICT =====\n",
    "print(f\"\\n{'='*90}\")\n",
    "print(\"HYPOTHESIS VERDICT\")\n",
    "print(f\"{'='*90}\")\n",
    "\n",
    "# Overall d for key conditions\n",
    "d_1x = cohens_d(c['bare'] - c['prefix_1x'])\n",
    "d_5x = cohens_d(c['bare'] - c['prefix_5x'])\n",
    "d_20x = cohens_d(c['bare'] - c['prefix_20x'])\n",
    "d_amp2 = cohens_d(c['bare'] - c['amplify_2x'])\n",
    "d_amp5 = cohens_d(c['bare'] - c['amplify_5x'])\n",
    "d_suffix = cohens_d(c['bare'] - c['suffix'])\n",
    "d_no_rope = cohens_d(c['bare'] - c['no_rope'])\n",
    "d_layers = cohens_d(c['bare'] - c['layers_0_15'])\n",
    "\n",
    "print(f\"\\n  Hypothesis A (Signal Dilution):\")\n",
    "print(f\"    1x d={d_1x:+.3f} → 5x d={d_5x:+.3f} → 20x d={d_20x:+.3f}\")\n",
    "if d_20x > d_1x + 0.05:\n",
    "    print(f\"    SUPPORTED: More repetition helps (+{d_20x - d_1x:.3f})\")\n",
    "elif d_20x < d_1x - 0.05:\n",
    "    print(f\"    REFUTED: More repetition HURTS ({d_20x - d_1x:.3f})\")\n",
    "else:\n",
    "    print(f\"    INCONCLUSIVE: Repetition has negligible effect\")\n",
    "\n",
    "print(f\"\\n  Hypothesis A+B (Amplification):\")\n",
    "print(f\"    1x d={d_1x:+.3f} → amp2x d={d_amp2:+.3f} → amp5x d={d_amp5:+.3f}\")\n",
    "if d_amp2 > d_1x + 0.05:\n",
    "    print(f\"    SUPPORTED: Amplification helps (direction is correct, magnitude too small)\")\n",
    "elif d_amp5 < d_1x - 0.1:\n",
    "    print(f\"    REFUTED: Amplification hurts (contamination is noise, not signal)\")\n",
    "else:\n",
    "    print(f\"    INCONCLUSIVE\")\n",
    "\n",
    "print(f\"\\n  Hypothesis C (RoPE Interference):\")\n",
    "print(f\"    1x d={d_1x:+.3f} vs suffix d={d_suffix:+.3f} vs no_rope d={d_no_rope:+.3f}\")\n",
    "if d_suffix > d_1x + 0.05 or d_no_rope > d_1x + 0.05:\n",
    "    print(f\"    SUPPORTED: RoPE correction is part of the problem\")\n",
    "else:\n",
    "    print(f\"    REFUTED: RoPE is not the issue\")\n",
    "\n",
    "print(f\"\\n  Signal Localization:\")\n",
    "print(f\"    1x d={d_1x:+.3f} vs layers_0_15 d={d_layers:+.3f}\")\n",
    "if d_layers > d_1x + 0.05:\n",
    "    print(f\"    SUPPORTED: Late layers add noise on long docs\")\n",
    "else:\n",
    "    print(f\"    Layers 16+ do not hurt on long docs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac9b068c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-13T16:55:36.075803Z",
     "iopub.status.busy": "2026-02-13T16:55:36.075106Z",
     "iopub.status.idle": "2026-02-13T16:55:39.233551Z",
     "shell.execute_reply": "2026-02-13T16:55:39.232691Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plots saved to results/exp12/analysis_plots.png\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Plots (2x2 grid + dose-response subplot)\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "\n",
    "colors = {\n",
    "    'prefix_1x': '#d62728',\n",
    "    'prefix_5x': '#ff7f0e',\n",
    "    'prefix_20x': '#e377c2',\n",
    "    'amplify_2x': '#2ca02c',\n",
    "    'amplify_5x': '#17becf',\n",
    "    'layers_0_15': '#9467bd',\n",
    "    'suffix': '#1f77b4',\n",
    "    'no_rope': '#7f7f7f',\n",
    "}\n",
    "\n",
    "# --- Plot 1: Per-bin Cohen's d for ALL conditions ---\n",
    "ax = axes[0, 0]\n",
    "x = np.arange(len(bin_names_ordered))\n",
    "width = 0.09\n",
    "conds_to_plot = [cn for cn in CONDITION_NAMES if cn != 'bare']\n",
    "for i, cname in enumerate(conds_to_plot):\n",
    "    ds = per_bin_results[cname]['bin_ds']\n",
    "    ds_clean = [d if d is not None else 0 for d in ds]\n",
    "    offset = (i - len(conds_to_plot)/2 + 0.5) * width\n",
    "    ax.bar(x + offset, ds_clean, width, label=cname,\n",
    "           color=colors[cname], edgecolor='black', linewidth=0.3, alpha=0.85)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(bin_names_ordered)\n",
    "ax.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "ax.set_ylabel(\"Cohen's d vs Bare\")\n",
    "ax.set_xlabel(\"Document Length Bin\")\n",
    "ax.set_title(\"All Conditions by Length Bin\")\n",
    "ax.legend(fontsize=6, ncol=2)\n",
    "\n",
    "# --- Plot 2: Dose-response (1x, 5x, 20x) per bin ---\n",
    "ax = axes[0, 1]\n",
    "x = np.arange(len(bin_names_ordered))\n",
    "width = 0.25\n",
    "for i, (cname, label) in enumerate([('prefix_1x', '1x'), ('prefix_5x', '5x'), ('prefix_20x', '20x')]):\n",
    "    ds = per_bin_results[cname]['bin_ds']\n",
    "    ds_clean = [d if d is not None else 0 for d in ds]\n",
    "    offset = (i - 1) * width\n",
    "    bars = ax.bar(x + offset, ds_clean, width, label=label,\n",
    "                  color=colors[cname], edgecolor='black', linewidth=0.5, alpha=0.85)\n",
    "    for j, d_val in enumerate(ds):\n",
    "        if d_val is not None:\n",
    "            ax.text(x[j] + offset, d_val + 0.005, f\"{d_val:+.2f}\", ha='center', va='bottom', fontsize=7)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(bin_names_ordered)\n",
    "ax.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "ax.set_ylabel(\"Cohen's d vs Bare\")\n",
    "ax.set_title(\"Dose-Response: Prefix Repetitions\")\n",
    "ax.legend()\n",
    "\n",
    "# --- Plot 3: Amplification response per bin ---\n",
    "ax = axes[0, 2]\n",
    "for i, (cname, label) in enumerate([('prefix_1x', '1x (baseline)'), ('amplify_2x', 'amplify 2x'), ('amplify_5x', 'amplify 5x')]):\n",
    "    ds = per_bin_results[cname]['bin_ds']\n",
    "    ds_clean = [d if d is not None else 0 for d in ds]\n",
    "    offset = (i - 1) * width\n",
    "    bars = ax.bar(x + offset, ds_clean, width, label=label,\n",
    "                  color=colors[cname], edgecolor='black', linewidth=0.5, alpha=0.85)\n",
    "    for j, d_val in enumerate(ds):\n",
    "        if d_val is not None:\n",
    "            ax.text(x[j] + offset, d_val + 0.005, f\"{d_val:+.2f}\", ha='center', va='bottom', fontsize=7)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(bin_names_ordered)\n",
    "ax.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "ax.set_ylabel(\"Cohen's d vs Bare\")\n",
    "ax.set_title(\"Amplification: Boosting Value Delta\")\n",
    "ax.legend()\n",
    "\n",
    "# --- Plot 4: Overall bar chart (all conditions, sorted) ---\n",
    "ax = axes[1, 0]\n",
    "conds_sorted = sorted(\n",
    "    [(cn, cohens_d(c['bare'] - c[cn])) for cn in CONDITION_NAMES if cn != 'bare'],\n",
    "    key=lambda x: x[1], reverse=True\n",
    ")\n",
    "names_sorted = [x[0] for x in conds_sorted]\n",
    "ds_sorted = [x[1] for x in conds_sorted]\n",
    "bar_colors = [colors.get(cn, 'gray') for cn in names_sorted]\n",
    "bars = ax.barh(range(len(names_sorted)), ds_sorted, color=bar_colors,\n",
    "               edgecolor='black', linewidth=0.5)\n",
    "ax.set_yticks(range(len(names_sorted)))\n",
    "ax.set_yticklabels(names_sorted, fontsize=9)\n",
    "for i, (name, d_val) in enumerate(conds_sorted):\n",
    "    ax.text(d_val + 0.005, i, f\"d={d_val:+.3f}\", va='center', fontsize=8)\n",
    "ax.axvline(x=0, color='gray', linestyle='--')\n",
    "ax.set_xlabel(\"Cohen's d vs Bare\")\n",
    "ax.set_title(\"Overall Effect (All Bins Combined)\")\n",
    "ax.invert_yaxis()\n",
    "# Reference line from MS MARCO\n",
    "ax.axvline(x=0.472, color='red', linestyle=':', alpha=0.5, label='Exp07 MARCO static_fact')\n",
    "ax.legend(fontsize=7)\n",
    "\n",
    "# --- Plot 5: RoPE hypothesis (1x vs suffix vs no_rope) per bin ---\n",
    "ax = axes[1, 1]\n",
    "for i, (cname, label) in enumerate([('prefix_1x', '1x (with RoPE)'), ('suffix', 'suffix (no RoPE)'), ('no_rope', 'no_rope (skip correction)')]):\n",
    "    ds = per_bin_results[cname]['bin_ds']\n",
    "    ds_clean = [d if d is not None else 0 for d in ds]\n",
    "    offset = (i - 1) * width\n",
    "    bars = ax.bar(x + offset, ds_clean, width, label=label,\n",
    "                  color=colors[cname], edgecolor='black', linewidth=0.5, alpha=0.85)\n",
    "    for j, d_val in enumerate(ds):\n",
    "        if d_val is not None:\n",
    "            ax.text(x[j] + offset, d_val + 0.005, f\"{d_val:+.2f}\", ha='center', va='bottom', fontsize=7)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(bin_names_ordered)\n",
    "ax.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "ax.set_ylabel(\"Cohen's d vs Bare\")\n",
    "ax.set_title(\"Hypothesis C: RoPE Interference\")\n",
    "ax.legend()\n",
    "\n",
    "# --- Plot 6: Scatter — word count vs benefit for key conditions ---\n",
    "ax = axes[1, 2]\n",
    "for cname in ['prefix_1x', 'prefix_20x', 'amplify_2x']:\n",
    "    delta = c['bare'] - c[cname]\n",
    "    ax.scatter(word_counts_arr, delta, alpha=0.15, s=8, color=colors[cname], label=cname)\n",
    "    # Binned trend\n",
    "    n_trend_bins = 15\n",
    "    edges = np.linspace(word_counts_arr.min(), word_counts_arr.max(), n_trend_bins + 1)\n",
    "    for k in range(n_trend_bins):\n",
    "        mask_k = (word_counts_arr >= edges[k]) & (word_counts_arr < edges[k+1])\n",
    "        if np.sum(mask_k) > 5:\n",
    "            ax.scatter((edges[k] + edges[k+1])/2, np.mean(delta[mask_k]),\n",
    "                      s=40, color=colors[cname], edgecolor='black', linewidth=0.5, zorder=5)\n",
    "ax.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "ax.set_xlabel(\"Document Word Count\")\n",
    "ax.set_ylabel(\"NLL Reduction (bare - primed)\")\n",
    "ax.set_title(\"NLL Reduction vs Document Length\")\n",
    "ax.legend(fontsize=8)\n",
    "\n",
    "plt.suptitle('Exp 12: Why Does Priming Fail on Long Documents?', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'analysis_plots.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"Plots saved to {RESULTS_DIR / 'analysis_plots.png'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb5a9a78",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-13T16:55:39.236914Z",
     "iopub.status.busy": "2026-02-13T16:55:39.236431Z",
     "iopub.status.idle": "2026-02-13T16:55:39.255397Z",
     "shell.execute_reply": "2026-02-13T16:55:39.254687Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to results/exp12/results.json\n",
      "File size: 152.0 KB\n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Cell 10: Save comprehensive results JSON\n",
    "\n",
    "nll_summary = {}\n",
    "for cname in CONDITION_NAMES:\n",
    "    nll_summary[cname] = {\n",
    "        'mean': float(np.mean(c[cname])),\n",
    "        'std': float(np.std(c[cname])),\n",
    "        'cohens_d_vs_bare': float(cohens_d(c['bare'] - c[cname])) if cname != 'bare' else 0.0,\n",
    "    }\n",
    "\n",
    "final = {\n",
    "    'experiment': 'exp12_long_doc_priming_diagnostic',\n",
    "    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'config': {\n",
    "        'model_name': config.model_name,\n",
    "        'seed': SEED,\n",
    "        'n_eval': N,\n",
    "        'n_valid': n_valid,\n",
    "        'n_excluded': n_excluded,\n",
    "        'n_conditions': N_CONDITIONS,\n",
    "        'n_comparisons': N_COMPARISONS,\n",
    "        'bonferroni_alpha': BONFERRONI_ALPHA,\n",
    "        'dataset': 'google-research-datasets/natural_questions',\n",
    "        'dataset_split': 'validation',\n",
    "        'length_bins': LENGTH_BINS,\n",
    "        'max_doc_words': MAX_DOC_WORDS,\n",
    "        'static_fact': STATIC_FACT,\n",
    "        'sf_prefix_tokens': sf_prefix_len,\n",
    "        'rep_counts': REP_COUNTS,\n",
    "        'amp_alphas': {k: v for k, v in AMP_ALPHAS.items()},\n",
    "    },\n",
    "    'condition_names': CONDITION_NAMES,\n",
    "    'nll_summary': nll_summary,\n",
    "    'primary_comparisons': comparison_results,\n",
    "    'hypothesis_comparisons': hyp_results,\n",
    "    'per_bin_results': per_bin_results,\n",
    "    'dose_response': dose_response,\n",
    "    'amplification_response': amp_response,\n",
    "    'interaction_results': interaction_results,\n",
    "    'per_sample_results': results,\n",
    "}\n",
    "\n",
    "with open(FINAL_RESULTS_PATH, 'w') as f:\n",
    "    json.dump(final, f, indent=2)\n",
    "\n",
    "print(f\"Results saved to {FINAL_RESULTS_PATH}\")\n",
    "print(f\"File size: {FINAL_RESULTS_PATH.stat().st_size / 1024:.1f} KB\")\n",
    "print(\"\\nDone!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "73930319",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-13T16:55:39.258298Z",
     "iopub.status.busy": "2026-02-13T16:55:39.257867Z",
     "iopub.status.idle": "2026-02-13T16:55:39.798559Z",
     "shell.execute_reply": "2026-02-13T16:55:39.797604Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning up GPU memory...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory: 4.14 GB -> 0.01 GB\n",
      "Cleanup complete.\n"
     ]
    }
   ],
   "source": [
    "# Cell 11: GPU cleanup\n",
    "import gc\n",
    "\n",
    "print(\"Cleaning up GPU memory...\")\n",
    "mem_before = torch.cuda.memory_allocated() / 1e9\n",
    "\n",
    "del model\n",
    "del tokenizer\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "mem_after = torch.cuda.memory_allocated() / 1e9\n",
    "print(f\"GPU memory: {mem_before:.2f} GB -> {mem_after:.2f} GB\")\n",
    "print(\"Cleanup complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "062f996598164b5a9e322c5957911570": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "0f49cb0e5c9d47d7bc463ffd6a7c42c7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "17bfcc6d5ebb4e00b336c7ed705c856a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "2970d9157c1d44a6ad8fbf0b21a497de": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2b8e865ba71b4102ab57a9a2e950d5e6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_6c3624a1966f44cfaa2680287fe2f8d4",
        "IPY_MODEL_ad118d354d5748d38b80d3d985d8cdde",
        "IPY_MODEL_9045be0aa4dd47fd9edfcf42242a5454"
       ],
       "layout": "IPY_MODEL_0f49cb0e5c9d47d7bc463ffd6a7c42c7",
       "tabbable": null,
       "tooltip": null
      }
     },
     "4856451a0aa14f54b842e0ba926e5989": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "4f51b4f55ced4aa8964f6ad37deeb0ac": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "5772a2d584294c349e3b275adcea3b9a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_062f996598164b5a9e322c5957911570",
       "placeholder": "​",
       "style": "IPY_MODEL_a68eb3bf71ae416e9bb17ac3c1fd9959",
       "tabbable": null,
       "tooltip": null,
       "value": " 315/315 [47:49&lt;00:00, 22.21s/it]"
      }
     },
     "66509bcabe064154b21c122ad4c68bcf": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6a07f08830ed42fba7c19fb40bdd3a79": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "6c3624a1966f44cfaa2680287fe2f8d4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_4f51b4f55ced4aa8964f6ad37deeb0ac",
       "placeholder": "​",
       "style": "IPY_MODEL_17bfcc6d5ebb4e00b336c7ed705c856a",
       "tabbable": null,
       "tooltip": null,
       "value": "Loading weights: 100%"
      }
     },
     "81c16ea13922402aaa5344adead0a3bd": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "89a6587eaba7457491cb2d9f0840ab77": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "8b4f84da5ff149c8a0382bd14520991f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "9045be0aa4dd47fd9edfcf42242a5454": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_89a6587eaba7457491cb2d9f0840ab77",
       "placeholder": "​",
       "style": "IPY_MODEL_6a07f08830ed42fba7c19fb40bdd3a79",
       "tabbable": null,
       "tooltip": null,
       "value": " 291/291 [00:29&lt;00:00, 11.93it/s, Materializing param=model.norm.weight]"
      }
     },
     "a50dd674b3604299b92edd98c87cc435": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "a68eb3bf71ae416e9bb17ac3c1fd9959": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "ad118d354d5748d38b80d3d985d8cdde": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_4856451a0aa14f54b842e0ba926e5989",
       "max": 291.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_8b4f84da5ff149c8a0382bd14520991f",
       "tabbable": null,
       "tooltip": null,
       "value": 291.0
      }
     },
     "cb957a6f9109409f8b68daa47f0025a0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_fba76ce43f1b4548975ce9b6381ebf19",
        "IPY_MODEL_f180774938b546ebba59e8f6275ae5f1",
        "IPY_MODEL_5772a2d584294c349e3b275adcea3b9a"
       ],
       "layout": "IPY_MODEL_66509bcabe064154b21c122ad4c68bcf",
       "tabbable": null,
       "tooltip": null
      }
     },
     "f180774938b546ebba59e8f6275ae5f1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_81c16ea13922402aaa5344adead0a3bd",
       "max": 315.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_f94bdf8e8422416593f54e406f5a3f6a",
       "tabbable": null,
       "tooltip": null,
       "value": 315.0
      }
     },
     "f94bdf8e8422416593f54e406f5a3f6a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "fba76ce43f1b4548975ce9b6381ebf19": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_2970d9157c1d44a6ad8fbf0b21a497de",
       "placeholder": "​",
       "style": "IPY_MODEL_a50dd674b3604299b92edd98c87cc435",
       "tabbable": null,
       "tooltip": null,
       "value": "Evaluating: 100%"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
