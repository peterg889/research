{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd08f329",
   "metadata": {},
   "source": [
    "# Exp 30: Retrieval vs Reasoning Task-Type Dissociation (Gemma 3 4B)\n",
    "\n",
    "## Motivation\n",
    "\n",
    "Exp 29 showed hero layers significantly **hurt** on DROP (d=-0.152, p=0.009) but were neutral on\n",
    "AdversarialQA/CoQA. The hypothesis is that priming helps *retrieval* but not *reasoning/computation*.\n",
    "However, this conclusion rests on a single dataset per type.\n",
    "\n",
    "This experiment provides a clean test: **does task type predict hero layer effect beyond difficulty?**\n",
    "\n",
    "## Dataset Taxonomy\n",
    "\n",
    "| Dataset | Task Type | Why Chosen | N |\n",
    "|---------|-----------|-----------|---|\n",
    "| **NQ** | Retrieval (factoid) | Known positive control — hero d=+0.213 in Exp 27b | 300 |\n",
    "| **DROP** | Mixed (computation + extraction) | Known negative — hero d=-0.152 in Exp 29 | 300 |\n",
    "| **BoolQ** | Retrieval (binary judgment) | New — pure passage-based yes/no, second retrieval data point | 300 |\n",
    "\n",
    "DROP gets split by answer type (number vs span) for within-dataset comparison:\n",
    "- **DROP-number**: Computational answers (counting, arithmetic) → tagged \"computation\"\n",
    "- **DROP-span**: Extractive answers → tagged \"retrieval\"\n",
    "\n",
    "## Conditions (reduced to 4)\n",
    "\n",
    "| # | Condition | Description |\n",
    "|---|-----------|-------------|\n",
    "| 1 | bare | Baseline |\n",
    "| 2 | sf_trunc | Standard priming (truncate + RoPE correct) |\n",
    "| 3 | values_early | Bare keys + primed values layers 0-15 |\n",
    "| 4 | values_hero | Bare keys + primed values at hero layers {10,12,14,15,20} |\n",
    "\n",
    "Dropped sf_trunc_bias2 (always hurts on these datasets) and values_only (dominated by values_early).\n",
    "\n",
    "## Key Question\n",
    "\n",
    "Does task type predict hero layer effect **beyond** difficulty?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92b68602",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-17T14:57:27.787066Z",
     "iopub.status.busy": "2026-02-17T14:57:27.786666Z",
     "iopub.status.idle": "2026-02-17T14:57:31.639472Z",
     "shell.execute_reply": "2026-02-17T14:57:31.638212Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEED: 42\n",
      "Results directory: results/exp30\n",
      "CUDA available: True\n",
      "GPU: NVIDIA L4\n",
      "GPU memory: 23.6 GB\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Setup\n",
    "import os\n",
    "os.umask(0o000)\n",
    "\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "import gc\n",
    "import csv\n",
    "import numpy as np\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "RESULTS_DIR = Path(\"results/exp30\")\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CHECKPOINT_PATH = RESULTS_DIR / \"checkpoint.json\"\n",
    "FINAL_RESULTS_PATH = RESULTS_DIR / \"results.json\"\n",
    "CSV_PATH = RESULTS_DIR / \"results.csv\"\n",
    "\n",
    "print(f\"SEED: {SEED}\")\n",
    "print(f\"Results directory: {RESULTS_DIR}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "035a851e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-17T14:57:31.643787Z",
     "iopub.status.busy": "2026-02-17T14:57:31.642982Z",
     "iopub.status.idle": "2026-02-17T14:58:22.510616Z",
     "shell.execute_reply": "2026-02-17T14:58:22.509628Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading google/gemma-3-4b-it (4-bit, bfloat16)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8d75906da044c42b744d0e3e1962249",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/883 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model loaded successfully.\n",
      "  Num layers: 34\n",
      "  Head dim: 256\n",
      "  Model dtype: torch.bfloat16\n",
      "  Sliding window: 1024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Cache key dtype: torch.bfloat16\n",
      "  Cache key shape: torch.Size([1, 4, 2, 256])\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Load Gemma 3 4B via load_model()\n",
    "sys.path.insert(0, \".\")\n",
    "\n",
    "from lib.config import ExperimentConfig\n",
    "from lib.model_utils import load_model\n",
    "\n",
    "MODEL_NAME = \"google/gemma-3-4b-it\"\n",
    "\n",
    "exp_config = ExperimentConfig(\n",
    "    model_name=MODEL_NAME,\n",
    "    model_type=\"gemma3\",\n",
    "    compute_dtype=\"auto\",  # resolves to bfloat16 for Gemma\n",
    "    use_4bit=True,\n",
    "    num_samples=2000,\n",
    "    seed=SEED,\n",
    ")\n",
    "\n",
    "print(f\"Loading {MODEL_NAME} (4-bit, bfloat16)...\")\n",
    "model, tokenizer = load_model(exp_config)\n",
    "\n",
    "from lib.kv_cache import _get_text_config, _get_head_dim, _ensure_dynamic_cache, _get_cache_keys\n",
    "\n",
    "text_config = _get_text_config(model.config)\n",
    "N_LAYERS = text_config.num_hidden_layers\n",
    "print(f\"\\nModel loaded successfully.\")\n",
    "print(f\"  Num layers: {N_LAYERS}\")\n",
    "print(f\"  Head dim: {_get_head_dim(model.config)}\")\n",
    "print(f\"  Model dtype: {model.dtype}\")\n",
    "print(f\"  Sliding window: {getattr(text_config, 'sliding_window', 'N/A')}\")\n",
    "\n",
    "# Verify with test forward pass\n",
    "sample_ids = tokenizer(\"test\", return_tensors=\"pt\")['input_ids'].to(exp_config.device)\n",
    "with torch.no_grad():\n",
    "    out = model(sample_ids, use_cache=True)\n",
    "    cache_check = _ensure_dynamic_cache(out.past_key_values)\n",
    "    k0 = _get_cache_keys(cache_check, 0)\n",
    "    print(f\"  Cache key dtype: {k0.dtype}\")\n",
    "    print(f\"  Cache key shape: {k0.shape}\")\n",
    "del out, sample_ids, cache_check\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ac3c6b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-17T14:58:22.514440Z",
     "iopub.status.busy": "2026-02-17T14:58:22.513850Z",
     "iopub.status.idle": "2026-02-17T14:58:22.522141Z",
     "shell.execute_reply": "2026-02-17T14:58:22.521267Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config ready\n",
      "  Model: google/gemma-3-4b-it\n",
      "  N per dataset: 300\n",
      "  MAX_DOC_TOKENS: 900 (sliding window constraint)\n",
      "  N_LAYERS: 34\n",
      "  EARLY_LAYER_CUTOFF: 16\n",
      "  HERO_LAYERS: [10, 12, 14, 15, 20]\n",
      "  Conditions: ['bare', 'sf_trunc', 'values_early', 'values_hero']\n",
      "  Static fact prefix: 'What are the key facts I need to know?'\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Lib imports + constants\n",
    "from lib.kv_cache import (\n",
    "    _get_cache_keys,\n",
    "    _get_cache_values,\n",
    "    _set_cache_keys,\n",
    "    _set_cache_values,\n",
    "    _ensure_dynamic_cache,\n",
    "    extract_and_truncate_cache_with_bos,\n",
    "    correct_rope_positions_with_bos,\n",
    "    score_answer_with_cache,\n",
    "    deepcopy_cache,\n",
    "    replace_values_at_layers,\n",
    ")\n",
    "from lib.analysis import cohens_d\n",
    "from lib.data import count_words\n",
    "from scipy import stats\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Templates -- bare text, no \"Document:\\n\" framing\n",
    "SURROGATE_PREFIX_TEMPLATE = \"{surrogate}\\n\"\n",
    "DOCUMENT_TEMPLATE = \"{document}\"\n",
    "QUERY_TEMPLATE = \"\\nQuestion: {question}\\nAnswer:\"\n",
    "ANSWER_TEMPLATE = \" {answer}\"\n",
    "\n",
    "# Prefix text\n",
    "from lib.surrogate import STATIC_SURROGATE_QUERIES\n",
    "STATIC_FACT = STATIC_SURROGATE_QUERIES['static_factual']['query']\n",
    "\n",
    "# Experiment parameters\n",
    "N_PER_DATASET = 300\n",
    "MAX_DOC_TOKENS = 900\n",
    "CHECKPOINT_EVERY = 25\n",
    "\n",
    "# Conditions (reduced to 4 -- dropped sf_trunc_bias2, values_only)\n",
    "CONDITION_NAMES = ['bare', 'sf_trunc', 'values_early', 'values_hero']\n",
    "\n",
    "# Layer-selective conditions from Exps 19/21/24\n",
    "EARLY_LAYER_CUTOFF = 16  # layers 0-15\n",
    "HERO_LAYERS = [10, 12, 14, 15, 20]  # from Exp 24 single-layer scan\n",
    "\n",
    "print(\"Config ready\")\n",
    "print(f\"  Model: {MODEL_NAME}\")\n",
    "print(f\"  N per dataset: {N_PER_DATASET}\")\n",
    "print(f\"  MAX_DOC_TOKENS: {MAX_DOC_TOKENS} (sliding window constraint)\")\n",
    "print(f\"  N_LAYERS: {N_LAYERS}\")\n",
    "print(f\"  EARLY_LAYER_CUTOFF: {EARLY_LAYER_CUTOFF}\")\n",
    "print(f\"  HERO_LAYERS: {HERO_LAYERS}\")\n",
    "print(f\"  Conditions: {CONDITION_NAMES}\")\n",
    "print(f\"  Static fact prefix: '{STATIC_FACT}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52575daa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-17T14:58:22.525970Z",
     "iopub.status.busy": "2026-02-17T14:58:22.525665Z",
     "iopub.status.idle": "2026-02-17T15:00:10.296766Z",
     "shell.execute_reply": "2026-02-17T15:00:10.295668Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "LOADING NATURAL QUESTIONS (validation, streaming)\n",
      "======================================================================\n",
      "Factoid retrieval QA. Known positive control for hero layers (d=+0.213 in Exp 27b).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fc094f8885949209751b3d9742e69d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/287 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d3adf55b33c4250b119f9e0c2d5f04b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/287 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1c36224442f4fb2acdedd61c899c017",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing NQ: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cached 300 samples (processed 4079)\n",
      "NQ samples: 300\n",
      "  Word counts: mean=2073, min=135, max=3986\n",
      "  Example 1:\n",
      "    Q: who sings the theme song for the tv show cops\n",
      "    A: Inner Circle\n",
      "  Example 2:\n",
      "    Q: when did the book thief movie come out\n",
      "    A: 2013\n",
      "  Example 3:\n",
      "    Q: where did the book small steps take place\n",
      "    A: Austin, Texas\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Load Natural Questions (streaming, same approach as Exp 27b)\n",
    "from datasets import load_dataset\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"LOADING NATURAL QUESTIONS (validation, streaming)\")\n",
    "print(\"=\" * 70)\n",
    "print(\"Factoid retrieval QA. Known positive control for hero layers (d=+0.213 in Exp 27b).\")\n",
    "\n",
    "NQ_CACHE = RESULTS_DIR / \"nq_samples.json\"\n",
    "\n",
    "if NQ_CACHE.exists():\n",
    "    with open(NQ_CACHE, 'r') as f:\n",
    "        nq_samples = json.load(f)\n",
    "    print(f\"Loaded {len(nq_samples)} cached NQ samples\")\n",
    "else:\n",
    "    nq_ds = load_dataset(\n",
    "        \"google-research-datasets/natural_questions\",\n",
    "        split=\"validation\",\n",
    "        streaming=True,\n",
    "    )\n",
    "\n",
    "    nq_samples = []\n",
    "    n_processed = 0\n",
    "\n",
    "    for example in tqdm(nq_ds, desc=\"Processing NQ\"):\n",
    "        n_processed += 1\n",
    "\n",
    "        doc_tokens = example['document']['tokens']\n",
    "        if isinstance(doc_tokens, dict):\n",
    "            token_strs = doc_tokens['token']\n",
    "            is_html_flags = doc_tokens['is_html']\n",
    "            clean_tokens = [t for t, h in zip(token_strs, is_html_flags) if not h]\n",
    "        else:\n",
    "            clean_tokens = [t['token'] for t in doc_tokens if not t['is_html']]\n",
    "\n",
    "        doc_text = ' '.join(clean_tokens)\n",
    "        wc = count_words(doc_text)\n",
    "\n",
    "        if wc < 50 or wc > 4000:\n",
    "            continue\n",
    "\n",
    "        annotations = example['annotations']\n",
    "        short_answers_list = annotations['short_answers']\n",
    "\n",
    "        answer_text = None\n",
    "        for annotator_sa in short_answers_list:\n",
    "            if not annotator_sa:\n",
    "                continue\n",
    "            texts = annotator_sa.get('text', [])\n",
    "            if texts:\n",
    "                answer_text = texts[0]\n",
    "                break\n",
    "            starts = annotator_sa.get('start_token', [])\n",
    "            ends = annotator_sa.get('end_token', [])\n",
    "            if not starts or not ends:\n",
    "                continue\n",
    "            start_tok = starts[0] if isinstance(starts, list) else starts\n",
    "            end_tok = ends[0] if isinstance(ends, list) else ends\n",
    "            if start_tok >= 0 and end_tok > start_tok:\n",
    "                if isinstance(doc_tokens, dict):\n",
    "                    ans_tokens = [\n",
    "                        doc_tokens['token'][i]\n",
    "                        for i in range(start_tok, min(end_tok, len(doc_tokens['token'])))\n",
    "                        if not doc_tokens['is_html'][i]\n",
    "                    ]\n",
    "                else:\n",
    "                    ans_tokens = [\n",
    "                        doc_tokens[i]['token']\n",
    "                        for i in range(start_tok, min(end_tok, len(doc_tokens)))\n",
    "                        if not doc_tokens[i]['is_html']\n",
    "                    ]\n",
    "                if ans_tokens:\n",
    "                    answer_text = ' '.join(ans_tokens)\n",
    "                    break\n",
    "\n",
    "        if not answer_text or len(answer_text.strip()) == 0:\n",
    "            continue\n",
    "        if len(answer_text.split()) > 20:\n",
    "            continue\n",
    "\n",
    "        question = example['question']\n",
    "        if isinstance(question, dict):\n",
    "            query = question.get('text', '')\n",
    "        else:\n",
    "            query = str(question)\n",
    "        if not query.strip():\n",
    "            continue\n",
    "\n",
    "        nq_samples.append({\n",
    "            'passage': doc_text,\n",
    "            'query': query,\n",
    "            'answer': answer_text,\n",
    "            'word_count': wc,\n",
    "            'dataset': 'nq',\n",
    "        })\n",
    "\n",
    "        if len(nq_samples) >= N_PER_DATASET * 3:\n",
    "            break\n",
    "\n",
    "    np.random.seed(SEED)\n",
    "    np.random.shuffle(nq_samples)\n",
    "    nq_samples = nq_samples[:N_PER_DATASET]\n",
    "\n",
    "    with open(NQ_CACHE, 'w') as f:\n",
    "        json.dump(nq_samples, f)\n",
    "    print(f\"Cached {len(nq_samples)} samples (processed {n_processed})\")\n",
    "\n",
    "print(f\"NQ samples: {len(nq_samples)}\")\n",
    "wcs = [s['word_count'] for s in nq_samples]\n",
    "print(f\"  Word counts: mean={np.mean(wcs):.0f}, min={min(wcs)}, max={max(wcs)}\")\n",
    "if nq_samples:\n",
    "    for i in range(min(3, len(nq_samples))):\n",
    "        print(f\"  Example {i+1}:\")\n",
    "        print(f\"    Q: {nq_samples[i]['query']}\")\n",
    "        print(f\"    A: {nq_samples[i]['answer']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2109e4de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-17T15:00:10.300476Z",
     "iopub.status.busy": "2026-02-17T15:00:10.300194Z",
     "iopub.status.idle": "2026-02-17T15:00:11.685971Z",
     "shell.execute_reply": "2026-02-17T15:00:11.685062Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "LOADING DROP (validation)\n",
      "======================================================================\n",
      "Mixed computation + extraction. Known negative for hero layers (d=-0.152 in Exp 29).\n",
      "Each sample tagged with answer_type: 'number' or 'span'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DROP validation size: 9535\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6ffc687abe341f4b7eb59507c1a5ac2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filtering DROP:   0%|          | 0/9535 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cached 300 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DROP samples: 300\n",
      "  Answer type distribution: number=178, span=122\n",
      "  Word counts: mean=174, min=67, max=488\n",
      "  Answer word lengths: mean=1.5, min=1, max=8\n",
      "\n",
      "  Borderline cases (span with digits): 20\n",
      "    '7-7' -> tagged as 'span'\n",
      "    '25 to 44' -> tagged as 'span'\n",
      "    '2-yard' -> tagged as 'span'\n",
      "    '20.1% from 45 to 64' -> tagged as 'span'\n",
      "    '45 to 64' -> tagged as 'span'\n",
      "  Example 1 (number):\n",
      "    Q: How many years after the last Battle IN Guadalajara did king Fernando III give a new fuero to the city?\n",
      "    A: 7\n",
      "    Passage (first 120 chars): In 1085, Guadalajara was retaken by the Christian forces of Alfonso VI . The chronicles say that the Christian army was ...\n",
      "  Example 2 (span):\n",
      "    Q: Which group from the census in Skopje is larger: Macedonians or Serbs?\n",
      "    A: Macedonians\n",
      "    Passage (first 120 chars): Skopje, as the Republic of Macedonia as a whole, is characterised by a large ethnic diversity. The city is located in a ...\n",
      "  Example 3 (number):\n",
      "    Q: How many points were scored in the first quarter?\n",
      "    A: 0\n",
      "    Passage (first 120 chars):  Coming off their impressive road win over the 49ers, the Falcons went home for a Week 6 Sunday night duel with the Chic...\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Load DROP dataset (numerical/discrete reasoning + extraction)\n",
    "print(\"=\" * 70)\n",
    "print(\"LOADING DROP (validation)\")\n",
    "print(\"=\" * 70)\n",
    "print(\"Mixed computation + extraction. Known negative for hero layers (d=-0.152 in Exp 29).\")\n",
    "print(\"Each sample tagged with answer_type: 'number' or 'span'.\")\n",
    "\n",
    "DROP_CACHE = RESULTS_DIR / \"drop_samples.json\"\n",
    "\n",
    "# Regex for number answers: integers, decimals, comma-separated numbers\n",
    "NUMBER_PATTERN = re.compile(r'^\\d[\\d,\\.]*$')\n",
    "\n",
    "if DROP_CACHE.exists():\n",
    "    with open(DROP_CACHE, 'r') as f:\n",
    "        drop_samples = json.load(f)\n",
    "    print(f\"Loaded {len(drop_samples)} cached DROP samples\")\n",
    "else:\n",
    "    drop_ds = load_dataset(\"drop\", split=\"validation\")\n",
    "    print(f\"DROP validation size: {len(drop_ds)}\")\n",
    "\n",
    "    drop_samples = []\n",
    "    np.random.seed(SEED)\n",
    "\n",
    "    for item in tqdm(drop_ds, desc=\"Filtering DROP\"):\n",
    "        passage = item.get('passage', '')\n",
    "        question = item.get('question', '')\n",
    "        answers_info = item.get('answers_spans', {})\n",
    "\n",
    "        spans = answers_info.get('spans', [])\n",
    "        if not spans:\n",
    "            continue\n",
    "        answer_text = spans[0]\n",
    "\n",
    "        if not question or not answer_text or not passage:\n",
    "            continue\n",
    "        if len(answer_text.strip()) == 0:\n",
    "            continue\n",
    "\n",
    "        wc = count_words(passage)\n",
    "        if wc < 30 or wc > 2000:\n",
    "            continue\n",
    "\n",
    "        # Tag answer type\n",
    "        answer_type = 'number' if NUMBER_PATTERN.match(answer_text.strip()) else 'span'\n",
    "\n",
    "        drop_samples.append({\n",
    "            'passage': passage,\n",
    "            'query': question,\n",
    "            'answer': answer_text,\n",
    "            'word_count': wc,\n",
    "            'dataset': 'drop',\n",
    "            'answer_type': answer_type,\n",
    "            'all_answers': spans,\n",
    "        })\n",
    "\n",
    "        if len(drop_samples) >= N_PER_DATASET * 3:\n",
    "            break\n",
    "\n",
    "    np.random.shuffle(drop_samples)\n",
    "    drop_samples = drop_samples[:N_PER_DATASET]\n",
    "\n",
    "    with open(DROP_CACHE, 'w') as f:\n",
    "        json.dump(drop_samples, f)\n",
    "    print(f\"Cached {len(drop_samples)} samples\")\n",
    "\n",
    "    del drop_ds\n",
    "    gc.collect()\n",
    "\n",
    "# Print answer type distribution\n",
    "n_number = sum(1 for s in drop_samples if s.get('answer_type') == 'number')\n",
    "n_span = sum(1 for s in drop_samples if s.get('answer_type') == 'span')\n",
    "print(f\"DROP samples: {len(drop_samples)}\")\n",
    "print(f\"  Answer type distribution: number={n_number}, span={n_span}\")\n",
    "wcs = [s['word_count'] for s in drop_samples]\n",
    "ans_lens = [len(s['answer'].split()) for s in drop_samples]\n",
    "print(f\"  Word counts: mean={np.mean(wcs):.0f}, min={min(wcs)}, max={max(wcs)}\")\n",
    "print(f\"  Answer word lengths: mean={np.mean(ans_lens):.1f}, min={min(ans_lens)}, max={max(ans_lens)}\")\n",
    "\n",
    "# Show borderline cases (answers that look numeric but don't match regex)\n",
    "borderline = [s for s in drop_samples\n",
    "              if s.get('answer_type') == 'span' and any(c.isdigit() for c in s['answer'])]\n",
    "if borderline:\n",
    "    print(f\"\\n  Borderline cases (span with digits): {len(borderline)}\")\n",
    "    for b in borderline[:5]:\n",
    "        print(f\"    '{b['answer']}' -> tagged as '{b['answer_type']}'\")\n",
    "\n",
    "if drop_samples:\n",
    "    for i in range(min(3, len(drop_samples))):\n",
    "        print(f\"  Example {i+1} ({drop_samples[i].get('answer_type', '?')}):\")\n",
    "        print(f\"    Q: {drop_samples[i]['query']}\")\n",
    "        print(f\"    A: {drop_samples[i]['answer']}\")\n",
    "        print(f\"    Passage (first 120 chars): {drop_samples[i]['passage'][:120]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1a9dae6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-17T15:00:11.690373Z",
     "iopub.status.busy": "2026-02-17T15:00:11.689875Z",
     "iopub.status.idle": "2026-02-17T15:00:12.576644Z",
     "shell.execute_reply": "2026-02-17T15:00:12.575697Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "LOADING BOOLQ (validation)\n",
      "======================================================================\n",
      "Pure passage-based yes/no questions. New retrieval data point.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoolQ validation size: 3270\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0c75fea79994db8a1a6fbea54ee772f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filtering BoolQ:   0%|          | 0/3270 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cached 300 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoolQ samples: 300\n",
      "  Answer distribution: Yes=184, No=116\n",
      "  Word counts: mean=98, min=30, max=386\n",
      "  Example 1 (Yes):\n",
      "    Q: did bruce forsyth do the price is right\n",
      "    Passage (first 120 chars): It returned to ITV, as Bruce's Price is Right, from 4 September 1995 to 16 December 2001 with Bruce Forsyth hosting for ...\n",
      "  Example 2 (No):\n",
      "    Q: can the time value of an option be negative\n",
      "    Passage (first 120 chars): In finance, the time value (TV) (extrinsic or instrumental value) of an option is the premium a rational investor would ...\n",
      "  Example 3 (Yes):\n",
      "    Q: has anyone won the grand slam in golf\n",
      "    Passage (first 120 chars): Only Bobby Jones has ever completed a Grand Slam. No man has ever achieved a modern era Grand Slam. Tiger Woods won all ...\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Load BoolQ dataset (binary judgment retrieval)\n",
    "print(\"=\" * 70)\n",
    "print(\"LOADING BOOLQ (validation)\")\n",
    "print(\"=\" * 70)\n",
    "print(\"Pure passage-based yes/no questions. New retrieval data point.\")\n",
    "\n",
    "BOOLQ_CACHE = RESULTS_DIR / \"boolq_samples.json\"\n",
    "\n",
    "if BOOLQ_CACHE.exists():\n",
    "    with open(BOOLQ_CACHE, 'r') as f:\n",
    "        boolq_samples = json.load(f)\n",
    "    print(f\"Loaded {len(boolq_samples)} cached BoolQ samples\")\n",
    "else:\n",
    "    boolq_ds = load_dataset(\"google/boolq\", split=\"validation\")\n",
    "    print(f\"BoolQ validation size: {len(boolq_ds)}\")\n",
    "\n",
    "    boolq_samples = []\n",
    "    np.random.seed(SEED)\n",
    "\n",
    "    for item in tqdm(boolq_ds, desc=\"Filtering BoolQ\"):\n",
    "        passage = item.get('passage', '')\n",
    "        question = item.get('question', '')\n",
    "        answer_bool = item.get('answer', None)\n",
    "\n",
    "        if not question or not passage or answer_bool is None:\n",
    "            continue\n",
    "\n",
    "        # Map boolean to text answer\n",
    "        answer_text = \"Yes\" if answer_bool else \"No\"\n",
    "\n",
    "        wc = count_words(passage)\n",
    "        if wc < 30 or wc > 2000:\n",
    "            continue\n",
    "\n",
    "        boolq_samples.append({\n",
    "            'passage': passage,\n",
    "            'query': question,\n",
    "            'answer': answer_text,\n",
    "            'word_count': wc,\n",
    "            'dataset': 'boolq',\n",
    "        })\n",
    "\n",
    "    np.random.shuffle(boolq_samples)\n",
    "    boolq_samples = boolq_samples[:N_PER_DATASET]\n",
    "\n",
    "    with open(BOOLQ_CACHE, 'w') as f:\n",
    "        json.dump(boolq_samples, f)\n",
    "    print(f\"Cached {len(boolq_samples)} samples\")\n",
    "\n",
    "    del boolq_ds\n",
    "    gc.collect()\n",
    "\n",
    "print(f\"BoolQ samples: {len(boolq_samples)}\")\n",
    "wcs = [s['word_count'] for s in boolq_samples]\n",
    "n_yes = sum(1 for s in boolq_samples if s['answer'] == 'Yes')\n",
    "n_no = sum(1 for s in boolq_samples if s['answer'] == 'No')\n",
    "print(f\"  Answer distribution: Yes={n_yes}, No={n_no}\")\n",
    "print(f\"  Word counts: mean={np.mean(wcs):.0f}, min={min(wcs)}, max={max(wcs)}\")\n",
    "if boolq_samples:\n",
    "    for i in range(min(3, len(boolq_samples))):\n",
    "        print(f\"  Example {i+1} ({boolq_samples[i]['answer']}):\")\n",
    "        print(f\"    Q: {boolq_samples[i]['query']}\")\n",
    "        print(f\"    Passage (first 120 chars): {boolq_samples[i]['passage'][:120]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ecddf573",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-17T15:00:12.580345Z",
     "iopub.status.busy": "2026-02-17T15:00:12.580040Z",
     "iopub.status.idle": "2026-02-17T15:00:46.787991Z",
     "shell.execute_reply": "2026-02-17T15:00:46.787008Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "UNIFIED SAMPLE POOL\n",
      "======================================================================\n",
      "Total samples: 900\n",
      "  nq: n=300, mean_words=2073, range=[135, 3986]\n",
      "  drop: n=300, mean_words=174, range=[67, 488]\n",
      "  boolq: n=300, mean_words=98, range=[30, 386]\n",
      "\n",
      "Prefix: 'What are the key facts I need to know?'\n",
      "  Token length (no BOS): 11\n",
      "  Max primed sequence: 1 + 11 + 900 = 912\n",
      "  Sliding window: 1024\n",
      "  SAFE: 912 < 1024\n",
      "\n",
      "Tokenizing documents to measure token lengths...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9534b59ecd6454f83963e65619b7182",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing:   0%|          | 0/900 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Documents truncated to 900: 274/900 (30%)\n",
      "  nq: mean_tok=880, median=900, truncated=274/300 (91%), mean_ans_tok=5.0\n",
      "  drop: mean_tok=268, median=256, truncated=0/300 (0%), mean_ans_tok=2.7\n",
      "  boolq: mean_tok=130, median=119, truncated=0/300 (0%), mean_ans_tok=1.0\n",
      "\n",
      "======================================================================\n",
      "PRE-SCREENING: Bare NLL distribution check (20 samples/dataset)\n",
      "If median bare NLL < 0.05, ceiling effects may dominate.\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  nq             : median=0.107, mean=1.071, pct_floor(<0.01)=45% -> MARGINAL\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  drop           : median=0.018, mean=1.427, pct_floor(<0.01)=35% -> MARGINAL\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  boolq          : median=0.000, mean=0.000, pct_floor(<0.01)=100% -> WARNING: CEILING\n",
      "\n",
      "Pre-screening complete. Proceeding with full experiment.\n",
      "\n",
      "======================================================================\n",
      "EXPERIMENTAL CONDITIONS (Gemma 3 4B) -- 4 conditions\n",
      "======================================================================\n",
      "\n",
      "### 1. bare ###\n",
      "  Forward: [BOS][doc]\n",
      "  Baseline. Standard causal attention.\n",
      "\n",
      "### 2. sf_trunc (standard priming) ###\n",
      "  Forward: [BOS][prefix_11][doc]\n",
      "  Standard causal, truncate + RoPE. Keys carry negative interference on Gemma.\n",
      "\n",
      "### 3. values_early (layers 0-15 only) ###\n",
      "  Bare keys + primed values from layers 0-15 only.\n",
      "  Expected: d ~ +0.211 (Exp 19 on MARCO). Late layers carry interference.\n",
      "\n",
      "### 4. values_hero (layers {10,12,14,15,20}) ###\n",
      "  Bare keys + primed values from 5 hero layers identified in Exp 24.\n",
      "  NQ: d=+0.213 (Exp 27b). DROP: d=-0.152 (Exp 29).\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Unified sample pool + tokenization + pre-screening\n",
    "print(\"=\" * 70)\n",
    "print(\"UNIFIED SAMPLE POOL\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "all_samples = []\n",
    "for ds_name, ds_samples in [(\"nq\", nq_samples),\n",
    "                              (\"drop\", drop_samples),\n",
    "                              (\"boolq\", boolq_samples)]:\n",
    "    for sample in ds_samples:\n",
    "        sample['dataset'] = ds_name\n",
    "    all_samples.extend(ds_samples)\n",
    "\n",
    "print(f\"Total samples: {len(all_samples)}\")\n",
    "for ds_name in ['nq', 'drop', 'boolq']:\n",
    "    ds_s = [s for s in all_samples if s['dataset'] == ds_name]\n",
    "    wcs = [s['word_count'] for s in ds_s]\n",
    "    print(f\"  {ds_name}: n={len(ds_s)}, mean_words={np.mean(wcs):.0f}, \"\n",
    "          f\"range=[{min(wcs)}, {max(wcs)}]\")\n",
    "\n",
    "# Tokenize prefix\n",
    "sf_str = SURROGATE_PREFIX_TEMPLATE.format(surrogate=STATIC_FACT)\n",
    "sf_ids = tokenizer(sf_str, return_tensors=\"pt\",\n",
    "                    add_special_tokens=False)['input_ids'].to(exp_config.device)\n",
    "PREFIX_TOKEN_LEN = sf_ids.shape[1]\n",
    "\n",
    "print(f\"\\nPrefix: '{STATIC_FACT}'\")\n",
    "print(f\"  Token length (no BOS): {PREFIX_TOKEN_LEN}\")\n",
    "\n",
    "# Verify sliding window safety\n",
    "max_primed_seq = 1 + PREFIX_TOKEN_LEN + MAX_DOC_TOKENS\n",
    "print(f\"  Max primed sequence: 1 + {PREFIX_TOKEN_LEN} + {MAX_DOC_TOKENS} = {max_primed_seq}\")\n",
    "print(f\"  Sliding window: 1024\")\n",
    "assert max_primed_seq < 1024, f\"UNSAFE: {max_primed_seq} >= 1024\"\n",
    "print(f\"  SAFE: {max_primed_seq} < 1024\")\n",
    "\n",
    "# Tokenize doc lengths\n",
    "print(f\"\\nTokenizing documents to measure token lengths...\")\n",
    "n_truncated = 0\n",
    "for sample in tqdm(all_samples, desc=\"Tokenizing\"):\n",
    "    tok_len = len(tokenizer.encode(sample['passage'], add_special_tokens=False))\n",
    "    if tok_len > MAX_DOC_TOKENS:\n",
    "        n_truncated += 1\n",
    "    sample['doc_token_len'] = min(tok_len, MAX_DOC_TOKENS)\n",
    "    sample['answer_token_len'] = len(tokenizer.encode(sample['answer'], add_special_tokens=False))\n",
    "\n",
    "print(f\"  Documents truncated to {MAX_DOC_TOKENS}: {n_truncated}/{len(all_samples)} \"\n",
    "      f\"({100*n_truncated/len(all_samples):.0f}%)\")\n",
    "\n",
    "for ds_name in ['nq', 'drop', 'boolq']:\n",
    "    ds_s = [s for s in all_samples if s['dataset'] == ds_name]\n",
    "    tls = [s['doc_token_len'] for s in ds_s]\n",
    "    atls = [s['answer_token_len'] for s in ds_s]\n",
    "    n_trunc = sum(1 for s in ds_s if s['doc_token_len'] == MAX_DOC_TOKENS)\n",
    "    print(f\"  {ds_name}: mean_tok={np.mean(tls):.0f}, median={np.median(tls):.0f}, \"\n",
    "          f\"truncated={n_trunc}/{len(ds_s)} ({100*n_trunc/len(ds_s):.0f}%), \"\n",
    "          f\"mean_ans_tok={np.mean(atls):.1f}\")\n",
    "\n",
    "# === PRE-SCREENING: Bare NLL check ===\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"PRE-SCREENING: Bare NLL distribution check (20 samples/dataset)\")\n",
    "print(\"If median bare NLL < 0.05, ceiling effects may dominate.\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for ds_name in ['nq', 'drop', 'boolq']:\n",
    "    ds_s = [s for s in all_samples if s['dataset'] == ds_name][:20]\n",
    "    bare_nlls = []\n",
    "    for sample in ds_s:\n",
    "        passage = sample['passage']\n",
    "        question = sample['query']\n",
    "        answer = sample['answer']\n",
    "\n",
    "        document_text = DOCUMENT_TEMPLATE.format(document=passage)\n",
    "        query_prompt = QUERY_TEMPLATE.format(question=question)\n",
    "        answer_text = ANSWER_TEMPLATE.format(answer=answer)\n",
    "\n",
    "        doc_ids = tokenizer(document_text, return_tensors=\"pt\",\n",
    "                            add_special_tokens=False)['input_ids'].to(exp_config.device)\n",
    "        if doc_ids.shape[1] > MAX_DOC_TOKENS:\n",
    "            doc_ids = doc_ids[:, :MAX_DOC_TOKENS]\n",
    "\n",
    "        bos_id = tokenizer.bos_token_id\n",
    "        if bos_id is None:\n",
    "            bos_id = tokenizer.encode(\"\", add_special_tokens=True)[0]\n",
    "        bos_tensor = torch.tensor([[bos_id]], device=exp_config.device)\n",
    "        bare_input = torch.cat([bos_tensor, doc_ids], dim=1)\n",
    "        context_len = bare_input.shape[1]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            bare_out = model(input_ids=bare_input,\n",
    "                             attention_mask=torch.ones_like(bare_input),\n",
    "                             use_cache=True, return_dict=True)\n",
    "        bare_cache = _ensure_dynamic_cache(bare_out.past_key_values)\n",
    "        del bare_out\n",
    "\n",
    "        nll = score_answer_with_cache(\n",
    "            deepcopy_cache(bare_cache), context_len,\n",
    "            query_prompt, answer_text, model, tokenizer, exp_config)\n",
    "        bare_nlls.append(nll)\n",
    "        del bare_cache, bare_input, doc_ids\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    bare_arr = np.array(bare_nlls)\n",
    "    pct_floor = 100 * np.mean(bare_arr < 0.01)\n",
    "    median = np.median(bare_arr)\n",
    "    mean = np.mean(bare_arr)\n",
    "    status = \"WARNING: CEILING\" if pct_floor > 50 else \"OK\" if pct_floor < 30 else \"MARGINAL\"\n",
    "    print(f\"  {ds_name:15s}: median={median:.3f}, mean={mean:.3f}, \"\n",
    "          f\"pct_floor(<0.01)={pct_floor:.0f}% -> {status}\")\n",
    "\n",
    "print(\"\\nPre-screening complete. Proceeding with full experiment.\")\n",
    "\n",
    "# Condition explanation\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"EXPERIMENTAL CONDITIONS (Gemma 3 4B) -- 4 conditions\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\n### 1. bare ###\")\n",
    "print(\"  Forward: [BOS][doc]\")\n",
    "print(\"  Baseline. Standard causal attention.\")\n",
    "\n",
    "print(\"\\n### 2. sf_trunc (standard priming) ###\")\n",
    "print(f\"  Forward: [BOS][prefix_{PREFIX_TOKEN_LEN}][doc]\")\n",
    "print(\"  Standard causal, truncate + RoPE. Keys carry negative interference on Gemma.\")\n",
    "\n",
    "print(\"\\n### 3. values_early (layers 0-15 only) ###\")\n",
    "print(\"  Bare keys + primed values from layers 0-15 only.\")\n",
    "print(\"  Expected: d ~ +0.211 (Exp 19 on MARCO). Late layers carry interference.\")\n",
    "\n",
    "print(\"\\n### 4. values_hero (layers {10,12,14,15,20}) ###\")\n",
    "print(\"  Bare keys + primed values from 5 hero layers identified in Exp 24.\")\n",
    "print(\"  NQ: d=+0.213 (Exp 27b). DROP: d=-0.152 (Exp 29).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4fd4ea06",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-17T15:00:46.791840Z",
     "iopub.status.busy": "2026-02-17T15:00:46.791552Z",
     "iopub.status.idle": "2026-02-17T15:00:46.807508Z",
     "shell.execute_reply": "2026-02-17T15:00:46.806535Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper function defined: run_single_sample_4cond()\n",
      "  Conditions: bare, sf_trunc, values_early, values_hero\n",
      "  No bias mask needed (sf_trunc_bias2 dropped)\n",
      "  No values_only (dominated by values_early)\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Helper function — run_single_sample_4cond()\n",
    "\n",
    "def run_single_sample_4cond(sample, model, tokenizer, exp_config, sf_ids, sf_str,\n",
    "                             PREFIX_TOKEN_LEN, N_LAYERS, EARLY_LAYER_CUTOFF, HERO_LAYERS):\n",
    "    \"\"\"Run 4 conditions for a single sample. Returns dict of NLLs + metadata.\n",
    "\n",
    "    Conditions:\n",
    "      1. bare: [BOS][doc] standard causal\n",
    "      2. sf_trunc: [BOS][prefix][doc] truncate + RoPE correct\n",
    "      3. values_early: bare keys + primed values layers 0-15\n",
    "      4. values_hero: bare keys + primed values at hero layers\n",
    "    \"\"\"\n",
    "    passage = sample['passage']\n",
    "    query = sample['query']\n",
    "    answer = sample['answer']\n",
    "    ds_name = sample['dataset']\n",
    "\n",
    "    query_prompt = QUERY_TEMPLATE.format(question=query)\n",
    "    answer_text = ANSWER_TEMPLATE.format(answer=answer)\n",
    "    document_text = DOCUMENT_TEMPLATE.format(document=passage)\n",
    "\n",
    "    # === Matched tokenization ===\n",
    "    full_text = sf_str + document_text\n",
    "    full_enc = tokenizer(full_text, return_tensors=\"pt\",\n",
    "                          add_special_tokens=True, padding=False, truncation=False)\n",
    "    full_ids = full_enc['input_ids'].to(exp_config.device)\n",
    "\n",
    "    sf_prefix_enc = tokenizer(sf_str, return_tensors=\"pt\",\n",
    "                               add_special_tokens=True, padding=False, truncation=False)\n",
    "    sf_prefix_len_with_bos = sf_prefix_enc['input_ids'].shape[1]\n",
    "\n",
    "    bos_id = full_ids[:, :1]\n",
    "    doc_ids = full_ids[:, sf_prefix_len_with_bos:]\n",
    "\n",
    "    # Truncate long docs\n",
    "    if doc_ids.shape[1] > MAX_DOC_TOKENS:\n",
    "        doc_ids = doc_ids[:, :MAX_DOC_TOKENS]\n",
    "\n",
    "    doc_len = doc_ids.shape[1]\n",
    "    context_len = 1 + doc_len  # BOS + doc\n",
    "\n",
    "    del full_enc, full_ids, sf_prefix_enc\n",
    "\n",
    "    # === 1. BARE ===\n",
    "    bare_input = torch.cat([bos_id, doc_ids], dim=1)\n",
    "    with torch.no_grad():\n",
    "        bare_out = model(input_ids=bare_input,\n",
    "                         attention_mask=torch.ones_like(bare_input),\n",
    "                         use_cache=True, return_dict=True)\n",
    "    bare_cache = _ensure_dynamic_cache(bare_out.past_key_values)\n",
    "    del bare_out\n",
    "\n",
    "    bare_nll = score_answer_with_cache(\n",
    "        deepcopy_cache(bare_cache), context_len,\n",
    "        query_prompt, answer_text, model, tokenizer, exp_config)\n",
    "\n",
    "    # === 2. sf_trunc (standard priming) ===\n",
    "    primed_input = torch.cat([bos_id, sf_ids, doc_ids], dim=1)\n",
    "    prefix_offset = sf_ids.shape[1]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        primed_out = model(input_ids=primed_input,\n",
    "                           attention_mask=torch.ones_like(primed_input),\n",
    "                           use_cache=True, return_dict=True)\n",
    "    primed_full_std = _ensure_dynamic_cache(primed_out.past_key_values)\n",
    "    del primed_out\n",
    "\n",
    "    trunc_raw = extract_and_truncate_cache_with_bos(primed_full_std, doc_len)\n",
    "    del primed_full_std\n",
    "\n",
    "    sf_trunc_cache = deepcopy_cache(trunc_raw)\n",
    "    correct_rope_positions_with_bos(sf_trunc_cache, prefix_offset, model)\n",
    "    del trunc_raw\n",
    "\n",
    "    sf_trunc_nll = score_answer_with_cache(\n",
    "        deepcopy_cache(sf_trunc_cache), context_len,\n",
    "        query_prompt, answer_text, model, tokenizer, exp_config)\n",
    "\n",
    "    # === 3. values_early (layers 0 to EARLY_LAYER_CUTOFF-1) ===\n",
    "    early_layers = list(range(EARLY_LAYER_CUTOFF))\n",
    "    values_early_cache = replace_values_at_layers(bare_cache, sf_trunc_cache, early_layers)\n",
    "\n",
    "    values_early_nll = score_answer_with_cache(\n",
    "        deepcopy_cache(values_early_cache), context_len,\n",
    "        query_prompt, answer_text, model, tokenizer, exp_config)\n",
    "    del values_early_cache\n",
    "\n",
    "    # === 4. values_hero (hero layers only) ===\n",
    "    values_hero_cache = replace_values_at_layers(bare_cache, sf_trunc_cache, HERO_LAYERS)\n",
    "\n",
    "    values_hero_nll = score_answer_with_cache(\n",
    "        deepcopy_cache(values_hero_cache), context_len,\n",
    "        query_prompt, answer_text, model, tokenizer, exp_config)\n",
    "    del values_hero_cache\n",
    "\n",
    "    del bare_cache, sf_trunc_cache, bare_input, primed_input\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    result = {\n",
    "        'dataset': ds_name,\n",
    "        'query': query,\n",
    "        'answer': answer,\n",
    "        'word_count': sample['word_count'],\n",
    "        'doc_token_len': doc_len,\n",
    "        'answer_token_len': sample.get('answer_token_len', 0),\n",
    "        'bare': bare_nll,\n",
    "        'sf_trunc': sf_trunc_nll,\n",
    "        'values_early': values_early_nll,\n",
    "        'values_hero': values_hero_nll,\n",
    "    }\n",
    "    # Carry forward answer_type for DROP\n",
    "    if 'answer_type' in sample:\n",
    "        result['answer_type'] = sample['answer_type']\n",
    "    return result\n",
    "\n",
    "\n",
    "print(\"Helper function defined: run_single_sample_4cond()\")\n",
    "print(\"  Conditions: bare, sf_trunc, values_early, values_hero\")\n",
    "print(\"  No bias mask needed (sf_trunc_bias2 dropped)\")\n",
    "print(\"  No values_only (dominated by values_early)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5a44cca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-17T15:00:46.810959Z",
     "iopub.status.busy": "2026-02-17T15:00:46.810301Z",
     "iopub.status.idle": "2026-02-17T15:30:44.799411Z",
     "shell.execute_reply": "2026-02-17T15:30:44.798452Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "EXPERIMENT 30: 900 samples, 4 conditions\n",
      "Model: Gemma 3 4B, MAX_DOC_TOKENS: 900\n",
      "Datasets: NQ, DROP, BoolQ\n",
      "======================================================================\n",
      "No checkpoint found. Starting fresh.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1650067daed4df8a5a66a1d8ca8f2cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Exp 30:   0%|          | 0/900 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 25/900 | 25 done in 0.9m | ETA: 30.5 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 50/900 | 50 done in 1.7m | ETA: 29.6 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 75/900 | 75 done in 2.6m | ETA: 28.9 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 100/900 | 100 done in 3.5m | ETA: 28.0 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 125/900 | 125 done in 4.4m | ETA: 27.1 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 150/900 | 150 done in 5.2m | ETA: 26.2 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 175/900 | 175 done in 6.1m | ETA: 25.3 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 200/900 | 200 done in 7.0m | ETA: 24.4 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 225/900 | 225 done in 7.9m | ETA: 23.6 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 250/900 | 250 done in 8.8m | ETA: 22.9 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 275/900 | 275 done in 9.7m | ETA: 22.0 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 300/900 | 300 done in 10.6m | ETA: 21.2 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 325/900 | 325 done in 11.4m | ETA: 20.2 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 350/900 | 350 done in 12.3m | ETA: 19.3 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 375/900 | 375 done in 13.1m | ETA: 18.4 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 400/900 | 400 done in 13.9m | ETA: 17.4 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 425/900 | 425 done in 14.8m | ETA: 16.5 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 450/900 | 450 done in 15.6m | ETA: 15.6 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 475/900 | 475 done in 16.4m | ETA: 14.7 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 500/900 | 500 done in 17.3m | ETA: 13.8 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 525/900 | 525 done in 18.1m | ETA: 12.9 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 550/900 | 550 done in 18.9m | ETA: 12.0 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 575/900 | 575 done in 19.8m | ETA: 11.2 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 600/900 | 600 done in 20.6m | ETA: 10.3 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 625/900 | 625 done in 21.4m | ETA: 9.4 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 650/900 | 650 done in 22.2m | ETA: 8.5 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 675/900 | 675 done in 23.0m | ETA: 7.7 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 700/900 | 700 done in 23.8m | ETA: 6.8 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 725/900 | 725 done in 24.5m | ETA: 5.9 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 750/900 | 750 done in 25.3m | ETA: 5.1 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 775/900 | 775 done in 26.1m | ETA: 4.2 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 800/900 | 800 done in 26.8m | ETA: 3.4 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 825/900 | 825 done in 27.6m | ETA: 2.5 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 850/900 | 850 done in 28.4m | ETA: 1.7 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 875/900 | 875 done in 29.2m | ETA: 0.8 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 900/900 | 900 done in 30.0m | ETA: 0.0 min\n",
      "\n",
      "Experiment complete: 900 samples in 30.0 min\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Main experiment loop\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"EXPERIMENT 30: {len(all_samples)} samples, {len(CONDITION_NAMES)} conditions\")\n",
    "print(f\"Model: Gemma 3 4B, MAX_DOC_TOKENS: {MAX_DOC_TOKENS}\")\n",
    "print(f\"Datasets: NQ, DROP, BoolQ\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Checkpoint resume\n",
    "all_results = []\n",
    "start_idx = 0\n",
    "\n",
    "if CHECKPOINT_PATH.exists():\n",
    "    with open(CHECKPOINT_PATH, 'r') as f:\n",
    "        ckpt = json.load(f)\n",
    "    ckpt_queries = ckpt.get('sample_queries', [])\n",
    "    current_queries = [s['query'] for s in all_samples]\n",
    "    if ckpt_queries == current_queries:\n",
    "        all_results = ckpt['results']\n",
    "        start_idx = len(all_results)\n",
    "        print(f\"Resuming from checkpoint: {start_idx}/{len(all_samples)}\")\n",
    "    else:\n",
    "        print(\"Checkpoint query mismatch. Starting fresh.\")\n",
    "else:\n",
    "    print(\"No checkpoint found. Starting fresh.\")\n",
    "\n",
    "t_start = time.time()\n",
    "N_TOTAL = len(all_samples)\n",
    "\n",
    "for qidx in tqdm(range(start_idx, N_TOTAL), initial=start_idx, total=N_TOTAL,\n",
    "                  desc=\"Exp 30\"):\n",
    "    sample = all_samples[qidx]\n",
    "\n",
    "    result = run_single_sample_4cond(\n",
    "        sample, model, tokenizer, exp_config,\n",
    "        sf_ids, sf_str, PREFIX_TOKEN_LEN, N_LAYERS,\n",
    "        EARLY_LAYER_CUTOFF, HERO_LAYERS)\n",
    "    result['query_idx'] = qidx\n",
    "    all_results.append(result)\n",
    "\n",
    "    # Checkpoint\n",
    "    if (qidx + 1) % CHECKPOINT_EVERY == 0 or qidx == N_TOTAL - 1:\n",
    "        ckpt_data = {\n",
    "            'results': all_results,\n",
    "            'sample_queries': [s['query'] for s in all_samples],\n",
    "            'completed': len(all_results),\n",
    "            'total': N_TOTAL,\n",
    "            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        }\n",
    "        with open(CHECKPOINT_PATH, 'w') as f:\n",
    "            json.dump(ckpt_data, f)\n",
    "        elapsed = time.time() - t_start\n",
    "        n_done = qidx - start_idx + 1\n",
    "        rate = n_done / elapsed if elapsed > 0 else 0\n",
    "        remaining = (N_TOTAL - qidx - 1) / rate if rate > 0 else 0\n",
    "        tqdm.write(f\"  Checkpoint {qidx+1}/{N_TOTAL} | {n_done} done in {elapsed/60:.1f}m | \"\n",
    "                   f\"ETA: {remaining/60:.1f} min\")\n",
    "\n",
    "elapsed_total = time.time() - t_start\n",
    "print(f\"\\nExperiment complete: {len(all_results)} samples in {elapsed_total/60:.1f} min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a6a1c56b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-17T15:30:44.804606Z",
     "iopub.status.busy": "2026-02-17T15:30:44.804298Z",
     "iopub.status.idle": "2026-02-17T15:30:44.950752Z",
     "shell.execute_reply": "2026-02-17T15:30:44.949782Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ANALYSIS: PER-DATASET RESULTS (Gemma 3 4B)\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "DATASET: NQ (n=300/300, median bare NLL=0.006, pct_floor=55%)\n",
      "======================================================================\n",
      "\n",
      "Condition             Mean Bare  Mean Cond     Mean D        d    Win%            p   sig\n",
      "------------------------------------------------------------------------------------------\n",
      "sf_trunc                 0.9125     0.8922    +0.0202   +0.033   50.3%     5.68e-01    ns\n",
      "values_early             0.9125     0.9083    +0.0041   +0.016   45.0%     7.86e-01    ns\n",
      "values_hero              0.9125     0.8846    +0.0279   +0.213   35.0%     2.62e-04   ***\n",
      "\n",
      "======================================================================\n",
      "DATASET: DROP (n=300/300, median bare NLL=0.028, pct_floor=44%)\n",
      "======================================================================\n",
      "\n",
      "Condition             Mean Bare  Mean Cond     Mean D        d    Win%            p   sig\n",
      "------------------------------------------------------------------------------------------\n",
      "sf_trunc                 1.3794     1.4918    -0.1124   -0.084   32.7%     1.49e-01    ns\n",
      "values_early             1.3794     1.4739    -0.0945   -0.090   34.3%     1.19e-01    ns\n",
      "values_hero              1.3794     1.4837    -0.1043   -0.152   26.3%     9.12e-03    **\n",
      "\n",
      "======================================================================\n",
      "DATASET: BOOLQ (n=300/300, median bare NLL=0.000, pct_floor=100%)\n",
      "======================================================================\n",
      "\n",
      "Condition             Mean Bare  Mean Cond     Mean D        d    Win%            p   sig\n",
      "------------------------------------------------------------------------------------------\n",
      "sf_trunc                 0.0000     0.0000    +0.0000   +0.000    0.0%          nan    ns\n",
      "values_early             0.0000     0.0000    +0.0000   +0.000    0.0%          nan    ns\n",
      "values_hero              0.0000     0.0000    +0.0000   +0.000    0.0%          nan    ns\n",
      "\n",
      "\n",
      "==========================================================================================\n",
      "CROSS-DATASET SUMMARY: Cohen's d vs bare (Gemma 3 4B)\n",
      "==========================================================================================\n",
      "\n",
      "Condition                         nq            drop           boolq\n",
      "--------------------------------------------------------------------\n",
      "sf_trunc                  +0.033          -0.084          +0.000    \n",
      "values_early              +0.016          -0.090          +0.000    \n",
      "values_hero               +0.213 ***      -0.152  **      +0.000    \n",
      "\n",
      "\n",
      "BARE NLL DISTRIBUTIONS (ceiling effect check):\n",
      "  nq             : mean=0.912, median=0.006, IQR=0.689, pct_floor=55%\n",
      "  drop           : mean=1.379, median=0.028, IQR=2.458, pct_floor=44%\n",
      "  boolq          : mean=0.000, median=0.000, IQR=0.000, pct_floor=100%\n",
      "\n",
      "\n",
      "==========================================================================================\n",
      "COMPARISON WITH PRIOR EXPERIMENTS\n",
      "==========================================================================================\n",
      "\n",
      "Exp 27b (Gemma, NQ/TriviaQA/HotpotQA):\n",
      "  NQ:       values_hero d=+0.213***\n",
      "  TriviaQA: values_hero d=+0.000 (77% at floor)\n",
      "  HotpotQA: values_hero d=-0.069 (56% at floor)\n",
      "\n",
      "Exp 29 (Gemma, DROP/AdvQA/CoQA):\n",
      "  DROP:     values_hero d=-0.152**\n",
      "  AdvQA:    values_hero d=+0.026 (72% at floor)\n",
      "  CoQA:     values_hero d=+0.070 (65% at floor)\n"
     ]
    }
   ],
   "source": [
    "# Cell 10: Per-dataset results table\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ANALYSIS: PER-DATASET RESULTS (Gemma 3 4B)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "dataset_names = ['nq', 'drop', 'boolq']\n",
    "analysis = {}\n",
    "\n",
    "for ds_name in dataset_names:\n",
    "    ds_results = [r for r in all_results if r['dataset'] == ds_name]\n",
    "    n_ds = len(ds_results)\n",
    "    if n_ds == 0:\n",
    "        continue\n",
    "\n",
    "    bare_arr = np.array([r['bare'] for r in ds_results])\n",
    "\n",
    "    # Filter invalid (keep zeros -- valid for some datasets)\n",
    "    valid = np.isfinite(bare_arr)\n",
    "    for cname in CONDITION_NAMES:\n",
    "        if cname == 'bare':\n",
    "            continue\n",
    "        c_arr = np.array([r[cname] for r in ds_results])\n",
    "        valid &= np.isfinite(c_arr)\n",
    "\n",
    "    n_valid = int(np.sum(valid))\n",
    "\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    pct_floor = 100 * np.mean(bare_arr < 0.01)\n",
    "    print(f\"DATASET: {ds_name.upper()} (n={n_valid}/{n_ds}, \"\n",
    "          f\"median bare NLL={np.median(bare_arr):.3f}, \"\n",
    "          f\"pct_floor={pct_floor:.0f}%)\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "    print(f\"\\n{'Condition':<20} {'Mean Bare':>10} {'Mean Cond':>10} \"\n",
    "          f\"{'Mean D':>10} {'d':>8} {'Win%':>7} {'p':>12} {'sig':>5}\")\n",
    "    print(\"-\" * 90)\n",
    "\n",
    "    ds_analysis = {}\n",
    "    for cname in CONDITION_NAMES:\n",
    "        if cname == 'bare':\n",
    "            continue\n",
    "        c_arr = np.array([r[cname] for r in ds_results])\n",
    "        delta = bare_arr[valid] - c_arr[valid]\n",
    "        d = cohens_d(delta)\n",
    "        win = np.mean(delta > 0) * 100\n",
    "        t_stat, p_val = stats.ttest_1samp(delta, 0)\n",
    "        sig = '***' if p_val < 0.001 else '**' if p_val < 0.01 else '*' if p_val < 0.05 else 'ns'\n",
    "        print(f\"{cname:<20} {np.mean(bare_arr[valid]):>10.4f} {np.mean(c_arr[valid]):>10.4f} \"\n",
    "              f\"{np.mean(delta):>+10.4f} {d:>+8.3f} {win:>6.1f}% {p_val:>12.2e} {sig:>5}\")\n",
    "        ds_analysis[cname] = {\n",
    "            'n_valid': n_valid,\n",
    "            'mean_bare': float(np.mean(bare_arr[valid])),\n",
    "            'mean_cond': float(np.mean(c_arr[valid])),\n",
    "            'mean_delta': float(np.mean(delta)),\n",
    "            'cohens_d': float(d),\n",
    "            'win_pct': float(win),\n",
    "            't_stat': float(t_stat),\n",
    "            'p_value': float(p_val),\n",
    "        }\n",
    "\n",
    "    analysis[ds_name] = ds_analysis\n",
    "\n",
    "# Cross-dataset summary table\n",
    "print(f\"\\n\\n{'='*90}\")\n",
    "print(\"CROSS-DATASET SUMMARY: Cohen's d vs bare (Gemma 3 4B)\")\n",
    "print(f\"{'='*90}\")\n",
    "print(f\"\\n{'Condition':<20}\", end='')\n",
    "for ds in dataset_names:\n",
    "    print(f\"{'  ' + ds:>16}\", end='')\n",
    "print()\n",
    "print(\"-\" * 68)\n",
    "for cname in CONDITION_NAMES:\n",
    "    if cname == 'bare':\n",
    "        continue\n",
    "    print(f\"{cname:<20}\", end='')\n",
    "    for ds in dataset_names:\n",
    "        if ds in analysis and cname in analysis[ds]:\n",
    "            d = analysis[ds][cname]['cohens_d']\n",
    "            p = analysis[ds][cname]['p_value']\n",
    "            sig = '***' if p < 0.001 else '**' if p < 0.01 else '*' if p < 0.05 else ''\n",
    "            print(f\"{d:>+12.3f}{sig:>4}\", end='')\n",
    "        else:\n",
    "            print(f\"{'n/a':>16}\", end='')\n",
    "    print()\n",
    "\n",
    "# Bare NLL distributions\n",
    "print(f\"\\n\\nBARE NLL DISTRIBUTIONS (ceiling effect check):\")\n",
    "for ds in dataset_names:\n",
    "    ds_r = [r for r in all_results if r['dataset'] == ds]\n",
    "    bare = [r['bare'] for r in ds_r]\n",
    "    pct_zero = 100 * np.mean(np.array(bare) < 0.01)\n",
    "    iqr = np.percentile(bare, 75) - np.percentile(bare, 25)\n",
    "    print(f\"  {ds:15s}: mean={np.mean(bare):.3f}, median={np.median(bare):.3f}, \"\n",
    "          f\"IQR={iqr:.3f}, pct_floor={pct_zero:.0f}%\")\n",
    "\n",
    "# Reference from prior exps\n",
    "print(f\"\\n\\n{'='*90}\")\n",
    "print(\"COMPARISON WITH PRIOR EXPERIMENTS\")\n",
    "print(f\"{'='*90}\")\n",
    "print(\"\\nExp 27b (Gemma, NQ/TriviaQA/HotpotQA):\")\n",
    "print(\"  NQ:       values_hero d=+0.213***\")\n",
    "print(\"  TriviaQA: values_hero d=+0.000 (77% at floor)\")\n",
    "print(\"  HotpotQA: values_hero d=-0.069 (56% at floor)\")\n",
    "print(\"\\nExp 29 (Gemma, DROP/AdvQA/CoQA):\")\n",
    "print(\"  DROP:     values_hero d=-0.152**\")\n",
    "print(\"  AdvQA:    values_hero d=+0.026 (72% at floor)\")\n",
    "print(\"  CoQA:     values_hero d=+0.070 (65% at floor)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b71eb707",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-17T15:30:44.955792Z",
     "iopub.status.busy": "2026-02-17T15:30:44.954984Z",
     "iopub.status.idle": "2026-02-17T15:30:44.983140Z",
     "shell.execute_reply": "2026-02-17T15:30:44.982238Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "WITHIN-DROP SPLIT: Number vs Span Answer Types\n",
      "======================================================================\n",
      "This is the most important analysis: same dataset, same passages,\n",
      "different answer types. Number = computation, Span = retrieval.\n",
      "\n",
      "DROP-number: n=178\n",
      "DROP-span:   n=122\n",
      "\n",
      "======================================================================\n",
      "DROP_NUMBER (n=178, median bare NLL=1.291, pct_floor=20%)\n",
      "======================================================================\n",
      "\n",
      "Condition                   d    Win%            p   sig\n",
      "------------------------------------------------------------\n",
      "sf_trunc               -0.096   34.8%     2.01e-01    ns\n",
      "values_early           -0.117   39.9%     1.21e-01    ns\n",
      "values_hero            -0.198   33.1%     8.82e-03    **\n",
      "\n",
      "======================================================================\n",
      "DROP_SPAN (n=122, median bare NLL=0.000, pct_floor=80%)\n",
      "======================================================================\n",
      "\n",
      "Condition                   d    Win%            p   sig\n",
      "------------------------------------------------------------\n",
      "sf_trunc               -0.141   29.5%     1.23e-01    ns\n",
      "values_early           -0.035   26.2%     6.97e-01    ns\n",
      "values_hero            +0.016   16.4%     8.56e-01    ns\n",
      "\n",
      "\n",
      "BARE NLL DISTRIBUTIONS BY ANSWER TYPE:\n",
      "  drop_number    : mean=2.245, median=1.291, IQR=4.077, pct_floor=20%\n",
      "  drop_span      : mean=0.116, median=0.000, IQR=0.004, pct_floor=80%\n",
      "\n",
      "\n",
      "DIFFICULTY-MATCHED WITHIN DROP (bare > 0.5):\n",
      "  drop_number (n_hard=102): hero d=-0.220, win=33%, p=2.88e-02 *\n",
      "  drop_span: n_hard=9 (too few)\n",
      "\n",
      "======================================================================\n",
      "INTERPRETATION\n",
      "======================================================================\n",
      "  DROP-number hero d: -0.198\n",
      "  DROP-span hero d:   +0.016\n",
      "  -> CONSISTENT with task-type hypothesis: span (retrieval) > number (computation)\n"
     ]
    }
   ],
   "source": [
    "# Cell 11: Within-DROP split by answer type (THE KEY TEST)\n",
    "print(\"=\" * 70)\n",
    "print(\"WITHIN-DROP SPLIT: Number vs Span Answer Types\")\n",
    "print(\"=\" * 70)\n",
    "print(\"This is the most important analysis: same dataset, same passages,\")\n",
    "print(\"different answer types. Number = computation, Span = retrieval.\")\n",
    "\n",
    "drop_results = [r for r in all_results if r['dataset'] == 'drop']\n",
    "\n",
    "# Split by answer_type\n",
    "drop_number = [r for r in drop_results if r.get('answer_type') == 'number']\n",
    "drop_span = [r for r in drop_results if r.get('answer_type') == 'span']\n",
    "\n",
    "print(f\"\\nDROP-number: n={len(drop_number)}\")\n",
    "print(f\"DROP-span:   n={len(drop_span)}\")\n",
    "\n",
    "drop_split_analysis = {}\n",
    "\n",
    "for split_name, split_results in [('drop_number', drop_number), ('drop_span', drop_span)]:\n",
    "    if len(split_results) < 20:\n",
    "        print(f\"\\n  {split_name}: too few samples ({len(split_results)}), skipping\")\n",
    "        drop_split_analysis[split_name] = {'n': len(split_results), 'skipped': True}\n",
    "        continue\n",
    "\n",
    "    bare_arr = np.array([r['bare'] for r in split_results])\n",
    "    pct_floor = 100 * np.mean(bare_arr < 0.01)\n",
    "\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"{split_name.upper()} (n={len(split_results)}, \"\n",
    "          f\"median bare NLL={np.median(bare_arr):.3f}, pct_floor={pct_floor:.0f}%)\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "    print(f\"\\n{'Condition':<20} {'d':>8} {'Win%':>7} {'p':>12} {'sig':>5}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    split_data = {'n': len(split_results), 'pct_floor': float(pct_floor)}\n",
    "    for cname in ['sf_trunc', 'values_early', 'values_hero']:\n",
    "        c_arr = np.array([r[cname] for r in split_results])\n",
    "        delta = bare_arr - c_arr\n",
    "        valid = np.isfinite(delta)\n",
    "        delta = delta[valid]\n",
    "        if len(delta) < 10:\n",
    "            print(f\"{cname:<20} {'n/a (too few valid)':>40}\")\n",
    "            continue\n",
    "        d = cohens_d(delta)\n",
    "        win = np.mean(delta > 0) * 100\n",
    "        t_stat, p_val = stats.ttest_1samp(delta, 0)\n",
    "        sig = '***' if p_val < 0.001 else '**' if p_val < 0.01 else '*' if p_val < 0.05 else 'ns'\n",
    "        print(f\"{cname:<20} {d:>+8.3f} {win:>6.1f}% {p_val:>12.2e} {sig:>5}\")\n",
    "        split_data[cname] = {\n",
    "            'cohens_d': float(d),\n",
    "            'win_pct': float(win),\n",
    "            'p_value': float(p_val),\n",
    "        }\n",
    "\n",
    "    drop_split_analysis[split_name] = split_data\n",
    "\n",
    "# Bare NLL distributions by type\n",
    "print(f\"\\n\\nBARE NLL DISTRIBUTIONS BY ANSWER TYPE:\")\n",
    "for split_name, split_results in [('drop_number', drop_number), ('drop_span', drop_span)]:\n",
    "    if not split_results:\n",
    "        continue\n",
    "    bare = np.array([r['bare'] for r in split_results])\n",
    "    pct_floor = 100 * np.mean(bare < 0.01)\n",
    "    print(f\"  {split_name:15s}: mean={np.mean(bare):.3f}, median={np.median(bare):.3f}, \"\n",
    "          f\"IQR={np.percentile(bare,75)-np.percentile(bare,25):.3f}, \"\n",
    "          f\"pct_floor={pct_floor:.0f}%\")\n",
    "\n",
    "# Difficulty-matched within DROP\n",
    "print(f\"\\n\\nDIFFICULTY-MATCHED WITHIN DROP (bare > 0.5):\")\n",
    "for split_name, split_results in [('drop_number', drop_number), ('drop_span', drop_span)]:\n",
    "    hard = [r for r in split_results if r['bare'] > 0.5]\n",
    "    if len(hard) < 10:\n",
    "        print(f\"  {split_name}: n_hard={len(hard)} (too few)\")\n",
    "        continue\n",
    "    bare_h = np.array([r['bare'] for r in hard])\n",
    "    hero_h = np.array([r['values_hero'] for r in hard])\n",
    "    delta_h = bare_h - hero_h\n",
    "    d_h = cohens_d(delta_h)\n",
    "    win_h = np.mean(delta_h > 0) * 100\n",
    "    _, p_h = stats.ttest_1samp(delta_h, 0)\n",
    "    sig_h = '***' if p_h < 0.001 else '**' if p_h < 0.01 else '*' if p_h < 0.05 else 'ns'\n",
    "    print(f\"  {split_name} (n_hard={len(hard)}): hero d={d_h:+.3f}, \"\n",
    "          f\"win={win_h:.0f}%, p={p_h:.2e} {sig_h}\")\n",
    "\n",
    "# Key interpretation\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"INTERPRETATION\")\n",
    "print(f\"{'='*70}\")\n",
    "hero_num = drop_split_analysis.get('drop_number', {}).get('values_hero', {})\n",
    "hero_span = drop_split_analysis.get('drop_span', {}).get('values_hero', {})\n",
    "d_num = hero_num.get('cohens_d', float('nan'))\n",
    "d_span = hero_span.get('cohens_d', float('nan'))\n",
    "print(f\"  DROP-number hero d: {d_num:+.3f}\")\n",
    "print(f\"  DROP-span hero d:   {d_span:+.3f}\")\n",
    "if d_span > d_num + 0.05:\n",
    "    print(\"  -> CONSISTENT with task-type hypothesis: span (retrieval) > number (computation)\")\n",
    "elif abs(d_span - d_num) < 0.05:\n",
    "    print(\"  -> INCONCLUSIVE: number and span show similar effects\")\n",
    "else:\n",
    "    print(\"  -> INCONSISTENT: number benefits MORE than span (unexpected)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "855d93a0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-17T15:30:44.986714Z",
     "iopub.status.busy": "2026-02-17T15:30:44.986429Z",
     "iopub.status.idle": "2026-02-17T15:30:45.357216Z",
     "shell.execute_reply": "2026-02-17T15:30:45.356117Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "DIFFICULTY-MATCHED CROSS-DATASET COMPARISON\n",
      "======================================================================\n",
      "Filter to hard samples (bare > 0.5) across all datasets.\n",
      "Compare hero d for retrieval vs computation subsets.\n",
      "  nq: 84/300 hard samples (bare > 0.5)\n",
      "  drop: 111/300 hard samples (bare > 0.5)\n",
      "  boolq: 0/300 hard samples (bare > 0.5)\n",
      "  drop_number: 102 hard\n",
      "  drop_span:   9 hard\n",
      "\n",
      "======================================================================\n",
      "HERO d ON HARD SAMPLES (bare > 0.5)\n",
      "======================================================================\n",
      "\n",
      "Subset                   N   hero d               95% CI            p   sig\n",
      "---------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nq                      84   +0.429 [  +0.237,   +0.617]     1.74e-04   ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drop_number            102   -0.220 [  -0.470,   -0.018]     2.88e-02     *\n",
      "drop_span                9                             (n<20, skip)\n",
      "boolq                    0                             (n<20, skip)\n",
      "\n",
      "\n",
      "WELCH'S T-TEST: Retrieval vs Computation (hard samples)\n",
      "----------------------------------------------------------------------\n",
      "  Retrieval hard samples: n=93\n",
      "  Computation hard samples: n=102\n",
      "  Retrieval mean delta: +0.0897\n",
      "  Computation mean delta: -0.2437\n",
      "  Welch's t=2.970, p=3.66e-03 **\n",
      "  -> SIGNIFICANT difference between retrieval and computation hard-sample hero effects\n"
     ]
    }
   ],
   "source": [
    "# Cell 12: Difficulty-matched cross-dataset comparison\n",
    "print(\"=\" * 70)\n",
    "print(\"DIFFICULTY-MATCHED CROSS-DATASET COMPARISON\")\n",
    "print(\"=\" * 70)\n",
    "print(\"Filter to hard samples (bare > 0.5) across all datasets.\")\n",
    "print(\"Compare hero d for retrieval vs computation subsets.\")\n",
    "\n",
    "# Collect hard samples by subset\n",
    "hard_subsets = {}\n",
    "for ds_name in ['nq', 'drop', 'boolq']:\n",
    "    ds_r = [r for r in all_results if r['dataset'] == ds_name]\n",
    "    hard = [r for r in ds_r if r['bare'] > 0.5]\n",
    "    hard_subsets[ds_name] = hard\n",
    "    print(f\"  {ds_name}: {len(hard)}/{len(ds_r)} hard samples (bare > 0.5)\")\n",
    "\n",
    "# Also split DROP hard by type\n",
    "drop_hard = hard_subsets.get('drop', [])\n",
    "drop_hard_number = [r for r in drop_hard if r.get('answer_type') == 'number']\n",
    "drop_hard_span = [r for r in drop_hard if r.get('answer_type') == 'span']\n",
    "hard_subsets['drop_number'] = drop_hard_number\n",
    "hard_subsets['drop_span'] = drop_hard_span\n",
    "print(f\"  drop_number: {len(drop_hard_number)} hard\")\n",
    "print(f\"  drop_span:   {len(drop_hard_span)} hard\")\n",
    "\n",
    "# Hero d for each hard subset\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"HERO d ON HARD SAMPLES (bare > 0.5)\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"\\n{'Subset':<20} {'N':>5} {'hero d':>8} {'95% CI':>20} {'p':>12} {'sig':>5}\")\n",
    "print(\"-\" * 75)\n",
    "\n",
    "hard_hero_ds = {}\n",
    "for subset_name in ['nq', 'drop_number', 'drop_span', 'boolq']:\n",
    "    subset = hard_subsets.get(subset_name, [])\n",
    "    if len(subset) < 20:\n",
    "        print(f\"{subset_name:<20} {len(subset):>5} {'(n<20, skip)':>40}\")\n",
    "        hard_hero_ds[subset_name] = {'n': len(subset), 'skipped': True}\n",
    "        continue\n",
    "\n",
    "    bare_h = np.array([r['bare'] for r in subset])\n",
    "    hero_h = np.array([r['values_hero'] for r in subset])\n",
    "    delta_h = bare_h - hero_h\n",
    "\n",
    "    d_h = cohens_d(delta_h)\n",
    "    _, p_h = stats.ttest_1samp(delta_h, 0)\n",
    "    sig_h = '***' if p_h < 0.001 else '**' if p_h < 0.01 else '*' if p_h < 0.05 else 'ns'\n",
    "\n",
    "    # Bootstrap 95% CI for hero d\n",
    "    np.random.seed(SEED)\n",
    "    boot_ds = []\n",
    "    for _ in range(2000):\n",
    "        idx = np.random.choice(len(delta_h), len(delta_h), replace=True)\n",
    "        boot_ds.append(cohens_d(delta_h[idx]))\n",
    "    ci_lo = np.percentile(boot_ds, 2.5)\n",
    "    ci_hi = np.percentile(boot_ds, 97.5)\n",
    "\n",
    "    print(f\"{subset_name:<20} {len(subset):>5} {d_h:>+8.3f} \"\n",
    "          f\"[{ci_lo:>+8.3f}, {ci_hi:>+8.3f}] {p_h:>12.2e} {sig_h:>5}\")\n",
    "    hard_hero_ds[subset_name] = {\n",
    "        'n': len(subset),\n",
    "        'cohens_d': float(d_h),\n",
    "        'ci_lo': float(ci_lo),\n",
    "        'ci_hi': float(ci_hi),\n",
    "        'p_value': float(p_h),\n",
    "    }\n",
    "\n",
    "# Welch's t-test: retrieval vs computation hard-subset deltas\n",
    "print(f\"\\n\\nWELCH'S T-TEST: Retrieval vs Computation (hard samples)\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "retrieval_deltas = []\n",
    "computation_deltas = []\n",
    "\n",
    "# Retrieval: NQ-hard + DROP-span-hard + BoolQ-hard\n",
    "for subset_name in ['nq', 'drop_span', 'boolq']:\n",
    "    subset = hard_subsets.get(subset_name, [])\n",
    "    for r in subset:\n",
    "        retrieval_deltas.append(r['bare'] - r['values_hero'])\n",
    "\n",
    "# Computation: DROP-number-hard\n",
    "for r in hard_subsets.get('drop_number', []):\n",
    "    computation_deltas.append(r['bare'] - r['values_hero'])\n",
    "\n",
    "retrieval_deltas = np.array(retrieval_deltas)\n",
    "computation_deltas = np.array(computation_deltas)\n",
    "\n",
    "print(f\"  Retrieval hard samples: n={len(retrieval_deltas)}\")\n",
    "print(f\"  Computation hard samples: n={len(computation_deltas)}\")\n",
    "\n",
    "if len(retrieval_deltas) >= 10 and len(computation_deltas) >= 10:\n",
    "    t_stat, p_welch = stats.ttest_ind(retrieval_deltas, computation_deltas, equal_var=False)\n",
    "    sig_w = '***' if p_welch < 0.001 else '**' if p_welch < 0.01 else '*' if p_welch < 0.05 else 'ns'\n",
    "    print(f\"  Retrieval mean delta: {np.mean(retrieval_deltas):+.4f}\")\n",
    "    print(f\"  Computation mean delta: {np.mean(computation_deltas):+.4f}\")\n",
    "    print(f\"  Welch's t={t_stat:.3f}, p={p_welch:.2e} {sig_w}\")\n",
    "    if p_welch < 0.05:\n",
    "        print(\"  -> SIGNIFICANT difference between retrieval and computation hard-sample hero effects\")\n",
    "    else:\n",
    "        print(\"  -> No significant difference (may be underpowered or confounded)\")\n",
    "else:\n",
    "    p_welch = float('nan')\n",
    "    print(\"  -> Too few samples for Welch's t-test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cb672949",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-17T15:30:45.361121Z",
     "iopub.status.busy": "2026-02-17T15:30:45.360792Z",
     "iopub.status.idle": "2026-02-17T15:30:45.410876Z",
     "shell.execute_reply": "2026-02-17T15:30:45.409429Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "TASK-TYPE REGRESSION\n",
      "======================================================================\n",
      "Linear regression: delta_i = b0 + b1*bare_i + b2*is_retrieval_i\n",
      "Tests whether task type predicts hero effect BEYOND difficulty.\n",
      "\n",
      "Regression samples: 900\n",
      "  Retrieval: 722\n",
      "  Computation: 178\n",
      "\n",
      "OLS Regression: delta_hero = b0 + b1*bare + b2*is_retrieval\n",
      "  R-squared: 0.0527\n",
      "\n",
      "Parameter         Estimate         SE        t            p   sig\n",
      "-----------------------------------------------------------------\n",
      "intercept          -0.2493     0.0344   -7.242     9.51e-13   ***\n",
      "bare_nll           +0.0326     0.0077    4.249     2.38e-05   ***\n",
      "is_retrieval       +0.2479     0.0362    6.857     1.31e-11   ***\n",
      "\n",
      "KEY RESULT: beta_2 (is_retrieval) = +0.2479, p = 1.31e-11\n",
      "  -> SIGNIFICANT: Retrieval tasks benefit MORE from hero layers, controlling for difficulty\n",
      "\n",
      "\n",
      "SPEARMAN CORRELATION: hero_d vs pct_floor (dataset level)\n",
      "  nq             : hero d=+0.213, pct_floor=55%\n",
      "  drop_number    : hero d=-0.198, pct_floor=20%\n",
      "  drop_span      : hero d=+0.016, pct_floor=80%\n",
      "  boolq          : hero d=+0.000, pct_floor=100%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Spearman rho=+0.200, p=0.800 (n=4 subsets)\n",
      "  -> Ceiling effects do NOT significantly predict hero d (but n is small)\n"
     ]
    }
   ],
   "source": [
    "# Cell 13: Task-type regression\n",
    "print(\"=\" * 70)\n",
    "print(\"TASK-TYPE REGRESSION\")\n",
    "print(\"=\" * 70)\n",
    "print(\"Linear regression: delta_i = b0 + b1*bare_i + b2*is_retrieval_i\")\n",
    "print(\"Tests whether task type predicts hero effect BEYOND difficulty.\")\n",
    "\n",
    "# Tag all samples by task type\n",
    "# Retrieval: NQ, BoolQ, DROP-span\n",
    "# Computation: DROP-number\n",
    "task_type_map = {}\n",
    "for r in all_results:\n",
    "    if r['dataset'] == 'nq':\n",
    "        task_type_map[r['query_idx']] = 'retrieval'\n",
    "    elif r['dataset'] == 'boolq':\n",
    "        task_type_map[r['query_idx']] = 'retrieval'\n",
    "    elif r['dataset'] == 'drop':\n",
    "        if r.get('answer_type') == 'number':\n",
    "            task_type_map[r['query_idx']] = 'computation'\n",
    "        else:\n",
    "            task_type_map[r['query_idx']] = 'retrieval'\n",
    "\n",
    "# Build regression data\n",
    "bare_vals = []\n",
    "delta_vals = []\n",
    "is_retrieval = []\n",
    "for r in all_results:\n",
    "    if r['query_idx'] not in task_type_map:\n",
    "        continue\n",
    "    b = r['bare']\n",
    "    h = r['values_hero']\n",
    "    if not (np.isfinite(b) and np.isfinite(h)):\n",
    "        continue\n",
    "    bare_vals.append(b)\n",
    "    delta_vals.append(b - h)\n",
    "    is_retrieval.append(1 if task_type_map[r['query_idx']] == 'retrieval' else 0)\n",
    "\n",
    "bare_vals = np.array(bare_vals)\n",
    "delta_vals = np.array(delta_vals)\n",
    "is_retrieval = np.array(is_retrieval)\n",
    "\n",
    "print(f\"\\nRegression samples: {len(bare_vals)}\")\n",
    "print(f\"  Retrieval: {np.sum(is_retrieval)}\")\n",
    "print(f\"  Computation: {np.sum(1 - is_retrieval)}\")\n",
    "\n",
    "# OLS regression: delta = b0 + b1*bare + b2*is_retrieval\n",
    "from numpy.linalg import lstsq\n",
    "\n",
    "X = np.column_stack([np.ones(len(bare_vals)), bare_vals, is_retrieval])\n",
    "beta, residuals, rank, sv = lstsq(X, delta_vals, rcond=None)\n",
    "\n",
    "b0, b1, b2 = beta\n",
    "y_hat = X @ beta\n",
    "ss_res = np.sum((delta_vals - y_hat) ** 2)\n",
    "ss_tot = np.sum((delta_vals - np.mean(delta_vals)) ** 2)\n",
    "r_squared = 1 - ss_res / ss_tot if ss_tot > 0 else 0\n",
    "\n",
    "# Standard errors\n",
    "n = len(delta_vals)\n",
    "p_params = X.shape[1]\n",
    "mse = ss_res / (n - p_params)\n",
    "var_beta = mse * np.linalg.inv(X.T @ X)\n",
    "se_beta = np.sqrt(np.diag(var_beta))\n",
    "t_stats = beta / se_beta\n",
    "p_values = 2 * (1 - stats.t.cdf(np.abs(t_stats), df=n - p_params))\n",
    "\n",
    "print(f\"\\nOLS Regression: delta_hero = b0 + b1*bare + b2*is_retrieval\")\n",
    "print(f\"  R-squared: {r_squared:.4f}\")\n",
    "print(f\"\\n{'Parameter':<15} {'Estimate':>10} {'SE':>10} {'t':>8} {'p':>12} {'sig':>5}\")\n",
    "print(\"-\" * 65)\n",
    "param_names = ['intercept', 'bare_nll', 'is_retrieval']\n",
    "for i, pname in enumerate(param_names):\n",
    "    sig = '***' if p_values[i] < 0.001 else '**' if p_values[i] < 0.01 else '*' if p_values[i] < 0.05 else 'ns'\n",
    "    print(f\"{pname:<15} {beta[i]:>+10.4f} {se_beta[i]:>10.4f} \"\n",
    "          f\"{t_stats[i]:>8.3f} {p_values[i]:>12.2e} {sig:>5}\")\n",
    "\n",
    "print(f\"\\nKEY RESULT: beta_2 (is_retrieval) = {b2:+.4f}, p = {p_values[2]:.2e}\")\n",
    "if p_values[2] < 0.05:\n",
    "    direction = \"MORE\" if b2 > 0 else \"LESS\"\n",
    "    print(f\"  -> SIGNIFICANT: Retrieval tasks benefit {direction} from hero layers, \"\n",
    "          f\"controlling for difficulty\")\n",
    "else:\n",
    "    print(\"  -> NOT SIGNIFICANT: Task type does not predict hero effect beyond difficulty\")\n",
    "\n",
    "# Spearman correlation at dataset-subset level\n",
    "print(f\"\\n\\nSPEARMAN CORRELATION: hero_d vs pct_floor (dataset level)\")\n",
    "subset_ds = []\n",
    "subset_floors = []\n",
    "for subset_name in ['nq', 'drop_number', 'drop_span', 'boolq']:\n",
    "    if subset_name == 'drop_number':\n",
    "        sr = [r for r in all_results if r['dataset'] == 'drop' and r.get('answer_type') == 'number']\n",
    "    elif subset_name == 'drop_span':\n",
    "        sr = [r for r in all_results if r['dataset'] == 'drop' and r.get('answer_type') == 'span']\n",
    "    else:\n",
    "        sr = [r for r in all_results if r['dataset'] == subset_name]\n",
    "    if len(sr) < 20:\n",
    "        continue\n",
    "    bare_s = np.array([r['bare'] for r in sr])\n",
    "    hero_s = np.array([r['values_hero'] for r in sr])\n",
    "    delta_s = bare_s - hero_s\n",
    "    d_s = cohens_d(delta_s)\n",
    "    floor_s = float(np.mean(bare_s < 0.01) * 100)\n",
    "    subset_ds.append(d_s)\n",
    "    subset_floors.append(floor_s)\n",
    "    print(f\"  {subset_name:15s}: hero d={d_s:+.3f}, pct_floor={floor_s:.0f}%\")\n",
    "\n",
    "if len(subset_ds) >= 3:\n",
    "    rho, p_spear = stats.spearmanr(subset_floors, subset_ds)\n",
    "    print(f\"  Spearman rho={rho:+.3f}, p={p_spear:.3f} (n={len(subset_ds)} subsets)\")\n",
    "    if p_spear < 0.05:\n",
    "        print(\"  -> Ceiling effects significantly predict hero d at dataset level\")\n",
    "    else:\n",
    "        print(\"  -> Ceiling effects do NOT significantly predict hero d (but n is small)\")\n",
    "else:\n",
    "    rho, p_spear = float('nan'), float('nan')\n",
    "    print(\"  -> Too few subsets for Spearman\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4d995c84",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-17T15:30:45.422054Z",
     "iopub.status.busy": "2026-02-17T15:30:45.419190Z",
     "iopub.status.idle": "2026-02-17T15:30:47.387931Z",
     "shell.execute_reply": "2026-02-17T15:30:47.386728Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plots saved to results/exp30/analysis_plots.png\n"
     ]
    }
   ],
   "source": [
    "# Cell 14: Multi-panel figure (2x2)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 14))\n",
    "\n",
    "colors_task = {'retrieval': '#2ca02c', 'computation': '#d62728', 'mixed': '#7f7f7f'}\n",
    "\n",
    "# ---- Panel (a): Hero d by dataset-subset, colored by task type ----\n",
    "ax = axes[0, 0]\n",
    "\n",
    "subset_info = [\n",
    "    ('NQ', 'nq', 'retrieval'),\n",
    "    ('DROP-num', 'drop_number', 'computation'),\n",
    "    ('DROP-span', 'drop_span', 'retrieval'),\n",
    "    ('BoolQ', 'boolq', 'retrieval'),\n",
    "]\n",
    "\n",
    "x_pos = np.arange(len(subset_info))\n",
    "bar_ds = []\n",
    "bar_colors = []\n",
    "bar_labels = []\n",
    "\n",
    "for label, subset_key, task_type in subset_info:\n",
    "    if subset_key == 'drop_number':\n",
    "        sr = [r for r in all_results if r['dataset'] == 'drop' and r.get('answer_type') == 'number']\n",
    "    elif subset_key == 'drop_span':\n",
    "        sr = [r for r in all_results if r['dataset'] == 'drop' and r.get('answer_type') == 'span']\n",
    "    else:\n",
    "        sr = [r for r in all_results if r['dataset'] == subset_key]\n",
    "\n",
    "    if len(sr) >= 10:\n",
    "        bare_s = np.array([r['bare'] for r in sr])\n",
    "        hero_s = np.array([r['values_hero'] for r in sr])\n",
    "        delta_s = bare_s - hero_s\n",
    "        d_s = cohens_d(delta_s)\n",
    "        _, p_s = stats.ttest_1samp(delta_s, 0)\n",
    "        sig_s = '***' if p_s < 0.001 else '**' if p_s < 0.01 else '*' if p_s < 0.05 else ''\n",
    "    else:\n",
    "        d_s = 0\n",
    "        sig_s = 'n/a'\n",
    "    bar_ds.append(d_s)\n",
    "    bar_colors.append(colors_task[task_type])\n",
    "    bar_labels.append(f\"{label}\\n({task_type})\")\n",
    "\n",
    "bars = ax.bar(x_pos, bar_ds, color=bar_colors, edgecolor='black', linewidth=0.5)\n",
    "for i, (d_val, sig_val) in enumerate(zip(bar_ds, [\n",
    "    '***' if subset_info[j][1] in ['nq'] else sig_s for j in range(len(subset_info))\n",
    "])):\n",
    "    ax.text(i, d_val + (0.01 if d_val >= 0 else -0.03),\n",
    "            f\"{d_val:+.3f}\", ha='center',\n",
    "            va='bottom' if d_val >= 0 else 'top', fontsize=9, fontweight='bold')\n",
    "\n",
    "# Re-compute significance for each bar\n",
    "for i, (label, subset_key, task_type) in enumerate(subset_info):\n",
    "    if subset_key == 'drop_number':\n",
    "        sr = [r for r in all_results if r['dataset'] == 'drop' and r.get('answer_type') == 'number']\n",
    "    elif subset_key == 'drop_span':\n",
    "        sr = [r for r in all_results if r['dataset'] == 'drop' and r.get('answer_type') == 'span']\n",
    "    else:\n",
    "        sr = [r for r in all_results if r['dataset'] == subset_key]\n",
    "    if len(sr) >= 10:\n",
    "        bare_s = np.array([r['bare'] for r in sr])\n",
    "        hero_s = np.array([r['values_hero'] for r in sr])\n",
    "        delta_s = bare_s - hero_s\n",
    "        _, p_s = stats.ttest_1samp(delta_s, 0)\n",
    "        sig_s = '***' if p_s < 0.001 else '**' if p_s < 0.01 else '*' if p_s < 0.05 else ''\n",
    "        if sig_s:\n",
    "            ax.text(i, bar_ds[i] + (0.035 if bar_ds[i] >= 0 else -0.055),\n",
    "                    sig_s, ha='center', va='bottom' if bar_ds[i] >= 0 else 'top', fontsize=10)\n",
    "\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels(bar_labels, fontsize=9)\n",
    "ax.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "ax.set_ylabel(\"Cohen's d (positive = helps)\")\n",
    "ax.set_title(\"(a) Hero Layer Effect by Dataset-Subset\\n(colored by task type)\")\n",
    "\n",
    "# Legend for task types\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [Patch(facecolor=colors_task['retrieval'], label='Retrieval'),\n",
    "                   Patch(facecolor=colors_task['computation'], label='Computation')]\n",
    "ax.legend(handles=legend_elements, fontsize=9)\n",
    "\n",
    "# ---- Panel (b): Hard-sample delta distributions (violin/box) ----\n",
    "ax = axes[0, 1]\n",
    "\n",
    "hard_data = []\n",
    "hard_labels = []\n",
    "hard_colors_list = []\n",
    "for label, subset_key, task_type in subset_info:\n",
    "    if subset_key == 'drop_number':\n",
    "        sr = [r for r in all_results if r['dataset'] == 'drop' and r.get('answer_type') == 'number']\n",
    "    elif subset_key == 'drop_span':\n",
    "        sr = [r for r in all_results if r['dataset'] == 'drop' and r.get('answer_type') == 'span']\n",
    "    else:\n",
    "        sr = [r for r in all_results if r['dataset'] == subset_key]\n",
    "    hard = [r['bare'] - r['values_hero'] for r in sr if r['bare'] > 0.5]\n",
    "    if len(hard) >= 10:\n",
    "        hard_data.append(hard)\n",
    "        hard_labels.append(f\"{label}\\n(n={len(hard)})\")\n",
    "        hard_colors_list.append(colors_task[task_type])\n",
    "\n",
    "if hard_data:\n",
    "    bp = ax.boxplot(hard_data, labels=hard_labels, showfliers=True, patch_artist=True,\n",
    "                    medianprops={'color': 'black', 'linewidth': 2},\n",
    "                    flierprops={'markersize': 3, 'alpha': 0.5})\n",
    "    for patch, color in zip(bp['boxes'], hard_colors_list):\n",
    "        patch.set_facecolor(color)\n",
    "        patch.set_alpha(0.7)\n",
    "\n",
    "ax.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "ax.set_ylabel(\"Delta (bare - hero, positive = hero helps)\")\n",
    "ax.set_title(\"(b) Hard-Sample Delta Distributions\\n(bare > 0.5)\")\n",
    "\n",
    "# ---- Panel (c): Hardness gradient: hero d by quintile for NQ vs DROP-number ----\n",
    "ax = axes[1, 0]\n",
    "quintile_labels = ['Q1\\n(easy)', 'Q2', 'Q3', 'Q4', 'Q5\\n(hard)']\n",
    "\n",
    "for ds_label, ds_filter, color, marker in [\n",
    "    ('NQ', lambda r: r['dataset'] == 'nq', '#2ca02c', 'o'),\n",
    "    ('DROP-number', lambda r: r['dataset'] == 'drop' and r.get('answer_type') == 'number', '#d62728', 's'),\n",
    "    ('DROP-span', lambda r: r['dataset'] == 'drop' and r.get('answer_type') == 'span', '#ff7f0e', '^'),\n",
    "    ('BoolQ', lambda r: r['dataset'] == 'boolq', '#1f77b4', 'D'),\n",
    "]:\n",
    "    ds_r = [r for r in all_results if ds_filter(r)]\n",
    "    if len(ds_r) < 50:\n",
    "        continue\n",
    "    bare_arr = np.array([r['bare'] for r in ds_r])\n",
    "    hero_arr = np.array([r['values_hero'] for r in ds_r])\n",
    "    delta_arr = bare_arr - hero_arr\n",
    "    quintile_boundaries = np.percentile(bare_arr, [20, 40, 60, 80])\n",
    "\n",
    "    q_ds = []\n",
    "    q_ns = []\n",
    "    for q in range(5):\n",
    "        if q < 4:\n",
    "            lo = quintile_boundaries[q-1] if q > 0 else -np.inf\n",
    "            hi = quintile_boundaries[q]\n",
    "        else:\n",
    "            lo = quintile_boundaries[3]\n",
    "            hi = np.inf\n",
    "        mask = (bare_arr > lo) & (bare_arr <= hi)\n",
    "        if q == 0:\n",
    "            mask = bare_arr <= quintile_boundaries[0]\n",
    "        n_q = int(np.sum(mask))\n",
    "        if n_q >= 5:\n",
    "            q_ds.append(cohens_d(delta_arr[mask]))\n",
    "        else:\n",
    "            q_ds.append(np.nan)\n",
    "        q_ns.append(n_q)\n",
    "\n",
    "    valid_q = [(i, d) for i, d in enumerate(q_ds) if not np.isnan(d)]\n",
    "    if valid_q:\n",
    "        xs, ys = zip(*valid_q)\n",
    "        ax.plot(xs, ys, marker=marker, linewidth=2, markersize=7, label=ds_label, color=color)\n",
    "\n",
    "ax.set_xticks(range(5))\n",
    "ax.set_xticklabels(quintile_labels, fontsize=8)\n",
    "ax.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "ax.set_ylabel(\"Cohen's d (hero vs bare)\")\n",
    "ax.set_xlabel(\"Bare NLL Quintile\")\n",
    "ax.set_title(\"(c) Hardness Gradient: Hero Effect by Quintile\")\n",
    "ax.legend(fontsize=8)\n",
    "\n",
    "# ---- Panel (d): Bare NLL distributions by dataset (ceiling check) ----\n",
    "ax = axes[1, 1]\n",
    "bare_by_subset = []\n",
    "subset_labels_plot = []\n",
    "for label, subset_key, task_type in subset_info:\n",
    "    if subset_key == 'drop_number':\n",
    "        sr = [r for r in all_results if r['dataset'] == 'drop' and r.get('answer_type') == 'number']\n",
    "    elif subset_key == 'drop_span':\n",
    "        sr = [r for r in all_results if r['dataset'] == 'drop' and r.get('answer_type') == 'span']\n",
    "    else:\n",
    "        sr = [r for r in all_results if r['dataset'] == subset_key]\n",
    "    if sr:\n",
    "        bare_by_subset.append([r['bare'] for r in sr])\n",
    "        pct_f = 100 * np.mean(np.array([r['bare'] for r in sr]) < 0.01)\n",
    "        subset_labels_plot.append(f\"{label}\\n({pct_f:.0f}% floor)\")\n",
    "\n",
    "if bare_by_subset:\n",
    "    bp = ax.boxplot(bare_by_subset, labels=subset_labels_plot, showfliers=False, patch_artist=True,\n",
    "                    medianprops={'color': 'red', 'linewidth': 2})\n",
    "    for patch in bp['boxes']:\n",
    "        patch.set_facecolor('#8ecae6')\n",
    "        patch.set_alpha(0.7)\n",
    "\n",
    "ax.axhline(y=0.01, color='red', linestyle='--', alpha=0.3, label='Floor threshold (0.01)')\n",
    "ax.set_ylabel(\"Bare NLL\")\n",
    "ax.set_title(\"(d) Bare NLL Distributions (ceiling check)\")\n",
    "ax.legend(fontsize=7)\n",
    "\n",
    "plt.suptitle('Exp 30: Retrieval vs Reasoning Task-Type Dissociation (Gemma 3 4B)',\n",
    "             fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'analysis_plots.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"Plots saved to {RESULTS_DIR / 'analysis_plots.png'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ff0e8051",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-17T15:30:47.391846Z",
     "iopub.status.busy": "2026-02-17T15:30:47.391539Z",
     "iopub.status.idle": "2026-02-17T15:30:47.443426Z",
     "shell.execute_reply": "2026-02-17T15:30:47.442334Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV saved: results/exp30/results.csv\n",
      "\n",
      "Verdict inputs:\n",
      "  Retrieval hard d: +0.402 (n=93)\n",
      "  Computation hard d: -0.220 (n=102)\n",
      "  Regression beta_2 p: 1.31e-11\n",
      "  Regression significant: True\n",
      "\n",
      "======================================================================\n",
      "VERDICT: SUPPORTED: Hero layers selectively help retrieval. Retrieval hard d=+0.402, computation hard d=-0.220, beta_2 p=1.31e-11\n",
      "======================================================================\n",
      "\n",
      "Hero scorecard (this experiment):\n",
      "  nq             : d=+0.213 ***\n",
      "  drop           : d=-0.152 **\n",
      "  boolq          : d=+0.000 ns\n",
      "\n",
      "Updated hero scorecard (all experiments):\n",
      "  MARCO:    d=+0.472*** (Exp 07, Mistral)\n",
      "  NQ:       d=+0.213*** (Exp 27b, Gemma)\n",
      "  NQ (30):  d=+0.213 (this experiment)\n",
      "  TriviaQA: d=+0.000 (Exp 27b, ceiling)\n",
      "  HotpotQA: d=-0.069 (Exp 27b)\n",
      "  AdvQA:    d=+0.026 (Exp 29, ceiling)\n",
      "  CoQA:     d=+0.070 (Exp 29, ceiling)\n",
      "  DROP:     d=-0.152** (Exp 29)\n",
      "  DROP (30): d=-0.152 (this experiment)\n",
      "  BoolQ:    d=+0.000 (this experiment, NEW)\n",
      "\n",
      "Results saved to results/exp30/results.json\n",
      "File size: 332.9 KB\n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Cell 15: Save results.json + CSV + verdict\n",
    "\n",
    "# --- CSV ---\n",
    "with open(CSV_PATH, 'w', newline='') as f:\n",
    "    fieldnames = ['query_idx', 'dataset', 'query', 'answer', 'word_count',\n",
    "                  'doc_token_len', 'answer_token_len', 'answer_type',\n",
    "                  'bare', 'sf_trunc', 'values_early', 'values_hero']\n",
    "    writer = csv.DictWriter(f, fieldnames=fieldnames, extrasaction='ignore')\n",
    "    writer.writeheader()\n",
    "    for r in all_results:\n",
    "        writer.writerow({k: r.get(k, '') for k in fieldnames})\n",
    "print(f\"CSV saved: {CSV_PATH}\")\n",
    "\n",
    "# --- Compute verdict inputs ---\n",
    "\n",
    "# Hero d on hard retrieval samples\n",
    "retrieval_hard = []\n",
    "for r in all_results:\n",
    "    task_type = task_type_map.get(r['query_idx'])\n",
    "    if task_type == 'retrieval' and r['bare'] > 0.5:\n",
    "        retrieval_hard.append(r['bare'] - r['values_hero'])\n",
    "retrieval_hard_d = cohens_d(np.array(retrieval_hard)) if len(retrieval_hard) >= 10 else float('nan')\n",
    "\n",
    "# Hero d on hard computation samples\n",
    "computation_hard = []\n",
    "for r in all_results:\n",
    "    task_type = task_type_map.get(r['query_idx'])\n",
    "    if task_type == 'computation' and r['bare'] > 0.5:\n",
    "        computation_hard.append(r['bare'] - r['values_hero'])\n",
    "computation_hard_d = cohens_d(np.array(computation_hard)) if len(computation_hard) >= 10 else float('nan')\n",
    "\n",
    "# Regression significance\n",
    "regression_sig = p_values[2] < 0.05 if len(p_values) > 2 else False\n",
    "\n",
    "print(f\"\\nVerdict inputs:\")\n",
    "print(f\"  Retrieval hard d: {retrieval_hard_d:+.3f} (n={len(retrieval_hard)})\")\n",
    "print(f\"  Computation hard d: {computation_hard_d:+.3f} (n={len(computation_hard)})\")\n",
    "print(f\"  Regression beta_2 p: {p_values[2]:.2e}\")\n",
    "print(f\"  Regression significant: {regression_sig}\")\n",
    "\n",
    "# --- Verdict ---\n",
    "if retrieval_hard_d > 0.15 and computation_hard_d < 0 and regression_sig:\n",
    "    verdict = (\"SUPPORTED: Hero layers selectively help retrieval. \"\n",
    "               f\"Retrieval hard d={retrieval_hard_d:+.3f}, \"\n",
    "               f\"computation hard d={computation_hard_d:+.3f}, \"\n",
    "               f\"beta_2 p={p_values[2]:.2e}\")\n",
    "elif np.mean(np.array([r['bare'] for r in all_results]) < 0.01) > 0.5 and not regression_sig:\n",
    "    verdict = (\"CONFOUNDED: Cannot separate task type from ceiling effects. \"\n",
    "               f\"Overall {100*np.mean(np.array([r['bare'] for r in all_results]) < 0.01):.0f}% at floor.\")\n",
    "else:\n",
    "    verdict = (\"INCONCLUSIVE: \" +\n",
    "               f\"Retrieval hard d={retrieval_hard_d:+.3f}, \"\n",
    "               f\"computation hard d={computation_hard_d:+.3f}, \"\n",
    "               f\"regression p={p_values[2]:.2e}\")\n",
    "\n",
    "# Hero scorecard\n",
    "hero_scorecard = {}\n",
    "for ds_name in dataset_names:\n",
    "    if ds_name in analysis and 'values_hero' in analysis[ds_name]:\n",
    "        hero_scorecard[ds_name] = analysis[ds_name]['values_hero']['cohens_d']\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"VERDICT: {verdict}\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"\\nHero scorecard (this experiment):\")\n",
    "for ds, d in hero_scorecard.items():\n",
    "    p = analysis[ds]['values_hero']['p_value']\n",
    "    sig = '***' if p < 0.001 else '**' if p < 0.01 else '*' if p < 0.05 else 'ns'\n",
    "    print(f\"  {ds:15s}: d={d:+.3f} {sig}\")\n",
    "\n",
    "# Reference table\n",
    "print(f\"\\nUpdated hero scorecard (all experiments):\")\n",
    "print(\"  MARCO:    d=+0.472*** (Exp 07, Mistral)\")\n",
    "print(\"  NQ:       d=+0.213*** (Exp 27b, Gemma)\")\n",
    "hero_nq = hero_scorecard.get('nq', '?')\n",
    "if isinstance(hero_nq, float):\n",
    "    print(f\"  NQ (30):  d={hero_nq:+.3f} (this experiment)\")\n",
    "print(\"  TriviaQA: d=+0.000 (Exp 27b, ceiling)\")\n",
    "print(\"  HotpotQA: d=-0.069 (Exp 27b)\")\n",
    "print(\"  AdvQA:    d=+0.026 (Exp 29, ceiling)\")\n",
    "print(\"  CoQA:     d=+0.070 (Exp 29, ceiling)\")\n",
    "print(\"  DROP:     d=-0.152** (Exp 29)\")\n",
    "hero_drop = hero_scorecard.get('drop', '?')\n",
    "if isinstance(hero_drop, float):\n",
    "    print(f\"  DROP (30): d={hero_drop:+.3f} (this experiment)\")\n",
    "hero_boolq = hero_scorecard.get('boolq', '?')\n",
    "if isinstance(hero_boolq, float):\n",
    "    print(f\"  BoolQ:    d={hero_boolq:+.3f} (this experiment, NEW)\")\n",
    "\n",
    "# --- results.json ---\n",
    "final = {\n",
    "    'experiment': 'exp30_retrieval_vs_reasoning',\n",
    "    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'config': {\n",
    "        'model_name': MODEL_NAME,\n",
    "        'model_type': 'gemma3',\n",
    "        'seed': SEED,\n",
    "        'n_per_dataset': N_PER_DATASET,\n",
    "        'max_doc_tokens': MAX_DOC_TOKENS,\n",
    "        'conditions': CONDITION_NAMES,\n",
    "        'early_layer_cutoff': EARLY_LAYER_CUTOFF,\n",
    "        'hero_layers': HERO_LAYERS,\n",
    "        'prefix': STATIC_FACT,\n",
    "        'prefix_token_len': PREFIX_TOKEN_LEN,\n",
    "        'datasets': dataset_names,\n",
    "    },\n",
    "    'per_dataset_analysis': analysis,\n",
    "    'drop_split_analysis': drop_split_analysis,\n",
    "    'hard_hero_analysis': hard_hero_ds,\n",
    "    'regression': {\n",
    "        'beta_0': float(b0),\n",
    "        'beta_1_bare': float(b1),\n",
    "        'beta_2_is_retrieval': float(b2),\n",
    "        'se_beta': [float(s) for s in se_beta],\n",
    "        'p_values': [float(p) for p in p_values],\n",
    "        'r_squared': float(r_squared),\n",
    "    },\n",
    "    'ceiling_status': {\n",
    "        ds: float(np.mean(np.array([r['bare'] for r in all_results if r['dataset'] == ds]) < 0.01) * 100)\n",
    "        for ds in dataset_names\n",
    "    },\n",
    "    'verdict': verdict,\n",
    "    'hero_scorecard': hero_scorecard,\n",
    "    'per_sample_results': all_results,\n",
    "}\n",
    "\n",
    "with open(FINAL_RESULTS_PATH, 'w') as f:\n",
    "    json.dump(final, f, indent=2)\n",
    "\n",
    "print(f\"\\nResults saved to {FINAL_RESULTS_PATH}\")\n",
    "print(f\"File size: {FINAL_RESULTS_PATH.stat().st_size / 1024:.1f} KB\")\n",
    "print(f\"\\nDone!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "20069ecc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-17T15:30:47.446850Z",
     "iopub.status.busy": "2026-02-17T15:30:47.446530Z",
     "iopub.status.idle": "2026-02-17T15:30:48.245397Z",
     "shell.execute_reply": "2026-02-17T15:30:48.244171Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning up GPU memory...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory: 3.24 GB -> 0.01 GB\n",
      "Cleanup complete.\n"
     ]
    }
   ],
   "source": [
    "# Cell 16: GPU cleanup\n",
    "import gc\n",
    "\n",
    "print(\"Cleaning up GPU memory...\")\n",
    "mem_before = torch.cuda.memory_allocated() / 1e9\n",
    "\n",
    "del model\n",
    "del tokenizer\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "mem_after = torch.cuda.memory_allocated() / 1e9\n",
    "print(f\"GPU memory: {mem_before:.2f} GB -> {mem_after:.2f} GB\")\n",
    "print(\"Cleanup complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "01bf7f7426f74faa96cb58b568106156": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "078c291fcd384697bb9cb26787b56b31": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "0c1d1d05077140cea542b7813d1f6835": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "0d5cc9308ec645e9b31625eb43f77354": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_3eb7cdd4cf06436a8dbdcfca24b07777",
       "placeholder": "​",
       "style": "IPY_MODEL_a1378c059f9549afb9e51ae87db0bc65",
       "tabbable": null,
       "tooltip": null,
       "value": " 287/287 [00:00&lt;00:00, 13532.45it/s]"
      }
     },
     "0ea1af0765dd40fc8e4610b09fc29f39": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "13a054da39a84b19805d810f2b6da62b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "177fe724fb40482ebbc094f2f93d6c80": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "1c3cf6343c59490cb25f61439c934020": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "1d3adf55b33c4250b119f9e0c2d5f04b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_e2d8b1cd8856430baa1995a8851a136e",
        "IPY_MODEL_61eb7d36e47b422ba609c5a8b8d4dc2a",
        "IPY_MODEL_0d5cc9308ec645e9b31625eb43f77354"
       ],
       "layout": "IPY_MODEL_26bcecd3873c45e5b9ccafae4bd0bc37",
       "tabbable": null,
       "tooltip": null
      }
     },
     "1f97c4f0c8b7454a83d72a3959d13eed": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "1fc094f8885949209751b3d9742e69d9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_4573be17b21d4359aee46d5d89fe2ca2",
        "IPY_MODEL_2ed4f9409b28443e84b0d9a0964ce953",
        "IPY_MODEL_3dbdbc61ea71449bb0c7fe587d9426ba"
       ],
       "layout": "IPY_MODEL_f6df288bdc244aad843901fea657d49d",
       "tabbable": null,
       "tooltip": null
      }
     },
     "25ba8b833e124fb39c874863dd3e1920": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_c9e2cfeaf7aa4a0194f25294394a3d35",
       "placeholder": "​",
       "style": "IPY_MODEL_66ed2d54394448ccaa4f6ea5df80a95f",
       "tabbable": null,
       "tooltip": null,
       "value": " 900/900 [29:57&lt;00:00,  1.92s/it]"
      }
     },
     "26bcecd3873c45e5b9ccafae4bd0bc37": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "27de9e9ef315428a8d7e78872fd17b1b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "28c1fd83594a4fa0a1e5020c878d4569": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_f3a401d4ca544b76b796118af5e71a1e",
       "placeholder": "​",
       "style": "IPY_MODEL_078c291fcd384697bb9cb26787b56b31",
       "tabbable": null,
       "tooltip": null,
       "value": " 900/900 [00:02&lt;00:00, 1148.92it/s]"
      }
     },
     "29fb6fd594de441ebbcfd1f8156593f2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "2ed20fcc527a4adfa7538ed81654ebf2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2ed4f9409b28443e84b0d9a0964ce953": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_bf067dce26474cfdacd5c30990358dca",
       "max": 287.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_6df4efd5fc834bc28c68fc055d2c77e0",
       "tabbable": null,
       "tooltip": null,
       "value": 287.0
      }
     },
     "390fecb3e2544415a5cac415d07722e8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "3b85a40695184e80978d2ac4d349ce49": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_74f490cd95a7406092173b5c7c7af6e2",
       "placeholder": "​",
       "style": "IPY_MODEL_a5725a93be5542bb843b45a0d65acd00",
       "tabbable": null,
       "tooltip": null,
       "value": "Loading weights: 100%"
      }
     },
     "3dbdbc61ea71449bb0c7fe587d9426ba": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_609fbc6e6bda4189aaf940f08d729171",
       "placeholder": "​",
       "style": "IPY_MODEL_49d990d0780143afa48f174640b5e7a5",
       "tabbable": null,
       "tooltip": null,
       "value": " 287/287 [00:00&lt;00:00, 10030.54it/s]"
      }
     },
     "3eb7cdd4cf06436a8dbdcfca24b07777": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "43691747a7064af4908b66d646637d61": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "4468464079cd4aebb62ff686cec99858": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_f5221a47a458491996377cb37ce013d8",
       "placeholder": "​",
       "style": "IPY_MODEL_390fecb3e2544415a5cac415d07722e8",
       "tabbable": null,
       "tooltip": null,
       "value": " 4078/? [01:46&lt;00:00, 39.62it/s]"
      }
     },
     "4573be17b21d4359aee46d5d89fe2ca2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_53d84934cebe4c53a7d5ed89203c394f",
       "placeholder": "​",
       "style": "IPY_MODEL_a491e198e6cc40848a48b7c13b9ec9d9",
       "tabbable": null,
       "tooltip": null,
       "value": "Resolving data files: 100%"
      }
     },
     "48c0cca53217432a8caacf0db5a1530a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "49d990d0780143afa48f174640b5e7a5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "4a3a9e2d6d98460e84df04f017a5da41": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "53d84934cebe4c53a7d5ed89203c394f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "53e9af5ebbb6474089bf71a72aa42dcf": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "5c050f3ceb424ab99b87a494a89987b3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "5c42139d3c654321ac34e778a8838c77": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_e9ac481482b44e30a935e070ba542815",
       "placeholder": "​",
       "style": "IPY_MODEL_13a054da39a84b19805d810f2b6da62b",
       "tabbable": null,
       "tooltip": null,
       "value": "Exp 30: 100%"
      }
     },
     "609fbc6e6bda4189aaf940f08d729171": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "61eb7d36e47b422ba609c5a8b8d4dc2a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_27de9e9ef315428a8d7e78872fd17b1b",
       "max": 287.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_29fb6fd594de441ebbcfd1f8156593f2",
       "tabbable": null,
       "tooltip": null,
       "value": 287.0
      }
     },
     "622ed851ec1f4d05ad1415703178950d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "66ed2d54394448ccaa4f6ea5df80a95f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "6df4efd5fc834bc28c68fc055d2c77e0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "6f62118128914eb7b5f4f3ec6dde7deb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "70021410906b42889cadedf9fe4820d4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "70c6a4d6cf8148afa1afe0af0f0ee478": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "716caaf240414b0894a65984e100ede4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_48c0cca53217432a8caacf0db5a1530a",
       "max": 3270.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_6f62118128914eb7b5f4f3ec6dde7deb",
       "tabbable": null,
       "tooltip": null,
       "value": 3270.0
      }
     },
     "74f490cd95a7406092173b5c7c7af6e2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "76ecd7ec0e474d11ac6295f0098eb5d9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "843b1f5728bd4bcc9aa198dd165935aa": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "8783916588d24534b655852f7b3a85b8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_01bf7f7426f74faa96cb58b568106156",
       "placeholder": "​",
       "style": "IPY_MODEL_53e9af5ebbb6474089bf71a72aa42dcf",
       "tabbable": null,
       "tooltip": null,
       "value": " 3270/3270 [00:00&lt;00:00, 21990.64it/s]"
      }
     },
     "896b03fc677249c68a51ba0e57fc86d8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "915bee407a9d400fa468df1ff4345744": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_f216947048b74407a6830211484b5749",
       "placeholder": "​",
       "style": "IPY_MODEL_d2d4c9ecdfa446dea9dc64b8308203d6",
       "tabbable": null,
       "tooltip": null,
       "value": "Filtering BoolQ: 100%"
      }
     },
     "93e91a9c9a3448c4b168f251aec7f368": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "974c7da066a349198ccbfb37d75d6ca6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "danger",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_dd993b88d5e74b898c875810e53a21d6",
       "max": 1.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_4a3a9e2d6d98460e84df04f017a5da41",
       "tabbable": null,
       "tooltip": null,
       "value": 1.0
      }
     },
     "98de35b56e564a7984b894268feb7b6f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a1378c059f9549afb9e51ae87db0bc65": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "a491e198e6cc40848a48b7c13b9ec9d9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "a5725a93be5542bb843b45a0d65acd00": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "aa5ad3f920684b9c96358b88eac3a434": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "aeffb20e17ef4797a6d5be562f67dbf0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b1650067daed4df8a5a66a1d8ca8f2cc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_5c42139d3c654321ac34e778a8838c77",
        "IPY_MODEL_dadd5a2c747642d6afdeb1c938ae9ce4",
        "IPY_MODEL_25ba8b833e124fb39c874863dd3e1920"
       ],
       "layout": "IPY_MODEL_93e91a9c9a3448c4b168f251aec7f368",
       "tabbable": null,
       "tooltip": null
      }
     },
     "b9534b59ecd6454f83963e65619b7182": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_c702e0df492148cda99328137e7ca1c9",
        "IPY_MODEL_de66e02dcac544fbbf45c451214e1025",
        "IPY_MODEL_28c1fd83594a4fa0a1e5020c878d4569"
       ],
       "layout": "IPY_MODEL_dd73e21350e8426caf0b4eb3bfefeca8",
       "tabbable": null,
       "tooltip": null
      }
     },
     "be34d49476a9449888507b7fbd29535a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "danger",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_0c1d1d05077140cea542b7813d1f6835",
       "max": 9535.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_d1a2e0da92804bb380ca8ac9644d133b",
       "tabbable": null,
       "tooltip": null,
       "value": 899.0
      }
     },
     "bf067dce26474cfdacd5c30990358dca": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c0c75fea79994db8a1a6fbea54ee772f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_915bee407a9d400fa468df1ff4345744",
        "IPY_MODEL_716caaf240414b0894a65984e100ede4",
        "IPY_MODEL_8783916588d24534b655852f7b3a85b8"
       ],
       "layout": "IPY_MODEL_177fe724fb40482ebbc094f2f93d6c80",
       "tabbable": null,
       "tooltip": null
      }
     },
     "c6ffc687abe341f4b7eb59507c1a5ac2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_e5797db2a09940fabe8ace438757b476",
        "IPY_MODEL_be34d49476a9449888507b7fbd29535a",
        "IPY_MODEL_f58bbe6dccd74448bf52e57e3ed32e8e"
       ],
       "layout": "IPY_MODEL_843b1f5728bd4bcc9aa198dd165935aa",
       "tabbable": null,
       "tooltip": null
      }
     },
     "c702e0df492148cda99328137e7ca1c9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_aeffb20e17ef4797a6d5be562f67dbf0",
       "placeholder": "​",
       "style": "IPY_MODEL_70c6a4d6cf8148afa1afe0af0f0ee478",
       "tabbable": null,
       "tooltip": null,
       "value": "Tokenizing: 100%"
      }
     },
     "c8d75906da044c42b744d0e3e1962249": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_3b85a40695184e80978d2ac4d349ce49",
        "IPY_MODEL_f32edf652d5f415989250e5e706a39bd",
        "IPY_MODEL_f0904c1240524e2e85f2f93f415bf44d"
       ],
       "layout": "IPY_MODEL_dc86ea61ae8b4e03b1cbc660cffa3311",
       "tabbable": null,
       "tooltip": null
      }
     },
     "c9e2cfeaf7aa4a0194f25294394a3d35": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d1a2e0da92804bb380ca8ac9644d133b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "d2d4c9ecdfa446dea9dc64b8308203d6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "da03c56a114045ef8808c4777f194257": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "dadd5a2c747642d6afdeb1c938ae9ce4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_aa5ad3f920684b9c96358b88eac3a434",
       "max": 900.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_dc6e55b626ef46bda8415f7e31f402bb",
       "tabbable": null,
       "tooltip": null,
       "value": 900.0
      }
     },
     "dc6e55b626ef46bda8415f7e31f402bb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "dc86ea61ae8b4e03b1cbc660cffa3311": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "dd73e21350e8426caf0b4eb3bfefeca8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "dd993b88d5e74b898c875810e53a21d6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": "20px"
      }
     },
     "de66e02dcac544fbbf45c451214e1025": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_76ecd7ec0e474d11ac6295f0098eb5d9",
       "max": 900.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_1f97c4f0c8b7454a83d72a3959d13eed",
       "tabbable": null,
       "tooltip": null,
       "value": 900.0
      }
     },
     "e05e3eb5549e4d589bf8daec380388ed": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_2ed20fcc527a4adfa7538ed81654ebf2",
       "placeholder": "​",
       "style": "IPY_MODEL_e81faa55f1904f52ad7c06e8fede2f2e",
       "tabbable": null,
       "tooltip": null,
       "value": "Processing NQ: "
      }
     },
     "e1c36224442f4fb2acdedd61c899c017": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_e05e3eb5549e4d589bf8daec380388ed",
        "IPY_MODEL_974c7da066a349198ccbfb37d75d6ca6",
        "IPY_MODEL_4468464079cd4aebb62ff686cec99858"
       ],
       "layout": "IPY_MODEL_43691747a7064af4908b66d646637d61",
       "tabbable": null,
       "tooltip": null
      }
     },
     "e2d8b1cd8856430baa1995a8851a136e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_70021410906b42889cadedf9fe4820d4",
       "placeholder": "​",
       "style": "IPY_MODEL_896b03fc677249c68a51ba0e57fc86d8",
       "tabbable": null,
       "tooltip": null,
       "value": "Resolving data files: 100%"
      }
     },
     "e5797db2a09940fabe8ace438757b476": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_98de35b56e564a7984b894268feb7b6f",
       "placeholder": "​",
       "style": "IPY_MODEL_622ed851ec1f4d05ad1415703178950d",
       "tabbable": null,
       "tooltip": null,
       "value": "Filtering DROP:   9%"
      }
     },
     "e81faa55f1904f52ad7c06e8fede2f2e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "e9ac481482b44e30a935e070ba542815": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f0904c1240524e2e85f2f93f415bf44d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_fc6a833903ad40598ba55e43e6433321",
       "placeholder": "​",
       "style": "IPY_MODEL_fd64308a859e4fb5aedc8f561f0ce2b3",
       "tabbable": null,
       "tooltip": null,
       "value": " 883/883 [00:35&lt;00:00, 617.78it/s, Materializing param=model.vision_tower.vision_model.post_layernorm.weight]"
      }
     },
     "f216947048b74407a6830211484b5749": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f32edf652d5f415989250e5e706a39bd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_1c3cf6343c59490cb25f61439c934020",
       "max": 883.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_da03c56a114045ef8808c4777f194257",
       "tabbable": null,
       "tooltip": null,
       "value": 883.0
      }
     },
     "f3a401d4ca544b76b796118af5e71a1e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f5221a47a458491996377cb37ce013d8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f58bbe6dccd74448bf52e57e3ed32e8e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_0ea1af0765dd40fc8e4610b09fc29f39",
       "placeholder": "​",
       "style": "IPY_MODEL_5c050f3ceb424ab99b87a494a89987b3",
       "tabbable": null,
       "tooltip": null,
       "value": " 899/9535 [00:00&lt;00:01, 5789.25it/s]"
      }
     },
     "f6df288bdc244aad843901fea657d49d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "fc6a833903ad40598ba55e43e6433321": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "fd64308a859e4fb5aedc8f561f0ce2b3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
