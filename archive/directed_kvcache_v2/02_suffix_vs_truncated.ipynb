{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 02: Suffix vs Truncated Priming\n",
    "\n",
    "**Goal**: Determine whether semantic content in a suffix (appended AFTER the passage) improves NLL scoring. This is the cleanest possible test of semantic signal because causal masking guarantees passage KV entries are byte-identical to bare — any benefit must come from query tokens attending to suffix KV entries.\n",
    "\n",
    "## Motivation (from Exp 01)\n",
    "\n",
    "Exp 01 found random prefix helps MORE than oracle prefix after truncation (d=+0.091 vs d=+0.023 ns). Oracle actually *interferes* vs random (d=-0.051, p=0.015). The truncation mechanism conflates two signals:\n",
    "- **Structural value contamination** (beneficial): prefix alters passage value vectors\n",
    "- **Semantic attention patterns** (harmful on average): oracle content creates specific interference\n",
    "\n",
    "**Suffix priming** isolates the semantic signal cleanly: passage KV entries are unchanged, so any effect must come from query → suffix attention.\n",
    "\n",
    "## Five Conditions\n",
    "\n",
    "| # | Condition | Cache Construction | Mechanism |\n",
    "|---|-----------|-------------------|----------|\n",
    "| 1 | **Bare** | `[BOS] + doc_ids` (matched tokenization) | Baseline |\n",
    "| 2 | **Oracle-truncated** | `[BOS][query\\n][doc_ids]` → truncate + RoPE correct | Value contamination (Exp 01 replication) |\n",
    "| 3 | **Random-truncated** | `[BOS][random\\n][doc_ids]` → truncate + RoPE correct | Structural control (Exp 01 replication) |\n",
    "| 4 | **Oracle-suffix** | `build_suffix_kv_cache(passage, query, sep)` | Clean semantic signal (NEW) |\n",
    "| 5 | **Random-suffix** | `build_suffix_kv_cache(passage, random_text, sep)` | Structural control for suffix (NEW) |\n",
    "\n",
    "## Six Comparisons\n",
    "\n",
    "**Primary (3):**\n",
    "\n",
    "| # | Comparison | Question |\n",
    "|---|-----------|----------|\n",
    "| P1 | Oracle-suffix vs Random-suffix | Is there semantic signal in suffix? (cleanest test) |\n",
    "| P2 | Oracle-suffix vs Bare | Does suffix priming help at all? |\n",
    "| P3 | Oracle-truncated vs Bare | Does Exp 01 replicate? (expect d~+0.023, ns) |\n",
    "\n",
    "**Secondary (3):**\n",
    "\n",
    "| # | Comparison | Question |\n",
    "|---|-----------|----------|\n",
    "| S1 | Random-suffix vs Bare | Does ANY suffix help? (structural attention benefit) |\n",
    "| S2 | Oracle-suffix vs Oracle-truncated | Which mechanism is better? |\n",
    "| S3 | Random-truncated vs Bare | Does Exp 01 random benefit replicate? (expect d~+0.091) |\n",
    "\n",
    "**Multiple comparisons:** Bonferroni correction, alpha = 0.05/6 = 0.0083.\n",
    "\n",
    "## Critical Design Details\n",
    "\n",
    "1. **Matched tokenization for truncated conditions**: Identical to Exp 01\n",
    "2. **Suffix uses `build_suffix_kv_cache()`**: Tokenizes `passage + separator + suffix` as one string\n",
    "3. **Same random text** for both random-truncated and random-suffix per sample\n",
    "4. **Separator**: `\"\\n\\nRelated question: \"` — identical for oracle-suffix and random-suffix\n",
    "5. **Same dataset**: N=2500, SEED=42, identical samples as Exp 01\n",
    "6. **All standard safeguards**: `deepcopy_cache()`, `os.umask(0o000)`, checkpoints, GPU cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEED: 42\n",
      "Results directory: results/exp02\n",
      "CUDA available: True\n",
      "GPU: NVIDIA L4\n",
      "GPU memory: 23.6 GB\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Setup — permissions, seeds, output directory\n",
    "import os\n",
    "os.umask(0o000)  # Required: two-user environment (jupyter + CLI user)\n",
    "\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# Output directory\n",
    "RESULTS_DIR = Path(\"results/exp02\")\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Paths\n",
    "CHECKPOINT_PATH = RESULTS_DIR / \"checkpoint.json\"\n",
    "FINAL_RESULTS_PATH = RESULTS_DIR / \"results.json\"\n",
    "\n",
    "print(f\"SEED: {SEED}\")\n",
    "print(f\"Results directory: {RESULTS_DIR}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading mistralai/Mistral-7B-Instruct-v0.2 (4-bit)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23f98506c6ac4a30b1f3525b4a4c01e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/291 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded. dtype=torch.float16, device=cuda:0\n",
      "Vocab size: 32000\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Load model (Mistral-7B 4-bit) — identical to Exp 01\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "print(f\"Loading {MODEL_NAME} (4-bit)...\")\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "print(f\"Model loaded. dtype={model.dtype}, device={model.device}\")\n",
    "print(f\"Vocab size: {len(tokenizer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config:\n",
      "  num_samples: 2500\n",
      "  passage words: 50-300\n",
      "  surrogate_prefix_template: '{surrogate}\\n'\n",
      "  document_template: '{document}'\n",
      "  query_template: '\\nQuery: {query}\\nAnswer:'\n",
      "  answer_template: ' {answer}'\n",
      "  suffix_separator: '\\n\\nRelated question: '\n",
      "  checkpoint_every: 50\n",
      "  bonferroni_alpha: 0.0083\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Library imports + config + templates\n",
    "sys.path.insert(0, \".\")\n",
    "\n",
    "from lib.config import ExperimentConfig\n",
    "from lib.kv_cache import (\n",
    "    build_kv_cache,\n",
    "    build_suffix_kv_cache,\n",
    "    score_answer_with_cache,\n",
    "    deepcopy_cache,\n",
    "    extract_and_truncate_cache_with_bos,\n",
    "    correct_rope_positions_with_bos,\n",
    ")\n",
    "from lib.data import load_ms_marco, load_evaluation_samples\n",
    "from lib.analysis import cohens_d\n",
    "from scipy import stats\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "config = ExperimentConfig(\n",
    "    model_name=MODEL_NAME,\n",
    "    num_samples=2500,\n",
    "    seed=SEED,\n",
    ")\n",
    "\n",
    "# Templates: NO framing to avoid \\\"Document:\\n\\\" artifact\n",
    "SURROGATE_PREFIX_TEMPLATE = \"{surrogate}\\n\"\n",
    "DOCUMENT_TEMPLATE = \"{document}\"\n",
    "\n",
    "# Query/answer format\n",
    "QUERY_TEMPLATE = \"\\nQuery: {query}\\nAnswer:\"\n",
    "ANSWER_TEMPLATE = \" {answer}\"  # Leading space for correct BPE of first word\n",
    "\n",
    "# Suffix separator (used for both oracle-suffix and random-suffix)\n",
    "SUFFIX_SEPARATOR = \"\\n\\nRelated question: \"\n",
    "\n",
    "# Checkpoint frequency\n",
    "CHECKPOINT_EVERY = 50\n",
    "\n",
    "# Bonferroni-corrected alpha for 6 comparisons\n",
    "BONFERRONI_ALPHA = 0.05 / 6\n",
    "\n",
    "print(\"Config:\")\n",
    "print(f\"  num_samples: {config.num_samples}\")\n",
    "print(f\"  passage words: {config.min_passage_words}-{config.max_passage_words}\")\n",
    "print(f\"  surrogate_prefix_template: {repr(SURROGATE_PREFIX_TEMPLATE)}\")\n",
    "print(f\"  document_template: {repr(DOCUMENT_TEMPLATE)}\")\n",
    "print(f\"  query_template: {repr(QUERY_TEMPLATE)}\")\n",
    "print(f\"  answer_template: {repr(ANSWER_TEMPLATE)}\")\n",
    "print(f\"  suffix_separator: {repr(SUFFIX_SEPARATOR)}\")\n",
    "print(f\"  checkpoint_every: {CHECKPOINT_EVERY}\")\n",
    "print(f\"  bonferroni_alpha: {BONFERRONI_ALPHA:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`trust_remote_code` is not supported anymore.\n",
      "Please check that the Hugging Face dataset 'microsoft/ms_marco' isn't based on a loading script and remove `trust_remote_code`.\n",
      "If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading microsoft/ms_marco dataset...\n",
      "Dataset loaded: 10047 samples\n",
      "Filtering samples...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "886089324e0a45bbaed37b4335b30cf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filtering:   0%|          | 0/10047 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 2500 samples\n",
      "\n",
      "Loaded 2500 samples\n",
      "\n",
      "Example sample:\n",
      "  Query: what temperature should it be to plant grass seeds...\n",
      "  Passage: Usually planted in the early fall, cool-season grass seeds prefer daytime temperatures ranging from ...\n",
      "  Answer: Between 50deg and 65deg F...\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Load dataset (seed immediately before for determinism)\n",
    "dataset = load_ms_marco(config)\n",
    "\n",
    "# CRITICAL: Set seed immediately before load_evaluation_samples\n",
    "# to ensure deterministic sample selection regardless of prior random state\n",
    "np.random.seed(SEED)\n",
    "samples = load_evaluation_samples(dataset, config, require_answer=True)\n",
    "\n",
    "print(f\"\\nLoaded {len(samples)} samples\")\n",
    "print(f\"\\nExample sample:\")\n",
    "print(f\"  Query: {samples[0]['query'][:100]}...\")\n",
    "print(f\"  Passage: {samples[0]['passage'][:100]}...\")\n",
    "print(f\"  Answer: {samples[0]['answer'][:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oracle query: 'what temperature should it be to plant grass seeds'\n",
      "Oracle tokens: 9\n",
      "Random prefix: 'Restaur Mars didova少 DATA luxwalkshine'...\n",
      "Random tokens: 9\n",
      "Length match: True\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: generate_random_prefix_text() — copy from Exp 01\n",
    "\n",
    "def generate_random_prefix_text(target_text, tokenizer, seed):\n",
    "    \"\"\"\n",
    "    Generate random text from vocabulary tokens that is length-matched\n",
    "    (in tokens) to target_text.\n",
    "    \n",
    "    Uses a decode->re-encode verification loop to ensure the random\n",
    "    prefix tokenizes to approximately the expected number of tokens.\n",
    "    \n",
    "    Args:\n",
    "        target_text: Text to match in token length\n",
    "        tokenizer: The tokenizer\n",
    "        seed: Random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "        Random text string with same token count as target_text\n",
    "    \"\"\"\n",
    "    # Get target token count (no special tokens — we want content tokens only)\n",
    "    target_ids = tokenizer.encode(target_text, add_special_tokens=False)\n",
    "    target_len = len(target_ids)\n",
    "    \n",
    "    if target_len == 0:\n",
    "        return \"\"\n",
    "    \n",
    "    rng = np.random.RandomState(seed)\n",
    "    vocab_size = len(tokenizer)\n",
    "    \n",
    "    # Sample random token IDs from the full vocabulary\n",
    "    # Exclude special tokens (typically IDs 0-2 for BOS, EOS, UNK)\n",
    "    min_id = 3  # Skip BOS, EOS, UNK\n",
    "    random_ids = rng.randint(min_id, vocab_size, size=target_len)\n",
    "    \n",
    "    # Decode to text\n",
    "    random_text = tokenizer.decode(random_ids.tolist(), skip_special_tokens=True)\n",
    "    \n",
    "    # Verification: re-encode and check length\n",
    "    reencoded = tokenizer.encode(random_text, add_special_tokens=False)\n",
    "    \n",
    "    # If lengths don't match after round-trip, truncate or pad\n",
    "    if len(reencoded) != target_len:\n",
    "        # Truncate the text and re-decode from exactly target_len tokens\n",
    "        if len(reencoded) > target_len:\n",
    "            random_text = tokenizer.decode(reencoded[:target_len], skip_special_tokens=True)\n",
    "        else:\n",
    "            # Pad with more random tokens\n",
    "            extra_needed = target_len - len(reencoded)\n",
    "            extra_ids = rng.randint(min_id, vocab_size, size=extra_needed)\n",
    "            extra_text = tokenizer.decode(extra_ids.tolist(), skip_special_tokens=True)\n",
    "            random_text = random_text + extra_text\n",
    "            reencoded2 = tokenizer.encode(random_text, add_special_tokens=False)\n",
    "            if len(reencoded2) > target_len:\n",
    "                random_text = tokenizer.decode(reencoded2[:target_len], skip_special_tokens=True)\n",
    "    \n",
    "    return random_text\n",
    "\n",
    "\n",
    "# Test the function\n",
    "test_query = samples[0]['query']\n",
    "test_random = generate_random_prefix_text(test_query, tokenizer, seed=SEED)\n",
    "\n",
    "oracle_tokens = tokenizer.encode(test_query, add_special_tokens=False)\n",
    "random_tokens = tokenizer.encode(test_random, add_special_tokens=False)\n",
    "\n",
    "print(f\"Oracle query: {repr(test_query)}\")\n",
    "print(f\"Oracle tokens: {len(oracle_tokens)}\")\n",
    "print(f\"Random prefix: {repr(test_random[:80])}...\")\n",
    "print(f\"Random tokens: {len(random_tokens)}\")\n",
    "print(f\"Length match: {len(oracle_tokens) == len(random_tokens)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BPE Boundary Diagnostics\n",
      "============================================================\n",
      "\n",
      "(a) TRUNCATED: Independent vs concatenated tokenization\n",
      "  Tested 100 samples\n",
      "  BPE mismatches: 100/100 (100%)\n",
      "  → Matched tokenization required for truncated conditions.\n",
      "\n",
      "(b) SUFFIX: Passage tokens consistency across different suffixes\n",
      "  Tested 50 samples\n",
      "  Passage tokens consistent: 50/50 (100%)\n",
      "  → PASS: Passage tokens are identical regardless of suffix content.\n",
      "  → Suffix has NO effect on passage KV entries (causal masking guarantees this).\n",
      "  → Bare baseline is fair for suffix comparisons.\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: BPE diagnostics\n",
    "# (a) Truncated mismatch confirmation (same as Exp 01)\n",
    "# (b) Suffix passage token consistency check — verify passage tokens are identical\n",
    "#     regardless of suffix content\n",
    "\n",
    "print(\"BPE Boundary Diagnostics\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# --- (a) Truncated mismatch confirmation ---\n",
    "print(\"\\n(a) TRUNCATED: Independent vs concatenated tokenization\")\n",
    "n_mismatch = 0\n",
    "n_total = min(100, len(samples))\n",
    "\n",
    "for i in range(n_total):\n",
    "    passage = samples[i]['passage']\n",
    "    query = samples[i]['query']\n",
    "    oracle_prefix = SURROGATE_PREFIX_TEMPLATE.format(surrogate=query)\n",
    "    document_text = DOCUMENT_TEMPLATE.format(document=passage)\n",
    "    full_text = oracle_prefix + document_text\n",
    "    full_ids = tokenizer.encode(full_text, add_special_tokens=True)\n",
    "    prefix_ids = tokenizer.encode(oracle_prefix, add_special_tokens=True)\n",
    "    prefix_len = len(prefix_ids)\n",
    "    doc_from_concat = full_ids[prefix_len:]\n",
    "    doc_independent = tokenizer.encode(passage, add_special_tokens=True)[1:]  # strip BOS\n",
    "    if doc_from_concat != doc_independent:\n",
    "        n_mismatch += 1\n",
    "\n",
    "print(f\"  Tested {n_total} samples\")\n",
    "print(f\"  BPE mismatches: {n_mismatch}/{n_total} ({100*n_mismatch/n_total:.0f}%)\")\n",
    "print(f\"  → Matched tokenization required for truncated conditions.\")\n",
    "\n",
    "# --- (b) Suffix passage token consistency ---\n",
    "print(\"\\n(b) SUFFIX: Passage tokens consistency across different suffixes\")\n",
    "n_suffix_test = min(50, len(samples))\n",
    "n_passage_consistent = 0\n",
    "\n",
    "for i in range(n_suffix_test):\n",
    "    passage = samples[i]['passage']\n",
    "    query = samples[i]['query']\n",
    "    random_text = generate_random_prefix_text(query, tokenizer, seed=SEED + i)\n",
    "\n",
    "    # Tokenize passage alone (bare reference)\n",
    "    bare_ids = tokenizer.encode(passage, add_special_tokens=True)\n",
    "\n",
    "    # Tokenize passage + separator + oracle suffix\n",
    "    oracle_suffix_text = passage + SUFFIX_SEPARATOR + query\n",
    "    oracle_suffix_ids = tokenizer.encode(oracle_suffix_text, add_special_tokens=True)\n",
    "\n",
    "    # Tokenize passage + separator + random suffix\n",
    "    random_suffix_text = passage + SUFFIX_SEPARATOR + random_text\n",
    "    random_suffix_ids = tokenizer.encode(random_suffix_text, add_special_tokens=True)\n",
    "\n",
    "    # Check: do the first len(bare_ids) tokens match across all three?\n",
    "    bare_len = len(bare_ids)\n",
    "    oracle_passage_part = oracle_suffix_ids[:bare_len]\n",
    "    random_passage_part = random_suffix_ids[:bare_len]\n",
    "\n",
    "    if bare_ids == oracle_passage_part == random_passage_part:\n",
    "        n_passage_consistent += 1\n",
    "\n",
    "print(f\"  Tested {n_suffix_test} samples\")\n",
    "print(f\"  Passage tokens consistent: {n_passage_consistent}/{n_suffix_test} ({100*n_passage_consistent/n_suffix_test:.0f}%)\")\n",
    "if n_passage_consistent == n_suffix_test:\n",
    "    print(f\"  → PASS: Passage tokens are identical regardless of suffix content.\")\n",
    "    print(f\"  → Suffix has NO effect on passage KV entries (causal masking guarantees this).\")\n",
    "    print(f\"  → Bare baseline is fair for suffix comparisons.\")\n",
    "else:\n",
    "    n_diff = n_suffix_test - n_passage_consistent\n",
    "    print(f\"  → WARNING: {n_diff} samples have different passage tokens with different suffixes.\")\n",
    "    print(f\"  → This could indicate BPE boundary effects at the passage/separator boundary.\")\n",
    "    print(f\"  → Check if the separator creates clean BPE boundaries.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Bare baseline fairness diagnostic\n",
    "# Compare matched bare [BOS]+doc_ids vs independent build_kv_cache(passage) NLLs\n",
    "# to verify that using matched tokenization for bare doesn't create a systematic bias\n",
    "# relative to the suffix conditions (which use independent tokenization via build_suffix_kv_cache).\n",
    "#\n",
    "# Why this matters: Truncated conditions use matched tokenization (doc_ids from\n",
    "# concatenated encoding). Suffix conditions use build_suffix_kv_cache() which\n",
    "# tokenizes passage independently. If matched bare and independent bare give\n",
    "# systematically different NLLs, the bare baseline would be unfair to one mechanism.\n",
    "\n",
    "print(\"Bare Baseline Fairness Diagnostic\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Comparing matched bare (from truncated tokenization) vs\")\n",
    "print(\"independent bare (build_kv_cache(passage)) on 20 samples.\")\n",
    "print()\n",
    "\n",
    "n_diag = 20\n",
    "matched_nlls = []\n",
    "independent_nlls = []\n",
    "\n",
    "for i in range(n_diag):\n",
    "    sample = samples[i]\n",
    "    passage = sample['passage']\n",
    "    query = sample['query']\n",
    "    answer = sample['answer']\n",
    "    query_prompt = QUERY_TEMPLATE.format(query=query)\n",
    "    answer_text = ANSWER_TEMPLATE.format(answer=answer)\n",
    "\n",
    "    # --- Matched bare: [BOS] + doc_ids from oracle concatenation ---\n",
    "    oracle_prefix = SURROGATE_PREFIX_TEMPLATE.format(surrogate=query)\n",
    "    document_text = DOCUMENT_TEMPLATE.format(document=passage)\n",
    "    full_oracle_enc = tokenizer(oracle_prefix + document_text, return_tensors=\"pt\",\n",
    "                                add_special_tokens=True, padding=False, truncation=False)\n",
    "    full_oracle_ids = full_oracle_enc['input_ids'].to(config.device)\n",
    "    oracle_prefix_enc = tokenizer(oracle_prefix, return_tensors=\"pt\",\n",
    "                                  add_special_tokens=True, padding=False, truncation=False)\n",
    "    oracle_prefix_len = oracle_prefix_enc['input_ids'].shape[1]\n",
    "    bos_id = full_oracle_ids[:, :1]\n",
    "    doc_ids = full_oracle_ids[:, oracle_prefix_len:]\n",
    "    bare_ids = torch.cat([bos_id, doc_ids], dim=1)\n",
    "    bare_len = bare_ids.shape[1]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        bare_out = model(input_ids=bare_ids, attention_mask=torch.ones_like(bare_ids),\n",
    "                         use_cache=True, return_dict=True)\n",
    "    matched_nll = score_answer_with_cache(\n",
    "        deepcopy_cache(bare_out.past_key_values), bare_len,\n",
    "        query_prompt, answer_text, model, tokenizer, config)\n",
    "    matched_nlls.append(matched_nll)\n",
    "\n",
    "    # --- Independent bare: build_kv_cache(passage) ---\n",
    "    indep_len, indep_cache = build_kv_cache(passage, model, tokenizer, config)\n",
    "    independent_nll = score_answer_with_cache(\n",
    "        deepcopy_cache(indep_cache), indep_len,\n",
    "        query_prompt, answer_text, model, tokenizer, config)\n",
    "    independent_nlls.append(independent_nll)\n",
    "\n",
    "    del bare_out, indep_cache\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "matched_nlls = np.array(matched_nlls)\n",
    "independent_nlls = np.array(independent_nlls)\n",
    "diffs = matched_nlls - independent_nlls\n",
    "\n",
    "print(f\"{'Sample':>8} {'Matched':>10} {'Independent':>12} {'Diff':>10}\")\n",
    "print(\"-\" * 45)\n",
    "for i in range(n_diag):\n",
    "    print(f\"{i:>8} {matched_nlls[i]:>10.4f} {independent_nlls[i]:>12.4f} {diffs[i]:>10.6f}\")\n",
    "\n",
    "# Filter out zero-NLL samples (single-token answers are uninformative)\n",
    "nonzero_mask = (matched_nlls != 0.0) & (independent_nlls != 0.0)\n",
    "diffs_nonzero = diffs[nonzero_mask]\n",
    "n_nonzero = np.sum(nonzero_mask)\n",
    "\n",
    "print(f\"\\nNon-zero NLL samples: {n_nonzero}/{n_diag}\")\n",
    "print(f\"Mean difference: {np.mean(diffs_nonzero):.6f}\")\n",
    "print(f\"Mean abs difference: {np.mean(np.abs(diffs_nonzero)):.6f}\")\n",
    "print(f\"Max abs difference: {np.max(np.abs(diffs_nonzero)):.6f}\")\n",
    "\n",
    "# Context: Exp 01 effect sizes were 0.009-0.029 in mean delta.\n",
    "# A systematic bias of ~0.005 would be small relative to these effects.\n",
    "mean_bias = np.mean(diffs_nonzero)\n",
    "mean_abs_diff = np.mean(np.abs(diffs_nonzero))\n",
    "\n",
    "if abs(mean_bias) < 0.01:\n",
    "    print(f\"\\nPASS: Mean systematic bias ({mean_bias:+.4f}) is negligible.\")\n",
    "    print(f\"Individual samples vary (max abs {np.max(np.abs(diffs_nonzero)):.3f}) due to\")\n",
    "    print(f\"BPE boundary effects on the first passage token, but this is symmetric noise,\")\n",
    "    print(f\"not a directional bias. Bare baseline is fair for both mechanisms.\")\n",
    "else:\n",
    "    print(f\"\\nCAUTION: Mean systematic bias ({mean_bias:+.4f}) detected.\")\n",
    "    print(f\"This is {'small' if abs(mean_bias) < 0.03 else 'non-trivial'} relative to\")\n",
    "    print(f\"expected effect sizes (Exp 01: d~0.02-0.09, Δ~0.009-0.029).\")\n",
    "    print(f\"Suffix comparisons (P1, P2, S1) are unaffected (both suffix conditions\")\n",
    "    print(f\"use independent tokenization). Cross-mechanism comparisons (S2) may\")\n",
    "    print(f\"have a small bias of ~{abs(mean_bias):.3f} in mean delta.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "EXPERIMENTAL CONDITIONS EXPLAINED\n",
      "======================================================================\n",
      "\n",
      "Example passage: 'Usually planted in the early fall, cool-season grass seeds prefer daytime temper...'\n",
      "Example query:   'what temperature should it be to plant grass seeds'\n",
      "Example answer:  'Between 50deg and 65deg F...'\n",
      "Example random:  'Restaur Mars didova少 DATA luxwalkshine'...\n",
      "\n",
      "──────────────────────────────────────────────────────────────────────\n",
      "MATCHED TOKENIZATION (for truncated conditions)\n",
      "  Tokenize oracle_prefix + passage together → 134 tokens\n",
      "  Oracle prefix tokens (with BOS): 11\n",
      "  Document tokens (shared by truncated conditions + bare): 123\n",
      "\n",
      "──────────────────────────────────────────────────────────────────────\n",
      "### CONDITION 1: BARE (baseline) ###\n",
      "  Input IDs:  [BOS] + doc_ids (124 tokens)\n",
      "  Key insight: Pure baseline. Same doc tokens as truncated conditions.\n",
      "  Also fair for suffix comparisons (Cell 7 diagnostic verifies this).\n",
      "\n",
      "──────────────────────────────────────────────────────────────────────\n",
      "### CONDITION 2: ORACLE-TRUNCATED (Exp 01 replication) ###\n",
      "  Build:  [BOS]['what temperature should it be to plant grass seeds\\n'][doc_ids] (134 tokens)\n",
      "  After:  Truncate prefix → [BOS] + doc_ids (124 tokens) + RoPE correct\n",
      "  Key insight: Value contamination from semantically relevant prefix.\n",
      "\n",
      "──────────────────────────────────────────────────────────────────────\n",
      "### CONDITION 3: RANDOM-TRUNCATED (Exp 01 replication) ###\n",
      "  Build:  [BOS]['Restaur Mars didova少 DATA luxw...\\n'][doc_ids]\n",
      "  After:  Truncate prefix → [BOS] + doc_ids (124 tokens) + RoPE correct\n",
      "  Key insight: Structural control — value contamination from random prefix.\n",
      "\n",
      "──────────────────────────────────────────────────────────────────────\n",
      "### CONDITION 4: ORACLE-SUFFIX (NEW — cleanest semantic test) ###\n",
      "  Build:  [BOS][passage]['\n",
      "\n",
      "Related question: ']['what temperature should it be to plant grass seeds'] (138 tokens)\n",
      "  Scoring: Query attends to passage + separator + suffix KV entries\n",
      "  Key insight: Passage KV entries are BYTE-IDENTICAL to bare (causal masking).\n",
      "  Any benefit must come from query → suffix attention.\n",
      "\n",
      "──────────────────────────────────────────────────────────────────────\n",
      "### CONDITION 5: RANDOM-SUFFIX (structural control for suffix) ###\n",
      "  Build:  [BOS][passage]['\n",
      "\n",
      "Related question: ']['Restaur Mars didova少 DATA luxw...'] (138 tokens)\n",
      "  Scoring: Query attends to passage + separator + suffix KV entries\n",
      "  Key insight: Same structure as oracle-suffix but random content.\n",
      "  P1 (oracle-suffix vs random-suffix) isolates semantic signal.\n",
      "\n",
      "──────────────────────────────────────────────────────────────────────\n",
      "DESIGN NOTES:\n",
      "  Same random text used for BOTH random-truncated and random-suffix per sample.\n",
      "  Suffix separator: '\\n\\nRelated question: ' (identical for oracle/random suffix).\n",
      "  CACHE SAFETY: deepcopy_cache() before every score call.\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Condition explanation printout (all 5 conditions with concrete examples)\n",
    "print(\"=\" * 70)\n",
    "print(\"EXPERIMENTAL CONDITIONS EXPLAINED\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "ex = samples[0]\n",
    "ex_query = ex['query']\n",
    "ex_passage = ex['passage'][:80] + \"...\"\n",
    "ex_answer = ex['answer'][:60] + \"...\"\n",
    "ex_random = generate_random_prefix_text(ex_query, tokenizer, seed=SEED)\n",
    "\n",
    "print(f\"\\nExample passage: {repr(ex_passage)}\")\n",
    "print(f\"Example query:   {repr(ex_query)}\")\n",
    "print(f\"Example answer:  {repr(ex_answer)}\")\n",
    "print(f\"Example random:  {repr(ex_random[:60])}...\")\n",
    "\n",
    "# Show matched tokenization\n",
    "oracle_prefix = SURROGATE_PREFIX_TEMPLATE.format(surrogate=ex_query)\n",
    "doc_text = DOCUMENT_TEMPLATE.format(document=ex['passage'])\n",
    "full_text = oracle_prefix + doc_text\n",
    "full_ids = tokenizer.encode(full_text, add_special_tokens=True)\n",
    "prefix_ids = tokenizer.encode(oracle_prefix, add_special_tokens=True)\n",
    "prefix_len = len(prefix_ids)\n",
    "doc_ids = full_ids[prefix_len:]\n",
    "\n",
    "print(f\"\\n{'─' * 70}\")\n",
    "print(\"MATCHED TOKENIZATION (for truncated conditions)\")\n",
    "print(f\"  Tokenize oracle_prefix + passage together → {len(full_ids)} tokens\")\n",
    "print(f\"  Oracle prefix tokens (with BOS): {prefix_len}\")\n",
    "print(f\"  Document tokens (shared by truncated conditions + bare): {len(doc_ids)}\")\n",
    "\n",
    "print(f\"\\n{'─' * 70}\")\n",
    "print(\"### CONDITION 1: BARE (baseline) ###\")\n",
    "print(f\"  Input IDs:  [BOS] + doc_ids ({1 + len(doc_ids)} tokens)\")\n",
    "print(f\"  Key insight: Pure baseline. Same doc tokens as truncated conditions.\")\n",
    "print(f\"  Also fair for suffix comparisons (Cell 7 diagnostic verifies this).\")\n",
    "\n",
    "print(f\"\\n{'─' * 70}\")\n",
    "print(\"### CONDITION 2: ORACLE-TRUNCATED (Exp 01 replication) ###\")\n",
    "print(f\"  Build:  [BOS]['{ex_query}\\\\n'][doc_ids] ({len(full_ids)} tokens)\")\n",
    "print(f\"  After:  Truncate prefix → [BOS] + doc_ids ({1 + len(doc_ids)} tokens) + RoPE correct\")\n",
    "print(f\"  Key insight: Value contamination from semantically relevant prefix.\")\n",
    "\n",
    "print(f\"\\n{'─' * 70}\")\n",
    "print(\"### CONDITION 3: RANDOM-TRUNCATED (Exp 01 replication) ###\")\n",
    "print(f\"  Build:  [BOS]['{ex_random[:30]}...\\\\n'][doc_ids]\")\n",
    "print(f\"  After:  Truncate prefix → [BOS] + doc_ids ({1 + len(doc_ids)} tokens) + RoPE correct\")\n",
    "print(f\"  Key insight: Structural control — value contamination from random prefix.\")\n",
    "\n",
    "# Show suffix examples\n",
    "suffix_oracle_text = ex['passage'] + SUFFIX_SEPARATOR + ex_query\n",
    "suffix_oracle_ids = tokenizer.encode(suffix_oracle_text, add_special_tokens=True)\n",
    "suffix_random_text = ex['passage'] + SUFFIX_SEPARATOR + ex_random\n",
    "suffix_random_ids = tokenizer.encode(suffix_random_text, add_special_tokens=True)\n",
    "\n",
    "print(f\"\\n{'─' * 70}\")\n",
    "print(\"### CONDITION 4: ORACLE-SUFFIX (NEW — cleanest semantic test) ###\")\n",
    "print(f\"  Build:  [BOS][passage]['{SUFFIX_SEPARATOR}']['{ex_query}'] ({len(suffix_oracle_ids)} tokens)\")\n",
    "print(f\"  Scoring: Query attends to passage + separator + suffix KV entries\")\n",
    "print(f\"  Key insight: Passage KV entries are BYTE-IDENTICAL to bare (causal masking).\")\n",
    "print(f\"  Any benefit must come from query → suffix attention.\")\n",
    "\n",
    "print(f\"\\n{'─' * 70}\")\n",
    "print(\"### CONDITION 5: RANDOM-SUFFIX (structural control for suffix) ###\")\n",
    "print(f\"  Build:  [BOS][passage]['{SUFFIX_SEPARATOR}']['{ex_random[:30]}...'] ({len(suffix_random_ids)} tokens)\")\n",
    "print(f\"  Scoring: Query attends to passage + separator + suffix KV entries\")\n",
    "print(f\"  Key insight: Same structure as oracle-suffix but random content.\")\n",
    "print(f\"  P1 (oracle-suffix vs random-suffix) isolates semantic signal.\")\n",
    "\n",
    "print(f\"\\n{'─' * 70}\")\n",
    "print(\"DESIGN NOTES:\")\n",
    "print(f\"  Same random text used for BOTH random-truncated and random-suffix per sample.\")\n",
    "print(f\"  Suffix separator: {repr(SUFFIX_SEPARATOR)} (identical for oracle/random suffix).\")\n",
    "print(f\"  CACHE SAFETY: deepcopy_cache() before every score call.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 2500\n",
      "No checkpoint found. Starting from scratch.\n",
      "Will evaluate samples 0 to 2499\n",
      "\n",
      "Conditions per sample: 5\n",
      "Total condition evaluations: 12500\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Evaluation parameters + checkpoint loading\n",
    "\n",
    "N = len(samples)\n",
    "print(f\"Total samples: {N}\")\n",
    "\n",
    "# Check for existing checkpoint\n",
    "results = []\n",
    "start_idx = 0\n",
    "\n",
    "if CHECKPOINT_PATH.exists():\n",
    "    with open(CHECKPOINT_PATH, 'r') as f:\n",
    "        checkpoint = json.load(f)\n",
    "\n",
    "    # Verify checkpoint matches current sample list (resume correctness)\n",
    "    ckpt_sample_ids = checkpoint.get('sample_queries', [])\n",
    "    current_sample_ids = [s['query'] for s in samples]\n",
    "\n",
    "    if ckpt_sample_ids == current_sample_ids:\n",
    "        results = checkpoint['results']\n",
    "        start_idx = len(results)\n",
    "        print(f\"Resuming from checkpoint: {start_idx}/{N} samples completed\")\n",
    "    else:\n",
    "        print(\"WARNING: Checkpoint sample list doesn't match current samples.\")\n",
    "        print(\"Starting from scratch.\")\n",
    "        results = []\n",
    "        start_idx = 0\n",
    "else:\n",
    "    print(\"No checkpoint found. Starting from scratch.\")\n",
    "\n",
    "print(f\"Will evaluate samples {start_idx} to {N-1}\")\n",
    "print(f\"\\nConditions per sample: 5\")\n",
    "print(f\"Total condition evaluations: {(N - start_idx) * 5}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d47ebea4643545a59873ad7b3d2ad740",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/2500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint at 50/2500 | Rate: 0.5 samples/s | ETA: 86.6 min\n",
      "  Checkpoint at 100/2500 | Rate: 0.5 samples/s | ETA: 86.0 min\n",
      "  Checkpoint at 150/2500 | Rate: 0.5 samples/s | ETA: 84.9 min\n",
      "  Checkpoint at 200/2500 | Rate: 0.5 samples/s | ETA: 83.4 min\n",
      "  Checkpoint at 250/2500 | Rate: 0.5 samples/s | ETA: 81.9 min\n",
      "  Checkpoint at 300/2500 | Rate: 0.5 samples/s | ETA: 80.3 min\n",
      "  Checkpoint at 350/2500 | Rate: 0.5 samples/s | ETA: 78.5 min\n",
      "  Checkpoint at 400/2500 | Rate: 0.5 samples/s | ETA: 76.6 min\n",
      "  Checkpoint at 450/2500 | Rate: 0.5 samples/s | ETA: 74.9 min\n",
      "  Checkpoint at 500/2500 | Rate: 0.5 samples/s | ETA: 73.1 min\n",
      "  Checkpoint at 550/2500 | Rate: 0.5 samples/s | ETA: 71.3 min\n",
      "  Checkpoint at 600/2500 | Rate: 0.5 samples/s | ETA: 69.5 min\n",
      "  Checkpoint at 650/2500 | Rate: 0.5 samples/s | ETA: 67.7 min\n",
      "  Checkpoint at 700/2500 | Rate: 0.5 samples/s | ETA: 65.8 min\n",
      "  Checkpoint at 750/2500 | Rate: 0.5 samples/s | ETA: 64.0 min\n",
      "  Checkpoint at 800/2500 | Rate: 0.5 samples/s | ETA: 62.2 min\n",
      "  Checkpoint at 850/2500 | Rate: 0.5 samples/s | ETA: 60.4 min\n",
      "  Checkpoint at 900/2500 | Rate: 0.5 samples/s | ETA: 58.5 min\n",
      "  Checkpoint at 950/2500 | Rate: 0.5 samples/s | ETA: 56.7 min\n",
      "  Checkpoint at 1000/2500 | Rate: 0.5 samples/s | ETA: 54.9 min\n",
      "  Checkpoint at 1050/2500 | Rate: 0.5 samples/s | ETA: 53.0 min\n",
      "  Checkpoint at 1100/2500 | Rate: 0.5 samples/s | ETA: 51.2 min\n",
      "  Checkpoint at 1150/2500 | Rate: 0.5 samples/s | ETA: 49.4 min\n",
      "  Checkpoint at 1200/2500 | Rate: 0.5 samples/s | ETA: 47.5 min\n",
      "  Checkpoint at 1250/2500 | Rate: 0.5 samples/s | ETA: 45.7 min\n",
      "  Checkpoint at 1300/2500 | Rate: 0.5 samples/s | ETA: 43.9 min\n",
      "  Checkpoint at 1350/2500 | Rate: 0.5 samples/s | ETA: 42.1 min\n",
      "  Checkpoint at 1400/2500 | Rate: 0.5 samples/s | ETA: 40.3 min\n",
      "  Checkpoint at 1450/2500 | Rate: 0.5 samples/s | ETA: 38.4 min\n",
      "  Checkpoint at 1500/2500 | Rate: 0.5 samples/s | ETA: 36.6 min\n",
      "  Checkpoint at 1550/2500 | Rate: 0.5 samples/s | ETA: 34.8 min\n",
      "  Checkpoint at 1600/2500 | Rate: 0.5 samples/s | ETA: 32.9 min\n",
      "  Checkpoint at 1650/2500 | Rate: 0.5 samples/s | ETA: 31.1 min\n",
      "  Checkpoint at 1700/2500 | Rate: 0.5 samples/s | ETA: 29.3 min\n",
      "  Checkpoint at 1750/2500 | Rate: 0.5 samples/s | ETA: 27.4 min\n",
      "  Checkpoint at 1800/2500 | Rate: 0.5 samples/s | ETA: 25.6 min\n",
      "  Checkpoint at 1850/2500 | Rate: 0.5 samples/s | ETA: 23.8 min\n",
      "  Checkpoint at 1900/2500 | Rate: 0.5 samples/s | ETA: 22.0 min\n",
      "  Checkpoint at 1950/2500 | Rate: 0.5 samples/s | ETA: 20.1 min\n",
      "  Checkpoint at 2000/2500 | Rate: 0.5 samples/s | ETA: 18.3 min\n",
      "  Checkpoint at 2050/2500 | Rate: 0.5 samples/s | ETA: 16.5 min\n",
      "  Checkpoint at 2100/2500 | Rate: 0.5 samples/s | ETA: 14.6 min\n",
      "  Checkpoint at 2150/2500 | Rate: 0.5 samples/s | ETA: 12.8 min\n",
      "  Checkpoint at 2200/2500 | Rate: 0.5 samples/s | ETA: 11.0 min\n",
      "  Checkpoint at 2250/2500 | Rate: 0.5 samples/s | ETA: 9.1 min\n",
      "  Checkpoint at 2300/2500 | Rate: 0.5 samples/s | ETA: 7.3 min\n",
      "  Checkpoint at 2350/2500 | Rate: 0.5 samples/s | ETA: 5.5 min\n",
      "  Checkpoint at 2400/2500 | Rate: 0.5 samples/s | ETA: 3.7 min\n",
      "  Checkpoint at 2450/2500 | Rate: 0.5 samples/s | ETA: 1.8 min\n",
      "  Checkpoint at 2500/2500 | Rate: 0.5 samples/s | ETA: 0.0 min\n",
      "\n",
      "Evaluation complete: 5000 samples in 91.4 minutes\n"
     ]
    }
   ],
   "source": [
    "# Cell 10: Main evaluation loop (5 conditions × N samples, with checkpointing)\n",
    "#\n",
    "# Conditions 1-3: Matched tokenization (identical to Exp 01)\n",
    "# Conditions 4-5: Suffix via build_suffix_kv_cache()\n",
    "#\n",
    "# Same random text used for both random-truncated (cond 3) and random-suffix (cond 5).\n",
    "\n",
    "t_start = time.time()\n",
    "\n",
    "for idx in tqdm(range(start_idx, N), initial=start_idx, total=N, desc=\"Evaluating\"):\n",
    "    sample = samples[idx]\n",
    "    passage = sample['passage']\n",
    "    query = sample['query']\n",
    "    answer = sample['answer']\n",
    "\n",
    "    query_prompt = QUERY_TEMPLATE.format(query=query)\n",
    "    answer_text = ANSWER_TEMPLATE.format(answer=answer)\n",
    "\n",
    "    # === Step 1: Determine canonical document token IDs (for truncated conditions) ===\n",
    "    oracle_prefix = SURROGATE_PREFIX_TEMPLATE.format(surrogate=query)\n",
    "    document_text = DOCUMENT_TEMPLATE.format(document=passage)\n",
    "    full_oracle_text = oracle_prefix + document_text\n",
    "\n",
    "    full_oracle_enc = tokenizer(full_oracle_text, return_tensors=\"pt\",\n",
    "                                add_special_tokens=True, padding=False, truncation=False)\n",
    "    full_oracle_ids = full_oracle_enc['input_ids'].to(config.device)\n",
    "\n",
    "    oracle_prefix_enc = tokenizer(oracle_prefix, return_tensors=\"pt\",\n",
    "                                  add_special_tokens=True, padding=False, truncation=False)\n",
    "    oracle_prefix_len = oracle_prefix_enc['input_ids'].shape[1]  # includes BOS\n",
    "\n",
    "    bos_id = full_oracle_ids[:, :1]\n",
    "    doc_ids = full_oracle_ids[:, oracle_prefix_len:]\n",
    "    doc_len = doc_ids.shape[1]\n",
    "\n",
    "    # Generate random text ONCE — used for BOTH random-truncated and random-suffix\n",
    "    random_text = generate_random_prefix_text(query, tokenizer, seed=SEED + idx)\n",
    "\n",
    "    # === Condition 1: BARE — [BOS] + doc_ids ===\n",
    "    bare_ids = torch.cat([bos_id, doc_ids], dim=1)\n",
    "    bare_len = bare_ids.shape[1]  # = 1 + doc_len\n",
    "    with torch.no_grad():\n",
    "        bare_out = model(input_ids=bare_ids, attention_mask=torch.ones_like(bare_ids),\n",
    "                         use_cache=True, return_dict=True)\n",
    "    bare_cache = bare_out.past_key_values\n",
    "    bare_nll = score_answer_with_cache(\n",
    "        deepcopy_cache(bare_cache), bare_len, query_prompt, answer_text,\n",
    "        model, tokenizer, config\n",
    "    )\n",
    "\n",
    "    # === Condition 2: ORACLE-TRUNCATED — [BOS][oracle_prefix][doc_ids] → truncate ===\n",
    "    with torch.no_grad():\n",
    "        oracle_trunc_out = model(input_ids=full_oracle_ids,\n",
    "                                 attention_mask=torch.ones_like(full_oracle_ids),\n",
    "                                 use_cache=True, return_dict=True)\n",
    "    oracle_trunc_cache = extract_and_truncate_cache_with_bos(oracle_trunc_out.past_key_values, doc_len)\n",
    "    oracle_trunc_len = 1 + doc_len\n",
    "    correct_rope_positions_with_bos(oracle_trunc_cache, oracle_prefix_len - 1, model)\n",
    "    oracle_trunc_nll = score_answer_with_cache(\n",
    "        deepcopy_cache(oracle_trunc_cache), oracle_trunc_len, query_prompt, answer_text,\n",
    "        model, tokenizer, config\n",
    "    )\n",
    "\n",
    "    # === Condition 3: RANDOM-TRUNCATED — [BOS][random_prefix][doc_ids] → truncate ===\n",
    "    random_prefix = SURROGATE_PREFIX_TEMPLATE.format(surrogate=random_text)\n",
    "    random_prefix_enc = tokenizer(random_prefix, return_tensors=\"pt\",\n",
    "                                  add_special_tokens=False, padding=False, truncation=False)\n",
    "    random_prefix_ids = random_prefix_enc['input_ids'].to(config.device)\n",
    "    random_full_ids = torch.cat([bos_id, random_prefix_ids, doc_ids], dim=1)\n",
    "    random_prefix_len = 1 + random_prefix_ids.shape[1]  # BOS + prefix tokens\n",
    "\n",
    "    with torch.no_grad():\n",
    "        random_trunc_out = model(input_ids=random_full_ids,\n",
    "                                 attention_mask=torch.ones_like(random_full_ids),\n",
    "                                 use_cache=True, return_dict=True)\n",
    "    random_trunc_cache = extract_and_truncate_cache_with_bos(random_trunc_out.past_key_values, doc_len)\n",
    "    random_trunc_len = 1 + doc_len\n",
    "    correct_rope_positions_with_bos(random_trunc_cache, random_prefix_len - 1, model)\n",
    "    random_trunc_nll = score_answer_with_cache(\n",
    "        deepcopy_cache(random_trunc_cache), random_trunc_len, query_prompt, answer_text,\n",
    "        model, tokenizer, config\n",
    "    )\n",
    "\n",
    "    # === Condition 4: ORACLE-SUFFIX — build_suffix_kv_cache(passage, query) ===\n",
    "    oracle_sfx_len, oracle_sfx_cache = build_suffix_kv_cache(\n",
    "        passage, query, model, tokenizer, config, separator=SUFFIX_SEPARATOR)\n",
    "    oracle_suffix_nll = score_answer_with_cache(\n",
    "        deepcopy_cache(oracle_sfx_cache), oracle_sfx_len, query_prompt, answer_text,\n",
    "        model, tokenizer, config\n",
    "    )\n",
    "\n",
    "    # === Condition 5: RANDOM-SUFFIX — build_suffix_kv_cache(passage, random_text) ===\n",
    "    random_sfx_len, random_sfx_cache = build_suffix_kv_cache(\n",
    "        passage, random_text, model, tokenizer, config, separator=SUFFIX_SEPARATOR)\n",
    "    random_suffix_nll = score_answer_with_cache(\n",
    "        deepcopy_cache(random_sfx_cache), random_sfx_len, query_prompt, answer_text,\n",
    "        model, tokenizer, config\n",
    "    )\n",
    "\n",
    "    # Record all 5 NLLs + cache lengths + precomputed deltas\n",
    "    result = {\n",
    "        'idx': idx,\n",
    "        'bare_nll': bare_nll,\n",
    "        'oracle_trunc_nll': oracle_trunc_nll,\n",
    "        'random_trunc_nll': random_trunc_nll,\n",
    "        'oracle_suffix_nll': oracle_suffix_nll,\n",
    "        'random_suffix_nll': random_suffix_nll,\n",
    "        'bare_len': bare_len,\n",
    "        'oracle_trunc_len': oracle_trunc_len,\n",
    "        'random_trunc_len': random_trunc_len,\n",
    "        'oracle_suffix_len': oracle_sfx_len,\n",
    "        'random_suffix_len': random_sfx_len,\n",
    "        'doc_len': doc_len,\n",
    "        # Precomputed deltas (positive = first condition has LOWER NLL = better)\n",
    "        # P1: oracle-suffix vs random-suffix\n",
    "        'delta_p1_oracle_sfx_vs_random_sfx': random_suffix_nll - oracle_suffix_nll,\n",
    "        # P2: oracle-suffix vs bare\n",
    "        'delta_p2_oracle_sfx_vs_bare': bare_nll - oracle_suffix_nll,\n",
    "        # P3: oracle-truncated vs bare\n",
    "        'delta_p3_oracle_trunc_vs_bare': bare_nll - oracle_trunc_nll,\n",
    "        # S1: random-suffix vs bare\n",
    "        'delta_s1_random_sfx_vs_bare': bare_nll - random_suffix_nll,\n",
    "        # S2: oracle-suffix vs oracle-truncated\n",
    "        'delta_s2_oracle_sfx_vs_oracle_trunc': oracle_trunc_nll - oracle_suffix_nll,\n",
    "        # S3: random-truncated vs bare\n",
    "        'delta_s3_random_trunc_vs_bare': bare_nll - random_trunc_nll,\n",
    "    }\n",
    "    results.append(result)\n",
    "\n",
    "    # GPU memory management\n",
    "    del bare_cache, bare_out, oracle_trunc_cache, oracle_trunc_out\n",
    "    del random_trunc_cache, random_trunc_out, oracle_sfx_cache, random_sfx_cache\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # Checkpoint\n",
    "    if (idx + 1) % CHECKPOINT_EVERY == 0 or idx == N - 1:\n",
    "        checkpoint_data = {\n",
    "            'results': results,\n",
    "            'sample_queries': [s['query'] for s in samples],\n",
    "            'completed': len(results),\n",
    "            'total': N,\n",
    "            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        }\n",
    "        with open(CHECKPOINT_PATH, 'w') as f:\n",
    "            json.dump(checkpoint_data, f)\n",
    "\n",
    "        elapsed = time.time() - t_start\n",
    "        samples_done = idx - start_idx + 1\n",
    "        rate = samples_done / elapsed if elapsed > 0 else 0\n",
    "        remaining = (N - idx - 1) / rate if rate > 0 else 0\n",
    "        tqdm.write(f\"  Checkpoint at {idx+1}/{N} | Rate: {rate:.1f} samples/s | ETA: {remaining/60:.1f} min\")\n",
    "\n",
    "elapsed_total = time.time() - t_start\n",
    "print(f\"\\nEvaluation complete: {len(results)} samples in {elapsed_total/60:.1f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache Length Diagnostics\n",
      "============================================================\n",
      "\n",
      "(a) TRUNCATED: All cache lengths match bare?\n",
      "  All truncated cache lengths match (bare == oracle-trunc == random-trunc == 1+doc): True\n",
      "  PASS: Matched tokenization guarantees identical cache lengths.\n",
      "\n",
      "(b) SUFFIX: Cache length distribution\n",
      "  Oracle-suffix extra tokens (vs bare):  mean=13.8, min=7, max=28\n",
      "  Random-suffix extra tokens (vs bare):  mean=13.8, min=7, max=28\n",
      "  Oracle-suffix total length:  mean=140.4, min=68, max=295\n",
      "  Random-suffix total length:  mean=140.4, min=68, max=295\n"
     ]
    }
   ],
   "source": [
    "# Cell 11: Cache length diagnostics\n",
    "print(\"Cache Length Diagnostics\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# --- Truncated match check ---\n",
    "print(\"\\n(a) TRUNCATED: All cache lengths match bare?\")\n",
    "bare_lens = [r['bare_len'] for r in results]\n",
    "oracle_trunc_lens = [r['oracle_trunc_len'] for r in results]\n",
    "random_trunc_lens = [r['random_trunc_len'] for r in results]\n",
    "doc_lens = [r['doc_len'] for r in results]\n",
    "\n",
    "all_trunc_match = all(\n",
    "    b == ot == rt == 1 + d\n",
    "    for b, ot, rt, d in zip(bare_lens, oracle_trunc_lens, random_trunc_lens, doc_lens)\n",
    ")\n",
    "print(f\"  All truncated cache lengths match (bare == oracle-trunc == random-trunc == 1+doc): {all_trunc_match}\")\n",
    "if all_trunc_match:\n",
    "    print(\"  PASS: Matched tokenization guarantees identical cache lengths.\")\n",
    "else:\n",
    "    mismatches = sum(1 for b, ot, rt in zip(bare_lens, oracle_trunc_lens, random_trunc_lens) if not (b == ot == rt))\n",
    "    print(f\"  WARNING: {mismatches} samples have mismatched truncated lengths!\")\n",
    "\n",
    "# --- Suffix length distribution ---\n",
    "print(\"\\n(b) SUFFIX: Cache length distribution\")\n",
    "oracle_sfx_lens = np.array([r['oracle_suffix_len'] for r in results])\n",
    "random_sfx_lens = np.array([r['random_suffix_len'] for r in results])\n",
    "bare_lens_arr = np.array(bare_lens)\n",
    "\n",
    "oracle_sfx_extra = oracle_sfx_lens - bare_lens_arr\n",
    "random_sfx_extra = random_sfx_lens - bare_lens_arr\n",
    "\n",
    "print(f\"  Oracle-suffix extra tokens (vs bare):  mean={np.mean(oracle_sfx_extra):.1f}, min={np.min(oracle_sfx_extra)}, max={np.max(oracle_sfx_extra)}\")\n",
    "print(f\"  Random-suffix extra tokens (vs bare):  mean={np.mean(random_sfx_extra):.1f}, min={np.min(random_sfx_extra)}, max={np.max(random_sfx_extra)}\")\n",
    "print(f\"  Oracle-suffix total length:  mean={np.mean(oracle_sfx_lens):.1f}, min={np.min(oracle_sfx_lens)}, max={np.max(oracle_sfx_lens)}\")\n",
    "print(f\"  Random-suffix total length:  mean={np.mean(random_sfx_lens):.1f}, min={np.min(random_sfx_lens)}, max={np.max(random_sfx_lens)}\")\n",
    "\n",
    "# Note: suffix length = passage tokens + separator tokens + suffix tokens\n",
    "# This won't exactly match bare_len because bare uses matched tokenization\n",
    "# while suffix uses independent tokenization via build_suffix_kv_cache.\n",
    "# Cell 7 verified this doesn't cause meaningful NLL differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PRIMARY ANALYSIS — 6 COMPARISONS WITH BONFERRONI CORRECTION\n",
      "======================================================================\n",
      "Total samples: 5000\n",
      "Excluded (zero NLL from single-token answers): 394\n",
      "Valid samples for analysis: 4606\n",
      "Bonferroni-corrected alpha: 0.0083 (0.05 / 6)\n",
      "\n",
      "Condition              Mean NLL        Std     Median\n",
      "-------------------------------------------------------\n",
      "Bare                     1.1455     1.5698     0.6206\n",
      "Oracle-truncated         1.1369     1.5553     0.6270\n",
      "Random-truncated         1.1171     1.5405     0.5923\n",
      "Oracle-suffix            1.1168     1.4874     0.6098\n",
      "Random-suffix            1.0513     1.4846     0.5464\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "PAIRED COMPARISONS (positive delta = first condition has LOWER NLL = better)\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "\n",
      "Comparison                            Mean Δ        d    Win%        t            p   Sig\n",
      "----------------------------------------------------------------------------------------\n",
      "P1: Oracle-sfx vs Random-sfx         -0.0655   -0.192   37.6%   -13.05    2.99e-38   ***\n",
      "P2: Oracle-sfx vs Bare                0.0287    0.071   52.9%     4.85    1.29e-06   ***\n",
      "P3: Oracle-trunc vs Bare              0.0086    0.023   50.0%     1.58    1.13e-01    ns\n",
      "S1: Random-sfx vs Bare                0.0942    0.264   65.2%    17.95    1.14e-69   ***\n",
      "S2: Oracle-sfx vs Oracle-trunc        0.0201    0.048   52.8%     3.27    1.09e-03    **\n",
      "S3: Random-trunc vs Bare              0.0285    0.091   59.5%     6.18    7.05e-10   ***\n",
      "\n",
      "Significance levels: *** p<0.001, ** p<0.0083 (Bonferroni), * p<0.05, ns = not significant\n",
      "Cohen's d interpretation: |d|<0.2 = negligible, 0.2-0.5 = small, 0.5-0.8 = medium, >0.8 = large\n"
     ]
    }
   ],
   "source": [
    "# Cell 12: Primary analysis — filter zeros, compute all 6 comparisons with Bonferroni\n",
    "print(\"=\" * 70)\n",
    "print(\"PRIMARY ANALYSIS — 6 COMPARISONS WITH BONFERRONI CORRECTION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "bare_nlls_raw = np.array([r['bare_nll'] for r in results])\n",
    "oracle_trunc_nlls_raw = np.array([r['oracle_trunc_nll'] for r in results])\n",
    "random_trunc_nlls_raw = np.array([r['random_trunc_nll'] for r in results])\n",
    "oracle_suffix_nlls_raw = np.array([r['oracle_suffix_nll'] for r in results])\n",
    "random_suffix_nlls_raw = np.array([r['random_suffix_nll'] for r in results])\n",
    "\n",
    "# Sanity checks\n",
    "for name, arr in [('bare', bare_nlls_raw), ('oracle_trunc', oracle_trunc_nlls_raw),\n",
    "                   ('random_trunc', random_trunc_nlls_raw), ('oracle_suffix', oracle_suffix_nlls_raw),\n",
    "                   ('random_suffix', random_suffix_nlls_raw)]:\n",
    "    assert not np.any(np.isnan(arr)), f\"NaN in {name} NLLs!\"\n",
    "\n",
    "# Filter out degenerate samples where ANY NLL is 0.0\n",
    "valid_mask = (\n",
    "    (bare_nlls_raw != 0.0) &\n",
    "    (oracle_trunc_nlls_raw != 0.0) &\n",
    "    (random_trunc_nlls_raw != 0.0) &\n",
    "    (oracle_suffix_nlls_raw != 0.0) &\n",
    "    (random_suffix_nlls_raw != 0.0)\n",
    ")\n",
    "n_invalid = np.sum(~valid_mask)\n",
    "n_valid = np.sum(valid_mask)\n",
    "print(f\"Total samples: {len(results)}\")\n",
    "print(f\"Excluded (zero NLL from single-token answers): {n_invalid}\")\n",
    "print(f\"Valid samples for analysis: {n_valid}\")\n",
    "print(f\"Bonferroni-corrected alpha: {BONFERRONI_ALPHA:.4f} (0.05 / 6)\\n\")\n",
    "\n",
    "bare = bare_nlls_raw[valid_mask]\n",
    "oracle_trunc = oracle_trunc_nlls_raw[valid_mask]\n",
    "random_trunc = random_trunc_nlls_raw[valid_mask]\n",
    "oracle_suffix = oracle_suffix_nlls_raw[valid_mask]\n",
    "random_suffix = random_suffix_nlls_raw[valid_mask]\n",
    "\n",
    "# NLL summary\n",
    "print(f\"{'Condition':<20} {'Mean NLL':>10} {'Std':>10} {'Median':>10}\")\n",
    "print(\"-\" * 55)\n",
    "for name, arr in [('Bare', bare), ('Oracle-truncated', oracle_trunc),\n",
    "                   ('Random-truncated', random_trunc), ('Oracle-suffix', oracle_suffix),\n",
    "                   ('Random-suffix', random_suffix)]:\n",
    "    print(f\"{name:<20} {np.mean(arr):>10.4f} {np.std(arr):>10.4f} {np.median(arr):>10.4f}\")\n",
    "\n",
    "# All 6 comparisons\n",
    "# Convention: positive delta = first condition is BETTER (lower NLL)\n",
    "comparisons = [\n",
    "    # Primary\n",
    "    ('P1: Oracle-sfx vs Random-sfx', random_suffix - oracle_suffix, 'Semantic signal in suffix?'),\n",
    "    ('P2: Oracle-sfx vs Bare', bare - oracle_suffix, 'Does suffix priming help?'),\n",
    "    ('P3: Oracle-trunc vs Bare', bare - oracle_trunc, 'Exp 01 replication (d~0.023 ns)?'),\n",
    "    # Secondary\n",
    "    ('S1: Random-sfx vs Bare', bare - random_suffix, 'Any suffix helps?'),\n",
    "    ('S2: Oracle-sfx vs Oracle-trunc', oracle_trunc - oracle_suffix, 'Suffix vs truncated?'),\n",
    "    ('S3: Random-trunc vs Bare', bare - random_trunc, 'Exp 01 random replication (d~0.091)?'),\n",
    "]\n",
    "\n",
    "print(f\"\\n{'─' * 80}\")\n",
    "print(\"PAIRED COMPARISONS (positive delta = first condition has LOWER NLL = better)\")\n",
    "print(f\"{'─' * 80}\")\n",
    "print(f\"\\n{'Comparison':<35} {'Mean Δ':>8} {'d':>8} {'Win%':>7} {'t':>8} {'p':>12} {'Sig':>5}\")\n",
    "print(\"-\" * 88)\n",
    "\n",
    "comparison_results = {}\n",
    "for name, delta, question in comparisons:\n",
    "    d = cohens_d(delta)\n",
    "    win_rate = np.mean(delta > 0) * 100\n",
    "    t_stat, p_val = stats.ttest_1samp(delta, 0)\n",
    "    sig = \"***\" if p_val < 0.001 else \"**\" if p_val < BONFERRONI_ALPHA else \"*\" if p_val < 0.05 else \"ns\"\n",
    "    bonf_sig = p_val < BONFERRONI_ALPHA\n",
    "    print(f\"{name:<35} {np.mean(delta):>8.4f} {d:>8.3f} {win_rate:>6.1f}% {t_stat:>8.2f} {p_val:>11.2e} {sig:>5}\")\n",
    "    comparison_results[name] = {\n",
    "        'mean_delta': float(np.mean(delta)),\n",
    "        'cohens_d': float(d),\n",
    "        'win_rate': float(win_rate / 100),\n",
    "        't_stat': float(t_stat),\n",
    "        'p_value': float(p_val),\n",
    "        'bonferroni_significant': bool(bonf_sig),\n",
    "        'question': question,\n",
    "    }\n",
    "\n",
    "print(f\"\\nSignificance levels: *** p<0.001, ** p<{BONFERRONI_ALPHA:.4f} (Bonferroni), * p<0.05, ns = not significant\")\n",
    "print(f\"Cohen's d interpretation: |d|<0.2 = negligible, 0.2-0.5 = small, 0.5-0.8 = medium, >0.8 = large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "SUMMARY TABLE WITH VERDICTS\n",
      "======================================================================\n",
      "\n",
      "N = 4606 valid samples (of 5000 total)\n",
      "Bonferroni-corrected alpha = 0.0083\n",
      "\n",
      "Q: Semantic signal in suffix?\n",
      "  P1: Oracle-sfx vs Random-sfx\n",
      "  Δ = -0.0655, d = -0.192, Win% = 37.6%, p = 2.99e-38\n",
      "  → YES — significant HARM (survives Bonferroni)\n",
      "\n",
      "Q: Does suffix priming help?\n",
      "  P2: Oracle-sfx vs Bare\n",
      "  Δ = +0.0287, d = +0.071, Win% = 52.9%, p = 1.29e-06\n",
      "  → YES — significant benefit (survives Bonferroni)\n",
      "\n",
      "Q: Exp 01 replication (d~0.023 ns)?\n",
      "  P3: Oracle-trunc vs Bare\n",
      "  Δ = +0.0086, d = +0.023, Win% = 50.0%, p = 1.13e-01\n",
      "  → NO — not significant\n",
      "\n",
      "Q: Any suffix helps?\n",
      "  S1: Random-sfx vs Bare\n",
      "  Δ = +0.0942, d = +0.264, Win% = 65.2%, p = 1.14e-69\n",
      "  → YES — significant benefit (survives Bonferroni)\n",
      "\n",
      "Q: Suffix vs truncated?\n",
      "  S2: Oracle-sfx vs Oracle-trunc\n",
      "  Δ = +0.0201, d = +0.048, Win% = 52.8%, p = 1.09e-03\n",
      "  → YES — significant benefit (survives Bonferroni)\n",
      "\n",
      "Q: Exp 01 random replication (d~0.091)?\n",
      "  S3: Random-trunc vs Bare\n",
      "  Δ = +0.0285, d = +0.091, Win% = 59.5%, p = 7.05e-10\n",
      "  → YES — significant benefit (survives Bonferroni)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 13: Summary table with verdicts\n",
    "print(\"=\" * 70)\n",
    "print(\"SUMMARY TABLE WITH VERDICTS\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nN = {n_valid} valid samples (of {len(results)} total)\")\n",
    "print(f\"Bonferroni-corrected alpha = {BONFERRONI_ALPHA:.4f}\\n\")\n",
    "\n",
    "for name, delta, question in comparisons:\n",
    "    cr = comparison_results[name]\n",
    "    d = cr['cohens_d']\n",
    "    p = cr['p_value']\n",
    "    bonf = cr['bonferroni_significant']\n",
    "    win = cr['win_rate'] * 100\n",
    "\n",
    "    if bonf:\n",
    "        if cr['mean_delta'] > 0:\n",
    "            verdict = \"YES — significant benefit (survives Bonferroni)\"\n",
    "        else:\n",
    "            verdict = \"YES — significant HARM (survives Bonferroni)\"\n",
    "    elif p < 0.05:\n",
    "        if cr['mean_delta'] > 0:\n",
    "            verdict = \"Suggestive benefit (p<0.05 but fails Bonferroni)\"\n",
    "        else:\n",
    "            verdict = \"Suggestive harm (p<0.05 but fails Bonferroni)\"\n",
    "    else:\n",
    "        verdict = \"NO — not significant\"\n",
    "\n",
    "    print(f\"Q: {question}\")\n",
    "    print(f\"  {name}\")\n",
    "    print(f\"  Δ = {cr['mean_delta']:+.4f}, d = {d:+.3f}, Win% = {win:.1f}%, p = {p:.2e}\")\n",
    "    print(f\"  → {verdict}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "HARDNESS INTERACTION ANALYSIS\n",
      "======================================================================\n",
      "\n",
      "Pearson r of bare NLL (hardness) with condition benefit:\n",
      "Condition                        r            p\n",
      "------------------------------------------------\n",
      "Oracle-truncated            0.1569     8.77e-27\n",
      "Random-truncated            0.1927     9.29e-40\n",
      "Oracle-suffix               0.3278    7.54e-116\n",
      "Random-suffix               0.3462    7.47e-130\n",
      "\n",
      "Quartile breakdown (by bare NLL hardness):\n",
      "Quartile         N     Bare  Δ_Ora-tru d_Ora-tru  Δ_Ran-tru d_Ran-tru  Δ_Ora-suf d_Ora-suf  Δ_Ran-suf d_Ran-suf\n",
      "---------------------------------------------------------------------------------------------------------------\n",
      "Q1 (easy)     1152    0.086    -0.0331   -0.195     0.0014    0.023    -0.0501   -0.377    -0.0023   -0.043\n",
      "Q2            1152    0.417    -0.0213   -0.127     0.0132    0.105    -0.0270   -0.132     0.0348    0.280\n",
      "Q3            1150    0.943     0.0114    0.044     0.0303    0.154     0.0056    0.019     0.0847    0.375\n",
      "Q4 (hard)     1152    3.136     0.0775    0.121     0.0690    0.120     0.1861    0.272     0.2597    0.411\n"
     ]
    }
   ],
   "source": [
    "# Cell 14: Hardness interaction (all 4 non-bare conditions)\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')  # Non-interactive backend\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"HARDNESS INTERACTION ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Deltas for all 4 non-bare conditions (positive = condition better than bare)\n",
    "delta_oracle_trunc = bare - oracle_trunc\n",
    "delta_random_trunc = bare - random_trunc\n",
    "delta_oracle_sfx = bare - oracle_suffix\n",
    "delta_random_sfx = bare - random_suffix\n",
    "\n",
    "conditions = [\n",
    "    ('Oracle-truncated', delta_oracle_trunc),\n",
    "    ('Random-truncated', delta_random_trunc),\n",
    "    ('Oracle-suffix', delta_oracle_sfx),\n",
    "    ('Random-suffix', delta_random_sfx),\n",
    "]\n",
    "\n",
    "print(f\"\\nPearson r of bare NLL (hardness) with condition benefit:\")\n",
    "print(f\"{'Condition':<25} {'r':>8} {'p':>12}\")\n",
    "print(\"-\" * 48)\n",
    "\n",
    "hardness_results = {}\n",
    "for name, delta in conditions:\n",
    "    r, p = stats.pearsonr(bare, delta)\n",
    "    print(f\"{name:<25} {r:>8.4f} {p:>12.2e}\")\n",
    "    hardness_results[name] = {'r': float(r), 'p': float(p)}\n",
    "\n",
    "# Quartile breakdown\n",
    "quartiles = np.percentile(bare, [25, 50, 75])\n",
    "q_labels = ['Q1 (easy)', 'Q2', 'Q3', 'Q4 (hard)']\n",
    "q_bounds = [(-np.inf, quartiles[0]), (quartiles[0], quartiles[1]),\n",
    "            (quartiles[1], quartiles[2]), (quartiles[2], np.inf)]\n",
    "\n",
    "print(f\"\\nQuartile breakdown (by bare NLL hardness):\")\n",
    "header = f\"{'Quartile':<12} {'N':>5} {'Bare':>8}\"\n",
    "for name, _ in conditions:\n",
    "    short = name.split('-')[0][:3] + '-' + name.split('-')[1][:3]\n",
    "    header += f\" {'Δ_'+short:>10} {'d_'+short:>8}\"\n",
    "print(header)\n",
    "print(\"-\" * len(header))\n",
    "\n",
    "for label, (lo, hi) in zip(q_labels, q_bounds):\n",
    "    mask = (bare > lo) & (bare <= hi)\n",
    "    n_q = np.sum(mask)\n",
    "    mean_bare = np.mean(bare[mask])\n",
    "    row = f\"{label:<12} {n_q:>5} {mean_bare:>8.3f}\"\n",
    "    for name, delta in conditions:\n",
    "        dq = delta[mask]\n",
    "        row += f\" {np.mean(dq):>10.4f} {cohens_d(dq):>8.3f}\"\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Mechanism Comparison: Suffix vs Truncated\n",
      "============================================================\n",
      "\n",
      "Correlation of truncated benefit vs suffix benefit:\n",
      "  Oracle: r = 0.4182, p = 2.01e-194\n",
      "  Random: r = 0.4651, p = 5.58e-246\n",
      "\n",
      "Plot saved to results/exp02/cross_mechanism_comparison.png\n"
     ]
    }
   ],
   "source": [
    "# Cell 15: Cross-mechanism comparison (suffix vs truncated scatter plot)\n",
    "print(\"Cross-Mechanism Comparison: Suffix vs Truncated\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Oracle: suffix benefit vs truncated benefit\n",
    "oracle_sfx_benefit = bare - oracle_suffix  # positive = suffix better than bare\n",
    "oracle_trunc_benefit = bare - oracle_trunc  # positive = trunc better than bare\n",
    "\n",
    "# Random: suffix benefit vs truncated benefit\n",
    "random_sfx_benefit = bare - random_suffix\n",
    "random_trunc_benefit = bare - random_trunc\n",
    "\n",
    "# Correlation\n",
    "r_oracle_cross, p_oracle_cross = stats.pearsonr(oracle_trunc_benefit, oracle_sfx_benefit)\n",
    "r_random_cross, p_random_cross = stats.pearsonr(random_trunc_benefit, random_sfx_benefit)\n",
    "\n",
    "print(f\"\\nCorrelation of truncated benefit vs suffix benefit:\")\n",
    "print(f\"  Oracle: r = {r_oracle_cross:.4f}, p = {p_oracle_cross:.2e}\")\n",
    "print(f\"  Random: r = {r_random_cross:.4f}, p = {p_random_cross:.2e}\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Oracle scatter\n",
    "axes[0].scatter(oracle_trunc_benefit, oracle_sfx_benefit, alpha=0.15, s=5, c='steelblue')\n",
    "axes[0].axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "axes[0].axvline(x=0, color='gray', linestyle='--', alpha=0.5)\n",
    "# Regression line\n",
    "z = np.polyfit(oracle_trunc_benefit, oracle_sfx_benefit, 1)\n",
    "p_fit = np.poly1d(z)\n",
    "x_range = np.linspace(oracle_trunc_benefit.min(), oracle_trunc_benefit.max(), 100)\n",
    "axes[0].plot(x_range, p_fit(x_range), 'r-', alpha=0.8, label=f'r={r_oracle_cross:.3f}')\n",
    "axes[0].set_xlabel('Oracle-truncated benefit (vs bare)')\n",
    "axes[0].set_ylabel('Oracle-suffix benefit (vs bare)')\n",
    "axes[0].set_title('Oracle: Truncated vs Suffix Benefit')\n",
    "axes[0].legend()\n",
    "\n",
    "# Random scatter\n",
    "axes[1].scatter(random_trunc_benefit, random_sfx_benefit, alpha=0.15, s=5, c='darkorange')\n",
    "axes[1].axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "axes[1].axvline(x=0, color='gray', linestyle='--', alpha=0.5)\n",
    "z2 = np.polyfit(random_trunc_benefit, random_sfx_benefit, 1)\n",
    "p_fit2 = np.poly1d(z2)\n",
    "x_range2 = np.linspace(random_trunc_benefit.min(), random_trunc_benefit.max(), 100)\n",
    "axes[1].plot(x_range2, p_fit2(x_range2), 'r-', alpha=0.8, label=f'r={r_random_cross:.3f}')\n",
    "axes[1].set_xlabel('Random-truncated benefit (vs bare)')\n",
    "axes[1].set_ylabel('Random-suffix benefit (vs bare)')\n",
    "axes[1].set_title('Random: Truncated vs Suffix Benefit')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'cross_mechanism_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"\\nPlot saved to {RESULTS_DIR / 'cross_mechanism_comparison.png'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plot saved to results/exp02/delta_distributions.png\n"
     ]
    }
   ],
   "source": [
    "# Cell 16: Delta distribution plots (2x3 grid)\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "\n",
    "plot_configs = [\n",
    "    # Row 1: Primary comparisons\n",
    "    ('P1: Oracle-sfx vs Random-sfx', random_suffix - oracle_suffix, 'steelblue'),\n",
    "    ('P2: Oracle-sfx vs Bare', bare - oracle_suffix, 'forestgreen'),\n",
    "    ('P3: Oracle-trunc vs Bare', bare - oracle_trunc, 'darkorange'),\n",
    "    # Row 2: Secondary comparisons\n",
    "    ('S1: Random-sfx vs Bare', bare - random_suffix, 'mediumpurple'),\n",
    "    ('S2: Oracle-sfx vs Oracle-trunc', oracle_trunc - oracle_suffix, 'crimson'),\n",
    "    ('S3: Random-trunc vs Bare', bare - random_trunc, 'teal'),\n",
    "]\n",
    "\n",
    "for ax, (title, delta, color) in zip(axes.flat, plot_configs):\n",
    "    cr = comparison_results[title]\n",
    "    ax.hist(delta, bins=80, color=color, alpha=0.7, edgecolor='black', linewidth=0.3)\n",
    "    ax.axvline(x=0, color='red', linestyle='--', alpha=0.7)\n",
    "    ax.axvline(x=np.mean(delta), color='black', linestyle='-', alpha=0.8,\n",
    "               label=f'mean={np.mean(delta):.4f}, d={cr[\"cohens_d\"]:+.3f}')\n",
    "    ax.set_xlabel('Delta NLL (+ = first better)')\n",
    "    ax.set_ylabel('Count')\n",
    "    ax.set_title(title, fontsize=10)\n",
    "    ax.legend(fontsize=8)\n",
    "\n",
    "plt.suptitle('Delta NLL Distributions — All 6 Comparisons', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'delta_distributions.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"Plot saved to {RESULTS_DIR / 'delta_distributions.png'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plot saved to results/exp02/hardness_interaction.png\n"
     ]
    }
   ],
   "source": [
    "# Cell 17: Hardness scatter plots (2x2 grid — all 4 non-bare conditions)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "\n",
    "scatter_configs = [\n",
    "    ('Oracle-truncated', delta_oracle_trunc, 'steelblue'),\n",
    "    ('Random-truncated', delta_random_trunc, 'darkorange'),\n",
    "    ('Oracle-suffix', delta_oracle_sfx, 'forestgreen'),\n",
    "    ('Random-suffix', delta_random_sfx, 'mediumpurple'),\n",
    "]\n",
    "\n",
    "for ax, (name, delta, color) in zip(axes.flat, scatter_configs):\n",
    "    r, p = hardness_results[name]['r'], hardness_results[name]['p']\n",
    "    ax.scatter(bare, delta, alpha=0.12, s=5, c=color)\n",
    "    ax.axhline(y=0, color='red', linestyle='--', alpha=0.5)\n",
    "    # Regression line\n",
    "    z = np.polyfit(bare, delta, 1)\n",
    "    p_fit = np.poly1d(z)\n",
    "    x_range = np.linspace(bare.min(), bare.max(), 100)\n",
    "    ax.plot(x_range, p_fit(x_range), 'r-', alpha=0.8, label=f'r={r:.3f}, p={p:.1e}')\n",
    "    ax.set_xlabel('Bare NLL (hardness)')\n",
    "    ax.set_ylabel(f'{name} benefit (+ = helps)')\n",
    "    ax.set_title(f'Hardness vs {name} Benefit')\n",
    "    ax.legend()\n",
    "\n",
    "plt.suptitle('Hardness Interaction — All 4 Non-Bare Conditions', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'hardness_interaction.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"Plot saved to {RESULTS_DIR / 'hardness_interaction.png'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final results saved to results/exp02/results.json\n",
      "File size: 3715.0 KB\n",
      "Total samples: 5000\n",
      "Valid samples: 4606\n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Cell 18: Save results JSON\n",
    "\n",
    "final_results = {\n",
    "    'experiment': 'exp02_suffix_vs_truncated',\n",
    "    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'config': {\n",
    "        'model_name': config.model_name,\n",
    "        'num_samples': config.num_samples,\n",
    "        'seed': SEED,\n",
    "        'min_passage_words': config.min_passage_words,\n",
    "        'max_passage_words': config.max_passage_words,\n",
    "        'surrogate_prefix_template': SURROGATE_PREFIX_TEMPLATE,\n",
    "        'document_template': DOCUMENT_TEMPLATE,\n",
    "        'query_template': QUERY_TEMPLATE,\n",
    "        'answer_template': ANSWER_TEMPLATE,\n",
    "        'suffix_separator': SUFFIX_SEPARATOR,\n",
    "        'bonferroni_alpha': BONFERRONI_ALPHA,\n",
    "    },\n",
    "    'summary': {\n",
    "        'n_total': len(results),\n",
    "        'n_valid': int(n_valid),\n",
    "        'n_excluded_zero_nll': int(n_invalid),\n",
    "        'nll_means': {\n",
    "            'bare': float(np.mean(bare)),\n",
    "            'oracle_truncated': float(np.mean(oracle_trunc)),\n",
    "            'random_truncated': float(np.mean(random_trunc)),\n",
    "            'oracle_suffix': float(np.mean(oracle_suffix)),\n",
    "            'random_suffix': float(np.mean(random_suffix)),\n",
    "        },\n",
    "        'nll_stds': {\n",
    "            'bare': float(np.std(bare)),\n",
    "            'oracle_truncated': float(np.std(oracle_trunc)),\n",
    "            'random_truncated': float(np.std(random_trunc)),\n",
    "            'oracle_suffix': float(np.std(oracle_suffix)),\n",
    "            'random_suffix': float(np.std(random_suffix)),\n",
    "        },\n",
    "        'comparisons': comparison_results,\n",
    "        'hardness_interaction': hardness_results,\n",
    "        'cross_mechanism_correlation': {\n",
    "            'oracle_r': float(r_oracle_cross),\n",
    "            'oracle_p': float(p_oracle_cross),\n",
    "            'random_r': float(r_random_cross),\n",
    "            'random_p': float(p_random_cross),\n",
    "        },\n",
    "    },\n",
    "    'per_sample_results': results,\n",
    "}\n",
    "\n",
    "with open(FINAL_RESULTS_PATH, 'w') as f:\n",
    "    json.dump(final_results, f, indent=2)\n",
    "\n",
    "print(f\"Final results saved to {FINAL_RESULTS_PATH}\")\n",
    "print(f\"File size: {FINAL_RESULTS_PATH.stat().st_size / 1024:.1f} KB\")\n",
    "print(f\"Total samples: {len(results)}\")\n",
    "print(f\"Valid samples: {n_valid}\")\n",
    "print(f\"\\nDone!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cu124.m127",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/base-cu124:m127"
  },
  "kernelspec": {
   "display_name": "Python 3 (Local)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
