{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exp 15: NLL Ensemble Ranking\n",
    "\n",
    "## Motivation\n",
    "\n",
    "Exp 14 found that combining bare + primed NLL marginally improves ranking\n",
    "(global \u0394MRR=+0.008), but cross-validated improvement was only +0.006 (p=0.21, ns).\n",
    "The per-query oracle alpha gap (+0.050) suggests latent signal exists but a single\n",
    "primed cache can't reliably extract it.\n",
    "\n",
    "**Core hypothesis:** Diverse priming caches produce NLL estimates with partially\n",
    "independent errors. Ensembling (averaging) these estimates reduces ranking noise,\n",
    "just as averaging multiple measurements improves precision.\n",
    "\n",
    "## Design\n",
    "\n",
    "**5 Scoring Signals** (each produces a per-passage NLL):\n",
    "\n",
    "| # | Signal | Cache | Scoring Prompt | Purpose |\n",
    "|---|--------|-------|---------------|---------|\n",
    "| 1 | `bare` | Bare cache | Standard prompt | Baseline |\n",
    "| 2 | `rescore` | Bare cache | Alt prompt | **Control**: diversity without priming |\n",
    "| 3 | `sf` | Static fact prefix | Standard prompt | Replicate Exp 14 |\n",
    "| 4 | `rand` | Random text prefix | Standard prompt | Is prefix content irrelevant? |\n",
    "| 5 | `intent` | Intent prefix | Standard prompt | Different semantic angle |\n",
    "\n",
    "**Ensemble Conditions** (equal-weight NLL average, no tuning):\n",
    "\n",
    "| Ensemble | Members | Tests |\n",
    "|----------|---------|-------|\n",
    "| `ens_2_sf` | bare + sf | Replicates Exp 14 |\n",
    "| `ens_2_rand` | bare + rand | Random prefix ensemble |\n",
    "| `ens_2_rescore` | bare + rescore | **Non-priming control** |\n",
    "| `ens_3` | bare + sf + rand | 3-member ensemble |\n",
    "| `ens_4` | bare + sf + rand + intent | 4-member ensemble |\n",
    "| `ens_5_all` | all 5 signals | Maximum diversity |\n",
    "\n",
    "**Critical comparison:** `ens_2_sf` vs `ens_2_rescore`. If the control matches\n",
    "priming, then priming isn't special \u2014 any scoring diversity helps."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 1: Setup\n",
    "import os\n",
    "os.umask(0o000)\n",
    "\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import gc\n",
    "from pathlib import Path\n",
    "\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "RESULTS_DIR = Path(\"results/exp15\")\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CHECKPOINT_PATH = RESULTS_DIR / \"checkpoint.json\"\n",
    "FINAL_RESULTS_PATH = RESULTS_DIR / \"results.json\"\n",
    "\n",
    "print(f\"SEED: {SEED}\")\n",
    "print(f\"Results directory: {RESULTS_DIR}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 2: Load model (Mistral-7B 4-bit)\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, DynamicCache\n",
    "\n",
    "MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "print(f\"Loading {MODEL_NAME} (4-bit)...\")\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "print(f\"Model loaded. dtype={model.dtype}, device={model.device}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 3: Config and library imports\n",
    "sys.path.insert(0, \".\")\n",
    "\n",
    "from lib.config import ExperimentConfig\n",
    "from lib.kv_cache import (\n",
    "    _get_cache_keys,\n",
    "    _get_cache_values,\n",
    "    _set_cache_keys,\n",
    "    _set_cache_values,\n",
    "    _ensure_dynamic_cache,\n",
    "    extract_and_truncate_cache_with_bos,\n",
    "    correct_rope_positions_with_bos,\n",
    "    score_answer_with_cache,\n",
    "    deepcopy_cache,\n",
    "    build_hybrid_cache,\n",
    ")\n",
    "from lib.analysis import compute_ranking_metrics\n",
    "from lib.surrogate import STATIC_SURROGATE_QUERIES\n",
    "from lib.data import count_words\n",
    "from scipy import stats\n",
    "from scipy.stats import wilcoxon\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "config = ExperimentConfig(\n",
    "    model_name=MODEL_NAME,\n",
    "    num_samples=2000,\n",
    "    seed=SEED,\n",
    ")\n",
    "\n",
    "# Templates\n",
    "SURROGATE_PREFIX_TEMPLATE = \"{surrogate}\\n\"\n",
    "DOCUMENT_TEMPLATE = \"{document}\"\n",
    "QUERY_TEMPLATE = \"\\nQuery: {query}\\nAnswer:\"\n",
    "ANSWER_TEMPLATE = \" {answer}\"\n",
    "\n",
    "# Alternative prompt for rescore control\n",
    "ALT_QUERY_TEMPLATE = \"\\nQuestion: {query}\\nResponse:\"\n",
    "\n",
    "# Prefix texts\n",
    "STATIC_FACT = STATIC_SURROGATE_QUERIES['static_factual']['query']\n",
    "RANDOM_PREFIX_TEXT = \"The purple elephant danced gracefully on the frozen lake during twilight\"\n",
    "INTENT_PREFIX_TEXT = \"What is this passage about?\"\n",
    "\n",
    "# Experiment parameters\n",
    "MAX_QUERIES = 300\n",
    "MAX_PASSAGE_WORDS = 300\n",
    "MIN_PASSAGES_PER_QUERY = 2\n",
    "CHECKPOINT_EVERY = 25\n",
    "\n",
    "SIGNAL_NAMES = ['bare', 'rescore', 'sf', 'rand', 'intent']\n",
    "\n",
    "print(\"Config ready\")\n",
    "print(f\"  MAX_QUERIES: {MAX_QUERIES}\")\n",
    "print(f\"  Prefixes:\")\n",
    "print(f\"    sf:     '{STATIC_FACT}'\")\n",
    "print(f\"    rand:   '{RANDOM_PREFIX_TEXT}'\")\n",
    "print(f\"    intent: '{INTENT_PREFIX_TEXT}'\")\n",
    "print(f\"  Alt prompt: '{ALT_QUERY_TEMPLATE.format(query='...')}'\")\n",
    "print(f\"  Signals: {SIGNAL_NAMES}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 4: Load MS MARCO v1.1 (same filtering as Exp 14)\n",
    "from datasets import load_dataset\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"LOADING MS MARCO v1.1 \u2014 ALL PASSAGES PER QUERY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "dataset = load_dataset(\"microsoft/ms_marco\", \"v1.1\", split=\"validation\",\n",
    "                        trust_remote_code=True)\n",
    "print(f\"Total items in validation: {len(dataset)}\")\n",
    "\n",
    "queries = []\n",
    "np.random.seed(SEED)\n",
    "\n",
    "for item in tqdm(dataset, desc=\"Filtering\"):\n",
    "    passages_info = item.get('passages', {})\n",
    "    passage_texts = passages_info.get('passage_text', [])\n",
    "    is_selected = passages_info.get('is_selected', [])\n",
    "    query = item.get('query', '')\n",
    "    answers = item.get('answers', [])\n",
    "    well_formed = item.get('wellFormedAnswers', [])\n",
    "\n",
    "    if not passage_texts or not query:\n",
    "        continue\n",
    "    if len(passage_texts) < MIN_PASSAGES_PER_QUERY:\n",
    "        continue\n",
    "    if not is_selected or sum(is_selected) == 0:\n",
    "        continue\n",
    "\n",
    "    word_counts = [count_words(p) for p in passage_texts]\n",
    "    if any(wc > MAX_PASSAGE_WORDS for wc in word_counts):\n",
    "        continue\n",
    "\n",
    "    answer = None\n",
    "    if well_formed and len(well_formed) > 0 and well_formed[0] != '[]':\n",
    "        answer = well_formed[0]\n",
    "    elif answers and len(answers) > 0 and answers[0] != 'No Answer Present.':\n",
    "        answer = answers[0]\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "    passage_list = []\n",
    "    for i, (ptext, sel) in enumerate(zip(passage_texts, is_selected)):\n",
    "        passage_list.append({\n",
    "            'passage': ptext,\n",
    "            'is_relevant': bool(sel == 1),\n",
    "            'word_count': word_counts[i],\n",
    "            'passage_idx': i,\n",
    "        })\n",
    "\n",
    "    queries.append({\n",
    "        'query': query,\n",
    "        'answer': answer,\n",
    "        'passages': passage_list,\n",
    "        'n_passages': len(passage_list),\n",
    "        'n_relevant': sum(1 for p in passage_list if p['is_relevant']),\n",
    "    })\n",
    "\n",
    "    if len(queries) >= MAX_QUERIES * 3:\n",
    "        break\n",
    "\n",
    "np.random.shuffle(queries)\n",
    "queries = queries[:MAX_QUERIES]\n",
    "N = len(queries)\n",
    "\n",
    "n_passages_list = [q['n_passages'] for q in queries]\n",
    "total_passages = sum(n_passages_list)\n",
    "\n",
    "print(f\"\\nSelected {N} queries ({total_passages} total passages)\")\n",
    "print(f\"Passages per query: mean={np.mean(n_passages_list):.1f}, \"\n",
    "      f\"min={min(n_passages_list)}, max={max(n_passages_list)}\")\n",
    "print(f\"Word counts: mean={np.mean([p['word_count'] for q in queries for p in q['passages']]):.0f}\")\n",
    "\n",
    "del dataset\n",
    "gc.collect()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 5: Tokenize prefixes and verify BPE boundaries\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"EXPERIMENTAL CONDITIONS \u2014 NLL ENSEMBLE RANKING\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Tokenize each prefix\n",
    "sf_str = SURROGATE_PREFIX_TEMPLATE.format(surrogate=STATIC_FACT)\n",
    "rand_str = SURROGATE_PREFIX_TEMPLATE.format(surrogate=RANDOM_PREFIX_TEXT)\n",
    "intent_str = SURROGATE_PREFIX_TEMPLATE.format(surrogate=INTENT_PREFIX_TEXT)\n",
    "\n",
    "sf_ids = tokenizer(sf_str, return_tensors=\"pt\",\n",
    "                    add_special_tokens=False)['input_ids'].to(config.device)\n",
    "rand_ids = tokenizer(rand_str, return_tensors=\"pt\",\n",
    "                      add_special_tokens=False)['input_ids'].to(config.device)\n",
    "intent_ids = tokenizer(intent_str, return_tensors=\"pt\",\n",
    "                        add_special_tokens=False)['input_ids'].to(config.device)\n",
    "\n",
    "PREFIX_CONFIGS = [\n",
    "    ('sf', STATIC_FACT, sf_str, sf_ids),\n",
    "    ('rand', RANDOM_PREFIX_TEXT, rand_str, rand_ids),\n",
    "    ('intent', INTENT_PREFIX_TEXT, intent_str, intent_ids),\n",
    "]\n",
    "\n",
    "print(\"\\nPREFIX TOKEN LENGTHS:\")\n",
    "for name, text, full_str, ids in PREFIX_CONFIGS:\n",
    "    print(f\"  {name:<8} {ids.shape[1]:>3} tokens | '{text}'\")\n",
    "\n",
    "# Verify BPE boundary consistency across prefixes\n",
    "print(\"\\nBPE BOUNDARY CHECK (first passage):\")\n",
    "example_doc = queries[0]['passages'][0]['passage']\n",
    "for name, text, full_str, ids in PREFIX_CONFIGS:\n",
    "    concat = full_str + DOCUMENT_TEMPLATE.format(document=example_doc)\n",
    "    concat_enc = tokenizer(concat, add_special_tokens=True)['input_ids']\n",
    "    prefix_enc = tokenizer(full_str, add_special_tokens=True)['input_ids']\n",
    "    doc_ids_from_concat = concat_enc[len(prefix_enc):]\n",
    "\n",
    "    bare_doc_enc = tokenizer(DOCUMENT_TEMPLATE.format(document=example_doc),\n",
    "                              add_special_tokens=False)['input_ids']\n",
    "    match = sum(1 for a, b in zip(doc_ids_from_concat, bare_doc_enc) if a == b)\n",
    "    total = max(len(bare_doc_enc), 1)\n",
    "    print(f\"  {name}: {match}/{total} tokens match ({100*match/total:.1f}%)\")\n",
    "\n",
    "# Explain conditions\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"CONDITION DETAILS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "conditions_detail = [\n",
    "    (\"bare\", \"Standard bare cache scored with standard prompt\",\n",
    "     \"Baseline. All other conditions are compared to this.\"),\n",
    "    (\"rescore\", \"Bare cache scored with alt prompt ('Question:...Response:')\",\n",
    "     \"NON-PRIMING CONTROL. Same cache, different prompt. Tests if scoring \"\n",
    "     \"diversity alone improves ensembles, without any cache modification.\"),\n",
    "    (\"sf (static_fact)\", f\"Prefix: '{STATIC_FACT}'\",\n",
    "     \"Replicates Exp 14's primed_1x. Bare keys + primed values (truncated).\"),\n",
    "    (\"rand (random)\", f\"Prefix: '{RANDOM_PREFIX_TEXT}'\",\n",
    "     \"Semantically unrelated prefix. Tests if ANY prefix content works or \"\n",
    "     \"if semantic relevance matters.\"),\n",
    "    (\"intent\", f\"Prefix: '{INTENT_PREFIX_TEXT}'\",\n",
    "     \"Different semantic angle than static_fact. Tests prefix diversity.\"),\n",
    "]\n",
    "\n",
    "for name, detail, purpose in conditions_detail:\n",
    "    print(f\"\\n### {name} ###\")\n",
    "    print(f\"  {detail}\")\n",
    "    print(f\"  Purpose: {purpose}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ENSEMBLE CONDITIONS (equal-weight NLL average)\")\n",
    "print(\"=\" * 70)\n",
    "print(\"  ens_2_sf:      bare + sf           (replicate Exp 14)\")\n",
    "print(\"  ens_2_rand:    bare + rand         (random prefix)\")\n",
    "print(\"  ens_2_rescore: bare + rescore      (NON-PRIMING CONTROL)\")\n",
    "print(\"  ens_3:         bare + sf + rand    (3-member)\")\n",
    "print(\"  ens_4:         bare + sf + rand + intent  (4-member)\")\n",
    "print(\"  ens_5_all:     all 5 signals       (maximum diversity)\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 6: Main loop \u2014 score all passages under all conditions\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"MAIN EVALUATION ({N} queries, ~{total_passages} passages)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Checkpoint resume\n",
    "all_results = []\n",
    "start_idx = 0\n",
    "\n",
    "if CHECKPOINT_PATH.exists():\n",
    "    with open(CHECKPOINT_PATH, 'r') as f:\n",
    "        ckpt = json.load(f)\n",
    "    ckpt_queries = ckpt.get('query_texts', [])\n",
    "    current_queries = [q['query'] for q in queries]\n",
    "    if ckpt_queries == current_queries:\n",
    "        all_results = ckpt['results']\n",
    "        start_idx = len(all_results)\n",
    "        print(f\"Resuming from checkpoint: {start_idx}/{N}\")\n",
    "    else:\n",
    "        print(\"Checkpoint query mismatch. Starting fresh.\")\n",
    "else:\n",
    "    print(\"No checkpoint found. Starting fresh.\")\n",
    "\n",
    "print(f\"Evaluating queries {start_idx} to {N-1}\")\n",
    "print(f\"Per passage: 4 forward passes (bare + 3 primed) + 5 scoring passes\")\n",
    "\n",
    "t_start = time.time()\n",
    "\n",
    "for qidx in tqdm(range(start_idx, N), initial=start_idx, total=N, desc=\"Queries\"):\n",
    "    query_data = queries[qidx]\n",
    "    query = query_data['query']\n",
    "    answer = query_data['answer']\n",
    "    query_prompt = QUERY_TEMPLATE.format(query=query)\n",
    "    alt_query_prompt = ALT_QUERY_TEMPLATE.format(query=query)\n",
    "    answer_text = ANSWER_TEMPLATE.format(answer=answer)\n",
    "\n",
    "    passage_results = []\n",
    "\n",
    "    for pidx, pinfo in enumerate(query_data['passages']):\n",
    "        passage = pinfo['passage']\n",
    "        document_text = DOCUMENT_TEMPLATE.format(document=passage)\n",
    "\n",
    "        # --- Matched tokenization (using sf prefix) ---\n",
    "        full_text = sf_str + document_text\n",
    "        full_enc = tokenizer(full_text, return_tensors=\"pt\",\n",
    "                              add_special_tokens=True, padding=False, truncation=False)\n",
    "        full_ids = full_enc['input_ids'].to(config.device)\n",
    "\n",
    "        sf_prefix_enc = tokenizer(sf_str, return_tensors=\"pt\",\n",
    "                                   add_special_tokens=True, padding=False, truncation=False)\n",
    "        sf_prefix_len_matched = sf_prefix_enc['input_ids'].shape[1]\n",
    "\n",
    "        bos_id = full_ids[:, :1]\n",
    "        doc_ids = full_ids[:, sf_prefix_len_matched:]\n",
    "        doc_len = doc_ids.shape[1]\n",
    "        context_len = 1 + doc_len  # BOS + doc\n",
    "\n",
    "        del full_enc, full_ids, sf_prefix_enc\n",
    "\n",
    "        # === 1. Build bare cache ===\n",
    "        bare_input = torch.cat([bos_id, doc_ids], dim=1)\n",
    "        with torch.no_grad():\n",
    "            bare_out = model(input_ids=bare_input,\n",
    "                             attention_mask=torch.ones_like(bare_input),\n",
    "                             use_cache=True, return_dict=True)\n",
    "        bare_cache = _ensure_dynamic_cache(bare_out.past_key_values)\n",
    "        del bare_out, bare_input\n",
    "\n",
    "        # === 2. Score rescore (deepcopy bare, alt prompt) ===\n",
    "        bare_copy = deepcopy_cache(bare_cache)\n",
    "        rescore_nll = score_answer_with_cache(\n",
    "            bare_copy, context_len, alt_query_prompt, answer_text,\n",
    "            model, tokenizer, config)\n",
    "        del bare_copy\n",
    "\n",
    "        # === 3-5. For each priming prefix: build, truncate, hybrid, score ===\n",
    "        primed_nlls = {}\n",
    "        for p_name, p_text, p_str, p_ids in PREFIX_CONFIGS:\n",
    "            primed_input = torch.cat([bos_id, p_ids, doc_ids], dim=1)\n",
    "            with torch.no_grad():\n",
    "                primed_out = model(input_ids=primed_input,\n",
    "                                   attention_mask=torch.ones_like(primed_input),\n",
    "                                   use_cache=True, return_dict=True)\n",
    "            primed_full = _ensure_dynamic_cache(primed_out.past_key_values)\n",
    "            del primed_out, primed_input\n",
    "\n",
    "            # Truncate + RoPE correct\n",
    "            primed_trunc = extract_and_truncate_cache_with_bos(primed_full, doc_len)\n",
    "            correct_rope_positions_with_bos(primed_trunc, p_ids.shape[1], model)\n",
    "            del primed_full\n",
    "\n",
    "            # Hybrid: bare keys + primed values (pure value contamination)\n",
    "            hybrid = build_hybrid_cache(bare_cache, primed_trunc)\n",
    "            del primed_trunc\n",
    "\n",
    "            primed_nlls[p_name] = score_answer_with_cache(\n",
    "                hybrid, context_len, query_prompt, answer_text,\n",
    "                model, tokenizer, config)\n",
    "            del hybrid\n",
    "\n",
    "        # === 6. Score bare LAST (mutates cache) ===\n",
    "        bare_nll = score_answer_with_cache(\n",
    "            bare_cache, context_len, query_prompt, answer_text,\n",
    "            model, tokenizer, config)\n",
    "        del bare_cache\n",
    "\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        passage_results.append({\n",
    "            'passage_idx': pinfo['passage_idx'],\n",
    "            'is_relevant': pinfo['is_relevant'],\n",
    "            'word_count': pinfo['word_count'],\n",
    "            'bare_nll': bare_nll,\n",
    "            'rescore_nll': rescore_nll,\n",
    "            'sf_nll': primed_nlls['sf'],\n",
    "            'rand_nll': primed_nlls['rand'],\n",
    "            'intent_nll': primed_nlls['intent'],\n",
    "        })\n",
    "\n",
    "    all_results.append({\n",
    "        'query_idx': qidx,\n",
    "        'query': query,\n",
    "        'n_passages': len(passage_results),\n",
    "        'n_relevant': query_data['n_relevant'],\n",
    "        'passage_data': passage_results,\n",
    "    })\n",
    "\n",
    "    # Checkpoint\n",
    "    if (qidx + 1) % CHECKPOINT_EVERY == 0 or qidx == N - 1:\n",
    "        ckpt_data = {\n",
    "            'results': all_results,\n",
    "            'query_texts': [q['query'] for q in queries],\n",
    "            'completed': len(all_results),\n",
    "            'total': N,\n",
    "            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        }\n",
    "        with open(CHECKPOINT_PATH, 'w') as f:\n",
    "            json.dump(ckpt_data, f)\n",
    "        elapsed = time.time() - t_start\n",
    "        n_done = qidx - start_idx + 1\n",
    "        rate = n_done / elapsed if elapsed > 0 else 0\n",
    "        remaining = (N - qidx - 1) / rate if rate > 0 else 0\n",
    "        tqdm.write(f\"  Checkpoint {qidx+1}/{N} | {n_done} done in {elapsed/60:.1f}m | \"\n",
    "                   f\"ETA: {remaining/60:.1f} min\")\n",
    "\n",
    "elapsed_total = time.time() - t_start\n",
    "print(f\"\\nEvaluation complete: {len(all_results)} queries in {elapsed_total/60:.1f} min\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 7: Analysis \u2014 individual signals, ensembles, significance, scaling\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "N_VALID = len(all_results)\n",
    "print(f\"Valid queries: {N_VALID}\")\n",
    "\n",
    "# --- Helper functions ---\n",
    "def mrr_for_signal(results, sig_name):\n",
    "    \"\"\"Compute per-query MRR ranking by a single NLL signal.\"\"\"\n",
    "    mrrs = []\n",
    "    for r in results:\n",
    "        pd = r['passage_data']\n",
    "        scores = {i: pd[i][f'{sig_name}_nll'] for i in range(len(pd))}\n",
    "        rel_idx = next(i for i, p in enumerate(pd) if p['is_relevant'])\n",
    "        m = compute_ranking_metrics(scores, relevant_idx=rel_idx)\n",
    "        mrrs.append(m['mrr'])\n",
    "    return np.array(mrrs)\n",
    "\n",
    "\n",
    "def mrr_for_ensemble(results, sig_names):\n",
    "    \"\"\"Compute per-query MRR ranking by equal-weight NLL average.\"\"\"\n",
    "    mrrs = []\n",
    "    for r in results:\n",
    "        pd = r['passage_data']\n",
    "        scores = {}\n",
    "        for i in range(len(pd)):\n",
    "            scores[i] = np.mean([pd[i][f'{s}_nll'] for s in sig_names])\n",
    "        rel_idx = next(i for i, p in enumerate(pd) if p['is_relevant'])\n",
    "        m = compute_ranking_metrics(scores, relevant_idx=rel_idx)\n",
    "        mrrs.append(m['mrr'])\n",
    "    return np.array(mrrs)\n",
    "\n",
    "\n",
    "def sig_test(mrrs_a, mrrs_b):\n",
    "    \"\"\"Wilcoxon signed-rank test, returns (delta, p, sig_str).\"\"\"\n",
    "    delta = float(np.mean(mrrs_a) - np.mean(mrrs_b))\n",
    "    nonzero = int(np.sum(mrrs_a != mrrs_b))\n",
    "    if nonzero > 10:\n",
    "        _, p = wilcoxon(mrrs_a, mrrs_b)\n",
    "    else:\n",
    "        p = 1.0\n",
    "    sig = \"***\" if p < 0.001 else \"**\" if p < 0.01 else \"*\" if p < 0.05 else \"ns\"\n",
    "    return delta, float(p), sig, nonzero\n",
    "\n",
    "\n",
    "# === 1. Individual signal MRR ===\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"INDIVIDUAL SIGNAL RANKING\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "individual_mrrs = {}\n",
    "for sig in SIGNAL_NAMES:\n",
    "    individual_mrrs[sig] = mrr_for_signal(all_results, sig)\n",
    "\n",
    "bare_mrrs = individual_mrrs['bare']\n",
    "print(f\"\\n{'Signal':<12} {'MRR':>8} {'\u0394MRR':>8} {'p':>12} {'Sig':>5} {'Changed':>8}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "individual_stats = {}\n",
    "for sig in SIGNAL_NAMES:\n",
    "    mrrs = individual_mrrs[sig]\n",
    "    if sig == 'bare':\n",
    "        print(f\"{sig:<12} {np.mean(mrrs):>8.4f} {'--':>8} {'--':>12} {'--':>5} {'--':>8}\")\n",
    "    else:\n",
    "        d, p, s_str, n_changed = sig_test(mrrs, bare_mrrs)\n",
    "        print(f\"{sig:<12} {np.mean(mrrs):>8.4f} {d:>+8.4f} {p:>12.3e} {s_str:>5} {n_changed:>8}\")\n",
    "        individual_stats[sig] = {'delta_mrr': d, 'p_value': p, 'significant': bool(p < 0.05)}\n",
    "\n",
    "\n",
    "# === 2. Ensemble MRR ===\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ENSEMBLE RANKING (EQUAL-WEIGHT NLL AVERAGE)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "ENSEMBLE_CONFIGS = {\n",
    "    'ens_2_sf':      ['bare', 'sf'],\n",
    "    'ens_2_rand':    ['bare', 'rand'],\n",
    "    'ens_2_intent':  ['bare', 'intent'],\n",
    "    'ens_2_rescore': ['bare', 'rescore'],\n",
    "    'ens_3':         ['bare', 'sf', 'rand'],\n",
    "    'ens_4':         ['bare', 'sf', 'rand', 'intent'],\n",
    "    'ens_5_all':     ['bare', 'sf', 'rand', 'intent', 'rescore'],\n",
    "}\n",
    "\n",
    "ensemble_mrrs = {}\n",
    "ensemble_stats = {}\n",
    "print(f\"\\n{'Ensemble':<20} {'Members':>4} {'MRR':>8} {'\u0394MRR':>8} {'p':>12} {'Sig':>5} {'Changed':>8}\")\n",
    "print(\"-\" * 72)\n",
    "\n",
    "for ens_name, members in ENSEMBLE_CONFIGS.items():\n",
    "    mrrs = mrr_for_ensemble(all_results, members)\n",
    "    ensemble_mrrs[ens_name] = mrrs\n",
    "    d, p, s_str, n_changed = sig_test(mrrs, bare_mrrs)\n",
    "    print(f\"{ens_name:<20} {len(members):>4} {np.mean(mrrs):>8.4f} {d:>+8.4f} \"\n",
    "          f\"{p:>12.3e} {s_str:>5} {n_changed:>8}\")\n",
    "    ensemble_stats[ens_name] = {\n",
    "        'members': members,\n",
    "        'mrr_mean': float(np.mean(mrrs)),\n",
    "        'delta_mrr': d,\n",
    "        'p_value': p,\n",
    "        'significant': bool(p < 0.05),\n",
    "        'n_changed': n_changed,\n",
    "    }\n",
    "\n",
    "\n",
    "# === 3. Critical comparison: priming vs non-priming control ===\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"CRITICAL: PRIMING vs NON-PRIMING CONTROL\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "sf_mrr = float(np.mean(ensemble_mrrs['ens_2_sf']))\n",
    "rescore_mrr = float(np.mean(ensemble_mrrs['ens_2_rescore']))\n",
    "d_sf_res, p_sf_res, s_sf_res, n_sf_res = sig_test(\n",
    "    ensemble_mrrs['ens_2_sf'], ensemble_mrrs['ens_2_rescore'])\n",
    "\n",
    "print(f\"  ens_2_sf (priming):      MRR={sf_mrr:.4f}\")\n",
    "print(f\"  ens_2_rescore (control): MRR={rescore_mrr:.4f}\")\n",
    "print(f\"  Difference:              {d_sf_res:+.4f}  (p={p_sf_res:.3e}, {s_sf_res})\")\n",
    "if sf_mrr > rescore_mrr + 0.001:\n",
    "    print(\"  => Priming adds value BEYOND prompt diversity\")\n",
    "elif rescore_mrr > sf_mrr + 0.001:\n",
    "    print(\"  => Prompt diversity alone BEATS priming\")\n",
    "else:\n",
    "    print(\"  => Priming and prompt diversity are equivalent\")\n",
    "\n",
    "\n",
    "# === 4. Greedy forward selection (scaling curve) ===\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"GREEDY SCALING CURVE: best member to add at each step\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "available = ['rescore', 'sf', 'rand', 'intent']\n",
    "selected = ['bare']\n",
    "greedy_results = [{'members': list(selected), 'mrr': float(np.mean(bare_mrrs))}]\n",
    "\n",
    "for step in range(len(available)):\n",
    "    best_next = None\n",
    "    best_mrr = -1\n",
    "    for candidate in available:\n",
    "        trial = selected + [candidate]\n",
    "        trial_mrrs = mrr_for_ensemble(all_results, trial)\n",
    "        mean_mrr = float(np.mean(trial_mrrs))\n",
    "        if mean_mrr > best_mrr:\n",
    "            best_mrr = mean_mrr\n",
    "            best_next = candidate\n",
    "    selected.append(best_next)\n",
    "    available.remove(best_next)\n",
    "    greedy_results.append({'members': list(selected), 'mrr': best_mrr})\n",
    "\n",
    "print(f\"\\n{'K':<4} {'Added':>10} {'Ensemble':<35} {'MRR':>8} {'\u0394MRR':>8}\")\n",
    "print(\"-\" * 70)\n",
    "for i, gr in enumerate(greedy_results):\n",
    "    added = gr['members'][-1] if i > 0 else '--'\n",
    "    delta = gr['mrr'] - greedy_results[0]['mrr']\n",
    "    members_str = '+'.join(gr['members'])\n",
    "    print(f\"{i+1:<4} {added:>10} {members_str:<35} {gr['mrr']:>8.4f} {delta:>+8.4f}\")\n",
    "\n",
    "\n",
    "# === 5. NLL correlation matrix ===\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"NLL CORRELATION MATRIX (Pearson, across all passages)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "all_nlls = {sig: [] for sig in SIGNAL_NAMES}\n",
    "for r in all_results:\n",
    "    for p in r['passage_data']:\n",
    "        for sig in SIGNAL_NAMES:\n",
    "            all_nlls[sig].append(p[f'{sig}_nll'])\n",
    "\n",
    "all_nlls = {sig: np.array(vals) for sig, vals in all_nlls.items()}\n",
    "\n",
    "print(f\"\\n{'':>12}\", end='')\n",
    "for sig in SIGNAL_NAMES:\n",
    "    print(f\" {sig:>10}\", end='')\n",
    "print()\n",
    "\n",
    "corr_matrix = {}\n",
    "for sig_a in SIGNAL_NAMES:\n",
    "    print(f\"{sig_a:>12}\", end='')\n",
    "    for sig_b in SIGNAL_NAMES:\n",
    "        r, _ = stats.pearsonr(all_nlls[sig_a], all_nlls[sig_b])\n",
    "        corr_matrix[f'{sig_a}_{sig_b}'] = float(r)\n",
    "        print(f\" {r:>10.4f}\", end='')\n",
    "    print()\n",
    "\n",
    "print(\"\\nNote: Lower correlation = more diversity = better ensembles\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 8: Plots (4-panel figure)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "colors_ens = {\n",
    "    'ens_2_sf': '#d62728',\n",
    "    'ens_2_rand': '#ff7f0e',\n",
    "    'ens_2_intent': '#9467bd',\n",
    "    'ens_2_rescore': '#2ca02c',\n",
    "    'ens_3': '#1f77b4',\n",
    "    'ens_4': '#e377c2',\n",
    "    'ens_5_all': '#17becf',\n",
    "}\n",
    "\n",
    "# --- Plot 1: Ensemble MRR bar chart ---\n",
    "ax = axes[0, 0]\n",
    "names = ['bare'] + list(ENSEMBLE_CONFIGS.keys())\n",
    "mrr_vals = [float(np.mean(bare_mrrs))] + [float(np.mean(ensemble_mrrs[e])) for e in ENSEMBLE_CONFIGS]\n",
    "bar_colors = ['#7f7f7f'] + [colors_ens.get(e, '#333') for e in ENSEMBLE_CONFIGS]\n",
    "bars = ax.bar(range(len(names)), mrr_vals, color=bar_colors, edgecolor='black', linewidth=0.5)\n",
    "for i, (n, m) in enumerate(zip(names, mrr_vals)):\n",
    "    ax.text(i, m + 0.002, f\"{m:.4f}\", ha='center', fontsize=7, rotation=45)\n",
    "ax.set_xticks(range(len(names)))\n",
    "ax.set_xticklabels(names, rotation=45, ha='right', fontsize=7)\n",
    "ax.set_ylabel(\"MRR\")\n",
    "ax.set_title(\"MRR by Condition\")\n",
    "ax.axhline(y=float(np.mean(bare_mrrs)), color='gray', linestyle='--', alpha=0.5, label='bare')\n",
    "ax.legend(fontsize=8)\n",
    "\n",
    "# --- Plot 2: Scaling curve ---\n",
    "ax = axes[0, 1]\n",
    "k_vals = list(range(1, len(greedy_results) + 1))\n",
    "mrr_curve = [gr['mrr'] for gr in greedy_results]\n",
    "ax.plot(k_vals, mrr_curve, 'o-', color='#1f77b4', linewidth=2, markersize=8)\n",
    "for i, gr in enumerate(greedy_results):\n",
    "    label = gr['members'][-1] if i > 0 else 'bare'\n",
    "    ax.annotate(f\"+{label}\" if i > 0 else label,\n",
    "                (k_vals[i], mrr_curve[i]),\n",
    "                textcoords=\"offset points\", xytext=(5, 8), fontsize=8)\n",
    "ax.axhline(y=float(np.mean(bare_mrrs)), color='gray', linestyle='--', alpha=0.5)\n",
    "ax.set_xlabel(\"Ensemble Size K\")\n",
    "ax.set_ylabel(\"MRR\")\n",
    "ax.set_title(\"Greedy Scaling Curve\")\n",
    "ax.set_xticks(k_vals)\n",
    "\n",
    "# --- Plot 3: Correlation heatmap ---\n",
    "ax = axes[1, 0]\n",
    "corr_data = np.zeros((len(SIGNAL_NAMES), len(SIGNAL_NAMES)))\n",
    "for i, sa in enumerate(SIGNAL_NAMES):\n",
    "    for j, sb in enumerate(SIGNAL_NAMES):\n",
    "        corr_data[i, j] = corr_matrix[f'{sa}_{sb}']\n",
    "im = ax.imshow(corr_data, vmin=0.9, vmax=1.0, cmap='YlOrRd', aspect='auto')\n",
    "ax.set_xticks(range(len(SIGNAL_NAMES)))\n",
    "ax.set_xticklabels(SIGNAL_NAMES, fontsize=8)\n",
    "ax.set_yticks(range(len(SIGNAL_NAMES)))\n",
    "ax.set_yticklabels(SIGNAL_NAMES, fontsize=8)\n",
    "for i in range(len(SIGNAL_NAMES)):\n",
    "    for j in range(len(SIGNAL_NAMES)):\n",
    "        ax.text(j, i, f\"{corr_data[i,j]:.3f}\", ha='center', va='center', fontsize=7)\n",
    "plt.colorbar(im, ax=ax, label=\"Pearson r\")\n",
    "ax.set_title(\"NLL Correlation Matrix\")\n",
    "\n",
    "# --- Plot 4: Per-query \u0394MRR distributions ---\n",
    "ax = axes[1, 1]\n",
    "for ens_name in ['ens_2_sf', 'ens_2_rescore', 'ens_4']:\n",
    "    deltas = ensemble_mrrs[ens_name] - bare_mrrs\n",
    "    ax.hist(deltas, bins=30, alpha=0.5, label=ens_name,\n",
    "            color=colors_ens.get(ens_name, 'gray'))\n",
    "ax.axvline(x=0, color='gray', linestyle='--', alpha=0.5)\n",
    "ax.set_xlabel(\"\u0394MRR (ensemble - bare)\")\n",
    "ax.set_ylabel(\"Count\")\n",
    "ax.set_title(\"Per-Query MRR Change Distribution\")\n",
    "ax.legend(fontsize=8)\n",
    "\n",
    "plt.suptitle('Exp 15: NLL Ensemble Ranking', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'analysis_plots.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"Plots saved to {RESULTS_DIR / 'analysis_plots.png'}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 9: Save results JSON\n",
    "final = {\n",
    "    'experiment': 'exp15_nll_ensemble_ranking',\n",
    "    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'config': {\n",
    "        'model_name': config.model_name,\n",
    "        'seed': SEED,\n",
    "        'n_queries': N,\n",
    "        'n_valid': N_VALID,\n",
    "        'max_passage_words': MAX_PASSAGE_WORDS,\n",
    "        'min_passages_per_query': MIN_PASSAGES_PER_QUERY,\n",
    "        'dataset': 'MS MARCO v1.1 validation',\n",
    "        'prefixes': {\n",
    "            'sf': STATIC_FACT,\n",
    "            'rand': RANDOM_PREFIX_TEXT,\n",
    "            'intent': INTENT_PREFIX_TEXT,\n",
    "        },\n",
    "        'alt_query_template': ALT_QUERY_TEMPLATE,\n",
    "    },\n",
    "    'signal_names': SIGNAL_NAMES,\n",
    "    'individual_mrrs': {sig: float(np.mean(individual_mrrs[sig])) for sig in SIGNAL_NAMES},\n",
    "    'individual_stats': individual_stats,\n",
    "    'ensemble_configs': {k: v for k, v in ENSEMBLE_CONFIGS.items()},\n",
    "    'ensemble_stats': ensemble_stats,\n",
    "    'priming_vs_control': {\n",
    "        'ens_2_sf_mrr': sf_mrr,\n",
    "        'ens_2_rescore_mrr': rescore_mrr,\n",
    "        'difference': float(d_sf_res),\n",
    "        'p_value': float(p_sf_res),\n",
    "        'priming_is_special': bool(sf_mrr > rescore_mrr + 0.001),\n",
    "    },\n",
    "    'greedy_scaling': greedy_results,\n",
    "    'correlation_matrix': corr_matrix,\n",
    "    'per_query_results': [\n",
    "        {k: v for k, v in r.items() if k != 'passage_data'}\n",
    "        for r in all_results\n",
    "    ],\n",
    "}\n",
    "\n",
    "with open(FINAL_RESULTS_PATH, 'w') as f:\n",
    "    json.dump(final, f, indent=2)\n",
    "\n",
    "print(f\"Results saved to {FINAL_RESULTS_PATH}\")\n",
    "print(f\"File size: {FINAL_RESULTS_PATH.stat().st_size / 1024:.1f} KB\")\n",
    "\n",
    "# Print summary\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Bare MRR:           {float(np.mean(bare_mrrs)):.4f}\")\n",
    "best_ens = max(ensemble_stats.items(), key=lambda x: x[1]['mrr_mean'])\n",
    "print(f\"Best ensemble:      {best_ens[0]} (MRR={best_ens[1]['mrr_mean']:.4f}, \"\n",
    "      f\"\u0394MRR={best_ens[1]['delta_mrr']:+.4f}, p={best_ens[1]['p_value']:.3e})\")\n",
    "print(f\"Priming vs control: {d_sf_res:+.4f} (p={p_sf_res:.3e})\")\n",
    "print(f\"Scaling saturates:  K={len(greedy_results[-1]['members'])} members, \"\n",
    "      f\"MRR={greedy_results[-1]['mrr']:.4f}\")\n",
    "print(\"\\nDone!\")"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}