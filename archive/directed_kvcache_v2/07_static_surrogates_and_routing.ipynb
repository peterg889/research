{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6e04ed2",
   "metadata": {},
   "source": [
    "# Exp 07: Static Surrogates, Dual-Mode Priming, Intent Routing\n",
    "\n",
    "## Motivation\n",
    "\n",
    "Exp 06 revealed that LLM surrogates significantly improve KV cache quality (d=0.23-0.30 vs\n",
    "bare), but the mechanism is NOT token overlap (r=-0.024). Separator-only (a suffix with no\n",
    "content) nearly matches LLM-keyword (d=0.231 vs 0.234). The hardness gradient is massive:\n",
    "LLM-keyword+sep goes from d=-0.226 (Q1 easiest) to d=+0.630 (Q5 hardest).\n",
    "\n",
    "## Core Question\n",
    "\n",
    "Can cheap static surrogates match LLM performance? Does the optimal strategy depend on query\n",
    "intent, answer length, or passage length?\n",
    "\n",
    "## Self-contained\n",
    "\n",
    "This experiment generates its own LLM surrogates and computes its own bare NLLs. No reuse of\n",
    "results across experiments.\n",
    "\n",
    "## 21 Conditions\n",
    "\n",
    "| # | Condition | Type |\n",
    "|---|-----------|------|\n",
    "| 1 | Bare | Baseline |\n",
    "| 2 | Random-truncated | Control |\n",
    "| 3 | Separator-only | Control |\n",
    "| 4-8 | Static-{definitional,procedural,quantitative,factual,problem}-trunc | Static prefix |\n",
    "| 9-13 | Static-{definitional,procedural,quantitative,factual,problem}-suffix | Static suffix |\n",
    "| 14-17 | LLM-{keyword,symptom,question,messy}-suffix | LLM suffix |\n",
    "| 18 | LLM-keyword-trunc | LLM prefix |\n",
    "| 19 | LLM-keyword+sep | Stacking |\n",
    "| 20 | LLM-keyword-full-context | Full context |\n",
    "| 21 | Novel-generic-trunc | Novel static prefix |\n",
    "\n",
    "## 10 Primary Comparisons (Bonferroni alpha = 0.005)\n",
    "\n",
    "| # | Comparison | Question |\n",
    "|---|-----------|----------|\n",
    "| C1 | Best-static-trunc vs Bare | Do static prefixes help? |\n",
    "| C2 | Best-static-suffix vs Bare | Do static suffixes help? |\n",
    "| C3 | Best-static-trunc vs Best-static-suffix | Which mode? |\n",
    "| C4 | Best-static vs LLM-keyword-trunc | Can statics match LLM? |\n",
    "| C5 | Oracle-routed-static-K5 vs Best-single-static | Routing help? |\n",
    "| C6 | Oracle-routed-static-K5 vs LLM-keyword-suffix | Routed statics vs LLM? |\n",
    "| C7 | LLM-keyword-suffix vs LLM-keyword-trunc | Suffix vs truncation? |\n",
    "| C8 | LLM-keyword-full-context vs LLM-keyword-trunc | Full-ctx vs truncated? |\n",
    "| C9 | LLM-keyword+sep vs LLM-keyword-suffix | Stacking replicates? |\n",
    "| C10 | Embedding-routed-LLM-K4 vs Oracle-routed-LLM-K4 | Practical routing? |\n",
    "\n",
    "## Analysis Dimensions\n",
    "\n",
    "1. Query intent (7 categories)\n",
    "2. Answer length: short (<5 tokens), medium (5-15), long (>15)\n",
    "3. Passage length: short (<80 words), medium (80-200), long (>200)\n",
    "4. Difficulty quintile (bare NLL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598b9946",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Setup — permissions, seeds, results directory\n",
    "import os\n",
    "os.umask(0o000)\n",
    "\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "import numpy as np\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "RESULTS_DIR = Path(\"results/exp07\")\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "SURROGATES_DIR = RESULTS_DIR / \"surrogates\"\n",
    "SURROGATES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CHECKPOINT_PATH = RESULTS_DIR / \"checkpoint.json\"\n",
    "FINAL_RESULTS_PATH = RESULTS_DIR / \"results.json\"\n",
    "\n",
    "print(f\"SEED: {SEED}\")\n",
    "print(f\"Results directory: {RESULTS_DIR}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67fa0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Load model (Mistral-7B 4-bit)\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "print(f\"Loading {MODEL_NAME} (4-bit)...\")\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "print(f\"Model loaded. dtype={model.dtype}, device={model.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed32c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Imports + config + templates + helpers\n",
    "sys.path.insert(0, \".\")\n",
    "\n",
    "from lib.config import ExperimentConfig\n",
    "from lib.kv_cache import (\n",
    "    build_kv_cache,\n",
    "    build_suffix_kv_cache,\n",
    "    score_answer_with_cache,\n",
    "    deepcopy_cache,\n",
    "    extract_and_truncate_cache_with_bos,\n",
    "    correct_rope_positions_with_bos,\n",
    "    build_hybrid_cache,\n",
    "    _get_cache_keys,\n",
    "    _get_cache_values,\n",
    "    _set_cache_keys,\n",
    "    _set_cache_values,\n",
    ")\n",
    "from lib.data import load_ms_marco, load_evaluation_samples\n",
    "from lib.analysis import cohens_d, compute_token_overlap\n",
    "from lib.surrogate import (\n",
    "    generate_all_5_surrogates,\n",
    "    STATIC_SURROGATE_QUERIES,\n",
    "    TOP_5_SURROGATE_TEMPLATES,\n",
    ")\n",
    "from scipy import stats\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "config = ExperimentConfig(\n",
    "    model_name=MODEL_NAME,\n",
    "    num_samples=4000,\n",
    "    min_passage_words=20,\n",
    "    max_passage_words=500,\n",
    "    seed=SEED,\n",
    ")\n",
    "\n",
    "# Templates — bare text, no \"Document:\\n\" framing (hurts NLL, d=-0.45)\n",
    "SURROGATE_PREFIX_TEMPLATE = \"{surrogate}\\n\"\n",
    "DOCUMENT_TEMPLATE = \"{document}\"\n",
    "QUERY_TEMPLATE = \"\\nQuery: {query}\\nAnswer:\"\n",
    "ANSWER_TEMPLATE = \" {answer}\"\n",
    "SUFFIX_SEPARATOR = \"\\n\\nRelated question: \"\n",
    "CHECKPOINT_EVERY = 50\n",
    "N_COMPARISONS = 10\n",
    "BONFERRONI_ALPHA = 0.05 / N_COMPARISONS\n",
    "N_EVAL = 2000\n",
    "\n",
    "# Static surrogate phrases (from lib)\n",
    "STATIC_PHRASES = {\n",
    "    'definitional': STATIC_SURROGATE_QUERIES['static_definitional']['query'],\n",
    "    'procedural': STATIC_SURROGATE_QUERIES['static_procedural']['query'],\n",
    "    'quantitative': STATIC_SURROGATE_QUERIES['static_quantitative']['query'],\n",
    "    'factual': STATIC_SURROGATE_QUERIES['static_factual']['query'],\n",
    "    'problem': STATIC_SURROGATE_QUERIES['static_problem']['query'],\n",
    "}\n",
    "\n",
    "# Novel generic phrase for condition 21\n",
    "NOVEL_GENERIC_PHRASE = \"What is this page about?\"\n",
    "\n",
    "\n",
    "def generate_random_prefix_text(target_text, tokenizer, seed):\n",
    "    target_ids = tokenizer.encode(target_text, add_special_tokens=False)\n",
    "    target_len = len(target_ids)\n",
    "    if target_len == 0:\n",
    "        return \"\"\n",
    "    rng = np.random.RandomState(seed)\n",
    "    vocab_size = len(tokenizer)\n",
    "    min_id = 3\n",
    "    random_ids = rng.randint(min_id, vocab_size, size=target_len)\n",
    "    random_text = tokenizer.decode(random_ids.tolist(), skip_special_tokens=True)\n",
    "    reencoded = tokenizer.encode(random_text, add_special_tokens=False)\n",
    "    if len(reencoded) != target_len:\n",
    "        if len(reencoded) > target_len:\n",
    "            random_text = tokenizer.decode(reencoded[:target_len], skip_special_tokens=True)\n",
    "        else:\n",
    "            extra_needed = target_len - len(reencoded)\n",
    "            extra_ids = rng.randint(min_id, vocab_size, size=extra_needed)\n",
    "            extra_text = tokenizer.decode(extra_ids.tolist(), skip_special_tokens=True)\n",
    "            random_text = random_text + extra_text\n",
    "            reencoded2 = tokenizer.encode(random_text, add_special_tokens=False)\n",
    "            if len(reencoded2) > target_len:\n",
    "                random_text = tokenizer.decode(reencoded2[:target_len], skip_special_tokens=True)\n",
    "    return random_text\n",
    "\n",
    "\n",
    "# Intent classification (rule-based)\n",
    "INTENT_RULES = {\n",
    "    'definitional': ['what is', 'define', 'meaning of', 'explain', 'what does', 'what are'],\n",
    "    'procedural': ['how to', 'how do', 'steps', 'tutorial', 'guide', 'instructions'],\n",
    "    'transactional': ['buy', 'cost', 'price', 'cheap', 'deal', 'order', 'purchase'],\n",
    "    'comparison': ['best', 'top', 'vs', 'compare', 'review', 'difference between'],\n",
    "    'factual': ['when did', 'where is', 'who', 'how many', 'how much', 'how long'],\n",
    "    'medical': ['symptoms', 'treatment', 'diagnosis', 'causes', 'cure', 'side effects'],\n",
    "}\n",
    "\n",
    "\n",
    "def classify_intent(query):\n",
    "    q = query.lower().strip()\n",
    "    for intent, patterns in INTENT_RULES.items():\n",
    "        for pattern in patterns:\n",
    "            if pattern in q:\n",
    "                return intent\n",
    "    return 'other'\n",
    "\n",
    "\n",
    "print(\"Config ready\")\n",
    "print(f\"  num_samples pool: {config.num_samples}\")\n",
    "print(f\"  eval samples: {N_EVAL}\")\n",
    "print(f\"  passage words: {config.min_passage_words}-{config.max_passage_words}\")\n",
    "print(f\"  bonferroni_alpha: {BONFERRONI_ALPHA:.4f} ({N_COMPARISONS} comparisons)\")\n",
    "print(f\"  conditions: 21\")\n",
    "print(f\"  static phrases: {list(STATIC_PHRASES.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462c1104",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Load MS MARCO (2000 samples, full distribution)\n",
    "dataset = load_ms_marco(config)\n",
    "\n",
    "np.random.seed(SEED)\n",
    "all_samples = load_evaluation_samples(dataset, config, require_answer=True)\n",
    "\n",
    "samples = all_samples[:N_EVAL]\n",
    "N = len(samples)\n",
    "print(f\"Loaded {len(all_samples)} candidates, using first {N} for evaluation\")\n",
    "print(f\"Example passage ({len(samples[0]['passage'].split())} words): {samples[0]['passage'][:100]}...\")\n",
    "print(f\"Example query: {samples[0]['query']}\")\n",
    "print(f\"Example answer: {samples[0]['answer']}\")\n",
    "\n",
    "# Classify all queries by intent\n",
    "intents = [classify_intent(s['query']) for s in samples]\n",
    "intent_counts = Counter(intents)\n",
    "print(f\"\\nIntent distribution:\")\n",
    "for intent, count in sorted(intent_counts.items(), key=lambda x: -x[1]):\n",
    "    print(f\"  {intent}: {count} ({count/N*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395484b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Generate ALL LLM surrogates (5 templates via generate_all_5_surrogates())\n",
    "# We need: keyword_query, target_question, symptom_scenario, messy_realworld\n",
    "# (misconception_negative not used in Exp 07 but generated anyway for consistency)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PHASE 1: LLM SURROGATE GENERATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "surrogates_5_path = SURROGATES_DIR / \"all_5_surrogates.json\"\n",
    "\n",
    "if surrogates_5_path.exists():\n",
    "    with open(surrogates_5_path, 'r') as f:\n",
    "        surrogates_5_data = json.load(f)\n",
    "    surrogates_5 = surrogates_5_data['surrogates']\n",
    "    print(f\"Loaded {len(surrogates_5)} sets of 5-template surrogates from cache\")\n",
    "else:\n",
    "    surrogates_5 = []\n",
    "\n",
    "start_5 = len(surrogates_5)\n",
    "if start_5 < N:\n",
    "    print(f\"Generating 5-template surrogates for samples {start_5} to {N-1}...\")\n",
    "    t_start = time.time()\n",
    "    for idx in tqdm(range(start_5, N), initial=start_5, total=N,\n",
    "                     desc=\"5-template surrogates\"):\n",
    "        passage = samples[idx]['passage']\n",
    "        try:\n",
    "            s5 = generate_all_5_surrogates(passage, model, tokenizer, config)\n",
    "        except Exception as e:\n",
    "            print(f\"  WARNING: 5-template generation failed for sample {idx}: {e}\")\n",
    "            s5 = {k: \"\" for k in TOP_5_SURROGATE_TEMPLATES.keys()}\n",
    "        surrogates_5.append(s5)\n",
    "\n",
    "        if (idx + 1) % 100 == 0 or idx == N - 1:\n",
    "            with open(surrogates_5_path, 'w') as f:\n",
    "                json.dump({'surrogates': surrogates_5}, f)\n",
    "            elapsed = time.time() - t_start\n",
    "            rate = (idx - start_5 + 1) / elapsed if elapsed > 0 else 0\n",
    "            remaining = (N - idx - 1) / rate if rate > 0 else 0\n",
    "            tqdm.write(f\"  Saved {idx+1}/{N} | {rate:.2f} s/s | ETA: {remaining/60:.1f} min\")\n",
    "\n",
    "    with open(surrogates_5_path, 'w') as f:\n",
    "        json.dump({'surrogates': surrogates_5}, f)\n",
    "    print(f\"5-template surrogates complete: {len(surrogates_5)} samples\")\n",
    "else:\n",
    "    print(f\"All 5-template surrogates already cached ({len(surrogates_5)} samples)\")\n",
    "\n",
    "# Validate\n",
    "n_empty_kw = sum(1 for s in surrogates_5 if not s.get('keyword_query', '').strip())\n",
    "n_empty_q = sum(1 for s in surrogates_5 if not s.get('target_question', '').strip())\n",
    "n_empty_symp = sum(1 for s in surrogates_5 if not s.get('symptom_scenario', '').strip())\n",
    "n_empty_messy = sum(1 for s in surrogates_5 if not s.get('messy_realworld', '').strip())\n",
    "print(f\"\\nValidation:\")\n",
    "print(f\"  Empty keyword: {n_empty_kw}/{N}\")\n",
    "print(f\"  Empty question: {n_empty_q}/{N}\")\n",
    "print(f\"  Empty symptom: {n_empty_symp}/{N}\")\n",
    "print(f\"  Empty messy: {n_empty_messy}/{N}\")\n",
    "\n",
    "print(f\"\\nExamples (sample 0):\")\n",
    "print(f\"  Passage: {samples[0]['passage'][:80]}...\")\n",
    "for k, v in surrogates_5[0].items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1203a5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Condition explanation with concrete examples\n",
    "print(\"=\" * 70)\n",
    "print(\"EXPERIMENTAL CONDITIONS EXPLAINED\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "ex_i = 0\n",
    "ex_passage = samples[ex_i]['passage']\n",
    "ex_query = samples[ex_i]['query']\n",
    "ex_llm_kw = surrogates_5[ex_i].get('keyword_query', '')\n",
    "\n",
    "conditions_explained = [\n",
    "    (\"1. Bare\", \"[BOS][doc]\", \"No prefix — baseline\"),\n",
    "    (\"2. Random-truncated\", \"[BOS][random\\\\n][doc] → truncate + RoPE\",\n",
    "     f\"Random text matching query token length\"),\n",
    "    (\"3. Separator-only\", \"[BOS][doc][\\\\n\\\\nRelated question: ]\",\n",
    "     \"Suffix framing only — no content after separator\"),\n",
    "]\n",
    "for name, phrase in STATIC_PHRASES.items():\n",
    "    i = list(STATIC_PHRASES.keys()).index(name)\n",
    "    conditions_explained.append(\n",
    "        (f\"{4+i}. Static-{name}-trunc\", f\"[BOS]['{phrase}'\\\\n][doc] → truncate + RoPE\",\n",
    "         f\"Static phrase as prefix\")\n",
    "    )\n",
    "for name, phrase in STATIC_PHRASES.items():\n",
    "    i = list(STATIC_PHRASES.keys()).index(name)\n",
    "    conditions_explained.append(\n",
    "        (f\"{9+i}. Static-{name}-suffix\", f\"[BOS][doc][\\\\n\\\\nRelated question: {phrase}]\",\n",
    "         f\"Static phrase as suffix\")\n",
    "    )\n",
    "conditions_explained += [\n",
    "    (\"14. LLM-keyword-suffix\", \"[BOS][doc][\\\\n\\\\nRelated question: llm_kw]\",\n",
    "     f\"LLM keyword: '{ex_llm_kw[:60]}'\"),\n",
    "    (\"15. LLM-symptom-suffix\", \"[BOS][doc][\\\\n\\\\nRelated question: llm_symp]\",\n",
    "     \"LLM symptom scenario as suffix\"),\n",
    "    (\"16. LLM-question-suffix\", \"[BOS][doc][\\\\n\\\\nRelated question: llm_q]\",\n",
    "     \"LLM question as suffix\"),\n",
    "    (\"17. LLM-messy-suffix\", \"[BOS][doc][\\\\n\\\\nRelated question: llm_messy]\",\n",
    "     \"LLM messy/informal as suffix\"),\n",
    "    (\"18. LLM-keyword-trunc\", \"[BOS][llm_kw\\\\n][doc] → truncate + RoPE\",\n",
    "     \"LLM keyword as truncated prefix\"),\n",
    "    (\"19. LLM-keyword+sep\", \"[BOS][llm_kw\\\\n][doc][\\\\n\\\\nRelated question: ] (prefix+suffix)\",\n",
    "     \"Stacking: truncated prefix + suffix separator (Exp 06 best)\"),\n",
    "    (\"20. LLM-keyword-full-context\", \"[BOS][llm_kw\\\\n][doc] — NOT truncated\",\n",
    "     \"Full context: query sees prefix + doc\"),\n",
    "    (\"21. Novel-generic-trunc\", f\"[BOS]['{NOVEL_GENERIC_PHRASE}'\\\\n][doc] → truncate + RoPE\",\n",
    "     \"Ultra-cheap novel generic prefix\"),\n",
    "]\n",
    "\n",
    "for name, pattern, detail in conditions_explained:\n",
    "    print(f\"\\n### {name} ###\")\n",
    "    print(f\"  Cache: {pattern}\")\n",
    "    print(f\"  Detail: {detail}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c6510e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Main eval loop — 21 conditions × 2000 samples\n",
    "print(\"=\" * 70)\n",
    "print(\"PHASE 2: MAIN EVALUATION (21 conditions × 2000 samples)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "CONDITION_NAMES = [\n",
    "    'bare', 'random_trunc', 'separator_only',\n",
    "    'static_def_trunc', 'static_proc_trunc', 'static_quant_trunc',\n",
    "    'static_fact_trunc', 'static_prob_trunc',\n",
    "    'static_def_suffix', 'static_proc_suffix', 'static_quant_suffix',\n",
    "    'static_fact_suffix', 'static_prob_suffix',\n",
    "    'llm_keyword_suffix', 'llm_symptom_suffix', 'llm_question_suffix', 'llm_messy_suffix',\n",
    "    'llm_keyword_trunc', 'llm_keyword_sep', 'llm_keyword_full_ctx',\n",
    "    'novel_generic_trunc',\n",
    "]\n",
    "\n",
    "STATIC_TRUNC_CONDS = ['static_def_trunc', 'static_proc_trunc', 'static_quant_trunc',\n",
    "                       'static_fact_trunc', 'static_prob_trunc']\n",
    "STATIC_SUFFIX_CONDS = ['static_def_suffix', 'static_proc_suffix', 'static_quant_suffix',\n",
    "                        'static_fact_suffix', 'static_prob_suffix']\n",
    "LLM_SUFFIX_CONDS = ['llm_keyword_suffix', 'llm_symptom_suffix', 'llm_question_suffix',\n",
    "                      'llm_messy_suffix']\n",
    "\n",
    "static_phrase_list = list(STATIC_PHRASES.values())\n",
    "static_name_list = list(STATIC_PHRASES.keys())\n",
    "\n",
    "results = []\n",
    "start_idx = 0\n",
    "\n",
    "if CHECKPOINT_PATH.exists():\n",
    "    with open(CHECKPOINT_PATH, 'r') as f:\n",
    "        ckpt = json.load(f)\n",
    "    ckpt_queries = ckpt.get('sample_queries', [])\n",
    "    current_queries = [s['query'] for s in samples]\n",
    "    if ckpt_queries == current_queries:\n",
    "        results = ckpt['results']\n",
    "        start_idx = len(results)\n",
    "        print(f\"Resuming from checkpoint: {start_idx}/{N}\")\n",
    "    else:\n",
    "        print(\"Checkpoint sample mismatch. Starting fresh.\")\n",
    "else:\n",
    "    print(\"No checkpoint found. Starting fresh.\")\n",
    "\n",
    "print(f\"Evaluating samples {start_idx} to {N-1}\")\n",
    "print(f\"Conditions: {len(CONDITION_NAMES)}\")\n",
    "\n",
    "t_start = time.time()\n",
    "\n",
    "for idx in tqdm(range(start_idx, N), initial=start_idx, total=N, desc=\"Evaluating\"):\n",
    "    sample = samples[idx]\n",
    "    passage = sample['passage']\n",
    "    query = sample['query']\n",
    "    answer = sample['answer']\n",
    "\n",
    "    query_prompt = QUERY_TEMPLATE.format(query=query)\n",
    "    answer_text = ANSWER_TEMPLATE.format(answer=answer)\n",
    "\n",
    "    # --- Matched tokenization (same doc_ids for ALL truncated conditions) ---\n",
    "    oracle_prefix = SURROGATE_PREFIX_TEMPLATE.format(surrogate=query)\n",
    "    document_text = DOCUMENT_TEMPLATE.format(document=passage)\n",
    "    full_oracle_text = oracle_prefix + document_text\n",
    "\n",
    "    full_oracle_enc = tokenizer(full_oracle_text, return_tensors=\"pt\",\n",
    "                                add_special_tokens=True, padding=False, truncation=False)\n",
    "    full_oracle_ids = full_oracle_enc['input_ids'].to(config.device)\n",
    "\n",
    "    oracle_prefix_enc = tokenizer(oracle_prefix, return_tensors=\"pt\",\n",
    "                                  add_special_tokens=True, padding=False, truncation=False)\n",
    "    oracle_prefix_len = oracle_prefix_enc['input_ids'].shape[1]\n",
    "\n",
    "    bos_id = full_oracle_ids[:, :1]\n",
    "    doc_ids = full_oracle_ids[:, oracle_prefix_len:]\n",
    "    doc_len = doc_ids.shape[1]\n",
    "\n",
    "    # Pre-compute LLM surrogate texts\n",
    "    llm_kw_text = surrogates_5[idx].get('keyword_query', '')\n",
    "    llm_symp_text = surrogates_5[idx].get('symptom_scenario', '')\n",
    "    llm_q_text = surrogates_5[idx].get('target_question', '')\n",
    "    llm_messy_text = surrogates_5[idx].get('messy_realworld', '')\n",
    "\n",
    "    # Helper: build truncated cache from prefix text\n",
    "    def build_trunc(prefix_text):\n",
    "        prefix_str = SURROGATE_PREFIX_TEMPLATE.format(surrogate=prefix_text)\n",
    "        prefix_enc = tokenizer(prefix_str, return_tensors=\"pt\",\n",
    "                               add_special_tokens=False, padding=False, truncation=False)\n",
    "        prefix_ids = prefix_enc['input_ids'].to(config.device)\n",
    "        full_ids = torch.cat([bos_id, prefix_ids, doc_ids], dim=1)\n",
    "        prefix_token_len = 1 + prefix_ids.shape[1]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            out = model(input_ids=full_ids,\n",
    "                        attention_mask=torch.ones_like(full_ids),\n",
    "                        use_cache=True, return_dict=True)\n",
    "        cache = extract_and_truncate_cache_with_bos(out.past_key_values, doc_len)\n",
    "        correct_rope_positions_with_bos(cache, prefix_token_len - 1, model)\n",
    "        nll = score_answer_with_cache(\n",
    "            deepcopy_cache(cache), 1 + doc_len,\n",
    "            query_prompt, answer_text, model, tokenizer, config)\n",
    "        del out, cache\n",
    "        return nll\n",
    "\n",
    "    # === Condition 1: BARE ===\n",
    "    bare_ids = torch.cat([bos_id, doc_ids], dim=1)\n",
    "    with torch.no_grad():\n",
    "        bare_out = model(input_ids=bare_ids, attention_mask=torch.ones_like(bare_ids),\n",
    "                         use_cache=True, return_dict=True)\n",
    "    nll_bare = score_answer_with_cache(\n",
    "        deepcopy_cache(bare_out.past_key_values), bare_ids.shape[1],\n",
    "        query_prompt, answer_text, model, tokenizer, config)\n",
    "    del bare_out\n",
    "\n",
    "    # === Condition 2: RANDOM-TRUNCATED ===\n",
    "    random_text = generate_random_prefix_text(query, tokenizer, seed=SEED + idx)\n",
    "    nll_random = build_trunc(random_text)\n",
    "\n",
    "    # === Condition 3: SEPARATOR-ONLY ===\n",
    "    sep_len, sep_cache = build_suffix_kv_cache(\n",
    "        passage, \"\", model, tokenizer, config, separator=SUFFIX_SEPARATOR)\n",
    "    nll_separator = score_answer_with_cache(\n",
    "        deepcopy_cache(sep_cache), sep_len,\n",
    "        query_prompt, answer_text, model, tokenizer, config)\n",
    "    del sep_cache\n",
    "\n",
    "    # === Conditions 4-8: STATIC TRUNCATED ===\n",
    "    nll_static_trunc = {}\n",
    "    for sname, sphrase in STATIC_PHRASES.items():\n",
    "        nll_static_trunc[sname] = build_trunc(sphrase)\n",
    "\n",
    "    # === Conditions 9-13: STATIC SUFFIX ===\n",
    "    nll_static_suffix = {}\n",
    "    for sname, sphrase in STATIC_PHRASES.items():\n",
    "        suf_len, suf_cache = build_suffix_kv_cache(\n",
    "            passage, sphrase, model, tokenizer, config, separator=SUFFIX_SEPARATOR)\n",
    "        nll_static_suffix[sname] = score_answer_with_cache(\n",
    "            deepcopy_cache(suf_cache), suf_len,\n",
    "            query_prompt, answer_text, model, tokenizer, config)\n",
    "        del suf_cache\n",
    "\n",
    "    # === Conditions 14-17: LLM SUFFIX ===\n",
    "    llm_suffix_texts = {\n",
    "        'keyword': llm_kw_text,\n",
    "        'symptom': llm_symp_text,\n",
    "        'question': llm_q_text,\n",
    "        'messy': llm_messy_text,\n",
    "    }\n",
    "    nll_llm_suffix = {}\n",
    "    for lname, ltext in llm_suffix_texts.items():\n",
    "        ls_len, ls_cache = build_suffix_kv_cache(\n",
    "            passage, ltext, model, tokenizer, config, separator=SUFFIX_SEPARATOR)\n",
    "        nll_llm_suffix[lname] = score_answer_with_cache(\n",
    "            deepcopy_cache(ls_cache), ls_len,\n",
    "            query_prompt, answer_text, model, tokenizer, config)\n",
    "        del ls_cache\n",
    "\n",
    "    # === Condition 18: LLM-KEYWORD-TRUNC ===\n",
    "    nll_llm_kw_trunc = build_trunc(llm_kw_text)\n",
    "\n",
    "    # === Condition 19: LLM-KEYWORD+SEP (stacking) ===\n",
    "    # Step 1: Build truncated cache with LLM keyword prefix\n",
    "    kw_prefix_str = SURROGATE_PREFIX_TEMPLATE.format(surrogate=llm_kw_text)\n",
    "    kw_prefix_enc = tokenizer(kw_prefix_str, return_tensors=\"pt\",\n",
    "                              add_special_tokens=False, padding=False, truncation=False)\n",
    "    kw_prefix_ids = kw_prefix_enc['input_ids'].to(config.device)\n",
    "    kw_full_ids = torch.cat([bos_id, kw_prefix_ids, doc_ids], dim=1)\n",
    "    kw_prefix_token_len = 1 + kw_prefix_ids.shape[1]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        kw_out = model(input_ids=kw_full_ids,\n",
    "                       attention_mask=torch.ones_like(kw_full_ids),\n",
    "                       use_cache=True, return_dict=True)\n",
    "    kw_trunc_cache = extract_and_truncate_cache_with_bos(kw_out.past_key_values, doc_len)\n",
    "    correct_rope_positions_with_bos(kw_trunc_cache, kw_prefix_token_len - 1, model)\n",
    "    del kw_out\n",
    "\n",
    "    # Step 2: Extend with suffix separator\n",
    "    suffix_enc = tokenizer(SUFFIX_SEPARATOR, return_tensors=\"pt\",\n",
    "                           add_special_tokens=False, padding=False, truncation=False)\n",
    "    suffix_ids = suffix_enc['input_ids'].to(config.device)\n",
    "    suffix_len_tok = suffix_ids.shape[1]\n",
    "    cache_len_before_suffix = 1 + doc_len\n",
    "\n",
    "    suffix_position_ids = torch.arange(\n",
    "        cache_len_before_suffix, cache_len_before_suffix + suffix_len_tok,\n",
    "        device=config.device\n",
    "    ).unsqueeze(0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        suffix_out = model(\n",
    "            input_ids=suffix_ids,\n",
    "            attention_mask=torch.ones(1, cache_len_before_suffix + suffix_len_tok,\n",
    "                                      device=config.device, dtype=torch.long),\n",
    "            position_ids=suffix_position_ids,\n",
    "            past_key_values=deepcopy_cache(kw_trunc_cache),\n",
    "            use_cache=True,\n",
    "            return_dict=True,\n",
    "        )\n",
    "    combo_cache = suffix_out.past_key_values\n",
    "    combo_len = cache_len_before_suffix + suffix_len_tok\n",
    "    nll_llm_kw_sep = score_answer_with_cache(\n",
    "        deepcopy_cache(combo_cache), combo_len,\n",
    "        query_prompt, answer_text, model, tokenizer, config)\n",
    "    del suffix_out, combo_cache, kw_trunc_cache\n",
    "\n",
    "    # === Condition 20: LLM-KEYWORD-FULL-CONTEXT (NOT truncated) ===\n",
    "    kw_full_prefix_str = SURROGATE_PREFIX_TEMPLATE.format(surrogate=llm_kw_text)\n",
    "    kw_full_text = kw_full_prefix_str + document_text\n",
    "    kw_full_enc = tokenizer(kw_full_text, return_tensors=\"pt\",\n",
    "                            add_special_tokens=True, padding=False, truncation=False)\n",
    "    kw_full_context_ids = kw_full_enc['input_ids'].to(config.device)\n",
    "    kw_full_context_len = kw_full_context_ids.shape[1]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        kw_full_out = model(input_ids=kw_full_context_ids,\n",
    "                            attention_mask=torch.ones_like(kw_full_context_ids),\n",
    "                            use_cache=True, return_dict=True)\n",
    "    nll_llm_kw_full_ctx = score_answer_with_cache(\n",
    "        deepcopy_cache(kw_full_out.past_key_values), kw_full_context_len,\n",
    "        query_prompt, answer_text, model, tokenizer, config)\n",
    "    del kw_full_out\n",
    "\n",
    "    # === Condition 21: NOVEL-GENERIC-TRUNC ===\n",
    "    nll_novel_generic = build_trunc(NOVEL_GENERIC_PHRASE)\n",
    "\n",
    "    # --- Store result ---\n",
    "    result = {\n",
    "        'idx': idx,\n",
    "        'doc_len': doc_len,\n",
    "        'passage_word_count': len(passage.split()),\n",
    "        'answer_token_count': len(tokenizer.encode(answer, add_special_tokens=False)),\n",
    "        'intent': intents[idx],\n",
    "        'bare': nll_bare,\n",
    "        'random_trunc': nll_random,\n",
    "        'separator_only': nll_separator,\n",
    "        'static_def_trunc': nll_static_trunc['definitional'],\n",
    "        'static_proc_trunc': nll_static_trunc['procedural'],\n",
    "        'static_quant_trunc': nll_static_trunc['quantitative'],\n",
    "        'static_fact_trunc': nll_static_trunc['factual'],\n",
    "        'static_prob_trunc': nll_static_trunc['problem'],\n",
    "        'static_def_suffix': nll_static_suffix['definitional'],\n",
    "        'static_proc_suffix': nll_static_suffix['procedural'],\n",
    "        'static_quant_suffix': nll_static_suffix['quantitative'],\n",
    "        'static_fact_suffix': nll_static_suffix['factual'],\n",
    "        'static_prob_suffix': nll_static_suffix['problem'],\n",
    "        'llm_keyword_suffix': nll_llm_suffix['keyword'],\n",
    "        'llm_symptom_suffix': nll_llm_suffix['symptom'],\n",
    "        'llm_question_suffix': nll_llm_suffix['question'],\n",
    "        'llm_messy_suffix': nll_llm_suffix['messy'],\n",
    "        'llm_keyword_trunc': nll_llm_kw_trunc,\n",
    "        'llm_keyword_sep': nll_llm_kw_sep,\n",
    "        'llm_keyword_full_ctx': nll_llm_kw_full_ctx,\n",
    "        'novel_generic_trunc': nll_novel_generic,\n",
    "    }\n",
    "    results.append(result)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    if (idx + 1) % CHECKPOINT_EVERY == 0 or idx == N - 1:\n",
    "        ckpt_data = {\n",
    "            'results': results,\n",
    "            'sample_queries': [s['query'] for s in samples],\n",
    "            'completed': len(results),\n",
    "            'total': N,\n",
    "            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        }\n",
    "        with open(CHECKPOINT_PATH, 'w') as f:\n",
    "            json.dump(ckpt_data, f)\n",
    "        elapsed = time.time() - t_start\n",
    "        rate = (idx - start_idx + 1) / elapsed if elapsed > 0 else 0\n",
    "        remaining = (N - idx - 1) / rate if rate > 0 else 0\n",
    "        tqdm.write(f\"  Checkpoint {idx+1}/{N} | {rate:.2f} s/s | ETA: {remaining/60:.1f} min\")\n",
    "\n",
    "elapsed_total = time.time() - t_start\n",
    "print(f\"\\nEvaluation complete: {len(results)} samples in {elapsed_total/60:.1f} min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30037ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Primary analysis — filter, NLL summary, 10 comparisons\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ANALYSIS — PRIMARY COMPARISONS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Extract arrays and filter zero NLLs\n",
    "cond_arrays = {}\n",
    "for cname in CONDITION_NAMES:\n",
    "    cond_arrays[cname] = np.array([r[cname] for r in results])\n",
    "\n",
    "valid = np.ones(len(results), dtype=bool)\n",
    "for cname in CONDITION_NAMES:\n",
    "    valid &= (cond_arrays[cname] != 0)\n",
    "n_valid = int(np.sum(valid))\n",
    "n_excluded = int(np.sum(~valid))\n",
    "print(f\"Total: {len(results)}, Valid: {n_valid}, Excluded: {n_excluded}\")\n",
    "\n",
    "c = {}\n",
    "for cname in CONDITION_NAMES:\n",
    "    c[cname] = cond_arrays[cname][valid]\n",
    "\n",
    "# Metadata arrays (filtered)\n",
    "intents_valid = np.array(intents)[valid]\n",
    "answer_tokens_valid = np.array([r['answer_token_count'] for r in results])[valid]\n",
    "passage_words_valid = np.array([r['passage_word_count'] for r in results])[valid]\n",
    "\n",
    "# NLL summary table\n",
    "print(f\"\\n{'Condition':<25} {'Mean NLL':>10} {'Std':>10} {'d vs Bare':>10}\")\n",
    "print(\"-\" * 60)\n",
    "for cname in CONDITION_NAMES:\n",
    "    mean_nll = np.mean(c[cname])\n",
    "    std_nll = np.std(c[cname])\n",
    "    if cname == 'bare':\n",
    "        d_str = \"—\"\n",
    "    else:\n",
    "        d = cohens_d(c['bare'] - c[cname])\n",
    "        d_str = f\"{d:+.3f}\"\n",
    "    print(f\"{cname:<25} {mean_nll:>10.4f} {std_nll:>10.4f} {d_str:>10}\")\n",
    "\n",
    "# Identify best statics\n",
    "best_static_trunc_d = -999\n",
    "best_static_trunc_name = ''\n",
    "for cname in STATIC_TRUNC_CONDS:\n",
    "    d = cohens_d(c['bare'] - c[cname])\n",
    "    if d > best_static_trunc_d:\n",
    "        best_static_trunc_d = d\n",
    "        best_static_trunc_name = cname\n",
    "\n",
    "best_static_suffix_d = -999\n",
    "best_static_suffix_name = ''\n",
    "for cname in STATIC_SUFFIX_CONDS:\n",
    "    d = cohens_d(c['bare'] - c[cname])\n",
    "    if d > best_static_suffix_d:\n",
    "        best_static_suffix_d = d\n",
    "        best_static_suffix_name = cname\n",
    "\n",
    "best_static_overall_d = max(best_static_trunc_d, best_static_suffix_d)\n",
    "best_static_overall_name = best_static_trunc_name if best_static_trunc_d > best_static_suffix_d else best_static_suffix_name\n",
    "\n",
    "print(f\"\\nBest static trunc: {best_static_trunc_name} (d={best_static_trunc_d:+.3f})\")\n",
    "print(f\"Best static suffix: {best_static_suffix_name} (d={best_static_suffix_d:+.3f})\")\n",
    "print(f\"Best static overall: {best_static_overall_name} (d={best_static_overall_d:+.3f})\")\n",
    "\n",
    "# Oracle-routed statics (per-sample min NLL across 5 statics)\n",
    "oracle_routed_static_trunc = np.minimum.reduce([c[cn] for cn in STATIC_TRUNC_CONDS])\n",
    "oracle_routed_static_suffix = np.minimum.reduce([c[cn] for cn in STATIC_SUFFIX_CONDS])\n",
    "oracle_routed_static_k5 = np.minimum(oracle_routed_static_trunc, oracle_routed_static_suffix)\n",
    "\n",
    "# Oracle-routed LLM (per-sample min NLL across 4 LLM suffix)\n",
    "oracle_routed_llm_k4 = np.minimum.reduce([c[cn] for cn in LLM_SUFFIX_CONDS])\n",
    "\n",
    "print(f\"\\nOracle-routed-static-K5 d vs bare: {cohens_d(c['bare'] - oracle_routed_static_k5):+.3f}\")\n",
    "print(f\"Oracle-routed-LLM-K4 d vs bare: {cohens_d(c['bare'] - oracle_routed_llm_k4):+.3f}\")\n",
    "\n",
    "# 10 primary comparisons\n",
    "print(f\"\\n{'='*85}\")\n",
    "print(f\"10 PRIMARY COMPARISONS (Bonferroni alpha = {BONFERRONI_ALPHA:.4f})\")\n",
    "print(f\"{'='*85}\")\n",
    "\n",
    "comparisons = [\n",
    "    ('C1: Best-static-trunc vs Bare',\n",
    "     c['bare'] - c[best_static_trunc_name],\n",
    "     'Do static prefixes help at all?'),\n",
    "    ('C2: Best-static-suffix vs Bare',\n",
    "     c['bare'] - c[best_static_suffix_name],\n",
    "     'Do static suffixes help at all?'),\n",
    "    ('C3: Best-static-trunc vs Best-static-suffix',\n",
    "     c[best_static_suffix_name] - c[best_static_trunc_name],\n",
    "     'Which mode is better?'),\n",
    "    ('C4: Best-static vs LLM-kw-trunc',\n",
    "     c[best_static_overall_name] - c['llm_keyword_trunc'],\n",
    "     'Can statics match LLM?'),\n",
    "    ('C5: Oracle-static-K5 vs Best-static',\n",
    "     c[best_static_overall_name] - oracle_routed_static_k5,\n",
    "     'Does routing help?'),\n",
    "    ('C6: Oracle-static-K5 vs LLM-kw-suf',\n",
    "     oracle_routed_static_k5 - c['llm_keyword_suffix'],\n",
    "     'Routed statics vs single LLM?'),\n",
    "    ('C7: LLM-kw-suffix vs LLM-kw-trunc',\n",
    "     c['llm_keyword_trunc'] - c['llm_keyword_suffix'],\n",
    "     'Suffix vs truncation for LLM?'),\n",
    "    ('C8: LLM-kw-full vs LLM-kw-trunc',\n",
    "     c['llm_keyword_trunc'] - c['llm_keyword_full_ctx'],\n",
    "     'Full-ctx vs truncated?'),\n",
    "    ('C9: LLM-kw+sep vs LLM-kw-suffix',\n",
    "     c['llm_keyword_suffix'] - c['llm_keyword_sep'],\n",
    "     'Does stacking replicate?'),\n",
    "    ('C10: Embed-routed-LLM vs Oracle-LLM',\n",
    "     None,  # Computed in routing analysis cell\n",
    "     'Is practical routing viable?'),\n",
    "]\n",
    "\n",
    "print(f\"\\n{'Comparison':<40} {'Mean Δ':>8} {'d':>8} {'Win%':>7} {'t':>8} {'p':>12} {'Sig':>5}\")\n",
    "print(\"-\" * 95)\n",
    "\n",
    "comparison_results = {}\n",
    "for name, delta, question in comparisons:\n",
    "    if delta is None:\n",
    "        print(f\"{name:<40} {'(computed later)':>50}\")\n",
    "        continue\n",
    "    d = cohens_d(delta)\n",
    "    win = np.mean(delta > 0) * 100\n",
    "    t_stat, p_val = stats.ttest_1samp(delta, 0)\n",
    "    sig = \"***\" if p_val < 0.001 else \"**\" if p_val < BONFERRONI_ALPHA else \"*\" if p_val < 0.05 else \"ns\"\n",
    "    print(f\"{name:<40} {np.mean(delta):>8.4f} {d:>8.3f} {win:>6.1f}% {t_stat:>8.2f} {p_val:>11.2e} {sig:>5}\")\n",
    "    comparison_results[name] = {\n",
    "        'mean_delta': float(np.mean(delta)),\n",
    "        'cohens_d': float(d),\n",
    "        'win_rate': float(win / 100),\n",
    "        't_stat': float(t_stat),\n",
    "        'p_value': float(p_val),\n",
    "        'bonferroni_significant': bool(p_val < BONFERRONI_ALPHA),\n",
    "        'question': question,\n",
    "    }\n",
    "\n",
    "# All vs Bare\n",
    "print(f\"\\n{'='*85}\")\n",
    "print(\"ALL CONDITIONS vs BARE\")\n",
    "print(f\"{'='*85}\")\n",
    "print(f\"\\n{'Condition':<25} {'d vs Bare':>10} {'Win%':>7} {'p':>12}\")\n",
    "print(\"-\" * 60)\n",
    "all_vs_bare = {}\n",
    "for cname in CONDITION_NAMES:\n",
    "    if cname == 'bare':\n",
    "        continue\n",
    "    delta = c['bare'] - c[cname]\n",
    "    d = cohens_d(delta)\n",
    "    win = np.mean(delta > 0) * 100\n",
    "    _, p_val = stats.ttest_1samp(delta, 0)\n",
    "    sig = \"***\" if p_val < 0.001 else \"**\" if p_val < BONFERRONI_ALPHA else \"*\" if p_val < 0.05 else \"ns\"\n",
    "    print(f\"{cname:<25} {d:>10.3f} {win:>6.1f}% {p_val:>11.2e} {sig:>5}\")\n",
    "    all_vs_bare[cname] = {'cohens_d': float(d), 'win_rate': float(win/100), 'p_value': float(p_val)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89948c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Hardness quintile breakdown (all conditions)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"HARDNESS QUINTILE BREAKDOWN\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "bare_valid = c['bare']\n",
    "quintile_boundaries = np.percentile(bare_valid, [20, 40, 60, 80])\n",
    "quintile_labels = ['Q1 (easy)', 'Q2', 'Q3', 'Q4', 'Q5 (hard)']\n",
    "\n",
    "def get_quintile(nll, boundaries):\n",
    "    for i, b in enumerate(boundaries):\n",
    "        if nll <= b:\n",
    "            return i\n",
    "    return len(boundaries)\n",
    "\n",
    "quintiles = np.array([get_quintile(nll, quintile_boundaries) for nll in bare_valid])\n",
    "\n",
    "conditions_to_show = [\n",
    "    'random_trunc', 'separator_only',\n",
    "    'static_def_trunc', 'static_proc_trunc', 'static_fact_trunc',\n",
    "    'static_def_suffix', 'static_proc_suffix', 'static_fact_suffix',\n",
    "    'llm_keyword_suffix', 'llm_symptom_suffix', 'llm_question_suffix',\n",
    "    'llm_keyword_trunc', 'llm_keyword_sep', 'llm_keyword_full_ctx',\n",
    "    'novel_generic_trunc',\n",
    "]\n",
    "\n",
    "header = f\"{'Condition':<25}\" + \"\".join(f\"{ql:>14}\" for ql in quintile_labels) + f\"{'Overall':>14}\"\n",
    "print(f\"\\n{header}\")\n",
    "print(\"-\" * (25 + 14 * 6))\n",
    "\n",
    "hardness_breakdown = {}\n",
    "for cname in conditions_to_show:\n",
    "    row = f\"{cname:<25}\"\n",
    "    quintile_ds = []\n",
    "    for q in range(5):\n",
    "        mask_q = quintiles == q\n",
    "        if np.sum(mask_q) < 10:\n",
    "            row += f\"{'n/a':>14}\"\n",
    "            quintile_ds.append(None)\n",
    "        else:\n",
    "            delta = bare_valid[mask_q] - c[cname][mask_q]\n",
    "            d = cohens_d(delta)\n",
    "            row += f\"{d:>+14.3f}\"\n",
    "            quintile_ds.append(float(d))\n",
    "    d_all = cohens_d(bare_valid - c[cname])\n",
    "    row += f\"{d_all:>+14.3f}\"\n",
    "    print(row)\n",
    "    hardness_breakdown[cname] = {\n",
    "        'quintile_ds': quintile_ds,\n",
    "        'overall_d': float(d_all),\n",
    "    }\n",
    "\n",
    "# Hardness interaction correlations\n",
    "print(f\"\\nHardness interaction (r between bare NLL and benefit):\")\n",
    "for cname in conditions_to_show:\n",
    "    delta = bare_valid - c[cname]\n",
    "    r, p = stats.pearsonr(bare_valid, delta)\n",
    "    sig = \"***\" if p < 0.001 else \"**\" if p < 0.01 else \"*\" if p < 0.05 else \"ns\"\n",
    "    print(f\"  {cname:<25}: r={r:+.3f}, p={p:.2e} {sig}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06f0520",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Stratification by intent, answer length, passage length\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"STRATIFICATION ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# --- 1. Intent stratification ---\n",
    "print(\"\\n--- Intent Stratification ---\")\n",
    "intent_categories = sorted(set(intents_valid))\n",
    "key_conditions = ['bare', 'separator_only', best_static_trunc_name, best_static_suffix_name,\n",
    "                   'llm_keyword_suffix', 'llm_keyword_trunc', 'llm_keyword_sep']\n",
    "\n",
    "header = f\"{'Intent':<15} {'N':>5}\"\n",
    "for cn in key_conditions:\n",
    "    if cn == 'bare':\n",
    "        header += f\"{'bare NLL':>12}\"\n",
    "    else:\n",
    "        header += f\"{cn[:10]:>12}\"\n",
    "print(header)\n",
    "print(\"-\" * (20 + 12 * len(key_conditions)))\n",
    "\n",
    "intent_analysis = {}\n",
    "for intent in intent_categories:\n",
    "    mask = intents_valid == intent\n",
    "    n_intent = int(np.sum(mask))\n",
    "    if n_intent < 10:\n",
    "        continue\n",
    "    row = f\"{intent:<15} {n_intent:>5}\"\n",
    "    intent_ds = {}\n",
    "    for cn in key_conditions:\n",
    "        if cn == 'bare':\n",
    "            row += f\"{np.mean(c['bare'][mask]):>12.3f}\"\n",
    "        else:\n",
    "            d = cohens_d(c['bare'][mask] - c[cn][mask])\n",
    "            row += f\"{d:>+12.3f}\"\n",
    "            intent_ds[cn] = float(d)\n",
    "    print(row)\n",
    "    intent_analysis[intent] = {'n': n_intent, 'ds': intent_ds}\n",
    "\n",
    "# Best surrogate per intent\n",
    "print(f\"\\nBest surrogate per intent:\")\n",
    "all_statics_for_intent = STATIC_TRUNC_CONDS + STATIC_SUFFIX_CONDS\n",
    "for intent in intent_categories:\n",
    "    mask = intents_valid == intent\n",
    "    if int(np.sum(mask)) < 10:\n",
    "        continue\n",
    "    best_d = -999\n",
    "    best_cn = ''\n",
    "    for cn in all_statics_for_intent:\n",
    "        d = cohens_d(c['bare'][mask] - c[cn][mask])\n",
    "        if d > best_d:\n",
    "            best_d = d\n",
    "            best_cn = cn\n",
    "    print(f\"  {intent:<15}: {best_cn} (d={best_d:+.3f})\")\n",
    "\n",
    "# --- 2. Answer length stratification ---\n",
    "print(\"\\n--- Answer Length Stratification ---\")\n",
    "ans_short = answer_tokens_valid < 5\n",
    "ans_medium = (answer_tokens_valid >= 5) & (answer_tokens_valid <= 15)\n",
    "ans_long = answer_tokens_valid > 15\n",
    "\n",
    "print(f\"Answer length bins: short(<5)={int(np.sum(ans_short))}, \"\n",
    "      f\"medium(5-15)={int(np.sum(ans_medium))}, long(>15)={int(np.sum(ans_long))}\")\n",
    "\n",
    "for label, mask in [('short', ans_short), ('medium', ans_medium), ('long', ans_long)]:\n",
    "    if int(np.sum(mask)) < 10:\n",
    "        continue\n",
    "    row = f\"  {label:<10} N={int(np.sum(mask)):>4}\"\n",
    "    for cn in ['separator_only', 'llm_keyword_suffix', 'llm_keyword_trunc', 'llm_keyword_sep']:\n",
    "        d = cohens_d(c['bare'][mask] - c[cn][mask])\n",
    "        row += f\"  {cn[:12]}={d:+.3f}\"\n",
    "    print(row)\n",
    "\n",
    "# --- 3. Passage length stratification ---\n",
    "print(\"\\n--- Passage Length Stratification ---\")\n",
    "psg_short = passage_words_valid < 80\n",
    "psg_medium = (passage_words_valid >= 80) & (passage_words_valid <= 200)\n",
    "psg_long = passage_words_valid > 200\n",
    "\n",
    "print(f\"Passage length bins: short(<80)={int(np.sum(psg_short))}, \"\n",
    "      f\"medium(80-200)={int(np.sum(psg_medium))}, long(>200)={int(np.sum(psg_long))}\")\n",
    "\n",
    "for label, mask in [('short', psg_short), ('medium', psg_medium), ('long', psg_long)]:\n",
    "    if int(np.sum(mask)) < 10:\n",
    "        continue\n",
    "    row = f\"  {label:<10} N={int(np.sum(mask)):>4}\"\n",
    "    for cn in ['separator_only', 'llm_keyword_suffix', 'llm_keyword_trunc', 'llm_keyword_sep']:\n",
    "        d = cohens_d(c['bare'][mask] - c[cn][mask])\n",
    "        row += f\"  {cn[:12]}={d:+.3f}\"\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e99d718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Routing analysis — oracle K-curve, embedding routing, intent-matched, complementarity\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ROUTING ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# --- 1. Oracle K-curve (best-of-K for statics and LLMs) ---\n",
    "print(\"\\n--- Oracle K-Curve ---\")\n",
    "all_static_conds = STATIC_TRUNC_CONDS + STATIC_SUFFIX_CONDS\n",
    "\n",
    "# For statics: sort by overall d, pick top K\n",
    "static_ds = [(cn, cohens_d(c['bare'] - c[cn])) for cn in all_static_conds]\n",
    "static_ds.sort(key=lambda x: -x[1])\n",
    "\n",
    "print(f\"\\nStatic surrogates ranked by d vs bare:\")\n",
    "for cn, d in static_ds:\n",
    "    print(f\"  {cn}: d={d:+.3f}\")\n",
    "\n",
    "print(f\"\\nOracle best-of-K (static):\")\n",
    "for K in range(1, 6):\n",
    "    top_k_conds = [cn for cn, _ in static_ds[:K]]\n",
    "    best_of_k = np.minimum.reduce([c[cn] for cn in top_k_conds])\n",
    "    d = cohens_d(c['bare'] - best_of_k)\n",
    "    print(f\"  K={K}: d={d:+.3f} (conditions: {', '.join(top_k_conds)})\")\n",
    "\n",
    "# LLM K-curve\n",
    "llm_ds = [(cn, cohens_d(c['bare'] - c[cn])) for cn in LLM_SUFFIX_CONDS]\n",
    "llm_ds.sort(key=lambda x: -x[1])\n",
    "print(f\"\\nOracle best-of-K (LLM suffix):\")\n",
    "for K in range(1, 5):\n",
    "    top_k_conds = [cn for cn, _ in llm_ds[:K]]\n",
    "    best_of_k = np.minimum.reduce([c[cn] for cn in top_k_conds])\n",
    "    d = cohens_d(c['bare'] - best_of_k)\n",
    "    print(f\"  K={K}: d={d:+.3f}\")\n",
    "\n",
    "# --- 2. Embedding routing ---\n",
    "print(\"\\n--- Embedding Routing ---\")\n",
    "print(\"Loading sentence-transformers model...\")\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity as cos_sim\n",
    "\n",
    "embed_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Embed all queries (valid subset)\n",
    "valid_indices = np.where(valid)[0]\n",
    "valid_queries = [samples[i]['query'] for i in valid_indices]\n",
    "print(f\"Embedding {len(valid_queries)} queries...\")\n",
    "query_embeddings = embed_model.encode(valid_queries, show_progress_bar=True)\n",
    "\n",
    "# Embed LLM surrogates for each sample\n",
    "llm_surrogate_keys = ['keyword', 'symptom', 'question', 'messy']\n",
    "llm_surrogate_cond_map = {\n",
    "    'keyword': 'llm_keyword_suffix',\n",
    "    'symptom': 'llm_symptom_suffix',\n",
    "    'question': 'llm_question_suffix',\n",
    "    'messy': 'llm_messy_suffix',\n",
    "}\n",
    "\n",
    "print(\"Embedding LLM surrogates...\")\n",
    "# For each sample, embed 4 surrogates and route by cosine sim to query\n",
    "embed_routed_nlls = np.zeros(n_valid)\n",
    "embed_routed_choices = []\n",
    "oracle_routed_choices = []\n",
    "\n",
    "for vi in tqdm(range(n_valid), desc=\"Embedding routing\"):\n",
    "    orig_idx = valid_indices[vi]\n",
    "    q_emb = query_embeddings[vi:vi+1]\n",
    "\n",
    "    # Get surrogate texts\n",
    "    surr_texts = {\n",
    "        'keyword': surrogates_5[orig_idx].get('keyword_query', ''),\n",
    "        'symptom': surrogates_5[orig_idx].get('symptom_scenario', ''),\n",
    "        'question': surrogates_5[orig_idx].get('target_question', ''),\n",
    "        'messy': surrogates_5[orig_idx].get('messy_realworld', ''),\n",
    "    }\n",
    "\n",
    "    # Embed surrogates\n",
    "    surr_embs = embed_model.encode([surr_texts[k] for k in llm_surrogate_keys])\n",
    "    sims = cos_sim(q_emb, surr_embs)[0]\n",
    "\n",
    "    # Route by max cosine sim\n",
    "    best_k_idx = np.argmax(sims)\n",
    "    best_k = llm_surrogate_keys[best_k_idx]\n",
    "    embed_routed_nlls[vi] = c[llm_surrogate_cond_map[best_k]][vi]\n",
    "    embed_routed_choices.append(best_k)\n",
    "\n",
    "    # Oracle: min NLL\n",
    "    nlls_4 = {k: c[llm_surrogate_cond_map[k]][vi] for k in llm_surrogate_keys}\n",
    "    oracle_k = min(nlls_4, key=nlls_4.get)\n",
    "    oracle_routed_choices.append(oracle_k)\n",
    "\n",
    "# Report\n",
    "d_embed_routed = cohens_d(c['bare'] - embed_routed_nlls)\n",
    "d_oracle_routed = cohens_d(c['bare'] - oracle_routed_llm_k4)\n",
    "oracle_accuracy = np.mean([e == o for e, o in zip(embed_routed_choices, oracle_routed_choices)])\n",
    "\n",
    "print(f\"\\nEmbedding-routed-LLM-K4: d={d_embed_routed:+.3f}\")\n",
    "print(f\"Oracle-routed-LLM-K4: d={d_oracle_routed:+.3f}\")\n",
    "print(f\"Embedding routing accuracy vs oracle: {oracle_accuracy*100:.1f}%\")\n",
    "\n",
    "# C10 comparison\n",
    "delta_c10 = embed_routed_nlls - oracle_routed_llm_k4\n",
    "d_c10 = cohens_d(delta_c10)\n",
    "t_c10, p_c10 = stats.ttest_1samp(delta_c10, 0)\n",
    "sig_c10 = \"***\" if p_c10 < 0.001 else \"**\" if p_c10 < BONFERRONI_ALPHA else \"*\" if p_c10 < 0.05 else \"ns\"\n",
    "print(f\"\\nC10: Embed-routed vs Oracle-routed: d={d_c10:+.3f}, p={p_c10:.2e} {sig_c10}\")\n",
    "comparison_results['C10: Embed-routed-LLM vs Oracle-LLM'] = {\n",
    "    'mean_delta': float(np.mean(delta_c10)),\n",
    "    'cohens_d': float(d_c10),\n",
    "    'win_rate': float(np.mean(delta_c10 > 0)),\n",
    "    't_stat': float(t_c10),\n",
    "    'p_value': float(p_c10),\n",
    "    'bonferroni_significant': bool(p_c10 < BONFERRONI_ALPHA),\n",
    "    'question': 'Is practical routing viable?',\n",
    "}\n",
    "\n",
    "# Routing choice distribution\n",
    "print(f\"\\nEmbedding routing choice distribution:\")\n",
    "for k in llm_surrogate_keys:\n",
    "    n_chosen = sum(1 for ch in embed_routed_choices if ch == k)\n",
    "    print(f\"  {k}: {n_chosen} ({n_chosen/n_valid*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nOracle routing choice distribution:\")\n",
    "for k in llm_surrogate_keys:\n",
    "    n_chosen = sum(1 for ch in oracle_routed_choices if ch == k)\n",
    "    print(f\"  {k}: {n_chosen} ({n_chosen/n_valid*100:.1f}%)\")\n",
    "\n",
    "# --- 3. Intent-matched static routing ---\n",
    "print(\"\\n--- Intent-Matched Static Routing ---\")\n",
    "intent_to_static = {\n",
    "    'definitional': 'static_def_suffix',\n",
    "    'procedural': 'static_proc_suffix',\n",
    "    'transactional': 'static_quant_suffix',\n",
    "    'comparison': 'static_fact_suffix',\n",
    "    'factual': 'static_fact_suffix',\n",
    "    'medical': 'static_prob_suffix',\n",
    "    'other': best_static_suffix_name,\n",
    "}\n",
    "\n",
    "intent_matched_nlls = np.zeros(n_valid)\n",
    "for vi in range(n_valid):\n",
    "    intent = intents_valid[vi]\n",
    "    matched_cond = intent_to_static.get(intent, best_static_suffix_name)\n",
    "    intent_matched_nlls[vi] = c[matched_cond][vi]\n",
    "\n",
    "d_intent_matched = cohens_d(c['bare'] - intent_matched_nlls)\n",
    "print(f\"Intent-matched-static d vs bare: {d_intent_matched:+.3f}\")\n",
    "print(f\"Best-single-static d vs bare: {best_static_overall_d:+.3f}\")\n",
    "print(f\"Improvement from intent matching: {d_intent_matched - best_static_overall_d:+.3f}\")\n",
    "\n",
    "# --- 4. Complementarity matrix ---\n",
    "print(\"\\n--- Complementarity Matrix (LLM suffix, fraction where row wins over column) ---\")\n",
    "llm_conds_for_comp = LLM_SUFFIX_CONDS\n",
    "header = f\"{'':>20}\" + \"\".join(f\"{cn[:12]:>14}\" for cn in llm_conds_for_comp)\n",
    "print(header)\n",
    "for cn_a in llm_conds_for_comp:\n",
    "    row = f\"{cn_a[:20]:<20}\"\n",
    "    for cn_b in llm_conds_for_comp:\n",
    "        if cn_a == cn_b:\n",
    "            row += f\"{'—':>14}\"\n",
    "        else:\n",
    "            frac = np.mean(c[cn_a] < c[cn_b])\n",
    "            row += f\"{frac:>14.3f}\"\n",
    "    print(row)\n",
    "\n",
    "# --- 5. Hardness-gated routing ---\n",
    "print(\"\\n--- Hardness-Gated Routing ---\")\n",
    "print(\"Prime only if bare_NLL > threshold. Sweep threshold.\")\n",
    "thresholds = [0.0, 0.3, 0.5, 0.8, 1.0, 1.5, 2.0, 3.0]\n",
    "for thresh in thresholds:\n",
    "    mask_prime = bare_valid > thresh\n",
    "    n_primed = int(np.sum(mask_prime))\n",
    "    if n_primed < 10:\n",
    "        continue\n",
    "    # For primed: use LLM-keyword-sep; for unprimed: use bare\n",
    "    gated_nlls = np.where(mask_prime, c['llm_keyword_sep'], c['bare'])\n",
    "    d_gated = cohens_d(c['bare'] - gated_nlls)\n",
    "    frac_primed = n_primed / n_valid * 100\n",
    "    print(f\"  threshold={thresh:.1f}: prime {frac_primed:.0f}% samples, d={d_gated:+.3f}\")\n",
    "\n",
    "del embed_model  # Free memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5ed7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Plots\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(22, 14))\n",
    "\n",
    "# --- Plot 1: All conditions bar chart (d vs bare) ---\n",
    "ax = axes[0, 0]\n",
    "cnames_sorted = sorted(\n",
    "    [cn for cn in CONDITION_NAMES if cn != 'bare'],\n",
    "    key=lambda cn: cohens_d(c['bare'] - c[cn]),\n",
    "    reverse=True\n",
    ")\n",
    "ds_bar = [cohens_d(c['bare'] - c[cn]) for cn in cnames_sorted]\n",
    "color_map = {}\n",
    "for cn in STATIC_TRUNC_CONDS:\n",
    "    color_map[cn] = 'steelblue'\n",
    "for cn in STATIC_SUFFIX_CONDS:\n",
    "    color_map[cn] = 'cornflowerblue'\n",
    "for cn in LLM_SUFFIX_CONDS:\n",
    "    color_map[cn] = 'forestgreen'\n",
    "color_map['llm_keyword_trunc'] = 'darkgreen'\n",
    "color_map['llm_keyword_sep'] = 'gold'\n",
    "color_map['llm_keyword_full_ctx'] = 'orange'\n",
    "color_map['novel_generic_trunc'] = 'mediumpurple'\n",
    "color_map['random_trunc'] = 'gray'\n",
    "color_map['separator_only'] = 'salmon'\n",
    "colors_bar = [color_map.get(cn, 'lightgray') for cn in cnames_sorted]\n",
    "\n",
    "bars = ax.barh(range(len(cnames_sorted)), ds_bar, color=colors_bar, edgecolor='black', linewidth=0.5)\n",
    "ax.set_yticks(range(len(cnames_sorted)))\n",
    "ax.set_yticklabels(cnames_sorted, fontsize=7)\n",
    "ax.axvline(x=0, color='gray', linestyle='--')\n",
    "ax.set_xlabel(\"Cohen's d vs Bare\")\n",
    "ax.set_title('All Conditions vs Bare')\n",
    "ax.invert_yaxis()\n",
    "\n",
    "# --- Plot 2: Static trunc vs suffix comparison ---\n",
    "ax = axes[0, 1]\n",
    "static_names = list(STATIC_PHRASES.keys())\n",
    "abbrev_map = {'definitional': 'def', 'procedural': 'proc', 'quantitative': 'quant', 'factual': 'fact', 'problem': 'prob'}\n",
    "trunc_ds = [cohens_d(c['bare'] - c[f'static_{abbrev_map[n]}_trunc']) for n in static_names]\n",
    "suffix_ds = [cohens_d(c['bare'] - c[f'static_{abbrev_map[n]}_suffix']) for n in static_names]\n",
    "x = np.arange(len(static_names))\n",
    "width = 0.35\n",
    "ax.bar(x - width/2, trunc_ds, width, label='Truncated prefix', color='steelblue', edgecolor='black', linewidth=0.5)\n",
    "ax.bar(x + width/2, suffix_ds, width, label='Suffix', color='cornflowerblue', edgecolor='black', linewidth=0.5)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([n[:6] for n in static_names], fontsize=8)\n",
    "ax.axhline(y=0, color='gray', linestyle='--')\n",
    "ax.set_ylabel(\"Cohen's d vs Bare\")\n",
    "ax.set_title('Static: Prefix vs Suffix Mode')\n",
    "ax.legend(fontsize=8)\n",
    "\n",
    "# --- Plot 3: Hardness × condition heatmap ---\n",
    "ax = axes[0, 2]\n",
    "hm_conditions = ['separator_only', best_static_trunc_name, best_static_suffix_name,\n",
    "                  'llm_keyword_suffix', 'llm_keyword_trunc', 'llm_keyword_sep', 'llm_keyword_full_ctx']\n",
    "hm_data = []\n",
    "for cname in hm_conditions:\n",
    "    row = []\n",
    "    for q in range(5):\n",
    "        mask_q = quintiles == q\n",
    "        if np.sum(mask_q) < 10:\n",
    "            row.append(0)\n",
    "        else:\n",
    "            delta = bare_valid[mask_q] - c[cname][mask_q]\n",
    "            row.append(cohens_d(delta))\n",
    "    hm_data.append(row)\n",
    "hm_data = np.array(hm_data)\n",
    "im = ax.imshow(hm_data, cmap='RdBu_r', vmin=-0.5, vmax=0.7, aspect='auto')\n",
    "ax.set_xticks(range(5))\n",
    "ax.set_xticklabels(quintile_labels, fontsize=7)\n",
    "ax.set_yticks(range(len(hm_conditions)))\n",
    "ax.set_yticklabels(hm_conditions, fontsize=7)\n",
    "for i in range(len(hm_conditions)):\n",
    "    for j in range(5):\n",
    "        ax.text(j, i, f\"{hm_data[i,j]:+.2f}\", ha='center', va='center', fontsize=6)\n",
    "plt.colorbar(im, ax=ax, label=\"Cohen's d vs bare\")\n",
    "ax.set_title('Hardness × Condition')\n",
    "\n",
    "# --- Plot 4: Oracle K-curve ---\n",
    "ax = axes[1, 0]\n",
    "static_k_ds = []\n",
    "for K in range(1, 6):\n",
    "    top_k = [cn for cn, _ in static_ds[:K]]\n",
    "    best_of_k = np.minimum.reduce([c[cn] for cn in top_k])\n",
    "    static_k_ds.append(cohens_d(c['bare'] - best_of_k))\n",
    "llm_k_ds = []\n",
    "for K in range(1, 5):\n",
    "    top_k = [cn for cn, _ in llm_ds[:K]]\n",
    "    best_of_k = np.minimum.reduce([c[cn] for cn in top_k])\n",
    "    llm_k_ds.append(cohens_d(c['bare'] - best_of_k))\n",
    "\n",
    "ax.plot(range(1, 6), static_k_ds, 'o-', color='steelblue', label='Static (10 total)')\n",
    "ax.plot(range(1, 5), llm_k_ds, 's-', color='forestgreen', label='LLM suffix (4 total)')\n",
    "ax.set_xlabel('K (best-of-K)')\n",
    "ax.set_ylabel(\"Cohen's d vs Bare\")\n",
    "ax.set_title('Oracle K-Curve')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# --- Plot 5: Intent stratification ---\n",
    "ax = axes[1, 1]\n",
    "intent_cats = sorted(intent_analysis.keys(), key=lambda x: -intent_analysis[x]['n'])\n",
    "intent_cats = [ic for ic in intent_cats if intent_analysis[ic]['n'] >= 10]\n",
    "x_int = np.arange(len(intent_cats))\n",
    "width_int = 0.25\n",
    "for j, cn in enumerate(['llm_keyword_suffix', 'llm_keyword_trunc', 'llm_keyword_sep']):\n",
    "    ds_int = [intent_analysis[ic]['ds'].get(cn, 0) for ic in intent_cats]\n",
    "    ax.bar(x_int + j * width_int - width_int, ds_int, width_int, label=cn[:15],\n",
    "           edgecolor='black', linewidth=0.5)\n",
    "ax.set_xticks(x_int)\n",
    "ax.set_xticklabels(intent_cats, fontsize=7, rotation=45)\n",
    "ax.axhline(y=0, color='gray', linestyle='--')\n",
    "ax.set_ylabel(\"Cohen's d vs Bare\")\n",
    "ax.set_title('Intent Stratification')\n",
    "ax.legend(fontsize=6)\n",
    "\n",
    "# --- Plot 6: Hardness-gated routing curve ---\n",
    "ax = axes[1, 2]\n",
    "thresh_vals = np.arange(0, 4.1, 0.2)\n",
    "gated_ds = []\n",
    "frac_primed_vals = []\n",
    "for thresh in thresh_vals:\n",
    "    mask_prime = bare_valid > thresh\n",
    "    n_primed = int(np.sum(mask_prime))\n",
    "    if n_primed < 5:\n",
    "        gated_ds.append(np.nan)\n",
    "        frac_primed_vals.append(0)\n",
    "        continue\n",
    "    gated_nlls = np.where(mask_prime, c['llm_keyword_sep'], c['bare'])\n",
    "    gated_ds.append(cohens_d(c['bare'] - gated_nlls))\n",
    "    frac_primed_vals.append(n_primed / n_valid * 100)\n",
    "\n",
    "ax2 = ax.twinx()\n",
    "ax.plot(thresh_vals, gated_ds, 'o-', color='forestgreen', markersize=3, label='d vs bare')\n",
    "ax2.plot(thresh_vals, frac_primed_vals, 's--', color='gray', markersize=3, alpha=0.5, label='% primed')\n",
    "ax.set_xlabel('Bare NLL Threshold')\n",
    "ax.set_ylabel(\"Cohen's d vs bare (gated)\", color='forestgreen')\n",
    "ax2.set_ylabel('% Samples Primed', color='gray')\n",
    "ax.set_title('Hardness-Gated Routing')\n",
    "ax.axhline(y=0, color='red', linestyle='--', alpha=0.3)\n",
    "ax.legend(loc='upper left', fontsize=7)\n",
    "ax2.legend(loc='upper right', fontsize=7)\n",
    "\n",
    "plt.suptitle('Exp 07: Static Surrogates, Dual-Mode Priming, Intent Routing', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'analysis_plots.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"Plots saved to {RESULTS_DIR / 'analysis_plots.png'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5423f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13: Save comprehensive results JSON\n",
    "\n",
    "final = {\n",
    "    'experiment': 'exp07_static_surrogates_and_routing',\n",
    "    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'config': {\n",
    "        'model_name': config.model_name,\n",
    "        'seed': SEED,\n",
    "        'n_eval': N,\n",
    "        'n_valid': n_valid,\n",
    "        'n_excluded': n_excluded,\n",
    "        'min_passage_words': config.min_passage_words,\n",
    "        'max_passage_words': config.max_passage_words,\n",
    "        'n_conditions': len(CONDITION_NAMES),\n",
    "        'n_comparisons': N_COMPARISONS,\n",
    "        'bonferroni_alpha': BONFERRONI_ALPHA,\n",
    "    },\n",
    "    'condition_names': CONDITION_NAMES,\n",
    "    'nll_summary': {\n",
    "        cname: {\n",
    "            'mean': float(np.mean(c[cname])),\n",
    "            'std': float(np.std(c[cname])),\n",
    "            'cohens_d_vs_bare': float(cohens_d(c['bare'] - c[cname])) if cname != 'bare' else 0.0,\n",
    "        }\n",
    "        for cname in CONDITION_NAMES\n",
    "    },\n",
    "    'primary_comparisons': comparison_results,\n",
    "    'all_vs_bare': all_vs_bare,\n",
    "    'hardness_breakdown': hardness_breakdown,\n",
    "    'intent_analysis': intent_analysis,\n",
    "    'routing_analysis': {\n",
    "        'oracle_static_k5_d': float(cohens_d(c['bare'] - oracle_routed_static_k5)),\n",
    "        'oracle_llm_k4_d': float(d_oracle_routed),\n",
    "        'embed_routed_llm_k4_d': float(d_embed_routed),\n",
    "        'embed_routing_accuracy': float(oracle_accuracy),\n",
    "        'intent_matched_static_d': float(d_intent_matched),\n",
    "        'best_static_trunc': best_static_trunc_name,\n",
    "        'best_static_suffix': best_static_suffix_name,\n",
    "        'best_static_overall': best_static_overall_name,\n",
    "    },\n",
    "    'per_sample_results': results,\n",
    "}\n",
    "\n",
    "with open(FINAL_RESULTS_PATH, 'w') as f:\n",
    "    json.dump(final, f, indent=2)\n",
    "\n",
    "print(f\"Results saved to {FINAL_RESULTS_PATH}\")\n",
    "print(f\"File size: {FINAL_RESULTS_PATH.stat().st_size / 1024:.1f} KB\")\n",
    "print(\"\\nDone!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462e9c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 14: GPU cleanup\n",
    "import gc\n",
    "\n",
    "print(\"Cleaning up GPU memory...\")\n",
    "mem_before = torch.cuda.memory_allocated() / 1e9\n",
    "\n",
    "del model\n",
    "del tokenizer\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "mem_after = torch.cuda.memory_allocated() / 1e9\n",
    "print(f\"GPU memory: {mem_before:.2f} GB -> {mem_after:.2f} GB\")\n",
    "print(\"Cleanup complete.\")"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cu124.m127",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/base-cu124:m127"
  },
  "kernelspec": {
   "display_name": "Python 3 (Local)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
