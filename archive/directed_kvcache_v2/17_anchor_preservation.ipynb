{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 17: Anchor Preservation\n",
    "\n",
    "**Hypothesis**: KV cache truncation fails on long documents (Exp 11) because RoPE\n",
    "correction shifts document positions down to [1, 2, ..., D], closing the positional\n",
    "gap. This disrupts the BOS token's attention-sink role and the model's internal\n",
    "position-based routing (the \"Attention Sink\" problem from StreamingLLM).\n",
    "\n",
    "**The fix**: After truncation, **skip RoPE correction** so document tokens retain\n",
    "their original positions [P, P+1, ..., P+D-1], creating a positional \"gap\" where\n",
    "the surrogate prefix was. BOS stays at position 0 as the attention sink anchor.\n",
    "\n",
    "**Key difference from current code**:\n",
    "- `_old`: BOS preserved + RoPE correction (positions shifted to [0, 1, ..., D])\n",
    "- `_anchor`: BOS preserved + NO RoPE correction (positions are [0, gap, P, ..., P+D-1])\n",
    "- At scoring time: query tokens must get positions starting at P+D (not 1+D)\n",
    "\n",
    "**Experiments revisited**:\n",
    "| Original Exp | Finding | Why Revisit |\n",
    "|---|---|---|\n",
    "| Exp 07 | static_fact_trunc d=+0.472 on short docs | Baseline: anchor should be neutral on short docs |\n",
    "| Exp 11 | ALL conditions fail on long docs | **Primary target**: anchor should recover long-doc performance |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Setup & Imports\n",
    "import os\n",
    "os.umask(0o000)  # File permission safety (two-user environment)\n",
    "\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from scipy import stats\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "from tqdm.auto import tqdm\n",
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from lib.config import ExperimentConfig\n",
    "from lib.kv_cache import (\n",
    "    build_kv_cache,\n",
    "    build_truncated_kv_cache_corrected,\n",
    "    build_truncated_cache_variable_prefix,\n",
    "    score_answer_with_cache,\n",
    "    deepcopy_cache,\n",
    "    extract_and_truncate_cache_with_bos,\n",
    "    correct_rope_positions_with_bos,\n",
    "    _get_cache_keys,\n",
    "    _get_cache_values,\n",
    ")\n",
    "from lib.analysis import cohens_d\n",
    "from lib.surrogate import STATIC_SURROGATE_QUERIES, generate_surrogate_with_template\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Paths\n",
    "RESULTS_DIR = Path('results/exp17')\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "FIGURES_DIR = RESULTS_DIR / 'figures'\n",
    "FIGURES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "SURROGATES_DIR = RESULTS_DIR / 'surrogates'\n",
    "SURROGATES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Model\n",
    "MODEL_NAME = 'mistralai/Mistral-7B-Instruct-v0.2'\n",
    "bnb_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.float16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME, quantization_config=bnb_config, device_map='auto'\n",
    ")\n",
    "config = ExperimentConfig(device='cuda')\n",
    "\n",
    "print(f'Model loaded: {MODEL_NAME}')\n",
    "print(f'Device: {config.device}')\n",
    "print(f'Results dir: {RESULTS_DIR}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Configuration\n",
    "\n",
    "# --- MS MARCO (short docs, replicating Exp 07 scale) ---\n",
    "N_MSMARCO = 300\n",
    "MSMARCO_CHECKPOINT_PATH = RESULTS_DIR / 'msmarco_checkpoint.json'\n",
    "MSMARCO_RESULTS_PATH = RESULTS_DIR / 'msmarco_results.json'\n",
    "CHECKPOINT_EVERY = 25\n",
    "\n",
    "# --- NQ (long docs, replicating Exp 11 scale) ---\n",
    "N_NQ = 300\n",
    "NQ_CHECKPOINT_PATH = RESULTS_DIR / 'nq_checkpoint.json'\n",
    "NQ_RESULTS_PATH = RESULTS_DIR / 'nq_results.json'\n",
    "NQ_SAMPLES_CACHE_PATH = RESULTS_DIR / 'nq_samples.json'\n",
    "\n",
    "# Length bins for NQ analysis (same as Exp 11)\n",
    "LENGTH_BINS = [\n",
    "    ('short',     100,  300),\n",
    "    ('medium',    300,  800),\n",
    "    ('long',      800,  2000),\n",
    "    ('very_long', 2000, 4000),\n",
    "]\n",
    "SAMPLES_PER_BIN = 75  # 75 * 4 = 300 total\n",
    "MAX_DOC_WORDS = 4000\n",
    "\n",
    "# Templates (matched to Exp 07/11)\n",
    "SURROGATE_PREFIX_TEMPLATE = '{surrogate}\\n'\n",
    "DOCUMENT_TEMPLATE = '{document}'\n",
    "QUERY_TEMPLATE = '\\nQuery: {query}\\nAnswer:'\n",
    "ANSWER_TEMPLATE = ' {answer}'\n",
    "\n",
    "# Static factual phrase\n",
    "STATIC_FACTUAL_PHRASE = STATIC_SURROGATE_QUERIES['static_factual']['query']\n",
    "\n",
    "# LLM keyword prompt\n",
    "LLM_KW_PROMPT = (\n",
    "    'You are helping index a document for search. Write a search query the way '\n",
    "    'real users type into Google: just keywords, no complete sentences, no question marks. '\n",
    "    'Think of someone quickly typing a few relevant words. '\n",
    "    'Output only the keyword query (3-6 words), nothing else.\\n\\n'\n",
    "    'Document:'\n",
    ")\n",
    "LLM_KW_MAX_DOC_WORDS = 500  # Truncate long docs for generation\n",
    "\n",
    "# Condition names\n",
    "CONDITION_NAMES = [\n",
    "    'bare',\n",
    "    'static_fact_old',\n",
    "    'static_fact_anchor',\n",
    "    'random_old',\n",
    "    'random_anchor',\n",
    "    'oracle_old',\n",
    "    'oracle_anchor',\n",
    "    'llm_kw_old',\n",
    "    'llm_kw_anchor',\n",
    "]\n",
    "\n",
    "print(f'MS MARCO: N={N_MSMARCO}, checkpoint every {CHECKPOINT_EVERY}')\n",
    "print(f'NQ: N={N_NQ}, {len(LENGTH_BINS)} length bins, {SAMPLES_PER_BIN} per bin')\n",
    "print(f'Conditions: {CONDITION_NAMES}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Explain Experimental Conditions\n",
    "print('=' * 70)\n",
    "print('EXPERIMENTAL CONDITIONS EXPLAINED')\n",
    "print('=' * 70)\n",
    "\n",
    "print('''\n",
    "Each sample is scored under 9 conditions. For primed conditions, we compare\n",
    "two truncation strategies:\n",
    "\n",
    "  _old    = Exp 07/11 method: truncate prefix + apply RoPE correction\n",
    "            Positions become: [BOS=0, doc_1=1, doc_2=2, ..., doc_D=D]\n",
    "            Query starts at position D+1\n",
    "\n",
    "  _anchor = NEW: truncate prefix + SKIP RoPE correction (anchor preservation)\n",
    "            Positions become: [BOS=0, <gap>, doc_1=P, doc_2=P+1, ..., doc_D=P+D-1]\n",
    "            Query starts at position P+D (preserving the positional gap)\n",
    "            BOS remains at position 0 as the attention sink anchor\n",
    "''')\n",
    "\n",
    "print('### bare ###')\n",
    "print('  Baseline: document cached in isolation, no priming.')\n",
    "print('  Cache: [BOS][doc tokens]')\n",
    "print('  Positions: [0, 1, 2, ..., D]')\n",
    "print()\n",
    "\n",
    "print('### static_fact_old ###')\n",
    "print(f'  Prefix: \"{STATIC_FACTUAL_PHRASE}\"')\n",
    "print('  Build: [BOS][prefix][doc] → truncate → RoPE correct')\n",
    "print('  Cache positions: [0, 1, 2, ..., D]  (gap closed)')\n",
    "print('  Replicates Exp 07 static_fact_trunc condition.')\n",
    "print()\n",
    "\n",
    "print('### static_fact_anchor ###')\n",
    "print(f'  Prefix: \"{STATIC_FACTUAL_PHRASE}\"')\n",
    "print('  Build: [BOS][prefix][doc] → truncate → NO RoPE correction')\n",
    "print('  Cache positions: [0, <gap>, P, P+1, ..., P+D-1]  (gap preserved)')\n",
    "print('  NEW: tests whether preserving the positional gap helps.')\n",
    "print()\n",
    "\n",
    "print('### random_old / random_anchor ###')\n",
    "print('  Prefix: random token string (same length as static_fact).')\n",
    "print('  Tests structural vs semantic effect with both truncation strategies.')\n",
    "print()\n",
    "\n",
    "print('### oracle_old / oracle_anchor ###')\n",
    "print('  Prefix: the actual query (perfect semantic match).')\n",
    "print('  Tests whether semantic content interacts with anchor preservation.')\n",
    "print()\n",
    "\n",
    "print('### llm_kw_old / llm_kw_anchor ###')\n",
    "print('  Prefix: LLM-generated keyword query from the passage.')\n",
    "print('  Tests practical surrogate generation with both strategies.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Helper Functions\n",
    "\n",
    "def build_matched_caches(passage, query, answer, keyword_surrogate, model, tokenizer, config):\n",
    "    \"\"\"Build bare + all condition caches for one sample with matched tokenization.\n",
    "    \n",
    "    Uses the oracle (longest) prefix to define the shared tokenization of the\n",
    "    document, ensuring all conditions compare identical document token sequences.\n",
    "    \n",
    "    Returns dict mapping condition name -> NLL.\n",
    "    \"\"\"\n",
    "    query_prompt = QUERY_TEMPLATE.format(query=query)\n",
    "    answer_text = ANSWER_TEMPLATE.format(answer=answer)\n",
    "    document_text = DOCUMENT_TEMPLATE.format(document=passage)\n",
    "    \n",
    "    # --- Matched tokenization from oracle (longest) prefix ---\n",
    "    oracle_prefix = SURROGATE_PREFIX_TEMPLATE.format(surrogate=query)\n",
    "    full_oracle_text = oracle_prefix + document_text\n",
    "    \n",
    "    full_oracle_enc = tokenizer(\n",
    "        full_oracle_text, return_tensors='pt', add_special_tokens=True,\n",
    "        padding=False, truncation=False\n",
    "    )\n",
    "    full_oracle_ids = full_oracle_enc['input_ids'].to(config.device)\n",
    "    \n",
    "    oracle_prefix_enc = tokenizer(\n",
    "        oracle_prefix, return_tensors='pt', add_special_tokens=True,\n",
    "        padding=False, truncation=False\n",
    "    )\n",
    "    oracle_prefix_len = oracle_prefix_enc['input_ids'].shape[1]\n",
    "    \n",
    "    bos_id = full_oracle_ids[:, :1]\n",
    "    doc_ids = full_oracle_ids[:, oracle_prefix_len:]\n",
    "    doc_len = doc_ids.shape[1]\n",
    "    \n",
    "    nll_results = {}\n",
    "    \n",
    "    # Helper: build primed cache with matched tokenization\n",
    "    def build_primed(prefix_text, apply_rope_correction):\n",
    "        \"\"\"Build a truncated cache from [BOS][prefix][doc] using matched doc tokens.\n",
    "        \n",
    "        Args:\n",
    "            prefix_text: The prefix string\n",
    "            apply_rope_correction: If True, apply RoPE correction (_old).\n",
    "                If False, skip it (_anchor).\n",
    "        \n",
    "        Returns:\n",
    "            (cache, keep_len, position_offset) where position_offset > 0 only\n",
    "            when apply_rope_correction is False.\n",
    "        \"\"\"\n",
    "        prefix_str = SURROGATE_PREFIX_TEMPLATE.format(surrogate=prefix_text)\n",
    "        prefix_enc = tokenizer(\n",
    "            prefix_str, return_tensors='pt', add_special_tokens=False,\n",
    "            padding=False, truncation=False\n",
    "        )\n",
    "        prefix_ids = prefix_enc['input_ids'].to(config.device)\n",
    "        prefix_token_len = 1 + prefix_ids.shape[1]  # +1 for BOS\n",
    "        \n",
    "        full_ids = torch.cat([bos_id, prefix_ids, doc_ids], dim=1)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            out = model(\n",
    "                input_ids=full_ids,\n",
    "                attention_mask=torch.ones_like(full_ids),\n",
    "                use_cache=True, return_dict=True\n",
    "            )\n",
    "        \n",
    "        cache = extract_and_truncate_cache_with_bos(out.past_key_values, doc_len)\n",
    "        keep_len = 1 + doc_len\n",
    "        \n",
    "        surrogate_offset = prefix_token_len - 1\n",
    "        if apply_rope_correction:\n",
    "            correct_rope_positions_with_bos(cache, surrogate_offset, model)\n",
    "            position_offset = 0\n",
    "        else:\n",
    "            position_offset = surrogate_offset\n",
    "        \n",
    "        del out\n",
    "        return cache, keep_len, position_offset\n",
    "    \n",
    "    def score(cache, keep_len, position_offset):\n",
    "        return score_answer_with_cache(\n",
    "            deepcopy_cache(cache), keep_len, query_prompt, answer_text,\n",
    "            model, tokenizer, config, position_offset=position_offset\n",
    "        )\n",
    "    \n",
    "    # --- 1. Bare ---\n",
    "    bare_ids = torch.cat([bos_id, doc_ids], dim=1)\n",
    "    with torch.no_grad():\n",
    "        bare_out = model(\n",
    "            input_ids=bare_ids,\n",
    "            attention_mask=torch.ones_like(bare_ids),\n",
    "            use_cache=True, return_dict=True\n",
    "        )\n",
    "    bare_cache = bare_out.past_key_values\n",
    "    nll_results['bare'] = score_answer_with_cache(\n",
    "        deepcopy_cache(bare_cache), bare_ids.shape[1],\n",
    "        query_prompt, answer_text, model, tokenizer, config\n",
    "    )\n",
    "    del bare_cache, bare_out\n",
    "    \n",
    "    # --- 2. Static fact ---\n",
    "    for suffix, apply_rope in [('old', True), ('anchor', False)]:\n",
    "        cache, kl, po = build_primed(STATIC_FACTUAL_PHRASE, apply_rope)\n",
    "        nll_results[f'static_fact_{suffix}'] = score(cache, kl, po)\n",
    "        del cache\n",
    "    \n",
    "    # --- 3. Random ---\n",
    "    n_random_tokens = max(5, len(tokenizer.encode(\n",
    "        STATIC_FACTUAL_PHRASE, add_special_tokens=False)))\n",
    "    random_ids = torch.randint(\n",
    "        100, tokenizer.vocab_size - 100, (n_random_tokens,), device='cpu'\n",
    "    )\n",
    "    random_text = tokenizer.decode(random_ids, skip_special_tokens=True)\n",
    "    \n",
    "    for suffix, apply_rope in [('old', True), ('anchor', False)]:\n",
    "        cache, kl, po = build_primed(random_text, apply_rope)\n",
    "        nll_results[f'random_{suffix}'] = score(cache, kl, po)\n",
    "        del cache\n",
    "    \n",
    "    # --- 4. Oracle ---\n",
    "    for suffix, apply_rope in [('old', True), ('anchor', False)]:\n",
    "        cache, kl, po = build_primed(query, apply_rope)\n",
    "        nll_results[f'oracle_{suffix}'] = score(cache, kl, po)\n",
    "        del cache\n",
    "    \n",
    "    # --- 5. LLM keyword ---\n",
    "    if keyword_surrogate and keyword_surrogate.strip():\n",
    "        for suffix, apply_rope in [('old', True), ('anchor', False)]:\n",
    "            cache, kl, po = build_primed(keyword_surrogate, apply_rope)\n",
    "            nll_results[f'llm_kw_{suffix}'] = score(cache, kl, po)\n",
    "            del cache\n",
    "    else:\n",
    "        nll_results['llm_kw_old'] = 0.0\n",
    "        nll_results['llm_kw_anchor'] = 0.0\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    return nll_results, doc_len\n",
    "\n",
    "\n",
    "def run_analysis(results, condition_names, dataset_label):\n",
    "    \"\"\"Run statistical analysis on experiment results.\n",
    "    \n",
    "    Returns analysis dict suitable for JSON serialization.\n",
    "    \"\"\"\n",
    "    cond_arrays = {cn: np.array([r[cn] for r in results]) for cn in condition_names}\n",
    "    \n",
    "    # Filter out failed samples (llm_kw can be 0.0 if generation failed)\n",
    "    valid = np.ones(len(results), dtype=bool)\n",
    "    for cn in condition_names:\n",
    "        valid &= (cond_arrays[cn] != 0)\n",
    "    n_valid = int(np.sum(valid))\n",
    "    n_excluded = len(results) - n_valid\n",
    "    \n",
    "    c = {cn: cond_arrays[cn][valid] for cn in condition_names}\n",
    "    \n",
    "    print(f'\\n{\"=\" * 70}')\n",
    "    print(f'{dataset_label} ANALYSIS (n_valid={n_valid}, excluded={n_excluded})')\n",
    "    print('=' * 70)\n",
    "    \n",
    "    # --- All conditions vs bare ---\n",
    "    print(f'\\n{\"Condition\":<25} {\"Mean Δ\":>10} {\"d\":>8} {\"Win%\":>7} {\"t\":>8} {\"p\":>12} {\"sig\":>5}')\n",
    "    print('-' * 75)\n",
    "    \n",
    "    all_vs_bare = {}\n",
    "    for cn in condition_names:\n",
    "        if cn == 'bare':\n",
    "            print(f'{cn:<25} {np.mean(c[cn]):>10.4f} {\"---\":>8} {\"---\":>7}')\n",
    "            continue\n",
    "        delta = c['bare'] - c[cn]  # positive = condition is better\n",
    "        d = cohens_d(delta)\n",
    "        win = np.mean(delta > 0) * 100\n",
    "        t_stat, p_val = stats.ttest_1samp(delta, 0)\n",
    "        sig = '***' if p_val < 0.001 else '**' if p_val < 0.01 else '*' if p_val < 0.05 else 'ns'\n",
    "        print(f'{cn:<25} {np.mean(delta):>10.4f} {d:>+8.3f} {win:>6.1f}% {t_stat:>8.2f} {p_val:>12.2e} {sig:>5}')\n",
    "        all_vs_bare[cn] = {\n",
    "            'mean_delta': float(np.mean(delta)),\n",
    "            'cohens_d': float(d),\n",
    "            'win_pct': float(win),\n",
    "            't_stat': float(t_stat),\n",
    "            'p_value': float(p_val),\n",
    "        }\n",
    "    \n",
    "    # --- Head-to-head: old vs anchor ---\n",
    "    print(f'\\n--- Old vs Anchor (head-to-head) ---')\n",
    "    print(f'{\"Comparison\":<35} {\"Mean Δ\":>10} {\"d\":>8} {\"Win%\":>7} {\"p\":>12} {\"sig\":>5}')\n",
    "    print('-' * 75)\n",
    "    \n",
    "    h2h = {}\n",
    "    for prefix in ['static_fact', 'random', 'oracle', 'llm_kw']:\n",
    "        old_cn = f'{prefix}_old'\n",
    "        new_cn = f'{prefix}_anchor'\n",
    "        delta = c[old_cn] - c[new_cn]  # positive = anchor is better (lower NLL)\n",
    "        d = cohens_d(delta)\n",
    "        win = np.mean(delta > 0) * 100\n",
    "        t_stat, p_val = stats.ttest_1samp(delta, 0)\n",
    "        sig = '***' if p_val < 0.001 else '**' if p_val < 0.01 else '*' if p_val < 0.05 else 'ns'\n",
    "        label = f'{old_cn} vs {new_cn}'\n",
    "        print(f'{label:<35} {np.mean(delta):>10.4f} {d:>+8.3f} {win:>6.1f}% {p_val:>12.2e} {sig:>5}')\n",
    "        h2h[prefix] = {\n",
    "            'mean_delta': float(np.mean(delta)),\n",
    "            'cohens_d': float(d),\n",
    "            'anchor_win_pct': float(win),\n",
    "            'p_value': float(p_val),\n",
    "        }\n",
    "    \n",
    "    # --- Hardness stratification ---\n",
    "    print(f'\\n--- Hardness Stratification (quintiles by bare NLL) ---')\n",
    "    bare_valid = c['bare']\n",
    "    quintile_bounds = np.percentile(bare_valid, [20, 40, 60, 80])\n",
    "    quintile_labels = ['Q1 (easy)', 'Q2', 'Q3', 'Q4', 'Q5 (hard)']\n",
    "    \n",
    "    def get_quintile(nll):\n",
    "        for i, b in enumerate(quintile_bounds):\n",
    "            if nll <= b:\n",
    "                return i\n",
    "        return len(quintile_bounds)\n",
    "    \n",
    "    quintiles = np.array([get_quintile(nll) for nll in bare_valid])\n",
    "    \n",
    "    hardness = {}\n",
    "    key_conds = ['static_fact_old', 'static_fact_anchor', 'oracle_old', 'oracle_anchor']\n",
    "    header = f'{\"Condition\":<25}'\n",
    "    for ql in quintile_labels:\n",
    "        header += f'{ql:>14}'\n",
    "    print(header)\n",
    "    print('-' * (25 + 14 * 5))\n",
    "    \n",
    "    for cn in key_conds:\n",
    "        row = f'{cn:<25}'\n",
    "        cond_hardness = {}\n",
    "        for q in range(5):\n",
    "            mask_q = quintiles == q\n",
    "            n_q = int(np.sum(mask_q))\n",
    "            if n_q < 5:\n",
    "                row += f'{\"n/a\":>14}'\n",
    "                continue\n",
    "            delta = bare_valid[mask_q] - c[cn][mask_q]\n",
    "            d = cohens_d(delta)\n",
    "            row += f'{d:>+14.3f}'\n",
    "            cond_hardness[quintile_labels[q]] = float(d)\n",
    "        print(row)\n",
    "        hardness[cn] = cond_hardness\n",
    "    \n",
    "    return {\n",
    "        'n_valid': n_valid,\n",
    "        'n_excluded': n_excluded,\n",
    "        'all_vs_bare': all_vs_bare,\n",
    "        'head_to_head': h2h,\n",
    "        'hardness': hardness,\n",
    "    }\n",
    "\n",
    "\n",
    "print('Helper functions defined.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: MS MARCO Evaluation (Short Docs)\n",
    "\n",
    "from lib.data import load_ms_marco, load_evaluation_samples\n",
    "\n",
    "# Load data\n",
    "msmarco_dataset = load_ms_marco(config)\n",
    "all_msmarco_samples = load_evaluation_samples(msmarco_dataset, config, require_answer=True)\n",
    "msmarco_samples = all_msmarco_samples[:N_MSMARCO]\n",
    "print(f'MS MARCO samples: {len(msmarco_samples)}')\n",
    "\n",
    "# Generate LLM keyword surrogates (or load from cache)\n",
    "msmarco_surr_path = SURROGATES_DIR / 'msmarco_keyword_surrogates.json'\n",
    "if msmarco_surr_path.exists():\n",
    "    with open(msmarco_surr_path, 'r') as f:\n",
    "        msmarco_kw_surrogates = json.load(f)['surrogates']\n",
    "else:\n",
    "    msmarco_kw_surrogates = []\n",
    "\n",
    "start_gen = len(msmarco_kw_surrogates)\n",
    "if start_gen < N_MSMARCO:\n",
    "    print(f'Generating keyword surrogates {start_gen}..{N_MSMARCO}')\n",
    "    for idx in tqdm(range(start_gen, N_MSMARCO), initial=start_gen, total=N_MSMARCO,\n",
    "                    desc='Keyword surrogates'):\n",
    "        passage = msmarco_samples[idx]['passage']\n",
    "        try:\n",
    "            kw = generate_surrogate_with_template(\n",
    "                passage, LLM_KW_PROMPT, model, tokenizer, config)\n",
    "        except Exception:\n",
    "            kw = ''\n",
    "        msmarco_kw_surrogates.append(kw)\n",
    "        if (idx + 1) % 50 == 0 or idx == N_MSMARCO - 1:\n",
    "            with open(msmarco_surr_path, 'w') as f:\n",
    "                json.dump({'surrogates': msmarco_kw_surrogates}, f)\n",
    "    print('Surrogates generated.')\n",
    "\n",
    "# Evaluate\n",
    "msmarco_results = []\n",
    "start_idx = 0\n",
    "\n",
    "if MSMARCO_CHECKPOINT_PATH.exists():\n",
    "    with open(MSMARCO_CHECKPOINT_PATH, 'r') as f:\n",
    "        ckpt = json.load(f)\n",
    "    ckpt_queries = ckpt.get('sample_queries', [])\n",
    "    current_queries = [s['query'] for s in msmarco_samples]\n",
    "    if ckpt_queries == current_queries:\n",
    "        msmarco_results = ckpt['results']\n",
    "        start_idx = len(msmarco_results)\n",
    "        print(f'Resuming from checkpoint: {start_idx}/{N_MSMARCO}')\n",
    "    else:\n",
    "        print('Checkpoint sample mismatch. Starting fresh.')\n",
    "\n",
    "for idx in tqdm(range(start_idx, N_MSMARCO), initial=start_idx, total=N_MSMARCO,\n",
    "                desc='MS MARCO eval'):\n",
    "    sample = msmarco_samples[idx]\n",
    "    kw_surr = msmarco_kw_surrogates[idx] if idx < len(msmarco_kw_surrogates) else ''\n",
    "    \n",
    "    nll_dict, doc_len = build_matched_caches(\n",
    "        sample['passage'], sample['query'], sample['answer'],\n",
    "        kw_surr, model, tokenizer, config\n",
    "    )\n",
    "    \n",
    "    result = {\n",
    "        'idx': idx,\n",
    "        'doc_len_tokens': doc_len,\n",
    "        'word_count': len(sample['passage'].split()),\n",
    "        **nll_dict,\n",
    "    }\n",
    "    msmarco_results.append(result)\n",
    "    \n",
    "    if (idx + 1) % CHECKPOINT_EVERY == 0 or idx == N_MSMARCO - 1:\n",
    "        ckpt_data = {\n",
    "            'results': msmarco_results,\n",
    "            'sample_queries': [s['query'] for s in msmarco_samples],\n",
    "            'completed': len(msmarco_results),\n",
    "            'total': N_MSMARCO,\n",
    "            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        }\n",
    "        with open(MSMARCO_CHECKPOINT_PATH, 'w') as f:\n",
    "            json.dump(ckpt_data, f)\n",
    "\n",
    "print(f'MS MARCO evaluation complete: {len(msmarco_results)} samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: MS MARCO Analysis\n",
    "\n",
    "msmarco_analysis = run_analysis(msmarco_results, CONDITION_NAMES, 'MS MARCO (Short Docs)')\n",
    "\n",
    "# Save\n",
    "msmarco_final = {\n",
    "    'experiment': 'exp17_anchor_preservation_msmarco',\n",
    "    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'config': {\n",
    "        'model_name': MODEL_NAME,\n",
    "        'seed': SEED,\n",
    "        'n_eval': N_MSMARCO,\n",
    "        'dataset': 'MS MARCO v1.1',\n",
    "    },\n",
    "    'condition_names': CONDITION_NAMES,\n",
    "    'analysis': msmarco_analysis,\n",
    "    'per_sample_results': msmarco_results,\n",
    "}\n",
    "with open(MSMARCO_RESULTS_PATH, 'w') as f:\n",
    "    json.dump(msmarco_final, f, indent=2)\n",
    "print(f'\\nSaved to {MSMARCO_RESULTS_PATH}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: NQ Evaluation (Long Docs)\n",
    "\n",
    "# --- Load NQ samples (stratified by length) ---\n",
    "if NQ_SAMPLES_CACHE_PATH.exists():\n",
    "    with open(NQ_SAMPLES_CACHE_PATH, 'r') as f:\n",
    "        nq_cache = json.load(f)\n",
    "    nq_samples = nq_cache['samples']\n",
    "    print(f'Loaded {len(nq_samples)} NQ samples from cache')\n",
    "else:\n",
    "    print('Loading NQ dataset (streaming)...')\n",
    "    nq = load_dataset(\n",
    "        'google-research-datasets/natural_questions',\n",
    "        split='validation',\n",
    "        streaming=True,\n",
    "    )\n",
    "    \n",
    "    bin_samples = {name: [] for name, _, _ in LENGTH_BINS}\n",
    "    n_processed = 0\n",
    "    \n",
    "    for example in tqdm(nq, desc='Processing NQ'):\n",
    "        n_processed += 1\n",
    "        \n",
    "        # Extract clean document text\n",
    "        doc_tokens = example['document']['tokens']\n",
    "        if isinstance(doc_tokens, dict):\n",
    "            token_strs = doc_tokens['token']\n",
    "            is_html_flags = doc_tokens['is_html']\n",
    "            clean_tokens = [t for t, h in zip(token_strs, is_html_flags) if not h]\n",
    "        else:\n",
    "            clean_tokens = [t['token'] for t in doc_tokens if not t['is_html']]\n",
    "        \n",
    "        doc_text = ' '.join(clean_tokens)\n",
    "        word_count = len(doc_text.split())\n",
    "        \n",
    "        if word_count < LENGTH_BINS[0][1]:\n",
    "            continue\n",
    "        if word_count > MAX_DOC_WORDS:\n",
    "            words = doc_text.split()\n",
    "            doc_text = ' '.join(words[:MAX_DOC_WORDS])\n",
    "            word_count = MAX_DOC_WORDS\n",
    "        \n",
    "        # Extract short answer\n",
    "        annotations = example['annotations']\n",
    "        short_answers_list = annotations['short_answers']\n",
    "        \n",
    "        answer_text = None\n",
    "        for annotator_sa in short_answers_list:\n",
    "            if not annotator_sa:\n",
    "                continue\n",
    "            texts = annotator_sa.get('text', [])\n",
    "            if texts:\n",
    "                answer_text = texts[0]\n",
    "                break\n",
    "            starts = annotator_sa.get('start_token', [])\n",
    "            ends = annotator_sa.get('end_token', [])\n",
    "            if not starts or not ends:\n",
    "                continue\n",
    "            start_tok = starts[0] if isinstance(starts, list) else starts\n",
    "            end_tok = ends[0] if isinstance(ends, list) else ends\n",
    "            if start_tok >= 0 and end_tok > start_tok:\n",
    "                if isinstance(doc_tokens, dict):\n",
    "                    ans_tokens = [\n",
    "                        doc_tokens['token'][i]\n",
    "                        for i in range(start_tok, min(end_tok, len(doc_tokens['token'])))\n",
    "                        if not doc_tokens['is_html'][i]\n",
    "                    ]\n",
    "                else:\n",
    "                    ans_tokens = [\n",
    "                        doc_tokens[i]['token']\n",
    "                        for i in range(start_tok, min(end_tok, len(doc_tokens)))\n",
    "                        if not doc_tokens[i]['is_html']\n",
    "                    ]\n",
    "                if ans_tokens:\n",
    "                    answer_text = ' '.join(ans_tokens)\n",
    "                    break\n",
    "        \n",
    "        if not answer_text or len(answer_text.strip()) == 0:\n",
    "            continue\n",
    "        if len(answer_text.split()) > 20:\n",
    "            continue\n",
    "        \n",
    "        question = example['question']\n",
    "        query = question.get('text', '') if isinstance(question, dict) else str(question)\n",
    "        \n",
    "        # Assign to length bin\n",
    "        for bin_name, bin_min, bin_max in LENGTH_BINS:\n",
    "            if bin_min <= word_count < bin_max:\n",
    "                if len(bin_samples[bin_name]) < SAMPLES_PER_BIN:\n",
    "                    bin_samples[bin_name].append({\n",
    "                        'passage': doc_text,\n",
    "                        'query': query,\n",
    "                        'answer': answer_text,\n",
    "                        'word_count': word_count,\n",
    "                        'length_bin': bin_name,\n",
    "                    })\n",
    "                break\n",
    "        \n",
    "        if all(len(bin_samples[name]) >= SAMPLES_PER_BIN for name, _, _ in LENGTH_BINS):\n",
    "            break\n",
    "    \n",
    "    # Combine\n",
    "    nq_samples = []\n",
    "    for bin_name, _, _ in LENGTH_BINS:\n",
    "        bs = bin_samples[bin_name]\n",
    "        np.random.seed(SEED)\n",
    "        np.random.shuffle(bs)\n",
    "        nq_samples.extend(bs)\n",
    "        print(f'  {bin_name}: {len(bs)} samples')\n",
    "    \n",
    "    with open(NQ_SAMPLES_CACHE_PATH, 'w') as f:\n",
    "        json.dump({'samples': nq_samples, 'n_processed': n_processed}, f)\n",
    "    print(f'Saved {len(nq_samples)} NQ samples')\n",
    "\n",
    "N_NQ = min(N_NQ, len(nq_samples))\n",
    "print(f'NQ samples to evaluate: {N_NQ}')\n",
    "\n",
    "# Generate NQ keyword surrogates\n",
    "nq_surr_path = SURROGATES_DIR / 'nq_keyword_surrogates.json'\n",
    "if nq_surr_path.exists():\n",
    "    with open(nq_surr_path, 'r') as f:\n",
    "        nq_kw_surrogates = json.load(f)['surrogates']\n",
    "else:\n",
    "    nq_kw_surrogates = []\n",
    "\n",
    "start_gen = len(nq_kw_surrogates)\n",
    "if start_gen < N_NQ:\n",
    "    print(f'Generating NQ keyword surrogates {start_gen}..{N_NQ}')\n",
    "    for idx in tqdm(range(start_gen, N_NQ), initial=start_gen, total=N_NQ,\n",
    "                    desc='NQ keyword surrogates'):\n",
    "        passage = nq_samples[idx]['passage']\n",
    "        words = passage.split()\n",
    "        if len(words) > LLM_KW_MAX_DOC_WORDS:\n",
    "            passage_for_gen = ' '.join(words[:LLM_KW_MAX_DOC_WORDS])\n",
    "        else:\n",
    "            passage_for_gen = passage\n",
    "        try:\n",
    "            kw = generate_surrogate_with_template(\n",
    "                passage_for_gen, LLM_KW_PROMPT, model, tokenizer, config)\n",
    "        except Exception:\n",
    "            kw = ''\n",
    "        nq_kw_surrogates.append(kw)\n",
    "        if (idx + 1) % 50 == 0 or idx == N_NQ - 1:\n",
    "            with open(nq_surr_path, 'w') as f:\n",
    "                json.dump({'surrogates': nq_kw_surrogates}, f)\n",
    "    print('NQ surrogates generated.')\n",
    "\n",
    "# Evaluate\n",
    "nq_results = []\n",
    "start_idx = 0\n",
    "\n",
    "if NQ_CHECKPOINT_PATH.exists():\n",
    "    with open(NQ_CHECKPOINT_PATH, 'r') as f:\n",
    "        ckpt = json.load(f)\n",
    "    ckpt_queries = ckpt.get('sample_queries', [])\n",
    "    current_queries = [s['query'] for s in nq_samples[:N_NQ]]\n",
    "    if ckpt_queries == current_queries:\n",
    "        nq_results = ckpt['results']\n",
    "        start_idx = len(nq_results)\n",
    "        print(f'Resuming from checkpoint: {start_idx}/{N_NQ}')\n",
    "    else:\n",
    "        print('Checkpoint sample mismatch. Starting fresh.')\n",
    "\n",
    "for idx in tqdm(range(start_idx, N_NQ), initial=start_idx, total=N_NQ,\n",
    "                desc='NQ eval'):\n",
    "    sample = nq_samples[idx]\n",
    "    kw_surr = nq_kw_surrogates[idx] if idx < len(nq_kw_surrogates) else ''\n",
    "    \n",
    "    nll_dict, doc_len = build_matched_caches(\n",
    "        sample['passage'], sample['query'], sample['answer'],\n",
    "        kw_surr, model, tokenizer, config\n",
    "    )\n",
    "    \n",
    "    result = {\n",
    "        'idx': idx,\n",
    "        'doc_len_tokens': doc_len,\n",
    "        'word_count': sample['word_count'],\n",
    "        'length_bin': sample['length_bin'],\n",
    "        **nll_dict,\n",
    "    }\n",
    "    nq_results.append(result)\n",
    "    \n",
    "    if (idx + 1) % CHECKPOINT_EVERY == 0 or idx == N_NQ - 1:\n",
    "        ckpt_data = {\n",
    "            'results': nq_results,\n",
    "            'sample_queries': [s['query'] for s in nq_samples[:N_NQ]],\n",
    "            'completed': len(nq_results),\n",
    "            'total': N_NQ,\n",
    "            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        }\n",
    "        with open(NQ_CHECKPOINT_PATH, 'w') as f:\n",
    "            json.dump(ckpt_data, f)\n",
    "\n",
    "print(f'NQ evaluation complete: {len(nq_results)} samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: NQ Analysis\n",
    "\n",
    "nq_analysis = run_analysis(nq_results, CONDITION_NAMES, 'NQ (Long Docs)')\n",
    "\n",
    "# --- Per Length Bin Analysis ---\n",
    "print(f'\\n{\"=\" * 70}')\n",
    "print('NQ: PER LENGTH BIN ANALYSIS')\n",
    "print('=' * 70)\n",
    "\n",
    "cond_arrays = {cn: np.array([r[cn] for r in nq_results]) for cn in CONDITION_NAMES}\n",
    "valid = np.ones(len(nq_results), dtype=bool)\n",
    "for cn in CONDITION_NAMES:\n",
    "    valid &= (cond_arrays[cn] != 0)\n",
    "c = {cn: cond_arrays[cn][valid] for cn in CONDITION_NAMES}\n",
    "\n",
    "length_bins_arr = np.array([r['length_bin'] for r in nq_results])[valid]\n",
    "word_counts_arr = np.array([r['word_count'] for r in nq_results])[valid]\n",
    "\n",
    "bin_names_ordered = [name for name, _, _ in LENGTH_BINS]\n",
    "per_bin_results = {}\n",
    "\n",
    "for cn in CONDITION_NAMES:\n",
    "    if cn == 'bare':\n",
    "        continue\n",
    "    print(f'\\n  {cn}:')\n",
    "    bin_ds = {}\n",
    "    for bin_name in bin_names_ordered:\n",
    "        mask = length_bins_arr == bin_name\n",
    "        n_bin = int(np.sum(mask))\n",
    "        if n_bin < 5:\n",
    "            continue\n",
    "        delta = c['bare'][mask] - c[cn][mask]\n",
    "        d = cohens_d(delta)\n",
    "        win = np.mean(delta > 0) * 100\n",
    "        _, p_val = stats.ttest_1samp(delta, 0)\n",
    "        sig = '***' if p_val < 0.001 else '**' if p_val < 0.01 else '*' if p_val < 0.05 else 'ns'\n",
    "        print(f'    {bin_name}: n={n_bin}, d={d:+.3f}, win={win:.1f}%, p={p_val:.2e} {sig}')\n",
    "        bin_ds[bin_name] = {'d': float(d), 'win': float(win), 'n': n_bin, 'p': float(p_val)}\n",
    "    per_bin_results[cn] = bin_ds\n",
    "\n",
    "# --- Old vs Anchor by length bin ---\n",
    "print(f'\\n--- Old vs Anchor by Length Bin (CRITICAL TEST) ---')\n",
    "print(f'{\"\":<15}', end='')\n",
    "for bn in bin_names_ordered:\n",
    "    print(f'{bn:>15}', end='')\n",
    "print()\n",
    "print('-' * (15 + 15 * len(bin_names_ordered)))\n",
    "\n",
    "h2h_by_bin = {}\n",
    "for prefix in ['static_fact', 'random', 'oracle', 'llm_kw']:\n",
    "    old_cn = f'{prefix}_old'\n",
    "    new_cn = f'{prefix}_anchor'\n",
    "    row = f'{prefix:<15}'\n",
    "    h2h_bin = {}\n",
    "    for bn in bin_names_ordered:\n",
    "        mask = length_bins_arr == bn\n",
    "        n_bin = int(np.sum(mask))\n",
    "        if n_bin < 5:\n",
    "            row += f'{\"n/a\":>15}'\n",
    "            continue\n",
    "        delta = c[old_cn][mask] - c[new_cn][mask]  # positive = anchor wins\n",
    "        d = cohens_d(delta)\n",
    "        win = np.mean(delta > 0) * 100\n",
    "        row += f'{d:>+7.3f} ({win:4.0f}%)'\n",
    "        h2h_bin[bn] = {'d': float(d), 'win': float(win)}\n",
    "    print(row)\n",
    "    h2h_by_bin[prefix] = h2h_bin\n",
    "\n",
    "# --- Length correlation ---\n",
    "print(f'\\n--- Length × Effect Size Correlation ---')\n",
    "interaction_results = {}\n",
    "for cn in CONDITION_NAMES:\n",
    "    if cn == 'bare':\n",
    "        continue\n",
    "    delta = c['bare'] - c[cn]\n",
    "    r_spear, p_spear = spearmanr(word_counts_arr, delta)\n",
    "    r_pears, p_pears = pearsonr(word_counts_arr, delta)\n",
    "    print(f'  {cn}: Spearman r={r_spear:+.3f} (p={p_spear:.3f}), '\n",
    "          f'Pearson r={r_pears:+.3f} (p={p_pears:.3f})')\n",
    "    interaction_results[cn] = {\n",
    "        'spearman_r': float(r_spear), 'spearman_p': float(p_spear),\n",
    "        'pearson_r': float(r_pears), 'pearson_p': float(p_pears),\n",
    "    }\n",
    "\n",
    "nq_analysis['per_bin'] = per_bin_results\n",
    "nq_analysis['h2h_by_bin'] = h2h_by_bin\n",
    "nq_analysis['length_interaction'] = interaction_results\n",
    "\n",
    "# Save\n",
    "nq_final = {\n",
    "    'experiment': 'exp17_anchor_preservation_nq',\n",
    "    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'config': {\n",
    "        'model_name': MODEL_NAME,\n",
    "        'seed': SEED,\n",
    "        'n_eval': N_NQ,\n",
    "        'dataset': 'google-research-datasets/natural_questions',\n",
    "        'length_bins': LENGTH_BINS,\n",
    "    },\n",
    "    'condition_names': CONDITION_NAMES,\n",
    "    'analysis': nq_analysis,\n",
    "    'per_sample_results': nq_results,\n",
    "}\n",
    "with open(NQ_RESULTS_PATH, 'w') as f:\n",
    "    json.dump(nq_final, f, indent=2)\n",
    "print(f'\\nSaved to {NQ_RESULTS_PATH}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Summary & Visualization\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# --- Plot 1: Effect sizes comparison (old vs anchor) ---\n",
    "ax = axes[0]\n",
    "prefixes = ['static_fact', 'random', 'oracle', 'llm_kw']\n",
    "x = np.arange(len(prefixes))\n",
    "width = 0.35\n",
    "\n",
    "# MS MARCO data\n",
    "msmarco_c = {}\n",
    "msmarco_arr = {cn: np.array([r[cn] for r in msmarco_results]) for cn in CONDITION_NAMES}\n",
    "msmarco_valid = np.ones(len(msmarco_results), dtype=bool)\n",
    "for cn in CONDITION_NAMES:\n",
    "    msmarco_valid &= (msmarco_arr[cn] != 0)\n",
    "msmarco_c = {cn: msmarco_arr[cn][msmarco_valid] for cn in CONDITION_NAMES}\n",
    "\n",
    "old_d_msmarco = [cohens_d(msmarco_c['bare'] - msmarco_c[f'{p}_old']) for p in prefixes]\n",
    "anchor_d_msmarco = [cohens_d(msmarco_c['bare'] - msmarco_c[f'{p}_anchor']) for p in prefixes]\n",
    "\n",
    "bars1 = ax.bar(x - width/2, old_d_msmarco, width, label='Old (RoPE corrected)', color='#2196F3', alpha=0.8)\n",
    "bars2 = ax.bar(x + width/2, anchor_d_msmarco, width, label='Anchor (gap preserved)', color='#FF9800', alpha=0.8)\n",
    "\n",
    "ax.set_ylabel(\"Cohen's d vs bare\")\n",
    "ax.set_title('MS MARCO (Short Docs)')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(prefixes, rotation=15)\n",
    "ax.legend()\n",
    "ax.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "ax.set_ylim(min(min(old_d_msmarco), min(anchor_d_msmarco)) - 0.1,\n",
    "            max(max(old_d_msmarco), max(anchor_d_msmarco)) + 0.1)\n",
    "\n",
    "# --- Plot 2: NQ effect sizes by length bin ---\n",
    "ax = axes[1]\n",
    "\n",
    "# For NQ, show static_fact old vs anchor across length bins\n",
    "nq_c = {}\n",
    "nq_arr = {cn: np.array([r[cn] for r in nq_results]) for cn in CONDITION_NAMES}\n",
    "nq_valid = np.ones(len(nq_results), dtype=bool)\n",
    "for cn in CONDITION_NAMES:\n",
    "    nq_valid &= (nq_arr[cn] != 0)\n",
    "nq_c = {cn: nq_arr[cn][nq_valid] for cn in CONDITION_NAMES}\n",
    "nq_bins_arr = np.array([r['length_bin'] for r in nq_results])[nq_valid]\n",
    "\n",
    "old_d_bins = []\n",
    "anchor_d_bins = []\n",
    "for bn in bin_names_ordered:\n",
    "    mask = nq_bins_arr == bn\n",
    "    if np.sum(mask) < 5:\n",
    "        old_d_bins.append(0)\n",
    "        anchor_d_bins.append(0)\n",
    "    else:\n",
    "        old_d_bins.append(cohens_d(nq_c['bare'][mask] - nq_c['static_fact_old'][mask]))\n",
    "        anchor_d_bins.append(cohens_d(nq_c['bare'][mask] - nq_c['static_fact_anchor'][mask]))\n",
    "\n",
    "x2 = np.arange(len(bin_names_ordered))\n",
    "bars3 = ax.bar(x2 - width/2, old_d_bins, width, label='Old (RoPE corrected)', color='#2196F3', alpha=0.8)\n",
    "bars4 = ax.bar(x2 + width/2, anchor_d_bins, width, label='Anchor (gap preserved)', color='#FF9800', alpha=0.8)\n",
    "\n",
    "ax.set_ylabel(\"Cohen's d vs bare\")\n",
    "ax.set_title('NQ: static_fact by Length Bin')\n",
    "ax.set_xticks(x2)\n",
    "ax.set_xticklabels(bin_names_ordered, rotation=15)\n",
    "ax.legend()\n",
    "ax.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'effect_sizes_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f'Saved to {FIGURES_DIR / \"effect_sizes_comparison.png\"}')\n",
    "\n",
    "# --- Length interaction plot ---\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "for cn, color, marker in [\n",
    "    ('static_fact_old', '#2196F3', 'o'),\n",
    "    ('static_fact_anchor', '#FF9800', 's'),\n",
    "    ('oracle_old', '#4CAF50', '^'),\n",
    "    ('oracle_anchor', '#F44336', 'D'),\n",
    "]:\n",
    "    ds = []\n",
    "    bin_centers = []\n",
    "    for bn, bmin, bmax in LENGTH_BINS:\n",
    "        mask = nq_bins_arr == bn\n",
    "        if np.sum(mask) < 5:\n",
    "            continue\n",
    "        d = cohens_d(nq_c['bare'][mask] - nq_c[cn][mask])\n",
    "        ds.append(d)\n",
    "        bin_centers.append((bmin + bmax) / 2)\n",
    "    ax.plot(bin_centers, ds, marker=marker, label=cn, color=color, linewidth=2, markersize=8)\n",
    "\n",
    "ax.set_xlabel('Document Length (words, bin center)')\n",
    "ax.set_ylabel(\"Cohen's d vs bare\")\n",
    "ax.set_title('NQ: Length × Method Interaction')\n",
    "ax.legend()\n",
    "ax.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "ax.set_xscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'length_interaction.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f'Saved to {FIGURES_DIR / \"length_interaction.png\"}')\n",
    "\n",
    "# --- Summary ---\n",
    "print('\\n' + '=' * 70)\n",
    "print('EXPERIMENT 17 SUMMARY')\n",
    "print('=' * 70)\n",
    "print()\n",
    "print('Key question: Does anchor preservation (skipping RoPE correction)')\n",
    "print('recover priming performance on long documents?')\n",
    "print()\n",
    "\n",
    "# Print summary comparison\n",
    "for dataset_name, analysis in [('MS MARCO', msmarco_analysis), ('NQ', nq_analysis)]:\n",
    "    print(f'\\n--- {dataset_name} ---')\n",
    "    h2h = analysis.get('head_to_head', {})\n",
    "    for prefix in prefixes:\n",
    "        if prefix in h2h:\n",
    "            d = h2h[prefix]['cohens_d']\n",
    "            win = h2h[prefix]['anchor_win_pct']\n",
    "            p = h2h[prefix]['p_value']\n",
    "            verdict = 'ANCHOR BETTER' if d > 0 and p < 0.05 else 'OLD BETTER' if d < 0 and p < 0.05 else 'NO DIFFERENCE'\n",
    "            print(f'  {prefix}: anchor d={d:+.3f}, win={win:.0f}%, p={p:.3e} → {verdict}')\n",
    "\n",
    "# Save combined analysis\n",
    "combined = {\n",
    "    'experiment': 'exp17_anchor_preservation',\n",
    "    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'msmarco': msmarco_analysis,\n",
    "    'nq': nq_analysis,\n",
    "}\n",
    "with open(RESULTS_DIR / 'analysis_summary.json', 'w') as f:\n",
    "    json.dump(combined, f, indent=2)\n",
    "print(f'\\nAll results saved to {RESULTS_DIR}/')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
