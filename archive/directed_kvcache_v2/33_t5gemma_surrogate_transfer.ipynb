{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 33: Encoder-Decoder Surrogate Query Transfer",
    "## Does bidirectional query-document encoding help? Does it transfer to surrogates?",
    "",
    "### Motivation",
    "Experiments 1-32 used decoder-only models (Gemma 3 4B, Mistral 7B) with causal attention.",
    "The core idea -- encoding [query + document] should produce better representations than",
    "[document] alone -- was limited by causal attention: query tokens influence document tokens",
    "but NOT vice versa. The query is already presented at inference time, so the one-directional",
    "influence during cache building adds little.",
    "",
    "**T5Gemma 2 4B-4B** is an encoder-decoder model with **bidirectional attention** in the encoder.",
    "Query and document tokens mutually influence each other during encoding. This is the natural",
    "architecture for the core hypothesis.",
    "",
    "### Core Question",
    "In an ad-serving scenario:",
    "1. **Offline**: Pre-compute encoder representations for each ad/document",
    "2. **Online**: User query arrives, decoder generates answer using pre-computed encoder output",
    "",
    "Does encoding [surrogate_query + document] offline help the decoder answer a *different*",
    "real user query at serving time?",
    "",
    "### Conditions (encoder input varies, decoder target is always the same)",
    "1. **bare**: encoder(\"[document]\")",
    "2. **oracle**: encoder(\"[query]\\n[document]\") -- same query at encode + decode time",
    "3. **static**: encoder(\"What are the key facts?\\n[document]\") -- content-agnostic prefix",
    "4. **surr_para**: encoder(\"[paraphrased_query]\\n[document]\") -- reordered keywords of real query",
    "5. **surr_doc**: encoder(\"[doc_keywords]\\n[document]\") -- keywords extracted from document",
    "",
    "Decoder (same for all): score NLL of answer tokens given \"[query] Answer: [answer]\"",
    "",
    "### Success Criteria",
    "- **Oracle d > +0.1**: Bidirectional encoding with query helps (unlike decoder-only d~0)",
    "- **Surrogate d > 0.05**: Surrogate query transfers partial benefit",
    "- **Transfer ratio > 30%**: surr_para captures >30% of oracle benefit",
    ""
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 2: Setup\n",
    "import os\n",
    "os.umask(0o000)\n",
    "\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "import gc\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from scipy import stats\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "sys.path.insert(0, \".\")\n",
    "\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "RESULTS_DIR = Path(\"results/exp33\")\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CHECKPOINT_PATH = RESULTS_DIR / \"checkpoint.json\"\n",
    "\n",
    "N_SAMPLES = 200\n",
    "MODEL_NAME = \"google/t5gemma-2-4b-4b\"\n",
    "\n",
    "# Load HF token from .env\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "HF_TOKEN = os.environ.get(\"HF_TOKEN\")\n",
    "\n",
    "print(f\"Experiment 33: Encoder-Decoder Surrogate Query Transfer\")\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"N: {N_SAMPLES}\")\n",
    "print(f\"CUDA: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 3: Load T5Gemma 2 4B-4B\n",
    "from transformers import AutoProcessor, AutoModelForSeq2SeqLM\n",
    "\n",
    "print(f\"Loading {MODEL_NAME}...\")\n",
    "processor = AutoProcessor.from_pretrained(MODEL_NAME, token=HF_TOKEN)\n",
    "tokenizer = processor.tokenizer\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    token=HF_TOKEN,\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "DEVICE = next(model.parameters()).device\n",
    "print(f\"Model loaded. dtype={next(model.parameters()).dtype}\")\n",
    "print(f\"GPU memory used: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 4: Scoring and surrogate helpers\n",
    "\n",
    "def score_answer_nll(encoder_text, query, answer):\n",
    "    '''Score NLL of answer tokens only.\n",
    "\n",
    "    Encoder: encoder_text (varies by condition)\n",
    "    Decoder: \"[query] Answer: [answer]\" -- NLL computed on answer tokens only\n",
    "    '''\n",
    "    # Encode\n",
    "    enc_ids = tokenizer(encoder_text, return_tensors=\"pt\",\n",
    "                        add_special_tokens=True, truncation=True,\n",
    "                        max_length=2048).input_ids.to(DEVICE)\n",
    "    enc_mask = torch.ones_like(enc_ids)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        encoder_outputs = model.get_encoder()(\n",
    "            input_ids=enc_ids, attention_mask=enc_mask\n",
    "        )\n",
    "\n",
    "    # Decoder target\n",
    "    full_target = f\"{query} Answer: {answer}\"\n",
    "    query_prefix = f\"{query} Answer: \"\n",
    "\n",
    "    full_ids = tokenizer(full_target, return_tensors=\"pt\",\n",
    "                         add_special_tokens=False, truncation=True,\n",
    "                         max_length=512).input_ids.to(DEVICE)\n",
    "    prefix_ids = tokenizer(query_prefix, return_tensors=\"pt\",\n",
    "                           add_special_tokens=False).input_ids.to(DEVICE)\n",
    "\n",
    "    prefix_len = prefix_ids.shape[1]\n",
    "    total_len = full_ids.shape[1]\n",
    "    answer_len = total_len - prefix_len\n",
    "\n",
    "    if answer_len <= 0:\n",
    "        return 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            encoder_outputs=encoder_outputs,\n",
    "            attention_mask=enc_mask,\n",
    "            labels=full_ids,\n",
    "        )\n",
    "\n",
    "    # Per-token log-probs, only over answer portion\n",
    "    logits = outputs.logits  # (1, total_len, vocab)\n",
    "    log_probs = F.log_softmax(logits, dim=-1)\n",
    "    token_log_probs = log_probs[0].gather(1, full_ids[0].unsqueeze(1)).squeeze(1)\n",
    "\n",
    "    answer_log_probs = token_log_probs[prefix_len:]\n",
    "    mean_nll = -answer_log_probs.mean().item()\n",
    "\n",
    "    del encoder_outputs, outputs, logits, log_probs\n",
    "    return mean_nll\n",
    "\n",
    "\n",
    "# === Surrogate query generation ===\n",
    "STOP_WORDS = {\n",
    "    'a', 'an', 'the', 'is', 'are', 'was', 'were', 'be', 'been', 'being',\n",
    "    'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could',\n",
    "    'should', 'may', 'might', 'can', 'shall', 'to', 'of', 'in', 'for',\n",
    "    'on', 'with', 'at', 'by', 'from', 'as', 'into', 'through', 'during',\n",
    "    'before', 'after', 'above', 'below', 'between', 'and', 'but', 'or',\n",
    "    'not', 'no', 'if', 'then', 'than', 'so', 'up', 'out', 'about',\n",
    "    'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those',\n",
    "    'it', 'its', 'i', 'me', 'my', 'we', 'our', 'you', 'your', 'he',\n",
    "    'him', 'his', 'she', 'her', 'they', 'them', 'their', 'how', 'when',\n",
    "    'where', 'why', 'much', 'many', 'some', 'any', 'all', 'each',\n",
    "    'does', 'also', 'just', 'more', 'most', 'very', 'too', 'only',\n",
    "}\n",
    "\n",
    "\n",
    "def extract_keywords(text):\n",
    "    words = re.sub(r'[^\\w\\s]', '', text.lower()).split()\n",
    "    return [w for w in words if w not in STOP_WORDS and len(w) > 2]\n",
    "\n",
    "\n",
    "def make_surrogate_paraphrase(query):\n",
    "    '''Paraphrase: reverse keyword order.\n",
    "    \"what is the capital of france\" -> \"france capital\"\n",
    "    '''\n",
    "    keywords = extract_keywords(query)\n",
    "    if not keywords:\n",
    "        return query\n",
    "    return \" \".join(keywords[::-1])\n",
    "\n",
    "\n",
    "def make_surrogate_from_doc(passage):\n",
    "    '''Extract top-5 keywords from document by frequency.\n",
    "    Most realistic: you only have the document, no query.\n",
    "    '''\n",
    "    content_words = extract_keywords(passage)\n",
    "    if not content_words:\n",
    "        return \"what is this about\"\n",
    "    counts = Counter(content_words)\n",
    "    return \" \".join(w for w, _ in counts.most_common(5))\n",
    "\n",
    "\n",
    "STATIC_PREFIX = \"What are the key facts?\"\n",
    "\n",
    "print(\"Helpers defined.\")\n",
    "print(\"  score_answer_nll(encoder_text, query, answer)\")\n",
    "print(\"  make_surrogate_paraphrase(query)\")\n",
    "print(\"  make_surrogate_from_doc(passage)\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 5: Load MS MARCO data\n",
    "from lib.data import count_words\n",
    "from datasets import load_dataset\n",
    "\n",
    "print(\"Loading MS MARCO v1.1 validation...\")\n",
    "ds = load_dataset(\"microsoft/ms_marco\", \"v1.1\", split=\"validation\")\n",
    "\n",
    "samples = []\n",
    "for item in ds:\n",
    "    if len(samples) >= N_SAMPLES * 3:\n",
    "        break\n",
    "    passages = item.get('passages', {})\n",
    "    ptexts = passages.get('passage_text', [])\n",
    "    is_sel = passages.get('is_selected', [])\n",
    "    query = item.get('query', '')\n",
    "    answers = item.get('answers', [])\n",
    "    well_formed = item.get('wellFormedAnswers', [])\n",
    "\n",
    "    answer = None\n",
    "    if well_formed and len(well_formed) > 0 and well_formed[0] not in ('[]', ''):\n",
    "        answer = well_formed[0]\n",
    "    elif answers and len(answers) > 0 and answers[0] != 'No Answer Present.':\n",
    "        answer = answers[0]\n",
    "    if not answer:\n",
    "        continue\n",
    "\n",
    "    for pt, sel in zip(ptexts, is_sel):\n",
    "        wc = count_words(pt)\n",
    "        if sel == 1 and 30 <= wc <= 300:\n",
    "            samples.append({\n",
    "                'passage': pt, 'query': query, 'answer': answer,\n",
    "                'word_count': wc\n",
    "            })\n",
    "            break\n",
    "\n",
    "np.random.seed(SEED)\n",
    "np.random.shuffle(samples)\n",
    "samples = samples[:N_SAMPLES]\n",
    "del ds\n",
    "gc.collect()\n",
    "\n",
    "# Generate surrogates\n",
    "for s in samples:\n",
    "    s['surrogate_para'] = make_surrogate_paraphrase(s['query'])\n",
    "    s['surrogate_doc_kw'] = make_surrogate_from_doc(s['passage'])\n",
    "\n",
    "print(f\"Selected {len(samples)} samples\")\n",
    "print(f\"Word counts: mean={np.mean([s['word_count'] for s in samples]):.0f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 6: Explain experimental conditions\n",
    "print(\"=\" * 70)\n",
    "print(\"EXPERIMENTAL CONDITIONS EXPLAINED\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "CONDITIONS = {\n",
    "    'bare':      lambda s: s['passage'],\n",
    "    'oracle':    lambda s: s['query'] + \"\\n\" + s['passage'],\n",
    "    'static':    lambda s: STATIC_PREFIX + \"\\n\" + s['passage'],\n",
    "    'surr_para': lambda s: s['surrogate_para'] + \"\\n\" + s['passage'],\n",
    "    'surr_doc':  lambda s: s['surrogate_doc_kw'] + \"\\n\" + s['passage'],\n",
    "}\n",
    "\n",
    "ex = samples[0]\n",
    "for name, fn in CONDITIONS.items():\n",
    "    enc_input = fn(ex)\n",
    "    n_tokens = len(tokenizer(enc_input, add_special_tokens=True).input_ids)\n",
    "    print(f\"\\n### {name} ###\")\n",
    "    print(f\"  Encoder input ({n_tokens} tokens):\")\n",
    "    print(f\"    {enc_input[:120]}...\")\n",
    "\n",
    "print(f\"\\n### Decoder (same for all conditions) ###\")\n",
    "dec_target = f\"{ex['query']} Answer: {ex['answer']}\"\n",
    "print(f\"  Target: {dec_target[:120]}...\")\n",
    "print(f\"  NLL computed on answer tokens only (after 'Answer: ')\")\n",
    "\n",
    "print(f\"\\n--- Example surrogates (first 5) ---\")\n",
    "for i in range(5):\n",
    "    s = samples[i]\n",
    "    print(f\"\\n  Query:    {s['query'][:60]}\")\n",
    "    print(f\"  Para:     {s['surrogate_para'][:60]}\")\n",
    "    print(f\"  Doc KW:   {s['surrogate_doc_kw'][:60]}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 7: Run scoring\n",
    "print(\"=\" * 70)\n",
    "print(\"RUNNING EXPERIMENT\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "cond_names = list(CONDITIONS.keys())\n",
    "\n",
    "# Resume from checkpoint\n",
    "all_results = []\n",
    "start_idx = 0\n",
    "\n",
    "if CHECKPOINT_PATH.exists():\n",
    "    ckpt = json.loads(CHECKPOINT_PATH.read_text())\n",
    "    if ckpt.get('n_total') == N_SAMPLES and len(ckpt.get('results', [])) > 0:\n",
    "        saved_queries = [r['query'][:50] for r in ckpt['results']]\n",
    "        current_queries = [s['query'][:50] for s in samples[:len(saved_queries)]]\n",
    "        if saved_queries == current_queries:\n",
    "            all_results = ckpt['results']\n",
    "            start_idx = len(all_results)\n",
    "            print(f\"Resuming from checkpoint: {start_idx}/{N_SAMPLES}\")\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "for i in tqdm(range(start_idx, N_SAMPLES), initial=start_idx, total=N_SAMPLES, desc=\"Scoring\"):\n",
    "    s = samples[i]\n",
    "    query, answer = s['query'], s['answer']\n",
    "\n",
    "    result = {\n",
    "        'query': query,\n",
    "        'answer': answer,\n",
    "        'passage_words': s['word_count'],\n",
    "        'surrogate_para': s['surrogate_para'],\n",
    "        'surrogate_doc_kw': s['surrogate_doc_kw'],\n",
    "    }\n",
    "\n",
    "    for cond_name, cond_fn in CONDITIONS.items():\n",
    "        encoder_text = cond_fn(s)\n",
    "        nll = score_answer_nll(encoder_text, query, answer)\n",
    "        result[f'nll_{cond_name}'] = nll\n",
    "\n",
    "    all_results.append(result)\n",
    "\n",
    "    if (i + 1) % 20 == 0 or i == N_SAMPLES - 1:\n",
    "        ckpt = {\n",
    "            'n_total': N_SAMPLES, 'results': all_results,\n",
    "            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        }\n",
    "        CHECKPOINT_PATH.write_text(json.dumps(ckpt))\n",
    "        elapsed = time.time() - t0\n",
    "        done = i - start_idx + 1\n",
    "        eta = (N_SAMPLES - i - 1) * elapsed / done if done > 0 else 0\n",
    "        tqdm.write(f\"  Checkpoint {i+1}/{N_SAMPLES} | {elapsed/60:.1f}m | ETA {eta/60:.1f}m\")\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "elapsed = time.time() - t0\n",
    "print(f\"\\nScoring complete: {len(all_results)} samples in {elapsed/60:.1f} min\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 8: Results\n",
    "from lib.analysis import cohens_d\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"RESULTS (N={len(all_results)})\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "bare_nlls = np.array([r['nll_bare'] for r in all_results])\n",
    "\n",
    "print(f\"\\n{'Condition':<15} {'Mean NLL':>10} {'vs Bare':>10} {'d':>8} {'Win%':>8} {'p':>12} {'sig':>5}\")\n",
    "print(\"-\" * 73)\n",
    "\n",
    "analysis = {}\n",
    "for cond in cond_names:\n",
    "    nlls = np.array([r[f'nll_{cond}'] for r in all_results])\n",
    "    mean_nll = nlls.mean()\n",
    "    diff = bare_nlls - nlls  # positive = condition better (lower NLL)\n",
    "    d = cohens_d(diff)\n",
    "    win_pct = 100 * np.mean(diff > 0)\n",
    "\n",
    "    if cond == 'bare':\n",
    "        print(f\"{cond:<15} {mean_nll:>10.4f} {'--':>10} {'--':>8} {'--':>8} {'--':>12} {'--':>5}\")\n",
    "        analysis[cond] = {'mean_nll': float(mean_nll)}\n",
    "    else:\n",
    "        t_stat, p_val = stats.ttest_1samp(diff, 0)\n",
    "        sig = '***' if p_val < 0.001 else '**' if p_val < 0.01 else '*' if p_val < 0.05 else 'ns'\n",
    "        print(f\"{cond:<15} {mean_nll:>10.4f} {diff.mean():>+10.4f} {d:>+8.3f} {win_pct:>7.1f}% {p_val:>12.2e} {sig:>5}\")\n",
    "        analysis[cond] = {\n",
    "            'mean_nll': float(mean_nll), 'delta_vs_bare': float(diff.mean()),\n",
    "            'cohens_d': float(d), 'win_pct': float(win_pct), 'p_value': float(p_val),\n",
    "        }\n",
    "\n",
    "# Pairwise d matrix\n",
    "print(f\"\\n--- Pairwise Cohen's d (row vs column, positive = row better) ---\")\n",
    "print(f\"{'':>15}\", end='')\n",
    "for c in cond_names:\n",
    "    print(f\" {c:>12}\", end='')\n",
    "print()\n",
    "for c1 in cond_names:\n",
    "    nlls1 = np.array([r[f'nll_{c1}'] for r in all_results])\n",
    "    print(f\"{c1:<15}\", end='')\n",
    "    for c2 in cond_names:\n",
    "        if c1 == c2:\n",
    "            print(f\" {'--':>12}\", end='')\n",
    "        else:\n",
    "            nlls2 = np.array([r[f'nll_{c2}'] for r in all_results])\n",
    "            diff = nlls2 - nlls1\n",
    "            d = cohens_d(diff)\n",
    "            print(f\" {d:>+12.3f}\", end='')\n",
    "    print()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 9: Hardness and transfer analysis\n",
    "\n",
    "# --- Hardness gradient ---\n",
    "print(\"=\" * 70)\n",
    "print(\"HARDNESS GRADIENT (by bare NLL quintile)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "quintile_bounds = np.percentile(bare_nlls, [20, 40, 60, 80])\n",
    "quintiles = np.digitize(bare_nlls, quintile_bounds)\n",
    "\n",
    "print(f\"\\n{'Quintile':<12} {'N':>4}\", end='')\n",
    "for cond in cond_names:\n",
    "    print(f\" {cond:>12}\", end='')\n",
    "print(f\" {'orc-bare':>10}\")\n",
    "print(\"-\" * (20 + 13 * len(cond_names) + 12))\n",
    "\n",
    "for q in range(5):\n",
    "    mask = quintiles == q\n",
    "    n_q = mask.sum()\n",
    "    if n_q < 3:\n",
    "        continue\n",
    "    qlabel = ['Q1 easy', 'Q2', 'Q3', 'Q4', 'Q5 hard'][q]\n",
    "    print(f\"{qlabel:<12} {n_q:>4}\", end='')\n",
    "    for cond in cond_names:\n",
    "        vals = np.array([all_results[j][f'nll_{cond}'] for j in range(len(all_results)) if mask[j]])\n",
    "        print(f\" {vals.mean():>12.4f}\", end='')\n",
    "    bare_q = np.array([all_results[j]['nll_bare'] for j in range(len(all_results)) if mask[j]])\n",
    "    orc_q = np.array([all_results[j]['nll_oracle'] for j in range(len(all_results)) if mask[j]])\n",
    "    print(f\" {(bare_q - orc_q).mean():>+10.4f}\")\n",
    "\n",
    "# --- Surrogate transfer ---\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"SURROGATE TRANSFER ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "oracle_delta = bare_nlls - np.array([r['nll_oracle'] for r in all_results])\n",
    "surr_para_delta = bare_nlls - np.array([r['nll_surr_para'] for r in all_results])\n",
    "surr_doc_delta = bare_nlls - np.array([r['nll_surr_doc'] for r in all_results])\n",
    "static_delta = bare_nlls - np.array([r['nll_static'] for r in all_results])\n",
    "\n",
    "# Correlations\n",
    "r_op, p_op = stats.pearsonr(oracle_delta, surr_para_delta)\n",
    "r_od, p_od = stats.pearsonr(oracle_delta, surr_doc_delta)\n",
    "r_pd, p_pd = stats.pearsonr(surr_para_delta, surr_doc_delta)\n",
    "r_os, p_os = stats.pearsonr(oracle_delta, static_delta)\n",
    "\n",
    "print(f\"\\nCorrelations between condition benefits (positive = same samples helped):\")\n",
    "print(f\"  oracle vs surr_para:  r={r_op:.3f} (p={p_op:.2e})\")\n",
    "print(f\"  oracle vs surr_doc:   r={r_od:.3f} (p={p_od:.2e})\")\n",
    "print(f\"  oracle vs static:     r={r_os:.3f} (p={p_os:.2e})\")\n",
    "print(f\"  surr_para vs surr_doc: r={r_pd:.3f} (p={p_pd:.2e})\")\n",
    "\n",
    "# Transfer ratios\n",
    "oracle_d = analysis.get('oracle', {}).get('cohens_d', 0)\n",
    "if oracle_d > 0:\n",
    "    for name, delta in [('static', static_delta), ('surr_para', surr_para_delta), ('surr_doc', surr_doc_delta)]:\n",
    "        d = cohens_d(delta)\n",
    "        ratio = d / oracle_d * 100\n",
    "        print(f\"\\n  {name} transfer: d={d:+.3f}, ratio={ratio:.0f}% of oracle (d={oracle_d:+.3f})\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 10: Verdict and save\n",
    "print(\"=\" * 70)\n",
    "print(\"VERDICT -- Exp 33: Encoder-Decoder Surrogate Transfer\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\nModel: {MODEL_NAME} (encoder-decoder, bidirectional encoder)\")\n",
    "print(f\"N: {len(all_results)} samples\")\n",
    "\n",
    "oracle_d = analysis.get('oracle', {}).get('cohens_d', 0)\n",
    "static_d = analysis.get('static', {}).get('cohens_d', 0)\n",
    "surr_para_d = analysis.get('surr_para', {}).get('cohens_d', 0)\n",
    "surr_doc_d = analysis.get('surr_doc', {}).get('cohens_d', 0)\n",
    "\n",
    "print(f\"\\n1. Does oracle query in encoder help? (target: d > +0.1)\")\n",
    "print(f\"   d = {oracle_d:+.3f} -> {'YES' if oracle_d > 0.1 else 'MARGINAL' if oracle_d > 0.03 else 'NO'}\")\n",
    "print(f\"   (Decoder-only comparison: d ~ 0.0)\")\n",
    "\n",
    "print(f\"\\n2. Does static prefix help?\")\n",
    "print(f\"   d = {static_d:+.3f} -> {'YES' if static_d > 0.1 else 'MARGINAL' if static_d > 0.03 else 'NO'}\")\n",
    "\n",
    "print(f\"\\n3. Does surrogate query transfer? (target: d > +0.05)\")\n",
    "print(f\"   Paraphrase: d = {surr_para_d:+.3f} -> {'YES' if surr_para_d > 0.05 else 'MARGINAL' if surr_para_d > 0.02 else 'NO'}\")\n",
    "print(f\"   Doc keywords: d = {surr_doc_d:+.3f} -> {'YES' if surr_doc_d > 0.05 else 'MARGINAL' if surr_doc_d > 0.02 else 'NO'}\")\n",
    "\n",
    "if oracle_d > 0:\n",
    "    print(f\"\\n4. Transfer efficiency:\")\n",
    "    print(f\"   Paraphrase:    {surr_para_d/oracle_d*100:.0f}% of oracle\")\n",
    "    print(f\"   Doc keywords:  {surr_doc_d/oracle_d*100:.0f}% of oracle\")\n",
    "    print(f\"   Static prefix: {static_d/oracle_d*100:.0f}% of oracle\")\n",
    "\n",
    "# Comparison to decoder-only\n",
    "print(f\"\\n5. Decoder-only comparison (Gemma 3 4B, Exps 1-32):\")\n",
    "print(f\"   Oracle priming (causal, full-context):  d ~ +0.023 (ns)\")\n",
    "print(f\"   Oracle priming (causal, values-only):   d ~ +0.211\")\n",
    "print(f\"   Static prefix (causal, values-only):    d ~ +0.288\")\n",
    "print(f\"   T5Gemma oracle (bidirectional):         d = {oracle_d:+.3f}\")\n",
    "\n",
    "verdict = \"CONFIRMED\" if oracle_d > 0.1 and surr_para_d > 0.05 else \\\n",
    "          \"PARTIAL\" if oracle_d > 0.05 or surr_para_d > 0.03 else \"NEGATIVE\"\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"OVERALL: {verdict}\")\n",
    "if verdict == \"CONFIRMED\":\n",
    "    print(\"Bidirectional encoding unlocks the query-conditioning benefit\")\n",
    "    print(\"that causal attention could not provide. Surrogate transfer works.\")\n",
    "elif verdict == \"PARTIAL\":\n",
    "    print(\"Some evidence for bidirectional benefit, but surrogate transfer\")\n",
    "    print(\"is weaker than hoped.\")\n",
    "else:\n",
    "    print(\"Even with bidirectional attention, query-conditioning during\")\n",
    "    print(\"encoding does not meaningfully help.\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "# Save\n",
    "final_results = {\n",
    "    'experiment': 'exp33_t5gemma_surrogate',\n",
    "    'model': MODEL_NAME,\n",
    "    'n_samples': len(all_results),\n",
    "    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'analysis': analysis,\n",
    "    'correlations': {\n",
    "        'oracle_vs_surr_para': float(r_op),\n",
    "        'oracle_vs_surr_doc': float(r_od),\n",
    "        'oracle_vs_static': float(r_os),\n",
    "    },\n",
    "}\n",
    "\n",
    "with open(RESULTS_DIR / 'results.json', 'w') as f:\n",
    "    json.dump(final_results, f, indent=2)\n",
    "print(f\"\\nResults saved to {RESULTS_DIR / 'results.json'}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 11: Cleanup\n",
    "print(\"Cleaning up GPU memory...\")\n",
    "mem_before = torch.cuda.memory_allocated() / 1e9\n",
    "\n",
    "del model, processor, tokenizer\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "mem_after = torch.cuda.memory_allocated() / 1e9\n",
    "print(f\"GPU memory: {mem_before:.2f} GB -> {mem_after:.2f} GB\")\n",
    "print(\"Done!\")"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}