{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exp 24: Gemma Layer-Selective Mechanism Deep Dive\n",
    "\n",
    "## Motivation\n",
    "\n",
    "Exp 21 confirmed that **layer-selective value contamination** (layers 0-15, cutoff=16) produces\n",
    "d=**+0.227** on Gemma 3 4B with MS MARCO, and is more length-robust than Mistral's full priming\n",
    "(still d=+0.173 at 512 tokens, vs Mistral dropping to +0.034). The layer boundary sweep confirmed\n",
    "cutoff=16 is optimal.\n",
    "\n",
    "**Four open questions:**\n",
    "\n",
    "### Q1: Which individual layers carry the signal?\n",
    "We know layers 0-15 collectively help and 16-33 collectively hurt, but which specific layers\n",
    "matter most? Is it a smooth gradient or concentrated in a few layers?\n",
    "\n",
    "### Q2: Does this generalize to SQuAD v2?\n",
    "MS MARCO-specific effects are a known risk. SQuAD v2 has short extractive QA passages\n",
    "(median 114 words, 92% under 200 words) \u2014 ideal for Gemma's sliding window constraint.\n",
    "\n",
    "### Q3: Does prefix content matter under layer selectivity?\n",
    "Exp 16 showed static_fact, random, and oracle all fail with full-cache replacement on Gemma.\n",
    "But with layer-selective values (the method that works), does prefix content make a difference?\n",
    "\n",
    "### Q4: What makes early-layer values different?\n",
    "L2 norms, cosine similarity between bare/primed values, and delta magnitudes per layer can\n",
    "reveal why early layers carry the useful signal.\n",
    "\n",
    "## Design\n",
    "\n",
    "| Part | Data | N | Conditions | Fwd/q | Score/q |\n",
    "|------|------|---|------------|-------|---------|\n",
    "| 1+4 | MS MARCO | 300 | 34 single-layer + bare + features | 2 | 35 |\n",
    "| 2 | SQuAD v2 | 400 | bare, values_all, values_cutoff_16 | 2 | 3 |\n",
    "| 3 | MS MARCO | 300 | bare + 3 prefix types at cutoff=16 | 4 | 4 |\n",
    "\n",
    "## Reference Values\n",
    "\n",
    "| Source | Condition | d |\n",
    "|--------|-----------|---|\n",
    "| Exp 19 | values_only (all 34 layers) | +0.056 |\n",
    "| Exp 19 | values_early_layers (0-16) | +0.211 |\n",
    "| Exp 21 | values_early_layers (0-15) @ original | +0.227 |\n",
    "| Exp 21 | cutoff sweep best (cutoff=16) | +0.161 |\n",
    "| Exp 16 | static_fact full-cache | -0.031 (ns) |\n",
    "| Exp 16 | random full-cache | -0.109 (***) |"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 1: Setup\n",
    "import os\n",
    "os.umask(0o000)\n",
    "\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import gc\n",
    "from pathlib import Path\n",
    "\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "RESULTS_DIR = Path(\"results/exp24\")\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CHECKPOINT_P1_PATH = RESULTS_DIR / \"checkpoint_part1.json\"\n",
    "CHECKPOINT_P2_PATH = RESULTS_DIR / \"checkpoint_part2.json\"\n",
    "CHECKPOINT_P3_PATH = RESULTS_DIR / \"checkpoint_part3.json\"\n",
    "FINAL_RESULTS_PATH = RESULTS_DIR / \"results.json\"\n",
    "CSV_P1_PATH = RESULTS_DIR / \"part1_layer_map.csv\"\n",
    "CSV_P2_PATH = RESULTS_DIR / \"part2_squad.csv\"\n",
    "CSV_P3_PATH = RESULTS_DIR / \"part3_prefix_content.csv\"\n",
    "CSV_P4_PATH = RESULTS_DIR / \"part4_value_features.csv\"\n",
    "\n",
    "print(f\"SEED: {SEED}\")\n",
    "print(f\"Results directory: {RESULTS_DIR}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 2: Load Gemma 3 4B via load_model()\n",
    "sys.path.insert(0, \".\")\n",
    "\n",
    "from lib.config import ExperimentConfig\n",
    "from lib.model_utils import load_model\n",
    "\n",
    "MODEL_NAME = \"google/gemma-3-4b-it\"\n",
    "\n",
    "exp_config = ExperimentConfig(\n",
    "    model_name=MODEL_NAME,\n",
    "    model_type=\"gemma3\",\n",
    "    compute_dtype=\"auto\",  # resolves to bfloat16 for gemma3\n",
    "    use_4bit=True,\n",
    "    num_samples=2000,\n",
    "    seed=SEED,\n",
    ")\n",
    "\n",
    "print(f\"Loading {MODEL_NAME} (4-bit, bfloat16)...\")\n",
    "model, tokenizer = load_model(exp_config)\n",
    "\n",
    "# Architecture diagnostics\n",
    "from lib.kv_cache import _get_text_config, _get_head_dim, _get_rope_theta_for_layer, _get_cache_keys, _ensure_dynamic_cache\n",
    "\n",
    "text_config = _get_text_config(model.config)\n",
    "NUM_LAYERS = text_config.num_hidden_layers\n",
    "print(f\"\\nModel loaded successfully.\")\n",
    "print(f\"  Model class: {type(model).__name__}\")\n",
    "print(f\"  Text config class: {type(text_config).__name__}\")\n",
    "print(f\"  Hidden size: {text_config.hidden_size}\")\n",
    "print(f\"  Num layers: {NUM_LAYERS}\")\n",
    "print(f\"  Num attention heads: {text_config.num_attention_heads}\")\n",
    "print(f\"  Num KV heads: {text_config.num_key_value_heads}\")\n",
    "print(f\"  Head dim: {_get_head_dim(model.config)}\")\n",
    "print(f\"  BOS token ID: {tokenizer.bos_token_id}\")\n",
    "\n",
    "# Per-layer RoPE diagnostics\n",
    "thetas = set()\n",
    "for layer_idx in range(NUM_LAYERS):\n",
    "    thetas.add(_get_rope_theta_for_layer(model.config, layer_idx))\n",
    "print(f\"  Unique RoPE thetas: {sorted(thetas)}\")\n",
    "\n",
    "# Verify dtype\n",
    "sample_ids = tokenizer(\"test\", return_tensors=\"pt\")['input_ids'].to(exp_config.device)\n",
    "with torch.no_grad():\n",
    "    out = model(sample_ids, use_cache=True)\n",
    "    cache_check = _ensure_dynamic_cache(out.past_key_values)\n",
    "    k0 = _get_cache_keys(cache_check, 0)\n",
    "    print(f\"  Cache key dtype: {k0.dtype}\")\n",
    "    print(f\"  Cache key shape: {k0.shape}  (batch, kv_heads, seq, head_dim)\")\n",
    "del out, sample_ids\n",
    "torch.cuda.empty_cache()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 3: Lib imports + templates + constants\n",
    "from lib.kv_cache import (\n",
    "    _get_cache_keys,\n",
    "    _get_cache_values,\n",
    "    _set_cache_keys,\n",
    "    _set_cache_values,\n",
    "    _ensure_dynamic_cache,\n",
    "    extract_and_truncate_cache_with_bos,\n",
    "    correct_rope_positions_with_bos,\n",
    "    score_answer_with_cache,\n",
    "    deepcopy_cache,\n",
    "    replace_values_at_layers,\n",
    "    build_hybrid_cache,\n",
    "    _get_text_config,\n",
    ")\n",
    "from lib.analysis import cohens_d\n",
    "from lib.data import count_words\n",
    "from scipy import stats\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Templates \u2014 bare text, no \"Document:\\n\" framing\n",
    "SURROGATE_PREFIX_TEMPLATE = \"{surrogate}\\n\"\n",
    "DOCUMENT_TEMPLATE = \"{document}\"\n",
    "QUERY_TEMPLATE = \"\\nQuery: {query}\\nAnswer:\"\n",
    "ANSWER_TEMPLATE = \" {answer}\"\n",
    "\n",
    "# Prefix text\n",
    "from lib.surrogate import STATIC_SURROGATE_QUERIES\n",
    "STATIC_FACT = STATIC_SURROGATE_QUERIES['static_factual']['query']\n",
    "\n",
    "# Experiment parameters\n",
    "N_PART1 = 300  # Part 1+4: individual layer map + features\n",
    "N_PART2 = 400  # Part 2: SQuAD v2\n",
    "N_PART3 = 300  # Part 3: prefix content (same queries as Part 1)\n",
    "MAX_PASSAGE_WORDS = 300\n",
    "CHECKPOINT_EVERY = 25\n",
    "CUTOFF = 16  # layers 0-15\n",
    "\n",
    "# Reference values\n",
    "EXP19_REF = {\n",
    "    'values_only_d': 0.056,\n",
    "    'values_early_layers_d': 0.211,  # layers 0-16 (17 layers)\n",
    "}\n",
    "EXP21_REF = {\n",
    "    'values_early_layers_d': 0.227,  # layers 0-15 (16 layers)\n",
    "    'cutoff_sweep_best_d': 0.161,    # cutoff=16 on N=200\n",
    "}\n",
    "EXP16_REF = {\n",
    "    'sf_trunc_full_d': -0.031,\n",
    "    'random_trunc_full_d': -0.109,\n",
    "}\n",
    "\n",
    "print(\"Config ready\")\n",
    "print(f\"  Model: {MODEL_NAME}\")\n",
    "print(f\"  Num layers: {NUM_LAYERS}\")\n",
    "print(f\"  Part 1+4: N={N_PART1}, individual layer map + features\")\n",
    "print(f\"  Part 2: N={N_PART2}, SQuAD v2 cross-dataset\")\n",
    "print(f\"  Part 3: N={N_PART3}, prefix content x layer selectivity (cutoff={CUTOFF})\")\n",
    "print(f\"  Static fact prefix: '{STATIC_FACT}'\")\n",
    "print(f\"\\nReference values:\")\n",
    "for ref_name, ref_dict in [('Exp 19', EXP19_REF), ('Exp 21', EXP21_REF), ('Exp 16', EXP16_REF)]:\n",
    "    for k, v in ref_dict.items():\n",
    "        print(f\"  {ref_name} {k}: {v:+.3f}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 4: Load MS MARCO (positive, <=300w) + SQuAD v2 (has-answer, <=300w)\n",
    "from datasets import load_dataset\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"LOADING MS MARCO v1.1 \u2014 POSITIVE PASSAGES ONLY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "dataset_marco = load_dataset(\"microsoft/ms_marco\", \"v1.1\", split=\"validation\",\n",
    "                              trust_remote_code=True)\n",
    "print(f\"Total items in validation: {len(dataset_marco)}\")\n",
    "\n",
    "marco_queries = []\n",
    "random_passages = []  # non-eval passages for Part 3 random prefix\n",
    "eval_passage_set = set()\n",
    "np.random.seed(SEED)\n",
    "\n",
    "for item in tqdm(dataset_marco, desc=\"Filtering MARCO\"):\n",
    "    passages_info = item.get('passages', {})\n",
    "    passage_texts = passages_info.get('passage_text', [])\n",
    "    is_selected = passages_info.get('is_selected', [])\n",
    "    query = item.get('query', '')\n",
    "    answers = item.get('answers', [])\n",
    "    well_formed = item.get('wellFormedAnswers', [])\n",
    "\n",
    "    if not passage_texts or not query:\n",
    "        continue\n",
    "\n",
    "    # Get best answer\n",
    "    answer = None\n",
    "    if well_formed and len(well_formed) > 0 and well_formed[0] != '[]':\n",
    "        answer = well_formed[0]\n",
    "    elif answers and len(answers) > 0 and answers[0] != 'No Answer Present.':\n",
    "        answer = answers[0]\n",
    "    else:\n",
    "        # Collect non-eval passages for random pool\n",
    "        for p in passage_texts:\n",
    "            if count_words(p) <= MAX_PASSAGE_WORDS:\n",
    "                random_passages.append(p)\n",
    "        continue\n",
    "\n",
    "    # Find positive passage(s)\n",
    "    for i, (ptext, sel) in enumerate(zip(passage_texts, is_selected)):\n",
    "        if sel == 1 and count_words(ptext) <= MAX_PASSAGE_WORDS:\n",
    "            if len(marco_queries) < N_PART1 * 3:\n",
    "                marco_queries.append({\n",
    "                    'query': query,\n",
    "                    'answer': answer,\n",
    "                    'passage': ptext,\n",
    "                    'word_count': count_words(ptext),\n",
    "                })\n",
    "                eval_passage_set.add(ptext)\n",
    "                break\n",
    "\n",
    "    # Collect non-eval passages for random pool\n",
    "    for p in passage_texts:\n",
    "        if p not in eval_passage_set and count_words(p) <= MAX_PASSAGE_WORDS:\n",
    "            random_passages.append(p)\n",
    "\n",
    "np.random.shuffle(marco_queries)\n",
    "marco_queries = marco_queries[:N_PART1]\n",
    "N_MARCO = len(marco_queries)\n",
    "\n",
    "print(f\"\\nSelected {N_MARCO} MS MARCO queries with positive passages\")\n",
    "print(f\"Word counts: mean={np.mean([q['word_count'] for q in marco_queries]):.0f}, \"\n",
    "      f\"min={min(q['word_count'] for q in marco_queries)}, \"\n",
    "      f\"max={max(q['word_count'] for q in marco_queries)}\")\n",
    "print(f\"Random passage pool: {len(random_passages):,}\")\n",
    "\n",
    "del dataset_marco\n",
    "gc.collect()\n",
    "\n",
    "# === SQuAD v2 ===\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"LOADING SQuAD v2 \u2014 HAS-ANSWER, <=300 WORDS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "ds_squad = load_dataset(\"rajpurkar/squad_v2\", split=\"validation\")\n",
    "print(f\"Total items in SQuAD v2 validation: {len(ds_squad)}\")\n",
    "\n",
    "squad_samples = []\n",
    "for item in ds_squad:\n",
    "    if len(item['answers']['text']) > 0 and count_words(item['context']) <= MAX_PASSAGE_WORDS:\n",
    "        squad_samples.append({\n",
    "            'query': item['question'],\n",
    "            'answer': item['answers']['text'][0],\n",
    "            'passage': item['context'],\n",
    "            'word_count': count_words(item['context']),\n",
    "        })\n",
    "\n",
    "print(f\"SQuAD v2 samples with answers & <=300 words: {len(squad_samples)}\")\n",
    "\n",
    "np.random.shuffle(squad_samples)\n",
    "squad_samples = squad_samples[:N_PART2]\n",
    "N_SQUAD = len(squad_samples)\n",
    "\n",
    "print(f\"Selected {N_SQUAD} SQuAD v2 samples\")\n",
    "print(f\"Word counts: mean={np.mean([q['word_count'] for q in squad_samples]):.0f}, \"\n",
    "      f\"median={np.median([q['word_count'] for q in squad_samples]):.0f}, \"\n",
    "      f\"min={min(q['word_count'] for q in squad_samples)}, \"\n",
    "      f\"max={max(q['word_count'] for q in squad_samples)}\")\n",
    "\n",
    "del ds_squad\n",
    "gc.collect()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 5: Tokenize prefixes (static_fact, random pool) + condition explanations\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PREFIX TOKENIZATION \u2014 GEMMA 3 4B\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Static fact prefix\n",
    "sf_str = SURROGATE_PREFIX_TEMPLATE.format(surrogate=STATIC_FACT)\n",
    "sf_ids = tokenizer(sf_str, return_tensors=\"pt\",\n",
    "                    add_special_tokens=False)['input_ids'].to(exp_config.device)\n",
    "\n",
    "print(f\"\\nStatic fact prefix: '{STATIC_FACT}'\")\n",
    "print(f\"  Formatted: '{sf_str.strip()}'\")\n",
    "print(f\"  Token length: {sf_ids.shape[1]}\")\n",
    "\n",
    "# Pre-select random passages for Part 3 (one per query)\n",
    "np.random.seed(SEED + 1)\n",
    "random_prefix_passages = []\n",
    "rand_idx_pool = np.random.permutation(len(random_passages))\n",
    "for i in range(N_PART3):\n",
    "    rp = random_passages[rand_idx_pool[i % len(rand_idx_pool)]]\n",
    "    random_prefix_passages.append(rp)\n",
    "print(f\"\\nRandom prefix passages pre-selected: {len(random_prefix_passages)}\")\n",
    "print(f\"  Example (first 80 chars): '{random_prefix_passages[0][:80]}...'\")\n",
    "\n",
    "# BPE boundary check\n",
    "print(\"\\nBPE BOUNDARY CHECK (first passage):\")\n",
    "example_doc = marco_queries[0]['passage']\n",
    "concat = sf_str + DOCUMENT_TEMPLATE.format(document=example_doc)\n",
    "concat_enc = tokenizer(concat, add_special_tokens=True)['input_ids']\n",
    "prefix_enc = tokenizer(sf_str, add_special_tokens=True)['input_ids']\n",
    "doc_ids_from_concat = concat_enc[len(prefix_enc):]\n",
    "bare_doc_enc = tokenizer(DOCUMENT_TEMPLATE.format(document=example_doc),\n",
    "                          add_special_tokens=False)['input_ids']\n",
    "match = sum(1 for a, b in zip(doc_ids_from_concat, bare_doc_enc) if a == b)\n",
    "total = max(len(bare_doc_enc), 1)\n",
    "print(f\"  static_fact: {match}/{total} tokens match ({100*match/total:.1f}%)\")\n",
    "\n",
    "# Condition explanations\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"EXPERIMENTAL CONDITIONS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\n### Part 1+4: Individual Layer Contribution Map + Value Features ###\")\n",
    "print(f\"  Data: {N_PART1} MS MARCO queries\")\n",
    "print(\"  Per query: 2 fwd passes (bare + sf_trunc primed)\")\n",
    "print(f\"  Then for each of {NUM_LAYERS} layers individually:\")\n",
    "print(\"    - Replace ONLY that layer's values from primed into bare -> score\")\n",
    "print(\"    - Collect value features: L2 norms, cosine sim, delta norm\")\n",
    "print(f\"  Output: {N_PART1} x {1 + NUM_LAYERS} NLL scores + {N_PART1} x {NUM_LAYERS} x 4 features\")\n",
    "\n",
    "print(f\"\\n### Part 2: Cross-Dataset \u2014 SQuAD v2 ###\")\n",
    "print(f\"  Data: {N_PART2} SQuAD v2 queries\")\n",
    "print(\"  Conditions: bare, values_all (34 layers), values_cutoff_16 (layers 0-15)\")\n",
    "print(\"  Per query: 2 fwd passes -> score 3 conditions\")\n",
    "\n",
    "print(f\"\\n### Part 3: Prefix Content x Layer Selectivity ###\")\n",
    "print(f\"  Data: {N_PART3} MS MARCO queries (same as Part 1)\")\n",
    "print(f\"  Prefix types: static_fact, random (random MARCO passage), oracle (query text)\")\n",
    "print(f\"  All at cutoff={CUTOFF} (layers 0-{CUTOFF-1})\")\n",
    "print(\"  Per query: 4 fwd passes (bare + 3 prefix types) -> truncate -> RoPE -> replace values -> score 4 conditions\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 6: Part 1+4 \u2014 Individual Layer Map + Value Features (300 queries, 34 layers)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"PART 1+4: INDIVIDUAL LAYER MAP + VALUE FEATURES ({N_PART1} queries, {NUM_LAYERS} layers)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Checkpoint resume\n",
    "p1_results = []\n",
    "p4_features = []\n",
    "p1_start_idx = 0\n",
    "\n",
    "if CHECKPOINT_P1_PATH.exists():\n",
    "    with open(CHECKPOINT_P1_PATH, 'r') as f:\n",
    "        ckpt = json.load(f)\n",
    "    ckpt_queries = ckpt.get('query_texts', [])\n",
    "    current_queries = [q['query'] for q in marco_queries[:N_PART1]]\n",
    "    if ckpt_queries == current_queries:\n",
    "        p1_results = ckpt['results']\n",
    "        p4_features = ckpt.get('features', [])\n",
    "        p1_start_idx = len(p1_results)\n",
    "        print(f\"Resuming from checkpoint: {p1_start_idx}/{N_PART1}\")\n",
    "    else:\n",
    "        print(\"Checkpoint query mismatch. Starting fresh.\")\n",
    "else:\n",
    "    print(\"No checkpoint found. Starting fresh.\")\n",
    "\n",
    "t_start = time.time()\n",
    "\n",
    "for qidx in tqdm(range(p1_start_idx, N_PART1), initial=p1_start_idx, total=N_PART1,\n",
    "                  desc=\"Part 1+4\"):\n",
    "    qdata = marco_queries[qidx]\n",
    "    query_prompt = QUERY_TEMPLATE.format(query=qdata['query'])\n",
    "    answer_text = ANSWER_TEMPLATE.format(answer=qdata['answer'])\n",
    "    passage = qdata['passage']\n",
    "    document_text = DOCUMENT_TEMPLATE.format(document=passage)\n",
    "\n",
    "    # Matched tokenization\n",
    "    full_text = sf_str + document_text\n",
    "    full_enc = tokenizer(full_text, return_tensors=\"pt\",\n",
    "                          add_special_tokens=True, padding=False, truncation=False)\n",
    "    full_ids = full_enc['input_ids'].to(exp_config.device)\n",
    "\n",
    "    sf_prefix_enc = tokenizer(sf_str, return_tensors=\"pt\",\n",
    "                               add_special_tokens=True, padding=False, truncation=False)\n",
    "    sf_prefix_len_matched = sf_prefix_enc['input_ids'].shape[1]\n",
    "\n",
    "    bos_id = full_ids[:, :1]\n",
    "    doc_ids = full_ids[:, sf_prefix_len_matched:]\n",
    "    doc_len = doc_ids.shape[1]\n",
    "    context_len = 1 + doc_len\n",
    "\n",
    "    del full_enc, full_ids, sf_prefix_enc\n",
    "\n",
    "    # Forward pass 1: BARE\n",
    "    bare_input = torch.cat([bos_id, doc_ids], dim=1)\n",
    "    with torch.no_grad():\n",
    "        bare_out = model(input_ids=bare_input,\n",
    "                         attention_mask=torch.ones_like(bare_input),\n",
    "                         use_cache=True, return_dict=True)\n",
    "    bare_cache = _ensure_dynamic_cache(bare_out.past_key_values)\n",
    "    del bare_out\n",
    "\n",
    "    # Forward pass 2: PRIMED (static_fact)\n",
    "    primed_input = torch.cat([bos_id, sf_ids, doc_ids], dim=1)\n",
    "    with torch.no_grad():\n",
    "        primed_out = model(input_ids=primed_input,\n",
    "                           attention_mask=torch.ones_like(primed_input),\n",
    "                           use_cache=True, return_dict=True)\n",
    "    primed_full = _ensure_dynamic_cache(primed_out.past_key_values)\n",
    "    del primed_out\n",
    "\n",
    "    # Truncate + RoPE correct\n",
    "    trunc_raw = extract_and_truncate_cache_with_bos(primed_full, doc_len)\n",
    "    prefix_offset = sf_ids.shape[1]\n",
    "    del primed_full\n",
    "\n",
    "    sf_trunc_cache = deepcopy_cache(trunc_raw)\n",
    "    correct_rope_positions_with_bos(sf_trunc_cache, prefix_offset, model)\n",
    "    del trunc_raw\n",
    "\n",
    "    # Score bare\n",
    "    bare_nll = score_answer_with_cache(\n",
    "        deepcopy_cache(bare_cache), context_len,\n",
    "        query_prompt, answer_text, model, tokenizer, exp_config)\n",
    "\n",
    "    # Part 4: Value features (cheap tensor ops on existing caches)\n",
    "    query_features = []\n",
    "    for l in range(NUM_LAYERS):\n",
    "        bare_v = _get_cache_values(bare_cache, l)[:, :, 1:, :]  # skip BOS\n",
    "        primed_v = _get_cache_values(sf_trunc_cache, l)[:, :, 1:, :]\n",
    "\n",
    "        # L2 norms (mean over positions and heads)\n",
    "        bare_l2 = bare_v.float().norm(dim=-1).mean().item()\n",
    "        primed_l2 = primed_v.float().norm(dim=-1).mean().item()\n",
    "\n",
    "        # Delta\n",
    "        delta_v = (primed_v.float() - bare_v.float())\n",
    "        delta_norm = delta_v.norm(dim=-1).mean().item()\n",
    "\n",
    "        # Cosine similarity (flatten heads and positions, compute per-vector)\n",
    "        bare_flat = bare_v.float().reshape(-1, bare_v.shape[-1])\n",
    "        primed_flat = primed_v.float().reshape(-1, primed_v.shape[-1])\n",
    "        cos_sim = torch.nn.functional.cosine_similarity(bare_flat, primed_flat, dim=-1).mean().item()\n",
    "\n",
    "        query_features.append({\n",
    "            'layer': l,\n",
    "            'bare_l2': bare_l2,\n",
    "            'primed_l2': primed_l2,\n",
    "            'delta_norm': delta_norm,\n",
    "            'cosine_sim': cos_sim,\n",
    "        })\n",
    "\n",
    "    # Part 1: Individual layer scoring\n",
    "    layer_nlls = {}\n",
    "    for l in range(NUM_LAYERS):\n",
    "        vel_cache = replace_values_at_layers(bare_cache, sf_trunc_cache, [l])\n",
    "        vel_nll = score_answer_with_cache(\n",
    "            deepcopy_cache(vel_cache), context_len,\n",
    "            query_prompt, answer_text, model, tokenizer, exp_config)\n",
    "        layer_nlls[l] = vel_nll\n",
    "        del vel_cache\n",
    "\n",
    "    del bare_cache, sf_trunc_cache, bare_input, primed_input\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    p1_results.append({\n",
    "        'query_idx': qidx,\n",
    "        'query': qdata['query'],\n",
    "        'doc_len': doc_len,\n",
    "        'bare_nll': bare_nll,\n",
    "        'layer_nlls': layer_nlls,\n",
    "    })\n",
    "    p4_features.append({\n",
    "        'query_idx': qidx,\n",
    "        'features': query_features,\n",
    "    })\n",
    "\n",
    "    # Checkpoint\n",
    "    if (qidx + 1) % CHECKPOINT_EVERY == 0 or qidx == N_PART1 - 1:\n",
    "        ckpt_data = {\n",
    "            'results': p1_results,\n",
    "            'features': p4_features,\n",
    "            'query_texts': [q['query'] for q in marco_queries[:N_PART1]],\n",
    "            'completed': len(p1_results),\n",
    "            'total': N_PART1,\n",
    "            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        }\n",
    "        with open(CHECKPOINT_P1_PATH, 'w') as f:\n",
    "            json.dump(ckpt_data, f)\n",
    "        elapsed = time.time() - t_start\n",
    "        n_done = qidx - p1_start_idx + 1\n",
    "        rate = n_done / elapsed if elapsed > 0 else 0\n",
    "        remaining = (N_PART1 - qidx - 1) / rate if rate > 0 else 0\n",
    "        tqdm.write(f\"  Checkpoint {qidx+1}/{N_PART1} | {n_done} done in {elapsed/60:.1f}m | \"\n",
    "                   f\"ETA: {remaining/60:.1f} min\")\n",
    "\n",
    "elapsed_total = time.time() - t_start\n",
    "print(f\"\\nPart 1+4 complete: {len(p1_results)} queries in {elapsed_total/60:.1f} min\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 7: Part 2 \u2014 Cross-Dataset SQuAD v2 (400 queries, 3 conditions)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"PART 2: CROSS-DATASET SQuAD v2 ({N_SQUAD} queries, 3 conditions)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Checkpoint resume\n",
    "p2_results = []\n",
    "p2_start_idx = 0\n",
    "\n",
    "if CHECKPOINT_P2_PATH.exists():\n",
    "    with open(CHECKPOINT_P2_PATH, 'r') as f:\n",
    "        ckpt = json.load(f)\n",
    "    ckpt_queries = ckpt.get('query_texts', [])\n",
    "    current_queries = [q['query'] for q in squad_samples[:N_SQUAD]]\n",
    "    if ckpt_queries == current_queries:\n",
    "        p2_results = ckpt['results']\n",
    "        p2_start_idx = len(p2_results)\n",
    "        print(f\"Resuming from checkpoint: {p2_start_idx}/{N_SQUAD}\")\n",
    "    else:\n",
    "        print(\"Checkpoint query mismatch. Starting fresh.\")\n",
    "else:\n",
    "    print(\"No checkpoint found. Starting fresh.\")\n",
    "\n",
    "layer_indices_all = list(range(NUM_LAYERS))\n",
    "layer_indices_cutoff = list(range(CUTOFF))\n",
    "\n",
    "t_start = time.time()\n",
    "\n",
    "for qidx in tqdm(range(p2_start_idx, N_SQUAD), initial=p2_start_idx, total=N_SQUAD,\n",
    "                  desc=\"Part 2\"):\n",
    "    qdata = squad_samples[qidx]\n",
    "    query_prompt = QUERY_TEMPLATE.format(query=qdata['query'])\n",
    "    answer_text = ANSWER_TEMPLATE.format(answer=qdata['answer'])\n",
    "    passage = qdata['passage']\n",
    "    document_text = DOCUMENT_TEMPLATE.format(document=passage)\n",
    "\n",
    "    # Matched tokenization\n",
    "    full_text = sf_str + document_text\n",
    "    full_enc = tokenizer(full_text, return_tensors=\"pt\",\n",
    "                          add_special_tokens=True, padding=False, truncation=False)\n",
    "    full_ids = full_enc['input_ids'].to(exp_config.device)\n",
    "\n",
    "    sf_prefix_enc = tokenizer(sf_str, return_tensors=\"pt\",\n",
    "                               add_special_tokens=True, padding=False, truncation=False)\n",
    "    sf_prefix_len_matched = sf_prefix_enc['input_ids'].shape[1]\n",
    "\n",
    "    bos_id = full_ids[:, :1]\n",
    "    doc_ids = full_ids[:, sf_prefix_len_matched:]\n",
    "    doc_len = doc_ids.shape[1]\n",
    "    context_len = 1 + doc_len\n",
    "\n",
    "    del full_enc, full_ids, sf_prefix_enc\n",
    "\n",
    "    # Forward pass 1: BARE\n",
    "    bare_input = torch.cat([bos_id, doc_ids], dim=1)\n",
    "    with torch.no_grad():\n",
    "        bare_out = model(input_ids=bare_input,\n",
    "                         attention_mask=torch.ones_like(bare_input),\n",
    "                         use_cache=True, return_dict=True)\n",
    "    bare_cache = _ensure_dynamic_cache(bare_out.past_key_values)\n",
    "    del bare_out\n",
    "\n",
    "    # Forward pass 2: PRIMED (static_fact)\n",
    "    primed_input = torch.cat([bos_id, sf_ids, doc_ids], dim=1)\n",
    "    with torch.no_grad():\n",
    "        primed_out = model(input_ids=primed_input,\n",
    "                           attention_mask=torch.ones_like(primed_input),\n",
    "                           use_cache=True, return_dict=True)\n",
    "    primed_full = _ensure_dynamic_cache(primed_out.past_key_values)\n",
    "    del primed_out\n",
    "\n",
    "    # Truncate + RoPE correct\n",
    "    trunc_raw = extract_and_truncate_cache_with_bos(primed_full, doc_len)\n",
    "    prefix_offset = sf_ids.shape[1]\n",
    "    del primed_full\n",
    "\n",
    "    sf_trunc_cache = deepcopy_cache(trunc_raw)\n",
    "    correct_rope_positions_with_bos(sf_trunc_cache, prefix_offset, model)\n",
    "    del trunc_raw\n",
    "\n",
    "    # Score bare\n",
    "    bare_nll = score_answer_with_cache(\n",
    "        deepcopy_cache(bare_cache), context_len,\n",
    "        query_prompt, answer_text, model, tokenizer, exp_config)\n",
    "\n",
    "    # values_all: bare keys + primed values at ALL layers\n",
    "    val_all_cache = build_hybrid_cache(\n",
    "        keys_source=bare_cache,\n",
    "        values_source=sf_trunc_cache,\n",
    "    )\n",
    "    val_all_nll = score_answer_with_cache(\n",
    "        deepcopy_cache(val_all_cache), context_len,\n",
    "        query_prompt, answer_text, model, tokenizer, exp_config)\n",
    "    del val_all_cache\n",
    "\n",
    "    # values_cutoff_16: bare keys + primed values at layers 0-15\n",
    "    val_cutoff_cache = replace_values_at_layers(\n",
    "        bare_cache, sf_trunc_cache, layer_indices_cutoff)\n",
    "    val_cutoff_nll = score_answer_with_cache(\n",
    "        deepcopy_cache(val_cutoff_cache), context_len,\n",
    "        query_prompt, answer_text, model, tokenizer, exp_config)\n",
    "    del val_cutoff_cache\n",
    "\n",
    "    del bare_cache, sf_trunc_cache, bare_input, primed_input\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    p2_results.append({\n",
    "        'query_idx': qidx,\n",
    "        'query': qdata['query'],\n",
    "        'doc_len': doc_len,\n",
    "        'word_count': qdata['word_count'],\n",
    "        'bare_nll': bare_nll,\n",
    "        'values_all_nll': val_all_nll,\n",
    "        'values_cutoff_16_nll': val_cutoff_nll,\n",
    "    })\n",
    "\n",
    "    # Checkpoint\n",
    "    if (qidx + 1) % CHECKPOINT_EVERY == 0 or qidx == N_SQUAD - 1:\n",
    "        ckpt_data = {\n",
    "            'results': p2_results,\n",
    "            'query_texts': [q['query'] for q in squad_samples[:N_SQUAD]],\n",
    "            'completed': len(p2_results),\n",
    "            'total': N_SQUAD,\n",
    "            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        }\n",
    "        with open(CHECKPOINT_P2_PATH, 'w') as f:\n",
    "            json.dump(ckpt_data, f)\n",
    "        elapsed = time.time() - t_start\n",
    "        n_done = qidx - p2_start_idx + 1\n",
    "        rate = n_done / elapsed if elapsed > 0 else 0\n",
    "        remaining = (N_SQUAD - qidx - 1) / rate if rate > 0 else 0\n",
    "        tqdm.write(f\"  Checkpoint {qidx+1}/{N_SQUAD} | {n_done} done in {elapsed/60:.1f}m | \"\n",
    "                   f\"ETA: {remaining/60:.1f} min\")\n",
    "\n",
    "elapsed_total = time.time() - t_start\n",
    "print(f\"\\nPart 2 complete: {len(p2_results)} queries in {elapsed_total/60:.1f} min\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 8: Part 3 \u2014 Prefix Content x Layer Selectivity (300 queries, 3 prefix types at cutoff=16)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"PART 3: PREFIX CONTENT x LAYER SELECTIVITY ({N_PART3} queries, cutoff={CUTOFF})\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Checkpoint resume\n",
    "p3_results = []\n",
    "p3_start_idx = 0\n",
    "\n",
    "if CHECKPOINT_P3_PATH.exists():\n",
    "    with open(CHECKPOINT_P3_PATH, 'r') as f:\n",
    "        ckpt = json.load(f)\n",
    "    ckpt_queries = ckpt.get('query_texts', [])\n",
    "    current_queries = [q['query'] for q in marco_queries[:N_PART3]]\n",
    "    if ckpt_queries == current_queries:\n",
    "        p3_results = ckpt['results']\n",
    "        p3_start_idx = len(p3_results)\n",
    "        print(f\"Resuming from checkpoint: {p3_start_idx}/{N_PART3}\")\n",
    "    else:\n",
    "        print(\"Checkpoint query mismatch. Starting fresh.\")\n",
    "else:\n",
    "    print(\"No checkpoint found. Starting fresh.\")\n",
    "\n",
    "layer_indices_cutoff = list(range(CUTOFF))\n",
    "\n",
    "t_start = time.time()\n",
    "\n",
    "for qidx in tqdm(range(p3_start_idx, N_PART3), initial=p3_start_idx, total=N_PART3,\n",
    "                  desc=\"Part 3\"):\n",
    "    qdata = marco_queries[qidx]\n",
    "    query_prompt = QUERY_TEMPLATE.format(query=qdata['query'])\n",
    "    answer_text = ANSWER_TEMPLATE.format(answer=qdata['answer'])\n",
    "    passage = qdata['passage']\n",
    "    document_text = DOCUMENT_TEMPLATE.format(document=passage)\n",
    "\n",
    "    # === Build 3 prefix strings ===\n",
    "    # 1. static_fact\n",
    "    prefix_sf = sf_str\n",
    "    # 2. random (random MS MARCO passage)\n",
    "    prefix_random = SURROGATE_PREFIX_TEMPLATE.format(surrogate=random_prefix_passages[qidx])\n",
    "    # 3. oracle (query text)\n",
    "    prefix_oracle = SURROGATE_PREFIX_TEMPLATE.format(surrogate=qdata['query'])\n",
    "\n",
    "    prefixes = {\n",
    "        'static_fact': prefix_sf,\n",
    "        'random': prefix_random,\n",
    "        'oracle': prefix_oracle,\n",
    "    }\n",
    "\n",
    "    # Matched tokenization for each prefix type\n",
    "    prefix_caches = {}\n",
    "    prefix_offsets = {}\n",
    "\n",
    "    for pname, pstr in prefixes.items():\n",
    "        full_text = pstr + document_text\n",
    "        full_enc = tokenizer(full_text, return_tensors=\"pt\",\n",
    "                              add_special_tokens=True, padding=False, truncation=False)\n",
    "        full_ids_p = full_enc['input_ids'].to(exp_config.device)\n",
    "\n",
    "        p_prefix_enc = tokenizer(pstr, return_tensors=\"pt\",\n",
    "                                  add_special_tokens=True, padding=False, truncation=False)\n",
    "        p_prefix_len = p_prefix_enc['input_ids'].shape[1]\n",
    "        p_ids_only = tokenizer(pstr, return_tensors=\"pt\",\n",
    "                                add_special_tokens=False)['input_ids']\n",
    "        p_offset = p_ids_only.shape[1]\n",
    "\n",
    "        bos_id = full_ids_p[:, :1]\n",
    "        doc_ids_p = full_ids_p[:, p_prefix_len:]\n",
    "        doc_len_p = doc_ids_p.shape[1]\n",
    "\n",
    "        # Forward pass: PRIMED\n",
    "        p_prefix_ids = tokenizer(pstr, return_tensors=\"pt\",\n",
    "                                  add_special_tokens=False)['input_ids'].to(exp_config.device)\n",
    "        primed_input_p = torch.cat([bos_id, p_prefix_ids, doc_ids_p], dim=1)\n",
    "        with torch.no_grad():\n",
    "            primed_out_p = model(input_ids=primed_input_p,\n",
    "                                 attention_mask=torch.ones_like(primed_input_p),\n",
    "                                 use_cache=True, return_dict=True)\n",
    "        primed_full_p = _ensure_dynamic_cache(primed_out_p.past_key_values)\n",
    "        del primed_out_p\n",
    "\n",
    "        # Truncate + RoPE correct\n",
    "        trunc_raw_p = extract_and_truncate_cache_with_bos(primed_full_p, doc_len_p)\n",
    "        del primed_full_p\n",
    "\n",
    "        trunc_cache_p = deepcopy_cache(trunc_raw_p)\n",
    "        correct_rope_positions_with_bos(trunc_cache_p, p_offset, model)\n",
    "        del trunc_raw_p\n",
    "\n",
    "        prefix_caches[pname] = trunc_cache_p\n",
    "        prefix_offsets[pname] = p_offset\n",
    "\n",
    "        del full_enc, full_ids_p, p_prefix_enc, p_ids_only, primed_input_p, p_prefix_ids\n",
    "\n",
    "    # Use static_fact's matched tokenization for bare cache\n",
    "    full_text = sf_str + document_text\n",
    "    full_enc = tokenizer(full_text, return_tensors=\"pt\",\n",
    "                          add_special_tokens=True, padding=False, truncation=False)\n",
    "    full_ids = full_enc['input_ids'].to(exp_config.device)\n",
    "    sf_prefix_enc = tokenizer(sf_str, return_tensors=\"pt\",\n",
    "                               add_special_tokens=True, padding=False, truncation=False)\n",
    "    sf_prefix_len_matched = sf_prefix_enc['input_ids'].shape[1]\n",
    "    bos_id = full_ids[:, :1]\n",
    "    doc_ids = full_ids[:, sf_prefix_len_matched:]\n",
    "    doc_len = doc_ids.shape[1]\n",
    "    context_len = 1 + doc_len\n",
    "    del full_enc, full_ids, sf_prefix_enc\n",
    "\n",
    "    # Forward pass: BARE\n",
    "    bare_input = torch.cat([bos_id, doc_ids], dim=1)\n",
    "    with torch.no_grad():\n",
    "        bare_out = model(input_ids=bare_input,\n",
    "                         attention_mask=torch.ones_like(bare_input),\n",
    "                         use_cache=True, return_dict=True)\n",
    "    bare_cache = _ensure_dynamic_cache(bare_out.past_key_values)\n",
    "    del bare_out\n",
    "\n",
    "    # Score bare\n",
    "    bare_nll = score_answer_with_cache(\n",
    "        deepcopy_cache(bare_cache), context_len,\n",
    "        query_prompt, answer_text, model, tokenizer, exp_config)\n",
    "\n",
    "    # Score each prefix type with layer-selective values\n",
    "    prefix_nlls = {}\n",
    "    for pname in prefixes:\n",
    "        vel_cache = replace_values_at_layers(\n",
    "            bare_cache, prefix_caches[pname], layer_indices_cutoff)\n",
    "        vel_nll = score_answer_with_cache(\n",
    "            deepcopy_cache(vel_cache), context_len,\n",
    "            query_prompt, answer_text, model, tokenizer, exp_config)\n",
    "        prefix_nlls[pname] = vel_nll\n",
    "        del vel_cache\n",
    "\n",
    "    del bare_cache, bare_input, prefix_caches\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    p3_results.append({\n",
    "        'query_idx': qidx,\n",
    "        'query': qdata['query'],\n",
    "        'doc_len': doc_len,\n",
    "        'bare_nll': bare_nll,\n",
    "        'static_fact_nll': prefix_nlls['static_fact'],\n",
    "        'random_nll': prefix_nlls['random'],\n",
    "        'oracle_nll': prefix_nlls['oracle'],\n",
    "    })\n",
    "\n",
    "    # Checkpoint\n",
    "    if (qidx + 1) % CHECKPOINT_EVERY == 0 or qidx == N_PART3 - 1:\n",
    "        ckpt_data = {\n",
    "            'results': p3_results,\n",
    "            'query_texts': [q['query'] for q in marco_queries[:N_PART3]],\n",
    "            'completed': len(p3_results),\n",
    "            'total': N_PART3,\n",
    "            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        }\n",
    "        with open(CHECKPOINT_P3_PATH, 'w') as f:\n",
    "            json.dump(ckpt_data, f)\n",
    "        elapsed = time.time() - t_start\n",
    "        n_done = qidx - p3_start_idx + 1\n",
    "        rate = n_done / elapsed if elapsed > 0 else 0\n",
    "        remaining = (N_PART3 - qidx - 1) / rate if rate > 0 else 0\n",
    "        tqdm.write(f\"  Checkpoint {qidx+1}/{N_PART3} | {n_done} done in {elapsed/60:.1f}m | \"\n",
    "                   f\"ETA: {remaining/60:.1f} min\")\n",
    "\n",
    "elapsed_total = time.time() - t_start\n",
    "print(f\"\\nPart 3 complete: {len(p3_results)} queries in {elapsed_total/60:.1f} min\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 9: Part 1 Analysis \u2014 per-layer d table, cumulative d, critical layers\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PART 1 ANALYSIS: INDIVIDUAL LAYER CONTRIBUTION MAP\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Collect per-layer deltas\n",
    "layer_deltas = {l: [] for l in range(NUM_LAYERS)}\n",
    "bare_nlls_p1 = []\n",
    "\n",
    "for r in p1_results:\n",
    "    bare_nll = r['bare_nll']\n",
    "    bare_nlls_p1.append(bare_nll)\n",
    "    for l in range(NUM_LAYERS):\n",
    "        delta = bare_nll - r['layer_nlls'][str(l)]\n",
    "        layer_deltas[l].append(delta)\n",
    "\n",
    "# Per-layer Cohen's d\n",
    "print(f\"\\n{'Layer':<8} {'Mean D':>10} {'d':>8} {'Win%':>7} {'p':>12} {'sig':>5}\")\n",
    "print(\"-\" * 55)\n",
    "\n",
    "layer_analysis = {}\n",
    "for l in range(NUM_LAYERS):\n",
    "    delta = np.array(layer_deltas[l])\n",
    "    valid = np.isfinite(delta)\n",
    "    delta = delta[valid]\n",
    "    d = cohens_d(delta)\n",
    "    win = np.mean(delta > 0) * 100\n",
    "    t_stat, p_val = stats.ttest_1samp(delta, 0)\n",
    "    sig = '***' if p_val < 0.001 else '**' if p_val < 0.01 else '*' if p_val < 0.05 else 'ns'\n",
    "    print(f\"{l:<8} {np.mean(delta):>+10.4f} {d:>+8.3f} {win:>6.1f}% {p_val:>12.2e} {sig:>5}\")\n",
    "    layer_analysis[l] = {\n",
    "        'mean_delta': float(np.mean(delta)),\n",
    "        'cohens_d': float(d),\n",
    "        'win_pct': float(win),\n",
    "        't_stat': float(t_stat),\n",
    "        'p_value': float(p_val),\n",
    "    }\n",
    "\n",
    "# Rank layers by effect size\n",
    "layer_ranking = sorted(layer_analysis.items(), key=lambda x: x[1]['cohens_d'], reverse=True)\n",
    "print(f\"\\nTop 10 layers by Cohen's d:\")\n",
    "for rank, (l, a) in enumerate(layer_ranking[:10], 1):\n",
    "    sig = '***' if a['p_value'] < 0.001 else '**' if a['p_value'] < 0.01 else '*' if a['p_value'] < 0.05 else 'ns'\n",
    "    print(f\"  #{rank}: layer {l:>2}  d={a['cohens_d']:>+.3f}  win={a['win_pct']:.0f}%  {sig}\")\n",
    "\n",
    "print(f\"\\nBottom 5 layers (most harmful):\")\n",
    "for rank, (l, a) in enumerate(layer_ranking[-5:], 1):\n",
    "    sig = '***' if a['p_value'] < 0.001 else '**' if a['p_value'] < 0.01 else '*' if a['p_value'] < 0.05 else 'ns'\n",
    "    print(f\"  #{rank}: layer {l:>2}  d={a['cohens_d']:>+.3f}  win={a['win_pct']:.0f}%  {sig}\")\n",
    "\n",
    "# Cumulative d: add layers in order of individual d (greedy)\n",
    "print(f\"\\n{'Cum Layers':<12} {'Layers Added':>15} {'d':>8}\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "ranked_layers = [l for l, _ in layer_ranking]\n",
    "cum_deltas = np.zeros(len(p1_results))\n",
    "cum_analysis = []\n",
    "\n",
    "for step, l in enumerate(ranked_layers):\n",
    "    # Can't actually compute cumulative scoring without re-running model\n",
    "    # Instead, approximate: sum of individual deltas as proxy\n",
    "    cum_deltas = cum_deltas + np.array(layer_deltas[l])\n",
    "    cum_d = cohens_d(cum_deltas)\n",
    "    cum_analysis.append({\n",
    "        'n_layers': step + 1,\n",
    "        'layer_added': l,\n",
    "        'cumulative_d_approx': float(cum_d),\n",
    "    })\n",
    "    if step < 20 or step == NUM_LAYERS - 1:\n",
    "        print(f\"{step+1:<12} layer {l:>3}         {cum_d:>+8.3f}\")\n",
    "\n",
    "# Early vs late summary\n",
    "early_ds = [layer_analysis[l]['cohens_d'] for l in range(CUTOFF)]\n",
    "late_ds = [layer_analysis[l]['cohens_d'] for l in range(CUTOFF, NUM_LAYERS)]\n",
    "print(f\"\\nEarly layers (0-{CUTOFF-1}): mean d = {np.mean(early_ds):+.3f}, \"\n",
    "      f\"positive = {sum(1 for d in early_ds if d > 0)}/{len(early_ds)}\")\n",
    "print(f\"Late layers ({CUTOFF}-{NUM_LAYERS-1}):  mean d = {np.mean(late_ds):+.3f}, \"\n",
    "      f\"positive = {sum(1 for d in late_ds if d > 0)}/{len(late_ds)}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 10: Part 2+3 Analysis \u2014 cross-dataset + prefix content results\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PART 2 ANALYSIS: CROSS-DATASET (SQuAD v2)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# SQuAD v2 results\n",
    "p2_bare = np.array([r['bare_nll'] for r in p2_results])\n",
    "p2_val_all = np.array([r['values_all_nll'] for r in p2_results])\n",
    "p2_val_cutoff = np.array([r['values_cutoff_16_nll'] for r in p2_results])\n",
    "\n",
    "valid_p2 = (p2_bare != 0) & np.isfinite(p2_bare) & (p2_val_all != 0) & (p2_val_cutoff != 0)\n",
    "p2_b = p2_bare[valid_p2]\n",
    "p2_va = p2_val_all[valid_p2]\n",
    "p2_vc = p2_val_cutoff[valid_p2]\n",
    "\n",
    "p2_analysis = {}\n",
    "for name, arr in [('values_all', p2_va), ('values_cutoff_16', p2_vc)]:\n",
    "    delta = p2_b - arr\n",
    "    d = cohens_d(delta)\n",
    "    win = np.mean(delta > 0) * 100\n",
    "    t_stat, p_val = stats.ttest_1samp(delta, 0)\n",
    "    sig = '***' if p_val < 0.001 else '**' if p_val < 0.01 else '*' if p_val < 0.05 else 'ns'\n",
    "    p2_analysis[name] = {\n",
    "        'n_valid': int(np.sum(valid_p2)),\n",
    "        'mean_bare': float(np.mean(p2_b)),\n",
    "        'mean_nll': float(np.mean(arr)),\n",
    "        'mean_delta': float(np.mean(delta)),\n",
    "        'cohens_d': float(d),\n",
    "        'win_pct': float(win),\n",
    "        't_stat': float(t_stat),\n",
    "        'p_value': float(p_val),\n",
    "    }\n",
    "    print(f\"  {name:<20} d={d:>+.3f}  win={win:.1f}%  p={p_val:.2e}  {sig}\")\n",
    "\n",
    "# MS MARCO comparison (from Part 1: values at all layers vs cutoff)\n",
    "# We can compute values_all and values_cutoff from Part 1 individual layer results\n",
    "# by summing all layer deltas (approx)\n",
    "p1_bare_arr = np.array(bare_nlls_p1)\n",
    "print(f\"\\nCross-dataset comparison:\")\n",
    "print(f\"  {'Dataset':<15} {'Condition':<20} {'d':>8} {'Win%':>7} {'p':>12}\")\n",
    "print(\"  \" + \"-\" * 68)\n",
    "\n",
    "# Exp 21 MS MARCO reference for comparison\n",
    "print(f\"  {'MS MARCO':<15} {'values_cutoff_16':<20} {'(Exp 21):':>8} {'+0.227':>7} {'\u2014':>12}\")\n",
    "for name, a in p2_analysis.items():\n",
    "    sig = '***' if a['p_value'] < 0.001 else '**' if a['p_value'] < 0.01 else '*' if a['p_value'] < 0.05 else 'ns'\n",
    "    print(f\"  {'SQuAD v2':<15} {name:<20} {a['cohens_d']:>+8.3f} {a['win_pct']:>6.1f}% {a['p_value']:>12.2e}  {sig}\")\n",
    "\n",
    "# Part 3 Analysis\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"PART 3 ANALYSIS: PREFIX CONTENT x LAYER SELECTIVITY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "p3_bare = np.array([r['bare_nll'] for r in p3_results])\n",
    "p3_sf = np.array([r['static_fact_nll'] for r in p3_results])\n",
    "p3_rand = np.array([r['random_nll'] for r in p3_results])\n",
    "p3_oracle = np.array([r['oracle_nll'] for r in p3_results])\n",
    "\n",
    "valid_p3 = (p3_bare != 0) & np.isfinite(p3_bare) & (p3_sf != 0) & (p3_rand != 0) & (p3_oracle != 0)\n",
    "\n",
    "p3_analysis = {}\n",
    "print(f\"\\n{'Prefix':<15} {'Mean NLL':>10} {'d vs bare':>10} {'Win%':>7} {'p':>12} {'sig':>5}\")\n",
    "print(\"-\" * 62)\n",
    "\n",
    "for pname, parr in [('static_fact', p3_sf), ('random', p3_rand), ('oracle', p3_oracle)]:\n",
    "    b = p3_bare[valid_p3]\n",
    "    a = parr[valid_p3]\n",
    "    delta = b - a\n",
    "    d = cohens_d(delta)\n",
    "    win = np.mean(delta > 0) * 100\n",
    "    t_stat, p_val = stats.ttest_1samp(delta, 0)\n",
    "    sig = '***' if p_val < 0.001 else '**' if p_val < 0.01 else '*' if p_val < 0.05 else 'ns'\n",
    "    print(f\"{pname:<15} {np.mean(a):>10.4f} {d:>+10.3f} {win:>6.1f}% {p_val:>12.2e} {sig:>5}\")\n",
    "    p3_analysis[pname] = {\n",
    "        'n_valid': int(np.sum(valid_p3)),\n",
    "        'mean_nll': float(np.mean(a)),\n",
    "        'mean_delta': float(np.mean(delta)),\n",
    "        'cohens_d': float(d),\n",
    "        'win_pct': float(win),\n",
    "        't_stat': float(t_stat),\n",
    "        'p_value': float(p_val),\n",
    "    }\n",
    "\n",
    "# Pairwise comparisons\n",
    "print(f\"\\nPairwise prefix comparisons (at cutoff={CUTOFF}):\")\n",
    "for n1, a1, n2, a2 in [\n",
    "    ('static_fact', p3_sf, 'random', p3_rand),\n",
    "    ('static_fact', p3_sf, 'oracle', p3_oracle),\n",
    "    ('oracle', p3_oracle, 'random', p3_rand),\n",
    "]:\n",
    "    delta = a2[valid_p3] - a1[valid_p3]  # positive = n1 better\n",
    "    d = cohens_d(delta)\n",
    "    t_stat, p_val = stats.ttest_1samp(delta, 0)\n",
    "    sig = '***' if p_val < 0.001 else '**' if p_val < 0.01 else '*' if p_val < 0.05 else 'ns'\n",
    "    print(f\"  {n1} vs {n2}: d={d:+.3f}, p={p_val:.2e} {sig}\")\n",
    "\n",
    "# Exp 16 comparison (full-cache)\n",
    "print(f\"\\nComparison with Exp 16 (full-cache replacement on Gemma):\")\n",
    "vel_header = f'VEL@{CUTOFF} d'\n",
    "print(f\"  {'Prefix':<15} {'Full-cache d':>13} {vel_header:>13} {'Gain':>8}\")\n",
    "print(\"  \" + \"-\" * 55)\n",
    "exp16_map = {'static_fact': -0.031, 'random': -0.109}\n",
    "for pname in ['static_fact', 'random']:\n",
    "    if pname in exp16_map:\n",
    "        full_d = exp16_map[pname]\n",
    "        vel_d = p3_analysis[pname]['cohens_d']\n",
    "        print(f\"  {pname:<15} {full_d:>+13.3f} {vel_d:>+13.3f} {vel_d - full_d:>+8.3f}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 11: Part 4 Analysis \u2014 Value features + correlation with Part 1 d\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PART 4 ANALYSIS: VALUE FEATURE ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Aggregate features per layer\n",
    "feat_agg = {l: {'bare_l2': [], 'primed_l2': [], 'delta_norm': [], 'cosine_sim': []}\n",
    "            for l in range(NUM_LAYERS)}\n",
    "\n",
    "for qf in p4_features:\n",
    "    for feat in qf['features']:\n",
    "        l = feat['layer']\n",
    "        feat_agg[l]['bare_l2'].append(feat['bare_l2'])\n",
    "        feat_agg[l]['primed_l2'].append(feat['primed_l2'])\n",
    "        feat_agg[l]['delta_norm'].append(feat['delta_norm'])\n",
    "        feat_agg[l]['cosine_sim'].append(feat['cosine_sim'])\n",
    "\n",
    "# Summary table\n",
    "print(f\"\\n{'Layer':<8} {'Bare L2':>10} {'Primed L2':>10} {'Delta Norm':>12} {'Cosine Sim':>12} {'d (Part1)':>10}\")\n",
    "print(\"-\" * 68)\n",
    "\n",
    "feat_summary = {}\n",
    "layer_ds = []\n",
    "delta_norms = []\n",
    "cosine_sims = []\n",
    "\n",
    "for l in range(NUM_LAYERS):\n",
    "    mean_bare_l2 = np.mean(feat_agg[l]['bare_l2'])\n",
    "    mean_primed_l2 = np.mean(feat_agg[l]['primed_l2'])\n",
    "    mean_delta_norm = np.mean(feat_agg[l]['delta_norm'])\n",
    "    mean_cosine_sim = np.mean(feat_agg[l]['cosine_sim'])\n",
    "    d_val = layer_analysis[l]['cohens_d']\n",
    "    print(f\"{l:<8} {mean_bare_l2:>10.3f} {mean_primed_l2:>10.3f} {mean_delta_norm:>12.4f} \"\n",
    "          f\"{mean_cosine_sim:>12.4f} {d_val:>+10.3f}\")\n",
    "    feat_summary[l] = {\n",
    "        'bare_l2': float(mean_bare_l2),\n",
    "        'primed_l2': float(mean_primed_l2),\n",
    "        'delta_norm': float(mean_delta_norm),\n",
    "        'cosine_sim': float(mean_cosine_sim),\n",
    "    }\n",
    "    layer_ds.append(d_val)\n",
    "    delta_norms.append(mean_delta_norm)\n",
    "    cosine_sims.append(mean_cosine_sim)\n",
    "\n",
    "layer_ds = np.array(layer_ds)\n",
    "delta_norms = np.array(delta_norms)\n",
    "cosine_sims = np.array(cosine_sims)\n",
    "\n",
    "# Correlations\n",
    "print(f\"\\nCorrelation: per-layer d vs delta_norm\")\n",
    "r_dn, p_dn = stats.pearsonr(layer_ds, delta_norms)\n",
    "print(f\"  Pearson r={r_dn:+.3f}, p={p_dn:.2e}\")\n",
    "\n",
    "print(f\"\\nCorrelation: per-layer d vs cosine_sim\")\n",
    "r_cs, p_cs = stats.pearsonr(layer_ds, cosine_sims)\n",
    "print(f\"  Pearson r={r_cs:+.3f}, p={p_cs:.2e}\")\n",
    "\n",
    "# Early vs late feature comparison\n",
    "print(f\"\\nEarly (0-{CUTOFF-1}) vs Late ({CUTOFF}-{NUM_LAYERS-1}) features:\")\n",
    "early_delta = np.mean([feat_summary[l]['delta_norm'] for l in range(CUTOFF)])\n",
    "late_delta = np.mean([feat_summary[l]['delta_norm'] for l in range(CUTOFF, NUM_LAYERS)])\n",
    "early_cos = np.mean([feat_summary[l]['cosine_sim'] for l in range(CUTOFF)])\n",
    "late_cos = np.mean([feat_summary[l]['cosine_sim'] for l in range(CUTOFF, NUM_LAYERS)])\n",
    "print(f\"  Mean delta_norm: early={early_delta:.4f}, late={late_delta:.4f}, ratio={early_delta/late_delta:.2f}\" if late_delta > 0 else f\"  Mean delta_norm: early={early_delta:.4f}, late={late_delta:.4f}\")\n",
    "print(f\"  Mean cosine_sim: early={early_cos:.4f}, late={late_cos:.4f}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 12: Plots \u2014 6-panel summary figure (2x3)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "\n",
    "# ---- Panel 1 (top-left): Per-layer d bar chart ----\n",
    "ax = axes[0, 0]\n",
    "ds_per_layer = [layer_analysis[l]['cohens_d'] for l in range(NUM_LAYERS)]\n",
    "colors_layer = []\n",
    "for l in range(NUM_LAYERS):\n",
    "    p_val = layer_analysis[l]['p_value']\n",
    "    d_val = layer_analysis[l]['cohens_d']\n",
    "    if p_val < 0.001:\n",
    "        colors_layer.append('#2ca02c' if d_val > 0 else '#d62728')\n",
    "    elif p_val < 0.05:\n",
    "        colors_layer.append('#98df8a' if d_val > 0 else '#ff9896')\n",
    "    else:\n",
    "        colors_layer.append('#cccccc')\n",
    "\n",
    "ax.bar(range(NUM_LAYERS), ds_per_layer, color=colors_layer, edgecolor='black', linewidth=0.3)\n",
    "ax.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "ax.axvline(x=CUTOFF - 0.5, color='red', linestyle='--', linewidth=1.5, alpha=0.7,\n",
    "           label=f'Cutoff={CUTOFF}')\n",
    "ax.set_xlabel('Layer')\n",
    "ax.set_ylabel(\"Cohen's d (single layer)\")\n",
    "ax.set_title(\"Part 1: Per-Layer Effect Size\")\n",
    "ax.legend(fontsize=8)\n",
    "\n",
    "# ---- Panel 2 (top-center): Cumulative d ----\n",
    "ax = axes[0, 1]\n",
    "cum_ds = [ca['cumulative_d_approx'] for ca in cum_analysis]\n",
    "cum_labels = [ca['layer_added'] for ca in cum_analysis]\n",
    "ax.plot(range(1, NUM_LAYERS + 1), cum_ds, marker='.', markersize=3, linewidth=1.5, color='#1f77b4')\n",
    "ax.axhline(y=EXP21_REF['values_early_layers_d'], color='#9467bd', linestyle='--', linewidth=1,\n",
    "           label=f\"Exp 21 VEL d={EXP21_REF['values_early_layers_d']:+.3f}\")\n",
    "ax.axhline(y=0, color='gray', linestyle='-', alpha=0.3)\n",
    "ax.axvline(x=CUTOFF, color='red', linestyle='--', linewidth=1, alpha=0.5)\n",
    "\n",
    "# Annotate peak\n",
    "peak_idx = int(np.argmax(cum_ds))\n",
    "ax.annotate(f\"Peak: {cum_ds[peak_idx]:+.3f}\\n({peak_idx+1} layers)\",\n",
    "            (peak_idx + 1, cum_ds[peak_idx]),\n",
    "            textcoords='offset points', xytext=(15, -10),\n",
    "            fontsize=8, arrowprops=dict(arrowstyle='->', color='black'))\n",
    "\n",
    "ax.set_xlabel('Number of layers added (ranked by d)')\n",
    "ax.set_ylabel(\"Cumulative d (approx)\")\n",
    "ax.set_title(\"Part 1: Cumulative d (greedy)\")\n",
    "ax.legend(fontsize=8)\n",
    "\n",
    "# ---- Panel 3 (top-right): Cosine similarity + delta norm by layer ----\n",
    "ax = axes[0, 2]\n",
    "ax2 = ax.twinx()\n",
    "\n",
    "x = range(NUM_LAYERS)\n",
    "ln1 = ax.plot(x, cosine_sims, 'b-', linewidth=1.5, label='Cosine sim')\n",
    "ln2 = ax2.plot(x, delta_norms, 'r-', linewidth=1.5, label='Delta norm')\n",
    "\n",
    "ax.set_xlabel('Layer')\n",
    "ax.set_ylabel('Cosine similarity', color='b')\n",
    "ax2.set_ylabel('Delta norm', color='r')\n",
    "ax.axvline(x=CUTOFF - 0.5, color='gray', linestyle='--', linewidth=1, alpha=0.5)\n",
    "ax.set_title(\"Part 4: Value Features by Layer\")\n",
    "\n",
    "lns = ln1 + ln2\n",
    "labs = [l.get_label() for l in lns]\n",
    "ax.legend(lns, labs, fontsize=8, loc='center right')\n",
    "\n",
    "# ---- Panel 4 (bottom-left): SQuAD v2 vs MS MARCO ----\n",
    "ax = axes[1, 0]\n",
    "\n",
    "# Grouped bars: dataset x condition\n",
    "conditions = ['values_all', 'values_cutoff_16']\n",
    "datasets = ['MS MARCO', 'SQuAD v2']\n",
    "\n",
    "# MS MARCO reference values\n",
    "marco_ds = [EXP19_REF['values_only_d'], EXP21_REF['values_early_layers_d']]\n",
    "squad_ds = [p2_analysis['values_all']['cohens_d'], p2_analysis['values_cutoff_16']['cohens_d']]\n",
    "\n",
    "x_pos = np.arange(len(conditions))\n",
    "w = 0.35\n",
    "bars1 = ax.bar(x_pos - w/2, marco_ds, w, color='#1f77b4', edgecolor='black', linewidth=0.5,\n",
    "               label='MS MARCO')\n",
    "bars2 = ax.bar(x_pos + w/2, squad_ds, w, color='#ff7f0e', edgecolor='black', linewidth=0.5,\n",
    "               label='SQuAD v2')\n",
    "\n",
    "ax.axhline(y=0, color='gray', linestyle='-', alpha=0.3)\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels(['values_all\\n(34 layers)', f'values_cutoff_{CUTOFF}\\n(layers 0-{CUTOFF-1})'])\n",
    "ax.set_ylabel(\"Cohen's d vs Bare\")\n",
    "ax.set_title(\"Part 2: Cross-Dataset Comparison\")\n",
    "ax.legend(fontsize=8)\n",
    "\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        h = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, h + 0.005 if h >= 0 else h - 0.015,\n",
    "                f\"{h:+.3f}\", ha='center', va='bottom' if h >= 0 else 'top', fontsize=8)\n",
    "\n",
    "# ---- Panel 5 (bottom-center): Prefix content bars ----\n",
    "ax = axes[1, 1]\n",
    "\n",
    "prefix_names = ['static_fact', 'random', 'oracle']\n",
    "prefix_ds = [p3_analysis[pn]['cohens_d'] for pn in prefix_names]\n",
    "prefix_colors = ['#2ca02c', '#d62728', '#9467bd']\n",
    "\n",
    "bars = ax.bar(range(len(prefix_names)), prefix_ds, color=prefix_colors,\n",
    "              edgecolor='black', linewidth=0.5)\n",
    "\n",
    "ax.axhline(y=0, color='gray', linestyle='-', alpha=0.3)\n",
    "ax.set_xticks(range(len(prefix_names)))\n",
    "ax.set_xticklabels(prefix_names)\n",
    "ax.set_ylabel(\"Cohen's d vs Bare\")\n",
    "ax.set_title(f\"Part 3: Prefix Content (cutoff={CUTOFF})\")\n",
    "\n",
    "for i, (d_val, pn) in enumerate(zip(prefix_ds, prefix_names)):\n",
    "    p_val = p3_analysis[pn]['p_value']\n",
    "    sig = '***' if p_val < 0.001 else '**' if p_val < 0.01 else '*' if p_val < 0.05 else 'ns'\n",
    "    ax.text(i, d_val + 0.005 if d_val >= 0 else d_val - 0.015,\n",
    "            f\"{d_val:+.3f} {sig}\", ha='center',\n",
    "            va='bottom' if d_val >= 0 else 'top', fontsize=9)\n",
    "\n",
    "# ---- Panel 6 (bottom-right): Scatter per-layer d vs delta norm ----\n",
    "ax = axes[1, 2]\n",
    "\n",
    "ax.scatter(delta_norms, layer_ds, c=range(NUM_LAYERS), cmap='viridis', s=60,\n",
    "           edgecolors='black', linewidths=0.5, zorder=3)\n",
    "\n",
    "# Color bar for layer index\n",
    "sm = plt.cm.ScalarMappable(cmap='viridis', norm=plt.Normalize(0, NUM_LAYERS - 1))\n",
    "sm.set_array([])\n",
    "cbar = fig.colorbar(sm, ax=ax, shrink=0.8, label='Layer index')\n",
    "\n",
    "# Fit line\n",
    "z = np.polyfit(delta_norms, layer_ds, 1)\n",
    "x_fit = np.linspace(delta_norms.min(), delta_norms.max(), 100)\n",
    "ax.plot(x_fit, np.polyval(z, x_fit), 'r--', linewidth=1.5, alpha=0.7)\n",
    "\n",
    "ax.set_xlabel('Mean delta norm (primed - bare values)')\n",
    "ax.set_ylabel(\"Cohen's d (single layer)\")\n",
    "ax.set_title(f\"Part 4: d vs Delta Norm (r={r_dn:+.3f})\")\n",
    "ax.axhline(y=0, color='gray', linestyle='-', alpha=0.3)\n",
    "\n",
    "# Annotate a few key layers\n",
    "for l in [0, CUTOFF-1, CUTOFF, NUM_LAYERS-1]:\n",
    "    ax.annotate(f\"L{l}\", (delta_norms[l], layer_ds[l]),\n",
    "                textcoords='offset points', xytext=(5, 5), fontsize=7)\n",
    "\n",
    "plt.suptitle('Exp 24: Gemma Layer-Selective Mechanism Deep Dive', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'analysis_plots.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"Plots saved to {RESULTS_DIR / 'analysis_plots.png'}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 13: Save results.json + CSVs\n",
    "import csv\n",
    "\n",
    "# --- Part 1 CSV (per query x per layer) ---\n",
    "with open(CSV_P1_PATH, 'w', newline='') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=[\n",
    "        'query_idx', 'layer', 'bare_nll', 'layer_nll', 'delta_nll', 'cohens_d'])\n",
    "    writer.writeheader()\n",
    "    for r in p1_results:\n",
    "        for l in range(NUM_LAYERS):\n",
    "            layer_nll = r['layer_nlls'][str(l)]\n",
    "            writer.writerow({\n",
    "                'query_idx': r['query_idx'],\n",
    "                'layer': l,\n",
    "                'bare_nll': r['bare_nll'],\n",
    "                'layer_nll': layer_nll,\n",
    "                'delta_nll': r['bare_nll'] - layer_nll,\n",
    "                'cohens_d': layer_analysis[l]['cohens_d'],\n",
    "            })\n",
    "print(f\"Part 1 CSV saved: {CSV_P1_PATH}\")\n",
    "\n",
    "# --- Part 2 CSV ---\n",
    "with open(CSV_P2_PATH, 'w', newline='') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=[\n",
    "        'query_idx', 'word_count', 'bare_nll', 'values_all_nll', 'values_cutoff_16_nll'])\n",
    "    writer.writeheader()\n",
    "    for r in p2_results:\n",
    "        writer.writerow({\n",
    "            'query_idx': r['query_idx'],\n",
    "            'word_count': r['word_count'],\n",
    "            'bare_nll': r['bare_nll'],\n",
    "            'values_all_nll': r['values_all_nll'],\n",
    "            'values_cutoff_16_nll': r['values_cutoff_16_nll'],\n",
    "        })\n",
    "print(f\"Part 2 CSV saved: {CSV_P2_PATH}\")\n",
    "\n",
    "# --- Part 3 CSV ---\n",
    "with open(CSV_P3_PATH, 'w', newline='') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=[\n",
    "        'query_idx', 'bare_nll', 'static_fact_nll', 'random_nll', 'oracle_nll'])\n",
    "    writer.writeheader()\n",
    "    for r in p3_results:\n",
    "        writer.writerow({\n",
    "            'query_idx': r['query_idx'],\n",
    "            'bare_nll': r['bare_nll'],\n",
    "            'static_fact_nll': r['static_fact_nll'],\n",
    "            'random_nll': r['random_nll'],\n",
    "            'oracle_nll': r['oracle_nll'],\n",
    "        })\n",
    "print(f\"Part 3 CSV saved: {CSV_P3_PATH}\")\n",
    "\n",
    "# --- Part 4 CSV ---\n",
    "with open(CSV_P4_PATH, 'w', newline='') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=[\n",
    "        'query_idx', 'layer', 'bare_l2', 'primed_l2', 'delta_norm', 'cosine_sim'])\n",
    "    writer.writeheader()\n",
    "    for qf in p4_features:\n",
    "        for feat in qf['features']:\n",
    "            writer.writerow({\n",
    "                'query_idx': qf['query_idx'],\n",
    "                'layer': feat['layer'],\n",
    "                'bare_l2': feat['bare_l2'],\n",
    "                'primed_l2': feat['primed_l2'],\n",
    "                'delta_norm': feat['delta_norm'],\n",
    "                'cosine_sim': feat['cosine_sim'],\n",
    "            })\n",
    "print(f\"Part 4 CSV saved: {CSV_P4_PATH}\")\n",
    "\n",
    "# --- Combined results.json ---\n",
    "final = {\n",
    "    'experiment': 'exp24_gemma_layer_mechanism_deep_dive',\n",
    "    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'config': {\n",
    "        'model_name': MODEL_NAME,\n",
    "        'model_type': 'gemma3',\n",
    "        'seed': SEED,\n",
    "        'dataset_marco': 'MS MARCO v1.1 validation',\n",
    "        'dataset_squad': 'SQuAD v2 validation',\n",
    "        'max_passage_words': MAX_PASSAGE_WORDS,\n",
    "        'part1': {'n_queries': N_PART1, 'n_layers': NUM_LAYERS},\n",
    "        'part2': {'n_queries': N_SQUAD, 'conditions': ['bare', 'values_all', 'values_cutoff_16']},\n",
    "        'part3': {'n_queries': N_PART3, 'cutoff': CUTOFF, 'prefix_types': ['static_fact', 'random', 'oracle']},\n",
    "        'part4': {'n_queries': N_PART1, 'features': ['bare_l2', 'primed_l2', 'delta_norm', 'cosine_sim']},\n",
    "    },\n",
    "    'gemma_architecture': {\n",
    "        'hidden_size': text_config.hidden_size,\n",
    "        'num_layers': NUM_LAYERS,\n",
    "        'num_attention_heads': text_config.num_attention_heads,\n",
    "        'num_kv_heads': text_config.num_key_value_heads,\n",
    "        'head_dim': _get_head_dim(model.config),\n",
    "        'rope_thetas': sorted(list(thetas)),\n",
    "    },\n",
    "    'part1_layer_analysis': {str(k): v for k, v in layer_analysis.items()},\n",
    "    'part1_layer_ranking': [{'layer': l, **a} for l, a in layer_ranking],\n",
    "    'part1_cumulative': cum_analysis,\n",
    "    'part1_early_vs_late': {\n",
    "        'early_mean_d': float(np.mean(early_ds)),\n",
    "        'early_positive_count': sum(1 for d in early_ds if d > 0),\n",
    "        'late_mean_d': float(np.mean(late_ds)),\n",
    "        'late_positive_count': sum(1 for d in late_ds if d > 0),\n",
    "    },\n",
    "    'part2_analysis': p2_analysis,\n",
    "    'part3_analysis': p3_analysis,\n",
    "    'part4_feature_summary': {str(k): v for k, v in feat_summary.items()},\n",
    "    'part4_correlations': {\n",
    "        'd_vs_delta_norm': {'r': float(r_dn), 'p': float(p_dn)},\n",
    "        'd_vs_cosine_sim': {'r': float(r_cs), 'p': float(p_cs)},\n",
    "    },\n",
    "    'reference_values': {\n",
    "        'exp19_gemma': EXP19_REF,\n",
    "        'exp21_gemma': EXP21_REF,\n",
    "        'exp16_gemma': EXP16_REF,\n",
    "    },\n",
    "    'part1_per_query': p1_results,\n",
    "    'part2_per_query': p2_results,\n",
    "    'part3_per_query': p3_results,\n",
    "}\n",
    "\n",
    "with open(FINAL_RESULTS_PATH, 'w') as f:\n",
    "    json.dump(final, f, indent=2)\n",
    "\n",
    "print(f\"\\nResults saved to {FINAL_RESULTS_PATH}\")\n",
    "print(f\"File size: {FINAL_RESULTS_PATH.stat().st_size / 1024:.1f} KB\")\n",
    "\n",
    "# Final summary\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"SUMMARY \u2014 Exp 24: Gemma Layer-Selective Mechanism Deep Dive\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Model: Gemma 3 4B ({NUM_LAYERS} layers, head_dim={_get_head_dim(model.config)}, bfloat16)\")\n",
    "\n",
    "print(f\"\\nPart 1: Individual Layer Map ({N_PART1} queries)\")\n",
    "top5_str = ', '.join(f'L{l}(d={a[\"cohens_d\"]:+.3f})' for l, a in layer_ranking[:5])\n",
    "print(f\"  Top 5 layers: {top5_str}\")\n",
    "print(f\"  Early (0-{CUTOFF-1}) mean d: {np.mean(early_ds):+.3f}\")\n",
    "print(f\"  Late ({CUTOFF}-{NUM_LAYERS-1}) mean d: {np.mean(late_ds):+.3f}\")\n",
    "\n",
    "print(f\"\\nPart 2: SQuAD v2 Cross-Dataset ({N_SQUAD} queries)\")\n",
    "for name, a in p2_analysis.items():\n",
    "    sig = '***' if a['p_value'] < 0.001 else '**' if a['p_value'] < 0.01 else '*' if a['p_value'] < 0.05 else 'ns'\n",
    "    print(f\"  {name}: d={a['cohens_d']:+.3f}, win={a['win_pct']:.0f}%, {sig}\")\n",
    "\n",
    "print(f\"\\nPart 3: Prefix Content (cutoff={CUTOFF}, {N_PART3} queries)\")\n",
    "for pn in ['static_fact', 'random', 'oracle']:\n",
    "    a = p3_analysis[pn]\n",
    "    sig = '***' if a['p_value'] < 0.001 else '**' if a['p_value'] < 0.01 else '*' if a['p_value'] < 0.05 else 'ns'\n",
    "    print(f\"  {pn}: d={a['cohens_d']:+.3f}, win={a['win_pct']:.0f}%, {sig}\")\n",
    "\n",
    "print(f\"\\nPart 4: Value Features\")\n",
    "print(f\"  d vs delta_norm: r={r_dn:+.3f} (p={p_dn:.2e})\")\n",
    "print(f\"  d vs cosine_sim: r={r_cs:+.3f} (p={p_cs:.2e})\")\n",
    "\n",
    "print(f\"\\nDone!\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 14: GPU cleanup\n",
    "import gc\n",
    "\n",
    "print(\"Cleaning up GPU memory...\")\n",
    "mem_before = torch.cuda.memory_allocated() / 1e9\n",
    "\n",
    "del model\n",
    "del tokenizer\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "mem_after = torch.cuda.memory_allocated() / 1e9\n",
    "print(f\"GPU memory: {mem_before:.2f} GB -> {mem_after:.2f} GB\")\n",
    "print(\"Cleanup complete.\")"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}