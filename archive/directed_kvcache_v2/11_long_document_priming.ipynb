{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exp 11: Long-Document Priming \u2014 Does It Scale?\n",
    "\n",
    "## Motivation\n",
    "\n",
    "All v2 experiments (01-10) used MS MARCO v1.1, where passages average ~60 words\n",
    "(max 300 words). In production ad-serving, documents will be much longer \u2014 full\n",
    "web pages, articles, product descriptions. The key question: **does our best\n",
    "priming approach (static_fact_trunc, d=+0.438 on MS MARCO) still work on longer\n",
    "documents?**\n",
    "\n",
    "Prior evidence (v1 Exp 19) showed priming hurts on datasets with longer documents\n",
    "(CNN/DailyMail d=-1.31, NarrativeQA d=-0.35), but that used full-context mode\n",
    "with known bugs (BPE mismatch, Document:\\n framing). This experiment re-tests\n",
    "with clean v2 methodology (truncated prefix, matched tokenization, no framing).\n",
    "\n",
    "## Dataset: Natural Questions\n",
    "\n",
    "Real Google search queries over full Wikipedia articles:\n",
    "- **Queries**: Short factoid Google queries (closest to production ad queries)\n",
    "- **Documents**: Full Wikipedia article text (100-10000+ words)\n",
    "- **Answers**: Short extractive answer spans (entity names, dates, numbers)\n",
    "\n",
    "## Design\n",
    "\n",
    "| Parameter | Value |\n",
    "|-----------|-------|\n",
    "| Dataset | Natural Questions (validation) |\n",
    "| Length bins | 100-300w, 300-800w, 800-2000w, 2000-4000w |\n",
    "| Samples per bin | ~125 (500 total) |\n",
    "| Conditions | 5: bare, static_fact_trunc, random_trunc, llm_kw_trunc, oracle_trunc |\n",
    "| Forward passes | 5 \u00d7 500 = 2500 |\n",
    "| Estimated runtime | 4-6 hours |\n",
    "\n",
    "## 5 Primary Comparisons (Bonferroni alpha = 0.01)\n",
    "\n",
    "| # | Comparison | Question |\n",
    "|---|-----------|----------|\n",
    "| C1 | static_fact_trunc vs bare | Does static_fact help overall? |\n",
    "| C2 | random_trunc vs bare | Does ANY prefix help overall? |\n",
    "| C3 | llm_kw_trunc vs bare | Do LLM keywords help overall? |\n",
    "| C4 | oracle_trunc vs bare | Does the perfect query help? |\n",
    "| C5 | static_fact_trunc vs random_trunc | Is content better than noise? |\n",
    "\n",
    "## Key Interaction Analysis\n",
    "\n",
    "For each comparison, test per length bin. If static_fact benefit decreases with\n",
    "document length, we expect a negative slope in the length \u00d7 d regression."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 1: Setup \u2014 permissions, seeds, results directory\n",
    "import os\n",
    "os.umask(0o000)\n",
    "\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "RESULTS_DIR = Path(\"results/exp11\")\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "SURROGATES_DIR = RESULTS_DIR / \"surrogates\"\n",
    "SURROGATES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CHECKPOINT_PATH = RESULTS_DIR / \"checkpoint.json\"\n",
    "FINAL_RESULTS_PATH = RESULTS_DIR / \"results.json\"\n",
    "\n",
    "print(f\"SEED: {SEED}\")\n",
    "print(f\"Results directory: {RESULTS_DIR}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 2: Load model (Mistral-7B 4-bit)\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "print(f\"Loading {MODEL_NAME} (4-bit)...\")\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "print(f\"Model loaded. dtype={model.dtype}, device={model.device}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 3: Config, constants, and helper functions\n",
    "sys.path.insert(0, \".\")\n",
    "\n",
    "from lib.config import ExperimentConfig\n",
    "from lib.kv_cache import (\n",
    "    deepcopy_cache,\n",
    "    extract_and_truncate_cache_with_bos,\n",
    "    correct_rope_positions_with_bos,\n",
    "    score_answer_with_cache,\n",
    ")\n",
    "from lib.analysis import cohens_d\n",
    "from lib.surrogate import STATIC_SURROGATE_QUERIES, generate_surrogate_with_template\n",
    "from scipy import stats\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "config = ExperimentConfig(\n",
    "    model_name=MODEL_NAME,\n",
    "    num_samples=2000,  # pool to draw from\n",
    "    seed=SEED,\n",
    ")\n",
    "\n",
    "# Templates \u2014 bare text, no \"Document:\\n\" framing\n",
    "SURROGATE_PREFIX_TEMPLATE = \"{surrogate}\\n\"\n",
    "DOCUMENT_TEMPLATE = \"{document}\"\n",
    "QUERY_TEMPLATE = \"\\nQuery: {query}\\nAnswer:\"\n",
    "ANSWER_TEMPLATE = \" {answer}\"\n",
    "\n",
    "N_EVAL = 500  # total target (125 per bin)\n",
    "N_COMPARISONS = 5\n",
    "BONFERRONI_ALPHA = 0.05 / N_COMPARISONS\n",
    "CHECKPOINT_EVERY = 25\n",
    "\n",
    "STATIC_FACTUAL_PHRASE = STATIC_SURROGATE_QUERIES['static_factual']['query']\n",
    "\n",
    "# Length bins (word count)\n",
    "LENGTH_BINS = [\n",
    "    ('short',     100,  300),   # MS MARCO-like\n",
    "    ('medium',    300,  800),   # Moderate web page\n",
    "    ('long',      800,  2000),  # Full article section\n",
    "    ('very_long', 2000, 4000),  # Full article\n",
    "]\n",
    "SAMPLES_PER_BIN = 125\n",
    "MAX_DOC_WORDS = 4000  # hard cap (context window safety)\n",
    "\n",
    "CONDITION_NAMES = [\n",
    "    'bare',\n",
    "    'static_fact_trunc',\n",
    "    'random_trunc',\n",
    "    'llm_kw_trunc',\n",
    "    'oracle_trunc',\n",
    "]\n",
    "\n",
    "# LLM keyword generation prompt (same as previous experiments)\n",
    "LLM_KW_PROMPT = (\n",
    "    \"You are helping index a document for search. Write a search query the way \"\n",
    "    \"real users type into Google: just keywords, no complete sentences, no question marks. \"\n",
    "    \"Think of someone quickly typing a few relevant words. \"\n",
    "    \"Output only the keyword query (3-6 words), nothing else.\\n\\n\"\n",
    "    \"Document:\"\n",
    ")\n",
    "\n",
    "# Max words of document to show for LLM keyword generation (truncate long docs)\n",
    "LLM_KW_MAX_DOC_WORDS = 500\n",
    "\n",
    "\n",
    "def build_primed_and_truncated(prefix_text, bos_id, doc_ids, doc_len, model, tokenizer, config):\n",
    "    \"\"\"Build a primed cache: tokenize prefix, concat [BOS][prefix][doc], forward, truncate+RoPE.\n",
    "\n",
    "    Returns:\n",
    "        (trunc_cache, prefix_token_len)\n",
    "    \"\"\"\n",
    "    prefix_str = SURROGATE_PREFIX_TEMPLATE.format(surrogate=prefix_text)\n",
    "    prefix_enc = tokenizer(prefix_str, return_tensors=\"pt\",\n",
    "                           add_special_tokens=False, padding=False, truncation=False)\n",
    "    prefix_ids = prefix_enc['input_ids'].to(config.device)\n",
    "    prefix_token_len = 1 + prefix_ids.shape[1]  # BOS + prefix tokens\n",
    "\n",
    "    full_ids = torch.cat([bos_id, prefix_ids, doc_ids], dim=1)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = model(input_ids=full_ids,\n",
    "                    attention_mask=torch.ones_like(full_ids),\n",
    "                    use_cache=True, return_dict=True)\n",
    "\n",
    "    trunc_cache = extract_and_truncate_cache_with_bos(out.past_key_values, doc_len)\n",
    "    correct_rope_positions_with_bos(trunc_cache, prefix_token_len - 1, model)\n",
    "\n",
    "    del out\n",
    "    return trunc_cache, prefix_token_len\n",
    "\n",
    "\n",
    "print(\"Config ready\")\n",
    "print(f\"  N_EVAL: {N_EVAL}\")\n",
    "print(f\"  SAMPLES_PER_BIN: {SAMPLES_PER_BIN}\")\n",
    "print(f\"  bonferroni_alpha: {BONFERRONI_ALPHA:.4f} ({N_COMPARISONS} comparisons)\")\n",
    "print(f\"  conditions: {len(CONDITION_NAMES)}\")\n",
    "print(f\"  static_factual_phrase: '{STATIC_FACTUAL_PHRASE}'\")\n",
    "print(f\"  length_bins: {LENGTH_BINS}\")\n",
    "print(f\"  max_doc_words: {MAX_DOC_WORDS}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 4: Load Natural Questions \u2014 extract clean text + short answers, stratify by length\n",
    "from datasets import load_dataset\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"LOADING NATURAL QUESTIONS (validation split)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Check for cached samples\n",
    "SAMPLES_CACHE_PATH = RESULTS_DIR / \"nq_samples.json\"\n",
    "\n",
    "if SAMPLES_CACHE_PATH.exists():\n",
    "    with open(SAMPLES_CACHE_PATH, 'r') as f:\n",
    "        cached = json.load(f)\n",
    "    samples = cached['samples']\n",
    "    print(f\"Loaded {len(samples)} cached NQ samples from {SAMPLES_CACHE_PATH}\")\n",
    "else:\n",
    "    print(\"Loading NQ dataset (streaming mode)...\")\n",
    "    nq = load_dataset(\n",
    "        \"google-research-datasets/natural_questions\",\n",
    "        split=\"validation\",\n",
    "        streaming=True,\n",
    "    )\n",
    "\n",
    "    # Collect samples into length bins\n",
    "    bin_samples = {name: [] for name, _, _ in LENGTH_BINS}\n",
    "    n_processed = 0\n",
    "    n_no_answer = 0\n",
    "    n_too_short = 0\n",
    "    n_too_long = 0\n",
    "\n",
    "    for example in tqdm(nq, desc=\"Processing NQ\"):\n",
    "        n_processed += 1\n",
    "\n",
    "        # Extract clean document text (non-HTML tokens)\n",
    "        doc_tokens = example['document']['tokens']\n",
    "        if isinstance(doc_tokens, dict):\n",
    "            # HF may return dict of lists instead of list of dicts\n",
    "            token_strs = doc_tokens['token']\n",
    "            is_html_flags = doc_tokens['is_html']\n",
    "            clean_tokens = [t for t, h in zip(token_strs, is_html_flags) if not h]\n",
    "        else:\n",
    "            clean_tokens = [t['token'] for t in doc_tokens if not t['is_html']]\n",
    "\n",
    "        doc_text = ' '.join(clean_tokens)\n",
    "        word_count = len(doc_text.split())\n",
    "\n",
    "        # Skip if outside our range\n",
    "        if word_count < LENGTH_BINS[0][1]:  # below minimum\n",
    "            n_too_short += 1\n",
    "            continue\n",
    "        if word_count > MAX_DOC_WORDS:\n",
    "            # Truncate to MAX_DOC_WORDS\n",
    "            words = doc_text.split()\n",
    "            doc_text = ' '.join(words[:MAX_DOC_WORDS])\n",
    "            word_count = MAX_DOC_WORDS\n",
    "\n",
    "        # Extract short answer\n",
    "        annotations = example['annotations']\n",
    "        short_answers_list = annotations['short_answers']\n",
    "\n",
    "        answer_text = None\n",
    "        # NQ short_answers is a list of dicts (one per annotator).\n",
    "        # Each dict has 'start_token', 'end_token', 'text' as lists (one entry per span).\n",
    "        # Use 'text' directly when available; fall back to token reconstruction.\n",
    "        for annotator_sa in short_answers_list:\n",
    "            if not annotator_sa:\n",
    "                continue\n",
    "            # Use pre-extracted text field if available\n",
    "            texts = annotator_sa.get('text', [])\n",
    "            if texts:\n",
    "                answer_text = texts[0]\n",
    "                break\n",
    "            # Fallback: reconstruct from document tokens\n",
    "            starts = annotator_sa.get('start_token', [])\n",
    "            ends = annotator_sa.get('end_token', [])\n",
    "            if not starts or not ends:\n",
    "                continue\n",
    "            start_tok = starts[0] if isinstance(starts, list) else starts\n",
    "            end_tok = ends[0] if isinstance(ends, list) else ends\n",
    "            if start_tok >= 0 and end_tok > start_tok:\n",
    "                if isinstance(doc_tokens, dict):\n",
    "                    ans_tokens = [\n",
    "                        doc_tokens['token'][i]\n",
    "                        for i in range(start_tok, min(end_tok, len(doc_tokens['token'])))\n",
    "                        if not doc_tokens['is_html'][i]\n",
    "                    ]\n",
    "                else:\n",
    "                    ans_tokens = [\n",
    "                        doc_tokens[i]['token']\n",
    "                        for i in range(start_tok, min(end_tok, len(doc_tokens)))\n",
    "                        if not doc_tokens[i]['is_html']\n",
    "                    ]\n",
    "                if ans_tokens:\n",
    "                    answer_text = ' '.join(ans_tokens)\n",
    "                    break\n",
    "\n",
    "        if not answer_text or len(answer_text.strip()) == 0:\n",
    "            n_no_answer += 1\n",
    "            continue\n",
    "\n",
    "        # Skip very long answers (>20 words) \u2014 we want factoid answers\n",
    "        if len(answer_text.split()) > 20:\n",
    "            continue\n",
    "\n",
    "        # Extract query\n",
    "        question = example['question']\n",
    "        if isinstance(question, dict):\n",
    "            query = question.get('text', '')\n",
    "        else:\n",
    "            query = str(question)\n",
    "\n",
    "        if not query.strip():\n",
    "            continue\n",
    "\n",
    "        # Assign to length bin\n",
    "        assigned = False\n",
    "        for bin_name, bin_min, bin_max in LENGTH_BINS:\n",
    "            if bin_min <= word_count < bin_max:\n",
    "                if len(bin_samples[bin_name]) < SAMPLES_PER_BIN:\n",
    "                    bin_samples[bin_name].append({\n",
    "                        'passage': doc_text,\n",
    "                        'query': query,\n",
    "                        'answer': answer_text,\n",
    "                        'word_count': word_count,\n",
    "                        'length_bin': bin_name,\n",
    "                    })\n",
    "                assigned = True\n",
    "                break\n",
    "\n",
    "        # Check if all bins are full\n",
    "        all_full = all(len(bin_samples[name]) >= SAMPLES_PER_BIN for name, _, _ in LENGTH_BINS)\n",
    "        if all_full:\n",
    "            print(f\"All bins full after processing {n_processed} examples.\")\n",
    "            break\n",
    "\n",
    "    # Combine all bins\n",
    "    samples = []\n",
    "    for bin_name, _, _ in LENGTH_BINS:\n",
    "        bin_s = bin_samples[bin_name]\n",
    "        np.random.seed(SEED)\n",
    "        np.random.shuffle(bin_s)\n",
    "        samples.extend(bin_s)\n",
    "        print(f\"  {bin_name}: {len(bin_s)} samples\")\n",
    "\n",
    "    print(f\"\\nTotal samples: {len(samples)}\")\n",
    "    print(f\"Processed: {n_processed}, No answer: {n_no_answer}, Too short: {n_too_short}\")\n",
    "\n",
    "    # Cache for fast reload\n",
    "    with open(SAMPLES_CACHE_PATH, 'w') as f:\n",
    "        json.dump({'samples': samples, 'n_processed': n_processed}, f)\n",
    "    print(f\"Cached to {SAMPLES_CACHE_PATH}\")\n",
    "\n",
    "N = len(samples)\n",
    "\n",
    "# Summary statistics\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"SAMPLE SUMMARY\")\n",
    "print(f\"{'='*70}\")\n",
    "for bin_name, bin_min, bin_max in LENGTH_BINS:\n",
    "    bin_s = [s for s in samples if s['length_bin'] == bin_name]\n",
    "    if bin_s:\n",
    "        wcs = [s['word_count'] for s in bin_s]\n",
    "        print(f\"  {bin_name} ({bin_min}-{bin_max}w): n={len(bin_s)}, \"\n",
    "              f\"mean={np.mean(wcs):.0f}w, range=[{min(wcs)}, {max(wcs)}]\")\n",
    "\n",
    "print(f\"\\nExample (short):\")\n",
    "ex_short = [s for s in samples if s['length_bin'] == 'short']\n",
    "if ex_short:\n",
    "    print(f\"  Q: {ex_short[0]['query']}\")\n",
    "    print(f\"  A: {ex_short[0]['answer']}\")\n",
    "    print(f\"  Doc ({ex_short[0]['word_count']}w): {ex_short[0]['passage'][:150]}...\")\n",
    "\n",
    "print(f\"\\nExample (very_long):\")\n",
    "ex_long = [s for s in samples if s['length_bin'] == 'very_long']\n",
    "if ex_long:\n",
    "    print(f\"  Q: {ex_long[0]['query']}\")\n",
    "    print(f\"  A: {ex_long[0]['answer']}\")\n",
    "    print(f\"  Doc ({ex_long[0]['word_count']}w): {ex_long[0]['passage'][:150]}...\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 5: Condition explanation\n",
    "print(\"=\" * 70)\n",
    "print(\"EXPERIMENTAL CONDITIONS EXPLAINED\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "conditions_explained = [\n",
    "    (\"1. bare\",\n",
    "     \"[BOS][doc]\",\n",
    "     \"No prefix \u2014 baseline. Identical to bare caches used in Exps 01-10.\"),\n",
    "    (\"2. static_fact_trunc\",\n",
    "     \"[BOS][static_fact\\\\n][doc] \u2192 truncate + RoPE\",\n",
    "     f\"Best condition from Exp 07/10 (d=+0.438 on MS MARCO). Phrase: '{STATIC_FACTUAL_PHRASE}'\"),\n",
    "    (\"3. random_trunc\",\n",
    "     \"[BOS][random_tokens\\\\n][doc] \u2192 truncate + RoPE\",\n",
    "     \"Structural control \u2014 random vocabulary tokens. Isolates non-semantic value contamination.\"),\n",
    "    (\"4. llm_kw_trunc\",\n",
    "     \"[BOS][llm_kw\\\\n][doc] \u2192 truncate + RoPE\",\n",
    "     \"LLM-generated keyword query from first 500 words of doc. Tests doc-specific surrogates.\"),\n",
    "    (\"5. oracle_trunc\",\n",
    "     \"[BOS][oracle_query\\\\n][doc] \u2192 truncate + RoPE\",\n",
    "     \"Oracle (actual NQ query) as prefix. Upper bound for query-specific priming.\"),\n",
    "]\n",
    "\n",
    "for name, pattern, detail in conditions_explained:\n",
    "    print(f\"\\n### {name} ###\")\n",
    "    print(f\"  Cache: {pattern}\")\n",
    "    print(f\"  Detail: {detail}\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"KEY QUESTION: How does each condition's d vs bare change across length bins?\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "# Show token counts for different length docs\n",
    "print(f\"\\nExpected token counts per bin (approx):\")\n",
    "for bin_name, bin_min, bin_max in LENGTH_BINS:\n",
    "    mid_words = (bin_min + bin_max) // 2\n",
    "    approx_tokens = int(mid_words * 1.5)  # rough word-to-token ratio\n",
    "    print(f\"  {bin_name} ({bin_min}-{bin_max}w): ~{approx_tokens} tokens per doc\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 6: Generate LLM keyword surrogates\n",
    "print(\"=\" * 70)\n",
    "print(\"PHASE 1: LLM KEYWORD GENERATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "surrogates_path = SURROGATES_DIR / \"keyword_surrogates.json\"\n",
    "\n",
    "if surrogates_path.exists():\n",
    "    with open(surrogates_path, 'r') as f:\n",
    "        surrogates_data = json.load(f)\n",
    "    keyword_surrogates = surrogates_data['surrogates']\n",
    "    print(f\"Loaded {len(keyword_surrogates)} keyword surrogates from cache\")\n",
    "else:\n",
    "    keyword_surrogates = []\n",
    "\n",
    "start_idx_gen = len(keyword_surrogates)\n",
    "if start_idx_gen < N:\n",
    "    print(f\"Generating keyword surrogates for samples {start_idx_gen} to {N-1}...\")\n",
    "    print(f\"(Using first {LLM_KW_MAX_DOC_WORDS} words of each doc for generation)\")\n",
    "    t_start = time.time()\n",
    "\n",
    "    for idx in tqdm(range(start_idx_gen, N), initial=start_idx_gen, total=N,\n",
    "                     desc=\"Keyword surrogates\"):\n",
    "        passage = samples[idx]['passage']\n",
    "        # Truncate to first LLM_KW_MAX_DOC_WORDS for generation efficiency\n",
    "        words = passage.split()\n",
    "        if len(words) > LLM_KW_MAX_DOC_WORDS:\n",
    "            passage_for_gen = ' '.join(words[:LLM_KW_MAX_DOC_WORDS])\n",
    "        else:\n",
    "            passage_for_gen = passage\n",
    "\n",
    "        try:\n",
    "            kw = generate_surrogate_with_template(\n",
    "                passage_for_gen, LLM_KW_PROMPT, model, tokenizer, config)\n",
    "        except Exception as e:\n",
    "            print(f\"  WARNING: Generation failed for sample {idx}: {e}\")\n",
    "            kw = \"\"\n",
    "        keyword_surrogates.append(kw)\n",
    "\n",
    "        if (idx + 1) % 50 == 0 or idx == N - 1:\n",
    "            with open(surrogates_path, 'w') as f:\n",
    "                json.dump({'surrogates': keyword_surrogates}, f)\n",
    "            elapsed = time.time() - t_start\n",
    "            rate = (idx - start_idx_gen + 1) / elapsed if elapsed > 0 else 0\n",
    "            remaining = (N - idx - 1) / rate if rate > 0 else 0\n",
    "            tqdm.write(f\"  Saved {idx+1}/{N} | {rate:.2f} s/s | ETA: {remaining/60:.1f} min\")\n",
    "\n",
    "    with open(surrogates_path, 'w') as f:\n",
    "        json.dump({'surrogates': keyword_surrogates}, f)\n",
    "    print(f\"Keyword surrogates complete: {len(keyword_surrogates)} samples\")\n",
    "else:\n",
    "    print(f\"All keyword surrogates already cached ({len(keyword_surrogates)} samples)\")\n",
    "\n",
    "n_empty = sum(1 for s in keyword_surrogates if not s.strip())\n",
    "print(f\"Empty surrogates: {n_empty}/{N}\")\n",
    "if keyword_surrogates:\n",
    "    print(f\"Example: '{keyword_surrogates[0]}'\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 7: Main eval loop \u2014 5 conditions \u00d7 N samples\n",
    "print(\"=\" * 70)\n",
    "print(f\"PHASE 2: MAIN EVALUATION (5 conditions \u00d7 {N} samples)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "results = []\n",
    "start_idx = 0\n",
    "\n",
    "if CHECKPOINT_PATH.exists():\n",
    "    with open(CHECKPOINT_PATH, 'r') as f:\n",
    "        ckpt = json.load(f)\n",
    "    ckpt_queries = ckpt.get('sample_queries', [])\n",
    "    current_queries = [s['query'] for s in samples]\n",
    "    if ckpt_queries == current_queries:\n",
    "        results = ckpt['results']\n",
    "        start_idx = len(results)\n",
    "        print(f\"Resuming from checkpoint: {start_idx}/{N}\")\n",
    "    else:\n",
    "        print(\"Checkpoint sample mismatch. Starting fresh.\")\n",
    "else:\n",
    "    print(\"No checkpoint found. Starting fresh.\")\n",
    "\n",
    "print(f\"Evaluating samples {start_idx} to {N-1}\")\n",
    "print(f\"Conditions: {len(CONDITION_NAMES)}\")\n",
    "\n",
    "t_start = time.time()\n",
    "\n",
    "for idx in tqdm(range(start_idx, N), initial=start_idx, total=N, desc=\"Evaluating\"):\n",
    "    sample = samples[idx]\n",
    "    passage = sample['passage']\n",
    "    query = sample['query']\n",
    "    answer = sample['answer']\n",
    "    word_count = sample['word_count']\n",
    "    length_bin = sample['length_bin']\n",
    "\n",
    "    query_prompt = QUERY_TEMPLATE.format(query=query)\n",
    "    answer_text = ANSWER_TEMPLATE.format(answer=answer)\n",
    "\n",
    "    # --- Matched tokenization ---\n",
    "    oracle_prefix = SURROGATE_PREFIX_TEMPLATE.format(surrogate=query)\n",
    "    document_text = DOCUMENT_TEMPLATE.format(document=passage)\n",
    "    full_oracle_text = oracle_prefix + document_text\n",
    "\n",
    "    full_oracle_enc = tokenizer(full_oracle_text, return_tensors=\"pt\",\n",
    "                                add_special_tokens=True, padding=False, truncation=False)\n",
    "    full_oracle_ids = full_oracle_enc['input_ids'].to(config.device)\n",
    "\n",
    "    oracle_prefix_enc = tokenizer(oracle_prefix, return_tensors=\"pt\",\n",
    "                                  add_special_tokens=True, padding=False, truncation=False)\n",
    "    oracle_prefix_len = oracle_prefix_enc['input_ids'].shape[1]\n",
    "\n",
    "    bos_id = full_oracle_ids[:, :1]\n",
    "    doc_ids = full_oracle_ids[:, oracle_prefix_len:]\n",
    "    doc_len = doc_ids.shape[1]\n",
    "\n",
    "    # ===== 1. BARE =====\n",
    "    bare_ids = torch.cat([bos_id, doc_ids], dim=1)\n",
    "    with torch.no_grad():\n",
    "        bare_out = model(input_ids=bare_ids, attention_mask=torch.ones_like(bare_ids),\n",
    "                         use_cache=True, return_dict=True)\n",
    "    bare_cache = bare_out.past_key_values\n",
    "    del bare_out\n",
    "\n",
    "    nll_bare = score_answer_with_cache(\n",
    "        deepcopy_cache(bare_cache), bare_ids.shape[1],\n",
    "        query_prompt, answer_text, model, tokenizer, config)\n",
    "\n",
    "    # ===== 2. static_fact_trunc =====\n",
    "    trunc_cache, _ = build_primed_and_truncated(\n",
    "        STATIC_FACTUAL_PHRASE, bos_id, doc_ids, doc_len, model, tokenizer, config)\n",
    "    nll_static = score_answer_with_cache(\n",
    "        deepcopy_cache(trunc_cache), 1 + doc_len,\n",
    "        query_prompt, answer_text, model, tokenizer, config)\n",
    "    del trunc_cache\n",
    "\n",
    "    # ===== 3. random_trunc =====\n",
    "    llm_kw_text = keyword_surrogates[idx] if idx < len(keyword_surrogates) else \"\"\n",
    "    n_random_tokens = max(5, len(tokenizer.encode(\n",
    "        llm_kw_text if llm_kw_text else STATIC_FACTUAL_PHRASE,\n",
    "        add_special_tokens=False)))\n",
    "    random_ids = torch.randint(100, tokenizer.vocab_size - 100, (n_random_tokens,), device='cpu')\n",
    "    random_text = tokenizer.decode(random_ids, skip_special_tokens=True)\n",
    "\n",
    "    trunc_cache, _ = build_primed_and_truncated(\n",
    "        random_text, bos_id, doc_ids, doc_len, model, tokenizer, config)\n",
    "    nll_random = score_answer_with_cache(\n",
    "        deepcopy_cache(trunc_cache), 1 + doc_len,\n",
    "        query_prompt, answer_text, model, tokenizer, config)\n",
    "    del trunc_cache\n",
    "\n",
    "    # ===== 4. llm_kw_trunc =====\n",
    "    if llm_kw_text.strip():\n",
    "        trunc_cache, _ = build_primed_and_truncated(\n",
    "            llm_kw_text, bos_id, doc_ids, doc_len, model, tokenizer, config)\n",
    "        nll_llm_kw = score_answer_with_cache(\n",
    "            deepcopy_cache(trunc_cache), 1 + doc_len,\n",
    "            query_prompt, answer_text, model, tokenizer, config)\n",
    "        del trunc_cache\n",
    "    else:\n",
    "        nll_llm_kw = 0.0  # empty surrogate \u2192 exclude from analysis\n",
    "\n",
    "    # ===== 5. oracle_trunc =====\n",
    "    trunc_cache, _ = build_primed_and_truncated(\n",
    "        query, bos_id, doc_ids, doc_len, model, tokenizer, config)\n",
    "    nll_oracle = score_answer_with_cache(\n",
    "        deepcopy_cache(trunc_cache), 1 + doc_len,\n",
    "        query_prompt, answer_text, model, tokenizer, config)\n",
    "    del trunc_cache\n",
    "\n",
    "    del bare_cache, bare_ids\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # --- Store result ---\n",
    "    result = {\n",
    "        'idx': idx,\n",
    "        'doc_len_tokens': doc_len,\n",
    "        'word_count': word_count,\n",
    "        'length_bin': length_bin,\n",
    "        'bare': nll_bare,\n",
    "        'static_fact_trunc': nll_static,\n",
    "        'random_trunc': nll_random,\n",
    "        'llm_kw_trunc': nll_llm_kw,\n",
    "        'oracle_trunc': nll_oracle,\n",
    "    }\n",
    "    results.append(result)\n",
    "\n",
    "    if (idx + 1) % CHECKPOINT_EVERY == 0 or idx == N - 1:\n",
    "        ckpt_data = {\n",
    "            'results': results,\n",
    "            'sample_queries': [s['query'] for s in samples],\n",
    "            'completed': len(results),\n",
    "            'total': N,\n",
    "            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        }\n",
    "        with open(CHECKPOINT_PATH, 'w') as f:\n",
    "            json.dump(ckpt_data, f)\n",
    "        elapsed = time.time() - t_start\n",
    "        rate = (idx - start_idx + 1) / elapsed if elapsed > 0 else 0\n",
    "        remaining = (N - idx - 1) / rate if rate > 0 else 0\n",
    "        tqdm.write(f\"  Checkpoint {idx+1}/{N} | {rate:.2f} s/s | ETA: {remaining/60:.1f} min\")\n",
    "\n",
    "elapsed_total = time.time() - t_start\n",
    "print(f\"\\nEvaluation complete: {len(results)} samples in {elapsed_total/60:.1f} min\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 8: Analysis \u2014 overall + per length bin\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ANALYSIS \u2014 LONG-DOCUMENT PRIMING\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Extract arrays and filter zero NLLs\n",
    "cond_arrays = {}\n",
    "for cname in CONDITION_NAMES:\n",
    "    cond_arrays[cname] = np.array([r[cname] for r in results])\n",
    "\n",
    "valid = np.ones(len(results), dtype=bool)\n",
    "for cname in CONDITION_NAMES:\n",
    "    valid &= (cond_arrays[cname] != 0)\n",
    "n_valid = int(np.sum(valid))\n",
    "n_excluded = int(np.sum(~valid))\n",
    "print(f\"Total: {len(results)}, Valid: {n_valid}, Excluded: {n_excluded}\")\n",
    "\n",
    "c = {}\n",
    "for cname in CONDITION_NAMES:\n",
    "    c[cname] = cond_arrays[cname][valid]\n",
    "\n",
    "length_bins_arr = np.array([r['length_bin'] for r in results])[valid]\n",
    "word_counts_arr = np.array([r['word_count'] for r in results])[valid]\n",
    "doc_lens_arr = np.array([r['doc_len_tokens'] for r in results])[valid]\n",
    "\n",
    "# ===== OVERALL NLL SUMMARY =====\n",
    "print(f\"\\n{'Condition':<25} {'Mean NLL':>10} {'Std':>10} {'d vs Bare':>10} {'Win%':>7}\")\n",
    "print(\"-\" * 67)\n",
    "for cname in CONDITION_NAMES:\n",
    "    mean_nll = np.mean(c[cname])\n",
    "    std_nll = np.std(c[cname])\n",
    "    if cname == 'bare':\n",
    "        print(f\"{cname:<25} {mean_nll:>10.4f} {std_nll:>10.4f} {'\u2014':>10} {'\u2014':>7}\")\n",
    "    else:\n",
    "        delta = c['bare'] - c[cname]\n",
    "        d = cohens_d(delta)\n",
    "        win = np.mean(delta > 0) * 100\n",
    "        _, p_val = stats.ttest_1samp(delta, 0)\n",
    "        sig = \"***\" if p_val < 0.001 else \"**\" if p_val < BONFERRONI_ALPHA else \"*\" if p_val < 0.05 else \"ns\"\n",
    "        print(f\"{cname:<25} {mean_nll:>10.4f} {std_nll:>10.4f} {d:>+10.3f} {win:>5.1f}% {sig}\")\n",
    "\n",
    "# ===== 5 PRIMARY COMPARISONS =====\n",
    "print(f\"\\n{'='*90}\")\n",
    "print(f\"5 PRIMARY COMPARISONS (Bonferroni alpha = {BONFERRONI_ALPHA:.4f})\")\n",
    "print(f\"{'='*90}\")\n",
    "\n",
    "comparisons = [\n",
    "    ('C1: static_fact vs bare',\n",
    "     c['bare'] - c['static_fact_trunc'],\n",
    "     'Does static_fact help overall?'),\n",
    "    ('C2: random vs bare',\n",
    "     c['bare'] - c['random_trunc'],\n",
    "     'Does ANY prefix help overall?'),\n",
    "    ('C3: llm_kw vs bare',\n",
    "     c['bare'] - c['llm_kw_trunc'],\n",
    "     'Do LLM keywords help overall?'),\n",
    "    ('C4: oracle vs bare',\n",
    "     c['bare'] - c['oracle_trunc'],\n",
    "     'Does the perfect query help?'),\n",
    "    ('C5: static_fact vs random',\n",
    "     c['random_trunc'] - c['static_fact_trunc'],\n",
    "     'Is content better than noise?'),\n",
    "]\n",
    "\n",
    "print(f\"\\n{'Comparison':<30} {'Mean delta':>10} {'d':>8} {'Win%':>7} {'t':>8} {'p':>12} {'Sig':>5}\")\n",
    "print(\"-\" * 85)\n",
    "\n",
    "comparison_results = {}\n",
    "for name, delta, question in comparisons:\n",
    "    d = cohens_d(delta)\n",
    "    win = np.mean(delta > 0) * 100\n",
    "    t_stat, p_val = stats.ttest_1samp(delta, 0)\n",
    "    sig = \"***\" if p_val < 0.001 else \"**\" if p_val < BONFERRONI_ALPHA else \"*\" if p_val < 0.05 else \"ns\"\n",
    "    print(f\"{name:<30} {np.mean(delta):>10.4f} {d:>8.3f} {win:>6.1f}% {t_stat:>8.2f} {p_val:>11.2e} {sig:>5}\")\n",
    "    comparison_results[name] = {\n",
    "        'mean_delta': float(np.mean(delta)),\n",
    "        'cohens_d': float(d),\n",
    "        'win_rate': float(win / 100),\n",
    "        't_stat': float(t_stat),\n",
    "        'p_value': float(p_val),\n",
    "        'bonferroni_significant': bool(p_val < BONFERRONI_ALPHA),\n",
    "        'question': question,\n",
    "    }\n",
    "\n",
    "# ===== PER LENGTH BIN ANALYSIS (KEY RESULT) =====\n",
    "print(f\"\\n{'='*90}\")\n",
    "print(\"PER LENGTH BIN ANALYSIS \u2014 Does priming effect change with document length?\")\n",
    "print(f\"{'='*90}\")\n",
    "\n",
    "bin_names_ordered = [name for name, _, _ in LENGTH_BINS]\n",
    "per_bin_results = {}\n",
    "\n",
    "for cname in CONDITION_NAMES:\n",
    "    if cname == 'bare':\n",
    "        continue\n",
    "    print(f\"\\n  {cname}:\")\n",
    "    bin_ds = []\n",
    "    bin_wins = []\n",
    "    bin_ns = []\n",
    "    for bin_name in bin_names_ordered:\n",
    "        mask = length_bins_arr == bin_name\n",
    "        n_bin = int(np.sum(mask))\n",
    "        if n_bin < 10:\n",
    "            print(f\"    {bin_name}: n={n_bin} (too few)\")\n",
    "            bin_ds.append(None)\n",
    "            bin_wins.append(None)\n",
    "            bin_ns.append(n_bin)\n",
    "            continue\n",
    "        delta = c['bare'][mask] - c[cname][mask]\n",
    "        d = cohens_d(delta)\n",
    "        win = np.mean(delta > 0) * 100\n",
    "        _, p_val = stats.ttest_1samp(delta, 0)\n",
    "        sig = \"***\" if p_val < 0.001 else \"**\" if p_val < BONFERRONI_ALPHA else \"*\" if p_val < 0.05 else \"ns\"\n",
    "        print(f\"    {bin_name}: n={n_bin}, d={d:+.3f}, win={win:.1f}%, p={p_val:.2e} {sig}\")\n",
    "        bin_ds.append(float(d))\n",
    "        bin_wins.append(float(win))\n",
    "        bin_ns.append(n_bin)\n",
    "\n",
    "    per_bin_results[cname] = {\n",
    "        'bin_names': bin_names_ordered,\n",
    "        'bin_ds': bin_ds,\n",
    "        'bin_wins': bin_wins,\n",
    "        'bin_ns': bin_ns,\n",
    "    }\n",
    "\n",
    "# ===== LENGTH INTERACTION: does d decrease with length? =====\n",
    "print(f\"\\n{'='*90}\")\n",
    "print(\"LENGTH INTERACTION \u2014 Correlation between document length and priming effect\")\n",
    "print(f\"{'='*90}\")\n",
    "\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "\n",
    "interaction_results = {}\n",
    "for cname in CONDITION_NAMES:\n",
    "    if cname == 'bare':\n",
    "        continue\n",
    "    delta = c['bare'] - c[cname]\n",
    "    r_spear, p_spear = spearmanr(word_counts_arr, delta)\n",
    "    r_pears, p_pears = pearsonr(word_counts_arr, delta)\n",
    "    print(f\"  {cname}: Spearman r={r_spear:+.3f} (p={p_spear:.3f}), Pearson r={r_pears:+.3f} (p={p_pears:.3f})\")\n",
    "    interaction_results[cname] = {\n",
    "        'spearman_r': float(r_spear), 'spearman_p': float(p_spear),\n",
    "        'pearson_r': float(r_pears), 'pearson_p': float(p_pears),\n",
    "    }\n",
    "\n",
    "# ===== HARDNESS QUINTILE (WITHIN EACH BIN) =====\n",
    "print(f\"\\n{'='*90}\")\n",
    "print(\"HARDNESS QUINTILE WITHIN EACH LENGTH BIN\")\n",
    "print(f\"{'='*90}\")\n",
    "\n",
    "hardness_x_length = {}\n",
    "for bin_name in bin_names_ordered:\n",
    "    mask_bin = length_bins_arr == bin_name\n",
    "    n_bin = int(np.sum(mask_bin))\n",
    "    if n_bin < 30:\n",
    "        continue\n",
    "    bare_bin = c['bare'][mask_bin]\n",
    "    median_nll = np.median(bare_bin)\n",
    "    hard_mask_within = bare_bin >= median_nll\n",
    "    easy_mask_within = bare_bin < median_nll\n",
    "\n",
    "    print(f\"\\n  {bin_name} (n={n_bin}, median bare NLL={median_nll:.3f}):\")\n",
    "    bin_results = {}\n",
    "    for cname in ['static_fact_trunc', 'random_trunc', 'oracle_trunc']:\n",
    "        cond_bin = c[cname][mask_bin]\n",
    "        delta_easy = bare_bin[easy_mask_within] - cond_bin[easy_mask_within]\n",
    "        delta_hard = bare_bin[hard_mask_within] - cond_bin[hard_mask_within]\n",
    "        d_easy = cohens_d(delta_easy) if len(delta_easy) > 5 else float('nan')\n",
    "        d_hard = cohens_d(delta_hard) if len(delta_hard) > 5 else float('nan')\n",
    "        print(f\"    {cname}: easy d={d_easy:+.3f}, hard d={d_hard:+.3f}\")\n",
    "        bin_results[cname] = {'easy_d': float(d_easy), 'hard_d': float(d_hard)}\n",
    "    hardness_x_length[bin_name] = bin_results"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 9: Plots (2x2 grid)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "\n",
    "colors = {\n",
    "    'static_fact_trunc': '#d62728',\n",
    "    'random_trunc': '#7f7f7f',\n",
    "    'llm_kw_trunc': '#2ca02c',\n",
    "    'oracle_trunc': '#1f77b4',\n",
    "}\n",
    "\n",
    "# --- Plot 1: Per-bin Cohen's d for each condition ---\n",
    "ax = axes[0, 0]\n",
    "x = np.arange(len(bin_names_ordered))\n",
    "width = 0.18\n",
    "for i, cname in enumerate(['static_fact_trunc', 'random_trunc', 'llm_kw_trunc', 'oracle_trunc']):\n",
    "    ds = per_bin_results[cname]['bin_ds']\n",
    "    ds_clean = [d if d is not None else 0 for d in ds]\n",
    "    offset = (i - 1.5) * width\n",
    "    bars = ax.bar(x + offset, ds_clean, width, label=cname.replace('_trunc', ''),\n",
    "                  color=colors[cname], edgecolor='black', linewidth=0.5, alpha=0.85)\n",
    "    for j, (d_val, bar) in enumerate(zip(ds, bars)):\n",
    "        if d_val is not None:\n",
    "            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005,\n",
    "                    f\"{d_val:+.2f}\", ha='center', va='bottom', fontsize=6)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(bin_names_ordered)\n",
    "ax.axhline(y=0, color='gray', linestyle='--')\n",
    "ax.set_ylabel(\"Cohen's d vs Bare\")\n",
    "ax.set_xlabel(\"Document Length Bin\")\n",
    "ax.set_title(\"Priming Effect by Document Length\")\n",
    "ax.legend(fontsize=8)\n",
    "\n",
    "# --- Plot 2: Scatter \u2014 word count vs per-sample NLL reduction ---\n",
    "ax = axes[0, 1]\n",
    "for cname in ['static_fact_trunc', 'oracle_trunc']:\n",
    "    delta = c['bare'] - c[cname]\n",
    "    ax.scatter(word_counts_arr, delta, alpha=0.15, s=8, color=colors[cname], label=cname.replace('_trunc', ''))\n",
    "    # Trend line (binned means)\n",
    "    n_trend_bins = 20\n",
    "    edges = np.linspace(word_counts_arr.min(), word_counts_arr.max(), n_trend_bins + 1)\n",
    "    for k in range(n_trend_bins):\n",
    "        mask_k = (word_counts_arr >= edges[k]) & (word_counts_arr < edges[k+1])\n",
    "        if np.sum(mask_k) > 5:\n",
    "            ax.scatter((edges[k] + edges[k+1])/2, np.mean(delta[mask_k]),\n",
    "                      s=40, color=colors[cname], edgecolor='black', linewidth=0.5, zorder=5)\n",
    "ax.axhline(y=0, color='gray', linestyle='--')\n",
    "ax.set_xlabel(\"Document Word Count\")\n",
    "ax.set_ylabel(\"NLL Reduction (bare - primed)\")\n",
    "ax.set_title(\"NLL Reduction vs Document Length\")\n",
    "ax.legend(fontsize=8)\n",
    "\n",
    "# --- Plot 3: Overall bar chart (all conditions) ---\n",
    "ax = axes[1, 0]\n",
    "conds_sorted = sorted(\n",
    "    [(cn, cohens_d(c['bare'] - c[cn])) for cn in CONDITION_NAMES if cn != 'bare'],\n",
    "    key=lambda x: x[1], reverse=True\n",
    ")\n",
    "names_sorted = [x[0] for x in conds_sorted]\n",
    "ds_sorted = [x[1] for x in conds_sorted]\n",
    "bar_colors = [colors.get(cn, 'gray') for cn in names_sorted]\n",
    "bars = ax.barh(range(len(names_sorted)), ds_sorted, color=bar_colors, edgecolor='black', linewidth=0.5)\n",
    "ax.set_yticks(range(len(names_sorted)))\n",
    "ax.set_yticklabels([n.replace('_trunc', '') for n in names_sorted], fontsize=9)\n",
    "for i, (name, d_val) in enumerate(conds_sorted):\n",
    "    ax.text(d_val + 0.005, i, f\"d={d_val:+.3f}\", va='center', fontsize=8)\n",
    "ax.axvline(x=0, color='gray', linestyle='--')\n",
    "ax.set_xlabel(\"Cohen's d vs Bare\")\n",
    "ax.set_title(\"Overall Priming Effect (All Length Bins)\")\n",
    "ax.invert_yaxis()\n",
    "\n",
    "# Reference line from MS MARCO\n",
    "ax.axvline(x=0.438, color='red', linestyle=':', alpha=0.5, label='Exp10 MS MARCO static_fact')\n",
    "ax.legend(fontsize=7)\n",
    "\n",
    "# --- Plot 4: Hardness \u00d7 Length heatmap for static_fact_trunc ---\n",
    "ax = axes[1, 1]\n",
    "hm_data = []\n",
    "hm_labels_y = []\n",
    "for bin_name in bin_names_ordered:\n",
    "    if bin_name in hardness_x_length:\n",
    "        hm_data.append([\n",
    "            hardness_x_length[bin_name]['static_fact_trunc']['easy_d'],\n",
    "            hardness_x_length[bin_name]['static_fact_trunc']['hard_d'],\n",
    "        ])\n",
    "        hm_labels_y.append(bin_name)\n",
    "if hm_data:\n",
    "    hm_data = np.array(hm_data)\n",
    "    im = ax.imshow(hm_data, cmap='RdBu_r', vmin=-0.5, vmax=1.0, aspect='auto')\n",
    "    ax.set_xticks([0, 1])\n",
    "    ax.set_xticklabels(['Easy (below median)', 'Hard (above median)'])\n",
    "    ax.set_yticks(range(len(hm_labels_y)))\n",
    "    ax.set_yticklabels(hm_labels_y)\n",
    "    for i in range(len(hm_labels_y)):\n",
    "        for j in range(2):\n",
    "            ax.text(j, i, f\"{hm_data[i,j]:+.2f}\", ha='center', va='center', fontsize=10, fontweight='bold')\n",
    "    plt.colorbar(im, ax=ax, label=\"Cohen's d vs bare\")\n",
    "    ax.set_title(\"static_fact_trunc: Hardness \u00d7 Length\")\n",
    "\n",
    "plt.suptitle('Exp 11: Long-Document Priming', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'analysis_plots.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"Plots saved to {RESULTS_DIR / 'analysis_plots.png'}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 10: Save comprehensive results JSON\n",
    "\n",
    "nll_summary = {}\n",
    "for cname in CONDITION_NAMES:\n",
    "    nll_summary[cname] = {\n",
    "        'mean': float(np.mean(c[cname])),\n",
    "        'std': float(np.std(c[cname])),\n",
    "        'cohens_d_vs_bare': float(cohens_d(c['bare'] - c[cname])) if cname != 'bare' else 0.0,\n",
    "    }\n",
    "\n",
    "final = {\n",
    "    'experiment': 'exp11_long_document_priming',\n",
    "    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'config': {\n",
    "        'model_name': config.model_name,\n",
    "        'seed': SEED,\n",
    "        'n_eval': N,\n",
    "        'n_valid': n_valid,\n",
    "        'n_excluded': n_excluded,\n",
    "        'n_conditions': len(CONDITION_NAMES),\n",
    "        'n_comparisons': N_COMPARISONS,\n",
    "        'bonferroni_alpha': BONFERRONI_ALPHA,\n",
    "        'dataset': 'google-research-datasets/natural_questions',\n",
    "        'dataset_split': 'validation',\n",
    "        'length_bins': LENGTH_BINS,\n",
    "        'max_doc_words': MAX_DOC_WORDS,\n",
    "    },\n",
    "    'condition_names': CONDITION_NAMES,\n",
    "    'nll_summary': nll_summary,\n",
    "    'primary_comparisons': comparison_results,\n",
    "    'per_bin_results': per_bin_results,\n",
    "    'interaction_results': interaction_results,\n",
    "    'hardness_x_length': hardness_x_length,\n",
    "    'per_sample_results': results,\n",
    "}\n",
    "\n",
    "with open(FINAL_RESULTS_PATH, 'w') as f:\n",
    "    json.dump(final, f, indent=2)\n",
    "\n",
    "print(f\"Results saved to {FINAL_RESULTS_PATH}\")\n",
    "print(f\"File size: {FINAL_RESULTS_PATH.stat().st_size / 1024:.1f} KB\")\n",
    "print(\"\\nDone!\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 11: GPU cleanup\n",
    "import gc\n",
    "\n",
    "print(\"Cleaning up GPU memory...\")\n",
    "mem_before = torch.cuda.memory_allocated() / 1e9\n",
    "\n",
    "del model\n",
    "del tokenizer\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "mem_after = torch.cuda.memory_allocated() / 1e9\n",
    "print(f\"GPU memory: {mem_before:.2f} GB -> {mem_after:.2f} GB\")\n",
    "print(\"Cleanup complete.\")"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}