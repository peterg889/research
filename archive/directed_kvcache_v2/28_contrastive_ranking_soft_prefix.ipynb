{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Exp 28: Contrastive Ranking Soft Prefix\n",
    "\n",
    "## Motivation\n",
    "\n",
    "Exps 22-23 proved that NLL-trained priming cannot improve document ranking: value contamination\n",
    "from a document-independent prefix lowers NLL equally for relevant and irrelevant passages.\n",
    "But those experiments used either discrete prefixes (static_fact) or NLL-optimized soft prefixes.\n",
    "**What if we train the soft prefix with a ranking loss instead?**\n",
    "\n",
    "## Hypothesis\n",
    "\n",
    "Training the soft prefix with a hinge-based ranking loss can create differential value\n",
    "contamination — reducing NLL more for relevant passages than irrelevant ones. The prefix\n",
    "values might learn to \"amplify\" answer-predictive tokens more than other tokens.\n",
    "\n",
    "## Core Mechanism\n",
    "\n",
    "MS MARCO provides ~8-10 candidate passages per query with relevance labels. For each step:\n",
    "1. Pick 1 relevant + 1 irrelevant passage for the same query\n",
    "2. Score both through hybrid cache (same soft prefix)\n",
    "3. Hinge loss: `max(0, margin + NLL_relevant - NLL_irrelevant)`\n",
    "4. Gradient pushes prefix to make relevant passages predict the answer better\n",
    "\n",
    "## Why This Might Fail\n",
    "\n",
    "The soft prefix is the same for all documents. It produces identical value contamination\n",
    "regardless of document content. The contrastive gradient tells it \"help relevant passages more\"\n",
    "but it has no mechanism to distinguish relevant from irrelevant at cache-build time (it doesn't\n",
    "see the query). The only hope: the prefix values create an \"amplifier\" that happens to boost\n",
    "answer-predictive tokens more than other tokens. Theoretically possible, practically unlikely.\n",
    "\n",
    "## Success Criteria\n",
    "\n",
    "- **Primary**: Contrastive prefix AUC > 0.835 (bare=0.828) or PMI AUC > 0.845 (bare PMI=0.841)\n",
    "- **Secondary**: Still helps average NLL (d > 0 vs bare)\n",
    "- **Failure is informative**: Confirms value contamination from document-independent prefix\n",
    "  fundamentally cannot create ranking signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": "# Cell 1: Setup\nimport os\nos.umask(0o000)\n\nimport sys\nimport json\nimport time\nimport csv\nimport numpy as np\nimport torch\nimport gc\nfrom pathlib import Path\nfrom scipy import stats\n\nSEED = 42\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed_all(SEED)\nnp.random.seed(SEED)\n\nRESULTS_DIR = Path(\"results/exp28\")\nRESULTS_DIR.mkdir(parents=True, exist_ok=True)\n\nEXP25_SOFT_FACT = Path(\"results/exp25/soft_prefix_fact.pt\")\n\nCHECKPOINT_TRAIN_WARM_PATH = RESULTS_DIR / \"checkpoint_train_warm.json\"\nCHECKPOINT_TRAIN_COLD_PATH = RESULTS_DIR / \"checkpoint_train_cold.json\"\nCHECKPOINT_EVAL_PATH = RESULTS_DIR / \"checkpoint_eval.json\"\nFINAL_RESULTS_PATH = RESULTS_DIR / \"results.json\"\nCSV_EVAL_PATH = RESULTS_DIR / \"passage_scores.csv\"\nSOFT_WARM_PATH = RESULTS_DIR / \"soft_prefix_contrastive_warm.pt\"\nSOFT_COLD_PATH = RESULTS_DIR / \"soft_prefix_contrastive_cold.pt\"\n\nprint(f\"SEED: {SEED}\")\nprint(f\"Results directory: {RESULTS_DIR}\")\nprint(f\"Exp 25 soft_prefix_fact: {EXP25_SOFT_FACT} (exists: {EXP25_SOFT_FACT.exists()})\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Load Gemma 3 4B\n",
    "\n",
    "sys.path.insert(0, \".\")\n",
    "\n",
    "from lib.config import ExperimentConfig\n",
    "from lib.model_utils import load_model\n",
    "\n",
    "MODEL_NAME = \"google/gemma-3-4b-it\"\n",
    "\n",
    "exp_config = ExperimentConfig(\n",
    "    model_name=MODEL_NAME,\n",
    "    model_type=\"gemma3\",\n",
    "    compute_dtype=\"auto\",\n",
    "    use_4bit=True,\n",
    "    num_samples=500,\n",
    "    seed=SEED,\n",
    ")\n",
    "\n",
    "print(f\"Loading {MODEL_NAME} (4-bit, bfloat16)...\")\n",
    "model, tokenizer = load_model(exp_config)\n",
    "\n",
    "from lib.kv_cache import (\n",
    "    _get_text_config, _get_head_dim,\n",
    "    _get_cache_keys, _get_cache_values,\n",
    "    _set_cache_keys, _set_cache_values,\n",
    "    _ensure_dynamic_cache,\n",
    "    extract_and_truncate_cache_with_bos,\n",
    "    correct_rope_positions_with_bos,\n",
    "    score_answer_with_cache,\n",
    "    deepcopy_cache,\n",
    "    replace_values_at_layers,\n",
    ")\n",
    "\n",
    "text_config = _get_text_config(model.config)\n",
    "NUM_LAYERS = text_config.num_hidden_layers\n",
    "HIDDEN_SIZE = text_config.hidden_size\n",
    "HEAD_DIM = _get_head_dim(model.config)\n",
    "\n",
    "print(f\"Model loaded.\")\n",
    "print(f\"  Layers: {NUM_LAYERS}, hidden: {HIDDEN_SIZE}, head_dim: {HEAD_DIM}\")\n",
    "print(f\"  KV heads: {text_config.num_key_value_heads}\")\n",
    "print(f\"  BOS token: {tokenizer.bos_token_id}\")\n",
    "\n",
    "# Verify cache dtype\n",
    "sample_ids = tokenizer(\"test\", return_tensors=\"pt\")['input_ids'].to(exp_config.device)\n",
    "with torch.no_grad():\n",
    "    out = model(sample_ids, use_cache=True)\n",
    "    cache_check = _ensure_dynamic_cache(out.past_key_values)\n",
    "    k0 = _get_cache_keys(cache_check, 0)\n",
    "    print(f\"  Cache dtype: {k0.dtype}\")\n",
    "del out, sample_ids, cache_check\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Constants\n",
    "\n",
    "from lib.analysis import cohens_d\n",
    "from lib.data import count_words\n",
    "from lib.surrogate import STATIC_SURROGATE_QUERIES\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Templates (same as Exp 25)\n",
    "STATIC_FACT = STATIC_SURROGATE_QUERIES['static_factual']['query']\n",
    "SURROGATE_PREFIX_TEMPLATE = \"{surrogate}\\n\"\n",
    "DOCUMENT_TEMPLATE = \"{document}\"\n",
    "QUERY_TEMPLATE = \"\\nQuery: {query}\\nAnswer:\"\n",
    "ANSWER_TEMPLATE = \" {answer}\"\n",
    "\n",
    "# Architecture\n",
    "CUTOFF = 16  # layers 0-15\n",
    "MAX_PASSAGE_WORDS = 300\n",
    "\n",
    "# Training hyperparameters\n",
    "MARGIN = 0.1        # Hinge loss margin\n",
    "LR = 0.05           # Lower than Exp 25 (0.1) for warm-start stability\n",
    "N_EPOCHS = 5\n",
    "GRAD_ACCUM = 4\n",
    "WARMUP_STEPS = 30\n",
    "N_TRAIN = 500       # queries (each with ~8 passages)\n",
    "N_EVAL = 200        # queries for ranking eval\n",
    "CHECKPOINT_EVERY = 50\n",
    "\n",
    "# Tokenize static fact prefix for matched tokenization reference\n",
    "sf_str = SURROGATE_PREFIX_TEMPLATE.format(surrogate=STATIC_FACT)\n",
    "sf_ids = tokenizer(sf_str, return_tensors=\"pt\",\n",
    "                    add_special_tokens=False)['input_ids'].to(exp_config.device)\n",
    "PREFIX_LEN = sf_ids.shape[1]\n",
    "\n",
    "# Get embedding layer\n",
    "embed_fn = model.get_input_embeddings()\n",
    "\n",
    "# Reference values from Exp 22\n",
    "EXP22_REF = {\n",
    "    'raw_bare_auc': 0.828,\n",
    "    'raw_primed_auc': 0.829,\n",
    "    'pmi_bare_auc': 0.841,\n",
    "    'pmi_primed_auc': 0.832,\n",
    "    'raw_bare_mrr': 0.860,\n",
    "}\n",
    "\n",
    "print(\"Config:\")\n",
    "print(f\"  MARGIN={MARGIN}, LR={LR}, N_EPOCHS={N_EPOCHS}, GRAD_ACCUM={GRAD_ACCUM}\")\n",
    "print(f\"  N_TRAIN={N_TRAIN} queries, N_EVAL={N_EVAL} queries\")\n",
    "print(f\"  CUTOFF={CUTOFF} (layers 0-{CUTOFF-1})\")\n",
    "print(f\"  PREFIX_LEN={PREFIX_LEN} tokens\")\n",
    "print(f\"  Trainable params: {PREFIX_LEN * HIDDEN_SIZE:,}\")\n",
    "print(f\"\\nReference (Exp 22):\")\n",
    "for k, v in EXP22_REF.items():\n",
    "    print(f\"  {k}: {v:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Load MS MARCO train — multi-passage format\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"LOADING MS MARCO v1.1 TRAIN — MULTI-PASSAGE FORMAT\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "dataset = load_dataset(\"microsoft/ms_marco\", \"v1.1\", split=\"train\")\n",
    "print(f\"Total items: {len(dataset)}\")\n",
    "\n",
    "train_queries = []\n",
    "np.random.seed(SEED)\n",
    "\n",
    "for item in tqdm(dataset, desc=\"Filtering train\"):\n",
    "    passages_info = item.get('passages', {})\n",
    "    passage_texts = passages_info.get('passage_text', [])\n",
    "    is_selected = passages_info.get('is_selected', [])\n",
    "    query = item.get('query', '')\n",
    "    answers = item.get('answers', [])\n",
    "    well_formed = item.get('wellFormedAnswers', [])\n",
    "\n",
    "    if not passage_texts or not query:\n",
    "        continue\n",
    "    if not is_selected or sum(is_selected) == 0:\n",
    "        continue\n",
    "\n",
    "    # Check word counts\n",
    "    word_counts = [count_words(p) for p in passage_texts]\n",
    "    if any(wc > MAX_PASSAGE_WORDS for wc in word_counts):\n",
    "        continue\n",
    "\n",
    "    # Require answer\n",
    "    answer = None\n",
    "    if well_formed and len(well_formed) > 0 and well_formed[0] != '[]':\n",
    "        answer = well_formed[0]\n",
    "    elif answers and len(answers) > 0 and answers[0] != 'No Answer Present.':\n",
    "        answer = answers[0]\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "    # Need at least 1 relevant and 1 irrelevant\n",
    "    n_rel = sum(1 for s in is_selected if s == 1)\n",
    "    n_irr = len(is_selected) - n_rel\n",
    "    if n_rel == 0 or n_irr == 0:\n",
    "        continue\n",
    "\n",
    "    passage_list = []\n",
    "    for i, (ptext, sel) in enumerate(zip(passage_texts, is_selected)):\n",
    "        passage_list.append({\n",
    "            'passage': ptext,\n",
    "            'is_relevant': bool(sel == 1),\n",
    "            'word_count': word_counts[i],\n",
    "            'passage_idx': i,\n",
    "        })\n",
    "\n",
    "    train_queries.append({\n",
    "        'query': query,\n",
    "        'answer': answer,\n",
    "        'passages': passage_list,\n",
    "        'n_passages': len(passage_list),\n",
    "        'n_relevant': n_rel,\n",
    "    })\n",
    "\n",
    "    if len(train_queries) >= N_TRAIN * 3:\n",
    "        break\n",
    "\n",
    "np.random.shuffle(train_queries)\n",
    "train_queries = train_queries[:N_TRAIN]\n",
    "\n",
    "n_passages_train = [q['n_passages'] for q in train_queries]\n",
    "total_train_passages = sum(n_passages_train)\n",
    "total_train_rel = sum(q['n_relevant'] for q in train_queries)\n",
    "\n",
    "print(f\"\\nSelected {len(train_queries)} training queries ({total_train_passages} passages)\")\n",
    "print(f\"Passages per query: mean={np.mean(n_passages_train):.1f}, \"\n",
    "      f\"min={min(n_passages_train)}, max={max(n_passages_train)}\")\n",
    "print(f\"Relevant: {total_train_rel} ({100*total_train_rel/total_train_passages:.1f}%)\")\n",
    "\n",
    "del dataset\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Explain experimental conditions\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"EXPERIMENTAL CONDITIONS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\"\"\n",
    "### Training: Contrastive (Hinge) Loss ###\n",
    "\n",
    "For each training query:\n",
    "  1. Sample 1 relevant passage (R) and 1 irrelevant passage (I)\n",
    "  2. Build hybrid cache with soft prefix for R -> compute NLL_R\n",
    "  3. Build hybrid cache with soft prefix for I -> compute NLL_I\n",
    "  4. Loss = max(0, MARGIN + NLL_R - NLL_I)\n",
    "     - If NLL_R < NLL_I - MARGIN: loss = 0 (satisfied)\n",
    "     - If NLL_R > NLL_I: loss > MARGIN (violated)\n",
    "  5. Gradient pushes soft prefix to reduce NLL_R relative to NLL_I\n",
    "\n",
    "Both forward passes MUST be in the same computation graph\n",
    "(both NLLs share the same soft_prefix) for loss.backward() to work.\n",
    "\n",
    "### Two Init Conditions ###\n",
    "\n",
    "  warm: Initialize from results/exp25/soft_prefix_fact.pt\n",
    "        Tests: can we add ranking signal to an already-useful NLL prefix?\n",
    "\n",
    "  cold: Initialize from random N(0, 0.02)\n",
    "        Tests: can contrastive loss learn ranking signal from scratch?\n",
    "\n",
    "### Evaluation Conditions (5 total) ###\n",
    "\n",
    "  bare:             No prefix, no cache modification\n",
    "  exp25_fact:       Exp 25 soft_prefix_fact (NLL-optimized)\n",
    "  contrastive_warm: This experiment's warm-start prefix\n",
    "  contrastive_cold: This experiment's cold-start prefix\n",
    "  baseline:         BOS-only cache (no document) for PMI computation\n",
    "\n",
    "### Ranking Metrics ###\n",
    "\n",
    "  AUC-ROC: Can the NLL scores separate relevant from irrelevant?\n",
    "  MRR@10:  Is the first relevant passage ranked near the top?\n",
    "  Both computed for raw NLL and PMI (NLL - baseline) scoring.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: differentiable_hybrid_score() — reuse from Exp 25 Cell 7 verbatim\n",
    "\n",
    "from lib.kv_cache import _get_rope_theta_for_layer, _build_rope_correction, _rotate_half\n",
    "\n",
    "print(f\"Embedding layer: {type(embed_fn).__name__}, shape={embed_fn.weight.shape}\")\n",
    "\n",
    "\n",
    "def differentiable_hybrid_score(\n",
    "    soft_prefix: torch.Tensor,       # (1, prefix_len, hidden_size), requires_grad\n",
    "    doc_ids: torch.Tensor,            # (1, doc_len)\n",
    "    bos_id: torch.Tensor,             # (1, 1)\n",
    "    query_prompt: str,\n",
    "    answer_text: str,\n",
    "    model,\n",
    "    tokenizer,\n",
    "    config,\n",
    "    cutoff: int,\n",
    "):\n",
    "    \"\"\"\n",
    "    Compute answer NLL through a hybrid cache built from soft prefix embeddings.\n",
    "    Returns a scalar loss tensor with gradients flowing back to soft_prefix.\n",
    "    \"\"\"\n",
    "    device = config.device\n",
    "    doc_len = doc_ids.shape[1]\n",
    "    prefix_len = soft_prefix.shape[1]\n",
    "    context_len = 1 + doc_len  # BOS + doc\n",
    "\n",
    "    # --- Step 1: Bare cache (no gradients needed) ---\n",
    "    bare_input = torch.cat([bos_id, doc_ids], dim=1)\n",
    "    with torch.no_grad():\n",
    "        bare_out = model(input_ids=bare_input,\n",
    "                         attention_mask=torch.ones_like(bare_input),\n",
    "                         use_cache=True, return_dict=True)\n",
    "    bare_cache = bare_out.past_key_values\n",
    "    del bare_out\n",
    "\n",
    "    # --- Step 2: Primed cache via inputs_embeds (gradients enabled) ---\n",
    "    with torch.no_grad():\n",
    "        bos_emb = embed_fn(bos_id)\n",
    "        doc_emb = embed_fn(doc_ids)\n",
    "\n",
    "    soft_cast = soft_prefix.to(dtype=bos_emb.dtype)\n",
    "    inputs_embeds = torch.cat([bos_emb.detach(), soft_cast, doc_emb.detach()], dim=1)\n",
    "    total_len = inputs_embeds.shape[1]\n",
    "    attn_mask = torch.ones((1, total_len), device=device, dtype=torch.long)\n",
    "\n",
    "    primed_out = model(inputs_embeds=inputs_embeds,\n",
    "                       attention_mask=attn_mask,\n",
    "                       use_cache=True, return_dict=True)\n",
    "    primed_cache = primed_out.past_key_values\n",
    "    del primed_out\n",
    "\n",
    "    # --- Step 3+4: Build hybrid cache ---\n",
    "    primed_cache_dc = _ensure_dynamic_cache(primed_cache)\n",
    "    bare_cache_dc = _ensure_dynamic_cache(bare_cache)\n",
    "\n",
    "    from transformers import DynamicCache\n",
    "    from transformers.cache_utils import DynamicSlidingWindowLayer, DynamicLayer\n",
    "\n",
    "    hybrid_cache = DynamicCache()\n",
    "    for layer_idx in range(NUM_LAYERS):\n",
    "        k = _get_cache_keys(bare_cache_dc, layer_idx)\n",
    "\n",
    "        if layer_idx < cutoff:\n",
    "            primed_v = _get_cache_values(primed_cache_dc, layer_idx)\n",
    "            bos_v = primed_v[:, :, :1, :]\n",
    "            doc_v = primed_v[:, :, -doc_len:, :]\n",
    "            v = torch.cat([bos_v, doc_v], dim=2)\n",
    "        else:\n",
    "            v = _get_cache_values(bare_cache_dc, layer_idx)\n",
    "\n",
    "        src_layer = bare_cache_dc.layers[layer_idx]\n",
    "        if isinstance(src_layer, DynamicSlidingWindowLayer):\n",
    "            new_layer = DynamicSlidingWindowLayer(sliding_window=src_layer.sliding_window)\n",
    "            new_layer.dtype = k.dtype\n",
    "            new_layer.device = k.device\n",
    "            new_layer.keys = k\n",
    "            new_layer.values = v\n",
    "            new_layer.is_initialized = True\n",
    "            new_layer.cumulative_length = src_layer.cumulative_length\n",
    "            new_layer._sliding_window_tensor = new_layer._sliding_window_tensor.to(k.device)\n",
    "        else:\n",
    "            new_layer = DynamicLayer()\n",
    "            new_layer.dtype = k.dtype\n",
    "            new_layer.device = k.device\n",
    "            new_layer.keys = k\n",
    "            new_layer.values = v\n",
    "            new_layer.is_initialized = True\n",
    "        hybrid_cache.layers.append(new_layer)\n",
    "\n",
    "    # --- Step 5: Score answer through hybrid cache ---\n",
    "    query_ids = tokenizer(query_prompt, return_tensors=\"pt\",\n",
    "                          add_special_tokens=False)['input_ids'].to(device)\n",
    "    answer_ids = tokenizer(answer_text, return_tensors=\"pt\",\n",
    "                           add_special_tokens=False)['input_ids'].to(device)\n",
    "    query_len = query_ids.shape[1]\n",
    "    answer_len = answer_ids.shape[1]\n",
    "\n",
    "    qa_ids = torch.cat([query_ids, answer_ids], dim=1)\n",
    "    qa_len = qa_ids.shape[1]\n",
    "    qa_attn_full = torch.ones((1, context_len + qa_len), device=device)\n",
    "\n",
    "    qa_out = model(input_ids=qa_ids,\n",
    "                   attention_mask=qa_attn_full,\n",
    "                   past_key_values=hybrid_cache,\n",
    "                   use_cache=False, return_dict=True)\n",
    "\n",
    "    logits = qa_out.logits\n",
    "    answer_logits = logits[:, query_len - 1 : query_len + answer_len - 1, :]\n",
    "\n",
    "    loss = torch.nn.functional.cross_entropy(\n",
    "        answer_logits.reshape(-1, answer_logits.shape[-1]),\n",
    "        answer_ids.reshape(-1),\n",
    "        reduction='mean'\n",
    "    )\n",
    "\n",
    "    del qa_out, logits, bare_cache, bare_cache_dc, primed_cache, primed_cache_dc\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "print(\"differentiable_hybrid_score() defined\")\n",
    "print(\"  Input: soft_prefix (requires_grad), doc_ids, query, answer\")\n",
    "print(\"  Output: scalar NLL loss with gradients to soft_prefix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Gradient flow sanity check with contrastive loss\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"GRADIENT FLOW SANITY CHECK — CONTRASTIVE LOSS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Create test prefix\n",
    "test_prefix = torch.randn(1, PREFIX_LEN, HIDDEN_SIZE,\n",
    "                           device=exp_config.device,\n",
    "                           dtype=torch.float32,\n",
    "                           requires_grad=True)\n",
    "\n",
    "# Pick a training query with both relevant and irrelevant passages\n",
    "test_q = train_queries[0]\n",
    "query_prompt = QUERY_TEMPLATE.format(query=test_q['query'])\n",
    "answer_text = ANSWER_TEMPLATE.format(answer=test_q['answer'])\n",
    "\n",
    "rel_passage = next(p for p in test_q['passages'] if p['is_relevant'])\n",
    "irr_passage = next(p for p in test_q['passages'] if not p['is_relevant'])\n",
    "\n",
    "print(f\"Test query: '{test_q['query'][:60]}...'\")\n",
    "print(f\"Relevant passage: '{rel_passage['passage'][:60]}...'\")\n",
    "print(f\"Irrelevant passage: '{irr_passage['passage'][:60]}...'\")\n",
    "\n",
    "# Helper to get doc_ids via matched tokenization\n",
    "def get_matched_doc_ids(passage_text):\n",
    "    doc_text = DOCUMENT_TEMPLATE.format(document=passage_text)\n",
    "    full_text = sf_str + doc_text\n",
    "    full_enc = tokenizer(full_text, return_tensors=\"pt\",\n",
    "                          add_special_tokens=True, padding=False, truncation=False)\n",
    "    full_ids = full_enc['input_ids'].to(exp_config.device)\n",
    "    sf_enc = tokenizer(sf_str, return_tensors=\"pt\",\n",
    "                        add_special_tokens=True, padding=False, truncation=False)\n",
    "    sf_len = sf_enc['input_ids'].shape[1]\n",
    "    bos_id = full_ids[:, :1]\n",
    "    doc_ids = full_ids[:, sf_len:]\n",
    "    return bos_id, doc_ids\n",
    "\n",
    "try:\n",
    "    # Score relevant passage\n",
    "    bos_rel, doc_rel = get_matched_doc_ids(rel_passage['passage'])\n",
    "    nll_rel = differentiable_hybrid_score(\n",
    "        test_prefix, doc_rel, bos_rel,\n",
    "        query_prompt, answer_text,\n",
    "        model, tokenizer, exp_config, CUTOFF)\n",
    "    print(f\"\\nNLL relevant: {nll_rel.item():.4f} (requires_grad: {nll_rel.requires_grad})\")\n",
    "\n",
    "    # Score irrelevant passage\n",
    "    bos_irr, doc_irr = get_matched_doc_ids(irr_passage['passage'])\n",
    "    nll_irr = differentiable_hybrid_score(\n",
    "        test_prefix, doc_irr, bos_irr,\n",
    "        query_prompt, answer_text,\n",
    "        model, tokenizer, exp_config, CUTOFF)\n",
    "    print(f\"NLL irrelevant: {nll_irr.item():.4f} (requires_grad: {nll_irr.requires_grad})\")\n",
    "\n",
    "    # Contrastive hinge loss\n",
    "    hinge_loss = torch.clamp(MARGIN + nll_rel - nll_irr, min=0.0)\n",
    "    print(f\"\\nHinge loss: max(0, {MARGIN} + {nll_rel.item():.4f} - {nll_irr.item():.4f}) = {hinge_loss.item():.4f}\")\n",
    "    print(f\"Hinge requires_grad: {hinge_loss.requires_grad}\")\n",
    "\n",
    "    # Backward\n",
    "    hinge_loss.backward()\n",
    "\n",
    "    print(f\"\\nBackward pass: SUCCESS\")\n",
    "    print(f\"Gradient shape: {test_prefix.grad.shape}\")\n",
    "    print(f\"Gradient norm: {test_prefix.grad.norm().item():.6f}\")\n",
    "\n",
    "    if hinge_loss.item() > 0 and test_prefix.grad.norm().item() > 0:\n",
    "        print(\"\\n>>> CONTRASTIVE GRADIENT FLOW CONFIRMED <<<\")\n",
    "    elif hinge_loss.item() == 0:\n",
    "        print(\"\\n>>> Hinge loss is 0 (margin satisfied). Gradient expected to be 0. <<<\")\n",
    "        print(\">>> This is correct behavior — try with different sample if needed. <<<\")\n",
    "    else:\n",
    "        print(\"\\n>>> WARNING: Non-zero loss but zero gradient. Debug needed. <<<\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n>>> GRADIENT FLOW FAILED: {e} <<<\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "finally:\n",
    "    del test_prefix\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: train_contrastive_soft_prefix()\n",
    "\n",
    "def train_contrastive_soft_prefix(init_mode, train_data, n_epochs, lr, grad_accum,\n",
    "                                   margin, checkpoint_path, save_path, warmup_steps=30):\n",
    "    \"\"\"\n",
    "    Train soft prefix with contrastive hinge loss for ranking.\n",
    "\n",
    "    Per step: sample 1 relevant + 1 irrelevant passage per query.\n",
    "    Loss = max(0, margin + NLL_relevant - NLL_irrelevant).\n",
    "    Both scored through the SAME soft prefix (sequential forward passes).\n",
    "\n",
    "    Args:\n",
    "        init_mode: 'warm' (from exp25_fact) or 'cold' (random)\n",
    "        train_data: list of query dicts with 'passages' containing relevance labels\n",
    "        n_epochs: number of passes\n",
    "        lr: learning rate\n",
    "        grad_accum: gradient accumulation steps\n",
    "        margin: hinge loss margin\n",
    "        checkpoint_path: path to save training checkpoints\n",
    "        save_path: path to save final embeddings\n",
    "        warmup_steps: linear warmup steps\n",
    "\n",
    "    Returns:\n",
    "        dict with training history and final embeddings\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'=' * 70}\")\n",
    "    print(f\"TRAINING CONTRASTIVE SOFT PREFIX — init={init_mode}\")\n",
    "    print(f\"{'=' * 70}\")\n",
    "\n",
    "    # Initialize soft prefix\n",
    "    if init_mode == 'warm':\n",
    "        soft_prefix = torch.load(EXP25_SOFT_FACT).to(exp_config.device).float()\n",
    "        print(f\"  Loaded warm-start from {EXP25_SOFT_FACT}\")\n",
    "    elif init_mode == 'cold':\n",
    "        soft_prefix = torch.randn(1, PREFIX_LEN, HIDDEN_SIZE,\n",
    "                                   device=exp_config.device, dtype=torch.float32) * 0.02\n",
    "        print(f\"  Cold-start: random N(0, 0.02)\")\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown init_mode: {init_mode}\")\n",
    "\n",
    "    soft_prefix = soft_prefix.detach().requires_grad_(True)\n",
    "    print(f\"  Shape: {soft_prefix.shape}, norm: {soft_prefix.norm().item():.4f}\")\n",
    "\n",
    "    optimizer = torch.optim.AdamW([soft_prefix], lr=lr, weight_decay=0.01)\n",
    "\n",
    "    total_steps = n_epochs * len(train_data)\n",
    "    total_optim_steps = total_steps // grad_accum\n",
    "    print(f\"  Total steps: {total_steps}, optim steps: {total_optim_steps}\")\n",
    "\n",
    "    # Checkpoint resume\n",
    "    history = []\n",
    "    start_step = 0\n",
    "    if checkpoint_path.exists():\n",
    "        ckpt = json.loads(checkpoint_path.read_text())\n",
    "        if ckpt.get('init_mode') == init_mode and ckpt.get('total_steps') == total_steps:\n",
    "            history = ckpt['history']\n",
    "            start_step = ckpt['completed_steps']\n",
    "            soft_prefix_data = torch.tensor(ckpt['soft_prefix'],\n",
    "                                            device=exp_config.device, dtype=torch.float32)\n",
    "            soft_prefix = soft_prefix_data.requires_grad_(True)\n",
    "            optimizer = torch.optim.AdamW([soft_prefix], lr=lr, weight_decay=0.01)\n",
    "            print(f\"  Resumed from checkpoint: step {start_step}/{total_steps}\")\n",
    "\n",
    "    t_start = time.time()\n",
    "    step = 0\n",
    "    optim_step = 0\n",
    "    running_loss = 0.0\n",
    "    running_nll_gap = 0.0  # NLL_irr - NLL_rel (positive = good)\n",
    "    running_satisfied = 0  # fraction where hinge loss = 0\n",
    "    running_count = 0\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        np.random.seed(SEED + epoch)\n",
    "        epoch_indices = np.random.permutation(len(train_data))\n",
    "\n",
    "        for data_idx in epoch_indices:\n",
    "            if step < start_step:\n",
    "                step += 1\n",
    "                continue\n",
    "\n",
    "            qdata = train_data[data_idx]\n",
    "            query_prompt = QUERY_TEMPLATE.format(query=qdata['query'])\n",
    "            answer_text = ANSWER_TEMPLATE.format(answer=qdata['answer'])\n",
    "\n",
    "            # Sample 1 relevant + 1 irrelevant passage\n",
    "            rel_passages = [p for p in qdata['passages'] if p['is_relevant']]\n",
    "            irr_passages = [p for p in qdata['passages'] if not p['is_relevant']]\n",
    "            rel_p = rel_passages[np.random.randint(len(rel_passages))]\n",
    "            irr_p = irr_passages[np.random.randint(len(irr_passages))]\n",
    "\n",
    "            try:\n",
    "                # Score relevant passage\n",
    "                bos_rel, doc_rel = get_matched_doc_ids(rel_p['passage'])\n",
    "                nll_rel = differentiable_hybrid_score(\n",
    "                    soft_prefix, doc_rel, bos_rel,\n",
    "                    query_prompt, answer_text,\n",
    "                    model, tokenizer, exp_config, CUTOFF)\n",
    "\n",
    "                # Score irrelevant passage\n",
    "                bos_irr, doc_irr = get_matched_doc_ids(irr_p['passage'])\n",
    "                nll_irr = differentiable_hybrid_score(\n",
    "                    soft_prefix, doc_irr, bos_irr,\n",
    "                    query_prompt, answer_text,\n",
    "                    model, tokenizer, exp_config, CUTOFF)\n",
    "\n",
    "                # Hinge loss\n",
    "                hinge_loss = torch.clamp(margin + nll_rel - nll_irr, min=0.0)\n",
    "                scaled_loss = hinge_loss / grad_accum\n",
    "                scaled_loss.backward()\n",
    "\n",
    "                running_loss += hinge_loss.item()\n",
    "                running_nll_gap += (nll_irr.item() - nll_rel.item())\n",
    "                running_satisfied += int(hinge_loss.item() == 0)\n",
    "                running_count += 1\n",
    "\n",
    "            except RuntimeError as e:\n",
    "                print(f\"  Step {step}: RuntimeError: {e}\")\n",
    "                optimizer.zero_grad()\n",
    "                gc.collect()\n",
    "                torch.cuda.empty_cache()\n",
    "                step += 1\n",
    "                continue\n",
    "\n",
    "            # Optimizer step\n",
    "            if (step + 1) % grad_accum == 0:\n",
    "                optim_step += 1\n",
    "                if optim_step <= warmup_steps:\n",
    "                    for pg in optimizer.param_groups:\n",
    "                        pg['lr'] = lr * (optim_step / warmup_steps)\n",
    "\n",
    "                grad_norm = soft_prefix.grad.norm().item() if soft_prefix.grad is not None else 0\n",
    "                torch.nn.utils.clip_grad_norm_([soft_prefix], max_norm=1.0)\n",
    "\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                avg_loss = running_loss / running_count if running_count > 0 else 0\n",
    "                avg_gap = running_nll_gap / running_count if running_count > 0 else 0\n",
    "                sat_frac = running_satisfied / running_count if running_count > 0 else 0\n",
    "\n",
    "                history.append({\n",
    "                    'step': step,\n",
    "                    'optim_step': optim_step,\n",
    "                    'epoch': epoch,\n",
    "                    'avg_loss': avg_loss,\n",
    "                    'avg_nll_gap': avg_gap,\n",
    "                    'satisfied_frac': sat_frac,\n",
    "                    'grad_norm': grad_norm,\n",
    "                    'prefix_norm': soft_prefix.norm().item(),\n",
    "                    'lr': optimizer.param_groups[0]['lr'],\n",
    "                })\n",
    "                running_loss = 0.0\n",
    "                running_nll_gap = 0.0\n",
    "                running_satisfied = 0\n",
    "                running_count = 0\n",
    "\n",
    "            # Cleanup\n",
    "            del hinge_loss, scaled_loss, nll_rel, nll_irr\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            step += 1\n",
    "\n",
    "            # Checkpoint\n",
    "            if step % CHECKPOINT_EVERY == 0 or step == total_steps:\n",
    "                elapsed = time.time() - t_start\n",
    "                steps_done = step - start_step\n",
    "                rate = steps_done / elapsed if elapsed > 0 else 0\n",
    "                remaining = (total_steps - step) / rate if rate > 0 else 0\n",
    "\n",
    "                last = history[-1] if history else {}\n",
    "                tqdm.write(\n",
    "                    f\"  [{init_mode}] Step {step}/{total_steps} | \"\n",
    "                    f\"loss={last.get('avg_loss', 0):.4f} | \"\n",
    "                    f\"gap={last.get('avg_nll_gap', 0):.3f} | \"\n",
    "                    f\"sat={last.get('satisfied_frac', 0):.1%} | \"\n",
    "                    f\"norm={soft_prefix.norm().item():.3f} | \"\n",
    "                    f\"ETA: {remaining/60:.1f}m\")\n",
    "\n",
    "                ckpt_data = {\n",
    "                    'init_mode': init_mode,\n",
    "                    'completed_steps': step,\n",
    "                    'total_steps': total_steps,\n",
    "                    'history': history,\n",
    "                    'soft_prefix': soft_prefix.detach().cpu().tolist(),\n",
    "                    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                }\n",
    "                with open(checkpoint_path, 'w') as f:\n",
    "                    json.dump(ckpt_data, f)\n",
    "\n",
    "    # Save final\n",
    "    torch.save(soft_prefix.detach().cpu(), save_path)\n",
    "\n",
    "    elapsed = time.time() - t_start\n",
    "    print(f\"\\n  Training complete: {step} steps in {elapsed/60:.1f} min\")\n",
    "    print(f\"  Final prefix norm: {soft_prefix.norm().item():.4f}\")\n",
    "    print(f\"  Saved to: {save_path}\")\n",
    "\n",
    "    return {\n",
    "        'soft_prefix': soft_prefix.detach(),\n",
    "        'history': history,\n",
    "        'init_mode': init_mode,\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"train_contrastive_soft_prefix() defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Train warm start (init from exp25 soft_prefix_fact.pt)\n",
    "\n",
    "result_warm = train_contrastive_soft_prefix(\n",
    "    init_mode='warm',\n",
    "    train_data=train_queries,\n",
    "    n_epochs=N_EPOCHS,\n",
    "    lr=LR,\n",
    "    grad_accum=GRAD_ACCUM,\n",
    "    margin=MARGIN,\n",
    "    checkpoint_path=CHECKPOINT_TRAIN_WARM_PATH,\n",
    "    save_path=SOFT_WARM_PATH,\n",
    "    warmup_steps=WARMUP_STEPS,\n",
    ")\n",
    "soft_contrastive_warm = result_warm['soft_prefix']\n",
    "history_warm = result_warm['history']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Train cold start (init from random)\n",
    "\n",
    "result_cold = train_contrastive_soft_prefix(\n",
    "    init_mode='cold',\n",
    "    train_data=train_queries,\n",
    "    n_epochs=N_EPOCHS,\n",
    "    lr=LR,\n",
    "    grad_accum=GRAD_ACCUM,\n",
    "    margin=MARGIN,\n",
    "    checkpoint_path=CHECKPOINT_TRAIN_COLD_PATH,\n",
    "    save_path=SOFT_COLD_PATH,\n",
    "    warmup_steps=WARMUP_STEPS,\n",
    ")\n",
    "soft_contrastive_cold = result_cold['soft_prefix']\n",
    "history_cold = result_cold['history']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Training curves\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "\n",
    "for hist, label, color in [(history_warm, 'warm (exp25_fact)', '#ff7f0e'),\n",
    "                            (history_cold, 'cold (random)', '#1f77b4')]:\n",
    "    if not hist:\n",
    "        continue\n",
    "    steps = [h['optim_step'] for h in hist]\n",
    "    losses = [h['avg_loss'] for h in hist]\n",
    "    gaps = [h['avg_nll_gap'] for h in hist]\n",
    "    sats = [h['satisfied_frac'] for h in hist]\n",
    "    pnorms = [h['prefix_norm'] for h in hist]\n",
    "\n",
    "    w = min(20, len(losses) // 3 + 1)\n",
    "\n",
    "    # Panel 1: Contrastive loss\n",
    "    ax = axes[0, 0]\n",
    "    ax.plot(steps, losses, alpha=0.2, color=color)\n",
    "    if len(losses) > w:\n",
    "        smoothed = np.convolve(losses, np.ones(w)/w, mode='valid')\n",
    "        ax.plot(steps[w-1:], smoothed, linewidth=2, color=color, label=label)\n",
    "    else:\n",
    "        ax.plot(steps, losses, linewidth=2, color=color, label=label)\n",
    "\n",
    "    # Panel 2: NLL gap (irr - rel)\n",
    "    ax = axes[0, 1]\n",
    "    ax.plot(steps, gaps, alpha=0.2, color=color)\n",
    "    if len(gaps) > w:\n",
    "        smoothed = np.convolve(gaps, np.ones(w)/w, mode='valid')\n",
    "        ax.plot(steps[w-1:], smoothed, linewidth=2, color=color, label=label)\n",
    "    else:\n",
    "        ax.plot(steps, gaps, linewidth=2, color=color, label=label)\n",
    "\n",
    "    # Panel 3: Satisfied fraction\n",
    "    ax = axes[1, 0]\n",
    "    ax.plot(steps, sats, alpha=0.2, color=color)\n",
    "    if len(sats) > w:\n",
    "        smoothed = np.convolve(sats, np.ones(w)/w, mode='valid')\n",
    "        ax.plot(steps[w-1:], smoothed, linewidth=2, color=color, label=label)\n",
    "    else:\n",
    "        ax.plot(steps, sats, linewidth=2, color=color, label=label)\n",
    "\n",
    "    # Panel 4: Prefix norm\n",
    "    ax = axes[1, 1]\n",
    "    ax.plot(steps, pnorms, linewidth=2, color=color, label=label)\n",
    "\n",
    "axes[0, 0].set_xlabel('Optimizer Step')\n",
    "axes[0, 0].set_ylabel('Hinge Loss')\n",
    "axes[0, 0].set_title('Contrastive Loss')\n",
    "axes[0, 0].legend(fontsize=8)\n",
    "axes[0, 0].grid(alpha=0.3)\n",
    "\n",
    "axes[0, 1].set_xlabel('Optimizer Step')\n",
    "axes[0, 1].set_ylabel('NLL Gap (irr - rel)')\n",
    "axes[0, 1].set_title('NLL Gap (positive = correct direction)')\n",
    "axes[0, 1].axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "axes[0, 1].legend(fontsize=8)\n",
    "axes[0, 1].grid(alpha=0.3)\n",
    "\n",
    "axes[1, 0].set_xlabel('Optimizer Step')\n",
    "axes[1, 0].set_ylabel('Fraction Satisfied')\n",
    "axes[1, 0].set_title('Hinge Margin Satisfied')\n",
    "axes[1, 0].set_ylim(-0.05, 1.05)\n",
    "axes[1, 0].legend(fontsize=8)\n",
    "axes[1, 0].grid(alpha=0.3)\n",
    "\n",
    "axes[1, 1].set_xlabel('Optimizer Step')\n",
    "axes[1, 1].set_ylabel('Norm')\n",
    "axes[1, 1].set_title('Prefix Embedding Norm')\n",
    "axes[1, 1].legend(fontsize=8)\n",
    "axes[1, 1].grid(alpha=0.3)\n",
    "\n",
    "plt.suptitle('Exp 28: Contrastive Training Curves', fontsize=13)\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'training_curves.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"Saved to {RESULTS_DIR / 'training_curves.png'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Load 200 validation queries — multi-passage format (like Exp 22)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"LOADING VALIDATION QUERIES — MULTI-PASSAGE FORMAT\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "dataset = load_dataset(\"microsoft/ms_marco\", \"v1.1\", split=\"validation\")\n",
    "print(f\"Total items: {len(dataset)}\")\n",
    "\n",
    "val_queries = []\n",
    "np.random.seed(SEED)  # Same seed as Exp 22 for comparability\n",
    "\n",
    "for item in tqdm(dataset, desc=\"Filtering val\"):\n",
    "    passages_info = item.get('passages', {})\n",
    "    passage_texts = passages_info.get('passage_text', [])\n",
    "    is_selected = passages_info.get('is_selected', [])\n",
    "    query = item.get('query', '')\n",
    "    answers = item.get('answers', [])\n",
    "    well_formed = item.get('wellFormedAnswers', [])\n",
    "\n",
    "    if not passage_texts or not query:\n",
    "        continue\n",
    "    if not is_selected or sum(is_selected) == 0:\n",
    "        continue\n",
    "\n",
    "    word_counts = [count_words(p) for p in passage_texts]\n",
    "    if any(wc > MAX_PASSAGE_WORDS for wc in word_counts):\n",
    "        continue\n",
    "\n",
    "    answer = None\n",
    "    if well_formed and len(well_formed) > 0 and well_formed[0] != '[]':\n",
    "        answer = well_formed[0]\n",
    "    elif answers and len(answers) > 0 and answers[0] != 'No Answer Present.':\n",
    "        answer = answers[0]\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "    passage_list = []\n",
    "    for i, (ptext, sel) in enumerate(zip(passage_texts, is_selected)):\n",
    "        passage_list.append({\n",
    "            'passage': ptext,\n",
    "            'is_relevant': bool(sel == 1),\n",
    "            'word_count': word_counts[i],\n",
    "            'passage_idx': i,\n",
    "        })\n",
    "\n",
    "    val_queries.append({\n",
    "        'query': query,\n",
    "        'answer': answer,\n",
    "        'passages': passage_list,\n",
    "        'n_passages': len(passage_list),\n",
    "        'n_relevant': sum(1 for p in passage_list if p['is_relevant']),\n",
    "    })\n",
    "\n",
    "    if len(val_queries) >= N_EVAL * 3:\n",
    "        break\n",
    "\n",
    "np.random.shuffle(val_queries)\n",
    "val_queries = val_queries[:N_EVAL]\n",
    "N = len(val_queries)\n",
    "\n",
    "n_val_passages = [q['n_passages'] for q in val_queries]\n",
    "total_val_passages = sum(n_val_passages)\n",
    "total_val_rel = sum(q['n_relevant'] for q in val_queries)\n",
    "\n",
    "print(f\"\\nSelected {N} val queries ({total_val_passages} passages)\")\n",
    "print(f\"Passages per query: mean={np.mean(n_val_passages):.1f}\")\n",
    "print(f\"Relevant: {total_val_rel} ({100*total_val_rel/total_val_passages:.1f}%)\")\n",
    "\n",
    "del dataset\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13: Ranking eval — score ALL passages per query under 5 conditions\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"RANKING EVALUATION ({N} queries, {total_val_passages} passages, 5 conditions)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Load soft prefixes (in case of restart)\n",
    "if 'soft_contrastive_warm' not in dir():\n",
    "    soft_contrastive_warm = torch.load(SOFT_WARM_PATH).to(exp_config.device)\n",
    "    print(f\"Loaded contrastive_warm from {SOFT_WARM_PATH}\")\n",
    "if 'soft_contrastive_cold' not in dir():\n",
    "    soft_contrastive_cold = torch.load(SOFT_COLD_PATH).to(exp_config.device)\n",
    "    print(f\"Loaded contrastive_cold from {SOFT_COLD_PATH}\")\n",
    "\n",
    "soft_exp25_fact = torch.load(EXP25_SOFT_FACT).to(exp_config.device)\n",
    "print(f\"Loaded exp25_fact from {EXP25_SOFT_FACT}\")\n",
    "\n",
    "layer_indices = list(range(CUTOFF))\n",
    "\n",
    "def score_baseline_nll(query_prompt, answer_text):\n",
    "    \"\"\"Score answer with BOS-only cache (no document).\"\"\"\n",
    "    bos_id = torch.tensor([[tokenizer.bos_token_id]], device=exp_config.device)\n",
    "    with torch.no_grad():\n",
    "        bos_out = model(input_ids=bos_id,\n",
    "                        attention_mask=torch.ones_like(bos_id),\n",
    "                        use_cache=True, return_dict=True)\n",
    "    bos_cache = _ensure_dynamic_cache(bos_out.past_key_values)\n",
    "    del bos_out\n",
    "    nll = score_answer_with_cache(\n",
    "        bos_cache, 1, query_prompt, answer_text,\n",
    "        model, tokenizer, exp_config)\n",
    "    return nll\n",
    "\n",
    "\n",
    "def score_soft_passage(soft_embs, doc_ids, bos_id, doc_len, context_len,\n",
    "                        bare_cache, query_prompt, answer_text):\n",
    "    \"\"\"Score a passage through soft prefix hybrid cache (no grad).\"\"\"\n",
    "    with torch.no_grad():\n",
    "        bos_emb = embed_fn(bos_id)\n",
    "        doc_emb = embed_fn(doc_ids)\n",
    "        soft_cast = soft_embs.to(device=exp_config.device, dtype=bos_emb.dtype)\n",
    "\n",
    "        inputs_embeds = torch.cat([bos_emb, soft_cast, doc_emb], dim=1)\n",
    "        total_len = inputs_embeds.shape[1]\n",
    "        attn_mask = torch.ones((1, total_len), device=exp_config.device, dtype=torch.long)\n",
    "\n",
    "        soft_out = model(inputs_embeds=inputs_embeds,\n",
    "                        attention_mask=attn_mask,\n",
    "                        use_cache=True, return_dict=True)\n",
    "        soft_cache = _ensure_dynamic_cache(soft_out.past_key_values)\n",
    "        del soft_out\n",
    "\n",
    "        soft_trunc = extract_and_truncate_cache_with_bos(soft_cache, doc_len)\n",
    "        del soft_cache\n",
    "\n",
    "        vel_cache = replace_values_at_layers(bare_cache, soft_trunc, layer_indices)\n",
    "        del soft_trunc\n",
    "\n",
    "        nll = score_answer_with_cache(\n",
    "            deepcopy_cache(vel_cache), context_len,\n",
    "            query_prompt, answer_text, model, tokenizer, exp_config)\n",
    "        del vel_cache\n",
    "\n",
    "    return nll\n",
    "\n",
    "\n",
    "# Checkpoint resume\n",
    "all_results = []\n",
    "start_idx = 0\n",
    "\n",
    "if CHECKPOINT_EVAL_PATH.exists():\n",
    "    with open(CHECKPOINT_EVAL_PATH, 'r') as f:\n",
    "        ckpt = json.load(f)\n",
    "    ckpt_queries = ckpt.get('query_texts', [])\n",
    "    current_queries = [q['query'] for q in val_queries]\n",
    "    if ckpt_queries == current_queries:\n",
    "        all_results = ckpt['results']\n",
    "        start_idx = len(all_results)\n",
    "        print(f\"Resuming from checkpoint: {start_idx}/{N}\")\n",
    "    else:\n",
    "        print(\"Checkpoint query mismatch. Starting fresh.\")\n",
    "else:\n",
    "    print(\"No checkpoint found. Starting fresh.\")\n",
    "\n",
    "EVAL_CHECKPOINT_EVERY = 10\n",
    "t_start = time.time()\n",
    "\n",
    "for qidx in tqdm(range(start_idx, N), initial=start_idx, total=N, desc=\"Ranking eval\"):\n",
    "    qdata = val_queries[qidx]\n",
    "    query_prompt = QUERY_TEMPLATE.format(query=qdata['query'])\n",
    "    answer_text = ANSWER_TEMPLATE.format(answer=qdata['answer'])\n",
    "\n",
    "    # Baseline NLL (BOS-only, once per query)\n",
    "    nll_baseline = score_baseline_nll(query_prompt, answer_text)\n",
    "\n",
    "    passage_results = []\n",
    "    for pidx, pinfo in enumerate(qdata['passages']):\n",
    "        passage_text = pinfo['passage']\n",
    "        document_text = DOCUMENT_TEMPLATE.format(document=passage_text)\n",
    "\n",
    "        # Matched tokenization\n",
    "        full_text = sf_str + document_text\n",
    "        full_enc = tokenizer(full_text, return_tensors=\"pt\",\n",
    "                              add_special_tokens=True, padding=False, truncation=False)\n",
    "        full_ids = full_enc['input_ids'].to(exp_config.device)\n",
    "        sf_enc = tokenizer(sf_str, return_tensors=\"pt\",\n",
    "                            add_special_tokens=True, padding=False, truncation=False)\n",
    "        sf_len = sf_enc['input_ids'].shape[1]\n",
    "        bos_id = full_ids[:, :1]\n",
    "        doc_ids = full_ids[:, sf_len:]\n",
    "        doc_len = doc_ids.shape[1]\n",
    "        context_len = 1 + doc_len\n",
    "\n",
    "        del full_enc, full_ids, sf_enc\n",
    "\n",
    "        # Build bare cache\n",
    "        bare_input = torch.cat([bos_id, doc_ids], dim=1)\n",
    "        with torch.no_grad():\n",
    "            bare_out = model(input_ids=bare_input,\n",
    "                             attention_mask=torch.ones_like(bare_input),\n",
    "                             use_cache=True, return_dict=True)\n",
    "        bare_cache = _ensure_dynamic_cache(bare_out.past_key_values)\n",
    "        del bare_out, bare_input\n",
    "\n",
    "        # Condition 1: bare\n",
    "        nll_bare = score_answer_with_cache(\n",
    "            deepcopy_cache(bare_cache), context_len,\n",
    "            query_prompt, answer_text, model, tokenizer, exp_config)\n",
    "\n",
    "        # Condition 2: exp25_fact\n",
    "        nll_exp25 = score_soft_passage(\n",
    "            soft_exp25_fact, doc_ids, bos_id, doc_len, context_len,\n",
    "            bare_cache, query_prompt, answer_text)\n",
    "\n",
    "        # Condition 3: contrastive_warm\n",
    "        nll_warm = score_soft_passage(\n",
    "            soft_contrastive_warm, doc_ids, bos_id, doc_len, context_len,\n",
    "            bare_cache, query_prompt, answer_text)\n",
    "\n",
    "        # Condition 4: contrastive_cold\n",
    "        nll_cold = score_soft_passage(\n",
    "            soft_contrastive_cold, doc_ids, bos_id, doc_len, context_len,\n",
    "            bare_cache, query_prompt, answer_text)\n",
    "\n",
    "        del bare_cache\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        passage_results.append({\n",
    "            'passage_idx': pinfo['passage_idx'],\n",
    "            'is_relevant': pinfo['is_relevant'],\n",
    "            'word_count': pinfo['word_count'],\n",
    "            'doc_len': doc_len,\n",
    "            'nll_bare': float(nll_bare),\n",
    "            'nll_exp25_fact': float(nll_exp25),\n",
    "            'nll_contrastive_warm': float(nll_warm),\n",
    "            'nll_contrastive_cold': float(nll_cold),\n",
    "            'nll_baseline': float(nll_baseline),\n",
    "        })\n",
    "\n",
    "    all_results.append({\n",
    "        'query_idx': qidx,\n",
    "        'query': qdata['query'],\n",
    "        'answer': qdata['answer'],\n",
    "        'n_passages': len(passage_results),\n",
    "        'n_relevant': qdata['n_relevant'],\n",
    "        'nll_baseline': float(nll_baseline),\n",
    "        'passage_data': passage_results,\n",
    "    })\n",
    "\n",
    "    # Checkpoint\n",
    "    if (qidx + 1) % EVAL_CHECKPOINT_EVERY == 0 or qidx == N - 1:\n",
    "        ckpt_data = {\n",
    "            'results': all_results,\n",
    "            'query_texts': [q['query'] for q in val_queries],\n",
    "            'completed': len(all_results),\n",
    "            'total': N,\n",
    "            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        }\n",
    "        with open(CHECKPOINT_EVAL_PATH, 'w') as f:\n",
    "            json.dump(ckpt_data, f)\n",
    "        elapsed = time.time() - t_start\n",
    "        n_done = qidx - start_idx + 1\n",
    "        rate = n_done / elapsed if elapsed > 0 else 0\n",
    "        remaining = (N - qidx - 1) / rate if rate > 0 else 0\n",
    "        tqdm.write(f\"  Checkpoint {qidx+1}/{N} | {n_done} done in {elapsed/60:.1f}m | \"\n",
    "                   f\"ETA: {remaining/60:.1f} min\")\n",
    "\n",
    "elapsed_total = time.time() - t_start\n",
    "print(f\"\\nEval complete: {len(all_results)} queries in {elapsed_total/60:.1f} min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 14: Ranking analysis — AUC-ROC, MRR@10, differential NLL\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"RANKING ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Flatten passage-level data\n",
    "is_relevant_all = []\n",
    "nll_bare_all = []\n",
    "nll_exp25_all = []\n",
    "nll_warm_all = []\n",
    "nll_cold_all = []\n",
    "nll_baseline_all = []\n",
    "\n",
    "for r in all_results:\n",
    "    for p in r['passage_data']:\n",
    "        is_relevant_all.append(int(p['is_relevant']))\n",
    "        nll_bare_all.append(p['nll_bare'])\n",
    "        nll_exp25_all.append(p['nll_exp25_fact'])\n",
    "        nll_warm_all.append(p['nll_contrastive_warm'])\n",
    "        nll_cold_all.append(p['nll_contrastive_cold'])\n",
    "        nll_baseline_all.append(p['nll_baseline'])\n",
    "\n",
    "is_relevant = np.array(is_relevant_all)\n",
    "nll_bare = np.array(nll_bare_all)\n",
    "nll_exp25 = np.array(nll_exp25_all)\n",
    "nll_warm = np.array(nll_warm_all)\n",
    "nll_cold = np.array(nll_cold_all)\n",
    "nll_baseline = np.array(nll_baseline_all)\n",
    "\n",
    "# PMI scores\n",
    "pmi_bare = nll_bare - nll_baseline\n",
    "pmi_exp25 = nll_exp25 - nll_baseline\n",
    "pmi_warm = nll_warm - nll_baseline\n",
    "pmi_cold = nll_cold - nll_baseline\n",
    "\n",
    "n_total = len(is_relevant)\n",
    "n_rel = int(is_relevant.sum())\n",
    "n_irr = n_total - n_rel\n",
    "\n",
    "print(f\"Total passages: {n_total}\")\n",
    "print(f\"Relevant: {n_rel} ({100*n_rel/n_total:.1f}%), Irrelevant: {n_irr}\")\n",
    "\n",
    "# === AUC-ROC ===\n",
    "scoring_methods = {\n",
    "    'Raw bare': nll_bare,\n",
    "    'Raw exp25_fact': nll_exp25,\n",
    "    'Raw contr_warm': nll_warm,\n",
    "    'Raw contr_cold': nll_cold,\n",
    "    'PMI bare': pmi_bare,\n",
    "    'PMI exp25_fact': pmi_exp25,\n",
    "    'PMI contr_warm': pmi_warm,\n",
    "    'PMI contr_cold': pmi_cold,\n",
    "}\n",
    "\n",
    "auc_results = {}\n",
    "print(f\"\\n{'Method':<20} {'AUC':>8}\")\n",
    "print(\"-\" * 30)\n",
    "for name, scores in scoring_methods.items():\n",
    "    auc = roc_auc_score(is_relevant, -scores)\n",
    "    auc_results[name] = float(auc)\n",
    "    marker = \" <<<\" if name in ['Raw contr_warm', 'PMI contr_warm'] else \"\"\n",
    "    print(f\"{name:<20} {auc:>8.3f}{marker}\")\n",
    "\n",
    "# === MRR@10 ===\n",
    "def compute_mrr_at_k(all_results, score_fn, k=10):\n",
    "    rr_list = []\n",
    "    for r in all_results:\n",
    "        passages = r['passage_data']\n",
    "        scored = [(score_fn(p), p['is_relevant']) for p in passages]\n",
    "        scored.sort(key=lambda x: x[0])\n",
    "        rr = 0.0\n",
    "        for rank, (score, rel) in enumerate(scored[:k], 1):\n",
    "            if rel:\n",
    "                rr = 1.0 / rank\n",
    "                break\n",
    "        rr_list.append(rr)\n",
    "    return np.mean(rr_list), rr_list\n",
    "\n",
    "mrr_fns = {\n",
    "    'Raw bare': lambda p: p['nll_bare'],\n",
    "    'Raw exp25_fact': lambda p: p['nll_exp25_fact'],\n",
    "    'Raw contr_warm': lambda p: p['nll_contrastive_warm'],\n",
    "    'Raw contr_cold': lambda p: p['nll_contrastive_cold'],\n",
    "    'PMI bare': lambda p: p['nll_bare'] - p['nll_baseline'],\n",
    "    'PMI exp25_fact': lambda p: p['nll_exp25_fact'] - p['nll_baseline'],\n",
    "    'PMI contr_warm': lambda p: p['nll_contrastive_warm'] - p['nll_baseline'],\n",
    "    'PMI contr_cold': lambda p: p['nll_contrastive_cold'] - p['nll_baseline'],\n",
    "}\n",
    "\n",
    "mrr_results = {}\n",
    "mrr_per_query = {}\n",
    "print(f\"\\n{'Method':<20} {'MRR@10':>8}\")\n",
    "print(\"-\" * 30)\n",
    "for name, fn in mrr_fns.items():\n",
    "    mrr, rr_list = compute_mrr_at_k(all_results, fn, k=10)\n",
    "    mrr_results[name] = float(mrr)\n",
    "    mrr_per_query[name] = rr_list\n",
    "    marker = \" <<<\" if name in ['Raw contr_warm', 'PMI contr_warm'] else \"\"\n",
    "    print(f\"{name:<20} {mrr:>8.3f}{marker}\")\n",
    "\n",
    "# === Differential NLL ===\n",
    "print(f\"\\n{'Method':<20} {'Mean Rel':>10} {'Mean Irr':>10} {'Diff':>10} {'d':>8}\")\n",
    "print(\"-\" * 62)\n",
    "diff_results = {}\n",
    "for name, scores in scoring_methods.items():\n",
    "    rel_vals = scores[is_relevant == 1]\n",
    "    irr_vals = scores[is_relevant == 0]\n",
    "    diff = np.mean(irr_vals) - np.mean(rel_vals)\n",
    "    pooled_std = np.sqrt(\n",
    "        (np.var(rel_vals) * (len(rel_vals)-1) + np.var(irr_vals) * (len(irr_vals)-1)) /\n",
    "        (len(rel_vals) + len(irr_vals) - 2)\n",
    "    )\n",
    "    d = diff / pooled_std if pooled_std > 0 else 0\n",
    "    t_stat, p_val = stats.ttest_ind(irr_vals, rel_vals)\n",
    "    diff_results[name] = {\n",
    "        'mean_relevant': float(np.mean(rel_vals)),\n",
    "        'mean_irrelevant': float(np.mean(irr_vals)),\n",
    "        'diff': float(diff),\n",
    "        'cohens_d': float(d),\n",
    "        't_stat': float(t_stat),\n",
    "        'p_value': float(p_val),\n",
    "    }\n",
    "    print(f\"{name:<20} {np.mean(rel_vals):>10.4f} {np.mean(irr_vals):>10.4f} \"\n",
    "          f\"{diff:>+10.4f} {d:>+8.3f}\")\n",
    "\n",
    "# === Summary ===\n",
    "print(f\"\\n{'=' * 70}\")\n",
    "print(\"SUMMARY TABLE\")\n",
    "print(f\"{'=' * 70}\")\n",
    "print(f\"{'Method':<20} {'AUC':>8} {'MRR@10':>8} {'Diff NLL':>10} {'d':>8}\")\n",
    "print(\"-\" * 58)\n",
    "for name in scoring_methods:\n",
    "    print(f\"{name:<20} {auc_results[name]:>8.3f} {mrr_results[name]:>8.3f} \"\n",
    "          f\"{diff_results[name]['diff']:>+10.4f} {diff_results[name]['cohens_d']:>+8.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 15: NLL improvement check — does contrastive prefix still help average NLL?\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"NLL IMPROVEMENT CHECK (avg NLL vs bare, per-query paired)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Compute per-query mean NLL for each condition\n",
    "per_query_bare = []\n",
    "per_query_exp25 = []\n",
    "per_query_warm = []\n",
    "per_query_cold = []\n",
    "\n",
    "for r in all_results:\n",
    "    # Use only the relevant passage(s) for NLL comparison (like Exp 25)\n",
    "    rel_passages = [p for p in r['passage_data'] if p['is_relevant']]\n",
    "    if not rel_passages:\n",
    "        continue\n",
    "    per_query_bare.append(np.mean([p['nll_bare'] for p in rel_passages]))\n",
    "    per_query_exp25.append(np.mean([p['nll_exp25_fact'] for p in rel_passages]))\n",
    "    per_query_warm.append(np.mean([p['nll_contrastive_warm'] for p in rel_passages]))\n",
    "    per_query_cold.append(np.mean([p['nll_contrastive_cold'] for p in rel_passages]))\n",
    "\n",
    "pq_bare = np.array(per_query_bare)\n",
    "pq_exp25 = np.array(per_query_exp25)\n",
    "pq_warm = np.array(per_query_warm)\n",
    "pq_cold = np.array(per_query_cold)\n",
    "\n",
    "nll_conditions = {\n",
    "    'exp25_fact': pq_exp25,\n",
    "    'contrastive_warm': pq_warm,\n",
    "    'contrastive_cold': pq_cold,\n",
    "}\n",
    "\n",
    "print(f\"\\n{'Condition':<20} {'Mean NLL':>10} {'Delta':>10} {'d':>8} {'Win%':>7} {'p':>12}\")\n",
    "print(\"-\" * 72)\n",
    "print(f\"{'bare':<20} {np.mean(pq_bare):>10.4f}\")\n",
    "\n",
    "nll_improvement = {}\n",
    "for name, arr in nll_conditions.items():\n",
    "    delta = pq_bare - arr\n",
    "    d = cohens_d(delta)\n",
    "    win = np.mean(delta > 0) * 100\n",
    "    _, p_val = stats.ttest_1samp(delta, 0)\n",
    "    sig = '***' if p_val < 0.001 else '**' if p_val < 0.01 else '*' if p_val < 0.05 else 'ns'\n",
    "    print(f\"{name:<20} {np.mean(arr):>10.4f} {np.mean(delta):>+10.4f} \"\n",
    "          f\"{d:>+8.3f} {win:>6.1f}% {p_val:>12.2e} {sig}\")\n",
    "    nll_improvement[name] = {\n",
    "        'mean_nll': float(np.mean(arr)),\n",
    "        'mean_delta': float(np.mean(delta)),\n",
    "        'cohens_d': float(d),\n",
    "        'win_pct': float(win),\n",
    "        'p_value': float(p_val),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 16: Plots — ROC curves, score distributions, MRR scatter, summary bars\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "colors = {\n",
    "    'Raw bare': '#1f77b4',\n",
    "    'Raw exp25_fact': '#2ca02c',\n",
    "    'Raw contr_warm': '#ff7f0e',\n",
    "    'Raw contr_cold': '#d62728',\n",
    "    'PMI bare': '#1f77b4',\n",
    "    'PMI exp25_fact': '#2ca02c',\n",
    "    'PMI contr_warm': '#ff7f0e',\n",
    "    'PMI contr_cold': '#d62728',\n",
    "}\n",
    "\n",
    "# --- Panel 1: ROC curves (Raw NLL) ---\n",
    "ax = axes[0, 0]\n",
    "for name in ['Raw bare', 'Raw exp25_fact', 'Raw contr_warm', 'Raw contr_cold']:\n",
    "    scores = scoring_methods[name]\n",
    "    fpr, tpr, _ = roc_curve(is_relevant, -scores)\n",
    "    ax.plot(fpr, tpr, color=colors[name], linewidth=2,\n",
    "            label=f\"{name} (AUC={auc_results[name]:.3f})\")\n",
    "ax.plot([0, 1], [0, 1], 'k--', alpha=0.3)\n",
    "ax.set_xlabel('False Positive Rate')\n",
    "ax.set_ylabel('True Positive Rate')\n",
    "ax.set_title('ROC Curves — Raw NLL')\n",
    "ax.legend(fontsize=8, loc='lower right')\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "# --- Panel 2: ROC curves (PMI) ---\n",
    "ax = axes[0, 1]\n",
    "for name in ['PMI bare', 'PMI exp25_fact', 'PMI contr_warm', 'PMI contr_cold']:\n",
    "    scores = scoring_methods[name]\n",
    "    fpr, tpr, _ = roc_curve(is_relevant, -scores)\n",
    "    ax.plot(fpr, tpr, color=colors[name], linewidth=2,\n",
    "            label=f\"{name} (AUC={auc_results[name]:.3f})\")\n",
    "ax.plot([0, 1], [0, 1], 'k--', alpha=0.3)\n",
    "ax.set_xlabel('False Positive Rate')\n",
    "ax.set_ylabel('True Positive Rate')\n",
    "ax.set_title('ROC Curves — PMI')\n",
    "ax.legend(fontsize=8, loc='lower right')\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "# --- Panel 3: AUC + MRR summary bars ---\n",
    "ax = axes[1, 0]\n",
    "methods = ['Raw bare', 'Raw exp25_fact', 'Raw contr_warm', 'Raw contr_cold']\n",
    "method_labels = ['bare', 'exp25\\nfact', 'contr\\nwarm', 'contr\\ncold']\n",
    "aucs = [auc_results[m] for m in methods]\n",
    "mrrs = [mrr_results[m] for m in methods]\n",
    "\n",
    "x = np.arange(len(methods))\n",
    "width = 0.35\n",
    "ax.bar(x - width/2, aucs, width, label='AUC', color=[colors[m] for m in methods], alpha=0.7)\n",
    "ax.bar(x + width/2, mrrs, width, label='MRR@10', color=[colors[m] for m in methods], alpha=0.4,\n",
    "       edgecolor=[colors[m] for m in methods], linewidth=2)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(method_labels, fontsize=8)\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_title('AUC & MRR@10 (Raw NLL)')\n",
    "ax.legend(fontsize=8)\n",
    "ax.set_ylim(0.5, 1.0)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "for i, (a, m) in enumerate(zip(aucs, mrrs)):\n",
    "    ax.text(i - width/2, a + 0.005, f'{a:.3f}', ha='center', va='bottom', fontsize=7)\n",
    "    ax.text(i + width/2, m + 0.005, f'{m:.3f}', ha='center', va='bottom', fontsize=7)\n",
    "\n",
    "# --- Panel 4: NLL improvement (per-query d vs bare) ---\n",
    "ax = axes[1, 1]\n",
    "nll_names = ['exp25_fact', 'contrastive_warm', 'contrastive_cold']\n",
    "nll_labels = ['exp25\\nfact', 'contr\\nwarm', 'contr\\ncold']\n",
    "nll_ds = [nll_improvement[n]['cohens_d'] for n in nll_names]\n",
    "nll_colors = ['#2ca02c', '#ff7f0e', '#d62728']\n",
    "\n",
    "bars = ax.bar(range(len(nll_names)), nll_ds, color=nll_colors,\n",
    "              edgecolor='black', linewidth=0.5)\n",
    "ax.axhline(y=0, color='gray', linestyle='-', alpha=0.5)\n",
    "ax.set_xticks(range(len(nll_names)))\n",
    "ax.set_xticklabels(nll_labels, fontsize=8)\n",
    "ax.set_ylabel(\"Cohen's d vs bare\")\n",
    "ax.set_title('NLL Improvement (relevant passages only)')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "for i, d_val in enumerate(nll_ds):\n",
    "    sig = '***' if nll_improvement[nll_names[i]]['p_value'] < 0.001 else \\\n",
    "          '**' if nll_improvement[nll_names[i]]['p_value'] < 0.01 else \\\n",
    "          '*' if nll_improvement[nll_names[i]]['p_value'] < 0.05 else 'ns'\n",
    "    ax.text(i, d_val + 0.01 if d_val >= 0 else d_val - 0.02,\n",
    "            f\"{d_val:+.3f} {sig}\", ha='center',\n",
    "            va='bottom' if d_val >= 0 else 'top', fontsize=9)\n",
    "\n",
    "plt.suptitle('Exp 28: Contrastive Ranking Soft Prefix', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'ranking_plots.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"Saved to {RESULTS_DIR / 'ranking_plots.png'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 17: Save results.json + CSV + final verdict\n",
    "\n",
    "# --- CSV ---\n",
    "csv_rows = []\n",
    "for r in all_results:\n",
    "    for p in r['passage_data']:\n",
    "        csv_rows.append({\n",
    "            'query_idx': r['query_idx'],\n",
    "            'passage_idx': p['passage_idx'],\n",
    "            'is_relevant': int(p['is_relevant']),\n",
    "            'nll_bare': p['nll_bare'],\n",
    "            'nll_exp25_fact': p['nll_exp25_fact'],\n",
    "            'nll_contrastive_warm': p['nll_contrastive_warm'],\n",
    "            'nll_contrastive_cold': p['nll_contrastive_cold'],\n",
    "            'nll_baseline': p['nll_baseline'],\n",
    "            'pmi_bare': p['nll_bare'] - p['nll_baseline'],\n",
    "            'pmi_exp25': p['nll_exp25_fact'] - p['nll_baseline'],\n",
    "            'pmi_warm': p['nll_contrastive_warm'] - p['nll_baseline'],\n",
    "            'pmi_cold': p['nll_contrastive_cold'] - p['nll_baseline'],\n",
    "        })\n",
    "\n",
    "with open(CSV_EVAL_PATH, 'w', newline='') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=csv_rows[0].keys())\n",
    "    writer.writeheader()\n",
    "    writer.writerows(csv_rows)\n",
    "print(f\"CSV saved: {CSV_EVAL_PATH} ({len(csv_rows)} rows)\")\n",
    "\n",
    "# --- Results JSON ---\n",
    "final = {\n",
    "    'experiment': 'exp28_contrastive_ranking_soft_prefix',\n",
    "    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'config': {\n",
    "        'model_name': MODEL_NAME,\n",
    "        'model_type': 'gemma3',\n",
    "        'seed': SEED,\n",
    "        'cutoff': CUTOFF,\n",
    "        'prefix_len': PREFIX_LEN,\n",
    "        'hidden_size': HIDDEN_SIZE,\n",
    "        'trainable_params': PREFIX_LEN * HIDDEN_SIZE,\n",
    "        'training': {\n",
    "            'margin': MARGIN,\n",
    "            'lr': LR,\n",
    "            'n_epochs': N_EPOCHS,\n",
    "            'grad_accum': GRAD_ACCUM,\n",
    "            'warmup_steps': WARMUP_STEPS,\n",
    "            'n_train_queries': N_TRAIN,\n",
    "            'loss': 'hinge: max(0, margin + NLL_rel - NLL_irrel)',\n",
    "        },\n",
    "        'eval': {\n",
    "            'n_queries': N,\n",
    "            'total_passages': total_val_passages,\n",
    "            'n_relevant': total_val_rel,\n",
    "            'conditions': ['bare', 'exp25_fact', 'contrastive_warm', 'contrastive_cold', 'baseline'],\n",
    "        },\n",
    "    },\n",
    "    'training_history': {\n",
    "        'warm': history_warm if 'history_warm' in dir() else [],\n",
    "        'cold': history_cold if 'history_cold' in dir() else [],\n",
    "    },\n",
    "    'ranking_analysis': {\n",
    "        'auc': auc_results,\n",
    "        'mrr_at_10': mrr_results,\n",
    "        'differential_nll': diff_results,\n",
    "    },\n",
    "    'nll_improvement': nll_improvement,\n",
    "    'reference_exp22': EXP22_REF,\n",
    "    'per_query_results': all_results,\n",
    "}\n",
    "\n",
    "with open(FINAL_RESULTS_PATH, 'w') as f:\n",
    "    json.dump(final, f, indent=2)\n",
    "print(f\"Results saved: {FINAL_RESULTS_PATH} ({FINAL_RESULTS_PATH.stat().st_size / 1024:.1f} KB)\")\n",
    "\n",
    "# --- Final verdict ---\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"FINAL VERDICT — Exp 28: Contrastive Ranking Soft Prefix\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\nModel: Gemma 3 4B | Cutoff: {CUTOFF} | Margin: {MARGIN}\")\n",
    "print(f\"Training: {N_TRAIN} queries x {N_EPOCHS} epochs, lr={LR}\")\n",
    "print(f\"Eval: {N} queries, {total_val_passages} passages\")\n",
    "\n",
    "print(f\"\\nRanking Results (Raw NLL):\")\n",
    "print(f\"  bare AUC:         {auc_results['Raw bare']:.3f} (ref: {EXP22_REF['raw_bare_auc']:.3f})\")\n",
    "print(f\"  exp25_fact AUC:   {auc_results['Raw exp25_fact']:.3f}\")\n",
    "print(f\"  contr_warm AUC:   {auc_results['Raw contr_warm']:.3f}\")\n",
    "print(f\"  contr_cold AUC:   {auc_results['Raw contr_cold']:.3f}\")\n",
    "\n",
    "print(f\"\\nRanking Results (PMI):\")\n",
    "print(f\"  bare PMI AUC:     {auc_results['PMI bare']:.3f} (ref: {EXP22_REF['pmi_bare_auc']:.3f})\")\n",
    "print(f\"  exp25 PMI AUC:    {auc_results['PMI exp25_fact']:.3f}\")\n",
    "print(f\"  warm PMI AUC:     {auc_results['PMI contr_warm']:.3f}\")\n",
    "print(f\"  cold PMI AUC:     {auc_results['PMI contr_cold']:.3f}\")\n",
    "\n",
    "print(f\"\\nNLL Improvement (relevant passages, d vs bare):\")\n",
    "for name, data in nll_improvement.items():\n",
    "    sig = '***' if data['p_value'] < 0.001 else '**' if data['p_value'] < 0.01 else \\\n",
    "          '*' if data['p_value'] < 0.05 else 'ns'\n",
    "    print(f\"  {name:<20} d={data['cohens_d']:+.3f}, win={data['win_pct']:.0f}% {sig}\")\n",
    "\n",
    "# Determine verdict\n",
    "best_raw_auc = max(auc_results['Raw contr_warm'], auc_results['Raw contr_cold'])\n",
    "best_pmi_auc = max(auc_results['PMI contr_warm'], auc_results['PMI contr_cold'])\n",
    "\n",
    "ranking_improved = (best_raw_auc > 0.835) or (best_pmi_auc > 0.845)\n",
    "nll_still_helps = any(d['cohens_d'] > 0 for d in nll_improvement.values())\n",
    "\n",
    "if ranking_improved:\n",
    "    print(f\"\\nVERDICT: Contrastive training IMPROVES ranking!\")\n",
    "    print(f\"  Best raw AUC: {best_raw_auc:.3f} (target: >0.835)\")\n",
    "    print(f\"  Best PMI AUC: {best_pmi_auc:.3f} (target: >0.845)\")\n",
    "    print(f\"  Value contamination CAN create differential ranking signal.\")\n",
    "else:\n",
    "    print(f\"\\nVERDICT: Contrastive training FAILS to improve ranking.\")\n",
    "    print(f\"  Best raw AUC: {best_raw_auc:.3f} (target: >0.835, bare: {auc_results['Raw bare']:.3f})\")\n",
    "    print(f\"  Best PMI AUC: {best_pmi_auc:.3f} (target: >0.845, bare: {auc_results['PMI bare']:.3f})\")\n",
    "    print(f\"  CONFIRMS: Document-independent prefix cannot create query-specific\")\n",
    "    print(f\"  relevance discrimination, even with ranking-aware training.\")\n",
    "\n",
    "if nll_still_helps:\n",
    "    print(f\"  Contrastive prefix still helps average NLL (secondary success).\")\n",
    "else:\n",
    "    print(f\"  Contrastive training also HURTS average NLL (full failure).\")\n",
    "\n",
    "print(f\"\\nDone!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 18: GPU cleanup\n",
    "\n",
    "print(\"Cleaning up GPU memory...\")\n",
    "mem_before = torch.cuda.memory_allocated() / 1e9\n",
    "\n",
    "del model\n",
    "del tokenizer\n",
    "for var_name in ['soft_contrastive_warm', 'soft_contrastive_cold', 'soft_exp25_fact']:\n",
    "    if var_name in dir():\n",
    "        exec(f'del {var_name}')\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "mem_after = torch.cuda.memory_allocated() / 1e9\n",
    "print(f\"GPU memory: {mem_before:.2f} GB -> {mem_after:.2f} GB\")\n",
    "print(\"Cleanup complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}