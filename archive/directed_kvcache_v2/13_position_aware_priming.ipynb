{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exp 13: Position-Aware Value Contamination for Long Documents\n",
    "\n",
    "## Background & Motivation\n",
    "\n",
    "Exp 12 tested three hypotheses for why priming fails on long documents:\n",
    "- Signal dilution (repetition) \u2192 REFUTED (more reps = worse)\n",
    "- Amplification \u2192 Partially promising (amplify_2x d=+0.090, best overall)\n",
    "- RoPE interference \u2192 REFUTED (suffix/no_rope both hurt)\n",
    "- Layer targeting (0-15) \u2192 Second best (d=+0.083)\n",
    "\n",
    "**But diagnostic analysis of the per-sample data revealed something deeper:**\n",
    "\n",
    "### The Answer Position Finding\n",
    "\n",
    "| Answer Location | % of Samples | static_fact d | Priming Effect |\n",
    "|-----------------|-------------|---------------|----------------|\n",
    "| First 25% of doc | **80%** | **-0.052** | **HURTS** |\n",
    "| Later 75% of doc | 20% | **+0.145** | **HELPS** |\n",
    "\n",
    "The contamination mechanism IS working \u2014 it helps when answers are far from\n",
    "the heavily-contaminated early positions. The problem is that **80% of NQ answers\n",
    "sit in the first 25% of the document, exactly where contamination is strongest.**\n",
    "\n",
    "### The Asymmetry Finding\n",
    "\n",
    "The delta distribution has extreme kurtosis (73-227):\n",
    "- Most samples: tiny positive or negative delta (near zero)\n",
    "- A few outliers: catastrophically harmed (\u0394 = -0.5 to -1.5)\n",
    "- Win rate is 65% but mean is negative because hurt magnitude is 2.16x help magnitude\n",
    "\n",
    "For `layers_0_15`: hurt magnitude is only 0.42x of help magnitude (best ratio).\n",
    "For `amplify_2x`: hurt magnitude is 0.55x of help magnitude.\n",
    "\n",
    "### The Correlation Finding\n",
    "\n",
    "`layers_0_15` and `amplify_2x` have r=0.982 per-sample correlation \u2014 they are\n",
    "essentially doing the same thing. This makes sense because the delta at layers\n",
    "16-31 is ~0 (Exp 09), so both operations effectively \"keep delta at layers 0-15.\"\n",
    "\n",
    "## This Experiment: Position-Selective Contamination\n",
    "\n",
    "**Core idea:** Instead of uniform contamination, modulate the contamination delta\n",
    "by position \u2014 reduce or eliminate it at early positions (where answers live) and\n",
    "optionally boost it at later positions (where it helps).\n",
    "\n",
    "10 conditions, all derived from just 2 forward passes (bare + primed) per sample."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 1: Setup\n",
    "import os\n",
    "os.umask(0o000)\n",
    "\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import gc\n",
    "from pathlib import Path\n",
    "\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "RESULTS_DIR = Path(\"results/exp13\")\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CHECKPOINT_PATH = RESULTS_DIR / \"checkpoint.json\"\n",
    "FINAL_RESULTS_PATH = RESULTS_DIR / \"results.json\"\n",
    "\n",
    "print(f\"SEED: {SEED}\")\n",
    "print(f\"Results directory: {RESULTS_DIR}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 2: Load model (Mistral-7B 4-bit)\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, DynamicCache\n",
    "\n",
    "MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "print(f\"Loading {MODEL_NAME} (4-bit)...\")\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "print(f\"Model loaded. dtype={model.dtype}, device={model.device}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 3: Config and library imports\n",
    "sys.path.insert(0, \".\")\n",
    "\n",
    "from lib.config import ExperimentConfig\n",
    "from lib.kv_cache import (\n",
    "    _get_cache_keys,\n",
    "    _get_cache_values,\n",
    "    _set_cache_keys,\n",
    "    _set_cache_values,\n",
    "    _ensure_dynamic_cache,\n",
    "    extract_and_truncate_cache_with_bos,\n",
    "    correct_rope_positions_with_bos,\n",
    "    score_answer_with_cache,\n",
    "    deepcopy_cache,\n",
    "    replace_values_at_layers,\n",
    "    replace_values_at_positions,\n",
    "    interpolate_values,\n",
    "    build_hybrid_cache,\n",
    ")\n",
    "from lib.analysis import cohens_d\n",
    "from lib.surrogate import STATIC_SURROGATE_QUERIES\n",
    "from scipy import stats\n",
    "from scipy.stats import spearmanr\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "config = ExperimentConfig(\n",
    "    model_name=MODEL_NAME,\n",
    "    num_samples=2000,\n",
    "    seed=SEED,\n",
    ")\n",
    "\n",
    "SURROGATE_PREFIX_TEMPLATE = \"{surrogate}\\n\"\n",
    "DOCUMENT_TEMPLATE = \"{document}\"\n",
    "QUERY_TEMPLATE = \"\\nQuery: {query}\\nAnswer:\"\n",
    "ANSWER_TEMPLATE = \" {answer}\"\n",
    "\n",
    "N_CONDITIONS = 10\n",
    "N_COMPARISONS = 9  # each non-bare vs bare\n",
    "BONFERRONI_ALPHA = 0.05 / N_COMPARISONS\n",
    "CHECKPOINT_EVERY = 25\n",
    "DELTA_FORENSICS_EVERY = 50  # log delta diagnostics for every Nth sample\n",
    "\n",
    "STATIC_FACT = STATIC_SURROGATE_QUERIES['static_factual']['query']\n",
    "\n",
    "LENGTH_BINS = [\n",
    "    ('short',     100,  300),\n",
    "    ('medium',    300,  800),\n",
    "    ('long',      800,  2000),\n",
    "    ('very_long', 2000, 4000),\n",
    "]\n",
    "\n",
    "CONDITION_NAMES = [\n",
    "    'bare',\n",
    "    'standard_1x',\n",
    "    'layers_0_15_amp2x',\n",
    "    'layers_0_15_amp3x',\n",
    "    'pos_normalized',\n",
    "    'attenuate_first_25',\n",
    "    'skip_first_25',\n",
    "    'last_50_only',\n",
    "    'window_25_75',\n",
    "    'pos_norm_L0_15',\n",
    "]\n",
    "\n",
    "print(\"Config ready\")\n",
    "print(f\"  N_CONDITIONS: {N_CONDITIONS}\")\n",
    "print(f\"  bonferroni_alpha: {BONFERRONI_ALPHA:.4f} ({N_COMPARISONS} comparisons)\")\n",
    "print(f\"  static_fact: '{STATIC_FACT}'\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 4: Load NQ samples (reuse exp 12's cached samples)\n",
    "print(\"=\" * 70)\n",
    "print(\"LOADING NATURAL QUESTIONS SAMPLES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "EXP12_SAMPLES_PATH = Path(\"results/exp12/nq_samples.json\")\n",
    "\n",
    "if not EXP12_SAMPLES_PATH.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Exp 12 samples not found at {EXP12_SAMPLES_PATH}. \"\n",
    "        \"Run exp 12 first.\"\n",
    "    )\n",
    "\n",
    "with open(EXP12_SAMPLES_PATH, 'r') as f:\n",
    "    cached = json.load(f)\n",
    "samples = cached['samples']\n",
    "N = len(samples)\n",
    "\n",
    "print(f\"Loaded {N} NQ samples from {EXP12_SAMPLES_PATH}\")\n",
    "print(f\"\\nSample distribution:\")\n",
    "for bin_name, bin_min, bin_max in LENGTH_BINS:\n",
    "    bin_s = [s for s in samples if s['length_bin'] == bin_name]\n",
    "    if bin_s:\n",
    "        wcs = [s['word_count'] for s in bin_s]\n",
    "        print(f\"  {bin_name} ({bin_min}-{bin_max}w): n={len(bin_s)}, \"\n",
    "              f\"mean={np.mean(wcs):.0f}w\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 5: Explain experimental conditions\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"EXPERIMENTAL CONDITIONS \u2014 POSITION-AWARE CONTAMINATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "sf_str = SURROGATE_PREFIX_TEMPLATE.format(surrogate=STATIC_FACT)\n",
    "sf_ids = tokenizer(sf_str, add_special_tokens=False)['input_ids']\n",
    "sf_tok_len = len(sf_ids)\n",
    "\n",
    "print(f\"\\nAll conditions use static_fact prefix ({sf_tok_len} tokens).\")\n",
    "print(\"All use bare keys. Only VALUES are modified (Exp 08: values carry 100% of signal).\")\n",
    "print(\"All are derived from 2 forward passes: bare + standard 1x primed.\")\n",
    "print(f\"\\nDiagram of a 1000-word NQ document (~1500 tokens):\")\n",
    "print()\n",
    "print(\"Position:     0     25%      50%      75%    100%\")\n",
    "print(\"              |------|--------|--------|-------|\")\n",
    "print(\"standard_1x:  \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591  (decay from left)\")\n",
    "print(\"pos_norm:     \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 (uniform)\")\n",
    "print(\"atten_25:     \u2591\u2591\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591  (reduce first 25%)\")\n",
    "print(\"skip_25:      \u00b7\u00b7\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591  (zero first 25%)\")\n",
    "print(\"last_50:      \u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  (only last 50%)\")\n",
    "print(\"window_25_75: \u00b7\u00b7\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u00b7\u00b7      (middle 50%)\")\n",
    "print()\n",
    "print(\"\u2588 = full delta  \u2591 = reduced delta  \u00b7 = zero delta\")\n",
    "\n",
    "conditions_explained = [\n",
    "    (\"1. bare\",\n",
    "     \"Baseline \u2014 no prefix\",\n",
    "     \"\u2014\"),\n",
    "    (\"2. standard_1x\",\n",
    "     \"Standard static_fact_trunc. Natural decay: first tokens get ~85% contamination \"\n",
    "     \"from prefix attention, last tokens get <1%. Replicates exp 12 prefix_1x.\",\n",
    "     \"Reference for position-modulated conditions\"),\n",
    "    (\"3. layers_0_15_amp2x\",\n",
    "     \"Amplify delta by 2x, but ONLY at layers 0-15 (layers 16-31 get bare values). \"\n",
    "     \"Combines the two best Exp 12 approaches.\",\n",
    "     \"Tests: Do amplification + layer targeting have independent benefits? \"\n",
    "     \"(r=0.982 in Exp 12 suggests they're nearly identical)\"),\n",
    "    (\"4. layers_0_15_amp3x\",\n",
    "     \"Same as above but 3x amplification. Pushes further to find the sweet spot.\",\n",
    "     \"Tests: Is 2x undershoot? Does 3x over-amplify?\"),\n",
    "    (\"5. pos_normalized\",\n",
    "     \"Normalize the delta at each position to the MEDIAN per-position L2 norm. \"\n",
    "     \"Early positions (over-contaminated) get reduced. Late positions (under-contaminated) \"\n",
    "     \"get amplified. Net contamination 'dose' stays the same but distributed evenly.\",\n",
    "     \"KEY TEST: If contamination DIRECTION is correct everywhere but MAGNITUDE is wrong, \"\n",
    "     \"normalization should fix it. This is the cleanest test of 'position-dependent dose.'\"),\n",
    "    (\"6. attenuate_first_25\",\n",
    "     \"Scale delta by 0.25 at first 25% of positions. Keep full delta elsewhere. \"\n",
    "     \"A gentler version of 'skip' \u2014 we know the first 25% has the answer.\",\n",
    "     \"Tests: Does REDUCING (not eliminating) early contamination help?\"),\n",
    "    (\"7. skip_first_25\",\n",
    "     \"Set delta to ZERO at first 25% of positions. Full delta elsewhere. \"\n",
    "     \"Since 80% of answers are in the first 25%, this protects the answer region.\",\n",
    "     \"KEY TEST: If early contamination hurts, zeroing it should flip the effect positive.\"),\n",
    "    (\"8. last_50_only\",\n",
    "     \"Delta only at positions 50-100% (last half). First half gets bare values. \"\n",
    "     \"Most aggressive answer-region protection.\",\n",
    "     \"Tests: Can contamination help even with zero dose in the answer region?\"),\n",
    "    (\"9. window_25_75\",\n",
    "     \"Delta only at positions 25-75% (middle half). Protects BOTH early (answer) \"\n",
    "     \"and late (possibly low-quality at extremes) positions.\",\n",
    "     \"Tests: Is there an optimal position window for contamination?\"),\n",
    "    (\"10. pos_norm_L0_15\",\n",
    "     \"Position-normalized + layers 0-15 only. The 'kitchen sink' \u2014 combines the \"\n",
    "     \"best layer targeting with position normalization.\",\n",
    "     \"Tests: Do position + layer targeting compound?\"),\n",
    "]\n",
    "\n",
    "for name, detail, test in conditions_explained:\n",
    "    print(f\"\\n### {name} ###\")\n",
    "    print(f\"  {detail}\")\n",
    "    print(f\"  Test: {test}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 6: Position-aware cache manipulation functions\n",
    "\n",
    "def compute_delta(bare_cache, primed_cache, layers=None):\n",
    "    \"\"\"Compute per-position value delta between primed and bare caches.\n",
    "\n",
    "    Returns dict mapping layer_idx -> delta tensor (same shape as values).\n",
    "    If layers is specified, only compute for those layers.\n",
    "    \"\"\"\n",
    "    bare_cache = _ensure_dynamic_cache(bare_cache)\n",
    "    primed_cache = _ensure_dynamic_cache(primed_cache)\n",
    "    n_layers = len(bare_cache)\n",
    "\n",
    "    deltas = {}\n",
    "    layer_range = layers if layers is not None else range(n_layers)\n",
    "    for li in layer_range:\n",
    "        v_bare = _get_cache_values(bare_cache, li)\n",
    "        v_primed = _get_cache_values(primed_cache, li)\n",
    "        deltas[li] = v_primed - v_bare\n",
    "\n",
    "    return deltas\n",
    "\n",
    "\n",
    "def apply_delta(bare_cache, deltas, scale=1.0, pos_start=None, pos_end=None):\n",
    "    \"\"\"Apply value delta to bare cache with optional scaling and position range.\n",
    "\n",
    "    Args:\n",
    "        bare_cache: Cache with correct keys and uncontaminated values\n",
    "        deltas: Dict mapping layer_idx -> delta tensor\n",
    "        scale: Scalar or per-position tensor to multiply delta by\n",
    "        pos_start: First position to modify (inclusive, 0-indexed in BOS+doc)\n",
    "        pos_end: Last position to modify (exclusive)\n",
    "\n",
    "    Returns:\n",
    "        New DynamicCache with bare keys and modified values\n",
    "    \"\"\"\n",
    "    bare_cache = _ensure_dynamic_cache(bare_cache)\n",
    "    n_layers = len(bare_cache)\n",
    "    new_cache = DynamicCache()\n",
    "\n",
    "    for li in range(n_layers):\n",
    "        k = _get_cache_keys(bare_cache, li)  # keys never modified \u2014 share, don't clone\n",
    "        v = _get_cache_values(bare_cache, li).clone()\n",
    "\n",
    "        if li in deltas:\n",
    "            delta = deltas[li]\n",
    "            if isinstance(scale, torch.Tensor):\n",
    "                # Per-position scaling: scale shape should be (1, 1, seq_len, 1)\n",
    "                # or broadcastable to delta shape\n",
    "                scaled_delta = delta * scale\n",
    "            else:\n",
    "                scaled_delta = delta * scale\n",
    "\n",
    "            if pos_start is not None or pos_end is not None:\n",
    "                ps = pos_start if pos_start is not None else 0\n",
    "                pe = pos_end if pos_end is not None else v.shape[2]\n",
    "                v[:, :, ps:pe, :] += scaled_delta[:, :, ps:pe, :]\n",
    "            else:\n",
    "                v += scaled_delta\n",
    "\n",
    "        new_cache.update(k, v, li)\n",
    "\n",
    "    return new_cache\n",
    "\n",
    "\n",
    "def compute_position_norms(deltas, start_pos=1):\n",
    "    \"\"\"Compute L2 norm of delta at each position (averaged across layers).\n",
    "\n",
    "    Args:\n",
    "        deltas: Dict mapping layer_idx -> delta tensor (1, n_heads, seq_len, head_dim)\n",
    "        start_pos: Skip BOS (position 0)\n",
    "\n",
    "    Returns:\n",
    "        Tensor of shape (seq_len,) with per-position L2 norms averaged across layers\n",
    "    \"\"\"\n",
    "    norms_list = []\n",
    "    for li, delta in deltas.items():\n",
    "        # L2 norm across head_dim, averaged across heads\n",
    "        # delta shape: (1, n_heads, seq_len, head_dim)\n",
    "        pos_norms = torch.norm(delta[0], dim=-1).mean(dim=0)  # (seq_len,)\n",
    "        norms_list.append(pos_norms)\n",
    "\n",
    "    avg_norms = torch.stack(norms_list).mean(dim=0)  # (seq_len,)\n",
    "    return avg_norms\n",
    "\n",
    "\n",
    "def position_normalize_delta(deltas, target_norm=None, start_pos=1, eps=1e-8):\n",
    "    \"\"\"Normalize delta to constant per-position L2 norm.\n",
    "\n",
    "    For each position, scale the delta so its L2 norm equals target_norm.\n",
    "    Default target_norm = median of non-BOS position norms.\n",
    "\n",
    "    Args:\n",
    "        deltas: Dict mapping layer_idx -> delta tensor\n",
    "        target_norm: Target L2 norm per position (default: median)\n",
    "        start_pos: Position to start normalization (skip BOS)\n",
    "        eps: Small constant to avoid division by zero\n",
    "\n",
    "    Returns:\n",
    "        New deltas dict with normalized values, plus the target_norm used\n",
    "    \"\"\"\n",
    "    # Compute per-position norms for each layer\n",
    "    normalized = {}\n",
    "\n",
    "    # First pass: compute all position norms per layer\n",
    "    layer_norms = {}\n",
    "    for li, delta in deltas.items():\n",
    "        # (1, n_heads, seq_len, head_dim) -> per-position norm\n",
    "        # Norm across head_dim for each head, then average across heads\n",
    "        pos_norms = torch.norm(delta[0], dim=-1).mean(dim=0)  # (seq_len,)\n",
    "        layer_norms[li] = pos_norms\n",
    "\n",
    "    # Compute target from average across layers\n",
    "    avg_norms = torch.stack(list(layer_norms.values())).mean(dim=0)\n",
    "    if target_norm is None:\n",
    "        # Use median of doc positions (skip BOS at 0)\n",
    "        target_norm = float(torch.median(avg_norms[start_pos:]).item())\n",
    "\n",
    "    # Second pass: normalize each layer's delta\n",
    "    for li, delta in deltas.items():\n",
    "        pos_norms = layer_norms[li]  # (seq_len,)\n",
    "        # Scale factor per position: target / current_norm\n",
    "        scale = torch.ones_like(pos_norms)\n",
    "        # Only normalize doc positions (not BOS)\n",
    "        for p in range(start_pos, len(pos_norms)):\n",
    "            if pos_norms[p] > eps:\n",
    "                scale[p] = target_norm / pos_norms[p]\n",
    "            else:\n",
    "                scale[p] = 0.0  # Zero delta stays zero\n",
    "\n",
    "        # Reshape for broadcasting: (1, 1, seq_len, 1)\n",
    "        scale_4d = scale.unsqueeze(0).unsqueeze(0).unsqueeze(-1)\n",
    "        normalized[li] = delta * scale_4d.to(delta.device, dtype=delta.dtype)\n",
    "\n",
    "    return normalized, target_norm\n",
    "\n",
    "\n",
    "def build_position_scaled_cache(bare_cache, deltas, scale_fn, n_layers_model):\n",
    "    \"\"\"Build cache with position-dependent scaling applied to deltas.\n",
    "\n",
    "    Args:\n",
    "        bare_cache: Uncontaminated cache\n",
    "        deltas: Dict mapping layer_idx -> delta tensor\n",
    "        scale_fn: Function(position_idx, seq_len) -> scale factor\n",
    "        n_layers_model: Total number of model layers\n",
    "\n",
    "    Returns:\n",
    "        New DynamicCache\n",
    "    \"\"\"\n",
    "    bare_cache = _ensure_dynamic_cache(bare_cache)\n",
    "    new_cache = DynamicCache()\n",
    "\n",
    "    # Pre-compute scale factors\n",
    "    sample_delta = next(iter(deltas.values()))\n",
    "    seq_len = sample_delta.shape[2]\n",
    "    scales = torch.tensor([scale_fn(p, seq_len) for p in range(seq_len)],\n",
    "                          dtype=sample_delta.dtype, device=sample_delta.device)\n",
    "    scales_4d = scales.unsqueeze(0).unsqueeze(0).unsqueeze(-1)  # (1, 1, seq_len, 1)\n",
    "\n",
    "    for li in range(n_layers_model):\n",
    "        k = _get_cache_keys(bare_cache, li)  # keys never modified \u2014 share, don't clone\n",
    "        v = _get_cache_values(bare_cache, li).clone()\n",
    "\n",
    "        if li in deltas:\n",
    "            v = v + deltas[li] * scales_4d\n",
    "\n",
    "        new_cache.update(k, v, li)\n",
    "\n",
    "    return new_cache\n",
    "\n",
    "\n",
    "print(\"Helper functions defined:\")\n",
    "print(\"  compute_delta(bare, primed, layers=None) -> dict\")\n",
    "print(\"  apply_delta(bare, deltas, scale, pos_start, pos_end) -> cache\")\n",
    "print(\"  compute_position_norms(deltas) -> tensor\")\n",
    "print(\"  position_normalize_delta(deltas, target_norm) -> (norm_deltas, target)\")\n",
    "print(\"  build_position_scaled_cache(bare, deltas, scale_fn, n_layers) -> cache\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 7: Main evaluation loop \u2014 10 conditions, 2 forward passes per sample\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"MAIN EVALUATION ({N_CONDITIONS} conditions x {N} samples)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Pre-tokenize prefix\n",
    "sf_prefix_str = SURROGATE_PREFIX_TEMPLATE.format(surrogate=STATIC_FACT)\n",
    "sf_prefix_enc = tokenizer(sf_prefix_str, return_tensors=\"pt\",\n",
    "                           add_special_tokens=False, padding=False, truncation=False)\n",
    "sf_prefix_ids = sf_prefix_enc['input_ids'].to(config.device)\n",
    "sf_prefix_len = sf_prefix_ids.shape[1]\n",
    "\n",
    "print(f\"Static fact prefix: '{STATIC_FACT}' ({sf_prefix_len} tokens)\")\n",
    "print(f\"Forward passes per sample: 2 (bare + primed)\")\n",
    "print(f\"All other conditions derived via delta manipulation (no extra forward passes)\")\n",
    "\n",
    "# Checkpoint resume\n",
    "results = []\n",
    "delta_forensics = []\n",
    "start_idx = 0\n",
    "\n",
    "if CHECKPOINT_PATH.exists():\n",
    "    with open(CHECKPOINT_PATH, 'r') as f:\n",
    "        ckpt = json.load(f)\n",
    "    ckpt_queries = ckpt.get('sample_queries', [])\n",
    "    current_queries = [s['query'] for s in samples]\n",
    "    if ckpt_queries == current_queries:\n",
    "        results = ckpt['results']\n",
    "        delta_forensics = ckpt.get('delta_forensics', [])\n",
    "        start_idx = len(results)\n",
    "        print(f\"Resuming from checkpoint: {start_idx}/{N}\")\n",
    "    else:\n",
    "        print(\"Checkpoint sample mismatch. Starting fresh.\")\n",
    "else:\n",
    "    print(\"No checkpoint found. Starting fresh.\")\n",
    "\n",
    "print(f\"Evaluating samples {start_idx} to {N-1}\")\n",
    "n_layers = model.config.num_hidden_layers\n",
    "\n",
    "t_start = time.time()\n",
    "\n",
    "for idx in tqdm(range(start_idx, N), initial=start_idx, total=N, desc=\"Evaluating\"):\n",
    "    sample = samples[idx]\n",
    "    passage = sample['passage']\n",
    "    query = sample['query']\n",
    "    answer = sample['answer']\n",
    "    word_count = sample['word_count']\n",
    "    length_bin = sample['length_bin']\n",
    "\n",
    "    query_prompt = QUERY_TEMPLATE.format(query=query)\n",
    "    answer_text = ANSWER_TEMPLATE.format(answer=answer)\n",
    "\n",
    "    # --- Matched tokenization ---\n",
    "    oracle_prefix = SURROGATE_PREFIX_TEMPLATE.format(surrogate=query)\n",
    "    document_text = DOCUMENT_TEMPLATE.format(document=passage)\n",
    "    full_oracle_text = oracle_prefix + document_text\n",
    "\n",
    "    full_oracle_enc = tokenizer(full_oracle_text, return_tensors=\"pt\",\n",
    "                                add_special_tokens=True, padding=False, truncation=False)\n",
    "    full_oracle_ids = full_oracle_enc['input_ids'].to(config.device)\n",
    "\n",
    "    oracle_prefix_enc = tokenizer(oracle_prefix, return_tensors=\"pt\",\n",
    "                                  add_special_tokens=True, padding=False, truncation=False)\n",
    "    oracle_prefix_len = oracle_prefix_enc['input_ids'].shape[1]\n",
    "\n",
    "    bos_id = full_oracle_ids[:, :1]\n",
    "    doc_ids = full_oracle_ids[:, oracle_prefix_len:]\n",
    "    doc_len = doc_ids.shape[1]\n",
    "    context_len = 1 + doc_len  # BOS + doc\n",
    "\n",
    "    # =================================================================\n",
    "    # PHASE 1: Build bare and primed caches (2 forward passes)\n",
    "    # =================================================================\n",
    "    bare_ids = torch.cat([bos_id, doc_ids], dim=1)\n",
    "    with torch.no_grad():\n",
    "        bare_out = model(input_ids=bare_ids, attention_mask=torch.ones_like(bare_ids),\n",
    "                         use_cache=True, return_dict=True)\n",
    "    bare_cache = _ensure_dynamic_cache(bare_out.past_key_values)\n",
    "    del bare_out\n",
    "\n",
    "    # Use static_fact prefix (matched tokenization: build from sf_prefix + same doc_ids)\n",
    "    primed_ids = torch.cat([bos_id, sf_prefix_ids, doc_ids], dim=1)\n",
    "    with torch.no_grad():\n",
    "        primed_out = model(input_ids=primed_ids,\n",
    "                           attention_mask=torch.ones_like(primed_ids),\n",
    "                           use_cache=True, return_dict=True)\n",
    "    primed_full = _ensure_dynamic_cache(primed_out.past_key_values)\n",
    "    del primed_out\n",
    "\n",
    "    # Truncate + RoPE correct -> standard primed cache\n",
    "    primed_trunc = extract_and_truncate_cache_with_bos(primed_full, doc_len)\n",
    "    correct_rope_positions_with_bos(primed_trunc, sf_prefix_len, model)\n",
    "    del primed_full\n",
    "\n",
    "    del bare_ids, primed_ids\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # =================================================================\n",
    "    # PHASE 2: Compute deltas (all layers, then layer-specific)\n",
    "    # =================================================================\n",
    "    deltas_all = compute_delta(bare_cache, primed_trunc)\n",
    "    deltas_0_15 = {li: deltas_all[li] for li in range(16)}\n",
    "\n",
    "    # =================================================================\n",
    "    # PHASE 3: Build all position-variant caches from deltas\n",
    "    # =================================================================\n",
    "    # Positions: BOS is at 0, doc tokens at 1..doc_len\n",
    "    # \"first 25%\" = positions 1..floor(doc_len*0.25)\n",
    "    p25 = 1 + max(1, int(doc_len * 0.25))  # position index for 25% boundary\n",
    "    p50 = 1 + max(1, int(doc_len * 0.50))\n",
    "    p75 = 1 + max(1, int(doc_len * 0.75))\n",
    "    p_end = 1 + doc_len\n",
    "\n",
    "    # =================================================================\n",
    "    # PHASE 3+4: Build, score, and free each condition one at a time\n",
    "    # (Avoids holding all 9 derived caches in GPU memory simultaneously)\n",
    "    # bare_cache must stay unmutated until all derived caches are scored,\n",
    "    # so we score it LAST.\n",
    "    # =================================================================\n",
    "\n",
    "    # 2. standard_1x\n",
    "    cache = build_hybrid_cache(bare_cache, primed_trunc)\n",
    "    nll_standard = score_answer_with_cache(\n",
    "        cache, context_len, query_prompt, answer_text, model, tokenizer, config)\n",
    "    del cache, primed_trunc\n",
    "\n",
    "    # 3. layers_0_15_amp2x\n",
    "    amp_deltas = {li: d * 2.0 for li, d in deltas_0_15.items()}\n",
    "    cache = apply_delta(bare_cache, amp_deltas)\n",
    "    nll_l15_a2 = score_answer_with_cache(\n",
    "        cache, context_len, query_prompt, answer_text, model, tokenizer, config)\n",
    "    del cache, amp_deltas\n",
    "\n",
    "    # 4. layers_0_15_amp3x\n",
    "    amp_deltas = {li: d * 3.0 for li, d in deltas_0_15.items()}\n",
    "    cache = apply_delta(bare_cache, amp_deltas)\n",
    "    nll_l15_a3 = score_answer_with_cache(\n",
    "        cache, context_len, query_prompt, answer_text, model, tokenizer, config)\n",
    "    del cache, amp_deltas\n",
    "\n",
    "    # 5. pos_normalized (compute norm_deltas just-in-time, free immediately after)\n",
    "    norm_deltas, target_norm = position_normalize_delta(deltas_all)\n",
    "    cache = apply_delta(bare_cache, norm_deltas)\n",
    "    nll_pos_norm = score_answer_with_cache(\n",
    "        cache, context_len, query_prompt, answer_text, model, tokenizer, config)\n",
    "    del cache, norm_deltas\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # 6. attenuate_first_25\n",
    "    def atten_25_scale(pos, seq_len):\n",
    "        if pos == 0:\n",
    "            return 0.0\n",
    "        if pos < p25:\n",
    "            return 0.25\n",
    "        return 1.0\n",
    "    cache = build_position_scaled_cache(bare_cache, deltas_all, atten_25_scale, n_layers)\n",
    "    nll_atten_25 = score_answer_with_cache(\n",
    "        cache, context_len, query_prompt, answer_text, model, tokenizer, config)\n",
    "    del cache\n",
    "\n",
    "    # 7. skip_first_25\n",
    "    def skip_25_scale(pos, seq_len):\n",
    "        if pos < p25:\n",
    "            return 0.0\n",
    "        return 1.0\n",
    "    cache = build_position_scaled_cache(bare_cache, deltas_all, skip_25_scale, n_layers)\n",
    "    nll_skip_25 = score_answer_with_cache(\n",
    "        cache, context_len, query_prompt, answer_text, model, tokenizer, config)\n",
    "    del cache\n",
    "\n",
    "    # 8. last_50_only\n",
    "    def last_50_scale(pos, seq_len):\n",
    "        if pos < p50:\n",
    "            return 0.0\n",
    "        return 1.0\n",
    "    cache = build_position_scaled_cache(bare_cache, deltas_all, last_50_scale, n_layers)\n",
    "    nll_last_50 = score_answer_with_cache(\n",
    "        cache, context_len, query_prompt, answer_text, model, tokenizer, config)\n",
    "    del cache\n",
    "\n",
    "    # 9. window_25_75\n",
    "    def window_scale(pos, seq_len):\n",
    "        if pos < p25 or pos >= p75:\n",
    "            return 0.0\n",
    "        return 1.0\n",
    "    cache = build_position_scaled_cache(bare_cache, deltas_all, window_scale, n_layers)\n",
    "    nll_window = score_answer_with_cache(\n",
    "        cache, context_len, query_prompt, answer_text, model, tokenizer, config)\n",
    "    del cache\n",
    "\n",
    "    # 10. pos_norm_L0_15\n",
    "    norm_deltas_0_15, _ = position_normalize_delta(deltas_0_15, target_norm=target_norm)\n",
    "    cache = apply_delta(bare_cache, norm_deltas_0_15)\n",
    "    nll_pnl15 = score_answer_with_cache(\n",
    "        cache, context_len, query_prompt, answer_text, model, tokenizer, config)\n",
    "    del cache, norm_deltas_0_15\n",
    "\n",
    "    # =================================================================\n",
    "    # Delta forensics (every Nth sample) \u2014 reuse existing deltas_all\n",
    "    # =================================================================\n",
    "    if idx % DELTA_FORENSICS_EVERY == 0:\n",
    "        pos_norms = compute_position_norms(deltas_all, start_pos=1)\n",
    "\n",
    "        n_positions = min(20, doc_len)\n",
    "        sample_positions = [1 + int(i * doc_len / n_positions) for i in range(n_positions)]\n",
    "        sample_norms = [float(pos_norms[p].item()) for p in sample_positions if p < len(pos_norms)]\n",
    "        pct_positions = [float((p-1)/doc_len) for p in sample_positions if p < len(pos_norms)]\n",
    "\n",
    "        layer_group_norms = {}\n",
    "        for group_name, layers in [(\"L0-7\", range(8)), (\"L8-15\", range(8,16)),\n",
    "                                    (\"L16-23\", range(16,24)), (\"L24-31\", range(24,32))]:\n",
    "            group_deltas = {li: deltas_all[li] for li in layers if li in deltas_all}\n",
    "            if group_deltas:\n",
    "                gn = compute_position_norms(group_deltas, start_pos=1)\n",
    "                layer_group_norms[group_name] = {\n",
    "                    'mean_norm': float(gn[1:].mean().item()),\n",
    "                    'first_25_norm': float(gn[1:p25].mean().item()) if p25 > 1 else 0,\n",
    "                    'last_25_norm': float(gn[p75:].mean().item()) if p75 < len(gn) else 0,\n",
    "                }\n",
    "\n",
    "        forensic_entry = {\n",
    "            'idx': idx,\n",
    "            'doc_len': doc_len,\n",
    "            'word_count': word_count,\n",
    "            'length_bin': length_bin,\n",
    "            'target_norm': float(target_norm),\n",
    "            'position_norms': sample_norms,\n",
    "            'position_pcts': pct_positions,\n",
    "            'decay_ratio': float(sample_norms[-1] / max(sample_norms[0], 1e-10))\n",
    "                if sample_norms else 0,\n",
    "            'layer_group_norms': layer_group_norms,\n",
    "        }\n",
    "        delta_forensics.append(forensic_entry)\n",
    "\n",
    "    # Clean up deltas, then score bare last (scoring mutates the cache)\n",
    "    del deltas_all, deltas_0_15\n",
    "    nll_bare = score_answer_with_cache(\n",
    "        bare_cache, context_len, query_prompt, answer_text, model, tokenizer, config)\n",
    "    del bare_cache\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # --- Store result ---\n",
    "    # Find answer position for per-sample analysis\n",
    "    ans_pos = None\n",
    "    ans_lower = answer.lower()\n",
    "    pass_lower = passage.lower()\n",
    "    char_idx = pass_lower.find(ans_lower)\n",
    "    if char_idx >= 0:\n",
    "        ans_pos = char_idx / max(len(passage), 1)\n",
    "\n",
    "    result = {\n",
    "        'idx': idx,\n",
    "        'doc_len_tokens': doc_len,\n",
    "        'word_count': word_count,\n",
    "        'length_bin': length_bin,\n",
    "        'answer_position': ans_pos,\n",
    "        'bare': nll_bare,\n",
    "        'standard_1x': nll_standard,\n",
    "        'layers_0_15_amp2x': nll_l15_a2,\n",
    "        'layers_0_15_amp3x': nll_l15_a3,\n",
    "        'pos_normalized': nll_pos_norm,\n",
    "        'attenuate_first_25': nll_atten_25,\n",
    "        'skip_first_25': nll_skip_25,\n",
    "        'last_50_only': nll_last_50,\n",
    "        'window_25_75': nll_window,\n",
    "        'pos_norm_L0_15': nll_pnl15,\n",
    "    }\n",
    "    results.append(result)\n",
    "\n",
    "    if (idx + 1) % CHECKPOINT_EVERY == 0 or idx == N - 1:\n",
    "        ckpt_data = {\n",
    "            'results': results,\n",
    "            'delta_forensics': delta_forensics,\n",
    "            'sample_queries': [s['query'] for s in samples],\n",
    "            'completed': len(results),\n",
    "            'total': N,\n",
    "            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        }\n",
    "        with open(CHECKPOINT_PATH, 'w') as f:\n",
    "            json.dump(ckpt_data, f)\n",
    "        elapsed = time.time() - t_start\n",
    "        rate = (idx - start_idx + 1) / elapsed if elapsed > 0 else 0\n",
    "        remaining = (N - idx - 1) / rate if rate > 0 else 0\n",
    "        tqdm.write(f\"  Checkpoint {idx+1}/{N} | {rate:.2f} s/s | ETA: {remaining/60:.1f} min\")\n",
    "\n",
    "elapsed_total = time.time() - t_start\n",
    "print(f\"\\nEvaluation complete: {len(results)} samples in {elapsed_total/60:.1f} min\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 8: Analysis \u2014 overall + per-bin + answer position interaction\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ANALYSIS \u2014 POSITION-AWARE VALUE CONTAMINATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Arrays\n",
    "cond_arrays = {}\n",
    "for cname in CONDITION_NAMES:\n",
    "    cond_arrays[cname] = np.array([r[cname] for r in results])\n",
    "\n",
    "# Filter zero NLLs\n",
    "valid = np.ones(len(results), dtype=bool)\n",
    "for cname in CONDITION_NAMES:\n",
    "    valid &= (cond_arrays[cname] != 0)\n",
    "n_valid = int(np.sum(valid))\n",
    "n_excluded = int(np.sum(~valid))\n",
    "print(f\"Total: {len(results)}, Valid: {n_valid}, Excluded: {n_excluded}\")\n",
    "\n",
    "c = {}\n",
    "for cname in CONDITION_NAMES:\n",
    "    c[cname] = cond_arrays[cname][valid]\n",
    "\n",
    "length_bins_arr = np.array([r['length_bin'] for r in results])[valid]\n",
    "word_counts_arr = np.array([r['word_count'] for r in results])[valid]\n",
    "answer_pos_arr = np.array([r.get('answer_position', None) for r in results], dtype=object)[valid]\n",
    "\n",
    "# ===== OVERALL RESULTS =====\n",
    "print(f\"\\n{'='*90}\")\n",
    "print(f\"OVERALL RESULTS (N={n_valid})\")\n",
    "print(f\"{'='*90}\")\n",
    "print(f\"\\n{'Condition':<22} {'Mean NLL':>10} {'d vs Bare':>10} {'Win%':>7} {'p':>12} {'Sig':>5}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "nll_summary = {}\n",
    "comparison_results = {}\n",
    "\n",
    "for cname in CONDITION_NAMES:\n",
    "    mean_nll = np.mean(c[cname])\n",
    "    std_nll = np.std(c[cname])\n",
    "\n",
    "    if cname == 'bare':\n",
    "        print(f\"{cname:<22} {mean_nll:>10.4f} {'--':>10} {'--':>7} {'--':>12}\")\n",
    "        nll_summary[cname] = {'mean': float(mean_nll), 'std': float(std_nll), 'cohens_d': 0.0}\n",
    "    else:\n",
    "        delta = c['bare'] - c[cname]\n",
    "        d = cohens_d(delta)\n",
    "        win = np.mean(delta > 0) * 100\n",
    "        _, p_val = stats.ttest_1samp(delta, 0)\n",
    "        sig = \"***\" if p_val < 0.001 else \"**\" if p_val < BONFERRONI_ALPHA else \"*\" if p_val < 0.05 else \"ns\"\n",
    "        print(f\"{cname:<22} {mean_nll:>10.4f} {d:>+10.3f} {win:>5.1f}% {p_val:>11.2e} {sig:>5}\")\n",
    "        nll_summary[cname] = {'mean': float(mean_nll), 'std': float(std_nll), 'cohens_d': float(d)}\n",
    "        comparison_results[f\"{cname} vs bare\"] = {\n",
    "            'mean_delta': float(np.mean(delta)),\n",
    "            'cohens_d': float(d),\n",
    "            'win_rate': float(win / 100),\n",
    "            'p_value': float(p_val),\n",
    "            'bonferroni_significant': bool(p_val < BONFERRONI_ALPHA),\n",
    "        }\n",
    "\n",
    "# Highlight best\n",
    "best_cond = max(comparison_results.items(), key=lambda x: x[1]['cohens_d'])\n",
    "print(f\"\\nBest condition: {best_cond[0]} (d={best_cond[1]['cohens_d']:+.3f})\")\n",
    "\n",
    "# ===== PER LENGTH BIN =====\n",
    "print(f\"\\n{'='*90}\")\n",
    "print(\"PER LENGTH BIN\")\n",
    "print(f\"{'='*90}\")\n",
    "\n",
    "bin_names_ordered = [name for name, _, _ in LENGTH_BINS]\n",
    "per_bin_results = {}\n",
    "\n",
    "for cname in CONDITION_NAMES:\n",
    "    if cname == 'bare':\n",
    "        continue\n",
    "    print(f\"\\n  {cname}:\")\n",
    "    bin_ds = []\n",
    "    bin_wins = []\n",
    "    bin_ns = []\n",
    "    for bin_name in bin_names_ordered:\n",
    "        mask = length_bins_arr == bin_name\n",
    "        n_bin = int(np.sum(mask))\n",
    "        if n_bin < 5:\n",
    "            bin_ds.append(None)\n",
    "            bin_wins.append(None)\n",
    "            bin_ns.append(n_bin)\n",
    "            continue\n",
    "        delta = c['bare'][mask] - c[cname][mask]\n",
    "        d = cohens_d(delta)\n",
    "        win = np.mean(delta > 0) * 100\n",
    "        _, p_val = stats.ttest_1samp(delta, 0)\n",
    "        sig = \"***\" if p_val < 0.001 else \"**\" if p_val < BONFERRONI_ALPHA else \"*\" if p_val < 0.05 else \"ns\"\n",
    "        print(f\"    {bin_name}: n={n_bin}, d={d:+.3f}, win={win:.1f}%, p={p_val:.2e} {sig}\")\n",
    "        bin_ds.append(float(d))\n",
    "        bin_wins.append(float(win))\n",
    "        bin_ns.append(n_bin)\n",
    "    per_bin_results[cname] = {\n",
    "        'bin_names': bin_names_ordered, 'bin_ds': bin_ds, 'bin_wins': bin_wins, 'bin_ns': bin_ns\n",
    "    }\n",
    "\n",
    "# ===== ANSWER POSITION INTERACTION =====\n",
    "print(f\"\\n{'='*90}\")\n",
    "print(\"ANSWER POSITION INTERACTION\")\n",
    "print(f\"{'='*90}\")\n",
    "\n",
    "# Split by answer position\n",
    "ans_pos_valid = np.array([\n",
    "    float(ap) if ap is not None else np.nan for ap in answer_pos_arr\n",
    "])\n",
    "has_ans_pos = ~np.isnan(ans_pos_valid)\n",
    "\n",
    "if np.sum(has_ans_pos) > 20:\n",
    "    print(f\"\\nSamples with answer position: {np.sum(has_ans_pos)}\")\n",
    "    early_mask = has_ans_pos & (ans_pos_valid < 0.25)\n",
    "    late_mask = has_ans_pos & (ans_pos_valid >= 0.25)\n",
    "    print(f\"  Early answers (<25%): {np.sum(early_mask)}\")\n",
    "    print(f\"  Late answers (>=25%): {np.sum(late_mask)}\")\n",
    "\n",
    "    answer_pos_results = {}\n",
    "    print(f\"\\n{'Condition':<22} {'Early d':>10} {'Late d':>10} {'Diff':>10}\")\n",
    "    print(\"-\" * 55)\n",
    "    for cname in CONDITION_NAMES:\n",
    "        if cname == 'bare':\n",
    "            continue\n",
    "        delta = c['bare'] - c[cname]\n",
    "        early_d = cohens_d(delta[early_mask]) if np.sum(early_mask) > 5 else float('nan')\n",
    "        late_d = cohens_d(delta[late_mask]) if np.sum(late_mask) > 5 else float('nan')\n",
    "        diff = late_d - early_d if not (np.isnan(early_d) or np.isnan(late_d)) else float('nan')\n",
    "        if not np.isnan(early_d):\n",
    "            print(f\"{cname:<22} {early_d:>+10.3f} {late_d:>+10.3f} {diff:>+10.3f}\")\n",
    "        answer_pos_results[cname] = {\n",
    "            'early_d': float(early_d) if not np.isnan(early_d) else None,\n",
    "            'late_d': float(late_d) if not np.isnan(late_d) else None,\n",
    "        }\n",
    "\n",
    "# ===== ASYMMETRY ANALYSIS =====\n",
    "print(f\"\\n{'='*90}\")\n",
    "print(\"ASYMMETRY ANALYSIS \u2014 Hurt:Help Magnitude Ratio\")\n",
    "print(f\"{'='*90}\")\n",
    "print(f\"\\n{'Condition':<22} {'Helped':>8} {'Hurt':>8} {'Help mag':>10} {'Hurt mag':>10} {'Ratio':>7}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "asymmetry_results = {}\n",
    "for cname in CONDITION_NAMES:\n",
    "    if cname == 'bare':\n",
    "        continue\n",
    "    delta = c['bare'] - c[cname]\n",
    "    helped = delta > 0\n",
    "    hurt = delta < 0\n",
    "    help_mag = np.mean(delta[helped]) if np.any(helped) else 0\n",
    "    hurt_mag = np.mean(delta[hurt]) if np.any(hurt) else 0\n",
    "    ratio = abs(hurt_mag / help_mag) if help_mag != 0 else float('inf')\n",
    "    print(f\"{cname:<22} {np.sum(helped):>8d} {np.sum(hurt):>8d} \"\n",
    "          f\"{help_mag:>+10.4f} {hurt_mag:>+10.4f} {ratio:>6.2f}x\")\n",
    "    asymmetry_results[cname] = {\n",
    "        'n_helped': int(np.sum(helped)),\n",
    "        'n_hurt': int(np.sum(hurt)),\n",
    "        'help_magnitude': float(help_mag),\n",
    "        'hurt_magnitude': float(hurt_mag),\n",
    "        'hurt_help_ratio': float(ratio),\n",
    "    }\n",
    "\n",
    "# ===== KEY COMPARISONS: Position conditions vs standard_1x =====\n",
    "print(f\"\\n{'='*90}\")\n",
    "print(\"KEY COMPARISONS: Position-aware vs Standard\")\n",
    "print(f\"{'='*90}\")\n",
    "\n",
    "position_conditions = ['pos_normalized', 'attenuate_first_25', 'skip_first_25',\n",
    "                        'last_50_only', 'window_25_75', 'pos_norm_L0_15']\n",
    "\n",
    "position_comparisons = {}\n",
    "print(f\"\\n{'Comparison':<35} {'d':>8} {'Win%':>7} {'p':>12}\")\n",
    "print(\"-\" * 65)\n",
    "for cname in position_conditions:\n",
    "    delta = c[cname] - c['standard_1x']  # negative = position version better\n",
    "    d = cohens_d(-delta)  # positive = position version wins\n",
    "    win = np.mean(delta < 0) * 100\n",
    "    _, p_val = stats.ttest_1samp(delta, 0)\n",
    "    sig = \"***\" if p_val < 0.001 else \"**\" if p_val < BONFERRONI_ALPHA else \"*\" if p_val < 0.05 else \"ns\"\n",
    "    print(f\"{cname + ' vs standard_1x':<35} {d:>+8.3f} {win:>5.1f}% {p_val:>11.2e} {sig}\")\n",
    "    position_comparisons[f\"{cname} vs standard_1x\"] = {\n",
    "        'cohens_d': float(d), 'win_rate': float(win/100), 'p_value': float(p_val),\n",
    "    }"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 9: Delta Forensics \u2014 How does contamination decay with position?\n",
    "\n",
    "if delta_forensics:\n",
    "    print(\"=\" * 70)\n",
    "    print(\"DELTA FORENSICS \u2014 Contamination Profile\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    # Group by length bin\n",
    "    for bin_name in bin_names_ordered:\n",
    "        bin_forensics = [f for f in delta_forensics if f['length_bin'] == bin_name]\n",
    "        if len(bin_forensics) < 2:\n",
    "            continue\n",
    "\n",
    "        print(f\"\\n  {bin_name} (n={len(bin_forensics)}):\")\n",
    "\n",
    "        # Average decay ratio\n",
    "        decay_ratios = [f['decay_ratio'] for f in bin_forensics]\n",
    "        print(f\"    Decay ratio (last/first norm): {np.mean(decay_ratios):.4f} \"\n",
    "              f\"(range: {min(decay_ratios):.4f} - {max(decay_ratios):.4f})\")\n",
    "\n",
    "        # Average target norm\n",
    "        target_norms = [f['target_norm'] for f in bin_forensics]\n",
    "        print(f\"    Target norm (median position): {np.mean(target_norms):.6f}\")\n",
    "\n",
    "        # Layer group norms\n",
    "        for group in [\"L0-7\", \"L8-15\", \"L16-23\", \"L24-31\"]:\n",
    "            group_means = [f['layer_group_norms'].get(group, {}).get('mean_norm', 0)\n",
    "                          for f in bin_forensics]\n",
    "            first_25_means = [f['layer_group_norms'].get(group, {}).get('first_25_norm', 0)\n",
    "                             for f in bin_forensics]\n",
    "            last_25_means = [f['layer_group_norms'].get(group, {}).get('last_25_norm', 0)\n",
    "                            for f in bin_forensics]\n",
    "            if any(g > 0 for g in group_means):\n",
    "                print(f\"    {group}: mean={np.mean(group_means):.6f}, \"\n",
    "                      f\"first_25%={np.mean(first_25_means):.6f}, \"\n",
    "                      f\"last_25%={np.mean(last_25_means):.6f}, \"\n",
    "                      f\"ratio={np.mean(last_25_means)/max(np.mean(first_25_means), 1e-10):.4f}\")\n",
    "\n",
    "    # Average contamination profile across all samples\n",
    "    print(f\"\\n  Average contamination profile (all samples, n={len(delta_forensics)}):\")\n",
    "    # Normalize to percentage positions\n",
    "    n_bins = 20\n",
    "    pct_bins = np.linspace(0, 1, n_bins + 1)\n",
    "    avg_profile = np.zeros(n_bins)\n",
    "    counts = np.zeros(n_bins)\n",
    "\n",
    "    for f in delta_forensics:\n",
    "        for pct, norm in zip(f['position_pcts'], f['position_norms']):\n",
    "            bin_idx = min(int(pct * n_bins), n_bins - 1)\n",
    "            avg_profile[bin_idx] += norm\n",
    "            counts[bin_idx] += 1\n",
    "\n",
    "    avg_profile = np.divide(avg_profile, counts, where=counts > 0)\n",
    "    for i in range(n_bins):\n",
    "        pct_lo = pct_bins[i] * 100\n",
    "        pct_hi = pct_bins[i+1] * 100\n",
    "        bar = \"\u2588\" * int(avg_profile[i] / max(avg_profile.max(), 1e-10) * 40)\n",
    "        print(f\"    {pct_lo:5.0f}-{pct_hi:3.0f}%: {avg_profile[i]:.6f}  {bar}\")\n",
    "else:\n",
    "    print(\"No delta forensics data collected.\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 10: Plots\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "\n",
    "colors = {\n",
    "    'standard_1x': '#d62728',\n",
    "    'layers_0_15_amp2x': '#ff7f0e',\n",
    "    'layers_0_15_amp3x': '#e377c2',\n",
    "    'pos_normalized': '#2ca02c',\n",
    "    'attenuate_first_25': '#17becf',\n",
    "    'skip_first_25': '#1f77b4',\n",
    "    'last_50_only': '#9467bd',\n",
    "    'window_25_75': '#8c564b',\n",
    "    'pos_norm_L0_15': '#bcbd22',\n",
    "}\n",
    "\n",
    "# --- Plot 1: Overall bar chart sorted by d ---\n",
    "ax = axes[0, 0]\n",
    "conds_sorted = sorted(\n",
    "    [(cn, cohens_d(c['bare'] - c[cn])) for cn in CONDITION_NAMES if cn != 'bare'],\n",
    "    key=lambda x: x[1], reverse=True\n",
    ")\n",
    "names_s = [x[0] for x in conds_sorted]\n",
    "ds_s = [x[1] for x in conds_sorted]\n",
    "bar_colors = [colors.get(cn, 'gray') for cn in names_s]\n",
    "bars = ax.barh(range(len(names_s)), ds_s, color=bar_colors, edgecolor='black', linewidth=0.5)\n",
    "ax.set_yticks(range(len(names_s)))\n",
    "ax.set_yticklabels(names_s, fontsize=8)\n",
    "for i, (name, dv) in enumerate(conds_sorted):\n",
    "    ax.text(dv + 0.003, i, f\"d={dv:+.3f}\", va='center', fontsize=7)\n",
    "ax.axvline(x=0, color='gray', linestyle='--')\n",
    "ax.axvline(x=0.472, color='red', linestyle=':', alpha=0.4, label='MARCO static_fact')\n",
    "ax.set_xlabel(\"Cohen's d vs Bare\")\n",
    "ax.set_title(\"Overall Effect (All Bins)\")\n",
    "ax.invert_yaxis()\n",
    "ax.legend(fontsize=7)\n",
    "\n",
    "# --- Plot 2: Per-bin for position conditions ---\n",
    "ax = axes[0, 1]\n",
    "x = np.arange(len(bin_names_ordered))\n",
    "width = 0.12\n",
    "plot_conds = ['standard_1x', 'pos_normalized', 'skip_first_25', 'last_50_only', 'pos_norm_L0_15']\n",
    "for i, cname in enumerate(plot_conds):\n",
    "    ds = per_bin_results[cname]['bin_ds']\n",
    "    ds_clean = [d if d is not None else 0 for d in ds]\n",
    "    offset = (i - len(plot_conds)/2 + 0.5) * width\n",
    "    ax.bar(x + offset, ds_clean, width, label=cname, color=colors.get(cname, 'gray'),\n",
    "           edgecolor='black', linewidth=0.3, alpha=0.85)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(bin_names_ordered)\n",
    "ax.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "ax.set_ylabel(\"Cohen's d vs Bare\")\n",
    "ax.set_title(\"Position Conditions by Length Bin\")\n",
    "ax.legend(fontsize=6)\n",
    "\n",
    "# --- Plot 3: Answer position interaction ---\n",
    "ax = axes[0, 2]\n",
    "if np.sum(has_ans_pos) > 20:\n",
    "    for cname in ['standard_1x', 'pos_normalized', 'skip_first_25', 'layers_0_15_amp2x']:\n",
    "        delta = c['bare'] - c[cname]\n",
    "        valid_ap = has_ans_pos\n",
    "        ax.scatter(ans_pos_valid[valid_ap], delta[valid_ap],\n",
    "                  alpha=0.15, s=8, color=colors.get(cname, 'gray'), label=cname)\n",
    "        # Trend line\n",
    "        bins_ap = np.linspace(0, 1, 8)\n",
    "        for k in range(len(bins_ap)-1):\n",
    "            mask_k = valid_ap & (ans_pos_valid >= bins_ap[k]) & (ans_pos_valid < bins_ap[k+1])\n",
    "            if np.sum(mask_k) > 5:\n",
    "                ax.scatter((bins_ap[k]+bins_ap[k+1])/2, np.mean(delta[mask_k]),\n",
    "                          s=50, color=colors.get(cname, 'gray'), edgecolor='black', linewidth=0.5, zorder=5)\n",
    "    ax.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "    ax.set_xlabel(\"Answer Position (0=start, 1=end)\")\n",
    "    ax.set_ylabel(\"NLL Reduction (positive = helps)\")\n",
    "    ax.set_title(\"Answer Position vs Priming Benefit\")\n",
    "    ax.legend(fontsize=7)\n",
    "else:\n",
    "    ax.text(0.5, 0.5, 'Insufficient answer position data', ha='center', transform=ax.transAxes)\n",
    "\n",
    "# --- Plot 4: Asymmetry comparison ---\n",
    "ax = axes[1, 0]\n",
    "asymmetry_conds = sorted(\n",
    "    [(cn, asymmetry_results[cn]['hurt_help_ratio']) for cn in asymmetry_results],\n",
    "    key=lambda x: x[1]\n",
    ")\n",
    "a_names = [x[0] for x in asymmetry_conds]\n",
    "a_ratios = [x[1] for x in asymmetry_conds]\n",
    "a_colors = [colors.get(cn, 'gray') for cn in a_names]\n",
    "ax.barh(range(len(a_names)), a_ratios, color=a_colors, edgecolor='black', linewidth=0.5)\n",
    "ax.set_yticks(range(len(a_names)))\n",
    "ax.set_yticklabels(a_names, fontsize=8)\n",
    "for i, (name, ratio) in enumerate(asymmetry_conds):\n",
    "    ax.text(ratio + 0.02, i, f\"{ratio:.2f}x\", va='center', fontsize=7)\n",
    "ax.axvline(x=1.0, color='red', linestyle='--', alpha=0.5, label='Equal hurt/help')\n",
    "ax.set_xlabel(\"Hurt:Help Magnitude Ratio (lower = better)\")\n",
    "ax.set_title(\"Asymmetry: How Bad is the Hurt Tail?\")\n",
    "ax.invert_yaxis()\n",
    "ax.legend(fontsize=7)\n",
    "\n",
    "# --- Plot 5: Contamination decay profile ---\n",
    "ax = axes[1, 1]\n",
    "if delta_forensics:\n",
    "    for bin_name in bin_names_ordered:\n",
    "        bin_f = [f for f in delta_forensics if f['length_bin'] == bin_name]\n",
    "        if len(bin_f) < 2:\n",
    "            continue\n",
    "        # Average profile\n",
    "        all_pcts = []\n",
    "        all_norms = []\n",
    "        for f in bin_f:\n",
    "            all_pcts.extend(f['position_pcts'])\n",
    "            all_norms.extend(f['position_norms'])\n",
    "        if all_pcts:\n",
    "            # Bin into 20 segments\n",
    "            n_seg = 20\n",
    "            seg_edges = np.linspace(0, 1, n_seg + 1)\n",
    "            seg_means = []\n",
    "            seg_centers = []\n",
    "            for k in range(n_seg):\n",
    "                mask_k = [(p >= seg_edges[k]) and (p < seg_edges[k+1])\n",
    "                         for p in all_pcts]\n",
    "                vals = [n for n, m in zip(all_norms, mask_k) if m]\n",
    "                if vals:\n",
    "                    seg_means.append(np.mean(vals))\n",
    "                    seg_centers.append((seg_edges[k] + seg_edges[k+1])/2)\n",
    "            ax.plot(seg_centers, seg_means, marker='o', markersize=3, label=bin_name)\n",
    "    ax.set_xlabel(\"Position in Document (%)\")\n",
    "    ax.set_ylabel(\"Delta L2 Norm (contamination strength)\")\n",
    "    ax.set_title(\"Contamination Decay by Document Length\")\n",
    "    ax.legend(fontsize=8)\n",
    "    ax.set_yscale('log')\n",
    "else:\n",
    "    ax.text(0.5, 0.5, 'No forensics data', ha='center', transform=ax.transAxes)\n",
    "\n",
    "# --- Plot 6: Key condition comparison per bin (heatmap style) ---\n",
    "ax = axes[1, 2]\n",
    "all_conds = [cn for cn in CONDITION_NAMES if cn != 'bare']\n",
    "heatmap_data = []\n",
    "for cname in all_conds:\n",
    "    row = []\n",
    "    for bin_name in bin_names_ordered:\n",
    "        d = None\n",
    "        if cname in per_bin_results:\n",
    "            bin_idx = bin_names_ordered.index(bin_name)\n",
    "            d = per_bin_results[cname]['bin_ds'][bin_idx]\n",
    "        row.append(d if d is not None else 0)\n",
    "    heatmap_data.append(row)\n",
    "\n",
    "heatmap_data = np.array(heatmap_data)\n",
    "im = ax.imshow(heatmap_data, aspect='auto', cmap='RdYlGn', vmin=-0.3, vmax=0.3)\n",
    "ax.set_xticks(range(len(bin_names_ordered)))\n",
    "ax.set_xticklabels(bin_names_ordered)\n",
    "ax.set_yticks(range(len(all_conds)))\n",
    "ax.set_yticklabels(all_conds, fontsize=8)\n",
    "for i in range(len(all_conds)):\n",
    "    for j in range(len(bin_names_ordered)):\n",
    "        ax.text(j, i, f\"{heatmap_data[i,j]:+.2f}\", ha='center', va='center', fontsize=7)\n",
    "plt.colorbar(im, ax=ax, label=\"Cohen's d vs Bare\")\n",
    "ax.set_title(\"Effect by Condition \u00d7 Length Bin\")\n",
    "\n",
    "plt.suptitle('Exp 13: Position-Aware Value Contamination', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'analysis_plots.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"Plots saved to {RESULTS_DIR / 'analysis_plots.png'}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 11: Save results\n",
    "final = {\n",
    "    'experiment': 'exp13_position_aware_priming',\n",
    "    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'config': {\n",
    "        'model_name': config.model_name,\n",
    "        'seed': SEED,\n",
    "        'n_eval': N,\n",
    "        'n_valid': n_valid,\n",
    "        'n_excluded': n_excluded,\n",
    "        'n_conditions': N_CONDITIONS,\n",
    "        'n_comparisons': N_COMPARISONS,\n",
    "        'bonferroni_alpha': BONFERRONI_ALPHA,\n",
    "        'dataset': 'Natural Questions (from exp 12 samples)',\n",
    "        'length_bins': LENGTH_BINS,\n",
    "        'static_fact': STATIC_FACT,\n",
    "    },\n",
    "    'condition_names': CONDITION_NAMES,\n",
    "    'nll_summary': nll_summary,\n",
    "    'primary_comparisons': comparison_results,\n",
    "    'position_comparisons': position_comparisons,\n",
    "    'per_bin_results': per_bin_results,\n",
    "    'answer_position_results': answer_pos_results if 'answer_pos_results' in dir() else {},\n",
    "    'asymmetry_results': asymmetry_results,\n",
    "    'delta_forensics_summary': {\n",
    "        'n_samples': len(delta_forensics),\n",
    "        'samples': delta_forensics,\n",
    "    },\n",
    "    'per_sample_results': results,\n",
    "}\n",
    "\n",
    "with open(FINAL_RESULTS_PATH, 'w') as f:\n",
    "    json.dump(final, f, indent=2)\n",
    "\n",
    "print(f\"Results saved to {FINAL_RESULTS_PATH}\")\n",
    "print(f\"File size: {FINAL_RESULTS_PATH.stat().st_size / 1024:.1f} KB\")\n",
    "print(\"\\nDone!\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 12: Cleanup\n",
    "import gc\n",
    "\n",
    "print(\"Cleaning up GPU memory...\")\n",
    "mem_before = torch.cuda.memory_allocated() / 1e9\n",
    "\n",
    "del model\n",
    "del tokenizer\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "mem_after = torch.cuda.memory_allocated() / 1e9\n",
    "print(f\"GPU memory: {mem_before:.2f} GB -> {mem_after:.2f} GB\")"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}