{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exp 29: Cross-Dataset Generalization on Hard QA Datasets (Gemma 3 4B)\n",
    "\n",
    "## Motivation\n",
    "\n",
    "Exp 27b established that Gemma hero layers generalize to NQ (d=+0.213, p<0.001), but\n",
    "TriviaQA and HotpotQA were dominated by ceiling effects (77% and 56% of samples with\n",
    "bare NLL < 0.01). This experiment tests three datasets **specifically chosen to avoid\n",
    "ceiling effects**:\n",
    "\n",
    "| Dataset | Why No Ceiling | Passage Length | Answer Type |\n",
    "|---------|---------------|----------------|-------------|\n",
    "| **DROP** | Requires counting/arithmetic \u2014 model must compute, not extract | ~150-300 tok | Numbers, dates, short spans |\n",
    "| **AdversarialQA** | Questions designed to fool RoBERTa \u2014 exploits model blind spots | ~100-300 tok | Extracted spans (but adversarial) |\n",
    "| **CoQA** | Abstractive answers \u2014 many valid phrasings spread probability mass | ~100-400 tok | Free-form natural language |\n",
    "\n",
    "## Conditions (same as Exp 27b)\n",
    "\n",
    "| # | Condition | Description |\n",
    "|---|-----------|-------------|\n",
    "| 1 | bare | Baseline |\n",
    "| 2 | sf_trunc | Standard priming |\n",
    "| 3 | sf_trunc_bias2 | +2.0 attention forcing |\n",
    "| 4 | values_only | All-layer value swap |\n",
    "| 5 | values_early | Layers 0-15 value swap |\n",
    "| 6 | values_hero | Layers {10,12,14,15,20} value swap |\n",
    "\n",
    "## Key Question\n",
    "\n",
    "Do hero layers (the best Gemma intervention) generalize to datasets with meaningful\n",
    "NLL spread? If so, this confirms the mechanism is broadly useful. If not, it may be\n",
    "NQ-specific."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 1: Setup\n",
    "import os\n",
    "os.umask(0o000)\n",
    "\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import gc\n",
    "import csv\n",
    "import numpy as np\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "RESULTS_DIR = Path(\"results/exp29\")\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CHECKPOINT_PATH = RESULTS_DIR / \"checkpoint.json\"\n",
    "FINAL_RESULTS_PATH = RESULTS_DIR / \"results.json\"\n",
    "CSV_PATH = RESULTS_DIR / \"results.csv\"\n",
    "\n",
    "print(f\"SEED: {SEED}\")\n",
    "print(f\"Results directory: {RESULTS_DIR}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 2: Load Gemma 3 4B via load_model()\n",
    "sys.path.insert(0, \".\")\n",
    "\n",
    "from lib.config import ExperimentConfig\n",
    "from lib.model_utils import load_model\n",
    "\n",
    "MODEL_NAME = \"google/gemma-3-4b-it\"\n",
    "\n",
    "exp_config = ExperimentConfig(\n",
    "    model_name=MODEL_NAME,\n",
    "    model_type=\"gemma3\",\n",
    "    compute_dtype=\"auto\",  # resolves to bfloat16 for Gemma\n",
    "    use_4bit=True,\n",
    "    num_samples=2000,\n",
    "    seed=SEED,\n",
    ")\n",
    "\n",
    "print(f\"Loading {MODEL_NAME} (4-bit, bfloat16)...\")\n",
    "model, tokenizer = load_model(exp_config)\n",
    "\n",
    "from lib.kv_cache import _get_text_config, _get_head_dim, _ensure_dynamic_cache, _get_cache_keys\n",
    "\n",
    "text_config = _get_text_config(model.config)\n",
    "N_LAYERS = text_config.num_hidden_layers\n",
    "print(f\"\\nModel loaded successfully.\")\n",
    "print(f\"  Num layers: {N_LAYERS}\")\n",
    "print(f\"  Head dim: {_get_head_dim(model.config)}\")\n",
    "print(f\"  Model dtype: {model.dtype}\")\n",
    "print(f\"  Sliding window: {getattr(text_config, 'sliding_window', 'N/A')}\")\n",
    "\n",
    "# Verify with test forward pass\n",
    "sample_ids = tokenizer(\"test\", return_tensors=\"pt\")['input_ids'].to(exp_config.device)\n",
    "with torch.no_grad():\n",
    "    out = model(sample_ids, use_cache=True)\n",
    "    cache_check = _ensure_dynamic_cache(out.past_key_values)\n",
    "    k0 = _get_cache_keys(cache_check, 0)\n",
    "    print(f\"  Cache key dtype: {k0.dtype}\")\n",
    "    print(f\"  Cache key shape: {k0.shape}\")\n",
    "del out, sample_ids, cache_check\n",
    "torch.cuda.empty_cache()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 3: Lib imports + constants\n",
    "from lib.kv_cache import (\n",
    "    _get_cache_keys,\n",
    "    _get_cache_values,\n",
    "    _set_cache_keys,\n",
    "    _set_cache_values,\n",
    "    _ensure_dynamic_cache,\n",
    "    extract_and_truncate_cache_with_bos,\n",
    "    correct_rope_positions_with_bos,\n",
    "    score_answer_with_cache,\n",
    "    deepcopy_cache,\n",
    "    replace_values_at_layers,\n",
    ")\n",
    "from lib.analysis import cohens_d\n",
    "from lib.data import count_words\n",
    "from scipy import stats\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Templates -- bare text, no \"Document:\\n\" framing\n",
    "SURROGATE_PREFIX_TEMPLATE = \"{surrogate}\\n\"\n",
    "DOCUMENT_TEMPLATE = \"{document}\"\n",
    "QUERY_TEMPLATE = \"\\nQuestion: {question}\\nAnswer:\"\n",
    "ANSWER_TEMPLATE = \" {answer}\"\n",
    "\n",
    "# Prefix text\n",
    "from lib.surrogate import STATIC_SURROGATE_QUERIES\n",
    "STATIC_FACT = STATIC_SURROGATE_QUERIES['static_factual']['query']\n",
    "\n",
    "# Experiment parameters\n",
    "N_PER_DATASET = 300\n",
    "# Gemma sliding window = 1024: total seq must be < 1024\n",
    "# Primed pass: 1(BOS) + ~10(prefix) + doc_len < 1024 -> doc_len < ~1013\n",
    "# Cap at 900 for safety (matching Exp 21/27b)\n",
    "MAX_DOC_TOKENS = 900\n",
    "CHECKPOINT_EVERY = 25\n",
    "\n",
    "# Conditions\n",
    "CONDITION_NAMES = ['bare', 'sf_trunc', 'sf_trunc_bias2', 'values_only',\n",
    "                   'values_early', 'values_hero']\n",
    "\n",
    "# Layer-selective conditions from Exps 19/21/24\n",
    "EARLY_LAYER_CUTOFF = 16  # layers 0-15\n",
    "HERO_LAYERS = [10, 12, 14, 15, 20]  # from Exp 24 single-layer scan\n",
    "\n",
    "# Length bins for stratified analysis (token count)\n",
    "LENGTH_BINS = [\n",
    "    ('<128', 0, 128),\n",
    "    ('128-256', 128, 256),\n",
    "    ('256-512', 256, 512),\n",
    "    ('512-900', 512, 901),\n",
    "]\n",
    "\n",
    "print(\"Config ready\")\n",
    "print(f\"  Model: {MODEL_NAME}\")\n",
    "print(f\"  N per dataset: {N_PER_DATASET}\")\n",
    "print(f\"  MAX_DOC_TOKENS: {MAX_DOC_TOKENS} (sliding window constraint)\")\n",
    "print(f\"  N_LAYERS: {N_LAYERS}\")\n",
    "print(f\"  EARLY_LAYER_CUTOFF: {EARLY_LAYER_CUTOFF}\")\n",
    "print(f\"  HERO_LAYERS: {HERO_LAYERS}\")\n",
    "print(f\"  Conditions: {CONDITION_NAMES}\")\n",
    "print(f\"  Static fact prefix: '{STATIC_FACT}'\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 4: Load DROP dataset (numerical/discrete reasoning)\n",
    "from datasets import load_dataset\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"LOADING DROP (validation)\")\n",
    "print(\"=\" * 70)\n",
    "print(\"DROP requires counting, sorting, arithmetic over passage content.\")\n",
    "print(\"Answers are numbers, dates, or short spans that the model must COMPUTE.\")\n",
    "\n",
    "DROP_CACHE = RESULTS_DIR / \"drop_samples.json\"\n",
    "\n",
    "if DROP_CACHE.exists():\n",
    "    with open(DROP_CACHE, 'r') as f:\n",
    "        drop_samples = json.load(f)\n",
    "    print(f\"Loaded {len(drop_samples)} cached DROP samples\")\n",
    "else:\n",
    "    drop_ds = load_dataset(\"drop\", split=\"validation\")\n",
    "    print(f\"DROP validation size: {len(drop_ds)}\")\n",
    "\n",
    "    drop_samples = []\n",
    "    np.random.seed(SEED)\n",
    "\n",
    "    for item in tqdm(drop_ds, desc=\"Filtering DROP\"):\n",
    "        passage = item.get('passage', '')\n",
    "        question = item.get('question', '')\n",
    "        answers_info = item.get('answers_spans', {})\n",
    "\n",
    "        # Extract answer spans\n",
    "        spans = answers_info.get('spans', [])\n",
    "        if not spans:\n",
    "            continue\n",
    "        answer_text = spans[0]  # use first valid answer\n",
    "\n",
    "        if not question or not answer_text or not passage:\n",
    "            continue\n",
    "        if len(answer_text.strip()) == 0:\n",
    "            continue\n",
    "\n",
    "        wc = count_words(passage)\n",
    "        if wc < 30 or wc > 2000:\n",
    "            continue\n",
    "\n",
    "        drop_samples.append({\n",
    "            'passage': passage,\n",
    "            'query': question,\n",
    "            'answer': answer_text,\n",
    "            'word_count': wc,\n",
    "            'dataset': 'drop',\n",
    "            'all_answers': spans,  # keep all valid answers for reference\n",
    "        })\n",
    "\n",
    "        if len(drop_samples) >= N_PER_DATASET * 3:\n",
    "            break\n",
    "\n",
    "    np.random.shuffle(drop_samples)\n",
    "    drop_samples = drop_samples[:N_PER_DATASET]\n",
    "\n",
    "    with open(DROP_CACHE, 'w') as f:\n",
    "        json.dump(drop_samples, f)\n",
    "    print(f\"Cached {len(drop_samples)} samples\")\n",
    "\n",
    "    del drop_ds\n",
    "    gc.collect()\n",
    "\n",
    "print(f\"DROP samples: {len(drop_samples)}\")\n",
    "wcs = [s['word_count'] for s in drop_samples]\n",
    "ans_lens = [len(s['answer'].split()) for s in drop_samples]\n",
    "print(f\"  Word counts: mean={np.mean(wcs):.0f}, min={min(wcs)}, max={max(wcs)}\")\n",
    "print(f\"  Answer word lengths: mean={np.mean(ans_lens):.1f}, min={min(ans_lens)}, max={max(ans_lens)}\")\n",
    "if drop_samples:\n",
    "    for i in range(min(3, len(drop_samples))):\n",
    "        print(f\"  Example {i+1}:\")\n",
    "        print(f\"    Q: {drop_samples[i]['query']}\")\n",
    "        print(f\"    A: {drop_samples[i]['answer']}\")\n",
    "        print(f\"    Passage (first 120 chars): {drop_samples[i]['passage'][:120]}...\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 5: Load AdversarialQA dataset (adversarially hard extractive QA)\n",
    "print(\"=\" * 70)\n",
    "print(\"LOADING ADVERSARIALQA (droberta, validation)\")\n",
    "print(\"=\" * 70)\n",
    "print(\"Questions written by humans specifically to fool RoBERTa.\")\n",
    "print(\"Same extractive format as SQuAD, but adversarially chosen hard cases.\")\n",
    "\n",
    "AQA_CACHE = RESULTS_DIR / \"aqa_samples.json\"\n",
    "\n",
    "if AQA_CACHE.exists():\n",
    "    with open(AQA_CACHE, 'r') as f:\n",
    "        aqa_samples = json.load(f)\n",
    "    print(f\"Loaded {len(aqa_samples)} cached AdversarialQA samples\")\n",
    "else:\n",
    "    aqa_ds = load_dataset(\"adversarial_qa\", \"droberta\", split=\"validation\")\n",
    "    print(f\"AdversarialQA droberta validation size: {len(aqa_ds)}\")\n",
    "\n",
    "    aqa_samples = []\n",
    "    np.random.seed(SEED)\n",
    "\n",
    "    for item in tqdm(aqa_ds, desc=\"Filtering AdversarialQA\"):\n",
    "        context = item.get('context', '')\n",
    "        question = item.get('question', '')\n",
    "        answers_info = item.get('answers', {})\n",
    "\n",
    "        answer_texts = answers_info.get('text', [])\n",
    "        if not answer_texts:\n",
    "            continue\n",
    "        answer_text = answer_texts[0]\n",
    "\n",
    "        if not question or not answer_text or not context:\n",
    "            continue\n",
    "\n",
    "        wc = count_words(context)\n",
    "        if wc < 30 or wc > 2000:\n",
    "            continue\n",
    "\n",
    "        aqa_samples.append({\n",
    "            'passage': context,\n",
    "            'query': question,\n",
    "            'answer': answer_text,\n",
    "            'word_count': wc,\n",
    "            'dataset': 'adversarialqa',\n",
    "        })\n",
    "\n",
    "    np.random.shuffle(aqa_samples)\n",
    "    aqa_samples = aqa_samples[:N_PER_DATASET]\n",
    "\n",
    "    with open(AQA_CACHE, 'w') as f:\n",
    "        json.dump(aqa_samples, f)\n",
    "    print(f\"Cached {len(aqa_samples)} samples\")\n",
    "\n",
    "    del aqa_ds\n",
    "    gc.collect()\n",
    "\n",
    "print(f\"AdversarialQA samples: {len(aqa_samples)}\")\n",
    "wcs = [s['word_count'] for s in aqa_samples]\n",
    "ans_lens = [len(s['answer'].split()) for s in aqa_samples]\n",
    "print(f\"  Word counts: mean={np.mean(wcs):.0f}, min={min(wcs)}, max={max(wcs)}\")\n",
    "print(f\"  Answer word lengths: mean={np.mean(ans_lens):.1f}, min={min(ans_lens)}, max={max(ans_lens)}\")\n",
    "if aqa_samples:\n",
    "    for i in range(min(3, len(aqa_samples))):\n",
    "        print(f\"  Example {i+1}:\")\n",
    "        print(f\"    Q: {aqa_samples[i]['query']}\")\n",
    "        print(f\"    A: {aqa_samples[i]['answer']}\")\n",
    "        print(f\"    Passage (first 120 chars): {aqa_samples[i]['passage'][:120]}...\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 6: Load CoQA dataset (abstractive conversational QA)\n",
    "print(\"=\" * 70)\n",
    "print(\"LOADING COQA (validation)\")\n",
    "print(\"=\" * 70)\n",
    "print(\"Free-form abstractive answers across 7 domains.\")\n",
    "print(\"Using FIRST question per story only (no conversational dependency).\")\n",
    "\n",
    "COQA_CACHE = RESULTS_DIR / \"coqa_samples.json\"\n",
    "\n",
    "if COQA_CACHE.exists():\n",
    "    with open(COQA_CACHE, 'r') as f:\n",
    "        coqa_samples = json.load(f)\n",
    "    print(f\"Loaded {len(coqa_samples)} cached CoQA samples\")\n",
    "else:\n",
    "    coqa_ds = load_dataset(\"stanfordnlp/coqa\", split=\"validation\")\n",
    "    print(f\"CoQA validation stories: {len(coqa_ds)}\")\n",
    "\n",
    "    coqa_samples = []\n",
    "    np.random.seed(SEED)\n",
    "\n",
    "    for item in tqdm(coqa_ds, desc=\"Processing CoQA\"):\n",
    "        story = item.get('story', '')\n",
    "        questions = item.get('questions', [])\n",
    "        answers_info = item.get('answers', {})\n",
    "        answer_texts = answers_info.get('input_text', [])\n",
    "\n",
    "        if not story or not questions or not answer_texts:\n",
    "            continue\n",
    "\n",
    "        # Use first question only (no conversational dependency)\n",
    "        question = questions[0]\n",
    "        answer_text = answer_texts[0]\n",
    "\n",
    "        if not question or not answer_text:\n",
    "            continue\n",
    "        if answer_text.strip().lower() in ('unknown', 'n/a', ''):\n",
    "            continue\n",
    "\n",
    "        wc = count_words(story)\n",
    "        if wc < 30 or wc > 2000:\n",
    "            continue\n",
    "\n",
    "        coqa_samples.append({\n",
    "            'passage': story,\n",
    "            'query': question,\n",
    "            'answer': answer_text,\n",
    "            'word_count': wc,\n",
    "            'dataset': 'coqa',\n",
    "            'source_domain': item.get('source', 'unknown'),\n",
    "        })\n",
    "\n",
    "    # CoQA has 500 stories; if we don't get 300 from first questions,\n",
    "    # add second questions from remaining stories\n",
    "    if len(coqa_samples) < N_PER_DATASET:\n",
    "        print(f\"  Only {len(coqa_samples)} first-question samples. Adding second questions...\")\n",
    "        used_stories = {s['passage'][:50] for s in coqa_samples}\n",
    "        for item in coqa_ds:\n",
    "            story = item.get('story', '')\n",
    "            questions = item.get('questions', [])\n",
    "            answers_info = item.get('answers', {})\n",
    "            answer_texts = answers_info.get('input_text', [])\n",
    "\n",
    "            if len(questions) < 2 or len(answer_texts) < 2:\n",
    "                continue\n",
    "            if story[:50] in used_stories:\n",
    "                # Second question from a story we already used\n",
    "                question = questions[1]\n",
    "                answer_text = answer_texts[1]\n",
    "\n",
    "                if not question or not answer_text:\n",
    "                    continue\n",
    "                if answer_text.strip().lower() in ('unknown', 'n/a', ''):\n",
    "                    continue\n",
    "\n",
    "                wc = count_words(story)\n",
    "                if wc < 30 or wc > 2000:\n",
    "                    continue\n",
    "\n",
    "                coqa_samples.append({\n",
    "                    'passage': story,\n",
    "                    'query': question,\n",
    "                    'answer': answer_text,\n",
    "                    'word_count': wc,\n",
    "                    'dataset': 'coqa',\n",
    "                    'source_domain': item.get('source', 'unknown'),\n",
    "                })\n",
    "\n",
    "            if len(coqa_samples) >= N_PER_DATASET * 2:\n",
    "                break\n",
    "\n",
    "    np.random.shuffle(coqa_samples)\n",
    "    coqa_samples = coqa_samples[:N_PER_DATASET]\n",
    "\n",
    "    with open(COQA_CACHE, 'w') as f:\n",
    "        json.dump(coqa_samples, f)\n",
    "    print(f\"Cached {len(coqa_samples)} samples\")\n",
    "\n",
    "    del coqa_ds\n",
    "    gc.collect()\n",
    "\n",
    "print(f\"CoQA samples: {len(coqa_samples)}\")\n",
    "wcs = [s['word_count'] for s in coqa_samples]\n",
    "ans_lens = [len(s['answer'].split()) for s in coqa_samples]\n",
    "# Domain distribution\n",
    "domains = {}\n",
    "for s_ in coqa_samples:\n",
    "    d = s_.get('source_domain', 'unknown')\n",
    "    domains[d] = domains.get(d, 0) + 1\n",
    "print(f\"  Word counts: mean={np.mean(wcs):.0f}, min={min(wcs)}, max={max(wcs)}\")\n",
    "print(f\"  Answer word lengths: mean={np.mean(ans_lens):.1f}, min={min(ans_lens)}, max={max(ans_lens)}\")\n",
    "print(f\"  Domain distribution: {domains}\")\n",
    "if coqa_samples:\n",
    "    for i in range(min(3, len(coqa_samples))):\n",
    "        print(f\"  Example {i+1} (domain={coqa_samples[i].get('source_domain', '?')}):\")\n",
    "        print(f\"    Q: {coqa_samples[i]['query']}\")\n",
    "        print(f\"    A: {coqa_samples[i]['answer']}\")\n",
    "        print(f\"    Passage (first 120 chars): {coqa_samples[i]['passage'][:120]}...\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 7: Unified sample pool + tokenization + pre-screening\n",
    "print(\"=\" * 70)\n",
    "print(\"UNIFIED SAMPLE POOL\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "all_samples = []\n",
    "for ds_name, ds_samples in [(\"drop\", drop_samples),\n",
    "                              (\"adversarialqa\", aqa_samples),\n",
    "                              (\"coqa\", coqa_samples)]:\n",
    "    for sample in ds_samples:\n",
    "        sample['dataset'] = ds_name\n",
    "    all_samples.extend(ds_samples)\n",
    "\n",
    "print(f\"Total samples: {len(all_samples)}\")\n",
    "for ds_name in ['drop', 'adversarialqa', 'coqa']:\n",
    "    ds_s = [s for s in all_samples if s['dataset'] == ds_name]\n",
    "    wcs = [s['word_count'] for s in ds_s]\n",
    "    print(f\"  {ds_name}: n={len(ds_s)}, mean_words={np.mean(wcs):.0f}, \"\n",
    "          f\"range=[{min(wcs)}, {max(wcs)}]\")\n",
    "\n",
    "# Tokenize prefix\n",
    "sf_str = SURROGATE_PREFIX_TEMPLATE.format(surrogate=STATIC_FACT)\n",
    "sf_ids = tokenizer(sf_str, return_tensors=\"pt\",\n",
    "                    add_special_tokens=False)['input_ids'].to(exp_config.device)\n",
    "PREFIX_TOKEN_LEN = sf_ids.shape[1]\n",
    "\n",
    "print(f\"\\nPrefix: '{STATIC_FACT}'\")\n",
    "print(f\"  Token length (no BOS): {PREFIX_TOKEN_LEN}\")\n",
    "\n",
    "# Verify sliding window safety\n",
    "max_primed_seq = 1 + PREFIX_TOKEN_LEN + MAX_DOC_TOKENS\n",
    "print(f\"  Max primed sequence: 1 + {PREFIX_TOKEN_LEN} + {MAX_DOC_TOKENS} = {max_primed_seq}\")\n",
    "print(f\"  Sliding window: 1024\")\n",
    "assert max_primed_seq < 1024, f\"UNSAFE: {max_primed_seq} >= 1024\"\n",
    "print(f\"  SAFE: {max_primed_seq} < 1024\")\n",
    "\n",
    "# Tokenize doc lengths\n",
    "print(f\"\\nTokenizing documents to measure token lengths...\")\n",
    "n_truncated = 0\n",
    "for sample in tqdm(all_samples, desc=\"Tokenizing\"):\n",
    "    tok_len = len(tokenizer.encode(sample['passage'], add_special_tokens=False))\n",
    "    if tok_len > MAX_DOC_TOKENS:\n",
    "        n_truncated += 1\n",
    "    sample['doc_token_len'] = min(tok_len, MAX_DOC_TOKENS)\n",
    "    sample['answer_token_len'] = len(tokenizer.encode(sample['answer'], add_special_tokens=False))\n",
    "\n",
    "print(f\"  Documents truncated to {MAX_DOC_TOKENS}: {n_truncated}/{len(all_samples)} \"\n",
    "      f\"({100*n_truncated/len(all_samples):.0f}%)\")\n",
    "\n",
    "for ds_name in ['drop', 'adversarialqa', 'coqa']:\n",
    "    ds_s = [s for s in all_samples if s['dataset'] == ds_name]\n",
    "    tls = [s['doc_token_len'] for s in ds_s]\n",
    "    atls = [s['answer_token_len'] for s in ds_s]\n",
    "    n_trunc = sum(1 for s in ds_s if s['doc_token_len'] == MAX_DOC_TOKENS)\n",
    "    print(f\"  {ds_name}: mean_tok={np.mean(tls):.0f}, median={np.median(tls):.0f}, \"\n",
    "          f\"truncated={n_trunc}/{len(ds_s)} ({100*n_trunc/len(ds_s):.0f}%), \"\n",
    "          f\"mean_ans_tok={np.mean(atls):.1f}\")\n",
    "\n",
    "# === PRE-SCREENING: Bare NLL check ===\n",
    "# Quick bare NLL on 20 samples per dataset to check for ceiling effects\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"PRE-SCREENING: Bare NLL distribution check (20 samples/dataset)\")\n",
    "print(\"If median bare NLL < 0.05, ceiling effects may dominate.\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for ds_name in ['drop', 'adversarialqa', 'coqa']:\n",
    "    ds_s = [s for s in all_samples if s['dataset'] == ds_name][:20]\n",
    "    bare_nlls = []\n",
    "    for sample in ds_s:\n",
    "        passage = sample['passage']\n",
    "        question = sample['query']\n",
    "        answer = sample['answer']\n",
    "\n",
    "        document_text = DOCUMENT_TEMPLATE.format(document=passage)\n",
    "        query_prompt = QUERY_TEMPLATE.format(question=question)\n",
    "        answer_text = ANSWER_TEMPLATE.format(answer=answer)\n",
    "\n",
    "        doc_ids = tokenizer(document_text, return_tensors=\"pt\",\n",
    "                            add_special_tokens=False)['input_ids'].to(exp_config.device)\n",
    "        if doc_ids.shape[1] > MAX_DOC_TOKENS:\n",
    "            doc_ids = doc_ids[:, :MAX_DOC_TOKENS]\n",
    "\n",
    "        bos_id = tokenizer.bos_token_id\n",
    "        if bos_id is None:\n",
    "            bos_id = tokenizer.encode(\"\", add_special_tokens=True)[0]\n",
    "        bos_tensor = torch.tensor([[bos_id]], device=exp_config.device)\n",
    "        bare_input = torch.cat([bos_tensor, doc_ids], dim=1)\n",
    "        context_len = bare_input.shape[1]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            bare_out = model(input_ids=bare_input,\n",
    "                             attention_mask=torch.ones_like(bare_input),\n",
    "                             use_cache=True, return_dict=True)\n",
    "        bare_cache = _ensure_dynamic_cache(bare_out.past_key_values)\n",
    "        del bare_out\n",
    "\n",
    "        nll = score_answer_with_cache(\n",
    "            deepcopy_cache(bare_cache), context_len,\n",
    "            query_prompt, answer_text, model, tokenizer, exp_config)\n",
    "        bare_nlls.append(nll)\n",
    "        del bare_cache, bare_input, doc_ids\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    bare_arr = np.array(bare_nlls)\n",
    "    pct_floor = 100 * np.mean(bare_arr < 0.01)\n",
    "    median = np.median(bare_arr)\n",
    "    mean = np.mean(bare_arr)\n",
    "    status = \"WARNING: CEILING\" if pct_floor > 50 else \"OK\" if pct_floor < 30 else \"MARGINAL\"\n",
    "    print(f\"  {ds_name:15s}: median={median:.3f}, mean={mean:.3f}, \"\n",
    "          f\"pct_floor(<0.01)={pct_floor:.0f}% -> {status}\")\n",
    "\n",
    "print(\"\\nPre-screening complete. Proceeding with full experiment.\")\n",
    "\n",
    "# Condition explanation\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"EXPERIMENTAL CONDITIONS (Gemma 3 4B)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\n### 1. bare ###\")\n",
    "print(\"  Forward: [BOS][doc]\")\n",
    "print(\"  Baseline. Standard causal attention.\")\n",
    "\n",
    "print(\"\\n### 2. sf_trunc (standard priming) ###\")\n",
    "print(f\"  Forward: [BOS][prefix_{PREFIX_TOKEN_LEN}][doc]\")\n",
    "print(\"  Standard causal, truncate + RoPE. Keys carry negative interference on Gemma.\")\n",
    "\n",
    "print(\"\\n### 3. sf_trunc_bias2 (attention forcing, bias=+2.0) ###\")\n",
    "print(f\"  Forward: [BOS][prefix_{PREFIX_TOKEN_LEN}][doc] with +2.0 bias\")\n",
    "print(\"  Novel: amplifies doc->prefix attention during cache building.\")\n",
    "\n",
    "print(\"\\n### 4. values_only (all layers) ###\")\n",
    "print(\"  Bare keys + all primed values from sf_trunc cache.\")\n",
    "print(\"  Expected d ~ +0.056 (Exp 16 on MARCO). Bypasses key interference.\")\n",
    "\n",
    "print(\"\\n### 5. values_early (layers 0-15 only) ###\")\n",
    "print(\"  Bare keys + primed values from layers 0-15 only.\")\n",
    "print(\"  Expected best: d ~ +0.211 (Exp 19 on MARCO). Late layers carry interference.\")\n",
    "\n",
    "print(\"\\n### 6. values_hero (layers {10,12,14,15,20}) ###\")\n",
    "print(\"  Bare keys + primed values from 5 hero layers identified in Exp 24.\")\n",
    "print(\"  NQ generalization: d=+0.213 (Exp 27b).\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 8: Helper functions\n",
    "\n",
    "def build_biased_causal_mask(total_len, prefix_start, prefix_end, bias_value, dtype, device):\n",
    "    \"\"\"Build a 4D causal attention mask with logit bias on doc->prefix attention.\"\"\"\n",
    "    mask = torch.zeros((total_len, total_len), dtype=dtype, device=device)\n",
    "    causal = torch.triu(\n",
    "        torch.ones(total_len, total_len, dtype=torch.bool, device=device),\n",
    "        diagonal=1\n",
    "    )\n",
    "    mask.masked_fill_(causal, float('-inf'))\n",
    "\n",
    "    if bias_value != 0.0:\n",
    "        doc_start = prefix_end\n",
    "        mask[doc_start:, prefix_start:prefix_end] += bias_value\n",
    "\n",
    "    return mask.unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "\n",
    "def run_single_sample(sample, model, tokenizer, exp_config, sf_ids, sf_str,\n",
    "                      PREFIX_TOKEN_LEN, N_LAYERS, EARLY_LAYER_CUTOFF, HERO_LAYERS):\n",
    "    \"\"\"Run all 6 conditions for a single sample. Returns dict of NLLs + metadata.\"\"\"\n",
    "    passage = sample['passage']\n",
    "    query = sample['query']\n",
    "    answer = sample['answer']\n",
    "    ds_name = sample['dataset']\n",
    "\n",
    "    query_prompt = QUERY_TEMPLATE.format(question=query)\n",
    "    answer_text = ANSWER_TEMPLATE.format(answer=answer)\n",
    "    document_text = DOCUMENT_TEMPLATE.format(document=passage)\n",
    "\n",
    "    # === Matched tokenization ===\n",
    "    full_text = sf_str + document_text\n",
    "    full_enc = tokenizer(full_text, return_tensors=\"pt\",\n",
    "                          add_special_tokens=True, padding=False, truncation=False)\n",
    "    full_ids = full_enc['input_ids'].to(exp_config.device)\n",
    "\n",
    "    sf_prefix_enc = tokenizer(sf_str, return_tensors=\"pt\",\n",
    "                               add_special_tokens=True, padding=False, truncation=False)\n",
    "    sf_prefix_len_with_bos = sf_prefix_enc['input_ids'].shape[1]\n",
    "\n",
    "    bos_id = full_ids[:, :1]\n",
    "    doc_ids = full_ids[:, sf_prefix_len_with_bos:]\n",
    "\n",
    "    # Truncate long docs\n",
    "    if doc_ids.shape[1] > MAX_DOC_TOKENS:\n",
    "        doc_ids = doc_ids[:, :MAX_DOC_TOKENS]\n",
    "\n",
    "    doc_len = doc_ids.shape[1]\n",
    "    context_len = 1 + doc_len  # BOS + doc\n",
    "\n",
    "    del full_enc, full_ids, sf_prefix_enc\n",
    "\n",
    "    # === 1. BARE ===\n",
    "    bare_input = torch.cat([bos_id, doc_ids], dim=1)\n",
    "    with torch.no_grad():\n",
    "        bare_out = model(input_ids=bare_input,\n",
    "                         attention_mask=torch.ones_like(bare_input),\n",
    "                         use_cache=True, return_dict=True)\n",
    "    bare_cache = _ensure_dynamic_cache(bare_out.past_key_values)\n",
    "    del bare_out\n",
    "\n",
    "    bare_nll = score_answer_with_cache(\n",
    "        deepcopy_cache(bare_cache), context_len,\n",
    "        query_prompt, answer_text, model, tokenizer, exp_config)\n",
    "\n",
    "    # === 2. sf_trunc (standard priming, bias=0) ===\n",
    "    primed_input = torch.cat([bos_id, sf_ids, doc_ids], dim=1)\n",
    "    total_seq_len = primed_input.shape[1]\n",
    "    prefix_start = 1\n",
    "    prefix_end = 1 + sf_ids.shape[1]\n",
    "    prefix_offset = sf_ids.shape[1]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        primed_out = model(input_ids=primed_input,\n",
    "                           attention_mask=torch.ones_like(primed_input),\n",
    "                           use_cache=True, return_dict=True)\n",
    "    primed_full_std = _ensure_dynamic_cache(primed_out.past_key_values)\n",
    "    del primed_out\n",
    "\n",
    "    trunc_raw = extract_and_truncate_cache_with_bos(primed_full_std, doc_len)\n",
    "    del primed_full_std\n",
    "\n",
    "    sf_trunc_cache = deepcopy_cache(trunc_raw)\n",
    "    correct_rope_positions_with_bos(sf_trunc_cache, prefix_offset, model)\n",
    "    del trunc_raw\n",
    "\n",
    "    sf_trunc_nll = score_answer_with_cache(\n",
    "        deepcopy_cache(sf_trunc_cache), context_len,\n",
    "        query_prompt, answer_text, model, tokenizer, exp_config)\n",
    "\n",
    "    # === 3. sf_trunc_bias2 (attention forcing, bias=+2.0) ===\n",
    "    mask_4d = build_biased_causal_mask(\n",
    "        total_seq_len, prefix_start, prefix_end,\n",
    "        2.0, model.dtype, exp_config.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        primed_out = model(input_ids=primed_input,\n",
    "                           attention_mask=mask_4d,\n",
    "                           use_cache=True, return_dict=True)\n",
    "    primed_full_b2 = _ensure_dynamic_cache(primed_out.past_key_values)\n",
    "    del primed_out, mask_4d\n",
    "\n",
    "    trunc_raw = extract_and_truncate_cache_with_bos(primed_full_b2, doc_len)\n",
    "    del primed_full_b2\n",
    "\n",
    "    bias2_cache = deepcopy_cache(trunc_raw)\n",
    "    correct_rope_positions_with_bos(bias2_cache, prefix_offset, model)\n",
    "    del trunc_raw\n",
    "\n",
    "    bias2_nll = score_answer_with_cache(\n",
    "        deepcopy_cache(bias2_cache), context_len,\n",
    "        query_prompt, answer_text, model, tokenizer, exp_config)\n",
    "    del bias2_cache\n",
    "\n",
    "    # === 4. values_only (all layers) ===\n",
    "    values_all_cache = deepcopy_cache(bare_cache)\n",
    "    for layer_idx in range(N_LAYERS):\n",
    "        primed_vals = _get_cache_values(sf_trunc_cache, layer_idx)\n",
    "        _set_cache_values(values_all_cache, layer_idx, primed_vals.clone())\n",
    "\n",
    "    values_only_nll = score_answer_with_cache(\n",
    "        deepcopy_cache(values_all_cache), context_len,\n",
    "        query_prompt, answer_text, model, tokenizer, exp_config)\n",
    "    del values_all_cache\n",
    "\n",
    "    # === 5. values_early (layers 0 to EARLY_LAYER_CUTOFF-1) ===\n",
    "    early_layers = list(range(EARLY_LAYER_CUTOFF))\n",
    "    values_early_cache = replace_values_at_layers(bare_cache, sf_trunc_cache, early_layers)\n",
    "\n",
    "    values_early_nll = score_answer_with_cache(\n",
    "        deepcopy_cache(values_early_cache), context_len,\n",
    "        query_prompt, answer_text, model, tokenizer, exp_config)\n",
    "    del values_early_cache\n",
    "\n",
    "    # === 6. values_hero (hero layers only) ===\n",
    "    values_hero_cache = replace_values_at_layers(bare_cache, sf_trunc_cache, HERO_LAYERS)\n",
    "\n",
    "    values_hero_nll = score_answer_with_cache(\n",
    "        deepcopy_cache(values_hero_cache), context_len,\n",
    "        query_prompt, answer_text, model, tokenizer, exp_config)\n",
    "    del values_hero_cache\n",
    "\n",
    "    del bare_cache, sf_trunc_cache, bare_input, primed_input\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return {\n",
    "        'dataset': ds_name,\n",
    "        'query': query,\n",
    "        'answer': answer,\n",
    "        'word_count': sample['word_count'],\n",
    "        'doc_token_len': doc_len,\n",
    "        'answer_token_len': sample.get('answer_token_len', 0),\n",
    "        'bare': bare_nll,\n",
    "        'sf_trunc': sf_trunc_nll,\n",
    "        'sf_trunc_bias2': bias2_nll,\n",
    "        'values_only': values_only_nll,\n",
    "        'values_early': values_early_nll,\n",
    "        'values_hero': values_hero_nll,\n",
    "    }\n",
    "\n",
    "\n",
    "# Verify mask for a toy example\n",
    "print(\"Mask verification (toy: BOS + 3 prefix + 5 doc = 9 total):\")\n",
    "toy_mask = build_biased_causal_mask(9, 1, 4, 2.0, model.dtype, 'cpu')\n",
    "m = toy_mask.squeeze()\n",
    "print(f\"  Shape: {toy_mask.shape}\")\n",
    "print(f\"  Doc->Prefix bias (row 4, col 1): {m[4, 1].item():.1f} (expect +2.0)\")\n",
    "print(f\"  Causal mask (row 3, col 5): {m[3, 5].item()} (expect -inf)\")\n",
    "del toy_mask, m\n",
    "print(\"OK\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 9: Main experiment loop\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"EXPERIMENT 29: {len(all_samples)} samples, {len(CONDITION_NAMES)} conditions\")\n",
    "print(f\"Model: Gemma 3 4B, MAX_DOC_TOKENS: {MAX_DOC_TOKENS}\")\n",
    "print(f\"Datasets: DROP, AdversarialQA, CoQA\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Checkpoint resume\n",
    "all_results = []\n",
    "start_idx = 0\n",
    "\n",
    "if CHECKPOINT_PATH.exists():\n",
    "    with open(CHECKPOINT_PATH, 'r') as f:\n",
    "        ckpt = json.load(f)\n",
    "    ckpt_queries = ckpt.get('sample_queries', [])\n",
    "    current_queries = [s['query'] for s in all_samples]\n",
    "    if ckpt_queries == current_queries:\n",
    "        all_results = ckpt['results']\n",
    "        start_idx = len(all_results)\n",
    "        print(f\"Resuming from checkpoint: {start_idx}/{len(all_samples)}\")\n",
    "    else:\n",
    "        print(\"Checkpoint query mismatch. Starting fresh.\")\n",
    "else:\n",
    "    print(\"No checkpoint found. Starting fresh.\")\n",
    "\n",
    "t_start = time.time()\n",
    "N_TOTAL = len(all_samples)\n",
    "\n",
    "for qidx in tqdm(range(start_idx, N_TOTAL), initial=start_idx, total=N_TOTAL,\n",
    "                  desc=\"Exp 29\"):\n",
    "    sample = all_samples[qidx]\n",
    "\n",
    "    result = run_single_sample(\n",
    "        sample, model, tokenizer, exp_config,\n",
    "        sf_ids, sf_str, PREFIX_TOKEN_LEN, N_LAYERS,\n",
    "        EARLY_LAYER_CUTOFF, HERO_LAYERS)\n",
    "    result['query_idx'] = qidx\n",
    "    all_results.append(result)\n",
    "\n",
    "    # Checkpoint\n",
    "    if (qidx + 1) % CHECKPOINT_EVERY == 0 or qidx == N_TOTAL - 1:\n",
    "        ckpt_data = {\n",
    "            'results': all_results,\n",
    "            'sample_queries': [s['query'] for s in all_samples],\n",
    "            'completed': len(all_results),\n",
    "            'total': N_TOTAL,\n",
    "            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        }\n",
    "        with open(CHECKPOINT_PATH, 'w') as f:\n",
    "            json.dump(ckpt_data, f)\n",
    "        elapsed = time.time() - t_start\n",
    "        n_done = qidx - start_idx + 1\n",
    "        rate = n_done / elapsed if elapsed > 0 else 0\n",
    "        remaining = (N_TOTAL - qidx - 1) / rate if rate > 0 else 0\n",
    "        tqdm.write(f\"  Checkpoint {qidx+1}/{N_TOTAL} | {n_done} done in {elapsed/60:.1f}m | \"\n",
    "                   f\"ETA: {remaining/60:.1f} min\")\n",
    "\n",
    "elapsed_total = time.time() - t_start\n",
    "print(f\"\\nExperiment complete: {len(all_results)} samples in {elapsed_total/60:.1f} min\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 10: Per-dataset analysis\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ANALYSIS: PER-DATASET RESULTS (Gemma 3 4B)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "dataset_names = ['drop', 'adversarialqa', 'coqa']\n",
    "analysis = {}\n",
    "\n",
    "for ds_name in dataset_names:\n",
    "    ds_results = [r for r in all_results if r['dataset'] == ds_name]\n",
    "    n_ds = len(ds_results)\n",
    "    if n_ds == 0:\n",
    "        continue\n",
    "\n",
    "    bare_arr = np.array([r['bare'] for r in ds_results])\n",
    "\n",
    "    # Filter invalid (but keep zeros \u2014 they ARE valid for some datasets)\n",
    "    valid = np.isfinite(bare_arr)\n",
    "    for cname in CONDITION_NAMES:\n",
    "        if cname == 'bare':\n",
    "            continue\n",
    "        c_arr = np.array([r[cname] for r in ds_results])\n",
    "        valid &= np.isfinite(c_arr)\n",
    "\n",
    "    n_valid = int(np.sum(valid))\n",
    "\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    pct_floor = 100 * np.mean(bare_arr < 0.01)\n",
    "    print(f\"DATASET: {ds_name.upper()} (n={n_valid}/{n_ds}, \"\n",
    "          f\"median bare NLL={np.median(bare_arr):.3f}, \"\n",
    "          f\"pct_floor={pct_floor:.0f}%)\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "    print(f\"\\n{'Condition':<20} {'Mean Bare':>10} {'Mean Cond':>10} \"\n",
    "          f\"{'Mean D':>10} {'d':>8} {'Win%':>7} {'p':>12} {'sig':>5}\")\n",
    "    print(\"-\" * 90)\n",
    "\n",
    "    ds_analysis = {}\n",
    "    for cname in CONDITION_NAMES:\n",
    "        if cname == 'bare':\n",
    "            continue\n",
    "        c_arr = np.array([r[cname] for r in ds_results])\n",
    "        delta = bare_arr[valid] - c_arr[valid]\n",
    "        d = cohens_d(delta)\n",
    "        win = np.mean(delta > 0) * 100\n",
    "        t_stat, p_val = stats.ttest_1samp(delta, 0)\n",
    "        sig = '***' if p_val < 0.001 else '**' if p_val < 0.01 else '*' if p_val < 0.05 else 'ns'\n",
    "        print(f\"{cname:<20} {np.mean(bare_arr[valid]):>10.4f} {np.mean(c_arr[valid]):>10.4f} \"\n",
    "              f\"{np.mean(delta):>+10.4f} {d:>+8.3f} {win:>6.1f}% {p_val:>12.2e} {sig:>5}\")\n",
    "        ds_analysis[cname] = {\n",
    "            'n_valid': n_valid,\n",
    "            'mean_bare': float(np.mean(bare_arr[valid])),\n",
    "            'mean_cond': float(np.mean(c_arr[valid])),\n",
    "            'mean_delta': float(np.mean(delta)),\n",
    "            'cohens_d': float(d),\n",
    "            'win_pct': float(win),\n",
    "            't_stat': float(t_stat),\n",
    "            'p_value': float(p_val),\n",
    "        }\n",
    "\n",
    "    analysis[ds_name] = ds_analysis\n",
    "\n",
    "# Cross-dataset summary table\n",
    "print(f\"\\n\\n{'='*90}\")\n",
    "print(\"CROSS-DATASET SUMMARY: Cohen's d vs bare (Gemma 3 4B)\")\n",
    "print(f\"{'='*90}\")\n",
    "print(f\"\\n{'Condition':<20}\", end='')\n",
    "for ds in dataset_names:\n",
    "    print(f\"{'  ' + ds:>16}\", end='')\n",
    "print()\n",
    "print(\"-\" * 68)\n",
    "for cname in CONDITION_NAMES:\n",
    "    if cname == 'bare':\n",
    "        continue\n",
    "    print(f\"{cname:<20}\", end='')\n",
    "    for ds in dataset_names:\n",
    "        if ds in analysis and cname in analysis[ds]:\n",
    "            d = analysis[ds][cname]['cohens_d']\n",
    "            p = analysis[ds][cname]['p_value']\n",
    "            sig = '***' if p < 0.001 else '**' if p < 0.01 else '*' if p < 0.05 else ''\n",
    "            print(f\"{d:>+12.3f}{sig:>4}\", end='')\n",
    "        else:\n",
    "            print(f\"{'n/a':>16}\", end='')\n",
    "    print()\n",
    "\n",
    "# Bare NLL distributions\n",
    "print(f\"\\n\\nBARE NLL DISTRIBUTIONS (ceiling effect check):\")\n",
    "for ds in dataset_names:\n",
    "    ds_r = [r for r in all_results if r['dataset'] == ds]\n",
    "    bare = [r['bare'] for r in ds_r]\n",
    "    pct_zero = 100 * np.mean(np.array(bare) < 0.01)\n",
    "    iqr = np.percentile(bare, 75) - np.percentile(bare, 25)\n",
    "    print(f\"  {ds:15s}: mean={np.mean(bare):.3f}, median={np.median(bare):.3f}, \"\n",
    "          f\"IQR={iqr:.3f}, pct_floor={pct_zero:.0f}%\")\n",
    "\n",
    "# Compare with Exp 27b (Gemma on previous datasets)\n",
    "print(f\"\\n\\n{'='*90}\")\n",
    "print(\"COMPARISON WITH EXP 27b (Gemma: TriviaQA, NQ, HotpotQA)\")\n",
    "print(f\"{'='*90}\")\n",
    "print(\"\\nExp 27b results (for reference):\")\n",
    "print(\"  TriviaQA: values_hero d=+0.000 (77% at floor)\")\n",
    "print(\"  NQ:       values_hero d=+0.213*** (55% at floor)\")\n",
    "print(\"  HotpotQA: values_hero d=-0.069 (56% at floor)\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 11: Length stratification + hardness interaction\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"LENGTH STRATIFICATION (Gemma 3 4B)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "length_strat = {}\n",
    "\n",
    "for ds_name in dataset_names:\n",
    "    ds_results = [r for r in all_results if r['dataset'] == ds_name]\n",
    "    if not ds_results:\n",
    "        continue\n",
    "\n",
    "    print(f\"\\n--- {ds_name.upper()} ---\")\n",
    "    ds_length_data = {}\n",
    "\n",
    "    for cname in ['sf_trunc', 'sf_trunc_bias2', 'values_only', 'values_early', 'values_hero']:\n",
    "        print(f\"  {cname}:\")\n",
    "        bin_ds = []\n",
    "        for bin_label, bin_min, bin_max in LENGTH_BINS:\n",
    "            bin_results = [r for r in ds_results\n",
    "                          if bin_min <= r['doc_token_len'] < bin_max]\n",
    "            n_bin = len(bin_results)\n",
    "            if n_bin < 10:\n",
    "                print(f\"    {bin_label}: n={n_bin} (too few)\")\n",
    "                bin_ds.append({'label': bin_label, 'n': n_bin, 'd': None})\n",
    "                continue\n",
    "            bare = np.array([r['bare'] for r in bin_results])\n",
    "            cond = np.array([r[cname] for r in bin_results])\n",
    "            delta = bare - cond\n",
    "            d = cohens_d(delta)\n",
    "            _, p_val = stats.ttest_1samp(delta, 0)\n",
    "            sig = '***' if p_val < 0.001 else '**' if p_val < 0.01 else '*' if p_val < 0.05 else 'ns'\n",
    "            print(f\"    {bin_label}: n={n_bin}, d={d:+.3f}, p={p_val:.2e} {sig}\")\n",
    "            bin_ds.append({'label': bin_label, 'n': n_bin, 'd': float(d), 'p': float(p_val)})\n",
    "        ds_length_data[cname] = bin_ds\n",
    "\n",
    "    length_strat[ds_name] = ds_length_data\n",
    "\n",
    "# === HARDNESS QUINTILE INTERACTION ===\n",
    "print(f\"\\n\\n{'='*70}\")\n",
    "print(\"HARDNESS QUINTILE INTERACTION (Gemma 3 4B)\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "hardness_data = {}\n",
    "quintile_labels = ['Q1(easy)', 'Q2', 'Q3', 'Q4', 'Q5(hard)']\n",
    "\n",
    "for ds_name in dataset_names:\n",
    "    ds_results = [r for r in all_results if r['dataset'] == ds_name]\n",
    "    if len(ds_results) < 50:\n",
    "        continue\n",
    "\n",
    "    bare_arr = np.array([r['bare'] for r in ds_results])\n",
    "    quintile_boundaries = np.percentile(bare_arr, [20, 40, 60, 80])\n",
    "    print(f\"\\n--- {ds_name.upper()} (boundaries: {[f'{b:.3f}' for b in quintile_boundaries]}) ---\")\n",
    "\n",
    "    def get_quintile(nll):\n",
    "        for i, b in enumerate(quintile_boundaries):\n",
    "            if nll <= b:\n",
    "                return i\n",
    "        return 4\n",
    "\n",
    "    quintiles = np.array([get_quintile(r['bare']) for r in ds_results])\n",
    "\n",
    "    ds_hardness = {}\n",
    "    for cname in ['sf_trunc_bias2', 'values_only', 'values_early', 'values_hero']:\n",
    "        cond_arr = np.array([r[cname] for r in ds_results])\n",
    "        delta = bare_arr - cond_arr\n",
    "        q_header = \"\".join(f\"{ql:>12}\" for ql in quintile_labels) + f\"{'Overall':>12}\"\n",
    "        row = f\"  {cname:<20}\"\n",
    "        q_ds = []\n",
    "        for q in range(5):\n",
    "            mask_q = quintiles == q\n",
    "            n_q = int(np.sum(mask_q))\n",
    "            if n_q < 5:\n",
    "                row += f\"{'n/a':>12}\"\n",
    "                q_ds.append(None)\n",
    "            else:\n",
    "                d_q = cohens_d(delta[mask_q])\n",
    "                row += f\"{d_q:>+12.3f}\"\n",
    "                q_ds.append(float(d_q))\n",
    "        d_all = cohens_d(delta)\n",
    "        row += f\"{d_all:>+12.3f}\"\n",
    "        print(f\"  {'':20}\" + q_header)\n",
    "        print(row)\n",
    "        ds_hardness[cname] = q_ds\n",
    "\n",
    "    hardness_data[ds_name] = ds_hardness"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 12: Multi-panel figure\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 14))\n",
    "\n",
    "colors = {\n",
    "    'sf_trunc': '#1f77b4',\n",
    "    'sf_trunc_bias2': '#d62728',\n",
    "    'values_only': '#7f7f7f',\n",
    "    'values_early': '#2ca02c',\n",
    "    'values_hero': '#ff7f0e',\n",
    "}\n",
    "\n",
    "# ---- Panel (a): Cohen's d by dataset x condition ----\n",
    "ax = axes[0, 0]\n",
    "x = np.arange(len(dataset_names))\n",
    "width = 0.15\n",
    "conds_plot = ['sf_trunc', 'sf_trunc_bias2', 'values_only', 'values_early', 'values_hero']\n",
    "for i, cname in enumerate(conds_plot):\n",
    "    ds_vals = []\n",
    "    for ds in dataset_names:\n",
    "        if ds in analysis and cname in analysis[ds]:\n",
    "            ds_vals.append(analysis[ds][cname]['cohens_d'])\n",
    "        else:\n",
    "            ds_vals.append(0)\n",
    "    offset = (i - 2) * width\n",
    "    bars = ax.bar(x + offset, ds_vals, width, label=cname, color=colors[cname],\n",
    "                  edgecolor='black', linewidth=0.5)\n",
    "    for j, val in enumerate(ds_vals):\n",
    "        ax.text(x[j] + offset, val + (0.01 if val >= 0 else -0.03),\n",
    "                f\"{val:+.2f}\", ha='center', va='bottom' if val >= 0 else 'top', fontsize=6)\n",
    "\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([ds.upper() for ds in dataset_names])\n",
    "ax.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "ax.set_ylabel(\"Cohen's d (positive = helps)\")\n",
    "ax.set_title(\"(a) Gemma 3 4B: Effect Size by Dataset x Condition\")\n",
    "ax.legend(fontsize=6, loc='best')\n",
    "\n",
    "# ---- Panel (b): Length stratification for values_hero across datasets ----\n",
    "ax = axes[0, 1]\n",
    "for ds_name in dataset_names:\n",
    "    if ds_name not in length_strat:\n",
    "        continue\n",
    "    cname = 'values_hero'\n",
    "    if cname not in length_strat[ds_name]:\n",
    "        continue\n",
    "    bins_data = length_strat[ds_name][cname]\n",
    "    valid_idx = [i for i, b in enumerate(bins_data) if b['d'] is not None]\n",
    "    if valid_idx:\n",
    "        x_vals = valid_idx\n",
    "        y_vals = [bins_data[i]['d'] for i in valid_idx]\n",
    "        ns = [bins_data[i]['n'] for i in valid_idx]\n",
    "        ax.plot(x_vals, y_vals, marker='o', linewidth=2, markersize=6, label=ds_name)\n",
    "        for xv, yv, n in zip(x_vals, y_vals, ns):\n",
    "            ax.annotate(f\"n={n}\", (xv, yv), fontsize=6, textcoords=\"offset points\",\n",
    "                       xytext=(0, 8), ha='center')\n",
    "\n",
    "bin_labels_all = [b[0] for b in LENGTH_BINS]\n",
    "ax.set_xticks(range(len(bin_labels_all)))\n",
    "ax.set_xticklabels(bin_labels_all, rotation=30, ha='right', fontsize=8)\n",
    "ax.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "ax.set_ylabel(\"Cohen's d\")\n",
    "ax.set_xlabel(\"Document Token Length Bin\")\n",
    "ax.set_title(\"(b) values_hero by Length Bin\")\n",
    "ax.legend(fontsize=8)\n",
    "\n",
    "# ---- Panel (c): Hardness heatmap for values_hero ----\n",
    "ax = axes[1, 0]\n",
    "hm_rows = []\n",
    "hm_ylabels = []\n",
    "for ds_name in dataset_names:\n",
    "    if ds_name in hardness_data and 'values_hero' in hardness_data[ds_name]:\n",
    "        row = hardness_data[ds_name]['values_hero']\n",
    "        hm_rows.append([v if v is not None else 0 for v in row])\n",
    "        hm_ylabels.append(ds_name.upper())\n",
    "\n",
    "if hm_rows:\n",
    "    hm_arr = np.array(hm_rows)\n",
    "    im = ax.imshow(hm_arr, cmap='RdBu', aspect='auto', vmin=-0.5, vmax=0.5)\n",
    "    ax.set_xticks(range(5))\n",
    "    ax.set_xticklabels(quintile_labels, fontsize=8)\n",
    "    ax.set_yticks(range(len(hm_ylabels)))\n",
    "    ax.set_yticklabels(hm_ylabels)\n",
    "    for i in range(len(hm_ylabels)):\n",
    "        for j in range(5):\n",
    "            val = hm_arr[i, j]\n",
    "            ax.text(j, i, f\"{val:+.2f}\", ha='center', va='center',\n",
    "                    fontsize=9, color='white' if abs(val) > 0.25 else 'black')\n",
    "    fig.colorbar(im, ax=ax, shrink=0.8, label=\"Cohen's d\")\n",
    "ax.set_title(\"(c) values_hero: Hardness x Dataset\")\n",
    "\n",
    "# ---- Panel (d): Bare NLL distributions (box/violin) ----\n",
    "ax = axes[1, 1]\n",
    "bare_by_ds = []\n",
    "ds_labels_plot = []\n",
    "for ds in dataset_names:\n",
    "    ds_r = [r for r in all_results if r['dataset'] == ds]\n",
    "    bare_by_ds.append([r['bare'] for r in ds_r])\n",
    "    ds_labels_plot.append(ds.upper())\n",
    "\n",
    "# Also add Exp 27b reference datasets for comparison\n",
    "# (hardcoded from prior results)\n",
    "ref_medians = {'TRIVIAQA\\n(27b)': 0.000, 'NQ\\n(27b)': 0.006, 'HOTPOTQA\\n(27b)': 0.003}\n",
    "\n",
    "bp = ax.boxplot(bare_by_ds, labels=ds_labels_plot, showfliers=False, patch_artist=True,\n",
    "                medianprops={'color': 'red', 'linewidth': 2})\n",
    "for patch in bp['boxes']:\n",
    "    patch.set_facecolor('#8ecae6')\n",
    "    patch.set_alpha(0.7)\n",
    "\n",
    "# Add reference lines for 27b medians\n",
    "for i, (label, med) in enumerate(ref_medians.items()):\n",
    "    ax.axhline(y=med, color='gray', linestyle=':', alpha=0.4)\n",
    "ax.axhline(y=0.01, color='red', linestyle='--', alpha=0.3, label='Floor threshold (0.01)')\n",
    "\n",
    "ax.set_ylabel(\"Bare NLL\")\n",
    "ax.set_title(\"(d) Bare NLL Distributions (ceiling check)\")\n",
    "ax.legend(fontsize=7)\n",
    "\n",
    "plt.suptitle('Exp 29: Hard QA Datasets (Gemma 3 4B)', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'analysis_plots.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"Plots saved to {RESULTS_DIR / 'analysis_plots.png'}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 13: Save results.json + CSV\n",
    "\n",
    "# --- CSV ---\n",
    "with open(CSV_PATH, 'w', newline='') as f:\n",
    "    fieldnames = ['query_idx', 'dataset', 'query', 'answer', 'word_count',\n",
    "                  'doc_token_len', 'answer_token_len',\n",
    "                  'bare', 'sf_trunc', 'sf_trunc_bias2',\n",
    "                  'values_only', 'values_early', 'values_hero']\n",
    "    writer = csv.DictWriter(f, fieldnames=fieldnames, extrasaction='ignore')\n",
    "    writer.writeheader()\n",
    "    for r in all_results:\n",
    "        writer.writerow({k: r.get(k, '') for k in fieldnames})\n",
    "print(f\"CSV saved: {CSV_PATH}\")\n",
    "\n",
    "# --- Verdict ---\n",
    "best_ds = None\n",
    "best_cond = None\n",
    "best_d = -999\n",
    "for ds_name in dataset_names:\n",
    "    if ds_name not in analysis:\n",
    "        continue\n",
    "    for cname in CONDITION_NAMES:\n",
    "        if cname == 'bare':\n",
    "            continue\n",
    "        if cname in analysis[ds_name]:\n",
    "            d = analysis[ds_name][cname]['cohens_d']\n",
    "            if d > best_d:\n",
    "                best_d = d\n",
    "                best_ds = ds_name\n",
    "                best_cond = cname\n",
    "\n",
    "if best_d > 0.15:\n",
    "    verdict = (f\"SUCCESS: Gemma toolkit generalizes! Best: {best_ds}/{best_cond} \"\n",
    "               f\"d={best_d:+.3f}\")\n",
    "elif best_d > 0.05:\n",
    "    verdict = (f\"PARTIAL: Weak generalization. Best: {best_ds}/{best_cond} \"\n",
    "               f\"d={best_d:+.3f}\")\n",
    "else:\n",
    "    verdict = (f\"FAILURE: Gemma toolkit does NOT generalize to hard QA datasets. \"\n",
    "               f\"Best: {best_ds}/{best_cond} d={best_d:+.3f}\")\n",
    "\n",
    "# Check values_hero on each dataset\n",
    "hero_results = {}\n",
    "for ds_name in dataset_names:\n",
    "    if ds_name in analysis and 'values_hero' in analysis[ds_name]:\n",
    "        hero_results[ds_name] = analysis[ds_name]['values_hero']['cohens_d']\n",
    "hero_verdict = \"values_hero results: \" + \", \".join(\n",
    "    f\"{ds}={d:+.3f}\" for ds, d in hero_results.items())\n",
    "\n",
    "# Check ceiling status\n",
    "ceiling_status = {}\n",
    "for ds_name in dataset_names:\n",
    "    ds_r = [r for r in all_results if r['dataset'] == ds_name]\n",
    "    pct_floor = 100 * np.mean(np.array([r['bare'] for r in ds_r]) < 0.01)\n",
    "    ceiling_status[ds_name] = pct_floor\n",
    "ceiling_verdict = \"Ceiling check: \" + \", \".join(\n",
    "    f\"{ds}={pct:.0f}% floor\" for ds, pct in ceiling_status.items())\n",
    "\n",
    "print(f\"\\nVERDICT: {verdict}\")\n",
    "print(f\"HERO: {hero_verdict}\")\n",
    "print(f\"CEILING: {ceiling_verdict}\")\n",
    "\n",
    "# --- results.json ---\n",
    "final = {\n",
    "    'experiment': 'exp29_hard_datasets_gemma',\n",
    "    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'config': {\n",
    "        'model_name': MODEL_NAME,\n",
    "        'model_type': 'gemma3',\n",
    "        'seed': SEED,\n",
    "        'n_per_dataset': N_PER_DATASET,\n",
    "        'max_doc_tokens': MAX_DOC_TOKENS,\n",
    "        'conditions': CONDITION_NAMES,\n",
    "        'early_layer_cutoff': EARLY_LAYER_CUTOFF,\n",
    "        'hero_layers': HERO_LAYERS,\n",
    "        'prefix': STATIC_FACT,\n",
    "        'prefix_token_len': PREFIX_TOKEN_LEN,\n",
    "        'datasets': dataset_names,\n",
    "        'length_bins': LENGTH_BINS,\n",
    "    },\n",
    "    'per_dataset_analysis': analysis,\n",
    "    'length_stratification': length_strat,\n",
    "    'hardness_data': hardness_data,\n",
    "    'ceiling_status': ceiling_status,\n",
    "    'verdict': verdict,\n",
    "    'hero_verdict': hero_verdict,\n",
    "    'ceiling_verdict': ceiling_verdict,\n",
    "    'per_sample_results': all_results,\n",
    "}\n",
    "\n",
    "with open(FINAL_RESULTS_PATH, 'w') as f:\n",
    "    json.dump(final, f, indent=2)\n",
    "\n",
    "print(f\"\\nResults saved to {FINAL_RESULTS_PATH}\")\n",
    "print(f\"File size: {FINAL_RESULTS_PATH.stat().st_size / 1024:.1f} KB\")\n",
    "\n",
    "# Final summary\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"SUMMARY -- Exp 29: Hard QA Datasets (Gemma 3 4B)\")\n",
    "print(\"=\" * 70)\n",
    "for ds_name in dataset_names:\n",
    "    if ds_name not in analysis:\n",
    "        continue\n",
    "    ds_r = [r for r in all_results if r['dataset'] == ds_name]\n",
    "    pct_floor = 100 * np.mean(np.array([r['bare'] for r in ds_r]) < 0.01)\n",
    "    print(f\"\\n  {ds_name.upper()} (floor: {pct_floor:.0f}%):\")\n",
    "    for cname in CONDITION_NAMES:\n",
    "        if cname == 'bare':\n",
    "            continue\n",
    "        if cname in analysis[ds_name]:\n",
    "            a = analysis[ds_name][cname]\n",
    "            sig = '***' if a['p_value'] < 0.001 else '**' if a['p_value'] < 0.01 else '*' if a['p_value'] < 0.05 else 'ns'\n",
    "            print(f\"    {cname:<20} d={a['cohens_d']:>+.3f}  win={a['win_pct']:.0f}%  {sig}\")\n",
    "\n",
    "print(f\"\\nVERDICT: {verdict}\")\n",
    "print(f\"HERO: {hero_verdict}\")\n",
    "print(f\"CEILING: {ceiling_verdict}\")\n",
    "print(f\"\\nDone!\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 14: GPU cleanup\n",
    "import gc\n",
    "\n",
    "print(\"Cleaning up GPU memory...\")\n",
    "mem_before = torch.cuda.memory_allocated() / 1e9\n",
    "\n",
    "del model\n",
    "del tokenizer\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "mem_after = torch.cuda.memory_allocated() / 1e9\n",
    "print(f\"GPU memory: {mem_before:.2f} GB -> {mem_after:.2f} GB\")\n",
    "print(\"Cleanup complete.\")"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}