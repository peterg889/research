{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Exp 05: Hardness-Gated Semantic Priming + LLM Surrogates\n\n## Motivation\n\nExp 01-03 consistently show priming helps MORE for hard samples (r=0.15-0.30 hardness\ncorrelation). But the overall effect is dragged to zero by easy samples where priming\ninterferes. This experiment asks: **if we only prime hard samples, does oracle beat random?**\n\nAlso tests LLM-generated surrogates — the production scenario where you generate a topic\nquery rather than having the oracle.\n\n## Design (two-pass)\n\n1. **First pass**: Score bare NLL on 4000 MS MARCO samples\n2. **Filter**: Keep hardest 50% (bare NLL > median) → ~2000 samples\n3. **Generate**: LLM surrogates for each hard sample using Mistral-7B chat\n4. **Second pass**: Full 5-condition eval on hard samples\n\n## Conditions (5)\n\n| # | Condition | Cache | Tests |\n|---|-----------|-------|-------|\n| 1 | Bare | `[BOS][doc]` | Baseline |\n| 2 | Oracle-truncated | `[BOS][query\\n][doc]` → truncate + RoPE | Semantic signal |\n| 3 | Random-truncated | `[BOS][random\\n][doc]` → truncate + RoPE | Structural control |\n| 4 | Separator-only | `[BOS][doc][\\n\\nRelated question: ]` | Framing effect |\n| 5 | LLM-surrogate-truncated | `[BOS][llm_query\\n][doc]` → truncate + RoPE | Production scenario |"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Setup\n",
    "import os\n",
    "os.umask(0o000)\n",
    "\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "RESULTS_DIR = Path(\"results/exp05\")\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CHECKPOINT_PATH = RESULTS_DIR / \"checkpoint.json\"\n",
    "SURROGATES_PATH = RESULTS_DIR / \"surrogates.json\"\n",
    "FINAL_RESULTS_PATH = RESULTS_DIR / \"results.json\"\n",
    "\n",
    "print(f\"SEED: {SEED}\")\n",
    "print(f\"Results directory: {RESULTS_DIR}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Load model\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "print(f\"Loading {MODEL_NAME} (4-bit)...\")\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "print(f\"Model loaded. dtype={model.dtype}, device={model.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Imports + config + templates + shared functions\n",
    "sys.path.insert(0, \".\")\n",
    "\n",
    "from lib.config import ExperimentConfig\n",
    "from lib.kv_cache import (\n",
    "    build_kv_cache,\n",
    "    build_suffix_kv_cache,\n",
    "    score_answer_with_cache,\n",
    "    deepcopy_cache,\n",
    "    extract_and_truncate_cache_with_bos,\n",
    "    correct_rope_positions_with_bos,\n",
    ")\n",
    "from lib.data import load_ms_marco, load_evaluation_samples\n",
    "from lib.analysis import cohens_d\n",
    "from lib.surrogate import generate_surrogate\n",
    "from scipy import stats\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "config = ExperimentConfig(\n",
    "    model_name=MODEL_NAME,\n",
    "    num_samples=4000,\n",
    "    min_passage_words=20,\n",
    "    max_passage_words=500,\n",
    "    seed=SEED,\n",
    ")\n",
    "\n",
    "SURROGATE_PREFIX_TEMPLATE = \"{surrogate}\\n\"\n",
    "DOCUMENT_TEMPLATE = \"{document}\"\n",
    "QUERY_TEMPLATE = \"\\nQuery: {query}\\nAnswer:\"\n",
    "ANSWER_TEMPLATE = \" {answer}\"\n",
    "SUFFIX_SEPARATOR = \"\\n\\nRelated question: \"\n",
    "CHECKPOINT_EVERY = 50\n",
    "N_COMPARISONS = 7\n",
    "BONFERRONI_ALPHA = 0.05 / N_COMPARISONS\n",
    "\n",
    "\n",
    "def generate_random_prefix_text(target_text, tokenizer, seed):\n",
    "    target_ids = tokenizer.encode(target_text, add_special_tokens=False)\n",
    "    target_len = len(target_ids)\n",
    "    if target_len == 0:\n",
    "        return \"\"\n",
    "    rng = np.random.RandomState(seed)\n",
    "    vocab_size = len(tokenizer)\n",
    "    min_id = 3\n",
    "    random_ids = rng.randint(min_id, vocab_size, size=target_len)\n",
    "    random_text = tokenizer.decode(random_ids.tolist(), skip_special_tokens=True)\n",
    "    reencoded = tokenizer.encode(random_text, add_special_tokens=False)\n",
    "    if len(reencoded) != target_len:\n",
    "        if len(reencoded) > target_len:\n",
    "            random_text = tokenizer.decode(reencoded[:target_len], skip_special_tokens=True)\n",
    "        else:\n",
    "            extra_needed = target_len - len(reencoded)\n",
    "            extra_ids = rng.randint(min_id, vocab_size, size=extra_needed)\n",
    "            extra_text = tokenizer.decode(extra_ids.tolist(), skip_special_tokens=True)\n",
    "            random_text = random_text + extra_text\n",
    "            reencoded2 = tokenizer.encode(random_text, add_special_tokens=False)\n",
    "            if len(reencoded2) > target_len:\n",
    "                random_text = tokenizer.decode(reencoded2[:target_len], skip_special_tokens=True)\n",
    "    return random_text\n",
    "\n",
    "\n",
    "print(\"Config ready\")\n",
    "print(f\"  num_samples pool: {config.num_samples}\")\n",
    "print(f\"  passage words: {config.min_passage_words}-{config.max_passage_words}\")\n",
    "print(f\"  bonferroni_alpha: {BONFERRONI_ALPHA:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Load MS MARCO (large pool)\n",
    "dataset = load_ms_marco(config)\n",
    "\n",
    "np.random.seed(SEED)\n",
    "all_samples = load_evaluation_samples(dataset, config, require_answer=True)\n",
    "\n",
    "N_POOL = len(all_samples)\n",
    "print(f\"Loaded {N_POOL} samples for hardness screening\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: First pass — score bare NLL on all samples\n",
    "# This identifies the \"hard\" samples where priming is most likely to help.\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"FIRST PASS: BARE NLL SCORING\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "bare_nlls_path = RESULTS_DIR / \"bare_nlls.json\"\n",
    "\n",
    "if bare_nlls_path.exists():\n",
    "    with open(bare_nlls_path, 'r') as f:\n",
    "        bare_data = json.load(f)\n",
    "    bare_nlls_all = bare_data['bare_nlls']\n",
    "    print(f\"Loaded {len(bare_nlls_all)} bare NLLs from cache\")\n",
    "else:\n",
    "    bare_nlls_all = []\n",
    "    t_start = time.time()\n",
    "\n",
    "    for idx in tqdm(range(len(bare_nlls_all), N_POOL), initial=len(bare_nlls_all),\n",
    "                     total=N_POOL, desc=\"Bare NLL\"):\n",
    "        sample = all_samples[idx]\n",
    "        passage = sample['passage']\n",
    "        query = sample['query']\n",
    "        answer = sample['answer']\n",
    "        query_prompt = QUERY_TEMPLATE.format(query=query)\n",
    "        answer_text = ANSWER_TEMPLATE.format(answer=answer)\n",
    "\n",
    "        bare_len, bare_cache = build_kv_cache(passage, model, tokenizer, config)\n",
    "        bare_nll = score_answer_with_cache(\n",
    "            deepcopy_cache(bare_cache), bare_len,\n",
    "            query_prompt, answer_text, model, tokenizer, config)\n",
    "        bare_nlls_all.append(bare_nll)\n",
    "\n",
    "        del bare_cache\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        if (idx + 1) % 200 == 0:\n",
    "            with open(bare_nlls_path, 'w') as f:\n",
    "                json.dump({'bare_nlls': bare_nlls_all}, f)\n",
    "            elapsed = time.time() - t_start\n",
    "            rate = (idx + 1) / elapsed\n",
    "            print(f\"  {idx+1}/{N_POOL} | {rate:.1f} s/s | ETA: {(N_POOL-idx-1)/rate/60:.1f} min\")\n",
    "\n",
    "    with open(bare_nlls_path, 'w') as f:\n",
    "        json.dump({'bare_nlls': bare_nlls_all}, f)\n",
    "\n",
    "    elapsed = time.time() - t_start\n",
    "    print(f\"First pass: {N_POOL} samples in {elapsed/60:.1f} min\")\n",
    "\n",
    "bare_nlls_arr = np.array(bare_nlls_all)\n",
    "nonzero_mask = bare_nlls_arr > 0\n",
    "print(f\"\\nBare NLL distribution (non-zero only, N={np.sum(nonzero_mask)}):\")\n",
    "bnz = bare_nlls_arr[nonzero_mask]\n",
    "print(f\"  Mean: {bnz.mean():.3f}, Median: {np.median(bnz):.3f}\")\n",
    "print(f\"  Q25: {np.percentile(bnz, 25):.3f}, Q75: {np.percentile(bnz, 75):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Filter to hard samples + generate LLM surrogates\n",
    "print(\"=\" * 70)\n",
    "print(\"FILTERING TO HARD SAMPLES + GENERATING SURROGATES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Filter: keep samples with non-zero bare NLL above median\n",
    "median_nll = np.median(bare_nlls_arr[bare_nlls_arr > 0])\n",
    "print(f\"Median bare NLL (non-zero): {median_nll:.3f}\")\n",
    "\n",
    "hard_indices = []\n",
    "for i, nll in enumerate(bare_nlls_all):\n",
    "    if nll > median_nll:\n",
    "        hard_indices.append(i)\n",
    "\n",
    "np.random.seed(SEED + 100)\n",
    "np.random.shuffle(hard_indices)\n",
    "hard_indices = hard_indices[:2000]\n",
    "hard_indices.sort()  # sort for deterministic ordering\n",
    "\n",
    "hard_samples = [all_samples[i] for i in hard_indices]\n",
    "hard_bare_nlls = [bare_nlls_all[i] for i in hard_indices]\n",
    "\n",
    "N_HARD = len(hard_samples)\n",
    "print(f\"Selected {N_HARD} hard samples\")\n",
    "print(f\"Hard sample bare NLL: mean={np.mean(hard_bare_nlls):.3f}, \"\n",
    "      f\"median={np.median(hard_bare_nlls):.3f}\")\n",
    "\n",
    "# Generate LLM surrogates\n",
    "print(f\"\\nGenerating LLM surrogates for {N_HARD} samples...\")\n",
    "\n",
    "if SURROGATES_PATH.exists():\n",
    "    with open(SURROGATES_PATH, 'r') as f:\n",
    "        surrogates_data = json.load(f)\n",
    "    surrogates = surrogates_data['surrogates']\n",
    "    print(f\"Loaded {len(surrogates)} surrogates from cache\")\n",
    "else:\n",
    "    surrogates = []\n",
    "\n",
    "start_gen = len(surrogates)\n",
    "if start_gen < N_HARD:\n",
    "    t_start = time.time()\n",
    "    for idx in tqdm(range(start_gen, N_HARD), initial=start_gen, total=N_HARD,\n",
    "                     desc=\"LLM Surrogates\"):\n",
    "        sample = hard_samples[idx]\n",
    "        try:\n",
    "            surrogate = generate_surrogate(sample['passage'], model, tokenizer, config)\n",
    "        except Exception as e:\n",
    "            surrogate = sample['query'][:30]  # fallback\n",
    "            print(f\"  WARNING: Generation failed for sample {idx}: {e}\")\n",
    "        surrogates.append(surrogate)\n",
    "\n",
    "        if (idx + 1) % 100 == 0 or idx == N_HARD - 1:\n",
    "            with open(SURROGATES_PATH, 'w') as f:\n",
    "                json.dump({'surrogates': surrogates}, f)\n",
    "            elapsed = time.time() - t_start\n",
    "            rate = (idx - start_gen + 1) / elapsed if elapsed > 0 else 0\n",
    "            remaining = (N_HARD - idx - 1) / rate if rate > 0 else 0\n",
    "            tqdm.write(f\"  Generated {idx+1}/{N_HARD} | {rate:.1f} s/s | ETA: {remaining/60:.1f} min\")\n",
    "\n",
    "    with open(SURROGATES_PATH, 'w') as f:\n",
    "        json.dump({'surrogates': surrogates}, f)\n",
    "\n",
    "# Show examples\n",
    "print(f\"\\nSurrogate examples:\")\n",
    "for i in range(min(5, N_HARD)):\n",
    "    print(f\"  Oracle:    {hard_samples[i]['query'][:60]}...\")\n",
    "    print(f\"  LLM:       {surrogates[i][:60]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Main eval loop — 5 conditions on hard samples\n",
    "print(\"=\" * 70)\n",
    "print(\"SECOND PASS: 5-CONDITION EVAL ON HARD SAMPLES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "results = []\n",
    "start_idx = 0\n",
    "\n",
    "if CHECKPOINT_PATH.exists():\n",
    "    with open(CHECKPOINT_PATH, 'r') as f:\n",
    "        ckpt = json.load(f)\n",
    "    ckpt_queries = ckpt.get('sample_queries', [])\n",
    "    current_queries = [s['query'] for s in hard_samples]\n",
    "    if ckpt_queries == current_queries:\n",
    "        results = ckpt['results']\n",
    "        start_idx = len(results)\n",
    "        print(f\"Resuming from checkpoint: {start_idx}/{N_HARD}\")\n",
    "    else:\n",
    "        print(\"Checkpoint mismatch. Starting fresh.\")\n",
    "else:\n",
    "    print(\"No checkpoint found. Starting fresh.\")\n",
    "\n",
    "print(f\"Evaluating samples {start_idx} to {N_HARD-1}\")\n",
    "print(f\"Conditions: 5 (bare, oracle, random, separator, LLM-surrogate)\")\n",
    "\n",
    "t_start = time.time()\n",
    "\n",
    "for idx in tqdm(range(start_idx, N_HARD), initial=start_idx, total=N_HARD,\n",
    "                 desc=\"Evaluating\"):\n",
    "    sample = hard_samples[idx]\n",
    "    passage = sample['passage']\n",
    "    query = sample['query']\n",
    "    answer = sample['answer']\n",
    "    llm_surrogate = surrogates[idx]\n",
    "\n",
    "    query_prompt = QUERY_TEMPLATE.format(query=query)\n",
    "    answer_text = ANSWER_TEMPLATE.format(answer=answer)\n",
    "\n",
    "    # Matched tokenization\n",
    "    oracle_prefix = SURROGATE_PREFIX_TEMPLATE.format(surrogate=query)\n",
    "    document_text = DOCUMENT_TEMPLATE.format(document=passage)\n",
    "    full_oracle_text = oracle_prefix + document_text\n",
    "\n",
    "    full_oracle_enc = tokenizer(full_oracle_text, return_tensors=\"pt\",\n",
    "                                add_special_tokens=True, padding=False, truncation=False)\n",
    "    full_oracle_ids = full_oracle_enc['input_ids'].to(config.device)\n",
    "\n",
    "    oracle_prefix_enc = tokenizer(oracle_prefix, return_tensors=\"pt\",\n",
    "                                  add_special_tokens=True, padding=False, truncation=False)\n",
    "    oracle_prefix_len = oracle_prefix_enc['input_ids'].shape[1]\n",
    "\n",
    "    bos_id = full_oracle_ids[:, :1]\n",
    "    doc_ids = full_oracle_ids[:, oracle_prefix_len:]\n",
    "    doc_len = doc_ids.shape[1]\n",
    "\n",
    "    random_text = generate_random_prefix_text(query, tokenizer, seed=SEED + idx)\n",
    "\n",
    "    # === Condition 1: BARE ===\n",
    "    bare_ids = torch.cat([bos_id, doc_ids], dim=1)\n",
    "    bare_len = bare_ids.shape[1]\n",
    "    with torch.no_grad():\n",
    "        bare_out = model(input_ids=bare_ids, attention_mask=torch.ones_like(bare_ids),\n",
    "                         use_cache=True, return_dict=True)\n",
    "    bare_nll = score_answer_with_cache(\n",
    "        deepcopy_cache(bare_out.past_key_values), bare_len,\n",
    "        query_prompt, answer_text, model, tokenizer, config)\n",
    "\n",
    "    # === Condition 2: ORACLE-TRUNCATED ===\n",
    "    with torch.no_grad():\n",
    "        oracle_out = model(input_ids=full_oracle_ids,\n",
    "                           attention_mask=torch.ones_like(full_oracle_ids),\n",
    "                           use_cache=True, return_dict=True)\n",
    "    oracle_cache = extract_and_truncate_cache_with_bos(oracle_out.past_key_values, doc_len)\n",
    "    correct_rope_positions_with_bos(oracle_cache, oracle_prefix_len - 1, model)\n",
    "    oracle_trunc_nll = score_answer_with_cache(\n",
    "        deepcopy_cache(oracle_cache), 1 + doc_len,\n",
    "        query_prompt, answer_text, model, tokenizer, config)\n",
    "\n",
    "    # === Condition 3: RANDOM-TRUNCATED ===\n",
    "    random_prefix = SURROGATE_PREFIX_TEMPLATE.format(surrogate=random_text)\n",
    "    random_prefix_enc = tokenizer(random_prefix, return_tensors=\"pt\",\n",
    "                                  add_special_tokens=False, padding=False, truncation=False)\n",
    "    random_prefix_ids = random_prefix_enc['input_ids'].to(config.device)\n",
    "    random_full_ids = torch.cat([bos_id, random_prefix_ids, doc_ids], dim=1)\n",
    "    random_prefix_len = 1 + random_prefix_ids.shape[1]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        random_out = model(input_ids=random_full_ids,\n",
    "                           attention_mask=torch.ones_like(random_full_ids),\n",
    "                           use_cache=True, return_dict=True)\n",
    "    random_cache = extract_and_truncate_cache_with_bos(random_out.past_key_values, doc_len)\n",
    "    correct_rope_positions_with_bos(random_cache, random_prefix_len - 1, model)\n",
    "    random_trunc_nll = score_answer_with_cache(\n",
    "        deepcopy_cache(random_cache), 1 + doc_len,\n",
    "        query_prompt, answer_text, model, tokenizer, config)\n",
    "\n",
    "    # === Condition 4: SEPARATOR-ONLY ===\n",
    "    sep_only_len, sep_only_cache = build_suffix_kv_cache(\n",
    "        passage, \"\", model, tokenizer, config, separator=SUFFIX_SEPARATOR)\n",
    "    separator_only_nll = score_answer_with_cache(\n",
    "        deepcopy_cache(sep_only_cache), sep_only_len,\n",
    "        query_prompt, answer_text, model, tokenizer, config)\n",
    "\n",
    "    # === Condition 5: LLM-SURROGATE-TRUNCATED ===\n",
    "    llm_prefix = SURROGATE_PREFIX_TEMPLATE.format(surrogate=llm_surrogate)\n",
    "    llm_prefix_enc = tokenizer(llm_prefix, return_tensors=\"pt\",\n",
    "                                add_special_tokens=False, padding=False, truncation=False)\n",
    "    llm_prefix_ids = llm_prefix_enc['input_ids'].to(config.device)\n",
    "    llm_full_ids = torch.cat([bos_id, llm_prefix_ids, doc_ids], dim=1)\n",
    "    llm_prefix_len = 1 + llm_prefix_ids.shape[1]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        llm_out = model(input_ids=llm_full_ids,\n",
    "                        attention_mask=torch.ones_like(llm_full_ids),\n",
    "                        use_cache=True, return_dict=True)\n",
    "    llm_cache = extract_and_truncate_cache_with_bos(llm_out.past_key_values, doc_len)\n",
    "    correct_rope_positions_with_bos(llm_cache, llm_prefix_len - 1, model)\n",
    "    llm_trunc_nll = score_answer_with_cache(\n",
    "        deepcopy_cache(llm_cache), 1 + doc_len,\n",
    "        query_prompt, answer_text, model, tokenizer, config)\n",
    "\n",
    "    result = {\n",
    "        'idx': idx,\n",
    "        'bare_nll': bare_nll,\n",
    "        'oracle_trunc_nll': oracle_trunc_nll,\n",
    "        'random_trunc_nll': random_trunc_nll,\n",
    "        'separator_only_nll': separator_only_nll,\n",
    "        'llm_trunc_nll': llm_trunc_nll,\n",
    "        'doc_len': doc_len,\n",
    "        'passage_word_count': len(passage.split()),\n",
    "        'pre_filter_bare_nll': hard_bare_nlls[idx],\n",
    "        'llm_surrogate': llm_surrogate,\n",
    "        'delta_oracle_vs_bare': bare_nll - oracle_trunc_nll,\n",
    "        'delta_random_vs_bare': bare_nll - random_trunc_nll,\n",
    "        'delta_oracle_vs_random': random_trunc_nll - oracle_trunc_nll,\n",
    "        'delta_seponly_vs_bare': bare_nll - separator_only_nll,\n",
    "        'delta_llm_vs_bare': bare_nll - llm_trunc_nll,\n",
    "        'delta_llm_vs_random': random_trunc_nll - llm_trunc_nll,\n",
    "        'delta_llm_vs_oracle': oracle_trunc_nll - llm_trunc_nll,\n",
    "    }\n",
    "    results.append(result)\n",
    "\n",
    "    del bare_out, oracle_out, oracle_cache, random_out, random_cache\n",
    "    del sep_only_cache, llm_out, llm_cache\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    if (idx + 1) % CHECKPOINT_EVERY == 0 or idx == N_HARD - 1:\n",
    "        ckpt_data = {\n",
    "            'results': results,\n",
    "            'sample_queries': [s['query'] for s in hard_samples],\n",
    "            'completed': len(results),\n",
    "            'total': N_HARD,\n",
    "            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        }\n",
    "        with open(CHECKPOINT_PATH, 'w') as f:\n",
    "            json.dump(ckpt_data, f)\n",
    "        elapsed = time.time() - t_start\n",
    "        rate = (idx - start_idx + 1) / elapsed if elapsed > 0 else 0\n",
    "        remaining = (N_HARD - idx - 1) / rate if rate > 0 else 0\n",
    "        tqdm.write(f\"  Checkpoint {idx+1}/{N_HARD} | {rate:.2f} s/s | ETA: {remaining/60:.1f} min\")\n",
    "\n",
    "elapsed_total = time.time() - t_start\n",
    "print(f\"\\nEvaluation complete: {len(results)} samples in {elapsed_total/60:.1f} min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Analysis\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ANALYSIS — HARDNESS-GATED SEMANTIC PRIMING\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "bare_raw = np.array([r['bare_nll'] for r in results])\n",
    "oracle_raw = np.array([r['oracle_trunc_nll'] for r in results])\n",
    "random_raw = np.array([r['random_trunc_nll'] for r in results])\n",
    "seponly_raw = np.array([r['separator_only_nll'] for r in results])\n",
    "llm_raw = np.array([r['llm_trunc_nll'] for r in results])\n",
    "\n",
    "valid = (bare_raw != 0) & (oracle_raw != 0) & (random_raw != 0) & (seponly_raw != 0) & (llm_raw != 0)\n",
    "n_valid = int(np.sum(valid))\n",
    "n_excluded = int(np.sum(~valid))\n",
    "\n",
    "bare = bare_raw[valid]\n",
    "oracle = oracle_raw[valid]\n",
    "random = random_raw[valid]\n",
    "sep_only = seponly_raw[valid]\n",
    "llm = llm_raw[valid]\n",
    "\n",
    "print(f\"Total: {len(results)}, Valid: {n_valid}, Excluded: {n_excluded}\")\n",
    "print(f\"Bonferroni alpha: {BONFERRONI_ALPHA:.4f} ({N_COMPARISONS} comparisons)\")\n",
    "\n",
    "# NLL summary\n",
    "print(f\"\\n{'Condition':<25} {'Mean NLL':>10} {'Std':>10}\")\n",
    "print(\"-\" * 50)\n",
    "for name, arr in [('Bare', bare), ('Oracle-truncated', oracle),\n",
    "                   ('Random-truncated', random), ('Separator-only', sep_only),\n",
    "                   ('LLM-surrogate-trunc', llm)]:\n",
    "    print(f\"{name:<25} {np.mean(arr):>10.4f} {np.std(arr):>10.4f}\")\n",
    "\n",
    "# Comparisons\n",
    "comparisons = [\n",
    "    ('Oracle vs Bare', bare - oracle, 'Does oracle help hard samples?'),\n",
    "    ('Random vs Bare', bare - random, 'Does random help hard samples?'),\n",
    "    ('Oracle vs Random', random - oracle, 'KEY: semantic priming in hard?'),\n",
    "    ('Sep-only vs Bare', bare - sep_only, 'Separator on hard samples?'),\n",
    "    ('LLM vs Bare', bare - llm, 'LLM surrogate overall?'),\n",
    "    ('LLM vs Random', random - llm, 'LLM better than random?'),\n",
    "    ('LLM vs Oracle', oracle - llm, 'LLM better than oracle?'),\n",
    "]\n",
    "\n",
    "print(f\"\\n{'Comparison':<25} {'Mean D':>8} {'d':>8} {'Win%':>7} {'t':>8} {'p':>12} {'Sig':>5}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "comparison_results = {}\n",
    "for name, delta, question in comparisons:\n",
    "    d = cohens_d(delta)\n",
    "    win = np.mean(delta > 0) * 100\n",
    "    t_stat, p_val = stats.ttest_1samp(delta, 0)\n",
    "    sig = \"***\" if p_val < 0.001 else \"**\" if p_val < BONFERRONI_ALPHA else \"*\" if p_val < 0.05 else \"ns\"\n",
    "    print(f\"{name:<25} {np.mean(delta):>8.4f} {d:>8.3f} {win:>6.1f}% {t_stat:>8.2f} {p_val:>11.2e} {sig:>5}\")\n",
    "    comparison_results[name] = {\n",
    "        'mean_delta': float(np.mean(delta)),\n",
    "        'cohens_d': float(d),\n",
    "        'win_rate': float(win / 100),\n",
    "        't_stat': float(t_stat),\n",
    "        'p_value': float(p_val),\n",
    "        'bonferroni_significant': bool(p_val < BONFERRONI_ALPHA),\n",
    "        'question': question,\n",
    "    }\n",
    "\n",
    "# Hardness interaction within hard samples\n",
    "print(f\"\\nHardness interaction (within hard subset):\")\n",
    "for name, delta in [('Oracle', bare - oracle), ('Random', bare - random),\n",
    "                     ('LLM', bare - llm), ('Sep-only', bare - sep_only)]:\n",
    "    r, p = stats.pearsonr(bare, delta)\n",
    "    print(f\"  {name:<15}: r={r:.3f}, p={p:.2e}\")\n",
    "\n",
    "# Summary verdict\n",
    "print(f\"\\n{'_' * 70}\")\n",
    "print(\"VERDICTS:\")\n",
    "d_or = comparison_results['Oracle vs Random']['cohens_d']\n",
    "p_or = comparison_results['Oracle vs Random']['p_value']\n",
    "d_lr = comparison_results['LLM vs Random']['cohens_d']\n",
    "p_lr = comparison_results['LLM vs Random']['p_value']\n",
    "\n",
    "if p_or < 0.05 and d_or > 0:\n",
    "    print(f\"  Oracle vs Random: SEMANTIC PRIMING DETECTED in hard samples (d={d_or:+.3f})\")\n",
    "elif p_or < 0.05 and d_or < 0:\n",
    "    print(f\"  Oracle vs Random: Oracle still INTERFERES even for hard samples (d={d_or:+.3f})\")\n",
    "else:\n",
    "    print(f\"  Oracle vs Random: No semantic signal even in hard samples (d={d_or:+.3f}, ns)\")\n",
    "\n",
    "if p_lr < 0.05 and d_lr > 0:\n",
    "    print(f\"  LLM vs Random: LLM surrogates ADD value beyond random (d={d_lr:+.3f})\")\n",
    "else:\n",
    "    print(f\"  LLM vs Random: LLM surrogates no better than random (d={d_lr:+.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Plots\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "\n",
    "# Plot 1: Delta distributions — Oracle vs Random (THE key test)\n",
    "delta_or = random - oracle\n",
    "axes[0, 0].hist(delta_or, bins=60, color='steelblue', alpha=0.7, edgecolor='black', linewidth=0.3)\n",
    "axes[0, 0].axvline(x=0, color='red', linestyle='--')\n",
    "axes[0, 0].axvline(x=np.mean(delta_or), color='black', linestyle='-',\n",
    "                    label=f'd={cohens_d(delta_or):+.3f}')\n",
    "axes[0, 0].set_title('Oracle vs Random (hard samples)')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# Plot 2: Delta distributions — LLM vs Random\n",
    "delta_lr = random - llm\n",
    "axes[0, 1].hist(delta_lr, bins=60, color='forestgreen', alpha=0.7, edgecolor='black', linewidth=0.3)\n",
    "axes[0, 1].axvline(x=0, color='red', linestyle='--')\n",
    "axes[0, 1].axvline(x=np.mean(delta_lr), color='black', linestyle='-',\n",
    "                    label=f'd={cohens_d(delta_lr):+.3f}')\n",
    "axes[0, 1].set_title('LLM Surrogate vs Random (hard samples)')\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# Plot 3: All conditions bar chart\n",
    "cond_names = ['Oracle', 'Random', 'Sep-only', 'LLM']\n",
    "cond_d = [cohens_d(bare - oracle), cohens_d(bare - random),\n",
    "          cohens_d(bare - sep_only), cohens_d(bare - llm)]\n",
    "colors = ['steelblue', 'darkorange', 'crimson', 'forestgreen']\n",
    "axes[0, 2].bar(range(4), cond_d, color=colors, alpha=0.8, edgecolor='black')\n",
    "axes[0, 2].set_xticks(range(4))\n",
    "axes[0, 2].set_xticklabels(cond_names)\n",
    "axes[0, 2].axhline(y=0, color='gray', linestyle='--')\n",
    "axes[0, 2].set_ylabel(\"Cohen's d vs Bare\")\n",
    "axes[0, 2].set_title('All Conditions vs Bare (hard samples)')\n",
    "\n",
    "# Plot 4: Hardness scatter — Oracle benefit\n",
    "axes[1, 0].scatter(bare, bare - oracle, alpha=0.15, s=5, c='steelblue')\n",
    "axes[1, 0].axhline(y=0, color='red', linestyle='--')\n",
    "z = np.polyfit(bare, bare - oracle, 1)\n",
    "x_range = np.linspace(bare.min(), bare.max(), 100)\n",
    "axes[1, 0].plot(x_range, np.polyval(z, x_range), 'r-', alpha=0.8)\n",
    "axes[1, 0].set_xlabel('Bare NLL')\n",
    "axes[1, 0].set_ylabel('Oracle benefit')\n",
    "axes[1, 0].set_title('Hardness vs Oracle Benefit')\n",
    "\n",
    "# Plot 5: Hardness scatter — LLM benefit\n",
    "axes[1, 1].scatter(bare, bare - llm, alpha=0.15, s=5, c='forestgreen')\n",
    "axes[1, 1].axhline(y=0, color='red', linestyle='--')\n",
    "z2 = np.polyfit(bare, bare - llm, 1)\n",
    "axes[1, 1].plot(x_range, np.polyval(z2, x_range), 'r-', alpha=0.8)\n",
    "axes[1, 1].set_xlabel('Bare NLL')\n",
    "axes[1, 1].set_ylabel('LLM benefit')\n",
    "axes[1, 1].set_title('Hardness vs LLM Surrogate Benefit')\n",
    "\n",
    "# Plot 6: LLM surrogate quality\n",
    "# Token overlap between LLM surrogate and oracle query\n",
    "from lib.analysis import compute_token_overlap\n",
    "overlaps = []\n",
    "for r in results[:min(500, len(results))]:\n",
    "    s = hard_samples[r['idx']]\n",
    "    overlap = compute_token_overlap(s['query'], r['llm_surrogate'], tokenizer)\n",
    "    overlaps.append(overlap)\n",
    "axes[1, 2].hist(overlaps, bins=40, color='mediumpurple', alpha=0.7, edgecolor='black', linewidth=0.3)\n",
    "axes[1, 2].set_xlabel('Token Jaccard similarity')\n",
    "axes[1, 2].set_ylabel('Count')\n",
    "axes[1, 2].set_title(f'LLM vs Oracle Query Similarity (mean={np.mean(overlaps):.3f})')\n",
    "\n",
    "plt.suptitle('Exp 05: Hardness-Gated Semantic Priming + LLM Surrogates', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'analysis_plots.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"Plot saved to {RESULTS_DIR / 'analysis_plots.png'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Save results\n",
    "final = {\n",
    "    'experiment': 'exp05_hardness_gated_surrogates',\n",
    "    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'config': {\n",
    "        'model_name': config.model_name,\n",
    "        'seed': SEED,\n",
    "        'n_pool': N_POOL,\n",
    "        'n_hard': N_HARD,\n",
    "        'median_nll_threshold': float(median_nll),\n",
    "        'min_passage_words': config.min_passage_words,\n",
    "        'max_passage_words': config.max_passage_words,\n",
    "        'bonferroni_alpha': BONFERRONI_ALPHA,\n",
    "    },\n",
    "    'summary': {\n",
    "        'n_total': len(results),\n",
    "        'n_valid': n_valid,\n",
    "        'n_excluded': n_excluded,\n",
    "        'nll_means': {\n",
    "            'bare': float(np.mean(bare)),\n",
    "            'oracle_trunc': float(np.mean(oracle)),\n",
    "            'random_trunc': float(np.mean(random)),\n",
    "            'separator_only': float(np.mean(sep_only)),\n",
    "            'llm_trunc': float(np.mean(llm)),\n",
    "        },\n",
    "        'comparisons': comparison_results,\n",
    "    },\n",
    "    'per_sample_results': results,\n",
    "}\n",
    "\n",
    "with open(FINAL_RESULTS_PATH, 'w') as f:\n",
    "    json.dump(final, f, indent=2)\n",
    "\n",
    "print(f\"Results saved to {FINAL_RESULTS_PATH}\")\n",
    "print(f\"File size: {FINAL_RESULTS_PATH.stat().st_size / 1024:.1f} KB\")\n",
    "print(f\"\\nDone!\")"
   ]
  },
  {
   "cell_type": "code",
   "source": "# Cell 11: GPU cleanup — free all VRAM\nimport gc\n\nprint(\"Cleaning up GPU memory...\")\nmem_before = torch.cuda.memory_allocated() / 1e9\n\ndel model\ndel tokenizer\n\ngc.collect()\ntorch.cuda.empty_cache()\ngc.collect()\n\nmem_after = torch.cuda.memory_allocated() / 1e9\nprint(f\"GPU memory: {mem_before:.2f} GB -> {mem_after:.2f} GB\")\nprint(\"Cleanup complete.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}