{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-md-0",
   "metadata": {},
   "source": [
    "# Experiment 18: Periodic Beacon Injection\n",
    "\n",
    "**Hypothesis**: Value contamination degrades over token distance. In long documents,\n",
    "later tokens are too far from the prefix to benefit. **Fix**: Inject static fact\n",
    "\"beacon\" tokens periodically throughout the document so every document token is\n",
    "within N positions of a contamination source.\n",
    "\n",
    "**Beacon phrase**: `\"What are the key facts I need to know?\"` (static_factual from Exp 07).\n",
    "\n",
    "| # | Condition | Build Method | Query-Time Cache | Tests |\n",
    "|---|-----------|-------------|------------------|-------|\n",
    "| 1 | `bare` | `[BOS][doc]` forward pass | `[BOS][doc]` | Baseline |\n",
    "| 2 | `single_prefix` | `[BOS][prefix][doc]` → truncate + RoPE correct | `[BOS][doc']` | Exp 07 replication |\n",
    "| 3 | `beacon_seq_256` | `[BOS][B][C1][B][C2]...` single forward pass | Full beacon-studded cache | Main test |\n",
    "| 4 | `beacon_seq_512` | `[BOS][B][C1][B][C2]...` single forward pass | Full beacon-studded cache | Stride comparison |\n",
    "| 5 | `beacon_trunc_256` | Same as #3 → extract doc positions + RoPE correct | `[BOS][doc']` | Value contamination only |\n",
    "| 6 | `random_beacon_256` | `[BOS][R][C1][R][C2]...` random token beacons | Full random-studded cache | Semantic vs structural |\n",
    "| 7 | `beacon_batch_256` | Chunks processed independently → flattened | Same structure as #3 | Cross-chunk attention test |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Setup & Imports\n",
    "import os\n",
    "os.umask(0o000)  # File permission safety (two-user environment)\n",
    "\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from scipy import stats\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "from tqdm.auto import tqdm\n",
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from lib.config import ExperimentConfig\n",
    "from lib.kv_cache import (\n",
    "    build_kv_cache,\n",
    "    build_beacon_cache_sequential,\n",
    "    build_beacon_cache_batch,\n",
    "    extract_cache_at_indices,\n",
    "    correct_rope_positions_chunked,\n",
    "    extract_and_truncate_cache_with_bos,\n",
    "    correct_rope_positions_with_bos,\n",
    "    score_answer_with_cache,\n",
    "    deepcopy_cache,\n",
    "    _get_cache_keys,\n",
    "    _get_cache_values,\n",
    "    _ensure_dynamic_cache,\n",
    ")\n",
    "from lib.analysis import cohens_d\n",
    "from lib.surrogate import STATIC_SURROGATE_QUERIES\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Paths\n",
    "RESULTS_DIR = Path('results/exp18')\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "FIGURES_DIR = RESULTS_DIR / 'figures'\n",
    "FIGURES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Model\n",
    "MODEL_NAME = 'mistralai/Mistral-7B-Instruct-v0.2'\n",
    "bnb_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.float16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME, quantization_config=bnb_config, device_map='auto'\n",
    ")\n",
    "config = ExperimentConfig(device='cuda')\n",
    "\n",
    "print(f'Model loaded: {MODEL_NAME}')\n",
    "print(f'Device: {config.device}')\n",
    "print(f'Results dir: {RESULTS_DIR}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Configuration\n",
    "\n",
    "# Beacon text (static_factual — best performer from Exp 07)\n",
    "BEACON_TEXT = STATIC_SURROGATE_QUERIES['static_factual']['query']\n",
    "BEACON_IDS = tokenizer.encode(BEACON_TEXT, add_special_tokens=False)\n",
    "BEACON_LEN = len(BEACON_IDS)\n",
    "\n",
    "# Random beacon (same length, random tokens)\n",
    "torch.manual_seed(SEED)\n",
    "RANDOM_BEACON_IDS = torch.randint(100, tokenizer.vocab_size - 100, (BEACON_LEN,)).tolist()\n",
    "\n",
    "# Chunk sizes\n",
    "CHUNK_SIZES = [256, 512]\n",
    "\n",
    "# MS MARCO\n",
    "N_MSMARCO = 300\n",
    "MSMARCO_CHECKPOINT_PATH = RESULTS_DIR / 'msmarco_checkpoint.json'\n",
    "MSMARCO_RESULTS_PATH = RESULTS_DIR / 'msmarco_results.json'\n",
    "CHECKPOINT_EVERY = 25\n",
    "\n",
    "# NQ — stratified sampling\n",
    "N_NQ = 240\n",
    "NQ_CHECKPOINT_PATH = RESULTS_DIR / 'nq_checkpoint.json'\n",
    "NQ_RESULTS_PATH = RESULTS_DIR / 'nq_results.json'\n",
    "NQ_SAMPLES_CACHE_PATH = RESULTS_DIR / 'nq_samples.json'\n",
    "\n",
    "LENGTH_BINS = [\n",
    "    ('short',     100,  300),\n",
    "    ('medium',    300,  800),\n",
    "    ('long',      800,  2000),\n",
    "    ('very_long', 2000, 4000),\n",
    "]\n",
    "SAMPLES_PER_BIN = {'short': 15, 'medium': 75, 'long': 75, 'very_long': 75}\n",
    "MAX_DOC_WORDS = 4000\n",
    "\n",
    "# Templates (matched to Exp 07/17)\n",
    "QUERY_TEMPLATE = '\\nQuery: {query}\\nAnswer:'\n",
    "ANSWER_TEMPLATE = ' {answer}'\n",
    "\n",
    "# Condition names\n",
    "MSMARCO_CONDITIONS = ['bare', 'single_prefix', 'beacon_seq_256', 'beacon_seq_512']\n",
    "NQ_CONDITIONS = [\n",
    "    'bare', 'single_prefix',\n",
    "    'beacon_seq_256', 'beacon_seq_512',\n",
    "    'beacon_trunc_256', 'random_beacon_256', 'beacon_batch_256',\n",
    "]\n",
    "\n",
    "print(f'Beacon text: \"{BEACON_TEXT}\"')\n",
    "print(f'Beacon token length: {BEACON_LEN}')\n",
    "print(f'Random beacon token length: {len(RANDOM_BEACON_IDS)}')\n",
    "print(f'MS MARCO: N={N_MSMARCO}, conditions={MSMARCO_CONDITIONS}')\n",
    "print(f'NQ: N={N_NQ}, conditions={NQ_CONDITIONS}')\n",
    "print(f'Samples per NQ bin: {SAMPLES_PER_BIN}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Explain Experimental Conditions\n",
    "print('=' * 70)\n",
    "print('EXPERIMENTAL CONDITIONS EXPLAINED')\n",
    "print('=' * 70)\n",
    "\n",
    "# Show concrete example\n",
    "example_doc = 'The quick brown fox jumps over the lazy dog. ' * 20  # ~180 words\n",
    "example_doc_ids = tokenizer.encode(example_doc, add_special_tokens=False)\n",
    "n_doc_tokens = len(example_doc_ids)\n",
    "bos_id = tokenizer.bos_token_id\n",
    "\n",
    "print(f'\\nExample document: {n_doc_tokens} tokens')\n",
    "print(f'Beacon: {BEACON_LEN} tokens (\"{BEACON_TEXT}\")')\n",
    "print()\n",
    "\n",
    "print('### 1. bare ###')\n",
    "print(f'  Cache: [BOS][doc({n_doc_tokens} tokens)]')\n",
    "print(f'  Total: {1 + n_doc_tokens} tokens')\n",
    "print(f'  Baseline — document cached in isolation.')\n",
    "print()\n",
    "\n",
    "print('### 2. single_prefix ###')\n",
    "prefix_ids = tokenizer.encode(BEACON_TEXT + '\\n', add_special_tokens=False)\n",
    "print(f'  Build: [BOS][prefix({len(prefix_ids)} tokens)][doc({n_doc_tokens} tokens)]')\n",
    "print(f'  → Truncate prefix → RoPE correct')\n",
    "print(f'  Query-time cache: [BOS][doc\\'({n_doc_tokens} tokens)]  (contaminated values, bare positions)')\n",
    "print(f'  Replicates Exp 07 static_fact_trunc.')\n",
    "print()\n",
    "\n",
    "n_chunks_256 = -(-n_doc_tokens // 256)\n",
    "seq_256 = 1 + n_chunks_256 * (BEACON_LEN + min(256, n_doc_tokens))\n",
    "# More accurate for variable last chunk\n",
    "chunk_sizes_actual = [min(256, n_doc_tokens - i*256) for i in range(n_chunks_256)]\n",
    "seq_256 = 1 + sum(BEACON_LEN + cs for cs in chunk_sizes_actual)\n",
    "print(f'### 3. beacon_seq_256 ###')\n",
    "print(f'  Build: [BOS][B({BEACON_LEN})][C1({chunk_sizes_actual[0]})][B({BEACON_LEN})][C2({chunk_sizes_actual[1] if len(chunk_sizes_actual) > 1 else \"...\"})]..')\n",
    "print(f'  {n_chunks_256} chunks of ≤256 tokens, {n_chunks_256} beacons')\n",
    "print(f'  Total: {seq_256} tokens (single forward pass, full causal attention)')\n",
    "print(f'  Query-time: full cache (beacons present). Query sees beacon+doc interleaved.')\n",
    "print(f'  MAIN TEST: periodic contamination sources throughout the document.')\n",
    "print()\n",
    "\n",
    "n_chunks_512 = -(-n_doc_tokens // 512)\n",
    "chunk_sizes_512 = [min(512, n_doc_tokens - i*512) for i in range(n_chunks_512)]\n",
    "seq_512 = 1 + sum(BEACON_LEN + cs for cs in chunk_sizes_512)\n",
    "print(f'### 4. beacon_seq_512 ###')\n",
    "print(f'  Same as #3 but stride=512. {n_chunks_512} chunks, total: {seq_512} tokens.')\n",
    "print(f'  Tests whether closer beacons (256 vs 512) matter.')\n",
    "print()\n",
    "\n",
    "print(f'### 5. beacon_trunc_256 ###')\n",
    "print(f'  Build: Same as #3 (beacon_seq_256)')\n",
    "print(f'  → Extract doc positions only + per-chunk RoPE correction')\n",
    "print(f'  Query-time cache: [BOS][doc\\'({n_doc_tokens} tokens)]  (beacons removed)')\n",
    "print(f'  Tests VALUE CONTAMINATION only (beacon attention removed at query time).')\n",
    "print(f'  If #5 ≈ #3: contamination is the mechanism.')\n",
    "print(f'  If #5 << #3: attention routing to beacons also matters.')\n",
    "print()\n",
    "\n",
    "print(f'### 6. random_beacon_256 ###')\n",
    "print(f'  Build: [BOS][R({BEACON_LEN})][C1][R({BEACON_LEN})][C2]... (random tokens as beacons)')\n",
    "print(f'  Same structure as #3 but beacons are random tokens, not semantic.')\n",
    "print(f'  If #6 ≈ #3: structural effect (any periodic injection helps).')\n",
    "print(f'  If #6 << #3: semantic content of beacons matters.')\n",
    "print()\n",
    "\n",
    "print(f'### 7. beacon_batch_256 ###')\n",
    "print(f'  Build: Each [BOS][beacon][chunk_i] processed independently')\n",
    "print(f'  → Flattened into single cache (BOS stripped from non-first chunks)')\n",
    "print(f'  Same STRUCTURE as #3 but no cross-chunk attention during build.')\n",
    "print(f'  If #7 ≈ #3: cross-chunk attention is irrelevant.')\n",
    "print(f'  If #7 << #3: cross-chunk attention matters.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Helper Functions\n",
    "\n",
    "def build_matched_beacon_caches(\n",
    "    passage, query, answer,\n",
    "    conditions, model, tokenizer, config,\n",
    "    beacon_ids=BEACON_IDS, random_beacon_ids=RANDOM_BEACON_IDS,\n",
    "    chunk_sizes=CHUNK_SIZES,\n",
    "):\n",
    "    \"\"\"Build all condition caches for one sample with matched tokenization.\n",
    "\n",
    "    Tokenizes the document once and reuses the same doc_ids across all conditions.\n",
    "\n",
    "    Args:\n",
    "        passage: Document text.\n",
    "        query: Query text.\n",
    "        answer: Answer text.\n",
    "        conditions: List of condition names to evaluate.\n",
    "        model, tokenizer, config: Standard model env.\n",
    "        beacon_ids: Token IDs for the semantic beacon.\n",
    "        random_beacon_ids: Token IDs for the random beacon.\n",
    "        chunk_sizes: List of chunk sizes [256, 512].\n",
    "\n",
    "    Returns:\n",
    "        dict mapping condition_name -> NLL, plus 'doc_len_tokens'.\n",
    "    \"\"\"\n",
    "    query_prompt = QUERY_TEMPLATE.format(query=query)\n",
    "    answer_text = ANSWER_TEMPLATE.format(answer=answer)\n",
    "\n",
    "    # Tokenize document once (no BOS, no framing)\n",
    "    doc_ids = tokenizer.encode(passage, add_special_tokens=False)\n",
    "    doc_len = len(doc_ids)\n",
    "    bos_id = tokenizer.bos_token_id if tokenizer.bos_token_id is not None else 1\n",
    "    bos_tensor = torch.tensor([[bos_id]], device=config.device)\n",
    "    doc_tensor = torch.tensor([doc_ids], device=config.device)\n",
    "\n",
    "    beacon_len = len(beacon_ids)\n",
    "\n",
    "    def score(cache, seq_len, position_offset=0):\n",
    "        return score_answer_with_cache(\n",
    "            deepcopy_cache(cache), seq_len, query_prompt, answer_text,\n",
    "            model, tokenizer, config, position_offset=position_offset\n",
    "        )\n",
    "\n",
    "    results = {'doc_len_tokens': doc_len}\n",
    "\n",
    "    # --- bare ---\n",
    "    if 'bare' in conditions:\n",
    "        bare_input = torch.cat([bos_tensor, doc_tensor], dim=1)\n",
    "        with torch.no_grad():\n",
    "            out = model(\n",
    "                input_ids=bare_input,\n",
    "                attention_mask=torch.ones_like(bare_input),\n",
    "                use_cache=True, return_dict=True,\n",
    "            )\n",
    "        results['bare'] = score(out.past_key_values, bare_input.shape[1])\n",
    "        del out\n",
    "\n",
    "    # --- single_prefix ---\n",
    "    if 'single_prefix' in conditions:\n",
    "        prefix_text = BEACON_TEXT + '\\n'\n",
    "        prefix_ids = tokenizer.encode(prefix_text, add_special_tokens=False)\n",
    "        prefix_tensor = torch.tensor([prefix_ids], device=config.device)\n",
    "        full_input = torch.cat([bos_tensor, prefix_tensor, doc_tensor], dim=1)\n",
    "        prefix_token_len = 1 + len(prefix_ids)  # BOS + prefix\n",
    "\n",
    "        with torch.no_grad():\n",
    "            out = model(\n",
    "                input_ids=full_input,\n",
    "                attention_mask=torch.ones_like(full_input),\n",
    "                use_cache=True, return_dict=True,\n",
    "            )\n",
    "        truncated = extract_and_truncate_cache_with_bos(out.past_key_values, doc_len)\n",
    "        keep_len = 1 + doc_len\n",
    "        surrogate_offset = prefix_token_len - 1\n",
    "        correct_rope_positions_with_bos(truncated, surrogate_offset, model)\n",
    "        results['single_prefix'] = score(truncated, keep_len)\n",
    "        del out, truncated\n",
    "\n",
    "    # --- beacon_seq_256 and beacon_seq_512 ---\n",
    "    for cs in chunk_sizes:\n",
    "        cond_name = f'beacon_seq_{cs}'\n",
    "        if cond_name not in conditions:\n",
    "            continue\n",
    "        cache, seq_len, _, bpos, dpos = build_beacon_cache_sequential(\n",
    "            passage, beacon_ids, cs, model, tokenizer, config\n",
    "        )\n",
    "        results[cond_name] = score(cache, seq_len)\n",
    "\n",
    "        # Save 256 cache artifacts for beacon_trunc_256\n",
    "        if cs == 256:\n",
    "            cache_256 = cache\n",
    "            bpos_256 = bpos\n",
    "            dpos_256 = dpos\n",
    "            seq_len_256 = seq_len\n",
    "        else:\n",
    "            del cache\n",
    "\n",
    "    # --- beacon_trunc_256 ---\n",
    "    if 'beacon_trunc_256' in conditions:\n",
    "        # Build beacon_seq_256 if not already done\n",
    "        if 'cache_256' not in dir():\n",
    "            cache_256, seq_len_256, _, bpos_256, dpos_256 = \\\n",
    "                build_beacon_cache_sequential(\n",
    "                    passage, beacon_ids, 256, model, tokenizer, config\n",
    "                )\n",
    "\n",
    "        extract_indices = [0] + dpos_256  # BOS + doc positions\n",
    "        extracted = extract_cache_at_indices(cache_256, extract_indices)\n",
    "\n",
    "        # Compute per-chunk RoPE correction\n",
    "        n_chunks = -(-doc_len // 256)\n",
    "        boundaries = []\n",
    "        offsets_list = []\n",
    "        pos = 1  # after BOS\n",
    "        for k in range(n_chunks):\n",
    "            boundaries.append(pos)\n",
    "            offsets_list.append((k + 1) * beacon_len)\n",
    "            actual_chunk = min(256, doc_len - k * 256)\n",
    "            pos += actual_chunk\n",
    "\n",
    "        correct_rope_positions_chunked(extracted, boundaries, offsets_list, model)\n",
    "        keep_len = 1 + doc_len\n",
    "        results['beacon_trunc_256'] = score(extracted, keep_len)\n",
    "        del extracted\n",
    "\n",
    "    # Clean up 256 cache if still around\n",
    "    if 'cache_256' in dir():\n",
    "        del cache_256\n",
    "\n",
    "    # --- random_beacon_256 ---\n",
    "    if 'random_beacon_256' in conditions:\n",
    "        cache, seq_len, _, _, _ = build_beacon_cache_sequential(\n",
    "            passage, random_beacon_ids, 256, model, tokenizer, config\n",
    "        )\n",
    "        results['random_beacon_256'] = score(cache, seq_len)\n",
    "        del cache\n",
    "\n",
    "    # --- beacon_batch_256 ---\n",
    "    if 'beacon_batch_256' in conditions:\n",
    "        cache, seq_len, _ = build_beacon_cache_batch(\n",
    "            passage, beacon_ids, 256, model, tokenizer, config\n",
    "        )\n",
    "        results['beacon_batch_256'] = score(cache, seq_len)\n",
    "        del cache\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    return results\n",
    "\n",
    "\n",
    "def run_analysis(results, condition_names, dataset_label):\n",
    "    \"\"\"Run statistical analysis on experiment results.\"\"\"\n",
    "    cond_arrays = {cn: np.array([r[cn] for r in results]) for cn in condition_names}\n",
    "\n",
    "    # Filter failed samples\n",
    "    valid = np.ones(len(results), dtype=bool)\n",
    "    for cn in condition_names:\n",
    "        valid &= np.isfinite(cond_arrays[cn])\n",
    "        valid &= (cond_arrays[cn] != 0)\n",
    "    n_valid = int(np.sum(valid))\n",
    "    n_excluded = len(results) - n_valid\n",
    "\n",
    "    c = {cn: cond_arrays[cn][valid] for cn in condition_names}\n",
    "\n",
    "    print(f'\\n{\"=\" * 70}')\n",
    "    print(f'{dataset_label} ANALYSIS (n_valid={n_valid}, excluded={n_excluded})')\n",
    "    print('=' * 70)\n",
    "\n",
    "    # All conditions vs bare\n",
    "    print(f'\\n{\"Condition\":<25} {\"Mean NLL\":>10} {\"Mean Δ\":>10} {\"d\":>8} {\"Win%\":>7} {\"p\":>12} {\"sig\":>5}')\n",
    "    print('-' * 80)\n",
    "\n",
    "    all_vs_bare = {}\n",
    "    for cn in condition_names:\n",
    "        if cn == 'bare':\n",
    "            print(f'{cn:<25} {np.mean(c[cn]):>10.4f} {\"---\":>10} {\"---\":>8} {\"---\":>7}')\n",
    "            continue\n",
    "        delta = c['bare'] - c[cn]  # positive = condition is better\n",
    "        d = cohens_d(delta)\n",
    "        win = np.mean(delta > 0) * 100\n",
    "        t_stat, p_val = stats.ttest_1samp(delta, 0)\n",
    "        sig = '***' if p_val < 0.001 else '**' if p_val < 0.01 else '*' if p_val < 0.05 else 'ns'\n",
    "        print(f'{cn:<25} {np.mean(c[cn]):>10.4f} {np.mean(delta):>+10.4f} {d:>+8.3f} {win:>6.1f}% {p_val:>12.2e} {sig:>5}')\n",
    "        all_vs_bare[cn] = {\n",
    "            'mean_nll': float(np.mean(c[cn])),\n",
    "            'mean_delta': float(np.mean(delta)),\n",
    "            'cohens_d': float(d),\n",
    "            'win_pct': float(win),\n",
    "            't_stat': float(t_stat),\n",
    "            'p_value': float(p_val),\n",
    "        }\n",
    "\n",
    "    return {'n_valid': n_valid, 'n_excluded': n_excluded, 'all_vs_bare': all_vs_bare, 'conditions': c}\n",
    "\n",
    "\n",
    "print('Helper functions defined.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: MS MARCO Evaluation\n",
    "\n",
    "from lib.data import load_ms_marco, load_evaluation_samples\n",
    "\n",
    "# Load data\n",
    "msmarco_dataset = load_ms_marco(config)\n",
    "all_msmarco_samples = load_evaluation_samples(msmarco_dataset, config, require_answer=True)\n",
    "msmarco_samples = all_msmarco_samples[:N_MSMARCO]\n",
    "print(f'MS MARCO samples: {len(msmarco_samples)}')\n",
    "\n",
    "# Evaluate conditions 1-4 (bare, single_prefix, beacon_seq_256, beacon_seq_512)\n",
    "msmarco_results = []\n",
    "start_idx = 0\n",
    "\n",
    "if MSMARCO_CHECKPOINT_PATH.exists():\n",
    "    with open(MSMARCO_CHECKPOINT_PATH, 'r') as f:\n",
    "        ckpt = json.load(f)\n",
    "    ckpt_queries = ckpt.get('sample_queries', [])\n",
    "    current_queries = [s['query'] for s in msmarco_samples]\n",
    "    if ckpt_queries == current_queries:\n",
    "        msmarco_results = ckpt['results']\n",
    "        start_idx = len(msmarco_results)\n",
    "        print(f'Resuming from checkpoint: {start_idx}/{N_MSMARCO}')\n",
    "    else:\n",
    "        print('Checkpoint sample mismatch. Starting fresh.')\n",
    "\n",
    "for idx in tqdm(range(start_idx, N_MSMARCO), initial=start_idx, total=N_MSMARCO,\n",
    "                desc='MS MARCO eval'):\n",
    "    sample = msmarco_samples[idx]\n",
    "\n",
    "    result = build_matched_beacon_caches(\n",
    "        sample['passage'], sample['query'], sample['answer'],\n",
    "        MSMARCO_CONDITIONS, model, tokenizer, config,\n",
    "    )\n",
    "    result['idx'] = idx\n",
    "    result['word_count'] = len(sample['passage'].split())\n",
    "    msmarco_results.append(result)\n",
    "\n",
    "    if (idx + 1) % CHECKPOINT_EVERY == 0 or idx == N_MSMARCO - 1:\n",
    "        ckpt_data = {\n",
    "            'results': msmarco_results,\n",
    "            'sample_queries': [s['query'] for s in msmarco_samples],\n",
    "            'completed': len(msmarco_results),\n",
    "            'total': N_MSMARCO,\n",
    "            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        }\n",
    "        with open(MSMARCO_CHECKPOINT_PATH, 'w') as f:\n",
    "            json.dump(ckpt_data, f)\n",
    "\n",
    "print(f'MS MARCO evaluation complete: {len(msmarco_results)} samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: MS MARCO Analysis\n",
    "\n",
    "msmarco_analysis = run_analysis(msmarco_results, MSMARCO_CONDITIONS, 'MS MARCO (Short Docs)')\n",
    "\n",
    "# Head-to-head comparisons\n",
    "c = msmarco_analysis['conditions']\n",
    "print(f'\\n--- Key Comparisons (MS MARCO) ---')\n",
    "print(f'{\"Comparison\":<45} {\"Mean Δ\":>10} {\"d\":>8} {\"p\":>12}')\n",
    "print('-' * 80)\n",
    "\n",
    "h2h = {}\n",
    "comparisons = [\n",
    "    ('beacon_seq_256 vs bare', 'beacon_seq_256', 'bare'),\n",
    "    ('single_prefix vs bare', 'single_prefix', 'bare'),\n",
    "    ('beacon_seq_256 vs single_prefix', 'beacon_seq_256', 'single_prefix'),\n",
    "    ('beacon_seq_256 vs beacon_seq_512', 'beacon_seq_256', 'beacon_seq_512'),\n",
    "]\n",
    "for label, cond_a, cond_b in comparisons:\n",
    "    delta = c[cond_b] - c[cond_a]  # positive = cond_a is better (lower NLL)\n",
    "    d = cohens_d(delta)\n",
    "    _, p_val = stats.ttest_1samp(delta, 0)\n",
    "    print(f'{label:<45} {np.mean(delta):>+10.4f} {d:>+8.3f} {p_val:>12.2e}')\n",
    "    h2h[label] = {'d': float(d), 'p': float(p_val)}\n",
    "\n",
    "# Save\n",
    "msmarco_final = {\n",
    "    'experiment': 'exp18_periodic_beacon_msmarco',\n",
    "    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'config': {\n",
    "        'model_name': MODEL_NAME, 'seed': SEED, 'n_eval': N_MSMARCO,\n",
    "        'dataset': 'MS MARCO v1.1', 'beacon_text': BEACON_TEXT,\n",
    "        'beacon_len': BEACON_LEN, 'chunk_sizes': CHUNK_SIZES,\n",
    "    },\n",
    "    'condition_names': MSMARCO_CONDITIONS,\n",
    "    'analysis': {k: v for k, v in msmarco_analysis.items() if k != 'conditions'},\n",
    "    'head_to_head': h2h,\n",
    "    'per_sample_results': msmarco_results,\n",
    "}\n",
    "with open(MSMARCO_RESULTS_PATH, 'w') as f:\n",
    "    json.dump(msmarco_final, f, indent=2)\n",
    "print(f'\\nSaved to {MSMARCO_RESULTS_PATH}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: NQ Evaluation (Long Docs)\n",
    "\n",
    "# --- Load NQ samples (stratified by length) ---\n",
    "if NQ_SAMPLES_CACHE_PATH.exists():\n",
    "    with open(NQ_SAMPLES_CACHE_PATH, 'r') as f:\n",
    "        nq_cache = json.load(f)\n",
    "    nq_samples = nq_cache['samples']\n",
    "    print(f'Loaded {len(nq_samples)} NQ samples from cache')\n",
    "else:\n",
    "    print('Loading NQ dataset (streaming)...')\n",
    "    nq = load_dataset(\n",
    "        'google-research-datasets/natural_questions',\n",
    "        split='validation',\n",
    "        streaming=True,\n",
    "    )\n",
    "\n",
    "    bin_samples = {name: [] for name, _, _ in LENGTH_BINS}\n",
    "    n_processed = 0\n",
    "\n",
    "    for example in tqdm(nq, desc='Processing NQ'):\n",
    "        n_processed += 1\n",
    "\n",
    "        # Extract clean document text\n",
    "        doc_tokens = example['document']['tokens']\n",
    "        if isinstance(doc_tokens, dict):\n",
    "            token_strs = doc_tokens['token']\n",
    "            is_html_flags = doc_tokens['is_html']\n",
    "            clean_tokens = [t for t, h in zip(token_strs, is_html_flags) if not h]\n",
    "        else:\n",
    "            clean_tokens = [t['token'] for t in doc_tokens if not t['is_html']]\n",
    "\n",
    "        doc_text = ' '.join(clean_tokens)\n",
    "        word_count = len(doc_text.split())\n",
    "\n",
    "        if word_count < LENGTH_BINS[0][1]:\n",
    "            continue\n",
    "        if word_count > MAX_DOC_WORDS:\n",
    "            words = doc_text.split()\n",
    "            doc_text = ' '.join(words[:MAX_DOC_WORDS])\n",
    "            word_count = MAX_DOC_WORDS\n",
    "\n",
    "        # Extract short answer\n",
    "        annotations = example['annotations']\n",
    "        short_answers_list = annotations['short_answers']\n",
    "\n",
    "        answer_text = None\n",
    "        for annotator_sa in short_answers_list:\n",
    "            if not annotator_sa:\n",
    "                continue\n",
    "            texts = annotator_sa.get('text', [])\n",
    "            if texts:\n",
    "                answer_text = texts[0]\n",
    "                break\n",
    "            starts = annotator_sa.get('start_token', [])\n",
    "            ends = annotator_sa.get('end_token', [])\n",
    "            if not starts or not ends:\n",
    "                continue\n",
    "            start_tok = starts[0] if isinstance(starts, list) else starts\n",
    "            end_tok = ends[0] if isinstance(ends, list) else ends\n",
    "            if start_tok >= 0 and end_tok > start_tok:\n",
    "                if isinstance(doc_tokens, dict):\n",
    "                    ans_tokens = [\n",
    "                        doc_tokens['token'][i]\n",
    "                        for i in range(start_tok, min(end_tok, len(doc_tokens['token'])))\n",
    "                        if not doc_tokens['is_html'][i]\n",
    "                    ]\n",
    "                else:\n",
    "                    ans_tokens = [\n",
    "                        doc_tokens[i]['token']\n",
    "                        for i in range(start_tok, min(end_tok, len(doc_tokens)))\n",
    "                        if not doc_tokens[i]['is_html']\n",
    "                    ]\n",
    "                if ans_tokens:\n",
    "                    answer_text = ' '.join(ans_tokens)\n",
    "                    break\n",
    "\n",
    "        if not answer_text or len(answer_text.strip()) == 0:\n",
    "            continue\n",
    "        if len(answer_text.split()) > 20:\n",
    "            continue\n",
    "\n",
    "        question = example['question']\n",
    "        query = question.get('text', '') if isinstance(question, dict) else str(question)\n",
    "\n",
    "        # Assign to length bin\n",
    "        for bin_name, bin_min, bin_max in LENGTH_BINS:\n",
    "            if bin_min <= word_count < bin_max:\n",
    "                if len(bin_samples[bin_name]) < SAMPLES_PER_BIN[bin_name]:\n",
    "                    bin_samples[bin_name].append({\n",
    "                        'passage': doc_text,\n",
    "                        'query': query,\n",
    "                        'answer': answer_text,\n",
    "                        'word_count': word_count,\n",
    "                        'length_bin': bin_name,\n",
    "                    })\n",
    "                break\n",
    "\n",
    "        if all(len(bin_samples[name]) >= SAMPLES_PER_BIN[name] for name, _, _ in LENGTH_BINS):\n",
    "            break\n",
    "\n",
    "    # Combine\n",
    "    nq_samples = []\n",
    "    for bin_name, _, _ in LENGTH_BINS:\n",
    "        bs = bin_samples[bin_name]\n",
    "        np.random.seed(SEED)\n",
    "        np.random.shuffle(bs)\n",
    "        nq_samples.extend(bs)\n",
    "        print(f'  {bin_name}: {len(bs)} samples')\n",
    "\n",
    "    with open(NQ_SAMPLES_CACHE_PATH, 'w') as f:\n",
    "        json.dump({'samples': nq_samples, 'n_processed': n_processed}, f)\n",
    "    print(f'Saved {len(nq_samples)} NQ samples')\n",
    "\n",
    "N_NQ = min(N_NQ, len(nq_samples))\n",
    "print(f'NQ samples to evaluate: {N_NQ}')\n",
    "\n",
    "# Evaluate all 7 conditions\n",
    "nq_results = []\n",
    "start_idx = 0\n",
    "\n",
    "if NQ_CHECKPOINT_PATH.exists():\n",
    "    with open(NQ_CHECKPOINT_PATH, 'r') as f:\n",
    "        ckpt = json.load(f)\n",
    "    ckpt_queries = ckpt.get('sample_queries', [])\n",
    "    current_queries = [s['query'] for s in nq_samples[:N_NQ]]\n",
    "    if ckpt_queries == current_queries:\n",
    "        nq_results = ckpt['results']\n",
    "        start_idx = len(nq_results)\n",
    "        print(f'Resuming from checkpoint: {start_idx}/{N_NQ}')\n",
    "    else:\n",
    "        print('Checkpoint sample mismatch. Starting fresh.')\n",
    "\n",
    "for idx in tqdm(range(start_idx, N_NQ), initial=start_idx, total=N_NQ,\n",
    "                desc='NQ eval'):\n",
    "    sample = nq_samples[idx]\n",
    "\n",
    "    result = build_matched_beacon_caches(\n",
    "        sample['passage'], sample['query'], sample['answer'],\n",
    "        NQ_CONDITIONS, model, tokenizer, config,\n",
    "    )\n",
    "    result['idx'] = idx\n",
    "    result['word_count'] = sample['word_count']\n",
    "    result['length_bin'] = sample['length_bin']\n",
    "    nq_results.append(result)\n",
    "\n",
    "    if (idx + 1) % CHECKPOINT_EVERY == 0 or idx == N_NQ - 1:\n",
    "        ckpt_data = {\n",
    "            'results': nq_results,\n",
    "            'sample_queries': [s['query'] for s in nq_samples[:N_NQ]],\n",
    "            'completed': len(nq_results),\n",
    "            'total': N_NQ,\n",
    "            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        }\n",
    "        with open(NQ_CHECKPOINT_PATH, 'w') as f:\n",
    "            json.dump(ckpt_data, f)\n",
    "\n",
    "print(f'NQ evaluation complete: {len(nq_results)} samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: NQ Analysis\n",
    "\n",
    "nq_analysis = run_analysis(nq_results, NQ_CONDITIONS, 'NQ (Long Docs)')\n",
    "c = nq_analysis['conditions']\n",
    "\n",
    "# --- Key Comparisons ---\n",
    "print(f'\\n--- Key Comparisons (NQ) ---')\n",
    "print(f'{\"Question\":<55} {\"Comparison\":<35} {\"d\":>8} {\"p\":>12}')\n",
    "print('-' * 115)\n",
    "\n",
    "nq_h2h = {}\n",
    "key_comparisons = [\n",
    "    ('Do periodic beacons help long docs?', 'beacon_seq_256', 'bare'),\n",
    "    ('Does stride matter?', 'beacon_seq_256', 'beacon_seq_512'),\n",
    "    ('Contamination or attention routing?', 'beacon_trunc_256', 'beacon_seq_256'),\n",
    "    ('Semantic or structural?', 'random_beacon_256', 'beacon_seq_256'),\n",
    "    ('Cross-chunk attention needed?', 'beacon_batch_256', 'beacon_seq_256'),\n",
    "    ('Better than single prefix?', 'beacon_seq_256', 'single_prefix'),\n",
    "]\n",
    "for question, cond_a, cond_b in key_comparisons:\n",
    "    delta = c[cond_b] - c[cond_a]  # positive = cond_a is better\n",
    "    d = cohens_d(delta)\n",
    "    _, p_val = stats.ttest_1samp(delta, 0)\n",
    "    sig = '***' if p_val < 0.001 else '**' if p_val < 0.01 else '*' if p_val < 0.05 else 'ns'\n",
    "    label = f'{cond_a} vs {cond_b}'\n",
    "    print(f'{question:<55} {label:<35} {d:>+8.3f} {p_val:>12.2e} {sig}')\n",
    "    nq_h2h[question] = {'cond_a': cond_a, 'cond_b': cond_b, 'd': float(d), 'p': float(p_val)}\n",
    "\n",
    "# --- Length-Stratified Analysis ---\n",
    "print(f'\\n--- Length-Stratified Analysis ---')\n",
    "length_bins_arr = np.array([r['length_bin'] for r in nq_results])\n",
    "valid_mask = np.ones(len(nq_results), dtype=bool)\n",
    "for cn in NQ_CONDITIONS:\n",
    "    arr = np.array([r[cn] for r in nq_results])\n",
    "    valid_mask &= np.isfinite(arr) & (arr != 0)\n",
    "length_bins_valid = length_bins_arr[valid_mask]\n",
    "word_counts_valid = np.array([r['word_count'] for r in nq_results])[valid_mask]\n",
    "\n",
    "c_valid = {cn: np.array([r[cn] for r in nq_results])[valid_mask] for cn in NQ_CONDITIONS}\n",
    "\n",
    "bin_names_ordered = [name for name, _, _ in LENGTH_BINS]\n",
    "per_bin = {}\n",
    "\n",
    "header = f'{\"Condition\":<25}'\n",
    "for bn in bin_names_ordered:\n",
    "    header += f'{bn:>15}'\n",
    "print(header)\n",
    "print('-' * (25 + 15 * len(bin_names_ordered)))\n",
    "\n",
    "for cn in NQ_CONDITIONS:\n",
    "    if cn == 'bare':\n",
    "        continue\n",
    "    row = f'{cn:<25}'\n",
    "    cn_bins = {}\n",
    "    for bn in bin_names_ordered:\n",
    "        mask = length_bins_valid == bn\n",
    "        n_bin = int(np.sum(mask))\n",
    "        if n_bin < 5:\n",
    "            row += f'{\"n/a\":>15}'\n",
    "            continue\n",
    "        delta = c_valid['bare'][mask] - c_valid[cn][mask]\n",
    "        d = cohens_d(delta)\n",
    "        win = np.mean(delta > 0) * 100\n",
    "        row += f'{d:>+7.3f} ({win:4.0f}%)'\n",
    "        cn_bins[bn] = {'d': float(d), 'win': float(win), 'n': n_bin}\n",
    "    print(row)\n",
    "    per_bin[cn] = cn_bins\n",
    "\n",
    "# --- Mechanism Decomposition ---\n",
    "print(f'\\n--- Mechanism Decomposition ---')\n",
    "print('beacon_seq_256 = value contamination + attention routing to beacons')\n",
    "print('beacon_trunc_256 = value contamination only (beacons removed at query time)')\n",
    "print()\n",
    "for bn in bin_names_ordered:\n",
    "    mask = length_bins_valid == bn\n",
    "    n_bin = int(np.sum(mask))\n",
    "    if n_bin < 5:\n",
    "        continue\n",
    "    d_seq = cohens_d(c_valid['bare'][mask] - c_valid['beacon_seq_256'][mask])\n",
    "    d_trunc = cohens_d(c_valid['bare'][mask] - c_valid['beacon_trunc_256'][mask])\n",
    "    d_diff = d_seq - d_trunc\n",
    "    print(f'  {bn}: seq d={d_seq:+.3f}, trunc d={d_trunc:+.3f}, attention contribution={d_diff:+.3f}')\n",
    "\n",
    "# --- Length correlation ---\n",
    "print(f'\\n--- Length × Effect Size Correlation ---')\n",
    "interaction = {}\n",
    "for cn in NQ_CONDITIONS:\n",
    "    if cn == 'bare':\n",
    "        continue\n",
    "    delta = c_valid['bare'] - c_valid[cn]\n",
    "    r_s, p_s = spearmanr(word_counts_valid, delta)\n",
    "    print(f'  {cn}: Spearman r={r_s:+.3f} (p={p_s:.3f})')\n",
    "    interaction[cn] = {'spearman_r': float(r_s), 'spearman_p': float(p_s)}\n",
    "\n",
    "nq_analysis_clean = {k: v for k, v in nq_analysis.items() if k != 'conditions'}\n",
    "nq_analysis_clean['head_to_head'] = nq_h2h\n",
    "nq_analysis_clean['per_bin'] = per_bin\n",
    "nq_analysis_clean['length_interaction'] = interaction\n",
    "\n",
    "# Save\n",
    "nq_final = {\n",
    "    'experiment': 'exp18_periodic_beacon_nq',\n",
    "    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'config': {\n",
    "        'model_name': MODEL_NAME, 'seed': SEED, 'n_eval': N_NQ,\n",
    "        'dataset': 'google-research-datasets/natural_questions',\n",
    "        'beacon_text': BEACON_TEXT, 'beacon_len': BEACON_LEN,\n",
    "        'chunk_sizes': CHUNK_SIZES, 'length_bins': LENGTH_BINS,\n",
    "        'samples_per_bin': SAMPLES_PER_BIN,\n",
    "    },\n",
    "    'condition_names': NQ_CONDITIONS,\n",
    "    'analysis': nq_analysis_clean,\n",
    "    'per_sample_results': nq_results,\n",
    "}\n",
    "with open(NQ_RESULTS_PATH, 'w') as f:\n",
    "    json.dump(nq_final, f, indent=2)\n",
    "print(f'\\nSaved to {NQ_RESULTS_PATH}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Summary & Figures\n",
    "\n",
    "# Reload results for analysis (in case cells are run out of order)\n",
    "nq_c = {}\n",
    "nq_arr = {cn: np.array([r[cn] for r in nq_results]) for cn in NQ_CONDITIONS}\n",
    "nq_valid = np.ones(len(nq_results), dtype=bool)\n",
    "for cn in NQ_CONDITIONS:\n",
    "    nq_valid &= np.isfinite(nq_arr[cn]) & (nq_arr[cn] != 0)\n",
    "nq_c = {cn: nq_arr[cn][nq_valid] for cn in NQ_CONDITIONS}\n",
    "nq_bins_arr = np.array([r['length_bin'] for r in nq_results])[nq_valid]\n",
    "nq_words_arr = np.array([r['word_count'] for r in nq_results])[nq_valid]\n",
    "\n",
    "# --- Figure 1: Effect Size Comparison (all conditions vs bare) ---\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "conds_to_plot = [cn for cn in NQ_CONDITIONS if cn != 'bare']\n",
    "ds = [cohens_d(nq_c['bare'] - nq_c[cn]) for cn in conds_to_plot]\n",
    "colors = ['#2196F3', '#4CAF50', '#8BC34A', '#FF9800', '#F44336', '#9C27B0']\n",
    "x_pos = np.arange(len(conds_to_plot))\n",
    "\n",
    "bars = ax.bar(x_pos, ds, color=colors[:len(conds_to_plot)], alpha=0.85, edgecolor='black', linewidth=0.5)\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels(conds_to_plot, rotation=25, ha='right', fontsize=9)\n",
    "ax.set_ylabel(\"Cohen's d vs bare (positive = better)\", fontsize=11)\n",
    "ax.set_title('Exp 18: Effect Sizes on NQ (All Conditions vs Bare)', fontsize=13)\n",
    "ax.axhline(y=0, color='black', linestyle='-', linewidth=0.8)\n",
    "for i, (d_val, bar) in enumerate(zip(ds, bars)):\n",
    "    ax.text(bar.get_x() + bar.get_width() / 2, d_val + 0.01 * np.sign(d_val),\n",
    "            f'{d_val:+.3f}', ha='center', va='bottom' if d_val >= 0 else 'top', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'effect_sizes_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f'Saved: {FIGURES_DIR / \"effect_sizes_comparison.png\"}')\n",
    "\n",
    "# --- Figure 2: Length × Method Interaction ---\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "plot_conds = [\n",
    "    ('single_prefix', '#2196F3', 'o', '--'),\n",
    "    ('beacon_seq_256', '#4CAF50', 's', '-'),\n",
    "    ('beacon_trunc_256', '#FF9800', '^', '-'),\n",
    "    ('random_beacon_256', '#F44336', 'D', ':'),\n",
    "    ('beacon_batch_256', '#9C27B0', 'v', ':'),\n",
    "]\n",
    "\n",
    "bin_names_ordered = [name for name, _, _ in LENGTH_BINS]\n",
    "for cn, color, marker, ls in plot_conds:\n",
    "    plot_ds = []\n",
    "    bin_centers = []\n",
    "    for bn, bmin, bmax in LENGTH_BINS:\n",
    "        mask = nq_bins_arr == bn\n",
    "        n_bin = int(np.sum(mask))\n",
    "        if n_bin < 5:\n",
    "            continue\n",
    "        d = cohens_d(nq_c['bare'][mask] - nq_c[cn][mask])\n",
    "        plot_ds.append(d)\n",
    "        bin_centers.append((bmin + bmax) / 2)\n",
    "    ax.plot(bin_centers, plot_ds, marker=marker, label=cn, color=color,\n",
    "            linewidth=2, markersize=8, linestyle=ls)\n",
    "\n",
    "ax.set_xlabel('Document Length (words, bin center)', fontsize=11)\n",
    "ax.set_ylabel(\"Cohen's d vs bare\", fontsize=11)\n",
    "ax.set_title('Exp 18: Length × Method Interaction (NQ)', fontsize=13)\n",
    "ax.legend(fontsize=9)\n",
    "ax.axhline(y=0, color='black', linestyle='-', linewidth=0.8)\n",
    "ax.set_xscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'length_interaction.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f'Saved: {FIGURES_DIR / \"length_interaction.png\"}')\n",
    "\n",
    "# --- Figure 3: Mechanism Decomposition ---\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "\n",
    "decomp_conds = ['beacon_seq_256', 'beacon_trunc_256']\n",
    "width = 0.35\n",
    "x = np.arange(len(bin_names_ordered))\n",
    "\n",
    "for i, (cn, color, label) in enumerate([\n",
    "    ('beacon_seq_256', '#4CAF50', 'Full (contamination + attention)'),\n",
    "    ('beacon_trunc_256', '#FF9800', 'Truncated (contamination only)'),\n",
    "]):\n",
    "    bin_ds = []\n",
    "    for bn in bin_names_ordered:\n",
    "        mask = nq_bins_arr == bn\n",
    "        if np.sum(mask) < 5:\n",
    "            bin_ds.append(0)\n",
    "        else:\n",
    "            bin_ds.append(cohens_d(nq_c['bare'][mask] - nq_c[cn][mask]))\n",
    "    ax.bar(x + i * width - width / 2, bin_ds, width, label=label,\n",
    "           color=color, alpha=0.85, edgecolor='black', linewidth=0.5)\n",
    "\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(bin_names_ordered)\n",
    "ax.set_ylabel(\"Cohen's d vs bare\")\n",
    "ax.set_title('Mechanism Decomposition: Contamination vs Attention')\n",
    "ax.legend()\n",
    "ax.axhline(y=0, color='black', linestyle='-', linewidth=0.8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'mechanism_decomposition.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f'Saved: {FIGURES_DIR / \"mechanism_decomposition.png\"}')\n",
    "\n",
    "# --- Summary ---\n",
    "print('\\n' + '=' * 70)\n",
    "print('EXPERIMENT 18 SUMMARY')\n",
    "print('=' * 70)\n",
    "print()\n",
    "print('Key question: Do periodic beacon injections recover priming benefit')\n",
    "print('on long documents where single-prefix priming fails?')\n",
    "print()\n",
    "print('NQ Results (all conditions vs bare):')\n",
    "for cn in NQ_CONDITIONS:\n",
    "    if cn == 'bare':\n",
    "        continue\n",
    "    delta = nq_c['bare'] - nq_c[cn]\n",
    "    d = cohens_d(delta)\n",
    "    win = np.mean(delta > 0) * 100\n",
    "    _, p = stats.ttest_1samp(delta, 0)\n",
    "    sig = '***' if p < 0.001 else '**' if p < 0.01 else '*' if p < 0.05 else 'ns'\n",
    "    verdict = 'HELPS' if d > 0 and p < 0.05 else 'HURTS' if d < 0 and p < 0.05 else 'NEUTRAL'\n",
    "    print(f'  {cn:<25} d={d:+.3f}  win={win:.0f}%  p={p:.2e}  {sig}  → {verdict}')\n",
    "\n",
    "# Save combined analysis\n",
    "combined = {\n",
    "    'experiment': 'exp18_periodic_beacon',\n",
    "    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'msmarco': {k: v for k, v in msmarco_analysis.items() if k != 'conditions'},\n",
    "    'nq': nq_analysis_clean,\n",
    "}\n",
    "with open(RESULTS_DIR / 'analysis_summary.json', 'w') as f:\n",
    "    json.dump(combined, f, indent=2)\n",
    "print(f'\\nAll results saved to {RESULTS_DIR}/')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
