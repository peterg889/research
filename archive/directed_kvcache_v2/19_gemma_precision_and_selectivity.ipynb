{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exp 19: Gemma Priming \u2014 Precision Fix & Selective Value Contamination\n",
    "\n",
    "## Motivation\n",
    "\n",
    "Exp 16 showed priming **FAILS** on Gemma 3 4B (`static_fact_trunc` d=-0.031, ns), but\n",
    "**`values_only` works** (d=+0.056, p=0.009). The gap reveals **-0.087 of key interference**.\n",
    "\n",
    "Two hypotheses explain this:\n",
    "\n",
    "### H1: Precision Hypothesis\n",
    "RoPE correction in bfloat16 (7-bit mantissa) with head_dim=256 introduces ~8.6x more\n",
    "quantization noise than Mistral's float16 (10-bit mantissa) / 128-dim. Computing the\n",
    "correction in float32 may recover the effect.\n",
    "\n",
    "### H2: Selectivity Hypothesis\n",
    "On Mistral (Exp 09), value contamination signal lives in layers 0-15 (88% of effect) and\n",
    "first 25% of positions (dominant). Targeting these on Gemma \u2014 and reducing the \"dose\" \u2014\n",
    "may amplify the weak d=+0.056.\n",
    "\n",
    "## Exp 16 Reference Values\n",
    "\n",
    "| Condition | Gemma d | Mistral d | Gap |\n",
    "|-----------|---------|-----------|-----|\n",
    "| static_fact_trunc | -0.031 (ns) | +0.472 | -0.503 |\n",
    "| random_trunc | -0.109 (***) | +0.091 | -0.200 |\n",
    "| values_only | +0.056 (**) | +0.275 | -0.219 |\n",
    "| Key interference | -0.087 | ~0 | \u2014 |\n",
    "\n",
    "## Design: 9 Conditions, N=300 queries (MS MARCO)\n",
    "\n",
    "| # | Condition | Description | Tests |\n",
    "|---|-----------|-------------|-------|\n",
    "| 1 | `bare` | Baseline | \u2014 |\n",
    "| 2 | `sf_trunc` | Standard truncated + bfloat16 RoPE correction | Replicate Exp 16 reference |\n",
    "| 3 | `sf_trunc_fp32` | Truncated + **float32** RoPE correction | **H1: precision hypothesis** |\n",
    "| 4 | `sf_trunc_nocorr` | Truncated, NO RoPE correction | Is correction worse than mismatch? |\n",
    "| 5 | `values_only` | Bare keys + sf primed values (all layers) | Replicate Exp 16 d=+0.056 |\n",
    "| 6 | `values_early_layers` | Values_only, layers 0-16 only | **H2: layer selectivity** |\n",
    "| 7 | `values_early_pos` | Values_only, first 25% of doc positions only | **H2: position selectivity** |\n",
    "| 8 | `values_alpha_25` | 25% primed / 75% bare value blend | **H2: dose reduction** |\n",
    "| 9 | `rope_roundtrip` | Bare cache + RoPE roundtrip noise on keys | **Control: noise vs content** |\n",
    "\n",
    "## 7 Primary Comparisons (Bonferroni \u03b1 = 0.05/7 = 0.00714)\n",
    "\n",
    "| # | Comparison | Question |\n",
    "|---|-----------|----------|\n",
    "| C1 | sf_trunc_fp32 vs sf_trunc | Does fp32 precision help? |\n",
    "| C2 | sf_trunc_nocorr vs bare | Does uncorrected truncation hurt? |\n",
    "| C3 | sf_trunc_nocorr vs sf_trunc | Is correction better than no correction? |\n",
    "| C4 | values_only vs bare | Replicate Exp 16 d=+0.056 |\n",
    "| C5 | values_early_layers vs values_only | Does layer selectivity help? |\n",
    "| C6 | values_early_pos vs values_only | Does position selectivity help? |\n",
    "| C7 | values_alpha_25 vs values_only | Does reduced dose help? |"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 1: Setup\n",
    "import os\n",
    "os.umask(0o000)\n",
    "\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import gc\n",
    "from pathlib import Path\n",
    "\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "RESULTS_DIR = Path(\"results/exp19\")\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CHECKPOINT_PATH = RESULTS_DIR / \"checkpoint.json\"\n",
    "FINAL_RESULTS_PATH = RESULTS_DIR / \"results.json\"\n",
    "\n",
    "print(f\"SEED: {SEED}\")\n",
    "print(f\"Results directory: {RESULTS_DIR}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 2: Load Gemma 3 4B via load_model()\n",
    "sys.path.insert(0, \".\")\n",
    "\n",
    "from lib.config import ExperimentConfig\n",
    "from lib.model_utils import load_model\n",
    "\n",
    "MODEL_NAME = \"google/gemma-3-4b-it\"\n",
    "\n",
    "exp_config = ExperimentConfig(\n",
    "    model_name=MODEL_NAME,\n",
    "    model_type=\"gemma3\",\n",
    "    compute_dtype=\"auto\",  # resolves to bfloat16 for gemma3\n",
    "    use_4bit=True,\n",
    "    num_samples=2000,\n",
    "    seed=SEED,\n",
    ")\n",
    "\n",
    "print(f\"Loading {MODEL_NAME} (4-bit, bfloat16)...\")\n",
    "model, tokenizer = load_model(exp_config)\n",
    "\n",
    "# Architecture diagnostics\n",
    "from lib.kv_cache import _get_text_config, _get_head_dim, _get_rope_theta_for_layer, _get_cache_keys, _ensure_dynamic_cache\n",
    "\n",
    "text_config = _get_text_config(model.config)\n",
    "print(f\"\\nModel loaded successfully.\")\n",
    "print(f\"  Model class: {type(model).__name__}\")\n",
    "print(f\"  Text config class: {type(text_config).__name__}\")\n",
    "print(f\"  Hidden size: {text_config.hidden_size}\")\n",
    "print(f\"  Num layers: {text_config.num_hidden_layers}\")\n",
    "print(f\"  Num attention heads: {text_config.num_attention_heads}\")\n",
    "print(f\"  Num KV heads: {text_config.num_key_value_heads}\")\n",
    "print(f\"  Head dim: {_get_head_dim(model.config)}\")\n",
    "print(f\"  BOS token ID: {tokenizer.bos_token_id}\")\n",
    "print(f\"  EOS token ID: {tokenizer.eos_token_id}\")\n",
    "\n",
    "# Per-layer RoPE diagnostics\n",
    "thetas = set()\n",
    "for layer_idx in range(text_config.num_hidden_layers):\n",
    "    thetas.add(_get_rope_theta_for_layer(model.config, layer_idx))\n",
    "print(f\"  Unique RoPE thetas: {sorted(thetas)}\")\n",
    "\n",
    "# Verify dtype\n",
    "sample_ids = tokenizer(\"test\", return_tensors=\"pt\")['input_ids'].to(exp_config.device)\n",
    "with torch.no_grad():\n",
    "    out = model(sample_ids, use_cache=True)\n",
    "    cache_check = _ensure_dynamic_cache(out.past_key_values)\n",
    "    k0 = _get_cache_keys(cache_check, 0)\n",
    "    print(f\"  Cache key dtype: {k0.dtype}\")\n",
    "    print(f\"  Cache key shape: {k0.shape}  (batch, kv_heads, seq, head_dim)\")\n",
    "del out, sample_ids\n",
    "torch.cuda.empty_cache()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 3: Lib imports + custom correct_rope_fp32() + templates + constants\n",
    "from lib.kv_cache import (\n",
    "    _get_cache_keys,\n",
    "    _get_cache_values,\n",
    "    _set_cache_keys,\n",
    "    _set_cache_values,\n",
    "    _ensure_dynamic_cache,\n",
    "    extract_and_truncate_cache_with_bos,\n",
    "    correct_rope_positions_with_bos,\n",
    "    score_answer_with_cache,\n",
    "    deepcopy_cache,\n",
    "    build_hybrid_cache,\n",
    "    replace_values_at_layers,\n",
    "    replace_values_at_positions,\n",
    "    interpolate_values,\n",
    "    apply_rope_roundtrip_noise,\n",
    "    _build_rope_correction,\n",
    "    _get_rope_theta_for_layer,\n",
    "    _get_head_dim,\n",
    "    _rotate_half,\n",
    "    _get_text_config,\n",
    ")\n",
    "from lib.analysis import cohens_d\n",
    "from lib.data import count_words\n",
    "from scipy import stats\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "# === Custom function: RoPE correction in float32 ===\n",
    "def correct_rope_fp32(cache, offset, model):\n",
    "    \"\"\"RoPE correction in float32 (not bfloat16) for Gemma precision test.\n",
    "\n",
    "    Upcasts keys to float32 before applying the correction, then downcasts\n",
    "    back to the original dtype. This isolates the precision hypothesis: if\n",
    "    bfloat16 quantization during RoPE correction is the bottleneck, fp32\n",
    "    should recover the priming effect.\n",
    "    \"\"\"\n",
    "    if offset == 0:\n",
    "        return cache\n",
    "    config = model.config\n",
    "    head_dim = _get_head_dim(config)\n",
    "    text_cfg = _get_text_config(config)\n",
    "    n_layers = text_cfg.num_hidden_layers\n",
    "\n",
    "    # Pre-compute corrections per unique theta (in float32)\n",
    "    corrections = {}\n",
    "    for layer_idx in range(n_layers):\n",
    "        theta = _get_rope_theta_for_layer(config, layer_idx)\n",
    "        if theta not in corrections:\n",
    "            corrections[theta] = _build_rope_correction(offset, head_dim, theta)\n",
    "\n",
    "    for layer_idx in range(n_layers):\n",
    "        theta = _get_rope_theta_for_layer(config, layer_idx)\n",
    "        cos_a, sin_a = corrections[theta]\n",
    "        keys = _get_cache_keys(cache, layer_idx)\n",
    "        device = keys.device\n",
    "        orig_dtype = keys.dtype\n",
    "\n",
    "        # Keep cos/sin in float32, upcast keys to float32\n",
    "        c = cos_a.to(device=device)       # stays float32\n",
    "        s_val = sin_a.to(device=device)    # stays float32\n",
    "\n",
    "        # Skip BOS at position 0, correct doc keys at positions 1:\n",
    "        doc_keys = keys[:, :, 1:, :].float()  # upcast bfloat16 -> float32\n",
    "        corrected = doc_keys * c + _rotate_half(doc_keys) * s_val\n",
    "        corrected = corrected.to(orig_dtype)   # downcast back\n",
    "\n",
    "        _set_cache_keys(cache, layer_idx,\n",
    "                       torch.cat([keys[:, :, :1, :], corrected], dim=2))\n",
    "    return cache\n",
    "\n",
    "\n",
    "# Templates \u2014 bare text, no \"Document:\\n\" framing\n",
    "SURROGATE_PREFIX_TEMPLATE = \"{surrogate}\\n\"\n",
    "DOCUMENT_TEMPLATE = \"{document}\"\n",
    "QUERY_TEMPLATE = \"\\nQuery: {query}\\nAnswer:\"\n",
    "ANSWER_TEMPLATE = \" {answer}\"\n",
    "\n",
    "# Prefix text\n",
    "from lib.surrogate import STATIC_SURROGATE_QUERIES\n",
    "STATIC_FACT = STATIC_SURROGATE_QUERIES['static_factual']['query']\n",
    "\n",
    "# Experiment parameters\n",
    "MAX_QUERIES = 300\n",
    "MAX_PASSAGE_WORDS = 300\n",
    "MIN_PASSAGES_PER_QUERY = 2\n",
    "CHECKPOINT_EVERY = 25\n",
    "\n",
    "CONDITION_NAMES = [\n",
    "    'bare', 'sf_trunc', 'sf_trunc_fp32', 'sf_trunc_nocorr',\n",
    "    'values_only', 'values_early_layers', 'values_early_pos',\n",
    "    'values_alpha_25', 'rope_roundtrip',\n",
    "]\n",
    "\n",
    "N_COMPARISONS = 7\n",
    "BONFERRONI_ALPHA = 0.05 / N_COMPARISONS\n",
    "\n",
    "# Exp 16 reference values (Gemma)\n",
    "EXP16_REF = {\n",
    "    'sf_trunc_d': -0.031,\n",
    "    'random_trunc_d': -0.109,\n",
    "    'values_only_d': 0.056,\n",
    "}\n",
    "\n",
    "# Exp 09 reference values (Mistral layer/position selectivity)\n",
    "EXP09_REF = {\n",
    "    'layers_0_7_d': 0.172,   # ~63% of values-only d=0.275\n",
    "    'layers_8_15_d': 0.069,  # ~25%\n",
    "    'layers_0_15_pct': 0.88, # 88% of total signal in first 16 layers\n",
    "    'first_quarter_pct': 'dominant',\n",
    "    'alpha_025_pct': 0.86,   # 86% of full values-only effect\n",
    "}\n",
    "\n",
    "# Mistral reference values\n",
    "MISTRAL_REF = {\n",
    "    'static_fact_trunc_d': 0.472,\n",
    "    'values_only_d': 0.275,\n",
    "}\n",
    "\n",
    "print(\"Config ready\")\n",
    "print(f\"  Model: {MODEL_NAME}\")\n",
    "print(f\"  MAX_QUERIES: {MAX_QUERIES}\")\n",
    "print(f\"  Conditions: {CONDITION_NAMES}\")\n",
    "print(f\"  N_COMPARISONS: {N_COMPARISONS}, Bonferroni alpha: {BONFERRONI_ALPHA:.4f}\")\n",
    "print(f\"  Static fact prefix: '{STATIC_FACT}'\")\n",
    "print(f\"\\nExp 16 Gemma reference:\")\n",
    "for k, v in EXP16_REF.items():\n",
    "    print(f\"    {k}: {v:+.3f}\")\n",
    "print(f\"\\nExp 09 Mistral selectivity reference:\")\n",
    "for k, v in EXP09_REF.items():\n",
    "    print(f\"    {k}: {v}\")\n",
    "\n",
    "# Verify correct_rope_fp32 works on a small test\n",
    "print(\"\\nVerifying correct_rope_fp32()...\")\n",
    "test_ids = tokenizer(\"Hello world\", return_tensors=\"pt\", add_special_tokens=True)['input_ids'].to(exp_config.device)\n",
    "with torch.no_grad():\n",
    "    test_out = model(test_ids, use_cache=True, return_dict=True)\n",
    "test_cache = _ensure_dynamic_cache(test_out.past_key_values)\n",
    "test_cache_copy = deepcopy_cache(test_cache)\n",
    "correct_rope_fp32(test_cache_copy, 5, model)\n",
    "k_before = _get_cache_keys(test_cache, 0)[:, :, 1:, :]\n",
    "k_after = _get_cache_keys(test_cache_copy, 0)[:, :, 1:, :]\n",
    "diff = (k_before.float() - k_after.float()).abs().mean().item()\n",
    "print(f\"  Mean key difference after fp32 correction (offset=5): {diff:.6f}\")\n",
    "print(f\"  BOS preserved: {torch.equal(_get_cache_keys(test_cache, 0)[:,:,:1,:], _get_cache_keys(test_cache_copy, 0)[:,:,:1,:])}\")\n",
    "del test_out, test_cache, test_cache_copy, test_ids\n",
    "torch.cuda.empty_cache()\n",
    "print(\"  correct_rope_fp32() verified OK\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 4: Load MS MARCO v1.1, filter \u2264300 words, \u22652 passages, limit 300 queries\n",
    "from datasets import load_dataset\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"LOADING MS MARCO v1.1 \u2014 ALL PASSAGES PER QUERY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "dataset = load_dataset(\"microsoft/ms_marco\", \"v1.1\", split=\"validation\",\n",
    "                        trust_remote_code=True)\n",
    "print(f\"Total items in validation: {len(dataset)}\")\n",
    "\n",
    "queries = []\n",
    "np.random.seed(SEED)\n",
    "\n",
    "for item in tqdm(dataset, desc=\"Filtering\"):\n",
    "    passages_info = item.get('passages', {})\n",
    "    passage_texts = passages_info.get('passage_text', [])\n",
    "    is_selected = passages_info.get('is_selected', [])\n",
    "    query = item.get('query', '')\n",
    "    answers = item.get('answers', [])\n",
    "    well_formed = item.get('wellFormedAnswers', [])\n",
    "\n",
    "    if not passage_texts or not query:\n",
    "        continue\n",
    "    if len(passage_texts) < MIN_PASSAGES_PER_QUERY:\n",
    "        continue\n",
    "    if not is_selected or sum(is_selected) == 0:\n",
    "        continue\n",
    "\n",
    "    word_counts = [count_words(p) for p in passage_texts]\n",
    "    if any(wc > MAX_PASSAGE_WORDS for wc in word_counts):\n",
    "        continue\n",
    "\n",
    "    answer = None\n",
    "    if well_formed and len(well_formed) > 0 and well_formed[0] != '[]':\n",
    "        answer = well_formed[0]\n",
    "    elif answers and len(answers) > 0 and answers[0] != 'No Answer Present.':\n",
    "        answer = answers[0]\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "    passage_list = []\n",
    "    for i, (ptext, sel) in enumerate(zip(passage_texts, is_selected)):\n",
    "        passage_list.append({\n",
    "            'passage': ptext,\n",
    "            'is_relevant': bool(sel == 1),\n",
    "            'word_count': word_counts[i],\n",
    "            'passage_idx': i,\n",
    "        })\n",
    "\n",
    "    queries.append({\n",
    "        'query': query,\n",
    "        'answer': answer,\n",
    "        'passages': passage_list,\n",
    "        'n_passages': len(passage_list),\n",
    "        'n_relevant': sum(1 for p in passage_list if p['is_relevant']),\n",
    "    })\n",
    "\n",
    "    if len(queries) >= MAX_QUERIES * 3:\n",
    "        break\n",
    "\n",
    "np.random.shuffle(queries)\n",
    "queries = queries[:MAX_QUERIES]\n",
    "N = len(queries)\n",
    "\n",
    "n_passages_list = [q['n_passages'] for q in queries]\n",
    "total_passages = sum(n_passages_list)\n",
    "\n",
    "print(f\"\\nSelected {N} queries ({total_passages} total passages)\")\n",
    "print(f\"Passages per query: mean={np.mean(n_passages_list):.1f}, \"\n",
    "      f\"min={min(n_passages_list)}, max={max(n_passages_list)}\")\n",
    "print(f\"Word counts: mean={np.mean([p['word_count'] for q in queries for p in q['passages']]):.0f}\")\n",
    "\n",
    "del dataset\n",
    "gc.collect()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 5: Tokenize prefix and verify BPE boundaries\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PREFIX TOKENIZATION \u2014 GEMMA 3 4B\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "sf_str = SURROGATE_PREFIX_TEMPLATE.format(surrogate=STATIC_FACT)\n",
    "\n",
    "sf_ids = tokenizer(sf_str, return_tensors=\"pt\",\n",
    "                    add_special_tokens=False)['input_ids'].to(exp_config.device)\n",
    "\n",
    "print(f\"\\nStatic fact prefix: '{STATIC_FACT}'\")\n",
    "print(f\"  Formatted: '{sf_str.strip()}'\")\n",
    "print(f\"  Token length: {sf_ids.shape[1]}\")\n",
    "\n",
    "# Verify BPE boundary consistency\n",
    "print(\"\\nBPE BOUNDARY CHECK (first passage):\")\n",
    "example_doc = queries[0]['passages'][0]['passage']\n",
    "concat = sf_str + DOCUMENT_TEMPLATE.format(document=example_doc)\n",
    "concat_enc = tokenizer(concat, add_special_tokens=True)['input_ids']\n",
    "prefix_enc = tokenizer(sf_str, add_special_tokens=True)['input_ids']\n",
    "doc_ids_from_concat = concat_enc[len(prefix_enc):]\n",
    "\n",
    "bare_doc_enc = tokenizer(DOCUMENT_TEMPLATE.format(document=example_doc),\n",
    "                          add_special_tokens=False)['input_ids']\n",
    "match = sum(1 for a, b in zip(doc_ids_from_concat, bare_doc_enc) if a == b)\n",
    "total = max(len(bare_doc_enc), 1)\n",
    "print(f\"  static_fact: {match}/{total} tokens match ({100*match/total:.1f}%)\")\n",
    "\n",
    "# Condition explanation\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"CONDITION DETAILS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "conditions_detail = [\n",
    "    (\"1. bare\",\n",
    "     \"[BOS][doc]\",\n",
    "     \"Baseline. All other conditions compared to this.\"),\n",
    "    (\"2. sf_trunc\",\n",
    "     f\"[BOS]['{STATIC_FACT}'\\n][doc] -> truncate + RoPE correct (bfloat16)\",\n",
    "     \"Standard Exp 16 condition. Expected d ~ -0.031 (ns).\"),\n",
    "    (\"3. sf_trunc_fp32\",\n",
    "     f\"Same as sf_trunc but RoPE correction computed in float32\",\n",
    "     \"H1 TEST: Does float32 precision recover the priming effect?\"),\n",
    "    (\"4. sf_trunc_nocorr\",\n",
    "     f\"[BOS]['{STATIC_FACT}'\\n][doc] -> truncate, NO RoPE correction\",\n",
    "     \"Control: Is correction worse than position mismatch?\"),\n",
    "    (\"5. values_only\",\n",
    "     \"Bare keys + sf primed values (all layers, all positions)\",\n",
    "     \"Replicate Exp 16 d=+0.056. Isolates value contamination.\"),\n",
    "    (\"6. values_early_layers\",\n",
    "     \"values_only but ONLY layers 0-16 (first 50% of 34 layers)\",\n",
    "     \"H2 TEST: Layer selectivity. On Mistral layers 0-15 = 88%.\"),\n",
    "    (\"7. values_early_pos\",\n",
    "     \"values_only but ONLY first 25% of document positions\",\n",
    "     \"H2 TEST: Position selectivity. On Mistral first 25% = dominant.\"),\n",
    "    (\"8. values_alpha_25\",\n",
    "     \"25% primed values + 75% bare values (linear blend)\",\n",
    "     \"H2 TEST: Dose reduction. On Mistral alpha=0.25 retains 86%.\"),\n",
    "    (\"9. rope_roundtrip\",\n",
    "     \"Bare cache with RoPE(+offset) then RoPE(-offset) applied to keys\",\n",
    "     \"CONTROL: Pure bfloat16 quantization noise, no content signal.\"),\n",
    "]\n",
    "\n",
    "for name, detail, purpose in conditions_detail:\n",
    "    print(f\"\\n### {name} ###\")\n",
    "    print(f\"  Cache: {detail}\")\n",
    "    print(f\"  Purpose: {purpose}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 6: Main loop \u2014 2 fwd passes per passage, build 9 cache variants, score all\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"MAIN EVALUATION ({N} queries, ~{total_passages} passages)\")\n",
    "print(\"Model: Gemma 3 4B | 9 conditions\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Checkpoint resume\n",
    "all_results = []\n",
    "start_idx = 0\n",
    "\n",
    "if CHECKPOINT_PATH.exists():\n",
    "    with open(CHECKPOINT_PATH, 'r') as f:\n",
    "        ckpt = json.load(f)\n",
    "    ckpt_queries = ckpt.get('query_texts', [])\n",
    "    current_queries = [q['query'] for q in queries]\n",
    "    if ckpt_queries == current_queries:\n",
    "        all_results = ckpt['results']\n",
    "        start_idx = len(all_results)\n",
    "        print(f\"Resuming from checkpoint: {start_idx}/{N}\")\n",
    "    else:\n",
    "        print(\"Checkpoint query mismatch. Starting fresh.\")\n",
    "else:\n",
    "    print(\"No checkpoint found. Starting fresh.\")\n",
    "\n",
    "print(f\"Evaluating queries {start_idx} to {N-1}\")\n",
    "print(f\"Per passage: 2 forward passes + 9 scoring passes\")\n",
    "\n",
    "t_start = time.time()\n",
    "\n",
    "for qidx in tqdm(range(start_idx, N), initial=start_idx, total=N, desc=\"Queries\"):\n",
    "    query_data = queries[qidx]\n",
    "    query = query_data['query']\n",
    "    answer = query_data['answer']\n",
    "    query_prompt = QUERY_TEMPLATE.format(query=query)\n",
    "    answer_text = ANSWER_TEMPLATE.format(answer=answer)\n",
    "\n",
    "    passage_results = []\n",
    "\n",
    "    for pidx, pinfo in enumerate(query_data['passages']):\n",
    "        passage = pinfo['passage']\n",
    "        document_text = DOCUMENT_TEMPLATE.format(document=passage)\n",
    "\n",
    "        # --- Matched tokenization (using sf prefix as reference) ---\n",
    "        full_text = sf_str + document_text\n",
    "        full_enc = tokenizer(full_text, return_tensors=\"pt\",\n",
    "                              add_special_tokens=True, padding=False, truncation=False)\n",
    "        full_ids = full_enc['input_ids'].to(exp_config.device)\n",
    "\n",
    "        sf_prefix_enc = tokenizer(sf_str, return_tensors=\"pt\",\n",
    "                                   add_special_tokens=True, padding=False, truncation=False)\n",
    "        sf_prefix_len_matched = sf_prefix_enc['input_ids'].shape[1]\n",
    "\n",
    "        bos_id = full_ids[:, :1]\n",
    "        doc_ids = full_ids[:, sf_prefix_len_matched:]\n",
    "        doc_len = doc_ids.shape[1]\n",
    "        context_len = 1 + doc_len  # BOS + doc\n",
    "\n",
    "        del full_enc, full_ids, sf_prefix_enc\n",
    "\n",
    "        # === Forward pass 1: BARE cache ===\n",
    "        bare_input = torch.cat([bos_id, doc_ids], dim=1)\n",
    "        with torch.no_grad():\n",
    "            bare_out = model(input_ids=bare_input,\n",
    "                             attention_mask=torch.ones_like(bare_input),\n",
    "                             use_cache=True, return_dict=True)\n",
    "        bare_cache = _ensure_dynamic_cache(bare_out.past_key_values)\n",
    "        del bare_out, bare_input\n",
    "\n",
    "        # === Forward pass 2: PRIMED cache (static_fact prefix) ===\n",
    "        primed_input = torch.cat([bos_id, sf_ids, doc_ids], dim=1)\n",
    "        with torch.no_grad():\n",
    "            primed_out = model(input_ids=primed_input,\n",
    "                               attention_mask=torch.ones_like(primed_input),\n",
    "                               use_cache=True, return_dict=True)\n",
    "        primed_full = _ensure_dynamic_cache(primed_out.past_key_values)\n",
    "        del primed_out, primed_input\n",
    "\n",
    "        # === Truncate once: keep [BOS] + [last doc_len positions] ===\n",
    "        trunc_raw = extract_and_truncate_cache_with_bos(primed_full, doc_len)\n",
    "        prefix_offset = sf_ids.shape[1]\n",
    "        del primed_full\n",
    "\n",
    "        # === Build 9 scoring caches ===\n",
    "\n",
    "        # (1) bare \u2014 use bare_cache directly\n",
    "        # Score bare LAST since score_answer_with_cache mutates\n",
    "\n",
    "        # (2) sf_trunc \u2014 standard bfloat16 correction\n",
    "        sf_trunc_cache = deepcopy_cache(trunc_raw)\n",
    "        correct_rope_positions_with_bos(sf_trunc_cache, prefix_offset, model)\n",
    "\n",
    "        # (3) sf_trunc_fp32 \u2014 float32 precision correction\n",
    "        sf_trunc_fp32_cache = deepcopy_cache(trunc_raw)\n",
    "        correct_rope_fp32(sf_trunc_fp32_cache, prefix_offset, model)\n",
    "\n",
    "        # (4) sf_trunc_nocorr \u2014 no correction at all\n",
    "        sf_trunc_nocorr_cache = deepcopy_cache(trunc_raw)\n",
    "        # no correction applied\n",
    "\n",
    "        # (5) values_only \u2014 bare keys + sf primed values (from corrected cache)\n",
    "        values_only_cache = build_hybrid_cache(\n",
    "            keys_source=bare_cache,\n",
    "            values_source=sf_trunc_cache,\n",
    "        )\n",
    "\n",
    "        # (6) values_early_layers \u2014 values from layers 0-16 only\n",
    "        values_early_layers_cache = replace_values_at_layers(\n",
    "            bare_cache, sf_trunc_cache, list(range(17))\n",
    "        )\n",
    "\n",
    "        # (7) values_early_pos \u2014 values from first 25% of doc positions only\n",
    "        pos_end = 1 + max(1, doc_len // 4)\n",
    "        values_early_pos_cache = replace_values_at_positions(\n",
    "            bare_cache, sf_trunc_cache, 1, pos_end\n",
    "        )\n",
    "\n",
    "        # (8) values_alpha_25 \u2014 25% primed / 75% bare value blend\n",
    "        values_alpha_25_cache = interpolate_values(\n",
    "            bare_cache, sf_trunc_cache, 0.25\n",
    "        )\n",
    "\n",
    "        # (9) rope_roundtrip \u2014 bare cache + RoPE roundtrip noise\n",
    "        rope_roundtrip_cache = deepcopy_cache(bare_cache)\n",
    "        apply_rope_roundtrip_noise(rope_roundtrip_cache, prefix_offset, model)\n",
    "\n",
    "        del trunc_raw\n",
    "\n",
    "        # === Score all 9 conditions (deepcopy before each except bare which is last) ===\n",
    "        nll_sf_trunc = score_answer_with_cache(\n",
    "            deepcopy_cache(sf_trunc_cache), context_len,\n",
    "            query_prompt, answer_text, model, tokenizer, exp_config)\n",
    "        del sf_trunc_cache\n",
    "\n",
    "        nll_sf_trunc_fp32 = score_answer_with_cache(\n",
    "            deepcopy_cache(sf_trunc_fp32_cache), context_len,\n",
    "            query_prompt, answer_text, model, tokenizer, exp_config)\n",
    "        del sf_trunc_fp32_cache\n",
    "\n",
    "        nll_sf_trunc_nocorr = score_answer_with_cache(\n",
    "            deepcopy_cache(sf_trunc_nocorr_cache), context_len,\n",
    "            query_prompt, answer_text, model, tokenizer, exp_config)\n",
    "        del sf_trunc_nocorr_cache\n",
    "\n",
    "        nll_values_only = score_answer_with_cache(\n",
    "            deepcopy_cache(values_only_cache), context_len,\n",
    "            query_prompt, answer_text, model, tokenizer, exp_config)\n",
    "        del values_only_cache\n",
    "\n",
    "        nll_values_early_layers = score_answer_with_cache(\n",
    "            deepcopy_cache(values_early_layers_cache), context_len,\n",
    "            query_prompt, answer_text, model, tokenizer, exp_config)\n",
    "        del values_early_layers_cache\n",
    "\n",
    "        nll_values_early_pos = score_answer_with_cache(\n",
    "            deepcopy_cache(values_early_pos_cache), context_len,\n",
    "            query_prompt, answer_text, model, tokenizer, exp_config)\n",
    "        del values_early_pos_cache\n",
    "\n",
    "        nll_values_alpha_25 = score_answer_with_cache(\n",
    "            deepcopy_cache(values_alpha_25_cache), context_len,\n",
    "            query_prompt, answer_text, model, tokenizer, exp_config)\n",
    "        del values_alpha_25_cache\n",
    "\n",
    "        nll_rope_roundtrip = score_answer_with_cache(\n",
    "            deepcopy_cache(rope_roundtrip_cache), context_len,\n",
    "            query_prompt, answer_text, model, tokenizer, exp_config)\n",
    "        del rope_roundtrip_cache\n",
    "\n",
    "        # Score bare LAST (mutates cache)\n",
    "        nll_bare = score_answer_with_cache(\n",
    "            bare_cache, context_len, query_prompt, answer_text,\n",
    "            model, tokenizer, exp_config)\n",
    "        del bare_cache\n",
    "\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        passage_results.append({\n",
    "            'passage_idx': pinfo['passage_idx'],\n",
    "            'is_relevant': pinfo['is_relevant'],\n",
    "            'word_count': pinfo['word_count'],\n",
    "            'doc_len': doc_len,\n",
    "            'bare_nll': nll_bare,\n",
    "            'sf_trunc_nll': nll_sf_trunc,\n",
    "            'sf_trunc_fp32_nll': nll_sf_trunc_fp32,\n",
    "            'sf_trunc_nocorr_nll': nll_sf_trunc_nocorr,\n",
    "            'values_only_nll': nll_values_only,\n",
    "            'values_early_layers_nll': nll_values_early_layers,\n",
    "            'values_early_pos_nll': nll_values_early_pos,\n",
    "            'values_alpha_25_nll': nll_values_alpha_25,\n",
    "            'rope_roundtrip_nll': nll_rope_roundtrip,\n",
    "        })\n",
    "\n",
    "    all_results.append({\n",
    "        'query_idx': qidx,\n",
    "        'query': query,\n",
    "        'n_passages': len(passage_results),\n",
    "        'n_relevant': query_data['n_relevant'],\n",
    "        'passage_data': passage_results,\n",
    "    })\n",
    "\n",
    "    # Checkpoint\n",
    "    if (qidx + 1) % CHECKPOINT_EVERY == 0 or qidx == N - 1:\n",
    "        ckpt_data = {\n",
    "            'results': all_results,\n",
    "            'query_texts': [q['query'] for q in queries],\n",
    "            'completed': len(all_results),\n",
    "            'total': N,\n",
    "            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        }\n",
    "        with open(CHECKPOINT_PATH, 'w') as f:\n",
    "            json.dump(ckpt_data, f)\n",
    "        elapsed = time.time() - t_start\n",
    "        n_done = qidx - start_idx + 1\n",
    "        rate = n_done / elapsed if elapsed > 0 else 0\n",
    "        remaining = (N - qidx - 1) / rate if rate > 0 else 0\n",
    "        tqdm.write(f\"  Checkpoint {qidx+1}/{N} | {n_done} done in {elapsed/60:.1f}m | \"\n",
    "                   f\"ETA: {remaining/60:.1f} min\")\n",
    "\n",
    "elapsed_total = time.time() - t_start\n",
    "print(f\"\\nEvaluation complete: {len(all_results)} queries in {elapsed_total/60:.1f} min\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 7: Analysis \u2014 H1 verdict, H2 verdict, key interference decomposition\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ANALYSIS \u2014 GEMMA PRECISION FIX & SELECTIVE VALUE CONTAMINATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "N_VALID = len(all_results)\n",
    "print(f\"Valid queries: {N_VALID}\")\n",
    "\n",
    "# --- Collect per-passage NLLs ---\n",
    "cond_nlls = {cn: [] for cn in CONDITION_NAMES}\n",
    "for r in all_results:\n",
    "    for p in r['passage_data']:\n",
    "        for cn in CONDITION_NAMES:\n",
    "            cond_nlls[cn].append(p[f'{cn}_nll'])\n",
    "\n",
    "cond_arrays = {cn: np.array(vals) for cn, vals in cond_nlls.items()}\n",
    "\n",
    "# Filter zero NLLs\n",
    "valid = np.ones(len(cond_arrays['bare']), dtype=bool)\n",
    "for cn in CONDITION_NAMES:\n",
    "    valid &= (cond_arrays[cn] != 0)\n",
    "n_passages_valid = int(np.sum(valid))\n",
    "n_excluded = int(np.sum(~valid))\n",
    "print(f\"Total passages: {len(valid)}, Valid: {n_passages_valid}, Excluded: {n_excluded}\")\n",
    "\n",
    "c = {}\n",
    "for cn in CONDITION_NAMES:\n",
    "    c[cn] = cond_arrays[cn][valid]\n",
    "\n",
    "# === 1. NLL Summary Table ===\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"NLL SUMMARY (per-passage, Gemma 3 4B)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\n{'Condition':<25} {'Mean NLL':>10} {'Std':>10} {'d vs Bare':>10} {'Win%':>8}\")\n",
    "print(\"-\" * 68)\n",
    "\n",
    "gemma_ds = {}\n",
    "for cn in CONDITION_NAMES:\n",
    "    mean_nll = np.mean(c[cn])\n",
    "    std_nll = np.std(c[cn])\n",
    "    if cn == 'bare':\n",
    "        print(f\"{cn:<25} {mean_nll:>10.4f} {std_nll:>10.4f} {'---':>10} {'---':>8}\")\n",
    "    else:\n",
    "        delta = c['bare'] - c[cn]\n",
    "        d = cohens_d(delta)\n",
    "        win = np.mean(delta > 0) * 100\n",
    "        gemma_ds[cn] = d\n",
    "        print(f\"{cn:<25} {mean_nll:>10.4f} {std_nll:>10.4f} {d:>+10.3f} {win:>7.1f}%\")\n",
    "\n",
    "# === 2. Statistical Tests (7 primary comparisons) ===\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(f\"7 PRIMARY COMPARISONS (Bonferroni alpha = {BONFERRONI_ALPHA:.4f})\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "comparisons = [\n",
    "    ('C1: fp32 vs bf16',\n",
    "     c['sf_trunc'] - c['sf_trunc_fp32'],\n",
    "     'Does fp32 precision help?'),\n",
    "    ('C2: nocorr vs bare',\n",
    "     c['bare'] - c['sf_trunc_nocorr'],\n",
    "     'Does uncorrected truncation hurt?'),\n",
    "    ('C3: nocorr vs bf16',\n",
    "     c['sf_trunc'] - c['sf_trunc_nocorr'],\n",
    "     'Is correction better than no correction?'),\n",
    "    ('C4: values_only vs bare',\n",
    "     c['bare'] - c['values_only'],\n",
    "     'Replicate Exp 16 d=+0.056'),\n",
    "    ('C5: early_layers vs values_only',\n",
    "     c['values_only'] - c['values_early_layers'],\n",
    "     'Does layer selectivity help?'),\n",
    "    ('C6: early_pos vs values_only',\n",
    "     c['values_only'] - c['values_early_pos'],\n",
    "     'Does position selectivity help?'),\n",
    "    ('C7: alpha_25 vs values_only',\n",
    "     c['values_only'] - c['values_alpha_25'],\n",
    "     'Does reduced dose help?'),\n",
    "]\n",
    "\n",
    "print(f\"\\n{'Comparison':<35} {'Mean D':>8} {'d':>8} {'Win%':>7} {'t':>8} {'p':>12} {'Sig':>5}\")\n",
    "print(\"-\" * 90)\n",
    "\n",
    "comparison_results = {}\n",
    "for name, delta, question in comparisons:\n",
    "    d = cohens_d(delta)\n",
    "    win = np.mean(delta > 0) * 100\n",
    "    t_stat, p_val = stats.ttest_1samp(delta, 0)\n",
    "    sig = \"***\" if p_val < 0.001 else \"**\" if p_val < BONFERRONI_ALPHA else \"*\" if p_val < 0.05 else \"ns\"\n",
    "    bonf_sig = p_val < BONFERRONI_ALPHA\n",
    "    print(f\"{name:<35} {np.mean(delta):>8.4f} {d:>+8.3f} {win:>6.1f}% {t_stat:>8.2f} {p_val:>11.2e} {sig:>5}\")\n",
    "    comparison_results[name] = {\n",
    "        'mean_delta': float(np.mean(delta)),\n",
    "        'cohens_d': float(d),\n",
    "        'win_rate': float(win / 100),\n",
    "        't_stat': float(t_stat),\n",
    "        'p_value': float(p_val),\n",
    "        'bonferroni_significant': bool(bonf_sig),\n",
    "        'question': question,\n",
    "    }\n",
    "\n",
    "# === 3. Key Derived Metrics ===\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"KEY DERIVED METRICS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "d_sf = gemma_ds.get('sf_trunc', 0)\n",
    "d_fp32 = gemma_ds.get('sf_trunc_fp32', 0)\n",
    "d_nocorr = gemma_ds.get('sf_trunc_nocorr', 0)\n",
    "d_vo = gemma_ds.get('values_only', 0)\n",
    "d_el = gemma_ds.get('values_early_layers', 0)\n",
    "d_ep = gemma_ds.get('values_early_pos', 0)\n",
    "d_a25 = gemma_ds.get('values_alpha_25', 0)\n",
    "d_rt = gemma_ds.get('rope_roundtrip', 0)\n",
    "\n",
    "precision_gain = d_fp32 - d_sf\n",
    "key_interference_bf16 = d_vo - d_sf\n",
    "key_interference_fp32 = d_vo - d_fp32\n",
    "noise_baseline = d_rt\n",
    "best_selective = max(d_el, d_ep, d_a25)\n",
    "best_selective_name = ['values_early_layers', 'values_early_pos', 'values_alpha_25'][\n",
    "    [d_el, d_ep, d_a25].index(best_selective)\n",
    "]\n",
    "\n",
    "print(f\"\\n  Precision gain (fp32 - bf16):    {precision_gain:+.3f}\")\n",
    "print(f\"    sf_trunc (bf16): d = {d_sf:+.3f}\")\n",
    "print(f\"    sf_trunc_fp32:   d = {d_fp32:+.3f}\")\n",
    "print(f\"\")\n",
    "print(f\"  Key interference (bf16):          {key_interference_bf16:+.3f}\")\n",
    "print(f\"    = d(values_only) - d(sf_trunc)\")\n",
    "print(f\"    = {d_vo:+.3f} - ({d_sf:+.3f})\")\n",
    "print(f\"\")\n",
    "print(f\"  Key interference (fp32):          {key_interference_fp32:+.3f}\")\n",
    "print(f\"    = d(values_only) - d(sf_trunc_fp32)\")\n",
    "print(f\"    = {d_vo:+.3f} - ({d_fp32:+.3f})\")\n",
    "print(f\"    (If smaller than bf16 interference, precision matters)\")\n",
    "print(f\"\")\n",
    "print(f\"  Noise baseline (rope_roundtrip):  {noise_baseline:+.3f}\")\n",
    "print(f\"    (Pure bfloat16 roundtrip noise cost, no content)\")\n",
    "print(f\"\")\n",
    "print(f\"  Best selective condition:          {best_selective_name} d={best_selective:+.3f}\")\n",
    "\n",
    "# === 4. H1 VERDICT ===\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"H1 VERDICT: PRECISION HYPOTHESIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "c1_result = comparison_results.get('C1: fp32 vs bf16', {})\n",
    "c1_sig = c1_result.get('bonferroni_significant', False)\n",
    "c1_d = c1_result.get('cohens_d', 0)\n",
    "\n",
    "if c1_sig and precision_gain > 0.05:\n",
    "    h1_verdict = \"SUPPORTED\"\n",
    "    h1_msg = f\"fp32 correction RECOVERS priming on Gemma (gain = {precision_gain:+.3f})\"\n",
    "elif precision_gain > 0 and not c1_sig:\n",
    "    h1_verdict = \"TREND\"\n",
    "    h1_msg = f\"fp32 shows trend toward recovery ({precision_gain:+.3f}) but not significant\"\n",
    "else:\n",
    "    h1_verdict = \"REJECTED\"\n",
    "    h1_msg = f\"fp32 correction DOES NOT RECOVER priming on Gemma (gain = {precision_gain:+.3f})\"\n",
    "\n",
    "print(f\"\\n  >>> H1 {h1_verdict}: {h1_msg}\")\n",
    "print(f\"\")\n",
    "print(f\"  Details:\")\n",
    "print(f\"    sf_trunc (bf16):     d = {d_sf:+.3f}\")\n",
    "print(f\"    sf_trunc_fp32:       d = {d_fp32:+.3f}\")\n",
    "print(f\"    precision gain:      {precision_gain:+.3f}\")\n",
    "print(f\"    C1 test p-value:     {c1_result.get('p_value', 1):.2e}\")\n",
    "print(f\"    noise baseline:      d = {d_rt:+.3f}\")\n",
    "print(f\"\")\n",
    "print(f\"  Cross-reference:\")\n",
    "print(f\"    Mistral static_fact: d = {MISTRAL_REF['static_fact_trunc_d']:+.3f}\")\n",
    "print(f\"    Gemma sf_trunc_fp32: d = {d_fp32:+.3f}\")\n",
    "print(f\"    Recovery fraction:   {d_fp32 / MISTRAL_REF['static_fact_trunc_d'] * 100:.1f}%\" if MISTRAL_REF['static_fact_trunc_d'] != 0 else \"\")\n",
    "\n",
    "# === 5. H2 VERDICT ===\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"H2 VERDICT: SELECTIVE CONTAMINATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "any_selective_better = (d_el > d_vo + 0.01) or (d_ep > d_vo + 0.01) or (d_a25 > d_vo + 0.01)\n",
    "c5_sig = comparison_results.get('C5: early_layers vs values_only', {}).get('bonferroni_significant', False)\n",
    "c6_sig = comparison_results.get('C6: early_pos vs values_only', {}).get('bonferroni_significant', False)\n",
    "c7_sig = comparison_results.get('C7: alpha_25 vs values_only', {}).get('bonferroni_significant', False)\n",
    "\n",
    "if any_selective_better and (c5_sig or c6_sig or c7_sig):\n",
    "    h2_verdict = \"SUPPORTED\"\n",
    "    h2_msg = \"Selective contamination AMPLIFIES the value signal\"\n",
    "elif any_selective_better:\n",
    "    h2_verdict = \"TREND\"\n",
    "    h2_msg = f\"Selective conditions show trends but not significant\"\n",
    "else:\n",
    "    h2_verdict = \"REJECTED\"\n",
    "    h2_msg = \"Selective contamination DOES NOT AMPLIFY the value signal\"\n",
    "\n",
    "print(f\"\\n  >>> H2 {h2_verdict}: {h2_msg}\")\n",
    "print(f\"\")\n",
    "print(f\"  Details:\")\n",
    "print(f\"    values_only (all):    d = {d_vo:+.3f}\")\n",
    "print(f\"    values_early_layers:  d = {d_el:+.3f} ({'*' if c5_sig else 'ns'})\")\n",
    "print(f\"    values_early_pos:     d = {d_ep:+.3f} ({'*' if c6_sig else 'ns'})\")\n",
    "print(f\"    values_alpha_25:      d = {d_a25:+.3f} ({'*' if c7_sig else 'ns'})\")\n",
    "print(f\"\")\n",
    "print(f\"  Exp 09 Mistral comparison:\")\n",
    "print(f\"    Mistral layers 0-15:  ~88% of full signal\")\n",
    "print(f\"    Gemma layers 0-16:    {d_el/d_vo*100:.0f}% of values_only\" if d_vo != 0 else \"    Gemma: d_vo = 0\")\n",
    "print(f\"    Mistral alpha=0.25:   ~86% of full signal\")\n",
    "print(f\"    Gemma alpha=0.25:     {d_a25/d_vo*100:.0f}% of values_only\" if d_vo != 0 else \"    Gemma: d_vo = 0\")\n",
    "\n",
    "# === 6. Key Interference Decomposition ===\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"KEY INTERFERENCE DECOMPOSITION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\n  bf16 pipeline:  values d={d_vo:+.3f} + keys d={d_sf - d_vo:+.3f} = total d={d_sf:+.3f}\")\n",
    "print(f\"  fp32 pipeline:  values d={d_vo:+.3f} + keys d={d_fp32 - d_vo:+.3f} = total d={d_fp32:+.3f}\")\n",
    "print(f\"\")\n",
    "print(f\"  Key interference reduced by fp32: {key_interference_bf16 - key_interference_fp32:+.3f}\")\n",
    "print(f\"    bf16 interference: {-key_interference_bf16:+.3f}\")\n",
    "print(f\"    fp32 interference: {-key_interference_fp32:+.3f}\")\n",
    "\n",
    "# === 7. Hardness Interaction ===\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"HARDNESS INTERACTION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "bare_all = c['bare']\n",
    "quintile_boundaries = np.percentile(bare_all, [20, 40, 60, 80])\n",
    "quintile_labels = ['Q1 (easy)', 'Q2', 'Q3', 'Q4', 'Q5 (hard)']\n",
    "\n",
    "def get_quintile(nll, boundaries):\n",
    "    for i, b in enumerate(boundaries):\n",
    "        if nll <= b:\n",
    "            return i\n",
    "    return len(boundaries)\n",
    "\n",
    "quintiles = np.array([get_quintile(nll, quintile_boundaries) for nll in bare_all])\n",
    "\n",
    "hardness_conds = ['sf_trunc', 'sf_trunc_fp32', 'values_only',\n",
    "                  'values_early_layers', 'values_alpha_25']\n",
    "\n",
    "header = f\"{'Condition':<25}\" + \"\".join(f\"{ql:>14}\" for ql in quintile_labels) + f\"{'Overall':>14}\"\n",
    "print(f\"\\n{header}\")\n",
    "print(\"-\" * (25 + 14 * 6))\n",
    "\n",
    "hardness_breakdown = {}\n",
    "for cn in hardness_conds:\n",
    "    row = f\"{cn:<25}\"\n",
    "    quintile_ds = []\n",
    "    for q in range(5):\n",
    "        mask_q = quintiles == q\n",
    "        if np.sum(mask_q) < 10:\n",
    "            row += f\"{'n/a':>14}\"\n",
    "            quintile_ds.append(None)\n",
    "        else:\n",
    "            delta = bare_all[mask_q] - c[cn][mask_q]\n",
    "            d_q = cohens_d(delta)\n",
    "            row += f\"{d_q:>+14.3f}\"\n",
    "            quintile_ds.append(float(d_q))\n",
    "    d_all = cohens_d(bare_all - c[cn])\n",
    "    row += f\"{d_all:>+14.3f}\"\n",
    "    print(row)\n",
    "    hardness_breakdown[cn] = {'quintile_ds': quintile_ds, 'overall_d': float(d_all)}\n",
    "\n",
    "# Hardness correlation\n",
    "print(\"\\nHardness correlation (bare NLL vs delta):\")\n",
    "for cn in ['sf_trunc', 'sf_trunc_fp32', 'values_only']:\n",
    "    delta = bare_all - c[cn]\n",
    "    r, p = stats.pearsonr(bare_all, delta)\n",
    "    sig = \"***\" if p < 0.001 else \"**\" if p < 0.01 else \"*\" if p < 0.05 else \"ns\"\n",
    "    print(f\"  {cn:<25} r={r:+.3f}  p={p:.2e}  {sig}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 8: Plots \u2014 4-panel figure\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Color scheme\n",
    "colors_h1 = {\n",
    "    'sf_trunc': '#d62728',\n",
    "    'sf_trunc_fp32': '#2ca02c',\n",
    "    'sf_trunc_nocorr': '#ff7f0e',\n",
    "    'rope_roundtrip': '#7f7f7f',\n",
    "}\n",
    "colors_h2 = {\n",
    "    'values_only': '#1f77b4',\n",
    "    'values_early_layers': '#9467bd',\n",
    "    'values_early_pos': '#8c564b',\n",
    "    'values_alpha_25': '#e377c2',\n",
    "}\n",
    "\n",
    "# --- Panel 1 (top-left): H1 \u2014 RoPE Precision ---\n",
    "ax = axes[0, 0]\n",
    "h1_conds = ['sf_trunc', 'sf_trunc_fp32', 'sf_trunc_nocorr', 'rope_roundtrip']\n",
    "h1_ds = [gemma_ds.get(cn, 0) for cn in h1_conds]\n",
    "h1_colors = [colors_h1[cn] for cn in h1_conds]\n",
    "h1_labels = ['sf_trunc\\n(bf16)', 'sf_trunc\\n(fp32)', 'sf_trunc\\n(no corr)', 'rope\\nroundtrip']\n",
    "\n",
    "bars = ax.bar(range(len(h1_conds)), h1_ds, color=h1_colors, edgecolor='black', linewidth=0.5)\n",
    "# values_only reference line\n",
    "ax.axhline(y=d_vo, color='#1f77b4', linestyle='--', linewidth=1.5,\n",
    "           label=f'values_only (d={d_vo:+.3f})')\n",
    "ax.axhline(y=0, color='gray', linestyle='-', alpha=0.3)\n",
    "ax.set_xticks(range(len(h1_conds)))\n",
    "ax.set_xticklabels(h1_labels, fontsize=8)\n",
    "ax.set_ylabel(\"Cohen's d vs Bare\")\n",
    "ax.set_title(\"H1: RoPE Precision\")\n",
    "ax.legend(fontsize=8)\n",
    "for i, v in enumerate(h1_ds):\n",
    "    ax.text(i, v + 0.003 if v >= 0 else v - 0.012, f\"{v:+.3f}\",\n",
    "            ha='center', va='bottom' if v >= 0 else 'top', fontsize=9)\n",
    "\n",
    "# --- Panel 2 (top-right): H2 \u2014 Selective Contamination ---\n",
    "ax = axes[0, 1]\n",
    "h2_conds = ['values_only', 'values_early_layers', 'values_early_pos', 'values_alpha_25']\n",
    "h2_ds = [gemma_ds.get(cn, 0) for cn in h2_conds]\n",
    "h2_colors = [colors_h2[cn] for cn in h2_conds]\n",
    "h2_labels = ['values\\nonly (all)', 'early\\nlayers', 'early\\npositions', 'alpha\\n0.25']\n",
    "\n",
    "bars = ax.bar(range(len(h2_conds)), h2_ds, color=h2_colors, edgecolor='black', linewidth=0.5)\n",
    "ax.axhline(y=0, color='gray', linestyle='-', alpha=0.3)\n",
    "ax.set_xticks(range(len(h2_conds)))\n",
    "ax.set_xticklabels(h2_labels, fontsize=8)\n",
    "ax.set_ylabel(\"Cohen's d vs Bare\")\n",
    "ax.set_title(\"H2: Selective Contamination\")\n",
    "for i, v in enumerate(h2_ds):\n",
    "    ax.text(i, v + 0.003 if v >= 0 else v - 0.012, f\"{v:+.3f}\",\n",
    "            ha='center', va='bottom' if v >= 0 else 'top', fontsize=9)\n",
    "\n",
    "# --- Panel 3 (bottom-left): Key Interference Decomposition (waterfall) ---\n",
    "ax = axes[1, 0]\n",
    "\n",
    "# Waterfall: show values contribution + key interference for bf16 and fp32\n",
    "categories = ['Values\\n(shared)', 'Keys\\n(bf16)', 'Total\\n(bf16)',\n",
    "              'Values\\n(shared)', 'Keys\\n(fp32)', 'Total\\n(fp32)']\n",
    "values_contrib = d_vo\n",
    "keys_bf16 = d_sf - d_vo\n",
    "total_bf16 = d_sf\n",
    "keys_fp32 = d_fp32 - d_vo\n",
    "total_fp32 = d_fp32\n",
    "\n",
    "# Draw as grouped bars\n",
    "x_pos = [0, 1, 2, 3.5, 4.5, 5.5]\n",
    "bar_vals = [values_contrib, keys_bf16, total_bf16, values_contrib, keys_fp32, total_fp32]\n",
    "bar_colors = ['#1f77b4', '#d62728', '#2c2c2c', '#1f77b4', '#2ca02c', '#2c2c2c']\n",
    "\n",
    "bars = ax.bar(x_pos, bar_vals, color=bar_colors, edgecolor='black', linewidth=0.5, width=0.8)\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels(categories, fontsize=7)\n",
    "ax.axhline(y=0, color='gray', linestyle='-', alpha=0.5)\n",
    "ax.set_ylabel(\"Cohen's d component\")\n",
    "ax.set_title(\"Key Interference Decomposition\")\n",
    "\n",
    "# Add labels\n",
    "for i, (xp, v) in enumerate(zip(x_pos, bar_vals)):\n",
    "    ax.text(xp, v + 0.003 if v >= 0 else v - 0.012, f\"{v:+.3f}\",\n",
    "            ha='center', va='bottom' if v >= 0 else 'top', fontsize=8)\n",
    "\n",
    "# Add bf16/fp32 section labels\n",
    "ax.text(1, ax.get_ylim()[1] * 0.9, 'bfloat16', ha='center', fontsize=10, fontstyle='italic')\n",
    "ax.text(4.5, ax.get_ylim()[1] * 0.9, 'float32', ha='center', fontsize=10, fontstyle='italic')\n",
    "\n",
    "# --- Panel 4 (bottom-right): Hardness Interaction scatter ---\n",
    "ax = axes[1, 1]\n",
    "\n",
    "# Scatter for sf_trunc_fp32 and values_only\n",
    "delta_fp32 = c['bare'] - c['sf_trunc_fp32']\n",
    "delta_vo = c['bare'] - c['values_only']\n",
    "\n",
    "ax.scatter(c['bare'], delta_fp32, alpha=0.1, s=6, color='#2ca02c', label='sf_trunc_fp32')\n",
    "ax.scatter(c['bare'], delta_vo, alpha=0.1, s=6, color='#1f77b4', label='values_only')\n",
    "\n",
    "# Fit lines\n",
    "z_fp32 = np.polyfit(c['bare'], delta_fp32, 1)\n",
    "z_vo = np.polyfit(c['bare'], delta_vo, 1)\n",
    "x_fit = np.linspace(c['bare'].min(), c['bare'].max(), 100)\n",
    "ax.plot(x_fit, np.polyval(z_fp32, x_fit), color='#2ca02c', linewidth=2, linestyle='--')\n",
    "ax.plot(x_fit, np.polyval(z_vo, x_fit), color='#1f77b4', linewidth=2, linestyle='--')\n",
    "\n",
    "r_fp32, p_fp32 = stats.pearsonr(c['bare'], delta_fp32)\n",
    "r_vo, p_vo = stats.pearsonr(c['bare'], delta_vo)\n",
    "\n",
    "ax.set_xlabel(\"Bare NLL (difficulty)\")\n",
    "ax.set_ylabel(\"\u0394NLL (bare - condition)\")\n",
    "ax.set_title(f\"Hardness: fp32 r={r_fp32:+.3f}, values r={r_vo:+.3f}\")\n",
    "ax.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "ax.legend(fontsize=8, markerscale=3)\n",
    "\n",
    "plt.suptitle('Exp 19: Gemma Precision Fix & Selective Value Contamination', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'analysis_plots.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"Plots saved to {RESULTS_DIR / 'analysis_plots.png'}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 9: Save results JSON\n",
    "final = {\n",
    "    'experiment': 'exp19_gemma_precision_and_selectivity',\n",
    "    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'config': {\n",
    "        'model_name': MODEL_NAME,\n",
    "        'model_type': 'gemma3',\n",
    "        'seed': SEED,\n",
    "        'n_queries': N,\n",
    "        'n_valid': N_VALID,\n",
    "        'n_passages_valid': n_passages_valid,\n",
    "        'n_passages_excluded': n_excluded,\n",
    "        'max_passage_words': MAX_PASSAGE_WORDS,\n",
    "        'min_passages_per_query': MIN_PASSAGES_PER_QUERY,\n",
    "        'dataset': 'MS MARCO v1.1 validation',\n",
    "        'n_comparisons': N_COMPARISONS,\n",
    "        'bonferroni_alpha': BONFERRONI_ALPHA,\n",
    "    },\n",
    "    'gemma_architecture': {\n",
    "        'hidden_size': text_config.hidden_size,\n",
    "        'num_layers': text_config.num_hidden_layers,\n",
    "        'num_attention_heads': text_config.num_attention_heads,\n",
    "        'num_kv_heads': text_config.num_key_value_heads,\n",
    "        'head_dim': _get_head_dim(model.config),\n",
    "        'rope_thetas': sorted(list(thetas)),\n",
    "    },\n",
    "    'condition_names': CONDITION_NAMES,\n",
    "    'nll_summary': {\n",
    "        cn: {\n",
    "            'mean': float(np.mean(c[cn])),\n",
    "            'std': float(np.std(c[cn])),\n",
    "            'cohens_d_vs_bare': float(cohens_d(c['bare'] - c[cn])) if cn != 'bare' else 0.0,\n",
    "        }\n",
    "        for cn in CONDITION_NAMES\n",
    "    },\n",
    "    'primary_comparisons': comparison_results,\n",
    "    'derived_metrics': {\n",
    "        'precision_gain': float(precision_gain),\n",
    "        'key_interference_bf16': float(key_interference_bf16),\n",
    "        'key_interference_fp32': float(key_interference_fp32),\n",
    "        'noise_baseline': float(noise_baseline),\n",
    "        'best_selective_condition': best_selective_name,\n",
    "        'best_selective_d': float(best_selective),\n",
    "    },\n",
    "    'verdicts': {\n",
    "        'h1_precision': h1_verdict,\n",
    "        'h1_message': h1_msg,\n",
    "        'h2_selectivity': h2_verdict,\n",
    "        'h2_message': h2_msg,\n",
    "    },\n",
    "    'reference_values': {\n",
    "        'exp16_gemma': EXP16_REF,\n",
    "        'exp09_mistral_selectivity': {k: str(v) for k, v in EXP09_REF.items()},\n",
    "        'mistral': MISTRAL_REF,\n",
    "    },\n",
    "    'hardness_breakdown': hardness_breakdown,\n",
    "    'per_query_results': all_results,\n",
    "}\n",
    "\n",
    "with open(FINAL_RESULTS_PATH, 'w') as f:\n",
    "    json.dump(final, f, indent=2)\n",
    "\n",
    "print(f\"Results saved to {FINAL_RESULTS_PATH}\")\n",
    "print(f\"File size: {FINAL_RESULTS_PATH.stat().st_size / 1024:.1f} KB\")\n",
    "\n",
    "# Print final summary\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"SUMMARY \u2014 Exp 19: Gemma Precision & Selectivity\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Model: Gemma 3 4B (34 layers, head_dim=256, bfloat16)\")\n",
    "print(f\"Dataset: MS MARCO v1.1 ({N} queries, {n_passages_valid} passages)\")\n",
    "print(f\"\\nEffect sizes (Cohen's d vs bare):\")\n",
    "for cn in CONDITION_NAMES[1:]:\n",
    "    d_val = gemma_ds.get(cn, 0)\n",
    "    print(f\"  {cn:<25} d={d_val:>+.3f}\")\n",
    "print(f\"\\nH1 (Precision):    {h1_verdict} \u2014 {h1_msg}\")\n",
    "print(f\"H2 (Selectivity):  {h2_verdict} \u2014 {h2_msg}\")\n",
    "print(f\"\\nDone!\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 10: GPU cleanup\n",
    "import gc\n",
    "\n",
    "print(\"Cleaning up GPU memory...\")\n",
    "mem_before = torch.cuda.memory_allocated() / 1e9\n",
    "\n",
    "del model\n",
    "del tokenizer\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "mem_after = torch.cuda.memory_allocated() / 1e9\n",
    "print(f\"GPU memory: {mem_before:.2f} GB -> {mem_after:.2f} GB\")\n",
    "print(\"Cleanup complete.\")"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}