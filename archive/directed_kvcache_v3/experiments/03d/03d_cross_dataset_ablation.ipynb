{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a03ee256",
   "metadata": {},
   "source": [
    "# Experiment 3D: Cross-Dataset Content Ablation (Long Queries)\n",
    "\n",
    "## Motivation\n",
    "\n",
    "Exp 2B on MS MARCO found that **85% of the oracle headroom is pure structure** —\n",
    "any prefix triggers a mode shift in the encoder. But MS MARCO queries are only\n",
    "~6 words long. For such short queries:\n",
    "- **Scrambled oracle** preserves bag-of-words, which IS most of the semantics\n",
    "- **Vocabulary test** (random → scrambled) is weak because 6 random words cover\n",
    "  a lot of vocabulary space relative to a 6-word query\n",
    "- **Word order** barely matters in 6 words\n",
    "\n",
    "This experiment tests the same decomposition on a dataset with **3x longer queries**\n",
    "(~18 words) and **5x longer documents** (~600 words). If the split changes, the\n",
    "MS MARCO finding was an artifact of short queries. If it holds, the structural\n",
    "mechanism is genuine.\n",
    "\n",
    "## Three-Point Framework\n",
    "\n",
    "| Label | Encoder input | Cost | Role |\n",
    "|-------|--------------|------|------|\n",
    "| **Upper bound** | [real_query + document], mask query from decoder | O(Q×D) | Ideal but too expensive |\n",
    "| **Lower bound** | [document] only | O(1) | Current worst case |\n",
    "| **Middle ground** | [surrogate + document], mask surrogate from decoder | O(1) | Our hypothesis |\n",
    "\n",
    "## Dataset: neural-bridge/rag-dataset-12000\n",
    "\n",
    "- Synthetic QA pairs with retrieved context passages\n",
    "- Filtered to: query ≥ 15 words, answer ≥ 5 words → ~3,384 samples\n",
    "- Mean query: 17.7 words (3x MS MARCO), document: 603 words, answer: 42.7 words\n",
    "- Ceiling pre-screen: PASS (mean bare NLL = 0.98, strong oracle headroom +0.255)\n",
    "\n",
    "## Design\n",
    "\n",
    "**Conditions** (all with truncation):\n",
    "1. `bare` — document only (lower bound)\n",
    "2. `oracle_trunc` — real query + document (upper bound)\n",
    "3. `random_matched_trunc` — N random words (N = query word count)\n",
    "4. `scrambled_oracle_trunc` — query words in random order\n",
    "5. `surr_template_trunc` — \"What is [top_keyword]?\"\n",
    "6. `repeat_the_trunc` — \"the\" repeated N times (N = query word count)\n",
    "7. `repeat_kw_trunc` — top doc keyword repeated N times\n",
    "\n",
    "**Content ablation decomposition**:\n",
    "- Structure = bare → random_matched\n",
    "- Vocabulary = random_matched → scrambled_oracle\n",
    "- Semantics = scrambled_oracle → oracle\n",
    "\n",
    "**Key prediction**: With 18-word queries, scrambling should lose more information\n",
    "than with 6-word queries. We expect the semantic component to grow from 10% to\n",
    "potentially 20-30%+."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e6c1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Setup\n",
    "import os\n",
    "os.umask(0o000)\n",
    "\n",
    "import sys, json, time, re, gc, random as pyrandom\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from scipy import stats\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "sys.path.insert(0, \"../..\")\n",
    "from lib.analysis import cohens_d\n",
    "\n",
    "SEED = 42\n",
    "N_SAMPLES = 500\n",
    "MODEL_NAME = \"google/t5gemma-2-4b-4b\"\n",
    "\n",
    "RESULTS_DIR = Path(\"../../results/exp03d\")\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CHECKPOINT_PATH = RESULTS_DIR / \"checkpoint.json\"\n",
    "\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "pyrandom.seed(SEED)\n",
    "\n",
    "print(\"Exp 3D: Cross-Dataset Content Ablation (Long Queries)\")\n",
    "print(f\"Dataset: neural-bridge/rag-dataset-12000\")\n",
    "print(f\"N: {N_SAMPLES}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239b8234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Load neural-bridge/rag-dataset-12000 and prepare samples\n",
    "from datasets import load_dataset\n",
    "\n",
    "print(\"Loading dataset...\")\n",
    "ds = load_dataset(\"neural-bridge/rag-dataset-12000\", split=\"train\")\n",
    "print(f\"Total samples: {len(ds)}\")\n",
    "\n",
    "# Filter to long queries with real answers\n",
    "all_candidates = []\n",
    "for row in ds:\n",
    "    q = row.get(\"question\", \"\")\n",
    "    doc = row.get(\"context\", \"\")\n",
    "    answer = row.get(\"answer\", \"\")\n",
    "    if not q or not doc or not answer:\n",
    "        continue\n",
    "    q_words = len(q.split())\n",
    "    a_words = len(answer.split())\n",
    "    if q_words >= 15 and a_words >= 5:\n",
    "        all_candidates.append({\n",
    "            \"query\": q,\n",
    "            \"document\": doc,\n",
    "            \"answer\": answer,\n",
    "            \"query_words\": q_words,\n",
    "            \"doc_words\": len(doc.split()),\n",
    "            \"answer_words\": a_words,\n",
    "        })\n",
    "\n",
    "print(f\"Candidates (q>=15w, a>=5w): {len(all_candidates)}\")\n",
    "\n",
    "# Shuffle and take N_SAMPLES\n",
    "np.random.seed(SEED)\n",
    "indices = np.random.permutation(len(all_candidates))\n",
    "samples = [all_candidates[i] for i in indices[:N_SAMPLES]]\n",
    "\n",
    "# Dataset statistics\n",
    "q_lens = np.array([s[\"query_words\"] for s in samples])\n",
    "d_lens = np.array([s[\"doc_words\"] for s in samples])\n",
    "a_lens = np.array([s[\"answer_words\"] for s in samples])\n",
    "\n",
    "print(f\"\\nSample statistics (N={N_SAMPLES}):\")\n",
    "print(f\"  Query length:  mean={q_lens.mean():.1f}, median={np.median(q_lens):.0f}, \"\n",
    "      f\"range=[{q_lens.min()}, {q_lens.max()}]\")\n",
    "print(f\"  Doc length:    mean={d_lens.mean():.1f}, median={np.median(d_lens):.0f}, \"\n",
    "      f\"range=[{d_lens.min()}, {d_lens.max()}]\")\n",
    "print(f\"  Answer length: mean={a_lens.mean():.1f}, median={np.median(a_lens):.0f}, \"\n",
    "      f\"range=[{a_lens.min()}, {a_lens.max()}]\")\n",
    "\n",
    "# Show 3 examples\n",
    "for i in range(3):\n",
    "    s = samples[i]\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"  Q ({s['query_words']}w): {s['query'][:120]}...\")\n",
    "    print(f\"  D ({s['doc_words']}w): {s['document'][:100]}...\")\n",
    "    print(f\"  A ({s['answer_words']}w): {s['answer'][:100]}...\")\n",
    "\n",
    "# Compare with MS MARCO distribution\n",
    "print(f\"\\n--- Comparison with MS MARCO ---\")\n",
    "print(f\"  MS MARCO: mean query = 6.0w, mean doc = ~60w, mean answer = ~20w\")\n",
    "print(f\"  This dataset: mean query = {q_lens.mean():.1f}w, \"\n",
    "      f\"mean doc = {d_lens.mean():.0f}w, mean answer = {a_lens.mean():.0f}w\")\n",
    "print(f\"  Query ratio: {q_lens.mean()/6.0:.1f}x longer\")\n",
    "print(f\"  Doc ratio: {d_lens.mean()/60:.1f}x longer\")\n",
    "\n",
    "del ds\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfeab356",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Load model and define scoring helpers\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv())\n",
    "HF_TOKEN = os.environ.get(\"HF_TOKEN\")\n",
    "\n",
    "from transformers import AutoProcessor, AutoModelForSeq2SeqLM\n",
    "\n",
    "print(f\"Loading {MODEL_NAME}...\")\n",
    "processor = AutoProcessor.from_pretrained(MODEL_NAME, token=HF_TOKEN)\n",
    "tokenizer = processor.tokenizer\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    MODEL_NAME, device_map=\"auto\", torch_dtype=torch.bfloat16, token=HF_TOKEN,\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "DEVICE = next(model.parameters()).device\n",
    "print(f\"Model loaded. dtype={next(model.parameters()).dtype}\")\n",
    "print(f\"GPU memory: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "\n",
    "\n",
    "def score_nll(encoder_text, answer_text, prefix_token_count=0, truncate=False):\n",
    "    # Score NLL of answer given encoder text, with optional prefix truncation.\n",
    "    enc_ids = tokenizer(encoder_text, return_tensors=\"pt\",\n",
    "                        add_special_tokens=True, truncation=True,\n",
    "                        max_length=2048).input_ids.to(DEVICE)\n",
    "    total_enc_len = enc_ids.shape[1]\n",
    "    enc_mask = torch.ones(1, total_enc_len, device=DEVICE, dtype=torch.long)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        encoder_outputs = model.get_encoder()(\n",
    "            input_ids=enc_ids, attention_mask=enc_mask\n",
    "        )\n",
    "\n",
    "    if truncate and prefix_token_count > 0:\n",
    "        cross_attn_mask = torch.ones(1, total_enc_len, device=DEVICE, dtype=torch.long)\n",
    "        cross_attn_mask[:, :prefix_token_count] = 0\n",
    "    else:\n",
    "        cross_attn_mask = torch.ones(1, total_enc_len, device=DEVICE, dtype=torch.long)\n",
    "\n",
    "    ans_ids = tokenizer(answer_text, return_tensors=\"pt\",\n",
    "                        add_special_tokens=False, truncation=True,\n",
    "                        max_length=256).input_ids.to(DEVICE)\n",
    "    if ans_ids.shape[1] == 0:\n",
    "        return 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            encoder_outputs=encoder_outputs,\n",
    "            attention_mask=cross_attn_mask,\n",
    "            labels=ans_ids,\n",
    "        )\n",
    "\n",
    "    logits = outputs.logits\n",
    "    log_probs = F.log_softmax(logits, dim=-1)\n",
    "    token_log_probs = log_probs[0].gather(1, ans_ids[0].unsqueeze(1)).squeeze(1)\n",
    "    mean_nll = -token_log_probs.mean().item()\n",
    "\n",
    "    del encoder_outputs, outputs, logits, log_probs\n",
    "    return mean_nll\n",
    "\n",
    "\n",
    "def count_prefix_tokens(prefix_text, document_text):\n",
    "    # Count how many tokens the prefix occupies in [prefix + newline + document].\n",
    "    full_text = prefix_text + \"\\n\" + document_text\n",
    "    full_ids = tokenizer(full_text, add_special_tokens=True, truncation=True,\n",
    "                         max_length=2048).input_ids\n",
    "    doc_ids = tokenizer(document_text, add_special_tokens=True, truncation=True,\n",
    "                        max_length=2048).input_ids\n",
    "    return len(full_ids) - len(doc_ids)\n",
    "\n",
    "\n",
    "STOP_WORDS = {\n",
    "    'a', 'an', 'the', 'is', 'are', 'was', 'were', 'be', 'been', 'being',\n",
    "    'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could',\n",
    "    'should', 'may', 'might', 'can', 'shall', 'to', 'of', 'in', 'for',\n",
    "    'on', 'with', 'at', 'by', 'from', 'as', 'into', 'through', 'during',\n",
    "    'before', 'after', 'above', 'below', 'between', 'and', 'but', 'or',\n",
    "    'not', 'no', 'if', 'then', 'than', 'so', 'up', 'out', 'about',\n",
    "    'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those',\n",
    "    'it', 'its', 'i', 'me', 'my', 'we', 'our', 'you', 'your', 'he',\n",
    "    'him', 'his', 'she', 'her', 'they', 'them', 'their', 'how', 'when',\n",
    "    'where', 'why', 'much', 'many', 'some', 'any', 'all', 'each',\n",
    "    'does', 'also', 'just', 'more', 'most', 'very', 'too', 'only',\n",
    "}\n",
    "\n",
    "print(\"Helpers defined.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e075a6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Generate surrogate conditions for each sample\n",
    "\n",
    "# Build a pool of \"other\" words from unrelated documents for random conditions\n",
    "other_words_pool = []\n",
    "for i, s in enumerate(samples):\n",
    "    other_idx = (i + N_SAMPLES // 2) % N_SAMPLES\n",
    "    other_doc = samples[other_idx]['document']\n",
    "    other_words_pool.append(other_doc.split())\n",
    "\n",
    "for i, s in enumerate(samples):\n",
    "    query_words = s['query'].split()\n",
    "    n_query_words = len(query_words)\n",
    "    other_words = other_words_pool[i]\n",
    "\n",
    "    # --- Content ablation (length-matched to oracle) ---\n",
    "\n",
    "    # scrambled_oracle: same words as query, random order\n",
    "    rng = np.random.RandomState(SEED + i)\n",
    "    shuffled = list(query_words)\n",
    "    rng.shuffle(shuffled)\n",
    "    s['scrambled_oracle'] = \" \".join(shuffled)\n",
    "\n",
    "    # random_matched: N random words from unrelated doc (same count as query)\n",
    "    if len(other_words) >= n_query_words:\n",
    "        s['random_matched'] = \" \".join(other_words[:n_query_words])\n",
    "    else:\n",
    "        # Pad with repeated words if other doc is too short\n",
    "        padded = other_words * ((n_query_words // len(other_words)) + 1)\n",
    "        s['random_matched'] = \" \".join(padded[:n_query_words])\n",
    "\n",
    "    # --- Token diversity (length-matched to oracle word count) ---\n",
    "\n",
    "    # repeat_the: \"the\" repeated N times\n",
    "    s['repeat_the'] = \" \".join([\"the\"] * n_query_words)\n",
    "\n",
    "    # repeat_kw: top document keyword repeated N times\n",
    "    doc_words_clean = re.sub(r'[^\\w\\s]', '', s['document'].lower()).split()\n",
    "    content = [w for w in doc_words_clean if w not in STOP_WORDS and len(w) > 2]\n",
    "    if content:\n",
    "        counts = Counter(content)\n",
    "        top_word = counts.most_common(1)[0][0]\n",
    "    else:\n",
    "        top_word = \"information\"\n",
    "    s['repeat_kw'] = \" \".join([top_word] * n_query_words)\n",
    "\n",
    "    # --- Practical surrogate ---\n",
    "\n",
    "    # surr_template: \"What is [top_keyword]?\"\n",
    "    if content:\n",
    "        kw = counts.most_common(1)[0][0]\n",
    "    else:\n",
    "        kw = \"information\"\n",
    "    s['surr_template'] = f\"What is {kw}?\"\n",
    "\n",
    "# Define all conditions to score\n",
    "COND_NAMES = [\n",
    "    'bare',\n",
    "    'oracle_trunc',\n",
    "    'random_matched_trunc',\n",
    "    'scrambled_oracle_trunc',\n",
    "    'surr_template_trunc',\n",
    "    'repeat_the_trunc',\n",
    "    'repeat_kw_trunc',\n",
    "]\n",
    "\n",
    "print(f\"Conditions to score: {len(COND_NAMES)}\")\n",
    "for c in COND_NAMES:\n",
    "    print(f\"  {c}\")\n",
    "\n",
    "# Show example\n",
    "ex = samples[0]\n",
    "print(f\"\\nExample (sample 0):\")\n",
    "print(f\"  Query ({ex['query_words']}w): {ex['query'][:120]}\")\n",
    "print(f\"  Document ({ex['doc_words']}w): {ex['document'][:80]}...\")\n",
    "print(f\"  Answer ({ex['answer_words']}w): {ex['answer'][:80]}...\")\n",
    "print()\n",
    "\n",
    "for c in COND_NAMES:\n",
    "    if c == 'bare':\n",
    "        print(f\"  {c:<28}: [document only]\")\n",
    "    elif c == 'oracle_trunc':\n",
    "        ptoks = count_prefix_tokens(ex['query'], ex['document'])\n",
    "        print(f\"  {c:<28} ({ptoks:>3} prefix toks): {ex['query'][:60]}...\")\n",
    "    else:\n",
    "        key = c.replace('_trunc', '')\n",
    "        text = ex[key]\n",
    "        ptoks = count_prefix_tokens(text, ex['document'])\n",
    "        print(f\"  {c:<28} ({ptoks:>3} prefix toks): {str(text)[:60]}\")\n",
    "\n",
    "# Report prefix token stats across first 50 samples\n",
    "print(f\"\\nPrefix token counts (first 50 samples):\")\n",
    "for c in COND_NAMES:\n",
    "    if c == 'bare':\n",
    "        continue\n",
    "    if c == 'oracle_trunc':\n",
    "        toks = [count_prefix_tokens(s['query'], s['document']) for s in samples[:50]]\n",
    "    else:\n",
    "        key = c.replace('_trunc', '')\n",
    "        toks = [count_prefix_tokens(s[key], s['document']) for s in samples[:50]]\n",
    "    print(f\"  {c:<28} mean={np.mean(toks):.1f}, range=[{min(toks)}, {max(toks)}]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff0950e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Run scoring with checkpointing\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"SCORING ALL CONDITIONS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "results = []\n",
    "start_idx = 0\n",
    "\n",
    "if CHECKPOINT_PATH.exists():\n",
    "    ckpt = json.loads(CHECKPOINT_PATH.read_text())\n",
    "    if ckpt.get('n_total') == N_SAMPLES and len(ckpt.get('results', [])) > 0:\n",
    "        saved_queries = [r['query'][:50] for r in ckpt['results']]\n",
    "        current_queries = [s['query'][:50] for s in samples[:len(saved_queries)]]\n",
    "        if saved_queries == current_queries:\n",
    "            results = ckpt['results']\n",
    "            start_idx = len(results)\n",
    "            print(f\"Resuming from checkpoint: {start_idx}/{N_SAMPLES}\")\n",
    "\n",
    "if start_idx == 0:\n",
    "    print(f\"Starting fresh: {len(COND_NAMES)} conditions x {N_SAMPLES} samples \"\n",
    "          f\"= {len(COND_NAMES) * N_SAMPLES} scorings\")\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "for i in tqdm(range(start_idx, N_SAMPLES), initial=start_idx, total=N_SAMPLES,\n",
    "              desc=\"Scoring\"):\n",
    "    s = samples[i]\n",
    "    result = {\n",
    "        'query': s['query'],\n",
    "        'answer': s['answer'],\n",
    "        'query_words': s['query_words'],\n",
    "        'doc_words': s['doc_words'],\n",
    "        'answer_words': s['answer_words'],\n",
    "    }\n",
    "\n",
    "    for cond in COND_NAMES:\n",
    "        if cond == 'bare':\n",
    "            nll = score_nll(s['document'], s['answer'])\n",
    "            result['nll_bare'] = nll\n",
    "        elif cond == 'oracle_trunc':\n",
    "            enc_text = s['query'] + \"\\n\" + s['document']\n",
    "            ptoks = count_prefix_tokens(s['query'], s['document'])\n",
    "            nll = score_nll(enc_text, s['answer'], ptoks, truncate=True)\n",
    "            result['nll_oracle_trunc'] = nll\n",
    "            result['ptoks_oracle'] = ptoks\n",
    "        else:\n",
    "            key = cond.replace('_trunc', '')\n",
    "            surr_text = s[key]\n",
    "            enc_text = surr_text + \"\\n\" + s['document']\n",
    "            ptoks = count_prefix_tokens(surr_text, s['document'])\n",
    "            nll = score_nll(enc_text, s['answer'], ptoks, truncate=True)\n",
    "            result[f'nll_{cond}'] = nll\n",
    "            result[f'ptoks_{cond}'] = ptoks\n",
    "\n",
    "    results.append(result)\n",
    "\n",
    "    if (i + 1) % 20 == 0 or i == N_SAMPLES - 1:\n",
    "        ckpt = {\n",
    "            'n_total': N_SAMPLES,\n",
    "            'results': results,\n",
    "            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        }\n",
    "        CHECKPOINT_PATH.write_text(json.dumps(ckpt))\n",
    "        elapsed = time.time() - t0\n",
    "        done = i - start_idx + 1\n",
    "        eta = (N_SAMPLES - i - 1) * elapsed / done if done > 0 else 0\n",
    "        tqdm.write(f\"  Checkpoint {i+1}/{N_SAMPLES} | {elapsed/60:.1f}m | ETA {eta/60:.1f}m\")\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "elapsed = time.time() - t0\n",
    "print(f\"\\nScoring complete: {len(results)} samples, \"\n",
    "      f\"{len(COND_NAMES)} conditions in {elapsed/60:.1f} min\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd74a1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Part 1 — Baseline Characterization\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PART 1: BASELINE CHARACTERIZATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "bare_nlls = np.array([r['nll_bare'] for r in results])\n",
    "oracle_nlls = np.array([r['nll_oracle_trunc'] for r in results])\n",
    "oracle_benefit = bare_nlls - oracle_nlls\n",
    "\n",
    "print(f\"\\nBaseline NLL (document only):  mean={bare_nlls.mean():.4f}, std={bare_nlls.std():.4f}\")\n",
    "print(f\"Oracle NLL (query + document): mean={oracle_nlls.mean():.4f}, std={oracle_nlls.std():.4f}\")\n",
    "print(f\"Oracle headroom: delta={oracle_benefit.mean():+.4f}, d={cohens_d(oracle_benefit):+.3f}\")\n",
    "\n",
    "# Win rate and significance\n",
    "win_rate = np.mean(oracle_benefit > 0) * 100\n",
    "_, p_oracle = stats.ttest_1samp(oracle_benefit, 0)\n",
    "print(f\"Oracle win rate: {win_rate:.1f}% (p={p_oracle:.2e})\")\n",
    "\n",
    "if oracle_benefit.mean() < 0.05:\n",
    "    print(\"\\nWARNING: Oracle headroom is very small. Results may not be meaningful.\")\n",
    "elif p_oracle > 0.05:\n",
    "    print(\"\\nWARNING: Oracle headroom is not statistically significant.\")\n",
    "else:\n",
    "    print(f\"\\nGood: Oracle headroom is significant (d={cohens_d(oracle_benefit):+.3f}, \"\n",
    "          f\"p={p_oracle:.2e})\")\n",
    "\n",
    "# Headroom distribution\n",
    "pcts = np.percentile(oracle_benefit, [10, 25, 50, 75, 90])\n",
    "print(f\"\\nOracle benefit distribution:\")\n",
    "print(f\"  10th pctile: {pcts[0]:+.4f}\")\n",
    "print(f\"  25th pctile: {pcts[1]:+.4f}\")\n",
    "print(f\"  Median:      {pcts[2]:+.4f}\")\n",
    "print(f\"  75th pctile: {pcts[3]:+.4f}\")\n",
    "print(f\"  90th pctile: {pcts[4]:+.4f}\")\n",
    "print(f\"  % negative:  {np.mean(oracle_benefit < 0)*100:.1f}%\")\n",
    "\n",
    "# Comparison with MS MARCO\n",
    "print(f\"\\n--- Comparison with MS MARCO Exp 02 ---\")\n",
    "print(f\"  MS MARCO: bare NLL=3.68, oracle NLL=2.99, headroom=+0.68, d=+0.376\")\n",
    "print(f\"  This:     bare NLL={bare_nlls.mean():.2f}, oracle NLL={oracle_nlls.mean():.2f}, \"\n",
    "      f\"headroom={oracle_benefit.mean():+.3f}, d={cohens_d(oracle_benefit):+.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e343e2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Part 2 — Content Ablation Decomposition\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PART 2: CONTENT ABLATION\")\n",
    "print(\"=\" * 70)\n",
    "print(\"Decompose: bare -> random_matched -> scrambled_oracle -> oracle\")\n",
    "print(\"  Structure:  bare -> random_matched (any prefix helps)\")\n",
    "print(\"  Vocabulary: random_matched -> scrambled (right words, wrong order)\")\n",
    "print(\"  Semantics:  scrambled -> oracle (right word order, full meaning)\\n\")\n",
    "\n",
    "randmatch_nlls = np.array([r['nll_random_matched_trunc'] for r in results])\n",
    "scrambled_nlls = np.array([r['nll_scrambled_oracle_trunc'] for r in results])\n",
    "\n",
    "# Component benefits\n",
    "struct_comp = bare_nlls - randmatch_nlls\n",
    "vocab_comp = randmatch_nlls - scrambled_nlls\n",
    "sem_comp = scrambled_nlls - oracle_nlls\n",
    "total_comp = bare_nlls - oracle_nlls\n",
    "\n",
    "total_mean = total_comp.mean()\n",
    "\n",
    "# Table: condition NLLs and incremental gains\n",
    "print(f\"{'Step':<32} {'Mean NLL':>10} {'Delta':>10} {'% total':>9} \"\n",
    "      f\"{'d':>8} {'p':>12} {'sig':>5}\")\n",
    "print(\"-\" * 90)\n",
    "\n",
    "print(f\"  {'bare (baseline)':<30} {bare_nlls.mean():>10.4f}\")\n",
    "\n",
    "for name, nlls, component, label in [\n",
    "    ('random_matched_trunc', randmatch_nlls, struct_comp, '+ Structure'),\n",
    "    ('scrambled_oracle_trunc', scrambled_nlls, vocab_comp, '+ Vocabulary'),\n",
    "    ('oracle_trunc', oracle_nlls, sem_comp, '+ Semantics'),\n",
    "]:\n",
    "    mu = component.mean()\n",
    "    pct = mu / total_mean * 100 if total_mean != 0 else 0\n",
    "    d = cohens_d(component)\n",
    "    _, p = stats.ttest_1samp(component, 0)\n",
    "    sig = '***' if p < 0.001 else '**' if p < 0.01 else '*' if p < 0.05 else 'ns'\n",
    "    print(f\"  {label:<30} {nlls.mean():>10.4f} {mu:>+10.4f} {pct:>8.1f}% \"\n",
    "          f\"{d:>+8.3f} {p:>12.2e} {sig}\")\n",
    "\n",
    "print(f\"  {'TOTAL':<30} {'':>10} {total_mean:>+10.4f} {'100.0%':>9}\")\n",
    "\n",
    "# Decomposition residual\n",
    "residual = total_mean - (struct_comp.mean() + vocab_comp.mean() + sem_comp.mean())\n",
    "print(f\"\\n  Decomposition residual: {residual:.6f} (should be ~0)\")\n",
    "\n",
    "# Percentages\n",
    "struct_pct = struct_comp.mean() / total_mean * 100 if total_mean != 0 else 0\n",
    "vocab_pct = vocab_comp.mean() / total_mean * 100 if total_mean != 0 else 0\n",
    "sem_pct = sem_comp.mean() / total_mean * 100 if total_mean != 0 else 0\n",
    "\n",
    "print(f\"\\n  SUMMARY: Structure={struct_pct:.1f}%, Vocabulary={vocab_pct:.1f}%, \"\n",
    "      f\"Semantics={sem_pct:.1f}%\")\n",
    "\n",
    "# Compare with MS MARCO Exp 2B\n",
    "print(f\"\\n--- Comparison with Exp 2B (MS MARCO, 6-word queries) ---\")\n",
    "print(f\"  MS MARCO:  Structure=84.7%, Vocabulary=5.5% (ns), Semantics=9.7% (***)\")\n",
    "print(f\"  This data: Structure={struct_pct:.1f}%, Vocabulary={vocab_pct:.1f}%, \"\n",
    "      f\"Semantics={sem_pct:.1f}%\")\n",
    "\n",
    "delta_struct = struct_pct - 84.7\n",
    "delta_sem = sem_pct - 9.7\n",
    "print(f\"  Structure shift: {delta_struct:+.1f} percentage points\")\n",
    "print(f\"  Semantics shift: {delta_sem:+.1f} percentage points\")\n",
    "\n",
    "if sem_pct > 20:\n",
    "    print(f\"\\n  --> SEMANTIC COMPONENT GREW with longer queries!\")\n",
    "    print(f\"      This suggests the MS MARCO finding was partly an artifact of short queries.\")\n",
    "elif sem_pct > 15:\n",
    "    print(f\"\\n  --> MODERATE semantic growth. Content matters somewhat more with longer queries.\")\n",
    "else:\n",
    "    print(f\"\\n  --> Semantic component is similar to MS MARCO.\")\n",
    "    print(f\"      The 85% structural finding appears to be a genuine property of the mechanism.\")\n",
    "\n",
    "# Token count verification\n",
    "print(f\"\\n--- Length matching verification ---\")\n",
    "oracle_toks = np.array([r['ptoks_oracle'] for r in results])\n",
    "scrambled_toks = np.array([r['ptoks_scrambled_oracle_trunc'] for r in results])\n",
    "randmatch_toks = np.array([r['ptoks_random_matched_trunc'] for r in results])\n",
    "print(f\"  Oracle:     mean={oracle_toks.mean():.1f} toks\")\n",
    "print(f\"  Scrambled:  mean={scrambled_toks.mean():.1f} toks\")\n",
    "print(f\"  RandMatch:  mean={randmatch_toks.mean():.1f} toks\")\n",
    "tok_diff = np.abs(oracle_toks - scrambled_toks)\n",
    "print(f\"  Oracle-Scrambled token diff: mean={tok_diff.mean():.1f}, max={tok_diff.max()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf8f485",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Part 3 — Token Diversity\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PART 3: TOKEN DIVERSITY\")\n",
    "print(\"=\" * 70)\n",
    "print(\"All conditions length-matched to query word count. Does content matter?\\n\")\n",
    "\n",
    "repeat_the_nlls = np.array([r['nll_repeat_the_trunc'] for r in results])\n",
    "repeat_kw_nlls = np.array([r['nll_repeat_kw_trunc'] for r in results])\n",
    "surr_tmpl_nlls = np.array([r['nll_surr_template_trunc'] for r in results])\n",
    "\n",
    "oracle_d = cohens_d(oracle_benefit)\n",
    "\n",
    "all_conds = [\n",
    "    ('oracle_trunc', oracle_nlls, 'Real query (upper bound)'),\n",
    "    ('scrambled_oracle_trunc', scrambled_nlls, 'Query words, shuffled'),\n",
    "    ('random_matched_trunc', randmatch_nlls, 'Random words, length-matched'),\n",
    "    ('surr_template_trunc', surr_tmpl_nlls, '\"What is [keyword]?\"'),\n",
    "    ('repeat_kw_trunc', repeat_kw_nlls, 'Doc keyword repeated N times'),\n",
    "    ('repeat_the_trunc', repeat_the_nlls, '\"the\" repeated N times'),\n",
    "]\n",
    "\n",
    "print(f\"{'Description':<42} {'NLL':>8} {'Delta':>10} {'d':>8} {'Win%':>7} {'%Orc':>6}\")\n",
    "print(\"-\" * 85)\n",
    "\n",
    "for name, nlls, desc in all_conds:\n",
    "    benefit = bare_nlls - nlls\n",
    "    d = cohens_d(benefit)\n",
    "    delta = benefit.mean()\n",
    "    win = 100 * np.mean(benefit > 0)\n",
    "    pct = d / oracle_d * 100 if oracle_d > 0 else 0\n",
    "    print(f\"  {desc:<40} {nlls.mean():>8.4f} {delta:>+10.4f} {d:>+8.3f} {win:>6.1f}% {pct:>5.0f}%\")\n",
    "\n",
    "print(f\"  {'bare (lower bound)':<40} {bare_nlls.mean():>8.4f}\")\n",
    "\n",
    "# Pairwise comparisons\n",
    "print(f\"\\n--- Pairwise head-to-head ---\")\n",
    "pairs = [\n",
    "    ('oracle', oracle_nlls, 'scrambled', scrambled_nlls,\n",
    "     \"Does word ORDER matter?\"),\n",
    "    ('scrambled', scrambled_nlls, 'random_matched', randmatch_nlls,\n",
    "     \"Do the right WORDS matter?\"),\n",
    "    ('oracle', oracle_nlls, 'random_matched', randmatch_nlls,\n",
    "     \"Does ANY content matter (oracle vs random)?\"),\n",
    "    ('repeat_the', repeat_the_nlls, 'random_matched', randmatch_nlls,\n",
    "     \"Does diversity help (uniform vs diverse)?\"),\n",
    "    ('repeat_kw', repeat_kw_nlls, 'repeat_the', repeat_the_nlls,\n",
    "     \"Does keyword content help vs pure filler?\"),\n",
    "    ('surr_template', surr_tmpl_nlls, 'random_matched', randmatch_nlls,\n",
    "     \"Does template surrogate beat random?\"),\n",
    "]\n",
    "\n",
    "for name_a, nlls_a, name_b, nlls_b, question in pairs:\n",
    "    diff = nlls_b - nlls_a  # positive = A is better\n",
    "    d = cohens_d(diff)\n",
    "    win = 100 * np.mean(diff > 0)\n",
    "    _, p = stats.ttest_1samp(diff, 0)\n",
    "    sig = '***' if p < 0.001 else '**' if p < 0.01 else '*' if p < 0.05 else 'ns'\n",
    "    winner = name_a if d > 0 else name_b\n",
    "    print(f\"  {question}\")\n",
    "    print(f\"    {name_a} vs {name_b}: d={d:+.3f}, win={win:.1f}%, p={p:.2e} {sig} [{winner}]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfe44d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Part 4 — Query Length Stratification (within-dataset)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PART 4: QUERY LENGTH STRATIFICATION\")\n",
    "print(\"=\" * 70)\n",
    "print(\"Does the structural/semantic balance shift with query length?\\n\")\n",
    "\n",
    "q_lens = np.array([r['query_words'] for r in results])\n",
    "\n",
    "# Use terciles for reasonable bin sizes\n",
    "tercile_bounds = np.percentile(q_lens, [33, 67])\n",
    "q_bins = np.digitize(q_lens, tercile_bounds)\n",
    "bin_labels = []\n",
    "for b in range(3):\n",
    "    mask = q_bins == b\n",
    "    bmin, bmax = q_lens[mask].min(), q_lens[mask].max()\n",
    "    bmean = q_lens[mask].mean()\n",
    "    label = f\"{'Short' if b==0 else 'Medium' if b==1 else 'Long'} ({bmin}-{bmax}w, mean={bmean:.0f})\"\n",
    "    bin_labels.append(label)\n",
    "\n",
    "print(f\"{'Query bin':<35} {'N':>4} {'Struct%':>9} {'Vocab%':>8} {'Sem%':>7} \"\n",
    "      f\"{'Oracle d':>10} {'Rand d':>9}\")\n",
    "print(\"-\" * 90)\n",
    "\n",
    "bin_struct_pcts = []\n",
    "bin_sem_pcts = []\n",
    "bin_q_means = []\n",
    "\n",
    "for b in range(3):\n",
    "    mask = q_bins == b\n",
    "    n = mask.sum()\n",
    "\n",
    "    # Compute components for this bin\n",
    "    b_struct = (bare_nlls[mask] - randmatch_nlls[mask]).mean()\n",
    "    b_vocab = (randmatch_nlls[mask] - scrambled_nlls[mask]).mean()\n",
    "    b_sem = (scrambled_nlls[mask] - oracle_nlls[mask]).mean()\n",
    "    b_total = (bare_nlls[mask] - oracle_nlls[mask]).mean()\n",
    "\n",
    "    if b_total > 0:\n",
    "        s_pct = b_struct / b_total * 100\n",
    "        v_pct = b_vocab / b_total * 100\n",
    "        sm_pct = b_sem / b_total * 100\n",
    "    else:\n",
    "        s_pct = v_pct = sm_pct = 0\n",
    "\n",
    "    o_d = cohens_d(bare_nlls[mask] - oracle_nlls[mask])\n",
    "    r_d = cohens_d(bare_nlls[mask] - randmatch_nlls[mask])\n",
    "\n",
    "    bin_struct_pcts.append(s_pct)\n",
    "    bin_sem_pcts.append(sm_pct)\n",
    "    bin_q_means.append(q_lens[mask].mean())\n",
    "\n",
    "    print(f\"  {bin_labels[b]:<33} {n:>4} {s_pct:>8.1f}% {v_pct:>7.1f}% {sm_pct:>6.1f}% \"\n",
    "          f\"{o_d:>+10.3f} {r_d:>+9.3f}\")\n",
    "\n",
    "# Correlation: query length vs each component\n",
    "print(f\"\\n--- Correlations ---\")\n",
    "r_struct_q, p_struct_q = stats.pearsonr(q_lens, struct_comp)\n",
    "r_vocab_q, p_vocab_q = stats.pearsonr(q_lens, vocab_comp)\n",
    "r_sem_q, p_sem_q = stats.pearsonr(q_lens, sem_comp)\n",
    "r_total_q, p_total_q = stats.pearsonr(q_lens, total_comp)\n",
    "print(f\"  query_len vs structure:  r={r_struct_q:+.3f} (p={p_struct_q:.3f})\")\n",
    "print(f\"  query_len vs vocabulary: r={r_vocab_q:+.3f} (p={p_vocab_q:.3f})\")\n",
    "print(f\"  query_len vs semantics:  r={r_sem_q:+.3f} (p={p_sem_q:.3f})\")\n",
    "print(f\"  query_len vs total:      r={r_total_q:+.3f} (p={p_total_q:.3f})\")\n",
    "\n",
    "# Semantic gap: does oracle beat random more for longer queries?\n",
    "semantic_gap = oracle_benefit - (bare_nlls - randmatch_nlls)\n",
    "r_gap_q, p_gap_q = stats.pearsonr(q_lens, semantic_gap)\n",
    "print(f\"  query_len vs semantic_gap (oracle-random): r={r_gap_q:+.3f} (p={p_gap_q:.3f})\")\n",
    "\n",
    "if r_gap_q > 0.1 and p_gap_q < 0.05:\n",
    "    print(f\"\\n  --> Semantic gap GROWS with query length!\")\n",
    "    print(f\"      Content matters MORE for longer queries, as hypothesized.\")\n",
    "elif r_gap_q < -0.1 and p_gap_q < 0.05:\n",
    "    print(f\"\\n  --> Semantic gap SHRINKS with query length.\")\n",
    "    print(f\"      Structure becomes MORE dominant for longer queries.\")\n",
    "else:\n",
    "    print(f\"\\n  --> Semantic gap is STABLE across query lengths (within this dataset).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e87b760",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Part 5 — Hardness Stratification\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PART 5: HARDNESS STRATIFICATION\")\n",
    "print(\"=\" * 70)\n",
    "print(\"Does the decomposition change for harder documents?\\n\")\n",
    "\n",
    "quintile_bounds = np.percentile(bare_nlls, [20, 40, 60, 80])\n",
    "quintiles = np.digitize(bare_nlls, quintile_bounds)\n",
    "\n",
    "print(f\"{'Quintile':<12} {'N':>4} {'Bare NLL':>10} {'Struct%':>9} {'Vocab%':>8} \"\n",
    "      f\"{'Sem%':>7} {'Oracle d':>10} {'Sem gap':>10}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for q in range(5):\n",
    "    mask = quintiles == q\n",
    "    n = mask.sum()\n",
    "    qlabel = ['Q1 easy', 'Q2', 'Q3', 'Q4', 'Q5 hard'][q]\n",
    "    bare_q = bare_nlls[mask].mean()\n",
    "\n",
    "    b_struct = (bare_nlls[mask] - randmatch_nlls[mask]).mean()\n",
    "    b_vocab = (randmatch_nlls[mask] - scrambled_nlls[mask]).mean()\n",
    "    b_sem = (scrambled_nlls[mask] - oracle_nlls[mask]).mean()\n",
    "    b_total = (bare_nlls[mask] - oracle_nlls[mask]).mean()\n",
    "\n",
    "    if b_total > 0:\n",
    "        s_pct = b_struct / b_total * 100\n",
    "        v_pct = b_vocab / b_total * 100\n",
    "        sm_pct = b_sem / b_total * 100\n",
    "    else:\n",
    "        s_pct = v_pct = sm_pct = 0\n",
    "\n",
    "    o_d = cohens_d(bare_nlls[mask] - oracle_nlls[mask])\n",
    "    gap = (oracle_benefit[mask] - (bare_nlls[mask] - randmatch_nlls[mask])).mean()\n",
    "\n",
    "    print(f\"  {qlabel:<10} {n:>4} {bare_q:>10.3f} {s_pct:>8.1f}% {v_pct:>7.1f}% \"\n",
    "          f\"{sm_pct:>6.1f}% {o_d:>+10.3f} {gap:>+10.4f}\")\n",
    "\n",
    "# Correlation: hardness vs each component\n",
    "print(f\"\\n--- Correlations ---\")\n",
    "r_s, p_s = stats.pearsonr(bare_nlls, struct_comp)\n",
    "r_v, p_v = stats.pearsonr(bare_nlls, vocab_comp)\n",
    "r_sm, p_sm = stats.pearsonr(bare_nlls, sem_comp)\n",
    "print(f\"  hardness vs structure:  r={r_s:+.3f} (p={p_s:.2e})\")\n",
    "print(f\"  hardness vs vocabulary: r={r_v:+.3f} (p={p_v:.2e})\")\n",
    "print(f\"  hardness vs semantics:  r={r_sm:+.3f} (p={p_sm:.2e})\")\n",
    "\n",
    "# Semantic component x hardness: does content matter more for hard samples?\n",
    "# (This was true on MS MARCO: Q1 gap=+0.013 -> Q5 gap=+0.397)\n",
    "print(f\"\\n--- Semantic gap by hardness ---\")\n",
    "print(f\"  MS MARCO Exp 2B: Q1 gap=+0.013, Q5 gap=+0.397\")\n",
    "for q in range(5):\n",
    "    mask = quintiles == q\n",
    "    qlabel = ['Q1 easy', 'Q2', 'Q3', 'Q4', 'Q5 hard'][q]\n",
    "    gap = (oracle_benefit[mask] - (bare_nlls[mask] - randmatch_nlls[mask])).mean()\n",
    "    print(f\"  This data {qlabel}: gap={gap:+.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca28a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Synthesis + Save\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"SYNTHESIS: CROSS-DATASET CONTENT ABLATION RESULTS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Key numbers\n",
    "oracle_d = cohens_d(oracle_benefit)\n",
    "struct_d = cohens_d(struct_comp)\n",
    "vocab_d = cohens_d(vocab_comp)\n",
    "sem_d = cohens_d(sem_comp)\n",
    "\n",
    "struct_pct = struct_comp.mean() / total_comp.mean() * 100 if total_comp.mean() != 0 else 0\n",
    "vocab_pct = vocab_comp.mean() / total_comp.mean() * 100 if total_comp.mean() != 0 else 0\n",
    "sem_pct = sem_comp.mean() / total_comp.mean() * 100 if total_comp.mean() != 0 else 0\n",
    "\n",
    "repeat_the_d = cohens_d(bare_nlls - repeat_the_nlls)\n",
    "repeat_kw_d = cohens_d(bare_nlls - repeat_kw_nlls)\n",
    "randmatch_d = cohens_d(bare_nlls - randmatch_nlls)\n",
    "surr_tmpl_d = cohens_d(bare_nlls - surr_tmpl_nlls)\n",
    "\n",
    "# All condition NLLs for the summary\n",
    "print(f\"\\n1. DATASET: neural-bridge/rag-dataset-12000\")\n",
    "print(f\"   Query length: {q_lens.mean():.1f}w (vs 6.0w MS MARCO)\")\n",
    "print(f\"   Doc length: {np.array([r['doc_words'] for r in results]).mean():.0f}w\")\n",
    "print(f\"   N: {N_SAMPLES}\")\n",
    "\n",
    "print(f\"\\n2. ORACLE HEADROOM:\")\n",
    "print(f\"   Cohen's d: {oracle_d:+.3f}\")\n",
    "print(f\"   NLL improvement: {oracle_benefit.mean():+.4f}\")\n",
    "_, p_oracle = stats.ttest_1samp(oracle_benefit, 0)\n",
    "print(f\"   Win rate: {np.mean(oracle_benefit > 0)*100:.1f}% (p={p_oracle:.2e})\")\n",
    "\n",
    "print(f\"\\n3. CONTENT ABLATION:\")\n",
    "print(f\"   {'Component':<15} {'This dataset':>15} {'MS MARCO':>15} {'Change':>10}\")\n",
    "print(f\"   {'-'*60}\")\n",
    "print(f\"   {'Structure':<15} {struct_pct:>14.1f}% {'84.7%':>15} {struct_pct-84.7:>+9.1f}pp\")\n",
    "print(f\"   {'Vocabulary':<15} {vocab_pct:>14.1f}% {'5.5%':>15} {vocab_pct-5.5:>+9.1f}pp\")\n",
    "print(f\"   {'Semantics':<15} {sem_pct:>14.1f}% {'9.7%':>15} {sem_pct-9.7:>+9.1f}pp\")\n",
    "\n",
    "print(f\"\\n4. TOKEN DIVERSITY:\")\n",
    "print(f\"   repeat_the: d={repeat_the_d:+.3f} ({repeat_the_d/oracle_d*100:.0f}% oracle)\")\n",
    "print(f\"   repeat_kw:  d={repeat_kw_d:+.3f} ({repeat_kw_d/oracle_d*100:.0f}% oracle)\")\n",
    "print(f\"   rand_match: d={randmatch_d:+.3f} ({randmatch_d/oracle_d*100:.0f}% oracle)\")\n",
    "print(f\"   surr_tmpl:  d={surr_tmpl_d:+.3f} ({surr_tmpl_d/oracle_d*100:.0f}% oracle)\")\n",
    "\n",
    "print(f\"\\n5. QUERY LENGTH EFFECT:\")\n",
    "for b in range(3):\n",
    "    mask = q_bins == b\n",
    "    b_total = (bare_nlls[mask] - oracle_nlls[mask]).mean()\n",
    "    if b_total > 0:\n",
    "        b_sem_pct = (scrambled_nlls[mask] - oracle_nlls[mask]).mean() / b_total * 100\n",
    "    else:\n",
    "        b_sem_pct = 0\n",
    "    print(f\"   {bin_labels[b]}: semantic={b_sem_pct:.1f}%\")\n",
    "\n",
    "# Determine overall conclusion\n",
    "if struct_pct > 70:\n",
    "    mechanism = \"STRUCTURAL\"\n",
    "elif struct_pct > 50:\n",
    "    mechanism = \"MIXED\"\n",
    "else:\n",
    "    mechanism = \"SEMANTIC\"\n",
    "\n",
    "if abs(struct_pct - 84.7) < 10:\n",
    "    cross_dataset = \"CONSISTENT\"\n",
    "elif struct_pct < 74.7:\n",
    "    cross_dataset = \"SEMANTIC_GREW\"\n",
    "else:\n",
    "    cross_dataset = \"STRUCTURAL_GREW\"\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"CONCLUSION:\")\n",
    "print(f\"  Mechanism: {mechanism}\")\n",
    "print(f\"  Cross-dataset consistency: {cross_dataset}\")\n",
    "if mechanism == \"STRUCTURAL\":\n",
    "    print(f\"  The 85% structural finding holds with 3x longer queries on a different dataset.\")\n",
    "    print(f\"  This is a genuine property of the T5Gemma bidirectional encoder mechanism,\")\n",
    "    print(f\"  not an artifact of MS MARCO's short queries.\")\n",
    "elif mechanism == \"MIXED\":\n",
    "    print(f\"  With longer queries, the semantic component grew meaningfully.\")\n",
    "    print(f\"  The mechanism is still primarily structural, but content matters more\")\n",
    "    print(f\"  when queries carry more semantic structure to preserve.\")\n",
    "else:\n",
    "    print(f\"  With longer queries, the semantic component dominates.\")\n",
    "    print(f\"  The MS MARCO finding WAS an artifact of short queries.\")\n",
    "    print(f\"  Content-aware surrogates are worthwhile for long-query use cases.\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "# Save results\n",
    "final_results = {\n",
    "    'experiment': 'exp03d_cross_dataset_content_ablation',\n",
    "    'model': MODEL_NAME,\n",
    "    'dataset': 'neural-bridge/rag-dataset-12000',\n",
    "    'n_samples': N_SAMPLES,\n",
    "    'mean_query_words': float(q_lens.mean()),\n",
    "    'mean_doc_words': float(np.array([r['doc_words'] for r in results]).mean()),\n",
    "    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'baseline': {\n",
    "        'bare_nll': float(bare_nlls.mean()),\n",
    "        'oracle_nll': float(oracle_nlls.mean()),\n",
    "        'oracle_d': float(oracle_d),\n",
    "        'oracle_headroom': float(oracle_benefit.mean()),\n",
    "        'oracle_p': float(p_oracle),\n",
    "    },\n",
    "    'ablation': {\n",
    "        'structure_pct': float(struct_pct),\n",
    "        'vocabulary_pct': float(vocab_pct),\n",
    "        'semantics_pct': float(sem_pct),\n",
    "        'structure_d': float(struct_d),\n",
    "        'vocabulary_d': float(vocab_d),\n",
    "        'semantics_d': float(sem_d),\n",
    "    },\n",
    "    'conditions': {\n",
    "        'oracle_trunc_d': float(oracle_d),\n",
    "        'scrambled_oracle_trunc_d': float(cohens_d(bare_nlls - scrambled_nlls)),\n",
    "        'random_matched_trunc_d': float(randmatch_d),\n",
    "        'surr_template_trunc_d': float(surr_tmpl_d),\n",
    "        'repeat_the_trunc_d': float(repeat_the_d),\n",
    "        'repeat_kw_trunc_d': float(repeat_kw_d),\n",
    "    },\n",
    "    'comparison_with_msmarco': {\n",
    "        'struct_pct_delta': float(struct_pct - 84.7),\n",
    "        'vocab_pct_delta': float(vocab_pct - 5.5),\n",
    "        'sem_pct_delta': float(sem_pct - 9.7),\n",
    "    },\n",
    "    'conclusion': {\n",
    "        'mechanism': mechanism,\n",
    "        'cross_dataset': cross_dataset,\n",
    "    },\n",
    "}\n",
    "\n",
    "with open(RESULTS_DIR / 'results.json', 'w') as f:\n",
    "    json.dump(final_results, f, indent=2)\n",
    "print(f\"\\nResults saved to {RESULTS_DIR / 'results.json'}\")\n",
    "\n",
    "# Cleanup\n",
    "print(f\"\\nCleaning up GPU memory...\")\n",
    "mem_before = torch.cuda.memory_allocated() / 1e9\n",
    "del model, processor, tokenizer\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "mem_after = torch.cuda.memory_allocated() / 1e9\n",
    "print(f\"GPU memory: {mem_before:.2f} GB -> {mem_after:.2f} GB\")\n",
    "print(\"Done!\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
