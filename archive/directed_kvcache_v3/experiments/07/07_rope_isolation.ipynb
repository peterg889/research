{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd433405",
   "metadata": {},
   "source": [
    "# Experiment 07: RoPE Isolation\n",
    "\n",
    "## Motivation\n",
    "\n",
    "When we prepend a prefix to the encoder input, two things change simultaneously:\n",
    "\n",
    "1. **Attention redistribution**: New tokens in the attention pool soak up attention mass,\n",
    "   reorganizing doc-doc attention patterns (KL up to 2.97 nats, Exp 3E).\n",
    "2. **RoPE position shift**: Document tokens shift from positions [0..L-1] to [N..N+L-1],\n",
    "   changing the absolute RoPE embeddings.\n",
    "\n",
    "Since RoPE is *purely relative* (attention score between positions i and j depends only\n",
    "on i-j, not on i or j individually), the doc-doc relative positions don't change when\n",
    "a prefix is prepended. Therefore the RoPE position shift should NOT affect doc-doc\n",
    "attention patterns. But this hasn't been empirically verified.\n",
    "\n",
    "## Design: 2x2 Factorial\n",
    "\n",
    "| | Prefix in attention (ON) | Prefix in attention (OFF) |\n",
    "|---|---|---|\n",
    "| **RoPE shift (ON)** | `random_trunc` (standard) | `prefix_encoder_blocked` |\n",
    "| **RoPE shift (OFF)** | `random_rope_neutralized` | `bare` (baseline) |\n",
    "\n",
    "Plus `oracle_trunc` as upper bound and `shifted_bare` as pure absolute position test.\n",
    "\n",
    "### 6 Conditions\n",
    "\n",
    "| # | Condition | Input | position_ids | Encoder attention | Cross-attn |\n",
    "|---|-----------|-------|-------------|-------------------|-----------|\n",
    "| 1 | `bare` | [doc] | [0..L-1] | normal | all |\n",
    "| 2 | `oracle_trunc` | [oracle+doc] | default | normal | mask prefix |\n",
    "| 3 | `random_trunc` | [random+doc] | default | normal | mask prefix |\n",
    "| 4 | `shifted_bare` | [doc] | [6..6+L-1] | normal | all |\n",
    "| 5 | `prefix_encoder_blocked` | [random+doc] | default | doc→prefix BLOCKED | mask prefix |\n",
    "| 6 | `random_rope_neutralized` | [random+doc] | [L..L+N, 0..L-1] | normal | mask prefix |\n",
    "\n",
    "### Predictions\n",
    "\n",
    "- **4 ≈ 1**: Shifting bare doc positions has no effect (RoPE is relative)\n",
    "- **5 ≈ 1**: Invisible prefix + position shift has no effect (no attention redistribution)\n",
    "- **6 ≈ 3**: Benefit comes from attention redistribution, not position shift\n",
    "- **3 >> 1**: The benefit is from having prefix tokens in the attention pool\n",
    "- If all predictions hold: **RoPE conclusively ruled out as a contributor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a077e5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Setup\n",
    "import os\n",
    "os.umask(0o000)\n",
    "\n",
    "import sys, json, time, gc, random as pyrandom\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "from scipy import stats\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "sys.path.insert(0, \"../..\")\n",
    "from lib.analysis import cohens_d\n",
    "\n",
    "SEED = 42  # Same samples as Exp 02\n",
    "N_SAMPLES = 500\n",
    "POSITION_OFFSET = 6  # Typical prefix length in tokens\n",
    "MODEL_NAME = \"google/t5gemma-2-4b-4b\"\n",
    "\n",
    "RESULTS_DIR = Path(\"../../results/exp07\")\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CHECKPOINT_PATH = RESULTS_DIR / \"checkpoint.json\"\n",
    "\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "pyrandom.seed(SEED)\n",
    "\n",
    "print(\"Exp 07: RoPE Isolation\")\n",
    "print(f\"N: {N_SAMPLES}, position offset: {POSITION_OFFSET}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5764b399",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Load MS MARCO — reconstruct same 500 samples as Exp 02\n",
    "from datasets import load_dataset\n",
    "\n",
    "print(\"Loading MS MARCO...\")\n",
    "ds = load_dataset(\"microsoft/ms_marco\", \"v1.1\", split=\"validation\")\n",
    "\n",
    "# Reconstruct same candidates as Exp 02\n",
    "from lib.data import count_words\n",
    "\n",
    "all_candidates = []\n",
    "for item in ds:\n",
    "    if len(all_candidates) >= 3 * N_SAMPLES:\n",
    "        break\n",
    "    passages = item.get('passages', {})\n",
    "    ptexts = passages.get('passage_text', [])\n",
    "    is_sel = passages.get('is_selected', [])\n",
    "    query = item.get('query', '')\n",
    "    answers = item.get('answers', [])\n",
    "    well_formed = item.get('wellFormedAnswers', [])\n",
    "    answer = None\n",
    "    if well_formed and len(well_formed) > 0 and well_formed[0] not in ('[]', ''):\n",
    "        answer = well_formed[0]\n",
    "    elif answers and len(answers) > 0 and answers[0] != 'No Answer Present.':\n",
    "        answer = answers[0]\n",
    "    if not answer:\n",
    "        continue\n",
    "    for pt, sel in zip(ptexts, is_sel):\n",
    "        wc = count_words(pt)\n",
    "        if sel == 1 and 30 <= wc <= 300:\n",
    "            all_candidates.append({\n",
    "                'passage': pt, 'query': query, 'answer': answer,\n",
    "                'word_count': wc,\n",
    "            })\n",
    "            break\n",
    "\n",
    "print(f\"Total candidates: {len(all_candidates)}\")\n",
    "np.random.seed(SEED)\n",
    "indices = np.random.permutation(len(all_candidates))\n",
    "samples = [all_candidates[i] for i in indices[:N_SAMPLES]]\n",
    "del ds, all_candidates\n",
    "gc.collect()\n",
    "\n",
    "print(f\"Loaded {len(samples)} samples\")\n",
    "print(f\"First query: {samples[0]['query'][:60]}...\")\n",
    "\n",
    "# Verify alignment with Exp 02\n",
    "exp02_ckpt = Path(\"../../results/exp02/checkpoint.json\")\n",
    "if exp02_ckpt.exists():\n",
    "    ckpt02 = json.loads(exp02_ckpt.read_text())\n",
    "    if 'results' in ckpt02 and len(ckpt02['results']) > 0:\n",
    "        match = all(\n",
    "            r['query'][:50] == s['query'][:50]\n",
    "            for r, s in zip(ckpt02['results'][:10], samples[:10])\n",
    "        )\n",
    "        print(f\"Sample alignment with Exp 02: {'MATCH' if match else 'MISMATCH'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91389fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Load model and define scoring functions\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv())\n",
    "HF_TOKEN = os.environ.get(\"HF_TOKEN\")\n",
    "\n",
    "from transformers import AutoProcessor, AutoModelForSeq2SeqLM\n",
    "\n",
    "print(f\"Loading {MODEL_NAME}...\")\n",
    "processor = AutoProcessor.from_pretrained(MODEL_NAME, token=HF_TOKEN)\n",
    "tokenizer = processor.tokenizer\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    MODEL_NAME, device_map=\"auto\", torch_dtype=torch.bfloat16, token=HF_TOKEN,\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "DEVICE = next(model.parameters()).device\n",
    "print(f\"Model loaded. dtype={next(model.parameters()).dtype}\")\n",
    "print(f\"GPU memory: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "\n",
    "# Verify encoder accepts position_ids\n",
    "import inspect\n",
    "enc_sig = inspect.signature(model.get_encoder().forward)\n",
    "assert 'position_ids' in enc_sig.parameters, \"Encoder does not accept position_ids!\"\n",
    "print(f\"Encoder accepts position_ids: YES\")\n",
    "\n",
    "# Get encoder layer types for attention mask dict\n",
    "encoder_text_model = model.model.encoder.text_model\n",
    "layer_types = set()\n",
    "for layer in encoder_text_model.layers:\n",
    "    layer_types.add(layer.attention_type)\n",
    "print(f\"Encoder layer types: {layer_types}\")\n",
    "\n",
    "\n",
    "def _decode_nll(encoder_outputs, cross_attn_mask, answer_text):\n",
    "    # Shared decoder scoring: NLL of answer given encoder output + mask.\n",
    "    ans_ids = tokenizer(answer_text, return_tensors=\"pt\",\n",
    "                        add_special_tokens=False, truncation=True,\n",
    "                        max_length=256).input_ids.to(DEVICE)\n",
    "    if ans_ids.shape[1] == 0:\n",
    "        return 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            encoder_outputs=encoder_outputs,\n",
    "            attention_mask=cross_attn_mask,\n",
    "            labels=ans_ids,\n",
    "        )\n",
    "\n",
    "    logits = outputs.logits\n",
    "    log_probs = F.log_softmax(logits, dim=-1)\n",
    "    token_log_probs = log_probs[0].gather(1, ans_ids[0].unsqueeze(1)).squeeze(1)\n",
    "    mean_nll = -token_log_probs.mean().item()\n",
    "    del outputs, logits, log_probs\n",
    "    return mean_nll\n",
    "\n",
    "\n",
    "def count_prefix_tokens(prefix_text, document_text):\n",
    "    # BPE-aware token count of prefix in [prefix + newline + document].\n",
    "    full_text = prefix_text + \"\\n\" + document_text\n",
    "    full_ids = tokenizer(full_text, add_special_tokens=True, truncation=True,\n",
    "                         max_length=2048).input_ids\n",
    "    doc_ids = tokenizer(document_text, add_special_tokens=True, truncation=True,\n",
    "                        max_length=2048).input_ids\n",
    "    return len(full_ids) - len(doc_ids)\n",
    "\n",
    "\n",
    "def score_bare(passage, answer):\n",
    "    # Condition 1: Standard bare encoding.\n",
    "    enc_ids = tokenizer(passage, return_tensors=\"pt\",\n",
    "                        add_special_tokens=True, truncation=True,\n",
    "                        max_length=2048).input_ids.to(DEVICE)\n",
    "    enc_mask = torch.ones(1, enc_ids.shape[1], device=DEVICE, dtype=torch.long)\n",
    "    with torch.no_grad():\n",
    "        encoder_outputs = model.get_encoder()(\n",
    "            input_ids=enc_ids, attention_mask=enc_mask\n",
    "        )\n",
    "    nll = _decode_nll(encoder_outputs, enc_mask, answer)\n",
    "    del encoder_outputs\n",
    "    return nll\n",
    "\n",
    "\n",
    "def score_prefix_trunc(prefix, passage, answer):\n",
    "    # Conditions 2-3: Standard prefix + truncation (oracle or random).\n",
    "    full_text = prefix + \"\\n\" + passage\n",
    "    enc_ids = tokenizer(full_text, return_tensors=\"pt\",\n",
    "                        add_special_tokens=True, truncation=True,\n",
    "                        max_length=2048).input_ids.to(DEVICE)\n",
    "    total_len = enc_ids.shape[1]\n",
    "    n_prefix = count_prefix_tokens(prefix, passage)\n",
    "    enc_mask = torch.ones(1, total_len, device=DEVICE, dtype=torch.long)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        encoder_outputs = model.get_encoder()(\n",
    "            input_ids=enc_ids, attention_mask=enc_mask\n",
    "        )\n",
    "\n",
    "    # Cross-attention mask: hide prefix from decoder\n",
    "    cross_mask = torch.ones(1, total_len, device=DEVICE, dtype=torch.long)\n",
    "    cross_mask[:, :n_prefix] = 0\n",
    "    nll = _decode_nll(encoder_outputs, cross_mask, answer)\n",
    "    del encoder_outputs\n",
    "    return nll\n",
    "\n",
    "\n",
    "def score_shifted_bare(passage, answer, offset=POSITION_OFFSET):\n",
    "    # Condition 4: Bare encoding with shifted position_ids.\n",
    "    # Document at positions [offset..offset+L-1] instead of [0..L-1].\n",
    "    # Tests pure absolute RoPE shift (no prefix tokens).\n",
    "    enc_ids = tokenizer(passage, return_tensors=\"pt\",\n",
    "                        add_special_tokens=True, truncation=True,\n",
    "                        max_length=2048).input_ids.to(DEVICE)\n",
    "    seq_len = enc_ids.shape[1]\n",
    "    enc_mask = torch.ones(1, seq_len, device=DEVICE, dtype=torch.long)\n",
    "    position_ids = torch.arange(offset, offset + seq_len,\n",
    "                                device=DEVICE).unsqueeze(0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        encoder_outputs = model.get_encoder()(\n",
    "            input_ids=enc_ids, attention_mask=enc_mask,\n",
    "            position_ids=position_ids,\n",
    "        )\n",
    "\n",
    "    nll = _decode_nll(encoder_outputs, enc_mask, answer)\n",
    "    del encoder_outputs\n",
    "    return nll\n",
    "\n",
    "\n",
    "def score_prefix_encoder_blocked(prefix, passage, answer):\n",
    "    # Condition 5: Prefix in sequence but blocked from encoder attention.\n",
    "    # Doc tokens cannot attend to prefix tokens. Positions ARE shifted.\n",
    "    # Tests: does RoPE shift + invisible prefix do anything?\n",
    "    full_text = prefix + \"\\n\" + passage\n",
    "    enc_ids = tokenizer(full_text, return_tensors=\"pt\",\n",
    "                        add_special_tokens=True, truncation=True,\n",
    "                        max_length=2048).input_ids.to(DEVICE)\n",
    "    total_len = enc_ids.shape[1]\n",
    "    n_prefix = count_prefix_tokens(prefix, passage)\n",
    "\n",
    "    # Build 4D additive attention mask: doc tokens cannot attend to prefix\n",
    "    # Shape: [1, 1, total_len, total_len], dtype matching model\n",
    "    # 0.0 = allowed, large negative = blocked\n",
    "    min_val = torch.finfo(torch.bfloat16).min\n",
    "    mask_4d = torch.zeros(1, 1, total_len, total_len,\n",
    "                          dtype=torch.bfloat16, device=DEVICE)\n",
    "    # Block: doc rows [n_prefix:] attending to prefix cols [:n_prefix]\n",
    "    mask_4d[:, :, n_prefix:, :n_prefix] = min_val\n",
    "\n",
    "    # Create dict for both layer types (sequences are short enough that\n",
    "    # sliding window doesn't actually mask anything)\n",
    "    encoder_mask = {\n",
    "        \"full_attention\": mask_4d,\n",
    "        \"sliding_attention\": mask_4d.clone(),\n",
    "    }\n",
    "\n",
    "    with torch.no_grad():\n",
    "        encoder_outputs = model.get_encoder()(\n",
    "            input_ids=enc_ids, attention_mask=encoder_mask,\n",
    "        )\n",
    "\n",
    "    # Cross-attention: also mask prefix from decoder\n",
    "    cross_mask = torch.ones(1, total_len, device=DEVICE, dtype=torch.long)\n",
    "    cross_mask[:, :n_prefix] = 0\n",
    "    nll = _decode_nll(encoder_outputs, cross_mask, answer)\n",
    "    del encoder_outputs, mask_4d, encoder_mask\n",
    "    return nll\n",
    "\n",
    "\n",
    "def score_prefix_rope_neutralized(prefix, passage, answer):\n",
    "    # Condition 6: Prefix in attention but doc positions NOT shifted.\n",
    "    # position_ids: prefix at [L..L+N-1], doc at [0..L-1].\n",
    "    # Tests: does attention redistribution work without RoPE position shift?\n",
    "    full_text = prefix + \"\\n\" + passage\n",
    "    enc_ids = tokenizer(full_text, return_tensors=\"pt\",\n",
    "                        add_special_tokens=True, truncation=True,\n",
    "                        max_length=2048).input_ids.to(DEVICE)\n",
    "    total_len = enc_ids.shape[1]\n",
    "    n_prefix = count_prefix_tokens(prefix, passage)\n",
    "    doc_len = total_len - n_prefix\n",
    "\n",
    "    # Custom position_ids: prefix at [doc_len .. doc_len+n_prefix-1],\n",
    "    # doc at [0 .. doc_len-1]\n",
    "    prefix_positions = torch.arange(doc_len, doc_len + n_prefix, device=DEVICE)\n",
    "    doc_positions = torch.arange(0, doc_len, device=DEVICE)\n",
    "    position_ids = torch.cat([prefix_positions, doc_positions]).unsqueeze(0)\n",
    "\n",
    "    enc_mask = torch.ones(1, total_len, device=DEVICE, dtype=torch.long)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        encoder_outputs = model.get_encoder()(\n",
    "            input_ids=enc_ids, attention_mask=enc_mask,\n",
    "            position_ids=position_ids,\n",
    "        )\n",
    "\n",
    "    # Cross-attention: mask prefix from decoder\n",
    "    cross_mask = torch.ones(1, total_len, device=DEVICE, dtype=torch.long)\n",
    "    cross_mask[:, :n_prefix] = 0\n",
    "    nll = _decode_nll(encoder_outputs, cross_mask, answer)\n",
    "    del encoder_outputs\n",
    "    return nll\n",
    "\n",
    "\n",
    "print(\"All scoring functions defined.\")\n",
    "\n",
    "# Quick sanity check on a single sample\n",
    "s = samples[0]\n",
    "nll_bare = score_bare(s['passage'], s['answer'])\n",
    "nll_shift = score_shifted_bare(s['passage'], s['answer'])\n",
    "print(f\"\\nSanity check (sample 0):\")\n",
    "print(f\"  bare NLL:          {nll_bare:.6f}\")\n",
    "print(f\"  shifted_bare NLL:  {nll_shift:.6f}\")\n",
    "print(f\"  difference:        {abs(nll_bare - nll_shift):.6f}\")\n",
    "if abs(nll_bare - nll_shift) < 0.01:\n",
    "    print(\"  -> Very close (consistent with RoPE being relative)\")\n",
    "else:\n",
    "    print(\"  -> DIFFERENT (absolute position matters!)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d86753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Generate random prefixes for each sample\n",
    "# Use the same random prefix generation as Exp 02/2B\n",
    "\n",
    "for i, s in enumerate(samples):\n",
    "    query = s['query']\n",
    "    query_words_list = query.split()\n",
    "\n",
    "    # Random prefix from unrelated passage, matched to query length\n",
    "    other_idx = (i + N_SAMPLES // 2) % len(samples)\n",
    "    other_words = samples[other_idx]['passage'].split()\n",
    "    random_matched = \" \".join(other_words[:len(query_words_list)])\n",
    "\n",
    "    s['oracle'] = query\n",
    "    s['random_matched'] = random_matched\n",
    "\n",
    "    # Count tokens for reference\n",
    "    s['n_prefix_oracle'] = count_prefix_tokens(query, s['passage'])\n",
    "    s['n_prefix_random'] = count_prefix_tokens(random_matched, s['passage'])\n",
    "\n",
    "COND_NAMES = [\n",
    "    'bare',\n",
    "    'oracle_trunc',\n",
    "    'random_trunc',\n",
    "    'shifted_bare',\n",
    "    'prefix_encoder_blocked',\n",
    "    'random_rope_neutralized',\n",
    "]\n",
    "\n",
    "print(f\"Conditions ({len(COND_NAMES)}):\")\n",
    "for c in COND_NAMES:\n",
    "    print(f\"  {c}\")\n",
    "\n",
    "# Show example\n",
    "ex = samples[0]\n",
    "print(f\"\\nExample (sample 0):\")\n",
    "print(f\"  Query: {ex['query']}\")\n",
    "print(f\"  Answer: {ex['answer']}\")\n",
    "print(f\"  Passage ({ex['word_count']}w): {ex['passage'][:80]}...\")\n",
    "print(f\"  Random prefix: {ex['random_matched']}\")\n",
    "print(f\"  Oracle prefix tokens: {ex['n_prefix_oracle']}, \"\n",
    "      f\"Random prefix tokens: {ex['n_prefix_random']}\")\n",
    "\n",
    "# Show what each condition actually does for sample 0\n",
    "n_prefix = ex['n_prefix_random']\n",
    "doc_len = len(tokenizer(ex['passage'], add_special_tokens=True).input_ids)\n",
    "print(f\"\\n--- What each condition does (sample 0) ---\")\n",
    "print(f\"  {'Condition':<28} {'Encoder text':<22} {'position_ids':<24} {'Decoder sees':<14}\")\n",
    "print(f\"  {'-'*90}\")\n",
    "print(f\"  {'bare':<28} {'[doc]':<22} {'[0..{0}]':<24} {'all':<14}\".format(doc_len-1))\n",
    "print(f\"  {'oracle_trunc':<28} {'[query + doc]':<22} {'[0..{0}] (default)':<24} {'doc only':<14}\".format(doc_len+n_prefix-1))\n",
    "print(f\"  {'random_trunc':<28} {'[random + doc]':<22} {'[0..{0}] (default)':<24} {'doc only':<14}\".format(doc_len+n_prefix-1))\n",
    "print(f\"  {'shifted_bare':<28} {'[doc]':<22} {'[{0}..{1}] (shifted)':<24} {'all':<14}\".format(n_prefix, n_prefix+doc_len-1))\n",
    "print(f\"  {'prefix_encoder_blocked':<28} {'[random + doc]':<22} {'[0..{0}] (default)':<24} {'doc only':<14}\".format(doc_len+n_prefix-1))\n",
    "print(f\"  {'':<28} {'  doc CANNOT attend':<22} {'':<24} {'':<14}\")\n",
    "print(f\"  {'':<28} {'  to prefix in encoder':<22} {'':<24} {'':<14}\")\n",
    "print(f\"  {'random_rope_neutralized':<28} {'[random + doc]':<22} {'prefix@[{0}..{1}],':<24} {'doc only':<14}\".format(doc_len, doc_len+n_prefix-1))\n",
    "print(f\"  {'':<28} {'':<22} {'doc@[0..{0}]':<24} {'':<14}\".format(doc_len-1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf79132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Scoring loop with checkpointing\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"SCORING ALL CONDITIONS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "results = []\n",
    "start_idx = 0\n",
    "\n",
    "if CHECKPOINT_PATH.exists():\n",
    "    ckpt = json.loads(CHECKPOINT_PATH.read_text())\n",
    "    if ckpt.get('n_total') == N_SAMPLES and len(ckpt.get('results', [])) > 0:\n",
    "        saved_queries = [r['query'][:50] for r in ckpt['results']]\n",
    "        current_queries = [s['query'][:50] for s in samples[:len(saved_queries)]]\n",
    "        if saved_queries == current_queries:\n",
    "            results = ckpt['results']\n",
    "            start_idx = len(results)\n",
    "            print(f\"Resuming from checkpoint: {start_idx}/{N_SAMPLES}\")\n",
    "\n",
    "if start_idx == 0:\n",
    "    print(f\"Starting fresh: {len(COND_NAMES)} conditions x {N_SAMPLES} samples \"\n",
    "          f\"= {len(COND_NAMES) * N_SAMPLES} scorings\")\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "for i in tqdm(range(start_idx, N_SAMPLES), initial=start_idx, total=N_SAMPLES,\n",
    "              desc=\"Scoring\"):\n",
    "    s = samples[i]\n",
    "    result = {\n",
    "        'query': s['query'],\n",
    "        'answer': s['answer'],\n",
    "        'passage_words': s['word_count'],\n",
    "    }\n",
    "\n",
    "    # Condition 1: bare\n",
    "    result['nll_bare'] = score_bare(s['passage'], s['answer'])\n",
    "\n",
    "    # Condition 2: oracle_trunc (standard)\n",
    "    result['nll_oracle_trunc'] = score_prefix_trunc(\n",
    "        s['oracle'], s['passage'], s['answer'])\n",
    "\n",
    "    # Condition 3: random_trunc (standard)\n",
    "    result['nll_random_trunc'] = score_prefix_trunc(\n",
    "        s['random_matched'], s['passage'], s['answer'])\n",
    "\n",
    "    # Condition 4: shifted_bare (pure RoPE shift)\n",
    "    result['nll_shifted_bare'] = score_shifted_bare(\n",
    "        s['passage'], s['answer'], offset=s['n_prefix_random'])\n",
    "\n",
    "    # Condition 5: prefix_encoder_blocked (RoPE shift + invisible prefix)\n",
    "    result['nll_prefix_encoder_blocked'] = score_prefix_encoder_blocked(\n",
    "        s['random_matched'], s['passage'], s['answer'])\n",
    "\n",
    "    # Condition 6: random_rope_neutralized (attention redistribution, no RoPE shift)\n",
    "    result['nll_random_rope_neutralized'] = score_prefix_rope_neutralized(\n",
    "        s['random_matched'], s['passage'], s['answer'])\n",
    "\n",
    "    results.append(result)\n",
    "\n",
    "    if (i + 1) % 20 == 0 or i == N_SAMPLES - 1:\n",
    "        ckpt = {\n",
    "            'n_total': N_SAMPLES,\n",
    "            'results': results,\n",
    "            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        }\n",
    "        CHECKPOINT_PATH.write_text(json.dumps(ckpt))\n",
    "        elapsed = time.time() - t0\n",
    "        done = i - start_idx + 1\n",
    "        eta = (N_SAMPLES - i - 1) * elapsed / done if done > 0 else 0\n",
    "        tqdm.write(f\"  Checkpoint {i+1}/{N_SAMPLES} | {elapsed/60:.1f}m | ETA {eta/60:.1f}m\")\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "elapsed = time.time() - t0\n",
    "print(f\"\\nScoring complete: {len(results)} samples, \"\n",
    "      f\"{len(COND_NAMES)} conditions in {elapsed/60:.1f} min\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64eeb8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Analysis — RoPE Isolation Results\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ROPE ISOLATION ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Extract arrays\n",
    "bare = np.array([r['nll_bare'] for r in results])\n",
    "oracle = np.array([r['nll_oracle_trunc'] for r in results])\n",
    "random_std = np.array([r['nll_random_trunc'] for r in results])\n",
    "shifted = np.array([r['nll_shifted_bare'] for r in results])\n",
    "blocked = np.array([r['nll_prefix_encoder_blocked'] for r in results])\n",
    "neutralized = np.array([r['nll_random_rope_neutralized'] for r in results])\n",
    "\n",
    "# Verify Exp 02 alignment\n",
    "print(f\"\\nBaseline verification:\")\n",
    "print(f\"  bare mean NLL:    {bare.mean():.6f}\")\n",
    "print(f\"  oracle mean NLL:  {oracle.mean():.6f}\")\n",
    "\n",
    "# ---- Test 1: shifted_bare vs bare (pure absolute RoPE) ----\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"TEST 1: Does absolute RoPE position shift matter?\")\n",
    "print(f\"  shifted_bare: doc at positions [N..N+L-1] instead of [0..L-1]\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "diff_shift = bare - shifted  # positive = shifted is better\n",
    "d_shift = cohens_d(diff_shift)\n",
    "_, p_shift = stats.ttest_1samp(diff_shift, 0)\n",
    "sig_shift = '***' if p_shift < 0.001 else '**' if p_shift < 0.01 else '*' if p_shift < 0.05 else 'ns'\n",
    "\n",
    "print(f\"  bare mean NLL:         {bare.mean():.6f}\")\n",
    "print(f\"  shifted_bare mean NLL: {shifted.mean():.6f}\")\n",
    "print(f\"  difference:            {diff_shift.mean():+.6f}\")\n",
    "print(f\"  Cohen's d:             {d_shift:+.4f} ({sig_shift})\")\n",
    "print(f\"  Max abs difference:    {np.abs(diff_shift).max():.6f}\")\n",
    "corr, _ = stats.pearsonr(bare, shifted)\n",
    "print(f\"  Correlation:           r={corr:.6f}\")\n",
    "\n",
    "if abs(d_shift) < 0.05:\n",
    "    print(f\"\\n  -> CONFIRMED: Absolute RoPE position shift has NO effect (d={d_shift:+.4f})\")\n",
    "    print(f\"     RoPE is purely relative — shifting all positions preserves doc-doc attention.\")\n",
    "else:\n",
    "    print(f\"\\n  -> UNEXPECTED: Position shift has an effect (d={d_shift:+.4f})\")\n",
    "\n",
    "# ---- Test 2: prefix_encoder_blocked vs bare (RoPE + invisible prefix) ----\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"TEST 2: Does invisible prefix + position shift matter?\")\n",
    "print(f\"  prefix is in sequence but doc can't attend to it\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "diff_blocked = bare - blocked  # positive = blocked is better\n",
    "d_blocked = cohens_d(diff_blocked)\n",
    "_, p_blocked = stats.ttest_1samp(diff_blocked, 0)\n",
    "sig_blocked = '***' if p_blocked < 0.001 else '**' if p_blocked < 0.01 else '*' if p_blocked < 0.05 else 'ns'\n",
    "\n",
    "print(f\"  bare mean NLL:                  {bare.mean():.6f}\")\n",
    "print(f\"  prefix_encoder_blocked mean NLL:{blocked.mean():.6f}\")\n",
    "print(f\"  difference:                     {diff_blocked.mean():+.6f}\")\n",
    "print(f\"  Cohen's d:                      {d_blocked:+.4f} ({sig_blocked})\")\n",
    "\n",
    "# Compare to shifted_bare — should be nearly identical\n",
    "diff_shift_vs_blocked = shifted - blocked\n",
    "d_sb = cohens_d(diff_shift_vs_blocked)\n",
    "print(f\"\\n  shifted_bare vs blocked: d={d_sb:+.4f} (should be ~0)\")\n",
    "\n",
    "if abs(d_blocked) < 0.05:\n",
    "    print(f\"\\n  -> CONFIRMED: Invisible prefix has NO effect beyond RoPE shift.\")\n",
    "    print(f\"     Prefix tokens must be attended to for benefit.\")\n",
    "else:\n",
    "    print(f\"\\n  -> UNEXPECTED: Invisible prefix has an effect (d={d_blocked:+.4f})\")\n",
    "\n",
    "# ---- Test 3: random_rope_neutralized vs random_trunc (attention only vs attention+RoPE) ----\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"TEST 3: Does neutralizing RoPE reduce the attention redistribution benefit?\")\n",
    "print(f\"  random_trunc: positions shifted (standard)\")\n",
    "print(f\"  random_rope_neutralized: doc keeps original positions\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "# Each vs bare\n",
    "diff_random = bare - random_std\n",
    "d_random = cohens_d(diff_random)\n",
    "_, p_random = stats.ttest_1samp(diff_random, 0)\n",
    "sig_random = '***' if p_random < 0.001 else '**' if p_random < 0.01 else '*' if p_random < 0.05 else 'ns'\n",
    "\n",
    "diff_neutral = bare - neutralized\n",
    "d_neutral = cohens_d(diff_neutral)\n",
    "_, p_neutral = stats.ttest_1samp(diff_neutral, 0)\n",
    "sig_neutral = '***' if p_neutral < 0.001 else '**' if p_neutral < 0.01 else '*' if p_neutral < 0.05 else 'ns'\n",
    "\n",
    "print(f\"  random_trunc vs bare:       d={d_random:+.4f} ({sig_random})\")\n",
    "print(f\"  rope_neutralized vs bare:   d={d_neutral:+.4f} ({sig_neutral})\")\n",
    "\n",
    "# Head-to-head\n",
    "diff_std_vs_neutral = neutralized - random_std  # positive = standard is better\n",
    "d_svn = cohens_d(diff_std_vs_neutral)\n",
    "_, p_svn = stats.ttest_1samp(diff_std_vs_neutral, 0)\n",
    "sig_svn = '***' if p_svn < 0.001 else '**' if p_svn < 0.01 else '*' if p_svn < 0.05 else 'ns'\n",
    "win_svn = np.mean(diff_std_vs_neutral > 0) * 100\n",
    "\n",
    "print(f\"\\n  Head-to-head (standard - neutralized):\")\n",
    "print(f\"    d={d_svn:+.4f}, win%={win_svn:.1f}%, p={p_svn:.2e} ({sig_svn})\")\n",
    "\n",
    "ratio = d_neutral / d_random * 100 if d_random != 0 else 0\n",
    "print(f\"\\n  Benefit ratio: neutralized/standard = {ratio:.1f}%\")\n",
    "\n",
    "if abs(d_svn) < 0.05:\n",
    "    print(f\"\\n  -> CONFIRMED: Neutralizing RoPE has NO effect on the benefit.\")\n",
    "    print(f\"     The benefit is entirely from attention redistribution.\")\n",
    "elif d_svn > 0.05:\n",
    "    print(f\"\\n  -> RoPE contributes: standard > neutralized by d={d_svn:+.4f}\")\n",
    "else:\n",
    "    print(f\"\\n  -> Neutralized is actually BETTER (d={d_svn:+.4f})\")\n",
    "\n",
    "# ---- Summary 2x2 Table ----\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"2x2 FACTORIAL RESULTS\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"\\n  {'':>30} | {'Prefix in attn':>16} | {'Prefix NOT in attn':>18}\")\n",
    "print(f\"  {'':>30} | {'(redistribution)':>16} | {'(no redistribution)':>18}\")\n",
    "print(f\"  {'-'*70}\")\n",
    "\n",
    "d_oracle = cohens_d(bare - oracle)\n",
    "print(f\"  {'RoPE shift ON':>30} | d={d_random:>+.3f} (***) | d={d_blocked:>+.3f} ({sig_blocked})\")\n",
    "print(f\"  {'RoPE shift OFF':>30} | d={d_neutral:>+.3f} ({sig_neutral}) | d=+0.000 (baseline)\")\n",
    "\n",
    "print(f\"\\n  Oracle reference: d={d_oracle:+.3f}\")\n",
    "\n",
    "# Row effect (prefix in attention matters?)\n",
    "row_effect = d_random - d_blocked\n",
    "# Column effect (RoPE shift matters?)\n",
    "col_effect = d_random - d_neutral\n",
    "\n",
    "print(f\"\\n  ROW effect (attention redistribution): {row_effect:+.3f}\")\n",
    "print(f\"  COLUMN effect (RoPE position shift):  {col_effect:+.3f}\")\n",
    "\n",
    "if abs(col_effect) < 0.05 and row_effect > 0.1:\n",
    "    print(f\"\\n  CONCLUSION: Benefit is ENTIRELY from attention redistribution.\")\n",
    "    print(f\"  RoPE position shifts contribute NOTHING to the structural mechanism.\")\n",
    "elif abs(col_effect) > abs(row_effect):\n",
    "    print(f\"\\n  CONCLUSION: RoPE is the dominant mechanism (unexpected!).\")\n",
    "else:\n",
    "    print(f\"\\n  CONCLUSION: Both contribute, but attention redistribution \"\n",
    "          f\"is {row_effect/col_effect:.1f}x larger.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82153a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Save results\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"FINAL SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Compute all d values\n",
    "d_oracle = cohens_d(bare - oracle)\n",
    "d_random = cohens_d(bare - random_std)\n",
    "d_shifted = cohens_d(bare - shifted)\n",
    "d_blocked = cohens_d(bare - blocked)\n",
    "d_neutralized = cohens_d(bare - neutralized)\n",
    "\n",
    "print(f\"\\n  {'Condition':<35} {'d vs bare':>10} {'NLL':>10} {'Interpretation':>30}\")\n",
    "print(f\"  {'-'*90}\")\n",
    "print(f\"  {'oracle_trunc':<35} {d_oracle:>+10.4f} {oracle.mean():>10.4f} {'upper bound':>30}\")\n",
    "print(f\"  {'random_trunc (standard)':<35} {d_random:>+10.4f} {random_std.mean():>10.4f} {'attention + RoPE':>30}\")\n",
    "print(f\"  {'random_rope_neutralized':<35} {d_neutralized:>+10.4f} {neutralized.mean():>10.4f} {'attention only':>30}\")\n",
    "print(f\"  {'shifted_bare':<35} {d_shifted:>+10.4f} {shifted.mean():>10.4f} {'RoPE only':>30}\")\n",
    "print(f\"  {'prefix_encoder_blocked':<35} {d_blocked:>+10.4f} {blocked.mean():>10.4f} {'RoPE + invisible prefix':>30}\")\n",
    "print(f\"  {'bare':<35} {'baseline':>10} {bare.mean():>10.4f} {'baseline':>30}\")\n",
    "\n",
    "_, p_shift = stats.ttest_1samp(bare - shifted, 0)\n",
    "_, p_blocked = stats.ttest_1samp(bare - blocked, 0)\n",
    "_, p_neutral = stats.ttest_1samp(bare - neutralized, 0)\n",
    "_, p_random = stats.ttest_1samp(bare - random_std, 0)\n",
    "diff_svn = neutralized - random_std\n",
    "_, p_svn = stats.ttest_1samp(diff_svn, 0)\n",
    "d_svn = cohens_d(diff_svn)\n",
    "\n",
    "rope_contributes = abs(d_shifted) > 0.05 and p_shift < 0.05\n",
    "attn_contributes = d_random > 0.1\n",
    "rope_amplifies = abs(d_svn) > 0.05 and p_svn < 0.05\n",
    "\n",
    "final_results = {\n",
    "    'experiment': 'exp07_rope_isolation',\n",
    "    'model': MODEL_NAME,\n",
    "    'dataset': 'ms_marco_v1.1',\n",
    "    'n_samples': N_SAMPLES,\n",
    "    'position_offset': POSITION_OFFSET,\n",
    "    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'conditions': {\n",
    "        'bare': {'d': 0.0, 'mean_nll': float(bare.mean())},\n",
    "        'oracle_trunc': {\n",
    "            'd': float(d_oracle), 'mean_nll': float(oracle.mean()),\n",
    "            'p': float(stats.ttest_1samp(bare - oracle, 0)[1]),\n",
    "        },\n",
    "        'random_trunc': {\n",
    "            'd': float(d_random), 'mean_nll': float(random_std.mean()),\n",
    "            'p': float(p_random),\n",
    "            'description': 'Standard: attention redistribution + RoPE shift',\n",
    "        },\n",
    "        'shifted_bare': {\n",
    "            'd': float(d_shifted), 'mean_nll': float(shifted.mean()),\n",
    "            'p': float(p_shift),\n",
    "            'description': 'Pure absolute RoPE shift, no prefix',\n",
    "        },\n",
    "        'prefix_encoder_blocked': {\n",
    "            'd': float(d_blocked), 'mean_nll': float(blocked.mean()),\n",
    "            'p': float(p_blocked),\n",
    "            'description': 'RoPE shift + invisible prefix in encoder',\n",
    "        },\n",
    "        'random_rope_neutralized': {\n",
    "            'd': float(d_neutralized), 'mean_nll': float(neutralized.mean()),\n",
    "            'p': float(p_neutral),\n",
    "            'description': 'Attention redistribution only, doc positions preserved',\n",
    "        },\n",
    "    },\n",
    "    'factorial': {\n",
    "        'row_effect_attention': float(d_random - d_blocked),\n",
    "        'col_effect_rope': float(d_random - d_neutralized),\n",
    "        'standard_vs_neutralized_d': float(d_svn),\n",
    "        'standard_vs_neutralized_p': float(p_svn),\n",
    "    },\n",
    "    'conclusion': {\n",
    "        'rope_contributes': bool(rope_contributes),\n",
    "        'attention_contributes': bool(attn_contributes),\n",
    "        'rope_amplifies_attention': bool(rope_amplifies),\n",
    "        'primary_mechanism': 'attention_redistribution' if not rope_contributes else 'both',\n",
    "    },\n",
    "}\n",
    "\n",
    "with open(RESULTS_DIR / 'results.json', 'w') as f:\n",
    "    json.dump(final_results, f, indent=2)\n",
    "print(f\"\\nResults saved to {RESULTS_DIR / 'results.json'}\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "if not rope_contributes and attn_contributes:\n",
    "    print(\"CONCLUSION: RoPE position shifts are RULED OUT.\")\n",
    "    print(\"The structural benefit comes ENTIRELY from attention redistribution.\")\n",
    "    print(\"Prepending a prefix adds new attention targets that reorganize\")\n",
    "    print(\"doc-doc attention patterns. The specific positions don't matter.\")\n",
    "elif rope_contributes:\n",
    "    print(\"CONCLUSION: RoPE position shifts DO contribute to the mechanism.\")\n",
    "    print(\"This needs further investigation.\")\n",
    "else:\n",
    "    print(\"CONCLUSION: Neither mechanism shows strong effects (unexpected).\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "# Cleanup\n",
    "print(f\"\\nCleaning up GPU memory...\")\n",
    "mem_before = torch.cuda.memory_allocated() / 1e9\n",
    "del model, processor, tokenizer\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "mem_after = torch.cuda.memory_allocated() / 1e9\n",
    "print(f\"GPU memory: {mem_before:.2f} GB -> {mem_after:.2f} GB\")\n",
    "print(\"Done!\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
