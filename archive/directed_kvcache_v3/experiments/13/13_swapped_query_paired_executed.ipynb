{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "913d1c8e",
   "metadata": {},
   "source": [
    "# Experiment 13: Swapped-Query Paired Contrasts\n",
    "\n",
    "## Motivation\n",
    "\n",
    "Exp 2B showed ~85% of the encoder priming benefit is structural, ~10% semantic.\n",
    "But measuring the semantic component precisely is confounded by vocabulary and\n",
    "length differences between conditions. This experiment provides the **purest\n",
    "possible test** of semantic relevance by comparing oracle vs swapped query on the\n",
    "**same document** — same structural perturbation, same prefix format, only semantic\n",
    "relevance differs.\n",
    "\n",
    "## Design\n",
    "\n",
    "For each (query, document, answer) triple, we score the answer NLL under two\n",
    "prefix conditions:\n",
    "- `oracle_trunc`: the real query (semantically relevant)\n",
    "- `swapped_trunc`: a query from a completely different sample (semantically irrelevant)\n",
    "\n",
    "The per-document paired contrast `swapped_nll - oracle_nll` isolates the semantic\n",
    "component with maximum statistical power.\n",
    "\n",
    "## Conditions (4)\n",
    "\n",
    "| # | Condition | Prefix |\n",
    "|---|-----------|--------|\n",
    "| 1 | `bare` | (none) |\n",
    "| 2 | `oracle_trunc` | real query |\n",
    "| 3 | `swapped_trunc` | query from sample (i + N//2) % N |\n",
    "| 4 | `random_matched_trunc` | words from random passage |\n",
    "\n",
    "## Analysis\n",
    "\n",
    "1. Standard condition table\n",
    "2. Paired semantic contrast (the key test)\n",
    "3. Effect distribution — per-sample histogram\n",
    "4. Predictors of semantic benefit\n",
    "5. Structural equivalence check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a223167a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-20T02:02:25.015726Z",
     "iopub.status.busy": "2026-02-20T02:02:25.015189Z",
     "iopub.status.idle": "2026-02-20T02:02:28.507685Z",
     "shell.execute_reply": "2026-02-20T02:02:28.506713Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exp 13: Swapped-Query Paired Contrasts\n",
      "N: 500, SEED: 43\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Setup\n",
    "import os\n",
    "os.umask(0o000)\n",
    "\n",
    "import sys, json, time, re, gc, random as pyrandom\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from scipy import stats\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "sys.path.insert(0, \"../..\")\n",
    "from lib.analysis import cohens_d\n",
    "\n",
    "SEED = 43  # Different seed from Exp 12 (42) for independent samples\n",
    "N_SAMPLES = 500\n",
    "MODEL_NAME = \"google/t5gemma-2-4b-4b\"\n",
    "\n",
    "RESULTS_DIR = Path(\"../../results/exp13\")\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CHECKPOINT_PATH = RESULTS_DIR / \"checkpoint.json\"\n",
    "\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "pyrandom.seed(SEED)\n",
    "\n",
    "print(\"Exp 13: Swapped-Query Paired Contrasts\")\n",
    "print(f\"N: {N_SAMPLES}, SEED: {SEED}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e268ad66",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-20T02:02:28.511486Z",
     "iopub.status.busy": "2026-02-20T02:02:28.510816Z",
     "iopub.status.idle": "2026-02-20T02:02:31.032595Z",
     "shell.execute_reply": "2026-02-20T02:02:31.031607Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MS MARCO...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 500 samples\n",
      "Document lengths: 30-157 words, mean=74\n",
      "Query lengths: 2-15 words, mean=6.1\n",
      "\n",
      "Swapped query assignment:\n",
      "  Sample 0: 'where was rev david cooper of unst brought up...'\n",
      "    Swapped: 'what are amalgam filling made of...'\n",
      "\n",
      "  Sample 1: 'why is genetic engineering bad for humans...'\n",
      "    Swapped: 'what causes radioactive pollution...'\n",
      "\n",
      "  Sample 2: 'negative side effects of lumigan...'\n",
      "    Swapped: 'is albufeira safe...'\n",
      "\n",
      "  Sample 3: 'what are isotopes of an element...'\n",
      "    Swapped: 'what is the cause of HPV...'\n",
      "\n",
      "  Sample 4: 'bad gasoline how to clean carburetor...'\n",
      "    Swapped: 'who is jake mason...'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Load MS MARCO and select samples\n",
    "from lib.data import count_words\n",
    "from datasets import load_dataset\n",
    "\n",
    "print(\"Loading MS MARCO...\")\n",
    "ds = load_dataset(\"microsoft/ms_marco\", \"v1.1\", split=\"validation\")\n",
    "\n",
    "samples = []\n",
    "for item in ds:\n",
    "    if len(samples) >= N_SAMPLES * 3:\n",
    "        break\n",
    "    passages = item.get('passages', {})\n",
    "    ptexts = passages.get('passage_text', [])\n",
    "    is_sel = passages.get('is_selected', [])\n",
    "    query = item.get('query', '')\n",
    "    answers = item.get('answers', [])\n",
    "    well_formed = item.get('wellFormedAnswers', [])\n",
    "    answer = None\n",
    "    if well_formed and len(well_formed) > 0 and well_formed[0] not in ('[]', ''):\n",
    "        answer = well_formed[0]\n",
    "    elif answers and len(answers) > 0 and answers[0] != 'No Answer Present.':\n",
    "        answer = answers[0]\n",
    "    if not answer:\n",
    "        continue\n",
    "    for pt, sel in zip(ptexts, is_sel):\n",
    "        wc = count_words(pt)\n",
    "        if sel == 1 and 30 <= wc <= 300:\n",
    "            samples.append({\n",
    "                'passage': pt, 'query': query, 'answer': answer,\n",
    "                'word_count': wc,\n",
    "            })\n",
    "            break\n",
    "\n",
    "np.random.seed(SEED)\n",
    "np.random.shuffle(samples)\n",
    "samples = samples[:N_SAMPLES]\n",
    "del ds\n",
    "gc.collect()\n",
    "\n",
    "passage_words = np.array([s['word_count'] for s in samples])\n",
    "query_words = np.array([len(s['query'].split()) for s in samples])\n",
    "print(f\"Selected {N_SAMPLES} samples\")\n",
    "print(f\"Document lengths: {passage_words.min()}-{passage_words.max()} words, \"\n",
    "      f\"mean={passage_words.mean():.0f}\")\n",
    "print(f\"Query lengths: {query_words.min()}-{query_words.max()} words, \"\n",
    "      f\"mean={query_words.mean():.1f}\")\n",
    "\n",
    "# Verify swapped queries are from different topics\n",
    "print(f\"\\nSwapped query assignment:\")\n",
    "for i in range(5):\n",
    "    j = (i + N_SAMPLES // 2) % N_SAMPLES\n",
    "    print(f\"  Sample {i}: '{samples[i]['query'][:50]}...'\")\n",
    "    print(f\"    Swapped: '{samples[j]['query'][:50]}...'\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22eb23e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-20T02:02:31.036304Z",
     "iopub.status.busy": "2026-02-20T02:02:31.035903Z",
     "iopub.status.idle": "2026-02-20T02:03:42.688225Z",
     "shell.execute_reply": "2026-02-20T02:03:42.687210Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading google/t5gemma-2-4b-4b...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b45d087eb2e4aa2a349a17ac3406986",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/1327 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded. dtype=torch.bfloat16\n",
      "GPU memory: 15.02 GB\n",
      "Helpers defined.\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Load model and define scoring helpers\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv())\n",
    "HF_TOKEN = os.environ.get(\"HF_TOKEN\")\n",
    "\n",
    "from transformers import AutoProcessor, AutoModelForSeq2SeqLM\n",
    "\n",
    "print(f\"Loading {MODEL_NAME}...\")\n",
    "processor = AutoProcessor.from_pretrained(MODEL_NAME, token=HF_TOKEN)\n",
    "tokenizer = processor.tokenizer\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    MODEL_NAME, device_map=\"auto\", torch_dtype=torch.bfloat16, token=HF_TOKEN,\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "DEVICE = next(model.parameters()).device\n",
    "print(f\"Model loaded. dtype={next(model.parameters()).dtype}\")\n",
    "print(f\"GPU memory: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "\n",
    "\n",
    "def score_nll(encoder_text, answer_text, prefix_token_count=0, truncate=False):\n",
    "    # Score NLL of answer given encoder text, with optional prefix truncation.\n",
    "    enc_ids = tokenizer(encoder_text, return_tensors=\"pt\",\n",
    "                        add_special_tokens=True, truncation=True,\n",
    "                        max_length=2048).input_ids.to(DEVICE)\n",
    "    total_enc_len = enc_ids.shape[1]\n",
    "    enc_mask = torch.ones(1, total_enc_len, device=DEVICE, dtype=torch.long)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        encoder_outputs = model.get_encoder()(\n",
    "            input_ids=enc_ids, attention_mask=enc_mask\n",
    "        )\n",
    "\n",
    "    if truncate and prefix_token_count > 0:\n",
    "        cross_attn_mask = torch.ones(1, total_enc_len, device=DEVICE, dtype=torch.long)\n",
    "        cross_attn_mask[:, :prefix_token_count] = 0\n",
    "    else:\n",
    "        cross_attn_mask = torch.ones(1, total_enc_len, device=DEVICE, dtype=torch.long)\n",
    "\n",
    "    ans_ids = tokenizer(answer_text, return_tensors=\"pt\",\n",
    "                        add_special_tokens=False, truncation=True,\n",
    "                        max_length=256).input_ids.to(DEVICE)\n",
    "    if ans_ids.shape[1] == 0:\n",
    "        return 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            encoder_outputs=encoder_outputs,\n",
    "            attention_mask=cross_attn_mask,\n",
    "            labels=ans_ids,\n",
    "        )\n",
    "\n",
    "    logits = outputs.logits\n",
    "    log_probs = F.log_softmax(logits, dim=-1)\n",
    "    token_log_probs = log_probs[0].gather(1, ans_ids[0].unsqueeze(1)).squeeze(1)\n",
    "    mean_nll = -token_log_probs.mean().item()\n",
    "\n",
    "    del encoder_outputs, outputs, logits, log_probs\n",
    "    return mean_nll\n",
    "\n",
    "\n",
    "def count_prefix_tokens(prefix_text, document_text):\n",
    "    # Count how many tokens the prefix occupies in [prefix + newline + document].\n",
    "    full_text = prefix_text + \"\\n\" + document_text\n",
    "    full_ids = tokenizer(full_text, add_special_tokens=True, truncation=True,\n",
    "                         max_length=2048).input_ids\n",
    "    doc_ids = tokenizer(document_text, add_special_tokens=True, truncation=True,\n",
    "                        max_length=2048).input_ids\n",
    "    return len(full_ids) - len(doc_ids)\n",
    "\n",
    "\n",
    "STOP_WORDS = {\n",
    "    'a', 'an', 'the', 'is', 'are', 'was', 'were', 'be', 'been', 'being',\n",
    "    'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could',\n",
    "    'should', 'may', 'might', 'can', 'shall', 'to', 'of', 'in', 'for',\n",
    "    'on', 'with', 'at', 'by', 'from', 'as', 'into', 'through', 'during',\n",
    "    'before', 'after', 'above', 'below', 'between', 'and', 'but', 'or',\n",
    "    'not', 'no', 'if', 'then', 'than', 'so', 'up', 'out', 'about',\n",
    "    'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those',\n",
    "    'it', 'its', 'i', 'me', 'my', 'we', 'our', 'you', 'your', 'he',\n",
    "    'him', 'his', 'she', 'her', 'they', 'them', 'their', 'how', 'when',\n",
    "    'where', 'why', 'much', 'many', 'some', 'any', 'all', 'each',\n",
    "    'does', 'also', 'just', 'more', 'most', 'very', 'too', 'only',\n",
    "}\n",
    "\n",
    "print(\"Helpers defined.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7529d2b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-20T02:03:42.694114Z",
     "iopub.status.busy": "2026-02-20T02:03:42.693001Z",
     "iopub.status.idle": "2026-02-20T02:03:42.716036Z",
     "shell.execute_reply": "2026-02-20T02:03:42.715164Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conditions (4):\n",
      "  bare\n",
      "  oracle_trunc\n",
      "  swapped_trunc\n",
      "  random_matched_trunc\n",
      "\n",
      "Example (sample 0):\n",
      "  Query:   where was rev david cooper of unst brought up\n",
      "  Answer:  County Durham\n",
      "  Passage: Whalsay has the largest population with approximately 1,000 residents, followed ...\n",
      "  bare                        : [document only]\n",
      "  oracle_trunc                 ( 10 toks): where was rev david cooper of unst brought up\n",
      "  swapped_trunc                (  7 toks): what are amalgam filling made of\n",
      "  random_matched_trunc         ( 13 toks): Most people recognize dental amalgams as silver filling\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Generate all 4 scoring conditions per sample\n",
    "\n",
    "for i, s in enumerate(samples):\n",
    "    query = s['query']\n",
    "    passage = s['passage']\n",
    "    query_words_list = query.split()\n",
    "    n_query_words = len(query_words_list)\n",
    "\n",
    "    # Swapped query: query from a distant sample (guaranteed different topic)\n",
    "    swapped_idx = (i + N_SAMPLES // 2) % N_SAMPLES\n",
    "    s['swapped'] = samples[swapped_idx]['query']\n",
    "\n",
    "    # Random matched: words from unrelated passage, same word count as oracle\n",
    "    other_idx = (i + N_SAMPLES // 2) % N_SAMPLES\n",
    "    other_words = samples[other_idx]['passage'].split()\n",
    "    s['random_matched'] = \" \".join(other_words[:n_query_words])\n",
    "\n",
    "    # Oracle (just the query)\n",
    "    s['oracle'] = query\n",
    "\n",
    "COND_NAMES = [\n",
    "    'bare',\n",
    "    'oracle_trunc',\n",
    "    'swapped_trunc',\n",
    "    'random_matched_trunc',\n",
    "]\n",
    "\n",
    "print(f\"Conditions ({len(COND_NAMES)}):\")\n",
    "for c in COND_NAMES:\n",
    "    print(f\"  {c}\")\n",
    "\n",
    "# Show example\n",
    "ex = samples[0]\n",
    "print(f\"\\nExample (sample 0):\")\n",
    "print(f\"  Query:   {ex['query']}\")\n",
    "print(f\"  Answer:  {ex['answer'][:80]}\")\n",
    "print(f\"  Passage: {ex['passage'][:80]}...\")\n",
    "for c in COND_NAMES:\n",
    "    if c == 'bare':\n",
    "        print(f\"  {c:<28}: [document only]\")\n",
    "    else:\n",
    "        key = c.replace('_trunc', '')\n",
    "        text = ex[key]\n",
    "        ptoks = count_prefix_tokens(text, ex['passage'])\n",
    "        print(f\"  {c:<28} ({ptoks:>3} toks): {str(text)[:55]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33250ba8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-20T02:03:42.719701Z",
     "iopub.status.busy": "2026-02-20T02:03:42.718909Z",
     "iopub.status.idle": "2026-02-20T02:12:03.176936Z",
     "shell.execute_reply": "2026-02-20T02:12:03.176069Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "SCORING ALL CONDITIONS\n",
      "======================================================================\n",
      "Starting fresh: 4 conditions x 500 samples = 2000 scorings\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc32b2aaae214e1098e28afb5e9e99dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Scoring:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 20/500 | 0.3m | ETA 8.3m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 40/500 | 0.7m | ETA 7.8m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 60/500 | 1.0m | ETA 7.4m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 80/500 | 1.3m | ETA 7.0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 100/500 | 1.7m | ETA 6.7m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 120/500 | 2.0m | ETA 6.3m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 140/500 | 2.3m | ETA 6.0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 160/500 | 2.7m | ETA 5.6m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 180/500 | 3.0m | ETA 5.3m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 200/500 | 3.3m | ETA 5.0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 220/500 | 3.6m | ETA 4.6m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 240/500 | 4.0m | ETA 4.3m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 260/500 | 4.3m | ETA 4.0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 280/500 | 4.6m | ETA 3.6m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 300/500 | 5.0m | ETA 3.3m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 320/500 | 5.3m | ETA 3.0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 340/500 | 5.7m | ETA 2.7m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 360/500 | 6.0m | ETA 2.3m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 380/500 | 6.4m | ETA 2.0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 400/500 | 6.7m | ETA 1.7m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 420/500 | 7.0m | ETA 1.3m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 440/500 | 7.3m | ETA 1.0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 460/500 | 7.7m | ETA 0.7m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 480/500 | 8.0m | ETA 0.3m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 500/500 | 8.3m | ETA 0.0m\n",
      "\n",
      "Scoring complete: 500 samples, 4 conditions in 8.3 min\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Scoring loop with checkpointing\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"SCORING ALL CONDITIONS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "results = []\n",
    "start_idx = 0\n",
    "\n",
    "if CHECKPOINT_PATH.exists():\n",
    "    ckpt = json.loads(CHECKPOINT_PATH.read_text())\n",
    "    if ckpt.get('n_total') == N_SAMPLES and len(ckpt.get('results', [])) > 0:\n",
    "        saved_queries = [r['query'][:50] for r in ckpt['results']]\n",
    "        current_queries = [s['query'][:50] for s in samples[:len(saved_queries)]]\n",
    "        if saved_queries == current_queries:\n",
    "            results = ckpt['results']\n",
    "            start_idx = len(results)\n",
    "            print(f\"Resuming from checkpoint: {start_idx}/{N_SAMPLES}\")\n",
    "\n",
    "if start_idx == 0:\n",
    "    print(f\"Starting fresh: {len(COND_NAMES)} conditions x {N_SAMPLES} samples \"\n",
    "          f\"= {len(COND_NAMES) * N_SAMPLES} scorings\")\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "for i in tqdm(range(start_idx, N_SAMPLES), initial=start_idx, total=N_SAMPLES,\n",
    "              desc=\"Scoring\"):\n",
    "    s = samples[i]\n",
    "    result = {\n",
    "        'query': s['query'],\n",
    "        'answer': s['answer'],\n",
    "        'passage_words': s['word_count'],\n",
    "        'swapped_query': s['swapped'],\n",
    "    }\n",
    "\n",
    "    for cond in COND_NAMES:\n",
    "        if cond == 'bare':\n",
    "            nll = score_nll(s['passage'], s['answer'])\n",
    "            result['nll_bare'] = nll\n",
    "        else:\n",
    "            key = cond.replace('_trunc', '')\n",
    "            prefix = s[key]\n",
    "            enc_text = prefix + \"\\n\" + s['passage']\n",
    "            ptoks = count_prefix_tokens(prefix, s['passage'])\n",
    "            nll = score_nll(enc_text, s['answer'], ptoks, truncate=True)\n",
    "            result[f'nll_{cond}'] = nll\n",
    "            result[f'ptoks_{cond}'] = ptoks\n",
    "\n",
    "    results.append(result)\n",
    "\n",
    "    if (i + 1) % 20 == 0 or i == N_SAMPLES - 1:\n",
    "        ckpt = {\n",
    "            'n_total': N_SAMPLES,\n",
    "            'results': results,\n",
    "            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        }\n",
    "        CHECKPOINT_PATH.write_text(json.dumps(ckpt))\n",
    "        elapsed = time.time() - t0\n",
    "        done = i - start_idx + 1\n",
    "        eta = (N_SAMPLES - i - 1) * elapsed / done if done > 0 else 0\n",
    "        tqdm.write(f\"  Checkpoint {i+1}/{N_SAMPLES} | {elapsed/60:.1f}m | ETA {eta/60:.1f}m\")\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "elapsed = time.time() - t0\n",
    "print(f\"\\nScoring complete: {len(results)} samples, \"\n",
    "      f\"{len(COND_NAMES)} conditions in {elapsed/60:.1f} min\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1c8e753",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-20T02:12:03.181766Z",
     "iopub.status.busy": "2026-02-20T02:12:03.181471Z",
     "iopub.status.idle": "2026-02-20T02:12:03.204171Z",
     "shell.execute_reply": "2026-02-20T02:12:03.203306Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PART 1: STANDARD CONDITION TABLE\n",
      "======================================================================\n",
      "\n",
      "Condition                                NLL    Delta        d    Win%   %Orc            p   sig\n",
      "-----------------------------------------------------------------------------------------------\n",
      "  Oracle (real query)                 2.8106  +0.5086   +0.497   92.0%   100%     8.50e-26 ***\n",
      "  Swapped (wrong query)               2.8918  +0.4274   +0.435   86.8%    88%     1.35e-20 ***\n",
      "  Random matched (structural)         2.9007  +0.4186   +0.381   83.8%    77%     1.82e-16 ***\n",
      "\n",
      "  bare (lower bound): 3.3192\n",
      "  Bonferroni threshold: alpha=0.0167\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Part 1 — Standard Condition Table\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PART 1: STANDARD CONDITION TABLE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "bare_nlls = np.array([r['nll_bare'] for r in results])\n",
    "oracle_nlls = np.array([r['nll_oracle_trunc'] for r in results])\n",
    "swapped_nlls = np.array([r['nll_swapped_trunc'] for r in results])\n",
    "random_nlls = np.array([r['nll_random_matched_trunc'] for r in results])\n",
    "\n",
    "oracle_benefit = bare_nlls - oracle_nlls\n",
    "oracle_d = cohens_d(oracle_benefit)\n",
    "\n",
    "all_conds = [\n",
    "    ('oracle_trunc', 'Oracle (real query)'),\n",
    "    ('swapped_trunc', 'Swapped (wrong query)'),\n",
    "    ('random_matched_trunc', 'Random matched (structural)'),\n",
    "]\n",
    "\n",
    "alpha_bonf = 0.05 / len(all_conds)\n",
    "\n",
    "print(f\"\\n{'Condition':<35} {'NLL':>8} {'Delta':>8} {'d':>8} \"\n",
    "      f\"{'Win%':>7} {'%Orc':>6} {'p':>12} {'sig':>5}\")\n",
    "print(\"-\" * 95)\n",
    "\n",
    "for cond, desc in all_conds:\n",
    "    nlls = np.array([r[f'nll_{cond}'] for r in results])\n",
    "    benefit = bare_nlls - nlls\n",
    "    d = cohens_d(benefit)\n",
    "    delta = benefit.mean()\n",
    "    win = 100 * np.mean(benefit > 0)\n",
    "    pct = d / oracle_d * 100 if oracle_d > 0 else 0\n",
    "    _, p = stats.ttest_1samp(benefit, 0)\n",
    "    sig = '***' if p < alpha_bonf / 10 else '**' if p < alpha_bonf else '*' if p < 0.05 else 'ns'\n",
    "    print(f\"  {desc:<33} {nlls.mean():>8.4f} {delta:>+8.4f} {d:>+8.3f} \"\n",
    "          f\"{win:>6.1f}% {pct:>5.0f}% {p:>12.2e} {sig}\")\n",
    "\n",
    "print(f\"\\n  bare (lower bound): {bare_nlls.mean():.4f}\")\n",
    "print(f\"  Bonferroni threshold: alpha={alpha_bonf:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f8483cf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-20T02:12:03.209535Z",
     "iopub.status.busy": "2026-02-20T02:12:03.209241Z",
     "iopub.status.idle": "2026-02-20T02:12:03.219675Z",
     "shell.execute_reply": "2026-02-20T02:12:03.218756Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PART 2: PAIRED SEMANTIC CONTRAST (the key test)\n",
      "======================================================================\n",
      "Per-document: Delta_semantic = swapped_nll - oracle_nll\n",
      "Both conditions have the same structural perturbation (a real query prefix).\n",
      "Only semantic relevance differs.\n",
      "\n",
      "  Mean(swapped_nll - oracle_nll): +0.0812\n",
      "  Cohen's d:                      +0.166\n",
      "  Oracle wins:                    63.4%\n",
      "  Paired t-test:                  t=3.713, p=2.28e-04\n",
      "  Significance:                   ***\n",
      "\n",
      "  --> SEMANTIC RELEVANCE MATTERS: oracle query produces significantly\n",
      "      lower NLL than a swapped query from a different topic.\n",
      "      The semantic component is d=+0.166 in paired comparison.\n",
      "\n",
      "--- Context ---\n",
      "  Overall oracle benefit (vs bare): d=+0.497\n",
      "  Semantic component (paired):      d=+0.166\n",
      "  Structural component (estimated): d=+0.331\n",
      "  Semantic fraction:                33.4% of total benefit\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Part 2 — Paired Semantic Contrast\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PART 2: PAIRED SEMANTIC CONTRAST (the key test)\")\n",
    "print(\"=\" * 70)\n",
    "print(\"Per-document: Delta_semantic = swapped_nll - oracle_nll\")\n",
    "print(\"Both conditions have the same structural perturbation (a real query prefix).\")\n",
    "print(\"Only semantic relevance differs.\\n\")\n",
    "\n",
    "semantic_effect = swapped_nlls - oracle_nlls  # positive = oracle is better\n",
    "\n",
    "# Paired t-test\n",
    "t_stat, p_paired = stats.ttest_rel(swapped_nlls, oracle_nlls)\n",
    "d_semantic = cohens_d(semantic_effect)\n",
    "win_oracle = 100 * np.mean(semantic_effect > 0)\n",
    "\n",
    "print(f\"  Mean(swapped_nll - oracle_nll): {semantic_effect.mean():+.4f}\")\n",
    "print(f\"  Cohen's d:                      {d_semantic:+.3f}\")\n",
    "print(f\"  Oracle wins:                    {win_oracle:.1f}%\")\n",
    "print(f\"  Paired t-test:                  t={t_stat:.3f}, p={p_paired:.2e}\")\n",
    "\n",
    "sig = '***' if p_paired < 0.001 else '**' if p_paired < 0.01 else '*' if p_paired < 0.05 else 'ns'\n",
    "print(f\"  Significance:                   {sig}\")\n",
    "\n",
    "if p_paired < 0.05 and d_semantic > 0:\n",
    "    print(f\"\\n  --> SEMANTIC RELEVANCE MATTERS: oracle query produces significantly\")\n",
    "    print(f\"      lower NLL than a swapped query from a different topic.\")\n",
    "    print(f\"      The semantic component is d={d_semantic:+.3f} in paired comparison.\")\n",
    "elif p_paired < 0.05 and d_semantic < 0:\n",
    "    print(f\"\\n  --> REVERSE: swapped query is actually BETTER than oracle.\")\n",
    "    print(f\"      This would suggest semantic interference from the real query.\")\n",
    "else:\n",
    "    print(f\"\\n  --> NO SIGNIFICANT SEMANTIC EFFECT: oracle and swapped queries\")\n",
    "    print(f\"      produce equivalent NLLs. The benefit is purely structural.\")\n",
    "\n",
    "# Context: how does this compare to overall benefit?\n",
    "print(f\"\\n--- Context ---\")\n",
    "print(f\"  Overall oracle benefit (vs bare): d={oracle_d:+.3f}\")\n",
    "print(f\"  Semantic component (paired):      d={d_semantic:+.3f}\")\n",
    "print(f\"  Structural component (estimated): d={oracle_d - d_semantic:+.3f}\")\n",
    "if oracle_d > 0:\n",
    "    sem_frac = d_semantic / oracle_d * 100\n",
    "    print(f\"  Semantic fraction:                {sem_frac:.1f}% of total benefit\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cfd68805",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-20T02:12:03.225364Z",
     "iopub.status.busy": "2026-02-20T02:12:03.225099Z",
     "iopub.status.idle": "2026-02-20T02:12:03.237154Z",
     "shell.execute_reply": "2026-02-20T02:12:03.236300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PART 3: EFFECT DISTRIBUTION\n",
      "======================================================================\n",
      "Per-sample distribution of swapped_nll - oracle_nll\n",
      "\n",
      "  Mean:   +0.0812\n",
      "  Median: +0.0337\n",
      "  Std:    0.4889\n",
      "  Min:    -2.6250\n",
      "  Max:    +4.2500\n",
      "\n",
      "  Oracle better (oracle < swapped): 317 (63.4%)\n",
      "  Swapped better (swapped < oracle): 173 (34.6%)\n",
      "  Tied:                              10\n",
      "\n",
      "--- Effect size distribution ---\n",
      "  |effect| > 0.01: 301 oracle wins, 157 swapped wins\n",
      "  |effect| > 0.05: 212 oracle wins, 96 swapped wins\n",
      "  |effect| > 0.10: 152 oracle wins, 58 swapped wins\n",
      "  |effect| > 0.20: 100 oracle wins, 34 swapped wins\n",
      "  |effect| > 0.50: 35 oracle wins, 17 swapped wins\n",
      "\n",
      "--- Per-sample semantic effect by quintile ---\n",
      "  Q1: mean=-0.3496, range=[-2.6250, -0.0508], N=96\n",
      "  Q2: mean=-0.0168, range=[-0.0469, +0.0098], N=103\n",
      "  Q3: mean=+0.0336, range=[+0.0117, +0.0605], N=98\n",
      "  Q4: mean=+0.1079, range=[+0.0625, +0.1875], N=103\n",
      "  Q5: mean=+0.6151, range=[+0.2031, +4.2500], N=100\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Part 3 — Effect Distribution\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PART 3: EFFECT DISTRIBUTION\")\n",
    "print(\"=\" * 70)\n",
    "print(\"Per-sample distribution of swapped_nll - oracle_nll\\n\")\n",
    "\n",
    "# Distribution statistics\n",
    "print(f\"  Mean:   {semantic_effect.mean():+.4f}\")\n",
    "print(f\"  Median: {np.median(semantic_effect):+.4f}\")\n",
    "print(f\"  Std:    {semantic_effect.std():.4f}\")\n",
    "print(f\"  Min:    {semantic_effect.min():+.4f}\")\n",
    "print(f\"  Max:    {semantic_effect.max():+.4f}\")\n",
    "\n",
    "# Fraction showing semantic benefit\n",
    "oracle_better = np.sum(semantic_effect > 0)\n",
    "swapped_better = np.sum(semantic_effect < 0)\n",
    "tied = np.sum(semantic_effect == 0)\n",
    "\n",
    "print(f\"\\n  Oracle better (oracle < swapped): {oracle_better} ({oracle_better/N_SAMPLES*100:.1f}%)\")\n",
    "print(f\"  Swapped better (swapped < oracle): {swapped_better} ({swapped_better/N_SAMPLES*100:.1f}%)\")\n",
    "print(f\"  Tied:                              {tied}\")\n",
    "\n",
    "# Effect size distribution\n",
    "print(f\"\\n--- Effect size distribution ---\")\n",
    "for threshold in [0.01, 0.05, 0.1, 0.2, 0.5]:\n",
    "    n_above = np.sum(semantic_effect > threshold)\n",
    "    n_below = np.sum(semantic_effect < -threshold)\n",
    "    print(f\"  |effect| > {threshold:.2f}: {n_above} oracle wins, {n_below} swapped wins\")\n",
    "\n",
    "# Quintile breakdown of per-sample semantic effect\n",
    "print(f\"\\n--- Per-sample semantic effect by quintile ---\")\n",
    "eff_quintile_bounds = np.percentile(semantic_effect, [20, 40, 60, 80])\n",
    "eff_quintiles = np.digitize(semantic_effect, eff_quintile_bounds)\n",
    "for q in range(5):\n",
    "    mask = eff_quintiles == q\n",
    "    eff_q = semantic_effect[mask]\n",
    "    print(f\"  Q{q+1}: mean={eff_q.mean():+.4f}, range=[{eff_q.min():+.4f}, {eff_q.max():+.4f}], \"\n",
    "          f\"N={mask.sum()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef979d6a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-20T02:12:03.240686Z",
     "iopub.status.busy": "2026-02-20T02:12:03.240412Z",
     "iopub.status.idle": "2026-02-20T02:12:03.293525Z",
     "shell.execute_reply": "2026-02-20T02:12:03.292509Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PART 4: PREDICTORS OF SEMANTIC BENEFIT\n",
      "======================================================================\n",
      "What sample characteristics predict whether oracle > swapped?\n",
      "\n",
      "  Predictor                       Pearson r            p   sig\n",
      "  --------------------------------------------------------------\n",
      "  Query-doc Jaccard overlap          +0.076     8.75e-02 ns\n",
      "  Document length (words)            +0.020     6.56e-01 ns\n",
      "  Bare NLL (hardness)                +0.120     7.44e-03 **\n",
      "  Answer length (words)              -0.121     6.79e-03 **\n",
      "  Query length (words)               +0.072     1.07e-01 ns\n",
      "\n",
      "--- Semantic effect by hardness quintile ---\n",
      "  Quintile        N   Bare NLL   Sem effect        d    Win%            p   sig\n",
      "  ---------------------------------------------------------------------------\n",
      "  Q1 easy       100      0.476      +0.0060   +0.142   53.0%     1.60e-01 ns\n",
      "  Q2             99      0.981      +0.0088   +0.090   61.6%     3.72e-01 ns\n",
      "  Q3            101      1.825      +0.0818   +0.568   73.3%     1.19e-07 ***\n",
      "  Q4             99      3.119      +0.0864   +0.298   63.6%     3.83e-03 **\n",
      "  Q5 hard       101     10.117      +0.2211   +0.216   65.3%     3.25e-02 *\n",
      "\n",
      "--- Semantic effect by query-doc overlap quintile ---\n",
      "  Q1 (Jaccard=0.023, N=99): effect=-0.0127, d=-0.031, p=7.58e-01 ns\n",
      "  Q2 (Jaccard=0.047, N=101): effect=+0.1167, d=+0.202, p=4.48e-02 *\n",
      "  Q3 (Jaccard=0.066, N=93): effect=+0.0251, d=+0.053, p=6.10e-01 ns\n",
      "  Q4 (Jaccard=0.087, N=107): effect=+0.1717, d=+0.282, p=4.30e-03 **\n",
      "  Q5 (Jaccard=0.144, N=100): effect=+0.0940, d=+0.332, p=1.27e-03 **\n"
     ]
    }
   ],
   "source": [
    "# Cell 10: Part 4 — Predictors of Semantic Benefit\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PART 4: PREDICTORS OF SEMANTIC BENEFIT\")\n",
    "print(\"=\" * 70)\n",
    "print(\"What sample characteristics predict whether oracle > swapped?\\n\")\n",
    "\n",
    "# (a) Query-document vocabulary overlap (Jaccard on content words)\n",
    "jaccard_overlaps = []\n",
    "for i in range(N_SAMPLES):\n",
    "    doc_words = set(re.sub(r'[^\\w\\s]', '', samples[i]['passage'].lower()).split())\n",
    "    doc_content = doc_words - STOP_WORDS\n",
    "    q_words = set(re.sub(r'[^\\w\\s]', '', samples[i]['query'].lower()).split())\n",
    "    q_content = q_words - STOP_WORDS\n",
    "    if len(doc_content | q_content) > 0:\n",
    "        jaccard = len(doc_content & q_content) / len(doc_content | q_content)\n",
    "    else:\n",
    "        jaccard = 0.0\n",
    "    jaccard_overlaps.append(jaccard)\n",
    "jaccard_overlaps = np.array(jaccard_overlaps)\n",
    "\n",
    "# (b) Document length (word count)\n",
    "doc_lengths = np.array([r['passage_words'] for r in results])\n",
    "\n",
    "# (c) Bare NLL (hardness)\n",
    "# bare_nlls already defined\n",
    "\n",
    "# (d) Answer length\n",
    "answer_lengths = np.array([len(r['answer'].split()) for r in results])\n",
    "\n",
    "# (e) Query length\n",
    "query_lengths = np.array([len(r['query'].split()) for r in results])\n",
    "\n",
    "predictors = [\n",
    "    ('Query-doc Jaccard overlap', jaccard_overlaps),\n",
    "    ('Document length (words)', doc_lengths),\n",
    "    ('Bare NLL (hardness)', bare_nlls),\n",
    "    ('Answer length (words)', answer_lengths),\n",
    "    ('Query length (words)', query_lengths),\n",
    "]\n",
    "\n",
    "print(f\"  {'Predictor':<30} {'Pearson r':>10} {'p':>12} {'sig':>5}\")\n",
    "print(f\"  {'-'*62}\")\n",
    "\n",
    "alpha_bonf_pred = 0.05 / len(predictors)\n",
    "\n",
    "for name, values in predictors:\n",
    "    r_val, p_val = stats.pearsonr(values, semantic_effect)\n",
    "    sig = '***' if p_val < alpha_bonf_pred / 10 else '**' if p_val < alpha_bonf_pred else \\\n",
    "          '*' if p_val < 0.05 else 'ns'\n",
    "    print(f\"  {name:<30} {r_val:>+10.3f} {p_val:>12.2e} {sig}\")\n",
    "\n",
    "# Hardness interaction (detailed)\n",
    "print(f\"\\n--- Semantic effect by hardness quintile ---\")\n",
    "quintile_bounds = np.percentile(bare_nlls, [20, 40, 60, 80])\n",
    "quintiles = np.digitize(bare_nlls, quintile_bounds)\n",
    "q_labels = ['Q1 easy', 'Q2', 'Q3', 'Q4', 'Q5 hard']\n",
    "\n",
    "print(f\"  {'Quintile':<12} {'N':>4} {'Bare NLL':>10} {'Sem effect':>12} {'d':>8} \"\n",
    "      f\"{'Win%':>7} {'p':>12} {'sig':>5}\")\n",
    "print(f\"  {'-'*75}\")\n",
    "\n",
    "for q in range(5):\n",
    "    mask = quintiles == q\n",
    "    n = mask.sum()\n",
    "    eff_q = semantic_effect[mask]\n",
    "    d_q = cohens_d(eff_q)\n",
    "    win_q = 100 * np.mean(eff_q > 0)\n",
    "    _, p_q = stats.ttest_1samp(eff_q, 0)\n",
    "    sig_q = '***' if p_q < 0.001 else '**' if p_q < 0.01 else '*' if p_q < 0.05 else 'ns'\n",
    "    print(f\"  {q_labels[q]:<12} {n:>4} {bare_nlls[mask].mean():>10.3f} \"\n",
    "          f\"{eff_q.mean():>+12.4f} {d_q:>+8.3f} {win_q:>6.1f}% {p_q:>12.2e} {sig_q}\")\n",
    "\n",
    "# Jaccard interaction (detailed)\n",
    "print(f\"\\n--- Semantic effect by query-doc overlap quintile ---\")\n",
    "jacc_bounds = np.percentile(jaccard_overlaps, [20, 40, 60, 80])\n",
    "jacc_quints = np.digitize(jaccard_overlaps, jacc_bounds)\n",
    "\n",
    "for q in range(5):\n",
    "    mask = jacc_quints == q\n",
    "    n = mask.sum()\n",
    "    eff_q = semantic_effect[mask]\n",
    "    d_q = cohens_d(eff_q)\n",
    "    jacc_q = jaccard_overlaps[mask].mean()\n",
    "    _, p_q = stats.ttest_1samp(eff_q, 0)\n",
    "    sig_q = '***' if p_q < 0.001 else '**' if p_q < 0.01 else '*' if p_q < 0.05 else 'ns'\n",
    "    print(f\"  Q{q+1} (Jaccard={jacc_q:.3f}, N={n}): effect={eff_q.mean():+.4f}, \"\n",
    "          f\"d={d_q:+.3f}, p={p_q:.2e} {sig_q}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c8b4c1c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-20T02:12:03.297943Z",
     "iopub.status.busy": "2026-02-20T02:12:03.297254Z",
     "iopub.status.idle": "2026-02-20T02:12:03.315777Z",
     "shell.execute_reply": "2026-02-20T02:12:03.314908Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PART 5: STRUCTURAL EQUIVALENCE CHECK\n",
      "======================================================================\n",
      "Confirm oracle and swapped have similar structural benefit vs bare.\n",
      "If structural effects differ, the 'semantic' contrast is confounded.\n",
      "\n",
      "  Condition vs bare (Cohen's d):\n",
      "    Oracle:  d=+0.497\n",
      "    Swapped: d=+0.435\n",
      "    Random:  d=+0.381\n",
      "\n",
      "  Condition vs random (semantic component):\n",
      "    Oracle - random:  d=+0.180, p=6.82e-05\n",
      "    Swapped - random: d=+0.019, p=6.66e-01\n",
      "\n",
      "  Oracle benefit - Swapped benefit: d=+0.166, p=2.28e-04 ***\n",
      "  (This should equal the semantic effect from Part 2: d=+0.166)\n",
      "  Consistency check: 0.0000 (should be ~0)\n",
      "\n",
      "  Prefix token counts:\n",
      "    Oracle:  mean=7.7, std=2.4\n",
      "    Swapped: mean=7.7, std=2.4\n",
      "    Random:  mean=9.2, std=4.2\n",
      "\n",
      "  Token count diff vs semantic effect: r=-0.049, p=2.76e-01 ns\n",
      "  CLEAN: no confound from prefix length differences.\n"
     ]
    }
   ],
   "source": [
    "# Cell 11: Part 5 — Structural Equivalence Check\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PART 5: STRUCTURAL EQUIVALENCE CHECK\")\n",
    "print(\"=\" * 70)\n",
    "print(\"Confirm oracle and swapped have similar structural benefit vs bare.\")\n",
    "print(\"If structural effects differ, the 'semantic' contrast is confounded.\\n\")\n",
    "\n",
    "# Structural benefit: condition vs bare\n",
    "oracle_struct = bare_nlls - oracle_nlls\n",
    "swapped_struct = bare_nlls - swapped_nlls\n",
    "random_struct = bare_nlls - random_nlls\n",
    "\n",
    "oracle_vs_bare_d = cohens_d(oracle_struct)\n",
    "swapped_vs_bare_d = cohens_d(swapped_struct)\n",
    "random_vs_bare_d = cohens_d(random_struct)\n",
    "\n",
    "print(f\"  Condition vs bare (Cohen's d):\")\n",
    "print(f\"    Oracle:  d={oracle_vs_bare_d:+.3f}\")\n",
    "print(f\"    Swapped: d={swapped_vs_bare_d:+.3f}\")\n",
    "print(f\"    Random:  d={random_vs_bare_d:+.3f}\")\n",
    "\n",
    "# Both should have similar structural benefit relative to random\n",
    "oracle_vs_random = random_nlls - oracle_nlls  # semantic component\n",
    "swapped_vs_random = random_nlls - swapped_nlls  # should be ~0 or small\n",
    "\n",
    "d_orac_rand = cohens_d(oracle_vs_random)\n",
    "d_swap_rand = cohens_d(swapped_vs_random)\n",
    "_, p_orac_rand = stats.ttest_1samp(oracle_vs_random, 0)\n",
    "_, p_swap_rand = stats.ttest_1samp(swapped_vs_random, 0)\n",
    "\n",
    "print(f\"\\n  Condition vs random (semantic component):\")\n",
    "print(f\"    Oracle - random:  d={d_orac_rand:+.3f}, p={p_orac_rand:.2e}\")\n",
    "print(f\"    Swapped - random: d={d_swap_rand:+.3f}, p={p_swap_rand:.2e}\")\n",
    "\n",
    "# The key check: oracle and swapped should have similar benefit over random\n",
    "# IF the semantic component is real, oracle should beat random more than swapped\n",
    "structural_diff = oracle_struct - swapped_struct\n",
    "d_struct_diff = cohens_d(structural_diff)\n",
    "_, p_struct = stats.ttest_1samp(structural_diff, 0)\n",
    "sig_struct = '***' if p_struct < 0.001 else '**' if p_struct < 0.01 else '*' if p_struct < 0.05 else 'ns'\n",
    "\n",
    "print(f\"\\n  Oracle benefit - Swapped benefit: d={d_struct_diff:+.3f}, p={p_struct:.2e} {sig_struct}\")\n",
    "print(f\"  (This should equal the semantic effect from Part 2: d={d_semantic:+.3f})\")\n",
    "print(f\"  Consistency check: {abs(d_struct_diff - d_semantic):.4f} (should be ~0)\")\n",
    "\n",
    "# Prefix token count comparison\n",
    "oracle_ptoks = np.array([r['ptoks_oracle_trunc'] for r in results])\n",
    "swapped_ptoks = np.array([r['ptoks_swapped_trunc'] for r in results])\n",
    "random_ptoks = np.array([r['ptoks_random_matched_trunc'] for r in results])\n",
    "\n",
    "print(f\"\\n  Prefix token counts:\")\n",
    "print(f\"    Oracle:  mean={oracle_ptoks.mean():.1f}, std={oracle_ptoks.std():.1f}\")\n",
    "print(f\"    Swapped: mean={swapped_ptoks.mean():.1f}, std={swapped_ptoks.std():.1f}\")\n",
    "print(f\"    Random:  mean={random_ptoks.mean():.1f}, std={random_ptoks.std():.1f}\")\n",
    "\n",
    "# Are token counts correlated with semantic effect? (potential confound)\n",
    "ptok_diff = swapped_ptoks - oracle_ptoks\n",
    "r_ptok, p_ptok = stats.pearsonr(ptok_diff.astype(float), semantic_effect)\n",
    "sig_ptok = '***' if p_ptok < 0.001 else '**' if p_ptok < 0.01 else '*' if p_ptok < 0.05 else 'ns'\n",
    "print(f\"\\n  Token count diff vs semantic effect: r={r_ptok:+.3f}, p={p_ptok:.2e} {sig_ptok}\")\n",
    "\n",
    "if abs(r_ptok) > 0.1 and p_ptok < 0.05:\n",
    "    print(f\"  WARNING: token count difference correlates with semantic effect.\")\n",
    "    print(f\"  The 'semantic' contrast may be partially confounded by prefix length.\")\n",
    "else:\n",
    "    print(f\"  CLEAN: no confound from prefix length differences.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "70db438f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-20T02:12:03.320175Z",
     "iopub.status.busy": "2026-02-20T02:12:03.319887Z",
     "iopub.status.idle": "2026-02-20T02:12:03.881226Z",
     "shell.execute_reply": "2026-02-20T02:12:03.880274Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "SYNTHESIS: SWAPPED-QUERY PAIRED CONTRAST RESULTS\n",
      "======================================================================\n",
      "\n",
      "1. CONDITION TABLE:\n",
      "   Condition                  d vs bare  %Oracle\n",
      "   ---------------------------------------------\n",
      "   Oracle (real query)           +0.497     100%\n",
      "   Swapped (wrong query)         +0.435      88%\n",
      "   Random matched (structural)     +0.381      77%\n",
      "\n",
      "2. PAIRED SEMANTIC CONTRAST:\n",
      "   swapped_nll - oracle_nll: mean=+0.0812, d=+0.166\n",
      "   Oracle win rate: 63.4%, p=2.28e-04\n",
      "   Semantic fraction of total benefit: 33.4%\n",
      "\n",
      "3. PREDICTORS OF SEMANTIC BENEFIT:\n",
      "   Strongest: Answer length (words) (r=-0.121)\n",
      "\n",
      "4. STRUCTURAL EQUIVALENCE:\n",
      "   Oracle vs bare:  d=+0.497\n",
      "   Swapped vs bare: d=+0.435\n",
      "   Token count confound: r=-0.049 (CLEAN)\n",
      "\n",
      "======================================================================\n",
      "CONCLUSIONS:\n",
      "  1. STRONG SEMANTIC SIGNAL: oracle significantly beats swapped\n",
      "     (d=+0.166, p=2.28e-04)\n",
      "  2. Result is CLEAN: no confound from prefix length\n",
      "\n",
      "  Cross-reference with Exp 12:\n",
      "  Exp 12 tests the GRADIENT; this experiment confirms/denies the binary signal.\n",
      "  If d_semantic > 0 here, the gradient in Exp 12 should be monotonic.\n",
      "  If d_semantic ~ 0 here, any gradient in Exp 12 is likely noise.\n",
      "======================================================================\n",
      "\n",
      "Results saved to ../../results/exp13/results.json\n",
      "\n",
      "Cleaning up GPU memory...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory: 15.03 GB -> 0.01 GB\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Cell 12: Synthesis + Save\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"SYNTHESIS: SWAPPED-QUERY PAIRED CONTRAST RESULTS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# 1. Summary\n",
    "print(f\"\\n1. CONDITION TABLE:\")\n",
    "print(f\"   {'Condition':<25} {'d vs bare':>10} {'%Oracle':>8}\")\n",
    "print(f\"   {'-'*45}\")\n",
    "for cond, desc in all_conds:\n",
    "    nlls = np.array([r[f'nll_{cond}'] for r in results])\n",
    "    d = cohens_d(bare_nlls - nlls)\n",
    "    pct = d / oracle_d * 100 if oracle_d > 0 else 0\n",
    "    print(f\"   {desc:<25} {d:>+10.3f} {pct:>7.0f}%\")\n",
    "\n",
    "# 2. The key result\n",
    "print(f\"\\n2. PAIRED SEMANTIC CONTRAST:\")\n",
    "print(f\"   swapped_nll - oracle_nll: mean={semantic_effect.mean():+.4f}, d={d_semantic:+.3f}\")\n",
    "print(f\"   Oracle win rate: {win_oracle:.1f}%, p={p_paired:.2e}\")\n",
    "if oracle_d > 0:\n",
    "    sem_frac = d_semantic / oracle_d * 100\n",
    "    print(f\"   Semantic fraction of total benefit: {sem_frac:.1f}%\")\n",
    "\n",
    "# 3. Strongest predictor\n",
    "print(f\"\\n3. PREDICTORS OF SEMANTIC BENEFIT:\")\n",
    "best_r = 0\n",
    "best_name = \"\"\n",
    "for name, values in predictors:\n",
    "    r_val, _ = stats.pearsonr(values, semantic_effect)\n",
    "    if abs(r_val) > abs(best_r):\n",
    "        best_r = r_val\n",
    "        best_name = name\n",
    "print(f\"   Strongest: {best_name} (r={best_r:+.3f})\")\n",
    "\n",
    "# 4. Structural equivalence\n",
    "print(f\"\\n4. STRUCTURAL EQUIVALENCE:\")\n",
    "print(f\"   Oracle vs bare:  d={oracle_vs_bare_d:+.3f}\")\n",
    "print(f\"   Swapped vs bare: d={swapped_vs_bare_d:+.3f}\")\n",
    "print(f\"   Token count confound: r={r_ptok:+.3f} ({'CLEAN' if abs(r_ptok) < 0.1 else 'WARNING'})\")\n",
    "\n",
    "# 5. Conclusions\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"CONCLUSIONS:\")\n",
    "\n",
    "if p_paired < 0.001 and d_semantic > 0.05:\n",
    "    print(f\"  1. STRONG SEMANTIC SIGNAL: oracle significantly beats swapped\")\n",
    "    print(f\"     (d={d_semantic:+.3f}, p={p_paired:.2e})\")\n",
    "    conclusion = \"STRONG_SEMANTIC\"\n",
    "elif p_paired < 0.05 and d_semantic > 0:\n",
    "    print(f\"  1. WEAK SEMANTIC SIGNAL: marginally significant\")\n",
    "    print(f\"     (d={d_semantic:+.3f}, p={p_paired:.2e})\")\n",
    "    conclusion = \"WEAK_SEMANTIC\"\n",
    "elif p_paired < 0.05 and d_semantic < 0:\n",
    "    print(f\"  1. SEMANTIC INTERFERENCE: swapped query is actually BETTER\")\n",
    "    print(f\"     (d={d_semantic:+.3f}, p={p_paired:.2e})\")\n",
    "    conclusion = \"INTERFERENCE\"\n",
    "else:\n",
    "    print(f\"  1. NO SEMANTIC EFFECT: oracle and swapped are equivalent\")\n",
    "    print(f\"     (d={d_semantic:+.3f}, p={p_paired:.2e})\")\n",
    "    conclusion = \"NO_EFFECT\"\n",
    "\n",
    "if abs(r_ptok) < 0.1:\n",
    "    print(f\"  2. Result is CLEAN: no confound from prefix length\")\n",
    "else:\n",
    "    print(f\"  2. Result is CONFOUNDED: token count correlates with effect (r={r_ptok:+.3f})\")\n",
    "\n",
    "# Cross-reference with Exp 12\n",
    "print(f\"\\n  Cross-reference with Exp 12:\")\n",
    "print(f\"  Exp 12 tests the GRADIENT; this experiment confirms/denies the binary signal.\")\n",
    "print(f\"  If d_semantic > 0 here, the gradient in Exp 12 should be monotonic.\")\n",
    "print(f\"  If d_semantic ~ 0 here, any gradient in Exp 12 is likely noise.\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "# Save results\n",
    "final_results = {\n",
    "    'experiment': 'exp13_swapped_query',\n",
    "    'model': MODEL_NAME,\n",
    "    'dataset': 'ms_marco_v1.1',\n",
    "    'n_samples': N_SAMPLES,\n",
    "    'seed': SEED,\n",
    "    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'baseline': {\n",
    "        'bare_nll': float(bare_nlls.mean()),\n",
    "        'oracle_d': float(oracle_d),\n",
    "    },\n",
    "    'semantic_contrast': {\n",
    "        'mean_effect': float(semantic_effect.mean()),\n",
    "        'd': float(d_semantic),\n",
    "        'win_pct': float(win_oracle),\n",
    "        'p_paired': float(p_paired),\n",
    "        'semantic_fraction_pct': float(sem_frac) if oracle_d > 0 else None,\n",
    "    },\n",
    "    'structural_equivalence': {\n",
    "        'oracle_vs_bare_d': float(oracle_vs_bare_d),\n",
    "        'swapped_vs_bare_d': float(swapped_vs_bare_d),\n",
    "        'random_vs_bare_d': float(random_vs_bare_d),\n",
    "        'token_count_confound_r': float(r_ptok),\n",
    "        'token_count_confound_p': float(p_ptok),\n",
    "    },\n",
    "    'predictors': {},\n",
    "    'conditions': {},\n",
    "    'conclusion': conclusion,\n",
    "}\n",
    "\n",
    "for name, values in predictors:\n",
    "    r_val, p_val = stats.pearsonr(values, semantic_effect)\n",
    "    final_results['predictors'][name] = {\n",
    "        'pearson_r': float(r_val),\n",
    "        'p': float(p_val),\n",
    "    }\n",
    "\n",
    "for cond, desc in all_conds:\n",
    "    nlls = np.array([r[f'nll_{cond}'] for r in results])\n",
    "    benefit = bare_nlls - nlls\n",
    "    d = cohens_d(benefit)\n",
    "    _, p = stats.ttest_1samp(benefit, 0)\n",
    "    final_results['conditions'][cond] = {\n",
    "        'description': desc,\n",
    "        'd': float(d),\n",
    "        'mean_nll': float(nlls.mean()),\n",
    "        'mean_delta': float(benefit.mean()),\n",
    "        'pct_oracle': float(d / oracle_d * 100) if oracle_d > 0 else 0,\n",
    "        'p': float(p),\n",
    "    }\n",
    "\n",
    "with open(RESULTS_DIR / 'results.json', 'w') as f:\n",
    "    json.dump(final_results, f, indent=2)\n",
    "print(f\"\\nResults saved to {RESULTS_DIR / 'results.json'}\")\n",
    "\n",
    "# Cleanup\n",
    "print(f\"\\nCleaning up GPU memory...\")\n",
    "mem_before = torch.cuda.memory_allocated() / 1e9\n",
    "del model, processor, tokenizer\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "mem_after = torch.cuda.memory_allocated() / 1e9\n",
    "print(f\"GPU memory: {mem_before:.2f} GB -> {mem_after:.2f} GB\")\n",
    "print(\"Done!\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "095b01b0c7494b238adcbc145078cf8f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_453a74be4cc14217b764a25f7c04ce8a",
       "placeholder": "​",
       "style": "IPY_MODEL_90c8d2e249fb4f8fb94daa814bfc555f",
       "tabbable": null,
       "tooltip": null,
       "value": " 500/500 [08:20&lt;00:00,  1.01it/s]"
      }
     },
     "0d1da6eef7204357b2219363285d3cfe": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_e68f8cf86b8045d297baf4854125d036",
       "placeholder": "​",
       "style": "IPY_MODEL_b5b2f923e4d841e8b89fca2e34576b2c",
       "tabbable": null,
       "tooltip": null,
       "value": "Loading weights: 100%"
      }
     },
     "27b3df63c1b647b0b8235b416f7228c4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2b45d087eb2e4aa2a349a17ac3406986": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_0d1da6eef7204357b2219363285d3cfe",
        "IPY_MODEL_8cc878f7ddf4487d9b4cbffa7da685ac",
        "IPY_MODEL_9bf58498b17c4c8e9b8200269c1f0ad6"
       ],
       "layout": "IPY_MODEL_65a7d795fefb4b5caea4f82340216b55",
       "tabbable": null,
       "tooltip": null
      }
     },
     "43aeb1f7187a4760a0e07d9be95c8d59": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "453a74be4cc14217b764a25f7c04ce8a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "523709204c7645e9b2a40ffe61a56f83": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_27b3df63c1b647b0b8235b416f7228c4",
       "placeholder": "​",
       "style": "IPY_MODEL_8c50b2b825dc4f53aa6b9b949578a88f",
       "tabbable": null,
       "tooltip": null,
       "value": "Scoring: 100%"
      }
     },
     "65a7d795fefb4b5caea4f82340216b55": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "8c50b2b825dc4f53aa6b9b949578a88f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "8cc878f7ddf4487d9b4cbffa7da685ac": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_b896e48d8fd14b478ce6db93ba4dd7aa",
       "max": 1327.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_9d8a94b220b44ac1b991e712bc8470ee",
       "tabbable": null,
       "tooltip": null,
       "value": 1327.0
      }
     },
     "902b1f627a454e028438a8636b28c34c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "90c8d2e249fb4f8fb94daa814bfc555f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "9bf58498b17c4c8e9b8200269c1f0ad6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_c0eb8d3283044d1d87cf1f4bb6b3a122",
       "placeholder": "​",
       "style": "IPY_MODEL_d776ab8eef384dd1a3a1009db2cede45",
       "tabbable": null,
       "tooltip": null,
       "value": " 1327/1327 [00:56&lt;00:00, 136.44it/s, Materializing param=model.encoder.vision_tower.vision_model.post_layernorm.weight]"
      }
     },
     "9d8a94b220b44ac1b991e712bc8470ee": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "b5b2f923e4d841e8b89fca2e34576b2c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "b896e48d8fd14b478ce6db93ba4dd7aa": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b9b7bc4af84e43eba2948589a1aad7fe": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_902b1f627a454e028438a8636b28c34c",
       "max": 500.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_43aeb1f7187a4760a0e07d9be95c8d59",
       "tabbable": null,
       "tooltip": null,
       "value": 500.0
      }
     },
     "c0eb8d3283044d1d87cf1f4bb6b3a122": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d776ab8eef384dd1a3a1009db2cede45": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "e68f8cf86b8045d297baf4854125d036": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "fc32b2aaae214e1098e28afb5e9e99dc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_523709204c7645e9b2a40ffe61a56f83",
        "IPY_MODEL_b9b7bc4af84e43eba2948589a1aad7fe",
        "IPY_MODEL_095b01b0c7494b238adcbc145078cf8f"
       ],
       "layout": "IPY_MODEL_fda736efa78b4059acbefe913924c647",
       "tabbable": null,
       "tooltip": null
      }
     },
     "fda736efa78b4059acbefe913924c647": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
