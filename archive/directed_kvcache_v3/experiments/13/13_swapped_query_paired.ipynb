{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "913d1c8e",
   "metadata": {},
   "source": [
    "# Experiment 13: Swapped-Query Paired Contrasts\n",
    "\n",
    "## Motivation\n",
    "\n",
    "Exp 2B showed ~85% of the encoder priming benefit is structural, ~10% semantic.\n",
    "But measuring the semantic component precisely is confounded by vocabulary and\n",
    "length differences between conditions. This experiment provides the **purest\n",
    "possible test** of semantic relevance by comparing oracle vs swapped query on the\n",
    "**same document** — same structural perturbation, same prefix format, only semantic\n",
    "relevance differs.\n",
    "\n",
    "## Design\n",
    "\n",
    "For each (query, document, answer) triple, we score the answer NLL under two\n",
    "prefix conditions:\n",
    "- `oracle_trunc`: the real query (semantically relevant)\n",
    "- `swapped_trunc`: a query from a completely different sample (semantically irrelevant)\n",
    "\n",
    "The per-document paired contrast `swapped_nll - oracle_nll` isolates the semantic\n",
    "component with maximum statistical power.\n",
    "\n",
    "## Conditions (4)\n",
    "\n",
    "| # | Condition | Prefix |\n",
    "|---|-----------|--------|\n",
    "| 1 | `bare` | (none) |\n",
    "| 2 | `oracle_trunc` | real query |\n",
    "| 3 | `swapped_trunc` | query from sample (i + N//2) % N |\n",
    "| 4 | `random_matched_trunc` | words from random passage |\n",
    "\n",
    "## Analysis\n",
    "\n",
    "1. Standard condition table\n",
    "2. Paired semantic contrast (the key test)\n",
    "3. Effect distribution — per-sample histogram\n",
    "4. Predictors of semantic benefit\n",
    "5. Structural equivalence check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a223167a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Setup\n",
    "import os\n",
    "os.umask(0o000)\n",
    "\n",
    "import sys, json, time, re, gc, random as pyrandom\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from scipy import stats\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "sys.path.insert(0, \"../..\")\n",
    "from lib.analysis import cohens_d\n",
    "\n",
    "SEED = 43  # Different seed from Exp 12 (42) for independent samples\n",
    "N_SAMPLES = 500\n",
    "MODEL_NAME = \"google/t5gemma-2-4b-4b\"\n",
    "\n",
    "RESULTS_DIR = Path(\"../../results/exp13\")\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CHECKPOINT_PATH = RESULTS_DIR / \"checkpoint.json\"\n",
    "\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "pyrandom.seed(SEED)\n",
    "\n",
    "print(\"Exp 13: Swapped-Query Paired Contrasts\")\n",
    "print(f\"N: {N_SAMPLES}, SEED: {SEED}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e268ad66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Load MS MARCO and select samples\n",
    "from lib.data import count_words\n",
    "from datasets import load_dataset\n",
    "\n",
    "print(\"Loading MS MARCO...\")\n",
    "ds = load_dataset(\"microsoft/ms_marco\", \"v1.1\", split=\"validation\")\n",
    "\n",
    "samples = []\n",
    "for item in ds:\n",
    "    if len(samples) >= N_SAMPLES * 3:\n",
    "        break\n",
    "    passages = item.get('passages', {})\n",
    "    ptexts = passages.get('passage_text', [])\n",
    "    is_sel = passages.get('is_selected', [])\n",
    "    query = item.get('query', '')\n",
    "    answers = item.get('answers', [])\n",
    "    well_formed = item.get('wellFormedAnswers', [])\n",
    "    answer = None\n",
    "    if well_formed and len(well_formed) > 0 and well_formed[0] not in ('[]', ''):\n",
    "        answer = well_formed[0]\n",
    "    elif answers and len(answers) > 0 and answers[0] != 'No Answer Present.':\n",
    "        answer = answers[0]\n",
    "    if not answer:\n",
    "        continue\n",
    "    for pt, sel in zip(ptexts, is_sel):\n",
    "        wc = count_words(pt)\n",
    "        if sel == 1 and 30 <= wc <= 300:\n",
    "            samples.append({\n",
    "                'passage': pt, 'query': query, 'answer': answer,\n",
    "                'word_count': wc,\n",
    "            })\n",
    "            break\n",
    "\n",
    "np.random.seed(SEED)\n",
    "np.random.shuffle(samples)\n",
    "samples = samples[:N_SAMPLES]\n",
    "del ds\n",
    "gc.collect()\n",
    "\n",
    "passage_words = np.array([s['word_count'] for s in samples])\n",
    "query_words = np.array([len(s['query'].split()) for s in samples])\n",
    "print(f\"Selected {N_SAMPLES} samples\")\n",
    "print(f\"Document lengths: {passage_words.min()}-{passage_words.max()} words, \"\n",
    "      f\"mean={passage_words.mean():.0f}\")\n",
    "print(f\"Query lengths: {query_words.min()}-{query_words.max()} words, \"\n",
    "      f\"mean={query_words.mean():.1f}\")\n",
    "\n",
    "# Verify swapped queries are from different topics\n",
    "print(f\"\\nSwapped query assignment:\")\n",
    "for i in range(5):\n",
    "    j = (i + N_SAMPLES // 2) % N_SAMPLES\n",
    "    print(f\"  Sample {i}: '{samples[i]['query'][:50]}...'\")\n",
    "    print(f\"    Swapped: '{samples[j]['query'][:50]}...'\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22eb23e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Load model and define scoring helpers\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv())\n",
    "HF_TOKEN = os.environ.get(\"HF_TOKEN\")\n",
    "\n",
    "from transformers import AutoProcessor, AutoModelForSeq2SeqLM\n",
    "\n",
    "print(f\"Loading {MODEL_NAME}...\")\n",
    "processor = AutoProcessor.from_pretrained(MODEL_NAME, token=HF_TOKEN)\n",
    "tokenizer = processor.tokenizer\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    MODEL_NAME, device_map=\"auto\", torch_dtype=torch.bfloat16, token=HF_TOKEN,\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "DEVICE = next(model.parameters()).device\n",
    "print(f\"Model loaded. dtype={next(model.parameters()).dtype}\")\n",
    "print(f\"GPU memory: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "\n",
    "\n",
    "def score_nll(encoder_text, answer_text, prefix_token_count=0, truncate=False):\n",
    "    # Score NLL of answer given encoder text, with optional prefix truncation.\n",
    "    enc_ids = tokenizer(encoder_text, return_tensors=\"pt\",\n",
    "                        add_special_tokens=True, truncation=True,\n",
    "                        max_length=2048).input_ids.to(DEVICE)\n",
    "    total_enc_len = enc_ids.shape[1]\n",
    "    enc_mask = torch.ones(1, total_enc_len, device=DEVICE, dtype=torch.long)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        encoder_outputs = model.get_encoder()(\n",
    "            input_ids=enc_ids, attention_mask=enc_mask\n",
    "        )\n",
    "\n",
    "    if truncate and prefix_token_count > 0:\n",
    "        cross_attn_mask = torch.ones(1, total_enc_len, device=DEVICE, dtype=torch.long)\n",
    "        cross_attn_mask[:, :prefix_token_count] = 0\n",
    "    else:\n",
    "        cross_attn_mask = torch.ones(1, total_enc_len, device=DEVICE, dtype=torch.long)\n",
    "\n",
    "    ans_ids = tokenizer(answer_text, return_tensors=\"pt\",\n",
    "                        add_special_tokens=False, truncation=True,\n",
    "                        max_length=256).input_ids.to(DEVICE)\n",
    "    if ans_ids.shape[1] == 0:\n",
    "        return 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            encoder_outputs=encoder_outputs,\n",
    "            attention_mask=cross_attn_mask,\n",
    "            labels=ans_ids,\n",
    "        )\n",
    "\n",
    "    logits = outputs.logits\n",
    "    log_probs = F.log_softmax(logits, dim=-1)\n",
    "    token_log_probs = log_probs[0].gather(1, ans_ids[0].unsqueeze(1)).squeeze(1)\n",
    "    mean_nll = -token_log_probs.mean().item()\n",
    "\n",
    "    del encoder_outputs, outputs, logits, log_probs\n",
    "    return mean_nll\n",
    "\n",
    "\n",
    "def count_prefix_tokens(prefix_text, document_text):\n",
    "    # Count how many tokens the prefix occupies in [prefix + newline + document].\n",
    "    full_text = prefix_text + \"\\n\" + document_text\n",
    "    full_ids = tokenizer(full_text, add_special_tokens=True, truncation=True,\n",
    "                         max_length=2048).input_ids\n",
    "    doc_ids = tokenizer(document_text, add_special_tokens=True, truncation=True,\n",
    "                        max_length=2048).input_ids\n",
    "    return len(full_ids) - len(doc_ids)\n",
    "\n",
    "\n",
    "STOP_WORDS = {\n",
    "    'a', 'an', 'the', 'is', 'are', 'was', 'were', 'be', 'been', 'being',\n",
    "    'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could',\n",
    "    'should', 'may', 'might', 'can', 'shall', 'to', 'of', 'in', 'for',\n",
    "    'on', 'with', 'at', 'by', 'from', 'as', 'into', 'through', 'during',\n",
    "    'before', 'after', 'above', 'below', 'between', 'and', 'but', 'or',\n",
    "    'not', 'no', 'if', 'then', 'than', 'so', 'up', 'out', 'about',\n",
    "    'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those',\n",
    "    'it', 'its', 'i', 'me', 'my', 'we', 'our', 'you', 'your', 'he',\n",
    "    'him', 'his', 'she', 'her', 'they', 'them', 'their', 'how', 'when',\n",
    "    'where', 'why', 'much', 'many', 'some', 'any', 'all', 'each',\n",
    "    'does', 'also', 'just', 'more', 'most', 'very', 'too', 'only',\n",
    "}\n",
    "\n",
    "print(\"Helpers defined.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7529d2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Generate all 4 scoring conditions per sample\n",
    "\n",
    "for i, s in enumerate(samples):\n",
    "    query = s['query']\n",
    "    passage = s['passage']\n",
    "    query_words_list = query.split()\n",
    "    n_query_words = len(query_words_list)\n",
    "\n",
    "    # Swapped query: query from a distant sample (guaranteed different topic)\n",
    "    swapped_idx = (i + N_SAMPLES // 2) % N_SAMPLES\n",
    "    s['swapped'] = samples[swapped_idx]['query']\n",
    "\n",
    "    # Random matched: words from unrelated passage, same word count as oracle\n",
    "    other_idx = (i + N_SAMPLES // 2) % N_SAMPLES\n",
    "    other_words = samples[other_idx]['passage'].split()\n",
    "    s['random_matched'] = \" \".join(other_words[:n_query_words])\n",
    "\n",
    "    # Oracle (just the query)\n",
    "    s['oracle'] = query\n",
    "\n",
    "COND_NAMES = [\n",
    "    'bare',\n",
    "    'oracle_trunc',\n",
    "    'swapped_trunc',\n",
    "    'random_matched_trunc',\n",
    "]\n",
    "\n",
    "print(f\"Conditions ({len(COND_NAMES)}):\")\n",
    "for c in COND_NAMES:\n",
    "    print(f\"  {c}\")\n",
    "\n",
    "# Show example\n",
    "ex = samples[0]\n",
    "print(f\"\\nExample (sample 0):\")\n",
    "print(f\"  Query:   {ex['query']}\")\n",
    "print(f\"  Answer:  {ex['answer'][:80]}\")\n",
    "print(f\"  Passage: {ex['passage'][:80]}...\")\n",
    "for c in COND_NAMES:\n",
    "    if c == 'bare':\n",
    "        print(f\"  {c:<28}: [document only]\")\n",
    "    else:\n",
    "        key = c.replace('_trunc', '')\n",
    "        text = ex[key]\n",
    "        ptoks = count_prefix_tokens(text, ex['passage'])\n",
    "        print(f\"  {c:<28} ({ptoks:>3} toks): {str(text)[:55]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33250ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Scoring loop with checkpointing\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"SCORING ALL CONDITIONS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "results = []\n",
    "start_idx = 0\n",
    "\n",
    "if CHECKPOINT_PATH.exists():\n",
    "    ckpt = json.loads(CHECKPOINT_PATH.read_text())\n",
    "    if ckpt.get('n_total') == N_SAMPLES and len(ckpt.get('results', [])) > 0:\n",
    "        saved_queries = [r['query'][:50] for r in ckpt['results']]\n",
    "        current_queries = [s['query'][:50] for s in samples[:len(saved_queries)]]\n",
    "        if saved_queries == current_queries:\n",
    "            results = ckpt['results']\n",
    "            start_idx = len(results)\n",
    "            print(f\"Resuming from checkpoint: {start_idx}/{N_SAMPLES}\")\n",
    "\n",
    "if start_idx == 0:\n",
    "    print(f\"Starting fresh: {len(COND_NAMES)} conditions x {N_SAMPLES} samples \"\n",
    "          f\"= {len(COND_NAMES) * N_SAMPLES} scorings\")\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "for i in tqdm(range(start_idx, N_SAMPLES), initial=start_idx, total=N_SAMPLES,\n",
    "              desc=\"Scoring\"):\n",
    "    s = samples[i]\n",
    "    result = {\n",
    "        'query': s['query'],\n",
    "        'answer': s['answer'],\n",
    "        'passage_words': s['word_count'],\n",
    "        'swapped_query': s['swapped'],\n",
    "    }\n",
    "\n",
    "    for cond in COND_NAMES:\n",
    "        if cond == 'bare':\n",
    "            nll = score_nll(s['passage'], s['answer'])\n",
    "            result['nll_bare'] = nll\n",
    "        else:\n",
    "            key = cond.replace('_trunc', '')\n",
    "            prefix = s[key]\n",
    "            enc_text = prefix + \"\\n\" + s['passage']\n",
    "            ptoks = count_prefix_tokens(prefix, s['passage'])\n",
    "            nll = score_nll(enc_text, s['answer'], ptoks, truncate=True)\n",
    "            result[f'nll_{cond}'] = nll\n",
    "            result[f'ptoks_{cond}'] = ptoks\n",
    "\n",
    "    results.append(result)\n",
    "\n",
    "    if (i + 1) % 20 == 0 or i == N_SAMPLES - 1:\n",
    "        ckpt = {\n",
    "            'n_total': N_SAMPLES,\n",
    "            'results': results,\n",
    "            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        }\n",
    "        CHECKPOINT_PATH.write_text(json.dumps(ckpt))\n",
    "        elapsed = time.time() - t0\n",
    "        done = i - start_idx + 1\n",
    "        eta = (N_SAMPLES - i - 1) * elapsed / done if done > 0 else 0\n",
    "        tqdm.write(f\"  Checkpoint {i+1}/{N_SAMPLES} | {elapsed/60:.1f}m | ETA {eta/60:.1f}m\")\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "elapsed = time.time() - t0\n",
    "print(f\"\\nScoring complete: {len(results)} samples, \"\n",
    "      f\"{len(COND_NAMES)} conditions in {elapsed/60:.1f} min\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c8e753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Part 1 — Standard Condition Table\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PART 1: STANDARD CONDITION TABLE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "bare_nlls = np.array([r['nll_bare'] for r in results])\n",
    "oracle_nlls = np.array([r['nll_oracle_trunc'] for r in results])\n",
    "swapped_nlls = np.array([r['nll_swapped_trunc'] for r in results])\n",
    "random_nlls = np.array([r['nll_random_matched_trunc'] for r in results])\n",
    "\n",
    "oracle_benefit = bare_nlls - oracle_nlls\n",
    "oracle_d = cohens_d(oracle_benefit)\n",
    "\n",
    "all_conds = [\n",
    "    ('oracle_trunc', 'Oracle (real query)'),\n",
    "    ('swapped_trunc', 'Swapped (wrong query)'),\n",
    "    ('random_matched_trunc', 'Random matched (structural)'),\n",
    "]\n",
    "\n",
    "alpha_bonf = 0.05 / len(all_conds)\n",
    "\n",
    "print(f\"\\n{'Condition':<35} {'NLL':>8} {'Delta':>8} {'d':>8} \"\n",
    "      f\"{'Win%':>7} {'%Orc':>6} {'p':>12} {'sig':>5}\")\n",
    "print(\"-\" * 95)\n",
    "\n",
    "for cond, desc in all_conds:\n",
    "    nlls = np.array([r[f'nll_{cond}'] for r in results])\n",
    "    benefit = bare_nlls - nlls\n",
    "    d = cohens_d(benefit)\n",
    "    delta = benefit.mean()\n",
    "    win = 100 * np.mean(benefit > 0)\n",
    "    pct = d / oracle_d * 100 if oracle_d > 0 else 0\n",
    "    _, p = stats.ttest_1samp(benefit, 0)\n",
    "    sig = '***' if p < alpha_bonf / 10 else '**' if p < alpha_bonf else '*' if p < 0.05 else 'ns'\n",
    "    print(f\"  {desc:<33} {nlls.mean():>8.4f} {delta:>+8.4f} {d:>+8.3f} \"\n",
    "          f\"{win:>6.1f}% {pct:>5.0f}% {p:>12.2e} {sig}\")\n",
    "\n",
    "print(f\"\\n  bare (lower bound): {bare_nlls.mean():.4f}\")\n",
    "print(f\"  Bonferroni threshold: alpha={alpha_bonf:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8483cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Part 2 — Paired Semantic Contrast\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PART 2: PAIRED SEMANTIC CONTRAST (the key test)\")\n",
    "print(\"=\" * 70)\n",
    "print(\"Per-document: Delta_semantic = swapped_nll - oracle_nll\")\n",
    "print(\"Both conditions have the same structural perturbation (a real query prefix).\")\n",
    "print(\"Only semantic relevance differs.\\n\")\n",
    "\n",
    "semantic_effect = swapped_nlls - oracle_nlls  # positive = oracle is better\n",
    "\n",
    "# Paired t-test\n",
    "t_stat, p_paired = stats.ttest_rel(swapped_nlls, oracle_nlls)\n",
    "d_semantic = cohens_d(semantic_effect)\n",
    "win_oracle = 100 * np.mean(semantic_effect > 0)\n",
    "\n",
    "print(f\"  Mean(swapped_nll - oracle_nll): {semantic_effect.mean():+.4f}\")\n",
    "print(f\"  Cohen's d:                      {d_semantic:+.3f}\")\n",
    "print(f\"  Oracle wins:                    {win_oracle:.1f}%\")\n",
    "print(f\"  Paired t-test:                  t={t_stat:.3f}, p={p_paired:.2e}\")\n",
    "\n",
    "sig = '***' if p_paired < 0.001 else '**' if p_paired < 0.01 else '*' if p_paired < 0.05 else 'ns'\n",
    "print(f\"  Significance:                   {sig}\")\n",
    "\n",
    "if p_paired < 0.05 and d_semantic > 0:\n",
    "    print(f\"\\n  --> SEMANTIC RELEVANCE MATTERS: oracle query produces significantly\")\n",
    "    print(f\"      lower NLL than a swapped query from a different topic.\")\n",
    "    print(f\"      The semantic component is d={d_semantic:+.3f} in paired comparison.\")\n",
    "elif p_paired < 0.05 and d_semantic < 0:\n",
    "    print(f\"\\n  --> REVERSE: swapped query is actually BETTER than oracle.\")\n",
    "    print(f\"      This would suggest semantic interference from the real query.\")\n",
    "else:\n",
    "    print(f\"\\n  --> NO SIGNIFICANT SEMANTIC EFFECT: oracle and swapped queries\")\n",
    "    print(f\"      produce equivalent NLLs. The benefit is purely structural.\")\n",
    "\n",
    "# Context: how does this compare to overall benefit?\n",
    "print(f\"\\n--- Context ---\")\n",
    "print(f\"  Overall oracle benefit (vs bare): d={oracle_d:+.3f}\")\n",
    "print(f\"  Semantic component (paired):      d={d_semantic:+.3f}\")\n",
    "print(f\"  Structural component (estimated): d={oracle_d - d_semantic:+.3f}\")\n",
    "if oracle_d > 0:\n",
    "    sem_frac = d_semantic / oracle_d * 100\n",
    "    print(f\"  Semantic fraction:                {sem_frac:.1f}% of total benefit\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd68805",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Part 3 — Effect Distribution\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PART 3: EFFECT DISTRIBUTION\")\n",
    "print(\"=\" * 70)\n",
    "print(\"Per-sample distribution of swapped_nll - oracle_nll\\n\")\n",
    "\n",
    "# Distribution statistics\n",
    "print(f\"  Mean:   {semantic_effect.mean():+.4f}\")\n",
    "print(f\"  Median: {np.median(semantic_effect):+.4f}\")\n",
    "print(f\"  Std:    {semantic_effect.std():.4f}\")\n",
    "print(f\"  Min:    {semantic_effect.min():+.4f}\")\n",
    "print(f\"  Max:    {semantic_effect.max():+.4f}\")\n",
    "\n",
    "# Fraction showing semantic benefit\n",
    "oracle_better = np.sum(semantic_effect > 0)\n",
    "swapped_better = np.sum(semantic_effect < 0)\n",
    "tied = np.sum(semantic_effect == 0)\n",
    "\n",
    "print(f\"\\n  Oracle better (oracle < swapped): {oracle_better} ({oracle_better/N_SAMPLES*100:.1f}%)\")\n",
    "print(f\"  Swapped better (swapped < oracle): {swapped_better} ({swapped_better/N_SAMPLES*100:.1f}%)\")\n",
    "print(f\"  Tied:                              {tied}\")\n",
    "\n",
    "# Effect size distribution\n",
    "print(f\"\\n--- Effect size distribution ---\")\n",
    "for threshold in [0.01, 0.05, 0.1, 0.2, 0.5]:\n",
    "    n_above = np.sum(semantic_effect > threshold)\n",
    "    n_below = np.sum(semantic_effect < -threshold)\n",
    "    print(f\"  |effect| > {threshold:.2f}: {n_above} oracle wins, {n_below} swapped wins\")\n",
    "\n",
    "# Quintile breakdown of per-sample semantic effect\n",
    "print(f\"\\n--- Per-sample semantic effect by quintile ---\")\n",
    "eff_quintile_bounds = np.percentile(semantic_effect, [20, 40, 60, 80])\n",
    "eff_quintiles = np.digitize(semantic_effect, eff_quintile_bounds)\n",
    "for q in range(5):\n",
    "    mask = eff_quintiles == q\n",
    "    eff_q = semantic_effect[mask]\n",
    "    print(f\"  Q{q+1}: mean={eff_q.mean():+.4f}, range=[{eff_q.min():+.4f}, {eff_q.max():+.4f}], \"\n",
    "          f\"N={mask.sum()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef979d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Part 4 — Predictors of Semantic Benefit\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PART 4: PREDICTORS OF SEMANTIC BENEFIT\")\n",
    "print(\"=\" * 70)\n",
    "print(\"What sample characteristics predict whether oracle > swapped?\\n\")\n",
    "\n",
    "# (a) Query-document vocabulary overlap (Jaccard on content words)\n",
    "jaccard_overlaps = []\n",
    "for i in range(N_SAMPLES):\n",
    "    doc_words = set(re.sub(r'[^\\w\\s]', '', samples[i]['passage'].lower()).split())\n",
    "    doc_content = doc_words - STOP_WORDS\n",
    "    q_words = set(re.sub(r'[^\\w\\s]', '', samples[i]['query'].lower()).split())\n",
    "    q_content = q_words - STOP_WORDS\n",
    "    if len(doc_content | q_content) > 0:\n",
    "        jaccard = len(doc_content & q_content) / len(doc_content | q_content)\n",
    "    else:\n",
    "        jaccard = 0.0\n",
    "    jaccard_overlaps.append(jaccard)\n",
    "jaccard_overlaps = np.array(jaccard_overlaps)\n",
    "\n",
    "# (b) Document length (word count)\n",
    "doc_lengths = np.array([r['passage_words'] for r in results])\n",
    "\n",
    "# (c) Bare NLL (hardness)\n",
    "# bare_nlls already defined\n",
    "\n",
    "# (d) Answer length\n",
    "answer_lengths = np.array([len(r['answer'].split()) for r in results])\n",
    "\n",
    "# (e) Query length\n",
    "query_lengths = np.array([len(r['query'].split()) for r in results])\n",
    "\n",
    "predictors = [\n",
    "    ('Query-doc Jaccard overlap', jaccard_overlaps),\n",
    "    ('Document length (words)', doc_lengths),\n",
    "    ('Bare NLL (hardness)', bare_nlls),\n",
    "    ('Answer length (words)', answer_lengths),\n",
    "    ('Query length (words)', query_lengths),\n",
    "]\n",
    "\n",
    "print(f\"  {'Predictor':<30} {'Pearson r':>10} {'p':>12} {'sig':>5}\")\n",
    "print(f\"  {'-'*62}\")\n",
    "\n",
    "alpha_bonf_pred = 0.05 / len(predictors)\n",
    "\n",
    "for name, values in predictors:\n",
    "    r_val, p_val = stats.pearsonr(values, semantic_effect)\n",
    "    sig = '***' if p_val < alpha_bonf_pred / 10 else '**' if p_val < alpha_bonf_pred else \\\n",
    "          '*' if p_val < 0.05 else 'ns'\n",
    "    print(f\"  {name:<30} {r_val:>+10.3f} {p_val:>12.2e} {sig}\")\n",
    "\n",
    "# Hardness interaction (detailed)\n",
    "print(f\"\\n--- Semantic effect by hardness quintile ---\")\n",
    "quintile_bounds = np.percentile(bare_nlls, [20, 40, 60, 80])\n",
    "quintiles = np.digitize(bare_nlls, quintile_bounds)\n",
    "q_labels = ['Q1 easy', 'Q2', 'Q3', 'Q4', 'Q5 hard']\n",
    "\n",
    "print(f\"  {'Quintile':<12} {'N':>4} {'Bare NLL':>10} {'Sem effect':>12} {'d':>8} \"\n",
    "      f\"{'Win%':>7} {'p':>12} {'sig':>5}\")\n",
    "print(f\"  {'-'*75}\")\n",
    "\n",
    "for q in range(5):\n",
    "    mask = quintiles == q\n",
    "    n = mask.sum()\n",
    "    eff_q = semantic_effect[mask]\n",
    "    d_q = cohens_d(eff_q)\n",
    "    win_q = 100 * np.mean(eff_q > 0)\n",
    "    _, p_q = stats.ttest_1samp(eff_q, 0)\n",
    "    sig_q = '***' if p_q < 0.001 else '**' if p_q < 0.01 else '*' if p_q < 0.05 else 'ns'\n",
    "    print(f\"  {q_labels[q]:<12} {n:>4} {bare_nlls[mask].mean():>10.3f} \"\n",
    "          f\"{eff_q.mean():>+12.4f} {d_q:>+8.3f} {win_q:>6.1f}% {p_q:>12.2e} {sig_q}\")\n",
    "\n",
    "# Jaccard interaction (detailed)\n",
    "print(f\"\\n--- Semantic effect by query-doc overlap quintile ---\")\n",
    "jacc_bounds = np.percentile(jaccard_overlaps, [20, 40, 60, 80])\n",
    "jacc_quints = np.digitize(jaccard_overlaps, jacc_bounds)\n",
    "\n",
    "for q in range(5):\n",
    "    mask = jacc_quints == q\n",
    "    n = mask.sum()\n",
    "    eff_q = semantic_effect[mask]\n",
    "    d_q = cohens_d(eff_q)\n",
    "    jacc_q = jaccard_overlaps[mask].mean()\n",
    "    _, p_q = stats.ttest_1samp(eff_q, 0)\n",
    "    sig_q = '***' if p_q < 0.001 else '**' if p_q < 0.01 else '*' if p_q < 0.05 else 'ns'\n",
    "    print(f\"  Q{q+1} (Jaccard={jacc_q:.3f}, N={n}): effect={eff_q.mean():+.4f}, \"\n",
    "          f\"d={d_q:+.3f}, p={p_q:.2e} {sig_q}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b4c1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Part 5 — Structural Equivalence Check\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PART 5: STRUCTURAL EQUIVALENCE CHECK\")\n",
    "print(\"=\" * 70)\n",
    "print(\"Confirm oracle and swapped have similar structural benefit vs bare.\")\n",
    "print(\"If structural effects differ, the 'semantic' contrast is confounded.\\n\")\n",
    "\n",
    "# Structural benefit: condition vs bare\n",
    "oracle_struct = bare_nlls - oracle_nlls\n",
    "swapped_struct = bare_nlls - swapped_nlls\n",
    "random_struct = bare_nlls - random_nlls\n",
    "\n",
    "oracle_vs_bare_d = cohens_d(oracle_struct)\n",
    "swapped_vs_bare_d = cohens_d(swapped_struct)\n",
    "random_vs_bare_d = cohens_d(random_struct)\n",
    "\n",
    "print(f\"  Condition vs bare (Cohen's d):\")\n",
    "print(f\"    Oracle:  d={oracle_vs_bare_d:+.3f}\")\n",
    "print(f\"    Swapped: d={swapped_vs_bare_d:+.3f}\")\n",
    "print(f\"    Random:  d={random_vs_bare_d:+.3f}\")\n",
    "\n",
    "# Both should have similar structural benefit relative to random\n",
    "oracle_vs_random = random_nlls - oracle_nlls  # semantic component\n",
    "swapped_vs_random = random_nlls - swapped_nlls  # should be ~0 or small\n",
    "\n",
    "d_orac_rand = cohens_d(oracle_vs_random)\n",
    "d_swap_rand = cohens_d(swapped_vs_random)\n",
    "_, p_orac_rand = stats.ttest_1samp(oracle_vs_random, 0)\n",
    "_, p_swap_rand = stats.ttest_1samp(swapped_vs_random, 0)\n",
    "\n",
    "print(f\"\\n  Condition vs random (semantic component):\")\n",
    "print(f\"    Oracle - random:  d={d_orac_rand:+.3f}, p={p_orac_rand:.2e}\")\n",
    "print(f\"    Swapped - random: d={d_swap_rand:+.3f}, p={p_swap_rand:.2e}\")\n",
    "\n",
    "# The key check: oracle and swapped should have similar benefit over random\n",
    "# IF the semantic component is real, oracle should beat random more than swapped\n",
    "structural_diff = oracle_struct - swapped_struct\n",
    "d_struct_diff = cohens_d(structural_diff)\n",
    "_, p_struct = stats.ttest_1samp(structural_diff, 0)\n",
    "sig_struct = '***' if p_struct < 0.001 else '**' if p_struct < 0.01 else '*' if p_struct < 0.05 else 'ns'\n",
    "\n",
    "print(f\"\\n  Oracle benefit - Swapped benefit: d={d_struct_diff:+.3f}, p={p_struct:.2e} {sig_struct}\")\n",
    "print(f\"  (This should equal the semantic effect from Part 2: d={d_semantic:+.3f})\")\n",
    "print(f\"  Consistency check: {abs(d_struct_diff - d_semantic):.4f} (should be ~0)\")\n",
    "\n",
    "# Prefix token count comparison\n",
    "oracle_ptoks = np.array([r['ptoks_oracle_trunc'] for r in results])\n",
    "swapped_ptoks = np.array([r['ptoks_swapped_trunc'] for r in results])\n",
    "random_ptoks = np.array([r['ptoks_random_matched_trunc'] for r in results])\n",
    "\n",
    "print(f\"\\n  Prefix token counts:\")\n",
    "print(f\"    Oracle:  mean={oracle_ptoks.mean():.1f}, std={oracle_ptoks.std():.1f}\")\n",
    "print(f\"    Swapped: mean={swapped_ptoks.mean():.1f}, std={swapped_ptoks.std():.1f}\")\n",
    "print(f\"    Random:  mean={random_ptoks.mean():.1f}, std={random_ptoks.std():.1f}\")\n",
    "\n",
    "# Are token counts correlated with semantic effect? (potential confound)\n",
    "ptok_diff = swapped_ptoks - oracle_ptoks\n",
    "r_ptok, p_ptok = stats.pearsonr(ptok_diff.astype(float), semantic_effect)\n",
    "sig_ptok = '***' if p_ptok < 0.001 else '**' if p_ptok < 0.01 else '*' if p_ptok < 0.05 else 'ns'\n",
    "print(f\"\\n  Token count diff vs semantic effect: r={r_ptok:+.3f}, p={p_ptok:.2e} {sig_ptok}\")\n",
    "\n",
    "if abs(r_ptok) > 0.1 and p_ptok < 0.05:\n",
    "    print(f\"  WARNING: token count difference correlates with semantic effect.\")\n",
    "    print(f\"  The 'semantic' contrast may be partially confounded by prefix length.\")\n",
    "else:\n",
    "    print(f\"  CLEAN: no confound from prefix length differences.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70db438f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Synthesis + Save\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"SYNTHESIS: SWAPPED-QUERY PAIRED CONTRAST RESULTS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# 1. Summary\n",
    "print(f\"\\n1. CONDITION TABLE:\")\n",
    "print(f\"   {'Condition':<25} {'d vs bare':>10} {'%Oracle':>8}\")\n",
    "print(f\"   {'-'*45}\")\n",
    "for cond, desc in all_conds:\n",
    "    nlls = np.array([r[f'nll_{cond}'] for r in results])\n",
    "    d = cohens_d(bare_nlls - nlls)\n",
    "    pct = d / oracle_d * 100 if oracle_d > 0 else 0\n",
    "    print(f\"   {desc:<25} {d:>+10.3f} {pct:>7.0f}%\")\n",
    "\n",
    "# 2. The key result\n",
    "print(f\"\\n2. PAIRED SEMANTIC CONTRAST:\")\n",
    "print(f\"   swapped_nll - oracle_nll: mean={semantic_effect.mean():+.4f}, d={d_semantic:+.3f}\")\n",
    "print(f\"   Oracle win rate: {win_oracle:.1f}%, p={p_paired:.2e}\")\n",
    "if oracle_d > 0:\n",
    "    sem_frac = d_semantic / oracle_d * 100\n",
    "    print(f\"   Semantic fraction of total benefit: {sem_frac:.1f}%\")\n",
    "\n",
    "# 3. Strongest predictor\n",
    "print(f\"\\n3. PREDICTORS OF SEMANTIC BENEFIT:\")\n",
    "best_r = 0\n",
    "best_name = \"\"\n",
    "for name, values in predictors:\n",
    "    r_val, _ = stats.pearsonr(values, semantic_effect)\n",
    "    if abs(r_val) > abs(best_r):\n",
    "        best_r = r_val\n",
    "        best_name = name\n",
    "print(f\"   Strongest: {best_name} (r={best_r:+.3f})\")\n",
    "\n",
    "# 4. Structural equivalence\n",
    "print(f\"\\n4. STRUCTURAL EQUIVALENCE:\")\n",
    "print(f\"   Oracle vs bare:  d={oracle_vs_bare_d:+.3f}\")\n",
    "print(f\"   Swapped vs bare: d={swapped_vs_bare_d:+.3f}\")\n",
    "print(f\"   Token count confound: r={r_ptok:+.3f} ({'CLEAN' if abs(r_ptok) < 0.1 else 'WARNING'})\")\n",
    "\n",
    "# 5. Conclusions\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"CONCLUSIONS:\")\n",
    "\n",
    "if p_paired < 0.001 and d_semantic > 0.05:\n",
    "    print(f\"  1. STRONG SEMANTIC SIGNAL: oracle significantly beats swapped\")\n",
    "    print(f\"     (d={d_semantic:+.3f}, p={p_paired:.2e})\")\n",
    "    conclusion = \"STRONG_SEMANTIC\"\n",
    "elif p_paired < 0.05 and d_semantic > 0:\n",
    "    print(f\"  1. WEAK SEMANTIC SIGNAL: marginally significant\")\n",
    "    print(f\"     (d={d_semantic:+.3f}, p={p_paired:.2e})\")\n",
    "    conclusion = \"WEAK_SEMANTIC\"\n",
    "elif p_paired < 0.05 and d_semantic < 0:\n",
    "    print(f\"  1. SEMANTIC INTERFERENCE: swapped query is actually BETTER\")\n",
    "    print(f\"     (d={d_semantic:+.3f}, p={p_paired:.2e})\")\n",
    "    conclusion = \"INTERFERENCE\"\n",
    "else:\n",
    "    print(f\"  1. NO SEMANTIC EFFECT: oracle and swapped are equivalent\")\n",
    "    print(f\"     (d={d_semantic:+.3f}, p={p_paired:.2e})\")\n",
    "    conclusion = \"NO_EFFECT\"\n",
    "\n",
    "if abs(r_ptok) < 0.1:\n",
    "    print(f\"  2. Result is CLEAN: no confound from prefix length\")\n",
    "else:\n",
    "    print(f\"  2. Result is CONFOUNDED: token count correlates with effect (r={r_ptok:+.3f})\")\n",
    "\n",
    "# Cross-reference with Exp 12\n",
    "print(f\"\\n  Cross-reference with Exp 12:\")\n",
    "print(f\"  Exp 12 tests the GRADIENT; this experiment confirms/denies the binary signal.\")\n",
    "print(f\"  If d_semantic > 0 here, the gradient in Exp 12 should be monotonic.\")\n",
    "print(f\"  If d_semantic ~ 0 here, any gradient in Exp 12 is likely noise.\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "# Save results\n",
    "final_results = {\n",
    "    'experiment': 'exp13_swapped_query',\n",
    "    'model': MODEL_NAME,\n",
    "    'dataset': 'ms_marco_v1.1',\n",
    "    'n_samples': N_SAMPLES,\n",
    "    'seed': SEED,\n",
    "    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'baseline': {\n",
    "        'bare_nll': float(bare_nlls.mean()),\n",
    "        'oracle_d': float(oracle_d),\n",
    "    },\n",
    "    'semantic_contrast': {\n",
    "        'mean_effect': float(semantic_effect.mean()),\n",
    "        'd': float(d_semantic),\n",
    "        'win_pct': float(win_oracle),\n",
    "        'p_paired': float(p_paired),\n",
    "        'semantic_fraction_pct': float(sem_frac) if oracle_d > 0 else None,\n",
    "    },\n",
    "    'structural_equivalence': {\n",
    "        'oracle_vs_bare_d': float(oracle_vs_bare_d),\n",
    "        'swapped_vs_bare_d': float(swapped_vs_bare_d),\n",
    "        'random_vs_bare_d': float(random_vs_bare_d),\n",
    "        'token_count_confound_r': float(r_ptok),\n",
    "        'token_count_confound_p': float(p_ptok),\n",
    "    },\n",
    "    'predictors': {},\n",
    "    'conditions': {},\n",
    "    'conclusion': conclusion,\n",
    "}\n",
    "\n",
    "for name, values in predictors:\n",
    "    r_val, p_val = stats.pearsonr(values, semantic_effect)\n",
    "    final_results['predictors'][name] = {\n",
    "        'pearson_r': float(r_val),\n",
    "        'p': float(p_val),\n",
    "    }\n",
    "\n",
    "for cond, desc in all_conds:\n",
    "    nlls = np.array([r[f'nll_{cond}'] for r in results])\n",
    "    benefit = bare_nlls - nlls\n",
    "    d = cohens_d(benefit)\n",
    "    _, p = stats.ttest_1samp(benefit, 0)\n",
    "    final_results['conditions'][cond] = {\n",
    "        'description': desc,\n",
    "        'd': float(d),\n",
    "        'mean_nll': float(nlls.mean()),\n",
    "        'mean_delta': float(benefit.mean()),\n",
    "        'pct_oracle': float(d / oracle_d * 100) if oracle_d > 0 else 0,\n",
    "        'p': float(p),\n",
    "    }\n",
    "\n",
    "with open(RESULTS_DIR / 'results.json', 'w') as f:\n",
    "    json.dump(final_results, f, indent=2)\n",
    "print(f\"\\nResults saved to {RESULTS_DIR / 'results.json'}\")\n",
    "\n",
    "# Cleanup\n",
    "print(f\"\\nCleaning up GPU memory...\")\n",
    "mem_before = torch.cuda.memory_allocated() / 1e9\n",
    "del model, processor, tokenizer\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "mem_after = torch.cuda.memory_allocated() / 1e9\n",
    "print(f\"GPU memory: {mem_before:.2f} GB -> {mem_after:.2f} GB\")\n",
    "print(\"Done!\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
