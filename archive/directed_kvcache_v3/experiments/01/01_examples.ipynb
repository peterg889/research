{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5f35ee1",
   "metadata": {},
   "source": [
    "# Experiment 01: Truncation Test â€” Condition Examples\n",
    "\n",
    "This notebook shows the actual text for each experimental condition using real data from the dataset. No GPU needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8683c5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, json, re\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "sys.path.insert(0, \"../..\")\n",
    "from lib.data import count_words\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "# ---- Load MS MARCO (same reconstruction as Exp 01/02/etc.) ----\n",
    "from datasets import load_dataset\n",
    "ds = load_dataset(\"microsoft/ms_marco\", \"v1.1\", split=\"validation\")\n",
    "\n",
    "samples = []\n",
    "for item in ds:\n",
    "    if len(samples) >= 1500:\n",
    "        break\n",
    "    passages = item.get('passages', {})\n",
    "    ptexts = passages.get('passage_text', [])\n",
    "    is_sel = passages.get('is_selected', [])\n",
    "    query = item.get('query', '')\n",
    "    answers = item.get('answers', [])\n",
    "    well_formed = item.get('wellFormedAnswers', [])\n",
    "    answer = None\n",
    "    if well_formed and len(well_formed) > 0 and well_formed[0] not in ('[]', ''):\n",
    "        answer = well_formed[0]\n",
    "    elif answers and len(answers) > 0 and answers[0] != 'No Answer Present.':\n",
    "        answer = answers[0]\n",
    "    if not answer:\n",
    "        continue\n",
    "    for pt, sel in zip(ptexts, is_sel):\n",
    "        wc = count_words(pt)\n",
    "        if sel == 1 and 30 <= wc <= 300:\n",
    "            samples.append({\n",
    "                'passage': pt, 'query': query, 'answer': answer,\n",
    "                'word_count': wc,\n",
    "            })\n",
    "            break\n",
    "\n",
    "np.random.seed(SEED)\n",
    "np.random.shuffle(samples)\n",
    "samples = samples[:500]\n",
    "del ds\n",
    "\n",
    "STOP_WORDS = {\n",
    "    'a', 'an', 'the', 'is', 'are', 'was', 'were', 'be', 'been', 'being',\n",
    "    'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could',\n",
    "    'should', 'may', 'might', 'can', 'shall', 'to', 'of', 'in', 'for',\n",
    "    'on', 'with', 'at', 'by', 'from', 'as', 'into', 'through', 'during',\n",
    "    'before', 'after', 'above', 'below', 'between', 'and', 'but', 'or',\n",
    "    'not', 'no', 'if', 'then', 'than', 'so', 'up', 'out', 'about',\n",
    "    'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those',\n",
    "    'it', 'its', 'i', 'me', 'my', 'we', 'our', 'you', 'your', 'he',\n",
    "    'him', 'his', 'she', 'her', 'they', 'them', 'their', 'how', 'when',\n",
    "    'where', 'why', 'much', 'many', 'some', 'any', 'all', 'each',\n",
    "    'does', 'also', 'just', 'more', 'most', 'very', 'too', 'only',\n",
    "}\n",
    "\n",
    "def extract_keywords(text):\n",
    "    words = re.sub(r'[^\\w\\s]', '', text.lower()).split()\n",
    "    return [w for w in words if w not in STOP_WORDS and len(w) > 2]\n",
    "\n",
    "def make_surrogate_paraphrase(query):\n",
    "    keywords = extract_keywords(query)\n",
    "    return \" \".join(keywords[::-1]) if keywords else query\n",
    "\n",
    "def make_surrogate_from_doc(passage):\n",
    "    content_words = extract_keywords(passage)\n",
    "    if not content_words:\n",
    "        return \"information\"\n",
    "    counts = Counter(content_words)\n",
    "    return \" \".join(w for w, _ in counts.most_common(5))\n",
    "\n",
    "def make_surrogate_template(passage):\n",
    "    content_words = extract_keywords(passage)\n",
    "    if not content_words:\n",
    "        return \"What is this about?\"\n",
    "    counts = Counter(content_words)\n",
    "    top_word = counts.most_common(1)[0][0]\n",
    "    return f\"What is {top_word}?\"\n",
    "\n",
    "# Verify against checkpoint\n",
    "def verify_checkpoint(exp_name):\n",
    "    ckpt_path = Path(f\"../../results/{exp_name}/checkpoint.json\")\n",
    "    if ckpt_path.exists():\n",
    "        ckpt = json.loads(ckpt_path.read_text())\n",
    "        results = ckpt.get('results', [])\n",
    "        if results and results[0].get('query', '')[:50] == samples[0]['query'][:50]:\n",
    "            print(f\"  Checkpoint verification: MATCH ({exp_name})\")\n",
    "            return True\n",
    "        elif results:\n",
    "            print(f\"  Checkpoint verification: MISMATCH ({exp_name})\")\n",
    "            print(f\"    Checkpoint: {results[0].get('query', '')[:50]}\")\n",
    "            print(f\"    Samples:    {samples[0]['query'][:50]}\")\n",
    "            return False\n",
    "    else:\n",
    "        print(f\"  No checkpoint found for {exp_name}\")\n",
    "    return None\n",
    "\n",
    "print(f\"Loaded {len(samples)} MS MARCO samples (SEED={SEED})\")\n",
    "print(f\"Sample 0 query: {samples[0]['query'][:70]}\")\n",
    "\n",
    "\n",
    "def show_sample(s, doc_key='passage', n=0):\n",
    "    # Show sample info\n",
    "    doc = s[doc_key]\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"SAMPLE {n}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"  Query:    {s['query']}\")\n",
    "    print(f\"  Answer:   {s['answer']}\")\n",
    "    print(f\"  Document: {doc[:100]}...\")\n",
    "    print(f\"  Doc words: {len(doc.split())}\")\n",
    "    print()\n",
    "\n",
    "def show_conditions(conditions, doc_text):\n",
    "    # conditions: list of (name, description, encoder_prefix_text_or_None)\n",
    "    # For bare conditions, encoder_prefix_text is None\n",
    "    print(f\"{'Condition':<30} {'Prefix':<14} {'Encoder input (first 70 chars)'}\")\n",
    "    print(f\"{'-'*100}\")\n",
    "    for name, desc, prefix_text in conditions:\n",
    "        if prefix_text is None:\n",
    "            enc_preview = doc_text[:70]\n",
    "            print(f\"{name:<30} {'(none)':<14} {enc_preview}...\")\n",
    "        else:\n",
    "            enc_text = prefix_text + \"\\n\" + doc_text\n",
    "            print(f\"{name:<30} {str(len(prefix_text.split()))+'w':<14} {enc_text[:70]}...\")\n",
    "        if desc:\n",
    "            print(f\"  {'':>28} ^ {desc}\")\n",
    "    print()\n",
    "\n",
    "\n",
    "verify_checkpoint(\"exp01\")\n",
    "\n",
    "ex = samples[0]\n",
    "surr_para = make_surrogate_paraphrase(ex['query'])\n",
    "surr_doc_kw = make_surrogate_from_doc(ex['passage'])\n",
    "doc_short = ex['passage'][:80]\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"SAMPLE\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"  Query:      {ex['query']}\")\n",
    "print(f\"  Answer:     {ex['answer']}\")\n",
    "print(f\"  Document:   {doc_short}...\")\n",
    "print(f\"  Doc words:  {len(ex['passage'].split())}\")\n",
    "print()\n",
    "print(\"=\" * 80)\n",
    "print(\"HOW THIS EXPERIMENT WORKS\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "print(\"  The T5Gemma encoder-decoder has two stages:\")\n",
    "print()\n",
    "print(\"    1. ENCODER: reads text with bidirectional attention (sees everything)\")\n",
    "print(\"    2. DECODER: generates the answer, cross-attending to encoder output\")\n",
    "print()\n",
    "print(\"  We vary two things:\")\n",
    "print(\"    - What PREFIX (if any) is prepended to the document in the encoder\")\n",
    "print(\"    - Whether the decoder can see the prefix tokens (full) or only the\")\n",
    "print(\"      document tokens (trunc = truncated cross-attention)\")\n",
    "print()\n",
    "print(\"  The decoder always scores the same answer text via NLL.\")\n",
    "print()\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"CONDITIONS (7 total)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print()\n",
    "print(\"--- CONDITION 1: bare (BASELINE) ---\")\n",
    "print()\n",
    "print(\"  Encoder input:      [document]\")\n",
    "print(\"  Decoder attends to: [document]\")\n",
    "print()\n",
    "print(\"  No prefix. This is the control -- how well does the model predict\")\n",
    "print(\"  the answer from the document alone?\")\n",
    "print()\n",
    "print(f\"  Encoder sees: \\\"{doc_short}...\\\"\")\n",
    "\n",
    "print()\n",
    "print(\"--- CONDITION 2: oracle_full ---\")\n",
    "print()\n",
    "print(f\"  Prefix:             \\\"{ex['query']}\\\"\")\n",
    "print(f\"  Encoder input:      [prefix] + [document]\")\n",
    "print(f\"  Decoder attends to: [prefix + document]  <-- decoder CAN read the query\")\n",
    "print()\n",
    "print(\"  Upper bound. But is the benefit because the decoder reads the query\")\n",
    "print(\"  directly, or because co-encoding improved the document representations?\")\n",
    "\n",
    "print()\n",
    "print(\"--- CONDITION 3: oracle_trunc  *** THE KEY CONDITION ***\")\n",
    "print()\n",
    "print(f\"  Prefix:             \\\"{ex['query']}\\\"\")\n",
    "print(f\"  Encoder input:      [prefix] + [document]  (same as oracle_full)\")\n",
    "print(f\"  Decoder attends to: [document ONLY]  <-- prefix tokens MASKED\")\n",
    "print()\n",
    "print(\"  Same encoder input as oracle_full, but the decoder CANNOT see the\")\n",
    "print(\"  query tokens. If this still beats bare, the document representations\")\n",
    "print(\"  themselves are improved by co-encoding with the query.\")\n",
    "\n",
    "print()\n",
    "print(\"--- CONDITION 4: surr_para_full ---\")\n",
    "print()\n",
    "print(f\"  Prefix:             \\\"{surr_para}\\\"  (query keywords reversed)\")\n",
    "print(f\"  Encoder input:      [prefix] + [document]\")\n",
    "print(f\"  Decoder attends to: [prefix + document]\")\n",
    "\n",
    "print()\n",
    "print(\"--- CONDITION 5: surr_para_trunc ---\")\n",
    "print()\n",
    "print(f\"  Prefix:             \\\"{surr_para}\\\"  (query keywords reversed)\")\n",
    "print(f\"  Encoder input:      [prefix] + [document]\")\n",
    "print(f\"  Decoder attends to: [document ONLY]  <-- prefix MASKED\")\n",
    "\n",
    "print()\n",
    "print(\"--- CONDITION 6: surr_doc_full ---\")\n",
    "print()\n",
    "print(f\"  Prefix:             \\\"{surr_doc_kw}\\\"  (top-5 TF keywords from document)\")\n",
    "print(f\"  Encoder input:      [prefix] + [document]\")\n",
    "print(f\"  Decoder attends to: [prefix + document]\")\n",
    "\n",
    "print()\n",
    "print(\"--- CONDITION 7: surr_doc_trunc ---\")\n",
    "print()\n",
    "print(f\"  Prefix:             \\\"{surr_doc_kw}\\\"  (top-5 TF keywords from document)\")\n",
    "print(f\"  Encoder input:      [prefix] + [document]\")\n",
    "print(f\"  Decoder attends to: [document ONLY]  <-- prefix MASKED\")\n",
    "\n",
    "print()\n",
    "print(\"=\" * 80)\n",
    "print(\"WHAT TO LOOK FOR IN RESULTS\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "print(\"  1. oracle_full >> bare?\")\n",
    "print(\"     Expected yes (replicates v2 Exp 33b, d ~ +0.35).\")\n",
    "print()\n",
    "print(\"  2. oracle_trunc > bare?\")\n",
    "print(\"     If YES: document reps are genuinely improved by co-encoding.\")\n",
    "print(\"     This is the key finding -- the benefit isn't just the decoder\")\n",
    "print(\"     reading the query from the encoder output.\")\n",
    "print()\n",
    "print(\"  3. oracle_trunc / oracle_full retention %?\")\n",
    "print(\"     High (>50%): most benefit is from improved doc representations.\")\n",
    "print(\"     Low  (<20%): decoder was mostly just reading the query directly.\")\n",
    "print()\n",
    "print(\"  4. surr_*_trunc > bare?\")\n",
    "print(\"     Do surrogate prefixes also improve doc representations,\")\n",
    "print(\"     even when the decoder can't see them?\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
