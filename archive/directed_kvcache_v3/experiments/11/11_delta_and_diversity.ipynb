{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 11: Delta-as-Feature and Prefix Diversity for Ranking\n",
    "## Can we extract better ranking signals from the structural mechanism?\n",
    "\n",
    "### Context\n",
    "Exp 04A found a paradox: **structural surrogates improve ranking but oracle doesn't**.\n",
    "- bare AUC = 0.845\n",
    "- oracle\\_trunc AUC = 0.853 (ns, d=-0.007 differential)\n",
    "- surr\\_doc\\_trunc AUC = 0.867 (\\*\\*, d=+0.053 differential)\n",
    "- static\\_fact\\_trunc AUC = 0.860 (\\*\\*, d=+0.153 differential)\n",
    "- random\\_trunc AUC = 0.866 (\\*\\*, d=+0.117 differential)\n",
    "\n",
    "The structural surrogates create **differential signal**: they help relevant passages\n",
    "MORE than irrelevant ones. This experiment explores two ways to exploit this:\n",
    "\n",
    "### Phase A: Delta-as-Feature (from existing 04A data, no GPU needed)\n",
    "For each passage, define: `delta = NLL_bare - NLL_primed` (positive = priming helped).\n",
    "Since delta is larger for relevant passages (positive differential from 04A):\n",
    "1. Rank by delta alone -- does higher delta = more relevant?\n",
    "2. Linear combination: `score = (1-lambda)*NLL_bare + lambda*NLL_primed` -- optimal weighting\n",
    "3. Ensemble: average NLL across multiple primed conditions\n",
    "\n",
    "### Phase B: Prefix Diversity (new GPU scoring, ~4.4 hours)\n",
    "Score each passage with K=10 DIFFERENT random prefixes (all with truncation).\n",
    "Compute per-passage NLL mean and std across prefixes.\n",
    "\n",
    "**Hypothesis**: Relevant passages respond more consistently to structural perturbation\n",
    "(lower NLL variance), while irrelevant passages show more erratic responses.\n",
    "\n",
    "### N=400 queries (same as Exp 04A), K=10 random prefixes, Bonferroni=8\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 2: Setup\n",
    "import os\n",
    "os.umask(0o000)\n",
    "\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "import gc\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "from collections import Counter, defaultdict\n",
    "from scipy import stats\n",
    "from scipy.stats import wilcoxon, pearsonr, spearmanr\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "sys.path.insert(0, \"../..\")\n",
    "\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "RESULTS_DIR = Path(\"../../results/exp11\")\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "N_SAMPLES = 400\n",
    "MODEL_NAME = \"google/t5gemma-2-4b-4b\"\n",
    "K_PREFIXES = 10\n",
    "N_BONFERRONI = 8\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv())\n",
    "HF_TOKEN = os.environ.get(\"HF_TOKEN\")\n",
    "\n",
    "print(f\"Exp 11: Delta-as-Feature and Prefix Diversity for Ranking\")\n",
    "print(f\"N queries: {N_SAMPLES}\")\n",
    "print(f\"K random prefixes: {K_PREFIXES}\")\n",
    "print(f\"Bonferroni comparisons: {N_BONFERRONI}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 3: Load Exp 04A checkpoint and compute delta features\n",
    "from lib.analysis import cohens_d\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"LOADING EXP 04A DATA\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "EXP04A_CKPT = Path(\"../../results/exp04a/checkpoint.json\")\n",
    "assert EXP04A_CKPT.exists(), f\"Exp 04A checkpoint not found at {EXP04A_CKPT}\"\n",
    "\n",
    "with open(EXP04A_CKPT) as f:\n",
    "    exp04a = json.load(f)\n",
    "\n",
    "results_04a = exp04a['results']\n",
    "print(f\"Loaded {len(results_04a)} queries from Exp 04A\")\n",
    "\n",
    "COND_NAMES_04A = ['bare', 'oracle_trunc', 'surr_template_trunc',\n",
    "                  'surr_doc_trunc', 'random_trunc', 'static_fact_trunc']\n",
    "PRIMED_CONDITIONS = COND_NAMES_04A[1:]\n",
    "\n",
    "# Ranking metrics (reused throughout)\n",
    "def compute_auc(nlls, relevant_idx):\n",
    "    rel_nll = nlls[relevant_idx]\n",
    "    irrel_nlls = [nlls[i] for i in range(len(nlls)) if i != relevant_idx]\n",
    "    if len(irrel_nlls) == 0:\n",
    "        return 0.5\n",
    "    wins = sum(1 for nll in irrel_nlls if nll > rel_nll)\n",
    "    ties = sum(1 for nll in irrel_nlls if nll == rel_nll)\n",
    "    return (wins + 0.5 * ties) / len(irrel_nlls)\n",
    "\n",
    "def compute_mrr_at_k(nlls, relevant_idx, k=3):\n",
    "    ranked = list(np.argsort(nlls))\n",
    "    for rank, idx in enumerate(ranked[:k], 1):\n",
    "        if idx == relevant_idx:\n",
    "            return 1.0 / rank\n",
    "    return 0.0\n",
    "\n",
    "def compute_hit_at_k(nlls, relevant_idx, k=1):\n",
    "    ranked = set(np.argsort(nlls)[:k].tolist())\n",
    "    return 1.0 if relevant_idx in ranked else 0.0\n",
    "\n",
    "# Compute delta features: delta = NLL_bare - NLL_primed\n",
    "print(f\"\\nComputing delta features for {len(PRIMED_CONDITIONS)} conditions...\")\n",
    "for r in results_04a:\n",
    "    bare_nlls = np.array(r['scores']['bare'])\n",
    "    r['deltas'] = {}\n",
    "    for cond in PRIMED_CONDITIONS:\n",
    "        primed_nlls = np.array(r['scores'][cond])\n",
    "        r['deltas'][cond] = (bare_nlls - primed_nlls).tolist()\n",
    "\n",
    "# Bare AUC reference\n",
    "bare_aucs_ref = np.array([\n",
    "    compute_auc(np.array(r['scores']['bare']), r['relevant_idx'])\n",
    "    for r in results_04a\n",
    "])\n",
    "print(f\"Bare AUC reference: {bare_aucs_ref.mean():.3f}\")\n",
    "\n",
    "# Correlation: delta vs bare NLL\n",
    "print(f\"\\n--- Delta vs bare NLL correlation ---\")\n",
    "for cond in PRIMED_CONDITIONS:\n",
    "    all_bare, all_delta = [], []\n",
    "    for r in results_04a:\n",
    "        all_bare.extend(r['scores']['bare'])\n",
    "        all_delta.extend(r['deltas'][cond])\n",
    "    r_s, p_s = spearmanr(all_bare, all_delta)\n",
    "    print(f\"  {cond:<22s}: Spearman rho={r_s:+.3f} (p={p_s:.2e})\")\n",
    "\n",
    "# Delta by relevance\n",
    "print(f\"\\n--- Mean delta by relevance ---\")\n",
    "print(f\"  {'Condition':<22s} {'delta_rel':>10} {'delta_irrel':>12} {'diff':>8}\")\n",
    "for cond in PRIMED_CONDITIONS:\n",
    "    delta_rels, delta_irrels = [], []\n",
    "    for r in results_04a:\n",
    "        rel_idx = r['relevant_idx']\n",
    "        deltas = r['deltas'][cond]\n",
    "        delta_rels.append(deltas[rel_idx])\n",
    "        for i, d in enumerate(deltas):\n",
    "            if i != rel_idx:\n",
    "                delta_irrels.append(d)\n",
    "    print(f\"  {cond:<22s} {np.mean(delta_rels):>+10.4f} {np.mean(delta_irrels):>+12.4f} \"\n",
    "          f\"{np.mean(delta_rels) - np.mean(delta_irrels):>+8.4f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 4: Phase A -- Delta-only ranking, lambda sweep, ensemble\n",
    "print(\"=\" * 70)\n",
    "print(\"PHASE A: DELTA-AS-FEATURE RANKING\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# === Test 1: Rank by delta alone ===\n",
    "print(f\"\\n--- Test 1: Rank by delta alone (higher delta = more relevant) ---\")\n",
    "print(f\"  {'Condition':<22s} {'AUC':>7} {'MRR@3':>7} {'Hit@1':>7} {'Hit@3':>7}\")\n",
    "print(f\"  {'-'*55}\")\n",
    "\n",
    "delta_only_aucs = {}\n",
    "for cond in PRIMED_CONDITIONS:\n",
    "    aucs, mrr3s, hit1s, hit3s = [], [], [], []\n",
    "    for r in results_04a:\n",
    "        rel_idx = r['relevant_idx']\n",
    "        neg_deltas = [-d for d in r['deltas'][cond]]\n",
    "        aucs.append(compute_auc(neg_deltas, rel_idx))\n",
    "        mrr3s.append(compute_mrr_at_k(neg_deltas, rel_idx, k=3))\n",
    "        hit1s.append(compute_hit_at_k(neg_deltas, rel_idx, k=1))\n",
    "        hit3s.append(compute_hit_at_k(neg_deltas, rel_idx, k=3))\n",
    "    delta_only_aucs[cond] = np.array(aucs)\n",
    "    print(f\"  {cond:<22s} {np.mean(aucs):>7.3f} {np.mean(mrr3s):>7.3f} \"\n",
    "          f\"{np.mean(hit1s):>7.3f} {np.mean(hit3s):>7.3f}\")\n",
    "\n",
    "print(f\"  {'bare NLL (ref)':<22s} {bare_aucs_ref.mean():>7.3f}\")\n",
    "\n",
    "# Test delta-only vs chance\n",
    "print(f\"\\n  Delta-only vs chance (AUC=0.5):\")\n",
    "for cond in PRIMED_CONDITIONS:\n",
    "    d = cohens_d(delta_only_aucs[cond] - 0.5)\n",
    "    nonzero = delta_only_aucs[cond] - 0.5\n",
    "    nonzero = nonzero[nonzero != 0]\n",
    "    _, p = wilcoxon(nonzero) if len(nonzero) >= 10 else (0, 1.0)\n",
    "    sig = ('***' if p < 0.001/N_BONFERRONI else '**' if p < 0.01/N_BONFERRONI\n",
    "           else '*' if p < 0.05/N_BONFERRONI else 'ns')\n",
    "    print(f\"    {cond:<22s}: AUC={delta_only_aucs[cond].mean():.3f}, d={d:+.3f}, p={p:.2e} {sig}\")\n",
    "\n",
    "# === Test 2: Lambda sweep ===\n",
    "print(f\"\\n--- Test 2: Lambda sweep: score = (1-lam)*NLL_bare + lam*NLL_primed ---\")\n",
    "LAMBDAS = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 1.5, 2.0, 3.0]\n",
    "\n",
    "lambda_results = {}\n",
    "for cond in PRIMED_CONDITIONS:\n",
    "    best_auc = 0\n",
    "    best_lambda = 0\n",
    "    lambda_aucs = {}\n",
    "\n",
    "    for lam in LAMBDAS:\n",
    "        aucs = []\n",
    "        for r in results_04a:\n",
    "            bare_nlls = np.array(r['scores']['bare'])\n",
    "            primed_nlls = np.array(r['scores'][cond])\n",
    "            combined = (1 - lam) * bare_nlls + lam * primed_nlls\n",
    "            aucs.append(compute_auc(combined, r['relevant_idx']))\n",
    "        mean_auc = np.mean(aucs)\n",
    "        lambda_aucs[lam] = np.array(aucs)\n",
    "        if mean_auc > best_auc:\n",
    "            best_auc = mean_auc\n",
    "            best_lambda = lam\n",
    "\n",
    "    lambda_results[cond] = {\n",
    "        'best_lambda': best_lambda,\n",
    "        'best_auc': best_auc,\n",
    "        'bare_auc': float(lambda_aucs[0.0].mean()),\n",
    "        'primed_auc': float(lambda_aucs[1.0].mean()),\n",
    "        'aucs_by_lambda': {str(l): float(a.mean()) for l, a in lambda_aucs.items()},\n",
    "        'best_lambda_aucs': lambda_aucs[best_lambda],\n",
    "    }\n",
    "\n",
    "print(f\"\\n  {'Condition':<22s} {'Best lam':>8} {'Best AUC':>9} {'bare':>7} {'Gain':>7}\")\n",
    "print(f\"  {'-'*58}\")\n",
    "for cond in PRIMED_CONDITIONS:\n",
    "    lr = lambda_results[cond]\n",
    "    gain = lr['best_auc'] - lr['bare_auc']\n",
    "    print(f\"  {cond:<22s} {lr['best_lambda']:>8.1f} {lr['best_auc']:>9.3f} \"\n",
    "          f\"{lr['bare_auc']:>7.3f} {gain:>+7.3f}\")\n",
    "\n",
    "# Test best lambda vs bare\n",
    "print(f\"\\n  Best lambda vs bare (Wilcoxon):\")\n",
    "for cond in PRIMED_CONDITIONS:\n",
    "    lr = lambda_results[cond]\n",
    "    diff = lr['best_lambda_aucs'] - bare_aucs_ref\n",
    "    d = cohens_d(diff)\n",
    "    nonzero = diff[diff != 0]\n",
    "    _, p = wilcoxon(nonzero) if len(nonzero) >= 10 else (0, 1.0)\n",
    "    sig = ('***' if p < 0.001/N_BONFERRONI else '**' if p < 0.01/N_BONFERRONI\n",
    "           else '*' if p < 0.05/N_BONFERRONI else 'ns')\n",
    "    print(f\"    {cond:<22s}: lam={lr['best_lambda']:.1f}, AUC={lr['best_auc']:.3f}, \"\n",
    "          f\"d={d:+.3f}, p={p:.2e} {sig}\")\n",
    "\n",
    "# === Test 3: Ensemble ===\n",
    "print(f\"\\n--- Test 3: Ensemble -- average NLL across conditions ---\")\n",
    "\n",
    "structural_conds = ['surr_template_trunc', 'surr_doc_trunc', 'random_trunc', 'static_fact_trunc']\n",
    "struct_ensemble_aucs = []\n",
    "for r in results_04a:\n",
    "    mean_nlls = np.zeros(r['n_passages'])\n",
    "    for cond in structural_conds:\n",
    "        mean_nlls += np.array(r['scores'][cond])\n",
    "    mean_nlls /= len(structural_conds)\n",
    "    struct_ensemble_aucs.append(compute_auc(mean_nlls, r['relevant_idx']))\n",
    "struct_ensemble_aucs = np.array(struct_ensemble_aucs)\n",
    "\n",
    "diff = struct_ensemble_aucs - bare_aucs_ref\n",
    "d = cohens_d(diff)\n",
    "nonzero = diff[diff != 0]\n",
    "_, p = wilcoxon(nonzero) if len(nonzero) >= 10 else (0, 1.0)\n",
    "sig = ('***' if p < 0.001/N_BONFERRONI else '**' if p < 0.01/N_BONFERRONI\n",
    "       else '*' if p < 0.05/N_BONFERRONI else 'ns')\n",
    "print(f\"  Structural (4 conds): AUC={struct_ensemble_aucs.mean():.3f}, \"\n",
    "      f\"d={d:+.3f}, p={p:.2e} {sig}\")\n",
    "\n",
    "all5_aucs = []\n",
    "for r in results_04a:\n",
    "    mean_nlls = np.zeros(r['n_passages'])\n",
    "    for cond in PRIMED_CONDITIONS:\n",
    "        mean_nlls += np.array(r['scores'][cond])\n",
    "    mean_nlls /= len(PRIMED_CONDITIONS)\n",
    "    all5_aucs.append(compute_auc(mean_nlls, r['relevant_idx']))\n",
    "all5_aucs = np.array(all5_aucs)\n",
    "\n",
    "diff = all5_aucs - bare_aucs_ref\n",
    "d = cohens_d(diff)\n",
    "nonzero = diff[diff != 0]\n",
    "_, p = wilcoxon(nonzero) if len(nonzero) >= 10 else (0, 1.0)\n",
    "sig = ('***' if p < 0.001/N_BONFERRONI else '**' if p < 0.01/N_BONFERRONI\n",
    "       else '*' if p < 0.05/N_BONFERRONI else 'ns')\n",
    "print(f\"  All 5 primed:         AUC={all5_aucs.mean():.3f}, \"\n",
    "      f\"d={d:+.3f}, p={p:.2e} {sig}\")\n",
    "\n",
    "print(f\"\\n--- Phase A Summary ---\")\n",
    "print(f\"  Bare AUC: {bare_aucs_ref.mean():.3f}\")\n",
    "best_combo_cond = max(lambda_results, key=lambda c: lambda_results[c]['best_auc'])\n",
    "best_combo = lambda_results[best_combo_cond]\n",
    "print(f\"  Best lambda: {best_combo_cond} lam={best_combo['best_lambda']:.1f}, \"\n",
    "      f\"AUC={best_combo['best_auc']:.3f}\")\n",
    "print(f\"  Best ensemble: structural 4-cond AUC={struct_ensemble_aucs.mean():.3f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 5: Phase A visualization\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Lambda sweep\n",
    "ax = axes[0]\n",
    "for cond in PRIMED_CONDITIONS:\n",
    "    lr = lambda_results[cond]\n",
    "    lams = sorted(lr['aucs_by_lambda'].keys(), key=float)\n",
    "    aucs = [lr['aucs_by_lambda'][l] for l in lams]\n",
    "    ax.plot([float(l) for l in lams], aucs, '-o', label=cond.replace('_trunc', ''), markersize=4)\n",
    "ax.axhline(y=bare_aucs_ref.mean(), color='black', linestyle='--', alpha=0.5, label='bare')\n",
    "ax.set_xlabel('Lambda (0=bare, 1=primed)')\n",
    "ax.set_ylabel('Mean AUC')\n",
    "ax.set_title('Lambda Sweep: (1-lam)*bare + lam*primed')\n",
    "ax.legend(fontsize=7)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Delta-only ranking\n",
    "ax = axes[1]\n",
    "cond_short = [c.replace('_trunc', '') for c in PRIMED_CONDITIONS]\n",
    "delta_vals = [delta_only_aucs[c].mean() for c in PRIMED_CONDITIONS]\n",
    "colors = ['C0' if v > 0.5 else 'gray' for v in delta_vals]\n",
    "ax.bar(range(len(PRIMED_CONDITIONS)), delta_vals, color=colors, alpha=0.7)\n",
    "ax.axhline(y=0.5, color='red', linestyle='--', alpha=0.5, label='chance')\n",
    "ax.axhline(y=bare_aucs_ref.mean(), color='black', linestyle='--', alpha=0.5, label='bare NLL')\n",
    "ax.set_xticks(range(len(PRIMED_CONDITIONS)))\n",
    "ax.set_xticklabels(cond_short, rotation=30, ha='right', fontsize=8)\n",
    "ax.set_ylabel('AUC')\n",
    "ax.set_title('Delta-Only Ranking (higher delta = more relevant)')\n",
    "ax.legend(fontsize=8)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plot_path = RESULTS_DIR / 'phase_a_results.png'\n",
    "plt.savefig(plot_path, dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"Plot saved to {plot_path}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 6: Load model and rebuild data for Phase B\n",
    "from lib.data import count_words\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoProcessor, AutoModelForSeq2SeqLM\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PHASE B SETUP: Loading model and rebuilding data\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"Loading {MODEL_NAME}...\")\n",
    "processor = AutoProcessor.from_pretrained(MODEL_NAME, token=HF_TOKEN)\n",
    "tokenizer = processor.tokenizer\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    token=HF_TOKEN,\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "DEVICE = next(model.parameters()).device\n",
    "print(f\"Model loaded. dtype={next(model.parameters()).dtype}\")\n",
    "print(f\"GPU memory used: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "\n",
    "# Scoring helpers\n",
    "def score_nll(encoder_text, answer_text, prefix_token_count=0, truncate=False):\n",
    "    enc_ids = tokenizer(encoder_text, return_tensors=\"pt\",\n",
    "                        add_special_tokens=True, truncation=True,\n",
    "                        max_length=8192).input_ids.to(DEVICE)\n",
    "    total_enc_len = enc_ids.shape[1]\n",
    "    enc_mask = torch.ones(1, total_enc_len, device=DEVICE, dtype=torch.long)\n",
    "    with torch.no_grad():\n",
    "        encoder_outputs = model.get_encoder()(input_ids=enc_ids, attention_mask=enc_mask)\n",
    "    if truncate and prefix_token_count > 0:\n",
    "        cross_attn_mask = torch.ones(1, total_enc_len, device=DEVICE, dtype=torch.long)\n",
    "        cross_attn_mask[:, :prefix_token_count] = 0\n",
    "    else:\n",
    "        cross_attn_mask = torch.ones(1, total_enc_len, device=DEVICE, dtype=torch.long)\n",
    "    ans_ids = tokenizer(answer_text, return_tensors=\"pt\",\n",
    "                        add_special_tokens=False, truncation=True,\n",
    "                        max_length=256).input_ids.to(DEVICE)\n",
    "    if ans_ids.shape[1] == 0:\n",
    "        return 0.0\n",
    "    with torch.no_grad():\n",
    "        outputs = model(encoder_outputs=encoder_outputs, attention_mask=cross_attn_mask, labels=ans_ids)\n",
    "    logits = outputs.logits\n",
    "    log_probs = F.log_softmax(logits, dim=-1)\n",
    "    token_log_probs = log_probs[0].gather(1, ans_ids[0].unsqueeze(1)).squeeze(1)\n",
    "    mean_nll = -token_log_probs.mean().item()\n",
    "    del encoder_outputs, outputs, logits, log_probs\n",
    "    return mean_nll\n",
    "\n",
    "def count_prefix_tokens(prefix_text, document_text):\n",
    "    full_text = prefix_text + \"\\n\" + document_text\n",
    "    full_ids = tokenizer(full_text, add_special_tokens=True).input_ids\n",
    "    doc_ids = tokenizer(document_text, add_special_tokens=True).input_ids\n",
    "    return len(full_ids) - len(doc_ids)\n",
    "\n",
    "# Rebuild query pools (same as 04A with SEED=42)\n",
    "print(f\"\\nRebuilding MS MARCO query pools (SEED={SEED})...\")\n",
    "ds = load_dataset(\"microsoft/ms_marco\", \"v1.1\", split=\"validation\")\n",
    "\n",
    "queries = []\n",
    "all_passage_texts = []\n",
    "\n",
    "for item in ds:\n",
    "    passages_data = item.get('passages', {})\n",
    "    ptexts = passages_data.get('passage_text', [])\n",
    "    is_sel = passages_data.get('is_selected', [])\n",
    "    query = item.get('query', '')\n",
    "    answers = item.get('answers', [])\n",
    "    well_formed = item.get('wellFormedAnswers', [])\n",
    "\n",
    "    answer = None\n",
    "    if well_formed and len(well_formed) > 0 and well_formed[0] not in ('[]', ''):\n",
    "        answer = well_formed[0]\n",
    "    elif answers and len(answers) > 0 and answers[0] != 'No Answer Present.':\n",
    "        answer = answers[0]\n",
    "    if not answer:\n",
    "        continue\n",
    "\n",
    "    word_counts = [count_words(pt) for pt in ptexts]\n",
    "    if not all(30 <= wc <= 300 for wc in word_counts):\n",
    "        continue\n",
    "\n",
    "    n_selected = sum(is_sel)\n",
    "    n_not_selected = len(is_sel) - n_selected\n",
    "    if n_selected != 1 or n_not_selected < 2:\n",
    "        continue\n",
    "\n",
    "    relevant_idx = is_sel.index(1)\n",
    "    passages = []\n",
    "    for p_idx, (pt, sel) in enumerate(zip(ptexts, is_sel)):\n",
    "        passages.append({'text': pt, 'is_selected': sel})\n",
    "\n",
    "    queries.append({\n",
    "        'query': query, 'answer': answer, 'passages': passages,\n",
    "        'relevant_idx': relevant_idx, 'n_passages': len(passages),\n",
    "    })\n",
    "    all_passage_texts.append(ptexts[0])\n",
    "\n",
    "    if len(queries) >= N_SAMPLES * 3:\n",
    "        break\n",
    "\n",
    "del ds\n",
    "gc.collect()\n",
    "\n",
    "np.random.seed(SEED)\n",
    "np.random.shuffle(queries)\n",
    "queries = queries[:N_SAMPLES]\n",
    "\n",
    "# Verify alignment with 04A\n",
    "mismatches = sum(1 for q, r in zip(queries, results_04a) if q['query'][:50] != r['query'][:50])\n",
    "assert mismatches == 0, f\"{mismatches} query mismatches with 04A checkpoint\"\n",
    "print(f\"Data alignment verified: all {N_SAMPLES} queries match 04A\")\n",
    "\n",
    "# Generate K=10 fixed random prefixes (SEED+100 for independence)\n",
    "np.random.seed(SEED + 100)\n",
    "prefix_indices = np.random.choice(len(all_passage_texts), K_PREFIXES, replace=False)\n",
    "RANDOM_PREFIXES = [\" \".join(all_passage_texts[idx].split()[:20]) for idx in prefix_indices]\n",
    "\n",
    "print(f\"\\nGenerated {K_PREFIXES} fixed random prefixes:\")\n",
    "for k, pref in enumerate(RANDOM_PREFIXES):\n",
    "    print(f\"  Prefix {k}: '{pref[:60]}...'\")\n",
    "\n",
    "total_calls = sum(q['n_passages'] for q in queries) * K_PREFIXES\n",
    "print(f\"\\nTotal scoring calls: {total_calls}\")\n",
    "print(f\"Estimated runtime: ~{total_calls * 0.4 / 3600:.1f} hours\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 7: Score with K=10 random prefixes (with checkpointing)\n",
    "print(\"=\" * 70)\n",
    "print(\"PHASE B: SCORING WITH K=%d RANDOM PREFIXES\" % K_PREFIXES)\n",
    "print(\"=\" * 70)\n",
    "\n",
    "DIVERSITY_CKPT = RESULTS_DIR / \"diversity_checkpoint.json\"\n",
    "\n",
    "diversity_results = []\n",
    "start_idx = 0\n",
    "if DIVERSITY_CKPT.exists():\n",
    "    saved = json.loads(DIVERSITY_CKPT.read_text())\n",
    "    if saved.get('n_total') == N_SAMPLES and saved.get('k_prefixes') == K_PREFIXES:\n",
    "        saved_results = saved.get('results', [])\n",
    "        saved_queries = [r['query'][:50] for r in saved_results]\n",
    "        current_queries = [q['query'][:50] for q in queries[:len(saved_results)]]\n",
    "        if saved_queries == current_queries:\n",
    "            diversity_results = saved_results\n",
    "            start_idx = len(diversity_results)\n",
    "            print(f\"Resumed from checkpoint: {start_idx}/{N_SAMPLES} queries\")\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "for q_idx in tqdm(range(start_idx, N_SAMPLES), initial=start_idx, total=N_SAMPLES,\n",
    "                  desc=\"Prefix diversity\"):\n",
    "    q = queries[q_idx]\n",
    "    answer = q['answer']\n",
    "\n",
    "    query_result = {\n",
    "        'query_idx': q_idx,\n",
    "        'query': q['query'],\n",
    "        'n_passages': q['n_passages'],\n",
    "        'relevant_idx': q['relevant_idx'],\n",
    "        'diversity_scores': [],\n",
    "    }\n",
    "\n",
    "    for p_idx, passage_data in enumerate(q['passages']):\n",
    "        passage_nlls = []\n",
    "        for k in range(K_PREFIXES):\n",
    "            prefix = RANDOM_PREFIXES[k]\n",
    "            enc_text = prefix + \"\\n\" + passage_data['text']\n",
    "            prefix_count = count_prefix_tokens(prefix, passage_data['text'])\n",
    "            nll = score_nll(enc_text, answer, prefix_count, truncate=True)\n",
    "            passage_nlls.append(nll)\n",
    "        query_result['diversity_scores'].append(passage_nlls)\n",
    "\n",
    "    diversity_results.append(query_result)\n",
    "\n",
    "    if (q_idx + 1) % 20 == 0 or q_idx == N_SAMPLES - 1:\n",
    "        ckpt = {\n",
    "            'n_total': N_SAMPLES,\n",
    "            'k_prefixes': K_PREFIXES,\n",
    "            'results': diversity_results,\n",
    "            'completed': len(diversity_results),\n",
    "            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        }\n",
    "        DIVERSITY_CKPT.write_text(json.dumps(ckpt))\n",
    "        elapsed = time.time() - t0\n",
    "        done = q_idx - start_idx + 1\n",
    "        eta = (N_SAMPLES - q_idx - 1) * elapsed / done if done > 0 else 0\n",
    "        tqdm.write(f\"  Checkpoint {q_idx+1}/{N_SAMPLES} | {elapsed/60:.1f}m | ETA {eta/60:.1f}m\")\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "elapsed_total = time.time() - t0\n",
    "print(f\"\\nDiversity scoring complete: {len(diversity_results)} queries in {elapsed_total/60:.1f} min\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 8: Phase B -- Diversity analysis and ranking\n",
    "print(\"=\" * 70)\n",
    "print(\"PHASE B: PREFIX DIVERSITY ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Compute per-passage diversity stats\n",
    "for dr in diversity_results:\n",
    "    dr['passage_stats'] = []\n",
    "    for p_scores in dr['diversity_scores']:\n",
    "        arr = np.array(p_scores)\n",
    "        dr['passage_stats'].append({\n",
    "            'mean': float(arr.mean()),\n",
    "            'std': float(arr.std()),\n",
    "            'range': float(arr.max() - arr.min()),\n",
    "            'cv': float(arr.std() / arr.mean()) if arr.mean() > 0 else 0,\n",
    "        })\n",
    "\n",
    "# Relevant vs irrelevant diversity\n",
    "print(f\"\\n--- NLL diversity by relevance ---\")\n",
    "for stat_name in ['mean', 'std', 'range', 'cv']:\n",
    "    rel_vals, irrel_vals = [], []\n",
    "    for dr in diversity_results:\n",
    "        rel_idx = dr['relevant_idx']\n",
    "        for p_idx, ps in enumerate(dr['passage_stats']):\n",
    "            if p_idx == rel_idx:\n",
    "                rel_vals.append(ps[stat_name])\n",
    "            else:\n",
    "                irrel_vals.append(ps[stat_name])\n",
    "    rel_mean = np.mean(rel_vals)\n",
    "    irrel_mean = np.mean(irrel_vals)\n",
    "    diff = rel_mean - irrel_mean\n",
    "    pooled_std = np.sqrt((np.var(rel_vals) * len(rel_vals) + np.var(irrel_vals) * len(irrel_vals))\n",
    "                          / (len(rel_vals) + len(irrel_vals)))\n",
    "    d = diff / pooled_std if pooled_std > 0 else 0\n",
    "    print(f\"  {stat_name:>5s}: relevant={rel_mean:.4f}, irrelevant={irrel_mean:.4f}, \"\n",
    "          f\"diff={diff:+.4f}, d={d:+.3f}\")\n",
    "\n",
    "# === Ranking by NLL_mean (ensemble of K random prefixes) ===\n",
    "print(f\"\\n--- Rank by NLL_mean (ensemble of K={K_PREFIXES} random prefixes) ---\")\n",
    "div_mean_aucs = []\n",
    "div_mean_mrr3s = []\n",
    "div_mean_hit1s = []\n",
    "for dr in diversity_results:\n",
    "    rel_idx = dr['relevant_idx']\n",
    "    mean_nlls = np.array([ps['mean'] for ps in dr['passage_stats']])\n",
    "    div_mean_aucs.append(compute_auc(mean_nlls, rel_idx))\n",
    "    div_mean_mrr3s.append(compute_mrr_at_k(mean_nlls, rel_idx, k=3))\n",
    "    div_mean_hit1s.append(compute_hit_at_k(mean_nlls, rel_idx, k=1))\n",
    "div_mean_aucs = np.array(div_mean_aucs)\n",
    "\n",
    "diff = div_mean_aucs - bare_aucs_ref\n",
    "d = cohens_d(diff)\n",
    "nonzero = diff[diff != 0]\n",
    "_, p = wilcoxon(nonzero) if len(nonzero) >= 10 else (0, 1.0)\n",
    "sig = ('***' if p < 0.001/N_BONFERRONI else '**' if p < 0.01/N_BONFERRONI\n",
    "       else '*' if p < 0.05/N_BONFERRONI else 'ns')\n",
    "print(f\"  AUC={div_mean_aucs.mean():.3f} (vs bare {bare_aucs_ref.mean():.3f}), \"\n",
    "      f\"d={d:+.3f}, p={p:.2e} {sig}\")\n",
    "print(f\"  MRR@3={np.mean(div_mean_mrr3s):.3f}, Hit@1={np.mean(div_mean_hit1s):.3f}\")\n",
    "\n",
    "# === Ranking by NLL_std (test both directions) ===\n",
    "print(f\"\\n--- Rank by NLL_std ---\")\n",
    "for direction, label in [(1, 'lower std = better'), (-1, 'higher std = better')]:\n",
    "    std_aucs = []\n",
    "    for dr in diversity_results:\n",
    "        rel_idx = dr['relevant_idx']\n",
    "        std_vals = np.array([ps['std'] for ps in dr['passage_stats']])\n",
    "        if direction == -1:\n",
    "            std_vals = -std_vals\n",
    "        std_aucs.append(compute_auc(std_vals, rel_idx))\n",
    "    std_aucs = np.array(std_aucs)\n",
    "    d_vs_chance = cohens_d(std_aucs - 0.5)\n",
    "    print(f\"  {label}: AUC={std_aucs.mean():.3f}, d vs 0.5={d_vs_chance:+.3f}\")\n",
    "\n",
    "# Store better direction\n",
    "std_aucs_lower = []\n",
    "std_aucs_higher = []\n",
    "for dr in diversity_results:\n",
    "    rel_idx = dr['relevant_idx']\n",
    "    std_vals = np.array([ps['std'] for ps in dr['passage_stats']])\n",
    "    std_aucs_lower.append(compute_auc(std_vals, rel_idx))\n",
    "    std_aucs_higher.append(compute_auc(-std_vals, rel_idx))\n",
    "std_aucs_lower = np.array(std_aucs_lower)\n",
    "std_aucs_higher = np.array(std_aucs_higher)\n",
    "if std_aucs_higher.mean() > std_aucs_lower.mean():\n",
    "    std_best_aucs = std_aucs_higher\n",
    "    std_direction = \"higher std = more relevant\"\n",
    "else:\n",
    "    std_best_aucs = std_aucs_lower\n",
    "    std_direction = \"lower std = more relevant\"\n",
    "print(f\"  Best direction: {std_direction}, AUC={std_best_aucs.mean():.3f}\")\n",
    "\n",
    "# === Lambda sweep: bare + diversity_mean ===\n",
    "print(f\"\\n--- Lambda sweep: (1-lam)*bare + lam*diversity_mean ---\")\n",
    "LAMBDAS_B = [0.0, 0.2, 0.4, 0.5, 0.6, 0.8, 1.0, 1.5, 2.0]\n",
    "best_div_auc = 0\n",
    "best_div_lambda = 0\n",
    "div_lambda_aucs = {}\n",
    "\n",
    "for lam in LAMBDAS_B:\n",
    "    aucs = []\n",
    "    for i, dr in enumerate(diversity_results):\n",
    "        rel_idx = dr['relevant_idx']\n",
    "        bare_nlls = np.array(results_04a[i]['scores']['bare'])\n",
    "        div_nlls = np.array([ps['mean'] for ps in dr['passage_stats']])\n",
    "        combined = (1 - lam) * bare_nlls + lam * div_nlls\n",
    "        aucs.append(compute_auc(combined, rel_idx))\n",
    "    mean_auc = np.mean(aucs)\n",
    "    div_lambda_aucs[lam] = np.array(aucs)\n",
    "    if mean_auc > best_div_auc:\n",
    "        best_div_auc = mean_auc\n",
    "        best_div_lambda = lam\n",
    "\n",
    "print(f\"  Best lambda={best_div_lambda:.1f}, AUC={best_div_auc:.3f}\")\n",
    "for lam in LAMBDAS_B:\n",
    "    print(f\"    lam={lam:.1f}: AUC={div_lambda_aucs[lam].mean():.3f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 9: Phase C -- Combined multi-feature ranking\n",
    "print(\"=\" * 70)\n",
    "print(\"PHASE C: COMBINED MULTI-FEATURE RANKING\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "best_single_cond = max(PRIMED_CONDITIONS, key=lambda c: lambda_results[c]['best_auc'])\n",
    "print(f\"Best single primed: {best_single_cond} (AUC={lambda_results[best_single_cond]['best_auc']:.3f})\")\n",
    "\n",
    "# Grid search: w_bare + w_primed + w_div = 1.0 (step 0.1)\n",
    "print(f\"\\n--- Grid search: w_bare*bare + w_primed*{best_single_cond.replace('_trunc','')} + w_div*div_mean ---\")\n",
    "grid_results = []\n",
    "\n",
    "for w_bare_pct in range(0, 11):\n",
    "    for w_primed_pct in range(0, 11 - w_bare_pct):\n",
    "        w_div_pct = 10 - w_bare_pct - w_primed_pct\n",
    "        w_bare = w_bare_pct / 10\n",
    "        w_primed = w_primed_pct / 10\n",
    "        w_div = w_div_pct / 10\n",
    "\n",
    "        aucs = []\n",
    "        for i, dr in enumerate(diversity_results):\n",
    "            rel_idx = dr['relevant_idx']\n",
    "            bare_nlls = np.array(results_04a[i]['scores']['bare'])\n",
    "            primed_nlls = np.array(results_04a[i]['scores'][best_single_cond])\n",
    "            div_nlls = np.array([ps['mean'] for ps in dr['passage_stats']])\n",
    "            combined = w_bare * bare_nlls + w_primed * primed_nlls + w_div * div_nlls\n",
    "            aucs.append(compute_auc(combined, rel_idx))\n",
    "\n",
    "        grid_results.append({\n",
    "            'w_bare': w_bare, 'w_primed': w_primed, 'w_div': w_div,\n",
    "            'auc': np.mean(aucs), 'aucs': np.array(aucs),\n",
    "        })\n",
    "\n",
    "grid_results.sort(key=lambda x: x['auc'], reverse=True)\n",
    "\n",
    "print(f\"\\n  Top 10 weight combinations:\")\n",
    "print(f\"  {'w_bare':>7} {'w_primed':>9} {'w_div':>6} {'AUC':>7}\")\n",
    "print(f\"  {'-'*33}\")\n",
    "for gr in grid_results[:10]:\n",
    "    print(f\"  {gr['w_bare']:>7.1f} {gr['w_primed']:>9.1f} {gr['w_div']:>6.1f} {gr['auc']:>7.3f}\")\n",
    "\n",
    "best_grid = grid_results[0]\n",
    "diff = best_grid['aucs'] - bare_aucs_ref\n",
    "d = cohens_d(diff)\n",
    "nonzero = diff[diff != 0]\n",
    "_, p = wilcoxon(nonzero) if len(nonzero) >= 10 else (0, 1.0)\n",
    "sig = ('***' if p < 0.001/N_BONFERRONI else '**' if p < 0.01/N_BONFERRONI\n",
    "       else '*' if p < 0.05/N_BONFERRONI else 'ns')\n",
    "print(f\"\\n  Best: w=({best_grid['w_bare']:.1f}, {best_grid['w_primed']:.1f}, {best_grid['w_div']:.1f})\")\n",
    "print(f\"  AUC={best_grid['auc']:.3f} vs bare {bare_aucs_ref.mean():.3f}, d={d:+.3f}, p={p:.2e} {sig}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 10: Grand comparison + verdict + save\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"GRAND COMPARISON: ALL RANKING METHODS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "methods = {'bare NLL': bare_aucs_ref}\n",
    "\n",
    "for cond in PRIMED_CONDITIONS:\n",
    "    lr = lambda_results[cond]\n",
    "    name = f\"{cond.replace('_trunc','')} (lam={lr['best_lambda']:.1f})\"\n",
    "    methods[name] = lr['best_lambda_aucs']\n",
    "\n",
    "methods['ensemble_structural_4'] = struct_ensemble_aucs\n",
    "methods[f'diversity_mean (K={K_PREFIXES})'] = div_mean_aucs\n",
    "methods[f'div_combo (lam={best_div_lambda:.1f})'] = div_lambda_aucs[best_div_lambda]\n",
    "methods[f'combined ({best_grid[\"w_bare\"]:.1f}/{best_grid[\"w_primed\"]:.1f}/{best_grid[\"w_div\"]:.1f})'] = best_grid['aucs']\n",
    "\n",
    "sorted_methods = sorted(methods.items(), key=lambda x: x[1].mean(), reverse=True)\n",
    "\n",
    "print(f\"\\n  {'#':>3} {'Method':<48} {'AUC':>7} {'vs bare':>8} {'d':>7} {'p':>12} {'sig':>5}\")\n",
    "print(f\"  {'-'*95}\")\n",
    "for rank, (name, aucs) in enumerate(sorted_methods, 1):\n",
    "    mean_auc = aucs.mean()\n",
    "    if name == 'bare NLL':\n",
    "        print(f\"  {rank:>3} {name:<48} {mean_auc:>7.3f} {'--':>8} {'--':>7} {'--':>12} {'--':>5}\")\n",
    "    else:\n",
    "        diff = aucs - bare_aucs_ref\n",
    "        d = cohens_d(diff)\n",
    "        nonzero = diff[diff != 0]\n",
    "        _, p = wilcoxon(nonzero) if len(nonzero) >= 10 else (0, 1.0)\n",
    "        sig = ('***' if p < 0.001/N_BONFERRONI else '**' if p < 0.01/N_BONFERRONI\n",
    "               else '*' if p < 0.05/N_BONFERRONI else 'ns')\n",
    "        delta_str = f\"{mean_auc - bare_aucs_ref.mean():+.3f}\"\n",
    "        print(f\"  {rank:>3} {name:<48} {mean_auc:>7.3f} {delta_str:>8} {d:>+7.3f} {p:>12.2e} {sig:>5}\")\n",
    "\n",
    "# Bar chart\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "names = [n for n, _ in sorted_methods]\n",
    "aucs_vals = [a.mean() for _, a in sorted_methods]\n",
    "colors = ['gray' if n == 'bare NLL' else 'C0' for n in names]\n",
    "ax.barh(range(len(names)), aucs_vals, color=colors, alpha=0.7)\n",
    "ax.axvline(x=bare_aucs_ref.mean(), color='red', linestyle='--', alpha=0.5, label='bare')\n",
    "ax.set_yticks(range(len(names)))\n",
    "ax.set_yticklabels(names, fontsize=8)\n",
    "ax.set_xlabel('Mean AUC')\n",
    "ax.set_title('Exp 11: All Ranking Methods Compared')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis='x')\n",
    "ax.invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plot_path = RESULTS_DIR / 'grand_comparison.png'\n",
    "plt.savefig(plot_path, dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"\\nPlot saved to {plot_path}\")\n",
    "\n",
    "# === VERDICT ===\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"VERDICT -- Exp 11: Delta-as-Feature and Prefix Diversity\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "best_delta_cond = max(PRIMED_CONDITIONS, key=lambda c: delta_only_aucs[c].mean())\n",
    "print(f\"\\n--- Phase A: Delta-as-Feature ---\")\n",
    "print(f\"  Best delta-only: {best_delta_cond.replace('_trunc','')}, AUC={delta_only_aucs[best_delta_cond].mean():.3f}\")\n",
    "if delta_only_aucs[best_delta_cond].mean() > 0.55:\n",
    "    print(f\"  -> Delta carries ranking information above chance\")\n",
    "else:\n",
    "    print(f\"  -> Delta alone is near chance (not useful standalone)\")\n",
    "print(f\"  Best lambda: {best_combo_cond} lam={best_combo['best_lambda']:.1f}, \"\n",
    "      f\"AUC={best_combo['best_auc']:.3f}\")\n",
    "print(f\"  Best ensemble: structural 4-cond AUC={struct_ensemble_aucs.mean():.3f}\")\n",
    "\n",
    "print(f\"\\n--- Phase B: Prefix Diversity ---\")\n",
    "print(f\"  NLL_mean (K={K_PREFIXES}): AUC={div_mean_aucs.mean():.3f}\")\n",
    "print(f\"  NLL_std direction: {std_direction}\")\n",
    "print(f\"  Best diversity combo: lam={best_div_lambda:.1f}, AUC={best_div_auc:.3f}\")\n",
    "\n",
    "print(f\"\\n--- Phase C: Combined ---\")\n",
    "print(f\"  Best multi-feature: w=({best_grid['w_bare']:.1f}, {best_grid['w_primed']:.1f}, \"\n",
    "      f\"{best_grid['w_div']:.1f}), AUC={best_grid['auc']:.3f}\")\n",
    "\n",
    "overall_best_name, overall_best_aucs = sorted_methods[0]\n",
    "gain = overall_best_aucs.mean() - bare_aucs_ref.mean()\n",
    "print(f\"\\n--- OVERALL ---\")\n",
    "print(f\"  Best method: {overall_best_name}\")\n",
    "print(f\"  AUC: {overall_best_aucs.mean():.3f} (bare: {bare_aucs_ref.mean():.3f}, gain: {gain:+.3f})\")\n",
    "\n",
    "if gain > 0.02:\n",
    "    print(f\"\\n  >>> PRACTICAL IMPROVEMENT: +{gain:.3f} AUC\")\n",
    "    print(f\"  >>> Structural perturbation signals ARE useful for ranking\")\n",
    "elif gain > 0.01:\n",
    "    print(f\"\\n  >>> MODERATE IMPROVEMENT: +{gain:.3f} AUC\")\n",
    "else:\n",
    "    print(f\"\\n  >>> MINIMAL IMPROVEMENT over single-condition ranking from 04A\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "\n",
    "# Save\n",
    "final_results = {\n",
    "    'experiment': 'exp11_delta_and_diversity',\n",
    "    'model': MODEL_NAME,\n",
    "    'n_queries': N_SAMPLES,\n",
    "    'k_prefixes': K_PREFIXES,\n",
    "    'n_bonferroni': N_BONFERRONI,\n",
    "    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'phase_a': {\n",
    "        'delta_only_aucs': {c: float(delta_only_aucs[c].mean()) for c in PRIMED_CONDITIONS},\n",
    "        'lambda_results': {c: {\n",
    "            'best_lambda': lr['best_lambda'],\n",
    "            'best_auc': lr['best_auc'],\n",
    "            'aucs_by_lambda': lr['aucs_by_lambda'],\n",
    "        } for c, lr in lambda_results.items()},\n",
    "        'ensemble_structural_4_auc': float(struct_ensemble_aucs.mean()),\n",
    "        'ensemble_all_5_auc': float(all5_aucs.mean()),\n",
    "    },\n",
    "    'phase_b': {\n",
    "        'diversity_mean_auc': float(div_mean_aucs.mean()),\n",
    "        'std_direction': std_direction,\n",
    "        'std_best_auc': float(std_best_aucs.mean()),\n",
    "        'best_diversity_lambda': float(best_div_lambda),\n",
    "        'best_diversity_auc': float(best_div_auc),\n",
    "    },\n",
    "    'phase_c': {\n",
    "        'best_weights': {\n",
    "            'w_bare': best_grid['w_bare'],\n",
    "            'w_primed': best_grid['w_primed'],\n",
    "            'w_div': best_grid['w_div'],\n",
    "        },\n",
    "        'best_combined_auc': float(best_grid['auc']),\n",
    "    },\n",
    "    'bare_auc': float(bare_aucs_ref.mean()),\n",
    "    'grand_comparison': [\n",
    "        {'method': name, 'auc': float(aucs.mean())} for name, aucs in sorted_methods\n",
    "    ],\n",
    "}\n",
    "\n",
    "with open(RESULTS_DIR / 'results.json', 'w') as f:\n",
    "    json.dump(final_results, f, indent=2)\n",
    "print(f\"\\nResults saved to {RESULTS_DIR / 'results.json'}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 11: Cleanup\n",
    "print(\"Cleaning up GPU memory...\")\n",
    "mem_before = torch.cuda.memory_allocated() / 1e9\n",
    "del model, processor, tokenizer\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "mem_after = torch.cuda.memory_allocated() / 1e9\n",
    "print(f\"GPU memory: {mem_before:.2f} GB -> {mem_after:.2f} GB\")\n",
    "print(\"Done!\")"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
