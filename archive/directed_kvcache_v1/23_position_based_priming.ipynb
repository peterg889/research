{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Experiment 23: Position-Based Priming Strategies\n",
    "\n",
    "**Date:** 2025-02-06\n",
    "\n",
    "## Motivation\n",
    "\n",
    "Experiment 22 revealed that **position matters more than early context contamination**:\n",
    "- Condition A (query at scoring time) beat Condition B (query in cache at start)\n",
    "- Document tokens only attend ~8% to query tokens in early positions\n",
    "- But cache content DOES matter (Condition C with wrong query was completely fooled)\n",
    "\n",
    "## Hypothesis\n",
    "\n",
    "**Recency bias**: Tokens closer to the generation point have more influence on the output.\n",
    "Instead of PREFIX priming (query before document), try SUFFIX priming (query after document).\n",
    "\n",
    "## Experimental Conditions\n",
    "\n",
    "| Condition | Cache Structure | Scoring Prompt | Rationale |\n",
    "|-----------|----------------|----------------|------------|\n",
    "| A | `[doc]` | `Query: X\\nAnswer:` | Baseline (query at scoring) |\n",
    "| B | `[query][doc]` | `Answer:` | Prefix priming (Exp 21 style) |\n",
    "| C | `[doc][query]` | `Answer:` | **Suffix priming** (query near generation) |\n",
    "| D | `[query][doc][query]` | `Answer:` | Bookend (query at both ends) |\n",
    "| E | `[doc]` | `Query: X\\nAnswer:` | Same as A (sanity check) |\n",
    "\n",
    "## Expected Outcome\n",
    "\n",
    "If recency bias is the key factor:\n",
    "- C (suffix) > B (prefix) — query closer to generation helps more\n",
    "- C ≈ A or C > A — suffix priming might match or beat query-at-scoring\n",
    "- D ≥ C — bookend provides both recency and early contamination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch: 2.10.0+cu128\n",
      "CUDA: True\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Setup\n",
    "import os\n",
    "os.umask(0o000)\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '/home/jupyter/research/directed_kvcache')\n",
    "\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from scipy import stats\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, DynamicCache\n",
    "from datasets import load_dataset\n",
    "\n",
    "from lib.kv_cache import (\n",
    "    deepcopy_cache,\n",
    "    _get_cache_keys,\n",
    "    _get_cache_values,\n",
    "    _ensure_dynamic_cache,\n",
    ")\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "OUTPUT_DIR = '/home/jupyter/research/directed_kvcache/results/exp23'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bc50ae6e8c44a1486c026fe751337f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/291 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Load Model\n",
    "MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "print(f\"Model loaded on {model.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Core functions defined.\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Core Functions\n",
    "\n",
    "def build_cache(text: str) -> DynamicCache:\n",
    "    \"\"\"Build cache from text.\"\"\"\n",
    "    ids = tokenizer.encode(text, return_tensors='pt', add_special_tokens=True).to(model.device)\n",
    "    with torch.no_grad():\n",
    "        out = model(ids, use_cache=True)\n",
    "    return _ensure_dynamic_cache(out.past_key_values)\n",
    "\n",
    "\n",
    "def score_answer_nll(cache: DynamicCache, prompt: str, answer: str) -> float:\n",
    "    \"\"\"Score P(answer | cache, prompt) as NLL.\"\"\"\n",
    "    cache = _ensure_dynamic_cache(cache)\n",
    "    cache_len = _get_cache_keys(cache, 0).shape[2]\n",
    "    \n",
    "    prompt_ids = tokenizer.encode(prompt, return_tensors='pt', add_special_tokens=False).to(model.device)\n",
    "    answer_ids = tokenizer.encode(answer, return_tensors='pt', add_special_tokens=False).to(model.device)\n",
    "    \n",
    "    input_ids = torch.cat([prompt_ids, answer_ids], dim=1)\n",
    "    total_len = cache_len + input_ids.shape[1]\n",
    "    attention_mask = torch.ones((1, total_len), device=model.device)\n",
    "    \n",
    "    cache_copy = deepcopy_cache(cache)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            past_key_values=cache_copy,\n",
    "            use_cache=True,\n",
    "            return_dict=True\n",
    "        )\n",
    "    \n",
    "    logits = outputs.logits\n",
    "    prompt_len = prompt_ids.shape[1]\n",
    "    answer_len = answer_ids.shape[1]\n",
    "    \n",
    "    if answer_len == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    answer_logits = logits[:, prompt_len-1:prompt_len+answer_len-1, :]\n",
    "    loss_fct = torch.nn.CrossEntropyLoss(reduction='mean')\n",
    "    loss = loss_fct(answer_logits.view(-1, answer_logits.size(-1)), answer_ids.view(-1))\n",
    "    \n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "print(\"Core functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Position conditions defined.\n",
      "\n",
      "Conditions:\n",
      "  A: [doc] + 'Question: X\\nAnswer:' at scoring\n",
      "  B: [Question: X][doc] + 'Answer:' at scoring (PREFIX)\n",
      "  C: [doc][Question: X] + 'Answer:' at scoring (SUFFIX)\n",
      "  D: [Question: X][doc][Question: X] + 'Answer:' (BOOKEND)\n",
      "  E: [doc][instruction][Question: X] + 'Answer:' (SUFFIX + instruction)\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Define Position-Based Conditions\n",
    "\n",
    "def evaluate_position_conditions(doc: str, query: str, answer: str) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Evaluate all position-based priming conditions.\n",
    "    \n",
    "    Returns dict of condition_name -> NLL\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    answer_with_space = \" \" + answer\n",
    "    \n",
    "    # Condition A: Bare doc, query at scoring (baseline)\n",
    "    cache_A = build_cache(doc)\n",
    "    prompt_A = f\"\\n\\nQuestion: {query}\\nAnswer:\"\n",
    "    results['A_bare_query_at_scoring'] = score_answer_nll(cache_A, prompt_A, answer_with_space)\n",
    "    \n",
    "    # Condition B: Prefix priming [query][doc], minimal scoring prompt\n",
    "    text_B = f\"Question: {query}\\n\\n{doc}\"\n",
    "    cache_B = build_cache(text_B)\n",
    "    prompt_B = \"\\n\\nAnswer:\"\n",
    "    results['B_prefix_priming'] = score_answer_nll(cache_B, prompt_B, answer_with_space)\n",
    "    \n",
    "    # Condition C: Suffix priming [doc][query], minimal scoring prompt\n",
    "    text_C = f\"{doc}\\n\\nQuestion: {query}\"\n",
    "    cache_C = build_cache(text_C)\n",
    "    prompt_C = \"\\nAnswer:\"\n",
    "    results['C_suffix_priming'] = score_answer_nll(cache_C, prompt_C, answer_with_space)\n",
    "    \n",
    "    # Condition D: Bookend [query][doc][query], minimal scoring prompt\n",
    "    text_D = f\"Question: {query}\\n\\n{doc}\\n\\nQuestion: {query}\"\n",
    "    cache_D = build_cache(text_D)\n",
    "    prompt_D = \"\\nAnswer:\"\n",
    "    results['D_bookend_priming'] = score_answer_nll(cache_D, prompt_D, answer_with_space)\n",
    "    \n",
    "    # Condition E: Suffix with explicit answer prompt structure\n",
    "    text_E = f\"{doc}\\n\\nBased on the above, answer the following question.\\nQuestion: {query}\"\n",
    "    cache_E = build_cache(text_E)\n",
    "    prompt_E = \"\\nAnswer:\"\n",
    "    results['E_suffix_explicit'] = score_answer_nll(cache_E, prompt_E, answer_with_space)\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "print(\"Position conditions defined.\")\n",
    "print()\n",
    "print(\"Conditions:\")\n",
    "print(\"  A: [doc] + 'Question: X\\\\nAnswer:' at scoring\")\n",
    "print(\"  B: [Question: X][doc] + 'Answer:' at scoring (PREFIX)\")\n",
    "print(\"  C: [doc][Question: X] + 'Answer:' at scoring (SUFFIX)\")\n",
    "print(\"  D: [Question: X][doc][Question: X] + 'Answer:' (BOOKEND)\")\n",
    "print(\"  E: [doc][instruction][Question: X] + 'Answer:' (SUFFIX + instruction)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "SANITY CHECK: Synthetic Examples\n",
      "======================================================================\n",
      "\n",
      "Example 1: What is the capital of France?\n",
      "  Condition                           NLL    Rank\n",
      "  --------------------------------------------------\n",
      "  A_bare_query_at_scoring           0.049       1  <-- BEST\n",
      "  E_suffix_explicit                 0.089       2  \n",
      "  D_bookend_priming                 0.181       3  \n",
      "  C_suffix_priming                  0.201       4  \n",
      "  B_prefix_priming                  0.898       5  \n",
      "\n",
      "Example 2: When was Apple founded?\n",
      "  Condition                           NLL    Rank\n",
      "  --------------------------------------------------\n",
      "  A_bare_query_at_scoring           0.447       1  <-- BEST\n",
      "  B_prefix_priming                  0.508       2  \n",
      "  C_suffix_priming                  0.707       3  \n",
      "  D_bookend_priming                 1.016       4  \n",
      "  E_suffix_explicit                 2.156       5  \n",
      "\n",
      "Example 3: How long is the Amazon river?\n",
      "  Condition                           NLL    Rank\n",
      "  --------------------------------------------------\n",
      "  A_bare_query_at_scoring           0.633       1  <-- BEST\n",
      "  C_suffix_priming                  1.148       2  \n",
      "  E_suffix_explicit                 1.453       3  \n",
      "  D_bookend_priming                 1.656       4  \n",
      "  B_prefix_priming                  1.797       5  \n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Sanity Check on Synthetic Examples\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"SANITY CHECK: Synthetic Examples\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "synthetic_examples = [\n",
    "    {\n",
    "        'doc': \"The capital of France is Paris. It is known for the Eiffel Tower.\",\n",
    "        'query': \"What is the capital of France?\",\n",
    "        'answer': \"Paris\",\n",
    "    },\n",
    "    {\n",
    "        'doc': \"Apple Inc. was founded in 1976. Microsoft was founded in 1975. Google was founded in 1998.\",\n",
    "        'query': \"When was Apple founded?\",\n",
    "        'answer': \"1976\",\n",
    "    },\n",
    "    {\n",
    "        'doc': \"The Nile is 6,650 km long. The Amazon is 6,400 km long. The Yangtze is 6,300 km long.\",\n",
    "        'query': \"How long is the Amazon river?\",\n",
    "        'answer': \"6,400 km\",\n",
    "    },\n",
    "]\n",
    "\n",
    "for i, ex in enumerate(synthetic_examples):\n",
    "    print(f\"\\nExample {i+1}: {ex['query']}\")\n",
    "    results = evaluate_position_conditions(ex['doc'], ex['query'], ex['answer'])\n",
    "    \n",
    "    # Sort by NLL (lower is better)\n",
    "    sorted_results = sorted(results.items(), key=lambda x: x[1])\n",
    "    \n",
    "    print(f\"  {'Condition':<30} {'NLL':>8}  {'Rank':>6}\")\n",
    "    print(f\"  {'-'*50}\")\n",
    "    for rank, (cond, nll) in enumerate(sorted_results, 1):\n",
    "        marker = \"<-- BEST\" if rank == 1 else \"\"\n",
    "        print(f\"  {cond:<30} {nll:>8.3f}  {rank:>6}  {marker}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cell-6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading MS MARCO...\n",
      "Loaded 250 samples\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Load MS MARCO for Full Evaluation\n",
    "\n",
    "print(\"\\nLoading MS MARCO...\")\n",
    "msmarco = load_dataset(\"ms_marco\", \"v1.1\", split=\"train\")\n",
    "\n",
    "samples = []\n",
    "for item in msmarco:\n",
    "    if len(samples) >= 250:\n",
    "        break\n",
    "    \n",
    "    query = item.get('query', '')\n",
    "    passages = item.get('passages', {}).get('passage_text', [])\n",
    "    answers = item.get('answers', [])\n",
    "    \n",
    "    if not query or not passages or not passages[0] or not answers or not answers[0]:\n",
    "        continue\n",
    "    \n",
    "    passage = passages[0]\n",
    "    answer = answers[0]\n",
    "    \n",
    "    # Filter reasonable lengths\n",
    "    if len(passage.split()) < 20 or len(passage.split()) > 150:\n",
    "        continue\n",
    "    if len(answer.split()) < 1 or len(answer.split()) > 30:\n",
    "        continue\n",
    "    \n",
    "    samples.append({\n",
    "        'query': query,\n",
    "        'passage': passage,\n",
    "        'answer': answer,\n",
    "    })\n",
    "\n",
    "print(f\"Loaded {len(samples)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cell-7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "MS MARCO EVALUATION: Position-Based Priming\n",
      "======================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b3f9019467341d89b0903eb96f88732",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation complete.\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Full Evaluation\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MS MARCO EVALUATION: Position-Based Priming\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "N_SAMPLES = 200\n",
    "\n",
    "all_results = {cond: [] for cond in [\n",
    "    'A_bare_query_at_scoring',\n",
    "    'B_prefix_priming',\n",
    "    'C_suffix_priming',\n",
    "    'D_bookend_priming',\n",
    "    'E_suffix_explicit',\n",
    "]}\n",
    "\n",
    "for sample in tqdm(samples[:N_SAMPLES], desc=\"Evaluating\"):\n",
    "    results = evaluate_position_conditions(\n",
    "        sample['passage'],\n",
    "        sample['query'],\n",
    "        sample['answer']\n",
    "    )\n",
    "    \n",
    "    for cond, nll in results.items():\n",
    "        all_results[cond].append(nll)\n",
    "\n",
    "print(\"\\nEvaluation complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cell-8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "RESULTS SUMMARY\n",
      "======================================================================\n",
      "\n",
      "Mean NLL by Condition (lower = better):\n",
      "------------------------------------------------------------\n",
      "Condition                        Mean NLL      Std       vs A     Win%\n",
      "----------------------------------------------------------------------\n",
      "A_bare_query_at_scoring            3.3912   2.7358    +0.0000     0.0% (BASELINE)\n",
      "C_suffix_priming                   3.4711   2.7604    -0.0798    37.0% \n",
      "D_bookend_priming                  3.7335   2.7164    -0.3423    32.5% \n",
      "B_prefix_priming                   3.7393   2.8655    -0.3481    36.0% \n",
      "E_suffix_explicit                  4.1673   3.1877    -0.7761    14.0% \n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Results Summary\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RESULTS SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Convert to numpy\n",
    "results_np = {k: np.array(v) for k, v in all_results.items()}\n",
    "\n",
    "# Compute statistics\n",
    "print(\"\\nMean NLL by Condition (lower = better):\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Sort by mean NLL\n",
    "sorted_conds = sorted(results_np.items(), key=lambda x: np.mean(x[1]))\n",
    "\n",
    "baseline = results_np['A_bare_query_at_scoring']\n",
    "baseline_mean = np.mean(baseline)\n",
    "\n",
    "print(f\"{'Condition':<30} {'Mean NLL':>10} {'Std':>8} {'vs A':>10} {'Win%':>8}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for cond, nlls in sorted_conds:\n",
    "    mean_nll = np.mean(nlls)\n",
    "    std_nll = np.std(nlls)\n",
    "    delta = baseline_mean - mean_nll  # Positive = better than A\n",
    "    win_rate = np.mean(baseline > nlls) * 100  # % where this condition beats A\n",
    "    \n",
    "    marker = \"(BASELINE)\" if cond == 'A_bare_query_at_scoring' else \"\"\n",
    "    print(f\"{cond:<30} {mean_nll:>10.4f} {std_nll:>8.4f} {delta:>+10.4f} {win_rate:>7.1f}% {marker}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cell-9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "KEY COMPARISONS\n",
      "======================================================================\n",
      "\n",
      "### Test 1: Suffix vs Prefix (C vs B) ###\n",
      "Does putting query AFTER document help more than BEFORE?\n",
      "\n",
      "C (suffix) vs B (prefix)\n",
      "  Mean: 3.4711 vs 3.7393\n",
      "  Delta: +0.2682 (positive = first is better)\n",
      "  Win rate: 57.0%\n",
      "  Cohen's d: +0.155\n",
      "  p-value: 0.0296\n",
      "  --> C (suffix) is significantly better\n",
      "\n",
      "### Test 2: Suffix vs Baseline (C vs A) ###\n",
      "Does suffix priming beat query-at-scoring?\n",
      "\n",
      "C (suffix) vs A (baseline)\n",
      "  Mean: 3.4711 vs 3.3912\n",
      "  Delta: -0.0798 (positive = first is better)\n",
      "  Win rate: 37.0%\n",
      "  Cohen's d: -0.233\n",
      "  p-value: 0.0012\n",
      "  --> A (baseline) is significantly better\n",
      "\n",
      "### Test 3: Bookend vs Suffix (D vs C) ###\n",
      "Does adding query at BOTH ends help?\n",
      "\n",
      "D (bookend) vs C (suffix)\n",
      "  Mean: 3.7335 vs 3.4711\n",
      "  Delta: -0.2624 (positive = first is better)\n",
      "  Win rate: 36.5%\n",
      "  Cohen's d: -0.206\n",
      "  p-value: 0.0040\n",
      "  --> C (suffix) is significantly better\n",
      "\n",
      "### Test 4: Prefix vs Baseline (B vs A) ###\n",
      "Sanity check: prefix priming vs baseline (should match Exp 21)\n",
      "\n",
      "B (prefix) vs A (baseline)\n",
      "  Mean: 3.7393 vs 3.3912\n",
      "  Delta: -0.3481 (positive = first is better)\n",
      "  Win rate: 36.0%\n",
      "  Cohen's d: -0.197\n",
      "  p-value: 0.0061\n",
      "  --> A (baseline) is significantly better\n",
      "\n",
      "### Test 5: Explicit Suffix vs Simple Suffix (E vs C) ###\n",
      "Does adding instruction help suffix priming?\n",
      "\n",
      "E (explicit) vs C (simple)\n",
      "  Mean: 4.1673 vs 3.4711\n",
      "  Delta: -0.6962 (positive = first is better)\n",
      "  Win rate: 16.5%\n",
      "  Cohen's d: -0.678\n",
      "  p-value: 0.0000\n",
      "  --> C (simple) is significantly better\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Key Comparisons\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"KEY COMPARISONS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "A = results_np['A_bare_query_at_scoring']\n",
    "B = results_np['B_prefix_priming']\n",
    "C = results_np['C_suffix_priming']\n",
    "D = results_np['D_bookend_priming']\n",
    "E = results_np['E_suffix_explicit']\n",
    "\n",
    "def compare(name, x, y):\n",
    "    \"\"\"Compare x vs y. Positive delta means x is better.\"\"\"\n",
    "    delta = np.mean(y) - np.mean(x)  # Positive = x better (lower NLL)\n",
    "    win_rate = np.mean(x < y)\n",
    "    t_stat, p_val = stats.ttest_rel(x, y)\n",
    "    d = delta / np.std(y - x) if np.std(y - x) > 0 else 0\n",
    "    \n",
    "    print(f\"\\n{name}\")\n",
    "    print(f\"  Mean: {np.mean(x):.4f} vs {np.mean(y):.4f}\")\n",
    "    print(f\"  Delta: {delta:+.4f} (positive = first is better)\")\n",
    "    print(f\"  Win rate: {win_rate*100:.1f}%\")\n",
    "    print(f\"  Cohen's d: {d:+.3f}\")\n",
    "    print(f\"  p-value: {p_val:.4f}\")\n",
    "    \n",
    "    if p_val < 0.05:\n",
    "        winner = name.split(' vs ')[0] if delta > 0 else name.split(' vs ')[1]\n",
    "        print(f\"  --> {winner} is significantly better\")\n",
    "    else:\n",
    "        print(f\"  --> No significant difference\")\n",
    "\n",
    "print(\"\\n### Test 1: Suffix vs Prefix (C vs B) ###\")\n",
    "print(\"Does putting query AFTER document help more than BEFORE?\")\n",
    "compare(\"C (suffix) vs B (prefix)\", C, B)\n",
    "\n",
    "print(\"\\n### Test 2: Suffix vs Baseline (C vs A) ###\")\n",
    "print(\"Does suffix priming beat query-at-scoring?\")\n",
    "compare(\"C (suffix) vs A (baseline)\", C, A)\n",
    "\n",
    "print(\"\\n### Test 3: Bookend vs Suffix (D vs C) ###\")\n",
    "print(\"Does adding query at BOTH ends help?\")\n",
    "compare(\"D (bookend) vs C (suffix)\", D, C)\n",
    "\n",
    "print(\"\\n### Test 4: Prefix vs Baseline (B vs A) ###\")\n",
    "print(\"Sanity check: prefix priming vs baseline (should match Exp 21)\")\n",
    "compare(\"B (prefix) vs A (baseline)\", B, A)\n",
    "\n",
    "print(\"\\n### Test 5: Explicit Suffix vs Simple Suffix (E vs C) ###\")\n",
    "print(\"Does adding instruction help suffix priming?\")\n",
    "compare(\"E (explicit) vs C (simple)\", E, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cell-10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "CONDITION RANKING PER SAMPLE\n",
      "======================================================================\n",
      "\n",
      "How often each condition ranks 1st, 2nd, etc.:\n",
      "Condition                          #1     #2     #3     #4     #5\n",
      "------------------------------------------------------------------\n",
      "A_bare_query_at_scoring           30%    35%    21%    12%     2%\n",
      "B_prefix_priming                  26%    13%    22%    19%    20%\n",
      "C_suffix_priming                  20%    32%    23%    20%     6%\n",
      "D_bookend_priming                 17%    16%    16%    34%    18%\n",
      "E_suffix_explicit                  7%     4%    18%    16%    55%\n"
     ]
    }
   ],
   "source": [
    "# Cell 10: Ranking Analysis\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CONDITION RANKING PER SAMPLE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# For each sample, rank the conditions\n",
    "condition_names = list(all_results.keys())\n",
    "n_conditions = len(condition_names)\n",
    "\n",
    "rank_counts = {cond: {r: 0 for r in range(1, n_conditions+1)} for cond in condition_names}\n",
    "\n",
    "for i in range(N_SAMPLES):\n",
    "    sample_nlls = [(cond, all_results[cond][i]) for cond in condition_names]\n",
    "    sorted_sample = sorted(sample_nlls, key=lambda x: x[1])\n",
    "    \n",
    "    for rank, (cond, _) in enumerate(sorted_sample, 1):\n",
    "        rank_counts[cond][rank] += 1\n",
    "\n",
    "print(\"\\nHow often each condition ranks 1st, 2nd, etc.:\")\n",
    "print(f\"{'Condition':<30} {'#1':>6} {'#2':>6} {'#3':>6} {'#4':>6} {'#5':>6}\")\n",
    "print(\"-\" * 66)\n",
    "\n",
    "for cond in condition_names:\n",
    "    counts = [rank_counts[cond][r] for r in range(1, n_conditions+1)]\n",
    "    pcts = [f\"{c/N_SAMPLES*100:.0f}%\" for c in counts]\n",
    "    print(f\"{cond:<30} {pcts[0]:>6} {pcts[1]:>6} {pcts[2]:>6} {pcts[3]:>6} {pcts[4]:>6}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cell-11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "DIAGNOSIS\n",
      "======================================================================\n",
      "\n",
      "Best condition: A_bare_query_at_scoring (mean NLL = 3.3912)\n",
      "Worst condition: E_suffix_explicit (mean NLL = 4.1673)\n",
      "Gap: 0.7761\n",
      "\n",
      "--------------------------------------------------\n",
      "Hypothesis Testing:\n",
      "--------------------------------------------------\n",
      "\n",
      "1. Suffix > Prefix (recency helps)?\n",
      "   C mean: 3.4711, B mean: 3.7393\n",
      "   Result: YES - Suffix is better\n",
      "\n",
      "2. Suffix >= Baseline?\n",
      "   C mean: 3.4711, A mean: 3.3912\n",
      "   Result: NO - Suffix is worse than baseline\n",
      "\n",
      "3. Bookend >= Suffix?\n",
      "   D mean: 3.7335, C mean: 3.4711\n",
      "   Result: NO\n"
     ]
    }
   ],
   "source": [
    "# Cell 11: Diagnosis and Conclusions\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DIAGNOSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Determine the winner\n",
    "means = {cond: np.mean(nlls) for cond, nlls in results_np.items()}\n",
    "best_cond = min(means, key=means.get)\n",
    "worst_cond = max(means, key=means.get)\n",
    "\n",
    "print(f\"\\nBest condition: {best_cond} (mean NLL = {means[best_cond]:.4f})\")\n",
    "print(f\"Worst condition: {worst_cond} (mean NLL = {means[worst_cond]:.4f})\")\n",
    "print(f\"Gap: {means[worst_cond] - means[best_cond]:.4f}\")\n",
    "\n",
    "# Check hypotheses\n",
    "print(\"\\n\" + \"-\"*50)\n",
    "print(\"Hypothesis Testing:\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "# H1: Suffix > Prefix\n",
    "suffix_beats_prefix = np.mean(C) < np.mean(B)\n",
    "print(f\"\\n1. Suffix > Prefix (recency helps)?\")\n",
    "print(f\"   C mean: {np.mean(C):.4f}, B mean: {np.mean(B):.4f}\")\n",
    "print(f\"   Result: {'YES' if suffix_beats_prefix else 'NO'} - Suffix is {'better' if suffix_beats_prefix else 'worse'}\")\n",
    "\n",
    "# H2: Suffix ≈ Baseline or better\n",
    "suffix_beats_baseline = np.mean(C) < np.mean(A)\n",
    "print(f\"\\n2. Suffix >= Baseline?\")\n",
    "print(f\"   C mean: {np.mean(C):.4f}, A mean: {np.mean(A):.4f}\")\n",
    "print(f\"   Result: {'YES' if suffix_beats_baseline else 'NO'} - Suffix is {'better' if suffix_beats_baseline else 'worse'} than baseline\")\n",
    "\n",
    "# H3: Bookend >= Suffix\n",
    "bookend_beats_suffix = np.mean(D) <= np.mean(C)\n",
    "print(f\"\\n3. Bookend >= Suffix?\")\n",
    "print(f\"   D mean: {np.mean(D):.4f}, C mean: {np.mean(C):.4f}\")\n",
    "print(f\"   Result: {'YES' if bookend_beats_suffix else 'NO'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cell-12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to /home/jupyter/research/directed_kvcache/results/exp23/results.json\n"
     ]
    }
   ],
   "source": [
    "# Cell 12: Save Results\n",
    "\n",
    "output = {\n",
    "    'n_samples': N_SAMPLES,\n",
    "    'conditions': {\n",
    "        cond: {\n",
    "            'mean': float(np.mean(nlls)),\n",
    "            'std': float(np.std(nlls)),\n",
    "        }\n",
    "        for cond, nlls in results_np.items()\n",
    "    },\n",
    "    'comparisons': {\n",
    "        'C_vs_B': {\n",
    "            'delta': float(np.mean(B) - np.mean(C)),\n",
    "            'p_value': float(stats.ttest_rel(C, B)[1]),\n",
    "        },\n",
    "        'C_vs_A': {\n",
    "            'delta': float(np.mean(A) - np.mean(C)),\n",
    "            'p_value': float(stats.ttest_rel(C, A)[1]),\n",
    "        },\n",
    "        'D_vs_C': {\n",
    "            'delta': float(np.mean(C) - np.mean(D)),\n",
    "            'p_value': float(stats.ttest_rel(D, C)[1]),\n",
    "        },\n",
    "    },\n",
    "    'best_condition': best_cond,\n",
    "    'raw_results': {k: [float(x) for x in v] for k, v in all_results.items()},\n",
    "}\n",
    "\n",
    "with open(f'{OUTPUT_DIR}/results.json', 'w') as f:\n",
    "    json.dump(output, f, indent=2)\n",
    "\n",
    "print(f\"Results saved to {OUTPUT_DIR}/results.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "### Key Question\n",
    "Does **suffix priming** (query after document) work better than **prefix priming** (query before document)?\n",
    "\n",
    "### Conditions Tested\n",
    "- **A**: Bare doc, query at scoring (baseline)\n",
    "- **B**: Prefix priming `[query][doc]`\n",
    "- **C**: Suffix priming `[doc][query]`\n",
    "- **D**: Bookend `[query][doc][query]`\n",
    "- **E**: Suffix with explicit instruction\n",
    "\n",
    "### Results\n",
    "[To be filled after running]"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cu124.m127",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/base-cu124:m127"
  },
  "kernelspec": {
   "display_name": "Python 3 (Local)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
