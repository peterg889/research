{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Experiment 20: Retrieval Ranking & Semantic Steering Survey\n",
    "\n",
    "**Date:** 2026-02-04\n",
    "\n",
    "## Purpose\n",
    "\n",
    "Exp 19 showed priming benefits are MS MARCO-specific for QA. This experiment surveys\n",
    "alternative framings where priming might show stronger effects:\n",
    "\n",
    "### Part A: Retrieval Ranking\n",
    "Instead of scoring P(answer|passage), score P(passage|query) for ranking.\n",
    "This is closer to ad-serving: given a query, which ad/document is most relevant?\n",
    "\n",
    "### Part B: Semantic Steering (Generation Diversity)\n",
    "Measure whether priming reduces output entropy and steers generation toward\n",
    "a target direction. Relevant for ad copy generation.\n",
    "\n",
    "### Part C: Multi-Document Focus\n",
    "Given multiple retrieved passages, does priming help the model focus on the\n",
    "relevant one? Simulates noisy retrieval in production.\n",
    "\n",
    "### Part D: Product Search / E-commerce\n",
    "Test on Amazon product search data - closest analogy to ad-serving.\n",
    "\n",
    "## Datasets\n",
    "\n",
    "| Part | Dataset | Why |\n",
    "|------|---------|-----|\n",
    "| A | MS MARCO (reframed) | Baseline comparison |\n",
    "| B | ELI5 | Long generative answers, high ambiguity |\n",
    "| C | Natural Questions (open) | Multi-passage retrieval |\n",
    "| D | Amazon ESCI | Product search relevance |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch: 2.10.0+cu128\n",
      "CUDA: True\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Setup\n",
    "import os\n",
    "os.umask(0o000)\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '/home/jupyter/research/directed_kvcache')\n",
    "\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from typing import Dict, List, Tuple, Any, Optional\n",
    "from dataclasses import dataclass, field\n",
    "from scipy import stats\n",
    "from collections import Counter\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, DynamicCache\n",
    "from datasets import load_dataset\n",
    "\n",
    "from lib.kv_cache import (\n",
    "    extract_and_truncate_cache_with_bos,\n",
    "    correct_rope_positions_with_bos,\n",
    "    score_answer_with_cache,\n",
    "    deepcopy_cache,\n",
    ")\n",
    "from lib.config import ExperimentConfig\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "OUTPUT_DIR = '/home/jupyter/research/directed_kvcache/results/exp20'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a28493e09264d7083f24e0d1b79441f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/291 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Load Model\n",
    "MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "config = ExperimentConfig(\n",
    "    model_name=MODEL_NAME,\n",
    "    device=model.device,\n",
    "    seed=SEED\n",
    ")\n",
    "\n",
    "print(f\"Model loaded on {model.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache functions defined.\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Core Cache Functions (from Exp 19, with additions)\n",
    "\n",
    "def build_bare_cache(text: str) -> Tuple[DynamicCache, int]:\n",
    "    \"\"\"Build baseline cache from text only.\"\"\"\n",
    "    ids = tokenizer.encode(text, return_tensors='pt', add_special_tokens=True).to(model.device)\n",
    "    with torch.no_grad():\n",
    "        out = model(ids, use_cache=True)\n",
    "    return out.past_key_values, ids.shape[1]\n",
    "\n",
    "def build_primed_cache_truncated(prefix: str, text: str) -> Tuple[DynamicCache, int]:\n",
    "    \"\"\"Build truncated cache: prefix removed after forward pass.\"\"\"\n",
    "    prefix_with_sep = prefix + \" \"\n",
    "    prefix_ids = tokenizer.encode(prefix_with_sep, return_tensors='pt', add_special_tokens=True)\n",
    "    prefix_len = prefix_ids.shape[1]\n",
    "    \n",
    "    full_text = prefix_with_sep + text\n",
    "    full_ids = tokenizer.encode(full_text, return_tensors='pt', add_special_tokens=True).to(model.device)\n",
    "    full_len = full_ids.shape[1]\n",
    "    doc_len = full_len - prefix_len\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        out = model(full_ids, use_cache=True)\n",
    "    \n",
    "    truncated_cache = extract_and_truncate_cache_with_bos(out.past_key_values, doc_len)\n",
    "    surrogate_offset = prefix_len - 1\n",
    "    correct_rope_positions_with_bos(truncated_cache, surrogate_offset, model)\n",
    "    \n",
    "    return truncated_cache, 1 + doc_len\n",
    "\n",
    "def score_continuation_nll(cache: DynamicCache, cache_len: int, continuation: str) -> float:\n",
    "    \"\"\"\n",
    "    Score negative log-likelihood of continuation text given cache.\n",
    "    \n",
    "    This scores P(continuation | cache) directly without any prompt.\n",
    "    \"\"\"\n",
    "    # Tokenize continuation (no special tokens - we're continuing from cache)\n",
    "    cont_ids = tokenizer.encode(continuation, return_tensors='pt', add_special_tokens=False).to(model.device)\n",
    "    \n",
    "    if cont_ids.shape[1] == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    # Get the full sequence length for attention mask\n",
    "    total_len = cache_len + cont_ids.shape[1]\n",
    "    attention_mask = torch.ones((1, total_len), device=model.device)\n",
    "    \n",
    "    # Forward pass with cache\n",
    "    cache_copy = deepcopy_cache(cache)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            input_ids=cont_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            past_key_values=cache_copy,\n",
    "            use_cache=True,\n",
    "            return_dict=True\n",
    "        )\n",
    "    \n",
    "    # Compute NLL over continuation tokens\n",
    "    logits = outputs.logits  # [1, cont_len, vocab]\n",
    "    \n",
    "    # Shift for next-token prediction: logits[:-1] predicts tokens[1:]\n",
    "    # But we also need the last token of cache to predict first continuation token\n",
    "    # The model output already accounts for this with past_key_values\n",
    "    \n",
    "    # For tokens 0..n-1 in continuation, logits[i] predicts continuation[i+1]\n",
    "    # We need to score continuation[0] using last cache position, which isn't in logits\n",
    "    # So we score continuation[1:] using logits[:-1]\n",
    "    \n",
    "    if cont_ids.shape[1] == 1:\n",
    "        # Only one token - can't compute NLL this way, use full forward\n",
    "        return 0.0\n",
    "    \n",
    "    shift_logits = logits[:, :-1, :].contiguous()  # [1, cont_len-1, vocab]\n",
    "    shift_labels = cont_ids[:, 1:].contiguous()     # [1, cont_len-1]\n",
    "    \n",
    "    # Compute cross-entropy loss\n",
    "    loss_fct = torch.nn.CrossEntropyLoss(reduction='mean')\n",
    "    loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "    \n",
    "    return loss.item()\n",
    "\n",
    "def score_with_prompt(cache: DynamicCache, cache_len: int, prompt: str, completion: str) -> float:\n",
    "    \"\"\"Score P(completion | cache, prompt).\"\"\"\n",
    "    return score_answer_with_cache(\n",
    "        cache, cache_len,\n",
    "        prompt,\n",
    "        completion,\n",
    "        model, tokenizer, config\n",
    "    )\n",
    "\n",
    "print(\"Cache functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "---\n",
    "# PART A: Retrieval Ranking\n",
    "\n",
    "**Task:** Given a query and N candidate documents, rank by relevance.\n",
    "\n",
    "**Scoring:** Instead of P(answer|doc, query), we score P(query|doc) â€” how well does the document \"predict\" or align with the query?\n",
    "\n",
    "**Hypothesis:** Priming documents with relevant queries should increase P(query|doc) for matching pairs, improving ranking.\n",
    "\n",
    "**Conditions:**\n",
    "- `bare`: Score P(query|doc) with unprimed document cache\n",
    "- `oracle_primed`: Prime doc with the target query, score P(query|doc)\n",
    "- `random_primed`: Prime doc with random query, score P(query|doc)\n",
    "- `topic_primed`: Prime doc with extracted topic/keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MS MARCO for retrieval ranking...\n",
      "Built 200 retrieval samples with 10 candidates each\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Load MS MARCO for Retrieval Ranking\n",
    "\n",
    "print(\"Loading MS MARCO for retrieval ranking...\")\n",
    "msmarco = load_dataset(\"ms_marco\", \"v1.1\", split=\"train\")\n",
    "\n",
    "# Build retrieval samples: 1 relevant doc + 9 distractors per query\n",
    "retrieval_samples = []\n",
    "all_passages = []\n",
    "\n",
    "# Collect passages\n",
    "for item in msmarco:\n",
    "    if item['passages']['passage_text']:\n",
    "        for p in item['passages']['passage_text']:\n",
    "            if p and len(p.split()) > 20:\n",
    "                all_passages.append(p)\n",
    "    if len(all_passages) > 10000:\n",
    "        break\n",
    "\n",
    "random.shuffle(all_passages)\n",
    "distractor_pool = all_passages[:5000]\n",
    "\n",
    "# Build samples\n",
    "N_RETRIEVAL_SAMPLES = 200\n",
    "N_CANDIDATES = 10  # 1 relevant + 9 distractors\n",
    "\n",
    "for item in msmarco:\n",
    "    if len(retrieval_samples) >= N_RETRIEVAL_SAMPLES:\n",
    "        break\n",
    "    if not item['passages']['passage_text'] or not item['query']:\n",
    "        continue\n",
    "    \n",
    "    relevant_passage = item['passages']['passage_text'][0]\n",
    "    if not relevant_passage or len(relevant_passage.split()) < 20:\n",
    "        continue\n",
    "    \n",
    "    # Sample distractors\n",
    "    distractors = random.sample(distractor_pool, N_CANDIDATES - 1)\n",
    "    \n",
    "    # Build candidate list with relevance labels\n",
    "    candidates = [(relevant_passage, 1)]  # (passage, is_relevant)\n",
    "    for d in distractors:\n",
    "        candidates.append((d, 0))\n",
    "    \n",
    "    random.shuffle(candidates)\n",
    "    \n",
    "    retrieval_samples.append({\n",
    "        'query': item['query'],\n",
    "        'candidates': candidates,\n",
    "        'relevant_idx': [i for i, (_, rel) in enumerate(candidates) if rel == 1][0]\n",
    "    })\n",
    "\n",
    "print(f\"Built {len(retrieval_samples)} retrieval samples with {N_CANDIDATES} candidates each\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieval evaluation function defined.\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Retrieval Ranking Evaluation\n",
    "\n",
    "def compute_mrr(rankings: List[int]) -> float:\n",
    "    \"\"\"Compute Mean Reciprocal Rank.\"\"\"\n",
    "    rrs = [1.0 / (r + 1) for r in rankings]\n",
    "    return np.mean(rrs)\n",
    "\n",
    "def compute_hit_at_k(rankings: List[int], k: int) -> float:\n",
    "    \"\"\"Compute Hit@k.\"\"\"\n",
    "    return np.mean([1 if r < k else 0 for r in rankings])\n",
    "\n",
    "def evaluate_retrieval_ranking(samples: List[dict], condition: str) -> dict:\n",
    "    \"\"\"\n",
    "    Evaluate retrieval ranking under a condition.\n",
    "    \n",
    "    For each sample:\n",
    "    - Build cache for each candidate document (with/without priming)\n",
    "    - Score P(query | doc_cache) for each candidate\n",
    "    - Rank by ascending NLL (lower = more likely = more relevant)\n",
    "    - Record rank of the relevant document\n",
    "    \"\"\"\n",
    "    rankings = []\n",
    "    \n",
    "    for sample in tqdm(samples, desc=f\"Retrieval ({condition})\"):\n",
    "        query = sample['query']\n",
    "        candidates = sample['candidates']\n",
    "        relevant_idx = sample['relevant_idx']\n",
    "        \n",
    "        scores = []  # (idx, nll)\n",
    "        \n",
    "        for idx, (passage, _) in enumerate(candidates):\n",
    "            passage_truncated = passage[:2000]  # Limit length\n",
    "            \n",
    "            if condition == 'bare':\n",
    "                cache, cache_len = build_bare_cache(passage_truncated)\n",
    "            elif condition == 'oracle_primed':\n",
    "                # Prime with the query we're about to score\n",
    "                prefix = \" \".join([query] * 3)\n",
    "                cache, cache_len = build_primed_cache_truncated(prefix, passage_truncated)\n",
    "            elif condition == 'random_primed':\n",
    "                # Prime with random query from pool\n",
    "                random_query = random.choice([s['query'] for s in samples if s['query'] != query])\n",
    "                prefix = \" \".join([random_query] * 3)\n",
    "                cache, cache_len = build_primed_cache_truncated(prefix, passage_truncated)\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown condition: {condition}\")\n",
    "            \n",
    "            # Score P(query | doc) using a prompt format\n",
    "            # We use score_with_prompt with a transition prompt\n",
    "            nll = score_with_prompt(\n",
    "                deepcopy_cache(cache), cache_len,\n",
    "                \"\\n\\nThis document is relevant to the query:\",\n",
    "                \" \" + query\n",
    "            )\n",
    "            scores.append((idx, nll))\n",
    "        \n",
    "        # Rank by ascending NLL\n",
    "        scores.sort(key=lambda x: x[1])\n",
    "        ranked_indices = [idx for idx, _ in scores]\n",
    "        \n",
    "        # Find rank of relevant document (0-indexed)\n",
    "        rank_of_relevant = ranked_indices.index(relevant_idx)\n",
    "        rankings.append(rank_of_relevant)\n",
    "    \n",
    "    return {\n",
    "        'condition': condition,\n",
    "        'mrr': compute_mrr(rankings),\n",
    "        'hit_at_1': compute_hit_at_k(rankings, 1),\n",
    "        'hit_at_3': compute_hit_at_k(rankings, 3),\n",
    "        'mean_rank': np.mean(rankings),\n",
    "        'rankings': rankings\n",
    "    }\n",
    "\n",
    "print(\"Retrieval evaluation function defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cell-7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PART A: RETRIEVAL RANKING\n",
      "======================================================================\n",
      "\n",
      "Scoring P(query | document) to rank candidates.\n",
      "Hypothesis: Priming documents with query should improve ranking.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d5f01cb95004d2c82ff5d822f4b3702",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Retrieval (bare):   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "bare:\n",
      "  MRR: 0.985\n",
      "  Hit@1: 97.5%\n",
      "  Hit@3: 99.5%\n",
      "  Mean Rank: 0.04\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa6c75612296461ab8e71b3f9a880369",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Retrieval (oracle_primed):   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "oracle_primed:\n",
      "  MRR: 0.962\n",
      "  Hit@1: 93.5%\n",
      "  Hit@3: 99.0%\n",
      "  Mean Rank: 0.11\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a772d42e369b48359cd31bfd50b68413",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Retrieval (random_primed):   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "random_primed:\n",
      "  MRR: 0.974\n",
      "  Hit@1: 95.5%\n",
      "  Hit@3: 99.5%\n",
      "  Mean Rank: 0.10\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Run Retrieval Ranking Evaluation\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"PART A: RETRIEVAL RANKING\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nScoring P(query | document) to rank candidates.\")\n",
    "print(\"Hypothesis: Priming documents with query should improve ranking.\\n\")\n",
    "\n",
    "retrieval_results = {}\n",
    "\n",
    "for condition in ['bare', 'oracle_primed', 'random_primed']:\n",
    "    result = evaluate_retrieval_ranking(retrieval_samples[:200], condition)\n",
    "    retrieval_results[condition] = result\n",
    "    print(f\"\\n{condition}:\")\n",
    "    print(f\"  MRR: {result['mrr']:.3f}\")\n",
    "    print(f\"  Hit@1: {result['hit_at_1']*100:.1f}%\")\n",
    "    print(f\"  Hit@3: {result['hit_at_3']*100:.1f}%\")\n",
    "    print(f\"  Mean Rank: {result['mean_rank']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cell-8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "RETRIEVAL RANKING ANALYSIS\n",
      "======================================================================\n",
      "\n",
      "Oracle vs Bare:\n",
      "  Oracle better: 2 (1.0%)\n",
      "  Oracle worse: 13 (6.5%)\n",
      "  Same: 185\n",
      "  Wilcoxon p-value (bare > oracle): 0.9899\n",
      "\n",
      "Random vs Bare:\n",
      "  Random better: 1 (0.5%)\n",
      "  Random worse: 5 (2.5%)\n",
      "\n",
      "Oracle vs Random:\n",
      "  Oracle better: 5 (2.5%)\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Retrieval Ranking Analysis\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RETRIEVAL RANKING ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "bare_rankings = retrieval_results['bare']['rankings']\n",
    "oracle_rankings = retrieval_results['oracle_primed']['rankings']\n",
    "random_rankings = retrieval_results['random_primed']['rankings']\n",
    "\n",
    "# Paired comparisons\n",
    "oracle_better = sum(1 for b, o in zip(bare_rankings, oracle_rankings) if o < b)\n",
    "oracle_worse = sum(1 for b, o in zip(bare_rankings, oracle_rankings) if o > b)\n",
    "oracle_same = sum(1 for b, o in zip(bare_rankings, oracle_rankings) if o == b)\n",
    "\n",
    "print(f\"\\nOracle vs Bare:\")\n",
    "print(f\"  Oracle better: {oracle_better} ({oracle_better/len(bare_rankings)*100:.1f}%)\")\n",
    "print(f\"  Oracle worse: {oracle_worse} ({oracle_worse/len(bare_rankings)*100:.1f}%)\")\n",
    "print(f\"  Same: {oracle_same}\")\n",
    "\n",
    "# Statistical test (Wilcoxon signed-rank)\n",
    "stat, p_value = stats.wilcoxon(bare_rankings, oracle_rankings, alternative='greater')\n",
    "print(f\"  Wilcoxon p-value (bare > oracle): {p_value:.4f}\")\n",
    "\n",
    "random_better = sum(1 for b, r in zip(bare_rankings, random_rankings) if r < b)\n",
    "random_worse = sum(1 for b, r in zip(bare_rankings, random_rankings) if r > b)\n",
    "\n",
    "print(f\"\\nRandom vs Bare:\")\n",
    "print(f\"  Random better: {random_better} ({random_better/len(bare_rankings)*100:.1f}%)\")\n",
    "print(f\"  Random worse: {random_worse} ({random_worse/len(bare_rankings)*100:.1f}%)\")\n",
    "\n",
    "# Oracle vs Random\n",
    "oracle_beats_random = sum(1 for o, r in zip(oracle_rankings, random_rankings) if o < r)\n",
    "print(f\"\\nOracle vs Random:\")\n",
    "print(f\"  Oracle better: {oracle_beats_random} ({oracle_beats_random/len(bare_rankings)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "---\n",
    "# PART B: Semantic Steering (Generation Diversity)\n",
    "\n",
    "**Task:** Generate explanations for questions. Measure if priming reduces diversity and steers toward target.\n",
    "\n",
    "**Dataset:** ELI5 (Explain Like I'm 5) - long generative answers with high ambiguity.\n",
    "\n",
    "**Hypothesis:** Priming with topic/intent keywords should:\n",
    "1. Reduce generation entropy (more focused)\n",
    "2. Increase overlap with reference answer\n",
    "3. Produce more consistent outputs across samples\n",
    "\n",
    "**Conditions:**\n",
    "- `bare`: Generate from question alone\n",
    "- `topic_primed`: Prime with extracted keywords from reference answer\n",
    "- `random_primed`: Prime with random keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cell-10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Q&A dataset for semantic steering...\n",
      "Trying Yahoo Answers Topics...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 250 samples from Yahoo Answers\n",
      "\n",
      "Total samples: 250\n",
      "\n",
      "Example:\n",
      "Q: why doesn't an optical mouse work on a glass table? or even on some surfaces?...\n",
      "A: Optical mice use an LED and a camera to rapidly capture images of the surface beneath the mouse.  The infomation from the camera is analyzed by a DSP (Digital Signal Processor) and used to detect impe...\n"
     ]
    }
   ],
   "source": [
    "# Cell 10: Load ELI5 (or fallback to similar Q&A dataset)\n",
    "\n",
    "print(\"Loading Q&A dataset for semantic steering...\")\n",
    "\n",
    "# Try multiple options for long-form Q&A\n",
    "eli5_samples = []\n",
    "\n",
    "# Option 1: Try Yahoo Answers (similar long-form Q&A)\n",
    "try:\n",
    "    print(\"Trying Yahoo Answers Topics...\")\n",
    "    yahoo = load_dataset(\"yahoo_answers_topics\", split=\"train\")\n",
    "    for item in yahoo:\n",
    "        if len(eli5_samples) >= 250:\n",
    "            break\n",
    "        question = item.get('question_title', '') + \" \" + item.get('question_content', '')\n",
    "        answer = item.get('best_answer', '')\n",
    "        if question.strip() and answer and len(answer.split()) > 20:\n",
    "            eli5_samples.append({\n",
    "                'question': question.strip()[:500],\n",
    "                'reference_answer': answer[:1000],\n",
    "            })\n",
    "    print(f\"Loaded {len(eli5_samples)} samples from Yahoo Answers\")\n",
    "except Exception as e:\n",
    "    print(f\"Yahoo Answers failed: {e}\")\n",
    "\n",
    "# Option 2: Try Natural Questions (open-domain)\n",
    "if len(eli5_samples) < 250:\n",
    "    try:\n",
    "        print(\"Trying Natural Questions...\")\n",
    "        nq = load_dataset(\"google-research-datasets/natural_questions\", \"default\", split=\"train\", streaming=True)\n",
    "        for item in nq:\n",
    "            if len(eli5_samples) >= 250:\n",
    "                break\n",
    "            question = item.get('question', {}).get('text', '')\n",
    "            # NQ has short answers - use the document text as context\n",
    "            annotations = item.get('annotations', [])\n",
    "            if annotations and annotations[0].get('long_answer', {}).get('candidate_index', -1) >= 0:\n",
    "                doc = item.get('document', {}).get('tokens', {}).get('token', [])\n",
    "                if doc:\n",
    "                    answer = ' '.join(doc[:200])  # First 200 tokens as \"answer\"\n",
    "                    if question and len(answer.split()) > 20:\n",
    "                        eli5_samples.append({\n",
    "                            'question': question,\n",
    "                            'reference_answer': answer[:1000],\n",
    "                        })\n",
    "        print(f\"Loaded {len(eli5_samples)} samples\")\n",
    "    except Exception as e:\n",
    "        print(f\"Natural Questions failed: {e}\")\n",
    "\n",
    "# Option 3: Use MS MARCO QA pairs (already loaded) as fallback\n",
    "if len(eli5_samples) < 250:\n",
    "    print(\"Using MS MARCO for semantic steering (already loaded)...\")\n",
    "    for item in msmarco:\n",
    "        if len(eli5_samples) >= 250:\n",
    "            break\n",
    "        query = item.get('query', '')\n",
    "        answers = item.get('answers', [])\n",
    "        passages = item.get('passages', {}).get('passage_text', [])\n",
    "        if query and passages and passages[0] and len(passages[0].split()) > 20:\n",
    "            # Use passage as \"reference answer\" for steering test\n",
    "            eli5_samples.append({\n",
    "                'question': query,\n",
    "                'reference_answer': passages[0][:1000],\n",
    "            })\n",
    "    print(f\"Using {len(eli5_samples)} MS MARCO samples for steering evaluation\")\n",
    "\n",
    "print(f\"\\nTotal samples: {len(eli5_samples)}\")\n",
    "if eli5_samples:\n",
    "    print(f\"\\nExample:\")\n",
    "    print(f\"Q: {eli5_samples[0]['question'][:100]}...\")\n",
    "    print(f\"A: {eli5_samples[0]['reference_answer'][:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cell-11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example keywords:\n",
      "  Q: why doesn't an optical mouse work on a glass table...\n",
      "  Keywords: ['surface', 'mouse', 'dsp', 'motion', 'camera']\n",
      "  Q: What is Trans Fat? How to reduce that? I heard tha...\n",
      "  Keywords: ['trans', 'fat', 'fats', 'foods', 'oil']\n",
      "  Q: How many planes Fedex has? I heard that it is the ...\n",
      "  Keywords: ['boeing', 'airbus', 'atr', 'cessna', 'according']\n"
     ]
    }
   ],
   "source": [
    "# Cell 11: Extract Keywords for Topic Priming\n",
    "\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# Simple keyword extraction (top TF words, excluding stopwords)\n",
    "STOPWORDS = set(['the', 'a', 'an', 'is', 'are', 'was', 'were', 'be', 'been', 'being',\n",
    "                 'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could',\n",
    "                 'should', 'may', 'might', 'must', 'shall', 'can', 'need', 'dare',\n",
    "                 'ought', 'used', 'to', 'of', 'in', 'for', 'on', 'with', 'at', 'by',\n",
    "                 'from', 'as', 'into', 'through', 'during', 'before', 'after',\n",
    "                 'above', 'below', 'between', 'under', 'again', 'further', 'then',\n",
    "                 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all',\n",
    "                 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no',\n",
    "                 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very',\n",
    "                 's', 't', 'just', 'don', 'now', 'and', 'but', 'or', 'because',\n",
    "                 'if', 'while', 'although', 'this', 'that', 'these', 'those',\n",
    "                 'it', 'its', 'they', 'them', 'their', 'what', 'which', 'who',\n",
    "                 'whom', 'i', 'you', 'he', 'she', 'we', 'your', 'his', 'her', 'my'])\n",
    "\n",
    "def extract_keywords(text: str, n: int = 5) -> List[str]:\n",
    "    \"\"\"Extract top-n keywords from text.\"\"\"\n",
    "    words = re.findall(r'\\b[a-z]+\\b', text.lower())\n",
    "    words = [w for w in words if w not in STOPWORDS and len(w) > 2]\n",
    "    counts = Counter(words)\n",
    "    return [w for w, _ in counts.most_common(n)]\n",
    "\n",
    "# Add keywords to samples\n",
    "for sample in eli5_samples:\n",
    "    sample['topic_keywords'] = extract_keywords(sample['reference_answer'], n=5)\n",
    "\n",
    "print(\"Example keywords:\")\n",
    "for i in range(3):\n",
    "    print(f\"  Q: {eli5_samples[i]['question'][:50]}...\")\n",
    "    print(f\"  Keywords: {eli5_samples[i]['topic_keywords']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cell-12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steering evaluation function defined (NLL-based).\n"
     ]
    }
   ],
   "source": [
    "# Cell 12: Semantic Steering Evaluation (NLL-based, no generation)\n",
    "\n",
    "def compute_keyword_overlap(text: str, keywords: List[str]) -> float:\n",
    "    \"\"\"Compute fraction of keywords present in text.\"\"\"\n",
    "    text_lower = text.lower()\n",
    "    return sum(1 for k in keywords if k in text_lower) / len(keywords) if keywords else 0\n",
    "\n",
    "def evaluate_steering(samples: List[dict], condition: str, n_samples: int = 200) -> dict:\n",
    "    \"\"\"\n",
    "    Evaluate semantic steering via NLL scoring.\n",
    "    \n",
    "    For each sample:\n",
    "    - Build context cache (with/without priming)\n",
    "    - Score P(reference_answer | context) via NLL\n",
    "    - Lower NLL = priming steers model toward reference content\n",
    "    \n",
    "    Note: Generation removed due to transformers compatibility issues with custom caches.\n",
    "    NLL is the primary metric for measuring steering effectiveness.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for sample in tqdm(samples[:n_samples], desc=f\"Steering ({condition})\"):\n",
    "        question = sample['question']\n",
    "        keywords = sample['topic_keywords']\n",
    "        reference = sample['reference_answer']\n",
    "        \n",
    "        # Build context\n",
    "        context = f\"Question: {question}\\n\\nExplain simply:\"\n",
    "        \n",
    "        if condition == 'bare':\n",
    "            cache, cache_len = build_bare_cache(context)\n",
    "        elif condition == 'topic_primed':\n",
    "            # Prime with topic keywords (oracle - knows the answer keywords)\n",
    "            prefix = \"Key topics: \" + \", \".join(keywords)\n",
    "            cache, cache_len = build_primed_cache_truncated(prefix, context)\n",
    "        elif condition == 'random_primed':\n",
    "            # Prime with random keywords from other samples\n",
    "            other_keywords = random.choice([s['topic_keywords'] for s in samples if s != sample])\n",
    "            prefix = \"Key topics: \" + \", \".join(other_keywords)\n",
    "            cache, cache_len = build_primed_cache_truncated(prefix, context)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown condition: {condition}\")\n",
    "        \n",
    "        # Score reference answer NLL (primary metric)\n",
    "        # Lower NLL = model assigns higher probability to reference content\n",
    "        ref_nll = score_continuation_nll(deepcopy_cache(cache), cache_len, \" \" + reference[:200])\n",
    "        \n",
    "        # Also compute keyword overlap in reference (sanity check)\n",
    "        ref_keyword_overlap = compute_keyword_overlap(reference, keywords)\n",
    "        \n",
    "        results.append({\n",
    "            'ref_nll': ref_nll,\n",
    "            'ref_keyword_overlap': ref_keyword_overlap,\n",
    "        })\n",
    "    \n",
    "    return {\n",
    "        'condition': condition,\n",
    "        'mean_ref_nll': np.mean([r['ref_nll'] for r in results]),\n",
    "        'std_ref_nll': np.std([r['ref_nll'] for r in results]),\n",
    "        'mean_keyword_overlap': np.mean([r['ref_keyword_overlap'] for r in results]),\n",
    "        'results': results\n",
    "    }\n",
    "\n",
    "print(\"Steering evaluation function defined (NLL-based).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cell-13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "PART B: SEMANTIC STEERING\n",
      "======================================================================\n",
      "\n",
      "Measuring if topic priming steers model toward reference content.\n",
      "Metric: NLL of reference answer (lower = better steering)\n",
      "Oracle condition: primed with keywords extracted from the reference answer.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6e5bf47ae184dcfba4f694efda8192e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Steering (bare):   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "bare:\n",
      "  Mean Reference NLL: 3.077 (+/- 0.792)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c24680fe09c407ab2f52dfa4382fbdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Steering (topic_primed):   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "topic_primed:\n",
      "  Mean Reference NLL: 3.484 (+/- 0.831)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4345ff14c9ca4dc599fcf10bccd152f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Steering (random_primed):   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "random_primed:\n",
      "  Mean Reference NLL: 3.457 (+/- 0.844)\n"
     ]
    }
   ],
   "source": [
    "# Cell 13: Run Semantic Steering Evaluation\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PART B: SEMANTIC STEERING\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nMeasuring if topic priming steers model toward reference content.\")\n",
    "print(\"Metric: NLL of reference answer (lower = better steering)\")\n",
    "print(\"Oracle condition: primed with keywords extracted from the reference answer.\\n\")\n",
    "\n",
    "steering_results = {}\n",
    "\n",
    "for condition in ['bare', 'topic_primed', 'random_primed']:\n",
    "    result = evaluate_steering(eli5_samples, condition, n_samples=200)\n",
    "    steering_results[condition] = result\n",
    "    print(f\"\\n{condition}:\")\n",
    "    print(f\"  Mean Reference NLL: {result['mean_ref_nll']:.3f} (+/- {result['std_ref_nll']:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cell-14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "SEMANTIC STEERING ANALYSIS\n",
      "======================================================================\n",
      "\n",
      "Topic Primed vs Bare:\n",
      "  Reference NLL: 3.484 vs 3.077\n",
      "    Delta: -0.407 (positive = priming helps)\n",
      "    p-value: 0.0000\n",
      "    Win Rate: 6.0%\n",
      "    Cohen's d: -1.376\n",
      "\n",
      "Random Primed vs Bare:\n",
      "  Reference NLL: 3.457 vs 3.077\n",
      "    Delta: -0.380\n",
      "    p-value: 0.0000\n",
      "\n",
      "Topic vs Random (semantic signal):\n",
      "  Topic NLL: 3.484 vs Random NLL: 3.457\n",
      "    Delta: -0.027 (positive = topic better)\n",
      "    p-value: 0.0276\n"
     ]
    }
   ],
   "source": [
    "# Cell 14: Steering Analysis\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SEMANTIC STEERING ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "bare_nlls = [r['ref_nll'] for r in steering_results['bare']['results']]\n",
    "topic_nlls = [r['ref_nll'] for r in steering_results['topic_primed']['results']]\n",
    "random_nlls = [r['ref_nll'] for r in steering_results['random_primed']['results']]\n",
    "\n",
    "# Statistical tests\n",
    "t_nll, p_nll = stats.ttest_rel(bare_nlls, topic_nlls)\n",
    "\n",
    "print(f\"\\nTopic Primed vs Bare:\")\n",
    "print(f\"  Reference NLL: {np.mean(topic_nlls):.3f} vs {np.mean(bare_nlls):.3f}\")\n",
    "delta_nll = np.mean(bare_nlls) - np.mean(topic_nlls)\n",
    "print(f\"    Delta: {delta_nll:+.3f} (positive = priming helps)\")\n",
    "print(f\"    p-value: {p_nll:.4f}\")\n",
    "\n",
    "# Win rate\n",
    "win_rate = np.mean(np.array(bare_nlls) > np.array(topic_nlls))\n",
    "print(f\"    Win Rate: {win_rate*100:.1f}%\")\n",
    "\n",
    "# Effect size (Cohen's d)\n",
    "d_nll = delta_nll / np.std(np.array(bare_nlls) - np.array(topic_nlls)) if np.std(np.array(bare_nlls) - np.array(topic_nlls)) > 0 else 0\n",
    "print(f\"    Cohen's d: {d_nll:+.3f}\")\n",
    "\n",
    "# Random vs Bare comparison\n",
    "t_random, p_random = stats.ttest_rel(bare_nlls, random_nlls)\n",
    "delta_random = np.mean(bare_nlls) - np.mean(random_nlls)\n",
    "print(f\"\\nRandom Primed vs Bare:\")\n",
    "print(f\"  Reference NLL: {np.mean(random_nlls):.3f} vs {np.mean(bare_nlls):.3f}\")\n",
    "print(f\"    Delta: {delta_random:+.3f}\")\n",
    "print(f\"    p-value: {p_random:.4f}\")\n",
    "\n",
    "# Topic vs Random (semantic signal test)\n",
    "t_topic_random, p_topic_random = stats.ttest_rel(topic_nlls, random_nlls)\n",
    "delta_topic_random = np.mean(random_nlls) - np.mean(topic_nlls)\n",
    "print(f\"\\nTopic vs Random (semantic signal):\")\n",
    "print(f\"  Topic NLL: {np.mean(topic_nlls):.3f} vs Random NLL: {np.mean(random_nlls):.3f}\")\n",
    "print(f\"    Delta: {delta_topic_random:+.3f} (positive = topic better)\")\n",
    "print(f\"    p-value: {p_topic_random:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "---\n",
    "# PART C: Multi-Document Focus\n",
    "\n",
    "**Task:** Given a question and multiple retrieved passages (some relevant, some not),\n",
    "can priming help the model focus on the relevant passage?\n",
    "\n",
    "**Setup:** Concatenate 3 passages (1 relevant + 2 distractors), ask model to answer.\n",
    "Score P(answer | multi-passage context).\n",
    "\n",
    "**Hypothesis:** Priming the relevant passage should help the model attend to it\n",
    "despite distractor noise.\n",
    "\n",
    "**Conditions:**\n",
    "- `bare`: No priming on any passage\n",
    "- `relevant_primed`: Prime only the relevant passage with the query\n",
    "- `all_primed`: Prime all passages with the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cell-16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building multi-document samples from MS MARCO...\n",
      "Built 250 multi-document samples\n"
     ]
    }
   ],
   "source": [
    "# Cell 16: Build Multi-Document Samples\n",
    "\n",
    "print(\"Building multi-document samples from MS MARCO...\")\n",
    "\n",
    "multidoc_samples = []\n",
    "\n",
    "for item in msmarco:\n",
    "    if len(multidoc_samples) >= 250:  # Extra buffer\n",
    "        break\n",
    "    \n",
    "    if not item['answers'] or not item['answers'][0]:\n",
    "        continue\n",
    "    if not item['passages']['passage_text'] or not item['passages']['passage_text'][0]:\n",
    "        continue\n",
    "    \n",
    "    relevant_passage = item['passages']['passage_text'][0][:500]\n",
    "    query = item['query']\n",
    "    answer = item['answers'][0]\n",
    "    \n",
    "    if len(relevant_passage.split()) < 20 or len(answer.split()) < 3:\n",
    "        continue\n",
    "    \n",
    "    # Sample 2 distractor passages\n",
    "    distractors = random.sample(distractor_pool, 2)\n",
    "    distractors = [d[:500] for d in distractors]\n",
    "    \n",
    "    # Random position for relevant passage\n",
    "    position = random.randint(0, 2)\n",
    "    passages = distractors[:position] + [relevant_passage] + distractors[position:]\n",
    "    passages = passages[:3]  # Ensure exactly 3\n",
    "    \n",
    "    multidoc_samples.append({\n",
    "        'query': query,\n",
    "        'answer': answer,\n",
    "        'passages': passages,\n",
    "        'relevant_position': position,\n",
    "    })\n",
    "\n",
    "print(f\"Built {len(multidoc_samples)} multi-document samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cell-17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-doc evaluation function defined.\n"
     ]
    }
   ],
   "source": [
    "# Cell 17: Multi-Document Evaluation\n",
    "\n",
    "def evaluate_multidoc(samples: List[dict], condition: str, n_samples: int = 200) -> dict:\n",
    "    \"\"\"\n",
    "    Evaluate multi-document focus.\n",
    "    \n",
    "    Build a combined context from 3 passages, score P(answer | context, query).\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for sample in tqdm(samples[:n_samples], desc=f\"MultiDoc ({condition})\"):\n",
    "        query = sample['query']\n",
    "        answer = sample['answer']\n",
    "        passages = sample['passages']\n",
    "        relevant_pos = sample['relevant_position']\n",
    "        \n",
    "        # Build combined context\n",
    "        if condition == 'bare':\n",
    "            # No priming\n",
    "            context_parts = [f\"Passage {i+1}: {p}\" for i, p in enumerate(passages)]\n",
    "        elif condition == 'relevant_primed':\n",
    "            # Prime only the relevant passage\n",
    "            context_parts = []\n",
    "            for i, p in enumerate(passages):\n",
    "                if i == relevant_pos:\n",
    "                    # This passage gets primed (we simulate by adding query context)\n",
    "                    context_parts.append(f\"Passage {i+1} [RELEVANT]: {p}\")\n",
    "                else:\n",
    "                    context_parts.append(f\"Passage {i+1}: {p}\")\n",
    "        elif condition == 'query_hint':\n",
    "            # Add query hint before relevant passage\n",
    "            context_parts = []\n",
    "            for i, p in enumerate(passages):\n",
    "                if i == relevant_pos:\n",
    "                    context_parts.append(f\"Passage {i+1} (answers: {query[:50]}): {p}\")\n",
    "                else:\n",
    "                    context_parts.append(f\"Passage {i+1}: {p}\")\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown condition: {condition}\")\n",
    "        \n",
    "        context = \"\\n\\n\".join(context_parts)\n",
    "        \n",
    "        # Build cache and score\n",
    "        cache, cache_len = build_bare_cache(context)\n",
    "        prompt = f\"\\n\\nQuery: {query}\\nAnswer:\"\n",
    "        nll = score_with_prompt(deepcopy_cache(cache), cache_len, prompt, \" \" + answer)\n",
    "        \n",
    "        results.append({\n",
    "            'nll': nll,\n",
    "            'relevant_position': relevant_pos,\n",
    "        })\n",
    "    \n",
    "    return {\n",
    "        'condition': condition,\n",
    "        'mean_nll': np.mean([r['nll'] for r in results]),\n",
    "        'std_nll': np.std([r['nll'] for r in results]),\n",
    "        'results': results\n",
    "    }\n",
    "\n",
    "print(\"Multi-doc evaluation function defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cell-18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "PART C: MULTI-DOCUMENT FOCUS\n",
      "======================================================================\n",
      "\n",
      "3 passages per sample (1 relevant + 2 distractors).\n",
      "Testing if hints/priming helps model focus on relevant passage.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a939ee83c25e4e5ab9085f9e71c91fde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MultiDoc (bare):   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "bare:\n",
      "  Mean NLL: 2.801 (+/- 1.734)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "494c0fbe29a84e238ad4b5ea52e98a00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MultiDoc (relevant_primed):   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "relevant_primed:\n",
      "  Mean NLL: 2.739 (+/- 1.688)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2ae93229b0e46c487b58f82399041ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MultiDoc (query_hint):   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "query_hint:\n",
      "  Mean NLL: 2.726 (+/- 1.738)\n"
     ]
    }
   ],
   "source": [
    "# Cell 18: Run Multi-Document Evaluation\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PART C: MULTI-DOCUMENT FOCUS\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\n3 passages per sample (1 relevant + 2 distractors).\")\n",
    "print(\"Testing if hints/priming helps model focus on relevant passage.\\n\")\n",
    "\n",
    "multidoc_results = {}\n",
    "\n",
    "for condition in ['bare', 'relevant_primed', 'query_hint']:\n",
    "    result = evaluate_multidoc(multidoc_samples, condition, n_samples=200)\n",
    "    multidoc_results[condition] = result\n",
    "    print(f\"\\n{condition}:\")\n",
    "    print(f\"  Mean NLL: {result['mean_nll']:.3f} (+/- {result['std_nll']:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cell-19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "MULTI-DOCUMENT ANALYSIS\n",
      "======================================================================\n",
      "\n",
      "Query Hint vs Bare:\n",
      "  Mean NLL: 2.726 vs 2.801\n",
      "  Delta: +0.074 (positive = hint helps)\n",
      "  Win Rate: 66.0%\n",
      "  Cohen's d: +0.195\n",
      "  p-value: 0.0064\n"
     ]
    }
   ],
   "source": [
    "# Cell 19: Multi-Doc Analysis\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MULTI-DOCUMENT ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "bare_nlls = [r['nll'] for r in multidoc_results['bare']['results']]\n",
    "hint_nlls = [r['nll'] for r in multidoc_results['query_hint']['results']]\n",
    "\n",
    "delta = np.array(bare_nlls) - np.array(hint_nlls)\n",
    "win_rate = np.mean(delta > 0)\n",
    "d = np.mean(delta) / np.std(delta) if np.std(delta) > 0 else 0\n",
    "\n",
    "t_stat, p_value = stats.ttest_rel(bare_nlls, hint_nlls)\n",
    "\n",
    "print(f\"\\nQuery Hint vs Bare:\")\n",
    "print(f\"  Mean NLL: {np.mean(hint_nlls):.3f} vs {np.mean(bare_nlls):.3f}\")\n",
    "print(f\"  Delta: {np.mean(delta):+.3f} (positive = hint helps)\")\n",
    "print(f\"  Win Rate: {win_rate*100:.1f}%\")\n",
    "print(f\"  Cohen's d: {d:+.3f}\")\n",
    "print(f\"  p-value: {p_value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "---\n",
    "# PART D: Product Search (Amazon ESCI)\n",
    "\n",
    "**Task:** Given a search query and product descriptions, rank products by relevance.\n",
    "\n",
    "**Dataset:** Amazon ESCI (Shopping Queries Dataset) - product search relevance.\n",
    "\n",
    "**Hypothesis:** Priming product descriptions with query intent should improve ranking.\n",
    "This is the closest analogy to ad-serving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cell-21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Amazon ESCI dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89ac640670514d0abb99353adf28874e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5740da35bb74444b8040217db44e9f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00000-of-00011-2d36455632bef8(â€¦):   0%|          | 0.00/115M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d470b492847d4638958410b7e981b14e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00001-of-00011-18b81793a48399(â€¦):   0%|          | 0.00/120M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66cdf599ea0d463e833501ade0a19718",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00002-of-00011-71f741fdff9a6f(â€¦):   0%|          | 0.00/144M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1ea315543d74af79fb9baffe55f83a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00003-of-00011-986bc53b83688d(â€¦):   0%|          | 0.00/155M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ab8d9f4c0f64b92bba733df9d3f913b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00004-of-00011-207d8e840a42bc(â€¦):   0%|          | 0.00/166M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7413d9117c624167b8b54d5a98713f0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00005-of-00011-14047762cd2d57(â€¦):   0%|          | 0.00/177M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bba3146692d642efa7deb7e72c5402ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00006-of-00011-8832797e39def5(â€¦):   0%|          | 0.00/184M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e53d5f4a3e11497ea8445834bdddbd0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00007-of-00011-75a55aecb7275f(â€¦):   0%|          | 0.00/189M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d6c77f6fd94479cb82266913d0537af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00008-of-00011-75a25564d1f0fd(â€¦):   0%|          | 0.00/206M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e035e8a31354471bbcab1f04f1fccb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00009-of-00011-5cd393dda922ee(â€¦):   0%|          | 0.00/182M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52245b136a9349b889661aaab716be3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00010-of-00011-232f0dd1a755c7(â€¦):   0%|          | 0.00/164M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bb8eb0f52884bbfa2d27b5b213f21d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/test-00000-of-00004-d48474212b95f33(â€¦):   0%|          | 0.00/161M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7bc407efd8d4ddf85f35eb380e143b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/test-00001-of-00004-b7602f1b5c13695(â€¦):   0%|          | 0.00/187M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0c26b1d289740cf88aa5c683d5facb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/test-00002-of-00004-a81cff173329b48(â€¦):   0%|          | 0.00/193M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2713e0a96e54385a3c203ef0f12b174",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/test-00003-of-00004-22af4ca7fa1313b(â€¦):   0%|          | 0.00/175M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f68add9d89b4f88a6e1694f47762073",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/2027874 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1af084a59a948fa9d37d961c9a69afd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/652490 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built 0 product search samples\n",
      "\n",
      "Example:\n",
      "Could not load ESCI: list index out of range\n",
      "Falling back to MS MARCO product-like queries...\n"
     ]
    }
   ],
   "source": [
    "# Cell 21: Load Amazon ESCI\n",
    "\n",
    "print(\"Loading Amazon ESCI dataset...\")\n",
    "\n",
    "try:\n",
    "    esci = load_dataset(\"tasksource/esci\", split=\"train\")\n",
    "    \n",
    "    # Build product search samples\n",
    "    esci_samples = []\n",
    "    query_products = {}  # Group products by query\n",
    "    \n",
    "    for item in esci:\n",
    "        query = item['query']\n",
    "        product = item['product_title']\n",
    "        label = item['esci_label']  # E (exact), S (substitute), C (complement), I (irrelevant)\n",
    "        \n",
    "        if query not in query_products:\n",
    "            query_products[query] = []\n",
    "        query_products[query].append((product, label))\n",
    "        \n",
    "        if len(query_products) > 1000:  # More queries for 200 samples\n",
    "            break\n",
    "    \n",
    "    # Build ranking samples (queries with both relevant and irrelevant products)\n",
    "    for query, products in query_products.items():\n",
    "        relevant = [p for p, l in products if l in ['E', 'S']]\n",
    "        irrelevant = [p for p, l in products if l == 'I']\n",
    "        \n",
    "        if relevant and irrelevant and len(irrelevant) >= 4:\n",
    "            # 1 relevant + 4 irrelevant\n",
    "            candidates = [(relevant[0], 1)] + [(p, 0) for p in irrelevant[:4]]\n",
    "            random.shuffle(candidates)\n",
    "            \n",
    "            esci_samples.append({\n",
    "                'query': query,\n",
    "                'candidates': candidates,\n",
    "                'relevant_idx': [i for i, (_, rel) in enumerate(candidates) if rel == 1][0]\n",
    "            })\n",
    "        \n",
    "        if len(esci_samples) >= 250:  # Buffer for 200\n",
    "            break\n",
    "    \n",
    "    print(f\"Built {len(esci_samples)} product search samples\")\n",
    "    print(f\"\\nExample:\")\n",
    "    print(f\"  Query: {esci_samples[0]['query']}\")\n",
    "    print(f\"  Relevant: {esci_samples[0]['candidates'][esci_samples[0]['relevant_idx']][0][:80]}...\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Could not load ESCI: {e}\")\n",
    "    print(\"Falling back to MS MARCO product-like queries...\")\n",
    "    esci_samples = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cell-22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping Part D (ESCI dataset not available)\n"
     ]
    }
   ],
   "source": [
    "# Cell 22: Product Search Evaluation\n",
    "\n",
    "if esci_samples:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"PART D: PRODUCT SEARCH (AMAZON ESCI)\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"\\nRanking products by P(query | product_description).\\n\")\n",
    "    \n",
    "    product_results = {}\n",
    "    \n",
    "    for condition in ['bare', 'oracle_primed']:\n",
    "        rankings = []\n",
    "        \n",
    "        for sample in tqdm(esci_samples[:200], desc=f\"Products ({condition})\"):\n",
    "            query = sample['query']\n",
    "            candidates = sample['candidates']\n",
    "            relevant_idx = sample['relevant_idx']\n",
    "            \n",
    "            scores = []\n",
    "            \n",
    "            for idx, (product, _) in enumerate(candidates):\n",
    "                product_text = f\"Product: {product}\"\n",
    "                \n",
    "                if condition == 'bare':\n",
    "                    cache, cache_len = build_bare_cache(product_text)\n",
    "                elif condition == 'oracle_primed':\n",
    "                    prefix = f\"Search query: {query} {query} {query}\"\n",
    "                    cache, cache_len = build_primed_cache_truncated(prefix, product_text)\n",
    "                \n",
    "                # Score P(query | product) with a transition prompt\n",
    "                nll = score_with_prompt(\n",
    "                    deepcopy_cache(cache), cache_len,\n",
    "                    \"\\n\\nRelevant search query:\",\n",
    "                    \" \" + query\n",
    "                )\n",
    "                scores.append((idx, nll))\n",
    "            \n",
    "            scores.sort(key=lambda x: x[1])\n",
    "            ranked_indices = [idx for idx, _ in scores]\n",
    "            rank_of_relevant = ranked_indices.index(relevant_idx)\n",
    "            rankings.append(rank_of_relevant)\n",
    "        \n",
    "        product_results[condition] = {\n",
    "            'mrr': compute_mrr(rankings),\n",
    "            'hit_at_1': compute_hit_at_k(rankings, 1),\n",
    "            'rankings': rankings\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n{condition}:\")\n",
    "        print(f\"  MRR: {product_results[condition]['mrr']:.3f}\")\n",
    "        print(f\"  Hit@1: {product_results[condition]['hit_at_1']*100:.1f}%\")\n",
    "else:\n",
    "    print(\"Skipping Part D (ESCI dataset not available)\")\n",
    "    product_results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cell-23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "EXPERIMENT 20: OVERALL SUMMARY\n",
      "======================================================================\n",
      "\n",
      "### PART A: Retrieval Ranking (MS MARCO) ###\n",
      "Task: Rank documents by P(query|doc)\n",
      "Bare MRR: 0.985\n",
      "Oracle Primed MRR: 0.962\n",
      "Delta: -0.023\n",
      "Verdict: HURTS\n",
      "\n",
      "### PART B: Semantic Steering (Yahoo Answers) ###\n",
      "Task: Score reference answer NLL with topic priming\n",
      "Bare Reference NLL: 3.077\n",
      "Topic Primed Reference NLL: 3.484\n",
      "Random Primed Reference NLL: 3.457\n",
      "Delta (bare - topic): -0.407\n",
      "Delta (random - topic): -0.027 (semantic signal)\n",
      "Verdict: HURTS\n",
      "\n",
      "### PART C: Multi-Document Focus (MS MARCO) ###\n",
      "Task: Answer from 3 passages (1 relevant + 2 distractors)\n",
      "Bare NLL: 2.801\n",
      "Query Hint NLL: 2.726\n",
      "Delta: +0.074\n",
      "Verdict: NEUTRAL\n",
      "\n",
      "### PART D: Product Search (Amazon ESCI) ###\n",
      "(Skipped - dataset not available)\n",
      "\n",
      "======================================================================\n",
      "KEY FINDINGS\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Cell 23: Overall Summary\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXPERIMENT 20: OVERALL SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n### PART A: Retrieval Ranking (MS MARCO) ###\")\n",
    "print(f\"Task: Rank documents by P(query|doc)\")\n",
    "if retrieval_results:\n",
    "    print(f\"Bare MRR: {retrieval_results['bare']['mrr']:.3f}\")\n",
    "    print(f\"Oracle Primed MRR: {retrieval_results['oracle_primed']['mrr']:.3f}\")\n",
    "    delta_mrr = retrieval_results['oracle_primed']['mrr'] - retrieval_results['bare']['mrr']\n",
    "    print(f\"Delta: {delta_mrr:+.3f}\")\n",
    "    verdict_a = \"HELPS\" if delta_mrr > 0.02 else \"HURTS\" if delta_mrr < -0.02 else \"NEUTRAL\"\n",
    "    print(f\"Verdict: {verdict_a}\")\n",
    "\n",
    "print(\"\\n### PART B: Semantic Steering (Yahoo Answers) ###\")\n",
    "print(f\"Task: Score reference answer NLL with topic priming\")\n",
    "if steering_results:\n",
    "    bare_nll = steering_results['bare']['mean_ref_nll']\n",
    "    topic_nll = steering_results['topic_primed']['mean_ref_nll']\n",
    "    random_nll = steering_results['random_primed']['mean_ref_nll']\n",
    "    print(f\"Bare Reference NLL: {bare_nll:.3f}\")\n",
    "    print(f\"Topic Primed Reference NLL: {topic_nll:.3f}\")\n",
    "    print(f\"Random Primed Reference NLL: {random_nll:.3f}\")\n",
    "    print(f\"Delta (bare - topic): {bare_nll - topic_nll:+.3f}\")\n",
    "    print(f\"Delta (random - topic): {random_nll - topic_nll:+.3f} (semantic signal)\")\n",
    "    verdict_b = \"HELPS\" if topic_nll < bare_nll - 0.1 else \"HURTS\" if topic_nll > bare_nll + 0.1 else \"NEUTRAL\"\n",
    "    print(f\"Verdict: {verdict_b}\")\n",
    "\n",
    "print(\"\\n### PART C: Multi-Document Focus (MS MARCO) ###\")\n",
    "print(f\"Task: Answer from 3 passages (1 relevant + 2 distractors)\")\n",
    "if multidoc_results:\n",
    "    bare_nll = multidoc_results['bare']['mean_nll']\n",
    "    hint_nll = multidoc_results['query_hint']['mean_nll']\n",
    "    print(f\"Bare NLL: {bare_nll:.3f}\")\n",
    "    print(f\"Query Hint NLL: {hint_nll:.3f}\")\n",
    "    print(f\"Delta: {bare_nll - hint_nll:+.3f}\")\n",
    "    verdict_c = \"HELPS\" if hint_nll < bare_nll - 0.1 else \"HURTS\" if hint_nll > bare_nll + 0.1 else \"NEUTRAL\"\n",
    "    print(f\"Verdict: {verdict_c}\")\n",
    "\n",
    "print(\"\\n### PART D: Product Search (Amazon ESCI) ###\")\n",
    "if product_results:\n",
    "    print(f\"Task: Rank products by P(query|product)\")\n",
    "    print(f\"Bare MRR: {product_results['bare']['mrr']:.3f}\")\n",
    "    print(f\"Oracle Primed MRR: {product_results['oracle_primed']['mrr']:.3f}\")\n",
    "    delta_mrr = product_results['oracle_primed']['mrr'] - product_results['bare']['mrr']\n",
    "    print(f\"Delta: {delta_mrr:+.3f}\")\n",
    "    verdict_d = \"HELPS\" if delta_mrr > 0.02 else \"HURTS\" if delta_mrr < -0.02 else \"NEUTRAL\"\n",
    "    print(f\"Verdict: {verdict_d}\")\n",
    "else:\n",
    "    print(\"(Skipped - dataset not available)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"KEY FINDINGS\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cell-24",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to /home/jupyter/research/directed_kvcache/results/exp20/results.json\n",
      "\n",
      "Steering results summary:\n",
      "  bare: NLL=3.077\n",
      "  topic_primed: NLL=3.484\n",
      "  random_primed: NLL=3.457\n"
     ]
    }
   ],
   "source": [
    "# Cell 24: Save Results\n",
    "\n",
    "all_results = {\n",
    "    'retrieval_ranking': {k: {kk: vv for kk, vv in v.items() if kk != 'rankings'} \n",
    "                         for k, v in retrieval_results.items()} if retrieval_results else {},\n",
    "    'semantic_steering': {k: {kk: vv for kk, vv in v.items() if kk != 'results'} \n",
    "                         for k, v in steering_results.items()} if steering_results else {},\n",
    "    'multi_document': {k: {kk: vv for kk, vv in v.items() if kk != 'results'} \n",
    "                      for k, v in multidoc_results.items()} if multidoc_results else {},\n",
    "    'product_search': {k: {kk: vv for kk, vv in v.items() if kk != 'rankings'} \n",
    "                      for k, v in product_results.items()} if product_results else {},\n",
    "}\n",
    "\n",
    "with open(f'{OUTPUT_DIR}/results.json', 'w') as f:\n",
    "    json.dump(all_results, f, indent=2)\n",
    "\n",
    "print(f\"Results saved to {OUTPUT_DIR}/results.json\")\n",
    "print(\"\\nSteering results summary:\")\n",
    "for cond, res in steering_results.items():\n",
    "    print(f\"  {cond}: NLL={res['mean_ref_nll']:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cu124.m127",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/base-cu124:m127"
  },
  "kernelspec": {
   "display_name": "Python 3 (Local)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
