{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": "# Experiment 10: Diagnosing the Truncation Mechanism\n\n## The Mystery\n\nAcross experiments 05, 06, and 09, truncated+corrected KV caches consistently beat bare caches — even when the truncated prefix is **irrelevant** text. RoPE correction restores key positions, so the corrected cache should be equivalent to a bare cache. But it isn't.\n\n## Three Competing Hypotheses\n\n**H1 — Value Vector Contamination (leading):** During forward pass through `[prefix][document]`, document tokens attend to prefix tokens. Their **value vectors** encode this attention. After truncation, prefix KV entries are removed but document values retain the \"fingerprint.\" RoPE correction only fixes keys — values are never corrected.\n\n**H2 — Float16 RoPE Residual:** Applying RoPE(+S) then RoPE(-S) in float16 is not identity — introduces ~2e-3 max error per element. This key perturbation might act as beneficial noise.\n\n**H3 — BOS Contamination:** The BOS token (position 0) is preserved during truncation. In the truncated cache, BOS attended to prefix tokens during the forward pass, so its KV entry differs from bare BOS. This contaminated BOS could be the benefit channel.\n\n## Discriminating Predictions\n\n| If H1 (values) | Cond 3 wins ~70% | Cond 4 ~50% | Cond 5 ~50% |\n| If H2 (RoPE noise) | Cond 3 ~50% | Cond 4 ~70% | Cond 5 ~70% |\n| If H3 (BOS) | Cond 6 drops | Cond 7 ~70% | |\n\n## Important methodological notes\n\n**Investigation A limitation:** The hybrid cache surgery (mixing keys from one forward pass with values from another) is inherently destructive. Keys and values are co-adapted within a single forward pass. Mixing them destroys this co-adaptation, producing catastrophic NLL (~4.0 vs ~1.5 baseline). This means Investigation A **cannot discriminate between hypotheses** — the surgery artifact dominates any signal.\n\n**Investigation C (layer ablation)** is a gentler test: replacing values at a *single layer* at a time, which preserves most of the co-adapted structure. This is the primary tool for understanding the mechanism.\n\n**BPE matching:** All investigations use `build_matched_caches` to ensure bare and truncated caches have identical token sequences, avoiding BPE boundary artifacts."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": "import sys\nimport os\nimport json\nimport time\nimport copy\nimport datetime\nfrom typing import Dict, List, Any, Optional, Tuple\n\nimport torch\nimport numpy as np\nfrom tqdm.auto import tqdm\nfrom scipy import stats\nimport matplotlib.pyplot as plt\nimport matplotlib\nmatplotlib.rcParams['figure.dpi'] = 120\n\nsys.path.insert(0, '.')\n\nfrom lib import (\n    ExperimentConfig,\n    build_kv_cache,\n    score_answer_with_cache,\n    score_answer_with_cache_and_attention,\n    build_truncated_kv_cache_corrected,\n    build_hybrid_cache,\n    swap_bos_entry,\n    apply_rope_roundtrip_noise,\n    replace_values_at_layers,\n    build_truncated_cache_variable_prefix,\n    extract_and_truncate_cache_with_bos,\n    correct_rope_positions_with_bos,\n    load_evaluation_samples,\n    load_ms_marco,\n    _ensure_dynamic_cache,\n)\n\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, DynamicCache\n\nprint(f\"PyTorch: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n\n# Configuration\nconfig = ExperimentConfig(\n    num_samples=2500,\n    min_passage_words=50,\n    max_passage_words=300,\n    seed=42,\n)\n\n# Set seeds\ntorch.manual_seed(config.seed)\nnp.random.seed(config.seed)\n\n# Load model (4-bit quantized)\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_compute_dtype=torch.float16,\n    bnb_4bit_quant_type=\"nf4\",\n)\n\nprint(f\"Loading {config.model_name}...\")\ntokenizer = AutoTokenizer.from_pretrained(config.model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\n    config.model_name,\n    quantization_config=bnb_config,\n    device_map=\"auto\",\n)\nmodel.eval()\nprint(f\"Model loaded. Layers: {model.config.num_hidden_layers}\")\nNUM_LAYERS = model.config.num_hidden_layers"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dataset = load_ms_marco(config)\n",
    "all_samples = load_evaluation_samples(dataset, config, require_answer=True)\n",
    "print(f\"Loaded {len(all_samples)} evaluation samples\")\n",
    "\n",
    "# We'll use subsets for different investigations\n",
    "samples_a = all_samples[:200]  # Investigation A: 200 samples\n",
    "samples_b = all_samples[:100]  # Investigation B: 100 samples\n",
    "samples_c_div = all_samples[:50]   # Investigation C divergence: 50 samples\n",
    "samples_c_abl = all_samples[:100]  # Investigation C ablation: 100 samples\n",
    "samples_d = all_samples[:30]   # Investigation D: 30 samples\n",
    "\n",
    "print(f\"Investigation A: {len(samples_a)} samples\")\n",
    "print(f\"Investigation B: {len(samples_b)} samples\")\n",
    "print(f\"Investigation C (divergence): {len(samples_c_div)} samples\")\n",
    "print(f\"Investigation C (ablation): {len(samples_c_abl)} samples\")\n",
    "print(f\"Investigation D: {len(samples_d)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": "# Irrelevant prefix text for truncation experiments\nIRRELEVANT_PREFIX = (\n    \"The quick brown fox jumps over the lazy dog. \"\n    \"Pack my box with five dozen liquor jugs. \"\n    \"How vexingly quick daft zebras jump.\"\n)\n\n# Import cache accessors\nfrom lib.kv_cache import _get_cache_keys, _get_cache_values\n\n\ndef build_bare_cache(passage, model, tokenizer, config):\n    \"\"\"Build bare document cache with standard framing. Returns (len, DynamicCache).\"\"\"\n    ctx = config.baseline_cache_template.format(document=passage)\n    length, cache = build_kv_cache(ctx, model, tokenizer, config)\n    return length, _ensure_dynamic_cache(cache)\n\n\ndef build_trunc_corrected_cache(passage, model, tokenizer, config,\n                                prefix_text=IRRELEVANT_PREFIX):\n    \"\"\"Build a truncated+corrected cache (standalone, no matched bare cache).\n\n    Used by Investigations C and D where we don't need hybrid caches\n    and thus don't need BPE-matched bare/trunc pairs.\n\n    Returns: (keep_len, trunc_corrected, trunc_uncorrected, offset)\n    \"\"\"\n    document_text = f\"Document:\\n{passage}\"\n    prefix_with_sep = prefix_text + \" \"\n\n    prefix_encoding = tokenizer(\n        prefix_with_sep, return_tensors=\"pt\", add_special_tokens=True,\n        padding=False, truncation=False\n    )\n    prefix_len = prefix_encoding['input_ids'].shape[1]\n\n    full_context = prefix_with_sep + document_text\n    full_encoding = tokenizer(\n        full_context, return_tensors=\"pt\", add_special_tokens=True,\n        padding=False, truncation=False\n    )\n    full_ids = full_encoding['input_ids'].to(config.device)\n    full_len = full_ids.shape[1]\n    doc_len = full_len - prefix_len\n\n    with torch.no_grad():\n        full_out = model(\n            input_ids=full_ids,\n            attention_mask=torch.ones_like(full_ids),\n            use_cache=True,\n            return_dict=True\n        )\n\n    truncated = extract_and_truncate_cache_with_bos(full_out.past_key_values, doc_len)\n    keep_len = 1 + doc_len\n\n    trunc_uncorrected = DynamicCache()\n    for li in range(len(truncated)):\n        trunc_uncorrected.update(\n            _get_cache_keys(truncated, li).clone(),\n            _get_cache_values(truncated, li).clone(),\n            li\n        )\n\n    surrogate_offset = prefix_len - 1\n    correct_rope_positions_with_bos(truncated, surrogate_offset, model)\n\n    return keep_len, truncated, trunc_uncorrected, surrogate_offset\n\n\ndef build_matched_caches(passage, model, tokenizer, config,\n                         prefix_text=IRRELEVANT_PREFIX):\n    \"\"\"Build bare and truncated+corrected caches with identical token sequences.\n\n    To ensure both caches have exactly the same document tokens (avoiding BPE\n    boundary mismatches), we:\n    1. Tokenize the full [prefix + document] context\n    2. Extract the document token IDs from that encoding\n    3. Build the bare cache from [BOS] + those exact document token IDs\n    4. Build the truncated cache from the full encoding, then truncate+correct\n\n    Returns: (cache_len, bare_cache, cache_len, trunc_corrected, trunc_uncorrected, offset)\n    \"\"\"\n    document_text = f\"Document:\\n{passage}\"\n    prefix_with_sep = prefix_text + \" \"\n\n    # Tokenize prefix alone (with BOS) to get prefix length\n    prefix_encoding = tokenizer(\n        prefix_with_sep, return_tensors=\"pt\", add_special_tokens=True,\n        padding=False, truncation=False\n    )\n    prefix_len = prefix_encoding['input_ids'].shape[1]  # includes BOS\n\n    # Tokenize full context\n    full_context = prefix_with_sep + document_text\n    full_encoding = tokenizer(\n        full_context, return_tensors=\"pt\", add_special_tokens=True,\n        padding=False, truncation=False\n    )\n    full_ids = full_encoding['input_ids'].to(config.device)\n    full_len = full_ids.shape[1]\n    doc_len = full_len - prefix_len\n\n    # Extract the exact document token IDs as they appear in the full encoding\n    doc_token_ids = full_ids[:, prefix_len:]  # (1, doc_len)\n\n    # Build bare cache from [BOS] + exact document tokens\n    bos_id = full_ids[:, :1]  # BOS token\n    bare_ids = torch.cat([bos_id, doc_token_ids], dim=1)  # (1, 1+doc_len)\n    bare_len = bare_ids.shape[1]\n\n    with torch.no_grad():\n        bare_out = model(\n            input_ids=bare_ids,\n            attention_mask=torch.ones_like(bare_ids),\n            use_cache=True,\n            return_dict=True\n        )\n    bare_cache = _ensure_dynamic_cache(bare_out.past_key_values)\n\n    # Build truncated cache from full context\n    with torch.no_grad():\n        full_out = model(\n            input_ids=full_ids,\n            attention_mask=torch.ones_like(full_ids),\n            use_cache=True,\n            return_dict=True\n        )\n\n    # Truncate: BOS + last doc_len positions\n    truncated = extract_and_truncate_cache_with_bos(full_out.past_key_values, doc_len)\n    keep_len = 1 + doc_len\n\n    # Verify match\n    assert bare_len == keep_len, f\"Length mismatch: bare_len={bare_len}, keep_len={keep_len}\"\n\n    # Clone before correction\n    trunc_uncorrected = DynamicCache()\n    for li in range(len(truncated)):\n        trunc_uncorrected.update(\n            _get_cache_keys(truncated, li).clone(),\n            _get_cache_values(truncated, li).clone(),\n            li\n        )\n\n    # RoPE correction on keys\n    surrogate_offset = prefix_len - 1\n    correct_rope_positions_with_bos(truncated, surrogate_offset, model)\n\n    return bare_len, bare_cache, keep_len, truncated, trunc_uncorrected, surrogate_offset\n\n\ndef evaluate_investigation_a(sample, model, tokenizer, config):\n    \"\"\"Evaluate one sample across all 7 Investigation A conditions.\n\n    Returns dict of NLLs or None if sample should be skipped.\n    \"\"\"\n    passage = sample['passage']\n    query = sample['query']\n    answer = sample['answer']\n\n    answer_ids = tokenizer(answer, return_tensors='pt', add_special_tokens=False)['input_ids']\n    if answer_ids.shape[1] < 2:\n        return None\n\n    query_prompt = config.query_template.format(query=query)\n\n    # Build matched caches (same token IDs for key/value swaps)\n    bare_len, bare_cache, keep_len, trunc_corrected, trunc_uncorrected, offset = \\\n        build_matched_caches(passage, model, tokenizer, config)\n\n    # --- Condition 1: Bare ---\n    nll_bare = score_answer_with_cache(\n        bare_cache, bare_len, query_prompt, answer, model, tokenizer, config\n    )\n\n    # --- Condition 2: Full truncation (corrected keys + truncated values) ---\n    nll_full_trunc = score_answer_with_cache(\n        trunc_corrected, keep_len, query_prompt, answer, model, tokenizer, config\n    )\n\n    # --- Condition 3: Value-only (bare keys + truncated values) -> tests H1 ---\n    hybrid_val_only = build_hybrid_cache(bare_cache, trunc_corrected)\n    nll_value_only = score_answer_with_cache(\n        hybrid_val_only, bare_len, query_prompt, answer, model, tokenizer, config\n    )\n\n    # --- Condition 4: Key-only (corrected trunc keys + bare values) -> tests H2 ---\n    hybrid_key_only = build_hybrid_cache(trunc_corrected, bare_cache)\n    nll_key_only = score_answer_with_cache(\n        hybrid_key_only, keep_len, query_prompt, answer, model, tokenizer, config\n    )\n\n    # --- Condition 5: RoPE noise only (bare + roundtrip noise, bare values) -> tests H2 ---\n    noisy_cache = DynamicCache()\n    for li in range(len(bare_cache)):\n        noisy_cache.update(\n            _get_cache_keys(bare_cache, li).clone(),\n            _get_cache_values(bare_cache, li).clone(),\n            li\n        )\n    apply_rope_roundtrip_noise(noisy_cache, offset, model)\n    nll_rope_noise = score_answer_with_cache(\n        noisy_cache, bare_len, query_prompt, answer, model, tokenizer, config\n    )\n\n    # --- Condition 6: Full trunc but with bare BOS -> tests H3 ---\n    trunc_bare_bos = swap_bos_entry(trunc_corrected, bare_cache)\n    nll_trunc_bare_bos = score_answer_with_cache(\n        trunc_bare_bos, keep_len, query_prompt, answer, model, tokenizer, config\n    )\n\n    # --- Condition 7: Bare but with truncated BOS -> tests H3 ---\n    bare_trunc_bos = swap_bos_entry(bare_cache, trunc_corrected)\n    nll_bare_trunc_bos = score_answer_with_cache(\n        bare_trunc_bos, bare_len, query_prompt, answer, model, tokenizer, config\n    )\n\n    return {\n        'nll_bare': nll_bare,\n        'nll_full_trunc': nll_full_trunc,\n        'nll_value_only': nll_value_only,\n        'nll_key_only': nll_key_only,\n        'nll_rope_noise': nll_rope_noise,\n        'nll_trunc_bare_bos': nll_trunc_bare_bos,\n        'nll_bare_trunc_bos': nll_bare_trunc_bos,\n    }"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# Investigation A: Key vs Value Separation (200 samples, 7 conditions)\n# ============================================================\n\nresults_a = []\nskipped_a = 0\nerrors_a = 0\nstart_a = time.time()\n\nCHECKPOINT_PATH_A = 'results/exp10/10_checkpoint_a.json'\n\nstart_idx_a = 0\nif os.path.exists(CHECKPOINT_PATH_A):\n    with open(CHECKPOINT_PATH_A) as f:\n        ckpt = json.load(f)\n    results_a = ckpt['results']\n    skipped_a = ckpt['skipped']\n    errors_a = ckpt['errors']\n    start_idx_a = ckpt['next_idx']\n    print(f\"Resumed from checkpoint: {len(results_a)} results\")\n\nprint(f\"Investigation A: {len(samples_a)} samples, 7 conditions each\")\n\nfor idx in tqdm(range(start_idx_a, len(samples_a)), desc=\"Inv A\", initial=start_idx_a, total=len(samples_a)):\n    sample = samples_a[idx]\n    try:\n        result = evaluate_investigation_a(sample, model, tokenizer, config)\n        if result is None:\n            skipped_a += 1\n            continue\n        results_a.append(result)\n    except Exception as e:\n        errors_a += 1\n        if errors_a <= 3:\n            print(f\"\\n  Error on sample {idx}: {e}\")\n        continue\n\n    if len(results_a) % 25 == 0:\n        with open(CHECKPOINT_PATH_A, 'w') as f:\n            json.dump({'results': results_a, 'skipped': skipped_a, 'errors': errors_a, 'next_idx': idx + 1}, f)\n        elapsed = time.time() - start_a\n        print(f\"\\n  [{len(results_a)} done | {elapsed/60:.0f}m]\")\n\nelapsed_a = time.time() - start_a\nprint(f\"\\nDone. {len(results_a)} evaluated, {skipped_a} skipped, {errors_a} errors. Time: {elapsed_a/60:.1f} min\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Investigation A: Results Table + Bar Chart\n",
    "# ============================================================\n",
    "\n",
    "conditions_a = [\n",
    "    ('1. Bare (baseline)', 'nll_bare'),\n",
    "    ('2. Full truncation', 'nll_full_trunc'),\n",
    "    ('3. Value-only (H1)', 'nll_value_only'),\n",
    "    ('4. Key-only (H2)', 'nll_key_only'),\n",
    "    ('5. RoPE noise (H2)', 'nll_rope_noise'),\n",
    "    ('6. Trunc, bare BOS (H3)', 'nll_trunc_bare_bos'),\n",
    "    ('7. Bare, trunc BOS (H3)', 'nll_bare_trunc_bos'),\n",
    "]\n",
    "\n",
    "bare_nlls = np.array([r['nll_bare'] for r in results_a])\n",
    "\n",
    "print('=' * 100)\n",
    "print('INVESTIGATION A: KEY vs VALUE SEPARATION')\n",
    "print('=' * 100)\n",
    "print(f\"{'#':<3} {'Condition':<30} {'Mean NLL':>10} {'Std':>8} {'Win% vs Bare':>14} {'Delta':>10} {'t-stat':>8} {'p-value':>10}\")\n",
    "print('-' * 100)\n",
    "\n",
    "inv_a_summary = {}\n",
    "for label, key in conditions_a:\n",
    "    nlls = np.array([r[key] for r in results_a])\n",
    "    deltas = bare_nlls - nlls\n",
    "    win_rate = np.mean(deltas > 0) * 100\n",
    "    if key == 'nll_bare':\n",
    "        print(f\"{label:<33} {np.mean(nlls):>10.4f} {np.std(nlls):>8.4f} {'--':>14} {'--':>10} {'--':>8} {'--':>10}\")\n",
    "    else:\n",
    "        t, p = stats.ttest_rel(bare_nlls, nlls)\n",
    "        print(f\"{label:<33} {np.mean(nlls):>10.4f} {np.std(nlls):>8.4f} {win_rate:>13.1f}% {np.mean(deltas):>+10.4f} {t:>8.2f} {p:>10.4f}\")\n",
    "    inv_a_summary[key] = {\n",
    "        'mean_nll': float(np.mean(nlls)),\n",
    "        'std_nll': float(np.std(nlls)),\n",
    "        'win_rate': float(win_rate / 100),\n",
    "        'mean_delta': float(np.mean(deltas)) if key != 'nll_bare' else 0.0,\n",
    "    }\n",
    "\n",
    "# Bar chart\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "labels = [l for l, _ in conditions_a]\n",
    "means = [inv_a_summary[k]['mean_nll'] for _, k in conditions_a]\n",
    "colors = ['#888888', '#4c72b0', '#c44e52', '#55a868', '#8c564b', '#e377c2', '#ff7f0e']\n",
    "bars = ax.bar(range(len(labels)), means, color=colors)\n",
    "ax.set_xticks(range(len(labels)))\n",
    "ax.set_xticklabels(labels, rotation=30, ha='right', fontsize=8)\n",
    "ax.set_ylabel('Mean NLL (lower = better)')\n",
    "ax.set_title('Investigation A: 7 Conditions')\n",
    "\n",
    "# Annotate win rates\n",
    "for i, (_, k) in enumerate(conditions_a):\n",
    "    wr = inv_a_summary[k]['win_rate'] * 100\n",
    "    if k != 'nll_bare':\n",
    "        ax.text(i, means[i], f'{wr:.0f}%', ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/exp10/10_investigation_a.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print('Saved: 10_investigation_a.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Investigation A: Statistical Tests + Verdict\n",
    "# ============================================================\n",
    "\n",
    "print('=' * 80)\n",
    "print('HYPOTHESIS DISCRIMINATION')\n",
    "print('=' * 80)\n",
    "\n",
    "# H1 test: value contamination\n",
    "val_only_wr = inv_a_summary['nll_value_only']['win_rate'] * 100\n",
    "key_only_wr = inv_a_summary['nll_key_only']['win_rate'] * 100\n",
    "rope_noise_wr = inv_a_summary['nll_rope_noise']['win_rate'] * 100\n",
    "full_trunc_wr = inv_a_summary['nll_full_trunc']['win_rate'] * 100\n",
    "trunc_bare_bos_wr = inv_a_summary['nll_trunc_bare_bos']['win_rate'] * 100\n",
    "bare_trunc_bos_wr = inv_a_summary['nll_bare_trunc_bos']['win_rate'] * 100\n",
    "\n",
    "print(f'\\nFull truncation win%: {full_trunc_wr:.1f}% (replication target: ~80%)')\n",
    "print(f'\\n--- H1: Value Contamination ---')\n",
    "print(f'  Cond 3 (value-only) win%: {val_only_wr:.1f}% (predict ~70% if H1)')\n",
    "print(f'  Cond 4 (key-only) win%:   {key_only_wr:.1f}% (predict ~50% if H1)')\n",
    "print(f'  Cond 5 (RoPE noise) win%: {rope_noise_wr:.1f}% (predict ~50% if H1)')\n",
    "h1_score = (val_only_wr > 60) + (key_only_wr < 55) + (rope_noise_wr < 55)\n",
    "\n",
    "print(f'\\n--- H2: RoPE Float16 Noise ---')\n",
    "print(f'  Cond 3 (value-only) win%: {val_only_wr:.1f}% (predict ~50% if H2)')\n",
    "print(f'  Cond 4 (key-only) win%:   {key_only_wr:.1f}% (predict ~70% if H2)')\n",
    "print(f'  Cond 5 (RoPE noise) win%: {rope_noise_wr:.1f}% (predict ~70% if H2)')\n",
    "h2_score = (val_only_wr < 55) + (key_only_wr > 60) + (rope_noise_wr > 60)\n",
    "\n",
    "print(f'\\n--- H3: BOS Contamination ---')\n",
    "print(f'  Cond 6 (trunc, bare BOS) win%: {trunc_bare_bos_wr:.1f}% (predict drops if H3)')\n",
    "print(f'  Cond 7 (bare, trunc BOS) win%: {bare_trunc_bos_wr:.1f}% (predict ~70% if H3)')\n",
    "h3_score = (trunc_bare_bos_wr < full_trunc_wr - 10) + (bare_trunc_bos_wr > 60)\n",
    "\n",
    "print(f'\\n--- Verdict Scores (higher = more consistent) ---')\n",
    "print(f'  H1 (value contamination): {h1_score}/3')\n",
    "print(f'  H2 (RoPE noise):          {h2_score}/3')\n",
    "print(f'  H3 (BOS contamination):   {h3_score}/2')\n",
    "\n",
    "# Pairwise comparisons between key conditions\n",
    "print('\\n--- Direct pairwise tests ---')\n",
    "for label_a, key_a, label_b, key_b in [\n",
    "    ('value-only', 'nll_value_only', 'key-only', 'nll_key_only'),\n",
    "    ('value-only', 'nll_value_only', 'rope-noise', 'nll_rope_noise'),\n",
    "    ('full-trunc', 'nll_full_trunc', 'trunc-bare-bos', 'nll_trunc_bare_bos'),\n",
    "]:\n",
    "    a = np.array([r[key_a] for r in results_a])\n",
    "    b = np.array([r[key_b] for r in results_a])\n",
    "    t, p = stats.ttest_rel(a, b)\n",
    "    print(f'  {label_a} vs {label_b}: t={t:.3f}, p={p:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Investigation B: Prefix Length Sensitivity (100 samples x 6 lengths)\n",
    "# ============================================================\n",
    "\n",
    "PREFIX_LENGTHS = [5, 10, 20, 50, 100, 200]  # target token counts\n",
    "\n",
    "# Generate random text of varying lengths\n",
    "def generate_random_text(n_tokens, tokenizer, seed=42):\n",
    "    \"\"\"Generate random text of approximately n_tokens length.\"\"\"\n",
    "    rng = np.random.RandomState(seed)\n",
    "    # Use common English words to generate somewhat natural text\n",
    "    words = ['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'lazy', 'dog',\n",
    "             'a', 'simple', 'test', 'of', 'random', 'text', 'generation',\n",
    "             'with', 'various', 'common', 'english', 'words', 'that', 'form',\n",
    "             'sentences', 'and', 'paragraphs', 'for', 'our', 'experiment',\n",
    "             'is', 'was', 'were', 'been', 'being', 'have', 'has', 'had',\n",
    "             'do', 'does', 'did', 'will', 'would', 'could', 'should', 'may',\n",
    "             'might', 'can', 'shall', 'must', 'need', 'dare', 'ought', 'used']\n",
    "    # Generate enough words, then trim to target token count\n",
    "    text = ' '.join(rng.choice(words, size=n_tokens * 2))\n",
    "    # Tokenize and trim\n",
    "    tokens = tokenizer.encode(text, add_special_tokens=False)[:n_tokens]\n",
    "    return tokenizer.decode(tokens)\n",
    "\n",
    "\n",
    "results_b = []\n",
    "skipped_b = 0\n",
    "errors_b = 0\n",
    "start_b = time.time()\n",
    "\n",
    "CHECKPOINT_PATH_B = 'results/exp10/10_checkpoint_b.json'\n",
    "\n",
    "start_idx_b = 0\n",
    "if os.path.exists(CHECKPOINT_PATH_B):\n",
    "    with open(CHECKPOINT_PATH_B) as f:\n",
    "        ckpt = json.load(f)\n",
    "    results_b = ckpt['results']\n",
    "    skipped_b = ckpt['skipped']\n",
    "    errors_b = ckpt['errors']\n",
    "    start_idx_b = ckpt['next_idx']\n",
    "    print(f\"Resumed from checkpoint: {len(results_b)} results\")\n",
    "\n",
    "print(f\"Investigation B: {len(samples_b)} samples x {len(PREFIX_LENGTHS)} prefix lengths\")\n",
    "\n",
    "for idx in tqdm(range(start_idx_b, len(samples_b)), desc=\"Inv B\", initial=start_idx_b, total=len(samples_b)):\n",
    "    sample = samples_b[idx]\n",
    "    passage = sample['passage']\n",
    "    query = sample['query']\n",
    "    answer = sample['answer']\n",
    "\n",
    "    answer_ids = tokenizer(answer, return_tensors='pt', add_special_tokens=False)['input_ids']\n",
    "    if answer_ids.shape[1] < 2:\n",
    "        skipped_b += 1\n",
    "        continue\n",
    "\n",
    "    query_prompt = config.query_template.format(query=query)\n",
    "\n",
    "    try:\n",
    "        # Bare baseline\n",
    "        bare_len, bare_cache = build_bare_cache(passage, model, tokenizer, config)\n",
    "        nll_bare = score_answer_with_cache(\n",
    "            bare_cache, bare_len, query_prompt, answer, model, tokenizer, config\n",
    "        )\n",
    "\n",
    "        length_nlls = {}\n",
    "        for n_tok in PREFIX_LENGTHS:\n",
    "            prefix = generate_random_text(n_tok, tokenizer, seed=config.seed + idx + n_tok)\n",
    "            keep_len, trunc_cache, _ = build_truncated_cache_variable_prefix(\n",
    "                prefix, passage, model, tokenizer, config\n",
    "            )\n",
    "            nll = score_answer_with_cache(\n",
    "                trunc_cache, keep_len, query_prompt, answer, model, tokenizer, config\n",
    "            )\n",
    "            length_nlls[n_tok] = nll\n",
    "\n",
    "        results_b.append({'nll_bare': nll_bare, 'length_nlls': length_nlls})\n",
    "\n",
    "    except Exception as e:\n",
    "        errors_b += 1\n",
    "        if errors_b <= 3:\n",
    "            print(f\"\\n  Error on sample {idx}: {e}\")\n",
    "        continue\n",
    "\n",
    "    if len(results_b) % 25 == 0:\n",
    "        with open(CHECKPOINT_PATH_B, 'w') as f:\n",
    "            json.dump({'results': results_b, 'skipped': skipped_b, 'errors': errors_b, 'next_idx': idx + 1}, f)\n",
    "        elapsed = time.time() - start_b\n",
    "        print(f\"\\n  [{len(results_b)} done | {elapsed/60:.0f}m]\")\n",
    "\n",
    "elapsed_b = time.time() - start_b\n",
    "print(f\"\\nDone. {len(results_b)} evaluated, {skipped_b} skipped, {errors_b} errors. Time: {elapsed_b/60:.1f} min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Investigation B: Length Curve Plot + Trend Test\n",
    "# ============================================================\n",
    "\n",
    "print('=' * 80)\n",
    "print('INVESTIGATION B: PREFIX LENGTH SENSITIVITY')\n",
    "print('=' * 80)\n",
    "\n",
    "# Compute win rates and mean deltas per length\n",
    "length_stats = {}\n",
    "for n_tok in PREFIX_LENGTHS:\n",
    "    deltas = [r['nll_bare'] - r['length_nlls'][n_tok] for r in results_b\n",
    "              if n_tok in r['length_nlls']]\n",
    "    nlls = [r['length_nlls'][n_tok] for r in results_b if n_tok in r['length_nlls']]\n",
    "    wr = np.mean([d > 0 for d in deltas]) * 100\n",
    "    length_stats[n_tok] = {\n",
    "        'mean_nll': np.mean(nlls),\n",
    "        'mean_delta': np.mean(deltas),\n",
    "        'win_rate': wr,\n",
    "        'n': len(deltas),\n",
    "    }\n",
    "    print(f\"  Prefix {n_tok:>3d} tokens: mean NLL={np.mean(nlls):.4f}, \"\n",
    "          f\"delta={np.mean(deltas):+.4f}, win%={wr:.1f}% (n={len(deltas)})\")\n",
    "\n",
    "# Trend test: Spearman correlation between prefix length and delta\n",
    "all_lengths = []\n",
    "all_deltas = []\n",
    "for r in results_b:\n",
    "    for n_tok in PREFIX_LENGTHS:\n",
    "        if n_tok in r['length_nlls']:\n",
    "            all_lengths.append(n_tok)\n",
    "            all_deltas.append(r['nll_bare'] - r['length_nlls'][n_tok])\n",
    "\n",
    "rho, p_spearman = stats.spearmanr(all_lengths, all_deltas)\n",
    "print(f\"\\nSpearman correlation (length vs delta): rho={rho:.3f}, p={p_spearman:.4f}\")\n",
    "\n",
    "# Per-sample Spearman (within each sample, does longer prefix = bigger delta?)\n",
    "per_sample_rhos = []\n",
    "for r in results_b:\n",
    "    lens = []\n",
    "    dels = []\n",
    "    for n_tok in PREFIX_LENGTHS:\n",
    "        if n_tok in r['length_nlls']:\n",
    "            lens.append(n_tok)\n",
    "            dels.append(r['nll_bare'] - r['length_nlls'][n_tok])\n",
    "    if len(lens) >= 4:\n",
    "        rho_i, _ = stats.spearmanr(lens, dels)\n",
    "        if not np.isnan(rho_i):\n",
    "            per_sample_rhos.append(rho_i)\n",
    "\n",
    "print(f\"Per-sample Spearman: mean={np.mean(per_sample_rhos):.3f}, \"\n",
    "      f\"median={np.median(per_sample_rhos):.3f}, \"\n",
    "      f\"% positive={np.mean(np.array(per_sample_rhos) > 0)*100:.1f}%\")\n",
    "\n",
    "# Interpretation\n",
    "print('\\nInterpretation:')\n",
    "if rho > 0.1 and p_spearman < 0.05:\n",
    "    print('  Benefit increases with prefix length -> supports H1 (more tokens = more value contamination)')\n",
    "    print('  Also consistent with H2 if monotonic (larger offset = more noise)')\n",
    "elif abs(rho) < 0.1:\n",
    "    print('  No length effect -> supports H3 (BOS is always 1 token regardless of prefix length)')\n",
    "else:\n",
    "    print(f'  Ambiguous: rho={rho:.3f}, p={p_spearman:.4f}')\n",
    "\n",
    "# Plot\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "lengths = sorted(length_stats.keys())\n",
    "win_rates = [length_stats[l]['win_rate'] for l in lengths]\n",
    "mean_deltas = [length_stats[l]['mean_delta'] for l in lengths]\n",
    "\n",
    "ax1.plot(lengths, win_rates, 'o-', color='#4c72b0', linewidth=2)\n",
    "ax1.axhline(50, color='gray', linestyle='--', linewidth=0.8)\n",
    "ax1.set_xlabel('Prefix Length (tokens)')\n",
    "ax1.set_ylabel('Win Rate vs Bare (%)')\n",
    "ax1.set_title(f'Win Rate by Prefix Length\\n(Spearman rho={rho:.3f}, p={p_spearman:.4f})')\n",
    "ax1.set_ylim(40, 90)\n",
    "\n",
    "ax2.plot(lengths, mean_deltas, 's-', color='#c44e52', linewidth=2)\n",
    "ax2.axhline(0, color='gray', linestyle='--', linewidth=0.8)\n",
    "ax2.set_xlabel('Prefix Length (tokens)')\n",
    "ax2.set_ylabel('Mean Delta NLL (positive = trunc better)')\n",
    "ax2.set_title('Mean Delta by Prefix Length')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/exp10/10_investigation_b.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print('Saved: 10_investigation_b.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# Investigation C: Value Divergence (50 samples)\n# Per-layer L2 distance between bare and truncated value vectors\n# FIX: Use build_matched_caches to ensure identical token sequences\n# ============================================================\n\nprint('Investigation C: Value Divergence (per-layer L2 distance)')\nprint('  Using build_matched_caches for proper BPE-matched comparison')\n\ndivergence_results = []  # list of (num_layers,) arrays\nskipped_c = 0\n\nfor idx in tqdm(range(len(samples_c_div)), desc=\"Inv C divergence\"):\n    sample = samples_c_div[idx]\n    passage = sample['passage']\n\n    answer_ids = tokenizer(sample['answer'], return_tensors='pt', add_special_tokens=False)['input_ids']\n    if answer_ids.shape[1] < 2:\n        skipped_c += 1\n        continue\n\n    try:\n        # Use build_matched_caches to ensure identical token sequences\n        bare_len, bare_cache, keep_len, trunc_corrected, _, offset = \\\n            build_matched_caches(passage, model, tokenizer, config)\n\n        assert bare_len == keep_len, f\"Length mismatch: {bare_len} vs {keep_len}\"\n\n        # Per-layer L2 distance of value vectors (skip BOS, compare doc portion)\n        layer_l2 = []\n        for li in range(NUM_LAYERS):\n            bv = _get_cache_values(bare_cache, li)\n            tv = _get_cache_values(trunc_corrected, li)\n            # Both should have identical seq_len now\n            assert bv.shape[2] == tv.shape[2], \\\n                f\"Layer {li}: seq_len mismatch {bv.shape[2]} vs {tv.shape[2]}\"\n            # Skip BOS at position 0\n            bv_doc = bv[:, :, 1:, :]\n            tv_doc = tv[:, :, 1:, :]\n            l2 = torch.norm(bv_doc.float() - tv_doc.float()).item()\n            # Normalize by number of elements\n            l2_norm = l2 / bv_doc.numel()**0.5\n            layer_l2.append(l2_norm)\n\n        divergence_results.append(np.array(layer_l2))\n\n    except Exception as e:\n        if len(divergence_results) < 3:\n            print(f\"  Error: {e}\")\n        continue\n\nprint(f\"Computed divergence for {len(divergence_results)} samples\")\n\n# Aggregate\ndiv_matrix = np.stack(divergence_results)  # (n_samples, n_layers)\nmean_div = div_matrix.mean(axis=0)\nstd_div = div_matrix.std(axis=0)\n\nprint(f\"\\nPer-layer mean value divergence (L2 norm, normalized):\")\nfor li in range(len(mean_div)):\n    print(f\"  Layer {li:>2d}: {mean_div[li]:.6f} +/- {std_div[li]:.6f}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# Investigation C: Layer Ablation (100 samples)\n# Replace values one layer at a time: truncated -> bare\n# FIX: Use build_matched_caches to ensure identical seq_len\n#      so replace_values_at_layers doesn't hit dimension mismatch\n# ============================================================\n\nprint(f'Investigation C: Layer Ablation ({len(samples_c_abl)} samples x {NUM_LAYERS} layers)')\nprint('  Using build_matched_caches for BPE-matched bare/trunc caches')\n\nablation_results = []  # list of dicts: {nll_bare, nll_trunc, layer_nlls: {layer_idx: nll}}\nskipped_c_abl = 0\nerrors_c_abl = 0\nstart_c = time.time()\n\nCHECKPOINT_PATH_C = 'results/exp10/10_checkpoint_c.json'\nstart_idx_c = 0\nif os.path.exists(CHECKPOINT_PATH_C):\n    with open(CHECKPOINT_PATH_C) as f:\n        ckpt = json.load(f)\n    results_loaded = ckpt['results']\n    # Validate checkpoint: layer_nlls keys should be string ints from old format\n    # Convert back to int keys for consistency\n    ablation_results = []\n    for r in results_loaded:\n        layer_nlls = {}\n        for k, v in r['layer_nlls'].items():\n            layer_nlls[int(k)] = v\n        ablation_results.append({\n            'nll_bare': r['nll_bare'],\n            'nll_trunc': r['nll_trunc'],\n            'layer_nlls': layer_nlls,\n        })\n    skipped_c_abl = ckpt['skipped']\n    errors_c_abl = ckpt['errors']\n    start_idx_c = ckpt['next_idx']\n    print(f\"Resumed from checkpoint: {len(ablation_results)} results\")\n\nfor idx in tqdm(range(start_idx_c, len(samples_c_abl)), desc=\"Inv C ablation\",\n                initial=start_idx_c, total=len(samples_c_abl)):\n    sample = samples_c_abl[idx]\n    passage = sample['passage']\n    query = sample['query']\n    answer = sample['answer']\n\n    answer_ids = tokenizer(answer, return_tensors='pt', add_special_tokens=False)['input_ids']\n    if answer_ids.shape[1] < 2:\n        skipped_c_abl += 1\n        continue\n\n    query_prompt = config.query_template.format(query=query)\n\n    try:\n        # FIX: Use build_matched_caches for identical token sequences\n        bare_len, bare_cache, keep_len, trunc_corrected, _, offset = \\\n            build_matched_caches(passage, model, tokenizer, config)\n\n        # Verify lengths match (critical for replace_values_at_layers)\n        assert bare_len == keep_len, f\"Length mismatch: bare_len={bare_len}, keep_len={keep_len}\"\n        for li in range(NUM_LAYERS):\n            bv = _get_cache_values(bare_cache, li)\n            tv = _get_cache_values(trunc_corrected, li)\n            assert bv.shape == tv.shape, \\\n                f\"Layer {li} shape mismatch: bare={bv.shape}, trunc={tv.shape}\"\n\n        nll_bare = score_answer_with_cache(\n            bare_cache, bare_len, query_prompt, answer, model, tokenizer, config\n        )\n        nll_trunc = score_answer_with_cache(\n            trunc_corrected, keep_len, query_prompt, answer, model, tokenizer, config\n        )\n\n        # For each layer, replace truncated values with bare values at that layer\n        layer_nlls = {}\n        for li in range(NUM_LAYERS):\n            ablated = replace_values_at_layers(trunc_corrected, bare_cache, [li])\n            nll_abl = score_answer_with_cache(\n                ablated, keep_len, query_prompt, answer, model, tokenizer, config\n            )\n            layer_nlls[li] = nll_abl\n\n        ablation_results.append({\n            'nll_bare': nll_bare,\n            'nll_trunc': nll_trunc,\n            'layer_nlls': layer_nlls,\n        })\n\n    except Exception as e:\n        errors_c_abl += 1\n        if errors_c_abl <= 5:\n            import traceback\n            print(f\"\\n  Error on sample {idx}: {e}\")\n            traceback.print_exc()\n        continue\n\n    if len(ablation_results) % 10 == 0:\n        with open(CHECKPOINT_PATH_C, 'w') as f:\n            json.dump({\n                'results': [{k: v if not isinstance(v, dict) else {str(kk): vv for kk, vv in v.items()}\n                             for k, v in r.items()} for r in ablation_results],\n                'skipped': skipped_c_abl, 'errors': errors_c_abl, 'next_idx': idx + 1\n            }, f)\n        elapsed = time.time() - start_c\n        print(f\"\\n  [{len(ablation_results)} done | {elapsed/60:.0f}m]\")\n\nelapsed_c = time.time() - start_c\nprint(f\"\\nDone. {len(ablation_results)} evaluated, {skipped_c_abl} skipped, {errors_c_abl} errors.\")\nprint(f\"Time: {elapsed_c/60:.1f} min\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Investigation C: Layer Heatmap + Correlation\n",
    "# ============================================================\n",
    "\n",
    "print('=' * 80)\n",
    "print('INVESTIGATION C: LAYER-BY-LAYER ANALYSIS')\n",
    "print('=' * 80)\n",
    "\n",
    "# Per-layer: how much does replacing trunc values -> bare values at that layer hurt?\n",
    "# \"hurt\" = NLL goes up (closer to bare). A layer that contributes more to the benefit\n",
    "# will show a bigger NLL increase when its values are replaced.\n",
    "\n",
    "per_layer_impact = np.zeros(NUM_LAYERS)  # mean NLL increase when layer ablated\n",
    "per_layer_pvals = np.zeros(NUM_LAYERS)\n",
    "\n",
    "for li in range(NUM_LAYERS):\n",
    "    trunc_nlls = np.array([r['nll_trunc'] for r in ablation_results])\n",
    "    ablated_nlls = np.array([r['layer_nlls'][li] for r in ablation_results])\n",
    "    impact = ablated_nlls - trunc_nlls  # positive = ablation hurt (layer contributed)\n",
    "    per_layer_impact[li] = np.mean(impact)\n",
    "    _, per_layer_pvals[li] = stats.ttest_rel(trunc_nlls, ablated_nlls)\n",
    "\n",
    "# Bonferroni correction\n",
    "bonferroni_alpha = 0.05 / NUM_LAYERS\n",
    "\n",
    "print(f\"\\nPer-layer ablation impact (replacing trunc values -> bare values):\")\n",
    "print(f\"{'Layer':>6} {'Impact':>10} {'p-value':>10} {'Sig':>5}\")\n",
    "print('-' * 35)\n",
    "for li in range(NUM_LAYERS):\n",
    "    sig = '*' if per_layer_pvals[li] < bonferroni_alpha else ''\n",
    "    print(f\"{li:>6d} {per_layer_impact[li]:>+10.5f} {per_layer_pvals[li]:>10.4f} {sig:>5}\")\n",
    "\n",
    "# Correlation with value divergence\n",
    "if len(divergence_results) > 0:\n",
    "    mean_div_per_layer = div_matrix.mean(axis=0)\n",
    "    # Ensure same number of layers\n",
    "    n_compare = min(len(mean_div_per_layer), len(per_layer_impact))\n",
    "    rho_div_impact, p_div_impact = stats.spearmanr(\n",
    "        mean_div_per_layer[:n_compare], per_layer_impact[:n_compare]\n",
    "    )\n",
    "    print(f\"\\nCorrelation (value divergence vs ablation impact):\")\n",
    "    print(f\"  Spearman rho={rho_div_impact:.3f}, p={p_div_impact:.4f}\")\n",
    "    if rho_div_impact > 0.3 and p_div_impact < 0.05:\n",
    "        print(\"  -> Layers with more divergent values contribute more to the benefit.\")\n",
    "        print(\"     This supports H1 (value contamination).\")\n",
    "\n",
    "# Plots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# Layer ablation impact\n",
    "ax = axes[0]\n",
    "colors = ['#c44e52' if p < bonferroni_alpha else '#4c72b0' for p in per_layer_pvals]\n",
    "ax.bar(range(NUM_LAYERS), per_layer_impact, color=colors)\n",
    "ax.set_xlabel('Layer')\n",
    "ax.set_ylabel('Mean NLL increase when ablated')\n",
    "ax.set_title('Per-Layer Ablation Impact\\n(red = significant after Bonferroni)')\n",
    "ax.axhline(0, color='gray', linestyle='--', linewidth=0.8)\n",
    "\n",
    "# Value divergence heatmap\n",
    "if len(divergence_results) > 0:\n",
    "    ax = axes[1]\n",
    "    ax.bar(range(len(mean_div_per_layer)), mean_div_per_layer, color='#55a868')\n",
    "    ax.set_xlabel('Layer')\n",
    "    ax.set_ylabel('Mean L2 Divergence (normalized)')\n",
    "    ax.set_title('Per-Layer Value Divergence\\n(bare vs truncated)')\n",
    "\n",
    "    # Correlation scatter\n",
    "    ax = axes[2]\n",
    "    ax.scatter(mean_div_per_layer[:n_compare], per_layer_impact[:n_compare],\n",
    "              c=range(n_compare), cmap='viridis', s=30)\n",
    "    ax.set_xlabel('Value Divergence')\n",
    "    ax.set_ylabel('Ablation Impact')\n",
    "    ax.set_title(f'Divergence vs Impact\\n(rho={rho_div_impact:.3f}, p={p_div_impact:.4f})')\n",
    "    # Label a few extreme points\n",
    "    for li in np.argsort(per_layer_impact)[-3:]:\n",
    "        if li < n_compare:\n",
    "            ax.annotate(f'L{li}', (mean_div_per_layer[li], per_layer_impact[li]),\n",
    "                       fontsize=7, ha='left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/exp10/10_investigation_c.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print('Saved: 10_investigation_c.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# Investigation D: Attention Pattern Analysis (30 samples)\n# FIX: Process bare and trunc attention sequentially to avoid OOM.\n#      Compute stats and free attention tensors before the second pass.\n#      Use build_matched_caches for proper comparison.\n# ============================================================\n\nprint(f'Investigation D: Attention Analysis ({len(samples_d)} samples)')\nprint('  Processing bare/trunc sequentially to avoid OOM')\n\nattn_results = []\nskipped_d = 0\nerrors_d = 0\n\nfor idx in tqdm(range(len(samples_d)), desc=\"Inv D\"):\n    sample = samples_d[idx]\n    passage = sample['passage']\n    query = sample['query']\n    answer = sample['answer']\n\n    answer_ids = tokenizer(answer, return_tensors='pt', add_special_tokens=False)['input_ids']\n    if answer_ids.shape[1] < 2:\n        skipped_d += 1\n        continue\n\n    query_prompt = config.query_template.format(query=query)\n\n    try:\n        # Build matched caches\n        bare_len, bare_cache, keep_len, trunc_corrected, _, offset = \\\n            build_matched_caches(passage, model, tokenizer, config)\n\n        # --- Process BARE first, compute stats, free memory ---\n        nll_bare, attn_bare = score_answer_with_cache_and_attention(\n            bare_cache, bare_len, query_prompt, answer, model, tokenizer, config\n        )\n\n        per_layer_entropy_bare = []\n        per_layer_bos_mass_bare = []\n        for li in range(NUM_LAYERS):\n            ab = attn_bare[li][0].float()  # (n_heads, answer_len, seq_len)\n            eps = 1e-10\n            ent_b = -(ab * (ab + eps).log()).sum(dim=-1).mean().item()\n            bos_b = ab[:, :, 0].mean().item()\n            per_layer_entropy_bare.append(ent_b)\n            per_layer_bos_mass_bare.append(bos_b)\n\n        # Free bare attention immediately\n        del attn_bare, bare_cache\n        torch.cuda.empty_cache()\n\n        # --- Now process TRUNCATED ---\n        nll_trunc, attn_trunc = score_answer_with_cache_and_attention(\n            trunc_corrected, keep_len, query_prompt, answer, model, tokenizer, config\n        )\n\n        per_layer_entropy_trunc = []\n        per_layer_bos_mass_trunc = []\n        for li in range(NUM_LAYERS):\n            at = attn_trunc[li][0].float()\n            eps = 1e-10\n            ent_t = -(at * (at + eps).log()).sum(dim=-1).mean().item()\n            bos_t = at[:, :, 0].mean().item()\n            per_layer_entropy_trunc.append(ent_t)\n            per_layer_bos_mass_trunc.append(bos_t)\n\n        # Free trunc attention\n        del attn_trunc, trunc_corrected\n        torch.cuda.empty_cache()\n\n        attn_results.append({\n            'nll_bare': nll_bare,\n            'nll_trunc': nll_trunc,\n            'entropy_bare': per_layer_entropy_bare,\n            'entropy_trunc': per_layer_entropy_trunc,\n            'bos_mass_bare': per_layer_bos_mass_bare,\n            'bos_mass_trunc': per_layer_bos_mass_trunc,\n        })\n\n    except Exception as e:\n        errors_d += 1\n        if errors_d <= 5:\n            print(f\"  Error on sample {idx}: {e}\")\n        # Clean up on error\n        torch.cuda.empty_cache()\n        continue\n\nprint(f\"\\nDone. {len(attn_results)} samples analyzed, {skipped_d} skipped, {errors_d} errors.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Investigation D: Attention Entropy + BOS Attention Plots\n",
    "# ============================================================\n",
    "\n",
    "print('=' * 80)\n",
    "print('INVESTIGATION D: ATTENTION PATTERN ANALYSIS')\n",
    "print('=' * 80)\n",
    "\n",
    "if len(attn_results) > 0:\n",
    "    ent_bare = np.array([r['entropy_bare'] for r in attn_results])  # (n_samples, n_layers)\n",
    "    ent_trunc = np.array([r['entropy_trunc'] for r in attn_results])\n",
    "    bos_bare = np.array([r['bos_mass_bare'] for r in attn_results])\n",
    "    bos_trunc = np.array([r['bos_mass_trunc'] for r in attn_results])\n",
    "\n",
    "    # Per-layer paired t-tests\n",
    "    print(f\"\\n{'Layer':>6} {'Ent Bare':>10} {'Ent Trunc':>10} {'p(ent)':>10} {'BOS Bare':>10} {'BOS Trunc':>10} {'p(bos)':>10}\")\n",
    "    print('-' * 70)\n",
    "    for li in range(min(NUM_LAYERS, ent_bare.shape[1])):\n",
    "        t_ent, p_ent = stats.ttest_rel(ent_bare[:, li], ent_trunc[:, li])\n",
    "        t_bos, p_bos = stats.ttest_rel(bos_bare[:, li], bos_trunc[:, li])\n",
    "        sig_ent = '*' if p_ent < 0.05 / NUM_LAYERS else ''\n",
    "        sig_bos = '*' if p_bos < 0.05 / NUM_LAYERS else ''\n",
    "        print(f\"{li:>6d} {ent_bare[:, li].mean():>10.4f} {ent_trunc[:, li].mean():>10.4f} {p_ent:>9.4f}{sig_ent}\"\n",
    "              f\" {bos_bare[:, li].mean():>10.6f} {bos_trunc[:, li].mean():>10.6f} {p_bos:>9.4f}{sig_bos}\")\n",
    "\n",
    "    # Overall: is truncated more uniform?\n",
    "    mean_ent_diff = (ent_trunc - ent_bare).mean()\n",
    "    t_overall, p_overall = stats.ttest_rel(ent_bare.mean(axis=1), ent_trunc.mean(axis=1))\n",
    "    print(f\"\\nOverall entropy: bare={ent_bare.mean():.4f}, trunc={ent_trunc.mean():.4f}\")\n",
    "    print(f\"  Trunc - Bare = {mean_ent_diff:+.4f}, paired t={t_overall:.3f}, p={p_overall:.4f}\")\n",
    "\n",
    "    # BOS overall\n",
    "    mean_bos_diff = (bos_trunc - bos_bare).mean()\n",
    "    t_bos_overall, p_bos_overall = stats.ttest_rel(bos_bare.mean(axis=1), bos_trunc.mean(axis=1))\n",
    "    print(f\"\\nOverall BOS mass: bare={bos_bare.mean():.6f}, trunc={bos_trunc.mean():.6f}\")\n",
    "    print(f\"  Trunc - Bare = {mean_bos_diff:+.6f}, paired t={t_bos_overall:.3f}, p={p_bos_overall:.4f}\")\n",
    "\n",
    "    # Plots\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "    ax = axes[0]\n",
    "    layers_x = range(ent_bare.shape[1])\n",
    "    ax.plot(layers_x, ent_bare.mean(axis=0), 'o-', label='Bare', color='#4c72b0', markersize=3)\n",
    "    ax.plot(layers_x, ent_trunc.mean(axis=0), 's-', label='Truncated', color='#c44e52', markersize=3)\n",
    "    ax.set_xlabel('Layer')\n",
    "    ax.set_ylabel('Mean Attention Entropy')\n",
    "    ax.set_title('Attention Entropy by Layer')\n",
    "    ax.legend()\n",
    "\n",
    "    ax = axes[1]\n",
    "    ax.plot(layers_x, bos_bare.mean(axis=0), 'o-', label='Bare', color='#4c72b0', markersize=3)\n",
    "    ax.plot(layers_x, bos_trunc.mean(axis=0), 's-', label='Truncated', color='#c44e52', markersize=3)\n",
    "    ax.set_xlabel('Layer')\n",
    "    ax.set_ylabel('Mean Attention Mass on BOS')\n",
    "    ax.set_title('BOS Attention by Layer')\n",
    "    ax.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('results/exp10/10_investigation_d.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print('Saved: 10_investigation_d.png')\n",
    "else:\n",
    "    print('No attention results to analyze.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Summary: All Results, Decision Matrix, Verdicts\n",
    "# ============================================================\n",
    "\n",
    "print('=' * 80)\n",
    "print('EXPERIMENT 10: COMPREHENSIVE SUMMARY')\n",
    "print('=' * 80)\n",
    "\n",
    "# --- Investigation A Summary ---\n",
    "print('\\n--- INVESTIGATION A: Key vs Value Separation ---')\n",
    "for label, key in conditions_a:\n",
    "    s = inv_a_summary[key]\n",
    "    wr_str = f\"{s['win_rate']*100:.1f}%\" if key != 'nll_bare' else '--'\n",
    "    print(f\"  {label:<33} NLL={s['mean_nll']:.4f}  Win%={wr_str}\")\n",
    "\n",
    "# --- Investigation B Summary ---\n",
    "print('\\n--- INVESTIGATION B: Prefix Length Sensitivity ---')\n",
    "for n_tok in PREFIX_LENGTHS:\n",
    "    s = length_stats[n_tok]\n",
    "    print(f\"  {n_tok:>3d} tokens: win%={s['win_rate']:.1f}%, delta={s['mean_delta']:+.4f}\")\n",
    "print(f\"  Spearman rho={rho:.3f}, p={p_spearman:.4f}\")\n",
    "\n",
    "# --- Investigation C Summary ---\n",
    "print('\\n--- INVESTIGATION C: Layer Analysis ---')\n",
    "top_layers = np.argsort(per_layer_impact)[-5:][::-1]\n",
    "print(f\"  Top contributing layers: {list(top_layers)}\")\n",
    "for li in top_layers:\n",
    "    sig = '*' if per_layer_pvals[li] < bonferroni_alpha else ''\n",
    "    print(f\"    Layer {li}: impact={per_layer_impact[li]:+.5f} {sig}\")\n",
    "if len(divergence_results) > 0:\n",
    "    print(f\"  Divergence-Impact correlation: rho={rho_div_impact:.3f}, p={p_div_impact:.4f}\")\n",
    "\n",
    "# --- Investigation D Summary ---\n",
    "if len(attn_results) > 0:\n",
    "    print('\\n--- INVESTIGATION D: Attention Patterns ---')\n",
    "    print(f\"  Entropy: bare={ent_bare.mean():.4f}, trunc={ent_trunc.mean():.4f}, p={p_overall:.4f}\")\n",
    "    print(f\"  BOS mass: bare={bos_bare.mean():.6f}, trunc={bos_trunc.mean():.6f}, p={p_bos_overall:.4f}\")\n",
    "\n",
    "# --- Decision Matrix ---\n",
    "print('\\n' + '=' * 80)\n",
    "print('DECISION MATRIX')\n",
    "print('=' * 80)\n",
    "\n",
    "print(f\"\\n{'Criterion':<50} {'H1':>8} {'H2':>8} {'H3':>8}\")\n",
    "print('-' * 80)\n",
    "\n",
    "# Cond 3 (value-only) wins big?\n",
    "val_big = 'YES' if val_only_wr > 60 else 'NO'\n",
    "val_h2 = 'NO' if val_only_wr > 60 else 'YES'\n",
    "print(f\"{'Cond 3 (value-only) win% > 60%':<50} {val_big:>8} {val_h2:>8} {'--':>8}\")\n",
    "\n",
    "# Cond 4 (key-only) wins big?\n",
    "key_big = 'NO' if key_only_wr > 60 else 'YES'\n",
    "key_h2 = 'YES' if key_only_wr > 60 else 'NO'\n",
    "print(f\"{'Cond 4 (key-only) win% > 60%':<50} {key_big:>8} {key_h2:>8} {'--':>8}\")\n",
    "\n",
    "# Cond 5 (RoPE noise) wins big?\n",
    "rope_big = 'NO' if rope_noise_wr > 60 else 'YES'\n",
    "rope_h2 = 'YES' if rope_noise_wr > 60 else 'NO'\n",
    "print(f\"{'Cond 5 (RoPE noise) win% > 60%':<50} {rope_big:>8} {rope_h2:>8} {'--':>8}\")\n",
    "\n",
    "# BOS swap effects\n",
    "bos_drop = full_trunc_wr - trunc_bare_bos_wr\n",
    "bos_h3 = 'YES' if bos_drop > 10 else 'NO'\n",
    "print(f\"{'Cond 6 drops >10% vs full trunc':<50} {'--':>8} {'--':>8} {bos_h3:>8}\")\n",
    "bos_gain = 'YES' if bare_trunc_bos_wr > 60 else 'NO'\n",
    "print(f\"{'Cond 7 (bare+trunc BOS) win% > 60%':<50} {'--':>8} {'--':>8} {bos_gain:>8}\")\n",
    "\n",
    "# Length trend\n",
    "len_h1 = 'YES' if rho > 0.1 and p_spearman < 0.05 else 'NO'\n",
    "len_h2 = 'YES' if rho > 0.1 and p_spearman < 0.05 else 'NO'\n",
    "len_h3 = 'YES' if abs(rho) < 0.1 else 'NO'\n",
    "print(f\"{'Benefit increases with prefix length':<50} {len_h1:>8} {len_h2:>8} {len_h3:>8}\")\n",
    "\n",
    "# Divergence-impact correlation\n",
    "if len(divergence_results) > 0:\n",
    "    div_h1 = 'YES' if rho_div_impact > 0.3 and p_div_impact < 0.05 else 'NO'\n",
    "    print(f\"{'Layer divergence predicts ablation impact':<50} {div_h1:>8} {'--':>8} {'--':>8}\")\n",
    "\n",
    "print('\\n--- VERDICT ---')\n",
    "verdicts = []\n",
    "if val_only_wr > 60 and key_only_wr < 55 and rope_noise_wr < 55:\n",
    "    verdicts.append('H1 (Value Contamination): STRONGLY SUPPORTED')\n",
    "elif val_only_wr > 55:\n",
    "    verdicts.append('H1 (Value Contamination): PARTIALLY SUPPORTED')\n",
    "else:\n",
    "    verdicts.append('H1 (Value Contamination): NOT SUPPORTED')\n",
    "\n",
    "if key_only_wr > 60 and rope_noise_wr > 60:\n",
    "    verdicts.append('H2 (RoPE Noise): STRONGLY SUPPORTED')\n",
    "elif key_only_wr > 55 or rope_noise_wr > 55:\n",
    "    verdicts.append('H2 (RoPE Noise): PARTIALLY SUPPORTED')\n",
    "else:\n",
    "    verdicts.append('H2 (RoPE Noise): NOT SUPPORTED')\n",
    "\n",
    "if bos_drop > 10 and bare_trunc_bos_wr > 60:\n",
    "    verdicts.append('H3 (BOS Contamination): STRONGLY SUPPORTED')\n",
    "elif bos_drop > 5 or bare_trunc_bos_wr > 55:\n",
    "    verdicts.append('H3 (BOS Contamination): PARTIALLY SUPPORTED')\n",
    "else:\n",
    "    verdicts.append('H3 (BOS Contamination): NOT SUPPORTED')\n",
    "\n",
    "for v in verdicts:\n",
    "    print(f'  {v}')\n",
    "\n",
    "# Summary visualization\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# Bar chart of Investigation A win rates\n",
    "ax = axes[0]\n",
    "labels_short = ['Bare', 'Full\\nTrunc', 'Value\\nOnly', 'Key\\nOnly', 'RoPE\\nNoise',\n",
    "                'Trunc\\nBare BOS', 'Bare\\nTrunc BOS']\n",
    "win_rates_a = [50] + [inv_a_summary[k]['win_rate']*100 for _, k in conditions_a[1:]]\n",
    "colors_a = ['#888888', '#4c72b0', '#c44e52', '#55a868', '#8c564b', '#e377c2', '#ff7f0e']\n",
    "ax.bar(range(len(labels_short)), win_rates_a, color=colors_a)\n",
    "ax.axhline(50, color='gray', linestyle='--')\n",
    "ax.set_xticks(range(len(labels_short)))\n",
    "ax.set_xticklabels(labels_short, fontsize=7)\n",
    "ax.set_ylabel('Win Rate vs Bare (%)')\n",
    "ax.set_title('Investigation A: Win Rates')\n",
    "for i, wr in enumerate(win_rates_a):\n",
    "    ax.text(i, wr + 1, f'{wr:.0f}%', ha='center', fontsize=7)\n",
    "\n",
    "# Prefix length curve\n",
    "ax = axes[1]\n",
    "ax.plot([l for l in sorted(length_stats.keys())],\n",
    "        [length_stats[l]['win_rate'] for l in sorted(length_stats.keys())],\n",
    "        'o-', color='#4c72b0', linewidth=2)\n",
    "ax.axhline(50, color='gray', linestyle='--')\n",
    "ax.set_xlabel('Prefix Length (tokens)')\n",
    "ax.set_ylabel('Win Rate vs Bare (%)')\n",
    "ax.set_title(f'Investigation B: Length Curve\\n(rho={rho:.3f})')\n",
    "\n",
    "# Layer heatmap\n",
    "ax = axes[2]\n",
    "ax.bar(range(NUM_LAYERS), per_layer_impact,\n",
    "       color=['#c44e52' if p < bonferroni_alpha else '#4c72b0' for p in per_layer_pvals])\n",
    "ax.set_xlabel('Layer')\n",
    "ax.set_ylabel('Ablation Impact')\n",
    "ax.set_title('Investigation C: Layer Impact')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/exp10/10_summary.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print('Saved: 10_summary.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Save All Results\n",
    "# ============================================================\n",
    "\n",
    "output = {\n",
    "    'metadata': {\n",
    "        'experiment': '10_truncation_mechanism_diagnostic',\n",
    "        'timestamp': datetime.datetime.now().isoformat(),\n",
    "        'model_name': config.model_name,\n",
    "        'seed': config.seed,\n",
    "    },\n",
    "    'investigation_a': {\n",
    "        'n_samples': len(results_a),\n",
    "        'summary': inv_a_summary,\n",
    "        'results': results_a,\n",
    "    },\n",
    "    'investigation_b': {\n",
    "        'n_samples': len(results_b),\n",
    "        'prefix_lengths': PREFIX_LENGTHS,\n",
    "        'length_stats': {str(k): v for k, v in length_stats.items()},\n",
    "        'spearman_rho': float(rho),\n",
    "        'spearman_p': float(p_spearman),\n",
    "        'results': results_b,\n",
    "    },\n",
    "    'investigation_c': {\n",
    "        'n_divergence_samples': len(divergence_results),\n",
    "        'n_ablation_samples': len(ablation_results),\n",
    "        'per_layer_impact': per_layer_impact.tolist(),\n",
    "        'per_layer_pvals': per_layer_pvals.tolist(),\n",
    "        'mean_divergence': mean_div.tolist() if len(divergence_results) > 0 else [],\n",
    "        'divergence_impact_rho': float(rho_div_impact) if len(divergence_results) > 0 else None,\n",
    "        'divergence_impact_p': float(p_div_impact) if len(divergence_results) > 0 else None,\n",
    "        'ablation_results': [\n",
    "            {k: v if not isinstance(v, dict) else {str(kk): vv for kk, vv in v.items()}\n",
    "             for k, v in r.items()} for r in ablation_results\n",
    "        ],\n",
    "    },\n",
    "    'investigation_d': {\n",
    "        'n_samples': len(attn_results),\n",
    "        'results': attn_results,\n",
    "    } if len(attn_results) > 0 else {'n_samples': 0},\n",
    "    'verdicts': verdicts,\n",
    "}\n",
    "\n",
    "output_path = 'results/exp10/10_diagnostic_results.json'\n",
    "with open(output_path, 'w') as f:\n",
    "    json.dump(output, f, indent=2, default=str)\n",
    "print(f\"Results saved to {output_path}\")\n",
    "print(f\"File size: {os.path.getsize(output_path) / 1e6:.1f} MB\")\n",
    "\n",
    "# Clean up checkpoints\n",
    "for cp in [CHECKPOINT_PATH_A, CHECKPOINT_PATH_B, CHECKPOINT_PATH_C]:\n",
    "    if os.path.exists(cp):\n",
    "        print(f\"  Checkpoint {cp} preserved for potential resume.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}