{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 08: Diagnosing the Suffix Priming Signal\n",
    "\n",
    "## Motivation\n",
    "\n",
    "Experiment 07 showed that suffix priming produces content-independent effects: relevant, irrelevant,\n",
    "and shuffled suffixes all perform similarly, with ~50% win rate against bare baseline. This mirrors\n",
    "the prefix result (r=0.924).\n",
    "\n",
    "Three hypotheses could explain this:\n",
    "\n",
    "1. **The query makes the suffix redundant.** At scoring time, the model sees\n",
    "   `[passage + suffix] + query + answer`. The real query already tells the model what to\n",
    "   focus on — the suffix is redundant information competing for attention.\n",
    "\n",
    "2. **The model ignores suffix tokens.** Query-time attention to suffix positions may be\n",
    "   negligible — the model has learned to attend primarily to document content.\n",
    "\n",
    "3. **MS MARCO is too easy.** Short passages with extractive answers may not require\n",
    "   better document representations.\n",
    "\n",
    "## Three Investigations\n",
    "\n",
    "**Investigation A: Query-free scoring.** Remove the query from the scoring pipeline.\n",
    "Cache is `[passage + suffix]`, then extend with just `\"\\n\\nAnswer:\"` and score the answer.\n",
    "Now the suffix is the *only* intent signal. If suffixes matter, this is where they'll show it.\n",
    "\n",
    "**Investigation B: Attention pattern analysis.** Extract attention weights during scoring\n",
    "to see whether query/answer tokens actually attend to suffix positions, and whether\n",
    "relevant vs irrelevant suffixes produce different attention patterns.\n",
    "\n",
    "**Investigation C: Hard-sample analysis.** Filter MS MARCO for samples where the bare\n",
    "model struggles (high NLL), where better representations should matter most."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59g849i9bo",
   "source": "## Experimental Notes\n\n- Motivated by Exp 07's failure to find a semantic signal with suffix placement — content-independent effects persisted even when document KV entries were guaranteed identical.\n- **Key discovery:** Suffixes STEAL attention from the query. Document tokens attend to suffix tokens instead of query tokens, reducing query attention share from ~20% to 9-10%. This explains why suffixes consistently hurt answer quality.\n- **Content-independence r=0.797**, still very high — confirming that the effect is structural, not semantic.\n- **Verdict:** Causal attention is the fundamental blocker. In a causal (autoregressive) model, passage tokens cannot \"see\" suffix tokens placed after them. Suffix tokens can only influence later tokens' attention patterns, where they act as distractors.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import sys\n",
    "import os\n",
    "import copy\n",
    "import json\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "from typing import Dict, List, Any, Optional, Tuple\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sys.path.insert(0, '.')\n",
    "\n",
    "from lib import (\n",
    "    ExperimentConfig,\n",
    "    build_kv_cache,\n",
    "    build_suffix_kv_cache,\n",
    "    score_answer_with_cache,\n",
    "    build_truncated_kv_cache_corrected,\n",
    "    generate_all_5_surrogates,\n",
    "    compute_similarity,\n",
    "    load_evaluation_samples,\n",
    "    load_ms_marco,\n",
    "    TOP_5_SURROGATE_TEMPLATES,\n",
    "    STATIC_SURROGATE_QUERIES,\n",
    ")\n",
    "from lib.analysis import cohens_d\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "config = ExperimentConfig(\n",
    "    num_samples=800,\n",
    "    min_passage_words=50,\n",
    "    max_passage_words=300,\n",
    "    surrogate_max_tokens=45,\n",
    "    surrogate_temperature=0.3,\n",
    "    seed=42,\n",
    ")\n",
    "print(f\"Model: {config.model_name}\")\n",
    "print(f\"Device: {config.device}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\nfrom sentence_transformers import SentenceTransformer\n\ntorch.manual_seed(config.seed)\nnp.random.seed(config.seed)\nrandom.seed(config.seed)\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.float16,\n    bnb_4bit_use_double_quant=True,\n)\n\nprint(f\"Loading {config.model_name}...\")\ntokenizer = AutoTokenizer.from_pretrained(config.model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\n    config.model_name,\n    quantization_config=bnb_config,\n    device_map=\"auto\",\n    torch_dtype=torch.float16,\n    attn_implementation=\"eager\",  # Required for output_attentions=True in Investigation B\n)\nmodel.eval()\nprint(f\"Model loaded. Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n\nembed_model = SentenceTransformer(config.embedding_model_name)\nprint(f\"Embedding model loaded: {config.embedding_model_name}\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "dataset = load_ms_marco(config)\n",
    "raw_samples = load_evaluation_samples(dataset, config, require_answer=True)\n",
    "print(f\"Raw samples after basic filtering: {len(raw_samples)}\")\n",
    "\n",
    "filtered_samples = []\n",
    "excluded_ratio = 0\n",
    "excluded_short_answer = 0\n",
    "\n",
    "for s in raw_samples:\n",
    "    if len(s['answer']) / max(len(s['passage']), 1) > 0.5:\n",
    "        excluded_ratio += 1\n",
    "        continue\n",
    "    answer_ids = tokenizer(s['answer'], return_tensors='pt', add_special_tokens=False)['input_ids']\n",
    "    if answer_ids.shape[1] < 2:\n",
    "        excluded_short_answer += 1\n",
    "        continue\n",
    "    filtered_samples.append(s)\n",
    "\n",
    "samples = filtered_samples\n",
    "print(f\"Remaining samples: {len(samples)}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def _get_kv(cache, layer_idx):\n",
    "    \"\"\"Get (keys, values) from cache layer — compatible across DynamicCache versions.\"\"\"\n",
    "    if hasattr(cache, 'key_cache'):\n",
    "        return cache.key_cache[layer_idx], cache.value_cache[layer_idx]\n",
    "    return cache.layers[layer_idx].keys, cache.layers[layer_idx].values\n",
    "\n",
    "\n",
    "def _num_layers(cache):\n",
    "    if hasattr(cache, 'key_cache'):\n",
    "        return len(cache.key_cache)\n",
    "    return len(cache.layers)\n",
    "\n",
    "\n",
    "def deep_copy_cache(cache):\n",
    "    return copy.deepcopy(cache)\n",
    "\n",
    "\n",
    "def get_irrelevant_query(samples, current_idx, rng):\n",
    "    other_idx = current_idx\n",
    "    while other_idx == current_idx:\n",
    "        other_idx = rng.randint(0, len(samples) - 1)\n",
    "    return samples[other_idx]['query']\n",
    "\n",
    "\n",
    "def shuffle_text(text, rng):\n",
    "    words = text.split()\n",
    "    rng.shuffle(words)\n",
    "    return ' '.join(words)\n",
    "\n",
    "\n",
    "def score_answer_queryless(\n",
    "    past_key_values, context_len, answer_prompt, answer,\n",
    "    model, tokenizer, config\n",
    "):\n",
    "    \"\"\"Score answer WITHOUT providing the query.\n",
    "\n",
    "    Instead of: [cache] + query + answer\n",
    "    We do:      [cache] + answer_prompt + answer\n",
    "\n",
    "    The answer_prompt is a short transition like '\\n\\nAnswer:' — no query.\n",
    "    The suffix (if present in cache) is the only intent signal.\n",
    "    \"\"\"\n",
    "    # Tokenize answer prompt (transition text)\n",
    "    prompt_enc = tokenizer(\n",
    "        answer_prompt, return_tensors=\"pt\", add_special_tokens=False,\n",
    "        padding=False, truncation=False\n",
    "    )\n",
    "    prompt_ids = prompt_enc['input_ids'].to(config.device)\n",
    "    prompt_len = prompt_ids.shape[1]\n",
    "\n",
    "    # Tokenize answer\n",
    "    answer_enc = tokenizer(\n",
    "        answer, return_tensors=\"pt\", add_special_tokens=False,\n",
    "        padding=False, truncation=False\n",
    "    )\n",
    "    answer_ids = answer_enc['input_ids'].to(config.device)\n",
    "    answer_len = answer_ids.shape[1]\n",
    "\n",
    "    # Extend cache with prompt\n",
    "    attn_mask = torch.ones((1, context_len + prompt_len), device=config.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        prompt_out = model(\n",
    "            input_ids=prompt_ids,\n",
    "            attention_mask=attn_mask,\n",
    "            past_key_values=past_key_values,\n",
    "            use_cache=True,\n",
    "            return_dict=True,\n",
    "        )\n",
    "        extended_cache = prompt_out.past_key_values\n",
    "\n",
    "    # Score answer\n",
    "    attn_mask_final = torch.ones(\n",
    "        (1, context_len + prompt_len + answer_len), device=config.device\n",
    "    )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        answer_out = model(\n",
    "            input_ids=answer_ids,\n",
    "            attention_mask=attn_mask_final,\n",
    "            past_key_values=extended_cache,\n",
    "            use_cache=False,\n",
    "            return_dict=True,\n",
    "        )\n",
    "\n",
    "    logits = answer_out.logits\n",
    "    shift_logits = logits[:, :-1, :].contiguous().view(-1, logits.size(-1))\n",
    "    shift_labels = answer_ids[:, 1:].contiguous().view(-1)\n",
    "\n",
    "    loss = torch.nn.CrossEntropyLoss(reduction='sum')\n",
    "    nll = loss(shift_logits, shift_labels).item()\n",
    "    num_scored = answer_len - 1\n",
    "    return nll / num_scored if num_scored > 0 else 0.0\n",
    "\n",
    "\n",
    "def extract_attention_to_suffix(\n",
    "    past_key_values, context_len, query_prompt, answer,\n",
    "    model, tokenizer, config,\n",
    "    passage_len, suffix_start,\n",
    "):\n",
    "    \"\"\"Run the scoring forward pass with output_attentions=True.\n",
    "\n",
    "    Returns a dict with attention statistics:\n",
    "    - attn_to_passage: mean attention weight from answer tokens to passage region\n",
    "    - attn_to_suffix: mean attention weight from answer tokens to suffix region\n",
    "    - attn_to_query: mean attention weight from answer tokens to query region\n",
    "    - per_layer_suffix_attn: list of per-layer mean attention to suffix\n",
    "    \"\"\"\n",
    "    cache_copy = deep_copy_cache(past_key_values)\n",
    "\n",
    "    # Tokenize query\n",
    "    query_enc = tokenizer(\n",
    "        query_prompt, return_tensors=\"pt\", add_special_tokens=False\n",
    "    )\n",
    "    query_ids = query_enc['input_ids'].to(config.device)\n",
    "    query_len = query_ids.shape[1]\n",
    "\n",
    "    # Tokenize answer\n",
    "    answer_enc = tokenizer(\n",
    "        answer, return_tensors=\"pt\", add_special_tokens=False\n",
    "    )\n",
    "    answer_ids = answer_enc['input_ids'].to(config.device)\n",
    "    answer_len = answer_ids.shape[1]\n",
    "\n",
    "    # Extend cache with query (need use_cache=True to build extended cache)\n",
    "    attn_mask = torch.ones((1, context_len + query_len), device=config.device)\n",
    "    with torch.no_grad():\n",
    "        query_out = model(\n",
    "            input_ids=query_ids,\n",
    "            attention_mask=attn_mask,\n",
    "            past_key_values=cache_copy,\n",
    "            use_cache=True,\n",
    "            return_dict=True,\n",
    "        )\n",
    "        extended_cache = query_out.past_key_values\n",
    "\n",
    "    # Score answer WITH attention output\n",
    "    total_len = context_len + query_len + answer_len\n",
    "    attn_mask_final = torch.ones((1, total_len), device=config.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        answer_out = model(\n",
    "            input_ids=answer_ids,\n",
    "            attention_mask=attn_mask_final,\n",
    "            past_key_values=extended_cache,\n",
    "            use_cache=False,\n",
    "            return_dict=True,\n",
    "            output_attentions=True,\n",
    "        )\n",
    "\n",
    "    # answer_out.attentions is a tuple of (n_layers,) tensors\n",
    "    # Each tensor shape: (batch, n_heads, answer_len, total_len)\n",
    "    # The total_len includes all cached positions + answer positions\n",
    "\n",
    "    # Define regions in the full sequence:\n",
    "    # [0, passage_len) = passage/BOS tokens\n",
    "    # [suffix_start, context_len) = suffix tokens (if suffix_start < context_len)\n",
    "    # [context_len, context_len + query_len) = query tokens\n",
    "    # [context_len + query_len, total_len) = answer tokens (but these only attend to prior)\n",
    "\n",
    "    # For answer tokens, attention is over [0, context_len + query_len + answer_pos]\n",
    "    # We look at the mean attention across all answer token positions\n",
    "\n",
    "    per_layer_suffix_attn = []\n",
    "    per_layer_passage_attn = []\n",
    "    per_layer_query_attn = []\n",
    "\n",
    "    suffix_len = context_len - suffix_start if suffix_start < context_len else 0\n",
    "    query_start = context_len\n",
    "\n",
    "    for layer_attn in answer_out.attentions:\n",
    "        # shape: (1, n_heads, answer_len, total_ctx_for_this_token)\n",
    "        # For Mistral with GQA, n_heads = 32 (query heads)\n",
    "        attn = layer_attn[0]  # (n_heads, answer_len, total_len)\n",
    "\n",
    "        # Mean over heads and answer positions\n",
    "        mean_attn = attn.mean(dim=(0, 1))  # (total_len,)\n",
    "\n",
    "        # Passage region: [0, suffix_start) — the pure passage tokens\n",
    "        passage_attn = mean_attn[:suffix_start].sum().item()\n",
    "        per_layer_passage_attn.append(passage_attn)\n",
    "\n",
    "        # Suffix region: [suffix_start, context_len)\n",
    "        if suffix_len > 0:\n",
    "            sfx_attn = mean_attn[suffix_start:context_len].sum().item()\n",
    "        else:\n",
    "            sfx_attn = 0.0\n",
    "        per_layer_suffix_attn.append(sfx_attn)\n",
    "\n",
    "        # Query region: [context_len, context_len + query_len)\n",
    "        q_attn = mean_attn[query_start:query_start + query_len].sum().item()\n",
    "        per_layer_query_attn.append(q_attn)\n",
    "\n",
    "    return {\n",
    "        'attn_to_passage': np.mean(per_layer_passage_attn),\n",
    "        'attn_to_suffix': np.mean(per_layer_suffix_attn),\n",
    "        'attn_to_query': np.mean(per_layer_query_attn),\n",
    "        'per_layer_suffix_attn': per_layer_suffix_attn,\n",
    "        'per_layer_passage_attn': per_layer_passage_attn,\n",
    "        'per_layer_query_attn': per_layer_query_attn,\n",
    "        'suffix_len': suffix_len,\n",
    "        'query_len': query_len,\n",
    "        'answer_len': answer_len,\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"Helper functions defined.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigation A: Query-Free Scoring\n",
    "\n",
    "**Key idea**: Remove the query from scoring. If the suffix is the only intent signal,\n",
    "relevant suffixes should dramatically outperform irrelevant ones.\n",
    "\n",
    "### Conditions (6):\n",
    "1. `bare` — passage only, no query, no suffix\n",
    "2. `bare_with_query` — passage only, with query (standard scoring, for calibration)\n",
    "3. `sfx_relevant_no_query` — passage + relevant suffix, NO query\n",
    "4. `sfx_irrel_no_query` — passage + irrelevant suffix, NO query\n",
    "5. `sfx_perfect_no_query` — passage + actual query as suffix, NO query\n",
    "6. `sfx_shuffled_no_query` — passage + shuffled suffix, NO query\n",
    "\n",
    "If the suffix carries semantic signal, conditions 3 and 5 should beat 4 and 6.\n",
    "The gap should be MUCH larger than in exp 07 (where query made suffix redundant)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Pipeline verification on one sample\n",
    "test_sample = samples[0]\n",
    "passage = test_sample['passage']\n",
    "query = test_sample['query']\n",
    "answer = test_sample['answer']\n",
    "query_prompt = config.query_template.format(query=query)\n",
    "answer_prompt = \"\\n\\nAnswer:\"\n",
    "\n",
    "print(f\"Passage: {passage[:80]}...\")\n",
    "print(f\"Query:   {query}\")\n",
    "print(f\"Answer:  {answer[:60]}\")\n",
    "print()\n",
    "\n",
    "# Generate surrogates\n",
    "test_surrogates = generate_all_5_surrogates(passage, model, tokenizer, config)\n",
    "test_sims = {k: compute_similarity(v, query, embed_model) for k, v in test_surrogates.items()}\n",
    "routed_key = max(test_sims, key=test_sims.get)\n",
    "routed_surr = test_surrogates[routed_key]\n",
    "print(f\"Routed surrogate ({routed_key}): {routed_surr}\")\n",
    "print()\n",
    "\n",
    "# --- With query (standard) ---\n",
    "bare_len, bare_cache = build_kv_cache(passage, model, tokenizer, config)\n",
    "nll_bare_q = score_answer_with_cache(\n",
    "    deep_copy_cache(bare_cache), bare_len, query_prompt, answer, model, tokenizer, config\n",
    ")\n",
    "print(f\"1. bare (with query):            NLL = {nll_bare_q:.4f}\")\n",
    "\n",
    "# --- Without query ---\n",
    "nll_bare_noq = score_answer_queryless(\n",
    "    deep_copy_cache(bare_cache), bare_len, answer_prompt, answer, model, tokenizer, config\n",
    ")\n",
    "print(f\"2. bare (NO query):              NLL = {nll_bare_noq:.4f}\")\n",
    "\n",
    "# --- Suffix relevant, no query ---\n",
    "sfx_len, sfx_cache = build_suffix_kv_cache(passage, routed_surr, model, tokenizer, config)\n",
    "nll_sfx_rel_noq = score_answer_queryless(\n",
    "    deep_copy_cache(sfx_cache), sfx_len, answer_prompt, answer, model, tokenizer, config\n",
    ")\n",
    "print(f\"3. sfx_relevant (NO query):      NLL = {nll_sfx_rel_noq:.4f}\")\n",
    "\n",
    "# --- Suffix irrelevant, no query ---\n",
    "rng = random.Random(config.seed)\n",
    "irrel_q = get_irrelevant_query(samples, 0, rng)\n",
    "sfx_len_i, sfx_cache_i = build_suffix_kv_cache(passage, irrel_q, model, tokenizer, config)\n",
    "nll_sfx_irrel_noq = score_answer_queryless(\n",
    "    deep_copy_cache(sfx_cache_i), sfx_len_i, answer_prompt, answer, model, tokenizer, config\n",
    ")\n",
    "print(f\"4. sfx_irrelevant (NO query):    NLL = {nll_sfx_irrel_noq:.4f}\")\n",
    "\n",
    "# --- Suffix perfect (actual query), no query ---\n",
    "sfx_len_p, sfx_cache_p = build_suffix_kv_cache(passage, query, model, tokenizer, config)\n",
    "nll_sfx_perf_noq = score_answer_queryless(\n",
    "    deep_copy_cache(sfx_cache_p), sfx_len_p, answer_prompt, answer, model, tokenizer, config\n",
    ")\n",
    "print(f\"5. sfx_perfect (NO query):       NLL = {nll_sfx_perf_noq:.4f}\")\n",
    "\n",
    "# --- Suffix shuffled, no query ---\n",
    "shuffled_surr = shuffle_text(routed_surr, rng)\n",
    "sfx_len_s, sfx_cache_s = build_suffix_kv_cache(passage, shuffled_surr, model, tokenizer, config)\n",
    "nll_sfx_shuf_noq = score_answer_queryless(\n",
    "    deep_copy_cache(sfx_cache_s), sfx_len_s, answer_prompt, answer, model, tokenizer, config\n",
    ")\n",
    "print(f\"6. sfx_shuffled (NO query):      NLL = {nll_sfx_shuf_noq:.4f}\")\n",
    "\n",
    "print(f\"\\n--- Deltas (lower = better) ---\")\n",
    "print(f\"Query-free: relevant - bare    = {nll_sfx_rel_noq - nll_bare_noq:+.4f}\")\n",
    "print(f\"Query-free: perfect - bare     = {nll_sfx_perf_noq - nll_bare_noq:+.4f}\")\n",
    "print(f\"Query-free: irrelevant - bare  = {nll_sfx_irrel_noq - nll_bare_noq:+.4f}\")\n",
    "print(f\"Query-free: shuffled - bare    = {nll_sfx_shuf_noq - nll_bare_noq:+.4f}\")\n",
    "print(f\"Query-free: relevant - irrel   = {nll_sfx_rel_noq - nll_sfx_irrel_noq:+.4f}\")\n",
    "print(f\"With-query (baseline ref):       {nll_bare_q:.4f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigation A: Main Loop"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def evaluate_queryless(sample, idx, all_samples, model, tokenizer, embed_model, config):\n",
    "    \"\"\"Evaluate query-free scoring for one sample.\"\"\"\n",
    "    passage = sample['passage']\n",
    "    query = sample['query']\n",
    "    answer = sample['answer']\n",
    "    query_prompt = config.query_template.format(query=query)\n",
    "    answer_prompt = \"\\n\\nAnswer:\"\n",
    "    rng = random.Random(config.seed + idx)\n",
    "\n",
    "    # Generate surrogates\n",
    "    surrogates = generate_all_5_surrogates(passage, model, tokenizer, config)\n",
    "    sims = {k: compute_similarity(v, query, embed_model) for k, v in surrogates.items()}\n",
    "    routed_key = max(sims, key=sims.get)\n",
    "    routed_surr = surrogates[routed_key]\n",
    "\n",
    "    # 1. Bare (with query — calibration)\n",
    "    bare_len, bare_cache = build_kv_cache(passage, model, tokenizer, config)\n",
    "    nll_bare_q = score_answer_with_cache(\n",
    "        deep_copy_cache(bare_cache), bare_len, query_prompt, answer, model, tokenizer, config\n",
    "    )\n",
    "\n",
    "    # 2. Bare (no query)\n",
    "    nll_bare_noq = score_answer_queryless(\n",
    "        deep_copy_cache(bare_cache), bare_len, answer_prompt, answer, model, tokenizer, config\n",
    "    )\n",
    "\n",
    "    # 3. Suffix relevant (no query)\n",
    "    sfx_len, sfx_cache = build_suffix_kv_cache(passage, routed_surr, model, tokenizer, config)\n",
    "    nll_sfx_rel_noq = score_answer_queryless(\n",
    "        deep_copy_cache(sfx_cache), sfx_len, answer_prompt, answer, model, tokenizer, config\n",
    "    )\n",
    "\n",
    "    # 4. Suffix irrelevant (no query)\n",
    "    irrel_q = get_irrelevant_query(all_samples, idx, rng)\n",
    "    sfx_len_i, sfx_cache_i = build_suffix_kv_cache(passage, irrel_q, model, tokenizer, config)\n",
    "    nll_sfx_irrel_noq = score_answer_queryless(\n",
    "        deep_copy_cache(sfx_cache_i), sfx_len_i, answer_prompt, answer, model, tokenizer, config\n",
    "    )\n",
    "\n",
    "    # 5. Suffix perfect (no query)\n",
    "    sfx_len_p, sfx_cache_p = build_suffix_kv_cache(passage, query, model, tokenizer, config)\n",
    "    nll_sfx_perf_noq = score_answer_queryless(\n",
    "        deep_copy_cache(sfx_cache_p), sfx_len_p, answer_prompt, answer, model, tokenizer, config\n",
    "    )\n",
    "\n",
    "    # 6. Suffix shuffled (no query)\n",
    "    shuffled = shuffle_text(routed_surr, rng)\n",
    "    sfx_len_s, sfx_cache_s = build_suffix_kv_cache(passage, shuffled, model, tokenizer, config)\n",
    "    nll_sfx_shuf_noq = score_answer_queryless(\n",
    "        deep_copy_cache(sfx_cache_s), sfx_len_s, answer_prompt, answer, model, tokenizer, config\n",
    "    )\n",
    "\n",
    "    # Also score WITH query for the suffix conditions (exp 07 replication)\n",
    "    nll_sfx_rel_wq = score_answer_with_cache(\n",
    "        deep_copy_cache(sfx_cache), sfx_len, query_prompt, answer, model, tokenizer, config\n",
    "    )\n",
    "    nll_sfx_irrel_wq = score_answer_with_cache(\n",
    "        deep_copy_cache(sfx_cache_i), sfx_len_i, query_prompt, answer, model, tokenizer, config\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        'idx': idx,\n",
    "        'query': query,\n",
    "        'passage_len': len(passage),\n",
    "        'answer_len': len(answer),\n",
    "        'routed_key': routed_key,\n",
    "        'routed_sim': sims[routed_key],\n",
    "\n",
    "        # With query\n",
    "        'bare_wq': nll_bare_q,\n",
    "        'sfx_rel_wq': nll_sfx_rel_wq,\n",
    "        'sfx_irrel_wq': nll_sfx_irrel_wq,\n",
    "\n",
    "        # Without query\n",
    "        'bare_noq': nll_bare_noq,\n",
    "        'sfx_rel_noq': nll_sfx_rel_noq,\n",
    "        'sfx_irrel_noq': nll_sfx_irrel_noq,\n",
    "        'sfx_perfect_noq': nll_sfx_perf_noq,\n",
    "        'sfx_shuffled_noq': nll_sfx_shuf_noq,\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"evaluate_queryless() defined — 8 conditions.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "results_a = []\n",
    "errors_a = 0\n",
    "start_time_a = time.time()\n",
    "checkpoint_path_a = 'results/exp08/08a_checkpoint.json'\n",
    "\n",
    "# Resume\n",
    "start_from_a = 0\n",
    "if os.path.exists(checkpoint_path_a):\n",
    "    with open(checkpoint_path_a) as f:\n",
    "        ckpt = json.load(f)\n",
    "    results_a = ckpt['results']\n",
    "    start_from_a = ckpt['n_done']\n",
    "    print(f\"Resuming Investigation A from sample {start_from_a}\")\n",
    "else:\n",
    "    print(\"Starting Investigation A from scratch.\")\n",
    "\n",
    "N_SAMPLES_A = min(200, len(samples))  # 200 samples for Investigation A\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"INVESTIGATION A: QUERY-FREE SCORING\")\n",
    "print(f\"Samples: {N_SAMPLES_A}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for idx in tqdm(range(start_from_a, N_SAMPLES_A), desc=\"Inv A\"):\n",
    "    sample = samples[idx]\n",
    "    try:\n",
    "        result = evaluate_queryless(\n",
    "            sample, idx, samples, model, tokenizer, embed_model, config\n",
    "        )\n",
    "        if result is not None:\n",
    "            results_a.append(result)\n",
    "    except Exception as e:\n",
    "        errors_a += 1\n",
    "        if errors_a <= 5:\n",
    "            print(f\"\\n  Error on sample {idx}: {type(e).__name__}: {e}\")\n",
    "        continue\n",
    "\n",
    "    if len(results_a) > 0 and len(results_a) % 10 == 0:\n",
    "        # Checkpoint\n",
    "        with open(checkpoint_path_a, 'w') as f:\n",
    "            json.dump({'n_done': len(results_a), 'results': results_a}, f, default=str)\n",
    "\n",
    "    if len(results_a) > 0 and len(results_a) % 50 == 0:\n",
    "        recent = results_a[-50:]\n",
    "        bare_noq_m = np.mean([r['bare_noq'] for r in recent])\n",
    "        rel_noq_m = np.mean([r['sfx_rel_noq'] for r in recent])\n",
    "        irrel_noq_m = np.mean([r['sfx_irrel_noq'] for r in recent])\n",
    "        perf_noq_m = np.mean([r['sfx_perfect_noq'] for r in recent])\n",
    "        wr_rel = np.mean([r['bare_noq'] > r['sfx_rel_noq'] for r in recent]) * 100\n",
    "        wr_perf = np.mean([r['bare_noq'] > r['sfx_perfect_noq'] for r in recent]) * 100\n",
    "        print(\n",
    "            f\"\\n  [{len(results_a)} done] bare_noq={bare_noq_m:.3f} \"\n",
    "            f\"rel_noq={rel_noq_m:.3f}({wr_rel:.0f}% win) \"\n",
    "            f\"perf_noq={perf_noq_m:.3f}({wr_perf:.0f}% win) \"\n",
    "            f\"irrel_noq={irrel_noq_m:.3f}\"\n",
    "        )\n",
    "\n",
    "elapsed_a = time.time() - start_time_a\n",
    "print(f\"\\nDone. {len(results_a)} evaluated, {errors_a} errors, {elapsed_a/60:.1f}m\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigation A: Results"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "n_a = len(results_a)\n",
    "print(\"=\" * 120)\n",
    "print(f\"INVESTIGATION A: QUERY-FREE SCORING RESULTS (N = {n_a})\")\n",
    "print(\"=\" * 120)\n",
    "\n",
    "conditions_a = [\n",
    "    ('bare (with query)',         'bare_wq'),\n",
    "    ('bare (NO query)',           'bare_noq'),\n",
    "    ('sfx_relevant (NO query)',   'sfx_rel_noq'),\n",
    "    ('sfx_perfect (NO query)',    'sfx_perfect_noq'),\n",
    "    ('sfx_irrelevant (NO query)', 'sfx_irrel_noq'),\n",
    "    ('sfx_shuffled (NO query)',   'sfx_shuffled_noq'),\n",
    "    ('sfx_relevant (with query)', 'sfx_rel_wq'),\n",
    "    ('sfx_irrel (with query)',    'sfx_irrel_wq'),\n",
    "]\n",
    "\n",
    "bare_noq = np.array([r['bare_noq'] for r in results_a])\n",
    "\n",
    "print(f\"\\n{'Condition':<32} {'Mean NLL':>10} {'Std':>8} {'Delta vs bare_noq':>18} {'Win%':>8} {'t':>8} {'p':>12} {'d':>8}\")\n",
    "print(\"-\" * 120)\n",
    "\n",
    "for label, key in conditions_a:\n",
    "    arr = np.array([r[key] for r in results_a])\n",
    "    delta = bare_noq - arr\n",
    "    mn = np.mean(arr)\n",
    "    sd = np.std(arr)\n",
    "    if key == 'bare_noq':\n",
    "        print(f\"{label:<32} {mn:>10.4f} {sd:>8.4f} {'BASELINE':>18} {'--':>8} {'--':>8} {'--':>12} {'--':>8}\")\n",
    "    else:\n",
    "        t, p = stats.ttest_rel(bare_noq, arr)\n",
    "        d = cohens_d(delta)\n",
    "        wr = np.mean(delta > 0) * 100\n",
    "        print(f\"{label:<32} {mn:>10.4f} {sd:>8.4f} {np.mean(delta):>+18.4f} {wr:>7.1f}% {t:>8.3f} {p:>12.6f} {d:>8.4f}\")\n",
    "\n",
    "# Key pairwise comparisons\n",
    "print(f\"\\n{'=' * 120}\")\n",
    "print(\"KEY PAIRWISE: Does removing the query expose semantic signal?\")\n",
    "print(\"=\" * 120)\n",
    "\n",
    "pairs_a = [\n",
    "    (\"relevant vs irrelevant (NO query)\",  'sfx_rel_noq',     'sfx_irrel_noq'),\n",
    "    (\"perfect vs irrelevant (NO query)\",   'sfx_perfect_noq', 'sfx_irrel_noq'),\n",
    "    (\"relevant vs shuffled (NO query)\",    'sfx_rel_noq',     'sfx_shuffled_noq'),\n",
    "    (\"relevant vs irrelevant (WITH query)\",'sfx_rel_wq',      'sfx_irrel_wq'),\n",
    "]\n",
    "\n",
    "for label, ka, kb in pairs_a:\n",
    "    a = np.array([r[ka] for r in results_a])\n",
    "    b = np.array([r[kb] for r in results_a])\n",
    "    diff = b - a  # positive = a better (lower NLL)\n",
    "    t, p = stats.ttest_rel(a, b)\n",
    "    d = cohens_d(diff)\n",
    "    print(f\"  {label}\")\n",
    "    print(f\"    mean A={np.mean(a):.4f}, mean B={np.mean(b):.4f}, delta={np.mean(diff):+.4f}, t={t:.3f}, p={p:.6f}, d={d:.4f}\")\n",
    "\n",
    "# Correlation: query-free deltas\n",
    "print(f\"\\n{'=' * 120}\")\n",
    "print(\"CORRELATION: query-free deltas (relevant vs shuffled)\")\n",
    "print(\"=\" * 120)\n",
    "rel_deltas = np.array([r['bare_noq'] - r['sfx_rel_noq'] for r in results_a])\n",
    "shuf_deltas = np.array([r['bare_noq'] - r['sfx_shuffled_noq'] for r in results_a])\n",
    "irrel_deltas = np.array([r['bare_noq'] - r['sfx_irrel_noq'] for r in results_a])\n",
    "\n",
    "r_shuf, p_shuf = stats.pearsonr(rel_deltas, shuf_deltas)\n",
    "r_irrel, p_irrel = stats.pearsonr(rel_deltas, irrel_deltas)\n",
    "print(f\"  relevant vs shuffled deltas:   r = {r_shuf:.4f} (p = {p_shuf:.6f})\")\n",
    "print(f\"  relevant vs irrelevant deltas: r = {r_irrel:.4f} (p = {p_irrel:.6f})\")\n",
    "print(f\"  (Exp 07 with-query prefix ref: r ~ 0.924)\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigation B: Attention Pattern Analysis\n",
    "\n",
    "Extract attention weights to see:\n",
    "1. How much do answer tokens attend to suffix positions vs passage vs query?\n",
    "2. Does this differ between relevant and irrelevant suffixes?\n",
    "3. Which layers attend most to the suffix?\n",
    "\n",
    "Run on a smaller subset (30 samples) since `output_attentions=True` is memory-intensive."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "N_SAMPLES_B = 30\nresults_b = []\nerrors_b = 0\n\n# Force eager attention for output_attentions=True (SDPA returns None for attention weights)\nmodel.config._attn_implementation = \"eager\"\nmodel.config._attn_implementation_internal = \"eager\"\n# Also patch each layer's attention module to use eager forward\nfor layer in model.model.layers:\n    layer.self_attn._attn_implementation = \"eager\"\n\nprint(\"=\" * 80)\nprint(f\"INVESTIGATION B: ATTENTION ANALYSIS (N = {N_SAMPLES_B})\")\nprint(\"=\" * 80)\n\nfor idx in tqdm(range(N_SAMPLES_B), desc=\"Inv B\"):\n    sample = samples[idx]\n    try:\n        passage = sample['passage']\n        query = sample['query']\n        answer = sample['answer']\n        query_prompt = config.query_template.format(query=query)\n        rng = random.Random(config.seed + idx)\n\n        # Generate surrogate\n        surrogates = generate_all_5_surrogates(passage, model, tokenizer, config)\n        sims = {k: compute_similarity(v, query, embed_model) for k, v in surrogates.items()}\n        routed_key = max(sims, key=sims.get)\n        routed_surr = surrogates[routed_key]\n\n        # Get bare passage length (for computing suffix start position)\n        bare_len, bare_cache = build_kv_cache(passage, model, tokenizer, config)\n\n        # Build suffix caches\n        sfx_len_rel, sfx_cache_rel = build_suffix_kv_cache(\n            passage, routed_surr, model, tokenizer, config\n        )\n        irrel_q = get_irrelevant_query(samples, idx, rng)\n        sfx_len_irrel, sfx_cache_irrel = build_suffix_kv_cache(\n            passage, irrel_q, model, tokenizer, config\n        )\n\n        # Extract attention for bare (no suffix, suffix_start = bare_len)\n        attn_bare = extract_attention_to_suffix(\n            deep_copy_cache(bare_cache), bare_len, query_prompt, answer,\n            model, tokenizer, config,\n            passage_len=bare_len, suffix_start=bare_len,\n        )\n\n        # Extract attention for relevant suffix\n        attn_rel = extract_attention_to_suffix(\n            deep_copy_cache(sfx_cache_rel), sfx_len_rel, query_prompt, answer,\n            model, tokenizer, config,\n            passage_len=bare_len, suffix_start=bare_len,\n        )\n\n        # Extract attention for irrelevant suffix\n        attn_irrel = extract_attention_to_suffix(\n            deep_copy_cache(sfx_cache_irrel), sfx_len_irrel, query_prompt, answer,\n            model, tokenizer, config,\n            passage_len=bare_len, suffix_start=bare_len,\n        )\n\n        results_b.append({\n            'idx': idx,\n            'bare_len': bare_len,\n            'sfx_rel_len': sfx_len_rel,\n            'sfx_irrel_len': sfx_len_irrel,\n            'attn_bare': attn_bare,\n            'attn_rel': attn_rel,\n            'attn_irrel': attn_irrel,\n        })\n\n        torch.cuda.empty_cache()\n\n    except Exception as e:\n        errors_b += 1\n        if errors_b <= 5:\n            print(f\"\\n  Error on sample {idx}: {type(e).__name__}: {e}\")\n        torch.cuda.empty_cache()\n        continue\n\nprint(f\"\\nDone. {len(results_b)} evaluated, {errors_b} errors.\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigation B: Results"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "n_b = len(results_b)\n",
    "print(\"=\" * 100)\n",
    "print(f\"ATTENTION ANALYSIS RESULTS (N = {n_b})\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "# Overall attention distribution\n",
    "print(f\"\\n{'Condition':<25} {'Attn to Passage':>18} {'Attn to Suffix':>18} {'Attn to Query':>18}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for label, key in [('Bare (no suffix)', 'attn_bare'), ('Relevant suffix', 'attn_rel'), ('Irrelevant suffix', 'attn_irrel')]:\n",
    "    passage_attn = np.mean([r[key]['attn_to_passage'] for r in results_b])\n",
    "    suffix_attn = np.mean([r[key]['attn_to_suffix'] for r in results_b])\n",
    "    query_attn = np.mean([r[key]['attn_to_query'] for r in results_b])\n",
    "    print(f\"{label:<25} {passage_attn:>18.4f} {suffix_attn:>18.4f} {query_attn:>18.4f}\")\n",
    "\n",
    "# Per-layer suffix attention\n",
    "print(f\"\\n{'=' * 100}\")\n",
    "print(\"PER-LAYER MEAN ATTENTION TO SUFFIX (relevant vs irrelevant)\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "n_layers_model = len(results_b[0]['attn_rel']['per_layer_suffix_attn'])\n",
    "print(f\"\\n{'Layer':<8} {'Relevant':>12} {'Irrelevant':>12} {'Delta':>12}\")\n",
    "print(\"-\" * 48)\n",
    "\n",
    "for layer_idx in range(n_layers_model):\n",
    "    rel_attn = np.mean([r['attn_rel']['per_layer_suffix_attn'][layer_idx] for r in results_b])\n",
    "    irrel_attn = np.mean([r['attn_irrel']['per_layer_suffix_attn'][layer_idx] for r in results_b])\n",
    "    delta = rel_attn - irrel_attn\n",
    "    print(f\"{layer_idx:<8} {rel_attn:>12.6f} {irrel_attn:>12.6f} {delta:>+12.6f}\")\n",
    "\n",
    "# Statistical test: does relevant suffix get more attention than irrelevant?\n",
    "rel_sfx_attn = np.array([r['attn_rel']['attn_to_suffix'] for r in results_b])\n",
    "irrel_sfx_attn = np.array([r['attn_irrel']['attn_to_suffix'] for r in results_b])\n",
    "t_attn, p_attn = stats.ttest_rel(rel_sfx_attn, irrel_sfx_attn)\n",
    "print(f\"\\nRelevant vs irrelevant suffix attention: t={t_attn:.3f}, p={p_attn:.6f}\")\n",
    "print(f\"  Mean relevant: {np.mean(rel_sfx_attn):.6f}\")\n",
    "print(f\"  Mean irrel:    {np.mean(irrel_sfx_attn):.6f}\")\n",
    "print(f\"  Delta:         {np.mean(rel_sfx_attn - irrel_sfx_attn):+.6f}\")\n",
    "\n",
    "# What fraction of total attention goes to suffix?\n",
    "rel_total = rel_sfx_attn / np.array([\n",
    "    r['attn_rel']['attn_to_passage'] + r['attn_rel']['attn_to_suffix'] + r['attn_rel']['attn_to_query']\n",
    "    for r in results_b\n",
    "])\n",
    "print(f\"\\nFraction of attention to suffix (relevant): {np.mean(rel_total)*100:.2f}%\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigation B: Visualization"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('Investigation B: Attention Pattern Analysis', fontsize=14, fontweight='bold')\n",
    "\n",
    "# (0,0) Attention distribution: bar chart\n",
    "ax = axes[0, 0]\n",
    "conditions_attn = ['Bare', 'Rel Suffix', 'Irrel Suffix']\n",
    "regions = ['Passage', 'Suffix', 'Query']\n",
    "data_attn = np.zeros((3, 3))  # conditions x regions\n",
    "\n",
    "for i, key in enumerate(['attn_bare', 'attn_rel', 'attn_irrel']):\n",
    "    data_attn[i, 0] = np.mean([r[key]['attn_to_passage'] for r in results_b])\n",
    "    data_attn[i, 1] = np.mean([r[key]['attn_to_suffix'] for r in results_b])\n",
    "    data_attn[i, 2] = np.mean([r[key]['attn_to_query'] for r in results_b])\n",
    "\n",
    "x = np.arange(len(conditions_attn))\n",
    "w = 0.25\n",
    "for j, (region, color) in enumerate(zip(regions, ['#4c72b0', '#55a868', '#dd8452'])):\n",
    "    ax.bar(x + j*w, data_attn[:, j], w, label=region, color=color)\n",
    "ax.set_xticks(x + w)\n",
    "ax.set_xticklabels(conditions_attn)\n",
    "ax.set_ylabel('Mean Attention Weight')\n",
    "ax.set_title('Attention Distribution by Region')\n",
    "ax.legend()\n",
    "\n",
    "# (0,1) Per-layer suffix attention\n",
    "ax = axes[0, 1]\n",
    "layers = list(range(n_layers_model))\n",
    "rel_per_layer = [np.mean([r['attn_rel']['per_layer_suffix_attn'][l] for r in results_b]) for l in layers]\n",
    "irrel_per_layer = [np.mean([r['attn_irrel']['per_layer_suffix_attn'][l] for r in results_b]) for l in layers]\n",
    "ax.plot(layers, rel_per_layer, 'b-', label='Relevant suffix', alpha=0.8)\n",
    "ax.plot(layers, irrel_per_layer, 'r-', label='Irrelevant suffix', alpha=0.8)\n",
    "ax.set_xlabel('Layer')\n",
    "ax.set_ylabel('Mean Attention to Suffix')\n",
    "ax.set_title('Per-Layer Suffix Attention')\n",
    "ax.legend()\n",
    "\n",
    "# (1,0) Suffix attention: relevant vs irrelevant scatter\n",
    "ax = axes[1, 0]\n",
    "ax.scatter(rel_sfx_attn, irrel_sfx_attn, alpha=0.5, s=20, c='#4c72b0')\n",
    "lims = [0, max(rel_sfx_attn.max(), irrel_sfx_attn.max()) * 1.1]\n",
    "ax.plot(lims, lims, 'r--', linewidth=1, alpha=0.5)\n",
    "ax.set_xlabel('Attention to Relevant Suffix')\n",
    "ax.set_ylabel('Attention to Irrelevant Suffix')\n",
    "ax.set_title('Suffix Attention: Relevant vs Irrelevant')\n",
    "\n",
    "# (1,1) Per-layer delta (relevant - irrelevant)\n",
    "ax = axes[1, 1]\n",
    "delta_per_layer = [r - i for r, i in zip(rel_per_layer, irrel_per_layer)]\n",
    "colors = ['#55a868' if d > 0 else '#c44e52' for d in delta_per_layer]\n",
    "ax.bar(layers, delta_per_layer, color=colors)\n",
    "ax.axhline(0, color='black', linestyle='-', linewidth=0.5)\n",
    "ax.set_xlabel('Layer')\n",
    "ax.set_ylabel('Attention Delta (rel - irrel)')\n",
    "ax.set_title('Per-Layer: Does Relevant Suffix Get More Attention?')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/exp08/08_attention_analysis.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"Saved: 08_attention_analysis.png\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigation C: Hard-Sample Analysis\n",
    "\n",
    "Filter for samples where bare NLL is high (model struggles). On these samples,\n",
    "better document representations should matter most. Use the exp 07 results\n",
    "if available, otherwise compute from Investigation A data."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Try to load exp 07 results for richer data\n",
    "exp07_path = 'results/exp07/07_suffix_priming_results.json'\n",
    "exp07_ckpt = 'results/exp07/07_checkpoint.json'\n",
    "\n",
    "if os.path.exists(exp07_path):\n",
    "    with open(exp07_path) as f:\n",
    "        exp07_data = json.load(f)\n",
    "    results_07 = exp07_data['results']\n",
    "    source = \"exp 07 results\"\n",
    "elif os.path.exists(exp07_ckpt):\n",
    "    with open(exp07_ckpt) as f:\n",
    "        exp07_data = json.load(f)\n",
    "    results_07 = exp07_data['results']\n",
    "    source = \"exp 07 checkpoint\"\n",
    "else:\n",
    "    results_07 = None\n",
    "    source = \"Investigation A data\"\n",
    "\n",
    "if results_07:\n",
    "    print(f\"Loaded {len(results_07)} samples from {source}\")\n",
    "else:\n",
    "    print(f\"No exp 07 data found. Using Investigation A data ({len(results_a)} samples).\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Use whichever data source is available\n",
    "if results_07:\n",
    "    hard_data = results_07\n",
    "    bare_key = 'bare_nll'\n",
    "    sfx_rel_key = 'sfx_gen_routed_nll'\n",
    "    sfx_irrel_key = 'sfx_irrel_nll'\n",
    "    sfx_shuf_key = 'sfx_shuffled_nll'\n",
    "    sfx_perf_key = 'sfx_perfect_nll'\n",
    "    pfx_key = 'pfx_trunc_routed_nll'\n",
    "    has_prefix = True\n",
    "else:\n",
    "    # Map Investigation A keys\n",
    "    hard_data = results_a\n",
    "    bare_key = 'bare_wq'\n",
    "    sfx_rel_key = 'sfx_rel_wq'\n",
    "    sfx_irrel_key = 'sfx_irrel_wq'\n",
    "    sfx_shuf_key = None\n",
    "    sfx_perf_key = None\n",
    "    pfx_key = None\n",
    "    has_prefix = False\n",
    "\n",
    "bare_nlls = np.array([r[bare_key] for r in hard_data])\n",
    "\n",
    "# Compute difficulty quartiles\n",
    "q25, q50, q75 = np.percentile(bare_nlls, [25, 50, 75])\n",
    "print(f\"Bare NLL quartiles: Q25={q25:.3f}, Q50={q50:.3f}, Q75={q75:.3f}\")\n",
    "print(f\"Range: [{bare_nlls.min():.3f}, {bare_nlls.max():.3f}]\")\n",
    "\n",
    "# Split into quartiles\n",
    "quartile_masks = {\n",
    "    'Q1 (easiest)': bare_nlls <= q25,\n",
    "    'Q2': (bare_nlls > q25) & (bare_nlls <= q50),\n",
    "    'Q3': (bare_nlls > q50) & (bare_nlls <= q75),\n",
    "    'Q4 (hardest)': bare_nlls > q75,\n",
    "}\n",
    "\n",
    "# Also define a \"hard\" subset: top 25%\n",
    "hard_mask = bare_nlls > q75\n",
    "# And \"very hard\": top 10%\n",
    "q90 = np.percentile(bare_nlls, 90)\n",
    "very_hard_mask = bare_nlls > q90\n",
    "\n",
    "print(f\"\\nHard samples (Q4, top 25%): {hard_mask.sum()}\")\n",
    "print(f\"Very hard samples (top 10%): {very_hard_mask.sum()}\")\n",
    "\n",
    "print(f\"\\n{'=' * 120}\")\n",
    "print(\"WIN RATES BY DIFFICULTY QUARTILE\")\n",
    "print(\"=\" * 120)\n",
    "\n",
    "conditions_c = [('sfx_relevant', sfx_rel_key), ('sfx_irrelevant', sfx_irrel_key)]\n",
    "if sfx_shuf_key:\n",
    "    conditions_c.append(('sfx_shuffled', sfx_shuf_key))\n",
    "if sfx_perf_key:\n",
    "    conditions_c.append(('sfx_perfect', sfx_perf_key))\n",
    "if pfx_key:\n",
    "    conditions_c.append(('pfx_trunc', pfx_key))\n",
    "\n",
    "header = f\"{'Quartile':<18} {'N':>5}\"\n",
    "for name, _ in conditions_c:\n",
    "    header += f\" {name:>16}\"\n",
    "print(header)\n",
    "print(\"-\" * 120)\n",
    "\n",
    "for q_label, mask in quartile_masks.items():\n",
    "    if mask.sum() == 0:\n",
    "        continue\n",
    "    row = f\"{q_label:<18} {mask.sum():>5}\"\n",
    "    for name, key in conditions_c:\n",
    "        if key is None:\n",
    "            row += f\" {'N/A':>16}\"\n",
    "            continue\n",
    "        cond_nlls = np.array([r[key] for r in hard_data])[mask]\n",
    "        bare_q = bare_nlls[mask]\n",
    "        wr = np.mean(bare_q > cond_nlls) * 100\n",
    "        delta = np.mean(bare_q - cond_nlls)\n",
    "        row += f\" {wr:>6.1f}% ({delta:+.3f})\"\n",
    "    print(row)\n",
    "\n",
    "# Detailed analysis on hard samples\n",
    "print(f\"\\n{'=' * 120}\")\n",
    "print(\"HARD SAMPLES ONLY (top 25% bare NLL)\")\n",
    "print(\"=\" * 120)\n",
    "\n",
    "hard_indices = np.where(hard_mask)[0]\n",
    "print(f\"N = {len(hard_indices)}\")\n",
    "\n",
    "for name, key in conditions_c:\n",
    "    if key is None:\n",
    "        continue\n",
    "    cond_nlls = np.array([r[key] for r in hard_data])[hard_mask]\n",
    "    bare_q = bare_nlls[hard_mask]\n",
    "    delta = bare_q - cond_nlls\n",
    "    t, p = stats.ttest_rel(bare_q, cond_nlls)\n",
    "    d = cohens_d(delta)\n",
    "    wr = np.mean(delta > 0) * 100\n",
    "    print(f\"  {name:<20} delta={np.mean(delta):+.4f}  win={wr:.1f}%  t={t:.3f}  p={p:.6f}  d={d:.4f}\")\n",
    "\n",
    "# Semantic separation on hard samples only\n",
    "if sfx_rel_key and sfx_irrel_key:\n",
    "    hard_rel = np.array([r[sfx_rel_key] for r in hard_data])[hard_mask]\n",
    "    hard_irrel = np.array([r[sfx_irrel_key] for r in hard_data])[hard_mask]\n",
    "    t_sep, p_sep = stats.ttest_rel(hard_rel, hard_irrel)\n",
    "    d_sep = cohens_d(hard_irrel - hard_rel)\n",
    "    print(f\"\\n  Semantic separation (rel vs irrel) on hard samples:\")\n",
    "    print(f\"    delta={np.mean(hard_irrel - hard_rel):+.4f}  t={t_sep:.3f}  p={p_sep:.6f}  d={d_sep:.4f}\")\n",
    "\n",
    "# Correlation test on hard samples\n",
    "if sfx_shuf_key:\n",
    "    hard_rel_delta = bare_nlls[hard_mask] - np.array([r[sfx_rel_key] for r in hard_data])[hard_mask]\n",
    "    hard_shuf_delta = bare_nlls[hard_mask] - np.array([r[sfx_shuf_key] for r in hard_data])[hard_mask]\n",
    "    r_hard, p_hard = stats.pearsonr(hard_rel_delta, hard_shuf_delta)\n",
    "    print(f\"\\n  Delta correlation on hard samples (rel vs shuffled): r={r_hard:.4f} (p={p_hard:.6f})\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigation C: Extractive vs Non-Extractive"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Measure answer extractiveness: token overlap between answer and passage\n",
    "print(\"=\" * 100)\n",
    "print(\"EXTRACTIVE vs NON-EXTRACTIVE ANALYSIS\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "def token_overlap_ratio(answer, passage):\n",
    "    \"\"\"Fraction of answer words found in passage.\"\"\"\n",
    "    answer_words = set(answer.lower().split())\n",
    "    passage_words = set(passage.lower().split())\n",
    "    if not answer_words:\n",
    "        return 1.0\n",
    "    return len(answer_words & passage_words) / len(answer_words)\n",
    "\n",
    "overlaps = np.array([token_overlap_ratio(r.get('answer', r.get('query', '')),\n",
    "                                          samples[r['idx']]['passage'] if r['idx'] < len(samples) else '')\n",
    "                      for r in hard_data])\n",
    "\n",
    "# If overlaps couldn't be computed properly, try direct\n",
    "if np.all(overlaps == 0):\n",
    "    print(\"Note: overlap computation fell back — using passage from samples directly\")\n",
    "    overlaps = np.array([token_overlap_ratio(samples[i]['answer'], samples[i]['passage'])\n",
    "                          for i in range(len(hard_data))])\n",
    "\n",
    "overlap_median = np.median(overlaps)\n",
    "extractive_mask = overlaps >= overlap_median\n",
    "nonextractive_mask = overlaps < overlap_median\n",
    "\n",
    "print(f\"Overlap median: {overlap_median:.3f}\")\n",
    "print(f\"Extractive (>= median): {extractive_mask.sum()}\")\n",
    "print(f\"Non-extractive (< median): {nonextractive_mask.sum()}\")\n",
    "\n",
    "for subset_name, mask in [(\"Extractive\", extractive_mask), (\"Non-extractive\", nonextractive_mask)]:\n",
    "    print(f\"\\n  --- {subset_name} (N={mask.sum()}) ---\")\n",
    "    for name, key in conditions_c:\n",
    "        if key is None:\n",
    "            continue\n",
    "        cond_nlls = np.array([r[key] for r in hard_data])[mask]\n",
    "        bare_q = bare_nlls[mask]\n",
    "        delta = bare_q - cond_nlls\n",
    "        wr = np.mean(delta > 0) * 100\n",
    "        if mask.sum() >= 5:\n",
    "            t, p = stats.ttest_rel(bare_q, cond_nlls)\n",
    "            print(f\"    {name:<20} delta={np.mean(delta):+.4f}  win={wr:.1f}%  t={t:.3f}  p={p:.6f}\")\n",
    "        else:\n",
    "            print(f\"    {name:<20} delta={np.mean(delta):+.4f}  win={wr:.1f}%  (too few for t-test)\")\n",
    "\n",
    "# Does semantic separation appear in non-extractive subset?\n",
    "if sfx_rel_key and sfx_irrel_key and nonextractive_mask.sum() >= 10:\n",
    "    ne_rel = np.array([r[sfx_rel_key] for r in hard_data])[nonextractive_mask]\n",
    "    ne_irrel = np.array([r[sfx_irrel_key] for r in hard_data])[nonextractive_mask]\n",
    "    t_ne, p_ne = stats.ttest_rel(ne_rel, ne_irrel)\n",
    "    d_ne = cohens_d(ne_irrel - ne_rel)\n",
    "    print(f\"\\n  Semantic separation on non-extractive samples:\")\n",
    "    print(f\"    rel vs irrel: delta={np.mean(ne_irrel - ne_rel):+.4f}  t={t_ne:.3f}  p={p_ne:.6f}  d={d_ne:.4f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigation C: Visualization"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('Investigation C: Hard-Sample and Extractiveness Analysis', fontsize=14, fontweight='bold')\n",
    "\n",
    "# (0,0) Win rate by difficulty\n",
    "ax = axes[0, 0]\n",
    "q_labels = list(quartile_masks.keys())\n",
    "if sfx_rel_key:\n",
    "    rel_wrs = []\n",
    "    irrel_wrs = []\n",
    "    for q_label, mask in quartile_masks.items():\n",
    "        if mask.sum() == 0:\n",
    "            rel_wrs.append(0)\n",
    "            irrel_wrs.append(0)\n",
    "            continue\n",
    "        rel_wrs.append(np.mean(bare_nlls[mask] > np.array([r[sfx_rel_key] for r in hard_data])[mask]) * 100)\n",
    "        irrel_wrs.append(np.mean(bare_nlls[mask] > np.array([r[sfx_irrel_key] for r in hard_data])[mask]) * 100)\n",
    "    x = np.arange(len(q_labels))\n",
    "    w = 0.35\n",
    "    ax.bar(x - w/2, rel_wrs, w, label='Relevant suffix', color='#4c72b0')\n",
    "    ax.bar(x + w/2, irrel_wrs, w, label='Irrelevant suffix', color='#c44e52')\n",
    "    ax.axhline(50, color='black', linestyle='--', linewidth=0.8, alpha=0.3)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels([l.replace(' ', '\\n') for l in q_labels], fontsize=8)\n",
    "    ax.set_ylabel('Win Rate vs Bare (%)')\n",
    "    ax.set_title('Win Rate by Difficulty Quartile')\n",
    "    ax.legend(fontsize=8)\n",
    "    ax.set_ylim(0, 100)\n",
    "\n",
    "# (0,1) Delta by difficulty\n",
    "ax = axes[0, 1]\n",
    "if sfx_rel_key:\n",
    "    rel_deltas_q = []\n",
    "    irrel_deltas_q = []\n",
    "    for q_label, mask in quartile_masks.items():\n",
    "        if mask.sum() == 0:\n",
    "            rel_deltas_q.append(0)\n",
    "            irrel_deltas_q.append(0)\n",
    "            continue\n",
    "        rel_deltas_q.append(np.mean(bare_nlls[mask] - np.array([r[sfx_rel_key] for r in hard_data])[mask]))\n",
    "        irrel_deltas_q.append(np.mean(bare_nlls[mask] - np.array([r[sfx_irrel_key] for r in hard_data])[mask]))\n",
    "    ax.bar(x - w/2, rel_deltas_q, w, label='Relevant', color='#4c72b0')\n",
    "    ax.bar(x + w/2, irrel_deltas_q, w, label='Irrelevant', color='#c44e52')\n",
    "    ax.axhline(0, color='black', linestyle='-', linewidth=0.5)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels([l.replace(' ', '\\n') for l in q_labels], fontsize=8)\n",
    "    ax.set_ylabel('Mean Delta NLL')\n",
    "    ax.set_title('Mean Improvement by Quartile')\n",
    "    ax.legend(fontsize=8)\n",
    "\n",
    "# (1,0) Overlap histogram\n",
    "ax = axes[1, 0]\n",
    "ax.hist(overlaps, bins=30, color='#4c72b0', alpha=0.7, edgecolor='black', linewidth=0.5)\n",
    "ax.axvline(overlap_median, color='red', linestyle='--', label=f'Median={overlap_median:.2f}')\n",
    "ax.set_xlabel('Answer-Passage Token Overlap')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('Answer Extractiveness Distribution')\n",
    "ax.legend()\n",
    "\n",
    "# (1,1) Extractive vs non-extractive semantic gap\n",
    "ax = axes[1, 1]\n",
    "if sfx_rel_key and sfx_irrel_key:\n",
    "    categories = ['Extractive', 'Non-extractive']\n",
    "    gaps = []\n",
    "    for mask in [extractive_mask, nonextractive_mask]:\n",
    "        if mask.sum() > 0:\n",
    "            rel = np.array([r[sfx_rel_key] for r in hard_data])[mask]\n",
    "            irrel = np.array([r[sfx_irrel_key] for r in hard_data])[mask]\n",
    "            gaps.append(np.mean(irrel - rel))\n",
    "        else:\n",
    "            gaps.append(0)\n",
    "    colors = ['#55a868' if g > 0 else '#c44e52' for g in gaps]\n",
    "    ax.bar(categories, gaps, color=colors)\n",
    "    ax.axhline(0, color='black', linestyle='-', linewidth=0.5)\n",
    "    ax.set_ylabel('Mean NLL Gap (irrel - rel)')\n",
    "    ax.set_title('Semantic Gap: Extractive vs Non-Extractive')\n",
    "    ax.annotate('Positive = relevant suffix\\nis better', xy=(0.02, 0.95),\n",
    "                xycoords='axes fraction', fontsize=8, va='top')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/exp08/08_hard_sample_analysis.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"Saved: 08_hard_sample_analysis.png\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"EXPERIMENT 08 SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n--- Investigation A: Query-Free Scoring ---\")\n",
    "rel_noq = np.array([r['sfx_rel_noq'] for r in results_a])\n",
    "irrel_noq = np.array([r['sfx_irrel_noq'] for r in results_a])\n",
    "perf_noq = np.array([r['sfx_perfect_noq'] for r in results_a])\n",
    "bare_noq_arr = np.array([r['bare_noq'] for r in results_a])\n",
    "\n",
    "t_ri, p_ri = stats.ttest_rel(rel_noq, irrel_noq)\n",
    "d_ri = cohens_d(irrel_noq - rel_noq)\n",
    "print(f\"  Relevant vs Irrelevant (no query): t={t_ri:.3f}, p={p_ri:.6f}, d={d_ri:.4f}\")\n",
    "\n",
    "t_pb, p_pb = stats.ttest_rel(perf_noq, bare_noq_arr)\n",
    "d_pb = cohens_d(bare_noq_arr - perf_noq)\n",
    "print(f\"  Perfect vs Bare (no query): t={t_pb:.3f}, p={p_pb:.6f}, d={d_pb:.4f}\")\n",
    "\n",
    "if p_ri < 0.01 and np.mean(rel_noq) < np.mean(irrel_noq):\n",
    "    print(\"  VERDICT: Semantic signal EXISTS when query is removed!\")\n",
    "    print(\"  => The query was masking the suffix effect in exp 07.\")\n",
    "elif p_pb < 0.01 and np.mean(perf_noq) < np.mean(bare_noq_arr):\n",
    "    print(\"  VERDICT: Perfect suffix helps but generated surrogates don't separate.\")\n",
    "    print(\"  => Surrogate quality is the bottleneck, not the approach.\")\n",
    "else:\n",
    "    print(\"  VERDICT: No semantic signal even without query.\")\n",
    "    print(\"  => The suffix mechanism itself is insufficient.\")\n",
    "\n",
    "print(\"\\n--- Investigation B: Attention Analysis ---\")\n",
    "mean_sfx_attn_rel = np.mean(rel_sfx_attn)\n",
    "mean_sfx_attn_irrel = np.mean(irrel_sfx_attn)\n",
    "sfx_fraction = np.mean(rel_total) * 100\n",
    "print(f\"  Mean attention to suffix: relevant={mean_sfx_attn_rel:.4f}, irrelevant={mean_sfx_attn_irrel:.4f}\")\n",
    "print(f\"  Suffix attention as % of total: {sfx_fraction:.2f}%\")\n",
    "if sfx_fraction < 1.0:\n",
    "    print(\"  VERDICT: Suffix receives negligible attention (<1%). Model ignores it.\")\n",
    "elif abs(mean_sfx_attn_rel - mean_sfx_attn_irrel) < 0.001:\n",
    "    print(\"  VERDICT: Suffix gets attention but same amount regardless of content.\")\n",
    "else:\n",
    "    print(\"  VERDICT: Differential attention to relevant vs irrelevant suffix detected.\")\n",
    "\n",
    "print(\"\\n--- Investigation C: Hard Samples ---\")\n",
    "if sfx_rel_key and sfx_irrel_key:\n",
    "    hard_rel_arr = np.array([r[sfx_rel_key] for r in hard_data])[hard_mask]\n",
    "    hard_irrel_arr = np.array([r[sfx_irrel_key] for r in hard_data])[hard_mask]\n",
    "    t_h, p_h = stats.ttest_rel(hard_rel_arr, hard_irrel_arr)\n",
    "    hard_wr = np.mean(hard_rel_arr < hard_irrel_arr) * 100\n",
    "    print(f\"  Hard samples — relevant beats irrelevant: {hard_wr:.1f}% (p={p_h:.6f})\")\n",
    "\n",
    "    if p_h < 0.05 and hard_wr > 55:\n",
    "        print(\"  VERDICT: Semantic signal appears on hard samples.\")\n",
    "        print(\"  => MS MARCO's easy samples are washing out the effect.\")\n",
    "    else:\n",
    "        print(\"  VERDICT: No semantic signal even on hard samples.\")\n",
    "\n",
    "print(\"\\n--- Overall Assessment ---\")\n",
    "print(\"If Investigation A shows a query-free effect but B shows low attention:\")\n",
    "print(\"  => Architectural issue — model can't leverage suffix via attention.\")\n",
    "print(\"If A shows effect AND B shows differential attention:\")\n",
    "print(\"  => Signal exists but too weak for the with-query setting.\")\n",
    "print(\"If nothing works:\")\n",
    "print(\"  => Move to long-document QA (Experiment 09) where priming should matter more.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "os.makedirs('results', exist_ok=True)\n",
    "timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "output = {\n",
    "    'metadata': {\n",
    "        'experiment': '08_diagnostic_suffix_signal',\n",
    "        'description': (\n",
    "            'Three investigations diagnosing why suffix priming shows no semantic signal: '\n",
    "            'A) query-free scoring (200 samples), B) attention pattern analysis (30 samples), '\n",
    "            'C) hard-sample and extractiveness stratification.'\n",
    "        ),\n",
    "        'timestamp': datetime.datetime.now().isoformat(),\n",
    "        'model_name': config.model_name,\n",
    "        'seed': config.seed,\n",
    "        'n_samples_a': len(results_a),\n",
    "        'n_samples_b': len(results_b),\n",
    "    },\n",
    "    'results_a': results_a,\n",
    "    'results_b': results_b,\n",
    "}\n",
    "\n",
    "output_path = f'results/08_diagnostic_results_{timestamp}.json'\n",
    "with open(output_path, 'w') as f:\n",
    "    json.dump(output, f, indent=2, default=str)\n",
    "print(f\"Saved: {output_path}\")\n",
    "\n",
    "output_path_canonical = 'results/exp08/08_diagnostic_results.json'\n",
    "with open(output_path_canonical, 'w') as f:\n",
    "    json.dump(output, f, indent=2, default=str)\n",
    "print(f\"Saved: {output_path_canonical}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Based on the results above:\n",
    "\n",
    "- **If semantic signal appears in query-free scoring (Investigation A)**: The approach works\n",
    "  but the with-query setting makes suffix redundant. Consider alternative scoring approaches\n",
    "  or tasks where the query isn't available at scoring time.\n",
    "\n",
    "- **If hard/non-extractive samples show separation (Investigation C)**: Move to long-document\n",
    "  QA where ALL samples are \"hard\" and extractive answers are rare. Create Experiment 09\n",
    "  with NarrativeQA or QuALITY.\n",
    "\n",
    "- **If nothing works**: The suffix KV mechanism may be fundamentally limited. Consider\n",
    "  alternative approaches: encoder-decoder models, cross-attention injection, or learned\n",
    "  prefix tuning."
   ]
  }
 ]
}