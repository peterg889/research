{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Experiment 21: Back to Basics\n",
    "\n",
    "**Date:** 2025-02-05\n",
    "\n",
    "## Purpose\n",
    "\n",
    "Previous experiments show KV cache priming often hurts or has no effect. This experiment\n",
    "isolates exactly WHERE the benefit disappears by testing the theory step-by-step.\n",
    "\n",
    "## The Theory\n",
    "\n",
    "1. **Ideal**: Build cache for `[query + document]` → document tokens attend to query during\n",
    "   encoding → representations are optimized for answering that query\n",
    "2. **Problem**: Can't precompute for all O(queries × documents) pairs\n",
    "3. **Hope**: Priming with a surrogate query approximates the ideal\n",
    "\n",
    "## Experimental Conditions\n",
    "\n",
    "| Condition | Cache Built From | Query at Scoring | What It Tests |\n",
    "|-----------|------------------|------------------|---------------|\n",
    "| **A: Bare** | `[doc]` | Provided fresh | Baseline |\n",
    "| **B: Full Oracle** | `[query + doc]` | Already in cache | Does query-in-context help? |\n",
    "| **C: Full + Repeat** | `[query + doc]` | Provided again | Upper bound |\n",
    "| **D: Truncated Oracle** | `[query + doc]` → keep `[doc]` | Provided fresh | Does truncation preserve benefit? |\n",
    "| **E: Truncated Random** | `[random + doc]` → keep `[doc]` | Provided fresh | Semantic signal test |\n",
    "\n",
    "## Expected Outcomes (if theory is correct)\n",
    "\n",
    "- B ≥ A (query context helps)\n",
    "- C ≥ B (redundancy OK)\n",
    "- **D ≥ A (truncation preserves benefit)** ← KEY TEST\n",
    "- D > E (semantic match matters)\n",
    "\n",
    "## Failure Diagnosis\n",
    "\n",
    "- If B ≈ A: Theory is wrong (query context doesn't help this model)\n",
    "- If D < A but B > A: Truncation destroys the benefit\n",
    "- If D ≈ E: No semantic signal (priming is just noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch: 2.10.0+cu128\n",
      "CUDA: True\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Setup\n",
    "import os\n",
    "os.umask(0o000)\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '/home/jupyter/research/directed_kvcache')\n",
    "\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from dataclasses import dataclass\n",
    "from scipy import stats\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, DynamicCache\n",
    "from datasets import load_dataset\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "OUTPUT_DIR = '/home/jupyter/research/directed_kvcache/results/exp21'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bb301a5d11e48b9ad6405ab4af42949",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/291 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on cuda:0\n",
      "Model dtype: torch.bfloat16\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Load Model\n",
    "MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "print(f\"Model loaded on {model.device}\")\n",
    "print(f\"Model dtype: {model.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utility functions defined.\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Core Utility Functions\n",
    "\n",
    "from lib.kv_cache import (\n",
    "    deepcopy_cache,\n",
    "    _get_cache_keys,\n",
    "    _get_cache_values,\n",
    "    _set_cache_keys,\n",
    "    _set_cache_values,\n",
    "    _ensure_dynamic_cache,\n",
    "    # RoPE correction\n",
    "    correct_rope_positions_with_bos,\n",
    "    extract_and_truncate_cache_with_bos,\n",
    ")\n",
    "\n",
    "\n",
    "def get_cache_len(cache: DynamicCache) -> int:\n",
    "    \"\"\"Get sequence length from cache.\"\"\"\n",
    "    cache = _ensure_dynamic_cache(cache)\n",
    "    return _get_cache_keys(cache, 0).shape[2]\n",
    "\n",
    "\n",
    "def score_answer_nll(cache: DynamicCache, prompt: str, answer: str) -> float:\n",
    "    \"\"\"\n",
    "    Score P(answer | cache, prompt) as negative log-likelihood.\n",
    "    \n",
    "    Lower NLL = higher probability = better.\n",
    "    \"\"\"\n",
    "    cache = _ensure_dynamic_cache(cache)\n",
    "    cache_len = get_cache_len(cache)\n",
    "    \n",
    "    # Tokenize prompt and answer\n",
    "    prompt_ids = tokenizer.encode(prompt, return_tensors='pt', add_special_tokens=False).to(model.device)\n",
    "    answer_ids = tokenizer.encode(answer, return_tensors='pt', add_special_tokens=False).to(model.device)\n",
    "    \n",
    "    # Combine prompt + answer\n",
    "    input_ids = torch.cat([prompt_ids, answer_ids], dim=1)\n",
    "    \n",
    "    # Attention mask for full sequence\n",
    "    total_len = cache_len + input_ids.shape[1]\n",
    "    attention_mask = torch.ones((1, total_len), device=model.device)\n",
    "    \n",
    "    # Forward pass\n",
    "    cache_copy = deepcopy_cache(cache)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            past_key_values=cache_copy,\n",
    "            use_cache=True,\n",
    "            return_dict=True\n",
    "        )\n",
    "    \n",
    "    # Score only the answer tokens\n",
    "    logits = outputs.logits  # [1, prompt_len + answer_len, vocab]\n",
    "    prompt_len = prompt_ids.shape[1]\n",
    "    answer_len = answer_ids.shape[1]\n",
    "    \n",
    "    if answer_len == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    # logits[prompt_len-1] predicts answer[0], logits[prompt_len] predicts answer[1], etc.\n",
    "    # So logits[prompt_len-1 : prompt_len+answer_len-1] predicts answer[0:answer_len]\n",
    "    answer_logits = logits[:, prompt_len-1:prompt_len+answer_len-1, :]\n",
    "    \n",
    "    # Compute cross-entropy\n",
    "    loss_fct = torch.nn.CrossEntropyLoss(reduction='mean')\n",
    "    loss = loss_fct(\n",
    "        answer_logits.view(-1, answer_logits.size(-1)),\n",
    "        answer_ids.view(-1)\n",
    "    )\n",
    "    \n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "print(\"Utility functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache building functions defined.\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Cache Building Functions for Each Condition\n",
    "\n",
    "def build_cache_A_bare(doc: str) -> Tuple[DynamicCache, str]:\n",
    "    \"\"\"\n",
    "    Condition A: Bare document cache.\n",
    "    \n",
    "    Cache: [BOS, doc_tokens]\n",
    "    Query: Provided fresh at scoring time\n",
    "    \"\"\"\n",
    "    ids = tokenizer.encode(doc, return_tensors='pt', add_special_tokens=True).to(model.device)\n",
    "    with torch.no_grad():\n",
    "        out = model(ids, use_cache=True)\n",
    "    return _ensure_dynamic_cache(out.past_key_values), \"bare\"\n",
    "\n",
    "\n",
    "def build_cache_B_full_oracle(query: str, doc: str) -> Tuple[DynamicCache, str]:\n",
    "    \"\"\"\n",
    "    Condition B: Full context with query (not truncated).\n",
    "    \n",
    "    Cache: [BOS, query_tokens, doc_tokens]\n",
    "    Query: Already in cache, not repeated at scoring\n",
    "    \n",
    "    This tests: Does having query in context during doc encoding help?\n",
    "    \"\"\"\n",
    "    full_text = query + \" \" + doc\n",
    "    ids = tokenizer.encode(full_text, return_tensors='pt', add_special_tokens=True).to(model.device)\n",
    "    with torch.no_grad():\n",
    "        out = model(ids, use_cache=True)\n",
    "    return _ensure_dynamic_cache(out.past_key_values), \"full_oracle\"\n",
    "\n",
    "\n",
    "def build_cache_C_full_repeat(query: str, doc: str) -> Tuple[DynamicCache, str]:\n",
    "    \"\"\"\n",
    "    Condition C: Full context, query will be repeated at scoring.\n",
    "    \n",
    "    Cache: [BOS, query_tokens, doc_tokens]\n",
    "    Query: Will be provided again at scoring (redundant but potentially helpful)\n",
    "    \n",
    "    This is the same cache as B, but scoring differs.\n",
    "    \"\"\"\n",
    "    full_text = query + \" \" + doc\n",
    "    ids = tokenizer.encode(full_text, return_tensors='pt', add_special_tokens=True).to(model.device)\n",
    "    with torch.no_grad():\n",
    "        out = model(ids, use_cache=True)\n",
    "    return _ensure_dynamic_cache(out.past_key_values), \"full_repeat\"\n",
    "\n",
    "\n",
    "def build_cache_D_truncated_oracle(query: str, doc: str) -> Tuple[DynamicCache, str]:\n",
    "    \"\"\"\n",
    "    Condition D: Build with query, then truncate to doc only.\n",
    "    \n",
    "    Build: [BOS, query_tokens, doc_tokens]\n",
    "    Truncate to: [BOS, doc_tokens] (with RoPE correction)\n",
    "    Query: Provided fresh at scoring\n",
    "    \n",
    "    This tests: Does truncation preserve the benefit of query-priming?\n",
    "    THIS IS THE KEY TEST.\n",
    "    \"\"\"\n",
    "    # Tokenize to find boundaries\n",
    "    query_with_space = query + \" \"\n",
    "    query_ids = tokenizer.encode(query_with_space, return_tensors='pt', add_special_tokens=True)\n",
    "    query_len = query_ids.shape[1]  # includes BOS\n",
    "    \n",
    "    full_text = query_with_space + doc\n",
    "    full_ids = tokenizer.encode(full_text, return_tensors='pt', add_special_tokens=True).to(model.device)\n",
    "    full_len = full_ids.shape[1]\n",
    "    doc_len = full_len - query_len  # tokens after query (not including BOS)\n",
    "    \n",
    "    # Build full cache\n",
    "    with torch.no_grad():\n",
    "        out = model(full_ids, use_cache=True)\n",
    "    full_cache = _ensure_dynamic_cache(out.past_key_values)\n",
    "    \n",
    "    # Use lib functions for truncation and RoPE correction\n",
    "    # extract_and_truncate_cache_with_bos keeps BOS + last doc_len tokens\n",
    "    truncated_cache = extract_and_truncate_cache_with_bos(full_cache, doc_len)\n",
    "    \n",
    "    # RoPE correction: doc tokens were at positions [query_len, ...], now at [1, ...]\n",
    "    # Offset = query_len - 1 (subtract this from original positions)\n",
    "    surrogate_offset = query_len - 1\n",
    "    correct_rope_positions_with_bos(truncated_cache, surrogate_offset, model)\n",
    "    \n",
    "    return truncated_cache, \"truncated_oracle\"\n",
    "\n",
    "\n",
    "def build_cache_E_truncated_random(random_query: str, doc: str) -> Tuple[DynamicCache, str]:\n",
    "    \"\"\"\n",
    "    Condition E: Build with RANDOM query, then truncate.\n",
    "    \n",
    "    Same as D but with a mismatched query.\n",
    "    \n",
    "    This tests: Is there semantic signal, or is any priming equivalent?\n",
    "    \"\"\"\n",
    "    # Reuse the truncated oracle logic with random query\n",
    "    cache, _ = build_cache_D_truncated_oracle(random_query, doc)\n",
    "    return cache, \"truncated_random\"\n",
    "\n",
    "\n",
    "print(\"Cache building functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1: Sanity Check with Synthetic Example\n",
    "\n",
    "Before running on real data, let's verify the setup with a simple example where we KNOW what should happen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "SANITY CHECK: Synthetic Example\n",
      "======================================================================\n",
      "\n",
      "Document: The capital of France is Paris. It is known for the Eiffel Tower.\n",
      "Query: What is the capital of France?\n",
      "Expected answer:  Paris\n",
      "Wrong query (for E): What is the population of Japan?\n",
      "\n",
      "--------------------------------------------------\n",
      "Scoring P(answer | cache, prompt)\n",
      "--------------------------------------------------\n",
      "A (bare, query at scoring):        NLL = 0.0486\n",
      "B (full oracle, no query repeat):  NLL = 1.2891\n",
      "C (full oracle, query repeated):   NLL = 0.2012\n",
      "D (truncated oracle):              NLL = 0.1797\n",
      "E (truncated random):              NLL = 0.0698\n",
      "\n",
      "--------------------------------------------------\n",
      "Analysis\n",
      "--------------------------------------------------\n",
      "B vs A (does query context help?):     -1.2405 (positive = B better)\n",
      "C vs A (upper bound with redundancy):  -0.1526 (positive = C better)\n",
      "D vs A (does truncation preserve?):    -0.1311 (positive = D better) <- KEY\n",
      "D vs E (semantic signal?):             -0.1099 (positive = D better)\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Synthetic Sanity Check\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"SANITY CHECK: Synthetic Example\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Simple factoid that the model should know\n",
    "doc = \"The capital of France is Paris. It is known for the Eiffel Tower.\"\n",
    "query = \"What is the capital of France?\"\n",
    "answer = \" Paris\"\n",
    "wrong_query = \"What is the population of Japan?\"\n",
    "\n",
    "print(f\"\\nDocument: {doc}\")\n",
    "print(f\"Query: {query}\")\n",
    "print(f\"Expected answer: {answer}\")\n",
    "print(f\"Wrong query (for E): {wrong_query}\")\n",
    "\n",
    "# Build all caches\n",
    "cache_A, _ = build_cache_A_bare(doc)\n",
    "cache_B, _ = build_cache_B_full_oracle(query, doc)\n",
    "cache_C, _ = build_cache_C_full_repeat(query, doc)  # Same as B\n",
    "cache_D, _ = build_cache_D_truncated_oracle(query, doc)\n",
    "cache_E, _ = build_cache_E_truncated_random(wrong_query, doc)\n",
    "\n",
    "# Scoring prompts differ by condition\n",
    "prompt_with_query = f\"\\n\\nQuestion: {query}\\nAnswer:\"\n",
    "prompt_no_query = \"\\n\\nAnswer:\"  # Query already in cache for B\n",
    "\n",
    "# Score each condition\n",
    "print(\"\\n\" + \"-\"*50)\n",
    "print(\"Scoring P(answer | cache, prompt)\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "nll_A = score_answer_nll(cache_A, prompt_with_query, answer)\n",
    "print(f\"A (bare, query at scoring):        NLL = {nll_A:.4f}\")\n",
    "\n",
    "nll_B = score_answer_nll(cache_B, prompt_no_query, answer)\n",
    "print(f\"B (full oracle, no query repeat):  NLL = {nll_B:.4f}\")\n",
    "\n",
    "nll_C = score_answer_nll(cache_C, prompt_with_query, answer)\n",
    "print(f\"C (full oracle, query repeated):   NLL = {nll_C:.4f}\")\n",
    "\n",
    "nll_D = score_answer_nll(cache_D, prompt_with_query, answer)\n",
    "print(f\"D (truncated oracle):              NLL = {nll_D:.4f}\")\n",
    "\n",
    "nll_E = score_answer_nll(cache_E, prompt_with_query, answer)\n",
    "print(f\"E (truncated random):              NLL = {nll_E:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*50)\n",
    "print(\"Analysis\")\n",
    "print(\"-\"*50)\n",
    "print(f\"B vs A (does query context help?):     {nll_A - nll_B:+.4f} (positive = B better)\")\n",
    "print(f\"C vs A (upper bound with redundancy):  {nll_A - nll_C:+.4f} (positive = C better)\")\n",
    "print(f\"D vs A (does truncation preserve?):    {nll_A - nll_D:+.4f} (positive = D better) <- KEY\")\n",
    "print(f\"D vs E (semantic signal?):             {nll_E - nll_D:+.4f} (positive = D better)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: MS MARCO Evaluation\n",
    "\n",
    "Now test on real data to see if the patterns hold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cell-8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MS MARCO...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built 300 evaluation samples\n",
      "Query pool size: 418\n",
      "\n",
      "Example:\n",
      "  Query: what is rba\n",
      "  Passage: Since 2007, the RBA's outstanding reputation has been affected by the 'Securency' or NPA scandal. Th...\n",
      "  Answer: Results-Based Accountability is a disciplined way of thinking and taking action that communities can use to improve the lives of children, youth, families, adults and the community as a whole.\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Load MS MARCO Data\n",
    "\n",
    "print(\"Loading MS MARCO...\")\n",
    "msmarco = load_dataset(\"ms_marco\", \"v1.1\", split=\"train\")\n",
    "\n",
    "# Build evaluation samples\n",
    "samples = []\n",
    "all_queries = []  # For random query sampling\n",
    "\n",
    "for item in msmarco:\n",
    "    if len(samples) >= 300:  # Buffer for 200\n",
    "        break\n",
    "    \n",
    "    query = item.get('query', '')\n",
    "    passages = item.get('passages', {}).get('passage_text', [])\n",
    "    answers = item.get('answers', [])\n",
    "    \n",
    "    if query:\n",
    "        all_queries.append(query)\n",
    "    \n",
    "    if not query or not passages or not passages[0] or not answers or not answers[0]:\n",
    "        continue\n",
    "    \n",
    "    passage = passages[0]\n",
    "    answer = answers[0]\n",
    "    \n",
    "    # Filter reasonable lengths\n",
    "    if len(passage.split()) < 20 or len(passage.split()) > 150:\n",
    "        continue\n",
    "    if len(answer.split()) < 2 or len(answer.split()) > 30:\n",
    "        continue\n",
    "    \n",
    "    samples.append({\n",
    "        'query': query,\n",
    "        'passage': passage,\n",
    "        'answer': answer,\n",
    "    })\n",
    "\n",
    "print(f\"Built {len(samples)} evaluation samples\")\n",
    "print(f\"Query pool size: {len(all_queries)}\")\n",
    "\n",
    "print(f\"\\nExample:\")\n",
    "print(f\"  Query: {samples[0]['query']}\")\n",
    "print(f\"  Passage: {samples[0]['passage'][:100]}...\")\n",
    "print(f\"  Answer: {samples[0]['answer']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cell-9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "MS MARCO EVALUATION\n",
      "======================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe5cf9c3e36847c2acacc07186a7e0ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation complete.\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Run Full Evaluation\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"MS MARCO EVALUATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "N_SAMPLES = 200\n",
    "\n",
    "results = {\n",
    "    'A_bare': [],\n",
    "    'B_full_oracle': [],\n",
    "    'C_full_repeat': [],\n",
    "    'D_truncated_oracle': [],\n",
    "    'E_truncated_random': [],\n",
    "}\n",
    "\n",
    "for sample in tqdm(samples[:N_SAMPLES], desc=\"Evaluating\"):\n",
    "    query = sample['query']\n",
    "    passage = sample['passage']\n",
    "    answer = \" \" + sample['answer']  # Leading space for tokenization\n",
    "    \n",
    "    # Random query for condition E (different from actual query)\n",
    "    random_query = random.choice([q for q in all_queries if q != query])\n",
    "    \n",
    "    # Prompts\n",
    "    prompt_with_query = f\"\\n\\nQuestion: {query}\\nAnswer:\"\n",
    "    prompt_no_query = \"\\n\\nAnswer:\"\n",
    "    \n",
    "    # Condition A: Bare\n",
    "    cache_A, _ = build_cache_A_bare(passage)\n",
    "    nll_A = score_answer_nll(cache_A, prompt_with_query, answer)\n",
    "    results['A_bare'].append(nll_A)\n",
    "    \n",
    "    # Condition B: Full Oracle (query in cache, not repeated)\n",
    "    cache_B, _ = build_cache_B_full_oracle(query, passage)\n",
    "    nll_B = score_answer_nll(cache_B, prompt_no_query, answer)\n",
    "    results['B_full_oracle'].append(nll_B)\n",
    "    \n",
    "    # Condition C: Full Oracle (query in cache, repeated at scoring)\n",
    "    cache_C, _ = build_cache_C_full_repeat(query, passage)\n",
    "    nll_C = score_answer_nll(cache_C, prompt_with_query, answer)\n",
    "    results['C_full_repeat'].append(nll_C)\n",
    "    \n",
    "    # Condition D: Truncated Oracle\n",
    "    cache_D, _ = build_cache_D_truncated_oracle(query, passage)\n",
    "    nll_D = score_answer_nll(cache_D, prompt_with_query, answer)\n",
    "    results['D_truncated_oracle'].append(nll_D)\n",
    "    \n",
    "    # Condition E: Truncated Random\n",
    "    cache_E, _ = build_cache_E_truncated_random(random_query, passage)\n",
    "    nll_E = score_answer_nll(cache_E, prompt_with_query, answer)\n",
    "    results['E_truncated_random'].append(nll_E)\n",
    "\n",
    "print(\"\\nEvaluation complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cell-10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "RESULTS SUMMARY\n",
      "======================================================================\n",
      "\n",
      "Mean NLL by Condition (lower = better):\n",
      "--------------------------------------------------\n",
      "  A_bare                   : 3.1649 (+/- 1.8655)\n",
      "  B_full_oracle            : 3.1718 (+/- 1.7439)\n",
      "  C_full_repeat            : 3.3721 (+/- 1.9134)\n",
      "  D_truncated_oracle       : 3.1574 (+/- 1.8237)\n",
      "  E_truncated_random       : 3.1839 (+/- 1.8390)\n",
      "\n",
      "======================================================================\n",
      "KEY COMPARISONS\n",
      "======================================================================\n",
      "\n",
      "### Test 1: Does query-in-context help at all? (B vs A) ###\n",
      "\n",
      "B (full oracle) vs A (bare)\n",
      "  Delta: -0.0069 (positive = test better)\n",
      "  Win rate: 49.0%\n",
      "  Cohen's d: -0.009\n",
      "  p-value: 0.9007\n",
      "  Status: no effect\n",
      "\n",
      "### Test 2: Upper bound with query redundancy? (C vs A) ###\n",
      "\n",
      "C (full + repeat) vs A (bare)\n",
      "  Delta: -0.2072 (positive = test better)\n",
      "  Win rate: 31.0%\n",
      "  Cohen's d: -0.370\n",
      "  p-value: 0.0000\n",
      "  Status: REVERSED!\n",
      "\n",
      "### Test 3: KEY - Does truncation preserve benefit? (D vs A) ###\n",
      "\n",
      "D (truncated oracle) vs A (bare)\n",
      "  Delta: +0.0075 (positive = test better)\n",
      "  Win rate: 48.5%\n",
      "  Cohen's d: +0.017\n",
      "  p-value: 0.8153\n",
      "  Status: no effect\n",
      "\n",
      "### Test 4: Is there semantic signal? (D vs E) ###\n",
      "\n",
      "D (truncated oracle) vs E (truncated random)\n",
      "  Delta: +0.0265 (positive = test better)\n",
      "  Win rate: 51.0%\n",
      "  Cohen's d: +0.072\n",
      "  p-value: 0.3082\n",
      "  Status: no effect\n",
      "\n",
      "### Test 5: Does truncation hurt vs full context? (D vs B) ###\n",
      "\n",
      "D (truncated) vs B (full)\n",
      "  Delta: +0.0144 (positive = test better)\n",
      "  Win rate: 49.5%\n",
      "  Cohen's d: +0.022\n",
      "  p-value: 0.7578\n",
      "  Status: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.0143817138671875, 0.495, 0.021894136136281748, 0.7577550859661021)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 10: Results Summary\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RESULTS SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nMean NLL by Condition (lower = better):\")\n",
    "print(\"-\"*50)\n",
    "for cond, nlls in results.items():\n",
    "    print(f\"  {cond:25s}: {np.mean(nlls):.4f} (+/- {np.std(nlls):.4f})\")\n",
    "\n",
    "# Key comparisons\n",
    "A = np.array(results['A_bare'])\n",
    "B = np.array(results['B_full_oracle'])\n",
    "C = np.array(results['C_full_repeat'])\n",
    "D = np.array(results['D_truncated_oracle'])\n",
    "E = np.array(results['E_truncated_random'])\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"KEY COMPARISONS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def compare(name, baseline, test, expect_positive=True):\n",
    "    delta = baseline - test  # positive means test is better (lower NLL)\n",
    "    win_rate = np.mean(delta > 0)\n",
    "    t_stat, p_val = stats.ttest_rel(baseline, test)\n",
    "    d = np.mean(delta) / np.std(delta) if np.std(delta) > 0 else 0\n",
    "    \n",
    "    status = \"\"\n",
    "    if expect_positive:\n",
    "        if delta.mean() > 0 and p_val < 0.05:\n",
    "            status = \"CONFIRMED\"\n",
    "        elif delta.mean() < 0 and p_val < 0.05:\n",
    "            status = \"REVERSED!\"\n",
    "        else:\n",
    "            status = \"no effect\"\n",
    "    \n",
    "    print(f\"\\n{name}\")\n",
    "    print(f\"  Delta: {np.mean(delta):+.4f} (positive = test better)\")\n",
    "    print(f\"  Win rate: {win_rate*100:.1f}%\")\n",
    "    print(f\"  Cohen's d: {d:+.3f}\")\n",
    "    print(f\"  p-value: {p_val:.4f}\")\n",
    "    print(f\"  Status: {status}\")\n",
    "    return delta.mean(), win_rate, d, p_val\n",
    "\n",
    "print(\"\\n### Test 1: Does query-in-context help at all? (B vs A) ###\")\n",
    "compare(\"B (full oracle) vs A (bare)\", A, B)\n",
    "\n",
    "print(\"\\n### Test 2: Upper bound with query redundancy? (C vs A) ###\")\n",
    "compare(\"C (full + repeat) vs A (bare)\", A, C)\n",
    "\n",
    "print(\"\\n### Test 3: KEY - Does truncation preserve benefit? (D vs A) ###\")\n",
    "d_vs_a = compare(\"D (truncated oracle) vs A (bare)\", A, D)\n",
    "\n",
    "print(\"\\n### Test 4: Is there semantic signal? (D vs E) ###\")\n",
    "d_vs_e = compare(\"D (truncated oracle) vs E (truncated random)\", E, D)\n",
    "\n",
    "print(\"\\n### Test 5: Does truncation hurt vs full context? (D vs B) ###\")\n",
    "compare(\"D (truncated) vs B (full)\", B, D, expect_positive=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cell-11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "DIAGNOSIS\n",
      "======================================================================\n",
      "\n",
      "Checking failure points...\n",
      "\n",
      "[UNCLEAR] Query-in-context has NO EFFECT (B ≈ A)\n",
      "     Difference: -0.0069 NLL\n",
      "     -> Document may already contain enough signal\n",
      "\n",
      "[UNCLEAR] Truncation has NO EFFECT (D ≈ A)\n",
      "\n",
      "[OK] Semantic signal EXISTS (D < E)\n",
      "     Oracle priming beats random by 0.0265 NLL\n",
      "\n",
      "======================================================================\n",
      "CONCLUSION\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Cell 11: Diagnosis\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DIAGNOSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "A_mean = np.mean(A)\n",
    "B_mean = np.mean(B)\n",
    "D_mean = np.mean(D)\n",
    "E_mean = np.mean(E)\n",
    "\n",
    "print(\"\\nChecking failure points...\\n\")\n",
    "\n",
    "# Check 1: Does query context help at all?\n",
    "if B_mean < A_mean - 0.05:\n",
    "    print(\"[OK] Query-in-context HELPS (B < A)\")\n",
    "    print(f\"     Full oracle improves by {A_mean - B_mean:.4f} NLL\")\n",
    "    theory_works = True\n",
    "elif B_mean > A_mean + 0.05:\n",
    "    print(\"[PROBLEM] Query-in-context HURTS (B > A)\")\n",
    "    print(f\"     Full oracle is WORSE by {B_mean - A_mean:.4f} NLL\")\n",
    "    print(\"     -> The basic theory may be wrong for this model/task\")\n",
    "    theory_works = False\n",
    "else:\n",
    "    print(\"[UNCLEAR] Query-in-context has NO EFFECT (B ≈ A)\")\n",
    "    print(f\"     Difference: {A_mean - B_mean:.4f} NLL\")\n",
    "    print(\"     -> Document may already contain enough signal\")\n",
    "    theory_works = None\n",
    "\n",
    "# Check 2: Does truncation preserve benefit?\n",
    "print()\n",
    "if D_mean < A_mean - 0.02:\n",
    "    print(\"[OK] Truncation PRESERVES benefit (D < A)\")\n",
    "    print(f\"     Truncated oracle improves by {A_mean - D_mean:.4f} NLL\")\n",
    "    truncation_ok = True\n",
    "elif D_mean > A_mean + 0.02:\n",
    "    print(\"[PROBLEM] Truncation HURTS (D > A)\")\n",
    "    print(f\"     Truncated oracle is WORSE by {D_mean - A_mean:.4f} NLL\")\n",
    "    truncation_ok = False\n",
    "    if theory_works:\n",
    "        print(\"     -> Truncation/RoPE correction is destroying the benefit!\")\n",
    "else:\n",
    "    print(\"[UNCLEAR] Truncation has NO EFFECT (D ≈ A)\")\n",
    "    truncation_ok = None\n",
    "\n",
    "# Check 3: Semantic signal?\n",
    "print()\n",
    "if D_mean < E_mean - 0.02:\n",
    "    print(\"[OK] Semantic signal EXISTS (D < E)\")\n",
    "    print(f\"     Oracle priming beats random by {E_mean - D_mean:.4f} NLL\")\n",
    "elif D_mean > E_mean + 0.02:\n",
    "    print(\"[PROBLEM] INVERTED signal (D > E)\")\n",
    "    print(f\"     Random priming beats oracle by {D_mean - E_mean:.4f} NLL\")\n",
    "    print(\"     -> This suggests interference, not semantic benefit\")\n",
    "else:\n",
    "    print(\"[UNCLEAR] No semantic signal (D ≈ E)\")\n",
    "    print(f\"     Difference: {E_mean - D_mean:.4f} NLL\")\n",
    "    print(\"     -> Priming effect is non-semantic (just noise)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CONCLUSION\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cell-12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to /home/jupyter/research/directed_kvcache/results/exp21/results.json\n"
     ]
    }
   ],
   "source": [
    "# Cell 12: Save Results\n",
    "\n",
    "output = {\n",
    "    'n_samples': N_SAMPLES,\n",
    "    'conditions': {\n",
    "        'A_bare': {'mean': float(np.mean(A)), 'std': float(np.std(A))},\n",
    "        'B_full_oracle': {'mean': float(np.mean(B)), 'std': float(np.std(B))},\n",
    "        'C_full_repeat': {'mean': float(np.mean(C)), 'std': float(np.std(C))},\n",
    "        'D_truncated_oracle': {'mean': float(np.mean(D)), 'std': float(np.std(D))},\n",
    "        'E_truncated_random': {'mean': float(np.mean(E)), 'std': float(np.std(E))},\n",
    "    },\n",
    "    'comparisons': {\n",
    "        'B_vs_A': {'delta': float(np.mean(A - B)), 'win_rate': float(np.mean(A > B))},\n",
    "        'D_vs_A': {'delta': float(np.mean(A - D)), 'win_rate': float(np.mean(A > D))},\n",
    "        'D_vs_E': {'delta': float(np.mean(E - D)), 'win_rate': float(np.mean(E > D))},\n",
    "    },\n",
    "    'raw_results': {k: [float(x) for x in v] for k, v in results.items()},\n",
    "}\n",
    "\n",
    "with open(f'{OUTPUT_DIR}/results.json', 'w') as f:\n",
    "    json.dump(output, f, indent=2)\n",
    "\n",
    "print(f\"Results saved to {OUTPUT_DIR}/results.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3: Deep Dive (if needed)\n",
    "\n",
    "If the above reveals where the breakdown occurs, we can add more targeted tests here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cell-14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache shape verification:\n",
      "--------------------------------------------------\n",
      "Query: 'what is rba...'\n",
      "Passage: 'Since 2007, the RBA's outstanding reputation has b...'\n",
      "\n",
      "Query tokens (with BOS): 6\n",
      "Passage tokens (with BOS): 121\n",
      "Full tokens (with BOS): 125\n",
      "\n",
      "Cache A (bare) length: 121\n",
      "Cache D (truncated) length: 120\n",
      "\n",
      "[WARNING] Cache lengths differ by 1\n",
      "  This could indicate a tokenization boundary issue\n"
     ]
    }
   ],
   "source": [
    "# Cell 14: Cache Shape Verification\n",
    "\n",
    "print(\"Cache shape verification:\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "sample = samples[0]\n",
    "query = sample['query']\n",
    "passage = sample['passage']\n",
    "\n",
    "cache_A, _ = build_cache_A_bare(passage)\n",
    "cache_D, _ = build_cache_D_truncated_oracle(query, passage)\n",
    "\n",
    "print(f\"Query: '{query[:50]}...'\")\n",
    "print(f\"Passage: '{passage[:50]}...'\")\n",
    "print()\n",
    "\n",
    "# Tokenize to check lengths\n",
    "query_ids = tokenizer.encode(query + \" \", add_special_tokens=True)\n",
    "passage_ids = tokenizer.encode(passage, add_special_tokens=True)\n",
    "full_ids = tokenizer.encode(query + \" \" + passage, add_special_tokens=True)\n",
    "\n",
    "print(f\"Query tokens (with BOS): {len(query_ids)}\")\n",
    "print(f\"Passage tokens (with BOS): {len(passage_ids)}\")\n",
    "print(f\"Full tokens (with BOS): {len(full_ids)}\")\n",
    "print()\n",
    "\n",
    "cache_A_len = get_cache_len(cache_A)\n",
    "cache_D_len = get_cache_len(cache_D)\n",
    "\n",
    "print(f\"Cache A (bare) length: {cache_A_len}\")\n",
    "print(f\"Cache D (truncated) length: {cache_D_len}\")\n",
    "print()\n",
    "\n",
    "# They should match if truncation is correct\n",
    "if cache_A_len == cache_D_len:\n",
    "    print(\"[OK] Cache lengths match\")\n",
    "else:\n",
    "    print(f\"[WARNING] Cache lengths differ by {abs(cache_A_len - cache_D_len)}\")\n",
    "    print(\"  This could indicate a tokenization boundary issue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cell-15",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value vector analysis:\n",
      "--------------------------------------------------\n",
      "Layer 0:\n",
      "  Cosine similarity: 0.073697\n",
      "  L2 distance: 11.2471\n",
      "  -> DIFFERENT\n",
      "Layer 16:\n",
      "  Cosine similarity: 0.484364\n",
      "  L2 distance: 148.6682\n",
      "  -> DIFFERENT\n",
      "Layer 31:\n",
      "  Cosine similarity: 0.278531\n",
      "  L2 distance: 396.0867\n",
      "  -> DIFFERENT\n"
     ]
    }
   ],
   "source": [
    "# Cell 15: Value Vector Analysis\n",
    "\n",
    "print(\"Value vector analysis:\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "# Compare value vectors between bare and truncated caches\n",
    "# If priming works, the value vectors should be DIFFERENT\n",
    "# (that's the whole point - document representations change)\n",
    "\n",
    "sample = samples[0]\n",
    "cache_A, _ = build_cache_A_bare(sample['passage'])\n",
    "cache_D, _ = build_cache_D_truncated_oracle(sample['query'], sample['passage'])\n",
    "\n",
    "# Get value vectors from first and last layers\n",
    "n_layers = len(cache_A)\n",
    "layers_to_check = [0, n_layers // 2, n_layers - 1]\n",
    "\n",
    "for layer in layers_to_check:\n",
    "    v_A = _get_cache_values(cache_A, layer)  # [batch, heads, seq, head_dim]\n",
    "    v_D = _get_cache_values(cache_D, layer)\n",
    "    \n",
    "    # Compare overlapping positions (skip BOS, compare doc tokens)\n",
    "    min_len = min(v_A.shape[2], v_D.shape[2])\n",
    "    v_A_doc = v_A[:, :, 1:min_len, :]  # Skip BOS\n",
    "    v_D_doc = v_D[:, :, 1:min_len, :]\n",
    "    \n",
    "    # Cosine similarity\n",
    "    v_A_flat = v_A_doc.reshape(-1).float()\n",
    "    v_D_flat = v_D_doc.reshape(-1).float()\n",
    "    cos_sim = torch.nn.functional.cosine_similarity(v_A_flat.unsqueeze(0), v_D_flat.unsqueeze(0)).item()\n",
    "    \n",
    "    # L2 distance\n",
    "    l2_dist = torch.norm(v_A_flat - v_D_flat).item()\n",
    "    \n",
    "    print(f\"Layer {layer}:\")\n",
    "    print(f\"  Cosine similarity: {cos_sim:.6f}\")\n",
    "    print(f\"  L2 distance: {l2_dist:.4f}\")\n",
    "    print(f\"  -> {'DIFFERENT' if cos_sim < 0.999 else 'NEARLY IDENTICAL'}\")"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cu124.m127",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/base-cu124:m127"
  },
  "kernelspec": {
   "display_name": "Python 3 (Local)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
