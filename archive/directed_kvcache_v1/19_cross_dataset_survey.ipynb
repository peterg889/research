{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 19: Cross-Dataset Survey\n",
    "\n",
    "**Date:** 2026-02-04\n",
    "\n",
    "## Purpose\n",
    "\n",
    "Exp 18 showed that priming hurts on MS MARCO. But MS MARCO may be the wrong task:\n",
    "- Short passages (~74 words)\n",
    "- Extractive answers\n",
    "- Model already performs well (low bare NLL)\n",
    "\n",
    "This experiment surveys multiple datasets to find where priming helps:\n",
    "\n",
    "| Dataset | Type | Why it might help |\n",
    "|---------|------|-------------------|\n",
    "| MS MARCO (hard) | Factoid QA | Filter to hard samples only |\n",
    "| NarrativeQA | Long-doc QA | Long documents need focus |\n",
    "| HotpotQA | Multi-hop | Reasoning chains need priming |\n",
    "| PubMedQA | Scientific | Domain expertise needed |\n",
    "| CNN/DailyMail | Summarization | Generative, needs focus |\n",
    "| Natural Questions | Wikipedia QA | Mixed difficulty |\n",
    "\n",
    "## Design\n",
    "\n",
    "For each dataset:\n",
    "- Sample 100 examples\n",
    "- Test 3 conditions: bare, oracle_truncated, oracle_fullctx\n",
    "- Report win rates and effect sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch: 2.10.0+cu128\n",
      "CUDA: True\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Setup\n",
    "import os\n",
    "os.umask(0o000)\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '/home/jupyter/research/directed_kvcache')\n",
    "\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from typing import Dict, List, Tuple, Any, Optional\n",
    "from dataclasses import dataclass\n",
    "from scipy import stats\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, DynamicCache\n",
    "from datasets import load_dataset\n",
    "\n",
    "from lib.kv_cache import (\n",
    "    extract_and_truncate_cache_with_bos,\n",
    "    correct_rope_positions_with_bos,\n",
    "    score_answer_with_cache,\n",
    "    deepcopy_cache,\n",
    ")\n",
    "from lib.config import ExperimentConfig\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "OUTPUT_DIR = '/home/jupyter/research/directed_kvcache/results/exp19'\n",
    "N_SAMPLES_PER_DATASET = 100  # Keep manageable for survey\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cd5b3af8f5741e7af99a15a087cbf9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/291 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on cuda:0, dtype=torch.bfloat16\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Load Model\n",
    "MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "config = ExperimentConfig(\n",
    "    model_name=MODEL_NAME,\n",
    "    device=model.device,\n",
    "    seed=SEED\n",
    ")\n",
    "\n",
    "print(f\"Model loaded on {model.device}, dtype={model.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation functions defined.\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Core Evaluation Functions\n",
    "\n",
    "@dataclass\n",
    "class Sample:\n",
    "    \"\"\"Unified sample format across datasets.\"\"\"\n",
    "    passage: str\n",
    "    query: str\n",
    "    answer: str\n",
    "    dataset: str\n",
    "    metadata: dict = None\n",
    "\n",
    "def build_bare_cache(passage: str) -> Tuple[DynamicCache, int]:\n",
    "    \"\"\"Build baseline cache from passage only.\"\"\"\n",
    "    ids = tokenizer.encode(passage, return_tensors='pt', add_special_tokens=True).to(model.device)\n",
    "    with torch.no_grad():\n",
    "        out = model(ids, use_cache=True)\n",
    "    return out.past_key_values, ids.shape[1]\n",
    "\n",
    "def build_primed_cache_fullcontext(prefix: str, passage: str) -> Tuple[DynamicCache, int]:\n",
    "    \"\"\"Build full-context cache: prefix stays visible.\"\"\"\n",
    "    text = prefix + \" \" + passage\n",
    "    ids = tokenizer.encode(text, return_tensors='pt', add_special_tokens=True).to(model.device)\n",
    "    with torch.no_grad():\n",
    "        out = model(ids, use_cache=True)\n",
    "    return out.past_key_values, ids.shape[1]\n",
    "\n",
    "def build_primed_cache_truncated(prefix: str, passage: str) -> Tuple[DynamicCache, int]:\n",
    "    \"\"\"Build truncated cache: prefix removed after forward pass.\"\"\"\n",
    "    prefix_with_sep = prefix + \" \"\n",
    "    prefix_ids = tokenizer.encode(prefix_with_sep, return_tensors='pt', add_special_tokens=True)\n",
    "    prefix_len = prefix_ids.shape[1]\n",
    "    \n",
    "    full_text = prefix_with_sep + passage\n",
    "    full_ids = tokenizer.encode(full_text, return_tensors='pt', add_special_tokens=True).to(model.device)\n",
    "    full_len = full_ids.shape[1]\n",
    "    doc_len = full_len - prefix_len\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        out = model(full_ids, use_cache=True)\n",
    "    \n",
    "    truncated_cache = extract_and_truncate_cache_with_bos(out.past_key_values, doc_len)\n",
    "    surrogate_offset = prefix_len - 1\n",
    "    correct_rope_positions_with_bos(truncated_cache, surrogate_offset, model)\n",
    "    \n",
    "    return truncated_cache, 1 + doc_len\n",
    "\n",
    "def score_sample(cache: DynamicCache, cache_len: int, query: str, answer: str) -> float:\n",
    "    \"\"\"Score P(answer | cache, query).\"\"\"\n",
    "    return score_answer_with_cache(\n",
    "        cache, cache_len,\n",
    "        f\"\\n\\nQuery: {query}\\nAnswer:\",\n",
    "        \" \" + answer,\n",
    "        model, tokenizer, config\n",
    "    )\n",
    "\n",
    "def evaluate_sample(sample: Sample) -> dict:\n",
    "    \"\"\"Evaluate a single sample across all conditions.\"\"\"\n",
    "    passage = sample.passage[:4000]  # Truncate very long passages\n",
    "    query = sample.query\n",
    "    answer = sample.answer[:500]  # Truncate very long answers\n",
    "    \n",
    "    # Oracle prefix: query repeated 5x\n",
    "    oracle_prefix = \" \".join([query] * 5)\n",
    "    \n",
    "    # Build caches\n",
    "    bare_cache, bare_len = build_bare_cache(passage)\n",
    "    trunc_cache, trunc_len = build_primed_cache_truncated(oracle_prefix, passage)\n",
    "    full_cache, full_len = build_primed_cache_fullcontext(oracle_prefix, passage)\n",
    "    \n",
    "    # Score\n",
    "    nll_bare = score_sample(deepcopy_cache(bare_cache), bare_len, query, answer)\n",
    "    nll_trunc = score_sample(deepcopy_cache(trunc_cache), trunc_len, query, answer)\n",
    "    nll_full = score_sample(deepcopy_cache(full_cache), full_len, query, answer)\n",
    "    \n",
    "    return {\n",
    "        'dataset': sample.dataset,\n",
    "        'passage_words': len(passage.split()),\n",
    "        'answer_words': len(answer.split()),\n",
    "        'nll_bare': nll_bare,\n",
    "        'nll_truncated': nll_trunc,\n",
    "        'nll_fullctx': nll_full,\n",
    "        'delta_truncated': nll_bare - nll_trunc,\n",
    "        'delta_fullctx': nll_bare - nll_full,\n",
    "    }\n",
    "\n",
    "print(\"Evaluation functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaders defined.\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Dataset Loaders\n",
    "\n",
    "def load_msmarco_hard(n: int) -> List[Sample]:\n",
    "    \"\"\"Load MS MARCO, filtered to hard samples (will filter after scoring bare).\"\"\"\n",
    "    print(\"Loading MS MARCO...\")\n",
    "    ds = load_dataset(\"ms_marco\", \"v1.1\", split=\"train\")\n",
    "    samples = []\n",
    "    for item in ds:\n",
    "        if item['answers'] and item['answers'][0] and item['passages']['passage_text']:\n",
    "            samples.append(Sample(\n",
    "                passage=item['passages']['passage_text'][0],\n",
    "                query=item['query'],\n",
    "                answer=item['answers'][0],\n",
    "                dataset='msmarco_hard'\n",
    "            ))\n",
    "        if len(samples) >= n * 3:  # Get extra to filter\n",
    "            break\n",
    "    random.shuffle(samples)\n",
    "    return samples[:n * 3]  # Will filter to hardest n later\n",
    "\n",
    "def load_narrativeqa(n: int) -> List[Sample]:\n",
    "    \"\"\"Load NarrativeQA - long document QA.\"\"\"\n",
    "    print(\"Loading NarrativeQA...\")\n",
    "    try:\n",
    "        ds = load_dataset(\"narrativeqa\", split=\"test\", trust_remote_code=True)\n",
    "        samples = []\n",
    "        for item in ds:\n",
    "            if item['document']['summary']['text']:\n",
    "                samples.append(Sample(\n",
    "                    passage=item['document']['summary']['text'][:6000],\n",
    "                    query=item['question']['text'],\n",
    "                    answer=item['answers'][0]['text'] if item['answers'] else \"\",\n",
    "                    dataset='narrativeqa',\n",
    "                    metadata={'doc_type': item['document'].get('kind', 'unknown')}\n",
    "                ))\n",
    "            if len(samples) >= n:\n",
    "                break\n",
    "        random.shuffle(samples)\n",
    "        return samples[:n]\n",
    "    except Exception as e:\n",
    "        print(f\"  Error loading NarrativeQA: {e}\")\n",
    "        return []\n",
    "\n",
    "def load_hotpotqa(n: int) -> List[Sample]:\n",
    "    \"\"\"Load HotpotQA - multi-hop reasoning.\"\"\"\n",
    "    print(\"Loading HotpotQA...\")\n",
    "    try:\n",
    "        ds = load_dataset(\"hotpot_qa\", \"fullwiki\", split=\"validation\")\n",
    "        samples = []\n",
    "        for item in ds:\n",
    "            # Combine supporting paragraphs\n",
    "            context_parts = []\n",
    "            for title, sentences in zip(item['context']['title'], item['context']['sentences']):\n",
    "                context_parts.append(f\"{title}: {' '.join(sentences)}\")\n",
    "            passage = \"\\n\\n\".join(context_parts[:3])  # Limit to 3 paragraphs\n",
    "            \n",
    "            if passage and item['answer']:\n",
    "                samples.append(Sample(\n",
    "                    passage=passage,\n",
    "                    query=item['question'],\n",
    "                    answer=item['answer'],\n",
    "                    dataset='hotpotqa',\n",
    "                    metadata={'type': item['type'], 'level': item['level']}\n",
    "                ))\n",
    "            if len(samples) >= n:\n",
    "                break\n",
    "        random.shuffle(samples)\n",
    "        return samples[:n]\n",
    "    except Exception as e:\n",
    "        print(f\"  Error loading HotpotQA: {e}\")\n",
    "        return []\n",
    "\n",
    "def load_pubmedqa(n: int) -> List[Sample]:\n",
    "    \"\"\"Load PubMedQA - scientific/medical QA.\"\"\"\n",
    "    print(\"Loading PubMedQA...\")\n",
    "    try:\n",
    "        ds = load_dataset(\"pubmed_qa\", \"pqa_labeled\", split=\"train\")\n",
    "        samples = []\n",
    "        for item in ds:\n",
    "            context = \" \".join(item['context']['contexts']) if item['context']['contexts'] else \"\"\n",
    "            if context and item['long_answer']:\n",
    "                samples.append(Sample(\n",
    "                    passage=context,\n",
    "                    query=item['question'],\n",
    "                    answer=item['long_answer'],\n",
    "                    dataset='pubmedqa',\n",
    "                    metadata={'final_decision': item['final_decision']}\n",
    "                ))\n",
    "            if len(samples) >= n:\n",
    "                break\n",
    "        random.shuffle(samples)\n",
    "        return samples[:n]\n",
    "    except Exception as e:\n",
    "        print(f\"  Error loading PubMedQA: {e}\")\n",
    "        return []\n",
    "\n",
    "def load_cnn_dailymail(n: int) -> List[Sample]:\n",
    "    \"\"\"Load CNN/DailyMail - summarization as QA.\"\"\"\n",
    "    print(\"Loading CNN/DailyMail...\")\n",
    "    try:\n",
    "        ds = load_dataset(\"cnn_dailymail\", \"3.0.0\", split=\"test\")\n",
    "        samples = []\n",
    "        for item in ds:\n",
    "            if item['article'] and item['highlights']:\n",
    "                samples.append(Sample(\n",
    "                    passage=item['article'][:5000],\n",
    "                    query=\"Summarize the key points of this article.\",\n",
    "                    answer=item['highlights'],\n",
    "                    dataset='cnn_dailymail'\n",
    "                ))\n",
    "            if len(samples) >= n:\n",
    "                break\n",
    "        random.shuffle(samples)\n",
    "        return samples[:n]\n",
    "    except Exception as e:\n",
    "        print(f\"  Error loading CNN/DailyMail: {e}\")\n",
    "        return []\n",
    "\n",
    "def load_natural_questions(n: int) -> List[Sample]:\n",
    "    \"\"\"Load Natural Questions - Wikipedia QA.\"\"\"\n",
    "    print(\"Loading Natural Questions...\")\n",
    "    try:\n",
    "        ds = load_dataset(\"natural_questions\", \"default\", split=\"validation\")\n",
    "        samples = []\n",
    "        for item in ds:\n",
    "            # Get long answer context\n",
    "            doc_tokens = item['document']['tokens']\n",
    "            doc_text = ' '.join([t['token'] for t in doc_tokens['token'][:1000]])  # Limit tokens\n",
    "            \n",
    "            # Get short answer if available\n",
    "            short_answers = item['annotations']['short_answers']\n",
    "            if short_answers and short_answers[0]['start_token'] >= 0:\n",
    "                ans_start = short_answers[0]['start_token'][0]\n",
    "                ans_end = short_answers[0]['end_token'][0]\n",
    "                answer = ' '.join([t['token'] for t in doc_tokens['token'][ans_start:ans_end]])\n",
    "            else:\n",
    "                continue  # Skip if no short answer\n",
    "            \n",
    "            if doc_text and answer:\n",
    "                samples.append(Sample(\n",
    "                    passage=doc_text,\n",
    "                    query=item['question']['text'],\n",
    "                    answer=answer,\n",
    "                    dataset='natural_questions'\n",
    "                ))\n",
    "            if len(samples) >= n:\n",
    "                break\n",
    "        random.shuffle(samples)\n",
    "        return samples[:n]\n",
    "    except Exception as e:\n",
    "        print(f\"  Error loading Natural Questions: {e}\")\n",
    "        return []\n",
    "\n",
    "def load_squad_v2(n: int) -> List[Sample]:\n",
    "    \"\"\"Load SQuAD v2 - reading comprehension with unanswerable questions.\"\"\"\n",
    "    print(\"Loading SQuAD v2...\")\n",
    "    try:\n",
    "        ds = load_dataset(\"squad_v2\", split=\"validation\")\n",
    "        samples = []\n",
    "        for item in ds:\n",
    "            # Only use answerable questions\n",
    "            if item['answers']['text']:\n",
    "                samples.append(Sample(\n",
    "                    passage=item['context'],\n",
    "                    query=item['question'],\n",
    "                    answer=item['answers']['text'][0],\n",
    "                    dataset='squad_v2'\n",
    "                ))\n",
    "            if len(samples) >= n:\n",
    "                break\n",
    "        random.shuffle(samples)\n",
    "        return samples[:n]\n",
    "    except Exception as e:\n",
    "        print(f\"  Error loading SQuAD v2: {e}\")\n",
    "        return []\n",
    "\n",
    "print(\"Dataset loaders defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "LOADING DATASETS\n",
      "======================================================================\n",
      "Loading MS MARCO...\n",
      "  msmarco_hard: 300 samples loaded\n",
      "Loading SQuAD v2...\n",
      "  squad_v2: 100 samples loaded\n",
      "Loading HotpotQA...\n",
      "  hotpotqa: 100 samples loaded\n",
      "Loading PubMedQA...\n",
      "  pubmedqa: 100 samples loaded\n",
      "Loading CNN/DailyMail...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`trust_remote_code` is not supported anymore.\n",
      "Please check that the Hugging Face dataset 'narrativeqa' isn't based on a loading script and remove `trust_remote_code`.\n",
      "If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  cnn_dailymail: 100 samples loaded\n",
      "Loading NarrativeQA...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a51a517a01e043229096b4a6729e01e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c511b5816044264acfa444ce08ef54b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  narrativeqa: 100 samples loaded\n",
      "\n",
      "Total datasets loaded: 6\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Load All Datasets\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"LOADING DATASETS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "all_samples = {}\n",
    "\n",
    "# Load each dataset\n",
    "loaders = [\n",
    "    ('msmarco_hard', load_msmarco_hard),\n",
    "    ('squad_v2', load_squad_v2),\n",
    "    ('hotpotqa', load_hotpotqa),\n",
    "    ('pubmedqa', load_pubmedqa),\n",
    "    ('cnn_dailymail', load_cnn_dailymail),\n",
    "    ('narrativeqa', load_narrativeqa),\n",
    "    # ('natural_questions', load_natural_questions),  # Often slow/large\n",
    "]\n",
    "\n",
    "for name, loader in loaders:\n",
    "    try:\n",
    "        samples = loader(N_SAMPLES_PER_DATASET)\n",
    "        if samples:\n",
    "            all_samples[name] = samples\n",
    "            print(f\"  {name}: {len(samples)} samples loaded\")\n",
    "        else:\n",
    "            print(f\"  {name}: No samples loaded\")\n",
    "    except Exception as e:\n",
    "        print(f\"  {name}: Error - {e}\")\n",
    "\n",
    "print(f\"\\nTotal datasets loaded: {len(all_samples)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "DATASET STATISTICS\n",
      "======================================================================\n",
      "\n",
      "Dataset                   N   Passage Words    Answer Words\n",
      "------------------------------------------------------------\n",
      "msmarco_hard            300            64.3            12.9\n",
      "squad_v2                100           105.4             1.6\n",
      "hotpotqa                100           303.2             2.1\n",
      "pubmedqa                100           187.5            44.6\n",
      "cnn_dailymail           100           521.6            34.0\n",
      "narrativeqa             100           301.6             3.4\n",
      "\n",
      "======================================================================\n",
      "SAMPLE EXAMPLES\n",
      "======================================================================\n",
      "\n",
      "### msmarco_hard ###\n",
      "Query: what was true of the desegregation of the armed forces under president truman\n",
      "Answer: An official government policy.\n",
      "Passage: 1 Full text of Executive Order 9981 from the Harry S. Truman Presidential Library and Museum. 2  Integration of the Armed Forces, 1940-1965 (Defense S...\n",
      "\n",
      "### squad_v2 ###\n",
      "Query: In what century did important classical music developments occur in Normandy?\n",
      "Answer: 11th\n",
      "Passage: Normandy was the site of several important developments in the history of classical music in the 11th century. FÃ©camp Abbey and Saint-Evroul Abbey wer...\n",
      "\n",
      "### hotpotqa ###\n",
      "Query: How many copies of Roald Dahl's variation on a popular anecdote sold?\n",
      "Answer: 250 million\n",
      "Passage: The Collected Short Stories of Roald Dahl: The Collected Short Stories of Roald Dahl is a 1991 short story collection for adults by Roald Dahl.  The c...\n",
      "\n",
      "### pubmedqa ###\n",
      "Query: Treatment of contralateral hydrocele in neonatal testicular torsion: Is less more?\n",
      "Answer: We have demonstrated that approaching a contralateral hydrocele in cases of neonatal testicular tors...\n",
      "Passage: Treatment of neonatal testicular torsion has two objectives: salvage of the involved testicle (which is rarely achieved) and preservation of the contr...\n",
      "\n",
      "### cnn_dailymail ###\n",
      "Query: Summarize the key points of this article.\n",
      "Answer: Once a super typhoon, Maysak is now a tropical storm with 70 mph winds .\n",
      "It could still cause floodi...\n",
      "Passage: (CNN)Filipinos are being warned to be on guard for flash floods and landslides as tropical storm Maysak approached the Asian island nation Saturday. J...\n",
      "\n",
      "### narrativeqa ###\n",
      "Query: What sort of ship did Maskull, Krag, and Nightspore set of in?\n",
      "Answer: A crystal ship\n",
      "Passage:  Maskull, a man longing for adventures, accepts an invitation from Krag, an acquaintance of his friend Nightspore, to travel to Tormance after a seanc...\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Show Dataset Statistics\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"DATASET STATISTICS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\n{'Dataset':<20} {'N':>6} {'Passage Words':>15} {'Answer Words':>15}\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "for name, samples in all_samples.items():\n",
    "    passage_words = [len(s.passage.split()) for s in samples[:50]]\n",
    "    answer_words = [len(s.answer.split()) for s in samples[:50]]\n",
    "    print(f\"{name:<20} {len(samples):>6} {np.mean(passage_words):>15.1f} {np.mean(answer_words):>15.1f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SAMPLE EXAMPLES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for name, samples in all_samples.items():\n",
    "    s = samples[0]\n",
    "    print(f\"\\n### {name} ###\")\n",
    "    print(f\"Query: {s.query[:100]}...\" if len(s.query) > 100 else f\"Query: {s.query}\")\n",
    "    print(f\"Answer: {s.answer[:100]}...\" if len(s.answer) > 100 else f\"Answer: {s.answer}\")\n",
    "    print(f\"Passage: {s.passage[:150]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "RUNNING EVALUATION\n",
      "======================================================================\n",
      "Loaded checkpoint with 6 datasets\n",
      "\n",
      "msmarco_hard: Already complete (100 samples)\n",
      "\n",
      "squad_v2: Already complete (100 samples)\n",
      "\n",
      "hotpotqa: Already complete (100 samples)\n",
      "\n",
      "pubmedqa: Already complete (100 samples)\n",
      "\n",
      "cnn_dailymail: Already complete (100 samples)\n",
      "\n",
      "narrativeqa: Already complete (100 samples)\n",
      "\n",
      "Saved all results to /home/jupyter/research/directed_kvcache/results/exp19/results.json\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Run Evaluation\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"RUNNING EVALUATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "all_results = {}\n",
    "CHECKPOINT_PATH = f'{OUTPUT_DIR}/checkpoint.json'\n",
    "\n",
    "# Load checkpoint if exists\n",
    "if os.path.exists(CHECKPOINT_PATH):\n",
    "    with open(CHECKPOINT_PATH, 'r') as f:\n",
    "        all_results = json.load(f)\n",
    "    print(f\"Loaded checkpoint with {len(all_results)} datasets\")\n",
    "\n",
    "for dataset_name, samples in all_samples.items():\n",
    "    if dataset_name in all_results and len(all_results[dataset_name]) >= N_SAMPLES_PER_DATASET:\n",
    "        print(f\"\\n{dataset_name}: Already complete ({len(all_results[dataset_name])} samples)\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\nProcessing {dataset_name}...\")\n",
    "    \n",
    "    # For MS MARCO hard, we need to evaluate bare first then filter\n",
    "    if dataset_name == 'msmarco_hard':\n",
    "        # First pass: get bare NLL for all\n",
    "        bare_nlls = []\n",
    "        print(\"  First pass: computing bare NLL to filter hard samples...\")\n",
    "        for s in tqdm(samples[:N_SAMPLES_PER_DATASET * 2], desc=\"  Bare NLL\"):\n",
    "            try:\n",
    "                cache, cache_len = build_bare_cache(s.passage[:4000])\n",
    "                nll = score_sample(deepcopy_cache(cache), cache_len, s.query, s.answer[:500])\n",
    "                bare_nlls.append((nll, s))\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        # Sort by NLL (descending) and take hardest N\n",
    "        bare_nlls.sort(key=lambda x: x[0], reverse=True)\n",
    "        samples = [s for _, s in bare_nlls[:N_SAMPLES_PER_DATASET]]\n",
    "        print(f\"  Filtered to {len(samples)} hardest samples (NLL range: {bare_nlls[0][0]:.2f} - {bare_nlls[N_SAMPLES_PER_DATASET-1][0]:.2f})\")\n",
    "    \n",
    "    results = []\n",
    "    for sample in tqdm(samples[:N_SAMPLES_PER_DATASET], desc=f\"  {dataset_name}\"):\n",
    "        try:\n",
    "            result = evaluate_sample(sample)\n",
    "            results.append(result)\n",
    "        except Exception as e:\n",
    "            continue\n",
    "        \n",
    "        # Checkpoint every 25 samples\n",
    "        if len(results) % 25 == 0:\n",
    "            all_results[dataset_name] = results\n",
    "            with open(CHECKPOINT_PATH, 'w') as f:\n",
    "                json.dump(all_results, f)\n",
    "    \n",
    "    all_results[dataset_name] = results\n",
    "    print(f\"  Completed: {len(results)} samples\")\n",
    "\n",
    "# Final save\n",
    "with open(f'{OUTPUT_DIR}/results.json', 'w') as f:\n",
    "    json.dump(all_results, f, indent=2)\n",
    "print(f\"\\nSaved all results to {OUTPUT_DIR}/results.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "CROSS-DATASET SURVEY RESULTS\n",
      "======================================================================\n",
      "\n",
      "Dataset                N   Bare NLL   Trunc Win%    Trunc d    Full Win%     Full d\n",
      "-------------------------------------------------------------------------------------\n",
      "msmarco_hard         100       3.59        59.0%     +0.190        32.0%     -0.358\n",
      "squad_v2             100       0.14        52.0%     +0.003        47.0%     -0.026\n",
      "hotpotqa             100       1.68        34.0%     -0.348        29.0%     -0.381\n",
      "pubmedqa             100       1.96        16.0%     -0.728         8.0%     -1.132\n",
      "cnn_dailymail        100       2.77         8.0%     -1.307         8.0%     -1.330\n",
      "narrativeqa          100       1.26        45.0%     -0.348        30.0%     -0.599\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Analysis - Summary Table\n",
    "\n",
    "def cohens_d(x):\n",
    "    return np.mean(x) / np.std(x, ddof=1) if np.std(x) > 0 else 0\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"CROSS-DATASET SURVEY RESULTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "summary = []\n",
    "\n",
    "print(f\"\\n{'Dataset':<18} {'N':>5} {'Bare NLL':>10} {'Trunc Win%':>12} {'Trunc d':>10} {'Full Win%':>12} {'Full d':>10}\")\n",
    "print(\"-\"*85)\n",
    "\n",
    "for dataset_name, results in all_results.items():\n",
    "    if not results:\n",
    "        continue\n",
    "    \n",
    "    n = len(results)\n",
    "    bare = np.array([r['nll_bare'] for r in results])\n",
    "    delta_trunc = np.array([r['delta_truncated'] for r in results])\n",
    "    delta_full = np.array([r['delta_fullctx'] for r in results])\n",
    "    \n",
    "    win_trunc = np.mean(delta_trunc > 0) * 100\n",
    "    win_full = np.mean(delta_full > 0) * 100\n",
    "    d_trunc = cohens_d(delta_trunc)\n",
    "    d_full = cohens_d(delta_full)\n",
    "    \n",
    "    print(f\"{dataset_name:<18} {n:>5} {np.mean(bare):>10.2f} {win_trunc:>11.1f}% {d_trunc:>+10.3f} {win_full:>11.1f}% {d_full:>+10.3f}\")\n",
    "    \n",
    "    summary.append({\n",
    "        'dataset': dataset_name,\n",
    "        'n': n,\n",
    "        'mean_bare_nll': np.mean(bare),\n",
    "        'mean_passage_words': np.mean([r['passage_words'] for r in results]),\n",
    "        'mean_answer_words': np.mean([r['answer_words'] for r in results]),\n",
    "        'truncated_win_rate': win_trunc,\n",
    "        'truncated_cohens_d': d_trunc,\n",
    "        'fullctx_win_rate': win_full,\n",
    "        'fullctx_cohens_d': d_full,\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "STATISTICAL TESTS: Does priming help? (one-sample t-test, H0: delta=0)\n",
      "======================================================================\n",
      "\n",
      "Dataset                            Truncated              Full-Context\n",
      "                         t-stat      p-value       t-stat      p-value\n",
      "----------------------------------------------------------------------\n",
      "msmarco_hard              +1.90      0.0603        -3.58      0.0005*\n",
      "squad_v2                  +0.03      0.9723        -0.26      0.7963\n",
      "hotpotqa                  -3.48      0.0008*        -3.81      0.0002*\n",
      "pubmedqa                  -7.28      0.0000*       -11.32      0.0000*\n",
      "cnn_dailymail            -13.07      0.0000*       -13.30      0.0000*\n",
      "narrativeqa               -3.48      0.0007*        -5.99      0.0000*\n",
      "\n",
      "* = p < 0.05 (significant)\n",
      "Positive t-stat = priming HELPS, Negative t-stat = priming HURTS\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Statistical Significance Tests\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STATISTICAL TESTS: Does priming help? (one-sample t-test, H0: delta=0)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\n{'Dataset':<18} {'Truncated':>25} {'Full-Context':>25}\")\n",
    "print(f\"{'':18} {'t-stat':>12} {'p-value':>12} {'t-stat':>12} {'p-value':>12}\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "for dataset_name, results in all_results.items():\n",
    "    if not results or len(results) < 10:\n",
    "        continue\n",
    "    \n",
    "    delta_trunc = np.array([r['delta_truncated'] for r in results])\n",
    "    delta_full = np.array([r['delta_fullctx'] for r in results])\n",
    "    \n",
    "    t_trunc, p_trunc = stats.ttest_1samp(delta_trunc, 0)\n",
    "    t_full, p_full = stats.ttest_1samp(delta_full, 0)\n",
    "    \n",
    "    # Mark significant results\n",
    "    sig_trunc = \"*\" if p_trunc < 0.05 else \"\"\n",
    "    sig_full = \"*\" if p_full < 0.05 else \"\"\n",
    "    \n",
    "    print(f\"{dataset_name:<18} {t_trunc:>+12.2f} {p_trunc:>11.4f}{sig_trunc} {t_full:>+12.2f} {p_full:>11.4f}{sig_full}\")\n",
    "\n",
    "print(\"\\n* = p < 0.05 (significant)\")\n",
    "print(\"Positive t-stat = priming HELPS, Negative t-stat = priming HURTS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "RANKING: Which datasets benefit most from priming?\n",
      "======================================================================\n",
      "\n",
      "### Ranked by TRUNCATED effect size (Cohen's d) ###\n",
      "Rank   Dataset                     d       Win%     Bare NLL   Passage Words\n",
      "---------------------------------------------------------------------------\n",
      "1      msmarco_hard           +0.190      59.0%         3.59              69  [HELPS]\n",
      "2      squad_v2               +0.003      52.0%         0.14             105  [NEUTRAL]\n",
      "3      hotpotqa               -0.348      34.0%         1.68             296  [HURTS]\n",
      "4      narrativeqa            -0.348      45.0%         1.26             323  [HURTS]\n",
      "5      pubmedqa               -0.728      16.0%         1.96             191  [HURTS]\n",
      "6      cnn_dailymail          -1.307       8.0%         2.77             446  [HURTS]\n",
      "\n",
      "### Ranked by FULL-CONTEXT effect size (Cohen's d) ###\n",
      "Rank   Dataset                     d       Win%\n",
      "--------------------------------------------------\n",
      "1      squad_v2               -0.026      47.0%  [NEUTRAL]\n",
      "2      msmarco_hard           -0.358      32.0%  [HURTS]\n",
      "3      hotpotqa               -0.381      29.0%  [HURTS]\n",
      "4      narrativeqa            -0.599      30.0%  [HURTS]\n",
      "5      pubmedqa               -1.132       8.0%  [HURTS]\n",
      "6      cnn_dailymail          -1.330       8.0%  [HURTS]\n"
     ]
    }
   ],
   "source": [
    "# Cell 10: Identify Best Datasets for Priming\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RANKING: Which datasets benefit most from priming?\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Sort by truncated Cohen's d\n",
    "summary_sorted = sorted(summary, key=lambda x: x['truncated_cohens_d'], reverse=True)\n",
    "\n",
    "print(\"\\n### Ranked by TRUNCATED effect size (Cohen's d) ###\")\n",
    "print(f\"{'Rank':<6} {'Dataset':<18} {'d':>10} {'Win%':>10} {'Bare NLL':>12} {'Passage Words':>15}\")\n",
    "print(\"-\"*75)\n",
    "for i, s in enumerate(summary_sorted, 1):\n",
    "    verdict = \"HELPS\" if s['truncated_cohens_d'] > 0.1 else \"HURTS\" if s['truncated_cohens_d'] < -0.1 else \"NEUTRAL\"\n",
    "    print(f\"{i:<6} {s['dataset']:<18} {s['truncated_cohens_d']:>+10.3f} {s['truncated_win_rate']:>9.1f}% {s['mean_bare_nll']:>12.2f} {s['mean_passage_words']:>15.0f}  [{verdict}]\")\n",
    "\n",
    "print(\"\\n### Ranked by FULL-CONTEXT effect size (Cohen's d) ###\")\n",
    "summary_sorted_full = sorted(summary, key=lambda x: x['fullctx_cohens_d'], reverse=True)\n",
    "print(f\"{'Rank':<6} {'Dataset':<18} {'d':>10} {'Win%':>10}\")\n",
    "print(\"-\"*50)\n",
    "for i, s in enumerate(summary_sorted_full, 1):\n",
    "    verdict = \"HELPS\" if s['fullctx_cohens_d'] > 0.1 else \"HURTS\" if s['fullctx_cohens_d'] < -0.1 else \"NEUTRAL\"\n",
    "    print(f\"{i:<6} {s['dataset']:<18} {s['fullctx_cohens_d']:>+10.3f} {s['fullctx_win_rate']:>9.1f}%  [{verdict}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "CORRELATION: What predicts priming benefit?\n",
      "======================================================================\n",
      "\n",
      "Total samples across all datasets: 600\n",
      "\n",
      "Predictor                 Truncated Delta   Full-Context Delta\n",
      "                              r         p          r         p\n",
      "--------------------------------------------------------------\n",
      "Bare NLL                 -0.175   0.0000*     -0.211   0.0000*\n",
      "Passage Words            -0.197   0.0000*     -0.139   0.0007*\n",
      "Answer Words             -0.027   0.5027     +0.079   0.0521\n",
      "\n",
      "* = p < 0.05\n"
     ]
    }
   ],
   "source": [
    "# Cell 11: Correlation Analysis - What predicts priming benefit?\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CORRELATION: What predicts priming benefit?\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Combine all results\n",
    "all_flat = []\n",
    "for dataset_name, results in all_results.items():\n",
    "    for r in results:\n",
    "        all_flat.append(r)\n",
    "\n",
    "if len(all_flat) > 50:\n",
    "    bare_nll = np.array([r['nll_bare'] for r in all_flat])\n",
    "    passage_words = np.array([r['passage_words'] for r in all_flat])\n",
    "    answer_words = np.array([r['answer_words'] for r in all_flat])\n",
    "    delta_trunc = np.array([r['delta_truncated'] for r in all_flat])\n",
    "    delta_full = np.array([r['delta_fullctx'] for r in all_flat])\n",
    "    \n",
    "    print(f\"\\nTotal samples across all datasets: {len(all_flat)}\")\n",
    "    \n",
    "    print(f\"\\n{'Predictor':<20} {'Truncated Delta':>20} {'Full-Context Delta':>20}\")\n",
    "    print(f\"{'':20} {'r':>10} {'p':>9} {'r':>10} {'p':>9}\")\n",
    "    print(\"-\"*62)\n",
    "    \n",
    "    for name, predictor in [('Bare NLL', bare_nll), ('Passage Words', passage_words), ('Answer Words', answer_words)]:\n",
    "        r1, p1 = stats.pearsonr(predictor, delta_trunc)\n",
    "        r2, p2 = stats.pearsonr(predictor, delta_full)\n",
    "        sig1 = \"*\" if p1 < 0.05 else \"\"\n",
    "        sig2 = \"*\" if p2 < 0.05 else \"\"\n",
    "        print(f\"{name:<20} {r1:>+10.3f} {p1:>8.4f}{sig1} {r2:>+10.3f} {p2:>8.4f}{sig2}\")\n",
    "    \n",
    "    print(\"\\n* = p < 0.05\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis saved to /home/jupyter/research/directed_kvcache/results/exp19/analysis.json\n",
      "\n",
      "======================================================================\n",
      "CONCLUSIONS\n",
      "======================================================================\n",
      "\n",
      "Datasets where TRUNCATED priming helps (d > 0.1):\n",
      "  - msmarco_hard: d=+0.190, win=59.0%\n",
      "\n",
      "Datasets where TRUNCATED priming hurts (d < -0.1):\n",
      "  - hotpotqa: d=-0.348, win=34.0%\n",
      "  - pubmedqa: d=-0.728, win=16.0%\n",
      "  - cnn_dailymail: d=-1.307, win=8.0%\n",
      "  - narrativeqa: d=-0.348, win=45.0%\n",
      "\n",
      "Datasets where FULL-CONTEXT priming helps (d > 0.1):\n"
     ]
    }
   ],
   "source": [
    "# Cell 12: Save Final Analysis\n",
    "\n",
    "analysis = {\n",
    "    'summary': summary,\n",
    "    'total_samples': len(all_flat) if 'all_flat' in dir() else sum(len(r) for r in all_results.values()),\n",
    "    'datasets_evaluated': list(all_results.keys()),\n",
    "}\n",
    "\n",
    "with open(f'{OUTPUT_DIR}/analysis.json', 'w') as f:\n",
    "    json.dump(analysis, f, indent=2)\n",
    "\n",
    "print(f\"Analysis saved to {OUTPUT_DIR}/analysis.json\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CONCLUSIONS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nDatasets where TRUNCATED priming helps (d > 0.1):\")\n",
    "for s in summary:\n",
    "    if s['truncated_cohens_d'] > 0.1:\n",
    "        print(f\"  - {s['dataset']}: d={s['truncated_cohens_d']:+.3f}, win={s['truncated_win_rate']:.1f}%\")\n",
    "\n",
    "print(\"\\nDatasets where TRUNCATED priming hurts (d < -0.1):\")\n",
    "for s in summary:\n",
    "    if s['truncated_cohens_d'] < -0.1:\n",
    "        print(f\"  - {s['dataset']}: d={s['truncated_cohens_d']:+.3f}, win={s['truncated_win_rate']:.1f}%\")\n",
    "\n",
    "print(\"\\nDatasets where FULL-CONTEXT priming helps (d > 0.1):\")\n",
    "for s in summary:\n",
    "    if s['fullctx_cohens_d'] > 0.1:\n",
    "        print(f\"  - {s['dataset']}: d={s['fullctx_cohens_d']:+.3f}, win={s['fullctx_win_rate']:.1f}%\")\n",
    "\n",
    "if not any(s['truncated_cohens_d'] > 0.1 for s in summary):\n",
    "    print(\"  (none)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "PER-DATASET DETAILS\n",
      "======================================================================\n",
      "\n",
      "### MSMARCO_HARD ###\n",
      "  N = 100\n",
      "  Mean passage words: 69\n",
      "  Mean answer words: 15\n",
      "  Mean bare NLL: 3.594\n",
      "  \n",
      "  TRUNCATED: win=59.0%, mean_delta=+0.068, d=+0.190\n",
      "  FULL-CTX:  win=32.0%, mean_delta=-0.177, d=-0.358\n",
      "  Hardness interaction (bare NLL vs delta_trunc): r=+0.255, p=0.0105\n",
      "\n",
      "### SQUAD_V2 ###\n",
      "  N = 100\n",
      "  Mean passage words: 105\n",
      "  Mean answer words: 2\n",
      "  Mean bare NLL: 0.139\n",
      "  \n",
      "  TRUNCATED: win=52.0%, mean_delta=+0.000, d=+0.003\n",
      "  FULL-CTX:  win=47.0%, mean_delta=-0.006, d=-0.026\n",
      "  Hardness interaction (bare NLL vs delta_trunc): r=+0.443, p=0.0000\n",
      "\n",
      "### HOTPOTQA ###\n",
      "  N = 100\n",
      "  Mean passage words: 296\n",
      "  Mean answer words: 2\n",
      "  Mean bare NLL: 1.678\n",
      "  \n",
      "  TRUNCATED: win=34.0%, mean_delta=-0.130, d=-0.348\n",
      "  FULL-CTX:  win=29.0%, mean_delta=-0.471, d=-0.381\n",
      "  Hardness interaction (bare NLL vs delta_trunc): r=-0.403, p=0.0000\n",
      "\n",
      "### PUBMEDQA ###\n",
      "  N = 100\n",
      "  Mean passage words: 191\n",
      "  Mean answer words: 42\n",
      "  Mean bare NLL: 1.963\n",
      "  \n",
      "  TRUNCATED: win=16.0%, mean_delta=-0.072, d=-0.728\n",
      "  FULL-CTX:  win=8.0%, mean_delta=-0.211, d=-1.132\n",
      "  Hardness interaction (bare NLL vs delta_trunc): r=+0.025, p=0.8024\n",
      "\n",
      "### CNN_DAILYMAIL ###\n",
      "  N = 100\n",
      "  Mean passage words: 446\n",
      "  Mean answer words: 34\n",
      "  Mean bare NLL: 2.766\n",
      "  \n",
      "  TRUNCATED: win=8.0%, mean_delta=-0.152, d=-1.307\n",
      "  FULL-CTX:  win=8.0%, mean_delta=-0.220, d=-1.330\n",
      "  Hardness interaction (bare NLL vs delta_trunc): r=-0.135, p=0.1796\n",
      "\n",
      "### NARRATIVEQA ###\n",
      "  N = 100\n",
      "  Mean passage words: 323\n",
      "  Mean answer words: 4\n",
      "  Mean bare NLL: 1.256\n",
      "  \n",
      "  TRUNCATED: win=45.0%, mean_delta=-0.162, d=-0.348\n",
      "  FULL-CTX:  win=30.0%, mean_delta=-0.460, d=-0.599\n",
      "  Hardness interaction (bare NLL vs delta_trunc): r=-0.583, p=0.0000\n"
     ]
    }
   ],
   "source": [
    "# Cell 13: Per-Dataset Deep Dive\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PER-DATASET DETAILS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for dataset_name, results in all_results.items():\n",
    "    if not results or len(results) < 10:\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\n### {dataset_name.upper()} ###\")\n",
    "    \n",
    "    bare = np.array([r['nll_bare'] for r in results])\n",
    "    trunc = np.array([r['nll_truncated'] for r in results])\n",
    "    full = np.array([r['nll_fullctx'] for r in results])\n",
    "    delta_trunc = np.array([r['delta_truncated'] for r in results])\n",
    "    delta_full = np.array([r['delta_fullctx'] for r in results])\n",
    "    \n",
    "    print(f\"  N = {len(results)}\")\n",
    "    print(f\"  Mean passage words: {np.mean([r['passage_words'] for r in results]):.0f}\")\n",
    "    print(f\"  Mean answer words: {np.mean([r['answer_words'] for r in results]):.0f}\")\n",
    "    print(f\"  Mean bare NLL: {np.mean(bare):.3f}\")\n",
    "    print(f\"  \")\n",
    "    print(f\"  TRUNCATED: win={np.mean(delta_trunc > 0)*100:.1f}%, mean_delta={np.mean(delta_trunc):+.3f}, d={cohens_d(delta_trunc):+.3f}\")\n",
    "    print(f\"  FULL-CTX:  win={np.mean(delta_full > 0)*100:.1f}%, mean_delta={np.mean(delta_full):+.3f}, d={cohens_d(delta_full):+.3f}\")\n",
    "    \n",
    "    # Hardness interaction within dataset\n",
    "    r_trunc, p_trunc = stats.pearsonr(bare, delta_trunc)\n",
    "    print(f\"  Hardness interaction (bare NLL vs delta_trunc): r={r_trunc:+.3f}, p={p_trunc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cu124.m127",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/base-cu124:m127"
  },
  "kernelspec": {
   "display_name": "Python 3 (Local)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
