{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": "# Experiment 11: Surrogate Quality Gradient & Ranking Evaluation\n\n## Motivation\n\nAcross experiments 01-10, we have established:\n\n1. **Oracle surrogates work dramatically** (NLL ~0.4 vs ~1.5 baseline) — the KV cache mechanism CAN be directed\n2. **Generated surrogates show no semantic signal** (r=0.924 correlation with shuffled text, Exp 06)\n3. **Any prefix helps equally** regardless of content — the benefit is positional/structural, not semantic\n4. **Truncation mechanics are correct** after bug fixes in Exp 05 (RoPE dimension pairing, BOS preservation, BPE boundaries)\n5. **Value contamination is the channel** but we can't isolate layers cleanly (Exp 10)\n\nThe fundamental unresolved question: **Is the lack of semantic signal because generated surrogates are too low quality (~0.66 similarity), or because the KV cache mechanism fundamentally cannot transmit semantic information through value contamination?**\n\n## Approach: Bridge the Oracle Gap with Real Queries\n\nPrevious experiments used model-generated surrogates (similarity ~0.66) or the oracle query itself (similarity 1.0). This leaves a massive unexplored gap. We fill it by using **real MS MARCO queries** as surrogates — queries that were asked about OTHER passages but happen to be semantically similar to the target query at varying degrees.\n\nThis is also closer to the production scenario: in an ad-serving system, we'd have a corpus of historical queries we could use as surrogates.\n\n## Three Investigations\n\n### Investigation A: Quality Gradient (N=300)\nFor each sample, find real queries at 5 similarity levels (0.0-0.3, 0.3-0.5, 0.5-0.7, 0.7-0.85, 0.85-1.0) and use them as surrogates. Plus oracle and bare baseline. This tells us whether there's a quality threshold where semantic priming starts working.\n\n### Investigation B: Ranking Task (N=300)\nThe NLL metric measures absolute quality. But in ad serving, what matters is **ranking**: does priming help the model rank the relevant ad higher among distractors? For each passage, we score 5 queries (1 correct + 4 distractors) and measure MRR/Hit@1.\n\n### Investigation C: Same-Passage Surrogates (N=200)\nMS MARCO has passages that were retrieved for multiple different queries. Use one query as surrogate, test with another. This is the most realistic production scenario: we know what queries a document has been relevant to in the past.\n\n## Key Controls\n- All caches use truncation + RoPE correction (production-relevant setting)\n- BPE-matched bare/truncated comparison using `build_matched_bare_and_truncated` pattern\n- Irrelevant real query baseline (same as Exp 06 random passage control, but real queries)\n- All conditions run on same samples for paired comparison\n- Bare document baseline uses no framing (confirmed better than \"Document:\\n\" in Exp 06)\n\n## Bug Log (from initial run)\n\n### Bug 11.1: BPE Token Mismatch (CRITICAL — same class as Exp 05 bug)\n- `build_bare_cache_no_framing()` tokenized the passage independently\n- `build_truncated_cache_from_prefix()` tokenized `prefix + passage` together, producing different BPE tokens at the join boundary\n- `build_matched_bare_and_truncated()` was defined correctly but NEVER CALLED in eval loops\n- **Fix**: All eval loops now use `build_matched_bare_and_truncated()` which extracts document token IDs from the concatenated encoding and builds bare cache from those exact IDs\n- **Impact**: Bare and truncated caches were being compared on different token sequences, invalidating all delta measurements\n\n### Bug 11.2: Cache Mutation in Investigation B (CRITICAL)\n- `score_answer_with_cache()` extends the cache in-place via `use_cache=True`\n- Investigation B scored 5 queries against the same cache object sequentially\n- Queries 2-5 saw a cache contaminated by previous queries' KV entries\n- **Fix**: Deep-copy (`copy.deepcopy()`) the cache before each `score_answer_with_cache()` call\n- **Impact**: Ranking results for bare/oracle/medium caches were all corrupted\n\n### Bug 11.3: Variable Cache Lengths Across Conditions\n- Different prefix lengths cause different BPE splits, leading to different `keep_len` values\n- **Fix**: `build_matched_bare_and_truncated()` asserts `bare_len == keep_len` for each condition; per-condition matched pairs ensure fair comparison"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": "import sys\nimport os\nimport json\nimport copy\nimport time\nimport datetime\nfrom typing import Dict, List, Any, Optional, Tuple\n\nimport torch\nimport numpy as np\nfrom tqdm.auto import tqdm\nfrom scipy import stats\nimport matplotlib.pyplot as plt\nimport matplotlib\nmatplotlib.rcParams['figure.dpi'] = 120\n\nsys.path.insert(0, '.')\n\nfrom lib import (\n    ExperimentConfig,\n    build_kv_cache,\n    score_answer_with_cache,\n    build_truncated_kv_cache_corrected,\n    extract_and_truncate_cache_with_bos,\n    correct_rope_positions_with_bos,\n    load_evaluation_samples,\n    load_ms_marco,\n    _ensure_dynamic_cache,\n)\nfrom lib.kv_cache import _get_cache_keys, _get_cache_values\n\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, DynamicCache\nfrom sentence_transformers import SentenceTransformer\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom datasets import load_dataset\n\nprint(f\"PyTorch: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Configuration\n",
    "# ============================================================\n",
    "\n",
    "config = ExperimentConfig(\n",
    "    num_samples=5000,  # Load more than we need for query pool\n",
    "    min_passage_words=50,\n",
    "    max_passage_words=300,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "N_INVESTIGATION_A = 300\n",
    "N_INVESTIGATION_B = 300\n",
    "N_INVESTIGATION_C = 200\n",
    "\n",
    "# Similarity bins for Investigation A\n",
    "SIMILARITY_BINS = [\n",
    "    (0.00, 0.30, 'very_low'),\n",
    "    (0.30, 0.50, 'low'),\n",
    "    (0.50, 0.70, 'medium'),\n",
    "    (0.70, 0.85, 'high'),\n",
    "    (0.85, 1.00, 'very_high'),\n",
    "]\n",
    "\n",
    "torch.manual_seed(config.seed)\n",
    "np.random.seed(config.seed)\n",
    "\n",
    "print(f\"Investigation A: {N_INVESTIGATION_A} samples x {len(SIMILARITY_BINS)+2} conditions\")\n",
    "print(f\"Investigation B: {N_INVESTIGATION_B} samples x ranking evaluation\")\n",
    "print(f\"Investigation C: {N_INVESTIGATION_C} samples (same-passage surrogates)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Load Model\n",
    "# ============================================================\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    ")\n",
    "\n",
    "print(f\"Loading {config.model_name}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "model.eval()\n",
    "print(f\"Model loaded. Layers: {model.config.num_hidden_layers}\")\n",
    "\n",
    "# Load embedding model for similarity computation\n",
    "print(\"Loading embedding model...\")\n",
    "embed_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "print(\"Embedding model loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# Load Dataset and Build Query Pool\n# ============================================================\n\nprint(\"Loading MS MARCO dataset...\")\ndataset = load_dataset(\n    config.dataset_name,\n    config.dataset_config,\n    split=config.dataset_split,\n    trust_remote_code=True\n)\nprint(f\"Dataset loaded: {len(dataset)} samples\")\n\n# Load evaluation samples (with answers)\nall_samples = load_evaluation_samples(dataset, config, require_answer=True)\nprint(f\"Loaded {len(all_samples)} evaluation samples with answers\")\n\n# Build query pool from the FULL dataset for maximum coverage\n# in high-similarity bins\nprint(\"\\nBuilding query pool (full dataset)...\")\nquery_pool = []\nseen_queries = set()\nfor item in tqdm(dataset, desc=\"Extracting queries\"):\n    q = item.get('query', '').strip()\n    if q and q not in seen_queries and len(q) > 10:\n        query_pool.append(q)\n        seen_queries.add(q)\n\nprint(f\"Query pool size: {len(query_pool)}\")\n\n# Embed all pool queries\nprint(\"Embedding query pool...\")\npool_embeddings = embed_model.encode(query_pool, show_progress_bar=True, batch_size=256)\nprint(f\"Pool embeddings shape: {pool_embeddings.shape}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Build Same-Passage Query Pairs for Investigation C\n",
    "# ============================================================\n",
    "\n",
    "# Find passages that appear with multiple different queries\n",
    "print(\"Finding passages with multiple queries...\")\n",
    "passage_to_queries = {}  # passage_text -> list of (query, answer) tuples\n",
    "\n",
    "for item in tqdm(dataset, desc=\"Building passage-query map\"):\n",
    "    passages = item.get('passages', {})\n",
    "    passage_texts = passages.get('passage_text', [])\n",
    "    is_selected = passages.get('is_selected', [])\n",
    "    query = item.get('query', '').strip()\n",
    "    answers = item.get('answers', [])\n",
    "    well_formed = item.get('wellFormedAnswers', [])\n",
    "\n",
    "    if not query:\n",
    "        continue\n",
    "\n",
    "    # Get best answer\n",
    "    answer = None\n",
    "    if well_formed and len(well_formed) > 0 and well_formed[0] != '[]':\n",
    "        answer = well_formed[0]\n",
    "    elif answers and len(answers) > 0 and answers[0] != 'No Answer Present.':\n",
    "        answer = answers[0]\n",
    "\n",
    "    if answer is None:\n",
    "        continue\n",
    "\n",
    "    for i, passage in enumerate(passage_texts):\n",
    "        if is_selected and i < len(is_selected) and is_selected[i] == 1:\n",
    "            word_count = len(passage.split())\n",
    "            if config.min_passage_words <= word_count <= config.max_passage_words:\n",
    "                key = passage.strip()[:200]  # Use prefix as key to handle near-dupes\n",
    "                if key not in passage_to_queries:\n",
    "                    passage_to_queries[key] = []\n",
    "                passage_to_queries[key].append({\n",
    "                    'passage': passage,\n",
    "                    'query': query,\n",
    "                    'answer': answer\n",
    "                })\n",
    "            break\n",
    "\n",
    "# Filter to passages with 2+ distinct queries\n",
    "multi_query_passages = {\n",
    "    k: v for k, v in passage_to_queries.items()\n",
    "    if len(v) >= 2 and len(set(item['query'] for item in v)) >= 2\n",
    "}\n",
    "\n",
    "print(f\"Passages with 2+ queries: {len(multi_query_passages)}\")\n",
    "print(f\"Total query pairs available: {sum(len(v) * (len(v)-1) for v in multi_query_passages.values())}\")\n",
    "\n",
    "# Build Investigation C samples: (passage, surrogate_query, test_query, test_answer)\n",
    "inv_c_samples = []\n",
    "for key, items in multi_query_passages.items():\n",
    "    # Use first query as surrogate, second as test\n",
    "    for i in range(len(items)):\n",
    "        for j in range(len(items)):\n",
    "            if i != j and items[i]['query'] != items[j]['query']:\n",
    "                inv_c_samples.append({\n",
    "                    'passage': items[i]['passage'],\n",
    "                    'surrogate_query': items[i]['query'],\n",
    "                    'test_query': items[j]['query'],\n",
    "                    'test_answer': items[j]['answer'],\n",
    "                })\n",
    "    if len(inv_c_samples) >= N_INVESTIGATION_C * 3:\n",
    "        break\n",
    "\n",
    "np.random.shuffle(inv_c_samples)\n",
    "inv_c_samples = inv_c_samples[:N_INVESTIGATION_C]\n",
    "print(f\"Investigation C samples: {len(inv_c_samples)}\")\n",
    "\n",
    "# Compute similarity between surrogate and test queries\n",
    "if inv_c_samples:\n",
    "    surr_qs = [s['surrogate_query'] for s in inv_c_samples]\n",
    "    test_qs = [s['test_query'] for s in inv_c_samples]\n",
    "    surr_embs = embed_model.encode(surr_qs)\n",
    "    test_embs = embed_model.encode(test_qs)\n",
    "    pair_sims = [float(cosine_similarity([surr_embs[i]], [test_embs[i]])[0][0])\n",
    "                 for i in range(len(inv_c_samples))]\n",
    "    for i, s in enumerate(inv_c_samples):\n",
    "        s['surrogate_similarity'] = pair_sims[i]\n",
    "    print(f\"Same-passage pair similarity: mean={np.mean(pair_sims):.3f}, \"\n",
    "          f\"std={np.std(pair_sims):.3f}, range=[{np.min(pair_sims):.3f}, {np.max(pair_sims):.3f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Helper Functions\n",
    "# ============================================================\n",
    "\n",
    "def find_surrogate_at_similarity(\n",
    "    target_query: str,\n",
    "    target_embedding: np.ndarray,\n",
    "    sim_low: float,\n",
    "    sim_high: float,\n",
    "    pool_queries: list,\n",
    "    pool_embs: np.ndarray,\n",
    "    rng: np.random.RandomState,\n",
    ") -> Optional[Tuple[str, float]]:\n",
    "    \"\"\"Find a real query from the pool within the specified similarity range.\n",
    "    \n",
    "    Returns (query, similarity) or None if no match found.\n",
    "    \"\"\"\n",
    "    sims = cosine_similarity([target_embedding], pool_embs)[0]\n",
    "    mask = (sims >= sim_low) & (sims < sim_high)\n",
    "    # Exclude exact match\n",
    "    for idx in np.where(mask)[0]:\n",
    "        if pool_queries[idx].strip().lower() == target_query.strip().lower():\n",
    "            mask[idx] = False\n",
    "    \n",
    "    candidates = np.where(mask)[0]\n",
    "    if len(candidates) == 0:\n",
    "        return None\n",
    "    \n",
    "    # Pick one near the middle of the bin for stability\n",
    "    mid = (sim_low + sim_high) / 2\n",
    "    distances_to_mid = np.abs(sims[candidates] - mid)\n",
    "    # Pick from the closest 10% to the bin midpoint (or at least 1)\n",
    "    n_pick = max(1, len(candidates) // 10)\n",
    "    best_idxs = candidates[np.argsort(distances_to_mid)[:n_pick]]\n",
    "    chosen = rng.choice(best_idxs)\n",
    "    return pool_queries[chosen], float(sims[chosen])\n",
    "\n",
    "\n",
    "def build_bare_cache_no_framing(passage, model, tokenizer, config):\n",
    "    \"\"\"Build bare document cache WITHOUT 'Document:\\n' framing.\n",
    "    \n",
    "    Exp 06 confirmed bare passage (no framing) is the correct baseline.\n",
    "    \"\"\"\n",
    "    length, cache = build_kv_cache(passage, model, tokenizer, config)\n",
    "    return length, _ensure_dynamic_cache(cache)\n",
    "\n",
    "\n",
    "def build_truncated_cache_from_prefix(\n",
    "    prefix_text: str,\n",
    "    passage: str,\n",
    "    model,\n",
    "    tokenizer,\n",
    "    config,\n",
    ") -> Tuple[int, DynamicCache]:\n",
    "    \"\"\"Build a truncated+corrected cache from arbitrary prefix text.\n",
    "    \n",
    "    Uses the passage directly (no 'Document:\\n' framing) to match\n",
    "    the bare baseline. Handles BPE boundary matching.\n",
    "    \n",
    "    Returns: (keep_len, corrected_cache)\n",
    "    \"\"\"\n",
    "    prefix_with_sep = prefix_text.strip() + \" \"\n",
    "    \n",
    "    # Tokenize prefix with BOS to get exact prefix length\n",
    "    prefix_encoding = tokenizer(\n",
    "        prefix_with_sep, return_tensors=\"pt\", add_special_tokens=True,\n",
    "        padding=False, truncation=False\n",
    "    )\n",
    "    prefix_len = prefix_encoding['input_ids'].shape[1]\n",
    "    \n",
    "    # Tokenize full context\n",
    "    full_context = prefix_with_sep + passage\n",
    "    full_encoding = tokenizer(\n",
    "        full_context, return_tensors=\"pt\", add_special_tokens=True,\n",
    "        padding=False, truncation=False\n",
    "    )\n",
    "    full_ids = full_encoding['input_ids'].to(config.device)\n",
    "    doc_len = full_ids.shape[1] - prefix_len\n",
    "    \n",
    "    # Generate full KV cache\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            input_ids=full_ids,\n",
    "            attention_mask=torch.ones_like(full_ids),\n",
    "            use_cache=True,\n",
    "            return_dict=True\n",
    "        )\n",
    "    \n",
    "    # Truncate: BOS + document portion\n",
    "    truncated = extract_and_truncate_cache_with_bos(\n",
    "        outputs.past_key_values, doc_len\n",
    "    )\n",
    "    keep_len = 1 + doc_len\n",
    "    \n",
    "    # RoPE correction\n",
    "    surrogate_offset = prefix_len - 1\n",
    "    correct_rope_positions_with_bos(truncated, surrogate_offset, model)\n",
    "    \n",
    "    return keep_len, truncated\n",
    "\n",
    "\n",
    "def build_matched_bare_and_truncated(\n",
    "    prefix_text: str,\n",
    "    passage: str,\n",
    "    model,\n",
    "    tokenizer,\n",
    "    config,\n",
    ") -> Tuple[int, DynamicCache, int, DynamicCache]:\n",
    "    \"\"\"Build BPE-matched bare and truncated caches.\n",
    "    \n",
    "    Ensures both caches see identical document tokens by extracting\n",
    "    the document token IDs from the full [prefix + passage] encoding\n",
    "    and building the bare cache from those exact IDs.\n",
    "    \n",
    "    No 'Document:\\n' framing (bare passage only).\n",
    "    \n",
    "    Returns: (bare_len, bare_cache, trunc_len, trunc_cache)\n",
    "    \"\"\"\n",
    "    prefix_with_sep = prefix_text.strip() + \" \"\n",
    "    \n",
    "    # Tokenize prefix with BOS\n",
    "    prefix_encoding = tokenizer(\n",
    "        prefix_with_sep, return_tensors=\"pt\", add_special_tokens=True,\n",
    "        padding=False, truncation=False\n",
    "    )\n",
    "    prefix_len = prefix_encoding['input_ids'].shape[1]\n",
    "    \n",
    "    # Tokenize full context\n",
    "    full_context = prefix_with_sep + passage\n",
    "    full_encoding = tokenizer(\n",
    "        full_context, return_tensors=\"pt\", add_special_tokens=True,\n",
    "        padding=False, truncation=False\n",
    "    )\n",
    "    full_ids = full_encoding['input_ids'].to(config.device)\n",
    "    doc_len = full_ids.shape[1] - prefix_len\n",
    "    \n",
    "    # Extract exact document token IDs\n",
    "    doc_token_ids = full_ids[:, prefix_len:]  # (1, doc_len)\n",
    "    bos_id = full_ids[:, :1]\n",
    "    bare_ids = torch.cat([bos_id, doc_token_ids], dim=1)  # (1, 1+doc_len)\n",
    "    bare_len = bare_ids.shape[1]\n",
    "    \n",
    "    # Build bare cache\n",
    "    with torch.no_grad():\n",
    "        bare_out = model(\n",
    "            input_ids=bare_ids,\n",
    "            attention_mask=torch.ones_like(bare_ids),\n",
    "            use_cache=True,\n",
    "            return_dict=True\n",
    "        )\n",
    "    bare_cache = _ensure_dynamic_cache(bare_out.past_key_values)\n",
    "    \n",
    "    # Build truncated cache from full context\n",
    "    with torch.no_grad():\n",
    "        full_out = model(\n",
    "            input_ids=full_ids,\n",
    "            attention_mask=torch.ones_like(full_ids),\n",
    "            use_cache=True,\n",
    "            return_dict=True\n",
    "        )\n",
    "    \n",
    "    truncated = extract_and_truncate_cache_with_bos(\n",
    "        full_out.past_key_values, doc_len\n",
    "    )\n",
    "    keep_len = 1 + doc_len\n",
    "    \n",
    "    assert bare_len == keep_len, f\"Length mismatch: {bare_len} vs {keep_len}\"\n",
    "    \n",
    "    # RoPE correction\n",
    "    surrogate_offset = prefix_len - 1\n",
    "    correct_rope_positions_with_bos(truncated, surrogate_offset, model)\n",
    "    \n",
    "    return bare_len, bare_cache, keep_len, truncated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Investigation A: Surrogate Quality Gradient\n",
    "# ============================================================\n",
    "#\n",
    "# For each sample, we find real queries at 5 similarity levels,\n",
    "# build truncated+corrected caches, and compare against bare.\n",
    "#\n",
    "# Conditions:\n",
    "#   1. bare       — no prefix, no framing\n",
    "#   2. oracle     — actual target query as prefix\n",
    "#   3-7. sim bins — real queries at varying similarity levels\n",
    "#   8. irrelevant — a real query with sim < 0.1 (random baseline)\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"INVESTIGATION A: SURROGATE QUALITY GRADIENT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "samples_a = all_samples[:N_INVESTIGATION_A]\n",
    "\n",
    "# Pre-embed all target queries\n",
    "print(\"Embedding target queries...\")\n",
    "target_queries_a = [s['query'] for s in samples_a]\n",
    "target_embeddings_a = embed_model.encode(target_queries_a, batch_size=128)\n",
    "print(f\"Embedded {len(target_queries_a)} target queries\")\n",
    "\n",
    "# Pre-select surrogates for each sample at each similarity level\n",
    "print(\"\\nSelecting surrogates at each similarity level...\")\n",
    "rng = np.random.RandomState(config.seed)\n",
    "\n",
    "sample_surrogates = []  # list of dicts: {bin_name: (query, sim), ...}\n",
    "skipped_bins = {b[2]: 0 for b in SIMILARITY_BINS}\n",
    "\n",
    "for i in tqdm(range(len(samples_a)), desc=\"Selecting surrogates\"):\n",
    "    surr = {}\n",
    "    for sim_low, sim_high, bin_name in SIMILARITY_BINS:\n",
    "        result = find_surrogate_at_similarity(\n",
    "            target_queries_a[i], target_embeddings_a[i],\n",
    "            sim_low, sim_high,\n",
    "            query_pool, pool_embeddings, rng\n",
    "        )\n",
    "        if result is not None:\n",
    "            surr[bin_name] = result\n",
    "        else:\n",
    "            skipped_bins[bin_name] += 1\n",
    "    sample_surrogates.append(surr)\n",
    "\n",
    "print(\"\\nSurrogate coverage per bin:\")\n",
    "for sim_low, sim_high, bin_name in SIMILARITY_BINS:\n",
    "    n_found = N_INVESTIGATION_A - skipped_bins[bin_name]\n",
    "    print(f\"  {bin_name} ({sim_low:.2f}-{sim_high:.2f}): {n_found}/{N_INVESTIGATION_A} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# Investigation A: Run Evaluation\n# ============================================================\n# FIX 11.1: Use build_matched_bare_and_truncated for BPE-matched\n# comparison. Oracle prefix defines the reference bare cache.\n# Per-bin conditions also use matched pairs for correctness.\n# ============================================================\n\nresults_a = []\nskipped_a = 0\nerrors_a = 0\nstart_a = time.time()\n\nCHECKPOINT_PATH_A = 'results/exp11/11_checkpoint_a.json'\nstart_idx_a = 0\nif os.path.exists(CHECKPOINT_PATH_A):\n    with open(CHECKPOINT_PATH_A) as f:\n        ckpt = json.load(f)\n    results_a = ckpt['results']\n    skipped_a = ckpt['skipped']\n    errors_a = ckpt['errors']\n    start_idx_a = ckpt['next_idx']\n    print(f\"Resumed from checkpoint: {len(results_a)} results, starting at idx {start_idx_a}\")\n\nfor idx in tqdm(range(start_idx_a, len(samples_a)), desc=\"Inv A\",\n                initial=start_idx_a, total=len(samples_a)):\n    sample = samples_a[idx]\n    passage = sample['passage']\n    query = sample['query']\n    answer = sample['answer']\n    \n    # Skip short answers\n    answer_ids = tokenizer(answer, return_tensors='pt', add_special_tokens=False)['input_ids']\n    if answer_ids.shape[1] < 2:\n        skipped_a += 1\n        continue\n    \n    query_prompt = config.query_template.format(query=query)\n    surrogates = sample_surrogates[idx]\n    \n    try:\n        result = {'idx': idx, 'query': query}\n        \n        # --- Oracle (also provides BPE-matched bare baseline) ---\n        oracle_prefix = f\"This document answers: {query}\"\n        bare_len, bare_cache, oracle_len, oracle_cache = build_matched_bare_and_truncated(\n            oracle_prefix, passage, model, tokenizer, config\n        )\n        \n        nll_bare = score_answer_with_cache(\n            bare_cache, bare_len, query_prompt, answer,\n            model, tokenizer, config\n        )\n        result['nll_bare'] = nll_bare\n        \n        nll_oracle = score_answer_with_cache(\n            oracle_cache, oracle_len, query_prompt, answer,\n            model, tokenizer, config\n        )\n        result['nll_oracle'] = nll_oracle\n        \n        # --- Similarity bins: matched pairs for each ---\n        for sim_low, sim_high, bin_name in SIMILARITY_BINS:\n            if bin_name in surrogates:\n                surr_query, surr_sim = surrogates[bin_name]\n                surr_prefix = f\"This document answers: {surr_query}\"\n                _, _, surr_len, surr_cache = build_matched_bare_and_truncated(\n                    surr_prefix, passage, model, tokenizer, config\n                )\n                nll_surr = score_answer_with_cache(\n                    surr_cache, surr_len, query_prompt, answer,\n                    model, tokenizer, config\n                )\n                result[f'nll_{bin_name}'] = nll_surr\n                result[f'sim_{bin_name}'] = surr_sim\n                result[f'surr_{bin_name}'] = surr_query\n            else:\n                result[f'nll_{bin_name}'] = None\n                result[f'sim_{bin_name}'] = None\n        \n        results_a.append(result)\n        \n    except Exception as e:\n        errors_a += 1\n        if errors_a <= 5:\n            print(f\"\\n  Error on sample {idx}: {e}\")\n        continue\n    finally:\n        torch.cuda.empty_cache()\n    \n    # Checkpoint\n    if len(results_a) % 25 == 0:\n        with open(CHECKPOINT_PATH_A, 'w') as f:\n            json.dump({\n                'results': results_a, 'skipped': skipped_a,\n                'errors': errors_a, 'next_idx': idx + 1\n            }, f)\n        elapsed = time.time() - start_a\n        rate = len(results_a) / (elapsed / 60)\n        print(f\"\\n  [{len(results_a)} done | {elapsed/60:.0f}m | {rate:.1f} samples/min]\")\n\nelapsed_a = time.time() - start_a\nprint(f\"\\nDone. {len(results_a)} evaluated, {skipped_a} skipped, {errors_a} errors.\")\nprint(f\"Time: {elapsed_a/60:.1f} min\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Investigation A: Analysis\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"INVESTIGATION A RESULTS: SURROGATE QUALITY GRADIENT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "bare_nlls_a = np.array([r['nll_bare'] for r in results_a])\n",
    "oracle_nlls_a = np.array([r['nll_oracle'] for r in results_a])\n",
    "\n",
    "# Oracle stats\n",
    "oracle_deltas = bare_nlls_a - oracle_nlls_a\n",
    "oracle_wr = np.mean(oracle_deltas > 0) * 100\n",
    "t_oracle, p_oracle = stats.ttest_rel(bare_nlls_a, oracle_nlls_a)\n",
    "d_oracle = np.mean(oracle_deltas) / np.std(oracle_deltas, ddof=1)\n",
    "\n",
    "print(f\"\\n{'Condition':<25} {'N':>5} {'Mean NLL':>10} {'Win%':>8} {'Delta':>10} {'Cohen d':>10} {'p-value':>10}\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'Bare (baseline)':<25} {len(results_a):>5} {np.mean(bare_nlls_a):>10.4f} {'--':>8} {'--':>10} {'--':>10} {'--':>10}\")\n",
    "print(f\"{'Oracle':<25} {len(results_a):>5} {np.mean(oracle_nlls_a):>10.4f} {oracle_wr:>7.1f}% {np.mean(oracle_deltas):>+10.4f} {d_oracle:>10.3f} {p_oracle:>10.6f}\")\n",
    "\n",
    "# Per-bin stats\n",
    "bin_stats = {}\n",
    "for sim_low, sim_high, bin_name in SIMILARITY_BINS:\n",
    "    valid = [r for r in results_a if r.get(f'nll_{bin_name}') is not None]\n",
    "    if len(valid) < 10:\n",
    "        print(f\"{bin_name:<25} {len(valid):>5} -- insufficient data\")\n",
    "        continue\n",
    "    \n",
    "    nlls = np.array([r[f'nll_{bin_name}'] for r in valid])\n",
    "    bares = np.array([r['nll_bare'] for r in valid])\n",
    "    sims = np.array([r[f'sim_{bin_name}'] for r in valid])\n",
    "    deltas = bares - nlls\n",
    "    wr = np.mean(deltas > 0) * 100\n",
    "    t, p = stats.ttest_rel(bares, nlls)\n",
    "    d = np.mean(deltas) / np.std(deltas, ddof=1) if np.std(deltas) > 0 else 0\n",
    "    \n",
    "    bin_stats[bin_name] = {\n",
    "        'n': len(valid),\n",
    "        'mean_nll': float(np.mean(nlls)),\n",
    "        'mean_sim': float(np.mean(sims)),\n",
    "        'win_rate': float(wr),\n",
    "        'mean_delta': float(np.mean(deltas)),\n",
    "        'cohens_d': float(d),\n",
    "        'p_value': float(p),\n",
    "    }\n",
    "    \n",
    "    label = f\"{bin_name} ({np.mean(sims):.2f})\"\n",
    "    print(f\"{label:<25} {len(valid):>5} {np.mean(nlls):>10.4f} {wr:>7.1f}% {np.mean(deltas):>+10.4f} {d:>10.3f} {p:>10.6f}\")\n",
    "\n",
    "# Key test: does similarity predict delta?\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CRITICAL TEST: Does surrogate quality predict improvement?\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Aggregate all (sim, delta) pairs across bins\n",
    "all_sims = []\n",
    "all_deltas_a = []\n",
    "for r in results_a:\n",
    "    for _, _, bin_name in SIMILARITY_BINS:\n",
    "        if r.get(f'nll_{bin_name}') is not None and r.get(f'sim_{bin_name}') is not None:\n",
    "            all_sims.append(r[f'sim_{bin_name}'])\n",
    "            all_deltas_a.append(r['nll_bare'] - r[f'nll_{bin_name}'])\n",
    "\n",
    "all_sims = np.array(all_sims)\n",
    "all_deltas_a = np.array(all_deltas_a)\n",
    "\n",
    "r_pearson, p_pearson = stats.pearsonr(all_sims, all_deltas_a)\n",
    "r_spearman, p_spearman = stats.spearmanr(all_sims, all_deltas_a)\n",
    "\n",
    "print(f\"\\nTotal (sim, delta) pairs: {len(all_sims)}\")\n",
    "print(f\"Pearson r = {r_pearson:.4f}, p = {p_pearson:.6f}\")\n",
    "print(f\"Spearman rho = {r_spearman:.4f}, p = {p_spearman:.6f}\")\n",
    "\n",
    "if r_pearson > 0.1 and p_pearson < 0.01:\n",
    "    print(\"\\n>>> SEMANTIC SIGNAL DETECTED: Higher similarity surrogates produce better caches.\")\n",
    "    print(\"    This would be the first evidence of a genuine semantic priming effect.\")\n",
    "elif r_pearson > 0.05 and p_pearson < 0.05:\n",
    "    print(\"\\n>>> WEAK SIGNAL: Marginal positive correlation. May warrant larger sample.\")\n",
    "else:\n",
    "    print(\"\\n>>> NO SEMANTIC SIGNAL: Surrogate quality does not predict improvement.\")\n",
    "    print(\"    Consistent with prior experiments (Exp 06: r=0.924 with shuffled).\")\n",
    "\n",
    "# Monotonicity test: do the bin means increase with similarity?\n",
    "if len(bin_stats) >= 3:\n",
    "    bin_sims_ordered = [bin_stats[b]['mean_sim'] for b in [bn for _, _, bn in SIMILARITY_BINS] if b in bin_stats]\n",
    "    bin_deltas_ordered = [bin_stats[b]['mean_delta'] for b in [bn for _, _, bn in SIMILARITY_BINS] if b in bin_stats]\n",
    "    rho_bins, p_bins = stats.spearmanr(bin_sims_ordered, bin_deltas_ordered)\n",
    "    print(f\"\\nBin-level monotonicity (Spearman on bin means): rho={rho_bins:.3f}, p={p_bins:.4f}\")\n",
    "    if rho_bins > 0.8 and p_bins < 0.1:\n",
    "        print(\"  -> Clear monotonic trend: better surrogates = better caches\")\n",
    "    else:\n",
    "        print(\"  -> No clear monotonic trend across bins\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Investigation A: Visualization\n",
    "# ============================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Plot 1: Bar chart of win rates by condition\n",
    "ax = axes[0]\n",
    "conditions = ['bare']\n",
    "win_rates_plot = [50.0]\n",
    "for _, _, bn in SIMILARITY_BINS:\n",
    "    if bn in bin_stats:\n",
    "        conditions.append(f\"{bn}\\n({bin_stats[bn]['mean_sim']:.2f})\")\n",
    "        win_rates_plot.append(bin_stats[bn]['win_rate'])\n",
    "conditions.append('oracle')\n",
    "win_rates_plot.append(oracle_wr)\n",
    "\n",
    "colors = ['#888888'] + ['#4c72b0'] * len(bin_stats) + ['#c44e52']\n",
    "bars = ax.bar(range(len(conditions)), win_rates_plot, color=colors)\n",
    "ax.axhline(50, color='gray', linestyle='--', linewidth=0.8)\n",
    "ax.set_xticks(range(len(conditions)))\n",
    "ax.set_xticklabels(conditions, fontsize=7, rotation=30, ha='right')\n",
    "ax.set_ylabel('Win Rate vs Bare (%)')\n",
    "ax.set_title('Win Rate by Surrogate Quality')\n",
    "for i, wr in enumerate(win_rates_plot):\n",
    "    ax.text(i, wr + 1, f'{wr:.0f}%', ha='center', fontsize=7)\n",
    "\n",
    "# Plot 2: Scatter of similarity vs delta\n",
    "ax = axes[1]\n",
    "ax.scatter(all_sims, all_deltas_a, alpha=0.1, s=5, color='#4c72b0')\n",
    "# Bin means\n",
    "for _, _, bn in SIMILARITY_BINS:\n",
    "    if bn in bin_stats:\n",
    "        ax.scatter(bin_stats[bn]['mean_sim'], bin_stats[bn]['mean_delta'],\n",
    "                  s=100, color='#c44e52', zorder=5, edgecolor='black')\n",
    "ax.axhline(0, color='gray', linestyle='--', linewidth=0.8)\n",
    "# Regression line\n",
    "z = np.polyfit(all_sims, all_deltas_a, 1)\n",
    "p_fit = np.poly1d(z)\n",
    "x_line = np.linspace(all_sims.min(), all_sims.max(), 100)\n",
    "ax.plot(x_line, p_fit(x_line), 'r-', linewidth=2, label=f'r={r_pearson:.3f}')\n",
    "ax.set_xlabel('Surrogate-Query Similarity')\n",
    "ax.set_ylabel('Delta NLL (positive = priming helped)')\n",
    "ax.set_title(f'Similarity vs Improvement\\n(Pearson r={r_pearson:.3f}, p={p_pearson:.4f})')\n",
    "ax.legend()\n",
    "\n",
    "# Plot 3: Mean NLL by condition (gradient from bare to oracle)\n",
    "ax = axes[2]\n",
    "cond_labels = ['bare']\n",
    "cond_nlls = [np.mean(bare_nlls_a)]\n",
    "cond_errs = [np.std(bare_nlls_a) / np.sqrt(len(bare_nlls_a))]\n",
    "for _, _, bn in SIMILARITY_BINS:\n",
    "    if bn in bin_stats:\n",
    "        cond_labels.append(f\"{bn}\\n({bin_stats[bn]['mean_sim']:.2f})\")\n",
    "        cond_nlls.append(bin_stats[bn]['mean_nll'])\n",
    "        valid = [r for r in results_a if r.get(f'nll_{bn}') is not None]\n",
    "        cond_errs.append(np.std([r[f'nll_{bn}'] for r in valid]) / np.sqrt(len(valid)))\n",
    "cond_labels.append('oracle')\n",
    "cond_nlls.append(np.mean(oracle_nlls_a))\n",
    "cond_errs.append(np.std(oracle_nlls_a) / np.sqrt(len(oracle_nlls_a)))\n",
    "\n",
    "ax.errorbar(range(len(cond_labels)), cond_nlls, yerr=cond_errs,\n",
    "           fmt='o-', color='#4c72b0', capsize=3)\n",
    "ax.set_xticks(range(len(cond_labels)))\n",
    "ax.set_xticklabels(cond_labels, fontsize=7, rotation=30, ha='right')\n",
    "ax.set_ylabel('Mean NLL (lower = better)')\n",
    "ax.set_title('NLL Gradient from Bare to Oracle')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/exp11/11_investigation_a.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print('Saved: 11_investigation_a.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# Investigation B: Ranking Task\n# ============================================================\n#\n# For ad serving, what matters is RANKING: given a page and\n# multiple candidate ads (queries), does priming help rank the\n# relevant ad higher?\n#\n# Setup:\n#   - Each sample has 1 correct query + 4 distractor queries\n#   - Score each query using bare cache and primed cache\n#   - Measure MRR and Hit@1 for both conditions\n#   - Priming uses the ORACLE query (best case) and a medium-sim\n#     real query (realistic case)\n# ============================================================\n\n# Ensure query pool is available (built in cell-4)\nassert 'query_pool' in dir() and len(query_pool) > 0, (\n    \"query_pool not found. Please run cell-4 (Load Dataset and Build Query Pool) first.\"\n)\nassert 'pool_embeddings' in dir(), (\n    \"pool_embeddings not found. Please run cell-4 first.\"\n)\n\nprint(\"=\"*80)\nprint(\"INVESTIGATION B: RANKING TASK\")\nprint(\"=\"*80)\n\nsamples_b = all_samples[:N_INVESTIGATION_B]\n\n# For each sample, select 4 distractor queries\n# Distractors should be from different topics (low similarity to target)\nprint(\"Selecting distractor queries...\")\ntarget_queries_b = [s['query'] for s in samples_b]\ntarget_embeddings_b = embed_model.encode(target_queries_b, batch_size=128)\n\nrng_b = np.random.RandomState(config.seed + 100)\n\nsample_distractors = []  # list of [query, ...] x 4\nfor i in tqdm(range(len(samples_b)), desc=\"Selecting distractors\"):\n    sims = cosine_similarity([target_embeddings_b[i]], pool_embeddings)[0]\n    \n    # Pick 4 distractors with varying similarity:\n    # 2 low similarity (0.1-0.3) and 2 medium similarity (0.3-0.6)\n    # This makes the ranking task non-trivial\n    distractors = []\n    for sim_lo, sim_hi, n_pick in [(0.1, 0.3, 2), (0.3, 0.6, 2)]:\n        mask = (sims >= sim_lo) & (sims < sim_hi)\n        candidates = np.where(mask)[0]\n        if len(candidates) >= n_pick:\n            chosen = rng_b.choice(candidates, size=n_pick, replace=False)\n            for c in chosen:\n                distractors.append(query_pool[c])\n        else:\n            # Fall back to whatever is available\n            for c in candidates[:n_pick]:\n                distractors.append(query_pool[c])\n    \n    # Pad if we don't have enough\n    while len(distractors) < 4:\n        rand_idx = rng_b.randint(0, len(query_pool))\n        distractors.append(query_pool[rand_idx])\n    \n    sample_distractors.append(distractors[:4])\n\nprint(f\"Selected distractors for {len(sample_distractors)} samples\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# Investigation B: Run Ranking Evaluation\n# ============================================================\n# FIX 11.1: Use build_matched_bare_and_truncated for BPE matching\n# FIX 11.2: Deep-copy cache before each score_answer_with_cache call\n#           to prevent cache mutation across queries\n# ============================================================\n\nresults_b = []\nskipped_b = 0\nerrors_b = 0\nstart_b = time.time()\n\nCHECKPOINT_PATH_B = 'results/exp11/11_checkpoint_b.json'\nstart_idx_b = 0\nif os.path.exists(CHECKPOINT_PATH_B):\n    with open(CHECKPOINT_PATH_B) as f:\n        ckpt = json.load(f)\n    results_b = ckpt['results']\n    skipped_b = ckpt['skipped']\n    errors_b = ckpt['errors']\n    start_idx_b = ckpt['next_idx']\n    print(f\"Resumed from checkpoint: {len(results_b)} results\")\n\nfor idx in tqdm(range(start_idx_b, len(samples_b)), desc=\"Inv B\",\n                initial=start_idx_b, total=len(samples_b)):\n    sample = samples_b[idx]\n    passage = sample['passage']\n    query = sample['query']\n    answer = sample['answer']\n    distractors = sample_distractors[idx]\n    \n    answer_ids = tokenizer(answer, return_tensors='pt', add_special_tokens=False)['input_ids']\n    if answer_ids.shape[1] < 2:\n        skipped_b += 1\n        continue\n    \n    # All candidate queries (correct first, then distractors)\n    all_queries = [query] + distractors\n    \n    try:\n        result = {'idx': idx}\n        \n        # Build BPE-matched caches using oracle prefix\n        oracle_prefix = f\"This document answers: {query}\"\n        bare_len, bare_cache, oracle_len, oracle_cache = build_matched_bare_and_truncated(\n            oracle_prefix, passage, model, tokenizer, config\n        )\n        \n        # Find a medium-similarity real query for this sample\n        surrogates_for_b = sample_surrogates[idx] if idx < len(sample_surrogates) else {}\n        medium_surr = None\n        for bn in ['medium', 'high', 'low']:  # prefer medium, fall back\n            if bn in surrogates_for_b:\n                medium_surr = surrogates_for_b[bn][0]\n                break\n        \n        medium_cache = None\n        medium_len = None\n        if medium_surr:\n            medium_prefix = f\"This document answers: {medium_surr}\"\n            _, _, medium_len, medium_cache = build_matched_bare_and_truncated(\n                medium_prefix, passage, model, tokenizer, config\n            )\n        \n        # Score each candidate query under each cache condition\n        # CRITICAL: deep-copy cache before each call to prevent mutation\n        bare_scores = []\n        oracle_scores = []\n        medium_scores = []\n        \n        for q in all_queries:\n            q_prompt = config.query_template.format(query=q)\n            \n            nll_b = score_answer_with_cache(\n                copy.deepcopy(bare_cache), bare_len, q_prompt, answer,\n                model, tokenizer, config\n            )\n            bare_scores.append(nll_b)\n            \n            nll_o = score_answer_with_cache(\n                copy.deepcopy(oracle_cache), oracle_len, q_prompt, answer,\n                model, tokenizer, config\n            )\n            oracle_scores.append(nll_o)\n            \n            if medium_cache is not None:\n                nll_m = score_answer_with_cache(\n                    copy.deepcopy(medium_cache), medium_len, q_prompt, answer,\n                    model, tokenizer, config\n                )\n                medium_scores.append(nll_m)\n        \n        # Compute rankings (rank by ascending NLL — lower = better match)\n        bare_rank = int(np.argsort(bare_scores).tolist().index(0)) + 1\n        oracle_rank = int(np.argsort(oracle_scores).tolist().index(0)) + 1\n        medium_rank = int(np.argsort(medium_scores).tolist().index(0)) + 1 if medium_scores else None\n        \n        result['bare_scores'] = bare_scores\n        result['oracle_scores'] = oracle_scores\n        result['medium_scores'] = medium_scores\n        result['bare_rank'] = bare_rank\n        result['oracle_rank'] = oracle_rank\n        result['medium_rank'] = medium_rank\n        result['has_medium'] = medium_cache is not None\n        \n        results_b.append(result)\n        \n    except Exception as e:\n        errors_b += 1\n        if errors_b <= 5:\n            print(f\"\\n  Error on sample {idx}: {e}\")\n        continue\n    finally:\n        torch.cuda.empty_cache()\n    \n    if len(results_b) % 25 == 0:\n        with open(CHECKPOINT_PATH_B, 'w') as f:\n            json.dump({\n                'results': results_b, 'skipped': skipped_b,\n                'errors': errors_b, 'next_idx': idx + 1\n            }, f)\n        elapsed = time.time() - start_b\n        print(f\"\\n  [{len(results_b)} done | {elapsed/60:.0f}m]\")\n\nelapsed_b = time.time() - start_b\nprint(f\"\\nDone. {len(results_b)} evaluated, {skipped_b} skipped, {errors_b} errors.\")\nprint(f\"Time: {elapsed_b/60:.1f} min\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Investigation B: Analysis\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"INVESTIGATION B RESULTS: RANKING TASK\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "n_b = len(results_b)\n",
    "\n",
    "# MRR and Hit@1\n",
    "bare_ranks = np.array([r['bare_rank'] for r in results_b])\n",
    "oracle_ranks = np.array([r['oracle_rank'] for r in results_b])\n",
    "medium_results = [r for r in results_b if r['has_medium']]\n",
    "medium_ranks = np.array([r['medium_rank'] for r in medium_results]) if medium_results else np.array([])\n",
    "\n",
    "bare_mrr = np.mean(1.0 / bare_ranks)\n",
    "bare_hit1 = np.mean(bare_ranks == 1)\n",
    "\n",
    "oracle_mrr = np.mean(1.0 / oracle_ranks)\n",
    "oracle_hit1 = np.mean(oracle_ranks == 1)\n",
    "\n",
    "print(f\"\\n{'Condition':<20} {'N':>5} {'MRR':>8} {'Hit@1':>8} {'Hit@3':>8} {'Mean Rank':>10}\")\n",
    "print(\"-\" * 65)\n",
    "print(f\"{'Bare':<20} {n_b:>5} {bare_mrr:>8.3f} {bare_hit1:>7.1%} {np.mean(bare_ranks <= 3):>7.1%} {np.mean(bare_ranks):>10.2f}\")\n",
    "print(f\"{'Oracle primed':<20} {n_b:>5} {oracle_mrr:>8.3f} {oracle_hit1:>7.1%} {np.mean(oracle_ranks <= 3):>7.1%} {np.mean(oracle_ranks):>10.2f}\")\n",
    "\n",
    "if len(medium_ranks) > 0:\n",
    "    medium_mrr = np.mean(1.0 / medium_ranks)\n",
    "    medium_hit1 = np.mean(medium_ranks == 1)\n",
    "    print(f\"{'Medium primed':<20} {len(medium_results):>5} {medium_mrr:>8.3f} {medium_hit1:>7.1%} {np.mean(medium_ranks <= 3):>7.1%} {np.mean(medium_ranks):>10.2f}\")\n",
    "\n",
    "# Statistical tests\n",
    "print(\"\\nStatistical tests (Wilcoxon signed-rank on ranks):\")\n",
    "# Bare vs Oracle\n",
    "if not np.all(bare_ranks == oracle_ranks):\n",
    "    stat_bo, p_bo = stats.wilcoxon(bare_ranks, oracle_ranks)\n",
    "    print(f\"  Bare vs Oracle: W={stat_bo:.0f}, p={p_bo:.6f}\")\n",
    "else:\n",
    "    print(f\"  Bare vs Oracle: identical rankings\")\n",
    "\n",
    "# Bare vs Medium\n",
    "if len(medium_ranks) > 0 and not np.all(bare_ranks[:len(medium_ranks)] == medium_ranks):\n",
    "    bare_sub = np.array([r['bare_rank'] for r in medium_results])\n",
    "    stat_bm, p_bm = stats.wilcoxon(bare_sub, medium_ranks)\n",
    "    print(f\"  Bare vs Medium: W={stat_bm:.0f}, p={p_bm:.6f}\")\n",
    "\n",
    "# Interpretation\n",
    "print(\"\\nInterpretation:\")\n",
    "if oracle_mrr > bare_mrr + 0.05:\n",
    "    print(f\"  Oracle priming improves ranking (MRR: {bare_mrr:.3f} -> {oracle_mrr:.3f})\")\n",
    "    if len(medium_ranks) > 0 and medium_mrr > bare_mrr + 0.02:\n",
    "        print(f\"  Medium-sim surrogates also help ranking (MRR: {medium_mrr:.3f})\")\n",
    "        print(\"  -> Practical benefit for ad serving with historical query surrogates\")\n",
    "    else:\n",
    "        print(\"  But medium-sim surrogates do NOT improve ranking.\")\n",
    "        print(\"  -> Oracle effect does not transfer to realistic surrogates.\")\n",
    "else:\n",
    "    print(f\"  No ranking improvement from priming (MRR bare={bare_mrr:.3f}, oracle={oracle_mrr:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Investigation B: Visualization\n",
    "# ============================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# Plot 1: Rank distribution\n",
    "ax = axes[0]\n",
    "rank_labels = [1, 2, 3, 4, 5]\n",
    "bare_dist = [np.sum(bare_ranks == r) / n_b * 100 for r in rank_labels]\n",
    "oracle_dist = [np.sum(oracle_ranks == r) / n_b * 100 for r in rank_labels]\n",
    "x = np.arange(len(rank_labels))\n",
    "w = 0.35\n",
    "ax.bar(x - w/2, bare_dist, w, label='Bare', color='#4c72b0')\n",
    "ax.bar(x + w/2, oracle_dist, w, label='Oracle', color='#c44e52')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(rank_labels)\n",
    "ax.set_xlabel('Rank of Correct Query')\n",
    "ax.set_ylabel('% of Samples')\n",
    "ax.set_title('Rank Distribution')\n",
    "ax.legend()\n",
    "\n",
    "# Plot 2: MRR comparison\n",
    "ax = axes[1]\n",
    "conditions_mrr = ['Bare', 'Oracle']\n",
    "mrrs = [bare_mrr, oracle_mrr]\n",
    "if len(medium_ranks) > 0:\n",
    "    conditions_mrr.append('Medium')\n",
    "    mrrs.append(medium_mrr)\n",
    "colors_mrr = ['#4c72b0', '#c44e52', '#55a868'][:len(conditions_mrr)]\n",
    "ax.bar(range(len(conditions_mrr)), mrrs, color=colors_mrr)\n",
    "ax.set_xticks(range(len(conditions_mrr)))\n",
    "ax.set_xticklabels(conditions_mrr)\n",
    "ax.set_ylabel('Mean Reciprocal Rank')\n",
    "ax.set_title('MRR by Cache Condition')\n",
    "ax.set_ylim(0, 1)\n",
    "for i, m in enumerate(mrrs):\n",
    "    ax.text(i, m + 0.02, f'{m:.3f}', ha='center', fontsize=10)\n",
    "\n",
    "# Plot 3: Per-sample rank change (bare -> oracle)\n",
    "ax = axes[2]\n",
    "rank_change = bare_ranks - oracle_ranks  # positive = oracle improved ranking\n",
    "change_vals, change_counts = np.unique(rank_change, return_counts=True)\n",
    "colors_change = ['#c44e52' if v < 0 else '#55a868' if v > 0 else '#888888' for v in change_vals]\n",
    "ax.bar(change_vals, change_counts / n_b * 100, color=colors_change)\n",
    "ax.set_xlabel('Rank Change (positive = oracle better)')\n",
    "ax.set_ylabel('% of Samples')\n",
    "ax.set_title(f'Rank Change: Bare -> Oracle\\n(improved: {np.mean(rank_change > 0)*100:.0f}%, '\n",
    "             f'worse: {np.mean(rank_change < 0)*100:.0f}%)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/exp11/11_investigation_b.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print('Saved: 11_investigation_b.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# Investigation C: Same-Passage Surrogates\n# ============================================================\n# FIX 11.1: Use build_matched_bare_and_truncated for BPE matching\n# Each condition builds a matched pair; bare from oracle match\n# is the reference baseline.\n# ============================================================\n\n# Ensure prerequisites\nassert 'query_pool' in dir() and len(query_pool) > 0, (\n    \"query_pool not found. Please run cell-4 first.\"\n)\nassert 'inv_c_samples' in dir() and len(inv_c_samples) > 0, (\n    \"inv_c_samples not found. Please run cell-5 first.\"\n)\n\nprint(\"=\"*80)\nprint(\"INVESTIGATION C: SAME-PASSAGE SURROGATES\")\nprint(\"=\"*80)\n\nresults_c = []\nskipped_c = 0\nerrors_c = 0\nstart_c = time.time()\n\nCHECKPOINT_PATH_C = 'results/exp11/11_checkpoint_c.json'\nstart_idx_c = 0\nif os.path.exists(CHECKPOINT_PATH_C):\n    with open(CHECKPOINT_PATH_C) as f:\n        ckpt = json.load(f)\n    results_c = ckpt['results']\n    skipped_c = ckpt['skipped']\n    errors_c = ckpt['errors']\n    start_idx_c = ckpt['next_idx']\n    print(f\"Resumed from checkpoint: {len(results_c)} results\")\n\n# Pre-select irrelevant queries for control\nrng_c = np.random.RandomState(config.seed + 200)\nirrelevant_queries_c = [query_pool[rng_c.randint(0, len(query_pool))]\n                        for _ in range(len(inv_c_samples))]\n\nfor idx in tqdm(range(start_idx_c, len(inv_c_samples)), desc=\"Inv C\",\n                initial=start_idx_c, total=len(inv_c_samples)):\n    sample = inv_c_samples[idx]\n    passage = sample['passage']\n    surrogate_query = sample['surrogate_query']\n    test_query = sample['test_query']\n    test_answer = sample['test_answer']\n    surr_sim = sample['surrogate_similarity']\n    \n    answer_ids = tokenizer(test_answer, return_tensors='pt', add_special_tokens=False)['input_ids']\n    if answer_ids.shape[1] < 2:\n        skipped_c += 1\n        continue\n    \n    query_prompt = config.query_template.format(query=test_query)\n    \n    try:\n        result = {\n            'idx': idx,\n            'surrogate_query': surrogate_query,\n            'test_query': test_query,\n            'surrogate_similarity': surr_sim,\n        }\n        \n        # 1. Oracle (also provides BPE-matched bare baseline)\n        oracle_prefix = f\"This document answers: {test_query}\"\n        bare_len, bare_cache, oracle_len, oracle_cache = build_matched_bare_and_truncated(\n            oracle_prefix, passage, model, tokenizer, config\n        )\n        \n        nll_bare = score_answer_with_cache(\n            bare_cache, bare_len, query_prompt, test_answer,\n            model, tokenizer, config\n        )\n        result['nll_bare'] = nll_bare\n        \n        nll_oracle = score_answer_with_cache(\n            oracle_cache, oracle_len, query_prompt, test_answer,\n            model, tokenizer, config\n        )\n        result['nll_oracle'] = nll_oracle\n        \n        # 2. Same-passage primed\n        surr_prefix = f\"This document answers: {surrogate_query}\"\n        _, _, surr_len, surr_cache = build_matched_bare_and_truncated(\n            surr_prefix, passage, model, tokenizer, config\n        )\n        nll_surr = score_answer_with_cache(\n            surr_cache, surr_len, query_prompt, test_answer,\n            model, tokenizer, config\n        )\n        result['nll_same_passage'] = nll_surr\n        \n        # 3. Irrelevant control\n        irrel_prefix = f\"This document answers: {irrelevant_queries_c[idx]}\"\n        _, _, irrel_len, irrel_cache = build_matched_bare_and_truncated(\n            irrel_prefix, passage, model, tokenizer, config\n        )\n        nll_irrel = score_answer_with_cache(\n            irrel_cache, irrel_len, query_prompt, test_answer,\n            model, tokenizer, config\n        )\n        result['nll_irrelevant'] = nll_irrel\n        \n        results_c.append(result)\n        \n    except Exception as e:\n        errors_c += 1\n        if errors_c <= 5:\n            print(f\"\\n  Error on sample {idx}: {e}\")\n        continue\n    finally:\n        torch.cuda.empty_cache()\n    \n    if len(results_c) % 25 == 0:\n        with open(CHECKPOINT_PATH_C, 'w') as f:\n            json.dump({\n                'results': results_c, 'skipped': skipped_c,\n                'errors': errors_c, 'next_idx': idx + 1\n            }, f)\n        elapsed = time.time() - start_c\n        print(f\"\\n  [{len(results_c)} done | {elapsed/60:.0f}m]\")\n\nelapsed_c = time.time() - start_c\nprint(f\"\\nDone. {len(results_c)} evaluated, {skipped_c} skipped, {errors_c} errors.\")\nprint(f\"Time: {elapsed_c/60:.1f} min\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Investigation C: Analysis\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"INVESTIGATION C RESULTS: SAME-PASSAGE SURROGATES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "n_c = len(results_c)\n",
    "bare_nlls_c = np.array([r['nll_bare'] for r in results_c])\n",
    "surr_nlls_c = np.array([r['nll_same_passage'] for r in results_c])\n",
    "oracle_nlls_c = np.array([r['nll_oracle'] for r in results_c])\n",
    "irrel_nlls_c = np.array([r['nll_irrelevant'] for r in results_c])\n",
    "surr_sims_c = np.array([r['surrogate_similarity'] for r in results_c])\n",
    "\n",
    "conditions_c = [\n",
    "    ('Bare', bare_nlls_c),\n",
    "    ('Same-passage', surr_nlls_c),\n",
    "    ('Oracle', oracle_nlls_c),\n",
    "    ('Irrelevant', irrel_nlls_c),\n",
    "]\n",
    "\n",
    "print(f\"\\nSame-passage surrogate similarity: mean={np.mean(surr_sims_c):.3f}, \"\n",
    "      f\"std={np.std(surr_sims_c):.3f}\")\n",
    "\n",
    "print(f\"\\n{'Condition':<20} {'N':>5} {'Mean NLL':>10} {'Win% vs Bare':>14} {'Delta':>10} {'Cohen d':>10} {'p-value':>10}\")\n",
    "print(\"-\" * 85)\n",
    "\n",
    "for label, nlls in conditions_c:\n",
    "    if label == 'Bare':\n",
    "        print(f\"{label:<20} {n_c:>5} {np.mean(nlls):>10.4f} {'--':>14} {'--':>10} {'--':>10} {'--':>10}\")\n",
    "    else:\n",
    "        deltas = bare_nlls_c - nlls\n",
    "        wr = np.mean(deltas > 0) * 100\n",
    "        t, p = stats.ttest_rel(bare_nlls_c, nlls)\n",
    "        d = np.mean(deltas) / np.std(deltas, ddof=1) if np.std(deltas) > 0 else 0\n",
    "        print(f\"{label:<20} {n_c:>5} {np.mean(nlls):>10.4f} {wr:>13.1f}% {np.mean(deltas):>+10.4f} {d:>10.3f} {p:>10.6f}\")\n",
    "\n",
    "# Key comparison: same-passage vs irrelevant\n",
    "print(\"\\n--- Key Comparisons ---\")\n",
    "t_si, p_si = stats.ttest_rel(surr_nlls_c, irrel_nlls_c)\n",
    "surr_beats_irrel = np.mean(surr_nlls_c < irrel_nlls_c) * 100\n",
    "print(f\"Same-passage vs Irrelevant: t={t_si:.3f}, p={p_si:.4f}, \"\n",
    "      f\"same-passage wins {surr_beats_irrel:.1f}%\")\n",
    "\n",
    "if p_si < 0.05 and surr_beats_irrel > 55:\n",
    "    print(\"  >>> SEMANTIC SIGNAL: Same-passage surrogates significantly beat irrelevant ones.\")\n",
    "    print(\"      This means the content of the surrogate matters when it's a real query\")\n",
    "    print(\"      that was previously relevant to the same document.\")\n",
    "else:\n",
    "    print(\"  >>> NO SEMANTIC SIGNAL even with real same-passage queries.\")\n",
    "\n",
    "# Correlation: does surrogate-test similarity predict delta?\n",
    "delta_surr_c = bare_nlls_c - surr_nlls_c\n",
    "r_c, p_rc = stats.pearsonr(surr_sims_c, delta_surr_c)\n",
    "print(f\"\\nSimilarity-Delta correlation: r={r_c:.4f}, p={p_rc:.6f}\")\n",
    "if r_c > 0.1 and p_rc < 0.05:\n",
    "    print(\"  -> More similar surrogates produce better caches\")\n",
    "else:\n",
    "    print(\"  -> No correlation between surrogate quality and cache quality\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Investigation C: Visualization\n",
    "# ============================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# Plot 1: Bar chart of conditions\n",
    "ax = axes[0]\n",
    "labels = ['Bare', 'Irrelevant', 'Same-Passage', 'Oracle']\n",
    "nlls_means = [np.mean(bare_nlls_c), np.mean(irrel_nlls_c),\n",
    "              np.mean(surr_nlls_c), np.mean(oracle_nlls_c)]\n",
    "nlls_errs = [np.std(bare_nlls_c)/np.sqrt(n_c), np.std(irrel_nlls_c)/np.sqrt(n_c),\n",
    "             np.std(surr_nlls_c)/np.sqrt(n_c), np.std(oracle_nlls_c)/np.sqrt(n_c)]\n",
    "colors_c = ['#888888', '#8c564b', '#4c72b0', '#c44e52']\n",
    "ax.bar(range(len(labels)), nlls_means, yerr=nlls_errs,\n",
    "       color=colors_c, capsize=3)\n",
    "ax.set_xticks(range(len(labels)))\n",
    "ax.set_xticklabels(labels, fontsize=9)\n",
    "ax.set_ylabel('Mean NLL (lower = better)')\n",
    "ax.set_title('Same-Passage Surrogates')\n",
    "\n",
    "# Plot 2: Similarity vs Delta scatter\n",
    "ax = axes[1]\n",
    "ax.scatter(surr_sims_c, delta_surr_c, alpha=0.3, s=20, color='#4c72b0')\n",
    "ax.axhline(0, color='gray', linestyle='--', linewidth=0.8)\n",
    "z_c = np.polyfit(surr_sims_c, delta_surr_c, 1)\n",
    "p_c = np.poly1d(z_c)\n",
    "x_c = np.linspace(surr_sims_c.min(), surr_sims_c.max(), 100)\n",
    "ax.plot(x_c, p_c(x_c), 'r-', linewidth=2)\n",
    "ax.set_xlabel('Surrogate-Test Query Similarity')\n",
    "ax.set_ylabel('Delta NLL (positive = priming helped)')\n",
    "ax.set_title(f'Similarity vs Improvement\\n(r={r_c:.3f}, p={p_rc:.4f})')\n",
    "\n",
    "# Plot 3: Distribution of deltas\n",
    "ax = axes[2]\n",
    "ax.hist(delta_surr_c, bins=30, alpha=0.6, label='Same-Passage', color='#4c72b0')\n",
    "ax.hist(bare_nlls_c - irrel_nlls_c, bins=30, alpha=0.6, label='Irrelevant', color='#8c564b')\n",
    "ax.axvline(0, color='gray', linestyle='--')\n",
    "ax.set_xlabel('Delta NLL vs Bare')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('Delta Distribution')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/exp11/11_investigation_c.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print('Saved: 11_investigation_c.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Comprehensive Summary\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"EXPERIMENT 11: COMPREHENSIVE SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n--- Investigation A: Surrogate Quality Gradient ---\")\n",
    "print(f\"  Oracle win rate: {oracle_wr:.1f}% (ceiling)\")\n",
    "for _, _, bn in SIMILARITY_BINS:\n",
    "    if bn in bin_stats:\n",
    "        s = bin_stats[bn]\n",
    "        print(f\"  {bn} (sim ~{s['mean_sim']:.2f}): win%={s['win_rate']:.1f}%, d={s['cohens_d']:.3f}\")\n",
    "print(f\"  Similarity-Delta correlation: r={r_pearson:.4f}, p={p_pearson:.6f}\")\n",
    "\n",
    "print(\"\\n--- Investigation B: Ranking Task ---\")\n",
    "print(f\"  Bare MRR: {bare_mrr:.3f}, Hit@1: {bare_hit1:.1%}\")\n",
    "print(f\"  Oracle MRR: {oracle_mrr:.3f}, Hit@1: {oracle_hit1:.1%}\")\n",
    "if len(medium_ranks) > 0:\n",
    "    print(f\"  Medium MRR: {medium_mrr:.3f}, Hit@1: {medium_hit1:.1%}\")\n",
    "\n",
    "print(\"\\n--- Investigation C: Same-Passage Surrogates ---\")\n",
    "print(f\"  Same-passage vs Bare: win%={np.mean(bare_nlls_c > surr_nlls_c)*100:.1f}%\")\n",
    "print(f\"  Same-passage vs Irrelevant: win%={surr_beats_irrel:.1f}% (p={p_si:.4f})\")\n",
    "print(f\"  Similarity-Delta correlation: r={r_c:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"OVERALL VERDICT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Determine overall conclusion\n",
    "semantic_signal_a = r_pearson > 0.1 and p_pearson < 0.01\n",
    "semantic_signal_c = p_si < 0.05 and surr_beats_irrel > 55\n",
    "ranking_benefit = oracle_mrr > bare_mrr + 0.05\n",
    "\n",
    "if semantic_signal_a and semantic_signal_c:\n",
    "    print(\"\\n  SEMANTIC PRIMING EFFECT CONFIRMED\")\n",
    "    print(\"  Higher-quality surrogates produce better KV caches.\")\n",
    "    print(\"  The effect was previously masked by low-quality generated surrogates.\")\n",
    "    if len(medium_ranks) > 0 and medium_mrr > bare_mrr + 0.02:\n",
    "        print(\"  The effect transfers to ranking, making it practically useful.\")\n",
    "    else:\n",
    "        print(\"  However, the effect may not be large enough for practical ranking benefit.\")\n",
    "elif semantic_signal_a or semantic_signal_c:\n",
    "    print(\"\\n  PARTIAL SEMANTIC SIGNAL\")\n",
    "    print(\"  Evidence is mixed across investigations.\")\n",
    "    if semantic_signal_a:\n",
    "        print(\"  Inv A (quality gradient) shows a signal.\")\n",
    "    if semantic_signal_c:\n",
    "        print(\"  Inv C (same-passage) shows a signal.\")\n",
    "else:\n",
    "    print(\"\\n  NO SEMANTIC PRIMING EFFECT\")\n",
    "    print(\"  Even with real queries at high similarity, no semantic signal emerges.\")\n",
    "    print(\"  The KV cache mechanism fundamentally cannot transmit semantic\")\n",
    "    print(\"  information through value contamination during truncated caching.\")\n",
    "    print(\"\")\n",
    "    print(\"  The oracle effect works because the query-as-prefix directly\")\n",
    "    print(\"  conditions the model's representations, but this information is\")\n",
    "    print(\"  lost when truncating back to just the document cache.\")\n",
    "\n",
    "if ranking_benefit:\n",
    "    print(\"\\n  RANKING NOTE: Oracle priming does improve ranking,\")\n",
    "    print(\"  confirming the mechanism works in principle for ad serving.\")\n",
    "    print(\"  The challenge remains: practical surrogates are not close enough to oracle.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Save All Results\n",
    "# ============================================================\n",
    "\n",
    "output = {\n",
    "    'metadata': {\n",
    "        'experiment': '11_surrogate_quality_gradient',\n",
    "        'timestamp': datetime.datetime.now().isoformat(),\n",
    "        'model_name': config.model_name,\n",
    "        'seed': config.seed,\n",
    "        'n_investigation_a': N_INVESTIGATION_A,\n",
    "        'n_investigation_b': N_INVESTIGATION_B,\n",
    "        'n_investigation_c': N_INVESTIGATION_C,\n",
    "    },\n",
    "    'investigation_a': {\n",
    "        'n_evaluated': len(results_a),\n",
    "        'n_skipped': skipped_a,\n",
    "        'n_errors': errors_a,\n",
    "        'bin_stats': bin_stats,\n",
    "        'oracle_win_rate': float(oracle_wr),\n",
    "        'pearson_r': float(r_pearson),\n",
    "        'pearson_p': float(p_pearson),\n",
    "        'spearman_rho': float(r_spearman),\n",
    "        'spearman_p': float(p_spearman),\n",
    "        'results': results_a,\n",
    "    },\n",
    "    'investigation_b': {\n",
    "        'n_evaluated': len(results_b),\n",
    "        'n_skipped': skipped_b,\n",
    "        'n_errors': errors_b,\n",
    "        'bare_mrr': float(bare_mrr),\n",
    "        'oracle_mrr': float(oracle_mrr),\n",
    "        'bare_hit1': float(bare_hit1),\n",
    "        'oracle_hit1': float(oracle_hit1),\n",
    "        'medium_mrr': float(medium_mrr) if len(medium_ranks) > 0 else None,\n",
    "        'medium_hit1': float(medium_hit1) if len(medium_ranks) > 0 else None,\n",
    "        'results': results_b,\n",
    "    },\n",
    "    'investigation_c': {\n",
    "        'n_evaluated': len(results_c),\n",
    "        'n_skipped': skipped_c,\n",
    "        'n_errors': errors_c,\n",
    "        'same_passage_vs_irrelevant_p': float(p_si),\n",
    "        'same_passage_beats_irrelevant': float(surr_beats_irrel),\n",
    "        'similarity_delta_r': float(r_c),\n",
    "        'similarity_delta_p': float(p_rc),\n",
    "        'results': results_c,\n",
    "    },\n",
    "    'semantic_signal_a': bool(semantic_signal_a),\n",
    "    'semantic_signal_c': bool(semantic_signal_c),\n",
    "    'ranking_benefit': bool(ranking_benefit),\n",
    "}\n",
    "\n",
    "output_path = 'results/exp11/11_results.json'\n",
    "with open(output_path, 'w') as f:\n",
    "    json.dump(output, f, indent=2, default=str)\n",
    "print(f\"Results saved to {output_path}\")\n",
    "print(f\"File size: {os.path.getsize(output_path) / 1e6:.1f} MB\")\n",
    "\n",
    "# Summary visualization\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# A: Quality gradient\n",
    "ax = axes[0]\n",
    "sims_plot = [0.0]  # bare\n",
    "deltas_plot = [0.0]\n",
    "for _, _, bn in SIMILARITY_BINS:\n",
    "    if bn in bin_stats:\n",
    "        sims_plot.append(bin_stats[bn]['mean_sim'])\n",
    "        deltas_plot.append(bin_stats[bn]['mean_delta'])\n",
    "sims_plot.append(1.0)\n",
    "deltas_plot.append(float(np.mean(oracle_deltas)))\n",
    "ax.plot(sims_plot, deltas_plot, 'o-', color='#4c72b0', linewidth=2, markersize=8)\n",
    "ax.axhline(0, color='gray', linestyle='--', linewidth=0.8)\n",
    "ax.set_xlabel('Surrogate Similarity to Target Query')\n",
    "ax.set_ylabel('Mean Delta NLL (positive = better)')\n",
    "ax.set_title('A: Quality Gradient\\n(0=bare, 1=oracle)')\n",
    "\n",
    "# B: Ranking\n",
    "ax = axes[1]\n",
    "conds_plot = ['Bare', 'Oracle']\n",
    "mrrs_plot = [bare_mrr, oracle_mrr]\n",
    "if len(medium_ranks) > 0:\n",
    "    conds_plot.insert(1, 'Medium')\n",
    "    mrrs_plot.insert(1, medium_mrr)\n",
    "ax.bar(range(len(conds_plot)), mrrs_plot, color=['#888888', '#55a868', '#c44e52'][:len(conds_plot)])\n",
    "ax.set_xticks(range(len(conds_plot)))\n",
    "ax.set_xticklabels(conds_plot)\n",
    "ax.set_ylabel('MRR')\n",
    "ax.set_title('B: Ranking Performance')\n",
    "ax.set_ylim(0, 1)\n",
    "\n",
    "# C: Same-passage\n",
    "ax = axes[2]\n",
    "c_labels = ['Bare', 'Irrelevant', 'Same-\\nPassage', 'Oracle']\n",
    "c_wins = [\n",
    "    50.0,\n",
    "    np.mean(bare_nlls_c > irrel_nlls_c) * 100,\n",
    "    np.mean(bare_nlls_c > surr_nlls_c) * 100,\n",
    "    np.mean(bare_nlls_c > oracle_nlls_c) * 100,\n",
    "]\n",
    "ax.bar(range(len(c_labels)), c_wins, color=['#888888', '#8c564b', '#4c72b0', '#c44e52'])\n",
    "ax.axhline(50, color='gray', linestyle='--')\n",
    "ax.set_xticks(range(len(c_labels)))\n",
    "ax.set_xticklabels(c_labels)\n",
    "ax.set_ylabel('Win Rate vs Bare (%)')\n",
    "ax.set_title('C: Same-Passage Surrogates')\n",
    "for i, w in enumerate(c_wins):\n",
    "    ax.text(i, w + 1, f'{w:.0f}%', ha='center', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/exp11/11_summary.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print('Saved: 11_summary.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}