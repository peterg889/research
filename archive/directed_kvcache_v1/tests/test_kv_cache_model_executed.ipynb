{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model-Dependent Tests for lib/kv_cache.py\n",
    "\n",
    "These tests require a loaded model and run in a Jupyter kernel that already has\n",
    "the model in GPU memory. They verify:\n",
    "\n",
    "1. RoPE correction matches HuggingFace's actual rotary embedding\n",
    "2. RoPE roundtrip is identity in float64, bounded error in float16\n",
    "3. `apply_rope_roundtrip_noise` preserves BOS, introduces small noise\n",
    "4. `correct_rope_positions_with_bos` preserves BOS, modifies doc keys\n",
    "5. `score_answer_with_cache` NLL matches full-sequence forward pass\n",
    "6. Truncated+corrected keys match bare cache keys\n",
    "7. Truncated values differ from bare values (prefix priming works)\n",
    "8. `build_truncated_kv_cache_corrected` produces correct length\n",
    "9. `build_truncated_cache_variable_prefix` correctness\n",
    "10. BPE boundary effect detection\n",
    "11. Suffix cache passage portion matches bare cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-01T13:23:33.831166Z",
     "iopub.status.busy": "2026-02-01T13:23:33.830143Z",
     "iopub.status.idle": "2026-02-01T13:24:43.420809Z",
     "shell.execute_reply": "2026-02-01T13:24:43.419785Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ca8f98f201a4e5d9e265a78e051780d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/291 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded: mistralai/Mistral-7B-Instruct-v0.2\n",
      "Layers: 32, Head dim: 128, RoPE theta: 1000000.0\n"
     ]
    }
   ],
   "source": [
    "import sys, os, torch, numpy as np\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, DynamicCache\n",
    "from lib.config import ExperimentConfig\n",
    "from lib.kv_cache import (\n",
    "    _rotate_half, _build_rope_correction, _get_rope_theta, _ensure_dynamic_cache,\n",
    "    _get_cache_keys, _get_cache_values, _set_cache_keys, _set_cache_values,\n",
    "    build_kv_cache, extract_and_truncate_cache, extract_and_truncate_cache_with_bos,\n",
    "    correct_rope_positions, correct_rope_positions_with_bos,\n",
    "    build_hybrid_cache, swap_bos_entry, apply_rope_roundtrip_noise,\n",
    "    replace_values_at_layers, score_answer_with_cache,\n",
    "    build_truncated_kv_cache, build_truncated_kv_cache_corrected,\n",
    "    build_truncated_cache_variable_prefix, build_suffix_kv_cache,\n",
    ")\n",
    "\n",
    "config = ExperimentConfig(num_samples=10, seed=42)\n",
    "\n",
    "# Load model\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, bnb_4bit_compute_dtype=torch.float16, bnb_4bit_quant_type='nf4',\n",
    ")\n",
    "model_name = config.model_name\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, quantization_config=bnb_config, device_map='auto',\n",
    ")\n",
    "model.eval()\n",
    "NUM_LAYERS = model.config.num_hidden_layers\n",
    "HEAD_DIM = model.config.hidden_size // model.config.num_attention_heads\n",
    "ROPE_THETA = _get_rope_theta(model.config)\n",
    "\n",
    "PASSAGE = (\n",
    "    'The Amazon rainforest produces approximately 20 percent of the world\\'s oxygen. '\n",
    "    'It covers over 5.5 million square kilometers and is home to roughly 10 percent '\n",
    "    'of all species on Earth.'\n",
    ")\n",
    "QUERY = 'how much oxygen does the amazon produce'\n",
    "ANSWER = 'approximately 20 percent'\n",
    "\n",
    "passed = 0\n",
    "failed = 0\n",
    "total = 0\n",
    "\n",
    "def check(name, condition, detail=''):\n",
    "    global passed, failed, total\n",
    "    total += 1\n",
    "    if condition:\n",
    "        passed += 1\n",
    "        print(f'  PASS: {name}  {detail}')\n",
    "    else:\n",
    "        failed += 1\n",
    "        print(f'  FAIL: {name}  {detail}')\n",
    "\n",
    "print(f'Model loaded: {model_name}')\n",
    "print(f'Layers: {NUM_LAYERS}, Head dim: {HEAD_DIM}, RoPE theta: {ROPE_THETA}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-01T13:24:43.425762Z",
     "iopub.status.busy": "2026-02-01T13:24:43.425227Z",
     "iopub.status.idle": "2026-02-01T13:24:43.698269Z",
     "shell.execute_reply": "2026-02-01T13:24:43.697280Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "TEST 1: RoPE correction matches HuggingFace rotary embedding\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  PASS: RoPE correction vs HF rotary_emb  max_err=4.88e-04\n",
      "  PASS:   offset=1  max_err=6.95e-04\n",
      "  PASS:   offset=5  max_err=6.86e-04\n",
      "  PASS:   offset=50  max_err=7.40e-04\n",
      "  PASS:   offset=200  max_err=5.63e-04\n"
     ]
    }
   ],
   "source": [
    "print('='*70)\n",
    "print('TEST 1: RoPE correction matches HuggingFace rotary embedding')\n",
    "print('='*70)\n",
    "\n",
    "# Get the model's actual rotary embedding module\n",
    "rotary_emb = model.model.rotary_emb\n",
    "\n",
    "S = 20  # offset\n",
    "# Generate cos/sin at specific positions using the model's own RoPE\n",
    "dummy_x = torch.zeros(1, 1, 1, HEAD_DIM, device=config.device, dtype=torch.float16)\n",
    "pos_1 = torch.tensor([[1]], device=config.device)\n",
    "pos_1_plus_S = torch.tensor([[1 + S]], device=config.device)\n",
    "\n",
    "cos_1, sin_1 = rotary_emb(dummy_x, pos_1)\n",
    "cos_1s, sin_1s = rotary_emb(dummy_x, pos_1_plus_S)\n",
    "\n",
    "# Apply HF RoPE at position 1 and 1+S to same pre-rope key\n",
    "key_pre = torch.randn(1, 1, 1, HEAD_DIM, device=config.device, dtype=torch.float32)\n",
    "c1, s1 = cos_1.squeeze().float(), sin_1.squeeze().float()\n",
    "c1s, s1s = cos_1s.squeeze().float(), sin_1s.squeeze().float()\n",
    "\n",
    "key_at_1 = key_pre * c1 + _rotate_half(key_pre) * s1\n",
    "key_at_1_plus_S = key_pre * c1s + _rotate_half(key_pre) * s1s\n",
    "\n",
    "# Apply our correction(-S) to key_at_1+S => should recover key_at_1\n",
    "cos_corr, sin_corr = _build_rope_correction(S, HEAD_DIM, ROPE_THETA)\n",
    "cos_corr = cos_corr.to(device=config.device, dtype=torch.float32)\n",
    "sin_corr = sin_corr.to(device=config.device, dtype=torch.float32)\n",
    "\n",
    "recovered = key_at_1_plus_S * cos_corr + _rotate_half(key_at_1_plus_S) * sin_corr\n",
    "max_err = (recovered - key_at_1).abs().max().item()\n",
    "# Tolerance is 1e-3 because HF stores inv_freq as a buffer (computed once)\n",
    "# while we recompute from scratch — float32 accumulation differs slightly\n",
    "check('RoPE correction vs HF rotary_emb', max_err < 1e-3, f'max_err={max_err:.2e}')\n",
    "\n",
    "# Test multiple offsets\n",
    "for S_test in [1, 5, 50, 200]:\n",
    "    pos_target = torch.tensor([[3]], device=config.device)\n",
    "    pos_shifted = torch.tensor([[3 + S_test]], device=config.device)\n",
    "    c_t, s_t = rotary_emb(dummy_x, pos_target)\n",
    "    c_s, s_s = rotary_emb(dummy_x, pos_shifted)\n",
    "    \n",
    "    k_t = key_pre * c_t.squeeze().float() + _rotate_half(key_pre) * s_t.squeeze().float()\n",
    "    k_s = key_pre * c_s.squeeze().float() + _rotate_half(key_pre) * s_s.squeeze().float()\n",
    "    \n",
    "    cc, sc = _build_rope_correction(S_test, HEAD_DIM, ROPE_THETA)\n",
    "    cc, sc = cc.to(device=config.device, dtype=torch.float32), sc.to(device=config.device, dtype=torch.float32)\n",
    "    rec = k_s * cc + _rotate_half(k_s) * sc\n",
    "    err = (rec - k_t).abs().max().item()\n",
    "    check(f'  offset={S_test}', err < 1e-3, f'max_err={err:.2e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-01T13:24:43.702745Z",
     "iopub.status.busy": "2026-02-01T13:24:43.702449Z",
     "iopub.status.idle": "2026-02-01T13:24:43.722377Z",
     "shell.execute_reply": "2026-02-01T13:24:43.721530Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "TEST 2: RoPE roundtrip identity (float64) and bounded error (float16)\n",
      "======================================================================\n",
      "  PASS: RoPE roundtrip identity (float64)  max_err=2.53e-07\n",
      "  PASS: RoPE roundtrip float16: nonzero error  err=0.003906\n",
      "  PASS: RoPE roundtrip float16: bounded error  err=0.003906\n"
     ]
    }
   ],
   "source": [
    "print('='*70)\n",
    "print('TEST 2: RoPE roundtrip identity (float64) and bounded error (float16)')\n",
    "print('='*70)\n",
    "\n",
    "keys_f64 = torch.randn(1, 8, 10, HEAD_DIM, dtype=torch.float64)\n",
    "offset = 15\n",
    "\n",
    "# Forward: RoPE(+S) — use correction(-S)\n",
    "cos_fwd, sin_fwd = _build_rope_correction(-offset, HEAD_DIM, ROPE_THETA)\n",
    "cos_fwd, sin_fwd = cos_fwd.double(), sin_fwd.double()\n",
    "rotated = keys_f64 * cos_fwd + _rotate_half(keys_f64) * sin_fwd\n",
    "\n",
    "# Inverse: correction(+S)\n",
    "cos_inv, sin_inv = _build_rope_correction(offset, HEAD_DIM, ROPE_THETA)\n",
    "cos_inv, sin_inv = cos_inv.double(), sin_inv.double()\n",
    "recovered = rotated * cos_inv + _rotate_half(rotated) * sin_inv\n",
    "\n",
    "err_f64 = (recovered - keys_f64).abs().max().item()\n",
    "# _build_rope_correction computes angles in float64 but returns float32,\n",
    "# so roundtrip through double has float32-level precision\n",
    "check('RoPE roundtrip identity (float64)', err_f64 < 1e-6, f'max_err={err_f64:.2e}')\n",
    "\n",
    "# Float16\n",
    "keys_f16 = torch.randn(1, 8, 10, HEAD_DIM, dtype=torch.float16, device=config.device)\n",
    "cf, sf = _build_rope_correction(-offset, HEAD_DIM, ROPE_THETA)\n",
    "cf, sf = cf.to(device=config.device, dtype=torch.float16), sf.to(device=config.device, dtype=torch.float16)\n",
    "rotated_16 = keys_f16 * cf + _rotate_half(keys_f16) * sf\n",
    "\n",
    "ci, si = _build_rope_correction(offset, HEAD_DIM, ROPE_THETA)\n",
    "ci, si = ci.to(device=config.device, dtype=torch.float16), si.to(device=config.device, dtype=torch.float16)\n",
    "recovered_16 = rotated_16 * ci + _rotate_half(rotated_16) * si\n",
    "\n",
    "err_f16 = (recovered_16.float() - keys_f16.float()).abs().max().item()\n",
    "check('RoPE roundtrip float16: nonzero error', err_f16 > 0, f'err={err_f16:.6f}')\n",
    "check('RoPE roundtrip float16: bounded error', err_f16 < 0.1, f'err={err_f16:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-01T13:24:43.726181Z",
     "iopub.status.busy": "2026-02-01T13:24:43.725904Z",
     "iopub.status.idle": "2026-02-01T13:24:43.852077Z",
     "shell.execute_reply": "2026-02-01T13:24:43.850856Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "TEST 3: apply_rope_roundtrip_noise\n",
      "======================================================================\n",
      "  PASS: roundtrip_noise: BOS preserved  \n",
      "  PASS: roundtrip_noise: nonzero perturbation  mean_max_err=0.003906\n",
      "  PASS: roundtrip_noise: bounded perturbation  mean_max_err=0.003906\n"
     ]
    }
   ],
   "source": [
    "print('='*70)\n",
    "print('TEST 3: apply_rope_roundtrip_noise')\n",
    "print('='*70)\n",
    "\n",
    "cache = DynamicCache()\n",
    "keys_before = []\n",
    "for li in range(NUM_LAYERS):\n",
    "    k = torch.randn(1, 8, 10, HEAD_DIM, device=config.device, dtype=torch.float16)\n",
    "    v = torch.randn(1, 8, 10, HEAD_DIM, device=config.device, dtype=torch.float16)\n",
    "    keys_before.append(k.clone())\n",
    "    cache.update(k, v, li)\n",
    "\n",
    "bos_before = [_get_cache_keys(cache, li)[:, :, 0:1, :].clone() for li in range(NUM_LAYERS)]\n",
    "apply_rope_roundtrip_noise(cache, offset=10, model=model)\n",
    "\n",
    "bos_preserved = all(\n",
    "    torch.allclose(_get_cache_keys(cache, li)[:, :, 0:1, :], bos_before[li], atol=0)\n",
    "    for li in range(NUM_LAYERS)\n",
    ")\n",
    "check('roundtrip_noise: BOS preserved', bos_preserved)\n",
    "\n",
    "max_errors = []\n",
    "for li in range(NUM_LAYERS):\n",
    "    doc_b = keys_before[li][:, :, 1:, :].float()\n",
    "    doc_a = _get_cache_keys(cache, li)[:, :, 1:, :].float()\n",
    "    max_errors.append((doc_a - doc_b).abs().max().item())\n",
    "\n",
    "mean_max = np.mean(max_errors)\n",
    "check('roundtrip_noise: nonzero perturbation', mean_max > 1e-5, f'mean_max_err={mean_max:.6f}')\n",
    "check('roundtrip_noise: bounded perturbation', mean_max < 0.1, f'mean_max_err={mean_max:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-01T13:24:43.856502Z",
     "iopub.status.busy": "2026-02-01T13:24:43.855929Z",
     "iopub.status.idle": "2026-02-01T13:24:43.889903Z",
     "shell.execute_reply": "2026-02-01T13:24:43.889013Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "TEST 4: correct_rope_positions_with_bos\n",
      "======================================================================\n",
      "  PASS: correct_rope_with_bos: BOS preserved  \n",
      "  PASS: correct_rope_with_bos: doc keys modified  \n",
      "  PASS: correct_rope_with_bos: offset=0 is noop  \n",
      "  PASS: correct_rope (no BOS): modifies all positions including 0  \n"
     ]
    }
   ],
   "source": [
    "print('='*70)\n",
    "print('TEST 4: correct_rope_positions_with_bos')\n",
    "print('='*70)\n",
    "\n",
    "# BOS preservation\n",
    "cache4 = DynamicCache()\n",
    "for li in range(NUM_LAYERS):\n",
    "    k = torch.randn(1, 8, 10, HEAD_DIM, device=config.device, dtype=torch.float16)\n",
    "    v = torch.randn(1, 8, 10, HEAD_DIM, device=config.device, dtype=torch.float16)\n",
    "    cache4.update(k, v, li)\n",
    "\n",
    "bos4 = [_get_cache_keys(cache4, li)[:, :, 0:1, :].clone() for li in range(NUM_LAYERS)]\n",
    "doc4 = [_get_cache_keys(cache4, li)[:, :, 1:, :].clone() for li in range(NUM_LAYERS)]\n",
    "\n",
    "correct_rope_positions_with_bos(cache4, offset=10, model=model)\n",
    "\n",
    "bos_ok = all(\n",
    "    torch.allclose(_get_cache_keys(cache4, li)[:, :, 0:1, :], bos4[li], atol=0)\n",
    "    for li in range(NUM_LAYERS)\n",
    ")\n",
    "check('correct_rope_with_bos: BOS preserved', bos_ok)\n",
    "\n",
    "doc_changed = all(\n",
    "    not torch.allclose(_get_cache_keys(cache4, li)[:, :, 1:, :], doc4[li], atol=1e-6)\n",
    "    for li in range(NUM_LAYERS)\n",
    ")\n",
    "check('correct_rope_with_bos: doc keys modified', doc_changed)\n",
    "\n",
    "# Zero offset = noop\n",
    "cache4b = DynamicCache()\n",
    "k4b = torch.randn(1, 8, 10, HEAD_DIM, device=config.device, dtype=torch.float16)\n",
    "v4b = torch.randn(1, 8, 10, HEAD_DIM, device=config.device, dtype=torch.float16)\n",
    "cache4b.update(k4b.clone(), v4b.clone(), 0)\n",
    "correct_rope_positions_with_bos(cache4b, offset=0, model=model)\n",
    "check('correct_rope_with_bos: offset=0 is noop',\n",
    "      torch.allclose(_get_cache_keys(cache4b, 0), k4b))\n",
    "\n",
    "# correct_rope_positions (no BOS) modifies ALL positions\n",
    "cache4c = DynamicCache()\n",
    "k4c = torch.randn(1, 8, 10, HEAD_DIM, device=config.device, dtype=torch.float16)\n",
    "cache4c.update(k4c.clone(), torch.randn_like(k4c), 0)\n",
    "correct_rope_positions(cache4c, offset=10, model=model)\n",
    "check('correct_rope (no BOS): modifies all positions including 0',\n",
    "      not torch.allclose(_get_cache_keys(cache4c, 0), k4c, atol=1e-6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-01T13:24:43.893607Z",
     "iopub.status.busy": "2026-02-01T13:24:43.893219Z",
     "iopub.status.idle": "2026-02-01T13:24:45.962448Z",
     "shell.execute_reply": "2026-02-01T13:24:45.961528Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "TEST 5: score_answer_with_cache NLL matches full forward pass\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  PASS: NLL cached vs full  cached=0.000557, full=0.000565, rel_err=0.013514\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  PASS: correct < wrong answer NLL  correct=0.0006, wrong=9.7500\n"
     ]
    }
   ],
   "source": [
    "print('='*70)\n",
    "print('TEST 5: score_answer_with_cache NLL matches full forward pass')\n",
    "print('='*70)\n",
    "\n",
    "context = f'Document:\\n{PASSAGE}'\n",
    "ctx_len, cache5 = build_kv_cache(context, model, tokenizer, config)\n",
    "query_prompt = config.query_template.format(query=QUERY)\n",
    "nll_cached = score_answer_with_cache(\n",
    "    cache5, ctx_len, query_prompt, ANSWER, model, tokenizer, config\n",
    ")\n",
    "\n",
    "# Full-sequence forward pass\n",
    "full_text = context + query_prompt + ANSWER\n",
    "full_ids = tokenizer(full_text, return_tensors='pt', add_special_tokens=True)['input_ids'].to(config.device)\n",
    "answer_ids = tokenizer(ANSWER, return_tensors='pt', add_special_tokens=False)['input_ids'].to(config.device)\n",
    "answer_len = answer_ids.shape[1]\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids=full_ids, attention_mask=torch.ones_like(full_ids), return_dict=True)\n",
    "\n",
    "answer_start = full_ids.shape[1] - answer_len\n",
    "answer_logits = outputs.logits[:, answer_start:-1, :]\n",
    "answer_labels = full_ids[:, answer_start + 1:]\n",
    "\n",
    "loss_fct = torch.nn.CrossEntropyLoss(reduction='sum')\n",
    "nll_full = loss_fct(\n",
    "    answer_logits.contiguous().view(-1, answer_logits.size(-1)),\n",
    "    answer_labels.contiguous().view(-1)\n",
    ").item() / (answer_len - 1)\n",
    "\n",
    "rel_err = abs(nll_cached - nll_full) / max(abs(nll_full), 1e-8)\n",
    "# 4-bit quantized models may have small numerical differences between\n",
    "# cached and full forward passes due to non-deterministic matmul ordering\n",
    "check('NLL cached vs full', rel_err < 0.05,\n",
    "      f'cached={nll_cached:.6f}, full={nll_full:.6f}, rel_err={rel_err:.6f}')\n",
    "\n",
    "# Correct answer should have lower NLL\n",
    "nll_wrong = score_answer_with_cache(\n",
    "    cache5, ctx_len, query_prompt, 'purple elephants flying sideways',\n",
    "    model, tokenizer, config\n",
    ")\n",
    "check('correct < wrong answer NLL', nll_cached < nll_wrong,\n",
    "      f'correct={nll_cached:.4f}, wrong={nll_wrong:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-01T13:24:45.966693Z",
     "iopub.status.busy": "2026-02-01T13:24:45.966391Z",
     "iopub.status.idle": "2026-02-01T13:24:46.499295Z",
     "shell.execute_reply": "2026-02-01T13:24:46.498316Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "TEST 6: Truncated+corrected keys match bare cache keys\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  PASS: layer 0 corrected keys match bare (4-bit tol)  max_err=0.062500\n",
      "  PASS: key divergence increases with depth  layer0=0.062500, last=3.453125\n",
      "  PASS: BOS keys match between bare and trunc  mean_bos_err=0.000000\n"
     ]
    }
   ],
   "source": [
    "print('='*70)\n",
    "print('TEST 6: Truncated+corrected keys match bare cache keys')\n",
    "print('='*70)\n",
    "\n",
    "prefix_text = 'Some irrelevant prefix text here. '\n",
    "document_text = f'Document:\\n{PASSAGE}'\n",
    "prefix_with_sep = prefix_text + ' '\n",
    "\n",
    "prefix_enc = tokenizer(prefix_with_sep, return_tensors='pt', add_special_tokens=True)\n",
    "prefix_len = prefix_enc['input_ids'].shape[1]\n",
    "\n",
    "full_context = prefix_with_sep + document_text\n",
    "full_enc = tokenizer(full_context, return_tensors='pt', add_special_tokens=True)\n",
    "full_ids = full_enc['input_ids'].to(config.device)\n",
    "doc_len = full_ids.shape[1] - prefix_len\n",
    "\n",
    "# Extract exact doc tokens for bare cache\n",
    "doc_token_ids = full_ids[:, prefix_len:]\n",
    "bos_id = full_ids[:, :1]\n",
    "bare_ids = torch.cat([bos_id, doc_token_ids], dim=1)\n",
    "\n",
    "with torch.no_grad():\n",
    "    bare_out = model(input_ids=bare_ids, attention_mask=torch.ones_like(bare_ids),\n",
    "                     use_cache=True, return_dict=True)\n",
    "    full_out = model(input_ids=full_ids, attention_mask=torch.ones_like(full_ids),\n",
    "                     use_cache=True, return_dict=True)\n",
    "\n",
    "bare_cache = _ensure_dynamic_cache(bare_out.past_key_values)\n",
    "trunc_cache = extract_and_truncate_cache_with_bos(full_out.past_key_values, doc_len)\n",
    "offset = prefix_len - 1\n",
    "correct_rope_positions_with_bos(trunc_cache, offset, model)\n",
    "\n",
    "# Layer 0: input is pure token embeddings (identical regardless of prefix),\n",
    "# so keys = RoPE(W_K * embed(token), pos). After correction, should match bare.\n",
    "# NOTE: 4-bit quantized matmul produces slightly different results for different\n",
    "# total sequence lengths, even for the same tokens. Tolerance reflects this.\n",
    "bk0 = _get_cache_keys(bare_cache, 0)[:, :, 1:, :].float()\n",
    "tk0 = _get_cache_keys(trunc_cache, 0)[:, :, 1:, :].float()\n",
    "err_layer0 = (bk0 - tk0).abs().max().item()\n",
    "check('layer 0 corrected keys match bare (4-bit tol)', err_layer0 < 0.1,\n",
    "      f'max_err={err_layer0:.6f}')\n",
    "\n",
    "# Layers >0: hidden states differ because doc tokens attended to prefix tokens,\n",
    "# so pre-RoPE keys differ. RoPE correction only fixes positional encoding.\n",
    "# We expect INCREASING divergence with layer depth.\n",
    "max_errors = []\n",
    "for li in range(NUM_LAYERS):\n",
    "    bk = _get_cache_keys(bare_cache, li)[:, :, 1:, :].float()\n",
    "    tk = _get_cache_keys(trunc_cache, li)[:, :, 1:, :].float()\n",
    "    err = (bk - tk).abs().max().item()\n",
    "    max_errors.append(err)\n",
    "\n",
    "check('key divergence increases with depth',\n",
    "      max_errors[0] < max_errors[-1],\n",
    "      f'layer0={max_errors[0]:.6f}, last={max_errors[-1]:.6f}')\n",
    "\n",
    "# BOS keys should be identical (position 0, same input embedding, causal = no attention)\n",
    "bos_errs = []\n",
    "for li in range(NUM_LAYERS):\n",
    "    bk_bos = _get_cache_keys(bare_cache, li)[:, :, 0:1, :].float()\n",
    "    tk_bos = _get_cache_keys(trunc_cache, li)[:, :, 0:1, :].float()\n",
    "    bos_errs.append((bk_bos - tk_bos).abs().max().item())\n",
    "\n",
    "mean_bos_err = np.mean(bos_errs)\n",
    "check('BOS keys match between bare and trunc', mean_bos_err < 1e-4,\n",
    "      f'mean_bos_err={mean_bos_err:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-01T13:24:46.503180Z",
     "iopub.status.busy": "2026-02-01T13:24:46.502876Z",
     "iopub.status.idle": "2026-02-01T13:24:46.556803Z",
     "shell.execute_reply": "2026-02-01T13:24:46.555885Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "TEST 7: Values differ between bare and truncated (prefix priming)\n",
      "======================================================================\n",
      "  PASS: values differ (prefix priming works)  min=0.000000, max=0.244783, mean=0.120358\n",
      "  PASS: layer 0 divergence < last layer divergence  layer0=0.000000, last=0.243647\n"
     ]
    }
   ],
   "source": [
    "print('='*70)\n",
    "print('TEST 7: Values differ between bare and truncated (prefix priming)')\n",
    "print('='*70)\n",
    "\n",
    "divergences = []\n",
    "for li in range(NUM_LAYERS):\n",
    "    bv = _get_cache_values(bare_cache, li)[:, :, 1:, :].float()\n",
    "    tv = _get_cache_values(trunc_cache, li)[:, :, 1:, :].float()\n",
    "    l2 = torch.norm(bv - tv).item() / bv.numel()**0.5\n",
    "    divergences.append(l2)\n",
    "\n",
    "check('values differ (prefix priming works)', max(divergences) > 0.01,\n",
    "      f'min={min(divergences):.6f}, max={max(divergences):.6f}, mean={np.mean(divergences):.6f}')\n",
    "\n",
    "# Layer 0 values should differ least (minimal cross-attention effect)\n",
    "# Later layers should diverge more\n",
    "check('layer 0 divergence < last layer divergence',\n",
    "      divergences[0] < divergences[-1],\n",
    "      f'layer0={divergences[0]:.6f}, last={divergences[-1]:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-01T13:24:46.560914Z",
     "iopub.status.busy": "2026-02-01T13:24:46.560160Z",
     "iopub.status.idle": "2026-02-01T13:24:47.322106Z",
     "shell.execute_reply": "2026-02-01T13:24:47.321105Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "TEST 8: build_truncated_kv_cache_corrected output length\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  PASS: build_truncated_kv_cache_corrected: keep_len  got=48, expected=48\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  PASS: build_truncated_kv_cache_corrected: valid NLL  nll=0.003555\n"
     ]
    }
   ],
   "source": [
    "print('='*70)\n",
    "print('TEST 8: build_truncated_kv_cache_corrected output length')\n",
    "print('='*70)\n",
    "\n",
    "surrogate = 'how much oxygen'\n",
    "keep_len8, cache8 = build_truncated_kv_cache_corrected(\n",
    "    surrogate, PASSAGE, model, tokenizer, config\n",
    ")\n",
    "\n",
    "surr_prefix = f'This document may be relevant to queries like: {surrogate}\\n\\n'\n",
    "doc_text8 = f'Document:\\n{PASSAGE}'\n",
    "prefix_enc8 = tokenizer(surr_prefix, return_tensors='pt', add_special_tokens=True)\n",
    "full_enc8 = tokenizer(surr_prefix + doc_text8, return_tensors='pt', add_special_tokens=True)\n",
    "expected_doc_len = full_enc8['input_ids'].shape[1] - prefix_enc8['input_ids'].shape[1]\n",
    "\n",
    "check('build_truncated_kv_cache_corrected: keep_len', keep_len8 == 1 + expected_doc_len,\n",
    "      f'got={keep_len8}, expected={1+expected_doc_len}')\n",
    "\n",
    "# Check cache is usable\n",
    "nll8 = score_answer_with_cache(\n",
    "    cache8, keep_len8, config.query_template.format(query=QUERY),\n",
    "    ANSWER, model, tokenizer, config\n",
    ")\n",
    "check('build_truncated_kv_cache_corrected: valid NLL', np.isfinite(nll8) and nll8 > 0,\n",
    "      f'nll={nll8:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-01T13:24:47.326546Z",
     "iopub.status.busy": "2026-02-01T13:24:47.325698Z",
     "iopub.status.idle": "2026-02-01T13:24:48.118713Z",
     "shell.execute_reply": "2026-02-01T13:24:48.117810Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "TEST 9: build_truncated_cache_variable_prefix\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  PASS: variable_prefix: prefix_token_len  got=6, expected=6\n",
      "  PASS: variable_prefix: keep_len > 1  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  PASS: variable_prefix: different prefixes -> different values  mean_diff=0.247062\n"
     ]
    }
   ],
   "source": [
    "print('='*70)\n",
    "print('TEST 9: build_truncated_cache_variable_prefix')\n",
    "print('='*70)\n",
    "\n",
    "prefix9 = 'Random prefix text here'\n",
    "keep9, cache9, prefix_tok_len9 = build_truncated_cache_variable_prefix(\n",
    "    prefix9, PASSAGE, model, tokenizer, config\n",
    ")\n",
    "prefix_enc9 = tokenizer(prefix9 + ' ', return_tensors='pt', add_special_tokens=True)\n",
    "check('variable_prefix: prefix_token_len',\n",
    "      prefix_tok_len9 == prefix_enc9['input_ids'].shape[1],\n",
    "      f'got={prefix_tok_len9}, expected={prefix_enc9[\"input_ids\"].shape[1]}')\n",
    "check('variable_prefix: keep_len > 1', keep9 > 1)\n",
    "\n",
    "# Different prefixes -> different caches\n",
    "_, cacheA, _ = build_truncated_cache_variable_prefix(\n",
    "    'Hello world', PASSAGE, model, tokenizer, config\n",
    ")\n",
    "_, cacheB, _ = build_truncated_cache_variable_prefix(\n",
    "    'Completely different text about quantum physics and black holes',\n",
    "    PASSAGE, model, tokenizer, config\n",
    ")\n",
    "v_a = _get_cache_values(cacheA, NUM_LAYERS - 1).float()\n",
    "v_b = _get_cache_values(cacheB, NUM_LAYERS - 1).float()\n",
    "min_len = min(v_a.shape[2], v_b.shape[2])\n",
    "diff9 = (v_a[:, :, :min_len, :] - v_b[:, :, :min_len, :]).abs().mean().item()\n",
    "check('variable_prefix: different prefixes -> different values', diff9 > 0.001,\n",
    "      f'mean_diff={diff9:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-01T13:24:48.123149Z",
     "iopub.status.busy": "2026-02-01T13:24:48.122288Z",
     "iopub.status.idle": "2026-02-01T13:24:48.137027Z",
     "shell.execute_reply": "2026-02-01T13:24:48.136183Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "TEST 10: BPE boundary effect detection\n",
      "======================================================================\n",
      "  BPE tokens match: False\n",
      "    Alone (no BOS): [14873, 28747, 13, 1014, 2936, 9060, 285, 1142]\n",
      "    From full:      [28747, 13, 1014, 2936, 9060, 285, 1142]\n",
      "  -> BPE splits DIFFER. build_matched_caches is NECESSARY.\n",
      "  Case 2 BPE match: False\n",
      "    Alone: [14873, 28747, 13, 17960, 1491]\n",
      "    Full:  [28747, 13, 17960, 1491]\n"
     ]
    }
   ],
   "source": [
    "print('='*70)\n",
    "print('TEST 10: BPE boundary effect detection')\n",
    "print('='*70)\n",
    "\n",
    "doc10 = 'Document:\\nThe quick brown fox'\n",
    "prefix10 = 'Some prefix text '\n",
    "\n",
    "ids_alone = tokenizer(doc10, return_tensors='pt', add_special_tokens=True)['input_ids'][0]\n",
    "prefix_ids10 = tokenizer(prefix10, return_tensors='pt', add_special_tokens=True)['input_ids'][0]\n",
    "full_ids10 = tokenizer(prefix10 + doc10, return_tensors='pt', add_special_tokens=True)['input_ids'][0]\n",
    "doc_ids_from_full = full_ids10[len(prefix_ids10):]\n",
    "ids_alone_no_bos = ids_alone[1:]\n",
    "\n",
    "match = torch.equal(ids_alone_no_bos, doc_ids_from_full)\n",
    "print(f'  BPE tokens match: {match}')\n",
    "print(f'    Alone (no BOS): {ids_alone_no_bos.tolist()[:10]}')\n",
    "print(f'    From full:      {doc_ids_from_full.tolist()[:10]}')\n",
    "if not match:\n",
    "    print('  -> BPE splits DIFFER. build_matched_caches is NECESSARY.')\n",
    "else:\n",
    "    print('  -> BPE splits match for this example (may not hold for all inputs).')\n",
    "\n",
    "# Test with a case more likely to show boundary effects\n",
    "doc10b = 'Document:\\nelectric'\n",
    "prefix10b = 'an '\n",
    "ids_alone_b = tokenizer(doc10b, return_tensors='pt', add_special_tokens=True)['input_ids'][0]\n",
    "prefix_ids_b = tokenizer(prefix10b, return_tensors='pt', add_special_tokens=True)['input_ids'][0]\n",
    "full_ids_b = tokenizer(prefix10b + doc10b, return_tensors='pt', add_special_tokens=True)['input_ids'][0]\n",
    "doc_from_full_b = full_ids_b[len(prefix_ids_b):]\n",
    "alone_no_bos_b = ids_alone_b[1:]\n",
    "\n",
    "match_b = torch.equal(alone_no_bos_b, doc_from_full_b)\n",
    "print(f'  Case 2 BPE match: {match_b}')\n",
    "print(f'    Alone: {alone_no_bos_b.tolist()}')\n",
    "print(f'    Full:  {doc_from_full_b.tolist()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-01T13:24:48.140596Z",
     "iopub.status.busy": "2026-02-01T13:24:48.140125Z",
     "iopub.status.idle": "2026-02-01T13:24:48.924046Z",
     "shell.execute_reply": "2026-02-01T13:24:48.923091Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "TEST 11: Suffix cache passage portion matches bare\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Determinism (same length): max_k_err=0.00e+00, max_v_err=0.00e+00\n",
      "  PASS: bare cache is deterministic (same length)  k=0.00e+00, v=0.00e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  PASS: suffix passage matches bare (4-bit causal invariance)  max_k_err=1.25e-01, max_v_err=7.81e-02\n",
      "  PASS: suffix errors are uniform (not growing with depth)  mean=0.0730, max=0.1250\n"
     ]
    }
   ],
   "source": [
    "print('='*70)\n",
    "print('TEST 11: Suffix cache passage portion matches bare')\n",
    "print('='*70)\n",
    "\n",
    "# First: check determinism by running bare cache twice with same length\n",
    "bare_len11a, bare_cache11a = build_kv_cache(PASSAGE, model, tokenizer, config)\n",
    "bare_len11b, bare_cache11b = build_kv_cache(PASSAGE, model, tokenizer, config)\n",
    "bare_cache11a = _ensure_dynamic_cache(bare_cache11a)\n",
    "bare_cache11b = _ensure_dynamic_cache(bare_cache11b)\n",
    "\n",
    "det_k_err = 0\n",
    "det_v_err = 0\n",
    "for li in range(NUM_LAYERS):\n",
    "    k_err = (_get_cache_keys(bare_cache11a, li).float() - _get_cache_keys(bare_cache11b, li).float()).abs().max().item()\n",
    "    v_err = (_get_cache_values(bare_cache11a, li).float() - _get_cache_values(bare_cache11b, li).float()).abs().max().item()\n",
    "    det_k_err = max(det_k_err, k_err)\n",
    "    det_v_err = max(det_v_err, v_err)\n",
    "\n",
    "print(f'  Determinism (same length): max_k_err={det_k_err:.2e}, max_v_err={det_v_err:.2e}')\n",
    "check('bare cache is deterministic (same length)', det_k_err < 1e-5 and det_v_err < 1e-5,\n",
    "      f'k={det_k_err:.2e}, v={det_v_err:.2e}')\n",
    "\n",
    "# Now test suffix causal invariance\n",
    "suffix_len11, suffix_cache11 = build_suffix_kv_cache(\n",
    "    PASSAGE, 'What is the oxygen percentage?', model, tokenizer, config\n",
    ")\n",
    "suffix_cache11 = _ensure_dynamic_cache(suffix_cache11)\n",
    "\n",
    "max_k_err = 0\n",
    "max_v_err = 0\n",
    "for li in range(NUM_LAYERS):\n",
    "    bk = _get_cache_keys(bare_cache11a, li).float()\n",
    "    sk = _get_cache_keys(suffix_cache11, li)[:, :, :bare_len11a, :].float()\n",
    "    bv = _get_cache_values(bare_cache11a, li).float()\n",
    "    sv = _get_cache_values(suffix_cache11, li)[:, :, :bare_len11a, :].float()\n",
    "    k_err = (bk - sk).abs().max().item()\n",
    "    v_err = (bv - sv).abs().max().item()\n",
    "    max_k_err = max(max_k_err, k_err)\n",
    "    max_v_err = max(max_v_err, v_err)\n",
    "\n",
    "# 4-bit quantized matmul produces different results for different total\n",
    "# sequence lengths even with identical prefix tokens and causal masking.\n",
    "# This is a known bitsandbytes limitation — the dequantization + GEMM\n",
    "# batching differs. Tolerance of 0.15 accommodates this.\n",
    "check('suffix passage matches bare (4-bit causal invariance)',\n",
    "      max_k_err < 0.15 and max_v_err < 0.15,\n",
    "      f'max_k_err={max_k_err:.2e}, max_v_err={max_v_err:.2e}')\n",
    "\n",
    "# The errors should be small and uniform across layers (not growing)\n",
    "# since it's quantization noise, not a systematic error\n",
    "layer_k_errs = []\n",
    "for li in range(NUM_LAYERS):\n",
    "    bk = _get_cache_keys(bare_cache11a, li).float()\n",
    "    sk = _get_cache_keys(suffix_cache11, li)[:, :, :bare_len11a, :].float()\n",
    "    layer_k_errs.append((bk - sk).abs().max().item())\n",
    "\n",
    "check('suffix errors are uniform (not growing with depth)',\n",
    "      max(layer_k_errs) < 3 * np.mean(layer_k_errs),\n",
    "      f'mean={np.mean(layer_k_errs):.4f}, max={max(layer_k_errs):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-01T13:24:48.928302Z",
     "iopub.status.busy": "2026-02-01T13:24:48.927486Z",
     "iopub.status.idle": "2026-02-01T13:24:48.934189Z",
     "shell.execute_reply": "2026-02-01T13:24:48.933122Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "SUMMARY: 30/30 passed, 0/30 failed\n",
      "======================================================================\n",
      "\n",
      "All tests passed.\n"
     ]
    }
   ],
   "source": [
    "print('\\n' + '='*70)\n",
    "print(f'SUMMARY: {passed}/{total} passed, {failed}/{total} failed')\n",
    "print('='*70)\n",
    "if failed > 0:\n",
    "    print('\\n*** FAILURES DETECTED - review output above ***')\n",
    "else:\n",
    "    print('\\nAll tests passed.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "3ca8f98f201a4e5d9e265a78e051780d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_ec17fa5c790e45d590dba16c0630ef20",
        "IPY_MODEL_973b4e608be74976b9499a80366d376d",
        "IPY_MODEL_8d8e041417ff45e38eed749be93e7a0c"
       ],
       "layout": "IPY_MODEL_fbf59ee31edc4ce085988f6d13bc9410",
       "tabbable": null,
       "tooltip": null
      }
     },
     "40b1108bef21455b9b01e4d3a752f8cd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "5cdeaee4ba3b44a98fa987d751a9a65f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "8d8e041417ff45e38eed749be93e7a0c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_a3584bdd04374f158f299bb6dbf1f8f7",
       "placeholder": "​",
       "style": "IPY_MODEL_fa6d3807eb6d42a5869e423dab25c876",
       "tabbable": null,
       "tooltip": null,
       "value": " 291/291 [00:56&lt;00:00, 67.02it/s, Materializing param=model.norm.weight]"
      }
     },
     "96488e68e89341c7980bc67a40218845": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "973b4e608be74976b9499a80366d376d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_5cdeaee4ba3b44a98fa987d751a9a65f",
       "max": 291.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_40b1108bef21455b9b01e4d3a752f8cd",
       "tabbable": null,
       "tooltip": null,
       "value": 291.0
      }
     },
     "a3584bdd04374f158f299bb6dbf1f8f7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "da47800b6ef94c90aa1386429afd3175": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ec17fa5c790e45d590dba16c0630ef20": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_da47800b6ef94c90aa1386429afd3175",
       "placeholder": "​",
       "style": "IPY_MODEL_96488e68e89341c7980bc67a40218845",
       "tabbable": null,
       "tooltip": null,
       "value": "Loading weights: 100%"
      }
     },
     "fa6d3807eb6d42a5869e423dab25c876": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "fbf59ee31edc4ce085988f6d13bc9410": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
