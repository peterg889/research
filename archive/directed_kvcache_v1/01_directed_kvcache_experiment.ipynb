{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Directed KV Cache: Comprehensive Experiment\n",
    "\n",
    "## Hypothesis: RoPE Position Mismatch Explains Truncation Failure\n",
    "\n",
    "When we build a KV cache from `[surrogate (S tokens)][document (D tokens)]` and then truncate\n",
    "the surrogate, document token `i` has its key stored as `RoPE(K_i, S+i)`. Queries at position\n",
    "`j` compute attention with relative position `j - (S+i)` instead of the correct `j - i`.\n",
    "Every document token appears S positions further away than it should.\n",
    "\n",
    "**Fix:** Apply `RoPE(-S)` to all cached keys after truncation.\n",
    "\n",
    "### Experimental Conditions\n",
    "\n",
    "| Group | ID | Name | Description |\n",
    "|-------|----|------|-------------|\n",
    "| A | A1 | baseline | `\"Document:\\n{document}\"` |\n",
    "| A | A2 | bare_doc | `\"{document}\"` — no framing |\n",
    "| A | A3 | baseline_offset | A1 with position IDs starting at offset N |\n",
    "| B | B1 | full_generated | Generated surrogate kept in context |\n",
    "| B | B2 | full_perfect | Actual query kept in context |\n",
    "| B | B3 | full_static | Static intent query kept in context |\n",
    "| B | B4 | full_suffix | Surrogate AFTER document, kept |\n",
    "| C | C1 | trunc_generated_broken | Truncated, NO position correction |\n",
    "| C | C2 | trunc_generated_corrected | Truncated, WITH RoPE correction |\n",
    "| C | C3 | trunc_perfect_broken | Actual query, truncated, no correction |\n",
    "| C | C4 | trunc_perfect_corrected | Actual query, truncated, WITH correction |\n",
    "| C | C5 | trunc_static_corrected | Static query, truncated, WITH correction |\n",
    "| D | D1 | trunc_suffix_corrected | Surrogate after doc, truncated with correction |\n",
    "| E | E1 | random_prefix_trunc_corrected | Random tokens, truncated with correction |\n",
    "| E | E2 | full_random_prefix | Random tokens kept in context |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from tqdm.auto import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, DynamicCache\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Add lib to path\n",
    "sys.path.insert(0, '.')\n",
    "from lib import (\n",
    "    ExperimentConfig,\n",
    "    build_kv_cache,\n",
    "    score_answer_with_cache,\n",
    "    extract_and_truncate_cache,\n",
    "    build_truncated_kv_cache,\n",
    "    correct_rope_positions,\n",
    "    build_truncated_kv_cache_corrected,\n",
    "    generate_surrogate,\n",
    "    compute_similarity,\n",
    "    load_evaluation_samples,\n",
    "    load_ms_marco,\n",
    "    STATIC_SURROGATE_QUERIES,\n",
    ")\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "config = ExperimentConfig(num_samples=200, seed=42)\n",
    "np.random.seed(config.seed)\n",
    "torch.manual_seed(config.seed)\n",
    "\n",
    "print(f\"Model: {config.model_name}\")\n",
    "print(f\"Samples: {config.num_samples}\")\n",
    "print(f\"Device: {config.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and tokenizer\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.model_name,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "print(f\"Model loaded: {config.model_name}\")\n",
    "print(f\"RoPE theta: {getattr(model.config, 'rope_theta', 10000.0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load embedding model\n",
    "embed_model = SentenceTransformer(config.embedding_model_name)\n",
    "print(f\"Embedding model loaded: {config.embedding_model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dataset = load_ms_marco(config)\n",
    "samples = load_evaluation_samples(dataset, config, require_answer=True)\n",
    "print(f\"Loaded {len(samples)} evaluation samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define Evaluation Conditions\n",
    "\n",
    "Each condition is a function that takes `(document, query, answer, surrogate, sample)` and returns an NLL score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_with_context(context_text, query, answer):\n",
    "    \"\"\"Evaluate NLL by building cache from context_text, then scoring query->answer.\"\"\"\n",
    "    ctx_len, cache = build_kv_cache(context_text, model, tokenizer, config)\n",
    "    query_prompt = config.query_template.format(query=query)\n",
    "    return score_answer_with_cache(cache, ctx_len, query_prompt, answer, model, tokenizer, config)\n",
    "\n",
    "\n",
    "def eval_with_offset(context_text, query, answer, offset):\n",
    "    \"\"\"Evaluate NLL with position IDs starting at an offset.\"\"\"\n",
    "    context_encoding = tokenizer(\n",
    "        context_text, return_tensors=\"pt\", add_special_tokens=True,\n",
    "        padding=False, truncation=False\n",
    "    )\n",
    "    context_ids = context_encoding['input_ids'].to(config.device)\n",
    "    seq_len = context_ids.shape[1]\n",
    "\n",
    "    position_ids = torch.arange(offset, offset + seq_len, device=config.device).unsqueeze(0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            input_ids=context_ids,\n",
    "            attention_mask=torch.ones_like(context_ids),\n",
    "            position_ids=position_ids,\n",
    "            use_cache=True,\n",
    "            return_dict=True\n",
    "        )\n",
    "\n",
    "    ctx_len = seq_len\n",
    "    cache = outputs.past_key_values\n",
    "    query_prompt = config.query_template.format(query=query)\n",
    "    return score_answer_with_cache(cache, ctx_len, query_prompt, answer, model, tokenizer, config)\n",
    "\n",
    "\n",
    "def eval_truncated_broken(surrogate_prefix, document_text, query, answer):\n",
    "    \"\"\"Truncated cache WITHOUT RoPE correction (the broken approach).\"\"\"\n",
    "    # Tokenize doc alone\n",
    "    doc_enc = tokenizer(document_text, return_tensors=\"pt\", add_special_tokens=False,\n",
    "                        padding=False, truncation=False)\n",
    "    doc_len = doc_enc['input_ids'].shape[1]\n",
    "\n",
    "    full_context = surrogate_prefix + document_text\n",
    "    full_enc = tokenizer(full_context, return_tensors=\"pt\", add_special_tokens=True,\n",
    "                         padding=False, truncation=False)\n",
    "    full_ids = full_enc['input_ids'].to(config.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            input_ids=full_ids,\n",
    "            attention_mask=torch.ones_like(full_ids),\n",
    "            use_cache=True,\n",
    "            return_dict=True\n",
    "        )\n",
    "\n",
    "    truncated_cache = extract_and_truncate_cache(outputs.past_key_values, doc_len)\n",
    "    query_prompt = config.query_template.format(query=query)\n",
    "    return score_answer_with_cache(truncated_cache, doc_len, query_prompt, answer, model, tokenizer, config)\n",
    "\n",
    "\n",
    "def eval_truncated_corrected(surrogate_prefix, document_text, query, answer):\n",
    "    \"\"\"Truncated cache WITH RoPE correction.\"\"\"\n",
    "    # Tokenize doc alone\n",
    "    doc_enc = tokenizer(document_text, return_tensors=\"pt\", add_special_tokens=False,\n",
    "                        padding=False, truncation=False)\n",
    "    doc_len = doc_enc['input_ids'].shape[1]\n",
    "\n",
    "    full_context = surrogate_prefix + document_text\n",
    "    full_enc = tokenizer(full_context, return_tensors=\"pt\", add_special_tokens=True,\n",
    "                         padding=False, truncation=False)\n",
    "    full_ids = full_enc['input_ids'].to(config.device)\n",
    "    surrogate_len = full_ids.shape[1] - doc_len\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            input_ids=full_ids,\n",
    "            attention_mask=torch.ones_like(full_ids),\n",
    "            use_cache=True,\n",
    "            return_dict=True\n",
    "        )\n",
    "\n",
    "    truncated_cache = extract_and_truncate_cache(outputs.past_key_values, doc_len)\n",
    "    correct_rope_positions(truncated_cache, surrogate_len, model)\n",
    "    query_prompt = config.query_template.format(query=query)\n",
    "    return score_answer_with_cache(truncated_cache, doc_len, query_prompt, answer, model, tokenizer, config)\n",
    "\n",
    "\n",
    "def eval_suffix_truncated_corrected(document_text, surrogate_suffix, query, answer):\n",
    "    \"\"\"Document THEN surrogate, truncate suffix (keep doc), correct RoPE.\"\"\"\n",
    "    # Tokenize doc alone (with BOS)\n",
    "    doc_enc = tokenizer(document_text, return_tensors=\"pt\", add_special_tokens=True,\n",
    "                        padding=False, truncation=False)\n",
    "    doc_len = doc_enc['input_ids'].shape[1]\n",
    "\n",
    "    # Full = doc + suffix\n",
    "    full_context = document_text + surrogate_suffix\n",
    "    full_enc = tokenizer(full_context, return_tensors=\"pt\", add_special_tokens=True,\n",
    "                         padding=False, truncation=False)\n",
    "    full_ids = full_enc['input_ids'].to(config.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            input_ids=full_ids,\n",
    "            attention_mask=torch.ones_like(full_ids),\n",
    "            use_cache=True,\n",
    "            return_dict=True\n",
    "        )\n",
    "\n",
    "    # Keep first doc_len entries (no RoPE correction needed — positions are already correct)\n",
    "    legacy = outputs.past_key_values\n",
    "    if hasattr(legacy, 'to_legacy_cache'):\n",
    "        legacy = legacy.to_legacy_cache()\n",
    "    elif not isinstance(legacy, (tuple, list)):\n",
    "        legacy = tuple(legacy)\n",
    "\n",
    "    new_cache = DynamicCache()\n",
    "    for layer_idx, layer_kv in enumerate(legacy):\n",
    "        key, value = layer_kv[0], layer_kv[1]\n",
    "        trunc_key = key[:, :, :doc_len, :].contiguous()\n",
    "        trunc_value = value[:, :, :doc_len, :].contiguous()\n",
    "        new_cache.update(trunc_key, trunc_value, layer_idx)\n",
    "\n",
    "    query_prompt = config.query_template.format(query=query)\n",
    "    return score_answer_with_cache(new_cache, doc_len, query_prompt, answer, model, tokenizer, config)\n",
    "\n",
    "\n",
    "def generate_random_prefix(length_tokens=20):\n",
    "    \"\"\"Generate a random token sequence as a nonsense prefix.\"\"\"\n",
    "    vocab_size = tokenizer.vocab_size\n",
    "    random_ids = torch.randint(100, vocab_size - 100, (length_tokens,))\n",
    "    return tokenizer.decode(random_ids, skip_special_tokens=True)\n",
    "\n",
    "\n",
    "print(\"Evaluation functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "static_query = STATIC_SURROGATE_QUERIES['static_definitional']['query']\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for idx, sample in enumerate(tqdm(samples, desc=\"Evaluating\")):\n",
    "    document = sample['passage']\n",
    "    query = sample['query']\n",
    "    answer = sample['answer']\n",
    "\n",
    "    doc_text = f\"Document:\\n{document}\"\n",
    "    query_prompt = config.query_template.format(query=query)\n",
    "\n",
    "    # Generate surrogate for this document\n",
    "    surrogate = generate_surrogate(document, model, tokenizer, config)\n",
    "\n",
    "    # Generate random prefix (fixed seed per sample for reproducibility)\n",
    "    np.random.seed(config.seed + idx)\n",
    "    random_prefix_text = generate_random_prefix(20)\n",
    "\n",
    "    result = {\n",
    "        'idx': idx,\n",
    "        'query': query,\n",
    "        'surrogate': surrogate,\n",
    "        'similarity': compute_similarity(surrogate, query, embed_model),\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # ===== Group A: Baselines =====\n",
    "        # A1: baseline\n",
    "        result['A1_baseline'] = eval_with_context(doc_text, query, answer)\n",
    "\n",
    "        # A2: bare_doc (no framing)\n",
    "        result['A2_bare_doc'] = eval_with_context(document, query, answer)\n",
    "\n",
    "        # A3: baseline_offset (position offset = 20, same as typical surrogate length)\n",
    "        result['A3_baseline_offset'] = eval_with_offset(doc_text, query, answer, offset=20)\n",
    "\n",
    "        # ===== Group B: Full context (surrogate kept) =====\n",
    "        surr_prefix = f\"This document may be relevant to queries like: {surrogate}\\n\\n\"\n",
    "        perfect_prefix = f\"This document may be relevant to queries like: {query}\\n\\n\"\n",
    "        static_prefix = f\"This document may be relevant to queries like: {static_query}\\n\\n\"\n",
    "\n",
    "        # B1: full_generated\n",
    "        result['B1_full_generated'] = eval_with_context(surr_prefix + doc_text, query, answer)\n",
    "\n",
    "        # B2: full_perfect\n",
    "        result['B2_full_perfect'] = eval_with_context(perfect_prefix + doc_text, query, answer)\n",
    "\n",
    "        # B3: full_static\n",
    "        result['B3_full_static'] = eval_with_context(static_prefix + doc_text, query, answer)\n",
    "\n",
    "        # B4: full_suffix\n",
    "        surr_suffix = f\"\\n\\nThis document may be relevant to queries like: {surrogate}\"\n",
    "        result['B4_full_suffix'] = eval_with_context(doc_text + surr_suffix, query, answer)\n",
    "\n",
    "        # ===== Group C: Truncated =====\n",
    "        # C1: trunc_generated_broken\n",
    "        result['C1_trunc_generated_broken'] = eval_truncated_broken(\n",
    "            surr_prefix, doc_text, query, answer)\n",
    "\n",
    "        # C2: trunc_generated_corrected\n",
    "        result['C2_trunc_generated_corrected'] = eval_truncated_corrected(\n",
    "            surr_prefix, doc_text, query, answer)\n",
    "\n",
    "        # C3: trunc_perfect_broken\n",
    "        result['C3_trunc_perfect_broken'] = eval_truncated_broken(\n",
    "            perfect_prefix, doc_text, query, answer)\n",
    "\n",
    "        # C4: trunc_perfect_corrected\n",
    "        result['C4_trunc_perfect_corrected'] = eval_truncated_corrected(\n",
    "            perfect_prefix, doc_text, query, answer)\n",
    "\n",
    "        # C5: trunc_static_corrected\n",
    "        result['C5_trunc_static_corrected'] = eval_truncated_corrected(\n",
    "            static_prefix, doc_text, query, answer)\n",
    "\n",
    "        # ===== Group D: Ordering =====\n",
    "        # D1: trunc_suffix_corrected\n",
    "        result['D1_trunc_suffix_corrected'] = eval_suffix_truncated_corrected(\n",
    "            doc_text, surr_suffix, query, answer)\n",
    "\n",
    "        # ===== Group E: Controls =====\n",
    "        random_prefix = f\"{random_prefix_text}\\n\\n\"\n",
    "\n",
    "        # E1: random_prefix_trunc_corrected\n",
    "        result['E1_random_prefix_trunc_corrected'] = eval_truncated_corrected(\n",
    "            random_prefix, doc_text, query, answer)\n",
    "\n",
    "        # E2: full_random_prefix\n",
    "        result['E2_full_random_prefix'] = eval_with_context(\n",
    "            random_prefix + doc_text, query, answer)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error on sample {idx}: {e}\")\n",
    "        result['error'] = str(e)\n",
    "\n",
    "    results.append(result)\n",
    "\n",
    "    # Progress update every 20 samples\n",
    "    if (idx + 1) % 20 == 0:\n",
    "        valid = [r for r in results if 'error' not in r]\n",
    "        if valid:\n",
    "            a1 = np.mean([r['A1_baseline'] for r in valid])\n",
    "            c1 = np.mean([r['C1_trunc_generated_broken'] for r in valid])\n",
    "            c2 = np.mean([r['C2_trunc_generated_corrected'] for r in valid])\n",
    "            b1 = np.mean([r['B1_full_generated'] for r in valid])\n",
    "            print(f\"  [{idx+1}/{len(samples)}] A1={a1:.4f}, C1(broken)={c1:.4f}, C2(corrected)={c2:.4f}, B1(full)={b1:.4f}\")\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"\\nDone. {len(results)} samples in {elapsed/60:.1f} minutes.\")\n",
    "print(f\"Errors: {sum(1 for r in results if 'error' in r)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out errors\n",
    "valid_results = [r for r in results if 'error' not in r]\n",
    "print(f\"Valid results: {len(valid_results)} / {len(results)}\")\n",
    "\n",
    "# Extract all condition keys\n",
    "condition_keys = [k for k in valid_results[0].keys()\n",
    "                  if k not in ('idx', 'query', 'surrogate', 'similarity', 'error')]\n",
    "print(f\"Conditions: {condition_keys}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build summary table\n",
    "df = pd.DataFrame(valid_results)\n",
    "\n",
    "summary_rows = []\n",
    "baseline = df['A1_baseline'].values\n",
    "\n",
    "for cond in condition_keys:\n",
    "    vals = df[cond].values\n",
    "    delta = baseline - vals  # positive = condition better than baseline\n",
    "    t_stat, p_val = stats.ttest_rel(baseline, vals)\n",
    "    d = np.mean(delta) / np.std(delta, ddof=1) if np.std(delta) > 0 else 0\n",
    "\n",
    "    summary_rows.append({\n",
    "        'Condition': cond,\n",
    "        'Mean NLL': np.mean(vals),\n",
    "        'Std NLL': np.std(vals),\n",
    "        'Mean Delta': np.mean(delta),\n",
    "        'Win Rate': np.mean(delta > 0),\n",
    "        't-stat': t_stat,\n",
    "        'p-value': p_val,\n",
    "        'Cohen\\'s d': d,\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_rows)\n",
    "summary_df = summary_df.sort_values('Mean NLL')\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"ALL CONDITIONS vs BASELINE (A1) — sorted by Mean NLL (lower = better)\")\n",
    "print(\"=\" * 100)\n",
    "print(\"Positive Delta = condition outperforms baseline\")\n",
    "print()\n",
    "print(summary_df.to_string(index=False, float_format='%.4f'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Critical Comparison: C1 (broken) vs C2 (corrected) =====\n",
    "print(\"=\" * 80)\n",
    "print(\"CRITICAL TEST: Does RoPE correction fix truncation?\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "c1 = df['C1_trunc_generated_broken'].values\n",
    "c2 = df['C2_trunc_generated_corrected'].values\n",
    "a1 = df['A1_baseline'].values\n",
    "\n",
    "t_c1_c2, p_c1_c2 = stats.ttest_rel(c1, c2)\n",
    "t_c2_a1, p_c2_a1 = stats.ttest_rel(a1, c2)\n",
    "\n",
    "print(f\"\\nC1 (broken truncation):    Mean NLL = {c1.mean():.4f} ± {c1.std():.4f}\")\n",
    "print(f\"C2 (corrected truncation): Mean NLL = {c2.mean():.4f} ± {c2.std():.4f}\")\n",
    "print(f\"A1 (baseline):             Mean NLL = {a1.mean():.4f} ± {a1.std():.4f}\")\n",
    "\n",
    "print(f\"\\nC1 vs C2: t={t_c1_c2:.3f}, p={p_c1_c2:.2e}\")\n",
    "print(f\"C2 vs A1: t={t_c2_a1:.3f}, p={p_c2_a1:.2e}\")\n",
    "print(f\"\\nC2 improvement over C1: {(c1.mean() - c2.mean()):.4f} NLL\")\n",
    "print(f\"C2 vs A1 delta: {(a1.mean() - c2.mean()):.4f} NLL (positive = C2 better)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Surrogate quality comparison =====\n",
    "print(\"=\" * 80)\n",
    "print(\"SURROGATE QUALITY COMPARISON (all with RoPE correction)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for cond, label in [\n",
    "    ('C2_trunc_generated_corrected', 'Generated surrogate'),\n",
    "    ('C4_trunc_perfect_corrected', 'Perfect (actual query)'),\n",
    "    ('C5_trunc_static_corrected', 'Static query'),\n",
    "    ('E1_random_prefix_trunc_corrected', 'Random prefix'),\n",
    "]:\n",
    "    vals = df[cond].values\n",
    "    delta = a1 - vals\n",
    "    print(f\"  {label:30s}: NLL={vals.mean():.4f}, delta vs baseline={delta.mean():+.4f}, win={np.mean(delta>0)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Full context vs Truncated comparison =====\n",
    "print(\"=\" * 80)\n",
    "print(\"FULL CONTEXT vs TRUNCATED (is benefit baked into cache or from inference-time attention?)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for full_cond, trunc_cond, label in [\n",
    "    ('B1_full_generated', 'C2_trunc_generated_corrected', 'Generated'),\n",
    "    ('B2_full_perfect', 'C4_trunc_perfect_corrected', 'Perfect (actual query)'),\n",
    "    ('B3_full_static', 'C5_trunc_static_corrected', 'Static'),\n",
    "]:\n",
    "    full_v = df[full_cond].values\n",
    "    trunc_v = df[trunc_cond].values\n",
    "    print(f\"  {label:25s}: Full={full_v.mean():.4f}, Trunc(corrected)={trunc_v.mean():.4f}, gap={full_v.mean()-trunc_v.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Ordering effects =====\n",
    "print(\"=\" * 80)\n",
    "print(\"ORDERING EFFECTS (prefix vs suffix)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "c2_v = df['C2_trunc_generated_corrected'].values\n",
    "d1_v = df['D1_trunc_suffix_corrected'].values\n",
    "t_ord, p_ord = stats.ttest_rel(c2_v, d1_v)\n",
    "\n",
    "print(f\"  C2 (prefix, truncated+corrected): NLL={c2_v.mean():.4f}\")\n",
    "print(f\"  D1 (suffix, truncated+corrected): NLL={d1_v.mean():.4f}\")\n",
    "print(f\"  t={t_ord:.3f}, p={p_ord:.4f}\")\n",
    "\n",
    "b1_v = df['B1_full_generated'].values\n",
    "b4_v = df['B4_full_suffix'].values\n",
    "t_ord2, p_ord2 = stats.ttest_rel(b1_v, b4_v)\n",
    "\n",
    "print(f\"\\n  B1 (prefix, full context): NLL={b1_v.mean():.4f}\")\n",
    "print(f\"  B4 (suffix, full context): NLL={b4_v.mean():.4f}\")\n",
    "print(f\"  t={t_ord2:.3f}, p={p_ord2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Similarity correlation =====\n",
    "print(\"=\" * 80)\n",
    "print(\"SURROGATE SIMILARITY CORRELATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "sims = df['similarity'].values\n",
    "\n",
    "for cond, label in [\n",
    "    ('C2_trunc_generated_corrected', 'C2 trunc corrected'),\n",
    "    ('B1_full_generated', 'B1 full generated'),\n",
    "]:\n",
    "    deltas = a1 - df[cond].values\n",
    "    r, p = stats.pearsonr(sims, deltas)\n",
    "    print(f\"  {label:30s}: r={r:.3f}, p={p:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Win rate by baseline quality bins =====\n",
    "print(\"=\" * 80)\n",
    "print(\"WIN RATE BY BASELINE QUALITY (C2 vs A1)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "baseline_q = pd.qcut(df['A1_baseline'], q=4, labels=['Easy (low NLL)', 'Medium-Easy', 'Medium-Hard', 'Hard (high NLL)'])\n",
    "\n",
    "for label in ['Easy (low NLL)', 'Medium-Easy', 'Medium-Hard', 'Hard (high NLL)']:\n",
    "    mask = baseline_q == label\n",
    "    n = mask.sum()\n",
    "    delta = df.loc[mask, 'A1_baseline'].values - df.loc[mask, 'C2_trunc_generated_corrected'].values\n",
    "    wr = np.mean(delta > 0) * 100\n",
    "    print(f\"  {label:20s} (n={n:3d}): win rate={wr:.1f}%, mean delta={delta.mean():+.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# --- Plot 1: Bar chart of all conditions ---\n",
    "ax = axes[0, 0]\n",
    "plot_conds = [\n",
    "    'A1_baseline', 'A2_bare_doc', 'A3_baseline_offset',\n",
    "    'B1_full_generated', 'B2_full_perfect', 'B3_full_static', 'B4_full_suffix',\n",
    "    'C1_trunc_generated_broken', 'C2_trunc_generated_corrected',\n",
    "    'C3_trunc_perfect_broken', 'C4_trunc_perfect_corrected', 'C5_trunc_static_corrected',\n",
    "    'D1_trunc_suffix_corrected',\n",
    "    'E1_random_prefix_trunc_corrected', 'E2_full_random_prefix',\n",
    "]\n",
    "means = [df[c].mean() for c in plot_conds]\n",
    "stds = [df[c].std() / np.sqrt(len(df)) for c in plot_conds]  # SEM\n",
    "colors = (['#2196F3'] * 3 + ['#4CAF50'] * 4 + ['#F44336', '#FF9800'] +\n",
    "          ['#F44336', '#FF9800', '#FF9800'] + ['#9C27B0'] + ['#795548'] * 2)\n",
    "\n",
    "bars = ax.barh(range(len(plot_conds)), means, xerr=stds, color=colors, alpha=0.8)\n",
    "ax.set_yticks(range(len(plot_conds)))\n",
    "ax.set_yticklabels([c.split('_', 1)[1] for c in plot_conds], fontsize=8)\n",
    "ax.set_xlabel('Mean NLL (lower = better)')\n",
    "ax.set_title('All Conditions: Mean NLL')\n",
    "ax.axvline(x=df['A1_baseline'].mean(), color='blue', linestyle='--', alpha=0.5, label='Baseline')\n",
    "ax.invert_yaxis()\n",
    "ax.legend(fontsize=8)\n",
    "\n",
    "# --- Plot 2: Critical comparison C1 vs C2 ---\n",
    "ax = axes[0, 1]\n",
    "conds = ['A1_baseline', 'C1_trunc_generated_broken', 'C2_trunc_generated_corrected', 'B1_full_generated']\n",
    "labels = ['A1: Baseline', 'C1: Broken\\n(no RoPE fix)', 'C2: Corrected\\n(RoPE fix)', 'B1: Full\\n(surrogate kept)']\n",
    "vals = [df[c].mean() for c in conds]\n",
    "errs = [df[c].std() / np.sqrt(len(df)) for c in conds]\n",
    "colors2 = ['#2196F3', '#F44336', '#FF9800', '#4CAF50']\n",
    "ax.bar(range(4), vals, yerr=errs, color=colors2, alpha=0.8)\n",
    "ax.set_xticks(range(4))\n",
    "ax.set_xticklabels(labels, fontsize=9)\n",
    "ax.set_ylabel('Mean NLL')\n",
    "ax.set_title('Critical Test: RoPE Correction Effect')\n",
    "ax.axhline(y=df['A1_baseline'].mean(), color='blue', linestyle='--', alpha=0.5)\n",
    "\n",
    "# --- Plot 3: Distribution of C2 deltas ---\n",
    "ax = axes[1, 0]\n",
    "delta_c2 = df['A1_baseline'].values - df['C2_trunc_generated_corrected'].values\n",
    "ax.hist(delta_c2, bins=30, color='#FF9800', alpha=0.7, edgecolor='black')\n",
    "ax.axvline(x=0, color='red', linestyle='--', linewidth=2)\n",
    "ax.axvline(x=delta_c2.mean(), color='blue', linestyle='-', linewidth=2, label=f'Mean={delta_c2.mean():.4f}')\n",
    "ax.set_xlabel('Delta NLL (positive = C2 better than baseline)')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('C2 (Corrected Truncation) vs Baseline: Delta Distribution')\n",
    "ax.legend()\n",
    "\n",
    "# --- Plot 4: Similarity vs Delta scatter ---\n",
    "ax = axes[1, 1]\n",
    "ax.scatter(df['similarity'], delta_c2, alpha=0.4, s=20, color='#FF9800')\n",
    "# Trend line\n",
    "z = np.polyfit(df['similarity'].values, delta_c2, 1)\n",
    "p_line = np.poly1d(z)\n",
    "x_line = np.linspace(df['similarity'].min(), df['similarity'].max(), 100)\n",
    "ax.plot(x_line, p_line(x_line), 'r-', linewidth=2)\n",
    "r, p = stats.pearsonr(df['similarity'].values, delta_c2)\n",
    "ax.set_xlabel('Surrogate-Query Similarity')\n",
    "ax.set_ylabel('Delta NLL (positive = C2 better)')\n",
    "ax.set_title(f'Similarity vs C2 Benefit (r={r:.3f}, p={p:.4f})')\n",
    "ax.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/exp01/directed_kvcache_results.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"Saved: directed_kvcache_results.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save raw results\n",
    "output_path = 'results/exp01/directed_kvcache_experiment_results.json'\n",
    "with open(output_path, 'w') as f:\n",
    "    json.dump(results, f, indent=2, default=str)\n",
    "print(f\"Saved raw results to {output_path}\")\n",
    "\n",
    "# Save summary table\n",
    "summary_path = 'results/exp01/directed_kvcache_experiment_summary.csv'\n",
    "summary_df.to_csv(summary_path, index=False)\n",
    "print(f\"Saved summary to {summary_path}\")\n",
    "\n",
    "# Print final summary\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"EXPERIMENT COMPLETE\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Samples: {len(valid_results)}\")\n",
    "print(f\"\\nKey findings:\")\n",
    "print(f\"  Baseline (A1):           {df['A1_baseline'].mean():.4f}\")\n",
    "print(f\"  Broken truncation (C1):  {df['C1_trunc_generated_broken'].mean():.4f}\")\n",
    "print(f\"  Corrected truncation (C2): {df['C2_trunc_generated_corrected'].mean():.4f}\")\n",
    "print(f\"  Full context (B1):       {df['B1_full_generated'].mean():.4f}\")\n",
    "print(f\"  Perfect+corrected (C4):  {df['C4_trunc_perfect_corrected'].mean():.4f}\")\n",
    "print(f\"\\n  RoPE fix improvement (C1→C2): {(df['C1_trunc_generated_broken'].mean() - df['C2_trunc_generated_corrected'].mean()):.4f}\")\n",
    "print(f\"  C2 vs baseline:          {(df['A1_baseline'].mean() - df['C2_trunc_generated_corrected'].mean()):+.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
