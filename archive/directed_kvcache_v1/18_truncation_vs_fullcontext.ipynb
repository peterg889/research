{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 18: Truncation vs Full-Context Comparison\n",
    "\n",
    "**Date:** 2026-02-04\n",
    "\n",
    "## Critical Question\n",
    "\n",
    "Experiments 15-17 found that \"random beats oracle\" - but they used **full-context** priming\n",
    "(prefix stays visible to query). This conflates two mechanisms:\n",
    "\n",
    "1. **Value contamination**: Document values change during forward pass through prefix\n",
    "2. **Attention interference**: Query competes with visible prefix for attention weight\n",
    "\n",
    "**This experiment** tests both **TRUNCATION** (prefix removed after building cache) and\n",
    "**FULL-CONTEXT** (prefix stays visible) to isolate these mechanisms.\n",
    "\n",
    "## Hypothesis\n",
    "\n",
    "- With **truncation**, semantic prefixes (oracle) should help (Exps 05-14 showed d=0.15-0.25)\n",
    "- With **full-context**, semantic prefixes may hurt due to interference (Exps 15-17)\n",
    "- The \"random beats oracle\" finding may be specific to full-context, not truncation\n",
    "\n",
    "## Experimental Conditions\n",
    "\n",
    "| Condition | Build | Score | What it tests |\n",
    "|-----------|-------|-------|---------------|\n",
    "| `bare` | `[document]` | Query sees document | Baseline |\n",
    "| `oracle_5x_truncated` | `[oracle×5][doc]` → truncate → RoPE | Query sees only doc | Value contamination (semantic) |\n",
    "| `random_5x_truncated` | `[random×5][doc]` → truncate → RoPE | Query sees only doc | Value contamination (structural) |\n",
    "| `oracle_5x_fullctx` | `[oracle×5][doc]` | Query sees prefix + doc | Contamination + attention |\n",
    "| `random_5x_fullctx` | `[random×5][doc]` | Query sees prefix + doc | Contamination + attention |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.10.0+cu128\n",
      "CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Setup and Imports\n",
    "import os\n",
    "os.umask(0o000)  # Fix permissions for two-user environment\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '/home/jupyter/research/directed_kvcache')\n",
    "\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from collections import defaultdict\n",
    "from typing import Dict, List, Tuple, Any, Optional\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, DynamicCache\n",
    "from datasets import load_dataset\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Library imports\n",
    "from lib.kv_cache import (\n",
    "    build_kv_cache,\n",
    "    build_cache_with_mask,\n",
    "    extract_and_truncate_cache_with_bos,\n",
    "    correct_rope_positions_with_bos,\n",
    "    score_answer_with_cache,\n",
    "    deepcopy_cache,\n",
    ")\n",
    "from lib.config import ExperimentConfig\n",
    "from lib.analysis import compute_ranking_metrics\n",
    "\n",
    "# Seed for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "050c69bc83734313bebf31f9fa8226cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/291 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on cuda:0\n",
      "Model dtype: torch.bfloat16\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Load Model and Tokenizer\n",
    "MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "# Create config\n",
    "config = ExperimentConfig(\n",
    "    model_name=MODEL_NAME,\n",
    "    device=model.device,\n",
    "    seed=SEED\n",
    ")\n",
    "\n",
    "print(f\"Model loaded on {model.device}\")\n",
    "print(f\"Model dtype: {model.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MS MARCO samples with answers: 80142\n",
      "Selected 500 samples for experiment\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Load Data\n",
    "# Load MS MARCO for main test\n",
    "marco = load_dataset(\"ms_marco\", \"v1.1\", split=\"train\")\n",
    "\n",
    "# Filter to samples with answers\n",
    "marco_with_answers = [s for s in marco if s['answers'] and s['answers'][0] and len(s['answers'][0]) > 0]\n",
    "print(f\"MS MARCO samples with answers: {len(marco_with_answers)}\")\n",
    "\n",
    "# Sample for experiment\n",
    "N_SAMPLES = 500\n",
    "random.shuffle(marco_with_answers)\n",
    "samples = marco_with_answers[:N_SAMPLES]\n",
    "\n",
    "print(f\"Selected {len(samples)} samples for experiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb51bed0418d4343be8c0bb405c8180c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing query embeddings...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea0e714927da4e3f844e3b96b98b3c40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed 500 query embeddings\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Load Embedding Model for Distractor Selection\n",
    "embed_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Pre-compute embeddings for all queries\n",
    "print(\"Computing query embeddings...\")\n",
    "all_queries = [s['query'] for s in samples]\n",
    "query_embeddings = embed_model.encode(all_queries, show_progress_bar=True)\n",
    "print(f\"Computed {len(query_embeddings)} query embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper functions defined.\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Helper Functions\n",
    "\n",
    "def build_bare_cache(passage: str) -> Tuple[DynamicCache, int]:\n",
    "    \"\"\"Build baseline cache from passage only.\"\"\"\n",
    "    ids = tokenizer.encode(passage, return_tensors='pt', add_special_tokens=True).to(model.device)\n",
    "    with torch.no_grad():\n",
    "        out = model(ids, use_cache=True)\n",
    "    return out.past_key_values, ids.shape[1]\n",
    "\n",
    "\n",
    "def build_primed_cache_fullcontext(prefix: str, passage: str) -> Tuple[DynamicCache, int]:\n",
    "    \"\"\"Build full-context cache: [prefix][passage], prefix stays visible.\"\"\"\n",
    "    text = prefix + \" \" + passage\n",
    "    ids = tokenizer.encode(text, return_tensors='pt', add_special_tokens=True).to(model.device)\n",
    "    with torch.no_grad():\n",
    "        out = model(ids, use_cache=True)\n",
    "    return out.past_key_values, ids.shape[1]\n",
    "\n",
    "\n",
    "def build_primed_cache_truncated(prefix: str, passage: str) -> Tuple[DynamicCache, int]:\n",
    "    \"\"\"\n",
    "    Build truncated cache: [prefix][passage] -> truncate prefix -> RoPE correct.\n",
    "    Returns cache containing only document tokens (with BOS).\n",
    "    \"\"\"\n",
    "    # Tokenize prefix to get its length\n",
    "    prefix_with_sep = prefix + \" \"\n",
    "    prefix_ids = tokenizer.encode(prefix_with_sep, return_tensors='pt', add_special_tokens=True)\n",
    "    prefix_len = prefix_ids.shape[1]  # includes BOS\n",
    "    \n",
    "    # Tokenize full text\n",
    "    full_text = prefix_with_sep + passage\n",
    "    full_ids = tokenizer.encode(full_text, return_tensors='pt', add_special_tokens=True).to(model.device)\n",
    "    full_len = full_ids.shape[1]\n",
    "    doc_len = full_len - prefix_len  # document tokens (without BOS)\n",
    "    \n",
    "    # Build full cache\n",
    "    with torch.no_grad():\n",
    "        out = model(full_ids, use_cache=True)\n",
    "    \n",
    "    # Truncate: keep BOS + document portion\n",
    "    truncated_cache = extract_and_truncate_cache_with_bos(out.past_key_values, doc_len)\n",
    "    \n",
    "    # RoPE correction: shift document keys back by (prefix_len - 1) positions\n",
    "    # (BOS stays at position 0, document tokens shift from prefix_len..full_len-1 to 1..doc_len)\n",
    "    surrogate_offset = prefix_len - 1\n",
    "    correct_rope_positions_with_bos(truncated_cache, surrogate_offset, model)\n",
    "    \n",
    "    keep_len = 1 + doc_len  # BOS + document\n",
    "    return truncated_cache, keep_len\n",
    "\n",
    "\n",
    "def score_with_cache(cache: DynamicCache, cache_len: int, query: str, answer: str) -> float:\n",
    "    \"\"\"Score P(answer | cache, query) using NLL.\"\"\"\n",
    "    return score_answer_with_cache(\n",
    "        cache, cache_len,\n",
    "        f\"\\n\\nQuery: {query}\\nAnswer:\",\n",
    "        \" \" + answer,\n",
    "        model, tokenizer, config\n",
    "    )\n",
    "\n",
    "\n",
    "def get_random_queries(n: int, exclude_idx: int) -> List[str]:\n",
    "    \"\"\"Get n random queries from dataset, excluding the specified index.\"\"\"\n",
    "    indices = [i for i in range(len(samples)) if i != exclude_idx]\n",
    "    selected = random.sample(indices, min(n, len(indices)))\n",
    "    return [samples[i]['query'] for i in selected]\n",
    "\n",
    "\n",
    "def make_prefix_5x(query: str) -> str:\n",
    "    \"\"\"Create 5x repeated prefix from query.\"\"\"\n",
    "    return \" \".join([query] * 5)\n",
    "\n",
    "\n",
    "print(\"Helper functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "EXPERIMENTAL CONDITIONS EXPLAINED\n",
      "======================================================================\n",
      "\n",
      "### BASELINE: bare ###\n",
      "Build:  [BOS][passage]\n",
      "Score:  Query attends to [BOS][passage]\n",
      "Cache:  'Paris is the capital and largest city of France. I...'\n",
      "Tests:  Baseline performance with no priming\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "### TRUNCATED CONDITIONS ###\n",
      "These test PURE VALUE CONTAMINATION (prefix removed before scoring)\n",
      "\n",
      "## oracle_5x_truncated ##\n",
      "Build:  [BOS][oracle oracle oracle oracle oracle][passage]\n",
      "        -> Forward pass (document values influenced by oracle)\n",
      "        -> Truncate to [BOS][passage]\n",
      "        -> Apply RoPE correction to fix key positions\n",
      "Score:  Query attends to [BOS][passage] (oracle REMOVED)\n",
      "Prefix: 'What is the capital of France? ' * 5\n",
      "Tests:  Does SEMANTIC value contamination help?\n",
      "\n",
      "## random_5x_truncated ##\n",
      "Build:  [BOS][rand1 rand2 rand3 rand4 rand5][passage]\n",
      "        -> Same truncation + RoPE process\n",
      "Score:  Query attends to [BOS][passage] (random REMOVED)\n",
      "Prefix: 'How to train a dog? What causes earthqua...'\n",
      "Tests:  Does STRUCTURAL value contamination help?\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "### FULL-CONTEXT CONDITIONS ###\n",
      "These test VALUE CONTAMINATION + ATTENTION INTERFERENCE (prefix visible)\n",
      "\n",
      "## oracle_5x_fullctx ##\n",
      "Build:  [BOS][oracle oracle oracle oracle oracle][passage]\n",
      "Score:  Query attends to [BOS][oracle×5][passage] (oracle VISIBLE)\n",
      "Prefix: 'What is the capital of France? ' * 5\n",
      "Tests:  Contamination + interference. Oracle may compete with real query.\n",
      "\n",
      "## random_5x_fullctx ##\n",
      "Build:  [BOS][rand1 rand2 rand3 rand4 rand5][passage]\n",
      "Score:  Query attends to [BOS][random×5][passage] (random VISIBLE)\n",
      "Prefix: 'How to train a dog? What causes earthqua...'\n",
      "Tests:  Structural benefit without semantic interference.\n",
      "\n",
      "======================================================================\n",
      "KEY COMPARISONS:\n",
      "- oracle_truncated vs random_truncated: Does semantic content help with pure contamination?\n",
      "- oracle_fullctx vs random_fullctx: Does interference hurt semantic prefixes?\n",
      "- truncated vs fullctx (same prefix): Does removing prefix help or hurt?\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Explain Experimental Conditions (Documentation)\n",
    "print(\"=\"*70)\n",
    "print(\"EXPERIMENTAL CONDITIONS EXPLAINED\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "example_query = \"What is the capital of France?\"\n",
    "example_passage = \"Paris is the capital and largest city of France. It is located on the Seine River.\"\n",
    "example_random = \"How to train a dog? What causes earthquakes? Best pizza recipe?\"\n",
    "\n",
    "print(\"\\n### BASELINE: bare ###\")\n",
    "print(\"Build:  [BOS][passage]\")\n",
    "print(\"Score:  Query attends to [BOS][passage]\")\n",
    "print(f\"Cache:  '{example_passage[:50]}...'\")\n",
    "print(\"Tests:  Baseline performance with no priming\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "\n",
    "print(\"\\n### TRUNCATED CONDITIONS ###\")\n",
    "print(\"These test PURE VALUE CONTAMINATION (prefix removed before scoring)\")\n",
    "\n",
    "print(\"\\n## oracle_5x_truncated ##\")\n",
    "print(\"Build:  [BOS][oracle oracle oracle oracle oracle][passage]\")\n",
    "print(\"        -> Forward pass (document values influenced by oracle)\")\n",
    "print(\"        -> Truncate to [BOS][passage]\")\n",
    "print(\"        -> Apply RoPE correction to fix key positions\")\n",
    "print(\"Score:  Query attends to [BOS][passage] (oracle REMOVED)\")\n",
    "print(f\"Prefix: '{example_query} ' * 5\")\n",
    "print(\"Tests:  Does SEMANTIC value contamination help?\")\n",
    "\n",
    "print(\"\\n## random_5x_truncated ##\")\n",
    "print(\"Build:  [BOS][rand1 rand2 rand3 rand4 rand5][passage]\")\n",
    "print(\"        -> Same truncation + RoPE process\")\n",
    "print(\"Score:  Query attends to [BOS][passage] (random REMOVED)\")\n",
    "print(f\"Prefix: '{example_random[:40]}...'\")\n",
    "print(\"Tests:  Does STRUCTURAL value contamination help?\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "\n",
    "print(\"\\n### FULL-CONTEXT CONDITIONS ###\")\n",
    "print(\"These test VALUE CONTAMINATION + ATTENTION INTERFERENCE (prefix visible)\")\n",
    "\n",
    "print(\"\\n## oracle_5x_fullctx ##\")\n",
    "print(\"Build:  [BOS][oracle oracle oracle oracle oracle][passage]\")\n",
    "print(\"Score:  Query attends to [BOS][oracle×5][passage] (oracle VISIBLE)\")\n",
    "print(f\"Prefix: '{example_query} ' * 5\")\n",
    "print(\"Tests:  Contamination + interference. Oracle may compete with real query.\")\n",
    "\n",
    "print(\"\\n## random_5x_fullctx ##\")\n",
    "print(\"Build:  [BOS][rand1 rand2 rand3 rand4 rand5][passage]\")\n",
    "print(\"Score:  Query attends to [BOS][random×5][passage] (random VISIBLE)\")\n",
    "print(f\"Prefix: '{example_random[:40]}...'\")\n",
    "print(\"Tests:  Structural benefit without semantic interference.\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"KEY COMPARISONS:\")\n",
    "print(\"- oracle_truncated vs random_truncated: Does semantic content help with pure contamination?\")\n",
    "print(\"- oracle_fullctx vs random_fullctx: Does interference hurt semantic prefixes?\")\n",
    "print(\"- truncated vs fullctx (same prefix): Does removing prefix help or hurt?\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 500 samples...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd8fffd8d9264eedb067fa75640c34a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at 50 samples\n",
      "Checkpoint saved at 100 samples\n",
      "Checkpoint saved at 150 samples\n",
      "Checkpoint saved at 200 samples\n",
      "Checkpoint saved at 250 samples\n",
      "Checkpoint saved at 300 samples\n",
      "Checkpoint saved at 350 samples\n",
      "Checkpoint saved at 400 samples\n",
      "Checkpoint saved at 450 samples\n",
      "Checkpoint saved at 500 samples\n",
      "Saved 500 results to /home/jupyter/research/directed_kvcache/results/exp18/results.json\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Main Experiment Loop\n",
    "\n",
    "results = []\n",
    "OUTPUT_DIR = '/home/jupyter/research/directed_kvcache/results/exp18'\n",
    "CHECKPOINT_PATH = f'{OUTPUT_DIR}/checkpoint.json'\n",
    "\n",
    "# Load checkpoint if exists\n",
    "start_idx = 0\n",
    "if os.path.exists(CHECKPOINT_PATH):\n",
    "    with open(CHECKPOINT_PATH, 'r') as f:\n",
    "        checkpoint = json.load(f)\n",
    "        results = checkpoint.get('results', [])\n",
    "        start_idx = len(results)\n",
    "        print(f\"Resuming from checkpoint at sample {start_idx}\")\n",
    "\n",
    "print(f\"Processing {N_SAMPLES - start_idx} samples...\")\n",
    "\n",
    "for idx in tqdm(range(start_idx, N_SAMPLES)):\n",
    "    sample = samples[idx]\n",
    "    passage = sample['passages']['passage_text'][0] if sample['passages']['passage_text'] else \"\"\n",
    "    query = sample['query']\n",
    "    answer = sample['answers'][0]\n",
    "    \n",
    "    if not passage or not answer:\n",
    "        continue\n",
    "    \n",
    "    # Create prefixes\n",
    "    oracle_prefix = make_prefix_5x(query)\n",
    "    random_queries = get_random_queries(5, idx)\n",
    "    random_prefix = \" \".join(random_queries)\n",
    "    \n",
    "    try:\n",
    "        # Build all caches\n",
    "        bare_cache, bare_len = build_bare_cache(passage)\n",
    "        oracle_trunc_cache, oracle_trunc_len = build_primed_cache_truncated(oracle_prefix, passage)\n",
    "        random_trunc_cache, random_trunc_len = build_primed_cache_truncated(random_prefix, passage)\n",
    "        oracle_full_cache, oracle_full_len = build_primed_cache_fullcontext(oracle_prefix, passage)\n",
    "        random_full_cache, random_full_len = build_primed_cache_fullcontext(random_prefix, passage)\n",
    "        \n",
    "        # Score all conditions (deepcopy to avoid mutation)\n",
    "        nll_bare = score_with_cache(deepcopy_cache(bare_cache), bare_len, query, answer)\n",
    "        nll_oracle_trunc = score_with_cache(deepcopy_cache(oracle_trunc_cache), oracle_trunc_len, query, answer)\n",
    "        nll_random_trunc = score_with_cache(deepcopy_cache(random_trunc_cache), random_trunc_len, query, answer)\n",
    "        nll_oracle_full = score_with_cache(deepcopy_cache(oracle_full_cache), oracle_full_len, query, answer)\n",
    "        nll_random_full = score_with_cache(deepcopy_cache(random_full_cache), random_full_len, query, answer)\n",
    "        \n",
    "        result = {\n",
    "            'idx': idx,\n",
    "            'query': query,\n",
    "            'answer': answer,\n",
    "            'passage_len': len(passage.split()),\n",
    "            'nll_bare': nll_bare,\n",
    "            'nll_oracle_truncated': nll_oracle_trunc,\n",
    "            'nll_random_truncated': nll_random_trunc,\n",
    "            'nll_oracle_fullctx': nll_oracle_full,\n",
    "            'nll_random_fullctx': nll_random_full,\n",
    "            # Deltas (positive = priming helped)\n",
    "            'delta_oracle_truncated': nll_bare - nll_oracle_trunc,\n",
    "            'delta_random_truncated': nll_bare - nll_random_trunc,\n",
    "            'delta_oracle_fullctx': nll_bare - nll_oracle_full,\n",
    "            'delta_random_fullctx': nll_bare - nll_random_full,\n",
    "        }\n",
    "        results.append(result)\n",
    "        \n",
    "        # Checkpoint every 50 samples\n",
    "        if len(results) % 50 == 0:\n",
    "            with open(CHECKPOINT_PATH, 'w') as f:\n",
    "                json.dump({'results': results}, f)\n",
    "            print(f\"Checkpoint saved at {len(results)} samples\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error at sample {idx}: {e}\")\n",
    "        continue\n",
    "\n",
    "# Final save\n",
    "with open(f'{OUTPUT_DIR}/results.json', 'w') as f:\n",
    "    json.dump({'results': results}, f, indent=2)\n",
    "print(f\"Saved {len(results)} results to {OUTPUT_DIR}/results.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "EXPERIMENT 18 RESULTS: Truncation vs Full-Context\n",
      "======================================================================\n",
      "\n",
      "Samples analyzed: 500\n",
      "\n",
      "======================================================================\n",
      "NLL BY CONDITION (lower is better)\n",
      "======================================================================\n",
      "Condition                     Mean NLL        Std       Win%    Cohen d\n",
      "----------------------------------------------------------------------\n",
      "bare (baseline)                 2.2359     1.8192         --         --\n",
      "----------------------------------------------------------------------\n",
      "oracle_5x_truncated             2.2722     1.8597      41.4%     -0.105\n",
      "random_5x_truncated             2.3750     1.9641      30.0%     -0.288\n",
      "----------------------------------------------------------------------\n",
      "oracle_5x_fullctx               2.4159     2.0258      31.6%     -0.293\n",
      "random_5x_fullctx               2.4214     2.0210      30.8%     -0.315\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Analysis\n",
    "from scipy import stats\n",
    "\n",
    "def cohens_d(x):\n",
    "    \"\"\"Cohen's d for a difference array.\"\"\"\n",
    "    return np.mean(x) / np.std(x, ddof=1) if np.std(x) > 0 else 0\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"EXPERIMENT 18 RESULTS: Truncation vs Full-Context\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "n = len(results)\n",
    "print(f\"\\nSamples analyzed: {n}\")\n",
    "\n",
    "# Extract arrays\n",
    "bare = np.array([r['nll_bare'] for r in results])\n",
    "oracle_trunc = np.array([r['nll_oracle_truncated'] for r in results])\n",
    "random_trunc = np.array([r['nll_random_truncated'] for r in results])\n",
    "oracle_full = np.array([r['nll_oracle_fullctx'] for r in results])\n",
    "random_full = np.array([r['nll_random_fullctx'] for r in results])\n",
    "\n",
    "delta_oracle_trunc = bare - oracle_trunc\n",
    "delta_random_trunc = bare - random_trunc\n",
    "delta_oracle_full = bare - oracle_full\n",
    "delta_random_full = bare - random_full\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"NLL BY CONDITION (lower is better)\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Condition':<25} {'Mean NLL':>12} {'Std':>10} {'Win%':>10} {'Cohen d':>10}\")\n",
    "print(\"-\"*70)\n",
    "print(f\"{'bare (baseline)':<25} {np.mean(bare):>12.4f} {np.std(bare):>10.4f} {'--':>10} {'--':>10}\")\n",
    "print(\"-\"*70)\n",
    "print(f\"{'oracle_5x_truncated':<25} {np.mean(oracle_trunc):>12.4f} {np.std(oracle_trunc):>10.4f} {np.mean(delta_oracle_trunc > 0)*100:>9.1f}% {cohens_d(delta_oracle_trunc):>10.3f}\")\n",
    "print(f\"{'random_5x_truncated':<25} {np.mean(random_trunc):>12.4f} {np.std(random_trunc):>10.4f} {np.mean(delta_random_trunc > 0)*100:>9.1f}% {cohens_d(delta_random_trunc):>10.3f}\")\n",
    "print(\"-\"*70)\n",
    "print(f\"{'oracle_5x_fullctx':<25} {np.mean(oracle_full):>12.4f} {np.std(oracle_full):>10.4f} {np.mean(delta_oracle_full > 0)*100:>9.1f}% {cohens_d(delta_oracle_full):>10.3f}\")\n",
    "print(f\"{'random_5x_fullctx':<25} {np.mean(random_full):>12.4f} {np.std(random_full):>10.4f} {np.mean(delta_random_full > 0)*100:>9.1f}% {cohens_d(delta_random_full):>10.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "KEY COMPARISONS (paired t-tests)\n",
      "======================================================================\n",
      "\n",
      "1. TRUNCATED: Oracle vs Random\n",
      "   Oracle wins: 61.8%\n",
      "   t=-5.309, p=0.0000\n",
      "   -> Oracle significantly better (p<0.05)\n",
      "\n",
      "2. FULL-CONTEXT: Oracle vs Random\n",
      "   Oracle wins: 50.0%\n",
      "   t=-0.206, p=0.8371\n",
      "   -> No significant difference\n",
      "\n",
      "3. ORACLE: Truncated vs Full-Context\n",
      "   Truncated wins: 62.6%\n",
      "   t=-7.089, p=0.0000\n",
      "   -> Truncated significantly better (p<0.05)\n",
      "\n",
      "4. RANDOM: Truncated vs Full-Context\n",
      "   Truncated wins: 48.2%\n",
      "   t=-2.956, p=0.0033\n",
      "   -> Truncated significantly better (p<0.05)\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Key Comparisons\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"KEY COMPARISONS (paired t-tests)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Comparison 1: Truncated oracle vs truncated random\n",
    "t1, p1 = stats.ttest_rel(oracle_trunc, random_trunc)\n",
    "oracle_wins_trunc = np.mean(oracle_trunc < random_trunc) * 100\n",
    "print(f\"\\n1. TRUNCATED: Oracle vs Random\")\n",
    "print(f\"   Oracle wins: {oracle_wins_trunc:.1f}%\")\n",
    "print(f\"   t={t1:.3f}, p={p1:.4f}\")\n",
    "if p1 < 0.05:\n",
    "    winner = \"Oracle\" if np.mean(oracle_trunc) < np.mean(random_trunc) else \"Random\"\n",
    "    print(f\"   -> {winner} significantly better (p<0.05)\")\n",
    "else:\n",
    "    print(f\"   -> No significant difference\")\n",
    "\n",
    "# Comparison 2: Full-context oracle vs full-context random\n",
    "t2, p2 = stats.ttest_rel(oracle_full, random_full)\n",
    "oracle_wins_full = np.mean(oracle_full < random_full) * 100\n",
    "print(f\"\\n2. FULL-CONTEXT: Oracle vs Random\")\n",
    "print(f\"   Oracle wins: {oracle_wins_full:.1f}%\")\n",
    "print(f\"   t={t2:.3f}, p={p2:.4f}\")\n",
    "if p2 < 0.05:\n",
    "    winner = \"Oracle\" if np.mean(oracle_full) < np.mean(random_full) else \"Random\"\n",
    "    print(f\"   -> {winner} significantly better (p<0.05)\")\n",
    "else:\n",
    "    print(f\"   -> No significant difference\")\n",
    "\n",
    "# Comparison 3: Truncated vs Full-context (oracle)\n",
    "t3, p3 = stats.ttest_rel(oracle_trunc, oracle_full)\n",
    "trunc_wins_oracle = np.mean(oracle_trunc < oracle_full) * 100\n",
    "print(f\"\\n3. ORACLE: Truncated vs Full-Context\")\n",
    "print(f\"   Truncated wins: {trunc_wins_oracle:.1f}%\")\n",
    "print(f\"   t={t3:.3f}, p={p3:.4f}\")\n",
    "if p3 < 0.05:\n",
    "    winner = \"Truncated\" if np.mean(oracle_trunc) < np.mean(oracle_full) else \"Full-context\"\n",
    "    print(f\"   -> {winner} significantly better (p<0.05)\")\n",
    "else:\n",
    "    print(f\"   -> No significant difference\")\n",
    "\n",
    "# Comparison 4: Truncated vs Full-context (random)\n",
    "t4, p4 = stats.ttest_rel(random_trunc, random_full)\n",
    "trunc_wins_random = np.mean(random_trunc < random_full) * 100\n",
    "print(f\"\\n4. RANDOM: Truncated vs Full-Context\")\n",
    "print(f\"   Truncated wins: {trunc_wins_random:.1f}%\")\n",
    "print(f\"   t={t4:.3f}, p={p4:.4f}\")\n",
    "if p4 < 0.05:\n",
    "    winner = \"Truncated\" if np.mean(random_trunc) < np.mean(random_full) else \"Full-context\"\n",
    "    print(f\"   -> {winner} significantly better (p<0.05)\")\n",
    "else:\n",
    "    print(f\"   -> No significant difference\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "INTERPRETATION\n",
      "======================================================================\n",
      "\n",
      "## Key Questions Answered:\n",
      "\n",
      "Q1: Does TRUNCATED oracle beat TRUNCATED random?\n",
      "    (Tests: Does semantic content help with pure value contamination?)\n",
      "    ANSWER: YES - Oracle wins 61.8% of the time (p=0.0000)\n",
      "    -> Semantic value contamination provides benefit beyond structural.\n",
      "\n",
      "Q2: Does FULL-CONTEXT replicate Exps 15-17 (random beats oracle)?\n",
      "    (Tests: Does semantic interference hurt with visible prefix?)\n",
      "    ANSWER: NO DIFFERENCE - Oracle wins 50.0% (p=0.8371, not significant)\n",
      "\n",
      "Q3: Is TRUNCATION better than FULL-CONTEXT?\n",
      "    (Tests: Does removing prefix help by eliminating interference?)\n",
      "    For ORACLE: YES - Truncated wins 62.6% (p=0.0000)\n",
      "    -> Removing oracle prefix eliminates interference, improves performance.\n",
      "    For RANDOM: YES - Truncated wins 48.2% (p=0.0033)\n"
     ]
    }
   ],
   "source": [
    "# Cell 10: Interpretation\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"INTERPRETATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n## Key Questions Answered:\\n\")\n",
    "\n",
    "# Q1: Does truncated oracle beat truncated random?\n",
    "print(\"Q1: Does TRUNCATED oracle beat TRUNCATED random?\")\n",
    "print(\"    (Tests: Does semantic content help with pure value contamination?)\")\n",
    "if p1 < 0.05 and np.mean(oracle_trunc) < np.mean(random_trunc):\n",
    "    print(f\"    ANSWER: YES - Oracle wins {oracle_wins_trunc:.1f}% of the time (p={p1:.4f})\")\n",
    "    print(\"    -> Semantic value contamination provides benefit beyond structural.\")\n",
    "elif p1 < 0.05 and np.mean(oracle_trunc) > np.mean(random_trunc):\n",
    "    print(f\"    ANSWER: NO - Random wins {100-oracle_wins_trunc:.1f}% of the time (p={p1:.4f})\")\n",
    "    print(\"    -> Even with truncation, random is better. Value contamination is structural.\")\n",
    "else:\n",
    "    print(f\"    ANSWER: NO DIFFERENCE - Oracle wins {oracle_wins_trunc:.1f}% (p={p1:.4f}, not significant)\")\n",
    "    print(\"    -> Semantic content doesn't matter for pure value contamination.\")\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "# Q2: Does full-context replicate Exps 15-17 (random beats oracle)?\n",
    "print(\"Q2: Does FULL-CONTEXT replicate Exps 15-17 (random beats oracle)?\")\n",
    "print(\"    (Tests: Does semantic interference hurt with visible prefix?)\")\n",
    "if p2 < 0.05 and np.mean(random_full) < np.mean(oracle_full):\n",
    "    print(f\"    ANSWER: YES - Random wins {100-oracle_wins_full:.1f}% of the time (p={p2:.4f})\")\n",
    "    print(\"    -> Confirms Exps 15-17: semantic interference hurts with visible prefix.\")\n",
    "elif p2 < 0.05 and np.mean(oracle_full) < np.mean(random_full):\n",
    "    print(f\"    ANSWER: NO - Oracle wins {oracle_wins_full:.1f}% of the time (p={p2:.4f})\")\n",
    "    print(\"    -> Does NOT replicate Exps 15-17. Oracle helps with visible prefix.\")\n",
    "else:\n",
    "    print(f\"    ANSWER: NO DIFFERENCE - Oracle wins {oracle_wins_full:.1f}% (p={p2:.4f}, not significant)\")\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "# Q3: Is truncation better than full-context?\n",
    "print(\"Q3: Is TRUNCATION better than FULL-CONTEXT?\")\n",
    "print(\"    (Tests: Does removing prefix help by eliminating interference?)\")\n",
    "if p3 < 0.05 and np.mean(oracle_trunc) < np.mean(oracle_full):\n",
    "    print(f\"    For ORACLE: YES - Truncated wins {trunc_wins_oracle:.1f}% (p={p3:.4f})\")\n",
    "    print(\"    -> Removing oracle prefix eliminates interference, improves performance.\")\n",
    "else:\n",
    "    print(f\"    For ORACLE: NO - Truncated wins {trunc_wins_oracle:.1f}% (p={p3:.4f})\")\n",
    "\n",
    "if p4 < 0.05 and np.mean(random_trunc) < np.mean(random_full):\n",
    "    print(f\"    For RANDOM: YES - Truncated wins {trunc_wins_random:.1f}% (p={p4:.4f})\")\n",
    "else:\n",
    "    print(f\"    For RANDOM: NO - Truncated wins {trunc_wins_random:.1f}% (p={p4:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis saved to /home/jupyter/research/directed_kvcache/results/exp18/analysis.json\n"
     ]
    }
   ],
   "source": [
    "# Cell 11: Save Final Analysis\n",
    "\n",
    "analysis = {\n",
    "    'n_samples': n,\n",
    "    'mean_bare_nll': float(np.mean(bare)),\n",
    "    'truncated': {\n",
    "        'oracle': {\n",
    "            'mean_nll': float(np.mean(oracle_trunc)),\n",
    "            'win_rate': float(np.mean(delta_oracle_trunc > 0)),\n",
    "            'cohens_d': float(cohens_d(delta_oracle_trunc)),\n",
    "        },\n",
    "        'random': {\n",
    "            'mean_nll': float(np.mean(random_trunc)),\n",
    "            'win_rate': float(np.mean(delta_random_trunc > 0)),\n",
    "            'cohens_d': float(cohens_d(delta_random_trunc)),\n",
    "        },\n",
    "        'oracle_vs_random': {\n",
    "            't_stat': float(t1),\n",
    "            'p_value': float(p1),\n",
    "            'oracle_wins_pct': float(oracle_wins_trunc),\n",
    "        },\n",
    "    },\n",
    "    'fullcontext': {\n",
    "        'oracle': {\n",
    "            'mean_nll': float(np.mean(oracle_full)),\n",
    "            'win_rate': float(np.mean(delta_oracle_full > 0)),\n",
    "            'cohens_d': float(cohens_d(delta_oracle_full)),\n",
    "        },\n",
    "        'random': {\n",
    "            'mean_nll': float(np.mean(random_full)),\n",
    "            'win_rate': float(np.mean(delta_random_full > 0)),\n",
    "            'cohens_d': float(cohens_d(delta_random_full)),\n",
    "        },\n",
    "        'oracle_vs_random': {\n",
    "            't_stat': float(t2),\n",
    "            'p_value': float(p2),\n",
    "            'oracle_wins_pct': float(oracle_wins_full),\n",
    "        },\n",
    "    },\n",
    "    'truncated_vs_fullcontext': {\n",
    "        'oracle': {\n",
    "            't_stat': float(t3),\n",
    "            'p_value': float(p3),\n",
    "            'truncated_wins_pct': float(trunc_wins_oracle),\n",
    "        },\n",
    "        'random': {\n",
    "            't_stat': float(t4),\n",
    "            'p_value': float(p4),\n",
    "            'truncated_wins_pct': float(trunc_wins_random),\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "with open(f'{OUTPUT_DIR}/analysis.json', 'w') as f:\n",
    "    json.dump(analysis, f, indent=2)\n",
    "\n",
    "print(f\"Analysis saved to {OUTPUT_DIR}/analysis.json\")"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cu124.m127",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/base-cu124:m127"
  },
  "kernelspec": {
   "display_name": "Python 3 (Local)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
