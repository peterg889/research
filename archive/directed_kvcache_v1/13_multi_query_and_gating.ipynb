{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6455a857",
   "metadata": {},
   "source": "# Experiment 13: Multi-Query Amplification + Hardness-Gated Priming (Parts A & B)\n\n## Motivation\nExp 12 confirmed a real but tiny semantic signal (r=0.034, CI excludes 0). Every prior experiment used a single prefix query. Two key observations suggest we can amplify the effect:\n\n1. **Multiple queries**: In production ad-serving, documents have click history with many queries. Concatenating K raw queries as prefix could amplify value contamination additively.\n2. **Hard-sample targeting**: Exp 03 found surrogates hurt easy samples (low baseline NLL) and help hard ones. Exp 12's sim_0.75 bin had higher baseline NLL and the biggest benefit (d=0.226). A gated strategy could double effective d.\n\n## Experiment 13, Part A: Multi-Query Prefix Amplification\n**Hypothesis**: Concatenating K high-similarity raw queries as prefix amplifies value contamination, producing larger deltas than a single query.\n\n11 conditions (MS MARCO N=1000, SQuAD N=1000):\n1. `bare` — no prefix\n2. `oracle_1q` — 1 oracle query, raw\n3. `oracle_2q` — oracle + 1 query at sim>0.85, raw\n4. `oracle_3q` — oracle + 2 queries at sim>0.85, raw\n5. `oracle_5q` — oracle + 4 queries at sim>0.85, raw\n6. `real_1q_0.70` — 1 real query at sim>=0.70, raw\n7. `real_3q_0.70` — 3 real queries at sim>=0.70, raw\n8. `real_5q_0.70` — 5 real queries at sim>=0.70, raw\n9. `real_5q_0.50` — 5 real queries at sim>=0.50 (easier to find)\n10. `random_5q` — 5 random queries (structural control)\n11. `repeated_1q_5x` — same single query repeated 5x (repetition control)\n\n## Experiment 13, Part B: Hardness-Gated Priming\n**Hypothesis**: Priming only benefits hard samples. A gating strategy that primes selectively can double the effective Cohen's d.\n\nUses Exp 13A MS MARCO data (bare + oracle_1q already computed) extended to N=2000 with 3 conditions:\n- bare, oracle_1q (raw), real_1q_0.70 (raw)\n- Bins by bare NLL quartile\n- Tests hardness predictors available at indexing time"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b02993",
   "metadata": {},
   "outputs": [],
   "source": "import sys, os, json, copy, time, datetime\nfrom typing import Dict, List, Any, Optional, Tuple\n\nimport torch\nimport numpy as np\nfrom tqdm.auto import tqdm\nfrom scipy import stats\nimport matplotlib.pyplot as plt\nimport matplotlib\nmatplotlib.rcParams['figure.dpi'] = 120\n\nsys.path.insert(0, '.')\n\nfrom lib import (\n    ExperimentConfig,\n    build_kv_cache,\n    score_answer_with_cache,\n    extract_and_truncate_cache_with_bos,\n    correct_rope_positions_with_bos,\n    load_evaluation_samples,\n    load_ms_marco,\n    _ensure_dynamic_cache,\n)\n\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, DynamicCache\nfrom sentence_transformers import SentenceTransformer\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom datasets import load_dataset\n\nprint(f\"PyTorch: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff510f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Configuration\n",
    "# ============================================================\n",
    "\n",
    "config = ExperimentConfig(\n",
    "    num_samples=8000,  # Large pool for surrogate selection\n",
    "    min_passage_words=50,\n",
    "    max_passage_words=300,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "N_EXP13_MARCO = 1000\n",
    "N_EXP13_SQUAD = 1000\n",
    "N_EXP14 = 2000  # Extends Exp 13 MS MARCO samples\n",
    "\n",
    "SEEDS = {'msmarco': 42, 'squad': 43}\n",
    "\n",
    "# Multi-query separator: space between concatenated queries\n",
    "# (Exp 12 showed raw queries work best, so just concatenate with spaces)\n",
    "MQ_SEP = \" \"\n",
    "\n",
    "torch.manual_seed(config.seed)\n",
    "np.random.seed(config.seed)\n",
    "\n",
    "print(f\"Exp 13 MS MARCO: {N_EXP13_MARCO} samples x 11 conditions\")\n",
    "print(f\"Exp 13 SQuAD:    {N_EXP13_SQUAD} samples x 11 conditions\")\n",
    "print(f\"Exp 14 MS MARCO: {N_EXP14} samples x 3 conditions (extends Exp 13)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca92a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Load Model\n",
    "# ============================================================\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    ")\n",
    "\n",
    "print(f\"Loading {config.model_name}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "model.eval()\n",
    "print(f\"Model loaded. Layers: {model.config.num_hidden_layers}\")\n",
    "\n",
    "print(\"Loading embedding model...\")\n",
    "embed_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "print(\"Embedding model loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e857da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Load MS MARCO + Build Query Pool\n",
    "# ============================================================\n",
    "\n",
    "print(\"Loading MS MARCO dataset...\")\n",
    "dataset = load_dataset(\n",
    "    config.dataset_name, config.dataset_config,\n",
    "    split=config.dataset_split, trust_remote_code=True\n",
    ")\n",
    "print(f\"Dataset loaded: {len(dataset)} samples\")\n",
    "\n",
    "all_samples = load_evaluation_samples(dataset, config, require_answer=True)\n",
    "print(f\"Loaded {len(all_samples)} evaluation samples with answers\")\n",
    "\n",
    "# Build query pool from FULL dataset\n",
    "print(\"\\nBuilding query pool (full dataset)...\")\n",
    "query_pool = []\n",
    "seen_queries = set()\n",
    "for item in tqdm(dataset, desc=\"Extracting queries\"):\n",
    "    q = item.get('query', '').strip()\n",
    "    if q and q not in seen_queries and len(q) > 10:\n",
    "        query_pool.append(q)\n",
    "        seen_queries.add(q)\n",
    "print(f\"Query pool size: {len(query_pool)}\")\n",
    "\n",
    "print(\"Embedding query pool...\")\n",
    "pool_embeddings = embed_model.encode(query_pool, show_progress_bar=True, batch_size=256)\n",
    "print(f\"Pool embeddings shape: {pool_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dbdac0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Load SQuAD v2 + Build Query Pool\n",
    "# ============================================================\n",
    "\n",
    "print(\"Loading SQuAD v2...\")\n",
    "squad_dataset = load_dataset(\"rajpurkar/squad_v2\", split=\"validation\")\n",
    "squad_train = load_dataset(\"rajpurkar/squad_v2\", split=\"train\")\n",
    "print(f\"SQuAD val: {len(squad_dataset)}, train: {len(squad_train)}\")\n",
    "\n",
    "np.random.seed(SEEDS['squad'])\n",
    "squad_samples = []\n",
    "for item in squad_dataset:\n",
    "    ctx = item.get('context', '').strip()\n",
    "    q = item.get('question', '').strip()\n",
    "    ans_texts = item.get('answers', {}).get('text', [])\n",
    "    if not ctx or not q or not ans_texts:\n",
    "        continue\n",
    "    wc = len(ctx.split())\n",
    "    if 50 <= wc <= 300:\n",
    "        squad_samples.append({'passage': ctx, 'query': q, 'answer': ans_texts[0]})\n",
    "np.random.shuffle(squad_samples)\n",
    "squad_samples = squad_samples[:N_EXP13_SQUAD]\n",
    "print(f\"SQuAD evaluation samples: {len(squad_samples)}\")\n",
    "\n",
    "# SQuAD query pool from train\n",
    "squad_query_pool = []\n",
    "seen_sq = set()\n",
    "for item in squad_train:\n",
    "    q = item.get('question', '').strip()\n",
    "    if q and q not in seen_sq and len(q) > 10:\n",
    "        squad_query_pool.append(q)\n",
    "        seen_sq.add(q)\n",
    "print(f\"SQuAD query pool: {len(squad_query_pool)}\")\n",
    "print(\"Embedding SQuAD query pool...\")\n",
    "squad_pool_embs = embed_model.encode(squad_query_pool, show_progress_bar=True, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef884232",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Helper Functions\n",
    "# ============================================================\n",
    "\n",
    "def build_matched_bare_and_truncated(\n",
    "    prefix_text: str,\n",
    "    passage: str,\n",
    "    model, tokenizer, config,\n",
    ") -> Tuple[int, DynamicCache, int, DynamicCache, int]:\n",
    "    \"\"\"Build BPE-matched bare and truncated caches.\n",
    "    Returns: (bare_len, bare_cache, trunc_len, trunc_cache, prefix_token_len)\n",
    "    \"\"\"\n",
    "    prefix_with_sep = prefix_text.strip() + \" \"\n",
    "\n",
    "    prefix_encoding = tokenizer(\n",
    "        prefix_with_sep, return_tensors=\"pt\", add_special_tokens=True,\n",
    "        padding=False, truncation=False\n",
    "    )\n",
    "    prefix_len = prefix_encoding['input_ids'].shape[1]\n",
    "\n",
    "    full_context = prefix_with_sep + passage\n",
    "    full_encoding = tokenizer(\n",
    "        full_context, return_tensors=\"pt\", add_special_tokens=True,\n",
    "        padding=False, truncation=False\n",
    "    )\n",
    "    full_ids = full_encoding['input_ids'].to(config.device)\n",
    "    doc_len = full_ids.shape[1] - prefix_len\n",
    "\n",
    "    doc_token_ids = full_ids[:, prefix_len:]\n",
    "    bos_id = full_ids[:, :1]\n",
    "    bare_ids = torch.cat([bos_id, doc_token_ids], dim=1)\n",
    "    bare_len = bare_ids.shape[1]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        bare_out = model(\n",
    "            input_ids=bare_ids,\n",
    "            attention_mask=torch.ones_like(bare_ids),\n",
    "            use_cache=True, return_dict=True\n",
    "        )\n",
    "    bare_cache = _ensure_dynamic_cache(bare_out.past_key_values)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        full_out = model(\n",
    "            input_ids=full_ids,\n",
    "            attention_mask=torch.ones_like(full_ids),\n",
    "            use_cache=True, return_dict=True\n",
    "        )\n",
    "\n",
    "    truncated = extract_and_truncate_cache_with_bos(full_out.past_key_values, doc_len)\n",
    "    keep_len = 1 + doc_len\n",
    "\n",
    "    assert bare_len == keep_len, f\"Length mismatch: {bare_len} vs {keep_len}\"\n",
    "\n",
    "    surrogate_offset = prefix_len - 1\n",
    "    correct_rope_positions_with_bos(truncated, surrogate_offset, model)\n",
    "\n",
    "    return bare_len, bare_cache, keep_len, truncated, prefix_len\n",
    "\n",
    "\n",
    "def find_queries_at_similarity(\n",
    "    target_query: str,\n",
    "    target_embedding: np.ndarray,\n",
    "    sim_low: float,\n",
    "    sim_high: float,\n",
    "    pool_queries: list,\n",
    "    pool_embs: np.ndarray,\n",
    "    rng: np.random.RandomState,\n",
    "    k: int = 1,\n",
    "    diverse: bool = True,\n",
    ") -> List[Tuple[str, float]]:\n",
    "    \"\"\"Find k real queries from the pool within the similarity range.\n",
    "\n",
    "    If diverse=True, selects queries that are maximally spread apart\n",
    "    (greedy max-min pairwise distance). Otherwise picks near bin midpoint.\n",
    "\n",
    "    Returns list of (query, similarity) tuples, may be shorter than k.\n",
    "    \"\"\"\n",
    "    sims = cosine_similarity([target_embedding], pool_embs)[0]\n",
    "    mask = (sims >= sim_low) & (sims < sim_high)\n",
    "    # Exclude exact match\n",
    "    for idx in np.where(mask)[0]:\n",
    "        if pool_queries[idx].strip().lower() == target_query.strip().lower():\n",
    "            mask[idx] = False\n",
    "\n",
    "    candidates = np.where(mask)[0]\n",
    "    if len(candidates) == 0:\n",
    "        return []\n",
    "\n",
    "    if k == 1 or not diverse or len(candidates) <= k:\n",
    "        # Simple: pick k nearest to bin midpoint\n",
    "        mid = (sim_low + sim_high) / 2\n",
    "        dist_to_mid = np.abs(sims[candidates] - mid)\n",
    "        chosen_idxs = candidates[np.argsort(dist_to_mid)[:k]]\n",
    "        return [(pool_queries[ci], float(sims[ci])) for ci in chosen_idxs]\n",
    "\n",
    "    # Diverse selection: greedy max-min distance\n",
    "    # Start with the candidate nearest to bin midpoint\n",
    "    mid = (sim_low + sim_high) / 2\n",
    "    dist_to_mid = np.abs(sims[candidates] - mid)\n",
    "    first = candidates[np.argmin(dist_to_mid)]\n",
    "\n",
    "    selected = [first]\n",
    "    cand_embs = pool_embs[candidates]\n",
    "    first_emb = pool_embs[first:first+1]\n",
    "\n",
    "    # Precompute pairwise distances from first\n",
    "    remaining = set(range(len(candidates)))\n",
    "    first_local = np.where(candidates == first)[0][0]\n",
    "    remaining.discard(first_local)\n",
    "\n",
    "    # min_dist[i] = min distance from candidate i to any selected\n",
    "    all_dists = 1 - cosine_similarity(cand_embs, first_emb).flatten()\n",
    "    min_dists = all_dists.copy()\n",
    "\n",
    "    for _ in range(k - 1):\n",
    "        if not remaining:\n",
    "            break\n",
    "        rem_list = list(remaining)\n",
    "        best_local = rem_list[np.argmax(min_dists[rem_list])]\n",
    "        selected.append(candidates[best_local])\n",
    "        remaining.discard(best_local)\n",
    "        # Update min distances\n",
    "        new_dists = 1 - cosine_similarity(cand_embs, cand_embs[best_local:best_local+1]).flatten()\n",
    "        min_dists = np.minimum(min_dists, new_dists)\n",
    "\n",
    "    return [(pool_queries[ci], float(sims[ci])) for ci in selected]\n",
    "\n",
    "\n",
    "def build_multi_query_prefix(queries: List[str], sep: str = \" \") -> str:\n",
    "    \"\"\"Concatenate multiple queries into a single raw prefix string.\"\"\"\n",
    "    return sep.join(q.strip() for q in queries)\n",
    "\n",
    "\n",
    "def bootstrap_ci(data, stat_fn=np.mean, n_boot=10000, ci=0.95, seed=42):\n",
    "    \"\"\"Compute bootstrap confidence interval.\"\"\"\n",
    "    rng = np.random.RandomState(seed)\n",
    "    n = len(data)\n",
    "    boot_stats = np.empty(n_boot)\n",
    "    for i in range(n_boot):\n",
    "        sample = rng.choice(data, size=n, replace=True)\n",
    "        boot_stats[i] = stat_fn(sample)\n",
    "    alpha = (1 - ci) / 2\n",
    "    lo = np.percentile(boot_stats, 100 * alpha)\n",
    "    hi = np.percentile(boot_stats, 100 * (1 - alpha))\n",
    "    return float(lo), float(hi), boot_stats\n",
    "\n",
    "\n",
    "def bootstrap_corr_ci(x, y, n_boot=10000, ci=0.95, seed=42):\n",
    "    \"\"\"Bootstrap CI for Pearson correlation.\"\"\"\n",
    "    rng = np.random.RandomState(seed)\n",
    "    n = len(x)\n",
    "    boot_rs = np.empty(n_boot)\n",
    "    for i in range(n_boot):\n",
    "        idx = rng.choice(n, size=n, replace=True)\n",
    "        r, _ = stats.pearsonr(x[idx], y[idx])\n",
    "        boot_rs[i] = r\n",
    "    alpha = (1 - ci) / 2\n",
    "    lo = np.percentile(boot_rs, 100 * alpha)\n",
    "    hi = np.percentile(boot_rs, 100 * (1 - alpha))\n",
    "    return float(lo), float(hi), boot_rs\n",
    "\n",
    "\n",
    "print(\"Helper functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e11f777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Exp 13: Surrogate Pre-Selection (MS MARCO)\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"EXP 13: MULTI-QUERY SURROGATE PRE-SELECTION (MS MARCO)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Use first N_EXP14 samples (superset of Exp 13)\n",
    "samples_marco = all_samples[:N_EXP14]\n",
    "print(f\"Using {len(samples_marco)} MS MARCO samples (first {N_EXP13_MARCO} for Exp 13, all {N_EXP14} for Exp 14)\")\n",
    "\n",
    "print(\"Embedding target queries...\")\n",
    "target_qs_marco = [s['query'] for s in samples_marco]\n",
    "target_embs_marco = embed_model.encode(target_qs_marco, show_progress_bar=True, batch_size=256)\n",
    "\n",
    "rng_m = np.random.RandomState(SEEDS['msmarco'])\n",
    "\n",
    "# For each sample, pre-select all needed surrogates\n",
    "marco_surrogates = []  # list of dicts per sample\n",
    "\n",
    "for i in tqdm(range(len(samples_marco)), desc=\"Selecting surrogates\"):\n",
    "    surr = {}\n",
    "\n",
    "    # Oracle companions: queries with sim>0.85 to the target (for oracle_Kq conditions)\n",
    "    oracle_companions = find_queries_at_similarity(\n",
    "        target_qs_marco[i], target_embs_marco[i],\n",
    "        0.85, 1.0, query_pool, pool_embeddings, rng_m, k=4, diverse=True\n",
    "    )\n",
    "    surr['oracle_companions'] = oracle_companions\n",
    "\n",
    "    # Real queries at sim>=0.70 (for real_Kq_0.70 conditions)\n",
    "    real_070 = find_queries_at_similarity(\n",
    "        target_qs_marco[i], target_embs_marco[i],\n",
    "        0.70, 1.0, query_pool, pool_embeddings, rng_m, k=5, diverse=True\n",
    "    )\n",
    "    surr['real_070'] = real_070\n",
    "\n",
    "    # Real queries at sim>=0.50 (for real_5q_0.50 condition)\n",
    "    real_050 = find_queries_at_similarity(\n",
    "        target_qs_marco[i], target_embs_marco[i],\n",
    "        0.50, 1.0, query_pool, pool_embeddings, rng_m, k=5, diverse=True\n",
    "    )\n",
    "    surr['real_050'] = real_050\n",
    "\n",
    "    # Random queries (for random_5q control)\n",
    "    rand_idxs = rng_m.choice(len(query_pool), size=5, replace=False)\n",
    "    surr['random_5'] = [(query_pool[ri], 0.0) for ri in rand_idxs]\n",
    "\n",
    "    marco_surrogates.append(surr)\n",
    "\n",
    "# Report coverage\n",
    "def coverage_report(surrogates, key, k_needed, n_total):\n",
    "    counts = [len(s[key]) for s in surrogates[:n_total]]\n",
    "    have_k = sum(1 for c in counts if c >= k_needed)\n",
    "    print(f\"  {key} (need {k_needed}): {have_k}/{n_total} samples ({100*have_k/n_total:.1f}%)\")\n",
    "    if counts:\n",
    "        print(f\"    mean found: {np.mean(counts):.1f}, min: {min(counts)}, max: {max(counts)}\")\n",
    "\n",
    "print(f\"\\nCoverage for Exp 13 ({N_EXP13_MARCO} samples):\")\n",
    "coverage_report(marco_surrogates, 'oracle_companions', 4, N_EXP13_MARCO)\n",
    "coverage_report(marco_surrogates, 'real_070', 5, N_EXP13_MARCO)\n",
    "coverage_report(marco_surrogates, 'real_050', 5, N_EXP13_MARCO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032211ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Exp 13: MS MARCO Evaluation Loop\n",
    "# ============================================================\n",
    "# All prefixes are RAW queries (no template framing).\n",
    "# Multi-query prefixes: queries joined by space.\n",
    "# ============================================================\n",
    "\n",
    "results_13m = []\n",
    "skipped_13m = 0\n",
    "errors_13m = 0\n",
    "start_13m = time.time()\n",
    "\n",
    "CKPT_13M = 'results/exp13/13_checkpoint_marco.json'\n",
    "if os.path.exists(CKPT_13M):\n",
    "    with open(CKPT_13M) as f:\n",
    "        ckpt = json.load(f)\n",
    "    if ckpt.get('experiment') == '13_marco':\n",
    "        results_13m = ckpt['results']\n",
    "        skipped_13m = ckpt['skipped']\n",
    "        errors_13m = ckpt['errors']\n",
    "        print(f\"Resumed: {len(results_13m)} results\")\n",
    "\n",
    "start_idx = len(results_13m) + skipped_13m + errors_13m\n",
    "\n",
    "for idx in tqdm(range(start_idx, N_EXP13_MARCO), desc=\"Exp13 MARCO\",\n",
    "                initial=start_idx, total=N_EXP13_MARCO):\n",
    "    sample = samples_marco[idx]\n",
    "    passage = sample['passage']\n",
    "    query = sample['query']\n",
    "    answer = sample['answer']\n",
    "\n",
    "    answer_ids = tokenizer(answer, return_tensors='pt', add_special_tokens=False)['input_ids']\n",
    "    if answer_ids.shape[1] < 2:\n",
    "        skipped_13m += 1\n",
    "        continue\n",
    "\n",
    "    query_prompt = config.query_template.format(query=query)\n",
    "    surr = marco_surrogates[idx]\n",
    "\n",
    "    try:\n",
    "        result = {'idx': idx, 'query': query}\n",
    "\n",
    "        # --- Condition 1: bare (use oracle prefix to get matched bare) ---\n",
    "        # We use the oracle query as the reference prefix for BPE matching\n",
    "        bare_len, bare_cache, _, oracle_cache, oracle_ptl = \\\n",
    "            build_matched_bare_and_truncated(query, passage, model, tokenizer, config)\n",
    "\n",
    "        nll_bare = score_answer_with_cache(\n",
    "            bare_cache, bare_len, query_prompt, answer, model, tokenizer, config)\n",
    "        result['nll_bare'] = nll_bare\n",
    "\n",
    "        # --- Condition 2: oracle_1q (single oracle query, raw) ---\n",
    "        nll_oracle_1q = score_answer_with_cache(\n",
    "            oracle_cache, bare_len, query_prompt, answer, model, tokenizer, config)\n",
    "        result['nll_oracle_1q'] = nll_oracle_1q\n",
    "        result['ptl_oracle_1q'] = oracle_ptl\n",
    "\n",
    "        # --- Conditions 3-5: oracle_2q, oracle_3q, oracle_5q ---\n",
    "        companions = surr['oracle_companions']\n",
    "        for k, cond_name in [(2, 'oracle_2q'), (3, 'oracle_3q'), (5, 'oracle_5q')]:\n",
    "            needed = k - 1  # We already have the oracle query\n",
    "            if len(companions) >= needed:\n",
    "                comp_queries = [c[0] for c in companions[:needed]]\n",
    "                multi_prefix = build_multi_query_prefix([query] + comp_queries, MQ_SEP)\n",
    "                _, _, ml, mc, mptl = build_matched_bare_and_truncated(\n",
    "                    multi_prefix, passage, model, tokenizer, config)\n",
    "                nll = score_answer_with_cache(\n",
    "                    mc, ml, query_prompt, answer, model, tokenizer, config)\n",
    "                result[f'nll_{cond_name}'] = nll\n",
    "                result[f'ptl_{cond_name}'] = mptl\n",
    "            else:\n",
    "                result[f'nll_{cond_name}'] = None\n",
    "                result[f'ptl_{cond_name}'] = None\n",
    "\n",
    "        # --- Condition 6: real_1q_0.70 ---\n",
    "        real070 = surr['real_070']\n",
    "        if len(real070) >= 1:\n",
    "            r1_prefix = real070[0][0]\n",
    "            _, _, r1l, r1c, r1ptl = build_matched_bare_and_truncated(\n",
    "                r1_prefix, passage, model, tokenizer, config)\n",
    "            result['nll_real_1q_070'] = score_answer_with_cache(\n",
    "                r1c, r1l, query_prompt, answer, model, tokenizer, config)\n",
    "            result['ptl_real_1q_070'] = r1ptl\n",
    "            result['sim_real_1q_070'] = real070[0][1]\n",
    "        else:\n",
    "            result['nll_real_1q_070'] = None\n",
    "\n",
    "        # --- Condition 7: real_3q_0.70 ---\n",
    "        if len(real070) >= 3:\n",
    "            r3_prefix = build_multi_query_prefix([q for q, s in real070[:3]], MQ_SEP)\n",
    "            _, _, r3l, r3c, r3ptl = build_matched_bare_and_truncated(\n",
    "                r3_prefix, passage, model, tokenizer, config)\n",
    "            result['nll_real_3q_070'] = score_answer_with_cache(\n",
    "                r3c, r3l, query_prompt, answer, model, tokenizer, config)\n",
    "            result['ptl_real_3q_070'] = r3ptl\n",
    "            result['mean_sim_real_3q_070'] = float(np.mean([s for _, s in real070[:3]]))\n",
    "        else:\n",
    "            result['nll_real_3q_070'] = None\n",
    "\n",
    "        # --- Condition 8: real_5q_0.70 ---\n",
    "        if len(real070) >= 5:\n",
    "            r5_prefix = build_multi_query_prefix([q for q, s in real070[:5]], MQ_SEP)\n",
    "            _, _, r5l, r5c, r5ptl = build_matched_bare_and_truncated(\n",
    "                r5_prefix, passage, model, tokenizer, config)\n",
    "            result['nll_real_5q_070'] = score_answer_with_cache(\n",
    "                r5c, r5l, query_prompt, answer, model, tokenizer, config)\n",
    "            result['ptl_real_5q_070'] = r5ptl\n",
    "        else:\n",
    "            result['nll_real_5q_070'] = None\n",
    "\n",
    "        # --- Condition 9: real_5q_0.50 ---\n",
    "        real050 = surr['real_050']\n",
    "        if len(real050) >= 5:\n",
    "            r5_050_prefix = build_multi_query_prefix([q for q, s in real050[:5]], MQ_SEP)\n",
    "            _, _, r5_050l, r5_050c, r5_050ptl = build_matched_bare_and_truncated(\n",
    "                r5_050_prefix, passage, model, tokenizer, config)\n",
    "            result['nll_real_5q_050'] = score_answer_with_cache(\n",
    "                r5_050c, r5_050l, query_prompt, answer, model, tokenizer, config)\n",
    "            result['ptl_real_5q_050'] = r5_050ptl\n",
    "        else:\n",
    "            result['nll_real_5q_050'] = None\n",
    "\n",
    "        # --- Condition 10: random_5q ---\n",
    "        rand5 = surr['random_5']\n",
    "        rand_prefix = build_multi_query_prefix([q for q, s in rand5], MQ_SEP)\n",
    "        _, _, randl, randc, randptl = build_matched_bare_and_truncated(\n",
    "            rand_prefix, passage, model, tokenizer, config)\n",
    "        result['nll_random_5q'] = score_answer_with_cache(\n",
    "            randc, randl, query_prompt, answer, model, tokenizer, config)\n",
    "        result['ptl_random_5q'] = randptl\n",
    "\n",
    "        # --- Condition 11: repeated_1q_5x ---\n",
    "        if len(real070) >= 1:\n",
    "            rep_query = real070[0][0]\n",
    "            rep_prefix = build_multi_query_prefix([rep_query] * 5, MQ_SEP)\n",
    "            _, _, repl, repc, repptl = build_matched_bare_and_truncated(\n",
    "                rep_prefix, passage, model, tokenizer, config)\n",
    "            result['nll_repeated_1q_5x'] = score_answer_with_cache(\n",
    "                repc, repl, query_prompt, answer, model, tokenizer, config)\n",
    "            result['ptl_repeated_1q_5x'] = repptl\n",
    "        else:\n",
    "            result['nll_repeated_1q_5x'] = None\n",
    "\n",
    "        results_13m.append(result)\n",
    "\n",
    "    except Exception as e:\n",
    "        errors_13m += 1\n",
    "        if errors_13m <= 5:\n",
    "            print(f\"\\n  Error on sample {idx}: {e}\")\n",
    "        continue\n",
    "    finally:\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    if len(results_13m) % 25 == 0:\n",
    "        with open(CKPT_13M, 'w') as f:\n",
    "            json.dump({'experiment': '13_marco', 'results': results_13m,\n",
    "                       'skipped': skipped_13m, 'errors': errors_13m}, f)\n",
    "        elapsed = time.time() - start_13m\n",
    "        rate = len(results_13m) / (elapsed / 60) if elapsed > 0 else 0\n",
    "        print(f\"\\n  [{len(results_13m)} done | {elapsed/60:.0f}m | {rate:.1f}/min]\")\n",
    "\n",
    "print(f\"\\nExp 13 MARCO done. {len(results_13m)} evaluated, {skipped_13m} skipped, {errors_13m} errors.\")\n",
    "print(f\"Time: {(time.time()-start_13m)/60:.1f} min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411ae448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Exp 13: MS MARCO Analysis\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"EXP 13 RESULTS: MULTI-QUERY AMPLIFICATION (MS MARCO)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "conditions_13 = [\n",
    "    ('bare', 'nll_bare'),\n",
    "    ('oracle_1q', 'nll_oracle_1q'),\n",
    "    ('oracle_2q', 'nll_oracle_2q'),\n",
    "    ('oracle_3q', 'nll_oracle_3q'),\n",
    "    ('oracle_5q', 'nll_oracle_5q'),\n",
    "    ('real_1q_070', 'nll_real_1q_070'),\n",
    "    ('real_3q_070', 'nll_real_3q_070'),\n",
    "    ('real_5q_070', 'nll_real_5q_070'),\n",
    "    ('real_5q_050', 'nll_real_5q_050'),\n",
    "    ('random_5q', 'nll_random_5q'),\n",
    "    ('repeated_1q_5x', 'nll_repeated_1q_5x'),\n",
    "]\n",
    "\n",
    "bare_nlls = np.array([r['nll_bare'] for r in results_13m])\n",
    "\n",
    "print(f\"\\n{'Condition':<20} {'N':>5} {'Mean NLL':>10} {'Win%':>8} {'Delta':>10} {'Cohen d':>10} {'p-value':>12}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "stats_13m = {}\n",
    "for cname, nll_key in conditions_13:\n",
    "    valid = [r for r in results_13m if r.get(nll_key) is not None]\n",
    "    if len(valid) < 10:\n",
    "        print(f\"{cname:<20} {len(valid):>5} -- insufficient data\")\n",
    "        continue\n",
    "    nlls = np.array([r[nll_key] for r in valid])\n",
    "    bares = np.array([r['nll_bare'] for r in valid])\n",
    "    deltas = bares - nlls\n",
    "    wr = np.mean(deltas > 0) * 100\n",
    "    if cname == 'bare':\n",
    "        print(f\"{cname:<20} {len(valid):>5} {np.mean(nlls):>10.4f}\")\n",
    "        stats_13m[cname] = {'n': len(valid), 'mean_nll': float(np.mean(nlls))}\n",
    "        continue\n",
    "    t, p = stats.ttest_rel(bares, nlls)\n",
    "    d = np.mean(deltas) / np.std(deltas, ddof=1) if np.std(deltas) > 0 else 0\n",
    "    print(f\"{cname:<20} {len(valid):>5} {np.mean(nlls):>10.4f} {wr:>7.1f}% {np.mean(deltas):>+10.4f} {d:>10.3f} {p:>12.2e}\")\n",
    "    stats_13m[cname] = {\n",
    "        'n': len(valid), 'mean_nll': float(np.mean(nlls)),\n",
    "        'win_rate': float(wr), 'mean_delta': float(np.mean(deltas)),\n",
    "        'cohens_d': float(d), 'p_value': float(p),\n",
    "    }\n",
    "\n",
    "# --- Key comparisons ---\n",
    "print(\"\\n--- Key Comparisons ---\")\n",
    "\n",
    "# Does K matter? oracle_1q vs oracle_3q vs oracle_5q\n",
    "for a, b in [('oracle_1q', 'oracle_3q'), ('oracle_1q', 'oracle_5q'), ('oracle_3q', 'oracle_5q')]:\n",
    "    va = [r for r in results_13m if r.get(f'nll_{a}') is not None and r.get(f'nll_{b}') is not None]\n",
    "    if len(va) >= 10:\n",
    "        na = np.array([r[f'nll_{a}'] for r in va])\n",
    "        nb = np.array([r[f'nll_{b}'] for r in va])\n",
    "        t, p = stats.ttest_rel(na, nb)\n",
    "        wr = np.mean(nb < na) * 100\n",
    "        print(f\"  {b} vs {a}: {b} wins {wr:.1f}%, p={p:.4f}\")\n",
    "\n",
    "# Diversity: real_5q_070 vs repeated_1q_5x\n",
    "va = [r for r in results_13m if r.get('nll_real_5q_070') is not None and r.get('nll_repeated_1q_5x') is not None]\n",
    "if len(va) >= 10:\n",
    "    n_div = np.array([r['nll_real_5q_070'] for r in va])\n",
    "    n_rep = np.array([r['nll_repeated_1q_5x'] for r in va])\n",
    "    t, p = stats.ttest_rel(n_div, n_rep)\n",
    "    print(f\"  real_5q_070 vs repeated_1q_5x: diverse wins {np.mean(n_div < n_rep)*100:.1f}%, p={p:.4f}\")\n",
    "\n",
    "# Semantic: real_5q_070 vs random_5q\n",
    "va = [r for r in results_13m if r.get('nll_real_5q_070') is not None and r.get('nll_random_5q') is not None]\n",
    "if len(va) >= 10:\n",
    "    n_real = np.array([r['nll_real_5q_070'] for r in va])\n",
    "    n_rand = np.array([r['nll_random_5q'] for r in va])\n",
    "    t, p = stats.ttest_rel(n_real, n_rand)\n",
    "    print(f\"  real_5q_070 vs random_5q: real wins {np.mean(n_real < n_rand)*100:.1f}%, p={p:.4f}\")\n",
    "\n",
    "# Practical: real_5q_050 vs real_1q_070\n",
    "va = [r for r in results_13m if r.get('nll_real_5q_050') is not None and r.get('nll_real_1q_070') is not None]\n",
    "if len(va) >= 10:\n",
    "    n_5_050 = np.array([r['nll_real_5q_050'] for r in va])\n",
    "    n_1_070 = np.array([r['nll_real_1q_070'] for r in va])\n",
    "    t, p = stats.ttest_rel(n_5_050, n_1_070)\n",
    "    print(f\"  real_5q_050 vs real_1q_070: 5q@0.50 wins {np.mean(n_5_050 < n_1_070)*100:.1f}%, p={p:.4f}\")\n",
    "\n",
    "# --- Visualization ---\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Plot 1: Delta by condition (bar chart)\n",
    "ax = axes[0]\n",
    "cond_order = ['oracle_1q', 'oracle_2q', 'oracle_3q', 'oracle_5q',\n",
    "              'real_1q_070', 'real_3q_070', 'real_5q_070', 'real_5q_050',\n",
    "              'random_5q', 'repeated_1q_5x']\n",
    "plot_conds = [c for c in cond_order if c in stats_13m]\n",
    "plot_deltas = [stats_13m[c]['mean_delta'] for c in plot_conds]\n",
    "colors_13 = ['#c44e52']*4 + ['#4c72b0']*4 + ['#888888', '#e377c2']\n",
    "colors_13 = colors_13[:len(plot_conds)]\n",
    "ax.bar(range(len(plot_conds)), plot_deltas, color=colors_13)\n",
    "ax.set_xticks(range(len(plot_conds)))\n",
    "ax.set_xticklabels(plot_conds, rotation=45, ha='right', fontsize=7)\n",
    "ax.set_ylabel('Mean Delta NLL vs Bare')\n",
    "ax.set_title('Multi-Query Amplification')\n",
    "ax.axhline(0, color='gray', linestyle='--')\n",
    "\n",
    "# Plot 2: Scaling curve (oracle K=1,2,3,5)\n",
    "ax = axes[1]\n",
    "oracle_ks = []\n",
    "oracle_ds = []\n",
    "for k, cname in [(1, 'oracle_1q'), (2, 'oracle_2q'), (3, 'oracle_3q'), (5, 'oracle_5q')]:\n",
    "    if cname in stats_13m:\n",
    "        oracle_ks.append(k)\n",
    "        oracle_ds.append(stats_13m[cname]['mean_delta'])\n",
    "if oracle_ks:\n",
    "    ax.plot(oracle_ks, oracle_ds, 'o-', color='#c44e52', linewidth=2, markersize=8, label='Oracle K-query')\n",
    "real_ks = []\n",
    "real_ds = []\n",
    "for k, cname in [(1, 'real_1q_070'), (3, 'real_3q_070'), (5, 'real_5q_070')]:\n",
    "    if cname in stats_13m:\n",
    "        real_ks.append(k)\n",
    "        real_ds.append(stats_13m[cname]['mean_delta'])\n",
    "if real_ks:\n",
    "    ax.plot(real_ks, real_ds, 's-', color='#4c72b0', linewidth=2, markersize=8, label='Real K-query (sim>=0.70)')\n",
    "ax.set_xlabel('Number of prefix queries (K)')\n",
    "ax.set_ylabel('Mean Delta NLL')\n",
    "ax.set_title('Scaling: Does K Help?')\n",
    "ax.legend()\n",
    "ax.axhline(0, color='gray', linestyle='--')\n",
    "\n",
    "# Plot 3: Win rates\n",
    "ax = axes[2]\n",
    "plot_wrs = [stats_13m[c]['win_rate'] for c in plot_conds]\n",
    "ax.bar(range(len(plot_conds)), plot_wrs, color=colors_13)\n",
    "ax.axhline(50, color='gray', linestyle='--')\n",
    "ax.set_xticks(range(len(plot_conds)))\n",
    "ax.set_xticklabels(plot_conds, rotation=45, ha='right', fontsize=7)\n",
    "ax.set_ylabel('Win Rate vs Bare (%)')\n",
    "ax.set_title('Win Rates')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/exp13/13_exp13_marco.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print('Saved: 13_exp13_marco.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6b327c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Exp 13: SQuAD Surrogate Selection + Evaluation\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"EXP 13: SQUAD REPLICATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"Embedding SQuAD target queries...\")\n",
    "squad_target_qs = [s['query'] for s in squad_samples]\n",
    "squad_target_embs = embed_model.encode(squad_target_qs, show_progress_bar=True, batch_size=256)\n",
    "\n",
    "rng_sq = np.random.RandomState(SEEDS['squad'])\n",
    "\n",
    "squad_surrogates = []\n",
    "for i in tqdm(range(len(squad_samples)), desc=\"SQuAD surrogates\"):\n",
    "    surr = {}\n",
    "    surr['oracle_companions'] = find_queries_at_similarity(\n",
    "        squad_target_qs[i], squad_target_embs[i],\n",
    "        0.85, 1.0, squad_query_pool, squad_pool_embs, rng_sq, k=4, diverse=True)\n",
    "    surr['real_070'] = find_queries_at_similarity(\n",
    "        squad_target_qs[i], squad_target_embs[i],\n",
    "        0.70, 1.0, squad_query_pool, squad_pool_embs, rng_sq, k=5, diverse=True)\n",
    "    surr['real_050'] = find_queries_at_similarity(\n",
    "        squad_target_qs[i], squad_target_embs[i],\n",
    "        0.50, 1.0, squad_query_pool, squad_pool_embs, rng_sq, k=5, diverse=True)\n",
    "    rand_idxs = rng_sq.choice(len(squad_query_pool), size=5, replace=False)\n",
    "    surr['random_5'] = [(squad_query_pool[ri], 0.0) for ri in rand_idxs]\n",
    "    squad_surrogates.append(surr)\n",
    "\n",
    "print(f\"\\nSQuAD coverage:\")\n",
    "coverage_report(squad_surrogates, 'oracle_companions', 4, N_EXP13_SQUAD)\n",
    "coverage_report(squad_surrogates, 'real_070', 5, N_EXP13_SQUAD)\n",
    "coverage_report(squad_surrogates, 'real_050', 5, N_EXP13_SQUAD)\n",
    "\n",
    "# --- Evaluation ---\n",
    "results_13s = []\n",
    "skipped_13s = 0\n",
    "errors_13s = 0\n",
    "start_13s = time.time()\n",
    "\n",
    "CKPT_13S = 'results/exp13/13_checkpoint_squad.json'\n",
    "if os.path.exists(CKPT_13S):\n",
    "    with open(CKPT_13S) as f:\n",
    "        ckpt = json.load(f)\n",
    "    if ckpt.get('experiment') == '13_squad':\n",
    "        results_13s = ckpt['results']\n",
    "        skipped_13s = ckpt['skipped']\n",
    "        errors_13s = ckpt['errors']\n",
    "        print(f\"Resumed: {len(results_13s)} results\")\n",
    "\n",
    "start_idx_s = len(results_13s) + skipped_13s + errors_13s\n",
    "\n",
    "for idx in tqdm(range(start_idx_s, N_EXP13_SQUAD), desc=\"Exp13 SQuAD\",\n",
    "                initial=start_idx_s, total=N_EXP13_SQUAD):\n",
    "    sample = squad_samples[idx]\n",
    "    passage, query, answer = sample['passage'], sample['query'], sample['answer']\n",
    "\n",
    "    answer_ids = tokenizer(answer, return_tensors='pt', add_special_tokens=False)['input_ids']\n",
    "    if answer_ids.shape[1] < 2:\n",
    "        skipped_13s += 1\n",
    "        continue\n",
    "\n",
    "    query_prompt = config.query_template.format(query=query)\n",
    "    surr = squad_surrogates[idx]\n",
    "\n",
    "    try:\n",
    "        result = {'idx': idx, 'query': query}\n",
    "\n",
    "        bare_len, bare_cache, _, oracle_cache, _ = \\\n",
    "            build_matched_bare_and_truncated(query, passage, model, tokenizer, config)\n",
    "        result['nll_bare'] = score_answer_with_cache(\n",
    "            bare_cache, bare_len, query_prompt, answer, model, tokenizer, config)\n",
    "        result['nll_oracle_1q'] = score_answer_with_cache(\n",
    "            oracle_cache, bare_len, query_prompt, answer, model, tokenizer, config)\n",
    "\n",
    "        # oracle_5q\n",
    "        companions = surr['oracle_companions']\n",
    "        if len(companions) >= 4:\n",
    "            mp = build_multi_query_prefix([query] + [c[0] for c in companions[:4]], MQ_SEP)\n",
    "            _, _, ml, mc, _ = build_matched_bare_and_truncated(mp, passage, model, tokenizer, config)\n",
    "            result['nll_oracle_5q'] = score_answer_with_cache(\n",
    "                mc, ml, query_prompt, answer, model, tokenizer, config)\n",
    "        else:\n",
    "            result['nll_oracle_5q'] = None\n",
    "\n",
    "        # real_1q_070, real_5q_070\n",
    "        real070 = surr['real_070']\n",
    "        if len(real070) >= 1:\n",
    "            _, _, rl, rc, _ = build_matched_bare_and_truncated(\n",
    "                real070[0][0], passage, model, tokenizer, config)\n",
    "            result['nll_real_1q_070'] = score_answer_with_cache(\n",
    "                rc, rl, query_prompt, answer, model, tokenizer, config)\n",
    "        else:\n",
    "            result['nll_real_1q_070'] = None\n",
    "\n",
    "        if len(real070) >= 5:\n",
    "            mp = build_multi_query_prefix([q for q, s in real070[:5]], MQ_SEP)\n",
    "            _, _, rl, rc, _ = build_matched_bare_and_truncated(mp, passage, model, tokenizer, config)\n",
    "            result['nll_real_5q_070'] = score_answer_with_cache(\n",
    "                rc, rl, query_prompt, answer, model, tokenizer, config)\n",
    "        else:\n",
    "            result['nll_real_5q_070'] = None\n",
    "\n",
    "        # random_5q\n",
    "        rand_prefix = build_multi_query_prefix([q for q, s in surr['random_5']], MQ_SEP)\n",
    "        _, _, randl, randc, _ = build_matched_bare_and_truncated(\n",
    "            rand_prefix, passage, model, tokenizer, config)\n",
    "        result['nll_random_5q'] = score_answer_with_cache(\n",
    "            randc, randl, query_prompt, answer, model, tokenizer, config)\n",
    "\n",
    "        results_13s.append(result)\n",
    "    except Exception as e:\n",
    "        errors_13s += 1\n",
    "        if errors_13s <= 3:\n",
    "            print(f\"\\n  Error: {e}\")\n",
    "        continue\n",
    "    finally:\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    if len(results_13s) % 25 == 0:\n",
    "        with open(CKPT_13S, 'w') as f:\n",
    "            json.dump({'experiment': '13_squad', 'results': results_13s,\n",
    "                       'skipped': skipped_13s, 'errors': errors_13s}, f)\n",
    "        elapsed = time.time() - start_13s\n",
    "        print(f\"\\n  [{len(results_13s)} done | {elapsed/60:.0f}m]\")\n",
    "\n",
    "print(f\"\\nSQuAD done. {len(results_13s)} evaluated, {skipped_13s} skipped, {errors_13s} errors.\")\n",
    "print(f\"Time: {(time.time()-start_13s)/60:.1f} min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6881e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Exp 13: SQuAD Analysis\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"EXP 13 RESULTS: SQUAD REPLICATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "squad_conds = [\n",
    "    ('bare', 'nll_bare'), ('oracle_1q', 'nll_oracle_1q'),\n",
    "    ('oracle_5q', 'nll_oracle_5q'),\n",
    "    ('real_1q_070', 'nll_real_1q_070'), ('real_5q_070', 'nll_real_5q_070'),\n",
    "    ('random_5q', 'nll_random_5q'),\n",
    "]\n",
    "\n",
    "print(f\"\\n{'Condition':<20} {'N':>5} {'Mean NLL':>10} {'Win%':>8} {'Delta':>10} {'Cohen d':>10}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "stats_13s = {}\n",
    "for cname, nll_key in squad_conds:\n",
    "    valid = [r for r in results_13s if r.get(nll_key) is not None]\n",
    "    if len(valid) < 10:\n",
    "        print(f\"{cname:<20} {len(valid):>5} -- insufficient\")\n",
    "        continue\n",
    "    nlls = np.array([r[nll_key] for r in valid])\n",
    "    bares = np.array([r['nll_bare'] for r in valid])\n",
    "    if cname == 'bare':\n",
    "        print(f\"{cname:<20} {len(valid):>5} {np.mean(nlls):>10.4f}\")\n",
    "        stats_13s[cname] = {'n': len(valid), 'mean_nll': float(np.mean(nlls))}\n",
    "        continue\n",
    "    deltas = bares - nlls\n",
    "    wr = np.mean(deltas > 0) * 100\n",
    "    d = np.mean(deltas) / np.std(deltas, ddof=1) if np.std(deltas) > 0 else 0\n",
    "    print(f\"{cname:<20} {len(valid):>5} {np.mean(nlls):>10.4f} {wr:>7.1f}% {np.mean(deltas):>+10.4f} {d:>10.3f}\")\n",
    "    stats_13s[cname] = {'n': len(valid), 'mean_delta': float(np.mean(deltas)),\n",
    "                         'win_rate': float(wr), 'cohens_d': float(d)}\n",
    "\n",
    "# Key: does oracle_5q beat oracle_1q on SQuAD?\n",
    "va = [r for r in results_13s if r.get('nll_oracle_1q') is not None and r.get('nll_oracle_5q') is not None]\n",
    "if len(va) >= 10:\n",
    "    o1 = np.array([r['nll_oracle_1q'] for r in va])\n",
    "    o5 = np.array([r['nll_oracle_5q'] for r in va])\n",
    "    t, p = stats.ttest_rel(o1, o5)\n",
    "    print(f\"\\n  oracle_5q vs oracle_1q: 5q wins {np.mean(o5 < o1)*100:.1f}%, p={p:.4f}\")\n",
    "\n",
    "va = [r for r in results_13s if r.get('nll_real_1q_070') is not None and r.get('nll_real_5q_070') is not None]\n",
    "if len(va) >= 10:\n",
    "    r1 = np.array([r['nll_real_1q_070'] for r in va])\n",
    "    r5 = np.array([r['nll_real_5q_070'] for r in va])\n",
    "    t, p = stats.ttest_rel(r1, r5)\n",
    "    print(f\"  real_5q_070 vs real_1q_070: 5q wins {np.mean(r5 < r1)*100:.1f}%, p={p:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b13489",
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# Exp 13B: Hardness-Gated Priming — Extend to N=2000\n# ============================================================\n# Reuse Exp 13A first 1000 samples, extend with 1000 more\n# Only 3 conditions: bare, oracle_1q (raw), real_1q_0.70 (raw)\n# ============================================================\n\nprint(\"=\"*80)\nprint(\"EXP 13B: HARDNESS-GATED PRIMING (MS MARCO, N=2000)\")\nprint(\"=\"*80)\n\n# Results from Exp 13A already have bare + oracle_1q for first 1000\n# Extend to samples 1000-1999 with just these 3 conditions\nresults_14 = []\n# Copy Exp 13A results (first 1000) that have the needed fields\nfor r in results_13m:\n    r14 = {\n        'idx': r['idx'], 'query': r['query'],\n        'nll_bare': r['nll_bare'],\n        'nll_oracle_1q': r['nll_oracle_1q'],\n        'nll_real_1q_070': r.get('nll_real_1q_070'),\n    }\n    results_14.append(r14)\n\nprint(f\"Copied {len(results_14)} results from Exp 13A\")\n\n# Extend with samples 1000-1999\nstart_14 = time.time()\nerrors_14 = 0\nskipped_14 = 0\n\nCKPT_14 = 'results/exp13/13b_checkpoint.json'\nstart_idx_14 = N_EXP13_MARCO  # Start where Exp 13A ended\n\nif os.path.exists(CKPT_14):\n    with open(CKPT_14) as f:\n        ckpt = json.load(f)\n    if ckpt.get('experiment') == '14_extend':\n        extra = ckpt['results']\n        results_14 = results_14[:N_EXP13_MARCO]  # Keep only Exp 13A portion\n        results_14.extend(extra)\n        errors_14 = ckpt['errors']\n        skipped_14 = ckpt['skipped']\n        start_idx_14 = N_EXP13_MARCO + len(extra) + errors_14 + skipped_14\n        print(f\"Resumed Exp 13B extension: {len(extra)} additional results\")\n\nfor idx in tqdm(range(start_idx_14, N_EXP14), desc=\"Exp13B extend\",\n                initial=start_idx_14 - N_EXP13_MARCO, total=N_EXP14 - N_EXP13_MARCO):\n    sample = samples_marco[idx]\n    passage, query, answer = sample['passage'], sample['query'], sample['answer']\n\n    answer_ids = tokenizer(answer, return_tensors='pt', add_special_tokens=False)['input_ids']\n    if answer_ids.shape[1] < 2:\n        skipped_14 += 1\n        continue\n\n    query_prompt = config.query_template.format(query=query)\n    surr = marco_surrogates[idx]\n\n    try:\n        result = {'idx': idx, 'query': query}\n\n        # bare + oracle_1q\n        bare_len, bare_cache, _, oracle_cache, _ = \\\n            build_matched_bare_and_truncated(query, passage, model, tokenizer, config)\n        result['nll_bare'] = score_answer_with_cache(\n            bare_cache, bare_len, query_prompt, answer, model, tokenizer, config)\n        result['nll_oracle_1q'] = score_answer_with_cache(\n            oracle_cache, bare_len, query_prompt, answer, model, tokenizer, config)\n\n        # real_1q_0.70\n        real070 = surr['real_070']\n        if len(real070) >= 1:\n            _, _, rl, rc, _ = build_matched_bare_and_truncated(\n                real070[0][0], passage, model, tokenizer, config)\n            result['nll_real_1q_070'] = score_answer_with_cache(\n                rc, rl, query_prompt, answer, model, tokenizer, config)\n        else:\n            result['nll_real_1q_070'] = None\n\n        results_14.append(result)\n    except Exception as e:\n        errors_14 += 1\n        if errors_14 <= 3:\n            print(f\"\\n  Error: {e}\")\n        continue\n    finally:\n        torch.cuda.empty_cache()\n\n    extra_count = len(results_14) - N_EXP13_MARCO\n    if extra_count > 0 and extra_count % 25 == 0:\n        with open(CKPT_14, 'w') as f:\n            json.dump({'experiment': '14_extend',\n                       'results': results_14[N_EXP13_MARCO:],\n                       'skipped': skipped_14, 'errors': errors_14}, f)\n        elapsed = time.time() - start_14\n        print(f\"\\n  [{len(results_14)} total | {elapsed/60:.0f}m]\")\n\nprint(f\"\\nExp 13B done. {len(results_14)} total, {errors_14} errors, {skipped_14} skipped.\")\nprint(f\"Extension time: {(time.time()-start_14)/60:.1f} min\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f905327b",
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# Exp 13B: Hardness Analysis\n# ============================================================\n\nprint(\"=\"*80)\nprint(\"EXP 13B RESULTS: HARDNESS-GATED PRIMING\")\nprint(\"=\"*80)\n\nbare_14 = np.array([r['nll_bare'] for r in results_14])\noracle_14 = np.array([r['nll_oracle_1q'] for r in results_14])\noracle_delta_14 = bare_14 - oracle_14\n\n# --- Phase 1: Bin by bare NLL quartile ---\nquartiles = np.percentile(bare_14, [25, 50, 75])\nq_labels = [\n    f'Q1 (bare<{quartiles[0]:.2f})',\n    f'Q2 ({quartiles[0]:.2f}-{quartiles[1]:.2f})',\n    f'Q3 ({quartiles[1]:.2f}-{quartiles[2]:.2f})',\n    f'Q4 (bare>{quartiles[2]:.2f})',\n]\nq_masks = [\n    bare_14 < quartiles[0],\n    (bare_14 >= quartiles[0]) & (bare_14 < quartiles[1]),\n    (bare_14 >= quartiles[1]) & (bare_14 < quartiles[2]),\n    bare_14 >= quartiles[2],\n]\n\nprint(f\"\\nBare NLL quartiles: {quartiles}\")\nprint(f\"Total samples: {len(results_14)}\")\n\nprint(f\"\\n{'Quartile':<35} {'N':>5} {'Oracle Win%':>12} {'Oracle Delta':>14} {'Oracle d':>10}\")\nprint(\"-\" * 80)\n\nquartile_stats = []\nfor ql, qm in zip(q_labels, q_masks):\n    n = np.sum(qm)\n    od = oracle_delta_14[qm]\n    wr = np.mean(od > 0) * 100\n    d = np.mean(od) / np.std(od, ddof=1) if np.std(od, ddof=1) > 0 else 0\n    print(f\"{ql:<35} {n:>5} {wr:>11.1f}% {np.mean(od):>+14.4f} {d:>10.3f}\")\n    quartile_stats.append({'label': ql, 'n': int(n), 'win_rate': float(wr),\n                           'mean_delta': float(np.mean(od)), 'cohens_d': float(d)})\n\n# Also for real_1q_070\nvalid_real = [i for i, r in enumerate(results_14) if r.get('nll_real_1q_070') is not None]\nif len(valid_real) >= 40:\n    bare_r = np.array([results_14[i]['nll_bare'] for i in valid_real])\n    real_r = np.array([results_14[i]['nll_real_1q_070'] for i in valid_real])\n    real_delta = bare_r - real_r\n\n    rq = np.percentile(bare_r, [25, 50, 75])\n    print(f\"\\n{'Quartile':<35} {'N':>5} {'Real070 Win%':>12} {'Real070 Delta':>14} {'Real070 d':>10}\")\n    print(\"-\" * 80)\n    for qi, (lo, hi) in enumerate([(0, rq[0]), (rq[0], rq[1]), (rq[1], rq[2]), (rq[2], np.inf)]):\n        mask = (bare_r >= lo) & (bare_r < hi)\n        n = np.sum(mask)\n        if n < 5:\n            continue\n        rd = real_delta[mask]\n        wr = np.mean(rd > 0) * 100\n        d = np.mean(rd) / np.std(rd, ddof=1) if np.std(rd, ddof=1) > 0 else 0\n        print(f\"Q{qi+1:<34} {n:>5} {wr:>11.1f}% {np.mean(rd):>+14.4f} {d:>10.3f}\")\n\n# --- Phase 2: Hardness predictors ---\nprint(\"\\n--- Hardness Predictors ---\")\n\n# Passage length\npass_lengths = np.array([len(results_14[i]['query'].split()) for i in range(len(results_14))])\n# We don't have passage text in results, but we can get it from samples\npass_word_counts = np.array([len(samples_marco[r['idx']]['passage'].split()) for r in results_14])\nans_lengths = np.array([len(tokenizer.encode(samples_marco[r['idx']]['answer'], add_special_tokens=False))\n                        for r in results_14])\n\n# Correlations with bare NLL (hardness predictors)\npredictors = [\n    ('passage_word_count', pass_word_counts),\n    ('answer_token_length', ans_lengths),\n]\nfor pname, pvals in predictors:\n    r_pred, p_pred = stats.pearsonr(pvals.astype(float), bare_14)\n    print(f\"  {pname} vs bare_NLL: r={r_pred:.4f}, p={p_pred:.2e}\")\n\n# Correlations with oracle delta (who benefits?)\nprint(\"\\n  Predictors of benefit (oracle delta):\")\nfor pname, pvals in predictors:\n    r_pred, p_pred = stats.pearsonr(pvals.astype(float), oracle_delta_14)\n    print(f\"  {pname} vs oracle_delta: r={r_pred:.4f}, p={p_pred:.2e}\")\n\n# Bare NLL itself as predictor\nr_bare_delta, p_bare_delta = stats.pearsonr(bare_14, oracle_delta_14)\nprint(f\"  bare_NLL vs oracle_delta: r={r_bare_delta:.4f}, p={p_bare_delta:.2e}\")\n\n# --- Phase 3: Gated strategies ---\nprint(\"\\n--- Gated Strategies ---\")\n\n# Oracle gate: only prime if bare NLL > T\nfor T_percentile in [25, 50, 75]:\n    T = np.percentile(bare_14, T_percentile)\n    prime_mask = bare_14 >= T\n    n_primed = np.sum(prime_mask)\n    # Gated NLL: use oracle where primed, bare where not\n    gated_nlls = np.where(prime_mask, oracle_14, bare_14)\n    gated_delta = bare_14 - gated_nlls\n    gated_wr = np.mean(gated_delta > 0) * 100\n    gated_d = np.mean(gated_delta) / np.std(gated_delta, ddof=1) if np.std(gated_delta, ddof=1) > 0 else 0\n    # Compare to always-prime\n    always_d = np.mean(oracle_delta_14) / np.std(oracle_delta_14, ddof=1)\n    print(f\"  Gate T=P{T_percentile} (NLL>{T:.2f}): prime {n_primed}/{len(bare_14)}, \"\n          f\"win%={gated_wr:.1f}%, d={gated_d:.3f} (vs always-prime d={always_d:.3f})\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fdb4c86",
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# Exp 13B: Visualization\n# ============================================================\n\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\n\n# Plot 1: Oracle delta by bare NLL quartile\nax = axes[0, 0]\nqs_d = [qs['mean_delta'] for qs in quartile_stats]\nqs_n = [qs['n'] for qs in quartile_stats]\ncolors_q = ['#55a868', '#4c72b0', '#dd8452', '#c44e52']\nbars = ax.bar(range(4), qs_d, color=colors_q)\nax.set_xticks(range(4))\nax.set_xticklabels([f'Q{i+1}\\n(N={n})' for i, n in enumerate(qs_n)], fontsize=8)\nax.set_ylabel('Mean Oracle Delta NLL')\nax.set_title('Oracle Benefit by Baseline Difficulty')\nax.axhline(0, color='gray', linestyle='--')\nfor i, d in enumerate(qs_d):\n    ax.text(i, d + 0.002 if d >= 0 else d - 0.005, f'd={quartile_stats[i][\"cohens_d\"]:.3f}',\n            ha='center', fontsize=8)\n\n# Plot 2: Scatter bare NLL vs oracle delta\nax = axes[0, 1]\nax.scatter(bare_14, oracle_delta_14, alpha=0.05, s=3, color='#4c72b0')\nax.axhline(0, color='gray', linestyle='--')\nax.axvline(np.median(bare_14), color='orange', linestyle=':', label=f'median={np.median(bare_14):.2f}')\nz = np.polyfit(bare_14, oracle_delta_14, 1)\nx_line = np.linspace(bare_14.min(), min(bare_14.max(), 10), 100)\nax.plot(x_line, np.poly1d(z)(x_line), 'r-', linewidth=2)\nax.set_xlabel('Bare NLL (baseline difficulty)')\nax.set_ylabel('Oracle Delta NLL')\nax.set_title(f'Difficulty vs Benefit (r={r_bare_delta:.3f})')\nax.set_xlim(0, min(10, np.percentile(bare_14, 99)))\nax.legend()\n\n# Plot 3: Gated strategy comparison\nax = axes[1, 0]\nalways_delta = np.mean(oracle_delta_14)\nstrat_labels = ['Never\\nprime', 'Always\\nprime']\nstrat_deltas = [0.0, always_delta]\nstrat_colors = ['#888888', '#4c72b0']\nfor T_pct in [25, 50, 75]:\n    T = np.percentile(bare_14, T_pct)\n    pm = bare_14 >= T\n    gated = np.where(pm, oracle_14, bare_14)\n    gd = np.mean(bare_14 - gated)\n    strat_labels.append(f'Gate\\nP{T_pct}')\n    strat_deltas.append(gd)\n    strat_colors.append('#c44e52')\nax.bar(range(len(strat_labels)), strat_deltas, color=strat_colors)\nax.set_xticks(range(len(strat_labels)))\nax.set_xticklabels(strat_labels, fontsize=8)\nax.set_ylabel('Mean Delta NLL')\nax.set_title('Gating Strategies (Oracle Gate)')\nax.axhline(0, color='gray', linestyle='--')\n\n# Plot 4: Win rate by strategy\nax = axes[1, 1]\nstrat_wrs = [50.0, np.mean(oracle_delta_14 > 0) * 100]\nfor T_pct in [25, 50, 75]:\n    T = np.percentile(bare_14, T_pct)\n    pm = bare_14 >= T\n    gated = np.where(pm, oracle_14, bare_14)\n    gwr = np.mean((bare_14 - gated) > 0) * 100\n    strat_wrs.append(gwr)\nax.bar(range(len(strat_labels)), strat_wrs, color=strat_colors)\nax.axhline(50, color='gray', linestyle='--')\nax.set_xticks(range(len(strat_labels)))\nax.set_xticklabels(strat_labels, fontsize=8)\nax.set_ylabel('Win Rate vs Bare (%)')\nax.set_title('Win Rate by Gating Strategy')\n\nplt.tight_layout()\nplt.savefig('results/exp13/13b_hardness_gating.png', dpi=150, bbox_inches='tight')\nplt.show()\nprint('Saved: 13b_hardness_gating.png')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82a4918",
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# Comprehensive Summary\n# ============================================================\n\nprint(\"=\"*80)\nprint(\"EXPERIMENT 13 (PARTS A & B): COMPREHENSIVE SUMMARY\")\nprint(\"=\"*80)\n\n# --- Exp 13A Summary ---\nprint(\"\\n--- Exp 13A: Multi-Query Amplification ---\")\nprint(\"\\nMS MARCO:\")\nfor cname in ['oracle_1q', 'oracle_2q', 'oracle_3q', 'oracle_5q',\n              'real_1q_070', 'real_3q_070', 'real_5q_070', 'real_5q_050',\n              'random_5q', 'repeated_1q_5x']:\n    if cname in stats_13m:\n        s = stats_13m[cname]\n        print(f\"  {cname:<20} d={s['cohens_d']:.3f}, win%={s['win_rate']:.1f}%, delta={s['mean_delta']:.4f}\")\n\nprint(\"\\nSQuAD:\")\nfor cname in ['oracle_1q', 'oracle_5q', 'real_1q_070', 'real_5q_070', 'random_5q']:\n    if cname in stats_13s:\n        s = stats_13s[cname]\n        print(f\"  {cname:<20} d={s['cohens_d']:.3f}, win%={s['win_rate']:.1f}%, delta={s.get('mean_delta', 0):.4f}\")\n\n# Key verdicts\nprint(\"\\n--- Key Verdicts ---\")\n# Multi-query amplification\nif 'oracle_5q' in stats_13m and 'oracle_1q' in stats_13m:\n    amp = stats_13m['oracle_5q']['mean_delta'] / stats_13m['oracle_1q']['mean_delta'] if stats_13m['oracle_1q']['mean_delta'] != 0 else 0\n    print(f\"  Multi-query amplification factor (oracle 5q/1q): {amp:.2f}x\")\nif 'real_5q_070' in stats_13m and 'real_1q_070' in stats_13m:\n    amp = stats_13m['real_5q_070']['mean_delta'] / stats_13m['real_1q_070']['mean_delta'] if stats_13m['real_1q_070']['mean_delta'] != 0 else 0\n    print(f\"  Multi-query amplification factor (real 5q/1q @0.70): {amp:.2f}x\")\n\n# Diversity\nif 'real_5q_070' in stats_13m and 'repeated_1q_5x' in stats_13m:\n    div_better = stats_13m['real_5q_070']['mean_delta'] > stats_13m['repeated_1q_5x']['mean_delta']\n    print(f\"  Diversity helps: {div_better} (5 diverse: d={stats_13m['real_5q_070']['cohens_d']:.3f}, \"\n          f\"1 repeated 5x: d={stats_13m['repeated_1q_5x']['cohens_d']:.3f})\")\n\n# Practical comparison\nif 'real_5q_050' in stats_13m and 'real_1q_070' in stats_13m:\n    prac = stats_13m['real_5q_050']['mean_delta'] > stats_13m['real_1q_070']['mean_delta']\n    print(f\"  5q@0.50 > 1q@0.70: {prac} (5q@0.50: d={stats_13m['real_5q_050']['cohens_d']:.3f}, \"\n          f\"1q@0.70: d={stats_13m['real_1q_070']['cohens_d']:.3f})\")\n\n# --- Exp 13B Summary ---\nprint(f\"\\n--- Exp 13B: Hardness Gating ---\")\nprint(f\"  bare NLL vs oracle delta correlation: r={r_bare_delta:.3f}\")\nfor qs in quartile_stats:\n    print(f\"  {qs['label']}: d={qs['cohens_d']:.3f}, win%={qs['win_rate']:.1f}%\")\n\n# Best gated strategy\nbest_gate_d = 0\nbest_gate_pct = 0\nalways_d_val = np.mean(oracle_delta_14) / np.std(oracle_delta_14, ddof=1)\nfor T_pct in [25, 50, 75]:\n    T = np.percentile(bare_14, T_pct)\n    pm = bare_14 >= T\n    gated = np.where(pm, oracle_14, bare_14)\n    gd = bare_14 - gated\n    gd_d = np.mean(gd) / np.std(gd, ddof=1) if np.std(gd, ddof=1) > 0 else 0\n    if gd_d > best_gate_d:\n        best_gate_d = gd_d\n        best_gate_pct = T_pct\n\nprint(f\"\\n  Best gate: P{best_gate_pct} (d={best_gate_d:.3f} vs always-prime d={always_d_val:.3f})\")\nif best_gate_d > always_d_val:\n    print(f\"  Gating IMPROVES over always-prime by {best_gate_d - always_d_val:.3f} d\")\nelse:\n    print(f\"  Gating does NOT improve over always-prime\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427b0201",
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# Save Results\n# ============================================================\n\noutput = {\n    'metadata': {\n        'experiment': '13_multi_query_and_gating',\n        'timestamp': datetime.datetime.now().isoformat(),\n        'model_name': config.model_name,\n        'seeds': SEEDS,\n        'n_exp13a_marco': N_EXP13_MARCO,\n        'n_exp13a_squad': N_EXP13_SQUAD,\n        'n_exp13b': N_EXP14,\n    },\n    'exp13a_marco': {\n        'n_evaluated': len(results_13m),\n        'skipped': skipped_13m, 'errors': errors_13m,\n        'stats': stats_13m,\n        'results': results_13m,\n    },\n    'exp13a_squad': {\n        'n_evaluated': len(results_13s),\n        'skipped': skipped_13s, 'errors': errors_13s,\n        'stats': stats_13s,\n        'results': results_13s,\n    },\n    'exp13b': {\n        'n_evaluated': len(results_14),\n        'quartile_stats': quartile_stats,\n        'bare_delta_r': float(r_bare_delta),\n        'results': results_14,\n    },\n}\n\noutput_path = 'results/exp13/13_results.json'\nwith open(output_path, 'w') as f:\n    json.dump(output, f, indent=2, default=str)\nprint(f\"Results saved to {output_path}\")\nprint(f\"File size: {os.path.getsize(output_path) / 1e6:.1f} MB\")\n\n# Final summary figure\nfig, axes = plt.subplots(1, 3, figsize=(18, 5))\n\n# Plot 1: Scaling curves (MS MARCO)\nax = axes[0]\nfor series, label, marker, color in [\n    ([(k, c) for k, c in [(1,'oracle_1q'),(2,'oracle_2q'),(3,'oracle_3q'),(5,'oracle_5q')]\n      if c in stats_13m], 'Oracle', 'o', '#c44e52'),\n    ([(k, c) for k, c in [(1,'real_1q_070'),(3,'real_3q_070'),(5,'real_5q_070')]\n      if c in stats_13m], 'Real (sim>=0.70)', 's', '#4c72b0'),\n]:\n    if series:\n        ks = [s[0] for s in series]\n        ds = [stats_13m[s[1]]['mean_delta'] for s in series]\n        ax.plot(ks, ds, f'{marker}-', color=color, linewidth=2, markersize=8, label=label)\nax.axhline(0, color='gray', linestyle='--')\nax.set_xlabel('Number of prefix queries (K)')\nax.set_ylabel('Mean Delta NLL')\nax.set_title('Exp 13A: Multi-Query Scaling')\nax.legend()\n\n# Plot 2: Hardness interaction\nax = axes[1]\nax.bar(range(4), [qs['cohens_d'] for qs in quartile_stats], color=colors_q)\nax.set_xticks(range(4))\nax.set_xticklabels([f'Q{i+1}' for i in range(4)])\nax.set_ylabel(\"Cohen's d\")\nax.set_title('Exp 13B: Effect Size by Difficulty Quartile')\nax.axhline(0, color='gray', linestyle='--')\n\n# Plot 3: Cross-dataset comparison (1q vs 5q)\nax = axes[2]\ndatasets_plot = []\nd1q_plot = []\nd5q_plot = []\nif 'oracle_1q' in stats_13m and 'oracle_5q' in stats_13m:\n    datasets_plot.append('MARCO')\n    d1q_plot.append(stats_13m['oracle_1q']['cohens_d'])\n    d5q_plot.append(stats_13m['oracle_5q']['cohens_d'])\nif 'oracle_1q' in stats_13s and 'oracle_5q' in stats_13s:\n    datasets_plot.append('SQuAD')\n    d1q_plot.append(stats_13s['oracle_1q']['cohens_d'])\n    d5q_plot.append(stats_13s['oracle_5q']['cohens_d'])\nif datasets_plot:\n    x = np.arange(len(datasets_plot))\n    w = 0.35\n    ax.bar(x - w/2, d1q_plot, w, label='1 query', color='#4c72b0')\n    ax.bar(x + w/2, d5q_plot, w, label='5 queries', color='#c44e52')\n    ax.set_xticks(x)\n    ax.set_xticklabels(datasets_plot)\n    ax.set_ylabel(\"Cohen's d\")\n    ax.set_title('1q vs 5q: Cross-Dataset')\n    ax.legend()\n\nplt.suptitle('Experiment 13 Summary (Parts A & B)', fontsize=14, fontweight='bold')\nplt.tight_layout()\nplt.savefig('results/exp13/13_summary.png', dpi=150, bbox_inches='tight')\nplt.show()\nprint('Saved: 13_summary.png')"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}