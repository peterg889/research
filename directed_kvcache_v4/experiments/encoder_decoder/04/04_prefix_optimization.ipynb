{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a20b662",
   "metadata": {},
   "source": [
    "# Experiment 04: Prefix Content Optimization\n",
    "\n",
    "## Motivation\n",
    "\n",
    "Exp 01 (MS MARCO, ~98 token docs) showed that when the decoder already has the query,\n",
    "the **content** of the encoder prefix matters:\n",
    "\n",
    "| Condition | d vs bare | % of oracle |\n",
    "|-----------|-----------|-------------|\n",
    "| oracle_trunc (real query) | +0.228 *** | 100% |\n",
    "| surr_doc_kw5 (top-5 keywords) | +0.148 ** | 65% |\n",
    "| random_trunc (unrelated words) | +0.080 ns | 35% |\n",
    "\n",
    "The structural fraction collapsed from 85% (v3, no query in decoder) to 35% (v4,\n",
    "query in decoder). This means 65% of the benefit comes from prefix **content** — and\n",
    "there's headroom between surr_kw5 (65%) and oracle (100%).\n",
    "\n",
    "However, Exp 02/03 showed that on longer documents (256+ tokens), the structural\n",
    "mechanism regains dominance and even random prefixes become highly effective. So this\n",
    "content optimization is most relevant for **short documents**.\n",
    "\n",
    "## Questions\n",
    "\n",
    "1. Does more keyword density help? (kw5 → kw10 → kw20)\n",
    "2. Does natural text (first sentence) beat keyword bags?\n",
    "3. Do document-SPECIFIC keywords matter, or do any keywords work? (random_kw5 control)\n",
    "4. Can any surrogate close the gap between kw5 (65%) and oracle (100%)?\n",
    "\n",
    "## Conditions (10 total)\n",
    "\n",
    "### With query in decoder (production-realistic):\n",
    "\n",
    "| # | Condition | Encoder prefix | Rationale |\n",
    "|---|-----------|---------------|-----------|\n",
    "| 1 | bare | (none) | Baseline |\n",
    "| 2 | oracle_trunc | real query | Upper bound |\n",
    "| 3 | random_trunc | random unrelated words | Structural-only control |\n",
    "| 4 | surr_kw5 | top-5 TF keywords | Exp 01 baseline surrogate |\n",
    "| 5 | surr_kw10 | top-10 TF keywords | More keywords |\n",
    "| 6 | surr_kw20 | top-20 TF keywords | Maximum keyword density |\n",
    "| 7 | surr_first_sent | first sentence of doc | Natural text, high density |\n",
    "| 8 | surr_random_kw5 | top-5 kw from WRONG doc | Vocabulary control |\n",
    "\n",
    "### Without query in decoder (v3 replication):\n",
    "\n",
    "| # | Condition | Purpose |\n",
    "|---|-----------|---------|\n",
    "| 9 | bare_nq | v3 baseline |\n",
    "| 10 | oracle_trunc_nq | v3 enrichment reference |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f495de59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Setup and model loading\n",
    "import os\n",
    "os.umask(0o000)\n",
    "\n",
    "import sys, json, time, gc, re\n",
    "import random as pyrandom\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from scipy import stats\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "sys.path.insert(0, \"../../..\")\n",
    "from lib.analysis import cohens_d\n",
    "\n",
    "SEED = 42\n",
    "N_SAMPLES = 500\n",
    "MODEL_NAME = \"google/t5gemma-2-4b-4b\"\n",
    "\n",
    "RESULTS_DIR = Path(\"../../../results/exp04\")\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CHECKPOINT_PATH = RESULTS_DIR / \"checkpoint.json\"\n",
    "\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "pyrandom.seed(SEED)\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv())\n",
    "HF_TOKEN = os.environ.get(\"HF_TOKEN\")\n",
    "\n",
    "from transformers import AutoProcessor, AutoModelForSeq2SeqLM\n",
    "\n",
    "print(f\"Loading {MODEL_NAME}...\")\n",
    "processor = AutoProcessor.from_pretrained(MODEL_NAME, token=HF_TOKEN)\n",
    "tokenizer = processor.tokenizer\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    MODEL_NAME, device_map=\"auto\", torch_dtype=torch.bfloat16, token=HF_TOKEN,\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "DEVICE = next(model.parameters()).device\n",
    "BOS_ID = getattr(model.config, 'decoder_start_token_id', None) or tokenizer.bos_token_id\n",
    "\n",
    "print(f\"Exp 04: Prefix Content Optimization\")\n",
    "print(f\"N: {N_SAMPLES}, Model: {MODEL_NAME}\")\n",
    "print(f\"DEVICE: {DEVICE}, dtype: {next(model.parameters()).dtype}\")\n",
    "print(f\"GPU memory: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2f9873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Scoring helpers\n",
    "\n",
    "def count_prefix_tokens(prefix_text, document_text):\n",
    "    full_text = prefix_text + \"\\n\" + document_text\n",
    "    full_ids = tokenizer(full_text, add_special_tokens=True, truncation=True,\n",
    "                         max_length=2048).input_ids\n",
    "    doc_ids = tokenizer(document_text, add_special_tokens=True, truncation=True,\n",
    "                        max_length=2048).input_ids\n",
    "    return len(full_ids) - len(doc_ids)\n",
    "\n",
    "\n",
    "def score_nll(encoder_text, answer_text, prefix_token_count=0, truncate=False):\n",
    "    # No query in decoder — used for _nq conditions (v3 replication).\n",
    "    enc_ids = tokenizer(encoder_text, return_tensors=\"pt\",\n",
    "                        add_special_tokens=True, truncation=True,\n",
    "                        max_length=2048).input_ids.to(DEVICE)\n",
    "    total_enc_len = enc_ids.shape[1]\n",
    "    enc_mask = torch.ones(1, total_enc_len, device=DEVICE, dtype=torch.long)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        encoder_outputs = model.get_encoder()(\n",
    "            input_ids=enc_ids, attention_mask=enc_mask\n",
    "        )\n",
    "\n",
    "    if truncate and prefix_token_count > 0:\n",
    "        cross_attn_mask = torch.ones(1, total_enc_len, device=DEVICE, dtype=torch.long)\n",
    "        cross_attn_mask[:, :prefix_token_count] = 0\n",
    "    else:\n",
    "        cross_attn_mask = torch.ones(1, total_enc_len, device=DEVICE, dtype=torch.long)\n",
    "\n",
    "    ans_ids = tokenizer(answer_text, return_tensors=\"pt\",\n",
    "                        add_special_tokens=False, truncation=True,\n",
    "                        max_length=256).input_ids.to(DEVICE)\n",
    "\n",
    "    if ans_ids.shape[1] == 0:\n",
    "        del encoder_outputs\n",
    "        return 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            encoder_outputs=encoder_outputs,\n",
    "            attention_mask=cross_attn_mask,\n",
    "            labels=ans_ids,\n",
    "        )\n",
    "\n",
    "    logits = outputs.logits\n",
    "    log_probs = F.log_softmax(logits, dim=-1)\n",
    "    token_log_probs = log_probs[0].gather(1, ans_ids[0].unsqueeze(1)).squeeze(1)\n",
    "    mean_nll = -token_log_probs.mean().item()\n",
    "\n",
    "    del encoder_outputs, outputs, logits, log_probs\n",
    "    return mean_nll\n",
    "\n",
    "\n",
    "def score_nll_query_prefix(encoder_text, query_text, answer_text,\n",
    "                           prefix_token_count=0, truncate=False):\n",
    "    # Query as decoder prefix — production-realistic.\n",
    "    enc_ids = tokenizer(encoder_text, return_tensors=\"pt\",\n",
    "                        add_special_tokens=True, truncation=True,\n",
    "                        max_length=2048).input_ids.to(DEVICE)\n",
    "    total_enc_len = enc_ids.shape[1]\n",
    "    enc_mask = torch.ones(1, total_enc_len, device=DEVICE, dtype=torch.long)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        encoder_outputs = model.get_encoder()(\n",
    "            input_ids=enc_ids, attention_mask=enc_mask\n",
    "        )\n",
    "\n",
    "    if truncate and prefix_token_count > 0:\n",
    "        cross_attn_mask = torch.ones(1, total_enc_len, device=DEVICE, dtype=torch.long)\n",
    "        cross_attn_mask[:, :prefix_token_count] = 0\n",
    "    else:\n",
    "        cross_attn_mask = torch.ones(1, total_enc_len, device=DEVICE, dtype=torch.long)\n",
    "\n",
    "    query_ids = tokenizer(query_text, add_special_tokens=False, truncation=True,\n",
    "                          max_length=512).input_ids\n",
    "    answer_ids = tokenizer(answer_text, add_special_tokens=False, truncation=True,\n",
    "                           max_length=256).input_ids\n",
    "\n",
    "    if len(answer_ids) == 0:\n",
    "        del encoder_outputs\n",
    "        return 0.0\n",
    "\n",
    "    dec_ids = [BOS_ID] + query_ids + answer_ids\n",
    "    dec_tensor = torch.tensor([dec_ids], dtype=torch.long, device=DEVICE)\n",
    "\n",
    "    n_query = len(query_ids)\n",
    "    n_answer = len(answer_ids)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            encoder_outputs=encoder_outputs,\n",
    "            attention_mask=cross_attn_mask,\n",
    "            decoder_input_ids=dec_tensor,\n",
    "        )\n",
    "\n",
    "    logits = outputs.logits\n",
    "    answer_logits = logits[0, n_query:n_query + n_answer, :]\n",
    "\n",
    "    answer_targets = torch.tensor(answer_ids, dtype=torch.long, device=DEVICE)\n",
    "    log_probs = F.log_softmax(answer_logits, dim=-1)\n",
    "    token_log_probs = log_probs.gather(1, answer_targets.unsqueeze(1)).squeeze(1)\n",
    "    mean_nll = -token_log_probs.mean().item()\n",
    "\n",
    "    del encoder_outputs, outputs, logits, log_probs\n",
    "    return mean_nll\n",
    "\n",
    "\n",
    "# === Surrogate generation ===\n",
    "STOP_WORDS = {\n",
    "    'a', 'an', 'the', 'is', 'are', 'was', 'were', 'be', 'been', 'being',\n",
    "    'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could',\n",
    "    'should', 'may', 'might', 'can', 'shall', 'to', 'of', 'in', 'for',\n",
    "    'on', 'with', 'at', 'by', 'from', 'as', 'into', 'through', 'during',\n",
    "    'before', 'after', 'above', 'below', 'between', 'and', 'but', 'or',\n",
    "    'not', 'no', 'if', 'then', 'than', 'so', 'up', 'out', 'about',\n",
    "    'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those',\n",
    "    'it', 'its', 'i', 'me', 'my', 'we', 'our', 'you', 'your', 'he',\n",
    "    'him', 'his', 'she', 'her', 'they', 'them', 'their', 'how', 'when',\n",
    "    'where', 'why', 'much', 'many', 'some', 'any', 'all', 'each',\n",
    "    'does', 'also', 'just', 'more', 'most', 'very', 'too', 'only',\n",
    "}\n",
    "\n",
    "def extract_keywords(text):\n",
    "    words = re.sub(r'[^\\w\\s]', '', text.lower()).split()\n",
    "    return [w for w in words if w not in STOP_WORDS and len(w) > 2]\n",
    "\n",
    "def make_kw_surrogate(passage, n_keywords):\n",
    "    # Extract top-N TF keywords from passage.\n",
    "    content_words = extract_keywords(passage)\n",
    "    if not content_words:\n",
    "        return \"information\"\n",
    "    counts = Counter(content_words)\n",
    "    return \" \".join(w for w, _ in counts.most_common(n_keywords))\n",
    "\n",
    "def get_first_sentence(text):\n",
    "    # Extract the first sentence by splitting on sentence-ending punctuation.\n",
    "    # Handle common abbreviations minimally.\n",
    "    parts = re.split(r'(?<=[.!?])\\s+', text.strip())\n",
    "    if parts:\n",
    "        return parts[0]\n",
    "    return text[:100]\n",
    "\n",
    "print(\"Scoring functions defined.\")\n",
    "print(\"Surrogate types: kw5, kw10, kw20, first_sent, random_kw5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62207db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Load MS MARCO data and generate surrogates\n",
    "from lib.data import count_words\n",
    "from datasets import load_dataset\n",
    "\n",
    "print(\"Loading MS MARCO v1.1 validation...\")\n",
    "ds = load_dataset(\"microsoft/ms_marco\", \"v1.1\", split=\"validation\")\n",
    "\n",
    "all_candidates = []\n",
    "for item in ds:\n",
    "    if len(all_candidates) >= 3 * N_SAMPLES:\n",
    "        break\n",
    "    passages = item.get('passages', {})\n",
    "    ptexts = passages.get('passage_text', [])\n",
    "    is_sel = passages.get('is_selected', [])\n",
    "    query = item.get('query', '')\n",
    "    answers = item.get('answers', [])\n",
    "    well_formed = item.get('wellFormedAnswers', [])\n",
    "    answer = None\n",
    "    if well_formed and len(well_formed) > 0 and well_formed[0] not in ('[]', ''):\n",
    "        answer = well_formed[0]\n",
    "    elif answers and len(answers) > 0 and answers[0] != 'No Answer Present.':\n",
    "        answer = answers[0]\n",
    "    if not answer:\n",
    "        continue\n",
    "    for pt, sel in zip(ptexts, is_sel):\n",
    "        wc = count_words(pt)\n",
    "        if sel == 1 and 30 <= wc <= 300:\n",
    "            all_candidates.append({\n",
    "                'passage': pt, 'query': query, 'answer': answer,\n",
    "                'word_count': wc,\n",
    "            })\n",
    "            break\n",
    "\n",
    "print(f\"Total candidates: {len(all_candidates)}\")\n",
    "np.random.seed(SEED)\n",
    "indices = np.random.permutation(len(all_candidates))\n",
    "samples = [all_candidates[i] for i in indices[:N_SAMPLES]]\n",
    "del ds, all_candidates\n",
    "gc.collect()\n",
    "\n",
    "# Generate all surrogate types\n",
    "for i, s in enumerate(samples):\n",
    "    passage = s['passage']\n",
    "\n",
    "    # Keyword surrogates at different densities\n",
    "    s['surr_kw5'] = make_kw_surrogate(passage, 5)\n",
    "    s['surr_kw10'] = make_kw_surrogate(passage, 10)\n",
    "    s['surr_kw20'] = make_kw_surrogate(passage, 20)\n",
    "\n",
    "    # First sentence\n",
    "    s['surr_first_sent'] = get_first_sentence(passage)\n",
    "\n",
    "    # Random words from unrelated passage (structural control, length-matched to query)\n",
    "    other_idx = (i + N_SAMPLES // 2) % len(samples)\n",
    "    other_words = samples[other_idx]['passage'].split()\n",
    "    query_word_count = len(s['query'].split())\n",
    "    s['random_prefix'] = \" \".join(other_words[:query_word_count])\n",
    "\n",
    "    # Keywords from WRONG document (vocabulary control)\n",
    "    wrong_idx = (i + 1) % len(samples)\n",
    "    s['surr_random_kw5'] = make_kw_surrogate(samples[wrong_idx]['passage'], 5)\n",
    "\n",
    "    # Count prefix tokens for each condition\n",
    "    s['n_pfx_oracle'] = count_prefix_tokens(s['query'], passage)\n",
    "    s['n_pfx_kw5'] = count_prefix_tokens(s['surr_kw5'], passage)\n",
    "    s['n_pfx_kw10'] = count_prefix_tokens(s['surr_kw10'], passage)\n",
    "    s['n_pfx_kw20'] = count_prefix_tokens(s['surr_kw20'], passage)\n",
    "    s['n_pfx_first_sent'] = count_prefix_tokens(s['surr_first_sent'], passage)\n",
    "    s['n_pfx_random'] = count_prefix_tokens(s['random_prefix'], passage)\n",
    "    s['n_pfx_random_kw5'] = count_prefix_tokens(s['surr_random_kw5'], passage)\n",
    "\n",
    "# Statistics\n",
    "print(f\"\\nLoaded {len(samples)} samples\")\n",
    "print(f\"Mean passage words: {np.mean([s['word_count'] for s in samples]):.0f}\")\n",
    "print(f\"Mean query words: {np.mean([count_words(s['query']) for s in samples]):.0f}\")\n",
    "print(f\"Mean answer words: {np.mean([count_words(s['answer']) for s in samples]):.0f}\")\n",
    "\n",
    "# Prefix token statistics\n",
    "print(f\"\\nPrefix token counts (mean):\")\n",
    "for key, label in [('n_pfx_oracle', 'oracle (query)'),\n",
    "                    ('n_pfx_kw5', 'kw5'), ('n_pfx_kw10', 'kw10'),\n",
    "                    ('n_pfx_kw20', 'kw20'), ('n_pfx_first_sent', 'first_sent'),\n",
    "                    ('n_pfx_random', 'random'), ('n_pfx_random_kw5', 'random_kw5')]:\n",
    "    vals = [s[key] for s in samples]\n",
    "    print(f\"  {label:<18}: mean={np.mean(vals):.1f}, min={np.min(vals)}, max={np.max(vals)}\")\n",
    "\n",
    "# How many unique content keywords do passages typically have?\n",
    "kw_counts = [len(set(extract_keywords(s['passage']))) for s in samples]\n",
    "print(f\"\\nUnique content keywords per passage:\")\n",
    "print(f\"  mean={np.mean(kw_counts):.1f}, median={np.median(kw_counts):.0f}, \"\n",
    "      f\"min={np.min(kw_counts)}, max={np.max(kw_counts)}\")\n",
    "\n",
    "print(f\"\\nFirst sample:\")\n",
    "print(f\"  Query:       {samples[0]['query'][:80]}...\")\n",
    "print(f\"  Answer:      {samples[0]['answer'][:80]}...\")\n",
    "print(f\"  Passage:     {samples[0]['passage'][:80]}...\")\n",
    "print(f\"  surr_kw5:    {samples[0]['surr_kw5']}\")\n",
    "print(f\"  surr_kw10:   {samples[0]['surr_kw10']}\")\n",
    "print(f\"  surr_kw20:   {samples[0]['surr_kw20']}\")\n",
    "print(f\"  first_sent:  {samples[0]['surr_first_sent'][:80]}\")\n",
    "print(f\"  random_kw5:  {samples[0]['surr_random_kw5']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc2b163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Show example conditions for sample 0\n",
    "print(\"=\" * 70)\n",
    "print(\"EXAMPLE CONDITIONS (sample 0)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "ex = samples[0]\n",
    "print(f\"\\nQuery:        {ex['query']}\")\n",
    "print(f\"Answer:       {ex['answer']}\")\n",
    "print(f\"Passage:      {ex['passage'][:120]}...\")\n",
    "\n",
    "print(f\"\\n  {'Condition':<18} {'Prefix':<35} {'Tokens':>6} {'Trunc':>6} {'Dec Q':>6}\")\n",
    "print(f\"  {'-'*75}\")\n",
    "\n",
    "conditions_display = [\n",
    "    ('bare',           '(none)',                         0,                   'no',  'yes'),\n",
    "    ('oracle_trunc',   ex['query'][:32],                 ex['n_pfx_oracle'],  'yes', 'yes'),\n",
    "    ('random_trunc',   ex['random_prefix'][:32],         ex['n_pfx_random'],  'yes', 'yes'),\n",
    "    ('surr_kw5',       ex['surr_kw5'][:32],              ex['n_pfx_kw5'],     'yes', 'yes'),\n",
    "    ('surr_kw10',      ex['surr_kw10'][:32],             ex['n_pfx_kw10'],    'yes', 'yes'),\n",
    "    ('surr_kw20',      ex['surr_kw20'][:32],             ex['n_pfx_kw20'],    'yes', 'yes'),\n",
    "    ('surr_first_sent', ex['surr_first_sent'][:32],      ex['n_pfx_first_sent'], 'yes', 'yes'),\n",
    "    ('surr_random_kw5', ex['surr_random_kw5'][:32],      ex['n_pfx_random_kw5'], 'yes', 'yes'),\n",
    "    ('bare_nq',        '(none)',                         0,                   'no',  'no'),\n",
    "    ('oracle_trunc_nq', ex['query'][:32],                ex['n_pfx_oracle'],  'yes', 'no'),\n",
    "]\n",
    "\n",
    "for name, prefix, n_tok, trunc, has_q in conditions_display:\n",
    "    print(f\"  {name:<18} {prefix:<35} {n_tok:>6} {trunc:>6} {has_q:>6}\")\n",
    "\n",
    "# Sanity check\n",
    "print(f\"\\nSanity check...\")\n",
    "nll_bare = score_nll_query_prefix(ex['passage'], ex['query'], ex['answer'])\n",
    "nll_oracle = score_nll_query_prefix(\n",
    "    ex['query'] + \"\\n\" + ex['passage'], ex['query'], ex['answer'],\n",
    "    prefix_token_count=ex['n_pfx_oracle'], truncate=True)\n",
    "nll_kw5 = score_nll_query_prefix(\n",
    "    ex['surr_kw5'] + \"\\n\" + ex['passage'], ex['query'], ex['answer'],\n",
    "    prefix_token_count=ex['n_pfx_kw5'], truncate=True)\n",
    "print(f\"  bare:         {nll_bare:.4f}\")\n",
    "print(f\"  oracle_trunc: {nll_oracle:.4f} (delta: {nll_bare - nll_oracle:+.4f})\")\n",
    "print(f\"  surr_kw5:     {nll_kw5:.4f} (delta: {nll_bare - nll_kw5:+.4f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74d0f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Scoring loop — 10 conditions x 500 samples\n",
    "print(\"=\" * 70)\n",
    "print(\"SCORING ALL CONDITIONS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "COND_NAMES = [\n",
    "    'bare', 'oracle_trunc', 'random_trunc',\n",
    "    'surr_kw5', 'surr_kw10', 'surr_kw20',\n",
    "    'surr_first_sent', 'surr_random_kw5',\n",
    "    'bare_nq', 'oracle_trunc_nq',\n",
    "]\n",
    "\n",
    "results = []\n",
    "start_idx = 0\n",
    "\n",
    "if CHECKPOINT_PATH.exists():\n",
    "    ckpt = json.loads(CHECKPOINT_PATH.read_text())\n",
    "    if ckpt.get('n_total') == N_SAMPLES and len(ckpt.get('results', [])) > 0:\n",
    "        saved_queries = [r['query'][:50] for r in ckpt['results']]\n",
    "        current_queries = [s['query'][:50] for s in samples[:len(saved_queries)]]\n",
    "        if saved_queries == current_queries:\n",
    "            results = ckpt['results']\n",
    "            start_idx = len(results)\n",
    "            print(f\"Resuming from checkpoint: {start_idx}/{N_SAMPLES}\")\n",
    "\n",
    "if start_idx == 0:\n",
    "    print(f\"Starting fresh: {len(COND_NAMES)} conditions x {N_SAMPLES} samples \"\n",
    "          f\"= {len(COND_NAMES) * N_SAMPLES} forward passes\")\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "for i in tqdm(range(start_idx, N_SAMPLES), initial=start_idx, total=N_SAMPLES,\n",
    "              desc=\"Scoring\"):\n",
    "    s = samples[i]\n",
    "    query = s['query']\n",
    "    passage = s['passage']\n",
    "    answer = s['answer']\n",
    "\n",
    "    result = {\n",
    "        'query': query,\n",
    "        'answer': answer,\n",
    "        'passage_words': s['word_count'],\n",
    "    }\n",
    "\n",
    "    # --- With query in decoder (production-realistic) ---\n",
    "\n",
    "    # 1. bare: encoder=[doc], no prefix\n",
    "    result['nll_bare'] = score_nll_query_prefix(passage, query, answer)\n",
    "\n",
    "    # 2. oracle_trunc: encoder=[query+doc], mask query\n",
    "    result['nll_oracle_trunc'] = score_nll_query_prefix(\n",
    "        query + \"\\n\" + passage, query, answer,\n",
    "        prefix_token_count=s['n_pfx_oracle'], truncate=True)\n",
    "\n",
    "    # 3. random_trunc: encoder=[random+doc], mask random\n",
    "    result['nll_random_trunc'] = score_nll_query_prefix(\n",
    "        s['random_prefix'] + \"\\n\" + passage, query, answer,\n",
    "        prefix_token_count=s['n_pfx_random'], truncate=True)\n",
    "\n",
    "    # 4. surr_kw5: top-5 TF keywords\n",
    "    result['nll_surr_kw5'] = score_nll_query_prefix(\n",
    "        s['surr_kw5'] + \"\\n\" + passage, query, answer,\n",
    "        prefix_token_count=s['n_pfx_kw5'], truncate=True)\n",
    "\n",
    "    # 5. surr_kw10: top-10 TF keywords\n",
    "    result['nll_surr_kw10'] = score_nll_query_prefix(\n",
    "        s['surr_kw10'] + \"\\n\" + passage, query, answer,\n",
    "        prefix_token_count=s['n_pfx_kw10'], truncate=True)\n",
    "\n",
    "    # 6. surr_kw20: top-20 TF keywords\n",
    "    result['nll_surr_kw20'] = score_nll_query_prefix(\n",
    "        s['surr_kw20'] + \"\\n\" + passage, query, answer,\n",
    "        prefix_token_count=s['n_pfx_kw20'], truncate=True)\n",
    "\n",
    "    # 7. surr_first_sent: first sentence of document\n",
    "    result['nll_surr_first_sent'] = score_nll_query_prefix(\n",
    "        s['surr_first_sent'] + \"\\n\" + passage, query, answer,\n",
    "        prefix_token_count=s['n_pfx_first_sent'], truncate=True)\n",
    "\n",
    "    # 8. surr_random_kw5: top-5 kw from WRONG document\n",
    "    result['nll_surr_random_kw5'] = score_nll_query_prefix(\n",
    "        s['surr_random_kw5'] + \"\\n\" + passage, query, answer,\n",
    "        prefix_token_count=s['n_pfx_random_kw5'], truncate=True)\n",
    "\n",
    "    # --- Without query in decoder (v3 replication) ---\n",
    "\n",
    "    # 9. bare_nq\n",
    "    result['nll_bare_nq'] = score_nll(passage, answer)\n",
    "\n",
    "    # 10. oracle_trunc_nq\n",
    "    result['nll_oracle_trunc_nq'] = score_nll(\n",
    "        query + \"\\n\" + passage, answer,\n",
    "        prefix_token_count=s['n_pfx_oracle'], truncate=True)\n",
    "\n",
    "    results.append(result)\n",
    "\n",
    "    if (i + 1) % 20 == 0 or i == N_SAMPLES - 1:\n",
    "        ckpt = {\n",
    "            'n_total': N_SAMPLES,\n",
    "            'results': results,\n",
    "            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        }\n",
    "        CHECKPOINT_PATH.write_text(json.dumps(ckpt))\n",
    "        elapsed = time.time() - t0\n",
    "        done = i - start_idx + 1\n",
    "        eta = (N_SAMPLES - i - 1) * elapsed / done if done > 0 else 0\n",
    "        tqdm.write(f\"  Checkpoint {i+1}/{N_SAMPLES} | {elapsed/60:.1f}m | ETA {eta/60:.1f}m\")\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "elapsed = time.time() - t0\n",
    "print(f\"\\nScoring complete: {len(results)} samples, \"\n",
    "      f\"{len(COND_NAMES)} conditions in {elapsed/60:.1f} min\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44443c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Results table — all conditions\n",
    "print(\"=\" * 70)\n",
    "print(f\"RESULTS (N={len(results)})\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Extract NLL arrays\n",
    "bare = np.array([r['nll_bare'] for r in results])\n",
    "oracle_trunc = np.array([r['nll_oracle_trunc'] for r in results])\n",
    "random_trunc = np.array([r['nll_random_trunc'] for r in results])\n",
    "surr_kw5 = np.array([r['nll_surr_kw5'] for r in results])\n",
    "surr_kw10 = np.array([r['nll_surr_kw10'] for r in results])\n",
    "surr_kw20 = np.array([r['nll_surr_kw20'] for r in results])\n",
    "surr_first_sent = np.array([r['nll_surr_first_sent'] for r in results])\n",
    "surr_random_kw5 = np.array([r['nll_surr_random_kw5'] for r in results])\n",
    "bare_nq = np.array([r['nll_bare_nq'] for r in results])\n",
    "oracle_nq = np.array([r['nll_oracle_trunc_nq'] for r in results])\n",
    "\n",
    "# Bonferroni: 7 comparisons vs bare (all with-query conditions except bare)\n",
    "N_BONF = 7\n",
    "\n",
    "print(f\"\\n--- With query in decoder (production-realistic) ---\")\n",
    "print(f\"  Bonferroni correction: {N_BONF} comparisons\")\n",
    "print(f\"\\n  {'Condition':<18} {'NLL':>8} {'delta':>8} {'d':>8} {'Win%':>8} {'p':>12} {'sig':>5} {'%orc':>6}\")\n",
    "print(f\"  {'-'*82}\")\n",
    "\n",
    "analysis = {}\n",
    "d_oracle = None\n",
    "for name, nlls in [\n",
    "    ('bare', bare),\n",
    "    ('oracle_trunc', oracle_trunc),\n",
    "    ('surr_kw20', surr_kw20),\n",
    "    ('surr_kw10', surr_kw10),\n",
    "    ('surr_kw5', surr_kw5),\n",
    "    ('surr_first_sent', surr_first_sent),\n",
    "    ('surr_random_kw5', surr_random_kw5),\n",
    "    ('random_trunc', random_trunc),\n",
    "]:\n",
    "    mean_nll = nlls.mean()\n",
    "    if name == 'bare':\n",
    "        print(f\"  {name:<18} {mean_nll:>8.4f} {'--':>8} {'--':>8} {'--':>8} {'--':>12} {'--':>5} {'--':>6}\")\n",
    "        analysis[name] = {'mean_nll': float(mean_nll)}\n",
    "    else:\n",
    "        diff = bare - nlls\n",
    "        d = cohens_d(diff)\n",
    "        win_pct = 100 * np.mean(diff > 0)\n",
    "        _, p_val = stats.ttest_1samp(diff, 0)\n",
    "        sig = '***' if p_val < 0.001/N_BONF else '**' if p_val < 0.01/N_BONF else '*' if p_val < 0.05/N_BONF else 'ns'\n",
    "        if name == 'oracle_trunc':\n",
    "            d_oracle = d\n",
    "        pct_orc = f\"{d/d_oracle*100:.0f}%\" if d_oracle and d_oracle > 0 else \"--\"\n",
    "        print(f\"  {name:<18} {mean_nll:>8.4f} {diff.mean():>+8.4f} {d:>+8.3f} {win_pct:>7.1f}% {p_val:>12.2e} {sig:>5} {pct_orc:>6}\")\n",
    "        analysis[name] = {\n",
    "            'mean_nll': float(mean_nll), 'delta': float(diff.mean()),\n",
    "            'd': float(d), 'win_pct': float(win_pct), 'p': float(p_val),\n",
    "        }\n",
    "\n",
    "# No-query conditions\n",
    "diff_nq = bare_nq - oracle_nq\n",
    "d_nq = cohens_d(diff_nq)\n",
    "win_nq = 100 * np.mean(diff_nq > 0)\n",
    "_, p_nq = stats.ttest_1samp(diff_nq, 0)\n",
    "sig_nq = '***' if p_nq < 0.001 else '**' if p_nq < 0.01 else '*' if p_nq < 0.05 else 'ns'\n",
    "\n",
    "print(f\"\\n--- Without query in decoder (v3 replication) ---\")\n",
    "print(f\"  {'bare_nq':<18} {bare_nq.mean():>8.4f}\")\n",
    "print(f\"  {'oracle_trunc_nq':<18} {oracle_nq.mean():>8.4f} {diff_nq.mean():>+8.4f} {d_nq:>+8.3f} {win_nq:>7.1f}% {p_nq:>12.2e} {sig_nq:>5}\")\n",
    "\n",
    "analysis['bare_nq'] = {'mean_nll': float(bare_nq.mean())}\n",
    "analysis['oracle_trunc_nq'] = {\n",
    "    'mean_nll': float(oracle_nq.mean()), 'delta': float(diff_nq.mean()),\n",
    "    'd': float(d_nq), 'win_pct': float(win_nq), 'p': float(p_nq),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61774fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Content type analysis — what type of prefix works best?\n",
    "print(\"=\" * 70)\n",
    "print(\"CONTENT TYPE ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Rank conditions by effect size\n",
    "condition_list = [\n",
    "    ('oracle_trunc', oracle_trunc),\n",
    "    ('surr_kw20', surr_kw20),\n",
    "    ('surr_kw10', surr_kw10),\n",
    "    ('surr_kw5', surr_kw5),\n",
    "    ('surr_first_sent', surr_first_sent),\n",
    "    ('surr_random_kw5', surr_random_kw5),\n",
    "    ('random_trunc', random_trunc),\n",
    "]\n",
    "\n",
    "ranked = []\n",
    "for name, nlls in condition_list:\n",
    "    d = cohens_d(bare - nlls)\n",
    "    ranked.append((name, d))\n",
    "ranked.sort(key=lambda x: -x[1])\n",
    "\n",
    "print(f\"\\n--- Conditions ranked by effect size ---\")\n",
    "for rank, (name, d) in enumerate(ranked, 1):\n",
    "    bar = '#' * max(0, int(d * 50))\n",
    "    pct = d / d_oracle * 100 if d_oracle and d_oracle > 0 else 0\n",
    "    print(f\"  {rank}. {name:<18} d={d:+.3f} ({pct:5.1f}% oracle) {bar}\")\n",
    "\n",
    "# Q1: Does more keyword density help?\n",
    "print(f\"\\n--- Q1: Keyword density sweep (kw5 -> kw10 -> kw20) ---\")\n",
    "d_kw5 = cohens_d(bare - surr_kw5)\n",
    "d_kw10 = cohens_d(bare - surr_kw10)\n",
    "d_kw20 = cohens_d(bare - surr_kw20)\n",
    "print(f\"  surr_kw5:  d={d_kw5:+.3f}\")\n",
    "print(f\"  surr_kw10: d={d_kw10:+.3f}\")\n",
    "print(f\"  surr_kw20: d={d_kw20:+.3f}\")\n",
    "\n",
    "# Direct pairwise: kw10 vs kw5\n",
    "diff_10v5 = surr_kw5 - surr_kw10  # positive = kw10 is better\n",
    "d_10v5 = cohens_d(diff_10v5)\n",
    "_, p_10v5 = stats.ttest_1samp(diff_10v5, 0)\n",
    "sig_10v5 = '***' if p_10v5 < 0.001 else '**' if p_10v5 < 0.01 else '*' if p_10v5 < 0.05 else 'ns'\n",
    "print(f\"  kw10 vs kw5:  d={d_10v5:+.3f} ({sig_10v5})\")\n",
    "\n",
    "# kw20 vs kw10\n",
    "diff_20v10 = surr_kw10 - surr_kw20  # positive = kw20 is better\n",
    "d_20v10 = cohens_d(diff_20v10)\n",
    "_, p_20v10 = stats.ttest_1samp(diff_20v10, 0)\n",
    "sig_20v10 = '***' if p_20v10 < 0.001 else '**' if p_20v10 < 0.01 else '*' if p_20v10 < 0.05 else 'ns'\n",
    "print(f\"  kw20 vs kw10: d={d_20v10:+.3f} ({sig_20v10})\")\n",
    "\n",
    "if d_kw20 > d_kw10 > d_kw5:\n",
    "    print(f\"  -> More keywords = better. Monotonic increase.\")\n",
    "elif abs(d_kw10 - d_kw5) < 0.03 and abs(d_kw20 - d_kw10) < 0.03:\n",
    "    print(f\"  -> Keyword count doesn't matter much. Plateau after kw5.\")\n",
    "else:\n",
    "    print(f\"  -> Non-monotonic or saturating pattern.\")\n",
    "\n",
    "# Q2: Natural text vs keyword bags\n",
    "print(f\"\\n--- Q2: Natural text vs keyword bags ---\")\n",
    "d_first = cohens_d(bare - surr_first_sent)\n",
    "print(f\"  surr_first_sent: d={d_first:+.3f} ({d_first/d_oracle*100:.0f}% oracle)\" if d_oracle else \"\")\n",
    "\n",
    "diff_sent_kw5 = surr_kw5 - surr_first_sent  # positive = first_sent is better\n",
    "d_sv5 = cohens_d(diff_sent_kw5)\n",
    "_, p_sv5 = stats.ttest_1samp(diff_sent_kw5, 0)\n",
    "sig_sv5 = '***' if p_sv5 < 0.001 else '**' if p_sv5 < 0.01 else '*' if p_sv5 < 0.05 else 'ns'\n",
    "print(f\"  first_sent vs kw5: d={d_sv5:+.3f} ({sig_sv5})\")\n",
    "\n",
    "if d_sv5 > 0.05:\n",
    "    print(f\"  -> Natural text beats keyword bags!\")\n",
    "elif d_sv5 < -0.05:\n",
    "    print(f\"  -> Keywords beat natural text.\")\n",
    "else:\n",
    "    print(f\"  -> No significant difference between natural text and keywords.\")\n",
    "\n",
    "# Q3: Document-specific vs generic keywords\n",
    "print(f\"\\n--- Q3: Document-specific vs wrong-document keywords ---\")\n",
    "d_rnd_kw5 = cohens_d(bare - surr_random_kw5)\n",
    "print(f\"  surr_kw5 (this doc):    d={d_kw5:+.3f}\")\n",
    "print(f\"  surr_random_kw5 (wrong): d={d_rnd_kw5:+.3f}\")\n",
    "\n",
    "diff_spec = surr_random_kw5 - surr_kw5  # positive = specific kw is better\n",
    "d_spec = cohens_d(diff_spec)\n",
    "_, p_spec = stats.ttest_1samp(diff_spec, 0)\n",
    "sig_spec = '***' if p_spec < 0.001 else '**' if p_spec < 0.01 else '*' if p_spec < 0.05 else 'ns'\n",
    "print(f\"  specific vs wrong-doc kw: d={d_spec:+.3f} ({sig_spec})\")\n",
    "\n",
    "if d_spec > 0.05:\n",
    "    print(f\"  -> Document-specific keywords matter! Content-matching is important.\")\n",
    "elif d_spec < -0.05:\n",
    "    print(f\"  -> Wrong-doc keywords work just as well — it's about keyword-like tokens, not specificity.\")\n",
    "else:\n",
    "    print(f\"  -> No significant specificity effect — any keywords work similarly.\")\n",
    "\n",
    "# Compare random_kw5 vs random_trunc\n",
    "d_random = cohens_d(bare - random_trunc)\n",
    "print(f\"\\n  Decomposition (random_kw5 sits between random_trunc and surr_kw5):\")\n",
    "print(f\"    random_trunc (no doc signal):   d={d_random:+.3f}\")\n",
    "print(f\"    random_kw5 (wrong-doc keywords): d={d_rnd_kw5:+.3f}\")\n",
    "print(f\"    surr_kw5 (this-doc keywords):   d={d_kw5:+.3f}\")\n",
    "vocab_component = d_rnd_kw5 - d_random\n",
    "semantic_component = d_kw5 - d_rnd_kw5\n",
    "total = d_kw5 - d_random\n",
    "print(f\"    Vocabulary component: {vocab_component:+.3f} ({vocab_component/total*100:.0f}% of kw5-random gap)\" if total > 0 else \"\")\n",
    "print(f\"    Semantic component:  {semantic_component:+.3f} ({semantic_component/total*100:.0f}% of kw5-random gap)\" if total > 0 else \"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2110555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Does prefix LENGTH explain the differences, or is it content?\n",
    "print(\"=\" * 70)\n",
    "print(\"PREFIX LENGTH ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Collect prefix token counts and effect sizes per sample\n",
    "prefix_data = {}\n",
    "for name, nlls, pfx_key in [\n",
    "    ('oracle_trunc', oracle_trunc, 'n_pfx_oracle'),\n",
    "    ('surr_kw5', surr_kw5, 'n_pfx_kw5'),\n",
    "    ('surr_kw10', surr_kw10, 'n_pfx_kw10'),\n",
    "    ('surr_kw20', surr_kw20, 'n_pfx_kw20'),\n",
    "    ('surr_first_sent', surr_first_sent, 'n_pfx_first_sent'),\n",
    "    ('surr_random_kw5', surr_random_kw5, 'n_pfx_random_kw5'),\n",
    "    ('random_trunc', random_trunc, 'n_pfx_random'),\n",
    "]:\n",
    "    pfx_tokens = np.array([r.get(pfx_key, s[pfx_key]) for r, s in zip(results, samples)])\n",
    "    d = cohens_d(bare - nlls)\n",
    "    prefix_data[name] = {\n",
    "        'mean_pfx_tok': float(pfx_tokens.mean()),\n",
    "        'd': float(d),\n",
    "    }\n",
    "    print(f\"  {name:<18}: mean prefix tokens = {pfx_tokens.mean():.1f}, d = {d:+.3f}\")\n",
    "\n",
    "# Is there a correlation between prefix length and benefit across conditions?\n",
    "lengths = [v['mean_pfx_tok'] for v in prefix_data.values()]\n",
    "ds = [v['d'] for v in prefix_data.values()]\n",
    "r_len, p_len = stats.pearsonr(lengths, ds)\n",
    "print(f\"\\nCorrelation (mean prefix tokens vs d) across conditions:\")\n",
    "print(f\"  r={r_len:.3f}, p={p_len:.3e}\")\n",
    "\n",
    "if abs(r_len) < 0.3:\n",
    "    print(f\"  -> Prefix length does NOT explain condition differences. Content matters.\")\n",
    "elif r_len > 0.3:\n",
    "    print(f\"  -> Longer prefixes tend to help more. Could be length or content confound.\")\n",
    "else:\n",
    "    print(f\"  -> Longer prefixes are WORSE. Content quality > quantity.\")\n",
    "\n",
    "# Within surr_kw5: does per-sample prefix token count predict benefit?\n",
    "kw5_pfx = np.array([s['n_pfx_kw5'] for s in samples])\n",
    "kw5_benefit = bare - surr_kw5\n",
    "r_within, p_within = stats.pearsonr(kw5_pfx, kw5_benefit)\n",
    "print(f\"\\nWithin surr_kw5: correlation of prefix tokens with per-sample benefit:\")\n",
    "print(f\"  r={r_within:.3f}, p={p_within:.3e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d5e9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Comparison with Exp 01 + verdict + save\n",
    "print(\"=\" * 70)\n",
    "print(\"VERDICT — Exp 04: Prefix Content Optimization\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Exp 01 reference\n",
    "exp01_ref = {\n",
    "    'oracle_trunc': 0.228, 'surr_doc_trunc (=kw5)': 0.148,\n",
    "    'random_trunc': 0.080,\n",
    "}\n",
    "\n",
    "print(f\"\\n--- Replication check vs Exp 01 ---\")\n",
    "print(f\"  {'Condition':<25} {'Exp 01 d':>10} {'Exp 04 d':>10}\")\n",
    "print(f\"  {'-'*48}\")\n",
    "for name, ref_d in exp01_ref.items():\n",
    "    if 'kw5' in name:\n",
    "        this_d = cohens_d(bare - surr_kw5)\n",
    "    elif 'oracle' in name:\n",
    "        this_d = cohens_d(bare - oracle_trunc)\n",
    "    else:\n",
    "        this_d = cohens_d(bare - random_trunc)\n",
    "    print(f\"  {name:<25} {ref_d:>+10.3f} {this_d:>+10.3f}\")\n",
    "\n",
    "# Summary\n",
    "print(f\"\\n--- Summary ---\")\n",
    "d_oracle_val = cohens_d(bare - oracle_trunc)\n",
    "best_surr_name, best_surr_d = None, -999\n",
    "for name, nlls in [('surr_kw5', surr_kw5), ('surr_kw10', surr_kw10),\n",
    "                     ('surr_kw20', surr_kw20), ('surr_first_sent', surr_first_sent),\n",
    "                     ('surr_random_kw5', surr_random_kw5)]:\n",
    "    d = cohens_d(bare - nlls)\n",
    "    if d > best_surr_d:\n",
    "        best_surr_d = d\n",
    "        best_surr_name = name\n",
    "\n",
    "d_random_val = cohens_d(bare - random_trunc)\n",
    "\n",
    "print(f\"  Oracle (ceiling):         d={d_oracle_val:+.3f}\")\n",
    "print(f\"  Best surrogate:           {best_surr_name} d={best_surr_d:+.3f} ({best_surr_d/d_oracle_val*100:.0f}% oracle)\")\n",
    "print(f\"  Random (structural floor): d={d_random_val:+.3f} ({d_random_val/d_oracle_val*100:.0f}% oracle)\")\n",
    "print(f\"  Content headroom closed:  {(best_surr_d - d_random_val)/(d_oracle_val - d_random_val)*100:.0f}% \"\n",
    "      f\"(from random {d_random_val:+.3f} toward oracle {d_oracle_val:+.3f})\"\n",
    "      if d_oracle_val > d_random_val else \"\")\n",
    "\n",
    "# Save\n",
    "final_results = {\n",
    "    'experiment': 'v4_exp04_prefix_optimization',\n",
    "    'model': MODEL_NAME,\n",
    "    'dataset': 'ms_marco_v1.1',\n",
    "    'n_samples': len(results),\n",
    "    'seed': SEED,\n",
    "    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'bonferroni': N_BONF,\n",
    "    'conditions': analysis,\n",
    "    'ranking': [{'name': n, 'd': d} for n, d in ranked],\n",
    "    'best_surrogate': {'name': best_surr_name, 'd': float(best_surr_d)},\n",
    "    'prefix_length_analysis': prefix_data,\n",
    "}\n",
    "\n",
    "with open(RESULTS_DIR / 'results.json', 'w') as f:\n",
    "    json.dump(final_results, f, indent=2)\n",
    "print(f\"\\nResults saved to {RESULTS_DIR / 'results.json'}\")\n",
    "\n",
    "# Cleanup\n",
    "print(f\"\\nCleaning up GPU memory...\")\n",
    "mem_before = torch.cuda.memory_allocated() / 1e9\n",
    "del model, processor, tokenizer\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "mem_after = torch.cuda.memory_allocated() / 1e9\n",
    "print(f\"GPU memory: {mem_before:.2f} GB -> {mem_after:.2f} GB\")\n",
    "print(\"Done!\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
