{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a20b662",
   "metadata": {},
   "source": [
    "# Experiment 04: Prefix Content Optimization\n",
    "\n",
    "## Motivation\n",
    "\n",
    "Exp 01 (MS MARCO, ~98 token docs) showed that when the decoder already has the query,\n",
    "the **content** of the encoder prefix matters:\n",
    "\n",
    "| Condition | d vs bare | % of oracle |\n",
    "|-----------|-----------|-------------|\n",
    "| oracle_trunc (real query) | +0.228 *** | 100% |\n",
    "| surr_doc_kw5 (top-5 keywords) | +0.148 ** | 65% |\n",
    "| random_trunc (unrelated words) | +0.080 ns | 35% |\n",
    "\n",
    "The structural fraction collapsed from 85% (v3, no query in decoder) to 35% (v4,\n",
    "query in decoder). This means 65% of the benefit comes from prefix **content** — and\n",
    "there's headroom between surr_kw5 (65%) and oracle (100%).\n",
    "\n",
    "However, Exp 02/03 showed that on longer documents (256+ tokens), the structural\n",
    "mechanism regains dominance and even random prefixes become highly effective. So this\n",
    "content optimization is most relevant for **short documents**.\n",
    "\n",
    "## Questions\n",
    "\n",
    "1. Does more keyword density help? (kw5 → kw10 → kw20)\n",
    "2. Does natural text (first sentence) beat keyword bags?\n",
    "3. Do document-SPECIFIC keywords matter, or do any keywords work? (random_kw5 control)\n",
    "4. Can any surrogate close the gap between kw5 (65%) and oracle (100%)?\n",
    "\n",
    "## Conditions (10 total)\n",
    "\n",
    "### With query in decoder (production-realistic):\n",
    "\n",
    "| # | Condition | Encoder prefix | Rationale |\n",
    "|---|-----------|---------------|-----------|\n",
    "| 1 | bare | (none) | Baseline |\n",
    "| 2 | oracle_trunc | real query | Upper bound |\n",
    "| 3 | random_trunc | random unrelated words | Structural-only control |\n",
    "| 4 | surr_kw5 | top-5 TF keywords | Exp 01 baseline surrogate |\n",
    "| 5 | surr_kw10 | top-10 TF keywords | More keywords |\n",
    "| 6 | surr_kw20 | top-20 TF keywords | Maximum keyword density |\n",
    "| 7 | surr_first_sent | first sentence of doc | Natural text, high density |\n",
    "| 8 | surr_random_kw5 | top-5 kw from WRONG doc | Vocabulary control |\n",
    "\n",
    "### Without query in decoder (v3 replication):\n",
    "\n",
    "| # | Condition | Purpose |\n",
    "|---|-----------|---------|\n",
    "| 9 | bare_nq | v3 baseline |\n",
    "| 10 | oracle_trunc_nq | v3 enrichment reference |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f495de59",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T20:32:48.052280Z",
     "iopub.status.busy": "2026-02-19T20:32:48.051808Z",
     "iopub.status.idle": "2026-02-19T20:33:09.391127Z",
     "shell.execute_reply": "2026-02-19T20:33:09.390357Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading google/t5gemma-2-4b-4b...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c185b70a01ff4379aab5d21ce96fcbc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/1327 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exp 04: Prefix Content Optimization\n",
      "N: 500, Model: google/t5gemma-2-4b-4b\n",
      "DEVICE: cuda:0, dtype: torch.bfloat16\n",
      "GPU memory: 15.02 GB\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Setup and model loading\n",
    "import os\n",
    "os.umask(0o000)\n",
    "\n",
    "import sys, json, time, gc, re\n",
    "import random as pyrandom\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from scipy import stats\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "sys.path.insert(0, \"../../..\")\n",
    "from lib.analysis import cohens_d\n",
    "\n",
    "SEED = 42\n",
    "N_SAMPLES = 500\n",
    "MODEL_NAME = \"google/t5gemma-2-4b-4b\"\n",
    "\n",
    "RESULTS_DIR = Path(\"../../../results/exp04\")\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CHECKPOINT_PATH = RESULTS_DIR / \"checkpoint.json\"\n",
    "\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "pyrandom.seed(SEED)\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv())\n",
    "HF_TOKEN = os.environ.get(\"HF_TOKEN\")\n",
    "\n",
    "from transformers import AutoProcessor, AutoModelForSeq2SeqLM\n",
    "\n",
    "print(f\"Loading {MODEL_NAME}...\")\n",
    "processor = AutoProcessor.from_pretrained(MODEL_NAME, token=HF_TOKEN)\n",
    "tokenizer = processor.tokenizer\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    MODEL_NAME, device_map=\"auto\", torch_dtype=torch.bfloat16, token=HF_TOKEN,\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "DEVICE = next(model.parameters()).device\n",
    "BOS_ID = getattr(model.config, 'decoder_start_token_id', None) or tokenizer.bos_token_id\n",
    "\n",
    "print(f\"Exp 04: Prefix Content Optimization\")\n",
    "print(f\"N: {N_SAMPLES}, Model: {MODEL_NAME}\")\n",
    "print(f\"DEVICE: {DEVICE}, dtype: {next(model.parameters()).dtype}\")\n",
    "print(f\"GPU memory: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a2f9873",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T20:33:09.395181Z",
     "iopub.status.busy": "2026-02-19T20:33:09.394603Z",
     "iopub.status.idle": "2026-02-19T20:33:09.415213Z",
     "shell.execute_reply": "2026-02-19T20:33:09.414556Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scoring functions defined.\n",
      "Surrogate types: kw5, kw10, kw20, first_sent, random_kw5\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Scoring helpers\n",
    "\n",
    "def count_prefix_tokens(prefix_text, document_text):\n",
    "    full_text = prefix_text + \"\\n\" + document_text\n",
    "    full_ids = tokenizer(full_text, add_special_tokens=True, truncation=True,\n",
    "                         max_length=2048).input_ids\n",
    "    doc_ids = tokenizer(document_text, add_special_tokens=True, truncation=True,\n",
    "                        max_length=2048).input_ids\n",
    "    return len(full_ids) - len(doc_ids)\n",
    "\n",
    "\n",
    "def score_nll(encoder_text, answer_text, prefix_token_count=0, truncate=False):\n",
    "    # No query in decoder — used for _nq conditions (v3 replication).\n",
    "    enc_ids = tokenizer(encoder_text, return_tensors=\"pt\",\n",
    "                        add_special_tokens=True, truncation=True,\n",
    "                        max_length=2048).input_ids.to(DEVICE)\n",
    "    total_enc_len = enc_ids.shape[1]\n",
    "    enc_mask = torch.ones(1, total_enc_len, device=DEVICE, dtype=torch.long)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        encoder_outputs = model.get_encoder()(\n",
    "            input_ids=enc_ids, attention_mask=enc_mask\n",
    "        )\n",
    "\n",
    "    if truncate and prefix_token_count > 0:\n",
    "        cross_attn_mask = torch.ones(1, total_enc_len, device=DEVICE, dtype=torch.long)\n",
    "        cross_attn_mask[:, :prefix_token_count] = 0\n",
    "    else:\n",
    "        cross_attn_mask = torch.ones(1, total_enc_len, device=DEVICE, dtype=torch.long)\n",
    "\n",
    "    ans_ids = tokenizer(answer_text, return_tensors=\"pt\",\n",
    "                        add_special_tokens=False, truncation=True,\n",
    "                        max_length=256).input_ids.to(DEVICE)\n",
    "\n",
    "    if ans_ids.shape[1] == 0:\n",
    "        del encoder_outputs\n",
    "        return 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            encoder_outputs=encoder_outputs,\n",
    "            attention_mask=cross_attn_mask,\n",
    "            labels=ans_ids,\n",
    "        )\n",
    "\n",
    "    logits = outputs.logits\n",
    "    log_probs = F.log_softmax(logits, dim=-1)\n",
    "    token_log_probs = log_probs[0].gather(1, ans_ids[0].unsqueeze(1)).squeeze(1)\n",
    "    mean_nll = -token_log_probs.mean().item()\n",
    "\n",
    "    del encoder_outputs, outputs, logits, log_probs\n",
    "    return mean_nll\n",
    "\n",
    "\n",
    "def score_nll_query_prefix(encoder_text, query_text, answer_text,\n",
    "                           prefix_token_count=0, truncate=False):\n",
    "    # Query as decoder prefix — production-realistic.\n",
    "    enc_ids = tokenizer(encoder_text, return_tensors=\"pt\",\n",
    "                        add_special_tokens=True, truncation=True,\n",
    "                        max_length=2048).input_ids.to(DEVICE)\n",
    "    total_enc_len = enc_ids.shape[1]\n",
    "    enc_mask = torch.ones(1, total_enc_len, device=DEVICE, dtype=torch.long)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        encoder_outputs = model.get_encoder()(\n",
    "            input_ids=enc_ids, attention_mask=enc_mask\n",
    "        )\n",
    "\n",
    "    if truncate and prefix_token_count > 0:\n",
    "        cross_attn_mask = torch.ones(1, total_enc_len, device=DEVICE, dtype=torch.long)\n",
    "        cross_attn_mask[:, :prefix_token_count] = 0\n",
    "    else:\n",
    "        cross_attn_mask = torch.ones(1, total_enc_len, device=DEVICE, dtype=torch.long)\n",
    "\n",
    "    query_ids = tokenizer(query_text, add_special_tokens=False, truncation=True,\n",
    "                          max_length=512).input_ids\n",
    "    answer_ids = tokenizer(answer_text, add_special_tokens=False, truncation=True,\n",
    "                           max_length=256).input_ids\n",
    "\n",
    "    if len(answer_ids) == 0:\n",
    "        del encoder_outputs\n",
    "        return 0.0\n",
    "\n",
    "    dec_ids = [BOS_ID] + query_ids + answer_ids\n",
    "    dec_tensor = torch.tensor([dec_ids], dtype=torch.long, device=DEVICE)\n",
    "\n",
    "    n_query = len(query_ids)\n",
    "    n_answer = len(answer_ids)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            encoder_outputs=encoder_outputs,\n",
    "            attention_mask=cross_attn_mask,\n",
    "            decoder_input_ids=dec_tensor,\n",
    "        )\n",
    "\n",
    "    logits = outputs.logits\n",
    "    answer_logits = logits[0, n_query:n_query + n_answer, :]\n",
    "\n",
    "    answer_targets = torch.tensor(answer_ids, dtype=torch.long, device=DEVICE)\n",
    "    log_probs = F.log_softmax(answer_logits, dim=-1)\n",
    "    token_log_probs = log_probs.gather(1, answer_targets.unsqueeze(1)).squeeze(1)\n",
    "    mean_nll = -token_log_probs.mean().item()\n",
    "\n",
    "    del encoder_outputs, outputs, logits, log_probs\n",
    "    return mean_nll\n",
    "\n",
    "\n",
    "# === Surrogate generation ===\n",
    "STOP_WORDS = {\n",
    "    'a', 'an', 'the', 'is', 'are', 'was', 'were', 'be', 'been', 'being',\n",
    "    'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could',\n",
    "    'should', 'may', 'might', 'can', 'shall', 'to', 'of', 'in', 'for',\n",
    "    'on', 'with', 'at', 'by', 'from', 'as', 'into', 'through', 'during',\n",
    "    'before', 'after', 'above', 'below', 'between', 'and', 'but', 'or',\n",
    "    'not', 'no', 'if', 'then', 'than', 'so', 'up', 'out', 'about',\n",
    "    'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those',\n",
    "    'it', 'its', 'i', 'me', 'my', 'we', 'our', 'you', 'your', 'he',\n",
    "    'him', 'his', 'she', 'her', 'they', 'them', 'their', 'how', 'when',\n",
    "    'where', 'why', 'much', 'many', 'some', 'any', 'all', 'each',\n",
    "    'does', 'also', 'just', 'more', 'most', 'very', 'too', 'only',\n",
    "}\n",
    "\n",
    "def extract_keywords(text):\n",
    "    words = re.sub(r'[^\\w\\s]', '', text.lower()).split()\n",
    "    return [w for w in words if w not in STOP_WORDS and len(w) > 2]\n",
    "\n",
    "def make_kw_surrogate(passage, n_keywords):\n",
    "    # Extract top-N TF keywords from passage.\n",
    "    content_words = extract_keywords(passage)\n",
    "    if not content_words:\n",
    "        return \"information\"\n",
    "    counts = Counter(content_words)\n",
    "    return \" \".join(w for w, _ in counts.most_common(n_keywords))\n",
    "\n",
    "def get_first_sentence(text):\n",
    "    # Extract the first sentence by splitting on sentence-ending punctuation.\n",
    "    # Handle common abbreviations minimally.\n",
    "    parts = re.split(r'(?<=[.!?])\\s+', text.strip())\n",
    "    if parts:\n",
    "        return parts[0]\n",
    "    return text[:100]\n",
    "\n",
    "print(\"Scoring functions defined.\")\n",
    "print(\"Surrogate types: kw5, kw10, kw20, first_sent, random_kw5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62207db6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T20:33:09.418307Z",
     "iopub.status.busy": "2026-02-19T20:33:09.418053Z",
     "iopub.status.idle": "2026-02-19T20:33:13.349724Z",
     "shell.execute_reply": "2026-02-19T20:33:13.348962Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MS MARCO v1.1 validation...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total candidates: 1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loaded 500 samples\n",
      "Mean passage words: 74\n",
      "Mean query words: 6\n",
      "Mean answer words: 14\n",
      "\n",
      "Prefix token counts (mean):\n",
      "  oracle (query)    : mean=7.5, min=3, max=16\n",
      "  kw5               : mean=7.3, min=6, max=14\n",
      "  kw10              : mean=13.6, min=11, max=32\n",
      "  kw20              : mean=25.8, min=12, max=71\n",
      "  first_sent        : mean=22.1, min=3, max=125\n",
      "  random            : mean=9.1, min=3, max=28\n",
      "  random_kw5        : mean=7.3, min=6, max=14\n",
      "\n",
      "Unique content keywords per passage:\n",
      "  mean=32.9, median=32, min=8, max=65\n",
      "\n",
      "First sample:\n",
      "  Query:       what is the link between alveoli and capillaries...\n",
      "  Answer:      Diffusion...\n",
      "  Passage:     Gas exchange in the lungs takes place between the blood in the capillary network...\n",
      "  surr_kw5:    alveoli gas partial pressure exchange\n",
      "  surr_kw10:   alveoli gas partial pressure exchange blood capillary network air pulmonary\n",
      "  surr_kw20:   alveoli gas partial pressure exchange blood capillary network air pulmonary region lungs takes place surrounding itself right ventricle flows artery\n",
      "  first_sent:  Gas exchange in the lungs takes place between the blood in the capillary network\n",
      "  random_kw5:  wall concrete walls 3ft thick\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Load MS MARCO data and generate surrogates\n",
    "from lib.data import count_words\n",
    "from datasets import load_dataset\n",
    "\n",
    "print(\"Loading MS MARCO v1.1 validation...\")\n",
    "ds = load_dataset(\"microsoft/ms_marco\", \"v1.1\", split=\"validation\")\n",
    "\n",
    "all_candidates = []\n",
    "for item in ds:\n",
    "    if len(all_candidates) >= 3 * N_SAMPLES:\n",
    "        break\n",
    "    passages = item.get('passages', {})\n",
    "    ptexts = passages.get('passage_text', [])\n",
    "    is_sel = passages.get('is_selected', [])\n",
    "    query = item.get('query', '')\n",
    "    answers = item.get('answers', [])\n",
    "    well_formed = item.get('wellFormedAnswers', [])\n",
    "    answer = None\n",
    "    if well_formed and len(well_formed) > 0 and well_formed[0] not in ('[]', ''):\n",
    "        answer = well_formed[0]\n",
    "    elif answers and len(answers) > 0 and answers[0] != 'No Answer Present.':\n",
    "        answer = answers[0]\n",
    "    if not answer:\n",
    "        continue\n",
    "    for pt, sel in zip(ptexts, is_sel):\n",
    "        wc = count_words(pt)\n",
    "        if sel == 1 and 30 <= wc <= 300:\n",
    "            all_candidates.append({\n",
    "                'passage': pt, 'query': query, 'answer': answer,\n",
    "                'word_count': wc,\n",
    "            })\n",
    "            break\n",
    "\n",
    "print(f\"Total candidates: {len(all_candidates)}\")\n",
    "np.random.seed(SEED)\n",
    "indices = np.random.permutation(len(all_candidates))\n",
    "samples = [all_candidates[i] for i in indices[:N_SAMPLES]]\n",
    "del ds, all_candidates\n",
    "gc.collect()\n",
    "\n",
    "# Generate all surrogate types\n",
    "for i, s in enumerate(samples):\n",
    "    passage = s['passage']\n",
    "\n",
    "    # Keyword surrogates at different densities\n",
    "    s['surr_kw5'] = make_kw_surrogate(passage, 5)\n",
    "    s['surr_kw10'] = make_kw_surrogate(passage, 10)\n",
    "    s['surr_kw20'] = make_kw_surrogate(passage, 20)\n",
    "\n",
    "    # First sentence\n",
    "    s['surr_first_sent'] = get_first_sentence(passage)\n",
    "\n",
    "    # Random words from unrelated passage (structural control, length-matched to query)\n",
    "    other_idx = (i + N_SAMPLES // 2) % len(samples)\n",
    "    other_words = samples[other_idx]['passage'].split()\n",
    "    query_word_count = len(s['query'].split())\n",
    "    s['random_prefix'] = \" \".join(other_words[:query_word_count])\n",
    "\n",
    "    # Keywords from WRONG document (vocabulary control)\n",
    "    wrong_idx = (i + 1) % len(samples)\n",
    "    s['surr_random_kw5'] = make_kw_surrogate(samples[wrong_idx]['passage'], 5)\n",
    "\n",
    "    # Count prefix tokens for each condition\n",
    "    s['n_pfx_oracle'] = count_prefix_tokens(s['query'], passage)\n",
    "    s['n_pfx_kw5'] = count_prefix_tokens(s['surr_kw5'], passage)\n",
    "    s['n_pfx_kw10'] = count_prefix_tokens(s['surr_kw10'], passage)\n",
    "    s['n_pfx_kw20'] = count_prefix_tokens(s['surr_kw20'], passage)\n",
    "    s['n_pfx_first_sent'] = count_prefix_tokens(s['surr_first_sent'], passage)\n",
    "    s['n_pfx_random'] = count_prefix_tokens(s['random_prefix'], passage)\n",
    "    s['n_pfx_random_kw5'] = count_prefix_tokens(s['surr_random_kw5'], passage)\n",
    "\n",
    "# Statistics\n",
    "print(f\"\\nLoaded {len(samples)} samples\")\n",
    "print(f\"Mean passage words: {np.mean([s['word_count'] for s in samples]):.0f}\")\n",
    "print(f\"Mean query words: {np.mean([count_words(s['query']) for s in samples]):.0f}\")\n",
    "print(f\"Mean answer words: {np.mean([count_words(s['answer']) for s in samples]):.0f}\")\n",
    "\n",
    "# Prefix token statistics\n",
    "print(f\"\\nPrefix token counts (mean):\")\n",
    "for key, label in [('n_pfx_oracle', 'oracle (query)'),\n",
    "                    ('n_pfx_kw5', 'kw5'), ('n_pfx_kw10', 'kw10'),\n",
    "                    ('n_pfx_kw20', 'kw20'), ('n_pfx_first_sent', 'first_sent'),\n",
    "                    ('n_pfx_random', 'random'), ('n_pfx_random_kw5', 'random_kw5')]:\n",
    "    vals = [s[key] for s in samples]\n",
    "    print(f\"  {label:<18}: mean={np.mean(vals):.1f}, min={np.min(vals)}, max={np.max(vals)}\")\n",
    "\n",
    "# How many unique content keywords do passages typically have?\n",
    "kw_counts = [len(set(extract_keywords(s['passage']))) for s in samples]\n",
    "print(f\"\\nUnique content keywords per passage:\")\n",
    "print(f\"  mean={np.mean(kw_counts):.1f}, median={np.median(kw_counts):.0f}, \"\n",
    "      f\"min={np.min(kw_counts)}, max={np.max(kw_counts)}\")\n",
    "\n",
    "print(f\"\\nFirst sample:\")\n",
    "print(f\"  Query:       {samples[0]['query'][:80]}...\")\n",
    "print(f\"  Answer:      {samples[0]['answer'][:80]}...\")\n",
    "print(f\"  Passage:     {samples[0]['passage'][:80]}...\")\n",
    "print(f\"  surr_kw5:    {samples[0]['surr_kw5']}\")\n",
    "print(f\"  surr_kw10:   {samples[0]['surr_kw10']}\")\n",
    "print(f\"  surr_kw20:   {samples[0]['surr_kw20']}\")\n",
    "print(f\"  first_sent:  {samples[0]['surr_first_sent'][:80]}\")\n",
    "print(f\"  random_kw5:  {samples[0]['surr_random_kw5']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6fc2b163",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T20:33:13.353351Z",
     "iopub.status.busy": "2026-02-19T20:33:13.352568Z",
     "iopub.status.idle": "2026-02-19T20:33:14.460732Z",
     "shell.execute_reply": "2026-02-19T20:33:14.460024Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "EXAMPLE CONDITIONS (sample 0)\n",
      "======================================================================\n",
      "\n",
      "Query:        what is the link between alveoli and capillaries\n",
      "Answer:       Diffusion\n",
      "Passage:      Gas exchange in the lungs takes place between the blood in the capillary network surrounding the alveoli, and the air in...\n",
      "\n",
      "  Condition          Prefix                              Tokens  Trunc  Dec Q\n",
      "  ---------------------------------------------------------------------------\n",
      "  bare               (none)                                   0     no    yes\n",
      "  oracle_trunc       what is the link between alveoli        10    yes    yes\n",
      "  random_trunc       You are here Donair History. Don        16    yes    yes\n",
      "  surr_kw5           alveoli gas partial pressure exc         7    yes    yes\n",
      "  surr_kw10          alveoli gas partial pressure exc        12    yes    yes\n",
      "  surr_kw20          alveoli gas partial pressure exc        22    yes    yes\n",
      "  surr_first_sent    Gas exchange in the lungs takes         29    yes    yes\n",
      "  surr_random_kw5    wall concrete walls 3ft thick            8    yes    yes\n",
      "  bare_nq            (none)                                   0     no     no\n",
      "  oracle_trunc_nq    what is the link between alveoli        10    yes     no\n",
      "\n",
      "Sanity check...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  bare:         9.6250\n",
      "  oracle_trunc: 12.1875 (delta: -2.5625)\n",
      "  surr_kw5:     10.7500 (delta: -1.1250)\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Show example conditions for sample 0\n",
    "print(\"=\" * 70)\n",
    "print(\"EXAMPLE CONDITIONS (sample 0)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "ex = samples[0]\n",
    "print(f\"\\nQuery:        {ex['query']}\")\n",
    "print(f\"Answer:       {ex['answer']}\")\n",
    "print(f\"Passage:      {ex['passage'][:120]}...\")\n",
    "\n",
    "print(f\"\\n  {'Condition':<18} {'Prefix':<35} {'Tokens':>6} {'Trunc':>6} {'Dec Q':>6}\")\n",
    "print(f\"  {'-'*75}\")\n",
    "\n",
    "conditions_display = [\n",
    "    ('bare',           '(none)',                         0,                   'no',  'yes'),\n",
    "    ('oracle_trunc',   ex['query'][:32],                 ex['n_pfx_oracle'],  'yes', 'yes'),\n",
    "    ('random_trunc',   ex['random_prefix'][:32],         ex['n_pfx_random'],  'yes', 'yes'),\n",
    "    ('surr_kw5',       ex['surr_kw5'][:32],              ex['n_pfx_kw5'],     'yes', 'yes'),\n",
    "    ('surr_kw10',      ex['surr_kw10'][:32],             ex['n_pfx_kw10'],    'yes', 'yes'),\n",
    "    ('surr_kw20',      ex['surr_kw20'][:32],             ex['n_pfx_kw20'],    'yes', 'yes'),\n",
    "    ('surr_first_sent', ex['surr_first_sent'][:32],      ex['n_pfx_first_sent'], 'yes', 'yes'),\n",
    "    ('surr_random_kw5', ex['surr_random_kw5'][:32],      ex['n_pfx_random_kw5'], 'yes', 'yes'),\n",
    "    ('bare_nq',        '(none)',                         0,                   'no',  'no'),\n",
    "    ('oracle_trunc_nq', ex['query'][:32],                ex['n_pfx_oracle'],  'yes', 'no'),\n",
    "]\n",
    "\n",
    "for name, prefix, n_tok, trunc, has_q in conditions_display:\n",
    "    print(f\"  {name:<18} {prefix:<35} {n_tok:>6} {trunc:>6} {has_q:>6}\")\n",
    "\n",
    "# Sanity check\n",
    "print(f\"\\nSanity check...\")\n",
    "nll_bare = score_nll_query_prefix(ex['passage'], ex['query'], ex['answer'])\n",
    "nll_oracle = score_nll_query_prefix(\n",
    "    ex['query'] + \"\\n\" + ex['passage'], ex['query'], ex['answer'],\n",
    "    prefix_token_count=ex['n_pfx_oracle'], truncate=True)\n",
    "nll_kw5 = score_nll_query_prefix(\n",
    "    ex['surr_kw5'] + \"\\n\" + ex['passage'], ex['query'], ex['answer'],\n",
    "    prefix_token_count=ex['n_pfx_kw5'], truncate=True)\n",
    "print(f\"  bare:         {nll_bare:.4f}\")\n",
    "print(f\"  oracle_trunc: {nll_oracle:.4f} (delta: {nll_bare - nll_oracle:+.4f})\")\n",
    "print(f\"  surr_kw5:     {nll_kw5:.4f} (delta: {nll_bare - nll_kw5:+.4f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f74d0f41",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T20:33:14.464176Z",
     "iopub.status.busy": "2026-02-19T20:33:14.463911Z",
     "iopub.status.idle": "2026-02-19T20:50:21.577317Z",
     "shell.execute_reply": "2026-02-19T20:50:21.576643Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "SCORING ALL CONDITIONS\n",
      "======================================================================\n",
      "Starting fresh: 10 conditions x 500 samples = 5000 forward passes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c926cedc05684021bc0b75311e8aaf83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Scoring:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 20/500 | 0.7m | ETA 16.3m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 40/500 | 1.4m | ETA 15.7m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 60/500 | 2.0m | ETA 15.0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 80/500 | 2.7m | ETA 14.3m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 100/500 | 3.4m | ETA 13.7m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 120/500 | 4.1m | ETA 13.0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 140/500 | 4.8m | ETA 12.3m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 160/500 | 5.5m | ETA 11.6m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 180/500 | 6.1m | ETA 10.9m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 200/500 | 6.8m | ETA 10.2m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 220/500 | 7.5m | ETA 9.6m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 240/500 | 8.2m | ETA 8.9m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 260/500 | 8.9m | ETA 8.2m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 280/500 | 9.6m | ETA 7.5m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 300/500 | 10.3m | ETA 6.8m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 320/500 | 10.9m | ETA 6.2m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 340/500 | 11.6m | ETA 5.5m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 360/500 | 12.3m | ETA 4.8m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 380/500 | 13.0m | ETA 4.1m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 400/500 | 13.7m | ETA 3.4m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 420/500 | 14.4m | ETA 2.7m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 440/500 | 15.1m | ETA 2.1m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 460/500 | 15.7m | ETA 1.4m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 480/500 | 16.4m | ETA 0.7m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 500/500 | 17.1m | ETA 0.0m\n",
      "\n",
      "Scoring complete: 500 samples, 10 conditions in 17.1 min\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Scoring loop — 10 conditions x 500 samples\n",
    "print(\"=\" * 70)\n",
    "print(\"SCORING ALL CONDITIONS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "COND_NAMES = [\n",
    "    'bare', 'oracle_trunc', 'random_trunc',\n",
    "    'surr_kw5', 'surr_kw10', 'surr_kw20',\n",
    "    'surr_first_sent', 'surr_random_kw5',\n",
    "    'bare_nq', 'oracle_trunc_nq',\n",
    "]\n",
    "\n",
    "results = []\n",
    "start_idx = 0\n",
    "\n",
    "if CHECKPOINT_PATH.exists():\n",
    "    ckpt = json.loads(CHECKPOINT_PATH.read_text())\n",
    "    if ckpt.get('n_total') == N_SAMPLES and len(ckpt.get('results', [])) > 0:\n",
    "        saved_queries = [r['query'][:50] for r in ckpt['results']]\n",
    "        current_queries = [s['query'][:50] for s in samples[:len(saved_queries)]]\n",
    "        if saved_queries == current_queries:\n",
    "            results = ckpt['results']\n",
    "            start_idx = len(results)\n",
    "            print(f\"Resuming from checkpoint: {start_idx}/{N_SAMPLES}\")\n",
    "\n",
    "if start_idx == 0:\n",
    "    print(f\"Starting fresh: {len(COND_NAMES)} conditions x {N_SAMPLES} samples \"\n",
    "          f\"= {len(COND_NAMES) * N_SAMPLES} forward passes\")\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "for i in tqdm(range(start_idx, N_SAMPLES), initial=start_idx, total=N_SAMPLES,\n",
    "              desc=\"Scoring\"):\n",
    "    s = samples[i]\n",
    "    query = s['query']\n",
    "    passage = s['passage']\n",
    "    answer = s['answer']\n",
    "\n",
    "    result = {\n",
    "        'query': query,\n",
    "        'answer': answer,\n",
    "        'passage_words': s['word_count'],\n",
    "    }\n",
    "\n",
    "    # --- With query in decoder (production-realistic) ---\n",
    "\n",
    "    # 1. bare: encoder=[doc], no prefix\n",
    "    result['nll_bare'] = score_nll_query_prefix(passage, query, answer)\n",
    "\n",
    "    # 2. oracle_trunc: encoder=[query+doc], mask query\n",
    "    result['nll_oracle_trunc'] = score_nll_query_prefix(\n",
    "        query + \"\\n\" + passage, query, answer,\n",
    "        prefix_token_count=s['n_pfx_oracle'], truncate=True)\n",
    "\n",
    "    # 3. random_trunc: encoder=[random+doc], mask random\n",
    "    result['nll_random_trunc'] = score_nll_query_prefix(\n",
    "        s['random_prefix'] + \"\\n\" + passage, query, answer,\n",
    "        prefix_token_count=s['n_pfx_random'], truncate=True)\n",
    "\n",
    "    # 4. surr_kw5: top-5 TF keywords\n",
    "    result['nll_surr_kw5'] = score_nll_query_prefix(\n",
    "        s['surr_kw5'] + \"\\n\" + passage, query, answer,\n",
    "        prefix_token_count=s['n_pfx_kw5'], truncate=True)\n",
    "\n",
    "    # 5. surr_kw10: top-10 TF keywords\n",
    "    result['nll_surr_kw10'] = score_nll_query_prefix(\n",
    "        s['surr_kw10'] + \"\\n\" + passage, query, answer,\n",
    "        prefix_token_count=s['n_pfx_kw10'], truncate=True)\n",
    "\n",
    "    # 6. surr_kw20: top-20 TF keywords\n",
    "    result['nll_surr_kw20'] = score_nll_query_prefix(\n",
    "        s['surr_kw20'] + \"\\n\" + passage, query, answer,\n",
    "        prefix_token_count=s['n_pfx_kw20'], truncate=True)\n",
    "\n",
    "    # 7. surr_first_sent: first sentence of document\n",
    "    result['nll_surr_first_sent'] = score_nll_query_prefix(\n",
    "        s['surr_first_sent'] + \"\\n\" + passage, query, answer,\n",
    "        prefix_token_count=s['n_pfx_first_sent'], truncate=True)\n",
    "\n",
    "    # 8. surr_random_kw5: top-5 kw from WRONG document\n",
    "    result['nll_surr_random_kw5'] = score_nll_query_prefix(\n",
    "        s['surr_random_kw5'] + \"\\n\" + passage, query, answer,\n",
    "        prefix_token_count=s['n_pfx_random_kw5'], truncate=True)\n",
    "\n",
    "    # --- Without query in decoder (v3 replication) ---\n",
    "\n",
    "    # 9. bare_nq\n",
    "    result['nll_bare_nq'] = score_nll(passage, answer)\n",
    "\n",
    "    # 10. oracle_trunc_nq\n",
    "    result['nll_oracle_trunc_nq'] = score_nll(\n",
    "        query + \"\\n\" + passage, answer,\n",
    "        prefix_token_count=s['n_pfx_oracle'], truncate=True)\n",
    "\n",
    "    results.append(result)\n",
    "\n",
    "    if (i + 1) % 20 == 0 or i == N_SAMPLES - 1:\n",
    "        ckpt = {\n",
    "            'n_total': N_SAMPLES,\n",
    "            'results': results,\n",
    "            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        }\n",
    "        CHECKPOINT_PATH.write_text(json.dumps(ckpt))\n",
    "        elapsed = time.time() - t0\n",
    "        done = i - start_idx + 1\n",
    "        eta = (N_SAMPLES - i - 1) * elapsed / done if done > 0 else 0\n",
    "        tqdm.write(f\"  Checkpoint {i+1}/{N_SAMPLES} | {elapsed/60:.1f}m | ETA {eta/60:.1f}m\")\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "elapsed = time.time() - t0\n",
    "print(f\"\\nScoring complete: {len(results)} samples, \"\n",
    "      f\"{len(COND_NAMES)} conditions in {elapsed/60:.1f} min\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44443c86",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T20:50:21.580633Z",
     "iopub.status.busy": "2026-02-19T20:50:21.580339Z",
     "iopub.status.idle": "2026-02-19T20:50:21.604641Z",
     "shell.execute_reply": "2026-02-19T20:50:21.603885Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "RESULTS (N=500)\n",
      "======================================================================\n",
      "\n",
      "--- With query in decoder (production-realistic) ---\n",
      "  Bonferroni correction: 7 comparisons\n",
      "\n",
      "  Condition               NLL    delta        d     Win%            p   sig   %orc\n",
      "  ----------------------------------------------------------------------------------\n",
      "  bare                 2.5544       --       --       --           --    --     --\n",
      "  oracle_trunc         2.4061  +0.1482   +0.228    67.8%     5.15e-07   ***   100%\n",
      "  surr_kw20            2.4970  +0.0573   +0.115    68.0%     1.07e-02    ns    50%\n",
      "  surr_kw10            2.4588  +0.0956   +0.186    65.8%     3.78e-05   ***    82%\n",
      "  surr_kw5             2.4467  +0.1076   +0.148    61.6%     1.03e-03    **    65%\n",
      "  surr_first_sent      2.8182  -0.2639   -0.298    44.4%     6.67e-11   ***  -131%\n",
      "  surr_random_kw5      2.4294  +0.1250   +0.129    61.0%     4.06e-03     *    57%\n",
      "  random_trunc         2.4899  +0.0644   +0.080    54.6%     7.31e-02    ns    35%\n",
      "\n",
      "--- Without query in decoder (v3 replication) ---\n",
      "  bare_nq              3.6765\n",
      "  oracle_trunc_nq      2.9929  +0.6836   +0.376    92.6%     4.79e-16   ***\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Results table — all conditions\n",
    "print(\"=\" * 70)\n",
    "print(f\"RESULTS (N={len(results)})\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Extract NLL arrays\n",
    "bare = np.array([r['nll_bare'] for r in results])\n",
    "oracle_trunc = np.array([r['nll_oracle_trunc'] for r in results])\n",
    "random_trunc = np.array([r['nll_random_trunc'] for r in results])\n",
    "surr_kw5 = np.array([r['nll_surr_kw5'] for r in results])\n",
    "surr_kw10 = np.array([r['nll_surr_kw10'] for r in results])\n",
    "surr_kw20 = np.array([r['nll_surr_kw20'] for r in results])\n",
    "surr_first_sent = np.array([r['nll_surr_first_sent'] for r in results])\n",
    "surr_random_kw5 = np.array([r['nll_surr_random_kw5'] for r in results])\n",
    "bare_nq = np.array([r['nll_bare_nq'] for r in results])\n",
    "oracle_nq = np.array([r['nll_oracle_trunc_nq'] for r in results])\n",
    "\n",
    "# Bonferroni: 7 comparisons vs bare (all with-query conditions except bare)\n",
    "N_BONF = 7\n",
    "\n",
    "print(f\"\\n--- With query in decoder (production-realistic) ---\")\n",
    "print(f\"  Bonferroni correction: {N_BONF} comparisons\")\n",
    "print(f\"\\n  {'Condition':<18} {'NLL':>8} {'delta':>8} {'d':>8} {'Win%':>8} {'p':>12} {'sig':>5} {'%orc':>6}\")\n",
    "print(f\"  {'-'*82}\")\n",
    "\n",
    "analysis = {}\n",
    "d_oracle = None\n",
    "for name, nlls in [\n",
    "    ('bare', bare),\n",
    "    ('oracle_trunc', oracle_trunc),\n",
    "    ('surr_kw20', surr_kw20),\n",
    "    ('surr_kw10', surr_kw10),\n",
    "    ('surr_kw5', surr_kw5),\n",
    "    ('surr_first_sent', surr_first_sent),\n",
    "    ('surr_random_kw5', surr_random_kw5),\n",
    "    ('random_trunc', random_trunc),\n",
    "]:\n",
    "    mean_nll = nlls.mean()\n",
    "    if name == 'bare':\n",
    "        print(f\"  {name:<18} {mean_nll:>8.4f} {'--':>8} {'--':>8} {'--':>8} {'--':>12} {'--':>5} {'--':>6}\")\n",
    "        analysis[name] = {'mean_nll': float(mean_nll)}\n",
    "    else:\n",
    "        diff = bare - nlls\n",
    "        d = cohens_d(diff)\n",
    "        win_pct = 100 * np.mean(diff > 0)\n",
    "        _, p_val = stats.ttest_1samp(diff, 0)\n",
    "        sig = '***' if p_val < 0.001/N_BONF else '**' if p_val < 0.01/N_BONF else '*' if p_val < 0.05/N_BONF else 'ns'\n",
    "        if name == 'oracle_trunc':\n",
    "            d_oracle = d\n",
    "        pct_orc = f\"{d/d_oracle*100:.0f}%\" if d_oracle and d_oracle > 0 else \"--\"\n",
    "        print(f\"  {name:<18} {mean_nll:>8.4f} {diff.mean():>+8.4f} {d:>+8.3f} {win_pct:>7.1f}% {p_val:>12.2e} {sig:>5} {pct_orc:>6}\")\n",
    "        analysis[name] = {\n",
    "            'mean_nll': float(mean_nll), 'delta': float(diff.mean()),\n",
    "            'd': float(d), 'win_pct': float(win_pct), 'p': float(p_val),\n",
    "        }\n",
    "\n",
    "# No-query conditions\n",
    "diff_nq = bare_nq - oracle_nq\n",
    "d_nq = cohens_d(diff_nq)\n",
    "win_nq = 100 * np.mean(diff_nq > 0)\n",
    "_, p_nq = stats.ttest_1samp(diff_nq, 0)\n",
    "sig_nq = '***' if p_nq < 0.001 else '**' if p_nq < 0.01 else '*' if p_nq < 0.05 else 'ns'\n",
    "\n",
    "print(f\"\\n--- Without query in decoder (v3 replication) ---\")\n",
    "print(f\"  {'bare_nq':<18} {bare_nq.mean():>8.4f}\")\n",
    "print(f\"  {'oracle_trunc_nq':<18} {oracle_nq.mean():>8.4f} {diff_nq.mean():>+8.4f} {d_nq:>+8.3f} {win_nq:>7.1f}% {p_nq:>12.2e} {sig_nq:>5}\")\n",
    "\n",
    "analysis['bare_nq'] = {'mean_nll': float(bare_nq.mean())}\n",
    "analysis['oracle_trunc_nq'] = {\n",
    "    'mean_nll': float(oracle_nq.mean()), 'delta': float(diff_nq.mean()),\n",
    "    'd': float(d_nq), 'win_pct': float(win_nq), 'p': float(p_nq),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61774fc1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T20:50:21.607835Z",
     "iopub.status.busy": "2026-02-19T20:50:21.607560Z",
     "iopub.status.idle": "2026-02-19T20:50:21.627546Z",
     "shell.execute_reply": "2026-02-19T20:50:21.626902Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "CONTENT TYPE ANALYSIS\n",
      "======================================================================\n",
      "\n",
      "--- Conditions ranked by effect size ---\n",
      "  1. oracle_trunc       d=+0.228 (100.0% oracle) ###########\n",
      "  2. surr_kw10          d=+0.186 ( 81.7% oracle) #########\n",
      "  3. surr_kw5           d=+0.148 ( 64.9% oracle) #######\n",
      "  4. surr_random_kw5    d=+0.129 ( 56.7% oracle) ######\n",
      "  5. surr_kw20          d=+0.115 ( 50.3% oracle) #####\n",
      "  6. random_trunc       d=+0.080 ( 35.3% oracle) ####\n",
      "  7. surr_first_sent    d=-0.298 (-131.2% oracle) \n",
      "\n",
      "--- Q1: Keyword density sweep (kw5 -> kw10 -> kw20) ---\n",
      "  surr_kw5:  d=+0.148\n",
      "  surr_kw10: d=+0.186\n",
      "  surr_kw20: d=+0.115\n",
      "  kw10 vs kw5:  d=-0.026 (ns)\n",
      "  kw20 vs kw10: d=-0.099 (*)\n",
      "  -> Non-monotonic or saturating pattern.\n",
      "\n",
      "--- Q2: Natural text vs keyword bags ---\n",
      "  surr_first_sent: d=-0.298 (-131% oracle)\n",
      "  first_sent vs kw5: d=-0.307 (***)\n",
      "  -> Keywords beat natural text.\n",
      "\n",
      "--- Q3: Document-specific vs wrong-document keywords ---\n",
      "  surr_kw5 (this doc):    d=+0.148\n",
      "  surr_random_kw5 (wrong): d=+0.129\n",
      "  specific vs wrong-doc kw: d=-0.024 (ns)\n",
      "  -> No significant specificity effect — any keywords work similarly.\n",
      "\n",
      "  Decomposition (random_kw5 sits between random_trunc and surr_kw5):\n",
      "    random_trunc (no doc signal):   d=+0.080\n",
      "    random_kw5 (wrong-doc keywords): d=+0.129\n",
      "    surr_kw5 (this-doc keywords):   d=+0.148\n",
      "    Vocabulary component: +0.049 (72% of kw5-random gap)\n",
      "    Semantic component:  +0.019 (28% of kw5-random gap)\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Content type analysis — what type of prefix works best?\n",
    "print(\"=\" * 70)\n",
    "print(\"CONTENT TYPE ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Rank conditions by effect size\n",
    "condition_list = [\n",
    "    ('oracle_trunc', oracle_trunc),\n",
    "    ('surr_kw20', surr_kw20),\n",
    "    ('surr_kw10', surr_kw10),\n",
    "    ('surr_kw5', surr_kw5),\n",
    "    ('surr_first_sent', surr_first_sent),\n",
    "    ('surr_random_kw5', surr_random_kw5),\n",
    "    ('random_trunc', random_trunc),\n",
    "]\n",
    "\n",
    "ranked = []\n",
    "for name, nlls in condition_list:\n",
    "    d = cohens_d(bare - nlls)\n",
    "    ranked.append((name, d))\n",
    "ranked.sort(key=lambda x: -x[1])\n",
    "\n",
    "print(f\"\\n--- Conditions ranked by effect size ---\")\n",
    "for rank, (name, d) in enumerate(ranked, 1):\n",
    "    bar = '#' * max(0, int(d * 50))\n",
    "    pct = d / d_oracle * 100 if d_oracle and d_oracle > 0 else 0\n",
    "    print(f\"  {rank}. {name:<18} d={d:+.3f} ({pct:5.1f}% oracle) {bar}\")\n",
    "\n",
    "# Q1: Does more keyword density help?\n",
    "print(f\"\\n--- Q1: Keyword density sweep (kw5 -> kw10 -> kw20) ---\")\n",
    "d_kw5 = cohens_d(bare - surr_kw5)\n",
    "d_kw10 = cohens_d(bare - surr_kw10)\n",
    "d_kw20 = cohens_d(bare - surr_kw20)\n",
    "print(f\"  surr_kw5:  d={d_kw5:+.3f}\")\n",
    "print(f\"  surr_kw10: d={d_kw10:+.3f}\")\n",
    "print(f\"  surr_kw20: d={d_kw20:+.3f}\")\n",
    "\n",
    "# Direct pairwise: kw10 vs kw5\n",
    "diff_10v5 = surr_kw5 - surr_kw10  # positive = kw10 is better\n",
    "d_10v5 = cohens_d(diff_10v5)\n",
    "_, p_10v5 = stats.ttest_1samp(diff_10v5, 0)\n",
    "sig_10v5 = '***' if p_10v5 < 0.001 else '**' if p_10v5 < 0.01 else '*' if p_10v5 < 0.05 else 'ns'\n",
    "print(f\"  kw10 vs kw5:  d={d_10v5:+.3f} ({sig_10v5})\")\n",
    "\n",
    "# kw20 vs kw10\n",
    "diff_20v10 = surr_kw10 - surr_kw20  # positive = kw20 is better\n",
    "d_20v10 = cohens_d(diff_20v10)\n",
    "_, p_20v10 = stats.ttest_1samp(diff_20v10, 0)\n",
    "sig_20v10 = '***' if p_20v10 < 0.001 else '**' if p_20v10 < 0.01 else '*' if p_20v10 < 0.05 else 'ns'\n",
    "print(f\"  kw20 vs kw10: d={d_20v10:+.3f} ({sig_20v10})\")\n",
    "\n",
    "if d_kw20 > d_kw10 > d_kw5:\n",
    "    print(f\"  -> More keywords = better. Monotonic increase.\")\n",
    "elif abs(d_kw10 - d_kw5) < 0.03 and abs(d_kw20 - d_kw10) < 0.03:\n",
    "    print(f\"  -> Keyword count doesn't matter much. Plateau after kw5.\")\n",
    "else:\n",
    "    print(f\"  -> Non-monotonic or saturating pattern.\")\n",
    "\n",
    "# Q2: Natural text vs keyword bags\n",
    "print(f\"\\n--- Q2: Natural text vs keyword bags ---\")\n",
    "d_first = cohens_d(bare - surr_first_sent)\n",
    "print(f\"  surr_first_sent: d={d_first:+.3f} ({d_first/d_oracle*100:.0f}% oracle)\" if d_oracle else \"\")\n",
    "\n",
    "diff_sent_kw5 = surr_kw5 - surr_first_sent  # positive = first_sent is better\n",
    "d_sv5 = cohens_d(diff_sent_kw5)\n",
    "_, p_sv5 = stats.ttest_1samp(diff_sent_kw5, 0)\n",
    "sig_sv5 = '***' if p_sv5 < 0.001 else '**' if p_sv5 < 0.01 else '*' if p_sv5 < 0.05 else 'ns'\n",
    "print(f\"  first_sent vs kw5: d={d_sv5:+.3f} ({sig_sv5})\")\n",
    "\n",
    "if d_sv5 > 0.05:\n",
    "    print(f\"  -> Natural text beats keyword bags!\")\n",
    "elif d_sv5 < -0.05:\n",
    "    print(f\"  -> Keywords beat natural text.\")\n",
    "else:\n",
    "    print(f\"  -> No significant difference between natural text and keywords.\")\n",
    "\n",
    "# Q3: Document-specific vs generic keywords\n",
    "print(f\"\\n--- Q3: Document-specific vs wrong-document keywords ---\")\n",
    "d_rnd_kw5 = cohens_d(bare - surr_random_kw5)\n",
    "print(f\"  surr_kw5 (this doc):    d={d_kw5:+.3f}\")\n",
    "print(f\"  surr_random_kw5 (wrong): d={d_rnd_kw5:+.3f}\")\n",
    "\n",
    "diff_spec = surr_random_kw5 - surr_kw5  # positive = specific kw is better\n",
    "d_spec = cohens_d(diff_spec)\n",
    "_, p_spec = stats.ttest_1samp(diff_spec, 0)\n",
    "sig_spec = '***' if p_spec < 0.001 else '**' if p_spec < 0.01 else '*' if p_spec < 0.05 else 'ns'\n",
    "print(f\"  specific vs wrong-doc kw: d={d_spec:+.3f} ({sig_spec})\")\n",
    "\n",
    "if d_spec > 0.05:\n",
    "    print(f\"  -> Document-specific keywords matter! Content-matching is important.\")\n",
    "elif d_spec < -0.05:\n",
    "    print(f\"  -> Wrong-doc keywords work just as well — it's about keyword-like tokens, not specificity.\")\n",
    "else:\n",
    "    print(f\"  -> No significant specificity effect — any keywords work similarly.\")\n",
    "\n",
    "# Compare random_kw5 vs random_trunc\n",
    "d_random = cohens_d(bare - random_trunc)\n",
    "print(f\"\\n  Decomposition (random_kw5 sits between random_trunc and surr_kw5):\")\n",
    "print(f\"    random_trunc (no doc signal):   d={d_random:+.3f}\")\n",
    "print(f\"    random_kw5 (wrong-doc keywords): d={d_rnd_kw5:+.3f}\")\n",
    "print(f\"    surr_kw5 (this-doc keywords):   d={d_kw5:+.3f}\")\n",
    "vocab_component = d_rnd_kw5 - d_random\n",
    "semantic_component = d_kw5 - d_rnd_kw5\n",
    "total = d_kw5 - d_random\n",
    "print(f\"    Vocabulary component: {vocab_component:+.3f} ({vocab_component/total*100:.0f}% of kw5-random gap)\" if total > 0 else \"\")\n",
    "print(f\"    Semantic component:  {semantic_component:+.3f} ({semantic_component/total*100:.0f}% of kw5-random gap)\" if total > 0 else \"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2110555",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T20:50:21.630326Z",
     "iopub.status.busy": "2026-02-19T20:50:21.630054Z",
     "iopub.status.idle": "2026-02-19T20:50:21.644638Z",
     "shell.execute_reply": "2026-02-19T20:50:21.643948Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PREFIX LENGTH ANALYSIS\n",
      "======================================================================\n",
      "  oracle_trunc      : mean prefix tokens = 7.5, d = +0.228\n",
      "  surr_kw5          : mean prefix tokens = 7.3, d = +0.148\n",
      "  surr_kw10         : mean prefix tokens = 13.6, d = +0.186\n",
      "  surr_kw20         : mean prefix tokens = 25.8, d = +0.115\n",
      "  surr_first_sent   : mean prefix tokens = 22.1, d = -0.298\n",
      "  surr_random_kw5   : mean prefix tokens = 7.3, d = +0.129\n",
      "  random_trunc      : mean prefix tokens = 9.1, d = +0.080\n",
      "\n",
      "Correlation (mean prefix tokens vs d) across conditions:\n",
      "  r=-0.546, p=2.047e-01\n",
      "  -> Longer prefixes are WORSE. Content quality > quantity.\n",
      "\n",
      "Within surr_kw5: correlation of prefix tokens with per-sample benefit:\n",
      "  r=0.036, p=4.208e-01\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Does prefix LENGTH explain the differences, or is it content?\n",
    "print(\"=\" * 70)\n",
    "print(\"PREFIX LENGTH ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Collect prefix token counts and effect sizes per sample\n",
    "prefix_data = {}\n",
    "for name, nlls, pfx_key in [\n",
    "    ('oracle_trunc', oracle_trunc, 'n_pfx_oracle'),\n",
    "    ('surr_kw5', surr_kw5, 'n_pfx_kw5'),\n",
    "    ('surr_kw10', surr_kw10, 'n_pfx_kw10'),\n",
    "    ('surr_kw20', surr_kw20, 'n_pfx_kw20'),\n",
    "    ('surr_first_sent', surr_first_sent, 'n_pfx_first_sent'),\n",
    "    ('surr_random_kw5', surr_random_kw5, 'n_pfx_random_kw5'),\n",
    "    ('random_trunc', random_trunc, 'n_pfx_random'),\n",
    "]:\n",
    "    pfx_tokens = np.array([r.get(pfx_key, s[pfx_key]) for r, s in zip(results, samples)])\n",
    "    d = cohens_d(bare - nlls)\n",
    "    prefix_data[name] = {\n",
    "        'mean_pfx_tok': float(pfx_tokens.mean()),\n",
    "        'd': float(d),\n",
    "    }\n",
    "    print(f\"  {name:<18}: mean prefix tokens = {pfx_tokens.mean():.1f}, d = {d:+.3f}\")\n",
    "\n",
    "# Is there a correlation between prefix length and benefit across conditions?\n",
    "lengths = [v['mean_pfx_tok'] for v in prefix_data.values()]\n",
    "ds = [v['d'] for v in prefix_data.values()]\n",
    "r_len, p_len = stats.pearsonr(lengths, ds)\n",
    "print(f\"\\nCorrelation (mean prefix tokens vs d) across conditions:\")\n",
    "print(f\"  r={r_len:.3f}, p={p_len:.3e}\")\n",
    "\n",
    "if abs(r_len) < 0.3:\n",
    "    print(f\"  -> Prefix length does NOT explain condition differences. Content matters.\")\n",
    "elif r_len > 0.3:\n",
    "    print(f\"  -> Longer prefixes tend to help more. Could be length or content confound.\")\n",
    "else:\n",
    "    print(f\"  -> Longer prefixes are WORSE. Content quality > quantity.\")\n",
    "\n",
    "# Within surr_kw5: does per-sample prefix token count predict benefit?\n",
    "kw5_pfx = np.array([s['n_pfx_kw5'] for s in samples])\n",
    "kw5_benefit = bare - surr_kw5\n",
    "r_within, p_within = stats.pearsonr(kw5_pfx, kw5_benefit)\n",
    "print(f\"\\nWithin surr_kw5: correlation of prefix tokens with per-sample benefit:\")\n",
    "print(f\"  r={r_within:.3f}, p={p_within:.3e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0d5e9dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T20:50:21.647613Z",
     "iopub.status.busy": "2026-02-19T20:50:21.647356Z",
     "iopub.status.idle": "2026-02-19T20:50:22.151696Z",
     "shell.execute_reply": "2026-02-19T20:50:22.150978Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "VERDICT — Exp 04: Prefix Content Optimization\n",
      "======================================================================\n",
      "\n",
      "--- Replication check vs Exp 01 ---\n",
      "  Condition                   Exp 01 d   Exp 04 d\n",
      "  ------------------------------------------------\n",
      "  oracle_trunc                  +0.228     +0.228\n",
      "  surr_doc_trunc (=kw5)         +0.148     +0.148\n",
      "  random_trunc                  +0.080     +0.080\n",
      "\n",
      "--- Summary ---\n",
      "  Oracle (ceiling):         d=+0.228\n",
      "  Best surrogate:           surr_kw10 d=+0.186 (82% oracle)\n",
      "  Random (structural floor): d=+0.080 (35% oracle)\n",
      "  Content headroom closed:  72% (from random +0.080 toward oracle +0.228)\n",
      "\n",
      "Results saved to ../../../results/exp04/results.json\n",
      "\n",
      "Cleaning up GPU memory...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory: 15.03 GB -> 0.01 GB\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Cell 10: Comparison with Exp 01 + verdict + save\n",
    "print(\"=\" * 70)\n",
    "print(\"VERDICT — Exp 04: Prefix Content Optimization\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Exp 01 reference\n",
    "exp01_ref = {\n",
    "    'oracle_trunc': 0.228, 'surr_doc_trunc (=kw5)': 0.148,\n",
    "    'random_trunc': 0.080,\n",
    "}\n",
    "\n",
    "print(f\"\\n--- Replication check vs Exp 01 ---\")\n",
    "print(f\"  {'Condition':<25} {'Exp 01 d':>10} {'Exp 04 d':>10}\")\n",
    "print(f\"  {'-'*48}\")\n",
    "for name, ref_d in exp01_ref.items():\n",
    "    if 'kw5' in name:\n",
    "        this_d = cohens_d(bare - surr_kw5)\n",
    "    elif 'oracle' in name:\n",
    "        this_d = cohens_d(bare - oracle_trunc)\n",
    "    else:\n",
    "        this_d = cohens_d(bare - random_trunc)\n",
    "    print(f\"  {name:<25} {ref_d:>+10.3f} {this_d:>+10.3f}\")\n",
    "\n",
    "# Summary\n",
    "print(f\"\\n--- Summary ---\")\n",
    "d_oracle_val = cohens_d(bare - oracle_trunc)\n",
    "best_surr_name, best_surr_d = None, -999\n",
    "for name, nlls in [('surr_kw5', surr_kw5), ('surr_kw10', surr_kw10),\n",
    "                     ('surr_kw20', surr_kw20), ('surr_first_sent', surr_first_sent),\n",
    "                     ('surr_random_kw5', surr_random_kw5)]:\n",
    "    d = cohens_d(bare - nlls)\n",
    "    if d > best_surr_d:\n",
    "        best_surr_d = d\n",
    "        best_surr_name = name\n",
    "\n",
    "d_random_val = cohens_d(bare - random_trunc)\n",
    "\n",
    "print(f\"  Oracle (ceiling):         d={d_oracle_val:+.3f}\")\n",
    "print(f\"  Best surrogate:           {best_surr_name} d={best_surr_d:+.3f} ({best_surr_d/d_oracle_val*100:.0f}% oracle)\")\n",
    "print(f\"  Random (structural floor): d={d_random_val:+.3f} ({d_random_val/d_oracle_val*100:.0f}% oracle)\")\n",
    "print(f\"  Content headroom closed:  {(best_surr_d - d_random_val)/(d_oracle_val - d_random_val)*100:.0f}% \"\n",
    "      f\"(from random {d_random_val:+.3f} toward oracle {d_oracle_val:+.3f})\"\n",
    "      if d_oracle_val > d_random_val else \"\")\n",
    "\n",
    "# Save\n",
    "final_results = {\n",
    "    'experiment': 'v4_exp04_prefix_optimization',\n",
    "    'model': MODEL_NAME,\n",
    "    'dataset': 'ms_marco_v1.1',\n",
    "    'n_samples': len(results),\n",
    "    'seed': SEED,\n",
    "    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'bonferroni': N_BONF,\n",
    "    'conditions': analysis,\n",
    "    'ranking': [{'name': n, 'd': d} for n, d in ranked],\n",
    "    'best_surrogate': {'name': best_surr_name, 'd': float(best_surr_d)},\n",
    "    'prefix_length_analysis': prefix_data,\n",
    "}\n",
    "\n",
    "with open(RESULTS_DIR / 'results.json', 'w') as f:\n",
    "    json.dump(final_results, f, indent=2)\n",
    "print(f\"\\nResults saved to {RESULTS_DIR / 'results.json'}\")\n",
    "\n",
    "# Cleanup\n",
    "print(f\"\\nCleaning up GPU memory...\")\n",
    "mem_before = torch.cuda.memory_allocated() / 1e9\n",
    "del model, processor, tokenizer\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "mem_after = torch.cuda.memory_allocated() / 1e9\n",
    "print(f\"GPU memory: {mem_before:.2f} GB -> {mem_after:.2f} GB\")\n",
    "print(\"Done!\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "1223498f925d41cba7f578d411abf83f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "21b77d995b47447cae705ad023012672": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_2eb907adbd394a18a3c82e83fe2df744",
       "placeholder": "​",
       "style": "IPY_MODEL_c54ee71a3f78492691e0f92001a864a7",
       "tabbable": null,
       "tooltip": null,
       "value": "Scoring: 100%"
      }
     },
     "27a47bdd93724454928fa8e33d91da86": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "2eb907adbd394a18a3c82e83fe2df744": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "36c8581aa4ee41eeb04741a8359c0fc4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_8b892bd416a842cf96f0d2db52706364",
       "placeholder": "​",
       "style": "IPY_MODEL_f434a1bad68143fdbc4a1ccc7559924a",
       "tabbable": null,
       "tooltip": null,
       "value": "Loading weights: 100%"
      }
     },
     "5325e2a88c5b4d8ea97d11e60884fc6b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "5462da7abb854d4390b737b4660d6619": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "5abf4d97480441d0a6b0a26ccd2fa675": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "81af430d6efc43c4a6db2028d3ffad1c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "88a1966a110c4d6381a512ff27527ab3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "8b892bd416a842cf96f0d2db52706364": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "98ca2d051c0346ea947d2803636ae3f8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_5abf4d97480441d0a6b0a26ccd2fa675",
       "max": 1327.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_27a47bdd93724454928fa8e33d91da86",
       "tabbable": null,
       "tooltip": null,
       "value": 1327.0
      }
     },
     "998eeb7b35014507a49f9e156e302f9a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_5325e2a88c5b4d8ea97d11e60884fc6b",
       "placeholder": "​",
       "style": "IPY_MODEL_d235f9ac51fd4034bcce46cf1f4dd41a",
       "tabbable": null,
       "tooltip": null,
       "value": " 500/500 [17:07&lt;00:00,  2.06s/it]"
      }
     },
     "9e557e8bcf2043bdac46260f7dbffa67": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a55c786bb30e4eab96531eb513485df4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_81af430d6efc43c4a6db2028d3ffad1c",
       "placeholder": "​",
       "style": "IPY_MODEL_88a1966a110c4d6381a512ff27527ab3",
       "tabbable": null,
       "tooltip": null,
       "value": " 1327/1327 [00:04&lt;00:00, 717.84it/s, Materializing param=model.encoder.vision_tower.vision_model.post_layernorm.weight]"
      }
     },
     "b78010cc9b644aaa8b57fd0affe72c4b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "c185b70a01ff4379aab5d21ce96fcbc7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_36c8581aa4ee41eeb04741a8359c0fc4",
        "IPY_MODEL_98ca2d051c0346ea947d2803636ae3f8",
        "IPY_MODEL_a55c786bb30e4eab96531eb513485df4"
       ],
       "layout": "IPY_MODEL_1223498f925d41cba7f578d411abf83f",
       "tabbable": null,
       "tooltip": null
      }
     },
     "c54ee71a3f78492691e0f92001a864a7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "c926cedc05684021bc0b75311e8aaf83": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_21b77d995b47447cae705ad023012672",
        "IPY_MODEL_f3355ecbc0f04b25bfe53dca36cd2918",
        "IPY_MODEL_998eeb7b35014507a49f9e156e302f9a"
       ],
       "layout": "IPY_MODEL_9e557e8bcf2043bdac46260f7dbffa67",
       "tabbable": null,
       "tooltip": null
      }
     },
     "d235f9ac51fd4034bcce46cf1f4dd41a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "f3355ecbc0f04b25bfe53dca36cd2918": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_5462da7abb854d4390b737b4660d6619",
       "max": 500.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_b78010cc9b644aaa8b57fd0affe72c4b",
       "tabbable": null,
       "tooltip": null,
       "value": 500.0
      }
     },
     "f434a1bad68143fdbc4a1ccc7559924a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
