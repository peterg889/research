{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a52a4b4e",
   "metadata": {},
   "source": [
    "# Experiment 03: Cross-Dataset Validation — Neural-Bridge (Long Documents)\n",
    "\n",
    "## Motivation\n",
    "\n",
    "Exp 01 (MS MARCO, ~98 token docs) showed the v4 enrichment benefit survives with query\n",
    "in decoder (d=+0.228). Exp 02 showed this benefit *grows* at longer padded documents\n",
    "(d=+0.43 at 4096 tokens). But Exp 02 used artificial padding with unrelated text.\n",
    "\n",
    "Neural-bridge/rag-dataset-12000 provides a natural test:\n",
    "- Documents are ~600 words (~800-1000 tokens) — genuinely long\n",
    "- Queries are ~18 words (3x MS MARCO)\n",
    "- Different domain and generation process\n",
    "\n",
    "v3 Exp 3D on this dataset (no query in decoder) found:\n",
    "- Structure = 84.3% (matched MS MARCO's 84.7%)\n",
    "- ALL surrogates beat oracle — the real query creates semantic interference\n",
    "- Oracle d was modest because of this interference\n",
    "\n",
    "**Key questions**:\n",
    "1. Does the v4 enrichment benefit hold on naturally long documents?\n",
    "2. Does the v4 mechanism shift (structural collapse, content dominance) replicate?\n",
    "3. Does the \"surrogates beat oracle\" phenomenon from v3 persist when the decoder has the query?\n",
    "\n",
    "## Conditions (6 total)\n",
    "\n",
    "### With query in decoder (production-realistic):\n",
    "\n",
    "| # | Condition | Encoder input | Cross-attn | Purpose |\n",
    "|---|-----------|--------------|------------|---------|\n",
    "| 1 | bare | [document] | all | Baseline |\n",
    "| 2 | oracle_trunc | [query + doc] | doc only | Upper bound |\n",
    "| 3 | surr_doc_trunc | [top-5 kw + doc] | doc only | Production surrogate |\n",
    "| 4 | random_trunc | [random words + doc] | doc only | Structural control |\n",
    "\n",
    "### Without query in decoder (v3 replication):\n",
    "\n",
    "| # | Condition | Encoder input | Cross-attn | Purpose |\n",
    "|---|-----------|--------------|------------|---------|\n",
    "| 5 | bare_nq | [document] | all | v3 baseline |\n",
    "| 6 | oracle_trunc_nq | [query + doc] | doc only | v3 enrichment reference |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a42c8346",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T20:11:46.555695Z",
     "iopub.status.busy": "2026-02-19T20:11:46.555404Z",
     "iopub.status.idle": "2026-02-19T20:12:08.114007Z",
     "shell.execute_reply": "2026-02-19T20:12:08.113174Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading google/t5gemma-2-4b-4b...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcdbd73721804f42b94c795437406995",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/1327 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exp 03: Cross-Dataset Validation — Neural-Bridge\n",
      "N: 500, Model: google/t5gemma-2-4b-4b\n",
      "DEVICE: cuda:0, dtype: torch.bfloat16\n",
      "GPU memory: 15.02 GB\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Setup and model loading\n",
    "import os\n",
    "os.umask(0o000)\n",
    "\n",
    "import sys, json, time, gc, re\n",
    "import random as pyrandom\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from scipy import stats\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "sys.path.insert(0, \"../../..\")\n",
    "from lib.analysis import cohens_d\n",
    "\n",
    "SEED = 42\n",
    "N_SAMPLES = 500\n",
    "MODEL_NAME = \"google/t5gemma-2-4b-4b\"\n",
    "\n",
    "RESULTS_DIR = Path(\"../../../results/exp03\")\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CHECKPOINT_PATH = RESULTS_DIR / \"checkpoint.json\"\n",
    "\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "pyrandom.seed(SEED)\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv())\n",
    "HF_TOKEN = os.environ.get(\"HF_TOKEN\")\n",
    "\n",
    "from transformers import AutoProcessor, AutoModelForSeq2SeqLM\n",
    "\n",
    "print(f\"Loading {MODEL_NAME}...\")\n",
    "processor = AutoProcessor.from_pretrained(MODEL_NAME, token=HF_TOKEN)\n",
    "tokenizer = processor.tokenizer\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    MODEL_NAME, device_map=\"auto\", torch_dtype=torch.bfloat16, token=HF_TOKEN,\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "DEVICE = next(model.parameters()).device\n",
    "BOS_ID = getattr(model.config, 'decoder_start_token_id', None) or tokenizer.bos_token_id\n",
    "\n",
    "print(f\"Exp 03: Cross-Dataset Validation — Neural-Bridge\")\n",
    "print(f\"N: {N_SAMPLES}, Model: {MODEL_NAME}\")\n",
    "print(f\"DEVICE: {DEVICE}, dtype: {next(model.parameters()).dtype}\")\n",
    "print(f\"GPU memory: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b416ef2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T20:12:08.117966Z",
     "iopub.status.busy": "2026-02-19T20:12:08.117353Z",
     "iopub.status.idle": "2026-02-19T20:12:08.137294Z",
     "shell.execute_reply": "2026-02-19T20:12:08.136628Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scoring functions defined.\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Scoring helpers\n",
    "\n",
    "def count_prefix_tokens(prefix_text, document_text):\n",
    "    full_text = prefix_text + \"\\n\" + document_text\n",
    "    full_ids = tokenizer(full_text, add_special_tokens=True, truncation=True,\n",
    "                         max_length=2048).input_ids\n",
    "    doc_ids = tokenizer(document_text, add_special_tokens=True, truncation=True,\n",
    "                        max_length=2048).input_ids\n",
    "    return len(full_ids) - len(doc_ids)\n",
    "\n",
    "\n",
    "def score_nll(encoder_text, answer_text, prefix_token_count=0, truncate=False):\n",
    "    # No query in decoder — used for _nq conditions (v3 replication).\n",
    "    enc_ids = tokenizer(encoder_text, return_tensors=\"pt\",\n",
    "                        add_special_tokens=True, truncation=True,\n",
    "                        max_length=2048).input_ids.to(DEVICE)\n",
    "    total_enc_len = enc_ids.shape[1]\n",
    "    enc_mask = torch.ones(1, total_enc_len, device=DEVICE, dtype=torch.long)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        encoder_outputs = model.get_encoder()(\n",
    "            input_ids=enc_ids, attention_mask=enc_mask\n",
    "        )\n",
    "\n",
    "    if truncate and prefix_token_count > 0:\n",
    "        cross_attn_mask = torch.ones(1, total_enc_len, device=DEVICE, dtype=torch.long)\n",
    "        cross_attn_mask[:, :prefix_token_count] = 0\n",
    "    else:\n",
    "        cross_attn_mask = torch.ones(1, total_enc_len, device=DEVICE, dtype=torch.long)\n",
    "\n",
    "    ans_ids = tokenizer(answer_text, return_tensors=\"pt\",\n",
    "                        add_special_tokens=False, truncation=True,\n",
    "                        max_length=256).input_ids.to(DEVICE)\n",
    "\n",
    "    if ans_ids.shape[1] == 0:\n",
    "        del encoder_outputs\n",
    "        return 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            encoder_outputs=encoder_outputs,\n",
    "            attention_mask=cross_attn_mask,\n",
    "            labels=ans_ids,\n",
    "        )\n",
    "\n",
    "    logits = outputs.logits\n",
    "    log_probs = F.log_softmax(logits, dim=-1)\n",
    "    token_log_probs = log_probs[0].gather(1, ans_ids[0].unsqueeze(1)).squeeze(1)\n",
    "    mean_nll = -token_log_probs.mean().item()\n",
    "\n",
    "    del encoder_outputs, outputs, logits, log_probs\n",
    "    return mean_nll\n",
    "\n",
    "\n",
    "def score_nll_query_prefix(encoder_text, query_text, answer_text,\n",
    "                           prefix_token_count=0, truncate=False):\n",
    "    # Query as decoder prefix — production-realistic.\n",
    "    enc_ids = tokenizer(encoder_text, return_tensors=\"pt\",\n",
    "                        add_special_tokens=True, truncation=True,\n",
    "                        max_length=2048).input_ids.to(DEVICE)\n",
    "    total_enc_len = enc_ids.shape[1]\n",
    "    enc_mask = torch.ones(1, total_enc_len, device=DEVICE, dtype=torch.long)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        encoder_outputs = model.get_encoder()(\n",
    "            input_ids=enc_ids, attention_mask=enc_mask\n",
    "        )\n",
    "\n",
    "    if truncate and prefix_token_count > 0:\n",
    "        cross_attn_mask = torch.ones(1, total_enc_len, device=DEVICE, dtype=torch.long)\n",
    "        cross_attn_mask[:, :prefix_token_count] = 0\n",
    "    else:\n",
    "        cross_attn_mask = torch.ones(1, total_enc_len, device=DEVICE, dtype=torch.long)\n",
    "\n",
    "    query_ids = tokenizer(query_text, add_special_tokens=False, truncation=True,\n",
    "                          max_length=512).input_ids\n",
    "    answer_ids = tokenizer(answer_text, add_special_tokens=False, truncation=True,\n",
    "                           max_length=256).input_ids\n",
    "\n",
    "    if len(answer_ids) == 0:\n",
    "        del encoder_outputs\n",
    "        return 0.0\n",
    "\n",
    "    dec_ids = [BOS_ID] + query_ids + answer_ids\n",
    "    dec_tensor = torch.tensor([dec_ids], dtype=torch.long, device=DEVICE)\n",
    "\n",
    "    n_query = len(query_ids)\n",
    "    n_answer = len(answer_ids)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            encoder_outputs=encoder_outputs,\n",
    "            attention_mask=cross_attn_mask,\n",
    "            decoder_input_ids=dec_tensor,\n",
    "        )\n",
    "\n",
    "    logits = outputs.logits\n",
    "    answer_logits = logits[0, n_query:n_query + n_answer, :]\n",
    "\n",
    "    answer_targets = torch.tensor(answer_ids, dtype=torch.long, device=DEVICE)\n",
    "    log_probs = F.log_softmax(answer_logits, dim=-1)\n",
    "    token_log_probs = log_probs.gather(1, answer_targets.unsqueeze(1)).squeeze(1)\n",
    "    mean_nll = -token_log_probs.mean().item()\n",
    "\n",
    "    del encoder_outputs, outputs, logits, log_probs\n",
    "    return mean_nll\n",
    "\n",
    "\n",
    "# === Surrogate generation ===\n",
    "STOP_WORDS = {\n",
    "    'a', 'an', 'the', 'is', 'are', 'was', 'were', 'be', 'been', 'being',\n",
    "    'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could',\n",
    "    'should', 'may', 'might', 'can', 'shall', 'to', 'of', 'in', 'for',\n",
    "    'on', 'with', 'at', 'by', 'from', 'as', 'into', 'through', 'during',\n",
    "    'before', 'after', 'above', 'below', 'between', 'and', 'but', 'or',\n",
    "    'not', 'no', 'if', 'then', 'than', 'so', 'up', 'out', 'about',\n",
    "    'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those',\n",
    "    'it', 'its', 'i', 'me', 'my', 'we', 'our', 'you', 'your', 'he',\n",
    "    'him', 'his', 'she', 'her', 'they', 'them', 'their', 'how', 'when',\n",
    "    'where', 'why', 'much', 'many', 'some', 'any', 'all', 'each',\n",
    "    'does', 'also', 'just', 'more', 'most', 'very', 'too', 'only',\n",
    "}\n",
    "\n",
    "def extract_keywords(text):\n",
    "    words = re.sub(r'[^\\w\\s]', '', text.lower()).split()\n",
    "    return [w for w in words if w not in STOP_WORDS and len(w) > 2]\n",
    "\n",
    "def make_surrogate_from_doc(passage):\n",
    "    content_words = extract_keywords(passage)\n",
    "    if not content_words:\n",
    "        return \"information\"\n",
    "    counts = Counter(content_words)\n",
    "    return \" \".join(w for w, _ in counts.most_common(5))\n",
    "\n",
    "print(\"Scoring functions defined.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a395d365",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T20:12:08.140273Z",
     "iopub.status.busy": "2026-02-19T20:12:08.139777Z",
     "iopub.status.idle": "2026-02-19T20:12:16.596050Z",
     "shell.execute_reply": "2026-02-19T20:12:16.595295Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading neural-bridge/rag-dataset-12000...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 9600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidates (q>=15w, a>=5w): 3384\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample statistics (N=500):\n",
      "  Query:    mean=17.8w, median=17w\n",
      "  Document: mean=604w, median=591w\n",
      "  Doc toks: mean=808, median=804, max=1667\n",
      "  Answer:   mean=42.9w, median=32w\n",
      "\n",
      "Comparison with MS MARCO (Exp 01):\n",
      "  MS MARCO: query=6.0w, doc=~60w (~98 tok), answer=~20w\n",
      "  This:     query=17.8w, doc=604w (~808 tok), answer=43w\n",
      "\n",
      "First sample:\n",
      "  Query:   What are some of the conditions that require a reduction in lumber or connector plate design values ...\n",
      "  Answer:  Conditions that require a reduction in lumber or connector plate design values according to ANSI/TPI...\n",
      "  Doc:     Designing for Damp Conditions\n",
      "Designing for Damp Conditions\n",
      "a reduction in lumber or plate design pr...\n",
      "  Surr:    wood moisture lumber percent design\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Load neural-bridge/rag-dataset-12000\n",
    "from datasets import load_dataset\n",
    "\n",
    "print(\"Loading neural-bridge/rag-dataset-12000...\")\n",
    "ds = load_dataset(\"neural-bridge/rag-dataset-12000\", split=\"train\")\n",
    "print(f\"Total samples: {len(ds)}\")\n",
    "\n",
    "# Filter to long queries with real answers (same filter as v3 Exp 3D)\n",
    "all_candidates = []\n",
    "for row in ds:\n",
    "    q = row.get(\"question\", \"\")\n",
    "    doc = row.get(\"context\", \"\")\n",
    "    answer = row.get(\"answer\", \"\")\n",
    "    if not q or not doc or not answer:\n",
    "        continue\n",
    "    q_words = len(q.split())\n",
    "    a_words = len(answer.split())\n",
    "    if q_words >= 15 and a_words >= 5:\n",
    "        all_candidates.append({\n",
    "            \"query\": q,\n",
    "            \"passage\": doc,\n",
    "            \"answer\": answer,\n",
    "            \"query_words\": q_words,\n",
    "            \"doc_words\": len(doc.split()),\n",
    "            \"answer_words\": a_words,\n",
    "        })\n",
    "\n",
    "print(f\"Candidates (q>=15w, a>=5w): {len(all_candidates)}\")\n",
    "\n",
    "np.random.seed(SEED)\n",
    "indices = np.random.permutation(len(all_candidates))\n",
    "samples = [all_candidates[i] for i in indices[:N_SAMPLES]]\n",
    "del ds, all_candidates\n",
    "gc.collect()\n",
    "\n",
    "# Generate surrogates\n",
    "for i, s in enumerate(samples):\n",
    "    s['surr_doc'] = make_surrogate_from_doc(s['passage'])\n",
    "\n",
    "    # Random prefix: words from unrelated document, matched to query word count\n",
    "    other_idx = (i + N_SAMPLES // 2) % len(samples)\n",
    "    other_words = samples[other_idx]['passage'].split()\n",
    "    query_word_count = len(s['query'].split())\n",
    "    s['random_prefix'] = \" \".join(other_words[:query_word_count])\n",
    "\n",
    "    # Count prefix tokens\n",
    "    s['n_prefix_oracle'] = count_prefix_tokens(s['query'], s['passage'])\n",
    "    s['n_prefix_doc'] = count_prefix_tokens(s['surr_doc'], s['passage'])\n",
    "    s['n_prefix_random'] = count_prefix_tokens(s['random_prefix'], s['passage'])\n",
    "\n",
    "# Dataset statistics\n",
    "q_lens = np.array([s['query_words'] for s in samples])\n",
    "d_lens = np.array([s['doc_words'] for s in samples])\n",
    "a_lens = np.array([s['answer_words'] for s in samples])\n",
    "doc_tok_counts = [len(tokenizer(s['passage'], add_special_tokens=True).input_ids)\n",
    "                  for s in samples]\n",
    "\n",
    "print(f\"\\nSample statistics (N={N_SAMPLES}):\")\n",
    "print(f\"  Query:    mean={q_lens.mean():.1f}w, median={np.median(q_lens):.0f}w\")\n",
    "print(f\"  Document: mean={d_lens.mean():.0f}w, median={np.median(d_lens):.0f}w\")\n",
    "print(f\"  Doc toks: mean={np.mean(doc_tok_counts):.0f}, median={np.median(doc_tok_counts):.0f}, \"\n",
    "      f\"max={np.max(doc_tok_counts)}\")\n",
    "print(f\"  Answer:   mean={a_lens.mean():.1f}w, median={np.median(a_lens):.0f}w\")\n",
    "\n",
    "print(f\"\\nComparison with MS MARCO (Exp 01):\")\n",
    "print(f\"  MS MARCO: query=6.0w, doc=~60w (~98 tok), answer=~20w\")\n",
    "print(f\"  This:     query={q_lens.mean():.1f}w, doc={d_lens.mean():.0f}w \"\n",
    "      f\"(~{np.mean(doc_tok_counts):.0f} tok), answer={a_lens.mean():.0f}w\")\n",
    "\n",
    "print(f\"\\nFirst sample:\")\n",
    "print(f\"  Query:   {samples[0]['query'][:100]}...\")\n",
    "print(f\"  Answer:  {samples[0]['answer'][:100]}...\")\n",
    "print(f\"  Doc:     {samples[0]['passage'][:100]}...\")\n",
    "print(f\"  Surr:    {samples[0]['surr_doc']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d3734f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T20:12:16.599536Z",
     "iopub.status.busy": "2026-02-19T20:12:16.598742Z",
     "iopub.status.idle": "2026-02-19T20:12:17.501776Z",
     "shell.execute_reply": "2026-02-19T20:12:17.500975Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "EXAMPLE CONDITIONS (sample 0)\n",
      "======================================================================\n",
      "\n",
      "Query (21w):  What are some of the conditions that require a reduction in lumber or connector plate design values ...\n",
      "Answer (42w): Conditions that require a reduction in lumber or connector plate design values according to ANSI/TPI...\n",
      "Doc (530w):    Designing for Damp Conditions\n",
      "Designing for Damp Conditions\n",
      "a reduction in lumber or plate design pr...\n",
      "\n",
      "  Condition            Enc prefix                 Trunc  Dec query  Pfx tok\n",
      "  ---------------------------------------------------------------------------\n",
      "  bare                 (none)                        no        yes        0\n",
      "  oracle_trunc         real query                   yes        yes       32\n",
      "  surr_doc_trunc       wood moisture lumber         yes        yes        6\n",
      "  random_trunc         (unrelated)                  yes        yes       27\n",
      "  bare_nq              (none)                        no         no        0\n",
      "  oracle_trunc_nq      real query                   yes         no       32\n",
      "\n",
      "Sanity check...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  bare:          0.5312\n",
      "  oracle_trunc:  0.4727\n",
      "  delta:         +0.0586\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Show example conditions\n",
    "print(\"=\" * 70)\n",
    "print(\"EXAMPLE CONDITIONS (sample 0)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "ex = samples[0]\n",
    "print(f\"\\nQuery ({ex['query_words']}w):  {ex['query'][:100]}...\")\n",
    "print(f\"Answer ({ex['answer_words']}w): {ex['answer'][:100]}...\")\n",
    "print(f\"Doc ({ex['doc_words']}w):    {ex['passage'][:100]}...\")\n",
    "\n",
    "print(f\"\\n  {'Condition':<20} {'Enc prefix':<25} {'Trunc':>6} {'Dec query':>10} {'Pfx tok':>8}\")\n",
    "print(f\"  {'-'*75}\")\n",
    "for name, prefix, trunc, has_q, n_pfx in [\n",
    "    ('bare',             '(none)',                'no',  'yes', 0),\n",
    "    ('oracle_trunc',     'real query',            'yes', 'yes', ex['n_prefix_oracle']),\n",
    "    ('surr_doc_trunc',   ex['surr_doc'][:20],     'yes', 'yes', ex['n_prefix_doc']),\n",
    "    ('random_trunc',     '(unrelated)',           'yes', 'yes', ex['n_prefix_random']),\n",
    "    ('bare_nq',          '(none)',                'no',  'no',  0),\n",
    "    ('oracle_trunc_nq',  'real query',            'yes', 'no',  ex['n_prefix_oracle']),\n",
    "]:\n",
    "    print(f\"  {name:<20} {prefix:<25} {trunc:>6} {has_q:>10} {n_pfx:>8}\")\n",
    "\n",
    "# Sanity check\n",
    "print(f\"\\nSanity check...\")\n",
    "nll_bare = score_nll_query_prefix(ex['passage'], ex['query'], ex['answer'])\n",
    "nll_oracle = score_nll_query_prefix(\n",
    "    ex['query'] + \"\\n\" + ex['passage'], ex['query'], ex['answer'],\n",
    "    prefix_token_count=ex['n_prefix_oracle'], truncate=True)\n",
    "print(f\"  bare:          {nll_bare:.4f}\")\n",
    "print(f\"  oracle_trunc:  {nll_oracle:.4f}\")\n",
    "print(f\"  delta:         {nll_bare - nll_oracle:+.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34a40b63",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T20:12:17.505121Z",
     "iopub.status.busy": "2026-02-19T20:12:17.504851Z",
     "iopub.status.idle": "2026-02-19T20:23:41.131176Z",
     "shell.execute_reply": "2026-02-19T20:23:41.130496Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "SCORING ALL CONDITIONS\n",
      "======================================================================\n",
      "Starting fresh: 6 conditions x 500 samples\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e163e33e3e144b8b30693c2e46662c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Scoring:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 20/500 | 0.5m | ETA 11.1m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 40/500 | 0.9m | ETA 10.5m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 60/500 | 1.4m | ETA 10.1m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 80/500 | 1.8m | ETA 9.6m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 100/500 | 2.3m | ETA 9.1m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 120/500 | 2.7m | ETA 8.7m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 140/500 | 3.2m | ETA 8.2m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 160/500 | 3.7m | ETA 7.8m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 180/500 | 4.1m | ETA 7.3m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 200/500 | 4.6m | ETA 6.8m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 220/500 | 5.0m | ETA 6.4m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 240/500 | 5.5m | ETA 5.9m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 260/500 | 5.9m | ETA 5.5m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 280/500 | 6.4m | ETA 5.0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 300/500 | 6.8m | ETA 4.6m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 320/500 | 7.3m | ETA 4.1m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 340/500 | 7.7m | ETA 3.6m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 360/500 | 8.2m | ETA 3.2m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 380/500 | 8.7m | ETA 2.7m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 400/500 | 9.1m | ETA 2.3m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 420/500 | 9.6m | ETA 1.8m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 440/500 | 10.0m | ETA 1.4m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 460/500 | 10.5m | ETA 0.9m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 480/500 | 10.9m | ETA 0.5m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 500/500 | 11.4m | ETA 0.0m\n",
      "\n",
      "Scoring complete: 500 samples, 6 conditions in 11.4 min\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Scoring loop — 6 conditions x 500 samples\n",
    "print(\"=\" * 70)\n",
    "print(\"SCORING ALL CONDITIONS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "COND_NAMES = [\n",
    "    'bare', 'oracle_trunc', 'surr_doc_trunc', 'random_trunc',\n",
    "    'bare_nq', 'oracle_trunc_nq',\n",
    "]\n",
    "\n",
    "results = []\n",
    "start_idx = 0\n",
    "\n",
    "if CHECKPOINT_PATH.exists():\n",
    "    ckpt = json.loads(CHECKPOINT_PATH.read_text())\n",
    "    if ckpt.get('n_total') == N_SAMPLES and len(ckpt.get('results', [])) > 0:\n",
    "        saved_queries = [r['query'][:50] for r in ckpt['results']]\n",
    "        current_queries = [s['query'][:50] for s in samples[:len(saved_queries)]]\n",
    "        if saved_queries == current_queries:\n",
    "            results = ckpt['results']\n",
    "            start_idx = len(results)\n",
    "            print(f\"Resuming from checkpoint: {start_idx}/{N_SAMPLES}\")\n",
    "\n",
    "if start_idx == 0:\n",
    "    print(f\"Starting fresh: {len(COND_NAMES)} conditions x {N_SAMPLES} samples\")\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "for i in tqdm(range(start_idx, N_SAMPLES), initial=start_idx, total=N_SAMPLES,\n",
    "              desc=\"Scoring\"):\n",
    "    s = samples[i]\n",
    "    query = s['query']\n",
    "    passage = s['passage']\n",
    "    answer = s['answer']\n",
    "\n",
    "    result = {\n",
    "        'query': query,\n",
    "        'answer': answer,\n",
    "        'query_words': s['query_words'],\n",
    "        'doc_words': s['doc_words'],\n",
    "        'answer_words': s['answer_words'],\n",
    "    }\n",
    "\n",
    "    # --- With query in decoder ---\n",
    "    result['nll_bare'] = score_nll_query_prefix(passage, query, answer)\n",
    "\n",
    "    result['nll_oracle_trunc'] = score_nll_query_prefix(\n",
    "        query + \"\\n\" + passage, query, answer,\n",
    "        prefix_token_count=s['n_prefix_oracle'], truncate=True)\n",
    "\n",
    "    result['nll_surr_doc_trunc'] = score_nll_query_prefix(\n",
    "        s['surr_doc'] + \"\\n\" + passage, query, answer,\n",
    "        prefix_token_count=s['n_prefix_doc'], truncate=True)\n",
    "\n",
    "    result['nll_random_trunc'] = score_nll_query_prefix(\n",
    "        s['random_prefix'] + \"\\n\" + passage, query, answer,\n",
    "        prefix_token_count=s['n_prefix_random'], truncate=True)\n",
    "\n",
    "    # --- Without query in decoder (v3 replication) ---\n",
    "    result['nll_bare_nq'] = score_nll(passage, answer)\n",
    "\n",
    "    result['nll_oracle_trunc_nq'] = score_nll(\n",
    "        query + \"\\n\" + passage, answer,\n",
    "        prefix_token_count=s['n_prefix_oracle'], truncate=True)\n",
    "\n",
    "    results.append(result)\n",
    "\n",
    "    if (i + 1) % 20 == 0 or i == N_SAMPLES - 1:\n",
    "        ckpt = {\n",
    "            'n_total': N_SAMPLES,\n",
    "            'results': results,\n",
    "            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        }\n",
    "        CHECKPOINT_PATH.write_text(json.dumps(ckpt))\n",
    "        elapsed = time.time() - t0\n",
    "        done = i - start_idx + 1\n",
    "        eta = (N_SAMPLES - i - 1) * elapsed / done if done > 0 else 0\n",
    "        tqdm.write(f\"  Checkpoint {i+1}/{N_SAMPLES} | {elapsed/60:.1f}m | ETA {eta/60:.1f}m\")\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "elapsed = time.time() - t0\n",
    "print(f\"\\nScoring complete: {len(results)} samples, \"\n",
    "      f\"{len(COND_NAMES)} conditions in {elapsed/60:.1f} min\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "009773f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T20:23:41.134602Z",
     "iopub.status.busy": "2026-02-19T20:23:41.133990Z",
     "iopub.status.idle": "2026-02-19T20:23:41.151019Z",
     "shell.execute_reply": "2026-02-19T20:23:41.150307Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "RESULTS (N=500)\n",
      "======================================================================\n",
      "\n",
      "--- With query in decoder (production-realistic) ---\n",
      "  Condition                 NLL    vs bare        d     Win%            p   sig\n",
      "  --------------------------------------------------------------------------\n",
      "  bare                   0.6755         --       --       --           --    --\n",
      "  oracle_trunc           0.6539    +0.0216   +0.306    64.6%     2.14e-11   ***\n",
      "  surr_doc_trunc         0.6515    +0.0239   +0.502    71.4%     2.94e-26   ***\n",
      "  random_trunc           0.6440    +0.0314   +0.624    75.6%     1.46e-37   ***\n",
      "\n",
      "--- Without query in decoder (v3 replication) ---\n",
      "  bare_nq                1.3135\n",
      "  oracle_trunc_nq        1.2239    +0.0896   +0.592    85.0%     1.88e-34   ***\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Results table\n",
    "print(\"=\" * 70)\n",
    "print(f\"RESULTS (N={len(results)})\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "bare = np.array([r['nll_bare'] for r in results])\n",
    "oracle_trunc = np.array([r['nll_oracle_trunc'] for r in results])\n",
    "surr_doc = np.array([r['nll_surr_doc_trunc'] for r in results])\n",
    "random_trunc = np.array([r['nll_random_trunc'] for r in results])\n",
    "bare_nq = np.array([r['nll_bare_nq'] for r in results])\n",
    "oracle_nq = np.array([r['nll_oracle_trunc_nq'] for r in results])\n",
    "\n",
    "# Bonferroni: 3 query-prefix + 1 nq = 4\n",
    "N_BONF = 4\n",
    "\n",
    "print(f\"\\n--- With query in decoder (production-realistic) ---\")\n",
    "print(f\"  {'Condition':<20} {'NLL':>8} {'vs bare':>10} {'d':>8} {'Win%':>8} {'p':>12} {'sig':>5}\")\n",
    "print(f\"  {'-'*74}\")\n",
    "\n",
    "analysis = {}\n",
    "for name, nlls in [('bare', bare), ('oracle_trunc', oracle_trunc),\n",
    "                    ('surr_doc_trunc', surr_doc), ('random_trunc', random_trunc)]:\n",
    "    mean_nll = nlls.mean()\n",
    "    if name == 'bare':\n",
    "        print(f\"  {name:<20} {mean_nll:>8.4f} {'--':>10} {'--':>8} {'--':>8} {'--':>12} {'--':>5}\")\n",
    "        analysis[name] = {'mean_nll': float(mean_nll)}\n",
    "    else:\n",
    "        diff = bare - nlls\n",
    "        d = cohens_d(diff)\n",
    "        win_pct = 100 * np.mean(diff > 0)\n",
    "        _, p_val = stats.ttest_1samp(diff, 0)\n",
    "        sig = '***' if p_val < 0.001/N_BONF else '**' if p_val < 0.01/N_BONF else '*' if p_val < 0.05/N_BONF else 'ns'\n",
    "        print(f\"  {name:<20} {mean_nll:>8.4f} {diff.mean():>+10.4f} {d:>+8.3f} {win_pct:>7.1f}% {p_val:>12.2e} {sig:>5}\")\n",
    "        analysis[name] = {\n",
    "            'mean_nll': float(mean_nll), 'delta': float(diff.mean()),\n",
    "            'd': float(d), 'win_pct': float(win_pct), 'p': float(p_val),\n",
    "        }\n",
    "\n",
    "# No-query conditions\n",
    "diff_nq = bare_nq - oracle_nq\n",
    "d_nq = cohens_d(diff_nq)\n",
    "win_nq = 100 * np.mean(diff_nq > 0)\n",
    "_, p_nq = stats.ttest_1samp(diff_nq, 0)\n",
    "sig_nq = '***' if p_nq < 0.001/N_BONF else '**' if p_nq < 0.01/N_BONF else '*' if p_nq < 0.05/N_BONF else 'ns'\n",
    "\n",
    "print(f\"\\n--- Without query in decoder (v3 replication) ---\")\n",
    "print(f\"  {'bare_nq':<20} {bare_nq.mean():>8.4f}\")\n",
    "print(f\"  {'oracle_trunc_nq':<20} {oracle_nq.mean():>8.4f} {diff_nq.mean():>+10.4f} {d_nq:>+8.3f} {win_nq:>7.1f}% {p_nq:>12.2e} {sig_nq:>5}\")\n",
    "\n",
    "analysis['bare_nq'] = {'mean_nll': float(bare_nq.mean())}\n",
    "analysis['oracle_trunc_nq'] = {\n",
    "    'mean_nll': float(oracle_nq.mean()), 'delta': float(diff_nq.mean()),\n",
    "    'd': float(d_nq), 'win_pct': float(win_nq), 'p': float(p_nq),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "582518eb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T20:23:41.154093Z",
     "iopub.status.busy": "2026-02-19T20:23:41.153547Z",
     "iopub.status.idle": "2026-02-19T20:23:41.163337Z",
     "shell.execute_reply": "2026-02-19T20:23:41.162701Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "CROSS-DATASET COMPARISON\n",
      "======================================================================\n",
      "\n",
      "  Condition              MS MARCO (Exp01)   Neural-Bridge    Ratio\n",
      "  -----------------------------------------------------------------\n",
      "  oracle_trunc                     +0.228          +0.306     1.3x\n",
      "  surr_doc_trunc                   +0.148          +0.502     3.4x\n",
      "  random_trunc                     +0.080          +0.624     7.8x\n",
      "  oracle_trunc_nq                  +0.376          +0.592     1.6x\n",
      "\n",
      "--- Structural Fraction (random/oracle) ---\n",
      "  MS MARCO Exp 01: 35%\n",
      "  Neural-Bridge:   204%\n",
      "\n",
      "--- Surrogate Efficiency (surr_doc/oracle) ---\n",
      "  MS MARCO Exp 01: 65%\n",
      "  Neural-Bridge:   164%\n",
      "\n",
      "--- v4/v3 Enrichment Ratio (how much survives with query in decoder) ---\n",
      "  MS MARCO:        61%\n",
      "  Neural-Bridge:   52%\n",
      "\n",
      "--- v3 Exp 3D Comparison (surrogates beat oracle phenomenon) ---\n",
      "  v3 Exp 3D: ALL surrogates beat oracle (150%+ of oracle d)\n",
      "  v3 explanation: real query creates semantic interference in encoder\n",
      "  v4: surr_doc (+0.502) STILL beats oracle (+0.306)\n",
      "       -> Semantic interference persists even with query in decoder\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Cross-dataset comparison with Exp 01 (MS MARCO)\n",
    "print(\"=\" * 70)\n",
    "print(\"CROSS-DATASET COMPARISON\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "d_oracle = cohens_d(bare - oracle_trunc)\n",
    "d_surr = cohens_d(bare - surr_doc)\n",
    "d_random = cohens_d(bare - random_trunc)\n",
    "d_oracle_nq = cohens_d(bare_nq - oracle_nq)\n",
    "\n",
    "# Exp 01 reference values\n",
    "exp01 = {\n",
    "    'oracle_trunc': 0.228, 'surr_doc_trunc': 0.148,\n",
    "    'random_trunc': 0.080, 'oracle_trunc_nq': 0.376,\n",
    "}\n",
    "\n",
    "print(f\"\\n  {'Condition':<20} {'MS MARCO (Exp01)':>18} {'Neural-Bridge':>15} {'Ratio':>8}\")\n",
    "print(f\"  {'-'*65}\")\n",
    "for name, exp01_d in exp01.items():\n",
    "    if name == 'oracle_trunc':\n",
    "        this_d = d_oracle\n",
    "    elif name == 'surr_doc_trunc':\n",
    "        this_d = d_surr\n",
    "    elif name == 'random_trunc':\n",
    "        this_d = d_random\n",
    "    else:\n",
    "        this_d = d_oracle_nq\n",
    "    ratio = this_d / exp01_d if exp01_d != 0 else 0\n",
    "    print(f\"  {name:<20} {exp01_d:>+18.3f} {this_d:>+15.3f} {ratio:>7.1f}x\")\n",
    "\n",
    "# Structural fraction comparison\n",
    "struct_marco = 0.080 / 0.228 * 100 if 0.228 > 0 else 0\n",
    "struct_nb = d_random / d_oracle * 100 if d_oracle > 0 else 0\n",
    "\n",
    "print(f\"\\n--- Structural Fraction (random/oracle) ---\")\n",
    "print(f\"  MS MARCO Exp 01: {struct_marco:.0f}%\")\n",
    "print(f\"  Neural-Bridge:   {struct_nb:.0f}%\")\n",
    "\n",
    "# Surrogate efficiency\n",
    "surr_pct_marco = 0.148 / 0.228 * 100 if 0.228 > 0 else 0\n",
    "surr_pct_nb = d_surr / d_oracle * 100 if d_oracle > 0 else 0\n",
    "\n",
    "print(f\"\\n--- Surrogate Efficiency (surr_doc/oracle) ---\")\n",
    "print(f\"  MS MARCO Exp 01: {surr_pct_marco:.0f}%\")\n",
    "print(f\"  Neural-Bridge:   {surr_pct_nb:.0f}%\")\n",
    "\n",
    "# v4/v3 ratio\n",
    "ratio_marco = 0.228 / 0.376 * 100\n",
    "ratio_nb = d_oracle / d_oracle_nq * 100 if d_oracle_nq > 0 else 0\n",
    "\n",
    "print(f\"\\n--- v4/v3 Enrichment Ratio (how much survives with query in decoder) ---\")\n",
    "print(f\"  MS MARCO:        {ratio_marco:.0f}%\")\n",
    "print(f\"  Neural-Bridge:   {ratio_nb:.0f}%\")\n",
    "\n",
    "# v3 Exp 3D comparison (surrogates beat oracle)\n",
    "print(f\"\\n--- v3 Exp 3D Comparison (surrogates beat oracle phenomenon) ---\")\n",
    "print(f\"  v3 Exp 3D: ALL surrogates beat oracle (150%+ of oracle d)\")\n",
    "print(f\"  v3 explanation: real query creates semantic interference in encoder\")\n",
    "if d_surr > d_oracle:\n",
    "    print(f\"  v4: surr_doc ({d_surr:+.3f}) STILL beats oracle ({d_oracle:+.3f})\")\n",
    "    print(f\"       -> Semantic interference persists even with query in decoder\")\n",
    "else:\n",
    "    print(f\"  v4: oracle ({d_oracle:+.3f}) beats surr_doc ({d_surr:+.3f})\")\n",
    "    print(f\"       -> Decoder query resolves the interference\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20a83c14",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T20:23:41.166222Z",
     "iopub.status.busy": "2026-02-19T20:23:41.165766Z",
     "iopub.status.idle": "2026-02-19T20:23:41.176291Z",
     "shell.execute_reply": "2026-02-19T20:23:41.175691Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "KEY COMPARISON: Is enrichment redundant when decoder has the query?\n",
      "======================================================================\n",
      "\n",
      "  Enrichment with query in decoder:    d=+0.306\n",
      "  Enrichment without query (v3 repl):  d=+0.592\n",
      "  Ratio: 52%\n",
      "  Per-sample correlation: r=0.093 (p=3.85e-02)\n",
      "\n",
      "--- Hardness gradient (with query in decoder) ---\n",
      "  Quintile        N     bare   oracle    delta        d\n",
      "  --------------------------------------------------\n",
      "  Q1 easy       100   0.3136   0.3143  -0.0007   -0.015\n",
      "  Q2             98   0.4775   0.4684  +0.0092   +0.181\n",
      "  Q3            102   0.6276   0.6097  +0.0179   +0.295\n",
      "  Q4            100   0.7889   0.7638  +0.0250   +0.340\n",
      "  Q5 hard       100   1.1668   1.1104  +0.0563   +0.578\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Key comparison — enrichment with query vs without query\n",
    "print(\"=\" * 70)\n",
    "print(\"KEY COMPARISON: Is enrichment redundant when decoder has the query?\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "enrichment_with_q = bare - oracle_trunc\n",
    "enrichment_no_q = bare_nq - oracle_nq\n",
    "d_with_q = cohens_d(enrichment_with_q)\n",
    "d_no_q = cohens_d(enrichment_no_q)\n",
    "\n",
    "ratio = d_with_q / d_no_q * 100 if d_no_q > 0 else 0\n",
    "\n",
    "print(f\"\\n  Enrichment with query in decoder:    d={d_with_q:+.3f}\")\n",
    "print(f\"  Enrichment without query (v3 repl):  d={d_no_q:+.3f}\")\n",
    "print(f\"  Ratio: {ratio:.0f}%\")\n",
    "\n",
    "# Per-sample correlation\n",
    "r_corr, p_corr = stats.pearsonr(enrichment_with_q, enrichment_no_q)\n",
    "print(f\"  Per-sample correlation: r={r_corr:.3f} (p={p_corr:.2e})\")\n",
    "\n",
    "# Hardness gradient\n",
    "print(f\"\\n--- Hardness gradient (with query in decoder) ---\")\n",
    "quintile_bounds = np.percentile(bare, [20, 40, 60, 80])\n",
    "quintiles = np.digitize(bare, quintile_bounds)\n",
    "\n",
    "print(f\"  {'Quintile':<12} {'N':>4} {'bare':>8} {'oracle':>8} {'delta':>8} {'d':>8}\")\n",
    "print(f\"  {'-'*50}\")\n",
    "for q in range(5):\n",
    "    mask = quintiles == q\n",
    "    n_q = mask.sum()\n",
    "    if n_q < 5:\n",
    "        continue\n",
    "    qlabel = ['Q1 easy', 'Q2', 'Q3', 'Q4', 'Q5 hard'][q]\n",
    "    b = bare[mask].mean()\n",
    "    o = oracle_trunc[mask].mean()\n",
    "    delta = (bare[mask] - oracle_trunc[mask]).mean()\n",
    "    d = cohens_d(bare[mask] - oracle_trunc[mask])\n",
    "    print(f\"  {qlabel:<12} {n_q:>4} {b:>8.4f} {o:>8.4f} {delta:>+8.4f} {d:>+8.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86aeb76b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T20:23:41.179150Z",
     "iopub.status.busy": "2026-02-19T20:23:41.178914Z",
     "iopub.status.idle": "2026-02-19T20:23:41.680567Z",
     "shell.execute_reply": "2026-02-19T20:23:41.679887Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "VERDICT -- Exp 03: Neural-Bridge Cross-Dataset Validation\n",
      "======================================================================\n",
      "\n",
      "Model: google/t5gemma-2-4b-4b\n",
      "Dataset: neural-bridge/rag-dataset-12000\n",
      "N: 500, mean doc: 604 words\n",
      "\n",
      "--- Key results ---\n",
      "  Oracle enrichment (query in decoder):  d=+0.306\n",
      "  Oracle enrichment (no query, v3 repl): d=+0.592\n",
      "  Ratio (v4/v3):                         52%\n",
      "  Structural fraction (random/oracle):   204%\n",
      "  Surrogate doc efficiency:              164% of oracle\n",
      "\n",
      "--- Significance ---\n",
      "  oracle_trunc: p=2.14e-11\n",
      "  surr_doc:     p=2.94e-26\n",
      "  random:       p=1.46e-37\n",
      "\n",
      "--- Cross-dataset consistency ---\n",
      "  MS MARCO (Exp 01): oracle d=+0.228, surr_doc d=+0.148, random d=+0.080\n",
      "  Neural-Bridge:     oracle d=+0.306, surr_doc d=+0.502, random d=+0.624\n",
      "\n",
      "  CONCLUSION: v4 enrichment benefit REPLICATES on naturally long documents.\n",
      "\n",
      "Results saved to ../../../results/exp03/results.json\n",
      "\n",
      "Cleaning up GPU memory...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory: 15.03 GB -> 0.01 GB\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Cell 10: Verdict and save\n",
    "print(\"=\" * 70)\n",
    "print(\"VERDICT -- Exp 03: Neural-Bridge Cross-Dataset Validation\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "d_oracle = cohens_d(bare - oracle_trunc)\n",
    "d_surr = cohens_d(bare - surr_doc)\n",
    "d_random = cohens_d(bare - random_trunc)\n",
    "d_oracle_nq = cohens_d(bare_nq - oracle_nq)\n",
    "ratio = d_oracle / d_oracle_nq * 100 if d_oracle_nq > 0 else 0\n",
    "struct_frac = d_random / d_oracle * 100 if d_oracle > 0 else 0\n",
    "\n",
    "print(f\"\\nModel: {MODEL_NAME}\")\n",
    "print(f\"Dataset: neural-bridge/rag-dataset-12000\")\n",
    "print(f\"N: {len(results)}, mean doc: {np.mean([r['doc_words'] for r in results]):.0f} words\")\n",
    "\n",
    "print(f\"\\n--- Key results ---\")\n",
    "print(f\"  Oracle enrichment (query in decoder):  d={d_oracle:+.3f}\")\n",
    "print(f\"  Oracle enrichment (no query, v3 repl): d={d_oracle_nq:+.3f}\")\n",
    "print(f\"  Ratio (v4/v3):                         {ratio:.0f}%\")\n",
    "print(f\"  Structural fraction (random/oracle):   {struct_frac:.0f}%\")\n",
    "print(f\"  Surrogate doc efficiency:              {d_surr/d_oracle*100:.0f}% of oracle\" if d_oracle > 0 else \"\")\n",
    "\n",
    "_, p_oracle = stats.ttest_1samp(bare - oracle_trunc, 0)\n",
    "_, p_surr = stats.ttest_1samp(bare - surr_doc, 0)\n",
    "_, p_rand = stats.ttest_1samp(bare - random_trunc, 0)\n",
    "\n",
    "print(f\"\\n--- Significance ---\")\n",
    "print(f\"  oracle_trunc: p={p_oracle:.2e}\")\n",
    "print(f\"  surr_doc:     p={p_surr:.2e}\")\n",
    "print(f\"  random:       p={p_rand:.2e}\")\n",
    "\n",
    "print(f\"\\n--- Cross-dataset consistency ---\")\n",
    "print(f\"  MS MARCO (Exp 01): oracle d=+0.228, surr_doc d=+0.148, random d=+0.080\")\n",
    "print(f\"  Neural-Bridge:     oracle d={d_oracle:+.3f}, surr_doc d={d_surr:+.3f}, random d={d_random:+.3f}\")\n",
    "\n",
    "if d_oracle > 0.1:\n",
    "    print(f\"\\n  CONCLUSION: v4 enrichment benefit REPLICATES on naturally long documents.\")\n",
    "else:\n",
    "    print(f\"\\n  CONCLUSION: v4 enrichment benefit does NOT replicate on this dataset.\")\n",
    "\n",
    "# Save\n",
    "final_results = {\n",
    "    'experiment': 'v4_exp03_neural_bridge',\n",
    "    'model': MODEL_NAME,\n",
    "    'dataset': 'neural-bridge/rag-dataset-12000',\n",
    "    'n_samples': len(results),\n",
    "    'seed': SEED,\n",
    "    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'dataset_stats': {\n",
    "        'mean_query_words': float(np.mean([r['query_words'] for r in results])),\n",
    "        'mean_doc_words': float(np.mean([r['doc_words'] for r in results])),\n",
    "        'mean_answer_words': float(np.mean([r['answer_words'] for r in results])),\n",
    "    },\n",
    "    'key_result': {\n",
    "        'enrichment_with_query_d': float(d_oracle),\n",
    "        'enrichment_no_query_d': float(d_oracle_nq),\n",
    "        'ratio_pct': float(ratio),\n",
    "        'structural_fraction_pct': float(struct_frac),\n",
    "    },\n",
    "    'conditions': analysis,\n",
    "}\n",
    "\n",
    "with open(RESULTS_DIR / 'results.json', 'w') as f:\n",
    "    json.dump(final_results, f, indent=2)\n",
    "print(f\"\\nResults saved to {RESULTS_DIR / 'results.json'}\")\n",
    "\n",
    "# Cleanup\n",
    "print(f\"\\nCleaning up GPU memory...\")\n",
    "mem_before = torch.cuda.memory_allocated() / 1e9\n",
    "del model, processor, tokenizer\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "mem_after = torch.cuda.memory_allocated() / 1e9\n",
    "print(f\"GPU memory: {mem_before:.2f} GB -> {mem_after:.2f} GB\")\n",
    "print(\"Done!\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "05e6a8e3b5d246bc8d31e91f8738f657": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_2329a522e67547219a836de8dff04ed9",
       "max": 1327.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_938be32b43d6498dbc8fd9981ffe040f",
       "tabbable": null,
       "tooltip": null,
       "value": 1327.0
      }
     },
     "1ee8c6798ed24251bcd86d99f2bc595e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_926f7999ba4549bba52efca028c16500",
       "max": 500.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_c227985870e64282b88da63edd3d076b",
       "tabbable": null,
       "tooltip": null,
       "value": 500.0
      }
     },
     "1ef045c3f26b4c3da2ed7bd39d858dbd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "20546980c5b241ff82947d00ce7be1ea": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "2329a522e67547219a836de8dff04ed9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2989091736b543759e485fe5448e6dd2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "35af777da9514c2a98962da34357705c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "39e5001d9fe6408996db4dd6a9a05bb9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_5b5e0904d4fa401c86cc3827ea018d7e",
       "placeholder": "​",
       "style": "IPY_MODEL_20546980c5b241ff82947d00ce7be1ea",
       "tabbable": null,
       "tooltip": null,
       "value": " 500/500 [11:23&lt;00:00,  1.38s/it]"
      }
     },
     "3e163e33e3e144b8b30693c2e46662c0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_af90f04ca22345669c645ef02cdf905e",
        "IPY_MODEL_1ee8c6798ed24251bcd86d99f2bc595e",
        "IPY_MODEL_39e5001d9fe6408996db4dd6a9a05bb9"
       ],
       "layout": "IPY_MODEL_2989091736b543759e485fe5448e6dd2",
       "tabbable": null,
       "tooltip": null
      }
     },
     "5b5e0904d4fa401c86cc3827ea018d7e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6a642aed7e4c455dbdc7dd8bc55d6742": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_35af777da9514c2a98962da34357705c",
       "placeholder": "​",
       "style": "IPY_MODEL_d2a0675dc09e493fa86aaa05efa1874a",
       "tabbable": null,
       "tooltip": null,
       "value": "Loading weights: 100%"
      }
     },
     "8387a2279aa34da5893f8c87a3da5ee9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "8a12fb3799d9466d857a01bb164ce34b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "926f7999ba4549bba52efca028c16500": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "938be32b43d6498dbc8fd9981ffe040f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "ac47bb9fa5c9449da3bb411c8177e865": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "af90f04ca22345669c645ef02cdf905e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_8a12fb3799d9466d857a01bb164ce34b",
       "placeholder": "​",
       "style": "IPY_MODEL_1ef045c3f26b4c3da2ed7bd39d858dbd",
       "tabbable": null,
       "tooltip": null,
       "value": "Scoring: 100%"
      }
     },
     "c227985870e64282b88da63edd3d076b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "d2a0675dc09e493fa86aaa05efa1874a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "f14d980175494255991ceafb5abf08aa": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "fc6b18a580ad4345a26d822728b43eb5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_f14d980175494255991ceafb5abf08aa",
       "placeholder": "​",
       "style": "IPY_MODEL_ac47bb9fa5c9449da3bb411c8177e865",
       "tabbable": null,
       "tooltip": null,
       "value": " 1327/1327 [00:04&lt;00:00, 691.72it/s, Materializing param=model.encoder.vision_tower.vision_model.post_layernorm.weight]"
      }
     },
     "fcdbd73721804f42b94c795437406995": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_6a642aed7e4c455dbdc7dd8bc55d6742",
        "IPY_MODEL_05e6a8e3b5d246bc8d31e91f8738f657",
        "IPY_MODEL_fc6b18a580ad4345a26d822728b43eb5"
       ],
       "layout": "IPY_MODEL_8387a2279aa34da5893f8c87a3da5ee9",
       "tabbable": null,
       "tooltip": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
