{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a52a4b4e",
   "metadata": {},
   "source": [
    "# Experiment 03: Cross-Dataset Validation — Neural-Bridge (Long Documents)\n",
    "\n",
    "## Motivation\n",
    "\n",
    "Exp 01 (MS MARCO, ~98 token docs) showed the v4 enrichment benefit survives with query\n",
    "in decoder (d=+0.228). Exp 02 showed this benefit *grows* at longer padded documents\n",
    "(d=+0.43 at 4096 tokens). But Exp 02 used artificial padding with unrelated text.\n",
    "\n",
    "Neural-bridge/rag-dataset-12000 provides a natural test:\n",
    "- Documents are ~600 words (~800-1000 tokens) — genuinely long\n",
    "- Queries are ~18 words (3x MS MARCO)\n",
    "- Different domain and generation process\n",
    "\n",
    "v3 Exp 3D on this dataset (no query in decoder) found:\n",
    "- Structure = 84.3% (matched MS MARCO's 84.7%)\n",
    "- ALL surrogates beat oracle — the real query creates semantic interference\n",
    "- Oracle d was modest because of this interference\n",
    "\n",
    "**Key questions**:\n",
    "1. Does the v4 enrichment benefit hold on naturally long documents?\n",
    "2. Does the v4 mechanism shift (structural collapse, content dominance) replicate?\n",
    "3. Does the \"surrogates beat oracle\" phenomenon from v3 persist when the decoder has the query?\n",
    "\n",
    "## Conditions (6 total)\n",
    "\n",
    "### With query in decoder (production-realistic):\n",
    "\n",
    "| # | Condition | Encoder input | Cross-attn | Purpose |\n",
    "|---|-----------|--------------|------------|---------|\n",
    "| 1 | bare | [document] | all | Baseline |\n",
    "| 2 | oracle_trunc | [query + doc] | doc only | Upper bound |\n",
    "| 3 | surr_doc_trunc | [top-5 kw + doc] | doc only | Production surrogate |\n",
    "| 4 | random_trunc | [random words + doc] | doc only | Structural control |\n",
    "\n",
    "### Without query in decoder (v3 replication):\n",
    "\n",
    "| # | Condition | Encoder input | Cross-attn | Purpose |\n",
    "|---|-----------|--------------|------------|---------|\n",
    "| 5 | bare_nq | [document] | all | v3 baseline |\n",
    "| 6 | oracle_trunc_nq | [query + doc] | doc only | v3 enrichment reference |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42c8346",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Setup and model loading\n",
    "import os\n",
    "os.umask(0o000)\n",
    "\n",
    "import sys, json, time, gc, re\n",
    "import random as pyrandom\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from scipy import stats\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "sys.path.insert(0, \"../../..\")\n",
    "from lib.analysis import cohens_d\n",
    "\n",
    "SEED = 42\n",
    "N_SAMPLES = 500\n",
    "MODEL_NAME = \"google/t5gemma-2-4b-4b\"\n",
    "\n",
    "RESULTS_DIR = Path(\"../../../results/exp03\")\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CHECKPOINT_PATH = RESULTS_DIR / \"checkpoint.json\"\n",
    "\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "pyrandom.seed(SEED)\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv())\n",
    "HF_TOKEN = os.environ.get(\"HF_TOKEN\")\n",
    "\n",
    "from transformers import AutoProcessor, AutoModelForSeq2SeqLM\n",
    "\n",
    "print(f\"Loading {MODEL_NAME}...\")\n",
    "processor = AutoProcessor.from_pretrained(MODEL_NAME, token=HF_TOKEN)\n",
    "tokenizer = processor.tokenizer\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    MODEL_NAME, device_map=\"auto\", torch_dtype=torch.bfloat16, token=HF_TOKEN,\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "DEVICE = next(model.parameters()).device\n",
    "BOS_ID = getattr(model.config, 'decoder_start_token_id', None) or tokenizer.bos_token_id\n",
    "\n",
    "print(f\"Exp 03: Cross-Dataset Validation — Neural-Bridge\")\n",
    "print(f\"N: {N_SAMPLES}, Model: {MODEL_NAME}\")\n",
    "print(f\"DEVICE: {DEVICE}, dtype: {next(model.parameters()).dtype}\")\n",
    "print(f\"GPU memory: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b416ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Scoring helpers\n",
    "\n",
    "def count_prefix_tokens(prefix_text, document_text):\n",
    "    full_text = prefix_text + \"\\n\" + document_text\n",
    "    full_ids = tokenizer(full_text, add_special_tokens=True, truncation=True,\n",
    "                         max_length=2048).input_ids\n",
    "    doc_ids = tokenizer(document_text, add_special_tokens=True, truncation=True,\n",
    "                        max_length=2048).input_ids\n",
    "    return len(full_ids) - len(doc_ids)\n",
    "\n",
    "\n",
    "def score_nll(encoder_text, answer_text, prefix_token_count=0, truncate=False):\n",
    "    # No query in decoder — used for _nq conditions (v3 replication).\n",
    "    enc_ids = tokenizer(encoder_text, return_tensors=\"pt\",\n",
    "                        add_special_tokens=True, truncation=True,\n",
    "                        max_length=2048).input_ids.to(DEVICE)\n",
    "    total_enc_len = enc_ids.shape[1]\n",
    "    enc_mask = torch.ones(1, total_enc_len, device=DEVICE, dtype=torch.long)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        encoder_outputs = model.get_encoder()(\n",
    "            input_ids=enc_ids, attention_mask=enc_mask\n",
    "        )\n",
    "\n",
    "    if truncate and prefix_token_count > 0:\n",
    "        cross_attn_mask = torch.ones(1, total_enc_len, device=DEVICE, dtype=torch.long)\n",
    "        cross_attn_mask[:, :prefix_token_count] = 0\n",
    "    else:\n",
    "        cross_attn_mask = torch.ones(1, total_enc_len, device=DEVICE, dtype=torch.long)\n",
    "\n",
    "    ans_ids = tokenizer(answer_text, return_tensors=\"pt\",\n",
    "                        add_special_tokens=False, truncation=True,\n",
    "                        max_length=256).input_ids.to(DEVICE)\n",
    "\n",
    "    if ans_ids.shape[1] == 0:\n",
    "        del encoder_outputs\n",
    "        return 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            encoder_outputs=encoder_outputs,\n",
    "            attention_mask=cross_attn_mask,\n",
    "            labels=ans_ids,\n",
    "        )\n",
    "\n",
    "    logits = outputs.logits\n",
    "    log_probs = F.log_softmax(logits, dim=-1)\n",
    "    token_log_probs = log_probs[0].gather(1, ans_ids[0].unsqueeze(1)).squeeze(1)\n",
    "    mean_nll = -token_log_probs.mean().item()\n",
    "\n",
    "    del encoder_outputs, outputs, logits, log_probs\n",
    "    return mean_nll\n",
    "\n",
    "\n",
    "def score_nll_query_prefix(encoder_text, query_text, answer_text,\n",
    "                           prefix_token_count=0, truncate=False):\n",
    "    # Query as decoder prefix — production-realistic.\n",
    "    enc_ids = tokenizer(encoder_text, return_tensors=\"pt\",\n",
    "                        add_special_tokens=True, truncation=True,\n",
    "                        max_length=2048).input_ids.to(DEVICE)\n",
    "    total_enc_len = enc_ids.shape[1]\n",
    "    enc_mask = torch.ones(1, total_enc_len, device=DEVICE, dtype=torch.long)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        encoder_outputs = model.get_encoder()(\n",
    "            input_ids=enc_ids, attention_mask=enc_mask\n",
    "        )\n",
    "\n",
    "    if truncate and prefix_token_count > 0:\n",
    "        cross_attn_mask = torch.ones(1, total_enc_len, device=DEVICE, dtype=torch.long)\n",
    "        cross_attn_mask[:, :prefix_token_count] = 0\n",
    "    else:\n",
    "        cross_attn_mask = torch.ones(1, total_enc_len, device=DEVICE, dtype=torch.long)\n",
    "\n",
    "    query_ids = tokenizer(query_text, add_special_tokens=False, truncation=True,\n",
    "                          max_length=512).input_ids\n",
    "    answer_ids = tokenizer(answer_text, add_special_tokens=False, truncation=True,\n",
    "                           max_length=256).input_ids\n",
    "\n",
    "    if len(answer_ids) == 0:\n",
    "        del encoder_outputs\n",
    "        return 0.0\n",
    "\n",
    "    dec_ids = [BOS_ID] + query_ids + answer_ids\n",
    "    dec_tensor = torch.tensor([dec_ids], dtype=torch.long, device=DEVICE)\n",
    "\n",
    "    n_query = len(query_ids)\n",
    "    n_answer = len(answer_ids)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            encoder_outputs=encoder_outputs,\n",
    "            attention_mask=cross_attn_mask,\n",
    "            decoder_input_ids=dec_tensor,\n",
    "        )\n",
    "\n",
    "    logits = outputs.logits\n",
    "    answer_logits = logits[0, n_query:n_query + n_answer, :]\n",
    "\n",
    "    answer_targets = torch.tensor(answer_ids, dtype=torch.long, device=DEVICE)\n",
    "    log_probs = F.log_softmax(answer_logits, dim=-1)\n",
    "    token_log_probs = log_probs.gather(1, answer_targets.unsqueeze(1)).squeeze(1)\n",
    "    mean_nll = -token_log_probs.mean().item()\n",
    "\n",
    "    del encoder_outputs, outputs, logits, log_probs\n",
    "    return mean_nll\n",
    "\n",
    "\n",
    "# === Surrogate generation ===\n",
    "STOP_WORDS = {\n",
    "    'a', 'an', 'the', 'is', 'are', 'was', 'were', 'be', 'been', 'being',\n",
    "    'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could',\n",
    "    'should', 'may', 'might', 'can', 'shall', 'to', 'of', 'in', 'for',\n",
    "    'on', 'with', 'at', 'by', 'from', 'as', 'into', 'through', 'during',\n",
    "    'before', 'after', 'above', 'below', 'between', 'and', 'but', 'or',\n",
    "    'not', 'no', 'if', 'then', 'than', 'so', 'up', 'out', 'about',\n",
    "    'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those',\n",
    "    'it', 'its', 'i', 'me', 'my', 'we', 'our', 'you', 'your', 'he',\n",
    "    'him', 'his', 'she', 'her', 'they', 'them', 'their', 'how', 'when',\n",
    "    'where', 'why', 'much', 'many', 'some', 'any', 'all', 'each',\n",
    "    'does', 'also', 'just', 'more', 'most', 'very', 'too', 'only',\n",
    "}\n",
    "\n",
    "def extract_keywords(text):\n",
    "    words = re.sub(r'[^\\w\\s]', '', text.lower()).split()\n",
    "    return [w for w in words if w not in STOP_WORDS and len(w) > 2]\n",
    "\n",
    "def make_surrogate_from_doc(passage):\n",
    "    content_words = extract_keywords(passage)\n",
    "    if not content_words:\n",
    "        return \"information\"\n",
    "    counts = Counter(content_words)\n",
    "    return \" \".join(w for w, _ in counts.most_common(5))\n",
    "\n",
    "print(\"Scoring functions defined.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a395d365",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Load neural-bridge/rag-dataset-12000\n",
    "from datasets import load_dataset\n",
    "\n",
    "print(\"Loading neural-bridge/rag-dataset-12000...\")\n",
    "ds = load_dataset(\"neural-bridge/rag-dataset-12000\", split=\"train\")\n",
    "print(f\"Total samples: {len(ds)}\")\n",
    "\n",
    "# Filter to long queries with real answers (same filter as v3 Exp 3D)\n",
    "all_candidates = []\n",
    "for row in ds:\n",
    "    q = row.get(\"question\", \"\")\n",
    "    doc = row.get(\"context\", \"\")\n",
    "    answer = row.get(\"answer\", \"\")\n",
    "    if not q or not doc or not answer:\n",
    "        continue\n",
    "    q_words = len(q.split())\n",
    "    a_words = len(answer.split())\n",
    "    if q_words >= 15 and a_words >= 5:\n",
    "        all_candidates.append({\n",
    "            \"query\": q,\n",
    "            \"passage\": doc,\n",
    "            \"answer\": answer,\n",
    "            \"query_words\": q_words,\n",
    "            \"doc_words\": len(doc.split()),\n",
    "            \"answer_words\": a_words,\n",
    "        })\n",
    "\n",
    "print(f\"Candidates (q>=15w, a>=5w): {len(all_candidates)}\")\n",
    "\n",
    "np.random.seed(SEED)\n",
    "indices = np.random.permutation(len(all_candidates))\n",
    "samples = [all_candidates[i] for i in indices[:N_SAMPLES]]\n",
    "del ds, all_candidates\n",
    "gc.collect()\n",
    "\n",
    "# Generate surrogates\n",
    "for i, s in enumerate(samples):\n",
    "    s['surr_doc'] = make_surrogate_from_doc(s['passage'])\n",
    "\n",
    "    # Random prefix: words from unrelated document, matched to query word count\n",
    "    other_idx = (i + N_SAMPLES // 2) % len(samples)\n",
    "    other_words = samples[other_idx]['passage'].split()\n",
    "    query_word_count = len(s['query'].split())\n",
    "    s['random_prefix'] = \" \".join(other_words[:query_word_count])\n",
    "\n",
    "    # Count prefix tokens\n",
    "    s['n_prefix_oracle'] = count_prefix_tokens(s['query'], s['passage'])\n",
    "    s['n_prefix_doc'] = count_prefix_tokens(s['surr_doc'], s['passage'])\n",
    "    s['n_prefix_random'] = count_prefix_tokens(s['random_prefix'], s['passage'])\n",
    "\n",
    "# Dataset statistics\n",
    "q_lens = np.array([s['query_words'] for s in samples])\n",
    "d_lens = np.array([s['doc_words'] for s in samples])\n",
    "a_lens = np.array([s['answer_words'] for s in samples])\n",
    "doc_tok_counts = [len(tokenizer(s['passage'], add_special_tokens=True).input_ids)\n",
    "                  for s in samples]\n",
    "\n",
    "print(f\"\\nSample statistics (N={N_SAMPLES}):\")\n",
    "print(f\"  Query:    mean={q_lens.mean():.1f}w, median={np.median(q_lens):.0f}w\")\n",
    "print(f\"  Document: mean={d_lens.mean():.0f}w, median={np.median(d_lens):.0f}w\")\n",
    "print(f\"  Doc toks: mean={np.mean(doc_tok_counts):.0f}, median={np.median(doc_tok_counts):.0f}, \"\n",
    "      f\"max={np.max(doc_tok_counts)}\")\n",
    "print(f\"  Answer:   mean={a_lens.mean():.1f}w, median={np.median(a_lens):.0f}w\")\n",
    "\n",
    "print(f\"\\nComparison with MS MARCO (Exp 01):\")\n",
    "print(f\"  MS MARCO: query=6.0w, doc=~60w (~98 tok), answer=~20w\")\n",
    "print(f\"  This:     query={q_lens.mean():.1f}w, doc={d_lens.mean():.0f}w \"\n",
    "      f\"(~{np.mean(doc_tok_counts):.0f} tok), answer={a_lens.mean():.0f}w\")\n",
    "\n",
    "print(f\"\\nFirst sample:\")\n",
    "print(f\"  Query:   {samples[0]['query'][:100]}...\")\n",
    "print(f\"  Answer:  {samples[0]['answer'][:100]}...\")\n",
    "print(f\"  Doc:     {samples[0]['passage'][:100]}...\")\n",
    "print(f\"  Surr:    {samples[0]['surr_doc']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3734f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Show example conditions\n",
    "print(\"=\" * 70)\n",
    "print(\"EXAMPLE CONDITIONS (sample 0)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "ex = samples[0]\n",
    "print(f\"\\nQuery ({ex['query_words']}w):  {ex['query'][:100]}...\")\n",
    "print(f\"Answer ({ex['answer_words']}w): {ex['answer'][:100]}...\")\n",
    "print(f\"Doc ({ex['doc_words']}w):    {ex['passage'][:100]}...\")\n",
    "\n",
    "print(f\"\\n  {'Condition':<20} {'Enc prefix':<25} {'Trunc':>6} {'Dec query':>10} {'Pfx tok':>8}\")\n",
    "print(f\"  {'-'*75}\")\n",
    "for name, prefix, trunc, has_q, n_pfx in [\n",
    "    ('bare',             '(none)',                'no',  'yes', 0),\n",
    "    ('oracle_trunc',     'real query',            'yes', 'yes', ex['n_prefix_oracle']),\n",
    "    ('surr_doc_trunc',   ex['surr_doc'][:20],     'yes', 'yes', ex['n_prefix_doc']),\n",
    "    ('random_trunc',     '(unrelated)',           'yes', 'yes', ex['n_prefix_random']),\n",
    "    ('bare_nq',          '(none)',                'no',  'no',  0),\n",
    "    ('oracle_trunc_nq',  'real query',            'yes', 'no',  ex['n_prefix_oracle']),\n",
    "]:\n",
    "    print(f\"  {name:<20} {prefix:<25} {trunc:>6} {has_q:>10} {n_pfx:>8}\")\n",
    "\n",
    "# Sanity check\n",
    "print(f\"\\nSanity check...\")\n",
    "nll_bare = score_nll_query_prefix(ex['passage'], ex['query'], ex['answer'])\n",
    "nll_oracle = score_nll_query_prefix(\n",
    "    ex['query'] + \"\\n\" + ex['passage'], ex['query'], ex['answer'],\n",
    "    prefix_token_count=ex['n_prefix_oracle'], truncate=True)\n",
    "print(f\"  bare:          {nll_bare:.4f}\")\n",
    "print(f\"  oracle_trunc:  {nll_oracle:.4f}\")\n",
    "print(f\"  delta:         {nll_bare - nll_oracle:+.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a40b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Scoring loop — 6 conditions x 500 samples\n",
    "print(\"=\" * 70)\n",
    "print(\"SCORING ALL CONDITIONS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "COND_NAMES = [\n",
    "    'bare', 'oracle_trunc', 'surr_doc_trunc', 'random_trunc',\n",
    "    'bare_nq', 'oracle_trunc_nq',\n",
    "]\n",
    "\n",
    "results = []\n",
    "start_idx = 0\n",
    "\n",
    "if CHECKPOINT_PATH.exists():\n",
    "    ckpt = json.loads(CHECKPOINT_PATH.read_text())\n",
    "    if ckpt.get('n_total') == N_SAMPLES and len(ckpt.get('results', [])) > 0:\n",
    "        saved_queries = [r['query'][:50] for r in ckpt['results']]\n",
    "        current_queries = [s['query'][:50] for s in samples[:len(saved_queries)]]\n",
    "        if saved_queries == current_queries:\n",
    "            results = ckpt['results']\n",
    "            start_idx = len(results)\n",
    "            print(f\"Resuming from checkpoint: {start_idx}/{N_SAMPLES}\")\n",
    "\n",
    "if start_idx == 0:\n",
    "    print(f\"Starting fresh: {len(COND_NAMES)} conditions x {N_SAMPLES} samples\")\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "for i in tqdm(range(start_idx, N_SAMPLES), initial=start_idx, total=N_SAMPLES,\n",
    "              desc=\"Scoring\"):\n",
    "    s = samples[i]\n",
    "    query = s['query']\n",
    "    passage = s['passage']\n",
    "    answer = s['answer']\n",
    "\n",
    "    result = {\n",
    "        'query': query,\n",
    "        'answer': answer,\n",
    "        'query_words': s['query_words'],\n",
    "        'doc_words': s['doc_words'],\n",
    "        'answer_words': s['answer_words'],\n",
    "    }\n",
    "\n",
    "    # --- With query in decoder ---\n",
    "    result['nll_bare'] = score_nll_query_prefix(passage, query, answer)\n",
    "\n",
    "    result['nll_oracle_trunc'] = score_nll_query_prefix(\n",
    "        query + \"\\n\" + passage, query, answer,\n",
    "        prefix_token_count=s['n_prefix_oracle'], truncate=True)\n",
    "\n",
    "    result['nll_surr_doc_trunc'] = score_nll_query_prefix(\n",
    "        s['surr_doc'] + \"\\n\" + passage, query, answer,\n",
    "        prefix_token_count=s['n_prefix_doc'], truncate=True)\n",
    "\n",
    "    result['nll_random_trunc'] = score_nll_query_prefix(\n",
    "        s['random_prefix'] + \"\\n\" + passage, query, answer,\n",
    "        prefix_token_count=s['n_prefix_random'], truncate=True)\n",
    "\n",
    "    # --- Without query in decoder (v3 replication) ---\n",
    "    result['nll_bare_nq'] = score_nll(passage, answer)\n",
    "\n",
    "    result['nll_oracle_trunc_nq'] = score_nll(\n",
    "        query + \"\\n\" + passage, answer,\n",
    "        prefix_token_count=s['n_prefix_oracle'], truncate=True)\n",
    "\n",
    "    results.append(result)\n",
    "\n",
    "    if (i + 1) % 20 == 0 or i == N_SAMPLES - 1:\n",
    "        ckpt = {\n",
    "            'n_total': N_SAMPLES,\n",
    "            'results': results,\n",
    "            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        }\n",
    "        CHECKPOINT_PATH.write_text(json.dumps(ckpt))\n",
    "        elapsed = time.time() - t0\n",
    "        done = i - start_idx + 1\n",
    "        eta = (N_SAMPLES - i - 1) * elapsed / done if done > 0 else 0\n",
    "        tqdm.write(f\"  Checkpoint {i+1}/{N_SAMPLES} | {elapsed/60:.1f}m | ETA {eta/60:.1f}m\")\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "elapsed = time.time() - t0\n",
    "print(f\"\\nScoring complete: {len(results)} samples, \"\n",
    "      f\"{len(COND_NAMES)} conditions in {elapsed/60:.1f} min\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009773f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Results table\n",
    "print(\"=\" * 70)\n",
    "print(f\"RESULTS (N={len(results)})\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "bare = np.array([r['nll_bare'] for r in results])\n",
    "oracle_trunc = np.array([r['nll_oracle_trunc'] for r in results])\n",
    "surr_doc = np.array([r['nll_surr_doc_trunc'] for r in results])\n",
    "random_trunc = np.array([r['nll_random_trunc'] for r in results])\n",
    "bare_nq = np.array([r['nll_bare_nq'] for r in results])\n",
    "oracle_nq = np.array([r['nll_oracle_trunc_nq'] for r in results])\n",
    "\n",
    "# Bonferroni: 3 query-prefix + 1 nq = 4\n",
    "N_BONF = 4\n",
    "\n",
    "print(f\"\\n--- With query in decoder (production-realistic) ---\")\n",
    "print(f\"  {'Condition':<20} {'NLL':>8} {'vs bare':>10} {'d':>8} {'Win%':>8} {'p':>12} {'sig':>5}\")\n",
    "print(f\"  {'-'*74}\")\n",
    "\n",
    "analysis = {}\n",
    "for name, nlls in [('bare', bare), ('oracle_trunc', oracle_trunc),\n",
    "                    ('surr_doc_trunc', surr_doc), ('random_trunc', random_trunc)]:\n",
    "    mean_nll = nlls.mean()\n",
    "    if name == 'bare':\n",
    "        print(f\"  {name:<20} {mean_nll:>8.4f} {'--':>10} {'--':>8} {'--':>8} {'--':>12} {'--':>5}\")\n",
    "        analysis[name] = {'mean_nll': float(mean_nll)}\n",
    "    else:\n",
    "        diff = bare - nlls\n",
    "        d = cohens_d(diff)\n",
    "        win_pct = 100 * np.mean(diff > 0)\n",
    "        _, p_val = stats.ttest_1samp(diff, 0)\n",
    "        sig = '***' if p_val < 0.001/N_BONF else '**' if p_val < 0.01/N_BONF else '*' if p_val < 0.05/N_BONF else 'ns'\n",
    "        print(f\"  {name:<20} {mean_nll:>8.4f} {diff.mean():>+10.4f} {d:>+8.3f} {win_pct:>7.1f}% {p_val:>12.2e} {sig:>5}\")\n",
    "        analysis[name] = {\n",
    "            'mean_nll': float(mean_nll), 'delta': float(diff.mean()),\n",
    "            'd': float(d), 'win_pct': float(win_pct), 'p': float(p_val),\n",
    "        }\n",
    "\n",
    "# No-query conditions\n",
    "diff_nq = bare_nq - oracle_nq\n",
    "d_nq = cohens_d(diff_nq)\n",
    "win_nq = 100 * np.mean(diff_nq > 0)\n",
    "_, p_nq = stats.ttest_1samp(diff_nq, 0)\n",
    "sig_nq = '***' if p_nq < 0.001/N_BONF else '**' if p_nq < 0.01/N_BONF else '*' if p_nq < 0.05/N_BONF else 'ns'\n",
    "\n",
    "print(f\"\\n--- Without query in decoder (v3 replication) ---\")\n",
    "print(f\"  {'bare_nq':<20} {bare_nq.mean():>8.4f}\")\n",
    "print(f\"  {'oracle_trunc_nq':<20} {oracle_nq.mean():>8.4f} {diff_nq.mean():>+10.4f} {d_nq:>+8.3f} {win_nq:>7.1f}% {p_nq:>12.2e} {sig_nq:>5}\")\n",
    "\n",
    "analysis['bare_nq'] = {'mean_nll': float(bare_nq.mean())}\n",
    "analysis['oracle_trunc_nq'] = {\n",
    "    'mean_nll': float(oracle_nq.mean()), 'delta': float(diff_nq.mean()),\n",
    "    'd': float(d_nq), 'win_pct': float(win_nq), 'p': float(p_nq),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582518eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Cross-dataset comparison with Exp 01 (MS MARCO)\n",
    "print(\"=\" * 70)\n",
    "print(\"CROSS-DATASET COMPARISON\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "d_oracle = cohens_d(bare - oracle_trunc)\n",
    "d_surr = cohens_d(bare - surr_doc)\n",
    "d_random = cohens_d(bare - random_trunc)\n",
    "d_oracle_nq = cohens_d(bare_nq - oracle_nq)\n",
    "\n",
    "# Exp 01 reference values\n",
    "exp01 = {\n",
    "    'oracle_trunc': 0.228, 'surr_doc_trunc': 0.148,\n",
    "    'random_trunc': 0.080, 'oracle_trunc_nq': 0.376,\n",
    "}\n",
    "\n",
    "print(f\"\\n  {'Condition':<20} {'MS MARCO (Exp01)':>18} {'Neural-Bridge':>15} {'Ratio':>8}\")\n",
    "print(f\"  {'-'*65}\")\n",
    "for name, exp01_d in exp01.items():\n",
    "    if name == 'oracle_trunc':\n",
    "        this_d = d_oracle\n",
    "    elif name == 'surr_doc_trunc':\n",
    "        this_d = d_surr\n",
    "    elif name == 'random_trunc':\n",
    "        this_d = d_random\n",
    "    else:\n",
    "        this_d = d_oracle_nq\n",
    "    ratio = this_d / exp01_d if exp01_d != 0 else 0\n",
    "    print(f\"  {name:<20} {exp01_d:>+18.3f} {this_d:>+15.3f} {ratio:>7.1f}x\")\n",
    "\n",
    "# Structural fraction comparison\n",
    "struct_marco = 0.080 / 0.228 * 100 if 0.228 > 0 else 0\n",
    "struct_nb = d_random / d_oracle * 100 if d_oracle > 0 else 0\n",
    "\n",
    "print(f\"\\n--- Structural Fraction (random/oracle) ---\")\n",
    "print(f\"  MS MARCO Exp 01: {struct_marco:.0f}%\")\n",
    "print(f\"  Neural-Bridge:   {struct_nb:.0f}%\")\n",
    "\n",
    "# Surrogate efficiency\n",
    "surr_pct_marco = 0.148 / 0.228 * 100 if 0.228 > 0 else 0\n",
    "surr_pct_nb = d_surr / d_oracle * 100 if d_oracle > 0 else 0\n",
    "\n",
    "print(f\"\\n--- Surrogate Efficiency (surr_doc/oracle) ---\")\n",
    "print(f\"  MS MARCO Exp 01: {surr_pct_marco:.0f}%\")\n",
    "print(f\"  Neural-Bridge:   {surr_pct_nb:.0f}%\")\n",
    "\n",
    "# v4/v3 ratio\n",
    "ratio_marco = 0.228 / 0.376 * 100\n",
    "ratio_nb = d_oracle / d_oracle_nq * 100 if d_oracle_nq > 0 else 0\n",
    "\n",
    "print(f\"\\n--- v4/v3 Enrichment Ratio (how much survives with query in decoder) ---\")\n",
    "print(f\"  MS MARCO:        {ratio_marco:.0f}%\")\n",
    "print(f\"  Neural-Bridge:   {ratio_nb:.0f}%\")\n",
    "\n",
    "# v3 Exp 3D comparison (surrogates beat oracle)\n",
    "print(f\"\\n--- v3 Exp 3D Comparison (surrogates beat oracle phenomenon) ---\")\n",
    "print(f\"  v3 Exp 3D: ALL surrogates beat oracle (150%+ of oracle d)\")\n",
    "print(f\"  v3 explanation: real query creates semantic interference in encoder\")\n",
    "if d_surr > d_oracle:\n",
    "    print(f\"  v4: surr_doc ({d_surr:+.3f}) STILL beats oracle ({d_oracle:+.3f})\")\n",
    "    print(f\"       -> Semantic interference persists even with query in decoder\")\n",
    "else:\n",
    "    print(f\"  v4: oracle ({d_oracle:+.3f}) beats surr_doc ({d_surr:+.3f})\")\n",
    "    print(f\"       -> Decoder query resolves the interference\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a83c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Key comparison — enrichment with query vs without query\n",
    "print(\"=\" * 70)\n",
    "print(\"KEY COMPARISON: Is enrichment redundant when decoder has the query?\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "enrichment_with_q = bare - oracle_trunc\n",
    "enrichment_no_q = bare_nq - oracle_nq\n",
    "d_with_q = cohens_d(enrichment_with_q)\n",
    "d_no_q = cohens_d(enrichment_no_q)\n",
    "\n",
    "ratio = d_with_q / d_no_q * 100 if d_no_q > 0 else 0\n",
    "\n",
    "print(f\"\\n  Enrichment with query in decoder:    d={d_with_q:+.3f}\")\n",
    "print(f\"  Enrichment without query (v3 repl):  d={d_no_q:+.3f}\")\n",
    "print(f\"  Ratio: {ratio:.0f}%\")\n",
    "\n",
    "# Per-sample correlation\n",
    "r_corr, p_corr = stats.pearsonr(enrichment_with_q, enrichment_no_q)\n",
    "print(f\"  Per-sample correlation: r={r_corr:.3f} (p={p_corr:.2e})\")\n",
    "\n",
    "# Hardness gradient\n",
    "print(f\"\\n--- Hardness gradient (with query in decoder) ---\")\n",
    "quintile_bounds = np.percentile(bare, [20, 40, 60, 80])\n",
    "quintiles = np.digitize(bare, quintile_bounds)\n",
    "\n",
    "print(f\"  {'Quintile':<12} {'N':>4} {'bare':>8} {'oracle':>8} {'delta':>8} {'d':>8}\")\n",
    "print(f\"  {'-'*50}\")\n",
    "for q in range(5):\n",
    "    mask = quintiles == q\n",
    "    n_q = mask.sum()\n",
    "    if n_q < 5:\n",
    "        continue\n",
    "    qlabel = ['Q1 easy', 'Q2', 'Q3', 'Q4', 'Q5 hard'][q]\n",
    "    b = bare[mask].mean()\n",
    "    o = oracle_trunc[mask].mean()\n",
    "    delta = (bare[mask] - oracle_trunc[mask]).mean()\n",
    "    d = cohens_d(bare[mask] - oracle_trunc[mask])\n",
    "    print(f\"  {qlabel:<12} {n_q:>4} {b:>8.4f} {o:>8.4f} {delta:>+8.4f} {d:>+8.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86aeb76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Verdict and save\n",
    "print(\"=\" * 70)\n",
    "print(\"VERDICT -- Exp 03: Neural-Bridge Cross-Dataset Validation\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "d_oracle = cohens_d(bare - oracle_trunc)\n",
    "d_surr = cohens_d(bare - surr_doc)\n",
    "d_random = cohens_d(bare - random_trunc)\n",
    "d_oracle_nq = cohens_d(bare_nq - oracle_nq)\n",
    "ratio = d_oracle / d_oracle_nq * 100 if d_oracle_nq > 0 else 0\n",
    "struct_frac = d_random / d_oracle * 100 if d_oracle > 0 else 0\n",
    "\n",
    "print(f\"\\nModel: {MODEL_NAME}\")\n",
    "print(f\"Dataset: neural-bridge/rag-dataset-12000\")\n",
    "print(f\"N: {len(results)}, mean doc: {np.mean([r['doc_words'] for r in results]):.0f} words\")\n",
    "\n",
    "print(f\"\\n--- Key results ---\")\n",
    "print(f\"  Oracle enrichment (query in decoder):  d={d_oracle:+.3f}\")\n",
    "print(f\"  Oracle enrichment (no query, v3 repl): d={d_oracle_nq:+.3f}\")\n",
    "print(f\"  Ratio (v4/v3):                         {ratio:.0f}%\")\n",
    "print(f\"  Structural fraction (random/oracle):   {struct_frac:.0f}%\")\n",
    "print(f\"  Surrogate doc efficiency:              {d_surr/d_oracle*100:.0f}% of oracle\" if d_oracle > 0 else \"\")\n",
    "\n",
    "_, p_oracle = stats.ttest_1samp(bare - oracle_trunc, 0)\n",
    "_, p_surr = stats.ttest_1samp(bare - surr_doc, 0)\n",
    "_, p_rand = stats.ttest_1samp(bare - random_trunc, 0)\n",
    "\n",
    "print(f\"\\n--- Significance ---\")\n",
    "print(f\"  oracle_trunc: p={p_oracle:.2e}\")\n",
    "print(f\"  surr_doc:     p={p_surr:.2e}\")\n",
    "print(f\"  random:       p={p_rand:.2e}\")\n",
    "\n",
    "print(f\"\\n--- Cross-dataset consistency ---\")\n",
    "print(f\"  MS MARCO (Exp 01): oracle d=+0.228, surr_doc d=+0.148, random d=+0.080\")\n",
    "print(f\"  Neural-Bridge:     oracle d={d_oracle:+.3f}, surr_doc d={d_surr:+.3f}, random d={d_random:+.3f}\")\n",
    "\n",
    "if d_oracle > 0.1:\n",
    "    print(f\"\\n  CONCLUSION: v4 enrichment benefit REPLICATES on naturally long documents.\")\n",
    "else:\n",
    "    print(f\"\\n  CONCLUSION: v4 enrichment benefit does NOT replicate on this dataset.\")\n",
    "\n",
    "# Save\n",
    "final_results = {\n",
    "    'experiment': 'v4_exp03_neural_bridge',\n",
    "    'model': MODEL_NAME,\n",
    "    'dataset': 'neural-bridge/rag-dataset-12000',\n",
    "    'n_samples': len(results),\n",
    "    'seed': SEED,\n",
    "    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'dataset_stats': {\n",
    "        'mean_query_words': float(np.mean([r['query_words'] for r in results])),\n",
    "        'mean_doc_words': float(np.mean([r['doc_words'] for r in results])),\n",
    "        'mean_answer_words': float(np.mean([r['answer_words'] for r in results])),\n",
    "    },\n",
    "    'key_result': {\n",
    "        'enrichment_with_query_d': float(d_oracle),\n",
    "        'enrichment_no_query_d': float(d_oracle_nq),\n",
    "        'ratio_pct': float(ratio),\n",
    "        'structural_fraction_pct': float(struct_frac),\n",
    "    },\n",
    "    'conditions': analysis,\n",
    "}\n",
    "\n",
    "with open(RESULTS_DIR / 'results.json', 'w') as f:\n",
    "    json.dump(final_results, f, indent=2)\n",
    "print(f\"\\nResults saved to {RESULTS_DIR / 'results.json'}\")\n",
    "\n",
    "# Cleanup\n",
    "print(f\"\\nCleaning up GPU memory...\")\n",
    "mem_before = torch.cuda.memory_allocated() / 1e9\n",
    "del model, processor, tokenizer\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "mem_after = torch.cuda.memory_allocated() / 1e9\n",
    "print(f\"GPU memory: {mem_before:.2f} GB -> {mem_after:.2f} GB\")\n",
    "print(\"Done!\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
