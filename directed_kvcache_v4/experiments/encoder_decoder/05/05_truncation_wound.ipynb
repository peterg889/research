{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7bb9e467",
   "metadata": {},
   "source": [
    "# Experiment 05: Truncation Wound Mechanism\n",
    "\n",
    "## Motivation\n",
    "\n",
    "Exp 04 revealed a striking finding: the first sentence of the document used as an\n",
    "encoder prefix is **catastrophic** (d=-0.298 ***), while disconnected keywords help\n",
    "(kw10 d=+0.186). This is the \"truncation wound\" phenomenon — the prefix creates\n",
    "strong bidirectional attention connections during encoding, and masking those tokens\n",
    "from cross-attention leaves the document representations *worse* than bare encoding.\n",
    "\n",
    "But what specifically causes the wound? Two hypotheses:\n",
    "\n",
    "1. **Overlap hypothesis**: The first sentence shares vocabulary and semantics with the\n",
    "   document. This creates unusually strong attention connections during encoding. When\n",
    "   truncated, the \"wound\" is proportional to connection strength.\n",
    "\n",
    "2. **Coherence hypothesis**: Any coherent natural text creates deep attention dependencies\n",
    "   during encoding, regardless of overlap. Keywords work specifically because they're\n",
    "   disconnected tokens that perturb without creating deep dependencies.\n",
    "\n",
    "## Conditions (10 total)\n",
    "\n",
    "### Text type sweep (all with query in decoder + truncation):\n",
    "\n",
    "| # | Condition | Prefix | Tests |\n",
    "|---|-----------|--------|-------|\n",
    "| 1 | bare | (none) | Baseline |\n",
    "| 2 | oracle_trunc | real query | Ceiling |\n",
    "| 3 | surr_kw10 | top-10 kw from THIS doc | Best surrogate (Exp 04) |\n",
    "| 4 | surr_first_sent | first sent of THIS doc | Catastrophic in Exp 04 |\n",
    "| 5 | surr_wrong_first_sent | first sent of WRONG doc | Coherence without overlap |\n",
    "| 6 | surr_shuffled_sent | shuffled first sent of THIS doc | Overlap without coherence |\n",
    "| 7 | surr_wrong_kw10 | top-10 kw from WRONG doc | Keywords without specificity |\n",
    "| 8 | surr_generic_sent | fixed generic sentence | Coherent, no overlap, no info |\n",
    "\n",
    "### Controls:\n",
    "\n",
    "| # | Condition | Purpose |\n",
    "|---|-----------|---------|\n",
    "| 9 | bare_nq | v3 baseline |\n",
    "| 10 | oracle_trunc_nq | v3 replication |\n",
    "\n",
    "## Key comparisons\n",
    "\n",
    "- **(4) vs (5)**: Same-doc vs wrong-doc first sentence → isolates **overlap**\n",
    "- **(4) vs (6)**: Coherent vs shuffled same-doc first sentence → isolates **coherence**\n",
    "- **(5) vs (8)**: Wrong-doc sentence vs generic sentence → information content within coherent text\n",
    "- **(3) vs (7)**: Same-doc vs wrong-doc keywords → replicates Exp 04 specificity test\n",
    "- **(6) vs (3)**: Shuffled sentence (long) vs keywords (short) → length/format comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752bfdb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Setup and model loading\n",
    "import os\n",
    "os.umask(0o000)\n",
    "\n",
    "import sys, json, time, gc, re\n",
    "import random as pyrandom\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from scipy import stats\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "sys.path.insert(0, \"../../..\")\n",
    "from lib.analysis import cohens_d\n",
    "\n",
    "SEED = 42\n",
    "N_SAMPLES = 500\n",
    "MODEL_NAME = \"google/t5gemma-2-4b-4b\"\n",
    "\n",
    "RESULTS_DIR = Path(\"../../../results/exp05\")\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CHECKPOINT_PATH = RESULTS_DIR / \"checkpoint.json\"\n",
    "\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "pyrandom.seed(SEED)\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv())\n",
    "HF_TOKEN = os.environ.get(\"HF_TOKEN\")\n",
    "\n",
    "from transformers import AutoProcessor, AutoModelForSeq2SeqLM\n",
    "\n",
    "print(f\"Loading {MODEL_NAME}...\")\n",
    "processor = AutoProcessor.from_pretrained(MODEL_NAME, token=HF_TOKEN)\n",
    "tokenizer = processor.tokenizer\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    MODEL_NAME, device_map=\"auto\", torch_dtype=torch.bfloat16, token=HF_TOKEN,\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "DEVICE = next(model.parameters()).device\n",
    "BOS_ID = getattr(model.config, 'decoder_start_token_id', None) or tokenizer.bos_token_id\n",
    "\n",
    "print(f\"Exp 05: Truncation Wound Mechanism\")\n",
    "print(f\"N: {N_SAMPLES}, Model: {MODEL_NAME}\")\n",
    "print(f\"DEVICE: {DEVICE}, dtype: {next(model.parameters()).dtype}\")\n",
    "print(f\"GPU memory: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ecdca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Scoring helpers\n",
    "\n",
    "def count_prefix_tokens(prefix_text, document_text):\n",
    "    full_text = prefix_text + \"\\n\" + document_text\n",
    "    full_ids = tokenizer(full_text, add_special_tokens=True, truncation=True,\n",
    "                         max_length=2048).input_ids\n",
    "    doc_ids = tokenizer(document_text, add_special_tokens=True, truncation=True,\n",
    "                        max_length=2048).input_ids\n",
    "    return len(full_ids) - len(doc_ids)\n",
    "\n",
    "\n",
    "def score_nll(encoder_text, answer_text, prefix_token_count=0, truncate=False):\n",
    "    # No query in decoder — used for _nq conditions.\n",
    "    enc_ids = tokenizer(encoder_text, return_tensors=\"pt\",\n",
    "                        add_special_tokens=True, truncation=True,\n",
    "                        max_length=2048).input_ids.to(DEVICE)\n",
    "    total_enc_len = enc_ids.shape[1]\n",
    "    enc_mask = torch.ones(1, total_enc_len, device=DEVICE, dtype=torch.long)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        encoder_outputs = model.get_encoder()(\n",
    "            input_ids=enc_ids, attention_mask=enc_mask\n",
    "        )\n",
    "\n",
    "    if truncate and prefix_token_count > 0:\n",
    "        cross_attn_mask = torch.ones(1, total_enc_len, device=DEVICE, dtype=torch.long)\n",
    "        cross_attn_mask[:, :prefix_token_count] = 0\n",
    "    else:\n",
    "        cross_attn_mask = torch.ones(1, total_enc_len, device=DEVICE, dtype=torch.long)\n",
    "\n",
    "    ans_ids = tokenizer(answer_text, return_tensors=\"pt\",\n",
    "                        add_special_tokens=False, truncation=True,\n",
    "                        max_length=256).input_ids.to(DEVICE)\n",
    "\n",
    "    if ans_ids.shape[1] == 0:\n",
    "        del encoder_outputs\n",
    "        return 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            encoder_outputs=encoder_outputs,\n",
    "            attention_mask=cross_attn_mask,\n",
    "            labels=ans_ids,\n",
    "        )\n",
    "\n",
    "    logits = outputs.logits\n",
    "    log_probs = F.log_softmax(logits, dim=-1)\n",
    "    token_log_probs = log_probs[0].gather(1, ans_ids[0].unsqueeze(1)).squeeze(1)\n",
    "    mean_nll = -token_log_probs.mean().item()\n",
    "\n",
    "    del encoder_outputs, outputs, logits, log_probs\n",
    "    return mean_nll\n",
    "\n",
    "\n",
    "def score_nll_query_prefix(encoder_text, query_text, answer_text,\n",
    "                           prefix_token_count=0, truncate=False):\n",
    "    # Query as decoder prefix — production-realistic.\n",
    "    enc_ids = tokenizer(encoder_text, return_tensors=\"pt\",\n",
    "                        add_special_tokens=True, truncation=True,\n",
    "                        max_length=2048).input_ids.to(DEVICE)\n",
    "    total_enc_len = enc_ids.shape[1]\n",
    "    enc_mask = torch.ones(1, total_enc_len, device=DEVICE, dtype=torch.long)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        encoder_outputs = model.get_encoder()(\n",
    "            input_ids=enc_ids, attention_mask=enc_mask\n",
    "        )\n",
    "\n",
    "    if truncate and prefix_token_count > 0:\n",
    "        cross_attn_mask = torch.ones(1, total_enc_len, device=DEVICE, dtype=torch.long)\n",
    "        cross_attn_mask[:, :prefix_token_count] = 0\n",
    "    else:\n",
    "        cross_attn_mask = torch.ones(1, total_enc_len, device=DEVICE, dtype=torch.long)\n",
    "\n",
    "    query_ids = tokenizer(query_text, add_special_tokens=False, truncation=True,\n",
    "                          max_length=512).input_ids\n",
    "    answer_ids = tokenizer(answer_text, add_special_tokens=False, truncation=True,\n",
    "                           max_length=256).input_ids\n",
    "\n",
    "    if len(answer_ids) == 0:\n",
    "        del encoder_outputs\n",
    "        return 0.0\n",
    "\n",
    "    dec_ids = [BOS_ID] + query_ids + answer_ids\n",
    "    dec_tensor = torch.tensor([dec_ids], dtype=torch.long, device=DEVICE)\n",
    "\n",
    "    n_query = len(query_ids)\n",
    "    n_answer = len(answer_ids)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            encoder_outputs=encoder_outputs,\n",
    "            attention_mask=cross_attn_mask,\n",
    "            decoder_input_ids=dec_tensor,\n",
    "        )\n",
    "\n",
    "    logits = outputs.logits\n",
    "    answer_logits = logits[0, n_query:n_query + n_answer, :]\n",
    "\n",
    "    answer_targets = torch.tensor(answer_ids, dtype=torch.long, device=DEVICE)\n",
    "    log_probs = F.log_softmax(answer_logits, dim=-1)\n",
    "    token_log_probs = log_probs.gather(1, answer_targets.unsqueeze(1)).squeeze(1)\n",
    "    mean_nll = -token_log_probs.mean().item()\n",
    "\n",
    "    del encoder_outputs, outputs, logits, log_probs\n",
    "    return mean_nll\n",
    "\n",
    "\n",
    "# === Surrogate helpers ===\n",
    "STOP_WORDS = {\n",
    "    'a', 'an', 'the', 'is', 'are', 'was', 'were', 'be', 'been', 'being',\n",
    "    'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could',\n",
    "    'should', 'may', 'might', 'can', 'shall', 'to', 'of', 'in', 'for',\n",
    "    'on', 'with', 'at', 'by', 'from', 'as', 'into', 'through', 'during',\n",
    "    'before', 'after', 'above', 'below', 'between', 'and', 'but', 'or',\n",
    "    'not', 'no', 'if', 'then', 'than', 'so', 'up', 'out', 'about',\n",
    "    'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those',\n",
    "    'it', 'its', 'i', 'me', 'my', 'we', 'our', 'you', 'your', 'he',\n",
    "    'him', 'his', 'she', 'her', 'they', 'them', 'their', 'how', 'when',\n",
    "    'where', 'why', 'much', 'many', 'some', 'any', 'all', 'each',\n",
    "    'does', 'also', 'just', 'more', 'most', 'very', 'too', 'only',\n",
    "}\n",
    "\n",
    "def extract_keywords(text):\n",
    "    words = re.sub(r'[^\\w\\s]', '', text.lower()).split()\n",
    "    return [w for w in words if w not in STOP_WORDS and len(w) > 2]\n",
    "\n",
    "def make_kw_surrogate(passage, n_keywords):\n",
    "    content_words = extract_keywords(passage)\n",
    "    if not content_words:\n",
    "        return \"information\"\n",
    "    counts = Counter(content_words)\n",
    "    return \" \".join(w for w, _ in counts.most_common(n_keywords))\n",
    "\n",
    "def get_first_sentence(text):\n",
    "    parts = re.split(r'(?<=[.!?])\\s+', text.strip())\n",
    "    if parts:\n",
    "        return parts[0]\n",
    "    return text[:100]\n",
    "\n",
    "def shuffle_sentence(text):\n",
    "    # Shuffle words in a sentence, preserving the word set but breaking coherence.\n",
    "    words = text.split()\n",
    "    pyrandom.shuffle(words)\n",
    "    return \" \".join(words)\n",
    "\n",
    "# Fixed generic sentence — coherent but zero information about any document.\n",
    "GENERIC_SENTENCE = \"The following passage contains relevant information about the topic.\"\n",
    "\n",
    "print(\"Scoring functions defined.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6f7653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Load MS MARCO data and generate all prefix variants\n",
    "from lib.data import count_words\n",
    "from datasets import load_dataset\n",
    "\n",
    "print(\"Loading MS MARCO v1.1 validation...\")\n",
    "ds = load_dataset(\"microsoft/ms_marco\", \"v1.1\", split=\"validation\")\n",
    "\n",
    "all_candidates = []\n",
    "for item in ds:\n",
    "    if len(all_candidates) >= 3 * N_SAMPLES:\n",
    "        break\n",
    "    passages = item.get('passages', {})\n",
    "    ptexts = passages.get('passage_text', [])\n",
    "    is_sel = passages.get('is_selected', [])\n",
    "    query = item.get('query', '')\n",
    "    answers = item.get('answers', [])\n",
    "    well_formed = item.get('wellFormedAnswers', [])\n",
    "    answer = None\n",
    "    if well_formed and len(well_formed) > 0 and well_formed[0] not in ('[]', ''):\n",
    "        answer = well_formed[0]\n",
    "    elif answers and len(answers) > 0 and answers[0] != 'No Answer Present.':\n",
    "        answer = answers[0]\n",
    "    if not answer:\n",
    "        continue\n",
    "    for pt, sel in zip(ptexts, is_sel):\n",
    "        wc = count_words(pt)\n",
    "        if sel == 1 and 30 <= wc <= 300:\n",
    "            all_candidates.append({\n",
    "                'passage': pt, 'query': query, 'answer': answer,\n",
    "                'word_count': wc,\n",
    "            })\n",
    "            break\n",
    "\n",
    "print(f\"Total candidates: {len(all_candidates)}\")\n",
    "np.random.seed(SEED)\n",
    "indices = np.random.permutation(len(all_candidates))\n",
    "samples = [all_candidates[i] for i in indices[:N_SAMPLES]]\n",
    "del ds, all_candidates\n",
    "gc.collect()\n",
    "\n",
    "# Generate all prefix variants\n",
    "pyrandom.seed(SEED)\n",
    "for i, s in enumerate(samples):\n",
    "    passage = s['passage']\n",
    "    wrong_idx = (i + N_SAMPLES // 2) % len(samples)\n",
    "\n",
    "    # Keywords from THIS document (best surrogate from Exp 04)\n",
    "    s['surr_kw10'] = make_kw_surrogate(passage, 10)\n",
    "\n",
    "    # First sentence of THIS document (catastrophic in Exp 04)\n",
    "    s['surr_first_sent'] = get_first_sentence(passage)\n",
    "\n",
    "    # First sentence of WRONG document (coherent, non-overlapping)\n",
    "    s['surr_wrong_first_sent'] = get_first_sentence(samples[wrong_idx]['passage'])\n",
    "\n",
    "    # Shuffled first sentence of THIS document (overlap without coherence)\n",
    "    s['surr_shuffled_sent'] = shuffle_sentence(get_first_sentence(passage))\n",
    "\n",
    "    # Keywords from WRONG document (replicates Exp 04 control)\n",
    "    s['surr_wrong_kw10'] = make_kw_surrogate(samples[wrong_idx]['passage'], 10)\n",
    "\n",
    "    # Generic sentence (coherent, zero information)\n",
    "    s['surr_generic_sent'] = GENERIC_SENTENCE\n",
    "\n",
    "    # Count prefix tokens for each\n",
    "    s['n_pfx_oracle'] = count_prefix_tokens(s['query'], passage)\n",
    "    s['n_pfx_kw10'] = count_prefix_tokens(s['surr_kw10'], passage)\n",
    "    s['n_pfx_first_sent'] = count_prefix_tokens(s['surr_first_sent'], passage)\n",
    "    s['n_pfx_wrong_first_sent'] = count_prefix_tokens(s['surr_wrong_first_sent'], passage)\n",
    "    s['n_pfx_shuffled_sent'] = count_prefix_tokens(s['surr_shuffled_sent'], passage)\n",
    "    s['n_pfx_wrong_kw10'] = count_prefix_tokens(s['surr_wrong_kw10'], passage)\n",
    "    s['n_pfx_generic_sent'] = count_prefix_tokens(s['surr_generic_sent'], passage)\n",
    "\n",
    "# Statistics\n",
    "print(f\"\\nLoaded {len(samples)} samples\")\n",
    "\n",
    "print(f\"\\nPrefix token counts (mean):\")\n",
    "for key, label in [\n",
    "    ('n_pfx_oracle', 'oracle (query)'),\n",
    "    ('n_pfx_kw10', 'kw10 (this doc)'),\n",
    "    ('n_pfx_first_sent', 'first_sent (this)'),\n",
    "    ('n_pfx_wrong_first_sent', 'wrong_first_sent'),\n",
    "    ('n_pfx_shuffled_sent', 'shuffled_sent'),\n",
    "    ('n_pfx_wrong_kw10', 'wrong_kw10'),\n",
    "    ('n_pfx_generic_sent', 'generic_sent'),\n",
    "]:\n",
    "    vals = [s[key] for s in samples]\n",
    "    print(f\"  {label:<22}: mean={np.mean(vals):.1f}, min={np.min(vals)}, max={np.max(vals)}\")\n",
    "\n",
    "print(f\"\\nFirst sample:\")\n",
    "print(f\"  Query:             {samples[0]['query']}\")\n",
    "print(f\"  kw10:              {samples[0]['surr_kw10']}\")\n",
    "print(f\"  first_sent:        {samples[0]['surr_first_sent'][:80]}\")\n",
    "print(f\"  wrong_first_sent:  {samples[0]['surr_wrong_first_sent'][:80]}\")\n",
    "print(f\"  shuffled_sent:     {samples[0]['surr_shuffled_sent'][:80]}\")\n",
    "print(f\"  wrong_kw10:        {samples[0]['surr_wrong_kw10']}\")\n",
    "print(f\"  generic_sent:      {samples[0]['surr_generic_sent']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e55c964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Scoring loop — 10 conditions x 500 samples\n",
    "print(\"=\" * 70)\n",
    "print(\"SCORING ALL CONDITIONS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "COND_NAMES = [\n",
    "    'bare', 'oracle_trunc', 'surr_kw10',\n",
    "    'surr_first_sent', 'surr_wrong_first_sent', 'surr_shuffled_sent',\n",
    "    'surr_wrong_kw10', 'surr_generic_sent',\n",
    "    'bare_nq', 'oracle_trunc_nq',\n",
    "]\n",
    "\n",
    "results = []\n",
    "start_idx = 0\n",
    "\n",
    "if CHECKPOINT_PATH.exists():\n",
    "    ckpt = json.loads(CHECKPOINT_PATH.read_text())\n",
    "    if ckpt.get('n_total') == N_SAMPLES and len(ckpt.get('results', [])) > 0:\n",
    "        saved_queries = [r['query'][:50] for r in ckpt['results']]\n",
    "        current_queries = [s['query'][:50] for s in samples[:len(saved_queries)]]\n",
    "        if saved_queries == current_queries:\n",
    "            results = ckpt['results']\n",
    "            start_idx = len(results)\n",
    "            print(f\"Resuming from checkpoint: {start_idx}/{N_SAMPLES}\")\n",
    "\n",
    "if start_idx == 0:\n",
    "    print(f\"Starting fresh: {len(COND_NAMES)} conditions x {N_SAMPLES} samples \"\n",
    "          f\"= {len(COND_NAMES) * N_SAMPLES} forward passes\")\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "for i in tqdm(range(start_idx, N_SAMPLES), initial=start_idx, total=N_SAMPLES,\n",
    "              desc=\"Scoring\"):\n",
    "    s = samples[i]\n",
    "    query = s['query']\n",
    "    passage = s['passage']\n",
    "    answer = s['answer']\n",
    "\n",
    "    result = {\n",
    "        'query': query,\n",
    "        'answer': answer,\n",
    "        'passage_words': s['word_count'],\n",
    "    }\n",
    "\n",
    "    # --- With query in decoder ---\n",
    "\n",
    "    # 1. bare\n",
    "    result['nll_bare'] = score_nll_query_prefix(passage, query, answer)\n",
    "\n",
    "    # 2. oracle_trunc\n",
    "    result['nll_oracle_trunc'] = score_nll_query_prefix(\n",
    "        query + \"\\n\" + passage, query, answer,\n",
    "        prefix_token_count=s['n_pfx_oracle'], truncate=True)\n",
    "\n",
    "    # 3. surr_kw10 (this doc — best surrogate from Exp 04)\n",
    "    result['nll_surr_kw10'] = score_nll_query_prefix(\n",
    "        s['surr_kw10'] + \"\\n\" + passage, query, answer,\n",
    "        prefix_token_count=s['n_pfx_kw10'], truncate=True)\n",
    "\n",
    "    # 4. surr_first_sent (this doc — catastrophic in Exp 04)\n",
    "    result['nll_surr_first_sent'] = score_nll_query_prefix(\n",
    "        s['surr_first_sent'] + \"\\n\" + passage, query, answer,\n",
    "        prefix_token_count=s['n_pfx_first_sent'], truncate=True)\n",
    "\n",
    "    # 5. surr_wrong_first_sent (wrong doc — coherent, non-overlapping)\n",
    "    result['nll_surr_wrong_first_sent'] = score_nll_query_prefix(\n",
    "        s['surr_wrong_first_sent'] + \"\\n\" + passage, query, answer,\n",
    "        prefix_token_count=s['n_pfx_wrong_first_sent'], truncate=True)\n",
    "\n",
    "    # 6. surr_shuffled_sent (this doc shuffled — overlap without coherence)\n",
    "    result['nll_surr_shuffled_sent'] = score_nll_query_prefix(\n",
    "        s['surr_shuffled_sent'] + \"\\n\" + passage, query, answer,\n",
    "        prefix_token_count=s['n_pfx_shuffled_sent'], truncate=True)\n",
    "\n",
    "    # 7. surr_wrong_kw10 (wrong doc keywords)\n",
    "    result['nll_surr_wrong_kw10'] = score_nll_query_prefix(\n",
    "        s['surr_wrong_kw10'] + \"\\n\" + passage, query, answer,\n",
    "        prefix_token_count=s['n_pfx_wrong_kw10'], truncate=True)\n",
    "\n",
    "    # 8. surr_generic_sent (fixed generic sentence)\n",
    "    result['nll_surr_generic_sent'] = score_nll_query_prefix(\n",
    "        s['surr_generic_sent'] + \"\\n\" + passage, query, answer,\n",
    "        prefix_token_count=s['n_pfx_generic_sent'], truncate=True)\n",
    "\n",
    "    # --- Without query in decoder ---\n",
    "\n",
    "    # 9. bare_nq\n",
    "    result['nll_bare_nq'] = score_nll(passage, answer)\n",
    "\n",
    "    # 10. oracle_trunc_nq\n",
    "    result['nll_oracle_trunc_nq'] = score_nll(\n",
    "        query + \"\\n\" + passage, answer,\n",
    "        prefix_token_count=s['n_pfx_oracle'], truncate=True)\n",
    "\n",
    "    results.append(result)\n",
    "\n",
    "    if (i + 1) % 20 == 0 or i == N_SAMPLES - 1:\n",
    "        ckpt = {\n",
    "            'n_total': N_SAMPLES,\n",
    "            'results': results,\n",
    "            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        }\n",
    "        CHECKPOINT_PATH.write_text(json.dumps(ckpt))\n",
    "        elapsed = time.time() - t0\n",
    "        done = i - start_idx + 1\n",
    "        eta = (N_SAMPLES - i - 1) * elapsed / done if done > 0 else 0\n",
    "        tqdm.write(f\"  Checkpoint {i+1}/{N_SAMPLES} | {elapsed/60:.1f}m | ETA {eta/60:.1f}m\")\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "elapsed = time.time() - t0\n",
    "print(f\"\\nScoring complete: {len(results)} samples, \"\n",
    "      f\"{len(COND_NAMES)} conditions in {elapsed/60:.1f} min\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78202ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Results table\n",
    "print(\"=\" * 70)\n",
    "print(f\"RESULTS (N={len(results)})\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Extract NLL arrays\n",
    "bare = np.array([r['nll_bare'] for r in results])\n",
    "oracle_trunc = np.array([r['nll_oracle_trunc'] for r in results])\n",
    "kw10 = np.array([r['nll_surr_kw10'] for r in results])\n",
    "first_sent = np.array([r['nll_surr_first_sent'] for r in results])\n",
    "wrong_first_sent = np.array([r['nll_surr_wrong_first_sent'] for r in results])\n",
    "shuffled_sent = np.array([r['nll_surr_shuffled_sent'] for r in results])\n",
    "wrong_kw10 = np.array([r['nll_surr_wrong_kw10'] for r in results])\n",
    "generic_sent = np.array([r['nll_surr_generic_sent'] for r in results])\n",
    "bare_nq = np.array([r['nll_bare_nq'] for r in results])\n",
    "oracle_nq = np.array([r['nll_oracle_trunc_nq'] for r in results])\n",
    "\n",
    "N_BONF = 7  # 7 comparisons vs bare (excluding bare itself and _nq conditions)\n",
    "d_oracle = cohens_d(bare - oracle_trunc)\n",
    "\n",
    "print(f\"\\n--- With query in decoder ---\")\n",
    "print(f\"  Bonferroni: {N_BONF} comparisons\")\n",
    "print(f\"\\n  {'Condition':<24} {'NLL':>8} {'delta':>8} {'d':>8} {'Win%':>8} {'p':>12} {'sig':>5}\")\n",
    "print(f\"  {'-'*78}\")\n",
    "\n",
    "analysis = {}\n",
    "for name, nlls in [\n",
    "    ('bare', bare),\n",
    "    ('oracle_trunc', oracle_trunc),\n",
    "    ('surr_kw10', kw10),\n",
    "    ('surr_wrong_kw10', wrong_kw10),\n",
    "    ('surr_shuffled_sent', shuffled_sent),\n",
    "    ('surr_generic_sent', generic_sent),\n",
    "    ('surr_wrong_first_sent', wrong_first_sent),\n",
    "    ('surr_first_sent', first_sent),\n",
    "]:\n",
    "    mean_nll = nlls.mean()\n",
    "    if name == 'bare':\n",
    "        print(f\"  {name:<24} {mean_nll:>8.4f} {'--':>8} {'--':>8} {'--':>8} {'--':>12} {'--':>5}\")\n",
    "        analysis[name] = {'mean_nll': float(mean_nll)}\n",
    "    else:\n",
    "        diff = bare - nlls\n",
    "        d = cohens_d(diff)\n",
    "        win_pct = 100 * np.mean(diff > 0)\n",
    "        _, p_val = stats.ttest_1samp(diff, 0)\n",
    "        sig = '***' if p_val < 0.001/N_BONF else '**' if p_val < 0.01/N_BONF else '*' if p_val < 0.05/N_BONF else 'ns'\n",
    "        print(f\"  {name:<24} {mean_nll:>8.4f} {diff.mean():>+8.4f} {d:>+8.3f} {win_pct:>7.1f}% {p_val:>12.2e} {sig:>5}\")\n",
    "        analysis[name] = {\n",
    "            'mean_nll': float(mean_nll), 'delta': float(diff.mean()),\n",
    "            'd': float(d), 'win_pct': float(win_pct), 'p': float(p_val),\n",
    "        }\n",
    "\n",
    "# v3 replication\n",
    "diff_nq = bare_nq - oracle_nq\n",
    "d_nq = cohens_d(diff_nq)\n",
    "_, p_nq = stats.ttest_1samp(diff_nq, 0)\n",
    "sig_nq = '***' if p_nq < 0.001 else 'ns'\n",
    "print(f\"\\n--- v3 replication ---\")\n",
    "print(f\"  oracle_trunc_nq: d={d_nq:+.3f} ({sig_nq})\")\n",
    "\n",
    "analysis['bare_nq'] = {'mean_nll': float(bare_nq.mean())}\n",
    "analysis['oracle_trunc_nq'] = {\n",
    "    'mean_nll': float(oracle_nq.mean()), 'd': float(d_nq), 'p': float(p_nq),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f4ca46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Hypothesis testing — overlap vs coherence\n",
    "print(\"=\" * 70)\n",
    "print(\"HYPOTHESIS TESTING: What causes the truncation wound?\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "d_first = cohens_d(bare - first_sent)\n",
    "d_wrong_first = cohens_d(bare - wrong_first_sent)\n",
    "d_shuffled = cohens_d(bare - shuffled_sent)\n",
    "d_generic = cohens_d(bare - generic_sent)\n",
    "d_kw10_val = cohens_d(bare - kw10)\n",
    "d_wrong_kw10 = cohens_d(bare - wrong_kw10)\n",
    "\n",
    "# === Test 1: Overlap hypothesis ===\n",
    "# If overlap causes the wound, same-doc first_sent should hurt MORE than wrong-doc first_sent\n",
    "print(f\"\\n--- Test 1: Does overlap matter for sentences? ---\")\n",
    "print(f\"  first_sent (this doc):    d={d_first:+.3f}\")\n",
    "print(f\"  wrong_first_sent (other): d={d_wrong_first:+.3f}\")\n",
    "diff_overlap = wrong_first_sent - first_sent  # positive = this-doc is worse\n",
    "d_overlap = cohens_d(diff_overlap)\n",
    "_, p_overlap = stats.ttest_1samp(diff_overlap, 0)\n",
    "sig_overlap = '***' if p_overlap < 0.001 else '**' if p_overlap < 0.01 else '*' if p_overlap < 0.05 else 'ns'\n",
    "print(f\"  Overlap effect (same vs wrong doc): d={d_overlap:+.3f} ({sig_overlap})\")\n",
    "if d_overlap > 0.05:\n",
    "    print(f\"  -> Same-doc sentence is WORSE. Overlap amplifies the wound.\")\n",
    "elif d_overlap < -0.05:\n",
    "    print(f\"  -> Wrong-doc sentence is WORSE. Overlap is NOT the driver.\")\n",
    "else:\n",
    "    print(f\"  -> No significant overlap effect. Both sentences hurt equally.\")\n",
    "\n",
    "# === Test 2: Coherence hypothesis ===\n",
    "# If coherence causes the wound, coherent first_sent should hurt MORE than shuffled\n",
    "print(f\"\\n--- Test 2: Does coherence matter? ---\")\n",
    "print(f\"  first_sent (coherent):    d={d_first:+.3f}\")\n",
    "print(f\"  shuffled_sent (broken):   d={d_shuffled:+.3f}\")\n",
    "diff_coherence = shuffled_sent - first_sent  # positive = coherent is worse\n",
    "d_coherence = cohens_d(diff_coherence)\n",
    "_, p_coherence = stats.ttest_1samp(diff_coherence, 0)\n",
    "sig_coherence = '***' if p_coherence < 0.001 else '**' if p_coherence < 0.01 else '*' if p_coherence < 0.05 else 'ns'\n",
    "print(f\"  Coherence effect (coherent vs shuffled): d={d_coherence:+.3f} ({sig_coherence})\")\n",
    "if d_coherence > 0.05:\n",
    "    print(f\"  -> Coherent text is WORSE. Word order creates deeper attention dependencies.\")\n",
    "elif d_coherence < -0.05:\n",
    "    print(f\"  -> Shuffled is WORSE. Coherence is NOT the problem.\")\n",
    "else:\n",
    "    print(f\"  -> No coherence effect. The wound is not about word order.\")\n",
    "\n",
    "# === Test 3: Generic sentence ===\n",
    "print(f\"\\n--- Test 3: Does information content matter within coherent text? ---\")\n",
    "print(f\"  wrong_first_sent (informative): d={d_wrong_first:+.3f}\")\n",
    "print(f\"  generic_sent (zero info):       d={d_generic:+.3f}\")\n",
    "diff_info = generic_sent - wrong_first_sent  # positive = informative is worse\n",
    "d_info = cohens_d(diff_info)\n",
    "_, p_info = stats.ttest_1samp(diff_info, 0)\n",
    "sig_info = '***' if p_info < 0.001 else '**' if p_info < 0.01 else '*' if p_info < 0.05 else 'ns'\n",
    "print(f\"  Info content effect: d={d_info:+.3f} ({sig_info})\")\n",
    "\n",
    "# === Test 4: Keywords specificity (replicates Exp 04) ===\n",
    "print(f\"\\n--- Test 4: Keyword specificity (Exp 04 replication) ---\")\n",
    "print(f\"  kw10 (this doc):   d={d_kw10_val:+.3f}\")\n",
    "print(f\"  wrong_kw10 (other): d={d_wrong_kw10:+.3f}\")\n",
    "diff_kw_spec = wrong_kw10 - kw10  # positive = this-doc kw is better\n",
    "d_kw_spec = cohens_d(diff_kw_spec)\n",
    "_, p_kw_spec = stats.ttest_1samp(diff_kw_spec, 0)\n",
    "sig_kw_spec = '***' if p_kw_spec < 0.001 else '**' if p_kw_spec < 0.01 else '*' if p_kw_spec < 0.05 else 'ns'\n",
    "print(f\"  Specificity effect: d={d_kw_spec:+.3f} ({sig_kw_spec})\")\n",
    "\n",
    "# === Summary: 2x2 decomposition ===\n",
    "print(f\"\\n--- 2x2 Decomposition: Coherence x Overlap ---\")\n",
    "print(f\"  (using d vs bare as the outcome)\")\n",
    "print(f\"\")\n",
    "print(f\"  {'':>24} {'Same-doc':>12} {'Wrong-doc':>12} {'Difference':>12}\")\n",
    "print(f\"  {'Coherent sentence':<24} {d_first:>+12.3f} {d_wrong_first:>+12.3f} {d_first-d_wrong_first:>+12.3f}\")\n",
    "print(f\"  {'Shuffled/Keywords':<24} {d_shuffled:>+12.3f} {d_wrong_kw10:>+12.3f} {d_shuffled-d_wrong_kw10:>+12.3f}\")\n",
    "print(f\"  {'Difference':<24} {d_first-d_shuffled:>+12.3f} {d_wrong_first-d_wrong_kw10:>+12.3f}\")\n",
    "print(f\"\")\n",
    "print(f\"  Coherence main effect: {(d_first + d_wrong_first)/2 - (d_shuffled + d_wrong_kw10)/2:+.3f}\")\n",
    "print(f\"  Overlap main effect:   {(d_first + d_shuffled)/2 - (d_wrong_first + d_wrong_kw10)/2:+.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6fbc38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Is the wound just a length effect?\n",
    "print(\"=\" * 70)\n",
    "print(\"LENGTH CONFOUND CHECK\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Sentences are longer than keywords — is length the real driver?\n",
    "print(f\"\\nMean prefix tokens and d for each condition:\")\n",
    "for name, nlls, pfx_key in [\n",
    "    ('oracle_trunc', oracle_trunc, 'n_pfx_oracle'),\n",
    "    ('surr_kw10', kw10, 'n_pfx_kw10'),\n",
    "    ('surr_wrong_kw10', wrong_kw10, 'n_pfx_wrong_kw10'),\n",
    "    ('surr_shuffled_sent', shuffled_sent, 'n_pfx_shuffled_sent'),\n",
    "    ('surr_generic_sent', generic_sent, 'n_pfx_generic_sent'),\n",
    "    ('surr_wrong_first_sent', wrong_first_sent, 'n_pfx_wrong_first_sent'),\n",
    "    ('surr_first_sent', first_sent, 'n_pfx_first_sent'),\n",
    "]:\n",
    "    pfx = np.array([s[pfx_key] for s in samples])\n",
    "    d = cohens_d(bare - nlls)\n",
    "    print(f\"  {name:<24}: {pfx.mean():>5.1f} tokens, d={d:+.3f}\")\n",
    "\n",
    "# Key comparison: shuffled_sent has SAME length as first_sent (same words)\n",
    "print(f\"\\nCritical length-controlled comparison:\")\n",
    "pfx_first = np.array([s['n_pfx_first_sent'] for s in samples])\n",
    "pfx_shuffled = np.array([s['n_pfx_shuffled_sent'] for s in samples])\n",
    "print(f\"  first_sent tokens:   mean={pfx_first.mean():.1f}\")\n",
    "print(f\"  shuffled_sent tokens: mean={pfx_shuffled.mean():.1f}\")\n",
    "print(f\"  (Same words, same length — only word order differs)\")\n",
    "print(f\"  first_sent d:   {cohens_d(bare - first_sent):+.3f}\")\n",
    "print(f\"  shuffled_sent d: {cohens_d(bare - shuffled_sent):+.3f}\")\n",
    "print(f\"  -> Any difference is purely due to coherence, not length.\")\n",
    "\n",
    "# Within first_sent: does per-sample prefix length predict damage?\n",
    "pfx_len = np.array([s['n_pfx_first_sent'] for s in samples])\n",
    "damage = first_sent - bare  # positive = first_sent is worse\n",
    "r_len, p_len = stats.pearsonr(pfx_len, damage)\n",
    "print(f\"\\nWithin first_sent: correlation of prefix length with damage:\")\n",
    "print(f\"  r={r_len:.3f}, p={p_len:.3e}\")\n",
    "if r_len > 0.1:\n",
    "    print(f\"  -> Longer first sentences cause MORE damage.\")\n",
    "else:\n",
    "    print(f\"  -> Length does not predict damage magnitude.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740e4c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Verdict and save\n",
    "print(\"=\" * 70)\n",
    "print(\"VERDICT — Exp 05: Truncation Wound Mechanism\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "d_first = cohens_d(bare - first_sent)\n",
    "d_wrong_first = cohens_d(bare - wrong_first_sent)\n",
    "d_shuffled = cohens_d(bare - shuffled_sent)\n",
    "d_generic = cohens_d(bare - generic_sent)\n",
    "d_kw10_val = cohens_d(bare - kw10)\n",
    "\n",
    "print(f\"\\nModel: {MODEL_NAME}\")\n",
    "print(f\"N: {len(results)} (MS MARCO v1.1)\")\n",
    "\n",
    "print(f\"\\n--- Condition summary ---\")\n",
    "print(f\"  {'Condition':<24} {'d vs bare':>10} {'Interpretation'}\")\n",
    "print(f\"  {'-'*70}\")\n",
    "for name, d, interp in [\n",
    "    ('oracle_trunc', cohens_d(bare - oracle_trunc), 'Ceiling'),\n",
    "    ('surr_kw10 (this)', d_kw10_val, 'Best surrogate'),\n",
    "    ('surr_wrong_kw10', cohens_d(bare - wrong_kw10), 'Keywords without specificity'),\n",
    "    ('surr_shuffled_sent', d_shuffled, 'Overlap without coherence'),\n",
    "    ('surr_generic_sent', d_generic, 'Coherence without information'),\n",
    "    ('surr_wrong_first_sent', d_wrong_first, 'Coherence without overlap'),\n",
    "    ('surr_first_sent', d_first, 'Coherence + overlap (catastrophic)'),\n",
    "]:\n",
    "    print(f\"  {name:<24} {d:>+10.3f}   {interp}\")\n",
    "\n",
    "# Determine which hypothesis won\n",
    "overlap_effect = d_first - d_wrong_first\n",
    "coherence_effect = d_first - d_shuffled\n",
    "coherence_main = (d_first + d_wrong_first)/2 - (d_shuffled + cohens_d(bare - wrong_kw10))/2\n",
    "overlap_main = (d_first + d_shuffled)/2 - (d_wrong_first + cohens_d(bare - wrong_kw10))/2\n",
    "\n",
    "print(f\"\\n--- Hypothesis verdict ---\")\n",
    "print(f\"  Coherence main effect: {coherence_main:+.3f}\")\n",
    "print(f\"  Overlap main effect:   {overlap_main:+.3f}\")\n",
    "\n",
    "if abs(coherence_main) > abs(overlap_main) and coherence_main < -0.05:\n",
    "    print(f\"  -> H2 (COHERENCE) is the primary driver.\")\n",
    "    print(f\"     Coherent text creates deep attention dependencies that truncation disrupts.\")\n",
    "elif abs(overlap_main) > abs(coherence_main) and overlap_main < -0.05:\n",
    "    print(f\"  -> H1 (OVERLAP) is the primary driver.\")\n",
    "    print(f\"     Shared content creates extra-strong attention connections.\")\n",
    "elif coherence_main < -0.05 and overlap_main < -0.05:\n",
    "    print(f\"  -> BOTH coherence and overlap contribute to the wound.\")\n",
    "else:\n",
    "    print(f\"  -> Neither factor dominates clearly. The wound mechanism is more complex.\")\n",
    "\n",
    "# Save\n",
    "final_results = {\n",
    "    'experiment': 'v4_exp05_truncation_wound',\n",
    "    'model': MODEL_NAME,\n",
    "    'dataset': 'ms_marco_v1.1',\n",
    "    'n_samples': len(results),\n",
    "    'seed': SEED,\n",
    "    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'bonferroni': N_BONF,\n",
    "    'conditions': analysis,\n",
    "    'hypothesis_tests': {\n",
    "        'overlap_effect': float(overlap_effect),\n",
    "        'coherence_effect': float(coherence_effect),\n",
    "        'coherence_main_effect': float(coherence_main),\n",
    "        'overlap_main_effect': float(overlap_main),\n",
    "    },\n",
    "}\n",
    "\n",
    "with open(RESULTS_DIR / 'results.json', 'w') as f:\n",
    "    json.dump(final_results, f, indent=2)\n",
    "print(f\"\\nResults saved to {RESULTS_DIR / 'results.json'}\")\n",
    "\n",
    "# Cleanup\n",
    "print(f\"\\nCleaning up GPU memory...\")\n",
    "mem_before = torch.cuda.memory_allocated() / 1e9\n",
    "del model, processor, tokenizer\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "mem_after = torch.cuda.memory_allocated() / 1e9\n",
    "print(f\"GPU memory: {mem_before:.2f} GB -> {mem_after:.2f} GB\")\n",
    "print(\"Done!\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
