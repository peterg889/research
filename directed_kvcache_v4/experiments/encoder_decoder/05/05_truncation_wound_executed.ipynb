{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7bb9e467",
   "metadata": {},
   "source": [
    "# Experiment 05: Truncation Wound Mechanism\n",
    "\n",
    "## Motivation\n",
    "\n",
    "Exp 04 revealed a striking finding: the first sentence of the document used as an\n",
    "encoder prefix is **catastrophic** (d=-0.298 ***), while disconnected keywords help\n",
    "(kw10 d=+0.186). This is the \"truncation wound\" phenomenon — the prefix creates\n",
    "strong bidirectional attention connections during encoding, and masking those tokens\n",
    "from cross-attention leaves the document representations *worse* than bare encoding.\n",
    "\n",
    "But what specifically causes the wound? Two hypotheses:\n",
    "\n",
    "1. **Overlap hypothesis**: The first sentence shares vocabulary and semantics with the\n",
    "   document. This creates unusually strong attention connections during encoding. When\n",
    "   truncated, the \"wound\" is proportional to connection strength.\n",
    "\n",
    "2. **Coherence hypothesis**: Any coherent natural text creates deep attention dependencies\n",
    "   during encoding, regardless of overlap. Keywords work specifically because they're\n",
    "   disconnected tokens that perturb without creating deep dependencies.\n",
    "\n",
    "## Conditions (10 total)\n",
    "\n",
    "### Text type sweep (all with query in decoder + truncation):\n",
    "\n",
    "| # | Condition | Prefix | Tests |\n",
    "|---|-----------|--------|-------|\n",
    "| 1 | bare | (none) | Baseline |\n",
    "| 2 | oracle_trunc | real query | Ceiling |\n",
    "| 3 | surr_kw10 | top-10 kw from THIS doc | Best surrogate (Exp 04) |\n",
    "| 4 | surr_first_sent | first sent of THIS doc | Catastrophic in Exp 04 |\n",
    "| 5 | surr_wrong_first_sent | first sent of WRONG doc | Coherence without overlap |\n",
    "| 6 | surr_shuffled_sent | shuffled first sent of THIS doc | Overlap without coherence |\n",
    "| 7 | surr_wrong_kw10 | top-10 kw from WRONG doc | Keywords without specificity |\n",
    "| 8 | surr_generic_sent | fixed generic sentence | Coherent, no overlap, no info |\n",
    "\n",
    "### Controls:\n",
    "\n",
    "| # | Condition | Purpose |\n",
    "|---|-----------|---------|\n",
    "| 9 | bare_nq | v3 baseline |\n",
    "| 10 | oracle_trunc_nq | v3 replication |\n",
    "\n",
    "## Key comparisons\n",
    "\n",
    "- **(4) vs (5)**: Same-doc vs wrong-doc first sentence → isolates **overlap**\n",
    "- **(4) vs (6)**: Coherent vs shuffled same-doc first sentence → isolates **coherence**\n",
    "- **(5) vs (8)**: Wrong-doc sentence vs generic sentence → information content within coherent text\n",
    "- **(3) vs (7)**: Same-doc vs wrong-doc keywords → replicates Exp 04 specificity test\n",
    "- **(6) vs (3)**: Shuffled sentence (long) vs keywords (short) → length/format comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "752bfdb9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T21:00:50.329239Z",
     "iopub.status.busy": "2026-02-19T21:00:50.328993Z",
     "iopub.status.idle": "2026-02-19T21:01:11.755549Z",
     "shell.execute_reply": "2026-02-19T21:01:11.754771Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading google/t5gemma-2-4b-4b...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a4b365a45774ae99d5873eec1f3d6a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/1327 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exp 05: Truncation Wound Mechanism\n",
      "N: 500, Model: google/t5gemma-2-4b-4b\n",
      "DEVICE: cuda:0, dtype: torch.bfloat16\n",
      "GPU memory: 15.02 GB\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Setup and model loading\n",
    "import os\n",
    "os.umask(0o000)\n",
    "\n",
    "import sys, json, time, gc, re\n",
    "import random as pyrandom\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from scipy import stats\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "sys.path.insert(0, \"../../..\")\n",
    "from lib.analysis import cohens_d\n",
    "\n",
    "SEED = 42\n",
    "N_SAMPLES = 500\n",
    "MODEL_NAME = \"google/t5gemma-2-4b-4b\"\n",
    "\n",
    "RESULTS_DIR = Path(\"../../../results/exp05\")\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CHECKPOINT_PATH = RESULTS_DIR / \"checkpoint.json\"\n",
    "\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "pyrandom.seed(SEED)\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv())\n",
    "HF_TOKEN = os.environ.get(\"HF_TOKEN\")\n",
    "\n",
    "from transformers import AutoProcessor, AutoModelForSeq2SeqLM\n",
    "\n",
    "print(f\"Loading {MODEL_NAME}...\")\n",
    "processor = AutoProcessor.from_pretrained(MODEL_NAME, token=HF_TOKEN)\n",
    "tokenizer = processor.tokenizer\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    MODEL_NAME, device_map=\"auto\", torch_dtype=torch.bfloat16, token=HF_TOKEN,\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "DEVICE = next(model.parameters()).device\n",
    "BOS_ID = getattr(model.config, 'decoder_start_token_id', None) or tokenizer.bos_token_id\n",
    "\n",
    "print(f\"Exp 05: Truncation Wound Mechanism\")\n",
    "print(f\"N: {N_SAMPLES}, Model: {MODEL_NAME}\")\n",
    "print(f\"DEVICE: {DEVICE}, dtype: {next(model.parameters()).dtype}\")\n",
    "print(f\"GPU memory: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5ecdca2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T21:01:11.759695Z",
     "iopub.status.busy": "2026-02-19T21:01:11.759127Z",
     "iopub.status.idle": "2026-02-19T21:01:11.780845Z",
     "shell.execute_reply": "2026-02-19T21:01:11.780198Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scoring functions defined.\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Scoring helpers\n",
    "\n",
    "def count_prefix_tokens(prefix_text, document_text):\n",
    "    full_text = prefix_text + \"\\n\" + document_text\n",
    "    full_ids = tokenizer(full_text, add_special_tokens=True, truncation=True,\n",
    "                         max_length=2048).input_ids\n",
    "    doc_ids = tokenizer(document_text, add_special_tokens=True, truncation=True,\n",
    "                        max_length=2048).input_ids\n",
    "    return len(full_ids) - len(doc_ids)\n",
    "\n",
    "\n",
    "def score_nll(encoder_text, answer_text, prefix_token_count=0, truncate=False):\n",
    "    # No query in decoder — used for _nq conditions.\n",
    "    enc_ids = tokenizer(encoder_text, return_tensors=\"pt\",\n",
    "                        add_special_tokens=True, truncation=True,\n",
    "                        max_length=2048).input_ids.to(DEVICE)\n",
    "    total_enc_len = enc_ids.shape[1]\n",
    "    enc_mask = torch.ones(1, total_enc_len, device=DEVICE, dtype=torch.long)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        encoder_outputs = model.get_encoder()(\n",
    "            input_ids=enc_ids, attention_mask=enc_mask\n",
    "        )\n",
    "\n",
    "    if truncate and prefix_token_count > 0:\n",
    "        cross_attn_mask = torch.ones(1, total_enc_len, device=DEVICE, dtype=torch.long)\n",
    "        cross_attn_mask[:, :prefix_token_count] = 0\n",
    "    else:\n",
    "        cross_attn_mask = torch.ones(1, total_enc_len, device=DEVICE, dtype=torch.long)\n",
    "\n",
    "    ans_ids = tokenizer(answer_text, return_tensors=\"pt\",\n",
    "                        add_special_tokens=False, truncation=True,\n",
    "                        max_length=256).input_ids.to(DEVICE)\n",
    "\n",
    "    if ans_ids.shape[1] == 0:\n",
    "        del encoder_outputs\n",
    "        return 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            encoder_outputs=encoder_outputs,\n",
    "            attention_mask=cross_attn_mask,\n",
    "            labels=ans_ids,\n",
    "        )\n",
    "\n",
    "    logits = outputs.logits\n",
    "    log_probs = F.log_softmax(logits, dim=-1)\n",
    "    token_log_probs = log_probs[0].gather(1, ans_ids[0].unsqueeze(1)).squeeze(1)\n",
    "    mean_nll = -token_log_probs.mean().item()\n",
    "\n",
    "    del encoder_outputs, outputs, logits, log_probs\n",
    "    return mean_nll\n",
    "\n",
    "\n",
    "def score_nll_query_prefix(encoder_text, query_text, answer_text,\n",
    "                           prefix_token_count=0, truncate=False):\n",
    "    # Query as decoder prefix — production-realistic.\n",
    "    enc_ids = tokenizer(encoder_text, return_tensors=\"pt\",\n",
    "                        add_special_tokens=True, truncation=True,\n",
    "                        max_length=2048).input_ids.to(DEVICE)\n",
    "    total_enc_len = enc_ids.shape[1]\n",
    "    enc_mask = torch.ones(1, total_enc_len, device=DEVICE, dtype=torch.long)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        encoder_outputs = model.get_encoder()(\n",
    "            input_ids=enc_ids, attention_mask=enc_mask\n",
    "        )\n",
    "\n",
    "    if truncate and prefix_token_count > 0:\n",
    "        cross_attn_mask = torch.ones(1, total_enc_len, device=DEVICE, dtype=torch.long)\n",
    "        cross_attn_mask[:, :prefix_token_count] = 0\n",
    "    else:\n",
    "        cross_attn_mask = torch.ones(1, total_enc_len, device=DEVICE, dtype=torch.long)\n",
    "\n",
    "    query_ids = tokenizer(query_text, add_special_tokens=False, truncation=True,\n",
    "                          max_length=512).input_ids\n",
    "    answer_ids = tokenizer(answer_text, add_special_tokens=False, truncation=True,\n",
    "                           max_length=256).input_ids\n",
    "\n",
    "    if len(answer_ids) == 0:\n",
    "        del encoder_outputs\n",
    "        return 0.0\n",
    "\n",
    "    dec_ids = [BOS_ID] + query_ids + answer_ids\n",
    "    dec_tensor = torch.tensor([dec_ids], dtype=torch.long, device=DEVICE)\n",
    "\n",
    "    n_query = len(query_ids)\n",
    "    n_answer = len(answer_ids)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            encoder_outputs=encoder_outputs,\n",
    "            attention_mask=cross_attn_mask,\n",
    "            decoder_input_ids=dec_tensor,\n",
    "        )\n",
    "\n",
    "    logits = outputs.logits\n",
    "    answer_logits = logits[0, n_query:n_query + n_answer, :]\n",
    "\n",
    "    answer_targets = torch.tensor(answer_ids, dtype=torch.long, device=DEVICE)\n",
    "    log_probs = F.log_softmax(answer_logits, dim=-1)\n",
    "    token_log_probs = log_probs.gather(1, answer_targets.unsqueeze(1)).squeeze(1)\n",
    "    mean_nll = -token_log_probs.mean().item()\n",
    "\n",
    "    del encoder_outputs, outputs, logits, log_probs\n",
    "    return mean_nll\n",
    "\n",
    "\n",
    "# === Surrogate helpers ===\n",
    "STOP_WORDS = {\n",
    "    'a', 'an', 'the', 'is', 'are', 'was', 'were', 'be', 'been', 'being',\n",
    "    'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could',\n",
    "    'should', 'may', 'might', 'can', 'shall', 'to', 'of', 'in', 'for',\n",
    "    'on', 'with', 'at', 'by', 'from', 'as', 'into', 'through', 'during',\n",
    "    'before', 'after', 'above', 'below', 'between', 'and', 'but', 'or',\n",
    "    'not', 'no', 'if', 'then', 'than', 'so', 'up', 'out', 'about',\n",
    "    'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those',\n",
    "    'it', 'its', 'i', 'me', 'my', 'we', 'our', 'you', 'your', 'he',\n",
    "    'him', 'his', 'she', 'her', 'they', 'them', 'their', 'how', 'when',\n",
    "    'where', 'why', 'much', 'many', 'some', 'any', 'all', 'each',\n",
    "    'does', 'also', 'just', 'more', 'most', 'very', 'too', 'only',\n",
    "}\n",
    "\n",
    "def extract_keywords(text):\n",
    "    words = re.sub(r'[^\\w\\s]', '', text.lower()).split()\n",
    "    return [w for w in words if w not in STOP_WORDS and len(w) > 2]\n",
    "\n",
    "def make_kw_surrogate(passage, n_keywords):\n",
    "    content_words = extract_keywords(passage)\n",
    "    if not content_words:\n",
    "        return \"information\"\n",
    "    counts = Counter(content_words)\n",
    "    return \" \".join(w for w, _ in counts.most_common(n_keywords))\n",
    "\n",
    "def get_first_sentence(text):\n",
    "    parts = re.split(r'(?<=[.!?])\\s+', text.strip())\n",
    "    if parts:\n",
    "        return parts[0]\n",
    "    return text[:100]\n",
    "\n",
    "def shuffle_sentence(text):\n",
    "    # Shuffle words in a sentence, preserving the word set but breaking coherence.\n",
    "    words = text.split()\n",
    "    pyrandom.shuffle(words)\n",
    "    return \" \".join(words)\n",
    "\n",
    "# Fixed generic sentence — coherent but zero information about any document.\n",
    "GENERIC_SENTENCE = \"The following passage contains relevant information about the topic.\"\n",
    "\n",
    "print(\"Scoring functions defined.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd6f7653",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T21:01:11.783952Z",
     "iopub.status.busy": "2026-02-19T21:01:11.783687Z",
     "iopub.status.idle": "2026-02-19T21:01:15.719849Z",
     "shell.execute_reply": "2026-02-19T21:01:15.719131Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MS MARCO v1.1 validation...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total candidates: 1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loaded 500 samples\n",
      "\n",
      "Prefix token counts (mean):\n",
      "  oracle (query)        : mean=7.5, min=3, max=16\n",
      "  kw10 (this doc)       : mean=13.6, min=11, max=32\n",
      "  first_sent (this)     : mean=22.1, min=3, max=125\n",
      "  wrong_first_sent      : mean=22.1, min=3, max=125\n",
      "  shuffled_sent         : mean=22.1, min=3, max=125\n",
      "  wrong_kw10            : mean=13.6, min=11, max=32\n",
      "  generic_sent          : mean=11.0, min=11, max=11\n",
      "\n",
      "First sample:\n",
      "  Query:             what is the link between alveoli and capillaries\n",
      "  kw10:              alveoli gas partial pressure exchange blood capillary network air pulmonary\n",
      "  first_sent:        Gas exchange in the lungs takes place between the blood in the capillary network\n",
      "  wrong_first_sent:  You are here Donair History.\n",
      "  shuffled_sent:     the takes the itself. network in and the surrounding alveoli, exchange capillary\n",
      "  wrong_kw10:        donair history either kebabs halifax here donairsin pastare traditionally greek\n",
      "  generic_sent:      The following passage contains relevant information about the topic.\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Load MS MARCO data and generate all prefix variants\n",
    "from lib.data import count_words\n",
    "from datasets import load_dataset\n",
    "\n",
    "print(\"Loading MS MARCO v1.1 validation...\")\n",
    "ds = load_dataset(\"microsoft/ms_marco\", \"v1.1\", split=\"validation\")\n",
    "\n",
    "all_candidates = []\n",
    "for item in ds:\n",
    "    if len(all_candidates) >= 3 * N_SAMPLES:\n",
    "        break\n",
    "    passages = item.get('passages', {})\n",
    "    ptexts = passages.get('passage_text', [])\n",
    "    is_sel = passages.get('is_selected', [])\n",
    "    query = item.get('query', '')\n",
    "    answers = item.get('answers', [])\n",
    "    well_formed = item.get('wellFormedAnswers', [])\n",
    "    answer = None\n",
    "    if well_formed and len(well_formed) > 0 and well_formed[0] not in ('[]', ''):\n",
    "        answer = well_formed[0]\n",
    "    elif answers and len(answers) > 0 and answers[0] != 'No Answer Present.':\n",
    "        answer = answers[0]\n",
    "    if not answer:\n",
    "        continue\n",
    "    for pt, sel in zip(ptexts, is_sel):\n",
    "        wc = count_words(pt)\n",
    "        if sel == 1 and 30 <= wc <= 300:\n",
    "            all_candidates.append({\n",
    "                'passage': pt, 'query': query, 'answer': answer,\n",
    "                'word_count': wc,\n",
    "            })\n",
    "            break\n",
    "\n",
    "print(f\"Total candidates: {len(all_candidates)}\")\n",
    "np.random.seed(SEED)\n",
    "indices = np.random.permutation(len(all_candidates))\n",
    "samples = [all_candidates[i] for i in indices[:N_SAMPLES]]\n",
    "del ds, all_candidates\n",
    "gc.collect()\n",
    "\n",
    "# Generate all prefix variants\n",
    "pyrandom.seed(SEED)\n",
    "for i, s in enumerate(samples):\n",
    "    passage = s['passage']\n",
    "    wrong_idx = (i + N_SAMPLES // 2) % len(samples)\n",
    "\n",
    "    # Keywords from THIS document (best surrogate from Exp 04)\n",
    "    s['surr_kw10'] = make_kw_surrogate(passage, 10)\n",
    "\n",
    "    # First sentence of THIS document (catastrophic in Exp 04)\n",
    "    s['surr_first_sent'] = get_first_sentence(passage)\n",
    "\n",
    "    # First sentence of WRONG document (coherent, non-overlapping)\n",
    "    s['surr_wrong_first_sent'] = get_first_sentence(samples[wrong_idx]['passage'])\n",
    "\n",
    "    # Shuffled first sentence of THIS document (overlap without coherence)\n",
    "    s['surr_shuffled_sent'] = shuffle_sentence(get_first_sentence(passage))\n",
    "\n",
    "    # Keywords from WRONG document (replicates Exp 04 control)\n",
    "    s['surr_wrong_kw10'] = make_kw_surrogate(samples[wrong_idx]['passage'], 10)\n",
    "\n",
    "    # Generic sentence (coherent, zero information)\n",
    "    s['surr_generic_sent'] = GENERIC_SENTENCE\n",
    "\n",
    "    # Count prefix tokens for each\n",
    "    s['n_pfx_oracle'] = count_prefix_tokens(s['query'], passage)\n",
    "    s['n_pfx_kw10'] = count_prefix_tokens(s['surr_kw10'], passage)\n",
    "    s['n_pfx_first_sent'] = count_prefix_tokens(s['surr_first_sent'], passage)\n",
    "    s['n_pfx_wrong_first_sent'] = count_prefix_tokens(s['surr_wrong_first_sent'], passage)\n",
    "    s['n_pfx_shuffled_sent'] = count_prefix_tokens(s['surr_shuffled_sent'], passage)\n",
    "    s['n_pfx_wrong_kw10'] = count_prefix_tokens(s['surr_wrong_kw10'], passage)\n",
    "    s['n_pfx_generic_sent'] = count_prefix_tokens(s['surr_generic_sent'], passage)\n",
    "\n",
    "# Statistics\n",
    "print(f\"\\nLoaded {len(samples)} samples\")\n",
    "\n",
    "print(f\"\\nPrefix token counts (mean):\")\n",
    "for key, label in [\n",
    "    ('n_pfx_oracle', 'oracle (query)'),\n",
    "    ('n_pfx_kw10', 'kw10 (this doc)'),\n",
    "    ('n_pfx_first_sent', 'first_sent (this)'),\n",
    "    ('n_pfx_wrong_first_sent', 'wrong_first_sent'),\n",
    "    ('n_pfx_shuffled_sent', 'shuffled_sent'),\n",
    "    ('n_pfx_wrong_kw10', 'wrong_kw10'),\n",
    "    ('n_pfx_generic_sent', 'generic_sent'),\n",
    "]:\n",
    "    vals = [s[key] for s in samples]\n",
    "    print(f\"  {label:<22}: mean={np.mean(vals):.1f}, min={np.min(vals)}, max={np.max(vals)}\")\n",
    "\n",
    "print(f\"\\nFirst sample:\")\n",
    "print(f\"  Query:             {samples[0]['query']}\")\n",
    "print(f\"  kw10:              {samples[0]['surr_kw10']}\")\n",
    "print(f\"  first_sent:        {samples[0]['surr_first_sent'][:80]}\")\n",
    "print(f\"  wrong_first_sent:  {samples[0]['surr_wrong_first_sent'][:80]}\")\n",
    "print(f\"  shuffled_sent:     {samples[0]['surr_shuffled_sent'][:80]}\")\n",
    "print(f\"  wrong_kw10:        {samples[0]['surr_wrong_kw10']}\")\n",
    "print(f\"  generic_sent:      {samples[0]['surr_generic_sent']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e55c964",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T21:01:15.724350Z",
     "iopub.status.busy": "2026-02-19T21:01:15.723879Z",
     "iopub.status.idle": "2026-02-19T21:18:33.228590Z",
     "shell.execute_reply": "2026-02-19T21:18:33.227921Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "SCORING ALL CONDITIONS\n",
      "======================================================================\n",
      "Starting fresh: 10 conditions x 500 samples = 5000 forward passes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fad632879b640fe929298b120fbb301",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Scoring:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 20/500 | 0.7m | ETA 17.3m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 40/500 | 1.4m | ETA 16.3m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 60/500 | 2.1m | ETA 15.5m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 80/500 | 2.8m | ETA 14.7m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 100/500 | 3.5m | ETA 14.0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 120/500 | 4.2m | ETA 13.2m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 140/500 | 4.9m | ETA 12.5m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 160/500 | 5.6m | ETA 11.8m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 180/500 | 6.3m | ETA 11.1m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 200/500 | 6.9m | ETA 10.4m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 220/500 | 7.6m | ETA 9.7m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 240/500 | 8.3m | ETA 9.0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 260/500 | 9.0m | ETA 8.3m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 280/500 | 9.7m | ETA 7.6m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 300/500 | 10.4m | ETA 6.9m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 320/500 | 11.1m | ETA 6.2m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 340/500 | 11.8m | ETA 5.5m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 360/500 | 12.5m | ETA 4.8m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 380/500 | 13.2m | ETA 4.2m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 400/500 | 13.8m | ETA 3.5m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 420/500 | 14.5m | ETA 2.8m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 440/500 | 15.2m | ETA 2.1m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 460/500 | 15.9m | ETA 1.4m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 480/500 | 16.6m | ETA 0.7m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 500/500 | 17.3m | ETA 0.0m\n",
      "\n",
      "Scoring complete: 500 samples, 10 conditions in 17.3 min\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Scoring loop — 10 conditions x 500 samples\n",
    "print(\"=\" * 70)\n",
    "print(\"SCORING ALL CONDITIONS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "COND_NAMES = [\n",
    "    'bare', 'oracle_trunc', 'surr_kw10',\n",
    "    'surr_first_sent', 'surr_wrong_first_sent', 'surr_shuffled_sent',\n",
    "    'surr_wrong_kw10', 'surr_generic_sent',\n",
    "    'bare_nq', 'oracle_trunc_nq',\n",
    "]\n",
    "\n",
    "results = []\n",
    "start_idx = 0\n",
    "\n",
    "if CHECKPOINT_PATH.exists():\n",
    "    ckpt = json.loads(CHECKPOINT_PATH.read_text())\n",
    "    if ckpt.get('n_total') == N_SAMPLES and len(ckpt.get('results', [])) > 0:\n",
    "        saved_queries = [r['query'][:50] for r in ckpt['results']]\n",
    "        current_queries = [s['query'][:50] for s in samples[:len(saved_queries)]]\n",
    "        if saved_queries == current_queries:\n",
    "            results = ckpt['results']\n",
    "            start_idx = len(results)\n",
    "            print(f\"Resuming from checkpoint: {start_idx}/{N_SAMPLES}\")\n",
    "\n",
    "if start_idx == 0:\n",
    "    print(f\"Starting fresh: {len(COND_NAMES)} conditions x {N_SAMPLES} samples \"\n",
    "          f\"= {len(COND_NAMES) * N_SAMPLES} forward passes\")\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "for i in tqdm(range(start_idx, N_SAMPLES), initial=start_idx, total=N_SAMPLES,\n",
    "              desc=\"Scoring\"):\n",
    "    s = samples[i]\n",
    "    query = s['query']\n",
    "    passage = s['passage']\n",
    "    answer = s['answer']\n",
    "\n",
    "    result = {\n",
    "        'query': query,\n",
    "        'answer': answer,\n",
    "        'passage_words': s['word_count'],\n",
    "    }\n",
    "\n",
    "    # --- With query in decoder ---\n",
    "\n",
    "    # 1. bare\n",
    "    result['nll_bare'] = score_nll_query_prefix(passage, query, answer)\n",
    "\n",
    "    # 2. oracle_trunc\n",
    "    result['nll_oracle_trunc'] = score_nll_query_prefix(\n",
    "        query + \"\\n\" + passage, query, answer,\n",
    "        prefix_token_count=s['n_pfx_oracle'], truncate=True)\n",
    "\n",
    "    # 3. surr_kw10 (this doc — best surrogate from Exp 04)\n",
    "    result['nll_surr_kw10'] = score_nll_query_prefix(\n",
    "        s['surr_kw10'] + \"\\n\" + passage, query, answer,\n",
    "        prefix_token_count=s['n_pfx_kw10'], truncate=True)\n",
    "\n",
    "    # 4. surr_first_sent (this doc — catastrophic in Exp 04)\n",
    "    result['nll_surr_first_sent'] = score_nll_query_prefix(\n",
    "        s['surr_first_sent'] + \"\\n\" + passage, query, answer,\n",
    "        prefix_token_count=s['n_pfx_first_sent'], truncate=True)\n",
    "\n",
    "    # 5. surr_wrong_first_sent (wrong doc — coherent, non-overlapping)\n",
    "    result['nll_surr_wrong_first_sent'] = score_nll_query_prefix(\n",
    "        s['surr_wrong_first_sent'] + \"\\n\" + passage, query, answer,\n",
    "        prefix_token_count=s['n_pfx_wrong_first_sent'], truncate=True)\n",
    "\n",
    "    # 6. surr_shuffled_sent (this doc shuffled — overlap without coherence)\n",
    "    result['nll_surr_shuffled_sent'] = score_nll_query_prefix(\n",
    "        s['surr_shuffled_sent'] + \"\\n\" + passage, query, answer,\n",
    "        prefix_token_count=s['n_pfx_shuffled_sent'], truncate=True)\n",
    "\n",
    "    # 7. surr_wrong_kw10 (wrong doc keywords)\n",
    "    result['nll_surr_wrong_kw10'] = score_nll_query_prefix(\n",
    "        s['surr_wrong_kw10'] + \"\\n\" + passage, query, answer,\n",
    "        prefix_token_count=s['n_pfx_wrong_kw10'], truncate=True)\n",
    "\n",
    "    # 8. surr_generic_sent (fixed generic sentence)\n",
    "    result['nll_surr_generic_sent'] = score_nll_query_prefix(\n",
    "        s['surr_generic_sent'] + \"\\n\" + passage, query, answer,\n",
    "        prefix_token_count=s['n_pfx_generic_sent'], truncate=True)\n",
    "\n",
    "    # --- Without query in decoder ---\n",
    "\n",
    "    # 9. bare_nq\n",
    "    result['nll_bare_nq'] = score_nll(passage, answer)\n",
    "\n",
    "    # 10. oracle_trunc_nq\n",
    "    result['nll_oracle_trunc_nq'] = score_nll(\n",
    "        query + \"\\n\" + passage, answer,\n",
    "        prefix_token_count=s['n_pfx_oracle'], truncate=True)\n",
    "\n",
    "    results.append(result)\n",
    "\n",
    "    if (i + 1) % 20 == 0 or i == N_SAMPLES - 1:\n",
    "        ckpt = {\n",
    "            'n_total': N_SAMPLES,\n",
    "            'results': results,\n",
    "            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        }\n",
    "        CHECKPOINT_PATH.write_text(json.dumps(ckpt))\n",
    "        elapsed = time.time() - t0\n",
    "        done = i - start_idx + 1\n",
    "        eta = (N_SAMPLES - i - 1) * elapsed / done if done > 0 else 0\n",
    "        tqdm.write(f\"  Checkpoint {i+1}/{N_SAMPLES} | {elapsed/60:.1f}m | ETA {eta/60:.1f}m\")\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "elapsed = time.time() - t0\n",
    "print(f\"\\nScoring complete: {len(results)} samples, \"\n",
    "      f\"{len(COND_NAMES)} conditions in {elapsed/60:.1f} min\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a78202ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T21:18:33.231769Z",
     "iopub.status.busy": "2026-02-19T21:18:33.231482Z",
     "iopub.status.idle": "2026-02-19T21:18:33.252855Z",
     "shell.execute_reply": "2026-02-19T21:18:33.252249Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "RESULTS (N=500)\n",
      "======================================================================\n",
      "\n",
      "--- With query in decoder ---\n",
      "  Bonferroni: 7 comparisons\n",
      "\n",
      "  Condition                     NLL    delta        d     Win%            p   sig\n",
      "  ------------------------------------------------------------------------------\n",
      "  bare                       2.5544       --       --       --           --    --\n",
      "  oracle_trunc               2.4061  +0.1482   +0.228    67.8%     5.15e-07   ***\n",
      "  surr_kw10                  2.4588  +0.0956   +0.186    65.8%     3.78e-05   ***\n",
      "  surr_wrong_kw10            2.4369  +0.1174   +0.131    62.4%     3.63e-03     *\n",
      "  surr_shuffled_sent         2.4796  +0.0748   +0.078    65.0%     8.07e-02    ns\n",
      "  surr_generic_sent          2.6102  -0.0558   -0.062    50.8%     1.68e-01    ns\n",
      "  surr_wrong_first_sent      2.4915  +0.0628   +0.063    59.0%     1.60e-01    ns\n",
      "  surr_first_sent            2.8182  -0.2639   -0.298    44.4%     6.67e-11   ***\n",
      "\n",
      "--- v3 replication ---\n",
      "  oracle_trunc_nq: d=+0.376 (***)\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Results table\n",
    "print(\"=\" * 70)\n",
    "print(f\"RESULTS (N={len(results)})\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Extract NLL arrays\n",
    "bare = np.array([r['nll_bare'] for r in results])\n",
    "oracle_trunc = np.array([r['nll_oracle_trunc'] for r in results])\n",
    "kw10 = np.array([r['nll_surr_kw10'] for r in results])\n",
    "first_sent = np.array([r['nll_surr_first_sent'] for r in results])\n",
    "wrong_first_sent = np.array([r['nll_surr_wrong_first_sent'] for r in results])\n",
    "shuffled_sent = np.array([r['nll_surr_shuffled_sent'] for r in results])\n",
    "wrong_kw10 = np.array([r['nll_surr_wrong_kw10'] for r in results])\n",
    "generic_sent = np.array([r['nll_surr_generic_sent'] for r in results])\n",
    "bare_nq = np.array([r['nll_bare_nq'] for r in results])\n",
    "oracle_nq = np.array([r['nll_oracle_trunc_nq'] for r in results])\n",
    "\n",
    "N_BONF = 7  # 7 comparisons vs bare (excluding bare itself and _nq conditions)\n",
    "d_oracle = cohens_d(bare - oracle_trunc)\n",
    "\n",
    "print(f\"\\n--- With query in decoder ---\")\n",
    "print(f\"  Bonferroni: {N_BONF} comparisons\")\n",
    "print(f\"\\n  {'Condition':<24} {'NLL':>8} {'delta':>8} {'d':>8} {'Win%':>8} {'p':>12} {'sig':>5}\")\n",
    "print(f\"  {'-'*78}\")\n",
    "\n",
    "analysis = {}\n",
    "for name, nlls in [\n",
    "    ('bare', bare),\n",
    "    ('oracle_trunc', oracle_trunc),\n",
    "    ('surr_kw10', kw10),\n",
    "    ('surr_wrong_kw10', wrong_kw10),\n",
    "    ('surr_shuffled_sent', shuffled_sent),\n",
    "    ('surr_generic_sent', generic_sent),\n",
    "    ('surr_wrong_first_sent', wrong_first_sent),\n",
    "    ('surr_first_sent', first_sent),\n",
    "]:\n",
    "    mean_nll = nlls.mean()\n",
    "    if name == 'bare':\n",
    "        print(f\"  {name:<24} {mean_nll:>8.4f} {'--':>8} {'--':>8} {'--':>8} {'--':>12} {'--':>5}\")\n",
    "        analysis[name] = {'mean_nll': float(mean_nll)}\n",
    "    else:\n",
    "        diff = bare - nlls\n",
    "        d = cohens_d(diff)\n",
    "        win_pct = 100 * np.mean(diff > 0)\n",
    "        _, p_val = stats.ttest_1samp(diff, 0)\n",
    "        sig = '***' if p_val < 0.001/N_BONF else '**' if p_val < 0.01/N_BONF else '*' if p_val < 0.05/N_BONF else 'ns'\n",
    "        print(f\"  {name:<24} {mean_nll:>8.4f} {diff.mean():>+8.4f} {d:>+8.3f} {win_pct:>7.1f}% {p_val:>12.2e} {sig:>5}\")\n",
    "        analysis[name] = {\n",
    "            'mean_nll': float(mean_nll), 'delta': float(diff.mean()),\n",
    "            'd': float(d), 'win_pct': float(win_pct), 'p': float(p_val),\n",
    "        }\n",
    "\n",
    "# v3 replication\n",
    "diff_nq = bare_nq - oracle_nq\n",
    "d_nq = cohens_d(diff_nq)\n",
    "_, p_nq = stats.ttest_1samp(diff_nq, 0)\n",
    "sig_nq = '***' if p_nq < 0.001 else 'ns'\n",
    "print(f\"\\n--- v3 replication ---\")\n",
    "print(f\"  oracle_trunc_nq: d={d_nq:+.3f} ({sig_nq})\")\n",
    "\n",
    "analysis['bare_nq'] = {'mean_nll': float(bare_nq.mean())}\n",
    "analysis['oracle_trunc_nq'] = {\n",
    "    'mean_nll': float(oracle_nq.mean()), 'd': float(d_nq), 'p': float(p_nq),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80f4ca46",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T21:18:33.255833Z",
     "iopub.status.busy": "2026-02-19T21:18:33.255581Z",
     "iopub.status.idle": "2026-02-19T21:18:33.272094Z",
     "shell.execute_reply": "2026-02-19T21:18:33.271464Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "HYPOTHESIS TESTING: What causes the truncation wound?\n",
      "======================================================================\n",
      "\n",
      "--- Test 1: Does overlap matter for sentences? ---\n",
      "  first_sent (this doc):    d=-0.298\n",
      "  wrong_first_sent (other): d=+0.063\n",
      "  Overlap effect (same vs wrong doc): d=-0.274 (***)\n",
      "  -> Wrong-doc sentence is WORSE. Overlap is NOT the driver.\n",
      "\n",
      "--- Test 2: Does coherence matter? ---\n",
      "  first_sent (coherent):    d=-0.298\n",
      "  shuffled_sent (broken):   d=+0.078\n",
      "  Coherence effect (coherent vs shuffled): d=-0.293 (***)\n",
      "  -> Shuffled is WORSE. Coherence is NOT the problem.\n",
      "\n",
      "--- Test 3: Does information content matter within coherent text? ---\n",
      "  wrong_first_sent (informative): d=+0.063\n",
      "  generic_sent (zero info):       d=-0.062\n",
      "  Info content effect: d=+0.163 (***)\n",
      "\n",
      "--- Test 4: Keyword specificity (Exp 04 replication) ---\n",
      "  kw10 (this doc):   d=+0.186\n",
      "  wrong_kw10 (other): d=+0.131\n",
      "  Specificity effect: d=-0.030 (ns)\n",
      "\n",
      "--- 2x2 Decomposition: Coherence x Overlap ---\n",
      "  (using d vs bare as the outcome)\n",
      "\n",
      "                               Same-doc    Wrong-doc   Difference\n",
      "  Coherent sentence              -0.298       +0.063       -0.361\n",
      "  Shuffled/Keywords              +0.078       +0.131       -0.052\n",
      "  Difference                     -0.377       -0.068\n",
      "\n",
      "  Coherence main effect: -0.222\n",
      "  Overlap main effect:   -0.207\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Hypothesis testing — overlap vs coherence\n",
    "print(\"=\" * 70)\n",
    "print(\"HYPOTHESIS TESTING: What causes the truncation wound?\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "d_first = cohens_d(bare - first_sent)\n",
    "d_wrong_first = cohens_d(bare - wrong_first_sent)\n",
    "d_shuffled = cohens_d(bare - shuffled_sent)\n",
    "d_generic = cohens_d(bare - generic_sent)\n",
    "d_kw10_val = cohens_d(bare - kw10)\n",
    "d_wrong_kw10 = cohens_d(bare - wrong_kw10)\n",
    "\n",
    "# === Test 1: Overlap hypothesis ===\n",
    "# If overlap causes the wound, same-doc first_sent should hurt MORE than wrong-doc first_sent\n",
    "print(f\"\\n--- Test 1: Does overlap matter for sentences? ---\")\n",
    "print(f\"  first_sent (this doc):    d={d_first:+.3f}\")\n",
    "print(f\"  wrong_first_sent (other): d={d_wrong_first:+.3f}\")\n",
    "diff_overlap = wrong_first_sent - first_sent  # positive = this-doc is worse\n",
    "d_overlap = cohens_d(diff_overlap)\n",
    "_, p_overlap = stats.ttest_1samp(diff_overlap, 0)\n",
    "sig_overlap = '***' if p_overlap < 0.001 else '**' if p_overlap < 0.01 else '*' if p_overlap < 0.05 else 'ns'\n",
    "print(f\"  Overlap effect (same vs wrong doc): d={d_overlap:+.3f} ({sig_overlap})\")\n",
    "if d_overlap > 0.05:\n",
    "    print(f\"  -> Same-doc sentence is WORSE. Overlap amplifies the wound.\")\n",
    "elif d_overlap < -0.05:\n",
    "    print(f\"  -> Wrong-doc sentence is WORSE. Overlap is NOT the driver.\")\n",
    "else:\n",
    "    print(f\"  -> No significant overlap effect. Both sentences hurt equally.\")\n",
    "\n",
    "# === Test 2: Coherence hypothesis ===\n",
    "# If coherence causes the wound, coherent first_sent should hurt MORE than shuffled\n",
    "print(f\"\\n--- Test 2: Does coherence matter? ---\")\n",
    "print(f\"  first_sent (coherent):    d={d_first:+.3f}\")\n",
    "print(f\"  shuffled_sent (broken):   d={d_shuffled:+.3f}\")\n",
    "diff_coherence = shuffled_sent - first_sent  # positive = coherent is worse\n",
    "d_coherence = cohens_d(diff_coherence)\n",
    "_, p_coherence = stats.ttest_1samp(diff_coherence, 0)\n",
    "sig_coherence = '***' if p_coherence < 0.001 else '**' if p_coherence < 0.01 else '*' if p_coherence < 0.05 else 'ns'\n",
    "print(f\"  Coherence effect (coherent vs shuffled): d={d_coherence:+.3f} ({sig_coherence})\")\n",
    "if d_coherence > 0.05:\n",
    "    print(f\"  -> Coherent text is WORSE. Word order creates deeper attention dependencies.\")\n",
    "elif d_coherence < -0.05:\n",
    "    print(f\"  -> Shuffled is WORSE. Coherence is NOT the problem.\")\n",
    "else:\n",
    "    print(f\"  -> No coherence effect. The wound is not about word order.\")\n",
    "\n",
    "# === Test 3: Generic sentence ===\n",
    "print(f\"\\n--- Test 3: Does information content matter within coherent text? ---\")\n",
    "print(f\"  wrong_first_sent (informative): d={d_wrong_first:+.3f}\")\n",
    "print(f\"  generic_sent (zero info):       d={d_generic:+.3f}\")\n",
    "diff_info = generic_sent - wrong_first_sent  # positive = informative is worse\n",
    "d_info = cohens_d(diff_info)\n",
    "_, p_info = stats.ttest_1samp(diff_info, 0)\n",
    "sig_info = '***' if p_info < 0.001 else '**' if p_info < 0.01 else '*' if p_info < 0.05 else 'ns'\n",
    "print(f\"  Info content effect: d={d_info:+.3f} ({sig_info})\")\n",
    "\n",
    "# === Test 4: Keywords specificity (replicates Exp 04) ===\n",
    "print(f\"\\n--- Test 4: Keyword specificity (Exp 04 replication) ---\")\n",
    "print(f\"  kw10 (this doc):   d={d_kw10_val:+.3f}\")\n",
    "print(f\"  wrong_kw10 (other): d={d_wrong_kw10:+.3f}\")\n",
    "diff_kw_spec = wrong_kw10 - kw10  # positive = this-doc kw is better\n",
    "d_kw_spec = cohens_d(diff_kw_spec)\n",
    "_, p_kw_spec = stats.ttest_1samp(diff_kw_spec, 0)\n",
    "sig_kw_spec = '***' if p_kw_spec < 0.001 else '**' if p_kw_spec < 0.01 else '*' if p_kw_spec < 0.05 else 'ns'\n",
    "print(f\"  Specificity effect: d={d_kw_spec:+.3f} ({sig_kw_spec})\")\n",
    "\n",
    "# === Summary: 2x2 decomposition ===\n",
    "print(f\"\\n--- 2x2 Decomposition: Coherence x Overlap ---\")\n",
    "print(f\"  (using d vs bare as the outcome)\")\n",
    "print(f\"\")\n",
    "print(f\"  {'':>24} {'Same-doc':>12} {'Wrong-doc':>12} {'Difference':>12}\")\n",
    "print(f\"  {'Coherent sentence':<24} {d_first:>+12.3f} {d_wrong_first:>+12.3f} {d_first-d_wrong_first:>+12.3f}\")\n",
    "print(f\"  {'Shuffled/Keywords':<24} {d_shuffled:>+12.3f} {d_wrong_kw10:>+12.3f} {d_shuffled-d_wrong_kw10:>+12.3f}\")\n",
    "print(f\"  {'Difference':<24} {d_first-d_shuffled:>+12.3f} {d_wrong_first-d_wrong_kw10:>+12.3f}\")\n",
    "print(f\"\")\n",
    "print(f\"  Coherence main effect: {(d_first + d_wrong_first)/2 - (d_shuffled + d_wrong_kw10)/2:+.3f}\")\n",
    "print(f\"  Overlap main effect:   {(d_first + d_shuffled)/2 - (d_wrong_first + d_wrong_kw10)/2:+.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e6fbc38",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T21:18:33.275005Z",
     "iopub.status.busy": "2026-02-19T21:18:33.274451Z",
     "iopub.status.idle": "2026-02-19T21:18:33.286428Z",
     "shell.execute_reply": "2026-02-19T21:18:33.285801Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "LENGTH CONFOUND CHECK\n",
      "======================================================================\n",
      "\n",
      "Mean prefix tokens and d for each condition:\n",
      "  oracle_trunc            :   7.5 tokens, d=+0.228\n",
      "  surr_kw10               :  13.6 tokens, d=+0.186\n",
      "  surr_wrong_kw10         :  13.6 tokens, d=+0.131\n",
      "  surr_shuffled_sent      :  22.1 tokens, d=+0.078\n",
      "  surr_generic_sent       :  11.0 tokens, d=-0.062\n",
      "  surr_wrong_first_sent   :  22.1 tokens, d=+0.063\n",
      "  surr_first_sent         :  22.1 tokens, d=-0.298\n",
      "\n",
      "Critical length-controlled comparison:\n",
      "  first_sent tokens:   mean=22.1\n",
      "  shuffled_sent tokens: mean=22.1\n",
      "  (Same words, same length — only word order differs)\n",
      "  first_sent d:   -0.298\n",
      "  shuffled_sent d: +0.078\n",
      "  -> Any difference is purely due to coherence, not length.\n",
      "\n",
      "Within first_sent: correlation of prefix length with damage:\n",
      "  r=0.230, p=2.107e-07\n",
      "  -> Longer first sentences cause MORE damage.\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Is the wound just a length effect?\n",
    "print(\"=\" * 70)\n",
    "print(\"LENGTH CONFOUND CHECK\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Sentences are longer than keywords — is length the real driver?\n",
    "print(f\"\\nMean prefix tokens and d for each condition:\")\n",
    "for name, nlls, pfx_key in [\n",
    "    ('oracle_trunc', oracle_trunc, 'n_pfx_oracle'),\n",
    "    ('surr_kw10', kw10, 'n_pfx_kw10'),\n",
    "    ('surr_wrong_kw10', wrong_kw10, 'n_pfx_wrong_kw10'),\n",
    "    ('surr_shuffled_sent', shuffled_sent, 'n_pfx_shuffled_sent'),\n",
    "    ('surr_generic_sent', generic_sent, 'n_pfx_generic_sent'),\n",
    "    ('surr_wrong_first_sent', wrong_first_sent, 'n_pfx_wrong_first_sent'),\n",
    "    ('surr_first_sent', first_sent, 'n_pfx_first_sent'),\n",
    "]:\n",
    "    pfx = np.array([s[pfx_key] for s in samples])\n",
    "    d = cohens_d(bare - nlls)\n",
    "    print(f\"  {name:<24}: {pfx.mean():>5.1f} tokens, d={d:+.3f}\")\n",
    "\n",
    "# Key comparison: shuffled_sent has SAME length as first_sent (same words)\n",
    "print(f\"\\nCritical length-controlled comparison:\")\n",
    "pfx_first = np.array([s['n_pfx_first_sent'] for s in samples])\n",
    "pfx_shuffled = np.array([s['n_pfx_shuffled_sent'] for s in samples])\n",
    "print(f\"  first_sent tokens:   mean={pfx_first.mean():.1f}\")\n",
    "print(f\"  shuffled_sent tokens: mean={pfx_shuffled.mean():.1f}\")\n",
    "print(f\"  (Same words, same length — only word order differs)\")\n",
    "print(f\"  first_sent d:   {cohens_d(bare - first_sent):+.3f}\")\n",
    "print(f\"  shuffled_sent d: {cohens_d(bare - shuffled_sent):+.3f}\")\n",
    "print(f\"  -> Any difference is purely due to coherence, not length.\")\n",
    "\n",
    "# Within first_sent: does per-sample prefix length predict damage?\n",
    "pfx_len = np.array([s['n_pfx_first_sent'] for s in samples])\n",
    "damage = first_sent - bare  # positive = first_sent is worse\n",
    "r_len, p_len = stats.pearsonr(pfx_len, damage)\n",
    "print(f\"\\nWithin first_sent: correlation of prefix length with damage:\")\n",
    "print(f\"  r={r_len:.3f}, p={p_len:.3e}\")\n",
    "if r_len > 0.1:\n",
    "    print(f\"  -> Longer first sentences cause MORE damage.\")\n",
    "else:\n",
    "    print(f\"  -> Length does not predict damage magnitude.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "740e4c9f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T21:18:33.289332Z",
     "iopub.status.busy": "2026-02-19T21:18:33.288799Z",
     "iopub.status.idle": "2026-02-19T21:18:33.759385Z",
     "shell.execute_reply": "2026-02-19T21:18:33.758685Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "VERDICT — Exp 05: Truncation Wound Mechanism\n",
      "======================================================================\n",
      "\n",
      "Model: google/t5gemma-2-4b-4b\n",
      "N: 500 (MS MARCO v1.1)\n",
      "\n",
      "--- Condition summary ---\n",
      "  Condition                 d vs bare Interpretation\n",
      "  ----------------------------------------------------------------------\n",
      "  oracle_trunc                 +0.228   Ceiling\n",
      "  surr_kw10 (this)             +0.186   Best surrogate\n",
      "  surr_wrong_kw10              +0.131   Keywords without specificity\n",
      "  surr_shuffled_sent           +0.078   Overlap without coherence\n",
      "  surr_generic_sent            -0.062   Coherence without information\n",
      "  surr_wrong_first_sent        +0.063   Coherence without overlap\n",
      "  surr_first_sent              -0.298   Coherence + overlap (catastrophic)\n",
      "\n",
      "--- Hypothesis verdict ---\n",
      "  Coherence main effect: -0.222\n",
      "  Overlap main effect:   -0.207\n",
      "  -> H2 (COHERENCE) is the primary driver.\n",
      "     Coherent text creates deep attention dependencies that truncation disrupts.\n",
      "\n",
      "Results saved to ../../../results/exp05/results.json\n",
      "\n",
      "Cleaning up GPU memory...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory: 15.03 GB -> 0.01 GB\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Verdict and save\n",
    "print(\"=\" * 70)\n",
    "print(\"VERDICT — Exp 05: Truncation Wound Mechanism\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "d_first = cohens_d(bare - first_sent)\n",
    "d_wrong_first = cohens_d(bare - wrong_first_sent)\n",
    "d_shuffled = cohens_d(bare - shuffled_sent)\n",
    "d_generic = cohens_d(bare - generic_sent)\n",
    "d_kw10_val = cohens_d(bare - kw10)\n",
    "\n",
    "print(f\"\\nModel: {MODEL_NAME}\")\n",
    "print(f\"N: {len(results)} (MS MARCO v1.1)\")\n",
    "\n",
    "print(f\"\\n--- Condition summary ---\")\n",
    "print(f\"  {'Condition':<24} {'d vs bare':>10} {'Interpretation'}\")\n",
    "print(f\"  {'-'*70}\")\n",
    "for name, d, interp in [\n",
    "    ('oracle_trunc', cohens_d(bare - oracle_trunc), 'Ceiling'),\n",
    "    ('surr_kw10 (this)', d_kw10_val, 'Best surrogate'),\n",
    "    ('surr_wrong_kw10', cohens_d(bare - wrong_kw10), 'Keywords without specificity'),\n",
    "    ('surr_shuffled_sent', d_shuffled, 'Overlap without coherence'),\n",
    "    ('surr_generic_sent', d_generic, 'Coherence without information'),\n",
    "    ('surr_wrong_first_sent', d_wrong_first, 'Coherence without overlap'),\n",
    "    ('surr_first_sent', d_first, 'Coherence + overlap (catastrophic)'),\n",
    "]:\n",
    "    print(f\"  {name:<24} {d:>+10.3f}   {interp}\")\n",
    "\n",
    "# Determine which hypothesis won\n",
    "overlap_effect = d_first - d_wrong_first\n",
    "coherence_effect = d_first - d_shuffled\n",
    "coherence_main = (d_first + d_wrong_first)/2 - (d_shuffled + cohens_d(bare - wrong_kw10))/2\n",
    "overlap_main = (d_first + d_shuffled)/2 - (d_wrong_first + cohens_d(bare - wrong_kw10))/2\n",
    "\n",
    "print(f\"\\n--- Hypothesis verdict ---\")\n",
    "print(f\"  Coherence main effect: {coherence_main:+.3f}\")\n",
    "print(f\"  Overlap main effect:   {overlap_main:+.3f}\")\n",
    "\n",
    "if abs(coherence_main) > abs(overlap_main) and coherence_main < -0.05:\n",
    "    print(f\"  -> H2 (COHERENCE) is the primary driver.\")\n",
    "    print(f\"     Coherent text creates deep attention dependencies that truncation disrupts.\")\n",
    "elif abs(overlap_main) > abs(coherence_main) and overlap_main < -0.05:\n",
    "    print(f\"  -> H1 (OVERLAP) is the primary driver.\")\n",
    "    print(f\"     Shared content creates extra-strong attention connections.\")\n",
    "elif coherence_main < -0.05 and overlap_main < -0.05:\n",
    "    print(f\"  -> BOTH coherence and overlap contribute to the wound.\")\n",
    "else:\n",
    "    print(f\"  -> Neither factor dominates clearly. The wound mechanism is more complex.\")\n",
    "\n",
    "# Save\n",
    "final_results = {\n",
    "    'experiment': 'v4_exp05_truncation_wound',\n",
    "    'model': MODEL_NAME,\n",
    "    'dataset': 'ms_marco_v1.1',\n",
    "    'n_samples': len(results),\n",
    "    'seed': SEED,\n",
    "    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'bonferroni': N_BONF,\n",
    "    'conditions': analysis,\n",
    "    'hypothesis_tests': {\n",
    "        'overlap_effect': float(overlap_effect),\n",
    "        'coherence_effect': float(coherence_effect),\n",
    "        'coherence_main_effect': float(coherence_main),\n",
    "        'overlap_main_effect': float(overlap_main),\n",
    "    },\n",
    "}\n",
    "\n",
    "with open(RESULTS_DIR / 'results.json', 'w') as f:\n",
    "    json.dump(final_results, f, indent=2)\n",
    "print(f\"\\nResults saved to {RESULTS_DIR / 'results.json'}\")\n",
    "\n",
    "# Cleanup\n",
    "print(f\"\\nCleaning up GPU memory...\")\n",
    "mem_before = torch.cuda.memory_allocated() / 1e9\n",
    "del model, processor, tokenizer\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "mem_after = torch.cuda.memory_allocated() / 1e9\n",
    "print(f\"GPU memory: {mem_before:.2f} GB -> {mem_after:.2f} GB\")\n",
    "print(\"Done!\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "02c2fd8611014cd586676bf9bf9eba83": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "1fad632879b640fe929298b120fbb301": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_c6ff4e3bd97a45ae8913908bf72f8232",
        "IPY_MODEL_79e3aed7710842308853580ee102dea1",
        "IPY_MODEL_2a6f08f8521b49939075cfcce91376ac"
       ],
       "layout": "IPY_MODEL_f007139399fe4e29addfd0ab49249839",
       "tabbable": null,
       "tooltip": null
      }
     },
     "2a6f08f8521b49939075cfcce91376ac": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_68144a4aa62042c281a5327ece4776b9",
       "placeholder": "​",
       "style": "IPY_MODEL_aa7df41b493b413f9f76f9b0d110b8eb",
       "tabbable": null,
       "tooltip": null,
       "value": " 500/500 [17:17&lt;00:00,  2.06s/it]"
      }
     },
     "2ff4d175c5154deb9099debc5f87a776": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "33697123b4d04d21baa951837593388a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_e1134f6ba34749abba15490581725f2c",
       "placeholder": "​",
       "style": "IPY_MODEL_4ff0e19d1f8c40b3a969f1e07c882222",
       "tabbable": null,
       "tooltip": null,
       "value": " 1327/1327 [00:04&lt;00:00, 687.32it/s, Materializing param=model.encoder.vision_tower.vision_model.post_layernorm.weight]"
      }
     },
     "4ff0e19d1f8c40b3a969f1e07c882222": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "5a4b365a45774ae99d5873eec1f3d6a5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_b4380bb42db94461b22ca5be5627d3b9",
        "IPY_MODEL_e21948455cc44298ac537a5ba4510236",
        "IPY_MODEL_33697123b4d04d21baa951837593388a"
       ],
       "layout": "IPY_MODEL_abcda3a902834b7c8e500b5eebe845d9",
       "tabbable": null,
       "tooltip": null
      }
     },
     "68144a4aa62042c281a5327ece4776b9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "7659a1eb05784408a771480009c822df": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "79e3aed7710842308853580ee102dea1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_fb8dd97041554885acde9e9858eb3bb8",
       "max": 500.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_d56900dfb6384711b561d3105af121eb",
       "tabbable": null,
       "tooltip": null,
       "value": 500.0
      }
     },
     "7af081e7c2904260b8965aef8ecfa3b1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "aa7df41b493b413f9f76f9b0d110b8eb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "aaafd41e9f324887872911b8f9aa42ba": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "abcda3a902834b7c8e500b5eebe845d9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b4380bb42db94461b22ca5be5627d3b9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_7659a1eb05784408a771480009c822df",
       "placeholder": "​",
       "style": "IPY_MODEL_aaafd41e9f324887872911b8f9aa42ba",
       "tabbable": null,
       "tooltip": null,
       "value": "Loading weights: 100%"
      }
     },
     "c6ff4e3bd97a45ae8913908bf72f8232": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_2ff4d175c5154deb9099debc5f87a776",
       "placeholder": "​",
       "style": "IPY_MODEL_02c2fd8611014cd586676bf9bf9eba83",
       "tabbable": null,
       "tooltip": null,
       "value": "Scoring: 100%"
      }
     },
     "d56900dfb6384711b561d3105af121eb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "e1134f6ba34749abba15490581725f2c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e21948455cc44298ac537a5ba4510236": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_7af081e7c2904260b8965aef8ecfa3b1",
       "max": 1327.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_e230b00d61b04603a7a30cc50855d1e4",
       "tabbable": null,
       "tooltip": null,
       "value": 1327.0
      }
     },
     "e230b00d61b04603a7a30cc50855d1e4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "f007139399fe4e29addfd0ab49249839": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "fb8dd97041554885acde9e9858eb3bb8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
