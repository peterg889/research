{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddaf14f5",
   "metadata": {},
   "source": [
    "# Experiment 09: Cross-Model Generalization Sweep\n",
    "\n",
    "## Motivation\n",
    "\n",
    "All v4 experiments (01-08) used T5Gemma 2 4B-4B. The enrichment effect\n",
    "(prepending a prefix to the encoder improves answer NLL) could be a T5Gemma\n",
    "quirk — especially since T5Gemma has unusual merged self+cross attention\n",
    "in the decoder.\n",
    "\n",
    "This experiment tests whether the core effect generalizes to:\n",
    "- **Flan-T5** (base/large/xl) — standard T5 architecture, instruction-tuned\n",
    "- **BART-large** — different pre-training (denoising AE vs span corruption)\n",
    "\n",
    "## Design\n",
    "\n",
    "**4 conditions per model:**\n",
    "\n",
    "| # | Condition | Encoder input | Cross-attn mask | Decoder input |\n",
    "|---|-----------|--------------|-----------------|---------------|\n",
    "| 1 | bare | [document] | all | [BOS, query, answer] |\n",
    "| 2 | oracle_trunc | [query + doc] | doc only | [BOS, query, answer] |\n",
    "| 3 | random_trunc | [random + doc] | doc only | [BOS, query, answer] |\n",
    "| 4 | bare_nq | [document] | all | [BOS, answer] |\n",
    "\n",
    "**Key comparisons (per model):**\n",
    "- **(2) vs (1)**: Does encoder enrichment help? (THE generalization test)\n",
    "- **(3) vs (1)**: Is it structural or content-dependent?\n",
    "- **(1) vs (4)**: How much does decoder query help?\n",
    "\n",
    "**Same 500 MS MARCO samples across all models.** N=500, SEED=42.\n",
    "NLL only — no attention probes (SDPA attention for speed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78306747",
   "metadata": {},
   "outputs": [],
   "source": "# Cell 2: Setup\nimport os\nos.umask(0o000)\n\nimport sys, json, time, gc, re\nimport random as pyrandom\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom pathlib import Path\nfrom collections import Counter, defaultdict\nfrom scipy import stats\nfrom tqdm.auto import tqdm\n\nsys.path.insert(0, \"../../..\")\nfrom lib.analysis import cohens_d\n\nSEED = 42\nN_SAMPLES = 500\n\nRESULTS_DIR = Path(\"../../../results/exp09\")\nRESULTS_DIR.mkdir(parents=True, exist_ok=True)\n\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\npyrandom.seed(SEED)\n\nfrom dotenv import load_dotenv, find_dotenv\nload_dotenv(find_dotenv())\nHF_TOKEN = os.environ.get(\"HF_TOKEN\")\n\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nMODELS = [\n    \"google/flan-t5-base\",\n    \"google/flan-t5-large\",\n    \"google/flan-t5-xl\",\n    \"facebook/bart-large\",\n]\n\nprint(f\"Exp 09: Cross-Model Generalization Sweep\")\nprint(f\"N: {N_SAMPLES}, Models: {len(MODELS)}\")\nprint(f\"DEVICE: {DEVICE}\")\nprint(f\"GPU: {torch.cuda.get_device_name(0)}\")\nprint(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25cea918",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Load MS MARCO data + generate random prefixes\n",
    "from lib.data import count_words\n",
    "from datasets import load_dataset\n",
    "\n",
    "print(\"Loading MS MARCO v1.1 validation...\")\n",
    "ds = load_dataset(\"microsoft/ms_marco\", \"v1.1\", split=\"validation\")\n",
    "\n",
    "all_candidates = []\n",
    "for item in ds:\n",
    "    if len(all_candidates) >= 3 * N_SAMPLES:\n",
    "        break\n",
    "    passages = item.get('passages', {})\n",
    "    ptexts = passages.get('passage_text', [])\n",
    "    is_sel = passages.get('is_selected', [])\n",
    "    query = item.get('query', '')\n",
    "    answers = item.get('answers', [])\n",
    "    well_formed = item.get('wellFormedAnswers', [])\n",
    "    answer = None\n",
    "    if well_formed and len(well_formed) > 0 and well_formed[0] not in ('[]', ''):\n",
    "        answer = well_formed[0]\n",
    "    elif answers and len(answers) > 0 and answers[0] != 'No Answer Present.':\n",
    "        answer = answers[0]\n",
    "    if not answer:\n",
    "        continue\n",
    "    for pt, sel in zip(ptexts, is_sel):\n",
    "        wc = count_words(pt)\n",
    "        if sel == 1 and 30 <= wc <= 300:\n",
    "            all_candidates.append({\n",
    "                'passage': pt, 'query': query, 'answer': answer,\n",
    "                'word_count': wc,\n",
    "            })\n",
    "            break\n",
    "\n",
    "print(f\"Total candidates: {len(all_candidates)}\")\n",
    "np.random.seed(SEED)\n",
    "indices = np.random.permutation(len(all_candidates))\n",
    "samples = [all_candidates[i] for i in indices[:N_SAMPLES]]\n",
    "del ds, all_candidates\n",
    "gc.collect()\n",
    "\n",
    "# Generate random prefix TEXT for each sample (shared across models)\n",
    "# 8 random common English words per sample\n",
    "WORD_POOL = [\n",
    "    \"computer\", \"mountain\", \"hospital\", \"children\", \"building\", \"national\",\n",
    "    \"business\", \"research\", \"students\", \"american\", \"possible\", \"economic\",\n",
    "    \"personal\", \"together\", \"products\", \"services\", \"actually\", \"remember\",\n",
    "    \"practice\", \"training\", \"industry\", \"complete\", \"critical\", \"function\",\n",
    "    \"language\", \"standard\", \"material\", \"original\", \"physical\", \"security\",\n",
    "    \"interest\", \"problems\", \"consider\", \"response\", \"pressure\", \"politics\",\n",
    "    \"movement\", \"evidence\", \"southern\", \"northern\", \"exchange\", \"decision\",\n",
    "    \"position\", \"increase\", \"describe\", \"military\", \"required\", \"approach\",\n",
    "    \"strategy\", \"customer\", \"resource\", \"employee\", \"audience\", \"location\",\n",
    "    \"property\", \"cultural\", \"activity\", \"strength\", \"analysis\", \"powerful\",\n",
    "    \"election\", \"argument\", \"campaign\", \"maintain\", \"question\", \"behavior\",\n",
    "    \"majority\", \"solution\", \"software\", \"consumer\", \"creative\", \"reaction\",\n",
    "    \"european\", \"delivery\", \"organize\", \"involved\", \"relative\", \"learning\",\n",
    "    \"positive\", \"numerous\", \"familiar\", \"engineer\", \"platform\", \"indicate\",\n",
    "    \"previous\", \"pleasure\", \"opposite\", \"magazine\", \"document\", \"religion\",\n",
    "    \"scenario\", \"workshop\", \"minority\", \"guidance\", \"estimate\", \"recently\",\n",
    "    \"surprise\", \"champion\", \"pleasant\", \"grateful\", \"moderate\", \"boundary\",\n",
    "]\n",
    "\n",
    "for i, s in enumerate(samples):\n",
    "    rng = np.random.RandomState(SEED + i + 20000)\n",
    "    words = rng.choice(WORD_POOL, size=8, replace=False)\n",
    "    s['random_prefix'] = \" \".join(words)\n",
    "\n",
    "print(f\"Loaded {len(samples)} samples\")\n",
    "print(f\"Mean passage words: {np.mean([s['word_count'] for s in samples]):.0f}\")\n",
    "print(f\"Mean query words: {np.mean([count_words(s['query']) for s in samples]):.0f}\")\n",
    "print(f\"Mean answer words: {np.mean([count_words(s['answer']) for s in samples]):.0f}\")\n",
    "print(f\"Example random prefix: '{samples[0]['random_prefix']}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd696c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Model-agnostic scoring function\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "def count_prefix_tokens(tokenizer, prefix_text, document_text):\n",
    "    full_text = prefix_text + \"\\n\" + document_text\n",
    "    full_ids = tokenizer(full_text, add_special_tokens=True, truncation=True,\n",
    "                         max_length=2048).input_ids\n",
    "    doc_ids = tokenizer(document_text, add_special_tokens=True, truncation=True,\n",
    "                        max_length=2048).input_ids\n",
    "    return len(full_ids) - len(doc_ids)\n",
    "\n",
    "\n",
    "def score_sample(model, tokenizer, sample, device):\n",
    "    # Score one sample under all 4 conditions.\n",
    "    # Returns dict with NLL per condition.\n",
    "    passage = sample['passage']\n",
    "    query = sample['query']\n",
    "    answer = sample['answer']\n",
    "    random_prefix = sample['random_prefix']\n",
    "\n",
    "    bos_id = model.config.decoder_start_token_id\n",
    "    if bos_id is None:\n",
    "        bos_id = tokenizer.pad_token_id or 0\n",
    "\n",
    "    query_ids = tokenizer(query, add_special_tokens=False, truncation=True,\n",
    "                          max_length=512).input_ids\n",
    "    answer_ids = tokenizer(answer, add_special_tokens=False, truncation=True,\n",
    "                           max_length=256).input_ids\n",
    "    if len(answer_ids) == 0:\n",
    "        return None\n",
    "\n",
    "    result = {}\n",
    "\n",
    "    # === Encoder pass 1: bare document ===\n",
    "    enc_ids_bare = tokenizer(passage, return_tensors=\"pt\",\n",
    "                             add_special_tokens=True, truncation=True,\n",
    "                             max_length=2048).input_ids.to(device)\n",
    "    enc_mask_bare = torch.ones(1, enc_ids_bare.shape[1], device=device, dtype=torch.long)\n",
    "    with torch.no_grad():\n",
    "        enc_out_bare = model.get_encoder()(\n",
    "            input_ids=enc_ids_bare, attention_mask=enc_mask_bare\n",
    "        )\n",
    "\n",
    "    # Condition 1: bare (decoder has query)\n",
    "    dec_q = torch.tensor([[bos_id] + query_ids + answer_ids],\n",
    "                          dtype=torch.long, device=device)\n",
    "    n_q = len(query_ids)\n",
    "    with torch.no_grad():\n",
    "        out = model(encoder_outputs=enc_out_bare, attention_mask=enc_mask_bare,\n",
    "                    decoder_input_ids=dec_q)\n",
    "    logits = out.logits[0, n_q:n_q + len(answer_ids), :]\n",
    "    targets = torch.tensor(answer_ids, dtype=torch.long, device=device)\n",
    "    nll = -F.log_softmax(logits, dim=-1).gather(1, targets.unsqueeze(1)).squeeze(1).mean().item()\n",
    "    result['nll_bare'] = nll\n",
    "\n",
    "    # Condition 4: bare_nq (no query in decoder)\n",
    "    dec_nq = torch.tensor([[bos_id] + answer_ids], dtype=torch.long, device=device)\n",
    "    with torch.no_grad():\n",
    "        out = model(encoder_outputs=enc_out_bare, attention_mask=enc_mask_bare,\n",
    "                    decoder_input_ids=dec_nq)\n",
    "    logits = out.logits[0, 0:len(answer_ids), :]\n",
    "    nll = -F.log_softmax(logits, dim=-1).gather(1, targets.unsqueeze(1)).squeeze(1).mean().item()\n",
    "    result['nll_bare_nq'] = nll\n",
    "\n",
    "    del enc_out_bare\n",
    "\n",
    "    # === Encoder pass 2: oracle (query + document) ===\n",
    "    oracle_text = query + \"\\n\" + passage\n",
    "    enc_ids_oracle = tokenizer(oracle_text, return_tensors=\"pt\",\n",
    "                               add_special_tokens=True, truncation=True,\n",
    "                               max_length=2048).input_ids.to(device)\n",
    "    enc_mask_oracle = torch.ones(1, enc_ids_oracle.shape[1], device=device, dtype=torch.long)\n",
    "    with torch.no_grad():\n",
    "        enc_out_oracle = model.get_encoder()(\n",
    "            input_ids=enc_ids_oracle, attention_mask=enc_mask_oracle\n",
    "        )\n",
    "\n",
    "    # Build cross-attention mask (hide prefix)\n",
    "    n_pfx = count_prefix_tokens(tokenizer, query, passage)\n",
    "    cross_mask_oracle = torch.ones(1, enc_ids_oracle.shape[1], device=device, dtype=torch.long)\n",
    "    cross_mask_oracle[:, :n_pfx] = 0\n",
    "\n",
    "    # Condition 2: oracle_trunc\n",
    "    with torch.no_grad():\n",
    "        out = model(encoder_outputs=enc_out_oracle, attention_mask=cross_mask_oracle,\n",
    "                    decoder_input_ids=dec_q)\n",
    "    logits = out.logits[0, n_q:n_q + len(answer_ids), :]\n",
    "    nll = -F.log_softmax(logits, dim=-1).gather(1, targets.unsqueeze(1)).squeeze(1).mean().item()\n",
    "    result['nll_oracle_trunc'] = nll\n",
    "\n",
    "    del enc_out_oracle\n",
    "\n",
    "    # === Encoder pass 3: random prefix ===\n",
    "    random_text = random_prefix + \"\\n\" + passage\n",
    "    enc_ids_random = tokenizer(random_text, return_tensors=\"pt\",\n",
    "                               add_special_tokens=True, truncation=True,\n",
    "                               max_length=2048).input_ids.to(device)\n",
    "    enc_mask_random = torch.ones(1, enc_ids_random.shape[1], device=device, dtype=torch.long)\n",
    "    with torch.no_grad():\n",
    "        enc_out_random = model.get_encoder()(\n",
    "            input_ids=enc_ids_random, attention_mask=enc_mask_random\n",
    "        )\n",
    "\n",
    "    n_pfx_rand = count_prefix_tokens(tokenizer, random_prefix, passage)\n",
    "    cross_mask_random = torch.ones(1, enc_ids_random.shape[1], device=device, dtype=torch.long)\n",
    "    cross_mask_random[:, :n_pfx_rand] = 0\n",
    "\n",
    "    # Condition 3: random_trunc\n",
    "    with torch.no_grad():\n",
    "        out = model(encoder_outputs=enc_out_random, attention_mask=cross_mask_random,\n",
    "                    decoder_input_ids=dec_q)\n",
    "    logits = out.logits[0, n_q:n_q + len(answer_ids), :]\n",
    "    nll = -F.log_softmax(logits, dim=-1).gather(1, targets.unsqueeze(1)).squeeze(1).mean().item()\n",
    "    result['nll_random_trunc'] = nll\n",
    "\n",
    "    del enc_out_random, out\n",
    "    return result\n",
    "\n",
    "\n",
    "print(\"Scoring function defined.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed32dc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Run sweep across all models\n",
    "print(\"=\" * 70)\n",
    "print(\"MODEL SWEEP\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "COND_NAMES = ['bare', 'oracle_trunc', 'random_trunc', 'bare_nq']\n",
    "\n",
    "all_model_results = {}\n",
    "\n",
    "for model_idx, model_name in enumerate(MODELS):\n",
    "    slug = model_name.replace(\"/\", \"_\")\n",
    "    ckpt_path = RESULTS_DIR / f\"{slug}_checkpoint.json\"\n",
    "\n",
    "    # Check if already completed\n",
    "    if ckpt_path.exists():\n",
    "        ckpt = json.loads(ckpt_path.read_text())\n",
    "        if len(ckpt.get('results', [])) == N_SAMPLES:\n",
    "            print(f\"\\n{'='*70}\")\n",
    "            print(f\"[{model_idx+1}/{len(MODELS)}] {model_name} — LOADED FROM CHECKPOINT\")\n",
    "            print(f\"{'='*70}\")\n",
    "            all_model_results[model_name] = ckpt['results']\n",
    "            continue\n",
    "\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"[{model_idx+1}/{len(MODELS)}] {model_name}\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "    # Load model\n",
    "    t0 = time.time()\n",
    "    print(f\"  Loading model...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, token=HF_TOKEN)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "        model_name, device_map=\"auto\", torch_dtype=torch.bfloat16, token=HF_TOKEN,\n",
    "    )\n",
    "    model.eval()\n",
    "\n",
    "    n_params = sum(p.numel() for p in model.parameters()) / 1e6\n",
    "    gpu_mem = torch.cuda.memory_allocated() / 1e9\n",
    "    print(f\"  Loaded: {n_params:.0f}M params, {gpu_mem:.1f} GB GPU\")\n",
    "    print(f\"  BOS/decoder_start_token_id: {model.config.decoder_start_token_id}\")\n",
    "\n",
    "    # Resume from partial checkpoint\n",
    "    model_results = []\n",
    "    start_idx = 0\n",
    "    if ckpt_path.exists():\n",
    "        ckpt = json.loads(ckpt_path.read_text())\n",
    "        if len(ckpt.get('results', [])) > 0:\n",
    "            saved_queries = [r['query'][:50] for r in ckpt['results']]\n",
    "            current_queries = [s['query'][:50] for s in samples[:len(saved_queries)]]\n",
    "            if saved_queries == current_queries:\n",
    "                model_results = ckpt['results']\n",
    "                start_idx = len(model_results)\n",
    "                print(f\"  Resuming from checkpoint: {start_idx}/{N_SAMPLES}\")\n",
    "\n",
    "    if start_idx == 0:\n",
    "        print(f\"  Starting fresh: {N_SAMPLES} samples x {len(COND_NAMES)} conditions\")\n",
    "\n",
    "    for i in tqdm(range(start_idx, N_SAMPLES), initial=start_idx, total=N_SAMPLES,\n",
    "                  desc=f\"  {model_name.split('/')[-1]}\"):\n",
    "        s = samples[i]\n",
    "        try:\n",
    "            result = score_sample(model, tokenizer, s, DEVICE)\n",
    "        except Exception as e:\n",
    "            print(f\"  ERROR at sample {i}: {e}\")\n",
    "            result = None\n",
    "\n",
    "        if result is None:\n",
    "            continue\n",
    "        result['query'] = s['query'][:50]\n",
    "        model_results.append(result)\n",
    "\n",
    "        if (i + 1) % 50 == 0 or i == N_SAMPLES - 1:\n",
    "            ckpt = {\n",
    "                'model': model_name,\n",
    "                'n_total': N_SAMPLES,\n",
    "                'results': model_results,\n",
    "                'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            }\n",
    "            ckpt_path.write_text(json.dumps(ckpt))\n",
    "\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    elapsed = time.time() - t0\n",
    "    print(f\"  Done: {len(model_results)} samples in {elapsed/60:.1f} min\")\n",
    "\n",
    "    # Quick summary\n",
    "    for cond in COND_NAMES:\n",
    "        vals = [r[f'nll_{cond}'] for r in model_results]\n",
    "        print(f\"    {cond:<20} NLL={np.mean(vals):.4f}\")\n",
    "\n",
    "    all_model_results[model_name] = model_results\n",
    "\n",
    "    # Unload model\n",
    "    del model, tokenizer\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    print(f\"  GPU freed: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"ALL MODELS COMPLETE\")\n",
    "print(f\"{'='*70}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42359dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Cross-model comparison\n",
    "print(\"=\" * 70)\n",
    "print(\"CROSS-MODEL COMPARISON\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Include T5Gemma reference from Exp 01\n",
    "T5GEMMA_REF = {\n",
    "    'nll_bare': 2.554,\n",
    "    'nll_oracle_trunc': 2.406,\n",
    "    'nll_bare_nq': 3.676,\n",
    "    'd_oracle': 0.228,\n",
    "    'd_random': 0.080,\n",
    "    'structural_frac': 0.35,\n",
    "}\n",
    "\n",
    "print(f\"\\n--- NLL by model and condition ---\")\n",
    "print(f\"\\n  {'Model':<25} {'bare':>8} {'oracle':>8} {'random':>8} {'bare_nq':>8}\")\n",
    "print(f\"  {'-'*62}\")\n",
    "\n",
    "# T5Gemma reference\n",
    "print(f\"  {'T5Gemma-2-4B (Exp 01)':<25} {T5GEMMA_REF['nll_bare']:>8.3f} \"\n",
    "      f\"{T5GEMMA_REF['nll_oracle_trunc']:>8.3f} {'—':>8} \"\n",
    "      f\"{T5GEMMA_REF['nll_bare_nq']:>8.3f}\")\n",
    "\n",
    "model_summary = {}\n",
    "\n",
    "for model_name in MODELS:\n",
    "    if model_name not in all_model_results:\n",
    "        continue\n",
    "    res = all_model_results[model_name]\n",
    "    short = model_name.split('/')[-1]\n",
    "\n",
    "    nll = {}\n",
    "    for cond in COND_NAMES:\n",
    "        nll[cond] = np.array([r[f'nll_{cond}'] for r in res])\n",
    "\n",
    "    print(f\"  {short:<25} {nll['bare'].mean():>8.3f} {nll['oracle_trunc'].mean():>8.3f} \"\n",
    "          f\"{nll['random_trunc'].mean():>8.3f} {nll['bare_nq'].mean():>8.3f}\")\n",
    "\n",
    "    model_summary[model_name] = nll\n",
    "\n",
    "# Effect sizes\n",
    "print(f\"\\n--- Effect sizes (Cohen's d, positive = condition helps) ---\")\n",
    "print(f\"\\n  {'Model':<25} {'d_oracle':>10} {'d_random':>10} {'struct%':>10} \"\n",
    "      f\"{'d_dec_q':>10} {'oracle_sig':>10}\")\n",
    "print(f\"  {'-'*80}\")\n",
    "\n",
    "# T5Gemma reference\n",
    "print(f\"  {'T5Gemma-2-4B (Exp 01)':<25} {T5GEMMA_REF['d_oracle']:>+10.3f} \"\n",
    "      f\"{T5GEMMA_REF['d_random']:>+10.3f} {T5GEMMA_REF['structural_frac']*100:>9.0f}% \"\n",
    "      f\"{'—':>10} {'***':>10}\")\n",
    "\n",
    "for model_name in MODELS:\n",
    "    if model_name not in model_summary:\n",
    "        continue\n",
    "    nll = model_summary[model_name]\n",
    "    short = model_name.split('/')[-1]\n",
    "\n",
    "    # Oracle enrichment: bare - oracle (positive = oracle helps)\n",
    "    oracle_diff = nll['bare'] - nll['oracle_trunc']\n",
    "    d_oracle = cohens_d(oracle_diff)\n",
    "    _, p_oracle = stats.ttest_1samp(oracle_diff, 0)\n",
    "    sig_o = '***' if p_oracle < 0.001 else '**' if p_oracle < 0.01 else '*' if p_oracle < 0.05 else 'ns'\n",
    "\n",
    "    # Random enrichment: bare - random (positive = random helps)\n",
    "    random_diff = nll['bare'] - nll['random_trunc']\n",
    "    d_random = cohens_d(random_diff)\n",
    "\n",
    "    # Structural fraction\n",
    "    struct_frac = d_random / d_oracle if d_oracle != 0 else float('nan')\n",
    "\n",
    "    # Decoder query effect: bare_nq - bare (positive = query helps)\n",
    "    dec_q_diff = nll['bare_nq'] - nll['bare']\n",
    "    d_dec_q = cohens_d(dec_q_diff)\n",
    "\n",
    "    print(f\"  {short:<25} {d_oracle:>+10.3f} {d_random:>+10.3f} \"\n",
    "          f\"{struct_frac*100:>9.0f}% {d_dec_q:>+10.3f} {sig_o:>10}\")\n",
    "\n",
    "# Detailed pairwise tests\n",
    "print(f\"\\n--- Pairwise significance tests ---\")\n",
    "for model_name in MODELS:\n",
    "    if model_name not in model_summary:\n",
    "        continue\n",
    "    nll = model_summary[model_name]\n",
    "    short = model_name.split('/')[-1]\n",
    "\n",
    "    print(f\"\\n  {short}:\")\n",
    "    for cond, label in [('oracle_trunc', 'oracle_trunc vs bare'),\n",
    "                        ('random_trunc', 'random_trunc vs bare'),\n",
    "                        ('bare_nq', 'bare vs bare_nq (query effect)')]:\n",
    "        if cond == 'bare_nq':\n",
    "            diff = nll['bare_nq'] - nll['bare']\n",
    "            d = cohens_d(diff)\n",
    "        else:\n",
    "            diff = nll['bare'] - nll[cond]\n",
    "            d = cohens_d(diff)\n",
    "        _, p = stats.ttest_1samp(diff, 0)\n",
    "        sig = '***' if p < 0.001 else '**' if p < 0.01 else '*' if p < 0.05 else 'ns'\n",
    "        win = (diff > 0).mean() * 100\n",
    "        print(f\"    {label:<30} d={d:+.3f}  win={win:.0f}%  p={p:.2e} {sig}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8edb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Summary and save\n",
    "print(\"=\" * 70)\n",
    "print(\"SUMMARY — Exp 09: Cross-Model Generalization\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Collect summary for all models\n",
    "summary = {}\n",
    "\n",
    "for model_name in MODELS:\n",
    "    if model_name not in model_summary:\n",
    "        continue\n",
    "    nll = model_summary[model_name]\n",
    "    short = model_name.split('/')[-1]\n",
    "    res = all_model_results[model_name]\n",
    "\n",
    "    oracle_diff = nll['bare'] - nll['oracle_trunc']\n",
    "    random_diff = nll['bare'] - nll['random_trunc']\n",
    "    dec_q_diff = nll['bare_nq'] - nll['bare']\n",
    "\n",
    "    d_oracle = cohens_d(oracle_diff)\n",
    "    d_random = cohens_d(random_diff)\n",
    "    d_dec_q = cohens_d(dec_q_diff)\n",
    "    _, p_oracle = stats.ttest_1samp(oracle_diff, 0)\n",
    "    _, p_random = stats.ttest_1samp(random_diff, 0)\n",
    "    struct_frac = d_random / d_oracle if d_oracle != 0 else float('nan')\n",
    "\n",
    "    summary[model_name] = {\n",
    "        'short_name': short,\n",
    "        'n_samples': len(res),\n",
    "        'nll_bare': float(nll['bare'].mean()),\n",
    "        'nll_oracle_trunc': float(nll['oracle_trunc'].mean()),\n",
    "        'nll_random_trunc': float(nll['random_trunc'].mean()),\n",
    "        'nll_bare_nq': float(nll['bare_nq'].mean()),\n",
    "        'd_oracle': float(d_oracle),\n",
    "        'd_random': float(d_random),\n",
    "        'd_dec_q': float(d_dec_q),\n",
    "        'p_oracle': float(p_oracle),\n",
    "        'p_random': float(p_random),\n",
    "        'structural_fraction': float(struct_frac),\n",
    "    }\n",
    "\n",
    "# Count models where oracle is significant\n",
    "n_sig = sum(1 for v in summary.values() if v['p_oracle'] < 0.05)\n",
    "n_total = len(summary)\n",
    "\n",
    "print(f\"\\nModels tested: {n_total}\")\n",
    "print(f\"Models with significant oracle enrichment (p<0.05): {n_sig}/{n_total}\")\n",
    "\n",
    "print(f\"\\n  {'Model':<25} {'d_oracle':>10} {'d_random':>10} {'struct%':>10} {'Sig':>5}\")\n",
    "print(f\"  {'-'*55}\")\n",
    "\n",
    "# T5Gemma reference\n",
    "print(f\"  {'T5Gemma-2-4B':<25} {'+0.228':>10} {'+0.080':>10} {'35%':>10} {'***':>5}\")\n",
    "\n",
    "for model_name in MODELS:\n",
    "    if model_name not in summary:\n",
    "        continue\n",
    "    s = summary[model_name]\n",
    "    sig = '***' if s['p_oracle'] < 0.001 else '**' if s['p_oracle'] < 0.01 else '*' if s['p_oracle'] < 0.05 else 'ns'\n",
    "    print(f\"  {s['short_name']:<25} {s['d_oracle']:>+10.3f} {s['d_random']:>+10.3f} \"\n",
    "          f\"{s['structural_fraction']*100:>9.0f}% {sig:>5}\")\n",
    "\n",
    "# Verdict\n",
    "if n_sig == n_total:\n",
    "    print(f\"\\n  VERDICT: Enrichment effect GENERALIZES across all {n_total} models tested.\")\n",
    "elif n_sig > 0:\n",
    "    print(f\"\\n  VERDICT: Enrichment effect generalizes to {n_sig}/{n_total} models.\")\n",
    "    print(f\"  Not universal — may depend on architecture or training.\")\n",
    "else:\n",
    "    print(f\"\\n  VERDICT: Enrichment effect does NOT generalize.\")\n",
    "    print(f\"  The effect appears specific to T5Gemma.\")\n",
    "\n",
    "# Save\n",
    "final_results = {\n",
    "    'experiment': 'v4_exp09_cross_model_sweep',\n",
    "    'dataset': 'ms_marco_v1.1',\n",
    "    'n_samples': N_SAMPLES,\n",
    "    'seed': SEED,\n",
    "    'models_tested': MODELS,\n",
    "    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'model_results': summary,\n",
    "    't5gemma_reference': T5GEMMA_REF,\n",
    "}\n",
    "\n",
    "with open(RESULTS_DIR / 'results.json', 'w') as f:\n",
    "    json.dump(final_results, f, indent=2)\n",
    "print(f\"\\nResults saved to {RESULTS_DIR / 'results.json'}\")\n",
    "print(\"Done!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}