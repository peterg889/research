{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d256461",
   "metadata": {},
   "source": [
    "# Experiment 07: Decoder Attention Probing\n",
    "\n",
    "## Hypothesis\n",
    "\n",
    "The v3-to-v4 structural collapse (85% → 35%) occurred when we gave the decoder\n",
    "the query as input. **Hypothesis: the query tokens act as attention buffers in the\n",
    "decoder's self-attention** — the same mechanism that the encoder prefix provides\n",
    "in the encoder's bidirectional attention.\n",
    "\n",
    "In the decoder, `[BOS, query_tokens, answer_tokens]` is processed with causal\n",
    "self-attention + merged cross-attention to encoder representations. The decoder's\n",
    "BOS token likely acts as an attention sink (like the encoder's BOS). The query\n",
    "tokens may absorb attention from answer tokens, redistributing the answer-token\n",
    "self-attention budget — exactly the \"attention buffer\" mechanism we identified\n",
    "in the encoder (v3 Exp 3E).\n",
    "\n",
    "If this is correct:\n",
    "1. The decoder's BOS should be a massive attention sink (like encoder BOS)\n",
    "2. Query tokens should absorb significant attention from answer tokens\n",
    "3. This absorption should come at the expense of answer-answer self-attention\n",
    "4. The encoder prefix's contribution to cross-attention redistribution should\n",
    "   shrink when the decoder already has query buffers (the interaction effect)\n",
    "\n",
    "## Design: 2×2 Factorial\n",
    "\n",
    "| # | Condition | Encoder input | Cross-attn mask | Decoder input |\n",
    "|---|-----------|--------------|-----------------|---------------|\n",
    "| 1 | bare_nq | [document] | all visible | [BOS, answer] |\n",
    "| 2 | bare_q | [document] | all visible | [BOS, query, answer] |\n",
    "| 3 | oracle_trunc_nq | [query + document] | doc only | [BOS, answer] |\n",
    "| 4 | oracle_trunc_q | [query + document] | doc only | [BOS, query, answer] |\n",
    "\n",
    "**Key comparisons:**\n",
    "- **(2) vs (1)**: Decoder query buffer effect (no encoder prefix)\n",
    "- **(4) vs (3)**: Decoder query buffer effect (with encoder prefix)\n",
    "- **(3) vs (1)**: Encoder prefix effect (no decoder query)\n",
    "- **(4) vs (2)**: Encoder prefix effect (with decoder query)\n",
    "- **Interaction**: Does having a decoder query reduce the encoder prefix's effect?\n",
    "\n",
    "## Probes (per decoder layer, per condition)\n",
    "\n",
    "**Self-attention budget** (from `decoder_attentions`):\n",
    "- `self_to_bos`: Answer tokens' attention to decoder BOS\n",
    "- `self_to_query`: Answer tokens' attention to query positions (0 for _nq)\n",
    "- `self_to_answer`: Answer tokens' attention to other answer positions\n",
    "- `self_entropy`: Entropy of answer-token self-attention distribution\n",
    "\n",
    "**Cross-attention budget** (from `cross_attentions`):\n",
    "- `cross_total`: Total cross-attention mass per answer token\n",
    "- `cross_entropy`: Entropy of cross-attention distribution over encoder positions\n",
    "\n",
    "**Budget check**: `self_total + cross_total = 1.0` (merged softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63a74cb7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T22:15:04.107850Z",
     "iopub.status.busy": "2026-02-19T22:15:04.107294Z",
     "iopub.status.idle": "2026-02-19T22:15:25.471768Z",
     "shell.execute_reply": "2026-02-19T22:15:25.470798Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading google/t5gemma-2-4b-4b with attn_implementation='eager'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4f85c31213949abbce44d27a0e69b72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/1327 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exp 07: Decoder Attention Probing\n",
      "N: 500, Model: google/t5gemma-2-4b-4b\n",
      "DEVICE: cuda:0, dtype: torch.bfloat16\n",
      "GPU memory: 15.02 GB\n",
      "Decoder layers: 34\n",
      "Probe layers: [0, 5, 11, 17, 22, 33]\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Setup and model loading (EAGER attention for weight extraction)\n",
    "import os\n",
    "os.umask(0o000)\n",
    "\n",
    "import sys, json, time, gc, re\n",
    "import random as pyrandom\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "from collections import Counter, defaultdict\n",
    "from scipy import stats\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "sys.path.insert(0, \"../../..\")\n",
    "from lib.analysis import cohens_d\n",
    "\n",
    "SEED = 42\n",
    "N_SAMPLES = 500\n",
    "MODEL_NAME = \"google/t5gemma-2-4b-4b\"\n",
    "\n",
    "RESULTS_DIR = Path(\"../../../results/exp07\")\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CHECKPOINT_PATH = RESULTS_DIR / \"checkpoint.json\"\n",
    "\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "pyrandom.seed(SEED)\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv())\n",
    "HF_TOKEN = os.environ.get(\"HF_TOKEN\")\n",
    "\n",
    "from transformers import AutoProcessor, AutoModelForSeq2SeqLM\n",
    "\n",
    "print(f\"Loading {MODEL_NAME} with attn_implementation='eager'...\")\n",
    "processor = AutoProcessor.from_pretrained(MODEL_NAME, token=HF_TOKEN)\n",
    "tokenizer = processor.tokenizer\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    MODEL_NAME, device_map=\"auto\", torch_dtype=torch.bfloat16, token=HF_TOKEN,\n",
    "    attn_implementation=\"eager\",\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "DEVICE = next(model.parameters()).device\n",
    "BOS_ID = getattr(model.config, 'decoder_start_token_id', None) or tokenizer.bos_token_id\n",
    "\n",
    "# Discover decoder layer count\n",
    "N_DEC_LAYERS = len(model.model.decoder.layers)\n",
    "# Probe 6 representative layers (evenly spaced)\n",
    "PROBE_LAYERS = [0, N_DEC_LAYERS // 6, N_DEC_LAYERS // 3,\n",
    "                N_DEC_LAYERS // 2, 2 * N_DEC_LAYERS // 3, N_DEC_LAYERS - 1]\n",
    "\n",
    "print(f\"Exp 07: Decoder Attention Probing\")\n",
    "print(f\"N: {N_SAMPLES}, Model: {MODEL_NAME}\")\n",
    "print(f\"DEVICE: {DEVICE}, dtype: {next(model.parameters()).dtype}\")\n",
    "print(f\"GPU memory: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "print(f\"Decoder layers: {N_DEC_LAYERS}\")\n",
    "print(f\"Probe layers: {PROBE_LAYERS}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b1ae121",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T22:15:25.475793Z",
     "iopub.status.busy": "2026-02-19T22:15:25.474777Z",
     "iopub.status.idle": "2026-02-19T22:15:27.383728Z",
     "shell.execute_reply": "2026-02-19T22:15:27.382804Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MS MARCO v1.1 validation...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total candidates: 1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 500 samples\n",
      "Mean passage words: 74\n",
      "Mean query words: 6\n",
      "Mean answer words: 14\n",
      "Mean oracle prefix tokens: 7.5\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Load MS MARCO data (same pipeline as Exp 01-06)\n",
    "from lib.data import count_words\n",
    "from datasets import load_dataset\n",
    "\n",
    "print(\"Loading MS MARCO v1.1 validation...\")\n",
    "ds = load_dataset(\"microsoft/ms_marco\", \"v1.1\", split=\"validation\")\n",
    "\n",
    "all_candidates = []\n",
    "for item in ds:\n",
    "    if len(all_candidates) >= 3 * N_SAMPLES:\n",
    "        break\n",
    "    passages = item.get('passages', {})\n",
    "    ptexts = passages.get('passage_text', [])\n",
    "    is_sel = passages.get('is_selected', [])\n",
    "    query = item.get('query', '')\n",
    "    answers = item.get('answers', [])\n",
    "    well_formed = item.get('wellFormedAnswers', [])\n",
    "    answer = None\n",
    "    if well_formed and len(well_formed) > 0 and well_formed[0] not in ('[]', ''):\n",
    "        answer = well_formed[0]\n",
    "    elif answers and len(answers) > 0 and answers[0] != 'No Answer Present.':\n",
    "        answer = answers[0]\n",
    "    if not answer:\n",
    "        continue\n",
    "    for pt, sel in zip(ptexts, is_sel):\n",
    "        wc = count_words(pt)\n",
    "        if sel == 1 and 30 <= wc <= 300:\n",
    "            all_candidates.append({\n",
    "                'passage': pt, 'query': query, 'answer': answer,\n",
    "                'word_count': wc,\n",
    "            })\n",
    "            break\n",
    "\n",
    "print(f\"Total candidates: {len(all_candidates)}\")\n",
    "np.random.seed(SEED)\n",
    "indices = np.random.permutation(len(all_candidates))\n",
    "samples = [all_candidates[i] for i in indices[:N_SAMPLES]]\n",
    "del ds, all_candidates\n",
    "gc.collect()\n",
    "\n",
    "# Count prefix tokens for oracle conditions\n",
    "def count_prefix_tokens(prefix_text, document_text):\n",
    "    full_text = prefix_text + \"\\n\" + document_text\n",
    "    full_ids = tokenizer(full_text, add_special_tokens=True, truncation=True,\n",
    "                         max_length=2048).input_ids\n",
    "    doc_ids = tokenizer(document_text, add_special_tokens=True, truncation=True,\n",
    "                        max_length=2048).input_ids\n",
    "    return len(full_ids) - len(doc_ids)\n",
    "\n",
    "for s in samples:\n",
    "    s['n_pfx_oracle'] = count_prefix_tokens(s['query'], s['passage'])\n",
    "\n",
    "print(f\"Loaded {len(samples)} samples\")\n",
    "print(f\"Mean passage words: {np.mean([s['word_count'] for s in samples]):.0f}\")\n",
    "print(f\"Mean query words: {np.mean([count_words(s['query']) for s in samples]):.0f}\")\n",
    "print(f\"Mean answer words: {np.mean([count_words(s['answer']) for s in samples]):.0f}\")\n",
    "print(f\"Mean oracle prefix tokens: {np.mean([s['n_pfx_oracle'] for s in samples]):.1f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94418e00",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T22:15:27.387399Z",
     "iopub.status.busy": "2026-02-19T22:15:27.386916Z",
     "iopub.status.idle": "2026-02-19T22:15:28.371297Z",
     "shell.execute_reply": "2026-02-19T22:15:28.370412Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verifying attention output structure...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Decoder seq len: 11 (1 BOS + 9 query + 1 answer)\n",
      "  Encoder seq len: 103\n",
      "  decoder_attentions: 34 layers\n",
      "    Shape per layer: torch.Size([1, 8, 11, 11])\n",
      "  cross_attentions: 34 layers\n",
      "    Shape per layer: torch.Size([1, 8, 11, 103])\n",
      "\n",
      "  Budget check (self + cross per position):\n",
      "    Min: 0.999672\n",
      "    Max: 1.000897\n",
      "    Mean: 1.000177\n",
      "    (Should be ~1.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probing function defined. Ready for scoring loop.\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Verify attention output structure and define probing function\n",
    "\n",
    "# Test forward pass to verify shapes\n",
    "print(\"Verifying attention output structure...\")\n",
    "s0 = samples[0]\n",
    "\n",
    "# Encode bare document\n",
    "enc_ids = tokenizer(s0['passage'], return_tensors=\"pt\",\n",
    "                    add_special_tokens=True, truncation=True,\n",
    "                    max_length=2048).input_ids.to(DEVICE)\n",
    "enc_mask = torch.ones(1, enc_ids.shape[1], device=DEVICE, dtype=torch.long)\n",
    "with torch.no_grad():\n",
    "    encoder_outputs = model.get_encoder()(input_ids=enc_ids, attention_mask=enc_mask)\n",
    "\n",
    "# Decoder with query prefix\n",
    "query_ids = tokenizer(s0['query'], add_special_tokens=False, truncation=True,\n",
    "                      max_length=512).input_ids\n",
    "answer_ids = tokenizer(s0['answer'], add_special_tokens=False, truncation=True,\n",
    "                       max_length=256).input_ids\n",
    "dec_ids = [BOS_ID] + query_ids + answer_ids\n",
    "dec_tensor = torch.tensor([dec_ids], dtype=torch.long, device=DEVICE)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(\n",
    "        encoder_outputs=encoder_outputs,\n",
    "        attention_mask=enc_mask,\n",
    "        decoder_input_ids=dec_tensor,\n",
    "        output_attentions=True,\n",
    "    )\n",
    "\n",
    "# Check what we got\n",
    "dec_len = len(dec_ids)\n",
    "enc_len = enc_ids.shape[1]\n",
    "print(f\"  Decoder seq len: {dec_len} (1 BOS + {len(query_ids)} query + {len(answer_ids)} answer)\")\n",
    "print(f\"  Encoder seq len: {enc_len}\")\n",
    "\n",
    "if outputs.decoder_attentions is not None:\n",
    "    print(f\"  decoder_attentions: {len(outputs.decoder_attentions)} layers\")\n",
    "    print(f\"    Shape per layer: {outputs.decoder_attentions[0].shape}\")\n",
    "    # Expected: [1, heads, dec_len, dec_len]\n",
    "else:\n",
    "    print(\"  WARNING: decoder_attentions is None!\")\n",
    "\n",
    "if outputs.cross_attentions is not None:\n",
    "    print(f\"  cross_attentions: {len(outputs.cross_attentions)} layers\")\n",
    "    print(f\"    Shape per layer: {outputs.cross_attentions[0].shape}\")\n",
    "    # Expected: [1, heads, dec_len, enc_len]\n",
    "else:\n",
    "    print(\"  WARNING: cross_attentions is None!\")\n",
    "\n",
    "# Verify merged softmax: self + cross should sum to 1.0\n",
    "sa = outputs.decoder_attentions[0][0].float().mean(dim=0)  # [dec_len, dec_len]\n",
    "ca = outputs.cross_attentions[0][0].float().mean(dim=0)    # [dec_len, enc_len]\n",
    "budget_sum = sa.sum(dim=1) + ca.sum(dim=1)  # [dec_len]\n",
    "print(f\"\\n  Budget check (self + cross per position):\")\n",
    "print(f\"    Min: {budget_sum.min().item():.6f}\")\n",
    "print(f\"    Max: {budget_sum.max().item():.6f}\")\n",
    "print(f\"    Mean: {budget_sum.mean().item():.6f}\")\n",
    "print(f\"    (Should be ~1.0)\")\n",
    "\n",
    "del outputs, encoder_outputs\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "# === Probing function ===\n",
    "def forward_with_probes(encoder_outputs, cross_attn_mask, decoder_input_ids,\n",
    "                        answer_start, answer_len, answer_ids_list):\n",
    "    # Forward pass with attention extraction.\n",
    "    # Returns (nll, probes_dict) where probes_dict is keyed by layer index.\n",
    "    dec_len = decoder_input_ids.shape[1]\n",
    "    n_query = answer_start - 1  # 0 for _nq, len(query_ids) for _q\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            encoder_outputs=encoder_outputs,\n",
    "            attention_mask=cross_attn_mask,\n",
    "            decoder_input_ids=decoder_input_ids,\n",
    "            output_attentions=True,\n",
    "        )\n",
    "\n",
    "    # --- NLL ---\n",
    "    logits = outputs.logits\n",
    "    answer_logits = logits[0, n_query:n_query + answer_len, :]\n",
    "    targets = torch.tensor(answer_ids_list, dtype=torch.long, device=DEVICE)\n",
    "    log_probs = F.log_softmax(answer_logits, dim=-1)\n",
    "    token_log_probs = log_probs.gather(1, targets.unsqueeze(1)).squeeze(1)\n",
    "    nll = -token_log_probs.mean().item()\n",
    "\n",
    "    # --- Probes ---\n",
    "    probes = {}\n",
    "    for layer_idx in PROBE_LAYERS:\n",
    "        # Self-attention: [1, heads, dec_len, dec_len]\n",
    "        sa = outputs.decoder_attentions[layer_idx][0].float().mean(dim=0)  # [dec_len, dec_len]\n",
    "        # Cross-attention: [1, heads, dec_len, enc_len]\n",
    "        ca = outputs.cross_attentions[layer_idx][0].float().mean(dim=0)  # [dec_len, enc_len]\n",
    "\n",
    "        # Extract answer-token rows\n",
    "        ans_sa = sa[answer_start:answer_start + answer_len]  # [M, dec_len]\n",
    "        ans_ca = ca[answer_start:answer_start + answer_len]  # [M, enc_len]\n",
    "\n",
    "        # Self-attention budget decomposition for answer tokens\n",
    "        self_to_bos = ans_sa[:, 0].mean().item()\n",
    "\n",
    "        if n_query > 0:\n",
    "            self_to_query = ans_sa[:, 1:1 + n_query].sum(dim=1).mean().item()\n",
    "        else:\n",
    "            self_to_query = 0.0\n",
    "\n",
    "        # Self-attention to answer positions (including self)\n",
    "        # For answer token t at absolute position p=answer_start+t,\n",
    "        # attend to positions answer_start..p (causal)\n",
    "        answer_mask = torch.zeros(answer_len, dec_len, device=DEVICE)\n",
    "        for t in range(answer_len):\n",
    "            p = answer_start + t\n",
    "            answer_mask[t, answer_start:p + 1] = 1.0\n",
    "        self_to_answer = (ans_sa * answer_mask).sum(dim=1).mean().item()\n",
    "\n",
    "        # Totals\n",
    "        self_total = ans_sa.sum(dim=1).mean().item()\n",
    "        cross_total = ans_ca.sum(dim=1).mean().item()\n",
    "\n",
    "        # Self-attention entropy (over causal positions 0..p for each answer token)\n",
    "        eps = 1e-10\n",
    "        # Build per-token causal mask\n",
    "        positions = torch.arange(dec_len, device=DEVICE)\n",
    "        abs_positions = torch.arange(answer_start, answer_start + answer_len, device=DEVICE)\n",
    "        causal = (positions.unsqueeze(0) <= abs_positions.unsqueeze(1)).float()  # [M, dec_len]\n",
    "        masked_sa = ans_sa * causal  # zero out non-causal\n",
    "        sa_clamped = masked_sa.clamp(min=eps)\n",
    "        self_entropy = -(masked_sa * sa_clamped.log()).sum(dim=1).mean().item()\n",
    "\n",
    "        # Cross-attention entropy (over all encoder positions)\n",
    "        ca_clamped = ans_ca.clamp(min=eps)\n",
    "        cross_entropy = -(ans_ca * ca_clamped.log()).sum(dim=1).mean().item()\n",
    "\n",
    "        probes[layer_idx] = {\n",
    "            'sb': round(self_to_bos, 6),\n",
    "            'sq': round(self_to_query, 6),\n",
    "            'sa': round(self_to_answer, 6),\n",
    "            'st': round(self_total, 6),\n",
    "            'ct': round(cross_total, 6),\n",
    "            'se': round(self_entropy, 4),\n",
    "            'ce': round(cross_entropy, 4),\n",
    "        }\n",
    "\n",
    "    del outputs, logits, log_probs\n",
    "    return nll, probes\n",
    "\n",
    "\n",
    "print(\"Probing function defined. Ready for scoring loop.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3056b333",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T22:15:28.374770Z",
     "iopub.status.busy": "2026-02-19T22:15:28.374466Z",
     "iopub.status.idle": "2026-02-20T09:44:55.577421Z",
     "shell.execute_reply": "2026-02-20T09:44:55.576392Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PROBING ALL CONDITIONS\n",
      "======================================================================\n",
      "Starting fresh: 4 conditions x 500 samples\n",
      "Probe layers: [0, 5, 11, 17, 22, 33]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42fcd42dbfa642edbda1c58392ff9df8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Probing:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 20/500 | 0.3m | ETA 7.4m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 40/500 | 0.7m | ETA 7.8m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 60/500 | 1.1m | ETA 8.3m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 80/500 | 1.9m | ETA 9.9m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 100/500 | 3.4m | ETA 13.7m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 120/500 | 5.8m | ETA 18.3m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 140/500 | 9.4m | ETA 24.3m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 160/500 | 14.7m | ETA 31.2m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 180/500 | 21.6m | ETA 38.5m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 200/500 | 31.0m | ETA 46.5m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 220/500 | 43.0m | ETA 54.8m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 240/500 | 57.6m | ETA 62.4m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 260/500 | 75.9m | ETA 70.1m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 280/500 | 97.8m | ETA 76.8m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 300/500 | 122.9m | ETA 81.9m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 320/500 | 152.7m | ETA 85.9m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 340/500 | 187.7m | ETA 88.3m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 360/500 | 227.2m | ETA 88.4m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 380/500 | 273.5m | ETA 86.4m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 400/500 | 324.5m | ETA 81.1m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 420/500 | 381.2m | ETA 72.6m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 440/500 | 446.2m | ETA 60.8m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 460/500 | 519.6m | ETA 45.2m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 480/500 | 600.2m | ETA 25.0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 500/500 | 689.4m | ETA 0.0m\n",
      "\n",
      "Probing complete: 500 samples, 4 conditions in 689.5 min\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Probing loop — 4 conditions x 500 samples\n",
    "print(\"=\" * 70)\n",
    "print(\"PROBING ALL CONDITIONS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "COND_NAMES = ['bare_nq', 'bare_q', 'oracle_trunc_nq', 'oracle_trunc_q']\n",
    "\n",
    "results = []\n",
    "start_idx = 0\n",
    "\n",
    "if CHECKPOINT_PATH.exists():\n",
    "    ckpt = json.loads(CHECKPOINT_PATH.read_text())\n",
    "    if ckpt.get('n_total') == N_SAMPLES and len(ckpt.get('results', [])) > 0:\n",
    "        saved_queries = [r['query'][:50] for r in ckpt['results']]\n",
    "        current_queries = [s['query'][:50] for s in samples[:len(saved_queries)]]\n",
    "        if saved_queries == current_queries:\n",
    "            results = ckpt['results']\n",
    "            # JSON converts int dict keys to strings — convert back\n",
    "            for r in results:\n",
    "                for cond in COND_NAMES:\n",
    "                    key = f'probes_{cond}'\n",
    "                    if key in r and isinstance(r[key], dict):\n",
    "                        r[key] = {int(k): v for k, v in r[key].items()}\n",
    "            start_idx = len(results)\n",
    "            print(f\"Resuming from checkpoint: {start_idx}/{N_SAMPLES}\")\n",
    "\n",
    "if start_idx == 0:\n",
    "    print(f\"Starting fresh: {len(COND_NAMES)} conditions x {N_SAMPLES} samples\")\n",
    "    print(f\"Probe layers: {PROBE_LAYERS}\")\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "for i in tqdm(range(start_idx, N_SAMPLES), initial=start_idx, total=N_SAMPLES,\n",
    "              desc=\"Probing\"):\n",
    "    s = samples[i]\n",
    "    query = s['query']\n",
    "    passage = s['passage']\n",
    "    answer = s['answer']\n",
    "\n",
    "    query_ids = tokenizer(query, add_special_tokens=False, truncation=True,\n",
    "                          max_length=512).input_ids\n",
    "    answer_ids = tokenizer(answer, add_special_tokens=False, truncation=True,\n",
    "                           max_length=256).input_ids\n",
    "\n",
    "    if len(answer_ids) == 0:\n",
    "        continue\n",
    "\n",
    "    result = {\n",
    "        'query': query[:50],\n",
    "        'n_query_toks': len(query_ids),\n",
    "        'n_answer_toks': len(answer_ids),\n",
    "    }\n",
    "\n",
    "    # --- Encoder pass 1: bare document ---\n",
    "    enc_ids_bare = tokenizer(passage, return_tensors=\"pt\",\n",
    "                             add_special_tokens=True, truncation=True,\n",
    "                             max_length=2048).input_ids.to(DEVICE)\n",
    "    enc_len_bare = enc_ids_bare.shape[1]\n",
    "    enc_mask_bare = torch.ones(1, enc_len_bare, device=DEVICE, dtype=torch.long)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        enc_out_bare = model.get_encoder()(\n",
    "            input_ids=enc_ids_bare, attention_mask=enc_mask_bare\n",
    "        )\n",
    "\n",
    "    # Condition 1: bare_nq — decoder=[BOS, answer]\n",
    "    dec_nq = torch.tensor([[BOS_ID] + answer_ids], dtype=torch.long, device=DEVICE)\n",
    "    nll, probes = forward_with_probes(\n",
    "        enc_out_bare, enc_mask_bare, dec_nq,\n",
    "        answer_start=1, answer_len=len(answer_ids), answer_ids_list=answer_ids)\n",
    "    result['nll_bare_nq'] = nll\n",
    "    result['probes_bare_nq'] = probes\n",
    "\n",
    "    # Condition 2: bare_q — decoder=[BOS, query, answer]\n",
    "    dec_q = torch.tensor([[BOS_ID] + query_ids + answer_ids],\n",
    "                         dtype=torch.long, device=DEVICE)\n",
    "    nll, probes = forward_with_probes(\n",
    "        enc_out_bare, enc_mask_bare, dec_q,\n",
    "        answer_start=1 + len(query_ids), answer_len=len(answer_ids),\n",
    "        answer_ids_list=answer_ids)\n",
    "    result['nll_bare_q'] = nll\n",
    "    result['probes_bare_q'] = probes\n",
    "\n",
    "    del enc_out_bare\n",
    "\n",
    "    # --- Encoder pass 2: oracle (query + document) ---\n",
    "    enc_text_oracle = query + \"\\n\" + passage\n",
    "    enc_ids_oracle = tokenizer(enc_text_oracle, return_tensors=\"pt\",\n",
    "                               add_special_tokens=True, truncation=True,\n",
    "                               max_length=2048).input_ids.to(DEVICE)\n",
    "    enc_len_oracle = enc_ids_oracle.shape[1]\n",
    "    enc_mask_oracle = torch.ones(1, enc_len_oracle, device=DEVICE, dtype=torch.long)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        enc_out_oracle = model.get_encoder()(\n",
    "            input_ids=enc_ids_oracle, attention_mask=enc_mask_oracle\n",
    "        )\n",
    "\n",
    "    # Cross-attention mask: hide prefix (query + BOS)\n",
    "    pfx_count = s['n_pfx_oracle']\n",
    "    cross_mask_trunc = torch.ones(1, enc_len_oracle, device=DEVICE, dtype=torch.long)\n",
    "    cross_mask_trunc[:, :pfx_count] = 0\n",
    "\n",
    "    # Condition 3: oracle_trunc_nq — decoder=[BOS, answer]\n",
    "    nll, probes = forward_with_probes(\n",
    "        enc_out_oracle, cross_mask_trunc, dec_nq,\n",
    "        answer_start=1, answer_len=len(answer_ids), answer_ids_list=answer_ids)\n",
    "    result['nll_oracle_trunc_nq'] = nll\n",
    "    result['probes_oracle_trunc_nq'] = probes\n",
    "\n",
    "    # Condition 4: oracle_trunc_q — decoder=[BOS, query, answer]\n",
    "    nll, probes = forward_with_probes(\n",
    "        enc_out_oracle, cross_mask_trunc, dec_q,\n",
    "        answer_start=1 + len(query_ids), answer_len=len(answer_ids),\n",
    "        answer_ids_list=answer_ids)\n",
    "    result['nll_oracle_trunc_q'] = nll\n",
    "    result['probes_oracle_trunc_q'] = probes\n",
    "\n",
    "    del enc_out_oracle\n",
    "    results.append(result)\n",
    "\n",
    "    if (i + 1) % 20 == 0 or i == N_SAMPLES - 1:\n",
    "        ckpt = {\n",
    "            'n_total': N_SAMPLES,\n",
    "            'probe_layers': PROBE_LAYERS,\n",
    "            'results': results,\n",
    "            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        }\n",
    "        CHECKPOINT_PATH.write_text(json.dumps(ckpt))\n",
    "        elapsed = time.time() - t0\n",
    "        done = i - start_idx + 1\n",
    "        eta = (N_SAMPLES - i - 1) * elapsed / done if done > 0 else 0\n",
    "        tqdm.write(f\"  Checkpoint {i+1}/{N_SAMPLES} | {elapsed/60:.1f}m | ETA {eta/60:.1f}m\")\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "elapsed = time.time() - t0\n",
    "print(f\"\\nProbing complete: {len(results)} samples, \"\n",
    "      f\"{len(COND_NAMES)} conditions in {elapsed/60:.1f} min\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64839a3d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-20T09:44:55.581663Z",
     "iopub.status.busy": "2026-02-20T09:44:55.580966Z",
     "iopub.status.idle": "2026-02-20T09:44:55.605728Z",
     "shell.execute_reply": "2026-02-20T09:44:55.605038Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "NLL CALIBRATION\n",
      "======================================================================\n",
      "\n",
      "  Condition                   Mean NLL  d vs bare   sig\n",
      "  -------------------------------------------------------\n",
      "  bare_nq                       3.6834         --    --\n",
      "  oracle_trunc_nq               2.9969     +0.366   ***\n",
      "  bare_q                        2.5537         --    --\n",
      "  oracle_trunc_q                2.3967     +0.238   ***\n",
      "\n",
      "  Query in decoder effect (bare_nq → bare_q): d=+0.309 (p=1.52e-11)\n",
      "  (Expected: large positive — query helps predict answer)\n",
      "\n",
      "======================================================================\n",
      "ATTENTION BUDGET OVERVIEW (last probe layer)\n",
      "======================================================================\n",
      "Layer 33 — mean over 500 samples, averaged over heads and answer tokens\n",
      "\n",
      "  Condition                   self_bos self_query   self_ans   self_tot  cross_tot    check\n",
      "  -----------------------------------------------------------------------------------\n",
      "  bare_nq                       0.1305     0.0000     0.4996     0.6301     0.3699   1.0000\n",
      "  bare_q                        0.1086     0.0546     0.4786     0.6417     0.3583   1.0000\n",
      "  oracle_trunc_nq               0.2123     0.0000     0.6589     0.8712     0.1289   1.0000\n",
      "  oracle_trunc_q                0.1731     0.0610     0.6591     0.8932     0.1068   1.0000\n",
      "\n",
      "  Budget check: self_total + cross_total should = 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: NLL calibration and attention budget overview\n",
    "print(\"=\" * 70)\n",
    "print(\"NLL CALIBRATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "nll_bare_nq = np.array([r['nll_bare_nq'] for r in results])\n",
    "nll_bare_q = np.array([r['nll_bare_q'] for r in results])\n",
    "nll_oracle_nq = np.array([r['nll_oracle_trunc_nq'] for r in results])\n",
    "nll_oracle_q = np.array([r['nll_oracle_trunc_q'] for r in results])\n",
    "\n",
    "# Expected from Exp 01: oracle_trunc_q vs bare_q: d~+0.228, oracle_trunc_nq vs bare_nq: d~+0.376\n",
    "print(f\"\\n  {'Condition':<25} {'Mean NLL':>10} {'d vs bare':>10} {'sig':>5}\")\n",
    "print(f\"  {'-'*55}\")\n",
    "\n",
    "for name, nlls, baseline, bl_name in [\n",
    "    ('bare_nq', nll_bare_nq, None, None),\n",
    "    ('oracle_trunc_nq', nll_oracle_nq, nll_bare_nq, 'bare_nq'),\n",
    "    ('bare_q', nll_bare_q, None, None),\n",
    "    ('oracle_trunc_q', nll_oracle_q, nll_bare_q, 'bare_q'),\n",
    "]:\n",
    "    if baseline is None:\n",
    "        print(f\"  {name:<25} {nlls.mean():>10.4f} {'--':>10} {'--':>5}\")\n",
    "    else:\n",
    "        diff = baseline - nlls\n",
    "        d = cohens_d(diff)\n",
    "        _, p = stats.ttest_1samp(diff, 0)\n",
    "        sig = '***' if p < 0.001 else '**' if p < 0.01 else '*' if p < 0.05 else 'ns'\n",
    "        print(f\"  {name:<25} {nlls.mean():>10.4f} {d:>+10.3f} {sig:>5}\")\n",
    "\n",
    "# Query-in-decoder effect on bare NLL\n",
    "diff_q = nll_bare_nq - nll_bare_q\n",
    "d_q = cohens_d(diff_q)\n",
    "_, p_q = stats.ttest_1samp(diff_q, 0)\n",
    "print(f\"\\n  Query in decoder effect (bare_nq → bare_q): d={d_q:+.3f} (p={p_q:.2e})\")\n",
    "print(f\"  (Expected: large positive — query helps predict answer)\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"ATTENTION BUDGET OVERVIEW (last probe layer)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "last_layer = PROBE_LAYERS[-1]\n",
    "print(f\"Layer {last_layer} — mean over {len(results)} samples, averaged over heads and answer tokens\")\n",
    "print(f\"\\n  {'Condition':<25} {'self_bos':>10} {'self_query':>10} {'self_ans':>10} \"\n",
    "      f\"{'self_tot':>10} {'cross_tot':>10} {'check':>8}\")\n",
    "print(f\"  {'-'*83}\")\n",
    "\n",
    "for cond in ['bare_nq', 'bare_q', 'oracle_trunc_nq', 'oracle_trunc_q']:\n",
    "    sb = np.mean([r[f'probes_{cond}'][last_layer]['sb'] for r in results])\n",
    "    sq = np.mean([r[f'probes_{cond}'][last_layer]['sq'] for r in results])\n",
    "    sa = np.mean([r[f'probes_{cond}'][last_layer]['sa'] for r in results])\n",
    "    st = np.mean([r[f'probes_{cond}'][last_layer]['st'] for r in results])\n",
    "    ct = np.mean([r[f'probes_{cond}'][last_layer]['ct'] for r in results])\n",
    "    check = st + ct\n",
    "    print(f\"  {cond:<25} {sb:>10.4f} {sq:>10.4f} {sa:>10.4f} \"\n",
    "          f\"{st:>10.4f} {ct:>10.4f} {check:>8.4f}\")\n",
    "\n",
    "print(f\"\\n  Budget check: self_total + cross_total should = 1.0000\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "daef60ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-20T09:44:55.608769Z",
     "iopub.status.busy": "2026-02-20T09:44:55.608478Z",
     "iopub.status.idle": "2026-02-20T09:44:55.653039Z",
     "shell.execute_reply": "2026-02-20T09:44:55.652108Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PROBE A: DECODER BOS SINK AND QUERY BUFFER\n",
      "======================================================================\n",
      "\n",
      "--- BOS attention sink (answer tokens → decoder BOS) ---\n",
      "\n",
      "   Layer    bare_nq     bare_q     orc_nq      orc_q    q effect          p\n",
      "  -------------------------------------------------------------------------\n",
      "       0     0.3826     0.3239     0.3905     0.3249      +1.316  3.91e-111 ***\n",
      "       5     0.4843     0.4790     0.4925     0.4976      +0.207   4.54e-06 ***\n",
      "      11     0.2826     0.2959     0.2934     0.3110      -0.353   1.84e-14 ***\n",
      "      17     0.3181     0.3138     0.3622     0.3511      +0.133   3.13e-03 **\n",
      "      22     0.2838     0.2779     0.3037     0.2982      +0.323   1.83e-12 ***\n",
      "      33     0.1305     0.1086     0.2123     0.1731      +1.641  8.31e-144 ***\n",
      "\n",
      "--- Query as attention buffer (answer tokens → query positions) ---\n",
      "  (Only nonzero for _q conditions)\n",
      "\n",
      "   Layer     bare_q      orc_q       diff\n",
      "  ----------------------------------------\n",
      "       0     0.1165     0.1178    +0.0013\n",
      "       5     0.0628     0.0587    -0.0041\n",
      "      11     0.0617     0.0554    -0.0063\n",
      "      17     0.0599     0.0474    -0.0126\n",
      "      22     0.0736     0.0609    -0.0127\n",
      "      33     0.0546     0.0610    +0.0064\n",
      "\n",
      "--- Where does query attention come from? ---\n",
      "  Compare bare_nq vs bare_q (no encoder prefix)\n",
      "\n",
      "   Layer   BOS change   Answer chg    Cross chg\n",
      "  --------------------------------------------------\n",
      "       0      -0.0586      -0.0021      -0.0558\n",
      "       5      -0.0053      +0.0048      -0.0624\n",
      "      11      +0.0133      +0.0137      -0.0888\n",
      "      17      -0.0043      +0.0116      -0.0672\n",
      "      22      -0.0059      +0.0025      -0.0703\n",
      "      33      -0.0219      -0.0210      -0.0116\n",
      "\n",
      "  (Negative = query steals FROM that budget. Positive = that budget grows.)\n",
      "  Query buffer mass at last layer:\n",
      "  = 0.0546 (5.5% of total attention budget)\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Probe A — Decoder BOS sink and query as attention buffer\n",
    "print(\"=\" * 70)\n",
    "print(\"PROBE A: DECODER BOS SINK AND QUERY BUFFER\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Layer-by-layer trajectory of BOS sink mass\n",
    "print(f\"\\n--- BOS attention sink (answer tokens → decoder BOS) ---\")\n",
    "print(f\"\\n  {'Layer':>6} {'bare_nq':>10} {'bare_q':>10} {'orc_nq':>10} {'orc_q':>10}  \"\n",
    "      f\"{'q effect':>10} {'p':>10}\")\n",
    "print(f\"  {'-'*73}\")\n",
    "\n",
    "for layer in PROBE_LAYERS:\n",
    "    vals = {}\n",
    "    for cond in ['bare_nq', 'bare_q', 'oracle_trunc_nq', 'oracle_trunc_q']:\n",
    "        vals[cond] = np.array([r[f'probes_{cond}'][layer]['sb'] for r in results])\n",
    "\n",
    "    # Query buffer effect on BOS: does adding query reduce BOS attention?\n",
    "    diff_bos = vals['bare_nq'] - vals['bare_q']\n",
    "    d_bos = cohens_d(diff_bos)\n",
    "    _, p_bos = stats.ttest_1samp(diff_bos, 0)\n",
    "    sig = '***' if p_bos < 0.001 else '**' if p_bos < 0.01 else '*' if p_bos < 0.05 else 'ns'\n",
    "\n",
    "    print(f\"  {layer:>6} {vals['bare_nq'].mean():>10.4f} {vals['bare_q'].mean():>10.4f} \"\n",
    "          f\"{vals['oracle_trunc_nq'].mean():>10.4f} {vals['oracle_trunc_q'].mean():>10.4f}  \"\n",
    "          f\"{d_bos:>+10.3f} {p_bos:>10.2e} {sig}\")\n",
    "\n",
    "# Query buffer mass: how much attention do answer tokens give to query positions?\n",
    "print(f\"\\n--- Query as attention buffer (answer tokens → query positions) ---\")\n",
    "print(f\"  (Only nonzero for _q conditions)\")\n",
    "print(f\"\\n  {'Layer':>6} {'bare_q':>10} {'orc_q':>10} {'diff':>10}\")\n",
    "print(f\"  {'-'*40}\")\n",
    "\n",
    "for layer in PROBE_LAYERS:\n",
    "    sq_bare = np.array([r['probes_bare_q'][layer]['sq'] for r in results])\n",
    "    sq_orc = np.array([r['probes_oracle_trunc_q'][layer]['sq'] for r in results])\n",
    "    print(f\"  {layer:>6} {sq_bare.mean():>10.4f} {sq_orc.mean():>10.4f} \"\n",
    "          f\"{sq_orc.mean() - sq_bare.mean():>+10.4f}\")\n",
    "\n",
    "# Where does the query buffer steal attention FROM?\n",
    "print(f\"\\n--- Where does query attention come from? ---\")\n",
    "print(f\"  Compare bare_nq vs bare_q (no encoder prefix)\")\n",
    "print(f\"\\n  {'Layer':>6} {'BOS change':>12} {'Answer chg':>12} {'Cross chg':>12}\")\n",
    "print(f\"  {'-'*50}\")\n",
    "\n",
    "for layer in PROBE_LAYERS:\n",
    "    sb_nq = np.mean([r['probes_bare_nq'][layer]['sb'] for r in results])\n",
    "    sb_q = np.mean([r['probes_bare_q'][layer]['sb'] for r in results])\n",
    "    sa_nq = np.mean([r['probes_bare_nq'][layer]['sa'] for r in results])\n",
    "    sa_q = np.mean([r['probes_bare_q'][layer]['sa'] for r in results])\n",
    "    ct_nq = np.mean([r['probes_bare_nq'][layer]['ct'] for r in results])\n",
    "    ct_q = np.mean([r['probes_bare_q'][layer]['ct'] for r in results])\n",
    "\n",
    "    print(f\"  {layer:>6} {sb_q - sb_nq:>+12.4f} {sa_q - sa_nq:>+12.4f} {ct_q - ct_nq:>+12.4f}\")\n",
    "\n",
    "print(f\"\\n  (Negative = query steals FROM that budget. Positive = that budget grows.)\")\n",
    "print(f\"  Query buffer mass at last layer:\")\n",
    "sq_last = np.mean([r['probes_bare_q'][PROBE_LAYERS[-1]]['sq'] for r in results])\n",
    "print(f\"  = {sq_last:.4f} ({sq_last*100:.1f}% of total attention budget)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37379840",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-20T09:44:55.656232Z",
     "iopub.status.busy": "2026-02-20T09:44:55.655974Z",
     "iopub.status.idle": "2026-02-20T09:44:55.706746Z",
     "shell.execute_reply": "2026-02-20T09:44:55.705962Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PROBE B: SELF VS CROSS ALLOCATION\n",
      "======================================================================\n",
      "\n",
      "--- Cross-attention total mass (answer tokens → encoder) ---\n",
      "\n",
      "   Layer    bare_nq     bare_q     orc_nq      orc_q\n",
      "  --------------------------------------------------\n",
      "       0     0.2707     0.2149     0.2558     0.2087\n",
      "       5     0.3783     0.3159     0.3725     0.3001\n",
      "      11     0.6319     0.5431     0.6114     0.5319\n",
      "      17     0.6024     0.5352     0.5660     0.5241\n",
      "      22     0.3509     0.2806     0.3140     0.2531\n",
      "      33     0.3699     0.3583     0.1289     0.1068\n",
      "\n",
      "--- Encoder prefix effect on cross-attention mass ---\n",
      "  (oracle_trunc vs bare, for each decoder condition)\n",
      "\n",
      "   Layer   nq: orc-bare    q: orc-bare\n",
      "  --------------------------------------\n",
      "       0        -0.0149        -0.0062\n",
      "       5        -0.0058        -0.0158\n",
      "      11        -0.0206        -0.0113\n",
      "      17        -0.0364        -0.0110\n",
      "      22        -0.0369        -0.0275\n",
      "      33        -0.2410        -0.2514\n",
      "\n",
      "  (Positive = encoder prefix increases cross-attention mass)\n",
      "\n",
      "--- Cross-attention entropy (answer → encoder) ---\n",
      "\n",
      "   Layer    bare_nq     bare_q     orc_nq      orc_q\n",
      "  --------------------------------------------------\n",
      "       0     1.1481     0.9821     1.0964     0.9567\n",
      "       5     1.6232     1.3680     1.5734     1.2841\n",
      "      11     2.3293     1.9714     2.1840     1.8719\n",
      "      17     2.3404     2.0538     2.1431     1.9571\n",
      "      22     1.6672     1.3721     1.5182     1.2560\n",
      "      33     0.8509     0.7376     0.6356     0.5166\n",
      "\n",
      "--- Self-attention entropy (answer → self positions) ---\n",
      "\n",
      "   Layer    bare_nq     bare_q     orc_nq      orc_q\n",
      "  --------------------------------------------------\n",
      "       0     1.2336     1.6202     1.2424     1.6307\n",
      "       5     0.8334     1.0960     0.8266     1.0830\n",
      "      11     0.6723     0.9350     0.6815     0.9031\n",
      "      17     0.6791     0.9441     0.6484     0.8517\n",
      "      22     1.2193     1.5033     1.2196     1.4648\n",
      "      33     1.0783     1.2643     1.1708     1.3850\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Probe B — Self vs cross allocation and cross-attention redistribution\n",
    "print(\"=\" * 70)\n",
    "print(\"PROBE B: SELF VS CROSS ALLOCATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# How does the self/cross split change across conditions?\n",
    "print(f\"\\n--- Cross-attention total mass (answer tokens → encoder) ---\")\n",
    "print(f\"\\n  {'Layer':>6} {'bare_nq':>10} {'bare_q':>10} {'orc_nq':>10} {'orc_q':>10}\")\n",
    "print(f\"  {'-'*50}\")\n",
    "\n",
    "for layer in PROBE_LAYERS:\n",
    "    ct = {}\n",
    "    for cond in ['bare_nq', 'bare_q', 'oracle_trunc_nq', 'oracle_trunc_q']:\n",
    "        ct[cond] = np.mean([r[f'probes_{cond}'][layer]['ct'] for r in results])\n",
    "    print(f\"  {layer:>6} {ct['bare_nq']:>10.4f} {ct['bare_q']:>10.4f} \"\n",
    "          f\"{ct['oracle_trunc_nq']:>10.4f} {ct['oracle_trunc_q']:>10.4f}\")\n",
    "\n",
    "# Does encoder prefix change cross-attention mass?\n",
    "print(f\"\\n--- Encoder prefix effect on cross-attention mass ---\")\n",
    "print(f\"  (oracle_trunc vs bare, for each decoder condition)\")\n",
    "print(f\"\\n  {'Layer':>6} {'nq: orc-bare':>14} {'q: orc-bare':>14}\")\n",
    "print(f\"  {'-'*38}\")\n",
    "\n",
    "for layer in PROBE_LAYERS:\n",
    "    ct_bare_nq = np.array([r['probes_bare_nq'][layer]['ct'] for r in results])\n",
    "    ct_orc_nq = np.array([r['probes_oracle_trunc_nq'][layer]['ct'] for r in results])\n",
    "    ct_bare_q = np.array([r['probes_bare_q'][layer]['ct'] for r in results])\n",
    "    ct_orc_q = np.array([r['probes_oracle_trunc_q'][layer]['ct'] for r in results])\n",
    "    print(f\"  {layer:>6} {(ct_orc_nq - ct_bare_nq).mean():>+14.4f} \"\n",
    "          f\"{(ct_orc_q - ct_bare_q).mean():>+14.4f}\")\n",
    "\n",
    "print(f\"\\n  (Positive = encoder prefix increases cross-attention mass)\")\n",
    "\n",
    "# Cross-attention entropy\n",
    "print(f\"\\n--- Cross-attention entropy (answer → encoder) ---\")\n",
    "print(f\"\\n  {'Layer':>6} {'bare_nq':>10} {'bare_q':>10} {'orc_nq':>10} {'orc_q':>10}\")\n",
    "print(f\"  {'-'*50}\")\n",
    "\n",
    "for layer in PROBE_LAYERS:\n",
    "    ce = {}\n",
    "    for cond in ['bare_nq', 'bare_q', 'oracle_trunc_nq', 'oracle_trunc_q']:\n",
    "        ce[cond] = np.mean([r[f'probes_{cond}'][layer]['ce'] for r in results])\n",
    "    print(f\"  {layer:>6} {ce['bare_nq']:>10.4f} {ce['bare_q']:>10.4f} \"\n",
    "          f\"{ce['oracle_trunc_nq']:>10.4f} {ce['oracle_trunc_q']:>10.4f}\")\n",
    "\n",
    "# Self-attention entropy\n",
    "print(f\"\\n--- Self-attention entropy (answer → self positions) ---\")\n",
    "print(f\"\\n  {'Layer':>6} {'bare_nq':>10} {'bare_q':>10} {'orc_nq':>10} {'orc_q':>10}\")\n",
    "print(f\"  {'-'*50}\")\n",
    "\n",
    "for layer in PROBE_LAYERS:\n",
    "    se = {}\n",
    "    for cond in ['bare_nq', 'bare_q', 'oracle_trunc_nq', 'oracle_trunc_q']:\n",
    "        se[cond] = np.mean([r[f'probes_{cond}'][layer]['se'] for r in results])\n",
    "    print(f\"  {layer:>6} {se['bare_nq']:>10.4f} {se['bare_q']:>10.4f} \"\n",
    "          f\"{se['oracle_trunc_nq']:>10.4f} {se['oracle_trunc_q']:>10.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "824fa7d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-20T09:44:55.710075Z",
     "iopub.status.busy": "2026-02-20T09:44:55.709788Z",
     "iopub.status.idle": "2026-02-20T09:44:55.736444Z",
     "shell.execute_reply": "2026-02-20T09:44:55.735747Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "THE 2x2 INTERACTION TEST\n",
      "======================================================================\n",
      "\n",
      "Layer 33 — 2x2 factorial decomposition\n",
      "  Factors: Encoder prefix (bare vs oracle_trunc) x Decoder query (nq vs q)\n",
      "\n",
      "--- NLL ---\n",
      "\n",
      "                               No query   With query   Difference\n",
      "  Bare encoder                   3.6834       2.5537      +1.1297\n",
      "  Oracle encoder                 2.9969       2.3967      +0.6003\n",
      "  Enc prefix effect             +0.6865      +0.1571\n",
      "\n",
      "  Encoder prefix effect:\n",
      "    Without decoder query: d=+0.366\n",
      "    With decoder query:    d=+0.238\n",
      "    Reduction: 35% (from 0.366 to 0.238)\n",
      "\n",
      "  Decoder query effect:\n",
      "    Without encoder prefix: d=+0.309\n",
      "    With encoder prefix:    d=+0.228\n",
      "\n",
      "  NLL INTERACTION (enc_prefix_benefit_nq - enc_prefix_benefit_q):\n",
      "    d=+0.316 (***)\n",
      "    Positive = decoder query makes encoder prefix benefit SMALLER\n",
      "\n",
      "--- Cross-attention mass: 2x2 ---\n",
      "\n",
      "                               No query   With query\n",
      "  Bare encoder                   0.3699       0.3583\n",
      "  Oracle encoder                 0.1289       0.1068\n",
      "\n",
      "  Encoder prefix changes cross-attn mass:\n",
      "    Without query: -0.2410\n",
      "    With query:    -0.2514\n",
      "  Interaction: +0.0104\n",
      "    (***, p=5.15e-45)\n",
      "\n",
      "--- Self-attention entropy: 2x2 ---\n",
      "  Encoder prefix changes self-attn entropy:\n",
      "    Without query: +0.0924\n",
      "    With query:    +0.1207\n",
      "  Decoder query changes self-attn entropy:\n",
      "    Without prefix: +0.1860\n",
      "  Interaction: -0.0282\n",
      "    (***, p=5.50e-41)\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: The 2x2 interaction test — does decoder query reduce encoder prefix effect?\n",
    "print(\"=\" * 70)\n",
    "print(\"THE 2x2 INTERACTION TEST\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# For each probe metric at the last layer, compute the 2x2 decomposition\n",
    "last_layer = PROBE_LAYERS[-1]\n",
    "\n",
    "print(f\"\\nLayer {last_layer} — 2x2 factorial decomposition\")\n",
    "print(f\"  Factors: Encoder prefix (bare vs oracle_trunc) x Decoder query (nq vs q)\")\n",
    "\n",
    "# NLL interaction\n",
    "print(f\"\\n--- NLL ---\")\n",
    "nll = {}\n",
    "for cond in ['bare_nq', 'bare_q', 'oracle_trunc_nq', 'oracle_trunc_q']:\n",
    "    nll[cond] = np.array([r[f'nll_{cond}'] for r in results])\n",
    "\n",
    "enc_effect_nq = nll['bare_nq'] - nll['oracle_trunc_nq']  # positive = prefix helps\n",
    "enc_effect_q = nll['bare_q'] - nll['oracle_trunc_q']\n",
    "dec_effect_bare = nll['bare_nq'] - nll['bare_q']  # positive = query helps\n",
    "dec_effect_oracle = nll['oracle_trunc_nq'] - nll['oracle_trunc_q']\n",
    "\n",
    "d_enc_nq = cohens_d(enc_effect_nq)\n",
    "d_enc_q = cohens_d(enc_effect_q)\n",
    "d_dec_bare = cohens_d(dec_effect_bare)\n",
    "d_dec_oracle = cohens_d(dec_effect_oracle)\n",
    "\n",
    "print(f\"\\n  {'':>24} {'No query':>12} {'With query':>12} {'Difference':>12}\")\n",
    "print(f\"  {'Bare encoder':<24} {nll['bare_nq'].mean():>12.4f} {nll['bare_q'].mean():>12.4f} \"\n",
    "      f\"{dec_effect_bare.mean():>+12.4f}\")\n",
    "print(f\"  {'Oracle encoder':<24} {nll['oracle_trunc_nq'].mean():>12.4f} {nll['oracle_trunc_q'].mean():>12.4f} \"\n",
    "      f\"{dec_effect_oracle.mean():>+12.4f}\")\n",
    "print(f\"  {'Enc prefix effect':<24} {enc_effect_nq.mean():>+12.4f} {enc_effect_q.mean():>+12.4f}\")\n",
    "\n",
    "print(f\"\\n  Encoder prefix effect:\")\n",
    "print(f\"    Without decoder query: d={d_enc_nq:+.3f}\")\n",
    "print(f\"    With decoder query:    d={d_enc_q:+.3f}\")\n",
    "print(f\"    Reduction: {(1 - d_enc_q/d_enc_nq)*100:.0f}% (from {d_enc_nq:.3f} to {d_enc_q:.3f})\")\n",
    "\n",
    "print(f\"\\n  Decoder query effect:\")\n",
    "print(f\"    Without encoder prefix: d={d_dec_bare:+.3f}\")\n",
    "print(f\"    With encoder prefix:    d={d_dec_oracle:+.3f}\")\n",
    "\n",
    "# Interaction test\n",
    "interaction = enc_effect_nq - enc_effect_q  # positive = query reduces prefix benefit\n",
    "d_interaction = cohens_d(interaction)\n",
    "_, p_interaction = stats.ttest_1samp(interaction, 0)\n",
    "sig_int = '***' if p_interaction < 0.001 else '**' if p_interaction < 0.01 else '*' if p_interaction < 0.05 else 'ns'\n",
    "print(f\"\\n  NLL INTERACTION (enc_prefix_benefit_nq - enc_prefix_benefit_q):\")\n",
    "print(f\"    d={d_interaction:+.3f} ({sig_int})\")\n",
    "print(f\"    Positive = decoder query makes encoder prefix benefit SMALLER\")\n",
    "\n",
    "# Attention budget interaction\n",
    "print(f\"\\n--- Cross-attention mass: 2x2 ---\")\n",
    "ct = {}\n",
    "for cond in ['bare_nq', 'bare_q', 'oracle_trunc_nq', 'oracle_trunc_q']:\n",
    "    ct[cond] = np.array([r[f'probes_{cond}'][last_layer]['ct'] for r in results])\n",
    "\n",
    "enc_ct_nq = ct['oracle_trunc_nq'] - ct['bare_nq']\n",
    "enc_ct_q = ct['oracle_trunc_q'] - ct['bare_q']\n",
    "dec_ct_bare = ct['bare_q'] - ct['bare_nq']\n",
    "dec_ct_oracle = ct['oracle_trunc_q'] - ct['oracle_trunc_nq']\n",
    "ct_interaction = enc_ct_nq - enc_ct_q\n",
    "\n",
    "print(f\"\\n  {'':>24} {'No query':>12} {'With query':>12}\")\n",
    "print(f\"  {'Bare encoder':<24} {ct['bare_nq'].mean():>12.4f} {ct['bare_q'].mean():>12.4f}\")\n",
    "print(f\"  {'Oracle encoder':<24} {ct['oracle_trunc_nq'].mean():>12.4f} {ct['oracle_trunc_q'].mean():>12.4f}\")\n",
    "print(f\"\\n  Encoder prefix changes cross-attn mass:\")\n",
    "print(f\"    Without query: {enc_ct_nq.mean():>+.4f}\")\n",
    "print(f\"    With query:    {enc_ct_q.mean():>+.4f}\")\n",
    "print(f\"  Interaction: {ct_interaction.mean():>+.4f}\")\n",
    "_, p_ct_int = stats.ttest_1samp(ct_interaction, 0)\n",
    "sig_ct = '***' if p_ct_int < 0.001 else '**' if p_ct_int < 0.01 else '*' if p_ct_int < 0.05 else 'ns'\n",
    "print(f\"    ({sig_ct}, p={p_ct_int:.2e})\")\n",
    "\n",
    "# Self-attention entropy interaction\n",
    "print(f\"\\n--- Self-attention entropy: 2x2 ---\")\n",
    "se = {}\n",
    "for cond in ['bare_nq', 'bare_q', 'oracle_trunc_nq', 'oracle_trunc_q']:\n",
    "    se[cond] = np.array([r[f'probes_{cond}'][last_layer]['se'] for r in results])\n",
    "\n",
    "enc_se_nq = se['oracle_trunc_nq'] - se['bare_nq']\n",
    "enc_se_q = se['oracle_trunc_q'] - se['bare_q']\n",
    "dec_se_bare = se['bare_q'] - se['bare_nq']\n",
    "se_interaction = enc_se_nq - enc_se_q\n",
    "\n",
    "print(f\"  Encoder prefix changes self-attn entropy:\")\n",
    "print(f\"    Without query: {enc_se_nq.mean():>+.4f}\")\n",
    "print(f\"    With query:    {enc_se_q.mean():>+.4f}\")\n",
    "print(f\"  Decoder query changes self-attn entropy:\")\n",
    "print(f\"    Without prefix: {dec_se_bare.mean():>+.4f}\")\n",
    "print(f\"  Interaction: {se_interaction.mean():>+.4f}\")\n",
    "_, p_se_int = stats.ttest_1samp(se_interaction, 0)\n",
    "sig_se = '***' if p_se_int < 0.001 else '**' if p_se_int < 0.01 else '*' if p_se_int < 0.05 else 'ns'\n",
    "print(f\"    ({sig_se}, p={p_se_int:.2e})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b22de6f9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-20T09:44:55.739618Z",
     "iopub.status.busy": "2026-02-20T09:44:55.739353Z",
     "iopub.status.idle": "2026-02-20T09:44:58.709287Z",
     "shell.execute_reply": "2026-02-20T09:44:58.708573Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "SUMMARY — Exp 07: Decoder Attention Probing\n",
      "======================================================================\n",
      "\n",
      "Model: google/t5gemma-2-4b-4b\n",
      "N: 500, Decoder layers: 34\n",
      "Probe layers: [0, 5, 11, 17, 22, 33]\n",
      "\n",
      "--- NLL 2x2 ---\n",
      "  Encoder prefix effect without query: d=+0.366\n",
      "  Encoder prefix effect WITH query:    d=+0.238\n",
      "  Decoder query effect without prefix: d=+0.309\n",
      "  Decoder query effect WITH prefix:    d=+0.228\n",
      "  Interaction: d=+0.316 (***)\n",
      "\n",
      "--- Decoder query as attention buffer (layer 33) ---\n",
      "  Query buffer absorbs 5.5% of answer-token attention (bare encoder)\n",
      "  Query buffer absorbs 6.1% of answer-token attention (oracle encoder)\n",
      "\n",
      "--- Cross-attention budget (layer 33) ---\n",
      "  bare_nq:         37.0%\n",
      "  bare_q:          35.8%\n",
      "  oracle_trunc_nq: 12.9%\n",
      "  oracle_trunc_q:  10.7%\n",
      "\n",
      "--- Hypothesis verdict ---\n",
      "  CONFIRMED: Query tokens absorb 5.5% of answer attention budget.\n",
      "  This is the decoder-side attention buffer mechanism.\n",
      "  Encoder prefix benefit reduced by 35% when decoder has query.\n",
      "  The two buffer mechanisms are PARTIALLY REDUNDANT.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results saved to ../../../results/exp07/results.json\n",
      "\n",
      "Cleaning up GPU memory...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory: 18.80 GB -> 0.01 GB\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Cell 10: Summary and save\n",
    "print(\"=\" * 70)\n",
    "print(\"SUMMARY — Exp 07: Decoder Attention Probing\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "last_layer = PROBE_LAYERS[-1]\n",
    "\n",
    "# Gather key metrics\n",
    "nll = {}\n",
    "for cond in ['bare_nq', 'bare_q', 'oracle_trunc_nq', 'oracle_trunc_q']:\n",
    "    nll[cond] = np.array([r[f'nll_{cond}'] for r in results])\n",
    "\n",
    "d_enc_nq = cohens_d(nll['bare_nq'] - nll['oracle_trunc_nq'])\n",
    "d_enc_q = cohens_d(nll['bare_q'] - nll['oracle_trunc_q'])\n",
    "d_dec_bare = cohens_d(nll['bare_nq'] - nll['bare_q'])\n",
    "d_dec_oracle = cohens_d(nll['oracle_trunc_nq'] - nll['oracle_trunc_q'])\n",
    "\n",
    "interaction = (nll['bare_nq'] - nll['oracle_trunc_nq']) - (nll['bare_q'] - nll['oracle_trunc_q'])\n",
    "d_interaction = cohens_d(interaction)\n",
    "_, p_interaction = stats.ttest_1samp(interaction, 0)\n",
    "\n",
    "# Query buffer mass at last layer\n",
    "sq_bare = np.mean([r['probes_bare_q'][last_layer]['sq'] for r in results])\n",
    "sq_oracle = np.mean([r['probes_oracle_trunc_q'][last_layer]['sq'] for r in results])\n",
    "\n",
    "# Cross-attention totals at last layer\n",
    "ct = {}\n",
    "for cond in ['bare_nq', 'bare_q', 'oracle_trunc_nq', 'oracle_trunc_q']:\n",
    "    ct[cond] = np.mean([r[f'probes_{cond}'][last_layer]['ct'] for r in results])\n",
    "\n",
    "print(f\"\\nModel: {MODEL_NAME}\")\n",
    "print(f\"N: {len(results)}, Decoder layers: {N_DEC_LAYERS}\")\n",
    "print(f\"Probe layers: {PROBE_LAYERS}\")\n",
    "\n",
    "print(f\"\\n--- NLL 2x2 ---\")\n",
    "print(f\"  Encoder prefix effect without query: d={d_enc_nq:+.3f}\")\n",
    "print(f\"  Encoder prefix effect WITH query:    d={d_enc_q:+.3f}\")\n",
    "print(f\"  Decoder query effect without prefix: d={d_dec_bare:+.3f}\")\n",
    "print(f\"  Decoder query effect WITH prefix:    d={d_dec_oracle:+.3f}\")\n",
    "sig_int = '***' if p_interaction < 0.001 else 'ns'\n",
    "print(f\"  Interaction: d={d_interaction:+.3f} ({sig_int})\")\n",
    "\n",
    "print(f\"\\n--- Decoder query as attention buffer (layer {last_layer}) ---\")\n",
    "print(f\"  Query buffer absorbs {sq_bare*100:.1f}% of answer-token attention (bare encoder)\")\n",
    "print(f\"  Query buffer absorbs {sq_oracle*100:.1f}% of answer-token attention (oracle encoder)\")\n",
    "\n",
    "print(f\"\\n--- Cross-attention budget (layer {last_layer}) ---\")\n",
    "print(f\"  bare_nq:         {ct['bare_nq']*100:.1f}%\")\n",
    "print(f\"  bare_q:          {ct['bare_q']*100:.1f}%\")\n",
    "print(f\"  oracle_trunc_nq: {ct['oracle_trunc_nq']*100:.1f}%\")\n",
    "print(f\"  oracle_trunc_q:  {ct['oracle_trunc_q']*100:.1f}%\")\n",
    "\n",
    "# Hypothesis verdict\n",
    "print(f\"\\n--- Hypothesis verdict ---\")\n",
    "if sq_bare > 0.05:\n",
    "    print(f\"  CONFIRMED: Query tokens absorb {sq_bare*100:.1f}% of answer attention budget.\")\n",
    "    print(f\"  This is the decoder-side attention buffer mechanism.\")\n",
    "else:\n",
    "    print(f\"  NOT CONFIRMED: Query tokens absorb only {sq_bare*100:.1f}% of attention.\")\n",
    "\n",
    "if d_interaction > 0.05 and p_interaction < 0.05:\n",
    "    redundancy = (1 - d_enc_q / d_enc_nq) * 100 if d_enc_nq > 0 else 0\n",
    "    print(f\"  Encoder prefix benefit reduced by {redundancy:.0f}% when decoder has query.\")\n",
    "    print(f\"  The two buffer mechanisms are PARTIALLY REDUNDANT.\")\n",
    "elif d_interaction < -0.05:\n",
    "    print(f\"  Encoder prefix benefit INCREASES when decoder has query.\")\n",
    "    print(f\"  The mechanisms are SYNERGISTIC, not redundant.\")\n",
    "else:\n",
    "    print(f\"  No significant interaction. The mechanisms appear INDEPENDENT.\")\n",
    "\n",
    "# Save results\n",
    "# Aggregate probe data per (condition, layer, metric)\n",
    "probe_summary = {}\n",
    "for cond in ['bare_nq', 'bare_q', 'oracle_trunc_nq', 'oracle_trunc_q']:\n",
    "    probe_summary[cond] = {}\n",
    "    for layer in PROBE_LAYERS:\n",
    "        layer_data = {}\n",
    "        for metric in ['sb', 'sq', 'sa', 'st', 'ct', 'se', 'ce']:\n",
    "            vals = [r[f'probes_{cond}'][layer][metric] for r in results]\n",
    "            layer_data[metric] = {\n",
    "                'mean': float(np.mean(vals)),\n",
    "                'std': float(np.std(vals)),\n",
    "            }\n",
    "        probe_summary[cond][str(layer)] = layer_data\n",
    "\n",
    "final_results = {\n",
    "    'experiment': 'v4_exp07_decoder_attention_probing',\n",
    "    'model': MODEL_NAME,\n",
    "    'dataset': 'ms_marco_v1.1',\n",
    "    'n_samples': len(results),\n",
    "    'seed': SEED,\n",
    "    'n_decoder_layers': N_DEC_LAYERS,\n",
    "    'probe_layers': PROBE_LAYERS,\n",
    "    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'nll': {\n",
    "        'bare_nq': float(nll['bare_nq'].mean()),\n",
    "        'bare_q': float(nll['bare_q'].mean()),\n",
    "        'oracle_trunc_nq': float(nll['oracle_trunc_nq'].mean()),\n",
    "        'oracle_trunc_q': float(nll['oracle_trunc_q'].mean()),\n",
    "    },\n",
    "    'nll_effects': {\n",
    "        'd_enc_nq': float(d_enc_nq),\n",
    "        'd_enc_q': float(d_enc_q),\n",
    "        'd_dec_bare': float(d_dec_bare),\n",
    "        'd_dec_oracle': float(d_dec_oracle),\n",
    "        'd_interaction': float(d_interaction),\n",
    "        'p_interaction': float(p_interaction),\n",
    "    },\n",
    "    'query_buffer_mass': {\n",
    "        'bare_q': float(sq_bare),\n",
    "        'oracle_q': float(sq_oracle),\n",
    "    },\n",
    "    'probe_summary': probe_summary,\n",
    "}\n",
    "\n",
    "with open(RESULTS_DIR / 'results.json', 'w') as f:\n",
    "    json.dump(final_results, f, indent=2)\n",
    "print(f\"\\nResults saved to {RESULTS_DIR / 'results.json'}\")\n",
    "\n",
    "# Cleanup\n",
    "print(f\"\\nCleaning up GPU memory...\")\n",
    "mem_before = torch.cuda.memory_allocated() / 1e9\n",
    "del model, processor, tokenizer\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "mem_after = torch.cuda.memory_allocated() / 1e9\n",
    "print(f\"GPU memory: {mem_before:.2f} GB -> {mem_after:.2f} GB\")\n",
    "print(\"Done!\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "3841f42ca48e4849add0a5208a2abae5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3cc0edcece7644ea9b837ab36e0069d8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "42fcd42dbfa642edbda1c58392ff9df8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_c5dea8c5c25149ffb1ef1a8cf965fc57",
        "IPY_MODEL_56285ec7841943e9ae32881133f74887",
        "IPY_MODEL_89e5e965ee3341f5a5b7fed091e494a5"
       ],
       "layout": "IPY_MODEL_3841f42ca48e4849add0a5208a2abae5",
       "tabbable": null,
       "tooltip": null
      }
     },
     "46569cdcbe044263b995e0e588828954": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "56285ec7841943e9ae32881133f74887": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_be16d501a64c4f2495cd90dd4ea26946",
       "max": 500.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_e0b3bf264dfc434f9159901cf5783448",
       "tabbable": null,
       "tooltip": null,
       "value": 500.0
      }
     },
     "5abef0c17107422998ceb22ffc7bcea0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_de3ce76269a44221b7b420543d499972",
       "placeholder": "​",
       "style": "IPY_MODEL_cfa5ba3bafeb4fa6acf9cf75a193df47",
       "tabbable": null,
       "tooltip": null,
       "value": " 1327/1327 [00:04&lt;00:00, 700.65it/s, Materializing param=model.encoder.vision_tower.vision_model.post_layernorm.weight]"
      }
     },
     "7f85140a36aa4ca694f41b056af755c4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "8277b72b8f6a40efbed41fbcb0832b87": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "89e5e965ee3341f5a5b7fed091e494a5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_bb3a61f5f7894965996c76031342954f",
       "placeholder": "​",
       "style": "IPY_MODEL_e15b7a74b93446f3b0e74cbef5504d4a",
       "tabbable": null,
       "tooltip": null,
       "value": " 500/500 [11:29:27&lt;00:00, 272.88s/it]"
      }
     },
     "a1d739eb21a340648e784efc1e77532c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_8277b72b8f6a40efbed41fbcb0832b87",
       "max": 1327.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_d679ee30049e4ede8d5c610ddc7fe397",
       "tabbable": null,
       "tooltip": null,
       "value": 1327.0
      }
     },
     "b05fdcf956d846829b24a6ddfdf091f6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "bb3a61f5f7894965996c76031342954f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "be16d501a64c4f2495cd90dd4ea26946": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c4f85c31213949abbce44d27a0e69b72": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_d1d5f71f47b74d90b1137d253fb26497",
        "IPY_MODEL_a1d739eb21a340648e784efc1e77532c",
        "IPY_MODEL_5abef0c17107422998ceb22ffc7bcea0"
       ],
       "layout": "IPY_MODEL_ff16f92d94bd49b7bab5a92fc59c2a9d",
       "tabbable": null,
       "tooltip": null
      }
     },
     "c5dea8c5c25149ffb1ef1a8cf965fc57": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_3cc0edcece7644ea9b837ab36e0069d8",
       "placeholder": "​",
       "style": "IPY_MODEL_b05fdcf956d846829b24a6ddfdf091f6",
       "tabbable": null,
       "tooltip": null,
       "value": "Probing: 100%"
      }
     },
     "cfa5ba3bafeb4fa6acf9cf75a193df47": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "d1d5f71f47b74d90b1137d253fb26497": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_46569cdcbe044263b995e0e588828954",
       "placeholder": "​",
       "style": "IPY_MODEL_7f85140a36aa4ca694f41b056af755c4",
       "tabbable": null,
       "tooltip": null,
       "value": "Loading weights: 100%"
      }
     },
     "d679ee30049e4ede8d5c610ddc7fe397": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "de3ce76269a44221b7b420543d499972": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e0b3bf264dfc434f9159901cf5783448": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "e15b7a74b93446f3b0e74cbef5504d4a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "ff16f92d94bd49b7bab5a92fc59c2a9d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
