{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9e7054c",
   "metadata": {},
   "source": [
    "# Experiment 02: Length Scaling with Query in Decoder\n",
    "\n",
    "## Motivation\n",
    "\n",
    "v3 Exp 03/03B showed NO decay of enrichment benefit up to 6144 tokens. But the v3\n",
    "decoder never had the query — the mechanism was 85% structural (any prefix worked).\n",
    "\n",
    "v4 Exp 01 showed that when the decoder has the query:\n",
    "- Structural component collapsed: 85% → ~35% (random d=+0.080, ns)\n",
    "- Content-dependent component now dominant: surr_doc d=+0.148 (65% of oracle)\n",
    "- Overall enrichment preserved at 61%: oracle d=+0.228 vs v3's d=+0.376\n",
    "\n",
    "**Key question**: Does the v4 content-dependent enrichment decay with document length?\n",
    "The structural mechanism was length-invariant (v3 Exp 03), but the content-dependent\n",
    "component might dilute as documents get longer — the prefix's semantic influence may\n",
    "not reach distant tokens as effectively.\n",
    "\n",
    "## Method\n",
    "\n",
    "Pad the same short MS MARCO passages to controlled token lengths using unrelated\n",
    "passages. Same questions, same answers — only document length varies. Within-subject\n",
    "design: every sample appears at every length.\n",
    "\n",
    "## Conditions (6 per length bin)\n",
    "\n",
    "### With query in decoder (production-realistic):\n",
    "\n",
    "| # | Condition | Encoder input | Cross-attn | Purpose |\n",
    "|---|-----------|--------------|------------|---------|\n",
    "| 1 | bare | [document] | all | Baseline |\n",
    "| 2 | oracle_trunc | [query + doc] | doc only | Upper bound |\n",
    "| 3 | surr_doc_trunc | [top-5 kw + doc] | doc only | Production surrogate |\n",
    "| 4 | random_trunc | [random words + doc] | doc only | Structural control |\n",
    "\n",
    "### Without query in decoder (v3 replication):\n",
    "\n",
    "| # | Condition | Encoder input | Cross-attn | Purpose |\n",
    "|---|-----------|--------------|------------|---------|\n",
    "| 5 | bare_nq | [document] | all | v3 baseline |\n",
    "| 6 | oracle_trunc_nq | [query + doc] | doc only | v3 enrichment reference |\n",
    "\n",
    "## Length bins\n",
    "\n",
    "| Bin | Target tokens | v4 Exp 01 | v3 Exp 03 |\n",
    "|-----|--------------|-----------|-----------|\n",
    "| original | ~98 tok (no padding) | d=+0.228 | d=+0.41 |\n",
    "| 256 | padded | ? | d=+0.42 |\n",
    "| 512 | padded | ? | d=+0.38 |\n",
    "| 1024 | padded | ? | d=+0.45 |\n",
    "| 2048 | padded | ? | d=+0.42 |\n",
    "| 4096 | padded | ? | d=+0.40 (03B) |\n",
    "\n",
    "## What to look for\n",
    "\n",
    "1. **d(oracle_trunc vs bare) at each length**: Does it decay? (v3: flat)\n",
    "2. **d(surr_doc_trunc vs bare)**: Does the production surrogate decay faster?\n",
    "3. **d(random_trunc vs bare)**: Does the structural component stay marginal?\n",
    "4. **d(oracle_trunc_nq vs bare_nq)**: v3 replication (should be flat)\n",
    "5. **Ratio: v4/v3 enrichment at each length**: Does the gap widen or narrow?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fff49bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Setup and model loading\n",
    "import os\n",
    "os.umask(0o000)\n",
    "\n",
    "import sys, json, time, gc, re\n",
    "import random as pyrandom\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from scipy import stats\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "sys.path.insert(0, \"../../..\")\n",
    "from lib.analysis import cohens_d\n",
    "\n",
    "SEED = 42\n",
    "N_SAMPLES = 500\n",
    "MODEL_NAME = \"google/t5gemma-2-4b-4b\"\n",
    "LENGTH_BINS = [\"original\", \"256\", \"512\", \"1024\", \"2048\", \"4096\"]\n",
    "TARGET_LENGTHS = {\n",
    "    \"original\": None,\n",
    "    \"256\": 256,\n",
    "    \"512\": 512,\n",
    "    \"1024\": 1024,\n",
    "    \"2048\": 2048,\n",
    "    \"4096\": 4096,\n",
    "}\n",
    "\n",
    "RESULTS_DIR = Path(\"../../../results/exp02\")\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CHECKPOINT_PATH = RESULTS_DIR / \"checkpoint.json\"\n",
    "\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "pyrandom.seed(SEED)\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv())\n",
    "HF_TOKEN = os.environ.get(\"HF_TOKEN\")\n",
    "\n",
    "from transformers import AutoProcessor, AutoModelForSeq2SeqLM\n",
    "\n",
    "print(f\"Loading {MODEL_NAME}...\")\n",
    "processor = AutoProcessor.from_pretrained(MODEL_NAME, token=HF_TOKEN)\n",
    "tokenizer = processor.tokenizer\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    MODEL_NAME, device_map=\"auto\", torch_dtype=torch.bfloat16, token=HF_TOKEN,\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "DEVICE = next(model.parameters()).device\n",
    "BOS_ID = getattr(model.config, 'decoder_start_token_id', None) or tokenizer.bos_token_id\n",
    "\n",
    "print(f\"Exp 02: Length Scaling with Query in Decoder\")\n",
    "print(f\"N: {N_SAMPLES}, Model: {MODEL_NAME}\")\n",
    "print(f\"Length bins: {LENGTH_BINS}\")\n",
    "print(f\"DEVICE: {DEVICE}, dtype: {next(model.parameters()).dtype}\")\n",
    "print(f\"GPU memory: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "print(f\"Decoder start token ID (BOS): {BOS_ID}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93c9c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Scoring helpers\n",
    "# NOTE: max_length=4608 for encoder (4096 doc + prefix tokens)\n",
    "\n",
    "def count_prefix_tokens(prefix_text, document_text):\n",
    "    # BPE-aware token count of prefix in [prefix + newline + document].\n",
    "    full_text = prefix_text + \"\\n\" + document_text\n",
    "    full_ids = tokenizer(full_text, add_special_tokens=True, truncation=True,\n",
    "                         max_length=4608).input_ids\n",
    "    doc_ids = tokenizer(document_text, add_special_tokens=True, truncation=True,\n",
    "                        max_length=4608).input_ids\n",
    "    return len(full_ids) - len(doc_ids)\n",
    "\n",
    "\n",
    "def score_nll(encoder_text, answer_text, prefix_token_count=0, truncate=False):\n",
    "    # Score NLL of answer tokens — decoder receives ONLY answer (no query).\n",
    "    # Used for _nq (no-query) conditions that replicate v3.\n",
    "    enc_ids = tokenizer(encoder_text, return_tensors=\"pt\",\n",
    "                        add_special_tokens=True, truncation=True,\n",
    "                        max_length=4608).input_ids.to(DEVICE)\n",
    "    total_enc_len = enc_ids.shape[1]\n",
    "    enc_mask = torch.ones(1, total_enc_len, device=DEVICE, dtype=torch.long)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        encoder_outputs = model.get_encoder()(\n",
    "            input_ids=enc_ids, attention_mask=enc_mask\n",
    "        )\n",
    "\n",
    "    if truncate and prefix_token_count > 0:\n",
    "        cross_attn_mask = torch.ones(1, total_enc_len, device=DEVICE, dtype=torch.long)\n",
    "        cross_attn_mask[:, :prefix_token_count] = 0\n",
    "    else:\n",
    "        cross_attn_mask = torch.ones(1, total_enc_len, device=DEVICE, dtype=torch.long)\n",
    "\n",
    "    ans_ids = tokenizer(answer_text, return_tensors=\"pt\",\n",
    "                        add_special_tokens=False, truncation=True,\n",
    "                        max_length=256).input_ids.to(DEVICE)\n",
    "\n",
    "    if ans_ids.shape[1] == 0:\n",
    "        del encoder_outputs\n",
    "        return 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            encoder_outputs=encoder_outputs,\n",
    "            attention_mask=cross_attn_mask,\n",
    "            labels=ans_ids,\n",
    "        )\n",
    "\n",
    "    logits = outputs.logits\n",
    "    log_probs = F.log_softmax(logits, dim=-1)\n",
    "    token_log_probs = log_probs[0].gather(1, ans_ids[0].unsqueeze(1)).squeeze(1)\n",
    "    mean_nll = -token_log_probs.mean().item()\n",
    "\n",
    "    del encoder_outputs, outputs, logits, log_probs\n",
    "    return mean_nll\n",
    "\n",
    "\n",
    "def score_nll_query_prefix(encoder_text, query_text, answer_text,\n",
    "                           prefix_token_count=0, truncate=False):\n",
    "    # Score NLL of answer tokens with query as decoder prefix.\n",
    "    # Production-realistic: decoder_input_ids = [BOS] + query_ids + answer_ids.\n",
    "    # NLL is computed only on answer tokens.\n",
    "\n",
    "    # 1. Encode\n",
    "    enc_ids = tokenizer(encoder_text, return_tensors=\"pt\",\n",
    "                        add_special_tokens=True, truncation=True,\n",
    "                        max_length=4608).input_ids.to(DEVICE)\n",
    "    total_enc_len = enc_ids.shape[1]\n",
    "    enc_mask = torch.ones(1, total_enc_len, device=DEVICE, dtype=torch.long)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        encoder_outputs = model.get_encoder()(\n",
    "            input_ids=enc_ids, attention_mask=enc_mask\n",
    "        )\n",
    "\n",
    "    # 2. Cross-attention mask\n",
    "    if truncate and prefix_token_count > 0:\n",
    "        cross_attn_mask = torch.ones(1, total_enc_len, device=DEVICE, dtype=torch.long)\n",
    "        cross_attn_mask[:, :prefix_token_count] = 0\n",
    "    else:\n",
    "        cross_attn_mask = torch.ones(1, total_enc_len, device=DEVICE, dtype=torch.long)\n",
    "\n",
    "    # 3. Tokenize query and answer for decoder\n",
    "    query_ids = tokenizer(query_text, add_special_tokens=False, truncation=True,\n",
    "                          max_length=512).input_ids\n",
    "    answer_ids = tokenizer(answer_text, add_special_tokens=False, truncation=True,\n",
    "                           max_length=256).input_ids\n",
    "\n",
    "    if len(answer_ids) == 0:\n",
    "        del encoder_outputs\n",
    "        return 0.0\n",
    "\n",
    "    # 4. Build decoder_input_ids = [BOS] + query_ids + answer_ids\n",
    "    dec_ids = [BOS_ID] + query_ids + answer_ids\n",
    "    dec_tensor = torch.tensor([dec_ids], dtype=torch.long, device=DEVICE)\n",
    "\n",
    "    n_query = len(query_ids)\n",
    "    n_answer = len(answer_ids)\n",
    "\n",
    "    # 5. Forward pass\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            encoder_outputs=encoder_outputs,\n",
    "            attention_mask=cross_attn_mask,\n",
    "            decoder_input_ids=dec_tensor,\n",
    "        )\n",
    "\n",
    "    # 6. Extract answer logits\n",
    "    # logits[0, t, :] predicts the token at position t+1\n",
    "    # Position n_query predicts the first answer token\n",
    "    logits = outputs.logits\n",
    "    answer_logits = logits[0, n_query:n_query + n_answer, :]\n",
    "\n",
    "    # 7. Compute NLL\n",
    "    answer_targets = torch.tensor(answer_ids, dtype=torch.long, device=DEVICE)\n",
    "    log_probs = F.log_softmax(answer_logits, dim=-1)\n",
    "    token_log_probs = log_probs.gather(1, answer_targets.unsqueeze(1)).squeeze(1)\n",
    "    mean_nll = -token_log_probs.mean().item()\n",
    "\n",
    "    del encoder_outputs, outputs, logits, log_probs\n",
    "    return mean_nll\n",
    "\n",
    "\n",
    "# === Surrogate generation ===\n",
    "STOP_WORDS = {\n",
    "    'a', 'an', 'the', 'is', 'are', 'was', 'were', 'be', 'been', 'being',\n",
    "    'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could',\n",
    "    'should', 'may', 'might', 'can', 'shall', 'to', 'of', 'in', 'for',\n",
    "    'on', 'with', 'at', 'by', 'from', 'as', 'into', 'through', 'during',\n",
    "    'before', 'after', 'above', 'below', 'between', 'and', 'but', 'or',\n",
    "    'not', 'no', 'if', 'then', 'than', 'so', 'up', 'out', 'about',\n",
    "    'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those',\n",
    "    'it', 'its', 'i', 'me', 'my', 'we', 'our', 'you', 'your', 'he',\n",
    "    'him', 'his', 'she', 'her', 'they', 'them', 'their', 'how', 'when',\n",
    "    'where', 'why', 'much', 'many', 'some', 'any', 'all', 'each',\n",
    "    'does', 'also', 'just', 'more', 'most', 'very', 'too', 'only',\n",
    "}\n",
    "\n",
    "def extract_keywords(text):\n",
    "    words = re.sub(r'[^\\w\\s]', '', text.lower()).split()\n",
    "    return [w for w in words if w not in STOP_WORDS and len(w) > 2]\n",
    "\n",
    "def make_surrogate_from_doc(passage):\n",
    "    content_words = extract_keywords(passage)\n",
    "    if not content_words:\n",
    "        return \"information\"\n",
    "    counts = Counter(content_words)\n",
    "    return \" \".join(w for w, _ in counts.most_common(5))\n",
    "\n",
    "print(\"Scoring functions defined:\")\n",
    "print(\"  score_nll(encoder_text, answer_text, prefix_token_count, truncate)\")\n",
    "print(\"  score_nll_query_prefix(encoder_text, query_text, answer_text, prefix_token_count, truncate)\")\n",
    "print(f\"  Encoder max_length: 4608 (supports 4096 doc + prefix)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c11505",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Load MS MARCO data and build padding pool\n",
    "from lib.data import count_words\n",
    "from datasets import load_dataset\n",
    "\n",
    "print(\"Loading MS MARCO v1.1 validation...\")\n",
    "ds = load_dataset(\"microsoft/ms_marco\", \"v1.1\", split=\"validation\")\n",
    "\n",
    "all_candidates = []\n",
    "padding_pool = []\n",
    "\n",
    "for item in ds:\n",
    "    passages = item.get('passages', {})\n",
    "    ptexts = passages.get('passage_text', [])\n",
    "    is_sel = passages.get('is_selected', [])\n",
    "    query = item.get('query', '')\n",
    "    answers = item.get('answers', [])\n",
    "    well_formed = item.get('wellFormedAnswers', [])\n",
    "    answer = None\n",
    "    if well_formed and len(well_formed) > 0 and well_formed[0] not in ('[]', ''):\n",
    "        answer = well_formed[0]\n",
    "    elif answers and len(answers) > 0 and answers[0] != 'No Answer Present.':\n",
    "        answer = answers[0]\n",
    "\n",
    "    for pt, sel in zip(ptexts, is_sel):\n",
    "        wc = count_words(pt)\n",
    "        if sel == 1 and 30 <= wc <= 300 and answer:\n",
    "            if len(all_candidates) < 3 * N_SAMPLES:\n",
    "                all_candidates.append({\n",
    "                    'passage': pt, 'query': query, 'answer': answer,\n",
    "                    'word_count': wc,\n",
    "                })\n",
    "        elif sel == 0 and 20 <= wc <= 200:\n",
    "            padding_pool.append(pt)\n",
    "\n",
    "    if len(all_candidates) >= 3 * N_SAMPLES and len(padding_pool) >= 10000:\n",
    "        break\n",
    "\n",
    "print(f\"Total candidates: {len(all_candidates)}\")\n",
    "print(f\"Padding pool: {len(padding_pool)} unrelated passages\")\n",
    "\n",
    "np.random.seed(SEED)\n",
    "indices = np.random.permutation(len(all_candidates))\n",
    "samples = [all_candidates[i] for i in indices[:N_SAMPLES]]\n",
    "del ds, all_candidates\n",
    "\n",
    "# Shuffle padding pool for random padding\n",
    "np.random.shuffle(padding_pool)\n",
    "\n",
    "# Generate surrogates for each sample\n",
    "for i, s in enumerate(samples):\n",
    "    s['surr_doc'] = make_surrogate_from_doc(s['passage'])\n",
    "\n",
    "    # Random prefix: words from unrelated passage, matched to query word count\n",
    "    other_idx = (i + N_SAMPLES // 2) % len(samples)\n",
    "    other_words = samples[other_idx]['passage'].split()\n",
    "    query_word_count = len(s['query'].split())\n",
    "    s['random_prefix'] = \" \".join(other_words[:query_word_count])\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "print(f\"\\nLoaded {len(samples)} samples (SEED={SEED})\")\n",
    "print(f\"Mean passage words: {np.mean([s['word_count'] for s in samples]):.0f}\")\n",
    "print(f\"Mean answer words: {np.mean([count_words(s['answer']) for s in samples]):.0f}\")\n",
    "print(f\"Mean query words: {np.mean([count_words(s['query']) for s in samples]):.0f}\")\n",
    "\n",
    "# Show token counts for original passages\n",
    "orig_tok_counts = [len(tokenizer(s['passage'], add_special_tokens=True).input_ids)\n",
    "                   for s in samples]\n",
    "print(f\"Original passage tokens: mean={np.mean(orig_tok_counts):.0f}, \"\n",
    "      f\"median={np.median(orig_tok_counts):.0f}, \"\n",
    "      f\"min={np.min(orig_tok_counts)}, max={np.max(orig_tok_counts)}\")\n",
    "\n",
    "print(f\"\\nFirst sample:\")\n",
    "print(f\"  Query:    {samples[0]['query'][:70]}...\")\n",
    "print(f\"  Answer:   {samples[0]['answer'][:70]}...\")\n",
    "print(f\"  Passage:  {samples[0]['passage'][:70]}...\")\n",
    "print(f\"  Surr doc: {samples[0]['surr_doc']}\")\n",
    "print(f\"  Random:   {samples[0]['random_prefix'][:60]}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb24c1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Build padded documents at each length bin\n",
    "print(\"=\" * 70)\n",
    "print(\"BUILDING PADDED DOCUMENTS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "def pad_passage_to_length(passage, target_tokens, pool, pool_offset):\n",
    "    # Pad a passage to target_tokens by appending unrelated passages.\n",
    "    if target_tokens is None:\n",
    "        toks = tokenizer(passage, add_special_tokens=True).input_ids\n",
    "        return passage, len(toks), 0\n",
    "\n",
    "    current_ids = tokenizer(passage, add_special_tokens=True).input_ids\n",
    "    if len(current_ids) >= target_tokens:\n",
    "        return passage, len(current_ids), 0\n",
    "\n",
    "    padded = passage\n",
    "    n_used = 0\n",
    "    idx = pool_offset\n",
    "\n",
    "    while True:\n",
    "        if idx >= len(pool):\n",
    "            idx = 0\n",
    "        candidate = padded + \"\\n\\n\" + pool[idx]\n",
    "        candidate_ids = tokenizer(candidate, add_special_tokens=True).input_ids\n",
    "        if len(candidate_ids) >= target_tokens:\n",
    "            # Add words from this passage until we hit target\n",
    "            pad_words = pool[idx].split()\n",
    "            for w_end in range(1, len(pad_words) + 1):\n",
    "                partial = padded + \"\\n\\n\" + \" \".join(pad_words[:w_end])\n",
    "                partial_ids = tokenizer(partial, add_special_tokens=True).input_ids\n",
    "                if len(partial_ids) >= target_tokens:\n",
    "                    padded = partial\n",
    "                    break\n",
    "            else:\n",
    "                padded = candidate\n",
    "            n_used += 1\n",
    "            break\n",
    "        padded = candidate\n",
    "        n_used += 1\n",
    "        idx += 1\n",
    "\n",
    "    final_ids = tokenizer(padded, add_special_tokens=True).input_ids\n",
    "    return padded, len(final_ids), n_used\n",
    "\n",
    "\n",
    "# Build padded versions for each sample at each length\n",
    "padded_docs = {}\n",
    "padded_stats = {}\n",
    "\n",
    "for length_bin, target_tokens in TARGET_LENGTHS.items():\n",
    "    padded_docs[length_bin] = []\n",
    "    tok_counts = []\n",
    "\n",
    "    for i, s in enumerate(samples):\n",
    "        pool_offset = i * 50\n",
    "        padded_text, actual_tokens, n_pad = pad_passage_to_length(\n",
    "            s['passage'], target_tokens, padding_pool, pool_offset\n",
    "        )\n",
    "        padded_docs[length_bin].append(padded_text)\n",
    "        tok_counts.append(actual_tokens)\n",
    "\n",
    "    padded_stats[length_bin] = {\n",
    "        'mean': float(np.mean(tok_counts)),\n",
    "        'min': int(np.min(tok_counts)),\n",
    "        'max': int(np.max(tok_counts)),\n",
    "        'median': float(np.median(tok_counts)),\n",
    "    }\n",
    "\n",
    "    print(f\"  {length_bin:>8s}: mean={padded_stats[length_bin]['mean']:.0f} tokens \"\n",
    "          f\"(min={padded_stats[length_bin]['min']}, max={padded_stats[length_bin]['max']}, \"\n",
    "          f\"median={padded_stats[length_bin]['median']:.0f})\")\n",
    "\n",
    "# Preview sample 0\n",
    "print(f\"\\n--- Sample 0 preview ---\")\n",
    "print(f\"  Query:  {samples[0]['query'][:80]}\")\n",
    "print(f\"  Answer: {samples[0]['answer'][:80]}\")\n",
    "for lb in LENGTH_BINS:\n",
    "    preview = padded_docs[lb][0]\n",
    "    tok_count = len(tokenizer(preview, add_special_tokens=True).input_ids)\n",
    "    print(f\"  {lb:>8s}: {tok_count} tokens, starts='{preview[:60]}...', ends='...{preview[-40:]}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3e85de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Show example conditions for sample 0 at original length\n",
    "print(\"=\" * 70)\n",
    "print(\"EXAMPLE CONDITIONS (sample 0, original length)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "ex = samples[0]\n",
    "doc = padded_docs[\"original\"][0]\n",
    "\n",
    "# Count prefix tokens\n",
    "n_prefix_oracle = count_prefix_tokens(ex['query'], doc)\n",
    "n_prefix_doc = count_prefix_tokens(ex['surr_doc'], doc)\n",
    "n_prefix_random = count_prefix_tokens(ex['random_prefix'], doc)\n",
    "\n",
    "print(f\"\\nQuery:          {ex['query']}\")\n",
    "print(f\"Answer:         {ex['answer']}\")\n",
    "print(f\"Surr doc kw:    {ex['surr_doc']}\")\n",
    "print(f\"Random prefix:  {ex['random_prefix'][:60]}...\")\n",
    "\n",
    "print(f\"\\n  {'Condition':<20} {'Enc prefix':<25} {'Trunc':>6} {'Dec query':>10} {'Pfx tok':>8}\")\n",
    "print(f\"  {'-'*75}\")\n",
    "for name, prefix, trunc, has_q, n_pfx in [\n",
    "    ('bare',             '(none)',           'no',  'yes', 0),\n",
    "    ('oracle_trunc',     'real query',       'yes', 'yes', n_prefix_oracle),\n",
    "    ('surr_doc_trunc',   ex['surr_doc'][:20],'yes', 'yes', n_prefix_doc),\n",
    "    ('random_trunc',     '(unrelated)',      'yes', 'yes', n_prefix_random),\n",
    "    ('bare_nq',          '(none)',           'no',  'no',  0),\n",
    "    ('oracle_trunc_nq',  'real query',       'yes', 'no',  n_prefix_oracle),\n",
    "]:\n",
    "    print(f\"  {name:<20} {prefix:<25} {trunc:>6} {has_q:>10} {n_pfx:>8}\")\n",
    "\n",
    "# Decoder input structure\n",
    "q_ids = tokenizer(ex['query'], add_special_tokens=False).input_ids\n",
    "a_ids = tokenizer(ex['answer'], add_special_tokens=False).input_ids\n",
    "print(f\"\\nDecoder input (query-prefix): [BOS] + query ({len(q_ids)} tok) + \"\n",
    "      f\"answer ({len(a_ids)} tok) = {1 + len(q_ids) + len(a_ids)} tok total\")\n",
    "print(f\"NLL computed on last {len(a_ids)} positions (answer only)\")\n",
    "\n",
    "# Quick sanity at original length\n",
    "print(f\"\\nSanity check (original length)...\")\n",
    "nll_bare = score_nll_query_prefix(doc, ex['query'], ex['answer'])\n",
    "nll_oracle = score_nll_query_prefix(\n",
    "    ex['query'] + \"\\n\" + doc, ex['query'], ex['answer'],\n",
    "    prefix_token_count=n_prefix_oracle, truncate=True)\n",
    "print(f\"  bare:          {nll_bare:.4f}\")\n",
    "print(f\"  oracle_trunc:  {nll_oracle:.4f}\")\n",
    "print(f\"  delta:         {nll_bare - nll_oracle:+.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f41478",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Scoring loop — 6 conditions x 6 length bins x 500 samples\n",
    "print(\"=\" * 70)\n",
    "print(\"SCORING ALL CONDITIONS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "COND_NAMES = [\n",
    "    'bare', 'oracle_trunc', 'surr_doc_trunc', 'random_trunc',\n",
    "    'bare_nq', 'oracle_trunc_nq',\n",
    "]\n",
    "\n",
    "# Checkpoint format: {bins: {length_bin: {results: [...]}}, n_total: N}\n",
    "all_checkpoint = {}\n",
    "if CHECKPOINT_PATH.exists():\n",
    "    saved = json.loads(CHECKPOINT_PATH.read_text())\n",
    "    if saved.get('n_total') == N_SAMPLES:\n",
    "        all_checkpoint = saved.get('bins', {})\n",
    "        summary = ', '.join(f'{k}={len(v.get(\"results\",[]))}' for k, v in all_checkpoint.items())\n",
    "        print(f\"Loaded checkpoint: {summary}\")\n",
    "\n",
    "t0_total = time.time()\n",
    "\n",
    "for length_bin in LENGTH_BINS:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"LENGTH BIN: {length_bin} \"\n",
    "          f\"(target={TARGET_LENGTHS[length_bin] or 'no padding'})\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "    # Check for existing results\n",
    "    bin_results = []\n",
    "    start_idx = 0\n",
    "    if length_bin in all_checkpoint:\n",
    "        bin_data = all_checkpoint[length_bin]\n",
    "        saved_results = bin_data.get('results', [])\n",
    "        saved_queries = [r['query'][:50] for r in saved_results]\n",
    "        current_queries = [s['query'][:50] for s in samples[:len(saved_queries)]]\n",
    "        if saved_queries == current_queries:\n",
    "            bin_results = saved_results\n",
    "            start_idx = len(bin_results)\n",
    "            print(f\"  Resuming from sample {start_idx}/{N_SAMPLES}\")\n",
    "\n",
    "    if start_idx >= N_SAMPLES:\n",
    "        print(f\"  Already complete ({len(bin_results)} results)\")\n",
    "        all_checkpoint[length_bin] = {\"results\": bin_results}\n",
    "        continue\n",
    "\n",
    "    t0_bin = time.time()\n",
    "\n",
    "    for i in tqdm(range(start_idx, N_SAMPLES), initial=start_idx, total=N_SAMPLES,\n",
    "                  desc=f\"  {length_bin}\"):\n",
    "        s = samples[i]\n",
    "        padded_passage = padded_docs[length_bin][i]\n",
    "        query = s['query']\n",
    "        answer = s['answer']\n",
    "\n",
    "        # Count prefix tokens for this (padded) passage\n",
    "        n_pfx_oracle = count_prefix_tokens(query, padded_passage)\n",
    "        n_pfx_doc = count_prefix_tokens(s['surr_doc'], padded_passage)\n",
    "        n_pfx_random = count_prefix_tokens(s['random_prefix'], padded_passage)\n",
    "\n",
    "        result = {\n",
    "            'query': query,\n",
    "            'answer': answer,\n",
    "            'passage_words': s['word_count'],\n",
    "            'padded_tokens': len(tokenizer(padded_passage, add_special_tokens=True).input_ids),\n",
    "        }\n",
    "\n",
    "        # --- Conditions 1-4: query in decoder (production-realistic) ---\n",
    "\n",
    "        # 1. bare: encoder=[doc], decoder=[query]->answer\n",
    "        result['nll_bare'] = score_nll_query_prefix(\n",
    "            padded_passage, query, answer)\n",
    "\n",
    "        # 2. oracle_trunc: encoder=[query+doc], decoder=[query]->answer, mask prefix\n",
    "        result['nll_oracle_trunc'] = score_nll_query_prefix(\n",
    "            query + \"\\n\" + padded_passage, query, answer,\n",
    "            prefix_token_count=n_pfx_oracle, truncate=True)\n",
    "\n",
    "        # 3. surr_doc_trunc: encoder=[kw+doc], decoder=[query]->answer, mask prefix\n",
    "        result['nll_surr_doc_trunc'] = score_nll_query_prefix(\n",
    "            s['surr_doc'] + \"\\n\" + padded_passage, query, answer,\n",
    "            prefix_token_count=n_pfx_doc, truncate=True)\n",
    "\n",
    "        # 4. random_trunc: encoder=[random+doc], decoder=[query]->answer, mask prefix\n",
    "        result['nll_random_trunc'] = score_nll_query_prefix(\n",
    "            s['random_prefix'] + \"\\n\" + padded_passage, query, answer,\n",
    "            prefix_token_count=n_pfx_random, truncate=True)\n",
    "\n",
    "        # --- Conditions 5-6: no query in decoder (v3 replication) ---\n",
    "\n",
    "        # 5. bare_nq: encoder=[doc], decoder=answer only\n",
    "        result['nll_bare_nq'] = score_nll(padded_passage, answer)\n",
    "\n",
    "        # 6. oracle_trunc_nq: encoder=[query+doc], decoder=answer only, mask prefix\n",
    "        result['nll_oracle_trunc_nq'] = score_nll(\n",
    "            query + \"\\n\" + padded_passage, answer,\n",
    "            prefix_token_count=n_pfx_oracle, truncate=True)\n",
    "\n",
    "        bin_results.append(result)\n",
    "\n",
    "        if (i + 1) % 20 == 0 or i == N_SAMPLES - 1:\n",
    "            all_checkpoint[length_bin] = {\"results\": bin_results}\n",
    "            ckpt = {\n",
    "                'n_total': N_SAMPLES,\n",
    "                'bins': all_checkpoint,\n",
    "                'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            }\n",
    "            CHECKPOINT_PATH.write_text(json.dumps(ckpt))\n",
    "            elapsed_bin = time.time() - t0_bin\n",
    "            done = i - start_idx + 1\n",
    "            eta = (N_SAMPLES - i - 1) * elapsed_bin / done if done > 0 else 0\n",
    "            tqdm.write(f\"    Checkpoint {i+1}/{N_SAMPLES} | \"\n",
    "                       f\"{elapsed_bin/60:.1f}m | ETA {eta/60:.1f}m\")\n",
    "\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    elapsed_bin = time.time() - t0_bin\n",
    "    print(f\"  {length_bin} complete: {len(bin_results)} samples in {elapsed_bin/60:.1f} min\")\n",
    "\n",
    "    # Quick peek\n",
    "    bare_arr = np.array([r['nll_bare'] for r in bin_results])\n",
    "    orc_arr = np.array([r['nll_oracle_trunc'] for r in bin_results])\n",
    "    d_peek = cohens_d(bare_arr - orc_arr)\n",
    "    print(f\"  Quick peek: oracle_trunc d={d_peek:+.3f}\")\n",
    "\n",
    "elapsed_total = time.time() - t0_total\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"ALL BINS COMPLETE: {elapsed_total/60:.1f} min total\")\n",
    "print(f\"{'='*70}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b5948b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Results table — all conditions at all lengths\n",
    "print(\"=\" * 70)\n",
    "print(f\"RESULTS (N={N_SAMPLES})\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# N_BONFERRONI: 3 query-prefix conditions x 6 lengths + 1 nq condition x 6 lengths = 24\n",
    "N_BONF = 24\n",
    "\n",
    "results_by_bin = {}\n",
    "analysis = {}\n",
    "\n",
    "for length_bin in LENGTH_BINS:\n",
    "    bin_results = all_checkpoint[length_bin]['results']\n",
    "    results_by_bin[length_bin] = bin_results\n",
    "    n = len(bin_results)\n",
    "    mean_tokens = np.mean([r['padded_tokens'] for r in bin_results])\n",
    "\n",
    "    # Extract NLL arrays\n",
    "    bare = np.array([r['nll_bare'] for r in bin_results])\n",
    "    oracle_trunc = np.array([r['nll_oracle_trunc'] for r in bin_results])\n",
    "    surr_doc = np.array([r['nll_surr_doc_trunc'] for r in bin_results])\n",
    "    random_trunc = np.array([r['nll_random_trunc'] for r in bin_results])\n",
    "    bare_nq = np.array([r['nll_bare_nq'] for r in bin_results])\n",
    "    oracle_nq = np.array([r['nll_oracle_trunc_nq'] for r in bin_results])\n",
    "\n",
    "    print(f\"\\n--- {length_bin} (mean {mean_tokens:.0f} tokens, N={n}) ---\")\n",
    "\n",
    "    # Query-in-decoder conditions\n",
    "    print(f\"  With query in decoder:\")\n",
    "    print(f\"  {'Condition':<20} {'NLL':>8} {'vs bare':>10} {'d':>8} {'Win%':>8} {'p':>12} {'sig':>5}\")\n",
    "    print(f\"  {'-'*74}\")\n",
    "\n",
    "    analysis[length_bin] = {'mean_tokens': float(mean_tokens)}\n",
    "    for name, nlls in [('bare', bare), ('oracle_trunc', oracle_trunc),\n",
    "                        ('surr_doc_trunc', surr_doc), ('random_trunc', random_trunc)]:\n",
    "        mean_nll = nlls.mean()\n",
    "        if name == 'bare':\n",
    "            print(f\"  {name:<20} {mean_nll:>8.4f} {'--':>10} {'--':>8} {'--':>8} {'--':>12} {'--':>5}\")\n",
    "            analysis[length_bin][name] = {'mean_nll': float(mean_nll)}\n",
    "        else:\n",
    "            diff = bare - nlls\n",
    "            d = cohens_d(diff)\n",
    "            win_pct = 100 * np.mean(diff > 0)\n",
    "            _, p_val = stats.ttest_1samp(diff, 0)\n",
    "            sig = '***' if p_val < 0.001/N_BONF else '**' if p_val < 0.01/N_BONF else '*' if p_val < 0.05/N_BONF else 'ns'\n",
    "            print(f\"  {name:<20} {mean_nll:>8.4f} {diff.mean():>+10.4f} {d:>+8.3f} {win_pct:>7.1f}% {p_val:>12.2e} {sig:>5}\")\n",
    "            analysis[length_bin][name] = {\n",
    "                'mean_nll': float(mean_nll), 'delta': float(diff.mean()),\n",
    "                'd': float(d), 'win_pct': float(win_pct), 'p': float(p_val),\n",
    "            }\n",
    "\n",
    "    # No-query conditions\n",
    "    diff_nq = bare_nq - oracle_nq\n",
    "    d_nq = cohens_d(diff_nq)\n",
    "    win_nq = 100 * np.mean(diff_nq > 0)\n",
    "    _, p_nq = stats.ttest_1samp(diff_nq, 0)\n",
    "    sig_nq = '***' if p_nq < 0.001/N_BONF else '**' if p_nq < 0.01/N_BONF else '*' if p_nq < 0.05/N_BONF else 'ns'\n",
    "\n",
    "    print(f\"\\n  Without query in decoder (v3 replication):\")\n",
    "    print(f\"  {'bare_nq':<20} {bare_nq.mean():>8.4f}\")\n",
    "    print(f\"  {'oracle_trunc_nq':<20} {oracle_nq.mean():>8.4f} {diff_nq.mean():>+10.4f} {d_nq:>+8.3f} {win_nq:>7.1f}% {p_nq:>12.2e} {sig_nq:>5}\")\n",
    "\n",
    "    analysis[length_bin]['bare_nq'] = {'mean_nll': float(bare_nq.mean())}\n",
    "    analysis[length_bin]['oracle_trunc_nq'] = {\n",
    "        'mean_nll': float(oracle_nq.mean()), 'delta': float(diff_nq.mean()),\n",
    "        'd': float(d_nq), 'win_pct': float(win_nq), 'p': float(p_nq),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110f7f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Decay curve analysis\n",
    "print(\"=\" * 70)\n",
    "print(\"DECAY CURVE ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# v3 Exp 03 reference data (from EXPERIMENT_PLAN.md)\n",
    "v3_oracle_d = {\n",
    "    \"original\": 0.41, \"256\": 0.42, \"512\": 0.38,\n",
    "    \"1024\": 0.45, \"2048\": 0.42, \"4096\": 0.40,\n",
    "}\n",
    "\n",
    "# v4 Exp 01 reference\n",
    "v4_exp01_oracle_d = 0.228  # at original length\n",
    "\n",
    "# Collect d values\n",
    "print(f\"\\n--- Cohen's d vs Length (with query in decoder) ---\")\n",
    "print(f\"  {'Length':<10} {'oracle':>10} {'surr_doc':>10} {'random':>10} {'tokens':>8} {'v3 oracle':>10} {'v4/v3':>8}\")\n",
    "print(f\"  {'-'*72}\")\n",
    "\n",
    "decay_data = {\n",
    "    'length_bin': [], 'mean_tokens': [],\n",
    "    'd_oracle': [], 'd_surr_doc': [], 'd_random': [],\n",
    "    'd_oracle_nq': [], 'd_v3_oracle': [],\n",
    "}\n",
    "\n",
    "for lb in LENGTH_BINS:\n",
    "    mean_tok = analysis[lb]['mean_tokens']\n",
    "    d_orc = analysis[lb].get('oracle_trunc', {}).get('d', 0)\n",
    "    d_doc = analysis[lb].get('surr_doc_trunc', {}).get('d', 0)\n",
    "    d_rnd = analysis[lb].get('random_trunc', {}).get('d', 0)\n",
    "    d_nq = analysis[lb].get('oracle_trunc_nq', {}).get('d', 0)\n",
    "    v3_d = v3_oracle_d.get(lb, 0)\n",
    "\n",
    "    decay_data['length_bin'].append(lb)\n",
    "    decay_data['mean_tokens'].append(mean_tok)\n",
    "    decay_data['d_oracle'].append(d_orc)\n",
    "    decay_data['d_surr_doc'].append(d_doc)\n",
    "    decay_data['d_random'].append(d_rnd)\n",
    "    decay_data['d_oracle_nq'].append(d_nq)\n",
    "    decay_data['d_v3_oracle'].append(v3_d)\n",
    "\n",
    "    ratio_v3 = d_orc / v3_d * 100 if v3_d > 0 else 0\n",
    "    print(f\"  {lb:<10} {d_orc:>+10.3f} {d_doc:>+10.3f} {d_rnd:>+10.3f} {mean_tok:>7.0f} {v3_d:>+10.3f} {ratio_v3:>7.0f}%\")\n",
    "\n",
    "# Decay rate\n",
    "print(f\"\\n--- Retention vs Original Length ---\")\n",
    "orig_d = analysis[\"original\"].get('oracle_trunc', {}).get('d', 0)\n",
    "for cond in ['oracle_trunc', 'surr_doc_trunc', 'random_trunc']:\n",
    "    orig_c = analysis[\"original\"].get(cond, {}).get('d', 0)\n",
    "    if orig_c == 0:\n",
    "        continue\n",
    "    print(f\"\\n  {cond}:\")\n",
    "    for lb in LENGTH_BINS:\n",
    "        d = analysis[lb].get(cond, {}).get('d', 0)\n",
    "        p = analysis[lb].get(cond, {}).get('p', 1)\n",
    "        retention = d / orig_c * 100 if orig_c > 0 else 0\n",
    "        sig = '***' if p < 0.001/N_BONF else '**' if p < 0.01/N_BONF else '*' if p < 0.05/N_BONF else 'ns'\n",
    "        print(f\"    {lb:>8s}: d={d:+.3f} ({retention:5.1f}% of original) {sig}\")\n",
    "\n",
    "# v3 replication: should show flat curve\n",
    "print(f\"\\n--- v3 Replication (no query in decoder) ---\")\n",
    "print(f\"  {'Length':<10} {'d (nq)':>10} {'v3 ref':>10} {'ratio':>8}\")\n",
    "print(f\"  {'-'*42}\")\n",
    "for i, lb in enumerate(LENGTH_BINS):\n",
    "    d_nq = decay_data['d_oracle_nq'][i]\n",
    "    v3_d = decay_data['d_v3_oracle'][i]\n",
    "    ratio = d_nq / v3_d * 100 if v3_d > 0 else 0\n",
    "    print(f\"  {lb:<10} {d_nq:>+10.3f} {v3_d:>+10.3f} {ratio:>7.0f}%\")\n",
    "\n",
    "# Structural fraction at each length\n",
    "print(f\"\\n--- Structural Fraction (random/oracle) at Each Length ---\")\n",
    "print(f\"  {'Length':<10} {'oracle':>10} {'random':>10} {'struct%':>10}\")\n",
    "print(f\"  {'-'*44}\")\n",
    "for i, lb in enumerate(LENGTH_BINS):\n",
    "    d_orc = decay_data['d_oracle'][i]\n",
    "    d_rnd = decay_data['d_random'][i]\n",
    "    frac = d_rnd / d_orc * 100 if d_orc > 0 else 0\n",
    "    print(f\"  {lb:<10} {d_orc:>+10.3f} {d_rnd:>+10.3f} {frac:>9.0f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3f549d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Decay curve plots\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "x_tokens = decay_data['mean_tokens']\n",
    "\n",
    "# --- Panel 1: v4 decay curves (with query in decoder) ---\n",
    "ax = axes[0]\n",
    "for cond, d_key, color, marker in [\n",
    "    ('oracle_trunc', 'd_oracle', 'tab:red', 'o'),\n",
    "    ('surr_doc_trunc', 'd_surr_doc', 'tab:blue', 's'),\n",
    "    ('random_trunc', 'd_random', 'tab:gray', '^'),\n",
    "]:\n",
    "    d_vals = decay_data[d_key]\n",
    "    ax.plot(x_tokens, d_vals, f'-{marker}', color=color, label=cond, markersize=8)\n",
    "    for j, (x, d) in enumerate(zip(x_tokens, d_vals)):\n",
    "        p = analysis[LENGTH_BINS[j]].get(cond, {}).get('p', 1)\n",
    "        if p < 0.05 / N_BONF:\n",
    "            ax.annotate('*', (x, d), textcoords=\"offset points\",\n",
    "                       xytext=(0, 8), ha='center', fontsize=14, color=color)\n",
    "\n",
    "ax.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "ax.set_xlabel('Document Length (tokens)')\n",
    "ax.set_ylabel(\"Cohen's d (vs bare)\")\n",
    "ax.set_title('v4: Effect Size vs Length\\n(query in decoder)')\n",
    "ax.legend(fontsize=8)\n",
    "ax.set_xscale('log', base=2)\n",
    "ax.set_xticks(x_tokens)\n",
    "ax.set_xticklabels(LENGTH_BINS, rotation=45)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# --- Panel 2: v4 oracle vs v3 oracle ---\n",
    "ax = axes[1]\n",
    "ax.plot(x_tokens, decay_data['d_oracle'], '-o', color='tab:red',\n",
    "        label='v4 oracle (query in decoder)', markersize=8)\n",
    "ax.plot(x_tokens, decay_data['d_oracle_nq'], '-s', color='tab:orange',\n",
    "        label='v4 oracle_nq (v3 replication)', markersize=8)\n",
    "ax.plot(x_tokens, decay_data['d_v3_oracle'], '--^', color='tab:purple',\n",
    "        label='v3 Exp 03 oracle (reference)', markersize=8, alpha=0.7)\n",
    "\n",
    "ax.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "ax.set_xlabel('Document Length (tokens)')\n",
    "ax.set_ylabel(\"Cohen's d (vs bare)\")\n",
    "ax.set_title('Oracle Enrichment: v4 vs v3\\nacross lengths')\n",
    "ax.legend(fontsize=8)\n",
    "ax.set_xscale('log', base=2)\n",
    "ax.set_xticks(x_tokens)\n",
    "ax.set_xticklabels(LENGTH_BINS, rotation=45)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# --- Panel 3: v4/v3 ratio across lengths ---\n",
    "ax = axes[2]\n",
    "ratios = [d4/d3 * 100 if d3 > 0 else 0\n",
    "          for d4, d3 in zip(decay_data['d_oracle'], decay_data['d_v3_oracle'])]\n",
    "ax.plot(x_tokens, ratios, '-o', color='tab:green', markersize=8)\n",
    "ax.axhline(y=60.6, color='tab:red', linestyle='--', alpha=0.5,\n",
    "           label=f'Exp 01 ratio: 60.6%')\n",
    "ax.axhline(y=100, color='gray', linestyle='--', alpha=0.3)\n",
    "\n",
    "ax.set_xlabel('Document Length (tokens)')\n",
    "ax.set_ylabel('v4/v3 enrichment ratio (%)')\n",
    "ax.set_title('Enrichment Preservation\\n(v4 oracle / v3 oracle)')\n",
    "ax.legend(fontsize=8)\n",
    "ax.set_xscale('log', base=2)\n",
    "ax.set_xticks(x_tokens)\n",
    "ax.set_xticklabels(LENGTH_BINS, rotation=45)\n",
    "ax.set_ylim(0, 120)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plot_path = RESULTS_DIR / 'decay_curves.png'\n",
    "plt.savefig(plot_path, dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"Plot saved to {plot_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8add32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Verdict and save\n",
    "print(\"=\" * 70)\n",
    "print(\"VERDICT -- Exp 02: Length Scaling with Query in Decoder\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\nModel: {MODEL_NAME}\")\n",
    "print(f\"N: {N_SAMPLES} samples per length bin\")\n",
    "print(f\"Length bins: {LENGTH_BINS}\")\n",
    "print(f\"Bonferroni correction: {N_BONF} comparisons\")\n",
    "\n",
    "# Key question: does the v4 benefit decay?\n",
    "print(f\"\\n--- Oracle Enrichment Decay (with query in decoder) ---\")\n",
    "orig_d = analysis[\"original\"].get('oracle_trunc', {}).get('d', 0)\n",
    "last_sig = \"none\"\n",
    "for lb in LENGTH_BINS:\n",
    "    d = analysis[lb].get('oracle_trunc', {}).get('d', 0)\n",
    "    p = analysis[lb].get('oracle_trunc', {}).get('p', 1)\n",
    "    sig = p < 0.05 / N_BONF\n",
    "    retention = d / orig_d * 100 if orig_d > 0 else 0\n",
    "    sig_str = \"SIG\" if sig else \"ns\"\n",
    "    print(f\"  {lb:>8s}: d={d:+.3f} ({retention:5.1f}% retained) [{sig_str}]\")\n",
    "    if sig:\n",
    "        last_sig = lb\n",
    "\n",
    "print(f\"\\n  Last significant bin: {last_sig}\")\n",
    "\n",
    "# Compare patterns\n",
    "print(f\"\\n--- Pattern Summary ---\")\n",
    "nq_flat = all(\n",
    "    analysis[lb].get('oracle_trunc_nq', {}).get('p', 1) < 0.05 / N_BONF\n",
    "    for lb in LENGTH_BINS\n",
    ")\n",
    "v4_decays = not all(\n",
    "    analysis[lb].get('oracle_trunc', {}).get('p', 1) < 0.05 / N_BONF\n",
    "    for lb in LENGTH_BINS\n",
    ")\n",
    "\n",
    "if nq_flat and not v4_decays:\n",
    "    print(\"  v3 replication: FLAT (all lengths significant) -- matches v3 Exp 03\")\n",
    "    print(\"  v4 enrichment:  FLAT (all lengths significant)\")\n",
    "    print(\"  -> Content-dependent enrichment is ALSO length-invariant.\")\n",
    "    print(\"     The mechanism works at all scales.\")\n",
    "elif nq_flat and v4_decays:\n",
    "    print(\"  v3 replication: FLAT (all lengths significant) -- matches v3 Exp 03\")\n",
    "    print(\"  v4 enrichment:  DECAYS (some lengths non-significant)\")\n",
    "    print(f\"  -> Content-dependent enrichment FADES at longer documents.\")\n",
    "    print(f\"     Last significant: {last_sig}. This limits production applicability.\")\n",
    "else:\n",
    "    print(\"  v3 replication: UNEXPECTED (check results)\")\n",
    "\n",
    "# Surrogate performance across lengths\n",
    "print(f\"\\n--- Surrogate vs Oracle at Each Length ---\")\n",
    "print(f\"  {'Length':<10} {'oracle d':>10} {'surr_doc d':>10} {'surr/oracle':>12}\")\n",
    "print(f\"  {'-'*46}\")\n",
    "for lb in LENGTH_BINS:\n",
    "    od = analysis[lb].get('oracle_trunc', {}).get('d', 0)\n",
    "    sd = analysis[lb].get('surr_doc_trunc', {}).get('d', 0)\n",
    "    ratio = sd / od * 100 if od > 0 else 0\n",
    "    print(f\"  {lb:<10} {od:>+10.3f} {sd:>+10.3f} {ratio:>11.0f}%\")\n",
    "\n",
    "# Structural fraction trend\n",
    "print(f\"\\n--- Structural Fraction Trend ---\")\n",
    "struct_fracs = []\n",
    "for lb in LENGTH_BINS:\n",
    "    od = analysis[lb].get('oracle_trunc', {}).get('d', 0)\n",
    "    rd = analysis[lb].get('random_trunc', {}).get('d', 0)\n",
    "    frac = rd / od * 100 if od > 0 else 0\n",
    "    struct_fracs.append(frac)\n",
    "    print(f\"  {lb:<10}: {frac:.0f}%\")\n",
    "\n",
    "mean_struct = np.mean(struct_fracs)\n",
    "print(f\"\\n  Mean structural fraction: {mean_struct:.0f}%\")\n",
    "if mean_struct < 50:\n",
    "    print(f\"  Content-dependent mechanism dominates at all lengths.\")\n",
    "else:\n",
    "    print(f\"  Structural component grows with length (decoder query less effective at long range).\")\n",
    "\n",
    "# Save\n",
    "final_results = {\n",
    "    'experiment': 'v4_exp02_length_scaling',\n",
    "    'model': MODEL_NAME,\n",
    "    'n_samples': N_SAMPLES,\n",
    "    'seed': SEED,\n",
    "    'length_bins': LENGTH_BINS,\n",
    "    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'bonferroni': N_BONF,\n",
    "    'analysis': analysis,\n",
    "    'decay_data': decay_data,\n",
    "    'padded_stats': padded_stats,\n",
    "    'v3_reference': v3_oracle_d,\n",
    "    'v4_exp01_reference': v4_exp01_oracle_d,\n",
    "}\n",
    "\n",
    "with open(RESULTS_DIR / 'results.json', 'w') as f:\n",
    "    json.dump(final_results, f, indent=2)\n",
    "print(f\"\\nResults saved to {RESULTS_DIR / 'results.json'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c233f59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Cleanup\n",
    "print(\"Cleaning up GPU memory...\")\n",
    "mem_before = torch.cuda.memory_allocated() / 1e9\n",
    "del model, processor, tokenizer\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "mem_after = torch.cuda.memory_allocated() / 1e9\n",
    "print(f\"GPU memory: {mem_before:.2f} GB -> {mem_after:.2f} GB\")\n",
    "print(\"Done!\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
