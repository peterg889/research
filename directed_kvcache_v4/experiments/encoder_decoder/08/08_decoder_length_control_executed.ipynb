{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e740caea",
   "metadata": {
    "papermill": {
     "duration": 0.003222,
     "end_time": "2026-02-20T22:25:11.777343",
     "exception": false,
     "start_time": "2026-02-20T22:25:11.774121",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Experiment 08: Decoder Length Control\n",
    "\n",
    "## Motivation\n",
    "\n",
    "Exp 07 found that decoder query tokens absorb 5.5% of answer-token attention and\n",
    "the nqâ†’q NLL improvement is d=+0.309. But the `_q` conditions have **longer decoder\n",
    "sequences** (`[BOS, query, answer]` vs `[BOS, answer]`). This means:\n",
    "\n",
    "1. The 5.5% \"query buffer\" could be a **length artifact** â€” any extra tokens would\n",
    "   absorb some attention mechanically\n",
    "2. The NLL improvement could partly be due to the decoder having more context positions\n",
    "   (longer causal window) rather than the query's semantic content\n",
    "\n",
    "## Design: 2Ã—3 Factorial\n",
    "\n",
    "Add `random_q` conditions where `decoder_input_ids = [BOS, random_tokens, answer]`\n",
    "with `len(random_tokens) == len(query_ids)` per sample. Random tokens are sampled\n",
    "uniformly from the vocabulary (avoiding special tokens).\n",
    "\n",
    "| # | Condition | Encoder input | Cross-attn mask | Decoder input |\n",
    "|---|-----------|--------------|-----------------|---------------|\n",
    "| 1 | bare_nq | [document] | all visible | [BOS, answer] |\n",
    "| 2 | bare_random_q | [document] | all visible | [BOS, random, answer] |\n",
    "| 3 | bare_q | [document] | all visible | [BOS, query, answer] |\n",
    "| 4 | oracle_trunc_nq | [query + doc] | doc only | [BOS, answer] |\n",
    "| 5 | oracle_trunc_random_q | [query + doc] | doc only | [BOS, random, answer] |\n",
    "| 6 | oracle_trunc_q | [query + doc] | doc only | [BOS, query, answer] |\n",
    "\n",
    "## Key comparisons\n",
    "\n",
    "**NLL decomposition of nqâ†’q improvement:**\n",
    "- `nq â†’ random_q`: Pure length effect (no semantics)\n",
    "- `random_q â†’ q`: Query-specific semantic effect (length-controlled)\n",
    "- If length accounts for most of the improvement, `random_q` â‰ˆ `q`\n",
    "\n",
    "**Attention decomposition:**\n",
    "- Does random prefix absorb similar attention as query (~5.5%)?\n",
    "- Does random prefix steal from the same budget (cross-attention)?\n",
    "- If yes, the \"query buffer\" is purely mechanical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8bc2b911",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-20T22:25:11.784989Z",
     "iopub.status.busy": "2026-02-20T22:25:11.784160Z",
     "iopub.status.idle": "2026-02-20T22:25:34.605010Z",
     "shell.execute_reply": "2026-02-20T22:25:34.604173Z"
    },
    "papermill": {
     "duration": 22.826517,
     "end_time": "2026-02-20T22:25:34.606730",
     "exception": false,
     "start_time": "2026-02-20T22:25:11.780213",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading google/t5gemma-2-4b-4b with attn_implementation='eager'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62098e306df8469e88a9e357999117a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/1327 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exp 08: Decoder Length Control\n",
      "N: 500, Model: google/t5gemma-2-4b-4b\n",
      "DEVICE: cuda:0, dtype: torch.bfloat16\n",
      "GPU memory: 15.02 GB\n",
      "Decoder layers: 34, Vocab size: 262144\n",
      "Probe layers: [0, 17, 33]\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Setup and model loading (EAGER attention for weight extraction)\n",
    "import os\n",
    "os.umask(0o000)\n",
    "\n",
    "import sys, json, time, gc, re\n",
    "import random as pyrandom\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "from collections import Counter, defaultdict\n",
    "from scipy import stats\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "sys.path.insert(0, \"../../..\")\n",
    "from lib.analysis import cohens_d\n",
    "\n",
    "SEED = 42\n",
    "N_SAMPLES = 500\n",
    "MODEL_NAME = \"google/t5gemma-2-4b-4b\"\n",
    "\n",
    "RESULTS_DIR = Path(\"../../../results/exp08\")\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CHECKPOINT_PATH = RESULTS_DIR / \"checkpoint.json\"\n",
    "\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "pyrandom.seed(SEED)\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv())\n",
    "HF_TOKEN = os.environ.get(\"HF_TOKEN\")\n",
    "\n",
    "from transformers import AutoProcessor, AutoModelForSeq2SeqLM\n",
    "\n",
    "print(f\"Loading {MODEL_NAME} with attn_implementation='eager'...\")\n",
    "processor = AutoProcessor.from_pretrained(MODEL_NAME, token=HF_TOKEN)\n",
    "tokenizer = processor.tokenizer\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    MODEL_NAME, device_map=\"auto\", torch_dtype=torch.bfloat16, token=HF_TOKEN,\n",
    "    attn_implementation=\"eager\",\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "DEVICE = next(model.parameters()).device\n",
    "BOS_ID = getattr(model.config, 'decoder_start_token_id', None) or tokenizer.bos_token_id\n",
    "\n",
    "# Discover decoder layer count\n",
    "N_DEC_LAYERS = len(model.model.decoder.layers)\n",
    "# Probe only 3 key layers: first, middle, last (faster than 6)\n",
    "PROBE_LAYERS = [0, N_DEC_LAYERS // 2, N_DEC_LAYERS - 1]\n",
    "\n",
    "VOCAB_SIZE = len(tokenizer)\n",
    "\n",
    "print(f\"Exp 08: Decoder Length Control\")\n",
    "print(f\"N: {N_SAMPLES}, Model: {MODEL_NAME}\")\n",
    "print(f\"DEVICE: {DEVICE}, dtype: {next(model.parameters()).dtype}\")\n",
    "print(f\"GPU memory: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "print(f\"Decoder layers: {N_DEC_LAYERS}, Vocab size: {VOCAB_SIZE}\")\n",
    "print(f\"Probe layers: {PROBE_LAYERS}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86cef680",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-20T22:25:34.616051Z",
     "iopub.status.busy": "2026-02-20T22:25:34.615130Z",
     "iopub.status.idle": "2026-02-20T22:25:36.862090Z",
     "shell.execute_reply": "2026-02-20T22:25:36.861291Z"
    },
    "papermill": {
     "duration": 2.253524,
     "end_time": "2026-02-20T22:25:36.863634",
     "exception": false,
     "start_time": "2026-02-20T22:25:34.610110",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MS MARCO v1.1 validation...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total candidates: 1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 500 samples\n",
      "Mean passage words: 74\n",
      "Mean query words: 6\n",
      "Mean answer words: 14\n",
      "Mean query tokens: 6.5\n",
      "Mean oracle prefix tokens: 7.5\n",
      "\n",
      "Example (sample 0):\n",
      "  Query: what is the link between alveoli and capillaries...\n",
      "  Query tokens (9): [14070, 563, 506, 3205, 1534, 98876, 8244, 532, 134053]...\n",
      "  Random tokens (9): [250759, 126464, 76825, 20959, 84768, 18667, 241532, 208087, 54427]...\n",
      "  Random decoded: ðŸŒ¦à¤®à¥Œç„¼ãultureqw Ð´Ñ€ÑƒÐ³Ð¸æ…¶Issuedà¸›à¸µ...\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Load MS MARCO data + generate matched-length random tokens\n",
    "from lib.data import count_words\n",
    "from datasets import load_dataset\n",
    "\n",
    "print(\"Loading MS MARCO v1.1 validation...\")\n",
    "ds = load_dataset(\"microsoft/ms_marco\", \"v1.1\", split=\"validation\")\n",
    "\n",
    "all_candidates = []\n",
    "for item in ds:\n",
    "    if len(all_candidates) >= 3 * N_SAMPLES:\n",
    "        break\n",
    "    passages = item.get('passages', {})\n",
    "    ptexts = passages.get('passage_text', [])\n",
    "    is_sel = passages.get('is_selected', [])\n",
    "    query = item.get('query', '')\n",
    "    answers = item.get('answers', [])\n",
    "    well_formed = item.get('wellFormedAnswers', [])\n",
    "    answer = None\n",
    "    if well_formed and len(well_formed) > 0 and well_formed[0] not in ('[]', ''):\n",
    "        answer = well_formed[0]\n",
    "    elif answers and len(answers) > 0 and answers[0] != 'No Answer Present.':\n",
    "        answer = answers[0]\n",
    "    if not answer:\n",
    "        continue\n",
    "    for pt, sel in zip(ptexts, is_sel):\n",
    "        wc = count_words(pt)\n",
    "        if sel == 1 and 30 <= wc <= 300:\n",
    "            all_candidates.append({\n",
    "                'passage': pt, 'query': query, 'answer': answer,\n",
    "                'word_count': wc,\n",
    "            })\n",
    "            break\n",
    "\n",
    "print(f\"Total candidates: {len(all_candidates)}\")\n",
    "np.random.seed(SEED)\n",
    "indices = np.random.permutation(len(all_candidates))\n",
    "samples = [all_candidates[i] for i in indices[:N_SAMPLES]]\n",
    "del ds, all_candidates\n",
    "gc.collect()\n",
    "\n",
    "# Count prefix tokens for oracle conditions\n",
    "def count_prefix_tokens(prefix_text, document_text):\n",
    "    full_text = prefix_text + \"\\n\" + document_text\n",
    "    full_ids = tokenizer(full_text, add_special_tokens=True, truncation=True,\n",
    "                         max_length=2048).input_ids\n",
    "    doc_ids = tokenizer(document_text, add_special_tokens=True, truncation=True,\n",
    "                        max_length=2048).input_ids\n",
    "    return len(full_ids) - len(doc_ids)\n",
    "\n",
    "# Pre-tokenize queries and generate matched-length random tokens\n",
    "for i, s in enumerate(samples):\n",
    "    s['n_pfx_oracle'] = count_prefix_tokens(s['query'], s['passage'])\n",
    "    q_ids = tokenizer(s['query'], add_special_tokens=False, truncation=True,\n",
    "                      max_length=512).input_ids\n",
    "    s['query_ids'] = q_ids\n",
    "    # Generate random token IDs of the same length as the query\n",
    "    # Avoid special tokens (IDs 0-99) and the very end of vocab\n",
    "    rng = np.random.RandomState(SEED + i + 10000)\n",
    "    s['random_ids'] = rng.randint(100, VOCAB_SIZE - 100, size=len(q_ids)).tolist()\n",
    "\n",
    "print(f\"Loaded {len(samples)} samples\")\n",
    "print(f\"Mean passage words: {np.mean([s['word_count'] for s in samples]):.0f}\")\n",
    "print(f\"Mean query words: {np.mean([count_words(s['query']) for s in samples]):.0f}\")\n",
    "print(f\"Mean answer words: {np.mean([count_words(s['answer']) for s in samples]):.0f}\")\n",
    "print(f\"Mean query tokens: {np.mean([len(s['query_ids']) for s in samples]):.1f}\")\n",
    "print(f\"Mean oracle prefix tokens: {np.mean([s['n_pfx_oracle'] for s in samples]):.1f}\")\n",
    "\n",
    "# Show example random vs query tokens\n",
    "s0 = samples[0]\n",
    "print(f\"\\nExample (sample 0):\")\n",
    "print(f\"  Query: {s0['query'][:60]}...\")\n",
    "print(f\"  Query tokens ({len(s0['query_ids'])}): {s0['query_ids'][:10]}...\")\n",
    "print(f\"  Random tokens ({len(s0['random_ids'])}): {s0['random_ids'][:10]}...\")\n",
    "print(f\"  Random decoded: {tokenizer.decode(s0['random_ids'][:10])}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80ac635f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-20T22:25:36.874146Z",
     "iopub.status.busy": "2026-02-20T22:25:36.873572Z",
     "iopub.status.idle": "2026-02-20T22:25:36.888511Z",
     "shell.execute_reply": "2026-02-20T22:25:36.887756Z"
    },
    "papermill": {
     "duration": 0.023103,
     "end_time": "2026-02-20T22:25:36.890118",
     "exception": false,
     "start_time": "2026-02-20T22:25:36.867015",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probing function defined. Ready for scoring loop.\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Define probing function (same as Exp 07 but with fewer layers)\n",
    "\n",
    "def forward_with_probes(encoder_outputs, cross_attn_mask, decoder_input_ids,\n",
    "                        answer_start, answer_len, answer_ids_list):\n",
    "    # Forward pass with attention extraction.\n",
    "    # Returns (nll, probes_dict) where probes_dict is keyed by layer index.\n",
    "    dec_len = decoder_input_ids.shape[1]\n",
    "    n_prefix = answer_start - 1  # 0 for _nq, len(prefix_ids) for _q/_random_q\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            encoder_outputs=encoder_outputs,\n",
    "            attention_mask=cross_attn_mask,\n",
    "            decoder_input_ids=decoder_input_ids,\n",
    "            output_attentions=True,\n",
    "        )\n",
    "\n",
    "    # --- NLL ---\n",
    "    logits = outputs.logits\n",
    "    answer_logits = logits[0, n_prefix:n_prefix + answer_len, :]\n",
    "    targets = torch.tensor(answer_ids_list, dtype=torch.long, device=DEVICE)\n",
    "    log_probs = F.log_softmax(answer_logits, dim=-1)\n",
    "    token_log_probs = log_probs.gather(1, targets.unsqueeze(1)).squeeze(1)\n",
    "    nll = -token_log_probs.mean().item()\n",
    "\n",
    "    # --- Probes ---\n",
    "    probes = {}\n",
    "    for layer_idx in PROBE_LAYERS:\n",
    "        # Self-attention: [1, heads, dec_len, dec_len]\n",
    "        sa = outputs.decoder_attentions[layer_idx][0].float().mean(dim=0)\n",
    "        # Cross-attention: [1, heads, dec_len, enc_len]\n",
    "        ca = outputs.cross_attentions[layer_idx][0].float().mean(dim=0)\n",
    "\n",
    "        # Extract answer-token rows\n",
    "        ans_sa = sa[answer_start:answer_start + answer_len]  # [M, dec_len]\n",
    "        ans_ca = ca[answer_start:answer_start + answer_len]  # [M, enc_len]\n",
    "\n",
    "        # Self-attention budget decomposition for answer tokens\n",
    "        self_to_bos = ans_sa[:, 0].mean().item()\n",
    "\n",
    "        if n_prefix > 0:\n",
    "            self_to_prefix = ans_sa[:, 1:1 + n_prefix].sum(dim=1).mean().item()\n",
    "        else:\n",
    "            self_to_prefix = 0.0\n",
    "\n",
    "        # Self-attention to answer positions (causal)\n",
    "        answer_mask = torch.zeros(answer_len, dec_len, device=DEVICE)\n",
    "        for t in range(answer_len):\n",
    "            p = answer_start + t\n",
    "            answer_mask[t, answer_start:p + 1] = 1.0\n",
    "        self_to_answer = (ans_sa * answer_mask).sum(dim=1).mean().item()\n",
    "\n",
    "        # Totals\n",
    "        self_total = ans_sa.sum(dim=1).mean().item()\n",
    "        cross_total = ans_ca.sum(dim=1).mean().item()\n",
    "\n",
    "        # Self-attention entropy\n",
    "        eps = 1e-10\n",
    "        positions = torch.arange(dec_len, device=DEVICE)\n",
    "        abs_positions = torch.arange(answer_start, answer_start + answer_len, device=DEVICE)\n",
    "        causal = (positions.unsqueeze(0) <= abs_positions.unsqueeze(1)).float()\n",
    "        masked_sa = ans_sa * causal\n",
    "        sa_clamped = masked_sa.clamp(min=eps)\n",
    "        self_entropy = -(masked_sa * sa_clamped.log()).sum(dim=1).mean().item()\n",
    "\n",
    "        # Cross-attention entropy\n",
    "        ca_clamped = ans_ca.clamp(min=eps)\n",
    "        cross_entropy = -(ans_ca * ca_clamped.log()).sum(dim=1).mean().item()\n",
    "\n",
    "        probes[layer_idx] = {\n",
    "            'sb': round(self_to_bos, 6),\n",
    "            'sp': round(self_to_prefix, 6),  # 'sp' = self_to_prefix (query or random)\n",
    "            'sa': round(self_to_answer, 6),\n",
    "            'st': round(self_total, 6),\n",
    "            'ct': round(cross_total, 6),\n",
    "            'se': round(self_entropy, 4),\n",
    "            'ce': round(cross_entropy, 4),\n",
    "        }\n",
    "\n",
    "    del outputs, logits, log_probs\n",
    "    return nll, probes\n",
    "\n",
    "\n",
    "print(\"Probing function defined. Ready for scoring loop.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd6a47b6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-20T22:25:36.898442Z",
     "iopub.status.busy": "2026-02-20T22:25:36.898122Z",
     "iopub.status.idle": "2026-02-21T00:28:25.879071Z",
     "shell.execute_reply": "2026-02-21T00:28:25.878348Z"
    },
    "papermill": {
     "duration": 7368.987931,
     "end_time": "2026-02-21T00:28:25.881249",
     "exception": false,
     "start_time": "2026-02-20T22:25:36.893318",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PROBING ALL CONDITIONS\n",
      "======================================================================\n",
      "Resuming from checkpoint: 300/500\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ebf83d1ef1b48a8842e8236fecf8a11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Probing:  60%|######    | 300/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 320/500 | 0.4m | ETA 4.0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 340/500 | 1.1m | ETA 4.3m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 360/500 | 2.6m | ETA 6.1m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 380/500 | 6.0m | ETA 9.0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 400/500 | 12.3m | ETA 12.3m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 420/500 | 22.2m | ETA 14.8m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 440/500 | 37.4m | ETA 16.0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 460/500 | 57.8m | ETA 14.5m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 480/500 | 86.4m | ETA 9.6m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 500/500 | 122.8m | ETA 0.0m\n",
      "\n",
      "Probing complete: 500 samples, 6 conditions in 122.8 min\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Probing loop â€” 6 conditions x 500 samples\n",
    "print(\"=\" * 70)\n",
    "print(\"PROBING ALL CONDITIONS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "COND_NAMES = ['bare_nq', 'bare_random_q', 'bare_q',\n",
    "              'oracle_trunc_nq', 'oracle_trunc_random_q', 'oracle_trunc_q']\n",
    "\n",
    "results = []\n",
    "start_idx = 0\n",
    "\n",
    "if CHECKPOINT_PATH.exists():\n",
    "    ckpt = json.loads(CHECKPOINT_PATH.read_text())\n",
    "    if ckpt.get('n_total') == N_SAMPLES and len(ckpt.get('results', [])) > 0:\n",
    "        saved_queries = [r['query'][:50] for r in ckpt['results']]\n",
    "        current_queries = [s['query'][:50] for s in samples[:len(saved_queries)]]\n",
    "        if saved_queries == current_queries:\n",
    "            results = ckpt['results']\n",
    "            # JSON converts int dict keys to strings â€” convert back\n",
    "            for r in results:\n",
    "                for cond in COND_NAMES:\n",
    "                    key = f'probes_{cond}'\n",
    "                    if key in r and isinstance(r[key], dict):\n",
    "                        r[key] = {int(k): v for k, v in r[key].items()}\n",
    "            start_idx = len(results)\n",
    "            print(f\"Resuming from checkpoint: {start_idx}/{N_SAMPLES}\")\n",
    "\n",
    "if start_idx == 0:\n",
    "    print(f\"Starting fresh: {len(COND_NAMES)} conditions x {N_SAMPLES} samples\")\n",
    "    print(f\"Probe layers: {PROBE_LAYERS}\")\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "for i in tqdm(range(start_idx, N_SAMPLES), initial=start_idx, total=N_SAMPLES,\n",
    "              desc=\"Probing\"):\n",
    "    s = samples[i]\n",
    "    passage = s['passage']\n",
    "    answer = s['answer']\n",
    "    query_ids = s['query_ids']\n",
    "    random_ids = s['random_ids']\n",
    "\n",
    "    answer_ids = tokenizer(answer, add_special_tokens=False, truncation=True,\n",
    "                           max_length=256).input_ids\n",
    "\n",
    "    if len(answer_ids) == 0:\n",
    "        continue\n",
    "\n",
    "    n_prefix = len(query_ids)  # same for query and random\n",
    "\n",
    "    result = {\n",
    "        'query': s['query'][:50],\n",
    "        'n_prefix_toks': n_prefix,\n",
    "        'n_answer_toks': len(answer_ids),\n",
    "    }\n",
    "\n",
    "    # Build decoder tensors once\n",
    "    dec_nq = torch.tensor([[BOS_ID] + answer_ids], dtype=torch.long, device=DEVICE)\n",
    "    dec_random = torch.tensor([[BOS_ID] + random_ids + answer_ids],\n",
    "                               dtype=torch.long, device=DEVICE)\n",
    "    dec_q = torch.tensor([[BOS_ID] + query_ids + answer_ids],\n",
    "                          dtype=torch.long, device=DEVICE)\n",
    "\n",
    "    # --- Encoder pass 1: bare document ---\n",
    "    enc_ids_bare = tokenizer(passage, return_tensors=\"pt\",\n",
    "                             add_special_tokens=True, truncation=True,\n",
    "                             max_length=2048).input_ids.to(DEVICE)\n",
    "    enc_len_bare = enc_ids_bare.shape[1]\n",
    "    enc_mask_bare = torch.ones(1, enc_len_bare, device=DEVICE, dtype=torch.long)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        enc_out_bare = model.get_encoder()(\n",
    "            input_ids=enc_ids_bare, attention_mask=enc_mask_bare\n",
    "        )\n",
    "\n",
    "    # Condition 1: bare_nq\n",
    "    nll, probes = forward_with_probes(\n",
    "        enc_out_bare, enc_mask_bare, dec_nq,\n",
    "        answer_start=1, answer_len=len(answer_ids), answer_ids_list=answer_ids)\n",
    "    result['nll_bare_nq'] = nll\n",
    "    result['probes_bare_nq'] = probes\n",
    "\n",
    "    # Condition 2: bare_random_q\n",
    "    nll, probes = forward_with_probes(\n",
    "        enc_out_bare, enc_mask_bare, dec_random,\n",
    "        answer_start=1 + n_prefix, answer_len=len(answer_ids),\n",
    "        answer_ids_list=answer_ids)\n",
    "    result['nll_bare_random_q'] = nll\n",
    "    result['probes_bare_random_q'] = probes\n",
    "\n",
    "    # Condition 3: bare_q\n",
    "    nll, probes = forward_with_probes(\n",
    "        enc_out_bare, enc_mask_bare, dec_q,\n",
    "        answer_start=1 + n_prefix, answer_len=len(answer_ids),\n",
    "        answer_ids_list=answer_ids)\n",
    "    result['nll_bare_q'] = nll\n",
    "    result['probes_bare_q'] = probes\n",
    "\n",
    "    del enc_out_bare\n",
    "\n",
    "    # --- Encoder pass 2: oracle (query + document) ---\n",
    "    enc_text_oracle = s['query'] + \"\\n\" + passage\n",
    "    enc_ids_oracle = tokenizer(enc_text_oracle, return_tensors=\"pt\",\n",
    "                               add_special_tokens=True, truncation=True,\n",
    "                               max_length=2048).input_ids.to(DEVICE)\n",
    "    enc_len_oracle = enc_ids_oracle.shape[1]\n",
    "    enc_mask_oracle = torch.ones(1, enc_len_oracle, device=DEVICE, dtype=torch.long)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        enc_out_oracle = model.get_encoder()(\n",
    "            input_ids=enc_ids_oracle, attention_mask=enc_mask_oracle\n",
    "        )\n",
    "\n",
    "    # Cross-attention mask: hide prefix\n",
    "    pfx_count = s['n_pfx_oracle']\n",
    "    cross_mask_trunc = torch.ones(1, enc_len_oracle, device=DEVICE, dtype=torch.long)\n",
    "    cross_mask_trunc[:, :pfx_count] = 0\n",
    "\n",
    "    # Condition 4: oracle_trunc_nq\n",
    "    nll, probes = forward_with_probes(\n",
    "        enc_out_oracle, cross_mask_trunc, dec_nq,\n",
    "        answer_start=1, answer_len=len(answer_ids), answer_ids_list=answer_ids)\n",
    "    result['nll_oracle_trunc_nq'] = nll\n",
    "    result['probes_oracle_trunc_nq'] = probes\n",
    "\n",
    "    # Condition 5: oracle_trunc_random_q\n",
    "    nll, probes = forward_with_probes(\n",
    "        enc_out_oracle, cross_mask_trunc, dec_random,\n",
    "        answer_start=1 + n_prefix, answer_len=len(answer_ids),\n",
    "        answer_ids_list=answer_ids)\n",
    "    result['nll_oracle_trunc_random_q'] = nll\n",
    "    result['probes_oracle_trunc_random_q'] = probes\n",
    "\n",
    "    # Condition 6: oracle_trunc_q\n",
    "    nll, probes = forward_with_probes(\n",
    "        enc_out_oracle, cross_mask_trunc, dec_q,\n",
    "        answer_start=1 + n_prefix, answer_len=len(answer_ids),\n",
    "        answer_ids_list=answer_ids)\n",
    "    result['nll_oracle_trunc_q'] = nll\n",
    "    result['probes_oracle_trunc_q'] = probes\n",
    "\n",
    "    del enc_out_oracle\n",
    "    results.append(result)\n",
    "\n",
    "    if (i + 1) % 20 == 0 or i == N_SAMPLES - 1:\n",
    "        ckpt = {\n",
    "            'n_total': N_SAMPLES,\n",
    "            'probe_layers': PROBE_LAYERS,\n",
    "            'results': results,\n",
    "            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        }\n",
    "        CHECKPOINT_PATH.write_text(json.dumps(ckpt))\n",
    "        elapsed = time.time() - t0\n",
    "        done = i - start_idx + 1\n",
    "        eta = (N_SAMPLES - i - 1) * elapsed / done if done > 0 else 0\n",
    "        tqdm.write(f\"  Checkpoint {i+1}/{N_SAMPLES} | {elapsed/60:.1f}m | ETA {eta/60:.1f}m\")\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "elapsed = time.time() - t0\n",
    "print(f\"\\nProbing complete: {len(results)} samples, \"\n",
    "      f\"{len(COND_NAMES)} conditions in {elapsed/60:.1f} min\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48e2d1b6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-21T00:28:25.890963Z",
     "iopub.status.busy": "2026-02-21T00:28:25.890673Z",
     "iopub.status.idle": "2026-02-21T00:28:25.913781Z",
     "shell.execute_reply": "2026-02-21T00:28:25.913139Z"
    },
    "papermill": {
     "duration": 0.030016,
     "end_time": "2026-02-21T00:28:25.915186",
     "exception": false,
     "start_time": "2026-02-21T00:28:25.885170",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "NLL DECOMPOSITION: LENGTH vs QUERY SEMANTICS\n",
      "======================================================================\n",
      "\n",
      "  Condition                        Mean NLL  d vs bare_*   sig\n",
      "  --------------------------------------------------------------\n",
      "  bare_nq                            3.6834            â€”     â€”\n",
      "  bare_random_q                      3.1135       +0.170   ***\n",
      "  bare_q                             2.5537       +0.309   ***\n",
      "\n",
      "  oracle_trunc_nq                    2.9969            â€”     â€”\n",
      "  oracle_trunc_random_q              3.0264       -0.014    ns\n",
      "  oracle_trunc_q                     2.3967       +0.228   ***\n",
      "\n",
      "======================================================================\n",
      "DECOMPOSITION OF nqâ†’q IMPROVEMENT\n",
      "======================================================================\n",
      "\n",
      "  Bare encoder:\n",
      "    Total (nq â†’ q):        d=+0.309\n",
      "    Length (nq â†’ random_q): d=+0.170\n",
      "    Semantic (random â†’ q):  d=+0.317 (***)\n",
      "    Length fraction:    55%\n",
      "    Semantic fraction: 103%\n",
      "\n",
      "  Oracle encoder:\n",
      "    Total (nq â†’ q):        d=+0.228\n",
      "    Length (nq â†’ random_q): d=-0.014\n",
      "    Semantic (random â†’ q):  d=+0.324 (***)\n",
      "    Length fraction:    -6%\n",
      "    Semantic fraction: 142%\n",
      "\n",
      "  Does random prefix hurt answer prediction?\n",
      "    bare: nq â†’ random_q NLL change: -0.5699\n",
      "    oracle: nq â†’ random_q NLL change: +0.0295\n",
      "    (Positive = random hurts, Negative = random helps)\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: NLL decomposition â€” length effect vs query-specific effect\n",
    "print(\"=\" * 70)\n",
    "print(\"NLL DECOMPOSITION: LENGTH vs QUERY SEMANTICS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "nll = {}\n",
    "for cond in COND_NAMES:\n",
    "    nll[cond] = np.array([r[f'nll_{cond}'] for r in results])\n",
    "\n",
    "print(f\"\\n  {'Condition':<30} {'Mean NLL':>10} {'d vs bare_*':>12} {'sig':>5}\")\n",
    "print(f\"  {'-'*62}\")\n",
    "\n",
    "# Bare encoder conditions\n",
    "print(f\"  {'bare_nq':<30} {nll['bare_nq'].mean():>10.4f} {'â€”':>12} {'â€”':>5}\")\n",
    "\n",
    "diff_rq = nll['bare_nq'] - nll['bare_random_q']\n",
    "d_rq = cohens_d(diff_rq)\n",
    "_, p_rq = stats.ttest_1samp(diff_rq, 0)\n",
    "sig_rq = '***' if p_rq < 0.001 else '**' if p_rq < 0.01 else '*' if p_rq < 0.05 else 'ns'\n",
    "print(f\"  {'bare_random_q':<30} {nll['bare_random_q'].mean():>10.4f} {d_rq:>+12.3f} {sig_rq:>5}\")\n",
    "\n",
    "diff_q = nll['bare_nq'] - nll['bare_q']\n",
    "d_q = cohens_d(diff_q)\n",
    "_, p_q = stats.ttest_1samp(diff_q, 0)\n",
    "sig_q = '***' if p_q < 0.001 else '**' if p_q < 0.01 else '*' if p_q < 0.05 else 'ns'\n",
    "print(f\"  {'bare_q':<30} {nll['bare_q'].mean():>10.4f} {d_q:>+12.3f} {sig_q:>5}\")\n",
    "\n",
    "# Oracle encoder conditions\n",
    "print(f\"\\n  {'oracle_trunc_nq':<30} {nll['oracle_trunc_nq'].mean():>10.4f} {'â€”':>12} {'â€”':>5}\")\n",
    "\n",
    "diff_o_rq = nll['oracle_trunc_nq'] - nll['oracle_trunc_random_q']\n",
    "d_o_rq = cohens_d(diff_o_rq)\n",
    "_, p_o_rq = stats.ttest_1samp(diff_o_rq, 0)\n",
    "sig_o_rq = '***' if p_o_rq < 0.001 else '**' if p_o_rq < 0.01 else '*' if p_o_rq < 0.05 else 'ns'\n",
    "print(f\"  {'oracle_trunc_random_q':<30} {nll['oracle_trunc_random_q'].mean():>10.4f} {d_o_rq:>+12.3f} {sig_o_rq:>5}\")\n",
    "\n",
    "diff_o_q = nll['oracle_trunc_nq'] - nll['oracle_trunc_q']\n",
    "d_o_q = cohens_d(diff_o_q)\n",
    "_, p_o_q = stats.ttest_1samp(diff_o_q, 0)\n",
    "sig_o_q = '***' if p_o_q < 0.001 else '**' if p_o_q < 0.01 else '*' if p_o_q < 0.05 else 'ns'\n",
    "print(f\"  {'oracle_trunc_q':<30} {nll['oracle_trunc_q'].mean():>10.4f} {d_o_q:>+12.3f} {sig_o_q:>5}\")\n",
    "\n",
    "# Decomposition\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"DECOMPOSITION OF nqâ†’q IMPROVEMENT\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "# Bare encoder\n",
    "total_bare = d_q\n",
    "length_bare = d_rq\n",
    "semantic_bare_diff = nll['bare_random_q'] - nll['bare_q']\n",
    "d_semantic_bare = cohens_d(semantic_bare_diff)\n",
    "_, p_sem_bare = stats.ttest_1samp(semantic_bare_diff, 0)\n",
    "sig_sem_bare = '***' if p_sem_bare < 0.001 else '**' if p_sem_bare < 0.01 else '*' if p_sem_bare < 0.05 else 'ns'\n",
    "\n",
    "print(f\"\\n  Bare encoder:\")\n",
    "print(f\"    Total (nq â†’ q):        d={total_bare:+.3f}\")\n",
    "print(f\"    Length (nq â†’ random_q): d={length_bare:+.3f}\")\n",
    "print(f\"    Semantic (random â†’ q):  d={d_semantic_bare:+.3f} ({sig_sem_bare})\")\n",
    "if total_bare > 0:\n",
    "    print(f\"    Length fraction:    {length_bare / total_bare * 100:.0f}%\")\n",
    "    print(f\"    Semantic fraction: {d_semantic_bare / total_bare * 100:.0f}%\")\n",
    "\n",
    "# Oracle encoder\n",
    "total_oracle = d_o_q\n",
    "length_oracle = d_o_rq\n",
    "semantic_oracle_diff = nll['oracle_trunc_random_q'] - nll['oracle_trunc_q']\n",
    "d_semantic_oracle = cohens_d(semantic_oracle_diff)\n",
    "_, p_sem_oracle = stats.ttest_1samp(semantic_oracle_diff, 0)\n",
    "sig_sem_oracle = '***' if p_sem_oracle < 0.001 else '**' if p_sem_oracle < 0.01 else '*' if p_sem_oracle < 0.05 else 'ns'\n",
    "\n",
    "print(f\"\\n  Oracle encoder:\")\n",
    "print(f\"    Total (nq â†’ q):        d={total_oracle:+.3f}\")\n",
    "print(f\"    Length (nq â†’ random_q): d={length_oracle:+.3f}\")\n",
    "print(f\"    Semantic (random â†’ q):  d={d_semantic_oracle:+.3f} ({sig_sem_oracle})\")\n",
    "if total_oracle > 0:\n",
    "    print(f\"    Length fraction:    {length_oracle / total_oracle * 100:.0f}%\")\n",
    "    print(f\"    Semantic fraction: {d_semantic_oracle / total_oracle * 100:.0f}%\")\n",
    "\n",
    "# Does random prefix HURT? (random tokens might confuse the decoder)\n",
    "print(f\"\\n  Does random prefix hurt answer prediction?\")\n",
    "print(f\"    bare: nq â†’ random_q NLL change: {(nll['bare_random_q'] - nll['bare_nq']).mean():+.4f}\")\n",
    "print(f\"    oracle: nq â†’ random_q NLL change: {(nll['oracle_trunc_random_q'] - nll['oracle_trunc_nq']).mean():+.4f}\")\n",
    "print(f\"    (Positive = random hurts, Negative = random helps)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6944ef4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-21T00:28:25.924224Z",
     "iopub.status.busy": "2026-02-21T00:28:25.923961Z",
     "iopub.status.idle": "2026-02-21T00:28:25.955186Z",
     "shell.execute_reply": "2026-02-21T00:28:25.954484Z"
    },
    "papermill": {
     "duration": 0.037694,
     "end_time": "2026-02-21T00:28:25.956659",
     "exception": false,
     "start_time": "2026-02-21T00:28:25.918965",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ATTENTION BUDGET: LENGTH ARTIFACT TEST\n",
      "======================================================================\n",
      "Layer 33 â€” mean over 500 samples\n",
      "\n",
      "  Condition                           BOS   Prefix   Answer     Self    Cross    Check\n",
      "  ----------------------------------------------------------------------------------\n",
      "  bare_nq                          0.1305   0.0000   0.4996   0.6301   0.3699   1.0000\n",
      "  bare_random_q                    0.0952   0.1430   0.4467   0.6849   0.3151   1.0000\n",
      "  bare_q                           0.1086   0.0546   0.4786   0.6417   0.3583   1.0000\n",
      "  oracle_trunc_nq                  0.2123   0.0000   0.6589   0.8712   0.1289   1.0000\n",
      "  oracle_trunc_random_q            0.1525   0.1480   0.5974   0.8979   0.1021   1.0000\n",
      "  oracle_trunc_q                   0.1731   0.0610   0.6591   0.8932   0.1068   1.0000\n",
      "\n",
      "--- Prefix attention absorption (query vs random, layer 33) ---\n",
      "  Bare encoder:\n",
      "    Random prefix absorbs: 14.3%\n",
      "    Query prefix absorbs:  5.5%\n",
      "    Difference: -8.8pp (d=-1.144, ***)\n",
      "\n",
      "  Oracle encoder:\n",
      "    Random prefix absorbs: 14.8%\n",
      "    Query prefix absorbs:  6.1%\n",
      "    Difference: -8.7pp (d=-1.093, ***)\n",
      "\n",
      "--- Where does random prefix steal from? (bare, layer 33) ---\n",
      "  BOS change:    -0.0353\n",
      "  Answer change: -0.0529\n",
      "  Cross change:  -0.0548\n",
      "  (Compare to query: BOS -0.0219, Ans -0.0210, Cross -0.0116)\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Attention budget â€” is the 5.5% query buffer a length artifact?\n",
    "print(\"=\" * 70)\n",
    "print(\"ATTENTION BUDGET: LENGTH ARTIFACT TEST\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "last_layer = PROBE_LAYERS[-1]\n",
    "print(f\"Layer {last_layer} â€” mean over {len(results)} samples\")\n",
    "\n",
    "print(f\"\\n  {'Condition':<30} {'BOS':>8} {'Prefix':>8} {'Answer':>8} \"\n",
    "      f\"{'Self':>8} {'Cross':>8} {'Check':>8}\")\n",
    "print(f\"  {'-'*82}\")\n",
    "\n",
    "for cond in COND_NAMES:\n",
    "    sb = np.mean([r[f'probes_{cond}'][last_layer]['sb'] for r in results])\n",
    "    sp = np.mean([r[f'probes_{cond}'][last_layer]['sp'] for r in results])\n",
    "    sa = np.mean([r[f'probes_{cond}'][last_layer]['sa'] for r in results])\n",
    "    st = np.mean([r[f'probes_{cond}'][last_layer]['st'] for r in results])\n",
    "    ct = np.mean([r[f'probes_{cond}'][last_layer]['ct'] for r in results])\n",
    "    check = st + ct\n",
    "    print(f\"  {cond:<30} {sb:>8.4f} {sp:>8.4f} {sa:>8.4f} \"\n",
    "          f\"{st:>8.4f} {ct:>8.4f} {check:>8.4f}\")\n",
    "\n",
    "# Key comparison: random prefix vs query prefix attention absorption\n",
    "sp_random_bare = np.array([r['probes_bare_random_q'][last_layer]['sp'] for r in results])\n",
    "sp_query_bare = np.array([r['probes_bare_q'][last_layer]['sp'] for r in results])\n",
    "sp_random_orc = np.array([r['probes_oracle_trunc_random_q'][last_layer]['sp'] for r in results])\n",
    "sp_query_orc = np.array([r['probes_oracle_trunc_q'][last_layer]['sp'] for r in results])\n",
    "\n",
    "print(f\"\\n--- Prefix attention absorption (query vs random, layer {last_layer}) ---\")\n",
    "print(f\"  Bare encoder:\")\n",
    "print(f\"    Random prefix absorbs: {sp_random_bare.mean()*100:.1f}%\")\n",
    "print(f\"    Query prefix absorbs:  {sp_query_bare.mean()*100:.1f}%\")\n",
    "diff_sp_bare = sp_query_bare - sp_random_bare\n",
    "d_sp_bare = cohens_d(diff_sp_bare)\n",
    "_, p_sp_bare = stats.ttest_1samp(diff_sp_bare, 0)\n",
    "sig_sp = '***' if p_sp_bare < 0.001 else '**' if p_sp_bare < 0.01 else '*' if p_sp_bare < 0.05 else 'ns'\n",
    "print(f\"    Difference: {diff_sp_bare.mean()*100:+.1f}pp (d={d_sp_bare:+.3f}, {sig_sp})\")\n",
    "\n",
    "print(f\"\\n  Oracle encoder:\")\n",
    "print(f\"    Random prefix absorbs: {sp_random_orc.mean()*100:.1f}%\")\n",
    "print(f\"    Query prefix absorbs:  {sp_query_orc.mean()*100:.1f}%\")\n",
    "diff_sp_orc = sp_query_orc - sp_random_orc\n",
    "d_sp_orc = cohens_d(diff_sp_orc)\n",
    "_, p_sp_orc = stats.ttest_1samp(diff_sp_orc, 0)\n",
    "sig_sp_o = '***' if p_sp_orc < 0.001 else '**' if p_sp_orc < 0.01 else '*' if p_sp_orc < 0.05 else 'ns'\n",
    "print(f\"    Difference: {diff_sp_orc.mean()*100:+.1f}pp (d={d_sp_orc:+.3f}, {sig_sp_o})\")\n",
    "\n",
    "# Where does the prefix steal from? (nq vs random_q, to isolate pure length)\n",
    "print(f\"\\n--- Where does random prefix steal from? (bare, layer {last_layer}) ---\")\n",
    "sb_nq = np.mean([r['probes_bare_nq'][last_layer]['sb'] for r in results])\n",
    "sb_rq = np.mean([r['probes_bare_random_q'][last_layer]['sb'] for r in results])\n",
    "sa_nq = np.mean([r['probes_bare_nq'][last_layer]['sa'] for r in results])\n",
    "sa_rq = np.mean([r['probes_bare_random_q'][last_layer]['sa'] for r in results])\n",
    "ct_nq = np.mean([r['probes_bare_nq'][last_layer]['ct'] for r in results])\n",
    "ct_rq = np.mean([r['probes_bare_random_q'][last_layer]['ct'] for r in results])\n",
    "\n",
    "print(f\"  BOS change:    {sb_rq - sb_nq:+.4f}\")\n",
    "print(f\"  Answer change: {sa_rq - sa_nq:+.4f}\")\n",
    "print(f\"  Cross change:  {ct_rq - ct_nq:+.4f}\")\n",
    "print(f\"  (Compare to query: BOS {np.mean([r['probes_bare_q'][last_layer]['sb'] for r in results]) - sb_nq:+.4f}, \"\n",
    "      f\"Ans {np.mean([r['probes_bare_q'][last_layer]['sa'] for r in results]) - sa_nq:+.4f}, \"\n",
    "      f\"Cross {np.mean([r['probes_bare_q'][last_layer]['ct'] for r in results]) - ct_nq:+.4f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45a2db71",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-21T00:28:25.966357Z",
     "iopub.status.busy": "2026-02-21T00:28:25.965698Z",
     "iopub.status.idle": "2026-02-21T00:28:25.983147Z",
     "shell.execute_reply": "2026-02-21T00:28:25.982346Z"
    },
    "papermill": {
     "duration": 0.023834,
     "end_time": "2026-02-21T00:28:25.984581",
     "exception": false,
     "start_time": "2026-02-21T00:28:25.960747",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ENTROPY ANALYSIS\n",
      "======================================================================\n",
      "\n",
      "--- Self-attention entropy (layer 33) ---\n",
      "  bare_nq                        1.0783\n",
      "  bare_random_q                  1.5214\n",
      "  bare_q                         1.2643\n",
      "  oracle_trunc_nq                1.1708\n",
      "  oracle_trunc_random_q          1.6418\n",
      "  oracle_trunc_q                 1.3850\n",
      "\n",
      "  Self-entropy change (bare):\n",
      "    nq â†’ random_q (length):   d=+1.951\n",
      "    random_q â†’ q (semantic):   d=-1.483\n",
      "    nq â†’ q (total):            d=+2.400\n",
      "\n",
      "--- Cross-attention entropy (layer 33) ---\n",
      "  bare_nq                        0.8509\n",
      "  bare_random_q                  0.7190\n",
      "  bare_q                         0.7376\n",
      "  oracle_trunc_nq                0.6356\n",
      "  oracle_trunc_random_q          0.5092\n",
      "  oracle_trunc_q                 0.5166\n",
      "\n",
      "  Cross-entropy change (bare):\n",
      "    nq â†’ random_q (length):   d=-1.034\n",
      "    random_q â†’ q (semantic):   d=+0.265\n",
      "    nq â†’ q (total):            d=-1.433\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Entropy analysis â€” does random prefix change attention patterns?\n",
    "print(\"=\" * 70)\n",
    "print(\"ENTROPY ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "last_layer = PROBE_LAYERS[-1]\n",
    "\n",
    "print(f\"\\n--- Self-attention entropy (layer {last_layer}) ---\")\n",
    "for cond in COND_NAMES:\n",
    "    se = np.mean([r[f'probes_{cond}'][last_layer]['se'] for r in results])\n",
    "    print(f\"  {cond:<30} {se:.4f}\")\n",
    "\n",
    "se_nq = np.array([r['probes_bare_nq'][last_layer]['se'] for r in results])\n",
    "se_rq = np.array([r['probes_bare_random_q'][last_layer]['se'] for r in results])\n",
    "se_q = np.array([r['probes_bare_q'][last_layer]['se'] for r in results])\n",
    "\n",
    "d_se_len = cohens_d(se_rq - se_nq)\n",
    "d_se_sem = cohens_d(se_q - se_rq)\n",
    "d_se_tot = cohens_d(se_q - se_nq)\n",
    "print(f\"\\n  Self-entropy change (bare):\")\n",
    "print(f\"    nq â†’ random_q (length):   d={d_se_len:+.3f}\")\n",
    "print(f\"    random_q â†’ q (semantic):   d={d_se_sem:+.3f}\")\n",
    "print(f\"    nq â†’ q (total):            d={d_se_tot:+.3f}\")\n",
    "\n",
    "print(f\"\\n--- Cross-attention entropy (layer {last_layer}) ---\")\n",
    "for cond in COND_NAMES:\n",
    "    ce = np.mean([r[f'probes_{cond}'][last_layer]['ce'] for r in results])\n",
    "    print(f\"  {cond:<30} {ce:.4f}\")\n",
    "\n",
    "ce_nq = np.array([r['probes_bare_nq'][last_layer]['ce'] for r in results])\n",
    "ce_rq = np.array([r['probes_bare_random_q'][last_layer]['ce'] for r in results])\n",
    "ce_q = np.array([r['probes_bare_q'][last_layer]['ce'] for r in results])\n",
    "\n",
    "d_ce_len = cohens_d(ce_rq - ce_nq)\n",
    "d_ce_sem = cohens_d(ce_q - ce_rq)\n",
    "d_ce_tot = cohens_d(ce_q - ce_nq)\n",
    "print(f\"\\n  Cross-entropy change (bare):\")\n",
    "print(f\"    nq â†’ random_q (length):   d={d_ce_len:+.3f}\")\n",
    "print(f\"    random_q â†’ q (semantic):   d={d_ce_sem:+.3f}\")\n",
    "print(f\"    nq â†’ q (total):            d={d_ce_tot:+.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6be0e981",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-21T00:28:25.994198Z",
     "iopub.status.busy": "2026-02-21T00:28:25.993705Z",
     "iopub.status.idle": "2026-02-21T00:28:26.009040Z",
     "shell.execute_reply": "2026-02-21T00:28:26.008407Z"
    },
    "papermill": {
     "duration": 0.021731,
     "end_time": "2026-02-21T00:28:26.010459",
     "exception": false,
     "start_time": "2026-02-21T00:28:25.988728",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ENCODER PREFIX EFFECT â€” WITH LENGTH CONTROL\n",
      "======================================================================\n",
      "\n",
      "--- Encoder prefix effect across decoder conditions ---\n",
      "  With nq decoder:        d=+0.366 (p=2.24e-15)\n",
      "  With random_q decoder:  d=+0.223 (p=8.40e-07)\n",
      "  With query decoder:     d=+0.238 (p=1.51e-07)\n",
      "\n",
      "--- Interaction decomposition ---\n",
      "  Total interaction (nq vs q):        d=+0.316 (***)\n",
      "  Length interaction (nq vs random_q): d=+0.324 (***)\n",
      "  Semantic interaction (random vs q):  d=-0.097 (*)\n",
      "\n",
      "  Of the total encoder prefix reduction:\n",
      "    Due to decoder LENGTH:    102%\n",
      "    Due to query SEMANTICS:   -31%\n",
      "\n",
      "--- Exp 07 replication ---\n",
      "  Exp 07: encoder prefix d_nq=+0.366, d_q=+0.238, interaction=+0.316\n",
      "  Exp 08: encoder prefix d_nq=+0.366, d_q=+0.238, interaction=+0.316\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Does the encoder prefix interaction survive length control?\n",
    "print(\"=\" * 70)\n",
    "print(\"ENCODER PREFIX EFFECT â€” WITH LENGTH CONTROL\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# The key question: In Exp 07, the 35% redundancy was between encoder prefix\n",
    "# and decoder query. But how much of that is just decoder LENGTH?\n",
    "\n",
    "print(f\"\\n--- Encoder prefix effect across decoder conditions ---\")\n",
    "enc_nq = nll['bare_nq'] - nll['oracle_trunc_nq']\n",
    "enc_rq = nll['bare_random_q'] - nll['oracle_trunc_random_q']\n",
    "enc_q = nll['bare_q'] - nll['oracle_trunc_q']\n",
    "\n",
    "d_enc_nq = cohens_d(enc_nq)\n",
    "d_enc_rq = cohens_d(enc_rq)\n",
    "d_enc_q = cohens_d(enc_q)\n",
    "\n",
    "_, p_enc_nq = stats.ttest_1samp(enc_nq, 0)\n",
    "_, p_enc_rq = stats.ttest_1samp(enc_rq, 0)\n",
    "_, p_enc_q = stats.ttest_1samp(enc_q, 0)\n",
    "\n",
    "print(f\"  With nq decoder:        d={d_enc_nq:+.3f} (p={p_enc_nq:.2e})\")\n",
    "print(f\"  With random_q decoder:  d={d_enc_rq:+.3f} (p={p_enc_rq:.2e})\")\n",
    "print(f\"  With query decoder:     d={d_enc_q:+.3f} (p={p_enc_q:.2e})\")\n",
    "\n",
    "# Interactions\n",
    "int_length = enc_nq - enc_rq  # nq vs random_q: pure length interaction\n",
    "int_semantic = enc_rq - enc_q  # random_q vs q: semantic interaction\n",
    "int_total = enc_nq - enc_q    # total (same as Exp 07)\n",
    "\n",
    "d_int_length = cohens_d(int_length)\n",
    "d_int_semantic = cohens_d(int_semantic)\n",
    "d_int_total = cohens_d(int_total)\n",
    "\n",
    "_, p_il = stats.ttest_1samp(int_length, 0)\n",
    "_, p_is = stats.ttest_1samp(int_semantic, 0)\n",
    "_, p_it = stats.ttest_1samp(int_total, 0)\n",
    "\n",
    "sig_il = '***' if p_il < 0.001 else '**' if p_il < 0.01 else '*' if p_il < 0.05 else 'ns'\n",
    "sig_is = '***' if p_is < 0.001 else '**' if p_is < 0.01 else '*' if p_is < 0.05 else 'ns'\n",
    "sig_it = '***' if p_it < 0.001 else '**' if p_it < 0.01 else '*' if p_it < 0.05 else 'ns'\n",
    "\n",
    "print(f\"\\n--- Interaction decomposition ---\")\n",
    "print(f\"  Total interaction (nq vs q):        d={d_int_total:+.3f} ({sig_it})\")\n",
    "print(f\"  Length interaction (nq vs random_q): d={d_int_length:+.3f} ({sig_il})\")\n",
    "print(f\"  Semantic interaction (random vs q):  d={d_int_semantic:+.3f} ({sig_is})\")\n",
    "\n",
    "if d_int_total != 0:\n",
    "    print(f\"\\n  Of the total encoder prefix reduction:\")\n",
    "    print(f\"    Due to decoder LENGTH:    {d_int_length / d_int_total * 100:.0f}%\")\n",
    "    print(f\"    Due to query SEMANTICS:   {d_int_semantic / d_int_total * 100:.0f}%\")\n",
    "\n",
    "# Exp 07 replication check\n",
    "print(f\"\\n--- Exp 07 replication ---\")\n",
    "print(f\"  Exp 07: encoder prefix d_nq={0.366:+.3f}, d_q={0.238:+.3f}, interaction={0.316:+.3f}\")\n",
    "print(f\"  Exp 08: encoder prefix d_nq={d_enc_nq:+.3f}, d_q={d_enc_q:+.3f}, interaction={d_int_total:+.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5bf28a7b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-21T00:28:26.020362Z",
     "iopub.status.busy": "2026-02-21T00:28:26.020062Z",
     "iopub.status.idle": "2026-02-21T00:28:27.577003Z",
     "shell.execute_reply": "2026-02-21T00:28:27.576012Z"
    },
    "papermill": {
     "duration": 1.564086,
     "end_time": "2026-02-21T00:28:27.578731",
     "exception": false,
     "start_time": "2026-02-21T00:28:26.014645",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "SUMMARY â€” Exp 08: Decoder Length Control\n",
      "======================================================================\n",
      "\n",
      "Model: google/t5gemma-2-4b-4b\n",
      "N: 500\n",
      "\n",
      "--- NLL Decomposition (nq â†’ q) ---\n",
      "  Bare encoder:\n",
      "    Total:    d=+0.309\n",
      "    Length:   d=+0.170 (55%)\n",
      "    Semantic: d=+0.317 (103%)\n",
      "  Oracle encoder:\n",
      "    Total:    d=+0.228\n",
      "    Length:   d=-0.014 (-6%)\n",
      "    Semantic: d=+0.324 (142%)\n",
      "\n",
      "--- Attention (layer 33) ---\n",
      "  Random prefix absorbs: 14.3% of answer attention\n",
      "  Query prefix absorbs:  5.5% of answer attention\n",
      "  Length accounts for:   262% of prefix buffer\n",
      "\n",
      "--- Encoder prefix Ã— decoder interaction ---\n",
      "  Total redundancy (Exp 07 replication): 35%\n",
      "  Due to length: 102%\n",
      "  Due to semantics: -31%\n",
      "\n",
      "--- Verdict ---\n",
      "  Both effects contribute: length d=+0.170, semantic d=+0.317\n",
      "  Query provides genuine semantic value beyond mere length.\n",
      "\n",
      "Results saved to ../../../results/exp08/results.json\n",
      "\n",
      "Cleaning up GPU memory...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory: 17.47 GB -> 0.01 GB\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Cell 10: Summary and save\n",
    "print(\"=\" * 70)\n",
    "print(\"SUMMARY â€” Exp 08: Decoder Length Control\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "last_layer = PROBE_LAYERS[-1]\n",
    "\n",
    "# NLL decomposition\n",
    "total_bare = cohens_d(nll['bare_nq'] - nll['bare_q'])\n",
    "length_bare = cohens_d(nll['bare_nq'] - nll['bare_random_q'])\n",
    "semantic_bare = cohens_d(nll['bare_random_q'] - nll['bare_q'])\n",
    "\n",
    "total_oracle = cohens_d(nll['oracle_trunc_nq'] - nll['oracle_trunc_q'])\n",
    "length_oracle = cohens_d(nll['oracle_trunc_nq'] - nll['oracle_trunc_random_q'])\n",
    "semantic_oracle = cohens_d(nll['oracle_trunc_random_q'] - nll['oracle_trunc_q'])\n",
    "\n",
    "print(f\"\\nModel: {MODEL_NAME}\")\n",
    "print(f\"N: {len(results)}\")\n",
    "\n",
    "print(f\"\\n--- NLL Decomposition (nq â†’ q) ---\")\n",
    "print(f\"  Bare encoder:\")\n",
    "print(f\"    Total:    d={total_bare:+.3f}\")\n",
    "print(f\"    Length:   d={length_bare:+.3f} ({length_bare/total_bare*100:.0f}%)\" if total_bare else \"\")\n",
    "print(f\"    Semantic: d={semantic_bare:+.3f} ({semantic_bare/total_bare*100:.0f}%)\" if total_bare else \"\")\n",
    "print(f\"  Oracle encoder:\")\n",
    "print(f\"    Total:    d={total_oracle:+.3f}\")\n",
    "print(f\"    Length:   d={length_oracle:+.3f} ({length_oracle/total_oracle*100:.0f}%)\" if total_oracle else \"\")\n",
    "print(f\"    Semantic: d={semantic_oracle:+.3f} ({semantic_oracle/total_oracle*100:.0f}%)\" if total_oracle else \"\")\n",
    "\n",
    "# Attention\n",
    "sp_random = np.mean([r['probes_bare_random_q'][last_layer]['sp'] for r in results])\n",
    "sp_query = np.mean([r['probes_bare_q'][last_layer]['sp'] for r in results])\n",
    "\n",
    "print(f\"\\n--- Attention (layer {last_layer}) ---\")\n",
    "print(f\"  Random prefix absorbs: {sp_random*100:.1f}% of answer attention\")\n",
    "print(f\"  Query prefix absorbs:  {sp_query*100:.1f}% of answer attention\")\n",
    "if sp_query > 0:\n",
    "    print(f\"  Length accounts for:   {sp_random/sp_query*100:.0f}% of prefix buffer\")\n",
    "\n",
    "# Interaction decomposition\n",
    "enc_nq = nll['bare_nq'] - nll['oracle_trunc_nq']\n",
    "enc_rq = nll['bare_random_q'] - nll['oracle_trunc_random_q']\n",
    "enc_q = nll['bare_q'] - nll['oracle_trunc_q']\n",
    "d_enc_nq = cohens_d(enc_nq)\n",
    "d_enc_q = cohens_d(enc_q)\n",
    "int_total = enc_nq - enc_q\n",
    "int_length = enc_nq - enc_rq\n",
    "int_semantic = enc_rq - enc_q\n",
    "d_int_total = cohens_d(int_total)\n",
    "d_int_length = cohens_d(int_length)\n",
    "d_int_semantic = cohens_d(int_semantic)\n",
    "\n",
    "print(f\"\\n--- Encoder prefix Ã— decoder interaction ---\")\n",
    "print(f\"  Total redundancy (Exp 07 replication): {(1-d_enc_q/d_enc_nq)*100:.0f}%\")\n",
    "if d_int_total != 0:\n",
    "    print(f\"  Due to length: {d_int_length/d_int_total*100:.0f}%\")\n",
    "    print(f\"  Due to semantics: {d_int_semantic/d_int_total*100:.0f}%\")\n",
    "\n",
    "# Verdict\n",
    "print(f\"\\n--- Verdict ---\")\n",
    "if abs(length_bare) < 0.05:\n",
    "    print(f\"  Length effect is NEGLIGIBLE (d={length_bare:+.3f})\")\n",
    "    print(f\"  The nqâ†’q improvement is almost entirely query-semantic.\")\n",
    "elif abs(length_bare) > abs(semantic_bare):\n",
    "    print(f\"  Length effect DOMINATES (d={length_bare:+.3f} vs semantic d={semantic_bare:+.3f})\")\n",
    "    print(f\"  The Exp 07 'query buffer' finding is largely a length artifact.\")\n",
    "else:\n",
    "    print(f\"  Both effects contribute: length d={length_bare:+.3f}, semantic d={semantic_bare:+.3f}\")\n",
    "    print(f\"  Query provides genuine semantic value beyond mere length.\")\n",
    "\n",
    "# Save results\n",
    "final_results = {\n",
    "    'experiment': 'v4_exp08_decoder_length_control',\n",
    "    'model': MODEL_NAME,\n",
    "    'dataset': 'ms_marco_v1.1',\n",
    "    'n_samples': len(results),\n",
    "    'seed': SEED,\n",
    "    'probe_layers': PROBE_LAYERS,\n",
    "    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'nll': {cond: float(nll[cond].mean()) for cond in COND_NAMES},\n",
    "    'nll_decomposition': {\n",
    "        'bare': {\n",
    "            'total': float(total_bare),\n",
    "            'length': float(length_bare),\n",
    "            'semantic': float(semantic_bare),\n",
    "        },\n",
    "        'oracle': {\n",
    "            'total': float(total_oracle),\n",
    "            'length': float(length_oracle),\n",
    "            'semantic': float(semantic_oracle),\n",
    "        },\n",
    "    },\n",
    "    'prefix_attention': {\n",
    "        'random_bare': float(sp_random),\n",
    "        'query_bare': float(sp_query),\n",
    "    },\n",
    "    'interaction_decomposition': {\n",
    "        'd_int_total': float(d_int_total),\n",
    "        'd_int_length': float(d_int_length),\n",
    "        'd_int_semantic': float(d_int_semantic),\n",
    "    },\n",
    "}\n",
    "\n",
    "with open(RESULTS_DIR / 'results.json', 'w') as f:\n",
    "    json.dump(final_results, f, indent=2)\n",
    "print(f\"\\nResults saved to {RESULTS_DIR / 'results.json'}\")\n",
    "\n",
    "# Cleanup\n",
    "print(f\"\\nCleaning up GPU memory...\")\n",
    "mem_before = torch.cuda.memory_allocated() / 1e9\n",
    "del model, processor, tokenizer\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "mem_after = torch.cuda.memory_allocated() / 1e9\n",
    "print(f\"GPU memory: {mem_before:.2f} GB -> {mem_after:.2f} GB\")\n",
    "print(\"Done!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 7399.815699,
   "end_time": "2026-02-21T00:28:30.621102",
   "environment_variables": {},
   "exception": null,
   "input_path": "08_decoder_length_control.ipynb",
   "output_path": "08_decoder_length_control_executed.ipynb",
   "parameters": {},
   "start_time": "2026-02-20T22:25:10.805403",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "140a345be54147b2883873702a05fa0b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "260f7f915b8445cea77bcc218c608c3f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2fc92bf58e384d12b59fff52f4b6f0f0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "3497df57887e417abd4b826baee0122a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "38737ed0430f49609d474f0ab50d177e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "62098e306df8469e88a9e357999117a5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_6de25b1c548b4068aa01ea81b9c7105f",
        "IPY_MODEL_8adaeb3ed895462ca55b6509e423a65e",
        "IPY_MODEL_eec071a3f305481083a68d319afa7707"
       ],
       "layout": "IPY_MODEL_260f7f915b8445cea77bcc218c608c3f",
       "tabbable": null,
       "tooltip": null
      }
     },
     "6de25b1c548b4068aa01ea81b9c7105f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_38737ed0430f49609d474f0ab50d177e",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_2fc92bf58e384d12b59fff52f4b6f0f0",
       "tabbable": null,
       "tooltip": null,
       "value": "Loadingâ€‡weights:â€‡100%"
      }
     },
     "6ebf83d1ef1b48a8842e8236fecf8a11": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_706203e8cee44693851a06cdb06b1cc8",
        "IPY_MODEL_81029ae56e3a47ffa4f340cf6fc54009",
        "IPY_MODEL_9bec8bc819ad4b9f9b19d72f6c97f88c"
       ],
       "layout": "IPY_MODEL_940512ea85db4656940bcd9c79141074",
       "tabbable": null,
       "tooltip": null
      }
     },
     "706203e8cee44693851a06cdb06b1cc8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_140a345be54147b2883873702a05fa0b",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_c45dd239fc684425b1f17f7c58913147",
       "tabbable": null,
       "tooltip": null,
       "value": "Probing:â€‡100%"
      }
     },
     "7c452d191de746039a3d063abfa746b5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "81029ae56e3a47ffa4f340cf6fc54009": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_cd103d974113420c8149cdac303acb01",
       "max": 500.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_3497df57887e417abd4b826baee0122a",
       "tabbable": null,
       "tooltip": null,
       "value": 500.0
      }
     },
     "8adaeb3ed895462ca55b6509e423a65e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_f2bc85fe621145fb85eea8c4db928c3b",
       "max": 1327.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_c0d52b8651f94f33a54a7a34fde05c87",
       "tabbable": null,
       "tooltip": null,
       "value": 1327.0
      }
     },
     "940512ea85db4656940bcd9c79141074": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "9bec8bc819ad4b9f9b19d72f6c97f88c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_e54728141b68445281f50d4382bbc635",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_bf63801d87964fd288c1036b44dc4e65",
       "tabbable": null,
       "tooltip": null,
       "value": "â€‡500/500â€‡[2:02:48&lt;00:00,â€‡119.98s/it]"
      }
     },
     "bf63801d87964fd288c1036b44dc4e65": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "c0d52b8651f94f33a54a7a34fde05c87": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "c45dd239fc684425b1f17f7c58913147": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "cd103d974113420c8149cdac303acb01": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e54728141b68445281f50d4382bbc635": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "eae9ca19d311479782b63544df973987": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "eec071a3f305481083a68d319afa7707": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_7c452d191de746039a3d063abfa746b5",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_eae9ca19d311479782b63544df973987",
       "tabbable": null,
       "tooltip": null,
       "value": "â€‡1327/1327â€‡[00:04&lt;00:00,â€‡643.87it/s,â€‡Materializingâ€‡param=model.encoder.vision_tower.vision_model.post_layernorm.weight]"
      }
     },
     "f2bc85fe621145fb85eea8c4db928c3b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}