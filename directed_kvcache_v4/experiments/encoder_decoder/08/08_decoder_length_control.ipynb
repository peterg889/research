{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e740caea",
   "metadata": {},
   "source": [
    "# Experiment 08: Decoder Length Control\n",
    "\n",
    "## Motivation\n",
    "\n",
    "Exp 07 found that decoder query tokens absorb 5.5% of answer-token attention and\n",
    "the nq\u2192q NLL improvement is d=+0.309. But the `_q` conditions have **longer decoder\n",
    "sequences** (`[BOS, query, answer]` vs `[BOS, answer]`). This means:\n",
    "\n",
    "1. The 5.5% \"query buffer\" could be a **length artifact** \u2014 any extra tokens would\n",
    "   absorb some attention mechanically\n",
    "2. The NLL improvement could partly be due to the decoder having more context positions\n",
    "   (longer causal window) rather than the query's semantic content\n",
    "\n",
    "## Design: 2\u00d73 Factorial\n",
    "\n",
    "Add `random_q` conditions where `decoder_input_ids = [BOS, random_tokens, answer]`\n",
    "with `len(random_tokens) == len(query_ids)` per sample. Random tokens are sampled\n",
    "uniformly from the vocabulary (avoiding special tokens).\n",
    "\n",
    "| # | Condition | Encoder input | Cross-attn mask | Decoder input |\n",
    "|---|-----------|--------------|-----------------|---------------|\n",
    "| 1 | bare_nq | [document] | all visible | [BOS, answer] |\n",
    "| 2 | bare_random_q | [document] | all visible | [BOS, random, answer] |\n",
    "| 3 | bare_q | [document] | all visible | [BOS, query, answer] |\n",
    "| 4 | oracle_trunc_nq | [query + doc] | doc only | [BOS, answer] |\n",
    "| 5 | oracle_trunc_random_q | [query + doc] | doc only | [BOS, random, answer] |\n",
    "| 6 | oracle_trunc_q | [query + doc] | doc only | [BOS, query, answer] |\n",
    "\n",
    "## Key comparisons\n",
    "\n",
    "**NLL decomposition of nq\u2192q improvement:**\n",
    "- `nq \u2192 random_q`: Pure length effect (no semantics)\n",
    "- `random_q \u2192 q`: Query-specific semantic effect (length-controlled)\n",
    "- If length accounts for most of the improvement, `random_q` \u2248 `q`\n",
    "\n",
    "**Attention decomposition:**\n",
    "- Does random prefix absorb similar attention as query (~5.5%)?\n",
    "- Does random prefix steal from the same budget (cross-attention)?\n",
    "- If yes, the \"query buffer\" is purely mechanical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc2b911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Setup and model loading (EAGER attention for weight extraction)\n",
    "import os\n",
    "os.umask(0o000)\n",
    "\n",
    "import sys, json, time, gc, re\n",
    "import random as pyrandom\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "from collections import Counter, defaultdict\n",
    "from scipy import stats\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "sys.path.insert(0, \"../../..\")\n",
    "from lib.analysis import cohens_d\n",
    "\n",
    "SEED = 42\n",
    "N_SAMPLES = 500\n",
    "MODEL_NAME = \"google/t5gemma-2-4b-4b\"\n",
    "\n",
    "RESULTS_DIR = Path(\"../../../results/exp08\")\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CHECKPOINT_PATH = RESULTS_DIR / \"checkpoint.json\"\n",
    "\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "pyrandom.seed(SEED)\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv())\n",
    "HF_TOKEN = os.environ.get(\"HF_TOKEN\")\n",
    "\n",
    "from transformers import AutoProcessor, AutoModelForSeq2SeqLM\n",
    "\n",
    "print(f\"Loading {MODEL_NAME} with attn_implementation='eager'...\")\n",
    "processor = AutoProcessor.from_pretrained(MODEL_NAME, token=HF_TOKEN)\n",
    "tokenizer = processor.tokenizer\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    MODEL_NAME, device_map=\"auto\", torch_dtype=torch.bfloat16, token=HF_TOKEN,\n",
    "    attn_implementation=\"eager\",\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "DEVICE = next(model.parameters()).device\n",
    "BOS_ID = getattr(model.config, 'decoder_start_token_id', None) or tokenizer.bos_token_id\n",
    "\n",
    "# Discover decoder layer count\n",
    "N_DEC_LAYERS = len(model.model.decoder.layers)\n",
    "# Probe only 3 key layers: first, middle, last (faster than 6)\n",
    "PROBE_LAYERS = [0, N_DEC_LAYERS // 2, N_DEC_LAYERS - 1]\n",
    "\n",
    "VOCAB_SIZE = len(tokenizer)\n",
    "\n",
    "print(f\"Exp 08: Decoder Length Control\")\n",
    "print(f\"N: {N_SAMPLES}, Model: {MODEL_NAME}\")\n",
    "print(f\"DEVICE: {DEVICE}, dtype: {next(model.parameters()).dtype}\")\n",
    "print(f\"GPU memory: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "print(f\"Decoder layers: {N_DEC_LAYERS}, Vocab size: {VOCAB_SIZE}\")\n",
    "print(f\"Probe layers: {PROBE_LAYERS}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86cef680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Load MS MARCO data + generate matched-length random tokens\n",
    "from lib.data import count_words\n",
    "from datasets import load_dataset\n",
    "\n",
    "print(\"Loading MS MARCO v1.1 validation...\")\n",
    "ds = load_dataset(\"microsoft/ms_marco\", \"v1.1\", split=\"validation\")\n",
    "\n",
    "all_candidates = []\n",
    "for item in ds:\n",
    "    if len(all_candidates) >= 3 * N_SAMPLES:\n",
    "        break\n",
    "    passages = item.get('passages', {})\n",
    "    ptexts = passages.get('passage_text', [])\n",
    "    is_sel = passages.get('is_selected', [])\n",
    "    query = item.get('query', '')\n",
    "    answers = item.get('answers', [])\n",
    "    well_formed = item.get('wellFormedAnswers', [])\n",
    "    answer = None\n",
    "    if well_formed and len(well_formed) > 0 and well_formed[0] not in ('[]', ''):\n",
    "        answer = well_formed[0]\n",
    "    elif answers and len(answers) > 0 and answers[0] != 'No Answer Present.':\n",
    "        answer = answers[0]\n",
    "    if not answer:\n",
    "        continue\n",
    "    for pt, sel in zip(ptexts, is_sel):\n",
    "        wc = count_words(pt)\n",
    "        if sel == 1 and 30 <= wc <= 300:\n",
    "            all_candidates.append({\n",
    "                'passage': pt, 'query': query, 'answer': answer,\n",
    "                'word_count': wc,\n",
    "            })\n",
    "            break\n",
    "\n",
    "print(f\"Total candidates: {len(all_candidates)}\")\n",
    "np.random.seed(SEED)\n",
    "indices = np.random.permutation(len(all_candidates))\n",
    "samples = [all_candidates[i] for i in indices[:N_SAMPLES]]\n",
    "del ds, all_candidates\n",
    "gc.collect()\n",
    "\n",
    "# Count prefix tokens for oracle conditions\n",
    "def count_prefix_tokens(prefix_text, document_text):\n",
    "    full_text = prefix_text + \"\\n\" + document_text\n",
    "    full_ids = tokenizer(full_text, add_special_tokens=True, truncation=True,\n",
    "                         max_length=2048).input_ids\n",
    "    doc_ids = tokenizer(document_text, add_special_tokens=True, truncation=True,\n",
    "                        max_length=2048).input_ids\n",
    "    return len(full_ids) - len(doc_ids)\n",
    "\n",
    "# Pre-tokenize queries and generate matched-length random tokens\n",
    "for i, s in enumerate(samples):\n",
    "    s['n_pfx_oracle'] = count_prefix_tokens(s['query'], s['passage'])\n",
    "    q_ids = tokenizer(s['query'], add_special_tokens=False, truncation=True,\n",
    "                      max_length=512).input_ids\n",
    "    s['query_ids'] = q_ids\n",
    "    # Generate random token IDs of the same length as the query\n",
    "    # Avoid special tokens (IDs 0-99) and the very end of vocab\n",
    "    rng = np.random.RandomState(SEED + i + 10000)\n",
    "    s['random_ids'] = rng.randint(100, VOCAB_SIZE - 100, size=len(q_ids)).tolist()\n",
    "\n",
    "print(f\"Loaded {len(samples)} samples\")\n",
    "print(f\"Mean passage words: {np.mean([s['word_count'] for s in samples]):.0f}\")\n",
    "print(f\"Mean query words: {np.mean([count_words(s['query']) for s in samples]):.0f}\")\n",
    "print(f\"Mean answer words: {np.mean([count_words(s['answer']) for s in samples]):.0f}\")\n",
    "print(f\"Mean query tokens: {np.mean([len(s['query_ids']) for s in samples]):.1f}\")\n",
    "print(f\"Mean oracle prefix tokens: {np.mean([s['n_pfx_oracle'] for s in samples]):.1f}\")\n",
    "\n",
    "# Show example random vs query tokens\n",
    "s0 = samples[0]\n",
    "print(f\"\\nExample (sample 0):\")\n",
    "print(f\"  Query: {s0['query'][:60]}...\")\n",
    "print(f\"  Query tokens ({len(s0['query_ids'])}): {s0['query_ids'][:10]}...\")\n",
    "print(f\"  Random tokens ({len(s0['random_ids'])}): {s0['random_ids'][:10]}...\")\n",
    "print(f\"  Random decoded: {tokenizer.decode(s0['random_ids'][:10])}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ac635f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Define probing function (same as Exp 07 but with fewer layers)\n",
    "\n",
    "def forward_with_probes(encoder_outputs, cross_attn_mask, decoder_input_ids,\n",
    "                        answer_start, answer_len, answer_ids_list):\n",
    "    # Forward pass with attention extraction.\n",
    "    # Returns (nll, probes_dict) where probes_dict is keyed by layer index.\n",
    "    dec_len = decoder_input_ids.shape[1]\n",
    "    n_prefix = answer_start - 1  # 0 for _nq, len(prefix_ids) for _q/_random_q\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            encoder_outputs=encoder_outputs,\n",
    "            attention_mask=cross_attn_mask,\n",
    "            decoder_input_ids=decoder_input_ids,\n",
    "            output_attentions=True,\n",
    "        )\n",
    "\n",
    "    # --- NLL ---\n",
    "    logits = outputs.logits\n",
    "    answer_logits = logits[0, n_prefix:n_prefix + answer_len, :]\n",
    "    targets = torch.tensor(answer_ids_list, dtype=torch.long, device=DEVICE)\n",
    "    log_probs = F.log_softmax(answer_logits, dim=-1)\n",
    "    token_log_probs = log_probs.gather(1, targets.unsqueeze(1)).squeeze(1)\n",
    "    nll = -token_log_probs.mean().item()\n",
    "\n",
    "    # --- Probes ---\n",
    "    probes = {}\n",
    "    for layer_idx in PROBE_LAYERS:\n",
    "        # Self-attention: [1, heads, dec_len, dec_len]\n",
    "        sa = outputs.decoder_attentions[layer_idx][0].float().mean(dim=0)\n",
    "        # Cross-attention: [1, heads, dec_len, enc_len]\n",
    "        ca = outputs.cross_attentions[layer_idx][0].float().mean(dim=0)\n",
    "\n",
    "        # Extract answer-token rows\n",
    "        ans_sa = sa[answer_start:answer_start + answer_len]  # [M, dec_len]\n",
    "        ans_ca = ca[answer_start:answer_start + answer_len]  # [M, enc_len]\n",
    "\n",
    "        # Self-attention budget decomposition for answer tokens\n",
    "        self_to_bos = ans_sa[:, 0].mean().item()\n",
    "\n",
    "        if n_prefix > 0:\n",
    "            self_to_prefix = ans_sa[:, 1:1 + n_prefix].sum(dim=1).mean().item()\n",
    "        else:\n",
    "            self_to_prefix = 0.0\n",
    "\n",
    "        # Self-attention to answer positions (causal)\n",
    "        answer_mask = torch.zeros(answer_len, dec_len, device=DEVICE)\n",
    "        for t in range(answer_len):\n",
    "            p = answer_start + t\n",
    "            answer_mask[t, answer_start:p + 1] = 1.0\n",
    "        self_to_answer = (ans_sa * answer_mask).sum(dim=1).mean().item()\n",
    "\n",
    "        # Totals\n",
    "        self_total = ans_sa.sum(dim=1).mean().item()\n",
    "        cross_total = ans_ca.sum(dim=1).mean().item()\n",
    "\n",
    "        # Self-attention entropy\n",
    "        eps = 1e-10\n",
    "        positions = torch.arange(dec_len, device=DEVICE)\n",
    "        abs_positions = torch.arange(answer_start, answer_start + answer_len, device=DEVICE)\n",
    "        causal = (positions.unsqueeze(0) <= abs_positions.unsqueeze(1)).float()\n",
    "        masked_sa = ans_sa * causal\n",
    "        sa_clamped = masked_sa.clamp(min=eps)\n",
    "        self_entropy = -(masked_sa * sa_clamped.log()).sum(dim=1).mean().item()\n",
    "\n",
    "        # Cross-attention entropy\n",
    "        ca_clamped = ans_ca.clamp(min=eps)\n",
    "        cross_entropy = -(ans_ca * ca_clamped.log()).sum(dim=1).mean().item()\n",
    "\n",
    "        probes[layer_idx] = {\n",
    "            'sb': round(self_to_bos, 6),\n",
    "            'sp': round(self_to_prefix, 6),  # 'sp' = self_to_prefix (query or random)\n",
    "            'sa': round(self_to_answer, 6),\n",
    "            'st': round(self_total, 6),\n",
    "            'ct': round(cross_total, 6),\n",
    "            'se': round(self_entropy, 4),\n",
    "            'ce': round(cross_entropy, 4),\n",
    "        }\n",
    "\n",
    "    del outputs, logits, log_probs\n",
    "    return nll, probes\n",
    "\n",
    "\n",
    "print(\"Probing function defined. Ready for scoring loop.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6a47b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Probing loop \u2014 6 conditions x 500 samples\n",
    "print(\"=\" * 70)\n",
    "print(\"PROBING ALL CONDITIONS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "COND_NAMES = ['bare_nq', 'bare_random_q', 'bare_q',\n",
    "              'oracle_trunc_nq', 'oracle_trunc_random_q', 'oracle_trunc_q']\n",
    "\n",
    "results = []\n",
    "start_idx = 0\n",
    "\n",
    "if CHECKPOINT_PATH.exists():\n",
    "    ckpt = json.loads(CHECKPOINT_PATH.read_text())\n",
    "    if ckpt.get('n_total') == N_SAMPLES and len(ckpt.get('results', [])) > 0:\n",
    "        saved_queries = [r['query'][:50] for r in ckpt['results']]\n",
    "        current_queries = [s['query'][:50] for s in samples[:len(saved_queries)]]\n",
    "        if saved_queries == current_queries:\n",
    "            results = ckpt['results']\n",
    "            # JSON converts int dict keys to strings \u2014 convert back\n",
    "            for r in results:\n",
    "                for cond in COND_NAMES:\n",
    "                    key = f'probes_{cond}'\n",
    "                    if key in r and isinstance(r[key], dict):\n",
    "                        r[key] = {int(k): v for k, v in r[key].items()}\n",
    "            start_idx = len(results)\n",
    "            print(f\"Resuming from checkpoint: {start_idx}/{N_SAMPLES}\")\n",
    "\n",
    "if start_idx == 0:\n",
    "    print(f\"Starting fresh: {len(COND_NAMES)} conditions x {N_SAMPLES} samples\")\n",
    "    print(f\"Probe layers: {PROBE_LAYERS}\")\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "for i in tqdm(range(start_idx, N_SAMPLES), initial=start_idx, total=N_SAMPLES,\n",
    "              desc=\"Probing\"):\n",
    "    s = samples[i]\n",
    "    passage = s['passage']\n",
    "    answer = s['answer']\n",
    "    query_ids = s['query_ids']\n",
    "    random_ids = s['random_ids']\n",
    "\n",
    "    answer_ids = tokenizer(answer, add_special_tokens=False, truncation=True,\n",
    "                           max_length=256).input_ids\n",
    "\n",
    "    if len(answer_ids) == 0:\n",
    "        continue\n",
    "\n",
    "    n_prefix = len(query_ids)  # same for query and random\n",
    "\n",
    "    result = {\n",
    "        'query': s['query'][:50],\n",
    "        'n_prefix_toks': n_prefix,\n",
    "        'n_answer_toks': len(answer_ids),\n",
    "    }\n",
    "\n",
    "    # Build decoder tensors once\n",
    "    dec_nq = torch.tensor([[BOS_ID] + answer_ids], dtype=torch.long, device=DEVICE)\n",
    "    dec_random = torch.tensor([[BOS_ID] + random_ids + answer_ids],\n",
    "                               dtype=torch.long, device=DEVICE)\n",
    "    dec_q = torch.tensor([[BOS_ID] + query_ids + answer_ids],\n",
    "                          dtype=torch.long, device=DEVICE)\n",
    "\n",
    "    # --- Encoder pass 1: bare document ---\n",
    "    enc_ids_bare = tokenizer(passage, return_tensors=\"pt\",\n",
    "                             add_special_tokens=True, truncation=True,\n",
    "                             max_length=2048).input_ids.to(DEVICE)\n",
    "    enc_len_bare = enc_ids_bare.shape[1]\n",
    "    enc_mask_bare = torch.ones(1, enc_len_bare, device=DEVICE, dtype=torch.long)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        enc_out_bare = model.get_encoder()(\n",
    "            input_ids=enc_ids_bare, attention_mask=enc_mask_bare\n",
    "        )\n",
    "\n",
    "    # Condition 1: bare_nq\n",
    "    nll, probes = forward_with_probes(\n",
    "        enc_out_bare, enc_mask_bare, dec_nq,\n",
    "        answer_start=1, answer_len=len(answer_ids), answer_ids_list=answer_ids)\n",
    "    result['nll_bare_nq'] = nll\n",
    "    result['probes_bare_nq'] = probes\n",
    "\n",
    "    # Condition 2: bare_random_q\n",
    "    nll, probes = forward_with_probes(\n",
    "        enc_out_bare, enc_mask_bare, dec_random,\n",
    "        answer_start=1 + n_prefix, answer_len=len(answer_ids),\n",
    "        answer_ids_list=answer_ids)\n",
    "    result['nll_bare_random_q'] = nll\n",
    "    result['probes_bare_random_q'] = probes\n",
    "\n",
    "    # Condition 3: bare_q\n",
    "    nll, probes = forward_with_probes(\n",
    "        enc_out_bare, enc_mask_bare, dec_q,\n",
    "        answer_start=1 + n_prefix, answer_len=len(answer_ids),\n",
    "        answer_ids_list=answer_ids)\n",
    "    result['nll_bare_q'] = nll\n",
    "    result['probes_bare_q'] = probes\n",
    "\n",
    "    del enc_out_bare\n",
    "\n",
    "    # --- Encoder pass 2: oracle (query + document) ---\n",
    "    enc_text_oracle = s['query'] + \"\\n\" + passage\n",
    "    enc_ids_oracle = tokenizer(enc_text_oracle, return_tensors=\"pt\",\n",
    "                               add_special_tokens=True, truncation=True,\n",
    "                               max_length=2048).input_ids.to(DEVICE)\n",
    "    enc_len_oracle = enc_ids_oracle.shape[1]\n",
    "    enc_mask_oracle = torch.ones(1, enc_len_oracle, device=DEVICE, dtype=torch.long)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        enc_out_oracle = model.get_encoder()(\n",
    "            input_ids=enc_ids_oracle, attention_mask=enc_mask_oracle\n",
    "        )\n",
    "\n",
    "    # Cross-attention mask: hide prefix\n",
    "    pfx_count = s['n_pfx_oracle']\n",
    "    cross_mask_trunc = torch.ones(1, enc_len_oracle, device=DEVICE, dtype=torch.long)\n",
    "    cross_mask_trunc[:, :pfx_count] = 0\n",
    "\n",
    "    # Condition 4: oracle_trunc_nq\n",
    "    nll, probes = forward_with_probes(\n",
    "        enc_out_oracle, cross_mask_trunc, dec_nq,\n",
    "        answer_start=1, answer_len=len(answer_ids), answer_ids_list=answer_ids)\n",
    "    result['nll_oracle_trunc_nq'] = nll\n",
    "    result['probes_oracle_trunc_nq'] = probes\n",
    "\n",
    "    # Condition 5: oracle_trunc_random_q\n",
    "    nll, probes = forward_with_probes(\n",
    "        enc_out_oracle, cross_mask_trunc, dec_random,\n",
    "        answer_start=1 + n_prefix, answer_len=len(answer_ids),\n",
    "        answer_ids_list=answer_ids)\n",
    "    result['nll_oracle_trunc_random_q'] = nll\n",
    "    result['probes_oracle_trunc_random_q'] = probes\n",
    "\n",
    "    # Condition 6: oracle_trunc_q\n",
    "    nll, probes = forward_with_probes(\n",
    "        enc_out_oracle, cross_mask_trunc, dec_q,\n",
    "        answer_start=1 + n_prefix, answer_len=len(answer_ids),\n",
    "        answer_ids_list=answer_ids)\n",
    "    result['nll_oracle_trunc_q'] = nll\n",
    "    result['probes_oracle_trunc_q'] = probes\n",
    "\n",
    "    del enc_out_oracle\n",
    "    results.append(result)\n",
    "\n",
    "    if (i + 1) % 20 == 0 or i == N_SAMPLES - 1:\n",
    "        ckpt = {\n",
    "            'n_total': N_SAMPLES,\n",
    "            'probe_layers': PROBE_LAYERS,\n",
    "            'results': results,\n",
    "            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        }\n",
    "        CHECKPOINT_PATH.write_text(json.dumps(ckpt))\n",
    "        elapsed = time.time() - t0\n",
    "        done = i - start_idx + 1\n",
    "        eta = (N_SAMPLES - i - 1) * elapsed / done if done > 0 else 0\n",
    "        tqdm.write(f\"  Checkpoint {i+1}/{N_SAMPLES} | {elapsed/60:.1f}m | ETA {eta/60:.1f}m\")\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "elapsed = time.time() - t0\n",
    "print(f\"\\nProbing complete: {len(results)} samples, \"\n",
    "      f\"{len(COND_NAMES)} conditions in {elapsed/60:.1f} min\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e2d1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: NLL decomposition \u2014 length effect vs query-specific effect\n",
    "print(\"=\" * 70)\n",
    "print(\"NLL DECOMPOSITION: LENGTH vs QUERY SEMANTICS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "nll = {}\n",
    "for cond in COND_NAMES:\n",
    "    nll[cond] = np.array([r[f'nll_{cond}'] for r in results])\n",
    "\n",
    "print(f\"\\n  {'Condition':<30} {'Mean NLL':>10} {'d vs bare_*':>12} {'sig':>5}\")\n",
    "print(f\"  {'-'*62}\")\n",
    "\n",
    "# Bare encoder conditions\n",
    "print(f\"  {'bare_nq':<30} {nll['bare_nq'].mean():>10.4f} {'\u2014':>12} {'\u2014':>5}\")\n",
    "\n",
    "diff_rq = nll['bare_nq'] - nll['bare_random_q']\n",
    "d_rq = cohens_d(diff_rq)\n",
    "_, p_rq = stats.ttest_1samp(diff_rq, 0)\n",
    "sig_rq = '***' if p_rq < 0.001 else '**' if p_rq < 0.01 else '*' if p_rq < 0.05 else 'ns'\n",
    "print(f\"  {'bare_random_q':<30} {nll['bare_random_q'].mean():>10.4f} {d_rq:>+12.3f} {sig_rq:>5}\")\n",
    "\n",
    "diff_q = nll['bare_nq'] - nll['bare_q']\n",
    "d_q = cohens_d(diff_q)\n",
    "_, p_q = stats.ttest_1samp(diff_q, 0)\n",
    "sig_q = '***' if p_q < 0.001 else '**' if p_q < 0.01 else '*' if p_q < 0.05 else 'ns'\n",
    "print(f\"  {'bare_q':<30} {nll['bare_q'].mean():>10.4f} {d_q:>+12.3f} {sig_q:>5}\")\n",
    "\n",
    "# Oracle encoder conditions\n",
    "print(f\"\\n  {'oracle_trunc_nq':<30} {nll['oracle_trunc_nq'].mean():>10.4f} {'\u2014':>12} {'\u2014':>5}\")\n",
    "\n",
    "diff_o_rq = nll['oracle_trunc_nq'] - nll['oracle_trunc_random_q']\n",
    "d_o_rq = cohens_d(diff_o_rq)\n",
    "_, p_o_rq = stats.ttest_1samp(diff_o_rq, 0)\n",
    "sig_o_rq = '***' if p_o_rq < 0.001 else '**' if p_o_rq < 0.01 else '*' if p_o_rq < 0.05 else 'ns'\n",
    "print(f\"  {'oracle_trunc_random_q':<30} {nll['oracle_trunc_random_q'].mean():>10.4f} {d_o_rq:>+12.3f} {sig_o_rq:>5}\")\n",
    "\n",
    "diff_o_q = nll['oracle_trunc_nq'] - nll['oracle_trunc_q']\n",
    "d_o_q = cohens_d(diff_o_q)\n",
    "_, p_o_q = stats.ttest_1samp(diff_o_q, 0)\n",
    "sig_o_q = '***' if p_o_q < 0.001 else '**' if p_o_q < 0.01 else '*' if p_o_q < 0.05 else 'ns'\n",
    "print(f\"  {'oracle_trunc_q':<30} {nll['oracle_trunc_q'].mean():>10.4f} {d_o_q:>+12.3f} {sig_o_q:>5}\")\n",
    "\n",
    "# Decomposition\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"DECOMPOSITION OF nq\u2192q IMPROVEMENT\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "# Bare encoder\n",
    "total_bare = d_q\n",
    "length_bare = d_rq\n",
    "semantic_bare_diff = nll['bare_random_q'] - nll['bare_q']\n",
    "d_semantic_bare = cohens_d(semantic_bare_diff)\n",
    "_, p_sem_bare = stats.ttest_1samp(semantic_bare_diff, 0)\n",
    "sig_sem_bare = '***' if p_sem_bare < 0.001 else '**' if p_sem_bare < 0.01 else '*' if p_sem_bare < 0.05 else 'ns'\n",
    "\n",
    "print(f\"\\n  Bare encoder:\")\n",
    "print(f\"    Total (nq \u2192 q):        d={total_bare:+.3f}\")\n",
    "print(f\"    Length (nq \u2192 random_q): d={length_bare:+.3f}\")\n",
    "print(f\"    Semantic (random \u2192 q):  d={d_semantic_bare:+.3f} ({sig_sem_bare})\")\n",
    "if total_bare > 0:\n",
    "    print(f\"    Length fraction:    {length_bare / total_bare * 100:.0f}%\")\n",
    "    print(f\"    Semantic fraction: {d_semantic_bare / total_bare * 100:.0f}%\")\n",
    "\n",
    "# Oracle encoder\n",
    "total_oracle = d_o_q\n",
    "length_oracle = d_o_rq\n",
    "semantic_oracle_diff = nll['oracle_trunc_random_q'] - nll['oracle_trunc_q']\n",
    "d_semantic_oracle = cohens_d(semantic_oracle_diff)\n",
    "_, p_sem_oracle = stats.ttest_1samp(semantic_oracle_diff, 0)\n",
    "sig_sem_oracle = '***' if p_sem_oracle < 0.001 else '**' if p_sem_oracle < 0.01 else '*' if p_sem_oracle < 0.05 else 'ns'\n",
    "\n",
    "print(f\"\\n  Oracle encoder:\")\n",
    "print(f\"    Total (nq \u2192 q):        d={total_oracle:+.3f}\")\n",
    "print(f\"    Length (nq \u2192 random_q): d={length_oracle:+.3f}\")\n",
    "print(f\"    Semantic (random \u2192 q):  d={d_semantic_oracle:+.3f} ({sig_sem_oracle})\")\n",
    "if total_oracle > 0:\n",
    "    print(f\"    Length fraction:    {length_oracle / total_oracle * 100:.0f}%\")\n",
    "    print(f\"    Semantic fraction: {d_semantic_oracle / total_oracle * 100:.0f}%\")\n",
    "\n",
    "# Does random prefix HURT? (random tokens might confuse the decoder)\n",
    "print(f\"\\n  Does random prefix hurt answer prediction?\")\n",
    "print(f\"    bare: nq \u2192 random_q NLL change: {(nll['bare_random_q'] - nll['bare_nq']).mean():+.4f}\")\n",
    "print(f\"    oracle: nq \u2192 random_q NLL change: {(nll['oracle_trunc_random_q'] - nll['oracle_trunc_nq']).mean():+.4f}\")\n",
    "print(f\"    (Positive = random hurts, Negative = random helps)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6944ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Attention budget \u2014 is the 5.5% query buffer a length artifact?\n",
    "print(\"=\" * 70)\n",
    "print(\"ATTENTION BUDGET: LENGTH ARTIFACT TEST\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "last_layer = PROBE_LAYERS[-1]\n",
    "print(f\"Layer {last_layer} \u2014 mean over {len(results)} samples\")\n",
    "\n",
    "print(f\"\\n  {'Condition':<30} {'BOS':>8} {'Prefix':>8} {'Answer':>8} \"\n",
    "      f\"{'Self':>8} {'Cross':>8} {'Check':>8}\")\n",
    "print(f\"  {'-'*82}\")\n",
    "\n",
    "for cond in COND_NAMES:\n",
    "    sb = np.mean([r[f'probes_{cond}'][last_layer]['sb'] for r in results])\n",
    "    sp = np.mean([r[f'probes_{cond}'][last_layer]['sp'] for r in results])\n",
    "    sa = np.mean([r[f'probes_{cond}'][last_layer]['sa'] for r in results])\n",
    "    st = np.mean([r[f'probes_{cond}'][last_layer]['st'] for r in results])\n",
    "    ct = np.mean([r[f'probes_{cond}'][last_layer]['ct'] for r in results])\n",
    "    check = st + ct\n",
    "    print(f\"  {cond:<30} {sb:>8.4f} {sp:>8.4f} {sa:>8.4f} \"\n",
    "          f\"{st:>8.4f} {ct:>8.4f} {check:>8.4f}\")\n",
    "\n",
    "# Key comparison: random prefix vs query prefix attention absorption\n",
    "sp_random_bare = np.array([r['probes_bare_random_q'][last_layer]['sp'] for r in results])\n",
    "sp_query_bare = np.array([r['probes_bare_q'][last_layer]['sp'] for r in results])\n",
    "sp_random_orc = np.array([r['probes_oracle_trunc_random_q'][last_layer]['sp'] for r in results])\n",
    "sp_query_orc = np.array([r['probes_oracle_trunc_q'][last_layer]['sp'] for r in results])\n",
    "\n",
    "print(f\"\\n--- Prefix attention absorption (query vs random, layer {last_layer}) ---\")\n",
    "print(f\"  Bare encoder:\")\n",
    "print(f\"    Random prefix absorbs: {sp_random_bare.mean()*100:.1f}%\")\n",
    "print(f\"    Query prefix absorbs:  {sp_query_bare.mean()*100:.1f}%\")\n",
    "diff_sp_bare = sp_query_bare - sp_random_bare\n",
    "d_sp_bare = cohens_d(diff_sp_bare)\n",
    "_, p_sp_bare = stats.ttest_1samp(diff_sp_bare, 0)\n",
    "sig_sp = '***' if p_sp_bare < 0.001 else '**' if p_sp_bare < 0.01 else '*' if p_sp_bare < 0.05 else 'ns'\n",
    "print(f\"    Difference: {diff_sp_bare.mean()*100:+.1f}pp (d={d_sp_bare:+.3f}, {sig_sp})\")\n",
    "\n",
    "print(f\"\\n  Oracle encoder:\")\n",
    "print(f\"    Random prefix absorbs: {sp_random_orc.mean()*100:.1f}%\")\n",
    "print(f\"    Query prefix absorbs:  {sp_query_orc.mean()*100:.1f}%\")\n",
    "diff_sp_orc = sp_query_orc - sp_random_orc\n",
    "d_sp_orc = cohens_d(diff_sp_orc)\n",
    "_, p_sp_orc = stats.ttest_1samp(diff_sp_orc, 0)\n",
    "sig_sp_o = '***' if p_sp_orc < 0.001 else '**' if p_sp_orc < 0.01 else '*' if p_sp_orc < 0.05 else 'ns'\n",
    "print(f\"    Difference: {diff_sp_orc.mean()*100:+.1f}pp (d={d_sp_orc:+.3f}, {sig_sp_o})\")\n",
    "\n",
    "# Where does the prefix steal from? (nq vs random_q, to isolate pure length)\n",
    "print(f\"\\n--- Where does random prefix steal from? (bare, layer {last_layer}) ---\")\n",
    "sb_nq = np.mean([r['probes_bare_nq'][last_layer]['sb'] for r in results])\n",
    "sb_rq = np.mean([r['probes_bare_random_q'][last_layer]['sb'] for r in results])\n",
    "sa_nq = np.mean([r['probes_bare_nq'][last_layer]['sa'] for r in results])\n",
    "sa_rq = np.mean([r['probes_bare_random_q'][last_layer]['sa'] for r in results])\n",
    "ct_nq = np.mean([r['probes_bare_nq'][last_layer]['ct'] for r in results])\n",
    "ct_rq = np.mean([r['probes_bare_random_q'][last_layer]['ct'] for r in results])\n",
    "\n",
    "print(f\"  BOS change:    {sb_rq - sb_nq:+.4f}\")\n",
    "print(f\"  Answer change: {sa_rq - sa_nq:+.4f}\")\n",
    "print(f\"  Cross change:  {ct_rq - ct_nq:+.4f}\")\n",
    "print(f\"  (Compare to query: BOS {np.mean([r['probes_bare_q'][last_layer]['sb'] for r in results]) - sb_nq:+.4f}, \"\n",
    "      f\"Ans {np.mean([r['probes_bare_q'][last_layer]['sa'] for r in results]) - sa_nq:+.4f}, \"\n",
    "      f\"Cross {np.mean([r['probes_bare_q'][last_layer]['ct'] for r in results]) - ct_nq:+.4f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a2db71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Entropy analysis \u2014 does random prefix change attention patterns?\n",
    "print(\"=\" * 70)\n",
    "print(\"ENTROPY ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "last_layer = PROBE_LAYERS[-1]\n",
    "\n",
    "print(f\"\\n--- Self-attention entropy (layer {last_layer}) ---\")\n",
    "for cond in COND_NAMES:\n",
    "    se = np.mean([r[f'probes_{cond}'][last_layer]['se'] for r in results])\n",
    "    print(f\"  {cond:<30} {se:.4f}\")\n",
    "\n",
    "se_nq = np.array([r['probes_bare_nq'][last_layer]['se'] for r in results])\n",
    "se_rq = np.array([r['probes_bare_random_q'][last_layer]['se'] for r in results])\n",
    "se_q = np.array([r['probes_bare_q'][last_layer]['se'] for r in results])\n",
    "\n",
    "d_se_len = cohens_d(se_rq - se_nq)\n",
    "d_se_sem = cohens_d(se_q - se_rq)\n",
    "d_se_tot = cohens_d(se_q - se_nq)\n",
    "print(f\"\\n  Self-entropy change (bare):\")\n",
    "print(f\"    nq \u2192 random_q (length):   d={d_se_len:+.3f}\")\n",
    "print(f\"    random_q \u2192 q (semantic):   d={d_se_sem:+.3f}\")\n",
    "print(f\"    nq \u2192 q (total):            d={d_se_tot:+.3f}\")\n",
    "\n",
    "print(f\"\\n--- Cross-attention entropy (layer {last_layer}) ---\")\n",
    "for cond in COND_NAMES:\n",
    "    ce = np.mean([r[f'probes_{cond}'][last_layer]['ce'] for r in results])\n",
    "    print(f\"  {cond:<30} {ce:.4f}\")\n",
    "\n",
    "ce_nq = np.array([r['probes_bare_nq'][last_layer]['ce'] for r in results])\n",
    "ce_rq = np.array([r['probes_bare_random_q'][last_layer]['ce'] for r in results])\n",
    "ce_q = np.array([r['probes_bare_q'][last_layer]['ce'] for r in results])\n",
    "\n",
    "d_ce_len = cohens_d(ce_rq - ce_nq)\n",
    "d_ce_sem = cohens_d(ce_q - ce_rq)\n",
    "d_ce_tot = cohens_d(ce_q - ce_nq)\n",
    "print(f\"\\n  Cross-entropy change (bare):\")\n",
    "print(f\"    nq \u2192 random_q (length):   d={d_ce_len:+.3f}\")\n",
    "print(f\"    random_q \u2192 q (semantic):   d={d_ce_sem:+.3f}\")\n",
    "print(f\"    nq \u2192 q (total):            d={d_ce_tot:+.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be0e981",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Does the encoder prefix interaction survive length control?\n",
    "print(\"=\" * 70)\n",
    "print(\"ENCODER PREFIX EFFECT \u2014 WITH LENGTH CONTROL\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# The key question: In Exp 07, the 35% redundancy was between encoder prefix\n",
    "# and decoder query. But how much of that is just decoder LENGTH?\n",
    "\n",
    "print(f\"\\n--- Encoder prefix effect across decoder conditions ---\")\n",
    "enc_nq = nll['bare_nq'] - nll['oracle_trunc_nq']\n",
    "enc_rq = nll['bare_random_q'] - nll['oracle_trunc_random_q']\n",
    "enc_q = nll['bare_q'] - nll['oracle_trunc_q']\n",
    "\n",
    "d_enc_nq = cohens_d(enc_nq)\n",
    "d_enc_rq = cohens_d(enc_rq)\n",
    "d_enc_q = cohens_d(enc_q)\n",
    "\n",
    "_, p_enc_nq = stats.ttest_1samp(enc_nq, 0)\n",
    "_, p_enc_rq = stats.ttest_1samp(enc_rq, 0)\n",
    "_, p_enc_q = stats.ttest_1samp(enc_q, 0)\n",
    "\n",
    "print(f\"  With nq decoder:        d={d_enc_nq:+.3f} (p={p_enc_nq:.2e})\")\n",
    "print(f\"  With random_q decoder:  d={d_enc_rq:+.3f} (p={p_enc_rq:.2e})\")\n",
    "print(f\"  With query decoder:     d={d_enc_q:+.3f} (p={p_enc_q:.2e})\")\n",
    "\n",
    "# Interactions\n",
    "int_length = enc_nq - enc_rq  # nq vs random_q: pure length interaction\n",
    "int_semantic = enc_rq - enc_q  # random_q vs q: semantic interaction\n",
    "int_total = enc_nq - enc_q    # total (same as Exp 07)\n",
    "\n",
    "d_int_length = cohens_d(int_length)\n",
    "d_int_semantic = cohens_d(int_semantic)\n",
    "d_int_total = cohens_d(int_total)\n",
    "\n",
    "_, p_il = stats.ttest_1samp(int_length, 0)\n",
    "_, p_is = stats.ttest_1samp(int_semantic, 0)\n",
    "_, p_it = stats.ttest_1samp(int_total, 0)\n",
    "\n",
    "sig_il = '***' if p_il < 0.001 else '**' if p_il < 0.01 else '*' if p_il < 0.05 else 'ns'\n",
    "sig_is = '***' if p_is < 0.001 else '**' if p_is < 0.01 else '*' if p_is < 0.05 else 'ns'\n",
    "sig_it = '***' if p_it < 0.001 else '**' if p_it < 0.01 else '*' if p_it < 0.05 else 'ns'\n",
    "\n",
    "print(f\"\\n--- Interaction decomposition ---\")\n",
    "print(f\"  Total interaction (nq vs q):        d={d_int_total:+.3f} ({sig_it})\")\n",
    "print(f\"  Length interaction (nq vs random_q): d={d_int_length:+.3f} ({sig_il})\")\n",
    "print(f\"  Semantic interaction (random vs q):  d={d_int_semantic:+.3f} ({sig_is})\")\n",
    "\n",
    "if d_int_total != 0:\n",
    "    print(f\"\\n  Of the total encoder prefix reduction:\")\n",
    "    print(f\"    Due to decoder LENGTH:    {d_int_length / d_int_total * 100:.0f}%\")\n",
    "    print(f\"    Due to query SEMANTICS:   {d_int_semantic / d_int_total * 100:.0f}%\")\n",
    "\n",
    "# Exp 07 replication check\n",
    "print(f\"\\n--- Exp 07 replication ---\")\n",
    "print(f\"  Exp 07: encoder prefix d_nq={0.366:+.3f}, d_q={0.238:+.3f}, interaction={0.316:+.3f}\")\n",
    "print(f\"  Exp 08: encoder prefix d_nq={d_enc_nq:+.3f}, d_q={d_enc_q:+.3f}, interaction={d_int_total:+.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf28a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Summary and save\n",
    "print(\"=\" * 70)\n",
    "print(\"SUMMARY \u2014 Exp 08: Decoder Length Control\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "last_layer = PROBE_LAYERS[-1]\n",
    "\n",
    "# NLL decomposition\n",
    "total_bare = cohens_d(nll['bare_nq'] - nll['bare_q'])\n",
    "length_bare = cohens_d(nll['bare_nq'] - nll['bare_random_q'])\n",
    "semantic_bare = cohens_d(nll['bare_random_q'] - nll['bare_q'])\n",
    "\n",
    "total_oracle = cohens_d(nll['oracle_trunc_nq'] - nll['oracle_trunc_q'])\n",
    "length_oracle = cohens_d(nll['oracle_trunc_nq'] - nll['oracle_trunc_random_q'])\n",
    "semantic_oracle = cohens_d(nll['oracle_trunc_random_q'] - nll['oracle_trunc_q'])\n",
    "\n",
    "print(f\"\\nModel: {MODEL_NAME}\")\n",
    "print(f\"N: {len(results)}\")\n",
    "\n",
    "print(f\"\\n--- NLL Decomposition (nq \u2192 q) ---\")\n",
    "print(f\"  Bare encoder:\")\n",
    "print(f\"    Total:    d={total_bare:+.3f}\")\n",
    "print(f\"    Length:   d={length_bare:+.3f} ({length_bare/total_bare*100:.0f}%)\" if total_bare else \"\")\n",
    "print(f\"    Semantic: d={semantic_bare:+.3f} ({semantic_bare/total_bare*100:.0f}%)\" if total_bare else \"\")\n",
    "print(f\"  Oracle encoder:\")\n",
    "print(f\"    Total:    d={total_oracle:+.3f}\")\n",
    "print(f\"    Length:   d={length_oracle:+.3f} ({length_oracle/total_oracle*100:.0f}%)\" if total_oracle else \"\")\n",
    "print(f\"    Semantic: d={semantic_oracle:+.3f} ({semantic_oracle/total_oracle*100:.0f}%)\" if total_oracle else \"\")\n",
    "\n",
    "# Attention\n",
    "sp_random = np.mean([r['probes_bare_random_q'][last_layer]['sp'] for r in results])\n",
    "sp_query = np.mean([r['probes_bare_q'][last_layer]['sp'] for r in results])\n",
    "\n",
    "print(f\"\\n--- Attention (layer {last_layer}) ---\")\n",
    "print(f\"  Random prefix absorbs: {sp_random*100:.1f}% of answer attention\")\n",
    "print(f\"  Query prefix absorbs:  {sp_query*100:.1f}% of answer attention\")\n",
    "if sp_query > 0:\n",
    "    print(f\"  Length accounts for:   {sp_random/sp_query*100:.0f}% of prefix buffer\")\n",
    "\n",
    "# Interaction decomposition\n",
    "enc_nq = nll['bare_nq'] - nll['oracle_trunc_nq']\n",
    "enc_rq = nll['bare_random_q'] - nll['oracle_trunc_random_q']\n",
    "enc_q = nll['bare_q'] - nll['oracle_trunc_q']\n",
    "d_enc_nq = cohens_d(enc_nq)\n",
    "d_enc_q = cohens_d(enc_q)\n",
    "int_total = enc_nq - enc_q\n",
    "int_length = enc_nq - enc_rq\n",
    "int_semantic = enc_rq - enc_q\n",
    "d_int_total = cohens_d(int_total)\n",
    "d_int_length = cohens_d(int_length)\n",
    "d_int_semantic = cohens_d(int_semantic)\n",
    "\n",
    "print(f\"\\n--- Encoder prefix \u00d7 decoder interaction ---\")\n",
    "print(f\"  Total redundancy (Exp 07 replication): {(1-d_enc_q/d_enc_nq)*100:.0f}%\")\n",
    "if d_int_total != 0:\n",
    "    print(f\"  Due to length: {d_int_length/d_int_total*100:.0f}%\")\n",
    "    print(f\"  Due to semantics: {d_int_semantic/d_int_total*100:.0f}%\")\n",
    "\n",
    "# Verdict\n",
    "print(f\"\\n--- Verdict ---\")\n",
    "if abs(length_bare) < 0.05:\n",
    "    print(f\"  Length effect is NEGLIGIBLE (d={length_bare:+.3f})\")\n",
    "    print(f\"  The nq\u2192q improvement is almost entirely query-semantic.\")\n",
    "elif abs(length_bare) > abs(semantic_bare):\n",
    "    print(f\"  Length effect DOMINATES (d={length_bare:+.3f} vs semantic d={semantic_bare:+.3f})\")\n",
    "    print(f\"  The Exp 07 'query buffer' finding is largely a length artifact.\")\n",
    "else:\n",
    "    print(f\"  Both effects contribute: length d={length_bare:+.3f}, semantic d={semantic_bare:+.3f}\")\n",
    "    print(f\"  Query provides genuine semantic value beyond mere length.\")\n",
    "\n",
    "# Save results\n",
    "final_results = {\n",
    "    'experiment': 'v4_exp08_decoder_length_control',\n",
    "    'model': MODEL_NAME,\n",
    "    'dataset': 'ms_marco_v1.1',\n",
    "    'n_samples': len(results),\n",
    "    'seed': SEED,\n",
    "    'probe_layers': PROBE_LAYERS,\n",
    "    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'nll': {cond: float(nll[cond].mean()) for cond in COND_NAMES},\n",
    "    'nll_decomposition': {\n",
    "        'bare': {\n",
    "            'total': float(total_bare),\n",
    "            'length': float(length_bare),\n",
    "            'semantic': float(semantic_bare),\n",
    "        },\n",
    "        'oracle': {\n",
    "            'total': float(total_oracle),\n",
    "            'length': float(length_oracle),\n",
    "            'semantic': float(semantic_oracle),\n",
    "        },\n",
    "    },\n",
    "    'prefix_attention': {\n",
    "        'random_bare': float(sp_random),\n",
    "        'query_bare': float(sp_query),\n",
    "    },\n",
    "    'interaction_decomposition': {\n",
    "        'd_int_total': float(d_int_total),\n",
    "        'd_int_length': float(d_int_length),\n",
    "        'd_int_semantic': float(d_int_semantic),\n",
    "    },\n",
    "}\n",
    "\n",
    "with open(RESULTS_DIR / 'results.json', 'w') as f:\n",
    "    json.dump(final_results, f, indent=2)\n",
    "print(f\"\\nResults saved to {RESULTS_DIR / 'results.json'}\")\n",
    "\n",
    "# Cleanup\n",
    "print(f\"\\nCleaning up GPU memory...\")\n",
    "mem_before = torch.cuda.memory_allocated() / 1e9\n",
    "del model, processor, tokenizer\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "mem_after = torch.cuda.memory_allocated() / 1e9\n",
    "print(f\"GPU memory: {mem_before:.2f} GB -> {mem_after:.2f} GB\")\n",
    "print(\"Done!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}