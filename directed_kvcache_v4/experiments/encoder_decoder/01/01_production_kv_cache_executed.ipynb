{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9c9d0d3",
   "metadata": {},
   "source": [
    "# Experiment 01: Production-Realistic KV Cache\n",
    "\n",
    "## Motivation\n",
    "\n",
    "v3 Experiment 01 proved that co-encoding [surrogate + document] enriches document\n",
    "representations (d=+0.408). But the decoder never saw the query — it only scored\n",
    "answer NLL from encoder states alone. This doesn't model any real production system.\n",
    "\n",
    "In production:\n",
    "1. **Offline**: Encode [surrogate + document] → cache encoder hidden states\n",
    "2. **Online**: Query arrives → decoder receives query as input, cross-attends to cached encoder states\n",
    "\n",
    "**The key question**: Does surrogate-enriched encoder caching still help when the\n",
    "decoder already has the query? If the decoder knowing the query makes enrichment\n",
    "redundant, the approach has no production value.\n",
    "\n",
    "## Method\n",
    "\n",
    "`T5Gemma2ForConditionalGeneration.forward()` accepts explicit `decoder_input_ids`\n",
    "alongside `encoder_outputs`. We build `decoder_input_ids = [BOS] + query_tokens + answer_tokens`\n",
    "and compute NLL only on the answer token positions.\n",
    "\n",
    "## Conditions (8 total)\n",
    "\n",
    "### With query in decoder (production-realistic):\n",
    "\n",
    "| # | Condition | Encoder input | Cross-attn | Decoder input |\n",
    "|---|-----------|--------------|------------|---------------|\n",
    "| 1 | bare | [document] | all | [query] → answer |\n",
    "| 2 | oracle_trunc | [query + document] | doc only | [query] → answer |\n",
    "| 3 | oracle_full | [query + document] | all | [query] → answer |\n",
    "| 4 | surr_template_trunc | [\"What is [kw]?\" + doc] | doc only | [query] → answer |\n",
    "| 5 | surr_doc_trunc | [top-5 kw + document] | doc only | [query] → answer |\n",
    "| 6 | random_trunc | [random words + doc] | doc only | [query] → answer |\n",
    "\n",
    "### Without query in decoder (replicates v3 Exp 01):\n",
    "\n",
    "| # | Condition | Encoder input | Cross-attn | Decoder input |\n",
    "|---|-----------|--------------|------------|---------------|\n",
    "| 7 | bare_nq | [document] | all | answer only |\n",
    "| 8 | oracle_trunc_nq | [query + document] | doc only | answer only |\n",
    "\n",
    "## Key comparisons\n",
    "\n",
    "- **(2) vs (1)**: Does enrichment help when decoder already has query? (**THE** question)\n",
    "- **(8) vs (7)**: Replicates v3 Exp 01 finding (expected d≈+0.4)\n",
    "- **(2)−(1) vs (8)−(7)**: Is enrichment redundant once decoder has query?\n",
    "- **(4) vs (1)**: Production-realistic surrogate benefit with query in decoder\n",
    "- **(3) vs (2)**: Does full cross-attention add value beyond enriched doc reps?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5be0586",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T17:18:06.728101Z",
     "iopub.status.busy": "2026-02-19T17:18:06.727492Z",
     "iopub.status.idle": "2026-02-19T17:18:27.854176Z",
     "shell.execute_reply": "2026-02-19T17:18:27.853235Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading google/t5gemma-2-4b-4b...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1ec83b1fdac49358b5baa6a9ef2601f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/1327 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exp 01: Production-Realistic KV Cache\n",
      "N: 500, Model: google/t5gemma-2-4b-4b\n",
      "DEVICE: cuda:0, dtype: torch.bfloat16\n",
      "GPU memory: 15.02 GB\n",
      "Decoder start token ID (BOS): 2\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Setup and model loading\n",
    "import os\n",
    "os.umask(0o000)\n",
    "\n",
    "import sys, json, time, gc, re\n",
    "import random as pyrandom\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from scipy import stats\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "sys.path.insert(0, \"../..\")\n",
    "from lib.analysis import cohens_d\n",
    "\n",
    "SEED = 42\n",
    "N_SAMPLES = 500\n",
    "MODEL_NAME = \"google/t5gemma-2-4b-4b\"\n",
    "\n",
    "RESULTS_DIR = Path(\"../../results/exp01\")\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CHECKPOINT_PATH = RESULTS_DIR / \"checkpoint.json\"\n",
    "\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "pyrandom.seed(SEED)\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv())\n",
    "HF_TOKEN = os.environ.get(\"HF_TOKEN\")\n",
    "\n",
    "from transformers import AutoProcessor, AutoModelForSeq2SeqLM\n",
    "\n",
    "print(f\"Loading {MODEL_NAME}...\")\n",
    "processor = AutoProcessor.from_pretrained(MODEL_NAME, token=HF_TOKEN)\n",
    "tokenizer = processor.tokenizer\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    MODEL_NAME, device_map=\"auto\", torch_dtype=torch.bfloat16, token=HF_TOKEN,\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "DEVICE = next(model.parameters()).device\n",
    "BOS_ID = getattr(model.config, 'decoder_start_token_id', None) or tokenizer.bos_token_id\n",
    "\n",
    "print(f\"Exp 01: Production-Realistic KV Cache\")\n",
    "print(f\"N: {N_SAMPLES}, Model: {MODEL_NAME}\")\n",
    "print(f\"DEVICE: {DEVICE}, dtype: {next(model.parameters()).dtype}\")\n",
    "print(f\"GPU memory: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "print(f\"Decoder start token ID (BOS): {BOS_ID}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db38b024",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T17:18:27.858305Z",
     "iopub.status.busy": "2026-02-19T17:18:27.857697Z",
     "iopub.status.idle": "2026-02-19T17:18:27.882592Z",
     "shell.execute_reply": "2026-02-19T17:18:27.881885Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scoring functions defined:\n",
      "  score_nll(encoder_text, answer_text, prefix_token_count, truncate)\n",
      "  score_nll_query_prefix(encoder_text, query_text, answer_text, prefix_token_count, truncate)\n",
      "\n",
      "Sanity check:\n",
      "  BOS_ID: 2 (token: '<bos>')\n",
      "  Query 'What is Python?' -> 4 tokens\n",
      "  Answer 'A programming language.' -> 4 tokens\n",
      "  Decoder input length: 1 + 4 + 4 = 9\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Scoring helpers\n",
    "\n",
    "def count_prefix_tokens(prefix_text, document_text):\n",
    "    # BPE-aware token count of prefix in [prefix + newline + document].\n",
    "    full_text = prefix_text + \"\\n\" + document_text\n",
    "    full_ids = tokenizer(full_text, add_special_tokens=True, truncation=True,\n",
    "                         max_length=2048).input_ids\n",
    "    doc_ids = tokenizer(document_text, add_special_tokens=True, truncation=True,\n",
    "                        max_length=2048).input_ids\n",
    "    return len(full_ids) - len(doc_ids)\n",
    "\n",
    "\n",
    "def score_nll(encoder_text, answer_text, prefix_token_count=0, truncate=False):\n",
    "    # Score NLL of answer tokens — decoder receives ONLY answer (no query).\n",
    "    # Used for _nq (no-query) conditions that replicate v3 Exp 01.\n",
    "    enc_ids = tokenizer(encoder_text, return_tensors=\"pt\",\n",
    "                        add_special_tokens=True, truncation=True,\n",
    "                        max_length=2048).input_ids.to(DEVICE)\n",
    "    total_enc_len = enc_ids.shape[1]\n",
    "    enc_mask = torch.ones(1, total_enc_len, device=DEVICE, dtype=torch.long)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        encoder_outputs = model.get_encoder()(\n",
    "            input_ids=enc_ids, attention_mask=enc_mask\n",
    "        )\n",
    "\n",
    "    if truncate and prefix_token_count > 0:\n",
    "        cross_attn_mask = torch.ones(1, total_enc_len, device=DEVICE, dtype=torch.long)\n",
    "        cross_attn_mask[:, :prefix_token_count] = 0\n",
    "    else:\n",
    "        cross_attn_mask = torch.ones(1, total_enc_len, device=DEVICE, dtype=torch.long)\n",
    "\n",
    "    ans_ids = tokenizer(answer_text, return_tensors=\"pt\",\n",
    "                        add_special_tokens=False, truncation=True,\n",
    "                        max_length=256).input_ids.to(DEVICE)\n",
    "\n",
    "    if ans_ids.shape[1] == 0:\n",
    "        del encoder_outputs\n",
    "        return 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            encoder_outputs=encoder_outputs,\n",
    "            attention_mask=cross_attn_mask,\n",
    "            labels=ans_ids,\n",
    "        )\n",
    "\n",
    "    logits = outputs.logits\n",
    "    log_probs = F.log_softmax(logits, dim=-1)\n",
    "    token_log_probs = log_probs[0].gather(1, ans_ids[0].unsqueeze(1)).squeeze(1)\n",
    "    mean_nll = -token_log_probs.mean().item()\n",
    "\n",
    "    del encoder_outputs, outputs, logits, log_probs\n",
    "    return mean_nll\n",
    "\n",
    "\n",
    "def score_nll_query_prefix(encoder_text, query_text, answer_text,\n",
    "                           prefix_token_count=0, truncate=False):\n",
    "    # Score NLL of answer tokens with query as decoder prefix.\n",
    "    # Production-realistic: decoder_input_ids = [BOS] + query_ids + answer_ids.\n",
    "    # NLL is computed only on answer tokens.\n",
    "    #\n",
    "    # Args:\n",
    "    #   encoder_text: Text for encoder (e.g., \"[prefix]\\n[document]\" or \"[document]\")\n",
    "    #   query_text: Query text fed to decoder as prefix\n",
    "    #   answer_text: Answer text whose NLL we measure\n",
    "    #   prefix_token_count: Number of encoder prefix tokens to potentially mask\n",
    "    #   truncate: If True, mask prefix tokens from decoder cross-attention\n",
    "\n",
    "    # 1. Encode\n",
    "    enc_ids = tokenizer(encoder_text, return_tensors=\"pt\",\n",
    "                        add_special_tokens=True, truncation=True,\n",
    "                        max_length=2048).input_ids.to(DEVICE)\n",
    "    total_enc_len = enc_ids.shape[1]\n",
    "    enc_mask = torch.ones(1, total_enc_len, device=DEVICE, dtype=torch.long)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        encoder_outputs = model.get_encoder()(\n",
    "            input_ids=enc_ids, attention_mask=enc_mask\n",
    "        )\n",
    "\n",
    "    # 2. Cross-attention mask\n",
    "    if truncate and prefix_token_count > 0:\n",
    "        cross_attn_mask = torch.ones(1, total_enc_len, device=DEVICE, dtype=torch.long)\n",
    "        cross_attn_mask[:, :prefix_token_count] = 0\n",
    "    else:\n",
    "        cross_attn_mask = torch.ones(1, total_enc_len, device=DEVICE, dtype=torch.long)\n",
    "\n",
    "    # 3. Tokenize query and answer for decoder\n",
    "    query_ids = tokenizer(query_text, add_special_tokens=False, truncation=True,\n",
    "                          max_length=512).input_ids\n",
    "    answer_ids = tokenizer(answer_text, add_special_tokens=False, truncation=True,\n",
    "                           max_length=256).input_ids\n",
    "\n",
    "    if len(answer_ids) == 0:\n",
    "        del encoder_outputs\n",
    "        return 0.0\n",
    "\n",
    "    # 4. Build decoder_input_ids = [BOS] + query_ids + answer_ids\n",
    "    dec_ids = [BOS_ID] + query_ids + answer_ids\n",
    "    dec_tensor = torch.tensor([dec_ids], dtype=torch.long, device=DEVICE)\n",
    "\n",
    "    n_query = len(query_ids)\n",
    "    n_answer = len(answer_ids)\n",
    "\n",
    "    # 5. Forward pass (no labels — we compute NLL manually)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            encoder_outputs=encoder_outputs,\n",
    "            attention_mask=cross_attn_mask,\n",
    "            decoder_input_ids=dec_tensor,\n",
    "        )\n",
    "\n",
    "    # 6. Extract answer logits\n",
    "    # logits[0, t, :] predicts the token at position t+1\n",
    "    # Positions: [BOS=0, q1=1, ..., qK=K, a1=K+1, ..., aM=K+M]\n",
    "    # To predict a1 at position K+1, use logits[0, K, :]\n",
    "    # To predict aM at position K+M, use logits[0, K+M-1, :]\n",
    "    logits = outputs.logits\n",
    "    answer_logits = logits[0, n_query:n_query + n_answer, :]\n",
    "\n",
    "    # 7. Compute NLL\n",
    "    answer_targets = torch.tensor(answer_ids, dtype=torch.long, device=DEVICE)\n",
    "    log_probs = F.log_softmax(answer_logits, dim=-1)\n",
    "    token_log_probs = log_probs.gather(1, answer_targets.unsqueeze(1)).squeeze(1)\n",
    "    mean_nll = -token_log_probs.mean().item()\n",
    "\n",
    "    del encoder_outputs, outputs, logits, log_probs\n",
    "    return mean_nll\n",
    "\n",
    "\n",
    "# === Surrogate generation ===\n",
    "STOP_WORDS = {\n",
    "    'a', 'an', 'the', 'is', 'are', 'was', 'were', 'be', 'been', 'being',\n",
    "    'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could',\n",
    "    'should', 'may', 'might', 'can', 'shall', 'to', 'of', 'in', 'for',\n",
    "    'on', 'with', 'at', 'by', 'from', 'as', 'into', 'through', 'during',\n",
    "    'before', 'after', 'above', 'below', 'between', 'and', 'but', 'or',\n",
    "    'not', 'no', 'if', 'then', 'than', 'so', 'up', 'out', 'about',\n",
    "    'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those',\n",
    "    'it', 'its', 'i', 'me', 'my', 'we', 'our', 'you', 'your', 'he',\n",
    "    'him', 'his', 'she', 'her', 'they', 'them', 'their', 'how', 'when',\n",
    "    'where', 'why', 'much', 'many', 'some', 'any', 'all', 'each',\n",
    "    'does', 'also', 'just', 'more', 'most', 'very', 'too', 'only',\n",
    "}\n",
    "\n",
    "def extract_keywords(text):\n",
    "    words = re.sub(r'[^\\w\\s]', '', text.lower()).split()\n",
    "    return [w for w in words if w not in STOP_WORDS and len(w) > 2]\n",
    "\n",
    "def make_surrogate_template(passage):\n",
    "    content_words = extract_keywords(passage)\n",
    "    if not content_words:\n",
    "        return \"What is this about?\"\n",
    "    counts = Counter(content_words)\n",
    "    top_word = counts.most_common(1)[0][0]\n",
    "    return f\"What is {top_word}?\"\n",
    "\n",
    "def make_surrogate_from_doc(passage):\n",
    "    content_words = extract_keywords(passage)\n",
    "    if not content_words:\n",
    "        return \"information\"\n",
    "    counts = Counter(content_words)\n",
    "    return \" \".join(w for w, _ in counts.most_common(5))\n",
    "\n",
    "print(\"Scoring functions defined:\")\n",
    "print(\"  score_nll(encoder_text, answer_text, prefix_token_count, truncate)\")\n",
    "print(\"  score_nll_query_prefix(encoder_text, query_text, answer_text, prefix_token_count, truncate)\")\n",
    "\n",
    "# Sanity check: verify BOS_ID and decoder behavior\n",
    "test_q_ids = tokenizer(\"What is Python?\", add_special_tokens=False).input_ids\n",
    "test_a_ids = tokenizer(\"A programming language.\", add_special_tokens=False).input_ids\n",
    "print(f\"\\nSanity check:\")\n",
    "print(f\"  BOS_ID: {BOS_ID} (token: '{tokenizer.decode([BOS_ID])}')\")\n",
    "print(f\"  Query 'What is Python?' -> {len(test_q_ids)} tokens\")\n",
    "print(f\"  Answer 'A programming language.' -> {len(test_a_ids)} tokens\")\n",
    "print(f\"  Decoder input length: 1 + {len(test_q_ids)} + {len(test_a_ids)} = {1 + len(test_q_ids) + len(test_a_ids)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dab31991",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T17:18:27.885898Z",
     "iopub.status.busy": "2026-02-19T17:18:27.885290Z",
     "iopub.status.idle": "2026-02-19T17:18:30.773092Z",
     "shell.execute_reply": "2026-02-19T17:18:30.771982Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MS MARCO v1.1 validation...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total candidates: 1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 500 samples\n",
      "Mean passage words: 74\n",
      "Mean answer words: 14\n",
      "Mean query words: 6\n",
      "\n",
      "First sample:\n",
      "  Query:  what is the link between alveoli and capillaries...\n",
      "  Answer: Diffusion...\n",
      "  Passage (92w): Gas exchange in the lungs takes place between the blood in the capilla...\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Load MS MARCO data and generate surrogates\n",
    "from lib.data import count_words\n",
    "from datasets import load_dataset\n",
    "\n",
    "print(\"Loading MS MARCO v1.1 validation...\")\n",
    "ds = load_dataset(\"microsoft/ms_marco\", \"v1.1\", split=\"validation\")\n",
    "\n",
    "all_candidates = []\n",
    "for item in ds:\n",
    "    if len(all_candidates) >= 3 * N_SAMPLES:\n",
    "        break\n",
    "    passages = item.get('passages', {})\n",
    "    ptexts = passages.get('passage_text', [])\n",
    "    is_sel = passages.get('is_selected', [])\n",
    "    query = item.get('query', '')\n",
    "    answers = item.get('answers', [])\n",
    "    well_formed = item.get('wellFormedAnswers', [])\n",
    "    answer = None\n",
    "    if well_formed and len(well_formed) > 0 and well_formed[0] not in ('[]', ''):\n",
    "        answer = well_formed[0]\n",
    "    elif answers and len(answers) > 0 and answers[0] != 'No Answer Present.':\n",
    "        answer = answers[0]\n",
    "    if not answer:\n",
    "        continue\n",
    "    for pt, sel in zip(ptexts, is_sel):\n",
    "        wc = count_words(pt)\n",
    "        if sel == 1 and 30 <= wc <= 300:\n",
    "            all_candidates.append({\n",
    "                'passage': pt, 'query': query, 'answer': answer,\n",
    "                'word_count': wc,\n",
    "            })\n",
    "            break\n",
    "\n",
    "print(f\"Total candidates: {len(all_candidates)}\")\n",
    "np.random.seed(SEED)\n",
    "indices = np.random.permutation(len(all_candidates))\n",
    "samples = [all_candidates[i] for i in indices[:N_SAMPLES]]\n",
    "del ds, all_candidates\n",
    "gc.collect()\n",
    "\n",
    "# Generate surrogates for each sample\n",
    "for i, s in enumerate(samples):\n",
    "    s['surr_template'] = make_surrogate_template(s['passage'])\n",
    "    s['surr_doc'] = make_surrogate_from_doc(s['passage'])\n",
    "\n",
    "    # Random prefix: words from unrelated passage, matched to query word count\n",
    "    other_idx = (i + N_SAMPLES // 2) % len(samples)\n",
    "    other_words = samples[other_idx]['passage'].split()\n",
    "    query_word_count = len(s['query'].split())\n",
    "    s['random_prefix'] = \" \".join(other_words[:query_word_count])\n",
    "\n",
    "    # Count prefix tokens for each condition\n",
    "    s['n_prefix_oracle'] = count_prefix_tokens(s['query'], s['passage'])\n",
    "    s['n_prefix_template'] = count_prefix_tokens(s['surr_template'], s['passage'])\n",
    "    s['n_prefix_doc'] = count_prefix_tokens(s['surr_doc'], s['passage'])\n",
    "    s['n_prefix_random'] = count_prefix_tokens(s['random_prefix'], s['passage'])\n",
    "\n",
    "print(f\"Loaded {len(samples)} samples\")\n",
    "print(f\"Mean passage words: {np.mean([s['word_count'] for s in samples]):.0f}\")\n",
    "print(f\"Mean answer words: {np.mean([count_words(s['answer']) for s in samples]):.0f}\")\n",
    "print(f\"Mean query words: {np.mean([count_words(s['query']) for s in samples]):.0f}\")\n",
    "\n",
    "print(f\"\\nFirst sample:\")\n",
    "print(f\"  Query:  {samples[0]['query'][:70]}...\")\n",
    "print(f\"  Answer: {samples[0]['answer'][:70]}...\")\n",
    "print(f\"  Passage ({samples[0]['word_count']}w): {samples[0]['passage'][:70]}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e12229d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T17:18:30.777000Z",
     "iopub.status.busy": "2026-02-19T17:18:30.776221Z",
     "iopub.status.idle": "2026-02-19T17:18:31.745089Z",
     "shell.execute_reply": "2026-02-19T17:18:31.744135Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "EXAMPLE CONDITIONS (sample 0)\n",
      "======================================================================\n",
      "\n",
      "Query:          what is the link between alveoli and capillaries\n",
      "Answer:         Diffusion\n",
      "Passage:        Gas exchange in the lungs takes place between the blood in the capillary network surrounding the alv...\n",
      "Surr template:  What is alveoli?\n",
      "Surr doc kw:    alveoli gas partial pressure exchange\n",
      "Random prefix:  You are here Donair History. Donairs-in the past-are...\n",
      "\n",
      "  Condition                 Enc prefix                      Trunc  Dec query\n",
      "  ---------------------------------------------------------------------------\n",
      "  bare                      (none)                             no        yes\n",
      "  oracle_trunc              real query                        yes        yes\n",
      "  oracle_full               real query                         no        yes\n",
      "  surr_template_trunc       What is alveoli?                  yes        yes\n",
      "  surr_doc_trunc            alveoli gas partial pressure      yes        yes\n",
      "  random_trunc              You are here Donair History.      yes        yes\n",
      "  bare_nq                   (none)                             no         no\n",
      "  oracle_trunc_nq           real query                        yes         no\n",
      "\n",
      "Decoder input (query-prefix conditions):\n",
      "  [BOS] + query (9 tok) + answer (1 tok) = 11 tok total\n",
      "  NLL computed on last 1 positions (answer only)\n",
      "\n",
      "Decoder input (no-query conditions):\n",
      "  Model creates [BOS, a1, ..., a_{M-1}] internally from labels\n",
      "  NLL computed on all 1 answer token positions\n",
      "\n",
      "Sanity check: bare NLL with and without query in decoder...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  bare (with query in decoder):    9.625000\n",
      "  bare_nq (no query in decoder):   41.750000\n",
      "  Difference: +32.125000\n",
      "  (Expected: query prefix should lower NLL substantially)\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Show example conditions for sample 0\n",
    "print(\"=\" * 70)\n",
    "print(\"EXAMPLE CONDITIONS (sample 0)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "ex = samples[0]\n",
    "print(f\"\\nQuery:          {ex['query']}\")\n",
    "print(f\"Answer:         {ex['answer']}\")\n",
    "print(f\"Passage:        {ex['passage'][:100]}...\")\n",
    "print(f\"Surr template:  {ex['surr_template']}\")\n",
    "print(f\"Surr doc kw:    {ex['surr_doc']}\")\n",
    "print(f\"Random prefix:  {ex['random_prefix'][:60]}...\")\n",
    "\n",
    "print(f\"\\n  {'Condition':<25} {'Enc prefix':<30} {'Trunc':>6} {'Dec query':>10}\")\n",
    "print(f\"  {'-'*75}\")\n",
    "\n",
    "cond_display = [\n",
    "    ('bare',                '(none)',               'no',  'yes'),\n",
    "    ('oracle_trunc',        'real query',           'yes', 'yes'),\n",
    "    ('oracle_full',         'real query',           'no',  'yes'),\n",
    "    ('surr_template_trunc', ex['surr_template'],    'yes', 'yes'),\n",
    "    ('surr_doc_trunc',      ex['surr_doc'][:28],    'yes', 'yes'),\n",
    "    ('random_trunc',        ex['random_prefix'][:28], 'yes', 'yes'),\n",
    "    ('bare_nq',             '(none)',               'no',  'no'),\n",
    "    ('oracle_trunc_nq',     'real query',           'yes', 'no'),\n",
    "]\n",
    "\n",
    "for name, prefix, trunc, has_q in cond_display:\n",
    "    print(f\"  {name:<25} {prefix:<30} {trunc:>6} {has_q:>10}\")\n",
    "\n",
    "# Show decoder input structure\n",
    "q_ids = tokenizer(ex['query'], add_special_tokens=False).input_ids\n",
    "a_ids = tokenizer(ex['answer'], add_special_tokens=False).input_ids\n",
    "print(f\"\\nDecoder input (query-prefix conditions):\")\n",
    "print(f\"  [BOS] + query ({len(q_ids)} tok) + answer ({len(a_ids)} tok) \"\n",
    "      f\"= {1 + len(q_ids) + len(a_ids)} tok total\")\n",
    "print(f\"  NLL computed on last {len(a_ids)} positions (answer only)\")\n",
    "\n",
    "print(f\"\\nDecoder input (no-query conditions):\")\n",
    "print(f\"  Model creates [BOS, a1, ..., a_{{M-1}}] internally from labels\")\n",
    "print(f\"  NLL computed on all {len(a_ids)} answer token positions\")\n",
    "\n",
    "# Quick sanity: compare query-prefix NLL vs no-query NLL for bare\n",
    "print(f\"\\nSanity check: bare NLL with and without query in decoder...\")\n",
    "nll_bare_q = score_nll_query_prefix(ex['passage'], ex['query'], ex['answer'])\n",
    "nll_bare_nq = score_nll(ex['passage'], ex['answer'])\n",
    "print(f\"  bare (with query in decoder):    {nll_bare_q:.6f}\")\n",
    "print(f\"  bare_nq (no query in decoder):   {nll_bare_nq:.6f}\")\n",
    "print(f\"  Difference: {nll_bare_nq - nll_bare_q:+.6f}\")\n",
    "print(f\"  (Expected: query prefix should lower NLL substantially)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa3fa2b8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T17:18:31.748833Z",
     "iopub.status.busy": "2026-02-19T17:18:31.748535Z",
     "iopub.status.idle": "2026-02-19T17:32:42.411578Z",
     "shell.execute_reply": "2026-02-19T17:32:42.410914Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "SCORING ALL CONDITIONS\n",
      "======================================================================\n",
      "Starting fresh: 8 conditions x 500 samples = 4000 forward passes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3dff8d4550e419ba1768d98979c200a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Scoring:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 20/500 | 0.6m | ETA 13.5m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 40/500 | 1.1m | ETA 13.0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 60/500 | 1.7m | ETA 12.4m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 80/500 | 2.3m | ETA 11.8m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 100/500 | 2.8m | ETA 11.3m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 120/500 | 3.4m | ETA 10.7m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 140/500 | 3.9m | ETA 10.1m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 160/500 | 4.5m | ETA 9.6m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 180/500 | 5.1m | ETA 9.0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 200/500 | 5.6m | ETA 8.5m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 220/500 | 6.2m | ETA 7.9m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 240/500 | 6.8m | ETA 7.4m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 260/500 | 7.4m | ETA 6.8m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 280/500 | 7.9m | ETA 6.2m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 300/500 | 8.5m | ETA 5.7m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 320/500 | 9.1m | ETA 5.1m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 340/500 | 9.6m | ETA 4.5m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 360/500 | 10.2m | ETA 4.0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 380/500 | 10.8m | ETA 3.4m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 400/500 | 11.3m | ETA 2.8m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 420/500 | 11.9m | ETA 2.3m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 440/500 | 12.5m | ETA 1.7m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 460/500 | 13.0m | ETA 1.1m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 480/500 | 13.6m | ETA 0.6m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 500/500 | 14.2m | ETA 0.0m\n",
      "\n",
      "Scoring complete: 500 samples, 8 conditions in 14.2 min\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Scoring loop — 8 conditions x 500 samples\n",
    "print(\"=\" * 70)\n",
    "print(\"SCORING ALL CONDITIONS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "COND_NAMES = [\n",
    "    'bare', 'oracle_trunc', 'oracle_full',\n",
    "    'surr_template_trunc', 'surr_doc_trunc', 'random_trunc',\n",
    "    'bare_nq', 'oracle_trunc_nq',\n",
    "]\n",
    "\n",
    "results = []\n",
    "start_idx = 0\n",
    "\n",
    "if CHECKPOINT_PATH.exists():\n",
    "    ckpt = json.loads(CHECKPOINT_PATH.read_text())\n",
    "    if ckpt.get('n_total') == N_SAMPLES and len(ckpt.get('results', [])) > 0:\n",
    "        saved_queries = [r['query'][:50] for r in ckpt['results']]\n",
    "        current_queries = [s['query'][:50] for s in samples[:len(saved_queries)]]\n",
    "        if saved_queries == current_queries:\n",
    "            results = ckpt['results']\n",
    "            start_idx = len(results)\n",
    "            print(f\"Resuming from checkpoint: {start_idx}/{N_SAMPLES}\")\n",
    "\n",
    "if start_idx == 0:\n",
    "    print(f\"Starting fresh: {len(COND_NAMES)} conditions x {N_SAMPLES} samples \"\n",
    "          f\"= {len(COND_NAMES) * N_SAMPLES} forward passes\")\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "for i in tqdm(range(start_idx, N_SAMPLES), initial=start_idx, total=N_SAMPLES,\n",
    "              desc=\"Scoring\"):\n",
    "    s = samples[i]\n",
    "    query = s['query']\n",
    "    passage = s['passage']\n",
    "    answer = s['answer']\n",
    "\n",
    "    result = {\n",
    "        'query': query,\n",
    "        'answer': answer,\n",
    "        'passage_words': s['word_count'],\n",
    "    }\n",
    "\n",
    "    # --- Conditions 1-6: query in decoder (production-realistic) ---\n",
    "\n",
    "    # 1. bare: encoder=[doc], cross-attn=all, decoder=[query]->answer\n",
    "    result['nll_bare'] = score_nll_query_prefix(\n",
    "        passage, query, answer)\n",
    "\n",
    "    # 2. oracle_trunc: encoder=[query+doc], cross-attn=doc only, decoder=[query]->answer\n",
    "    result['nll_oracle_trunc'] = score_nll_query_prefix(\n",
    "        query + \"\\n\" + passage, query, answer,\n",
    "        prefix_token_count=s['n_prefix_oracle'], truncate=True)\n",
    "\n",
    "    # 3. oracle_full: encoder=[query+doc], cross-attn=all, decoder=[query]->answer\n",
    "    result['nll_oracle_full'] = score_nll_query_prefix(\n",
    "        query + \"\\n\" + passage, query, answer)\n",
    "\n",
    "    # 4. surr_template_trunc: encoder=[\"What is [kw]?\"+doc], decoder=[query]->answer\n",
    "    result['nll_surr_template_trunc'] = score_nll_query_prefix(\n",
    "        s['surr_template'] + \"\\n\" + passage, query, answer,\n",
    "        prefix_token_count=s['n_prefix_template'], truncate=True)\n",
    "\n",
    "    # 5. surr_doc_trunc: encoder=[top5kw+doc], decoder=[query]->answer\n",
    "    result['nll_surr_doc_trunc'] = score_nll_query_prefix(\n",
    "        s['surr_doc'] + \"\\n\" + passage, query, answer,\n",
    "        prefix_token_count=s['n_prefix_doc'], truncate=True)\n",
    "\n",
    "    # 6. random_trunc: encoder=[random+doc], decoder=[query]->answer\n",
    "    result['nll_random_trunc'] = score_nll_query_prefix(\n",
    "        s['random_prefix'] + \"\\n\" + passage, query, answer,\n",
    "        prefix_token_count=s['n_prefix_random'], truncate=True)\n",
    "\n",
    "    # --- Conditions 7-8: no query in decoder (replicates v3 Exp 01) ---\n",
    "\n",
    "    # 7. bare_nq: encoder=[doc], decoder=answer only\n",
    "    result['nll_bare_nq'] = score_nll(passage, answer)\n",
    "\n",
    "    # 8. oracle_trunc_nq: encoder=[query+doc], decoder=answer only (masked)\n",
    "    result['nll_oracle_trunc_nq'] = score_nll(\n",
    "        query + \"\\n\" + passage, answer,\n",
    "        prefix_token_count=s['n_prefix_oracle'], truncate=True)\n",
    "\n",
    "    results.append(result)\n",
    "\n",
    "    if (i + 1) % 20 == 0 or i == N_SAMPLES - 1:\n",
    "        ckpt = {\n",
    "            'n_total': N_SAMPLES,\n",
    "            'results': results,\n",
    "            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        }\n",
    "        CHECKPOINT_PATH.write_text(json.dumps(ckpt))\n",
    "        elapsed = time.time() - t0\n",
    "        done = i - start_idx + 1\n",
    "        eta = (N_SAMPLES - i - 1) * elapsed / done if done > 0 else 0\n",
    "        tqdm.write(f\"  Checkpoint {i+1}/{N_SAMPLES} | {elapsed/60:.1f}m | ETA {eta/60:.1f}m\")\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "elapsed = time.time() - t0\n",
    "print(f\"\\nScoring complete: {len(results)} samples, \"\n",
    "      f\"{len(COND_NAMES)} conditions in {elapsed/60:.1f} min\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52c966c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T17:32:42.415203Z",
     "iopub.status.busy": "2026-02-19T17:32:42.414570Z",
     "iopub.status.idle": "2026-02-19T17:32:42.435363Z",
     "shell.execute_reply": "2026-02-19T17:32:42.434741Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "RESULTS (N=500)\n",
      "======================================================================\n",
      "\n",
      "--- Query in decoder (production-realistic) ---\n",
      "  Baseline: bare (decoder has query, encoder has document only)\n",
      "\n",
      "  Condition                      NLL    vs bare        d     Win%            p   sig\n",
      "  ------------------------------------------------------------------------------\n",
      "  bare                        2.5544         --       --       --           --    --\n",
      "  oracle_trunc                2.4061    +0.1482   +0.228    67.8%     5.15e-07   ***\n",
      "  oracle_full                 2.2597    +0.2946   +0.167    67.4%     2.19e-04   ***\n",
      "  surr_template_trunc         2.5929    -0.0385   -0.069    53.0%     1.22e-01    ns\n",
      "  surr_doc_trunc              2.4467    +0.1076   +0.148    61.6%     1.03e-03    **\n",
      "  random_trunc                2.4899    +0.0644   +0.080    54.6%     7.31e-02    ns\n",
      "\n",
      "--- No query in decoder (v3 Exp 01 replication) ---\n",
      "  Baseline: bare_nq (decoder has answer only)\n",
      "\n",
      "  Condition                      NLL vs bare_nq        d     Win%            p   sig\n",
      "  ------------------------------------------------------------------------------\n",
      "  bare_nq                     3.6765         --       --       --           --    --\n",
      "  oracle_trunc_nq             2.9929    +0.6836   +0.376    92.6%     4.79e-16   ***\n",
      "\n",
      "Expected: oracle_trunc_nq vs bare_nq should replicate d~+0.4 from v3 Exp 01\n",
      "Actual:   d=+0.376 (***)\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Results table — all conditions\n",
    "print(\"=\" * 70)\n",
    "print(f\"RESULTS (N={len(results)})\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Extract NLL arrays — query-prefix conditions\n",
    "bare = np.array([r['nll_bare'] for r in results])\n",
    "oracle_trunc = np.array([r['nll_oracle_trunc'] for r in results])\n",
    "oracle_full = np.array([r['nll_oracle_full'] for r in results])\n",
    "surr_template = np.array([r['nll_surr_template_trunc'] for r in results])\n",
    "surr_doc = np.array([r['nll_surr_doc_trunc'] for r in results])\n",
    "random_trunc = np.array([r['nll_random_trunc'] for r in results])\n",
    "\n",
    "# No-query conditions\n",
    "bare_nq = np.array([r['nll_bare_nq'] for r in results])\n",
    "oracle_trunc_nq = np.array([r['nll_oracle_trunc_nq'] for r in results])\n",
    "\n",
    "print(f\"\\n--- Query in decoder (production-realistic) ---\")\n",
    "print(f\"  Baseline: bare (decoder has query, encoder has document only)\")\n",
    "print(f\"\\n  {'Condition':<25} {'NLL':>8} {'vs bare':>10} {'d':>8} {'Win%':>8} {'p':>12} {'sig':>5}\")\n",
    "print(f\"  {'-'*78}\")\n",
    "\n",
    "analysis = {}\n",
    "for name, nlls in [\n",
    "    ('bare', bare),\n",
    "    ('oracle_trunc', oracle_trunc),\n",
    "    ('oracle_full', oracle_full),\n",
    "    ('surr_template_trunc', surr_template),\n",
    "    ('surr_doc_trunc', surr_doc),\n",
    "    ('random_trunc', random_trunc),\n",
    "]:\n",
    "    mean_nll = nlls.mean()\n",
    "    if name == 'bare':\n",
    "        print(f\"  {name:<25} {mean_nll:>8.4f} {'--':>10} {'--':>8} {'--':>8} {'--':>12} {'--':>5}\")\n",
    "        analysis[name] = {'mean_nll': float(mean_nll)}\n",
    "    else:\n",
    "        diff = bare - nlls  # positive = condition is better (lower NLL)\n",
    "        d = cohens_d(diff)\n",
    "        win_pct = 100 * np.mean(diff > 0)\n",
    "        _, p_val = stats.ttest_1samp(diff, 0)\n",
    "        sig = '***' if p_val < 0.001 else '**' if p_val < 0.01 else '*' if p_val < 0.05 else 'ns'\n",
    "        print(f\"  {name:<25} {mean_nll:>8.4f} {diff.mean():>+10.4f} {d:>+8.3f} {win_pct:>7.1f}% {p_val:>12.2e} {sig:>5}\")\n",
    "        analysis[name] = {\n",
    "            'mean_nll': float(mean_nll), 'delta': float(diff.mean()),\n",
    "            'd': float(d), 'win_pct': float(win_pct), 'p': float(p_val),\n",
    "        }\n",
    "\n",
    "print(f\"\\n--- No query in decoder (v3 Exp 01 replication) ---\")\n",
    "print(f\"  Baseline: bare_nq (decoder has answer only)\")\n",
    "print(f\"\\n  {'Condition':<25} {'NLL':>8} {'vs bare_nq':>10} {'d':>8} {'Win%':>8} {'p':>12} {'sig':>5}\")\n",
    "print(f\"  {'-'*78}\")\n",
    "\n",
    "diff_nq = bare_nq - oracle_trunc_nq\n",
    "d_nq = cohens_d(diff_nq)\n",
    "win_nq = 100 * np.mean(diff_nq > 0)\n",
    "_, p_nq = stats.ttest_1samp(diff_nq, 0)\n",
    "sig_nq = '***' if p_nq < 0.001 else '**' if p_nq < 0.01 else '*' if p_nq < 0.05 else 'ns'\n",
    "\n",
    "print(f\"  {'bare_nq':<25} {bare_nq.mean():>8.4f} {'--':>10} {'--':>8} {'--':>8} {'--':>12} {'--':>5}\")\n",
    "print(f\"  {'oracle_trunc_nq':<25} {oracle_trunc_nq.mean():>8.4f} {diff_nq.mean():>+10.4f} {d_nq:>+8.3f} {win_nq:>7.1f}% {p_nq:>12.2e} {sig_nq:>5}\")\n",
    "\n",
    "analysis['bare_nq'] = {'mean_nll': float(bare_nq.mean())}\n",
    "analysis['oracle_trunc_nq'] = {\n",
    "    'mean_nll': float(oracle_trunc_nq.mean()), 'delta': float(diff_nq.mean()),\n",
    "    'd': float(d_nq), 'win_pct': float(win_nq), 'p': float(p_nq),\n",
    "}\n",
    "\n",
    "print(f\"\\nExpected: oracle_trunc_nq vs bare_nq should replicate d~+0.4 from v3 Exp 01\")\n",
    "print(f\"Actual:   d={d_nq:+.3f} ({sig_nq})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "697e1da1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T17:32:42.438275Z",
     "iopub.status.busy": "2026-02-19T17:32:42.438019Z",
     "iopub.status.idle": "2026-02-19T17:32:42.458899Z",
     "shell.execute_reply": "2026-02-19T17:32:42.458267Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "KEY COMPARISON: Is enrichment redundant when decoder has the query?\n",
      "======================================================================\n",
      "\n",
      "  Enrichment benefit (oracle_trunc vs respective bare):\n",
      "    With query in decoder:    d=+0.2275 (***), mean delta=+0.1482\n",
      "    Without query in decoder: d=+0.3755 (***), mean delta=+0.6836\n",
      "\n",
      "  Ratio: with_q / without_q = 60.6%\n",
      "  Difference of differences: d=-0.3313 (***)\n",
      "\n",
      "  Interpretation:\n",
      "    ENRICHMENT STILL HELPS even when decoder has the query.\n",
      "    The approach has genuine production value.\n",
      "    Enrichment benefit is 61% of the no-query benefit — partially redundant.\n",
      "\n",
      "--- Full vs truncated cross-attention (with query in decoder) ---\n",
      "  oracle_trunc NLL: 2.4061\n",
      "  oracle_full NLL:  2.2597\n",
      "  full vs trunc: d=+0.0819 (ns)\n",
      "  -> Truncation is actually BETTER — same pattern as v3.\n",
      "\n",
      "--- Surrogate benefit with query in decoder ---\n",
      "  surr_template_trunc       d=-0.0693 (ns), -30% of oracle\n",
      "  surr_doc_trunc            d=+0.1477 (**), 65% of oracle\n",
      "  random_trunc              d=+0.0803 (ns), 35% of oracle\n",
      "\n",
      "Per-sample correlation (enrichment with q vs without q): r=0.475 (p=1.60e-29)\n",
      "  (High r = same samples benefit from enrichment regardless of decoder query)\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Key comparison — enrichment with query vs without query\n",
    "print(\"=\" * 70)\n",
    "print(\"KEY COMPARISON: Is enrichment redundant when decoder has the query?\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Enrichment benefit WITH query in decoder\n",
    "enrichment_with_q = bare - oracle_trunc  # positive = enrichment helps\n",
    "d_with_q = cohens_d(enrichment_with_q)\n",
    "_, p_with_q = stats.ttest_1samp(enrichment_with_q, 0)\n",
    "sig_with_q = '***' if p_with_q < 0.001 else '**' if p_with_q < 0.01 else '*' if p_with_q < 0.05 else 'ns'\n",
    "\n",
    "# Enrichment benefit WITHOUT query in decoder (v3 replication)\n",
    "enrichment_no_q = bare_nq - oracle_trunc_nq  # positive = enrichment helps\n",
    "d_no_q = cohens_d(enrichment_no_q)\n",
    "_, p_no_q = stats.ttest_1samp(enrichment_no_q, 0)\n",
    "sig_no_q = '***' if p_no_q < 0.001 else '**' if p_no_q < 0.01 else '*' if p_no_q < 0.05 else 'ns'\n",
    "\n",
    "print(f\"\\n  Enrichment benefit (oracle_trunc vs respective bare):\")\n",
    "print(f\"    With query in decoder:    d={d_with_q:+.4f} ({sig_with_q}), \"\n",
    "      f\"mean delta={enrichment_with_q.mean():+.4f}\")\n",
    "print(f\"    Without query in decoder: d={d_no_q:+.4f} ({sig_no_q}), \"\n",
    "      f\"mean delta={enrichment_no_q.mean():+.4f}\")\n",
    "\n",
    "# Are they different?\n",
    "diff_of_diffs = enrichment_with_q - enrichment_no_q\n",
    "d_diff = cohens_d(diff_of_diffs)\n",
    "_, p_diff = stats.ttest_1samp(diff_of_diffs, 0)\n",
    "sig_diff = '***' if p_diff < 0.001 else '**' if p_diff < 0.01 else '*' if p_diff < 0.05 else 'ns'\n",
    "\n",
    "# Ratio\n",
    "if d_no_q > 0:\n",
    "    ratio = d_with_q / d_no_q * 100\n",
    "else:\n",
    "    ratio = float('inf')\n",
    "\n",
    "print(f\"\\n  Ratio: with_q / without_q = {ratio:.1f}%\")\n",
    "print(f\"  Difference of differences: d={d_diff:+.4f} ({sig_diff})\")\n",
    "\n",
    "print(f\"\\n  Interpretation:\")\n",
    "if d_with_q > 0.1 and ratio > 50:\n",
    "    print(f\"    ENRICHMENT STILL HELPS even when decoder has the query.\")\n",
    "    print(f\"    The approach has genuine production value.\")\n",
    "    if ratio > 80:\n",
    "        print(f\"    Enrichment benefit is {ratio:.0f}% of the no-query benefit — mostly preserved.\")\n",
    "    else:\n",
    "        print(f\"    Enrichment benefit is {ratio:.0f}% of the no-query benefit — partially redundant.\")\n",
    "elif d_with_q > 0.05:\n",
    "    print(f\"    WEAK enrichment benefit with query. The decoder knowing the query\")\n",
    "    print(f\"    makes most of the enrichment redundant.\")\n",
    "else:\n",
    "    print(f\"    ENRICHMENT IS REDUNDANT when the decoder has the query.\")\n",
    "    print(f\"    The whole approach has no production value.\")\n",
    "\n",
    "# Full cross-attention vs truncation (with query)\n",
    "print(f\"\\n--- Full vs truncated cross-attention (with query in decoder) ---\")\n",
    "diff_full_trunc = oracle_trunc - oracle_full  # negative = full is better\n",
    "d_ft = cohens_d(diff_full_trunc)\n",
    "_, p_ft = stats.ttest_1samp(diff_full_trunc, 0)\n",
    "sig_ft = '***' if p_ft < 0.001 else '**' if p_ft < 0.01 else '*' if p_ft < 0.05 else 'ns'\n",
    "\n",
    "print(f\"  oracle_trunc NLL: {oracle_trunc.mean():.4f}\")\n",
    "print(f\"  oracle_full NLL:  {oracle_full.mean():.4f}\")\n",
    "print(f\"  full vs trunc: d={d_ft:+.4f} ({sig_ft})\")\n",
    "if abs(d_ft) < 0.05:\n",
    "    print(f\"  -> Full cross-attention adds minimal value beyond enriched doc reps.\")\n",
    "elif d_ft < -0.05:\n",
    "    print(f\"  -> Full cross-attention substantially better — decoder benefits from\")\n",
    "    print(f\"     reading query directly from encoder output as well.\")\n",
    "else:\n",
    "    print(f\"  -> Truncation is actually BETTER — same pattern as v3.\")\n",
    "\n",
    "# Surrogate conditions vs bare (with query)\n",
    "print(f\"\\n--- Surrogate benefit with query in decoder ---\")\n",
    "for name, nlls in [('surr_template_trunc', surr_template),\n",
    "                     ('surr_doc_trunc', surr_doc),\n",
    "                     ('random_trunc', random_trunc)]:\n",
    "    diff = bare - nlls\n",
    "    d = cohens_d(diff)\n",
    "    _, p = stats.ttest_1samp(diff, 0)\n",
    "    sig = '***' if p < 0.001 else '**' if p < 0.01 else '*' if p < 0.05 else 'ns'\n",
    "    if d_with_q > 0:\n",
    "        pct_oracle = d / d_with_q * 100\n",
    "    else:\n",
    "        pct_oracle = 0\n",
    "    print(f\"  {name:<25} d={d:+.4f} ({sig}), {pct_oracle:.0f}% of oracle\")\n",
    "\n",
    "# Per-sample correlation: does the same enrichment help the same samples?\n",
    "r_corr, p_corr = stats.pearsonr(enrichment_with_q, enrichment_no_q)\n",
    "print(f\"\\nPer-sample correlation (enrichment with q vs without q): r={r_corr:.3f} (p={p_corr:.2e})\")\n",
    "print(f\"  (High r = same samples benefit from enrichment regardless of decoder query)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a889beb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T17:32:42.462001Z",
     "iopub.status.busy": "2026-02-19T17:32:42.461444Z",
     "iopub.status.idle": "2026-02-19T17:32:42.474964Z",
     "shell.execute_reply": "2026-02-19T17:32:42.474298Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "HARDNESS GRADIENT\n",
      "======================================================================\n",
      "\n",
      "--- Enrichment benefit by hardness (with query in decoder) ---\n",
      "  Quintile        N     bare  orc_trunc    delta        d\n",
      "  -------------------------------------------------------\n",
      "  Q1 easy       100   0.3837     0.3628  +0.0209   +0.372\n",
      "  Q2            100   0.7962     0.7607  +0.0355   +0.380\n",
      "  Q3            100   1.3904     1.3138  +0.0766   +0.523\n",
      "  Q4            100   2.3363     2.1709  +0.1654   +0.520\n",
      "  Q5 hard       100   7.8653     7.4223  +0.4430   +0.323\n",
      "\n",
      "--- Enrichment benefit by hardness (no query in decoder) ---\n",
      "  Quintile        N  bare_nq  orc_tr_nq    delta        d\n",
      "  -------------------------------------------------------\n",
      "  Q1 easy       100   0.4870     0.4126  +0.0744   +1.027\n",
      "  Q2             99   1.0749     0.9169  +0.1580   +1.213\n",
      "  Q3             98   1.9053     1.6005  +0.3048   +1.518\n",
      "  Q4            103   3.0551     2.5792  +0.4759   +1.219\n",
      "  Q5 hard       100  11.8172     9.4189  +2.3983   +0.674\n",
      "\n",
      "Spearman correlation (hardness vs enrichment with query): rho=0.313 (p=8.27e-13)\n",
      "Spearman correlation (hardness vs enrichment no query):   rho=0.761 (p=1.00e-95)\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Hardness gradient — does enrichment benefit vary with difficulty?\n",
    "print(\"=\" * 70)\n",
    "print(\"HARDNESS GRADIENT\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Use bare NLL as hardness proxy\n",
    "quintile_bounds = np.percentile(bare, [20, 40, 60, 80])\n",
    "quintiles = np.digitize(bare, quintile_bounds)\n",
    "\n",
    "print(f\"\\n--- Enrichment benefit by hardness (with query in decoder) ---\")\n",
    "print(f\"  {'Quintile':<12} {'N':>4} {'bare':>8} {'orc_trunc':>10} {'delta':>8} {'d':>8}\")\n",
    "print(f\"  {'-'*55}\")\n",
    "\n",
    "for q in range(5):\n",
    "    mask = quintiles == q\n",
    "    n_q = mask.sum()\n",
    "    if n_q < 5:\n",
    "        continue\n",
    "    qlabel = ['Q1 easy', 'Q2', 'Q3', 'Q4', 'Q5 hard'][q]\n",
    "    b = bare[mask].mean()\n",
    "    ot = oracle_trunc[mask].mean()\n",
    "    delta = (bare[mask] - oracle_trunc[mask]).mean()\n",
    "    d = cohens_d(bare[mask] - oracle_trunc[mask])\n",
    "    print(f\"  {qlabel:<12} {n_q:>4} {b:>8.4f} {ot:>10.4f} {delta:>+8.4f} {d:>+8.3f}\")\n",
    "\n",
    "# Same for no-query\n",
    "print(f\"\\n--- Enrichment benefit by hardness (no query in decoder) ---\")\n",
    "quintile_bounds_nq = np.percentile(bare_nq, [20, 40, 60, 80])\n",
    "quintiles_nq = np.digitize(bare_nq, quintile_bounds_nq)\n",
    "\n",
    "print(f\"  {'Quintile':<12} {'N':>4} {'bare_nq':>8} {'orc_tr_nq':>10} {'delta':>8} {'d':>8}\")\n",
    "print(f\"  {'-'*55}\")\n",
    "\n",
    "for q in range(5):\n",
    "    mask = quintiles_nq == q\n",
    "    n_q = mask.sum()\n",
    "    if n_q < 5:\n",
    "        continue\n",
    "    qlabel = ['Q1 easy', 'Q2', 'Q3', 'Q4', 'Q5 hard'][q]\n",
    "    b = bare_nq[mask].mean()\n",
    "    ot = oracle_trunc_nq[mask].mean()\n",
    "    delta = (bare_nq[mask] - oracle_trunc_nq[mask]).mean()\n",
    "    d = cohens_d(bare_nq[mask] - oracle_trunc_nq[mask])\n",
    "    print(f\"  {qlabel:<12} {n_q:>4} {b:>8.4f} {ot:>10.4f} {delta:>+8.4f} {d:>+8.3f}\")\n",
    "\n",
    "# Spearman correlations with hardness\n",
    "r_hard, p_hard = stats.spearmanr(bare, bare - oracle_trunc)\n",
    "print(f\"\\nSpearman correlation (hardness vs enrichment with query): \"\n",
    "      f\"rho={r_hard:.3f} (p={p_hard:.2e})\")\n",
    "r_hard_nq, p_hard_nq = stats.spearmanr(bare_nq, bare_nq - oracle_trunc_nq)\n",
    "print(f\"Spearman correlation (hardness vs enrichment no query):   \"\n",
    "      f\"rho={r_hard_nq:.3f} (p={p_hard_nq:.2e})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "92fd1701",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T17:32:42.478302Z",
     "iopub.status.busy": "2026-02-19T17:32:42.477693Z",
     "iopub.status.idle": "2026-02-19T17:32:42.995360Z",
     "shell.execute_reply": "2026-02-19T17:32:42.994657Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "VERDICT -- Exp 01: Production-Realistic KV Cache\n",
      "======================================================================\n",
      "\n",
      "Model: google/t5gemma-2-4b-4b\n",
      "N: 500 samples (MS MARCO v1.1)\n",
      "\n",
      "--- THE key result ---\n",
      "  Enrichment without query in decoder (v3 baseline): d=+0.376\n",
      "  Enrichment WITH query in decoder (production):     d=+0.228\n",
      "  Ratio: 61%\n",
      "\n",
      "  CONCLUSION: Encoder enrichment provides genuine value\n",
      "  even in production where the decoder already has the query.\n",
      "  About 61% of the enrichment benefit survives.\n",
      "\n",
      "--- Surrogate summary (with query in decoder) ---\n",
      "  surr_template_trunc       d=-0.0693 (ns)\n",
      "  surr_doc_trunc            d=+0.1477 (**)\n",
      "  random_trunc              d=+0.0803 (ns)\n",
      "\n",
      "Results saved to ../../results/exp01/results.json\n",
      "\n",
      "Cleaning up GPU memory...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory: 15.03 GB -> 0.01 GB\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Cell 10: Verdict and save results\n",
    "print(\"=\" * 70)\n",
    "print(\"VERDICT -- Exp 01: Production-Realistic KV Cache\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "d_with_q = cohens_d(bare - oracle_trunc)\n",
    "d_no_q = cohens_d(bare_nq - oracle_trunc_nq)\n",
    "ratio = d_with_q / d_no_q * 100 if d_no_q > 0 else float('inf')\n",
    "\n",
    "print(f\"\\nModel: {MODEL_NAME}\")\n",
    "print(f\"N: {len(results)} samples (MS MARCO v1.1)\")\n",
    "\n",
    "print(f\"\\n--- THE key result ---\")\n",
    "print(f\"  Enrichment without query in decoder (v3 baseline): d={d_no_q:+.3f}\")\n",
    "print(f\"  Enrichment WITH query in decoder (production):     d={d_with_q:+.3f}\")\n",
    "print(f\"  Ratio: {ratio:.0f}%\")\n",
    "\n",
    "if d_with_q > 0.1:\n",
    "    print(f\"\\n  CONCLUSION: Encoder enrichment provides genuine value\")\n",
    "    print(f\"  even in production where the decoder already has the query.\")\n",
    "    if ratio > 80:\n",
    "        print(f\"  The enrichment benefit is almost fully preserved ({ratio:.0f}%).\")\n",
    "    elif ratio > 50:\n",
    "        print(f\"  About {ratio:.0f}% of the enrichment benefit survives.\")\n",
    "    else:\n",
    "        print(f\"  Only {ratio:.0f}% survives -- most benefit was query-reading.\")\n",
    "elif d_with_q > 0.05:\n",
    "    print(f\"\\n  CONCLUSION: Marginal benefit. The decoder knowing the query\")\n",
    "    print(f\"  makes most enrichment redundant. Production value is limited.\")\n",
    "else:\n",
    "    print(f\"\\n  CONCLUSION: No production value. The enrichment benefit\")\n",
    "    print(f\"  vanishes when the decoder already has the query.\")\n",
    "    print(f\"  The v3 benefit was primarily from the decoder reading the query\")\n",
    "    print(f\"  from encoder output, not from improved document representations.\")\n",
    "\n",
    "# Surrogate summary\n",
    "print(f\"\\n--- Surrogate summary (with query in decoder) ---\")\n",
    "for name in ['surr_template_trunc', 'surr_doc_trunc', 'random_trunc']:\n",
    "    nlls = np.array([r[f'nll_{name}'] for r in results])\n",
    "    d = cohens_d(bare - nlls)\n",
    "    _, p = stats.ttest_1samp(bare - nlls, 0)\n",
    "    sig = '***' if p < 0.001 else '**' if p < 0.01 else '*' if p < 0.05 else 'ns'\n",
    "    print(f\"  {name:<25} d={d:+.4f} ({sig})\")\n",
    "\n",
    "# Save\n",
    "final_results = {\n",
    "    'experiment': 'v4_exp01_production_kv_cache',\n",
    "    'model': MODEL_NAME,\n",
    "    'dataset': 'ms_marco_v1.1',\n",
    "    'n_samples': len(results),\n",
    "    'seed': SEED,\n",
    "    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'key_result': {\n",
    "        'enrichment_with_query_d': float(d_with_q),\n",
    "        'enrichment_no_query_d': float(d_no_q),\n",
    "        'ratio_pct': float(ratio),\n",
    "    },\n",
    "    'conditions': {k: v for k, v in analysis.items()},\n",
    "}\n",
    "\n",
    "with open(RESULTS_DIR / 'results.json', 'w') as f:\n",
    "    json.dump(final_results, f, indent=2)\n",
    "print(f\"\\nResults saved to {RESULTS_DIR / 'results.json'}\")\n",
    "\n",
    "# Cleanup\n",
    "print(f\"\\nCleaning up GPU memory...\")\n",
    "mem_before = torch.cuda.memory_allocated() / 1e9\n",
    "del model, processor, tokenizer\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "mem_after = torch.cuda.memory_allocated() / 1e9\n",
    "print(f\"GPU memory: {mem_before:.2f} GB -> {mem_after:.2f} GB\")\n",
    "print(\"Done!\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0129e7b56e3d45efb0e963bd524127b4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_805aba6251564a5cb09f5979ebea2388",
       "max": 1327.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_43bfd10c756248c6926f500425ac437e",
       "tabbable": null,
       "tooltip": null,
       "value": 1327.0
      }
     },
     "1c1f1637dabd461fb1dace92c98224a6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3b40546c6a594236afee7cf484ce14ea": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "43bfd10c756248c6926f500425ac437e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "4ad0491391034af1bedabf23630f652d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "4d93be9ffa814da9b2996eb58158532a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_76d9165bdbeb498792e1bbce93c3e5fc",
       "placeholder": "​",
       "style": "IPY_MODEL_f22264ecba5048da9a73e8c6eab32150",
       "tabbable": null,
       "tooltip": null,
       "value": "Loading weights: 100%"
      }
     },
     "5ce4f858df5f42288c6e7f6b88796ba9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_61bcab1c6c974c64ad073b71e44f14ae",
       "placeholder": "​",
       "style": "IPY_MODEL_3b40546c6a594236afee7cf484ce14ea",
       "tabbable": null,
       "tooltip": null,
       "value": "Scoring: 100%"
      }
     },
     "615081bc94d94f60bb00ae4bae78f9a1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "61bcab1c6c974c64ad073b71e44f14ae": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6404fb4edcf94710acf6340af265750e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "76d9165bdbeb498792e1bbce93c3e5fc": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "805aba6251564a5cb09f5979ebea2388": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "8a6d91938a2e40909c8a16adfbdc8ad0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_6404fb4edcf94710acf6340af265750e",
       "max": 500.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_4ad0491391034af1bedabf23630f652d",
       "tabbable": null,
       "tooltip": null,
       "value": 500.0
      }
     },
     "a3dff8d4550e419ba1768d98979c200a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_5ce4f858df5f42288c6e7f6b88796ba9",
        "IPY_MODEL_8a6d91938a2e40909c8a16adfbdc8ad0",
        "IPY_MODEL_b2ba56f58f9d49269782d2fbbd673cea"
       ],
       "layout": "IPY_MODEL_d2b2011440424de18abd0df08f5696a2",
       "tabbable": null,
       "tooltip": null
      }
     },
     "b2ba56f58f9d49269782d2fbbd673cea": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_1c1f1637dabd461fb1dace92c98224a6",
       "placeholder": "​",
       "style": "IPY_MODEL_c9f61b7703c84f9aa8fc9c87707f6fdf",
       "tabbable": null,
       "tooltip": null,
       "value": " 500/500 [14:10&lt;00:00,  1.70s/it]"
      }
     },
     "b32fb151466e4bffad1de28be8212fc8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "c9f61b7703c84f9aa8fc9c87707f6fdf": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "d2b2011440424de18abd0df08f5696a2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e1ec83b1fdac49358b5baa6a9ef2601f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_4d93be9ffa814da9b2996eb58158532a",
        "IPY_MODEL_0129e7b56e3d45efb0e963bd524127b4",
        "IPY_MODEL_f062d8c1ffae455487ad2f68152e1680"
       ],
       "layout": "IPY_MODEL_eba7deacd62e41f5b01d897a0682fcac",
       "tabbable": null,
       "tooltip": null
      }
     },
     "eba7deacd62e41f5b01d897a0682fcac": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f062d8c1ffae455487ad2f68152e1680": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_615081bc94d94f60bb00ae4bae78f9a1",
       "placeholder": "​",
       "style": "IPY_MODEL_b32fb151466e4bffad1de28be8212fc8",
       "tabbable": null,
       "tooltip": null,
       "value": " 1327/1327 [00:04&lt;00:00, 689.61it/s, Materializing param=model.encoder.vision_tower.vision_model.post_layernorm.weight]"
      }
     },
     "f22264ecba5048da9a73e8c6eab32150": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
