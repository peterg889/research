{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2126d81d",
   "metadata": {},
   "source": [
    "# Experiment 01: Production-Realistic KV Cache\n",
    "\n",
    "## Motivation\n",
    "\n",
    "v3 Experiment 01 proved that co-encoding [surrogate + document] enriches document\n",
    "representations (d=+0.408). But the decoder never saw the query — it only scored\n",
    "answer NLL from encoder states alone. This doesn't model any real production system.\n",
    "\n",
    "In production:\n",
    "1. **Offline**: Encode [surrogate + document] → cache encoder hidden states\n",
    "2. **Online**: Query arrives → decoder receives query as input, cross-attends to cached encoder states\n",
    "\n",
    "**The key question**: Does surrogate-enriched encoder caching still help when the\n",
    "decoder already has the query? If the decoder knowing the query makes enrichment\n",
    "redundant, the approach has no production value.\n",
    "\n",
    "## Method\n",
    "\n",
    "`T5Gemma2ForConditionalGeneration.forward()` accepts explicit `decoder_input_ids`\n",
    "alongside `encoder_outputs`. We build `decoder_input_ids = [BOS] + query_tokens + answer_tokens`\n",
    "and compute NLL only on the answer token positions.\n",
    "\n",
    "## Conditions (8 total)\n",
    "\n",
    "### With query in decoder (production-realistic):\n",
    "\n",
    "| # | Condition | Encoder input | Cross-attn | Decoder input |\n",
    "|---|-----------|--------------|------------|---------------|\n",
    "| 1 | bare | [document] | all | [query] → answer |\n",
    "| 2 | oracle_trunc | [query + document] | doc only | [query] → answer |\n",
    "| 3 | oracle_full | [query + document] | all | [query] → answer |\n",
    "| 4 | surr_template_trunc | [\"What is [kw]?\" + doc] | doc only | [query] → answer |\n",
    "| 5 | surr_doc_trunc | [top-5 kw + document] | doc only | [query] → answer |\n",
    "| 6 | random_trunc | [random words + doc] | doc only | [query] → answer |\n",
    "\n",
    "### Without query in decoder (replicates v3 Exp 01):\n",
    "\n",
    "| # | Condition | Encoder input | Cross-attn | Decoder input |\n",
    "|---|-----------|--------------|------------|---------------|\n",
    "| 7 | bare_nq | [document] | all | answer only |\n",
    "| 8 | oracle_trunc_nq | [query + document] | doc only | answer only |\n",
    "\n",
    "## Key comparisons\n",
    "\n",
    "- **(2) vs (1)**: Does enrichment help when decoder already has query? (**THE** question)\n",
    "- **(8) vs (7)**: Replicates v3 Exp 01 finding (expected d≈+0.4)\n",
    "- **(2)−(1) vs (8)−(7)**: Is enrichment redundant once decoder has query?\n",
    "- **(4) vs (1)**: Production-realistic surrogate benefit with query in decoder\n",
    "- **(3) vs (2)**: Does full cross-attention add value beyond enriched doc reps?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0e6ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Setup and model loading\n",
    "import os\n",
    "os.umask(0o000)\n",
    "\n",
    "import sys, json, time, gc, re\n",
    "import random as pyrandom\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from scipy import stats\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "sys.path.insert(0, \"../../..\")\n",
    "from lib.analysis import cohens_d\n",
    "\n",
    "SEED = 42\n",
    "N_SAMPLES = 500\n",
    "MODEL_NAME = \"google/t5gemma-2-4b-4b\"\n",
    "\n",
    "RESULTS_DIR = Path(\"../../../results/exp01\")\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CHECKPOINT_PATH = RESULTS_DIR / \"checkpoint.json\"\n",
    "\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "pyrandom.seed(SEED)\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv())\n",
    "HF_TOKEN = os.environ.get(\"HF_TOKEN\")\n",
    "\n",
    "from transformers import AutoProcessor, AutoModelForSeq2SeqLM\n",
    "\n",
    "print(f\"Loading {MODEL_NAME}...\")\n",
    "processor = AutoProcessor.from_pretrained(MODEL_NAME, token=HF_TOKEN)\n",
    "tokenizer = processor.tokenizer\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    MODEL_NAME, device_map=\"auto\", torch_dtype=torch.bfloat16, token=HF_TOKEN,\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "DEVICE = next(model.parameters()).device\n",
    "BOS_ID = getattr(model.config, 'decoder_start_token_id', None) or tokenizer.bos_token_id\n",
    "\n",
    "print(f\"Exp 01: Production-Realistic KV Cache\")\n",
    "print(f\"N: {N_SAMPLES}, Model: {MODEL_NAME}\")\n",
    "print(f\"DEVICE: {DEVICE}, dtype: {next(model.parameters()).dtype}\")\n",
    "print(f\"GPU memory: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "print(f\"Decoder start token ID (BOS): {BOS_ID}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413e5aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Scoring helpers\n",
    "\n",
    "def count_prefix_tokens(prefix_text, document_text):\n",
    "    # BPE-aware token count of prefix in [prefix + newline + document].\n",
    "    full_text = prefix_text + \"\\n\" + document_text\n",
    "    full_ids = tokenizer(full_text, add_special_tokens=True, truncation=True,\n",
    "                         max_length=2048).input_ids\n",
    "    doc_ids = tokenizer(document_text, add_special_tokens=True, truncation=True,\n",
    "                        max_length=2048).input_ids\n",
    "    return len(full_ids) - len(doc_ids)\n",
    "\n",
    "\n",
    "def score_nll(encoder_text, answer_text, prefix_token_count=0, truncate=False):\n",
    "    # Score NLL of answer tokens — decoder receives ONLY answer (no query).\n",
    "    # Used for _nq (no-query) conditions that replicate v3 Exp 01.\n",
    "    enc_ids = tokenizer(encoder_text, return_tensors=\"pt\",\n",
    "                        add_special_tokens=True, truncation=True,\n",
    "                        max_length=2048).input_ids.to(DEVICE)\n",
    "    total_enc_len = enc_ids.shape[1]\n",
    "    enc_mask = torch.ones(1, total_enc_len, device=DEVICE, dtype=torch.long)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        encoder_outputs = model.get_encoder()(\n",
    "            input_ids=enc_ids, attention_mask=enc_mask\n",
    "        )\n",
    "\n",
    "    if truncate and prefix_token_count > 0:\n",
    "        cross_attn_mask = torch.ones(1, total_enc_len, device=DEVICE, dtype=torch.long)\n",
    "        cross_attn_mask[:, :prefix_token_count] = 0\n",
    "    else:\n",
    "        cross_attn_mask = torch.ones(1, total_enc_len, device=DEVICE, dtype=torch.long)\n",
    "\n",
    "    ans_ids = tokenizer(answer_text, return_tensors=\"pt\",\n",
    "                        add_special_tokens=False, truncation=True,\n",
    "                        max_length=256).input_ids.to(DEVICE)\n",
    "\n",
    "    if ans_ids.shape[1] == 0:\n",
    "        del encoder_outputs\n",
    "        return 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            encoder_outputs=encoder_outputs,\n",
    "            attention_mask=cross_attn_mask,\n",
    "            labels=ans_ids,\n",
    "        )\n",
    "\n",
    "    logits = outputs.logits\n",
    "    log_probs = F.log_softmax(logits, dim=-1)\n",
    "    token_log_probs = log_probs[0].gather(1, ans_ids[0].unsqueeze(1)).squeeze(1)\n",
    "    mean_nll = -token_log_probs.mean().item()\n",
    "\n",
    "    del encoder_outputs, outputs, logits, log_probs\n",
    "    return mean_nll\n",
    "\n",
    "\n",
    "def score_nll_query_prefix(encoder_text, query_text, answer_text,\n",
    "                           prefix_token_count=0, truncate=False):\n",
    "    # Score NLL of answer tokens with query as decoder prefix.\n",
    "    # Production-realistic: decoder_input_ids = [BOS] + query_ids + answer_ids.\n",
    "    # NLL is computed only on answer tokens.\n",
    "    #\n",
    "    # Args:\n",
    "    #   encoder_text: Text for encoder (e.g., \"[prefix]\\n[document]\" or \"[document]\")\n",
    "    #   query_text: Query text fed to decoder as prefix\n",
    "    #   answer_text: Answer text whose NLL we measure\n",
    "    #   prefix_token_count: Number of encoder prefix tokens to potentially mask\n",
    "    #   truncate: If True, mask prefix tokens from decoder cross-attention\n",
    "\n",
    "    # 1. Encode\n",
    "    enc_ids = tokenizer(encoder_text, return_tensors=\"pt\",\n",
    "                        add_special_tokens=True, truncation=True,\n",
    "                        max_length=2048).input_ids.to(DEVICE)\n",
    "    total_enc_len = enc_ids.shape[1]\n",
    "    enc_mask = torch.ones(1, total_enc_len, device=DEVICE, dtype=torch.long)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        encoder_outputs = model.get_encoder()(\n",
    "            input_ids=enc_ids, attention_mask=enc_mask\n",
    "        )\n",
    "\n",
    "    # 2. Cross-attention mask\n",
    "    if truncate and prefix_token_count > 0:\n",
    "        cross_attn_mask = torch.ones(1, total_enc_len, device=DEVICE, dtype=torch.long)\n",
    "        cross_attn_mask[:, :prefix_token_count] = 0\n",
    "    else:\n",
    "        cross_attn_mask = torch.ones(1, total_enc_len, device=DEVICE, dtype=torch.long)\n",
    "\n",
    "    # 3. Tokenize query and answer for decoder\n",
    "    query_ids = tokenizer(query_text, add_special_tokens=False, truncation=True,\n",
    "                          max_length=512).input_ids\n",
    "    answer_ids = tokenizer(answer_text, add_special_tokens=False, truncation=True,\n",
    "                           max_length=256).input_ids\n",
    "\n",
    "    if len(answer_ids) == 0:\n",
    "        del encoder_outputs\n",
    "        return 0.0\n",
    "\n",
    "    # 4. Build decoder_input_ids = [BOS] + query_ids + answer_ids\n",
    "    dec_ids = [BOS_ID] + query_ids + answer_ids\n",
    "    dec_tensor = torch.tensor([dec_ids], dtype=torch.long, device=DEVICE)\n",
    "\n",
    "    n_query = len(query_ids)\n",
    "    n_answer = len(answer_ids)\n",
    "\n",
    "    # 5. Forward pass (no labels — we compute NLL manually)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            encoder_outputs=encoder_outputs,\n",
    "            attention_mask=cross_attn_mask,\n",
    "            decoder_input_ids=dec_tensor,\n",
    "        )\n",
    "\n",
    "    # 6. Extract answer logits\n",
    "    # logits[0, t, :] predicts the token at position t+1\n",
    "    # Positions: [BOS=0, q1=1, ..., qK=K, a1=K+1, ..., aM=K+M]\n",
    "    # To predict a1 at position K+1, use logits[0, K, :]\n",
    "    # To predict aM at position K+M, use logits[0, K+M-1, :]\n",
    "    logits = outputs.logits\n",
    "    answer_logits = logits[0, n_query:n_query + n_answer, :]\n",
    "\n",
    "    # 7. Compute NLL\n",
    "    answer_targets = torch.tensor(answer_ids, dtype=torch.long, device=DEVICE)\n",
    "    log_probs = F.log_softmax(answer_logits, dim=-1)\n",
    "    token_log_probs = log_probs.gather(1, answer_targets.unsqueeze(1)).squeeze(1)\n",
    "    mean_nll = -token_log_probs.mean().item()\n",
    "\n",
    "    del encoder_outputs, outputs, logits, log_probs\n",
    "    return mean_nll\n",
    "\n",
    "\n",
    "# === Surrogate generation ===\n",
    "STOP_WORDS = {\n",
    "    'a', 'an', 'the', 'is', 'are', 'was', 'were', 'be', 'been', 'being',\n",
    "    'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could',\n",
    "    'should', 'may', 'might', 'can', 'shall', 'to', 'of', 'in', 'for',\n",
    "    'on', 'with', 'at', 'by', 'from', 'as', 'into', 'through', 'during',\n",
    "    'before', 'after', 'above', 'below', 'between', 'and', 'but', 'or',\n",
    "    'not', 'no', 'if', 'then', 'than', 'so', 'up', 'out', 'about',\n",
    "    'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those',\n",
    "    'it', 'its', 'i', 'me', 'my', 'we', 'our', 'you', 'your', 'he',\n",
    "    'him', 'his', 'she', 'her', 'they', 'them', 'their', 'how', 'when',\n",
    "    'where', 'why', 'much', 'many', 'some', 'any', 'all', 'each',\n",
    "    'does', 'also', 'just', 'more', 'most', 'very', 'too', 'only',\n",
    "}\n",
    "\n",
    "def extract_keywords(text):\n",
    "    words = re.sub(r'[^\\w\\s]', '', text.lower()).split()\n",
    "    return [w for w in words if w not in STOP_WORDS and len(w) > 2]\n",
    "\n",
    "def make_surrogate_template(passage):\n",
    "    content_words = extract_keywords(passage)\n",
    "    if not content_words:\n",
    "        return \"What is this about?\"\n",
    "    counts = Counter(content_words)\n",
    "    top_word = counts.most_common(1)[0][0]\n",
    "    return f\"What is {top_word}?\"\n",
    "\n",
    "def make_surrogate_from_doc(passage):\n",
    "    content_words = extract_keywords(passage)\n",
    "    if not content_words:\n",
    "        return \"information\"\n",
    "    counts = Counter(content_words)\n",
    "    return \" \".join(w for w, _ in counts.most_common(5))\n",
    "\n",
    "print(\"Scoring functions defined:\")\n",
    "print(\"  score_nll(encoder_text, answer_text, prefix_token_count, truncate)\")\n",
    "print(\"  score_nll_query_prefix(encoder_text, query_text, answer_text, prefix_token_count, truncate)\")\n",
    "\n",
    "# Sanity check: verify BOS_ID and decoder behavior\n",
    "test_q_ids = tokenizer(\"What is Python?\", add_special_tokens=False).input_ids\n",
    "test_a_ids = tokenizer(\"A programming language.\", add_special_tokens=False).input_ids\n",
    "print(f\"\\nSanity check:\")\n",
    "print(f\"  BOS_ID: {BOS_ID} (token: '{tokenizer.decode([BOS_ID])}')\")\n",
    "print(f\"  Query 'What is Python?' -> {len(test_q_ids)} tokens\")\n",
    "print(f\"  Answer 'A programming language.' -> {len(test_a_ids)} tokens\")\n",
    "print(f\"  Decoder input length: 1 + {len(test_q_ids)} + {len(test_a_ids)} = {1 + len(test_q_ids) + len(test_a_ids)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41b5484",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Load MS MARCO data and generate surrogates\n",
    "from lib.data import count_words\n",
    "from datasets import load_dataset\n",
    "\n",
    "print(\"Loading MS MARCO v1.1 validation...\")\n",
    "ds = load_dataset(\"microsoft/ms_marco\", \"v1.1\", split=\"validation\")\n",
    "\n",
    "all_candidates = []\n",
    "for item in ds:\n",
    "    if len(all_candidates) >= 3 * N_SAMPLES:\n",
    "        break\n",
    "    passages = item.get('passages', {})\n",
    "    ptexts = passages.get('passage_text', [])\n",
    "    is_sel = passages.get('is_selected', [])\n",
    "    query = item.get('query', '')\n",
    "    answers = item.get('answers', [])\n",
    "    well_formed = item.get('wellFormedAnswers', [])\n",
    "    answer = None\n",
    "    if well_formed and len(well_formed) > 0 and well_formed[0] not in ('[]', ''):\n",
    "        answer = well_formed[0]\n",
    "    elif answers and len(answers) > 0 and answers[0] != 'No Answer Present.':\n",
    "        answer = answers[0]\n",
    "    if not answer:\n",
    "        continue\n",
    "    for pt, sel in zip(ptexts, is_sel):\n",
    "        wc = count_words(pt)\n",
    "        if sel == 1 and 30 <= wc <= 300:\n",
    "            all_candidates.append({\n",
    "                'passage': pt, 'query': query, 'answer': answer,\n",
    "                'word_count': wc,\n",
    "            })\n",
    "            break\n",
    "\n",
    "print(f\"Total candidates: {len(all_candidates)}\")\n",
    "np.random.seed(SEED)\n",
    "indices = np.random.permutation(len(all_candidates))\n",
    "samples = [all_candidates[i] for i in indices[:N_SAMPLES]]\n",
    "del ds, all_candidates\n",
    "gc.collect()\n",
    "\n",
    "# Generate surrogates for each sample\n",
    "for i, s in enumerate(samples):\n",
    "    s['surr_template'] = make_surrogate_template(s['passage'])\n",
    "    s['surr_doc'] = make_surrogate_from_doc(s['passage'])\n",
    "\n",
    "    # Random prefix: words from unrelated passage, matched to query word count\n",
    "    other_idx = (i + N_SAMPLES // 2) % len(samples)\n",
    "    other_words = samples[other_idx]['passage'].split()\n",
    "    query_word_count = len(s['query'].split())\n",
    "    s['random_prefix'] = \" \".join(other_words[:query_word_count])\n",
    "\n",
    "    # Count prefix tokens for each condition\n",
    "    s['n_prefix_oracle'] = count_prefix_tokens(s['query'], s['passage'])\n",
    "    s['n_prefix_template'] = count_prefix_tokens(s['surr_template'], s['passage'])\n",
    "    s['n_prefix_doc'] = count_prefix_tokens(s['surr_doc'], s['passage'])\n",
    "    s['n_prefix_random'] = count_prefix_tokens(s['random_prefix'], s['passage'])\n",
    "\n",
    "print(f\"Loaded {len(samples)} samples\")\n",
    "print(f\"Mean passage words: {np.mean([s['word_count'] for s in samples]):.0f}\")\n",
    "print(f\"Mean answer words: {np.mean([count_words(s['answer']) for s in samples]):.0f}\")\n",
    "print(f\"Mean query words: {np.mean([count_words(s['query']) for s in samples]):.0f}\")\n",
    "\n",
    "print(f\"\\nFirst sample:\")\n",
    "print(f\"  Query:  {samples[0]['query'][:70]}...\")\n",
    "print(f\"  Answer: {samples[0]['answer'][:70]}...\")\n",
    "print(f\"  Passage ({samples[0]['word_count']}w): {samples[0]['passage'][:70]}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5a1c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Show example conditions for sample 0\n",
    "print(\"=\" * 70)\n",
    "print(\"EXAMPLE CONDITIONS (sample 0)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "ex = samples[0]\n",
    "print(f\"\\nQuery:          {ex['query']}\")\n",
    "print(f\"Answer:         {ex['answer']}\")\n",
    "print(f\"Passage:        {ex['passage'][:100]}...\")\n",
    "print(f\"Surr template:  {ex['surr_template']}\")\n",
    "print(f\"Surr doc kw:    {ex['surr_doc']}\")\n",
    "print(f\"Random prefix:  {ex['random_prefix'][:60]}...\")\n",
    "\n",
    "print(f\"\\n  {'Condition':<25} {'Enc prefix':<30} {'Trunc':>6} {'Dec query':>10}\")\n",
    "print(f\"  {'-'*75}\")\n",
    "\n",
    "cond_display = [\n",
    "    ('bare',                '(none)',               'no',  'yes'),\n",
    "    ('oracle_trunc',        'real query',           'yes', 'yes'),\n",
    "    ('oracle_full',         'real query',           'no',  'yes'),\n",
    "    ('surr_template_trunc', ex['surr_template'],    'yes', 'yes'),\n",
    "    ('surr_doc_trunc',      ex['surr_doc'][:28],    'yes', 'yes'),\n",
    "    ('random_trunc',        ex['random_prefix'][:28], 'yes', 'yes'),\n",
    "    ('bare_nq',             '(none)',               'no',  'no'),\n",
    "    ('oracle_trunc_nq',     'real query',           'yes', 'no'),\n",
    "]\n",
    "\n",
    "for name, prefix, trunc, has_q in cond_display:\n",
    "    print(f\"  {name:<25} {prefix:<30} {trunc:>6} {has_q:>10}\")\n",
    "\n",
    "# Show decoder input structure\n",
    "q_ids = tokenizer(ex['query'], add_special_tokens=False).input_ids\n",
    "a_ids = tokenizer(ex['answer'], add_special_tokens=False).input_ids\n",
    "print(f\"\\nDecoder input (query-prefix conditions):\")\n",
    "print(f\"  [BOS] + query ({len(q_ids)} tok) + answer ({len(a_ids)} tok) \"\n",
    "      f\"= {1 + len(q_ids) + len(a_ids)} tok total\")\n",
    "print(f\"  NLL computed on last {len(a_ids)} positions (answer only)\")\n",
    "\n",
    "print(f\"\\nDecoder input (no-query conditions):\")\n",
    "print(f\"  Model creates [BOS, a1, ..., a_{{M-1}}] internally from labels\")\n",
    "print(f\"  NLL computed on all {len(a_ids)} answer token positions\")\n",
    "\n",
    "# Quick sanity: compare query-prefix NLL vs no-query NLL for bare\n",
    "print(f\"\\nSanity check: bare NLL with and without query in decoder...\")\n",
    "nll_bare_q = score_nll_query_prefix(ex['passage'], ex['query'], ex['answer'])\n",
    "nll_bare_nq = score_nll(ex['passage'], ex['answer'])\n",
    "print(f\"  bare (with query in decoder):    {nll_bare_q:.6f}\")\n",
    "print(f\"  bare_nq (no query in decoder):   {nll_bare_nq:.6f}\")\n",
    "print(f\"  Difference: {nll_bare_nq - nll_bare_q:+.6f}\")\n",
    "print(f\"  (Expected: query prefix should lower NLL substantially)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87dc235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Scoring loop — 8 conditions x 500 samples\n",
    "print(\"=\" * 70)\n",
    "print(\"SCORING ALL CONDITIONS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "COND_NAMES = [\n",
    "    'bare', 'oracle_trunc', 'oracle_full',\n",
    "    'surr_template_trunc', 'surr_doc_trunc', 'random_trunc',\n",
    "    'bare_nq', 'oracle_trunc_nq',\n",
    "]\n",
    "\n",
    "results = []\n",
    "start_idx = 0\n",
    "\n",
    "if CHECKPOINT_PATH.exists():\n",
    "    ckpt = json.loads(CHECKPOINT_PATH.read_text())\n",
    "    if ckpt.get('n_total') == N_SAMPLES and len(ckpt.get('results', [])) > 0:\n",
    "        saved_queries = [r['query'][:50] for r in ckpt['results']]\n",
    "        current_queries = [s['query'][:50] for s in samples[:len(saved_queries)]]\n",
    "        if saved_queries == current_queries:\n",
    "            results = ckpt['results']\n",
    "            start_idx = len(results)\n",
    "            print(f\"Resuming from checkpoint: {start_idx}/{N_SAMPLES}\")\n",
    "\n",
    "if start_idx == 0:\n",
    "    print(f\"Starting fresh: {len(COND_NAMES)} conditions x {N_SAMPLES} samples \"\n",
    "          f\"= {len(COND_NAMES) * N_SAMPLES} forward passes\")\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "for i in tqdm(range(start_idx, N_SAMPLES), initial=start_idx, total=N_SAMPLES,\n",
    "              desc=\"Scoring\"):\n",
    "    s = samples[i]\n",
    "    query = s['query']\n",
    "    passage = s['passage']\n",
    "    answer = s['answer']\n",
    "\n",
    "    result = {\n",
    "        'query': query,\n",
    "        'answer': answer,\n",
    "        'passage_words': s['word_count'],\n",
    "    }\n",
    "\n",
    "    # --- Conditions 1-6: query in decoder (production-realistic) ---\n",
    "\n",
    "    # 1. bare: encoder=[doc], cross-attn=all, decoder=[query]->answer\n",
    "    result['nll_bare'] = score_nll_query_prefix(\n",
    "        passage, query, answer)\n",
    "\n",
    "    # 2. oracle_trunc: encoder=[query+doc], cross-attn=doc only, decoder=[query]->answer\n",
    "    result['nll_oracle_trunc'] = score_nll_query_prefix(\n",
    "        query + \"\\n\" + passage, query, answer,\n",
    "        prefix_token_count=s['n_prefix_oracle'], truncate=True)\n",
    "\n",
    "    # 3. oracle_full: encoder=[query+doc], cross-attn=all, decoder=[query]->answer\n",
    "    result['nll_oracle_full'] = score_nll_query_prefix(\n",
    "        query + \"\\n\" + passage, query, answer)\n",
    "\n",
    "    # 4. surr_template_trunc: encoder=[\"What is [kw]?\"+doc], decoder=[query]->answer\n",
    "    result['nll_surr_template_trunc'] = score_nll_query_prefix(\n",
    "        s['surr_template'] + \"\\n\" + passage, query, answer,\n",
    "        prefix_token_count=s['n_prefix_template'], truncate=True)\n",
    "\n",
    "    # 5. surr_doc_trunc: encoder=[top5kw+doc], decoder=[query]->answer\n",
    "    result['nll_surr_doc_trunc'] = score_nll_query_prefix(\n",
    "        s['surr_doc'] + \"\\n\" + passage, query, answer,\n",
    "        prefix_token_count=s['n_prefix_doc'], truncate=True)\n",
    "\n",
    "    # 6. random_trunc: encoder=[random+doc], decoder=[query]->answer\n",
    "    result['nll_random_trunc'] = score_nll_query_prefix(\n",
    "        s['random_prefix'] + \"\\n\" + passage, query, answer,\n",
    "        prefix_token_count=s['n_prefix_random'], truncate=True)\n",
    "\n",
    "    # --- Conditions 7-8: no query in decoder (replicates v3 Exp 01) ---\n",
    "\n",
    "    # 7. bare_nq: encoder=[doc], decoder=answer only\n",
    "    result['nll_bare_nq'] = score_nll(passage, answer)\n",
    "\n",
    "    # 8. oracle_trunc_nq: encoder=[query+doc], decoder=answer only (masked)\n",
    "    result['nll_oracle_trunc_nq'] = score_nll(\n",
    "        query + \"\\n\" + passage, answer,\n",
    "        prefix_token_count=s['n_prefix_oracle'], truncate=True)\n",
    "\n",
    "    results.append(result)\n",
    "\n",
    "    if (i + 1) % 20 == 0 or i == N_SAMPLES - 1:\n",
    "        ckpt = {\n",
    "            'n_total': N_SAMPLES,\n",
    "            'results': results,\n",
    "            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        }\n",
    "        CHECKPOINT_PATH.write_text(json.dumps(ckpt))\n",
    "        elapsed = time.time() - t0\n",
    "        done = i - start_idx + 1\n",
    "        eta = (N_SAMPLES - i - 1) * elapsed / done if done > 0 else 0\n",
    "        tqdm.write(f\"  Checkpoint {i+1}/{N_SAMPLES} | {elapsed/60:.1f}m | ETA {eta/60:.1f}m\")\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "elapsed = time.time() - t0\n",
    "print(f\"\\nScoring complete: {len(results)} samples, \"\n",
    "      f\"{len(COND_NAMES)} conditions in {elapsed/60:.1f} min\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8b0dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Results table — all conditions\n",
    "print(\"=\" * 70)\n",
    "print(f\"RESULTS (N={len(results)})\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Extract NLL arrays — query-prefix conditions\n",
    "bare = np.array([r['nll_bare'] for r in results])\n",
    "oracle_trunc = np.array([r['nll_oracle_trunc'] for r in results])\n",
    "oracle_full = np.array([r['nll_oracle_full'] for r in results])\n",
    "surr_template = np.array([r['nll_surr_template_trunc'] for r in results])\n",
    "surr_doc = np.array([r['nll_surr_doc_trunc'] for r in results])\n",
    "random_trunc = np.array([r['nll_random_trunc'] for r in results])\n",
    "\n",
    "# No-query conditions\n",
    "bare_nq = np.array([r['nll_bare_nq'] for r in results])\n",
    "oracle_trunc_nq = np.array([r['nll_oracle_trunc_nq'] for r in results])\n",
    "\n",
    "print(f\"\\n--- Query in decoder (production-realistic) ---\")\n",
    "print(f\"  Baseline: bare (decoder has query, encoder has document only)\")\n",
    "print(f\"\\n  {'Condition':<25} {'NLL':>8} {'vs bare':>10} {'d':>8} {'Win%':>8} {'p':>12} {'sig':>5}\")\n",
    "print(f\"  {'-'*78}\")\n",
    "\n",
    "analysis = {}\n",
    "for name, nlls in [\n",
    "    ('bare', bare),\n",
    "    ('oracle_trunc', oracle_trunc),\n",
    "    ('oracle_full', oracle_full),\n",
    "    ('surr_template_trunc', surr_template),\n",
    "    ('surr_doc_trunc', surr_doc),\n",
    "    ('random_trunc', random_trunc),\n",
    "]:\n",
    "    mean_nll = nlls.mean()\n",
    "    if name == 'bare':\n",
    "        print(f\"  {name:<25} {mean_nll:>8.4f} {'--':>10} {'--':>8} {'--':>8} {'--':>12} {'--':>5}\")\n",
    "        analysis[name] = {'mean_nll': float(mean_nll)}\n",
    "    else:\n",
    "        diff = bare - nlls  # positive = condition is better (lower NLL)\n",
    "        d = cohens_d(diff)\n",
    "        win_pct = 100 * np.mean(diff > 0)\n",
    "        _, p_val = stats.ttest_1samp(diff, 0)\n",
    "        sig = '***' if p_val < 0.001 else '**' if p_val < 0.01 else '*' if p_val < 0.05 else 'ns'\n",
    "        print(f\"  {name:<25} {mean_nll:>8.4f} {diff.mean():>+10.4f} {d:>+8.3f} {win_pct:>7.1f}% {p_val:>12.2e} {sig:>5}\")\n",
    "        analysis[name] = {\n",
    "            'mean_nll': float(mean_nll), 'delta': float(diff.mean()),\n",
    "            'd': float(d), 'win_pct': float(win_pct), 'p': float(p_val),\n",
    "        }\n",
    "\n",
    "print(f\"\\n--- No query in decoder (v3 Exp 01 replication) ---\")\n",
    "print(f\"  Baseline: bare_nq (decoder has answer only)\")\n",
    "print(f\"\\n  {'Condition':<25} {'NLL':>8} {'vs bare_nq':>10} {'d':>8} {'Win%':>8} {'p':>12} {'sig':>5}\")\n",
    "print(f\"  {'-'*78}\")\n",
    "\n",
    "diff_nq = bare_nq - oracle_trunc_nq\n",
    "d_nq = cohens_d(diff_nq)\n",
    "win_nq = 100 * np.mean(diff_nq > 0)\n",
    "_, p_nq = stats.ttest_1samp(diff_nq, 0)\n",
    "sig_nq = '***' if p_nq < 0.001 else '**' if p_nq < 0.01 else '*' if p_nq < 0.05 else 'ns'\n",
    "\n",
    "print(f\"  {'bare_nq':<25} {bare_nq.mean():>8.4f} {'--':>10} {'--':>8} {'--':>8} {'--':>12} {'--':>5}\")\n",
    "print(f\"  {'oracle_trunc_nq':<25} {oracle_trunc_nq.mean():>8.4f} {diff_nq.mean():>+10.4f} {d_nq:>+8.3f} {win_nq:>7.1f}% {p_nq:>12.2e} {sig_nq:>5}\")\n",
    "\n",
    "analysis['bare_nq'] = {'mean_nll': float(bare_nq.mean())}\n",
    "analysis['oracle_trunc_nq'] = {\n",
    "    'mean_nll': float(oracle_trunc_nq.mean()), 'delta': float(diff_nq.mean()),\n",
    "    'd': float(d_nq), 'win_pct': float(win_nq), 'p': float(p_nq),\n",
    "}\n",
    "\n",
    "print(f\"\\nExpected: oracle_trunc_nq vs bare_nq should replicate d~+0.4 from v3 Exp 01\")\n",
    "print(f\"Actual:   d={d_nq:+.3f} ({sig_nq})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a21e5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Key comparison — enrichment with query vs without query\n",
    "print(\"=\" * 70)\n",
    "print(\"KEY COMPARISON: Is enrichment redundant when decoder has the query?\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Enrichment benefit WITH query in decoder\n",
    "enrichment_with_q = bare - oracle_trunc  # positive = enrichment helps\n",
    "d_with_q = cohens_d(enrichment_with_q)\n",
    "_, p_with_q = stats.ttest_1samp(enrichment_with_q, 0)\n",
    "sig_with_q = '***' if p_with_q < 0.001 else '**' if p_with_q < 0.01 else '*' if p_with_q < 0.05 else 'ns'\n",
    "\n",
    "# Enrichment benefit WITHOUT query in decoder (v3 replication)\n",
    "enrichment_no_q = bare_nq - oracle_trunc_nq  # positive = enrichment helps\n",
    "d_no_q = cohens_d(enrichment_no_q)\n",
    "_, p_no_q = stats.ttest_1samp(enrichment_no_q, 0)\n",
    "sig_no_q = '***' if p_no_q < 0.001 else '**' if p_no_q < 0.01 else '*' if p_no_q < 0.05 else 'ns'\n",
    "\n",
    "print(f\"\\n  Enrichment benefit (oracle_trunc vs respective bare):\")\n",
    "print(f\"    With query in decoder:    d={d_with_q:+.4f} ({sig_with_q}), \"\n",
    "      f\"mean delta={enrichment_with_q.mean():+.4f}\")\n",
    "print(f\"    Without query in decoder: d={d_no_q:+.4f} ({sig_no_q}), \"\n",
    "      f\"mean delta={enrichment_no_q.mean():+.4f}\")\n",
    "\n",
    "# Are they different?\n",
    "diff_of_diffs = enrichment_with_q - enrichment_no_q\n",
    "d_diff = cohens_d(diff_of_diffs)\n",
    "_, p_diff = stats.ttest_1samp(diff_of_diffs, 0)\n",
    "sig_diff = '***' if p_diff < 0.001 else '**' if p_diff < 0.01 else '*' if p_diff < 0.05 else 'ns'\n",
    "\n",
    "# Ratio\n",
    "if d_no_q > 0:\n",
    "    ratio = d_with_q / d_no_q * 100\n",
    "else:\n",
    "    ratio = float('inf')\n",
    "\n",
    "print(f\"\\n  Ratio: with_q / without_q = {ratio:.1f}%\")\n",
    "print(f\"  Difference of differences: d={d_diff:+.4f} ({sig_diff})\")\n",
    "\n",
    "print(f\"\\n  Interpretation:\")\n",
    "if d_with_q > 0.1 and ratio > 50:\n",
    "    print(f\"    ENRICHMENT STILL HELPS even when decoder has the query.\")\n",
    "    print(f\"    The approach has genuine production value.\")\n",
    "    if ratio > 80:\n",
    "        print(f\"    Enrichment benefit is {ratio:.0f}% of the no-query benefit — mostly preserved.\")\n",
    "    else:\n",
    "        print(f\"    Enrichment benefit is {ratio:.0f}% of the no-query benefit — partially redundant.\")\n",
    "elif d_with_q > 0.05:\n",
    "    print(f\"    WEAK enrichment benefit with query. The decoder knowing the query\")\n",
    "    print(f\"    makes most of the enrichment redundant.\")\n",
    "else:\n",
    "    print(f\"    ENRICHMENT IS REDUNDANT when the decoder has the query.\")\n",
    "    print(f\"    The whole approach has no production value.\")\n",
    "\n",
    "# Full cross-attention vs truncation (with query)\n",
    "print(f\"\\n--- Full vs truncated cross-attention (with query in decoder) ---\")\n",
    "diff_full_trunc = oracle_trunc - oracle_full  # negative = full is better\n",
    "d_ft = cohens_d(diff_full_trunc)\n",
    "_, p_ft = stats.ttest_1samp(diff_full_trunc, 0)\n",
    "sig_ft = '***' if p_ft < 0.001 else '**' if p_ft < 0.01 else '*' if p_ft < 0.05 else 'ns'\n",
    "\n",
    "print(f\"  oracle_trunc NLL: {oracle_trunc.mean():.4f}\")\n",
    "print(f\"  oracle_full NLL:  {oracle_full.mean():.4f}\")\n",
    "print(f\"  full vs trunc: d={d_ft:+.4f} ({sig_ft})\")\n",
    "if abs(d_ft) < 0.05:\n",
    "    print(f\"  -> Full cross-attention adds minimal value beyond enriched doc reps.\")\n",
    "elif d_ft < -0.05:\n",
    "    print(f\"  -> Full cross-attention substantially better — decoder benefits from\")\n",
    "    print(f\"     reading query directly from encoder output as well.\")\n",
    "else:\n",
    "    print(f\"  -> Truncation is actually BETTER — same pattern as v3.\")\n",
    "\n",
    "# Surrogate conditions vs bare (with query)\n",
    "print(f\"\\n--- Surrogate benefit with query in decoder ---\")\n",
    "for name, nlls in [('surr_template_trunc', surr_template),\n",
    "                     ('surr_doc_trunc', surr_doc),\n",
    "                     ('random_trunc', random_trunc)]:\n",
    "    diff = bare - nlls\n",
    "    d = cohens_d(diff)\n",
    "    _, p = stats.ttest_1samp(diff, 0)\n",
    "    sig = '***' if p < 0.001 else '**' if p < 0.01 else '*' if p < 0.05 else 'ns'\n",
    "    if d_with_q > 0:\n",
    "        pct_oracle = d / d_with_q * 100\n",
    "    else:\n",
    "        pct_oracle = 0\n",
    "    print(f\"  {name:<25} d={d:+.4f} ({sig}), {pct_oracle:.0f}% of oracle\")\n",
    "\n",
    "# Per-sample correlation: does the same enrichment help the same samples?\n",
    "r_corr, p_corr = stats.pearsonr(enrichment_with_q, enrichment_no_q)\n",
    "print(f\"\\nPer-sample correlation (enrichment with q vs without q): r={r_corr:.3f} (p={p_corr:.2e})\")\n",
    "print(f\"  (High r = same samples benefit from enrichment regardless of decoder query)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc61a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Hardness gradient — does enrichment benefit vary with difficulty?\n",
    "print(\"=\" * 70)\n",
    "print(\"HARDNESS GRADIENT\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Use bare NLL as hardness proxy\n",
    "quintile_bounds = np.percentile(bare, [20, 40, 60, 80])\n",
    "quintiles = np.digitize(bare, quintile_bounds)\n",
    "\n",
    "print(f\"\\n--- Enrichment benefit by hardness (with query in decoder) ---\")\n",
    "print(f\"  {'Quintile':<12} {'N':>4} {'bare':>8} {'orc_trunc':>10} {'delta':>8} {'d':>8}\")\n",
    "print(f\"  {'-'*55}\")\n",
    "\n",
    "for q in range(5):\n",
    "    mask = quintiles == q\n",
    "    n_q = mask.sum()\n",
    "    if n_q < 5:\n",
    "        continue\n",
    "    qlabel = ['Q1 easy', 'Q2', 'Q3', 'Q4', 'Q5 hard'][q]\n",
    "    b = bare[mask].mean()\n",
    "    ot = oracle_trunc[mask].mean()\n",
    "    delta = (bare[mask] - oracle_trunc[mask]).mean()\n",
    "    d = cohens_d(bare[mask] - oracle_trunc[mask])\n",
    "    print(f\"  {qlabel:<12} {n_q:>4} {b:>8.4f} {ot:>10.4f} {delta:>+8.4f} {d:>+8.3f}\")\n",
    "\n",
    "# Same for no-query\n",
    "print(f\"\\n--- Enrichment benefit by hardness (no query in decoder) ---\")\n",
    "quintile_bounds_nq = np.percentile(bare_nq, [20, 40, 60, 80])\n",
    "quintiles_nq = np.digitize(bare_nq, quintile_bounds_nq)\n",
    "\n",
    "print(f\"  {'Quintile':<12} {'N':>4} {'bare_nq':>8} {'orc_tr_nq':>10} {'delta':>8} {'d':>8}\")\n",
    "print(f\"  {'-'*55}\")\n",
    "\n",
    "for q in range(5):\n",
    "    mask = quintiles_nq == q\n",
    "    n_q = mask.sum()\n",
    "    if n_q < 5:\n",
    "        continue\n",
    "    qlabel = ['Q1 easy', 'Q2', 'Q3', 'Q4', 'Q5 hard'][q]\n",
    "    b = bare_nq[mask].mean()\n",
    "    ot = oracle_trunc_nq[mask].mean()\n",
    "    delta = (bare_nq[mask] - oracle_trunc_nq[mask]).mean()\n",
    "    d = cohens_d(bare_nq[mask] - oracle_trunc_nq[mask])\n",
    "    print(f\"  {qlabel:<12} {n_q:>4} {b:>8.4f} {ot:>10.4f} {delta:>+8.4f} {d:>+8.3f}\")\n",
    "\n",
    "# Spearman correlations with hardness\n",
    "r_hard, p_hard = stats.spearmanr(bare, bare - oracle_trunc)\n",
    "print(f\"\\nSpearman correlation (hardness vs enrichment with query): \"\n",
    "      f\"rho={r_hard:.3f} (p={p_hard:.2e})\")\n",
    "r_hard_nq, p_hard_nq = stats.spearmanr(bare_nq, bare_nq - oracle_trunc_nq)\n",
    "print(f\"Spearman correlation (hardness vs enrichment no query):   \"\n",
    "      f\"rho={r_hard_nq:.3f} (p={p_hard_nq:.2e})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9918f02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Verdict and save results\n",
    "print(\"=\" * 70)\n",
    "print(\"VERDICT -- Exp 01: Production-Realistic KV Cache\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "d_with_q = cohens_d(bare - oracle_trunc)\n",
    "d_no_q = cohens_d(bare_nq - oracle_trunc_nq)\n",
    "ratio = d_with_q / d_no_q * 100 if d_no_q > 0 else float('inf')\n",
    "\n",
    "print(f\"\\nModel: {MODEL_NAME}\")\n",
    "print(f\"N: {len(results)} samples (MS MARCO v1.1)\")\n",
    "\n",
    "print(f\"\\n--- THE key result ---\")\n",
    "print(f\"  Enrichment without query in decoder (v3 baseline): d={d_no_q:+.3f}\")\n",
    "print(f\"  Enrichment WITH query in decoder (production):     d={d_with_q:+.3f}\")\n",
    "print(f\"  Ratio: {ratio:.0f}%\")\n",
    "\n",
    "if d_with_q > 0.1:\n",
    "    print(f\"\\n  CONCLUSION: Encoder enrichment provides genuine value\")\n",
    "    print(f\"  even in production where the decoder already has the query.\")\n",
    "    if ratio > 80:\n",
    "        print(f\"  The enrichment benefit is almost fully preserved ({ratio:.0f}%).\")\n",
    "    elif ratio > 50:\n",
    "        print(f\"  About {ratio:.0f}% of the enrichment benefit survives.\")\n",
    "    else:\n",
    "        print(f\"  Only {ratio:.0f}% survives -- most benefit was query-reading.\")\n",
    "elif d_with_q > 0.05:\n",
    "    print(f\"\\n  CONCLUSION: Marginal benefit. The decoder knowing the query\")\n",
    "    print(f\"  makes most enrichment redundant. Production value is limited.\")\n",
    "else:\n",
    "    print(f\"\\n  CONCLUSION: No production value. The enrichment benefit\")\n",
    "    print(f\"  vanishes when the decoder already has the query.\")\n",
    "    print(f\"  The v3 benefit was primarily from the decoder reading the query\")\n",
    "    print(f\"  from encoder output, not from improved document representations.\")\n",
    "\n",
    "# Surrogate summary\n",
    "print(f\"\\n--- Surrogate summary (with query in decoder) ---\")\n",
    "for name in ['surr_template_trunc', 'surr_doc_trunc', 'random_trunc']:\n",
    "    nlls = np.array([r[f'nll_{name}'] for r in results])\n",
    "    d = cohens_d(bare - nlls)\n",
    "    _, p = stats.ttest_1samp(bare - nlls, 0)\n",
    "    sig = '***' if p < 0.001 else '**' if p < 0.01 else '*' if p < 0.05 else 'ns'\n",
    "    print(f\"  {name:<25} d={d:+.4f} ({sig})\")\n",
    "\n",
    "# Save\n",
    "final_results = {\n",
    "    'experiment': 'v4_exp01_production_kv_cache',\n",
    "    'model': MODEL_NAME,\n",
    "    'dataset': 'ms_marco_v1.1',\n",
    "    'n_samples': len(results),\n",
    "    'seed': SEED,\n",
    "    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'key_result': {\n",
    "        'enrichment_with_query_d': float(d_with_q),\n",
    "        'enrichment_no_query_d': float(d_no_q),\n",
    "        'ratio_pct': float(ratio),\n",
    "    },\n",
    "    'conditions': {k: v for k, v in analysis.items()},\n",
    "}\n",
    "\n",
    "with open(RESULTS_DIR / 'results.json', 'w') as f:\n",
    "    json.dump(final_results, f, indent=2)\n",
    "print(f\"\\nResults saved to {RESULTS_DIR / 'results.json'}\")\n",
    "\n",
    "# Cleanup\n",
    "print(f\"\\nCleaning up GPU memory...\")\n",
    "mem_before = torch.cuda.memory_allocated() / 1e9\n",
    "del model, processor, tokenizer\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "mem_after = torch.cuda.memory_allocated() / 1e9\n",
    "print(f\"GPU memory: {mem_before:.2f} GB -> {mem_after:.2f} GB\")\n",
    "print(\"Done!\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
