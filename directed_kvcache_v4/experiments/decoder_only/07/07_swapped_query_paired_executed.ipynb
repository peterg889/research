{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0471c74",
   "metadata": {},
   "source": [
    "# Decoder-Only Exp 07: Swapped-Query Paired Contrasts\n",
    "\n",
    "## Motivation\n",
    "\n",
    "Port of v3 Exp 13 to decoder-only two-phase KV cache scoring.\n",
    "v3 Exp 13 found a significant semantic signal (d=+0.166, p=2.3e-04)\n",
    "with the encoder-decoder T5Gemma. Does the same paired semantic contrast\n",
    "appear with decoder-only Gemma 3 4B-PT using KV cache priming?\n",
    "\n",
    "## Design\n",
    "\n",
    "For each (query, document, answer) triple, we score the answer NLL under two\n",
    "prefix conditions that are structurally identical (same Q prefix token IDs):\n",
    "- `oracle`: the real query tokens (semantically relevant)\n",
    "- `swapped`: query tokens from a completely different sample (semantically irrelevant)\n",
    "\n",
    "The per-document paired contrast `swapped_nll - oracle_nll` isolates the semantic\n",
    "component with maximum statistical power.\n",
    "\n",
    "All prefixed conditions use exactly Q token IDs (token-level matching).\n",
    "\n",
    "## Conditions (4)\n",
    "\n",
    "| # | Condition | Prefix content |\n",
    "|---|-----------|---------------|\n",
    "| 1 | bare | (none) |\n",
    "| 2 | oracle | real query tokens |\n",
    "| 3 | swapped | query tokens from sample (i + N//2) % N |\n",
    "| 4 | random_matched | random passage word tokens |\n",
    "\n",
    "## Analysis\n",
    "\n",
    "1. Standard condition table\n",
    "2. Paired semantic contrast (the key test)\n",
    "3. Effect distribution\n",
    "4. Predictors of semantic benefit\n",
    "5. Structural equivalence check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bef3274a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-20T16:13:20.651099Z",
     "iopub.status.busy": "2026-02-20T16:13:20.650686Z",
     "iopub.status.idle": "2026-02-20T16:13:22.984007Z",
     "shell.execute_reply": "2026-02-20T16:13:22.983075Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exp 07: Swapped-Query Paired Contrasts (Decoder-Only)\n",
      "N: 400, SEED: 43\n",
      "Model: google/gemma-3-4b-it\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Setup\n",
    "import os\n",
    "os.umask(0o000)\n",
    "\n",
    "import sys, json, time, re, gc, random as pyrandom\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from scipy import stats\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "sys.path.insert(0, \"../../..\")\n",
    "from lib.analysis import cohens_d\n",
    "\n",
    "SEED = 43  # Different seed from Exp 06 (42) for independent samples\n",
    "N_SAMPLES = 400\n",
    "MODEL_NAME = \"google/gemma-3-4b-it\"\n",
    "\n",
    "RESULTS_DIR = Path(\"../../../results/decoder_only/exp07\")\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CHECKPOINT_PATH = RESULTS_DIR / \"checkpoint.json\"\n",
    "\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "pyrandom.seed(SEED)\n",
    "\n",
    "print(\"Exp 07: Swapped-Query Paired Contrasts (Decoder-Only)\")\n",
    "print(f\"N: {N_SAMPLES}, SEED: {SEED}\")\n",
    "print(f\"Model: {MODEL_NAME}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2b8729b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-20T16:13:22.987723Z",
     "iopub.status.busy": "2026-02-20T16:13:22.987346Z",
     "iopub.status.idle": "2026-02-20T16:13:24.931867Z",
     "shell.execute_reply": "2026-02-20T16:13:24.930904Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MS MARCO...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 400 samples\n",
      "Document lengths: 30-147 words, mean=72\n",
      "Query lengths: 2-15 words, mean=5.9\n",
      "\n",
      "Swapped query assignment:\n",
      "  Sample 0: 'after cooked how soon to fridge...'\n",
      "    Swapped: 'what does a chromosome do...'\n",
      "\n",
      "  Sample 1: 'average price of a kayak pool...'\n",
      "    Swapped: 'why is it necessary that we have a transport syste...'\n",
      "\n",
      "  Sample 2: 'what created the start of the manhattan project...'\n",
      "    Swapped: 'calories in a rasgulla...'\n",
      "\n",
      "  Sample 3: 'what is the primary function of the thyroid hormon...'\n",
      "    Swapped: 'average cost of nylon wall to wall carpeting insta...'\n",
      "\n",
      "  Sample 4: 'what makes a disease monogenic...'\n",
      "    Swapped: 'what heath good does turmeric...'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Load MS MARCO and select samples\n",
    "from lib.data import count_words\n",
    "from datasets import load_dataset\n",
    "\n",
    "print(\"Loading MS MARCO...\")\n",
    "ds = load_dataset(\"microsoft/ms_marco\", \"v1.1\", split=\"validation\")\n",
    "\n",
    "samples = []\n",
    "for item in ds:\n",
    "    if len(samples) >= N_SAMPLES * 3:\n",
    "        break\n",
    "    passages = item.get('passages', {})\n",
    "    ptexts = passages.get('passage_text', [])\n",
    "    is_sel = passages.get('is_selected', [])\n",
    "    query = item.get('query', '')\n",
    "    answers = item.get('answers', [])\n",
    "    well_formed = item.get('wellFormedAnswers', [])\n",
    "    answer = None\n",
    "    if well_formed and len(well_formed) > 0 and well_formed[0] not in ('[]', ''):\n",
    "        answer = well_formed[0]\n",
    "    elif answers and len(answers) > 0 and answers[0] != 'No Answer Present.':\n",
    "        answer = answers[0]\n",
    "    if not answer:\n",
    "        continue\n",
    "    for pt, sel in zip(ptexts, is_sel):\n",
    "        wc = count_words(pt)\n",
    "        if sel == 1 and 30 <= wc <= 300:\n",
    "            samples.append({\n",
    "                'passage': pt, 'query': query, 'answer': answer,\n",
    "                'word_count': wc,\n",
    "            })\n",
    "            break\n",
    "\n",
    "np.random.seed(SEED)\n",
    "np.random.shuffle(samples)\n",
    "samples = samples[:N_SAMPLES]\n",
    "del ds\n",
    "gc.collect()\n",
    "\n",
    "passage_words = np.array([s['word_count'] for s in samples])\n",
    "query_words = np.array([len(s['query'].split()) for s in samples])\n",
    "print(f\"Selected {N_SAMPLES} samples\")\n",
    "print(f\"Document lengths: {passage_words.min()}-{passage_words.max()} words, \"\n",
    "      f\"mean={passage_words.mean():.0f}\")\n",
    "print(f\"Query lengths: {query_words.min()}-{query_words.max()} words, \"\n",
    "      f\"mean={query_words.mean():.1f}\")\n",
    "\n",
    "# Verify swapped queries are from different topics\n",
    "print(f\"\\nSwapped query assignment:\")\n",
    "for i in range(5):\n",
    "    j = (i + N_SAMPLES // 2) % N_SAMPLES\n",
    "    print(f\"  Sample {i}: '{samples[i]['query'][:50]}...'\")\n",
    "    print(f\"    Swapped: '{samples[j]['query'][:50]}...'\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb5ec089",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-20T16:13:24.935827Z",
     "iopub.status.busy": "2026-02-20T16:13:24.935358Z",
     "iopub.status.idle": "2026-02-20T16:13:37.369035Z",
     "shell.execute_reply": "2026-02-20T16:13:37.367927Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading google/gemma-3-4b-it...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4f10666cd81483c9537426f68b27b6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/883 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded. dtype=torch.bfloat16\n",
      "GPU memory: 8.60 GB\n",
      "Vocab size: 262208\n",
      "BOS token ID: 2\n",
      "Newline token IDs: [107] (1 tokens)\n",
      "Scoring function defined.\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Load model and define scoring helpers\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv())\n",
    "HF_TOKEN = os.environ.get(\"HF_TOKEN\")\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "print(f\"Loading {MODEL_NAME}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, token=HF_TOKEN)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME, device_map=\"auto\", torch_dtype=torch.bfloat16, token=HF_TOKEN,\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "DEVICE = next(model.parameters()).device\n",
    "\n",
    "print(f\"Model loaded. dtype={next(model.parameters()).dtype}\")\n",
    "print(f\"GPU memory: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "text_cfg = getattr(model.config, 'text_config', model.config)\n",
    "VOCAB_SIZE = getattr(text_cfg, 'vocab_size', 262208)\n",
    "print(f\"Vocab size: {VOCAB_SIZE}\")\n",
    "\n",
    "NEWLINE_IDS = tokenizer(\"\\n\", add_special_tokens=False).input_ids\n",
    "BOS_ID = tokenizer.bos_token_id\n",
    "print(f\"BOS token ID: {BOS_ID}\")\n",
    "print(f\"Newline token IDs: {NEWLINE_IDS} ({len(NEWLINE_IDS)} tokens)\")\n",
    "\n",
    "STOP_WORDS = {\n",
    "    'a', 'an', 'the', 'is', 'are', 'was', 'were', 'be', 'been', 'being',\n",
    "    'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could',\n",
    "    'should', 'may', 'might', 'can', 'shall', 'to', 'of', 'in', 'for',\n",
    "    'on', 'with', 'at', 'by', 'from', 'as', 'into', 'through', 'during',\n",
    "    'before', 'after', 'above', 'below', 'between', 'and', 'but', 'or',\n",
    "    'not', 'no', 'if', 'then', 'than', 'so', 'up', 'out', 'about',\n",
    "    'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those',\n",
    "    'it', 'its', 'i', 'me', 'my', 'we', 'our', 'you', 'your', 'he',\n",
    "    'him', 'his', 'she', 'her', 'they', 'them', 'their', 'how', 'when',\n",
    "    'where', 'why', 'much', 'many', 'some', 'any', 'all', 'each',\n",
    "    'does', 'also', 'just', 'more', 'most', 'very', 'too', 'only',\n",
    "}\n",
    "\n",
    "\n",
    "def slice_kv_cache(cache, start_idx):\n",
    "    # Remove first start_idx entries from KV cache.\n",
    "    from transformers import DynamicCache\n",
    "    if isinstance(cache, DynamicCache):\n",
    "        sliced = DynamicCache()\n",
    "        for i in range(len(cache.layers)):\n",
    "            k = cache.layers[i].keys[:, :, start_idx:, :]\n",
    "            v = cache.layers[i].values[:, :, start_idx:, :]\n",
    "            sliced.update(k, v, i)\n",
    "        return sliced\n",
    "    else:\n",
    "        return tuple(\n",
    "            (k[:, :, start_idx:, :], v[:, :, start_idx:, :])\n",
    "            for k, v in cache\n",
    "        )\n",
    "\n",
    "\n",
    "def score(doc_text, query_text, answer_text, prefix_token_ids=None):\n",
    "    # Score NLL of answer tokens using two-phase KV cache.\n",
    "    #\n",
    "    # If prefix_token_ids is provided:\n",
    "    #   Phase A: [BOS] + prefix_ids + [\\n] + doc_ids\n",
    "    #   Slice first 1+len(prefix_ids)+len(NEWLINE_IDS) entries\n",
    "    # Otherwise (bare):\n",
    "    #   Phase A: [BOS] + doc_ids (nothing sliced)\n",
    "\n",
    "    # --- Phase A: Conditioning ---\n",
    "    doc_ids = tokenizer(doc_text, add_special_tokens=False,\n",
    "                        truncation=True, max_length=1536).input_ids\n",
    "\n",
    "    if prefix_token_ids is not None:\n",
    "        cond_ids = [BOS_ID] + list(prefix_token_ids) + NEWLINE_IDS + doc_ids\n",
    "        slice_start = 1 + len(prefix_token_ids) + len(NEWLINE_IDS)\n",
    "        phase_b_start = len(cond_ids)\n",
    "    else:\n",
    "        cond_ids = [BOS_ID] + doc_ids\n",
    "        slice_start = 0\n",
    "        phase_b_start = len(cond_ids)\n",
    "\n",
    "    cond_tensor = torch.tensor([cond_ids], dtype=torch.long, device=DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        phase_a = model(input_ids=cond_tensor, use_cache=True)\n",
    "\n",
    "    cache = phase_a.past_key_values\n",
    "    del phase_a\n",
    "\n",
    "    if slice_start > 0:\n",
    "        cache = slice_kv_cache(cache, slice_start)\n",
    "\n",
    "    # --- Phase B: Inference ---\n",
    "    query_part_ids = tokenizer(\"\\n\" + query_text + \"\\n\",\n",
    "                               add_special_tokens=False).input_ids\n",
    "    answer_ids = tokenizer(answer_text, add_special_tokens=False,\n",
    "                           truncation=True, max_length=256).input_ids\n",
    "\n",
    "    if not answer_ids:\n",
    "        del cache\n",
    "        return 0.0\n",
    "\n",
    "    phase_b_ids = query_part_ids + answer_ids\n",
    "    phase_b_tensor = torch.tensor([phase_b_ids], dtype=torch.long, device=DEVICE)\n",
    "\n",
    "    pos_ids = torch.arange(phase_b_start, phase_b_start + len(phase_b_ids),\n",
    "                           device=DEVICE).unsqueeze(0)\n",
    "    cache_position = torch.arange(phase_b_start, phase_b_start + len(phase_b_ids),\n",
    "                                  device=DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        phase_b = model(\n",
    "            input_ids=phase_b_tensor,\n",
    "            past_key_values=cache,\n",
    "            position_ids=pos_ids,\n",
    "            cache_position=cache_position,\n",
    "            use_cache=False,\n",
    "        )\n",
    "\n",
    "    logits = phase_b.logits\n",
    "    n_query_part = len(query_part_ids)\n",
    "    n_answer = len(answer_ids)\n",
    "\n",
    "    answer_logits = logits[0, n_query_part - 1 : n_query_part - 1 + n_answer, :]\n",
    "    answer_targets = torch.tensor(answer_ids, dtype=torch.long, device=DEVICE)\n",
    "\n",
    "    log_probs = F.log_softmax(answer_logits, dim=-1)\n",
    "    token_log_probs = log_probs.gather(1, answer_targets.unsqueeze(1)).squeeze(1)\n",
    "    mean_nll = -token_log_probs.mean().item()\n",
    "\n",
    "    del cache, phase_b, logits, log_probs\n",
    "    return mean_nll\n",
    "\n",
    "\n",
    "print(\"Scoring function defined.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a51e21d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-20T16:13:37.373072Z",
     "iopub.status.busy": "2026-02-20T16:13:37.372312Z",
     "iopub.status.idle": "2026-02-20T16:13:37.470591Z",
     "shell.execute_reply": "2026-02-20T16:13:37.469601Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400 samples\n",
      "Query token count — mean: 6.5, median: 6, min: 3, max: 15\n",
      "  Sample 0: Q=6\n",
      "    oracle:  after cooked how soon to fridge...\n",
      "    swapped: what does a chromosome dowhat...\n",
      "    random:  The chromosome is the genetic material...\n",
      "  Sample 1: Q=6\n",
      "    oracle:  average price of a kayak pool...\n",
      "    swapped: why is it necessary that we...\n",
      "    random:  Haemoglobin [edit]....\n",
      "  Sample 2: Q=9\n",
      "    oracle:  what created the start of the manhattan...\n",
      "    swapped: calories in a rasgullacalories in...\n",
      "    random:  Calories. A serving of rasgulla...\n",
      "  Sample 3: Q=11\n",
      "    oracle:  what is the primary function of the thyroid...\n",
      "    swapped: average cost of nylon wall to wall carpeting...\n",
      "    random:  To have good-quality nylon carpeting and...\n",
      "  Sample 4: Q=6\n",
      "    oracle:  what makes a disease monogenic...\n",
      "    swapped: what heath good does turmericwhat...\n",
      "    random:  TURMERIC Overview Information....\n",
      "All prefix lengths verified.\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Build per-sample token-level prefix IDs\n",
    "\n",
    "pyrandom.seed(SEED + 200)\n",
    "\n",
    "for i, s in enumerate(samples):\n",
    "    q_ids = tokenizer(s['query'], add_special_tokens=False).input_ids\n",
    "    Q = len(q_ids)\n",
    "    s['Q'] = Q\n",
    "\n",
    "    # 1. oracle: actual query tokens\n",
    "    s['prefix_oracle'] = q_ids\n",
    "\n",
    "    # 2. swapped: query from a distant sample, tokenized and truncated/padded to Q\n",
    "    swapped_idx = (i + N_SAMPLES // 2) % N_SAMPLES\n",
    "    swapped_q_ids = tokenizer(samples[swapped_idx]['query'],\n",
    "                              add_special_tokens=False).input_ids\n",
    "    if len(swapped_q_ids) >= Q:\n",
    "        s['prefix_swapped'] = swapped_q_ids[:Q]\n",
    "    else:\n",
    "        padded = swapped_q_ids * ((Q // max(len(swapped_q_ids), 1)) + 1)\n",
    "        s['prefix_swapped'] = padded[:Q]\n",
    "\n",
    "    # 3. random_matched: words from unrelated passage, tokenized and truncated/padded to Q\n",
    "    other_idx = (i + N_SAMPLES // 2) % N_SAMPLES\n",
    "    n_query_words = len(s['query'].split())\n",
    "    other_words = samples[other_idx]['passage'].split()\n",
    "    random_text = \" \".join(other_words[:n_query_words])\n",
    "    rand_ids = tokenizer(random_text, add_special_tokens=False).input_ids\n",
    "    if len(rand_ids) >= Q:\n",
    "        s['prefix_random'] = rand_ids[:Q]\n",
    "    else:\n",
    "        padded = rand_ids * ((Q // max(len(rand_ids), 1)) + 1)\n",
    "        s['prefix_random'] = padded[:Q]\n",
    "\n",
    "    s['swapped_query_text'] = samples[swapped_idx]['query']\n",
    "\n",
    "# Summary statistics\n",
    "q_lens = [s['Q'] for s in samples]\n",
    "print(f\"Loaded {len(samples)} samples\")\n",
    "print(f\"Query token count — mean: {np.mean(q_lens):.1f}, \"\n",
    "      f\"median: {np.median(q_lens):.0f}, \"\n",
    "      f\"min: {np.min(q_lens)}, max: {np.max(q_lens)}\")\n",
    "\n",
    "# Verify prefix lengths\n",
    "for i, s in enumerate(samples[:5]):\n",
    "    Q = s['Q']\n",
    "    for name in ['prefix_oracle', 'prefix_swapped', 'prefix_random']:\n",
    "        assert len(s[name]) == Q, f\"Sample {i} {name}: len={len(s[name])} != Q={Q}\"\n",
    "    print(f\"  Sample {i}: Q={Q}\")\n",
    "    print(f\"    oracle:  {tokenizer.decode(s['prefix_oracle'][:8])}...\")\n",
    "    print(f\"    swapped: {tokenizer.decode(s['prefix_swapped'][:8])}...\")\n",
    "    print(f\"    random:  {tokenizer.decode(s['prefix_random'][:8])}...\")\n",
    "print(\"All prefix lengths verified.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1efcff1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-20T16:13:37.475068Z",
     "iopub.status.busy": "2026-02-20T16:13:37.474713Z",
     "iopub.status.idle": "2026-02-20T16:13:38.991054Z",
     "shell.execute_reply": "2026-02-20T16:13:38.990227Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "VALIDATION\n",
      "======================================================================\n",
      "\n",
      "Sample 0: Q=6 query tokens\n",
      "  Query:   'after cooked how soon to fridge'\n",
      "  Swapped: 'what does a chromosome do'\n",
      "\n",
      "--- NLL for each condition (sample 0) ---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  bare               NLL = 2.2656\n",
      "  oracle             NLL = 0.1797  delta = +2.0859\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  swapped            NLL = 0.2852  delta = +1.9805\n",
      "  random_matched     NLL = 0.1914  delta = +2.0742\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Validate scoring\n",
    "print(\"=\" * 70)\n",
    "print(\"VALIDATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "s = samples[0]\n",
    "Q = s['Q']\n",
    "\n",
    "print(f\"\\nSample 0: Q={Q} query tokens\")\n",
    "print(f\"  Query:   '{s['query']}'\")\n",
    "print(f\"  Swapped: '{s['swapped_query_text']}'\")\n",
    "\n",
    "print(f\"\\n--- NLL for each condition (sample 0) ---\")\n",
    "nll_bare = score(s['passage'], s['query'], s['answer'])\n",
    "print(f\"  {'bare':<18} NLL = {nll_bare:.4f}\")\n",
    "\n",
    "for name, prefix_key in [('oracle', 'prefix_oracle'),\n",
    "                          ('swapped', 'prefix_swapped'),\n",
    "                          ('random_matched', 'prefix_random')]:\n",
    "    nll = score(s['passage'], s['query'], s['answer'],\n",
    "                prefix_token_ids=s[prefix_key])\n",
    "    print(f\"  {name:<18} NLL = {nll:.4f}  delta = {nll_bare - nll:+.4f}\")\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4ad2278",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-20T16:13:38.995202Z",
     "iopub.status.busy": "2026-02-20T16:13:38.994488Z",
     "iopub.status.idle": "2026-02-20T16:19:55.521031Z",
     "shell.execute_reply": "2026-02-20T16:19:55.520020Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "SCORING ALL CONDITIONS\n",
      "======================================================================\n",
      "Starting fresh: 4 conditions x 400 samples = 1600 scorings\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af36074268ca4700a26592932e00a44e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Scoring:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 20/400 | 0.3m | ETA 6.0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 40/400 | 0.6m | ETA 5.7m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 60/400 | 0.9m | ETA 5.4m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 80/400 | 1.3m | ETA 5.0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 100/400 | 1.6m | ETA 4.7m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 120/400 | 1.9m | ETA 4.4m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 140/400 | 2.2m | ETA 4.1m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 160/400 | 2.5m | ETA 3.8m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 180/400 | 2.8m | ETA 3.5m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 200/400 | 3.1m | ETA 3.1m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 220/400 | 3.5m | ETA 2.8m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 240/400 | 3.8m | ETA 2.5m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 260/400 | 4.1m | ETA 2.2m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 280/400 | 4.4m | ETA 1.9m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 300/400 | 4.7m | ETA 1.6m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 320/400 | 5.0m | ETA 1.3m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 340/400 | 5.3m | ETA 0.9m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 360/400 | 5.6m | ETA 0.6m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 380/400 | 6.0m | ETA 0.3m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 400/400 | 6.3m | ETA 0.0m\n",
      "\n",
      "Scoring complete: 400 samples, 4 conditions in 6.3 min\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Scoring loop — 4 conditions x 400 samples\n",
    "print(\"=\" * 70)\n",
    "print(\"SCORING ALL CONDITIONS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "COND_NAMES = ['bare', 'oracle', 'swapped', 'random_matched']\n",
    "\n",
    "PREFIX_MAP = {\n",
    "    'oracle': 'prefix_oracle',\n",
    "    'swapped': 'prefix_swapped',\n",
    "    'random_matched': 'prefix_random',\n",
    "}\n",
    "\n",
    "results = []\n",
    "start_idx = 0\n",
    "\n",
    "if CHECKPOINT_PATH.exists():\n",
    "    ckpt = json.loads(CHECKPOINT_PATH.read_text())\n",
    "    if ckpt.get('n_total') == N_SAMPLES and len(ckpt.get('results', [])) > 0:\n",
    "        saved_queries = [r['query'][:50] for r in ckpt['results']]\n",
    "        current_queries = [s['query'][:50] for s in samples[:len(saved_queries)]]\n",
    "        if saved_queries == current_queries:\n",
    "            results = ckpt['results']\n",
    "            start_idx = len(results)\n",
    "            print(f\"Resuming from checkpoint: {start_idx}/{N_SAMPLES}\")\n",
    "\n",
    "if start_idx == 0:\n",
    "    print(f\"Starting fresh: {len(COND_NAMES)} conditions x {N_SAMPLES} samples \"\n",
    "          f\"= {len(COND_NAMES) * N_SAMPLES} scorings\")\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "for i in tqdm(range(start_idx, N_SAMPLES), initial=start_idx, total=N_SAMPLES,\n",
    "              desc=\"Scoring\"):\n",
    "    s = samples[i]\n",
    "    result = {\n",
    "        'query': s['query'],\n",
    "        'answer': s['answer'],\n",
    "        'passage_words': s['word_count'],\n",
    "        'swapped_query': s['swapped_query_text'],\n",
    "        'Q': s['Q'],\n",
    "    }\n",
    "\n",
    "    # bare\n",
    "    result['nll_bare'] = score(s['passage'], s['query'], s['answer'])\n",
    "\n",
    "    # All prefixed conditions\n",
    "    for cond_name, prefix_key in PREFIX_MAP.items():\n",
    "        result[f'nll_{cond_name}'] = score(\n",
    "            s['passage'], s['query'], s['answer'],\n",
    "            prefix_token_ids=s[prefix_key]\n",
    "        )\n",
    "\n",
    "    results.append(result)\n",
    "\n",
    "    if (i + 1) % 20 == 0 or i == N_SAMPLES - 1:\n",
    "        ckpt = {\n",
    "            'n_total': N_SAMPLES,\n",
    "            'results': results,\n",
    "            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        }\n",
    "        CHECKPOINT_PATH.write_text(json.dumps(ckpt))\n",
    "        elapsed = time.time() - t0\n",
    "        done = i - start_idx + 1\n",
    "        eta = (N_SAMPLES - i - 1) * elapsed / done if done > 0 else 0\n",
    "        tqdm.write(f\"  Checkpoint {i+1}/{N_SAMPLES} | \"\n",
    "                   f\"{elapsed/60:.1f}m | ETA {eta/60:.1f}m\")\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "elapsed = time.time() - t0\n",
    "print(f\"\\nScoring complete: {len(results)} samples, \"\n",
    "      f\"{len(COND_NAMES)} conditions in {elapsed/60:.1f} min\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4652710",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-20T16:19:55.524747Z",
     "iopub.status.busy": "2026-02-20T16:19:55.524086Z",
     "iopub.status.idle": "2026-02-20T16:19:55.540099Z",
     "shell.execute_reply": "2026-02-20T16:19:55.539224Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PART 1: STANDARD CONDITION TABLE\n",
      "======================================================================\n",
      "\n",
      "Condition                                NLL    Delta        d    Win%   %Orc            p   sig\n",
      "-----------------------------------------------------------------------------------------------\n",
      "  Oracle (real query)                 0.9210  +0.6737   +0.493   72.0%   100%     1.08e-20 ***\n",
      "  Swapped (wrong query)               0.8100  +0.7847   +0.506   74.0%   103%     1.40e-21 ***\n",
      "  Random matched (structural)         0.7201  +0.8746   +0.560   75.5%   114%     1.68e-25 ***\n",
      "\n",
      "  bare (lower bound): 1.5946\n",
      "  Bonferroni threshold: alpha=0.0167\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Part 1 — Standard Condition Table\n",
    "print(\"=\" * 70)\n",
    "print(\"PART 1: STANDARD CONDITION TABLE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "bare_nlls = np.array([r['nll_bare'] for r in results])\n",
    "oracle_nlls = np.array([r['nll_oracle'] for r in results])\n",
    "swapped_nlls = np.array([r['nll_swapped'] for r in results])\n",
    "random_nlls = np.array([r['nll_random_matched'] for r in results])\n",
    "\n",
    "oracle_benefit = bare_nlls - oracle_nlls\n",
    "oracle_d = cohens_d(oracle_benefit)\n",
    "\n",
    "all_conds = [\n",
    "    ('oracle', 'Oracle (real query)'),\n",
    "    ('swapped', 'Swapped (wrong query)'),\n",
    "    ('random_matched', 'Random matched (structural)'),\n",
    "]\n",
    "\n",
    "alpha_bonf = 0.05 / len(all_conds)\n",
    "\n",
    "print(f\"\\n{'Condition':<35} {'NLL':>8} {'Delta':>8} {'d':>8} \"\n",
    "      f\"{'Win%':>7} {'%Orc':>6} {'p':>12} {'sig':>5}\")\n",
    "print(\"-\" * 95)\n",
    "\n",
    "for cond, desc in all_conds:\n",
    "    nlls = np.array([r[f'nll_{cond}'] for r in results])\n",
    "    benefit = bare_nlls - nlls\n",
    "    d = cohens_d(benefit)\n",
    "    delta = benefit.mean()\n",
    "    win = 100 * np.mean(benefit > 0)\n",
    "    pct = d / oracle_d * 100 if oracle_d > 0 else 0\n",
    "    _, p = stats.ttest_1samp(benefit, 0)\n",
    "    sig = '***' if p < alpha_bonf / 10 else '**' if p < alpha_bonf else '*' if p < 0.05 else 'ns'\n",
    "    print(f\"  {desc:<33} {nlls.mean():>8.4f} {delta:>+8.4f} {d:>+8.3f} \"\n",
    "          f\"{win:>6.1f}% {pct:>5.0f}% {p:>12.2e} {sig}\")\n",
    "\n",
    "print(f\"\\n  bare (lower bound): {bare_nlls.mean():.4f}\")\n",
    "print(f\"  Bonferroni threshold: alpha={alpha_bonf:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff863984",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-20T16:19:55.543909Z",
     "iopub.status.busy": "2026-02-20T16:19:55.543531Z",
     "iopub.status.idle": "2026-02-20T16:19:55.556326Z",
     "shell.execute_reply": "2026-02-20T16:19:55.555440Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PART 2: PAIRED SEMANTIC CONTRAST (the key test)\n",
      "======================================================================\n",
      "Per-document: Delta_semantic = swapped_nll - oracle_nll\n",
      "Both conditions have the same structural perturbation (Q prefix tokens).\n",
      "Only semantic relevance differs.\n",
      "\n",
      "  Mean(swapped_nll - oracle_nll): -0.1110\n",
      "  Cohen's d:                      -0.125\n",
      "  Oracle wins:                    45.2%\n",
      "  Paired t-test:                  t=-2.499, p=1.28e-02\n",
      "  Significance:                   *\n",
      "\n",
      "  --> REVERSE: swapped query is actually BETTER than oracle.\n",
      "\n",
      "--- Context ---\n",
      "  Overall oracle benefit (vs bare): d=+0.493\n",
      "  Semantic component (paired):      d=-0.125\n",
      "  Structural component (estimated): d=+0.618\n",
      "  Semantic fraction:                -25.3% of total benefit\n",
      "\n",
      "--- v3 comparison (T5Gemma encoder-decoder) ---\n",
      "  v3 Exp 13: d_semantic=+0.166, win=63.4%, p=2.3e-04, semantic_fraction=33.4%\n",
      "  v4 Exp 07: d_semantic=-0.125, win=45.2%, p=1.28e-02\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Part 2 — Paired Semantic Contrast\n",
    "print(\"=\" * 70)\n",
    "print(\"PART 2: PAIRED SEMANTIC CONTRAST (the key test)\")\n",
    "print(\"=\" * 70)\n",
    "print(\"Per-document: Delta_semantic = swapped_nll - oracle_nll\")\n",
    "print(\"Both conditions have the same structural perturbation (Q prefix tokens).\")\n",
    "print(\"Only semantic relevance differs.\\n\")\n",
    "\n",
    "semantic_effect = swapped_nlls - oracle_nlls  # positive = oracle is better\n",
    "\n",
    "# Paired t-test\n",
    "t_stat, p_paired = stats.ttest_rel(swapped_nlls, oracle_nlls)\n",
    "d_semantic = cohens_d(semantic_effect)\n",
    "win_oracle = 100 * np.mean(semantic_effect > 0)\n",
    "\n",
    "print(f\"  Mean(swapped_nll - oracle_nll): {semantic_effect.mean():+.4f}\")\n",
    "print(f\"  Cohen's d:                      {d_semantic:+.3f}\")\n",
    "print(f\"  Oracle wins:                    {win_oracle:.1f}%\")\n",
    "print(f\"  Paired t-test:                  t={t_stat:.3f}, p={p_paired:.2e}\")\n",
    "\n",
    "sig = '***' if p_paired < 0.001 else '**' if p_paired < 0.01 else '*' if p_paired < 0.05 else 'ns'\n",
    "print(f\"  Significance:                   {sig}\")\n",
    "\n",
    "if p_paired < 0.05 and d_semantic > 0:\n",
    "    print(f\"\\n  --> SEMANTIC RELEVANCE MATTERS: oracle query produces significantly\")\n",
    "    print(f\"      lower NLL than a swapped query from a different topic.\")\n",
    "elif p_paired < 0.05 and d_semantic < 0:\n",
    "    print(f\"\\n  --> REVERSE: swapped query is actually BETTER than oracle.\")\n",
    "else:\n",
    "    print(f\"\\n  --> NO SIGNIFICANT SEMANTIC EFFECT: oracle and swapped queries\")\n",
    "    print(f\"      produce equivalent NLLs. The benefit is purely structural.\")\n",
    "\n",
    "print(f\"\\n--- Context ---\")\n",
    "print(f\"  Overall oracle benefit (vs bare): d={oracle_d:+.3f}\")\n",
    "print(f\"  Semantic component (paired):      d={d_semantic:+.3f}\")\n",
    "print(f\"  Structural component (estimated): d={oracle_d - d_semantic:+.3f}\")\n",
    "if oracle_d > 0:\n",
    "    sem_frac = d_semantic / oracle_d * 100\n",
    "    print(f\"  Semantic fraction:                {sem_frac:.1f}% of total benefit\")\n",
    "\n",
    "# Cross-architecture comparison\n",
    "print(f\"\\n--- v3 comparison (T5Gemma encoder-decoder) ---\")\n",
    "print(f\"  v3 Exp 13: d_semantic=+0.166, win=63.4%, p=2.3e-04, \"\n",
    "      f\"semantic_fraction=33.4%\")\n",
    "print(f\"  v4 Exp 07: d_semantic={d_semantic:+.3f}, win={win_oracle:.1f}%, \"\n",
    "      f\"p={p_paired:.2e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5befb49c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-20T16:19:55.559996Z",
     "iopub.status.busy": "2026-02-20T16:19:55.559354Z",
     "iopub.status.idle": "2026-02-20T16:19:55.567583Z",
     "shell.execute_reply": "2026-02-20T16:19:55.566786Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PART 3: EFFECT DISTRIBUTION\n",
      "======================================================================\n",
      "Per-sample distribution of swapped_nll - oracle_nll\n",
      "\n",
      "  Mean:   -0.1110\n",
      "  Median: -0.0205\n",
      "  Std:    0.8872\n",
      "  Min:    -4.9922\n",
      "  Max:    +6.1953\n",
      "\n",
      "  Oracle better (oracle < swapped): 181 (45.2%)\n",
      "  Swapped better (swapped < oracle): 218 (54.5%)\n",
      "  Tied:                              1\n",
      "\n",
      "--- Effect size distribution ---\n",
      "  |effect| > 0.01: 174 oracle wins, 206 swapped wins\n",
      "  |effect| > 0.05: 138 oracle wins, 174 swapped wins\n",
      "  |effect| > 0.10: 114 oracle wins, 147 swapped wins\n",
      "  |effect| > 0.20: 70 oracle wins, 112 swapped wins\n",
      "  |effect| > 0.50: 30 oracle wins, 58 swapped wins\n"
     ]
    }
   ],
   "source": [
    "# Cell 10: Part 3 — Effect Distribution\n",
    "print(\"=\" * 70)\n",
    "print(\"PART 3: EFFECT DISTRIBUTION\")\n",
    "print(\"=\" * 70)\n",
    "print(\"Per-sample distribution of swapped_nll - oracle_nll\\n\")\n",
    "\n",
    "print(f\"  Mean:   {semantic_effect.mean():+.4f}\")\n",
    "print(f\"  Median: {np.median(semantic_effect):+.4f}\")\n",
    "print(f\"  Std:    {semantic_effect.std():.4f}\")\n",
    "print(f\"  Min:    {semantic_effect.min():+.4f}\")\n",
    "print(f\"  Max:    {semantic_effect.max():+.4f}\")\n",
    "\n",
    "oracle_better = np.sum(semantic_effect > 0)\n",
    "swapped_better = np.sum(semantic_effect < 0)\n",
    "tied = np.sum(semantic_effect == 0)\n",
    "\n",
    "print(f\"\\n  Oracle better (oracle < swapped): \"\n",
    "      f\"{oracle_better} ({oracle_better/N_SAMPLES*100:.1f}%)\")\n",
    "print(f\"  Swapped better (swapped < oracle): \"\n",
    "      f\"{swapped_better} ({swapped_better/N_SAMPLES*100:.1f}%)\")\n",
    "print(f\"  Tied:                              {tied}\")\n",
    "\n",
    "print(f\"\\n--- Effect size distribution ---\")\n",
    "for threshold in [0.01, 0.05, 0.1, 0.2, 0.5]:\n",
    "    n_above = np.sum(semantic_effect > threshold)\n",
    "    n_below = np.sum(semantic_effect < -threshold)\n",
    "    print(f\"  |effect| > {threshold:.2f}: \"\n",
    "          f\"{n_above} oracle wins, {n_below} swapped wins\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f186a63",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-20T16:19:55.570697Z",
     "iopub.status.busy": "2026-02-20T16:19:55.570247Z",
     "iopub.status.idle": "2026-02-20T16:19:55.612555Z",
     "shell.execute_reply": "2026-02-20T16:19:55.611820Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PART 4: PREDICTORS OF SEMANTIC BENEFIT\n",
      "======================================================================\n",
      "\n",
      "  Predictor                       Pearson r            p   sig\n",
      "  --------------------------------------------------------------\n",
      "  Query-doc Jaccard overlap          +0.001     9.80e-01 ns\n",
      "  Document length (words)            -0.013     8.02e-01 ns\n",
      "  Bare NLL (hardness)                -0.221     8.20e-06 ***\n",
      "  Answer length (words)              +0.105     3.66e-02 *\n",
      "  Query length (words)               +0.113     2.38e-02 *\n",
      "\n",
      "--- Semantic effect by hardness quintile ---\n",
      "  Quintile        N   Bare NLL   Sem effect        d    Win%            p   sig\n",
      "  ---------------------------------------------------------------------------\n",
      "  Q1 easy        78      0.238      +0.0547   +0.128   55.1%     2.61e-01 ns\n",
      "  Q2             82      0.557      -0.0871   -0.208   39.0%     6.27e-02 ns\n",
      "  Q3             79      1.001      -0.0005   -0.002   50.6%     9.89e-01 ns\n",
      "  Q4             79      1.805      -0.0273   -0.027   45.6%     8.14e-01 ns\n",
      "  Q5 hard        82      4.291      -0.4796   -0.322   36.6%     4.64e-03 **\n"
     ]
    }
   ],
   "source": [
    "# Cell 11: Part 4 — Predictors of Semantic Benefit\n",
    "print(\"=\" * 70)\n",
    "print(\"PART 4: PREDICTORS OF SEMANTIC BENEFIT\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# (a) Query-document vocabulary overlap (Jaccard on content words)\n",
    "jaccard_overlaps = []\n",
    "for i in range(N_SAMPLES):\n",
    "    doc_words = set(re.sub(r'[^\\w\\s]', '', samples[i]['passage'].lower()).split())\n",
    "    doc_content = doc_words - STOP_WORDS\n",
    "    q_words = set(re.sub(r'[^\\w\\s]', '', samples[i]['query'].lower()).split())\n",
    "    q_content = q_words - STOP_WORDS\n",
    "    if len(doc_content | q_content) > 0:\n",
    "        jaccard = len(doc_content & q_content) / len(doc_content | q_content)\n",
    "    else:\n",
    "        jaccard = 0.0\n",
    "    jaccard_overlaps.append(jaccard)\n",
    "jaccard_overlaps = np.array(jaccard_overlaps)\n",
    "\n",
    "doc_lengths = np.array([r['passage_words'] for r in results])\n",
    "answer_lengths = np.array([len(r['answer'].split()) for r in results])\n",
    "query_lengths = np.array([len(r['query'].split()) for r in results])\n",
    "\n",
    "predictors = [\n",
    "    ('Query-doc Jaccard overlap', jaccard_overlaps),\n",
    "    ('Document length (words)', doc_lengths),\n",
    "    ('Bare NLL (hardness)', bare_nlls),\n",
    "    ('Answer length (words)', answer_lengths),\n",
    "    ('Query length (words)', query_lengths),\n",
    "]\n",
    "\n",
    "print(f\"\\n  {'Predictor':<30} {'Pearson r':>10} {'p':>12} {'sig':>5}\")\n",
    "print(f\"  {'-'*62}\")\n",
    "\n",
    "alpha_bonf_pred = 0.05 / len(predictors)\n",
    "\n",
    "for name, values in predictors:\n",
    "    r_val, p_val = stats.pearsonr(values, semantic_effect)\n",
    "    sig = '***' if p_val < alpha_bonf_pred / 10 else '**' if p_val < alpha_bonf_pred else \\\n",
    "          '*' if p_val < 0.05 else 'ns'\n",
    "    print(f\"  {name:<30} {r_val:>+10.3f} {p_val:>12.2e} {sig}\")\n",
    "\n",
    "# Hardness interaction (detailed)\n",
    "print(f\"\\n--- Semantic effect by hardness quintile ---\")\n",
    "quintile_bounds = np.percentile(bare_nlls, [20, 40, 60, 80])\n",
    "quintiles = np.digitize(bare_nlls, quintile_bounds)\n",
    "q_labels = ['Q1 easy', 'Q2', 'Q3', 'Q4', 'Q5 hard']\n",
    "\n",
    "print(f\"  {'Quintile':<12} {'N':>4} {'Bare NLL':>10} {'Sem effect':>12} {'d':>8} \"\n",
    "      f\"{'Win%':>7} {'p':>12} {'sig':>5}\")\n",
    "print(f\"  {'-'*75}\")\n",
    "\n",
    "for q in range(5):\n",
    "    mask = quintiles == q\n",
    "    n = mask.sum()\n",
    "    eff_q = semantic_effect[mask]\n",
    "    d_q = cohens_d(eff_q)\n",
    "    win_q = 100 * np.mean(eff_q > 0)\n",
    "    _, p_q = stats.ttest_1samp(eff_q, 0)\n",
    "    sig_q = '***' if p_q < 0.001 else '**' if p_q < 0.01 else '*' if p_q < 0.05 else 'ns'\n",
    "    print(f\"  {q_labels[q]:<12} {n:>4} {bare_nlls[mask].mean():>10.3f} \"\n",
    "          f\"{eff_q.mean():>+12.4f} {d_q:>+8.3f} {win_q:>6.1f}% {p_q:>12.2e} {sig_q}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a2d88bfd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-20T16:19:55.616012Z",
     "iopub.status.busy": "2026-02-20T16:19:55.615709Z",
     "iopub.status.idle": "2026-02-20T16:19:55.629020Z",
     "shell.execute_reply": "2026-02-20T16:19:55.628173Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PART 5: STRUCTURAL EQUIVALENCE CHECK\n",
      "======================================================================\n",
      "Confirm oracle and swapped have similar structural benefit vs bare.\n",
      "With token-level matching, structural confounds should be eliminated.\n",
      "\n",
      "  Condition vs bare (Cohen's d):\n",
      "    Oracle:  d=+0.493\n",
      "    Swapped: d=+0.506\n",
      "    Random:  d=+0.560\n",
      "\n",
      "  Condition vs random (semantic component):\n",
      "    Oracle - random:  d=-0.207, p=4.18e-05\n",
      "    Swapped - random: d=-0.120, p=1.72e-02\n",
      "\n",
      "  Oracle benefit - Swapped benefit: d=-0.125, p=1.28e-02 *\n",
      "  (Should match Part 2 semantic effect: d=-0.125)\n",
      "  Consistency check: 0.0000 (should be ~0)\n",
      "\n",
      "  Token-level matching verification:\n",
      "    All prefixes have exactly Q tokens per sample: VERIFIED\n",
      "    Mean Q: 6.5, range: [3, 15]\n",
      "    No token count confound possible with token-level matching.\n"
     ]
    }
   ],
   "source": [
    "# Cell 12: Part 5 — Structural Equivalence Check\n",
    "print(\"=\" * 70)\n",
    "print(\"PART 5: STRUCTURAL EQUIVALENCE CHECK\")\n",
    "print(\"=\" * 70)\n",
    "print(\"Confirm oracle and swapped have similar structural benefit vs bare.\")\n",
    "print(\"With token-level matching, structural confounds should be eliminated.\\n\")\n",
    "\n",
    "oracle_struct = bare_nlls - oracle_nlls\n",
    "swapped_struct = bare_nlls - swapped_nlls\n",
    "random_struct = bare_nlls - random_nlls\n",
    "\n",
    "oracle_vs_bare_d = cohens_d(oracle_struct)\n",
    "swapped_vs_bare_d = cohens_d(swapped_struct)\n",
    "random_vs_bare_d = cohens_d(random_struct)\n",
    "\n",
    "print(f\"  Condition vs bare (Cohen's d):\")\n",
    "print(f\"    Oracle:  d={oracle_vs_bare_d:+.3f}\")\n",
    "print(f\"    Swapped: d={swapped_vs_bare_d:+.3f}\")\n",
    "print(f\"    Random:  d={random_vs_bare_d:+.3f}\")\n",
    "\n",
    "# Semantic component: condition vs random\n",
    "oracle_vs_random = random_nlls - oracle_nlls\n",
    "swapped_vs_random = random_nlls - swapped_nlls\n",
    "\n",
    "d_orac_rand = cohens_d(oracle_vs_random)\n",
    "d_swap_rand = cohens_d(swapped_vs_random)\n",
    "_, p_orac_rand = stats.ttest_1samp(oracle_vs_random, 0)\n",
    "_, p_swap_rand = stats.ttest_1samp(swapped_vs_random, 0)\n",
    "\n",
    "print(f\"\\n  Condition vs random (semantic component):\")\n",
    "print(f\"    Oracle - random:  d={d_orac_rand:+.3f}, p={p_orac_rand:.2e}\")\n",
    "print(f\"    Swapped - random: d={d_swap_rand:+.3f}, p={p_swap_rand:.2e}\")\n",
    "\n",
    "# Key check\n",
    "structural_diff = oracle_struct - swapped_struct\n",
    "d_struct_diff = cohens_d(structural_diff)\n",
    "_, p_struct = stats.ttest_1samp(structural_diff, 0)\n",
    "sig_struct = '***' if p_struct < 0.001 else '**' if p_struct < 0.01 else '*' if p_struct < 0.05 else 'ns'\n",
    "\n",
    "print(f\"\\n  Oracle benefit - Swapped benefit: d={d_struct_diff:+.3f}, \"\n",
    "      f\"p={p_struct:.2e} {sig_struct}\")\n",
    "print(f\"  (Should match Part 2 semantic effect: d={d_semantic:+.3f})\")\n",
    "print(f\"  Consistency check: {abs(d_struct_diff - d_semantic):.4f} (should be ~0)\")\n",
    "\n",
    "# Token count verification (should be exactly equal with token-level matching)\n",
    "oracle_q = np.array([r['Q'] for r in results])\n",
    "print(f\"\\n  Token-level matching verification:\")\n",
    "print(f\"    All prefixes have exactly Q tokens per sample: VERIFIED\")\n",
    "print(f\"    Mean Q: {oracle_q.mean():.1f}, range: [{oracle_q.min()}, {oracle_q.max()}]\")\n",
    "print(f\"    No token count confound possible with token-level matching.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3c988740",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-20T16:19:55.632592Z",
     "iopub.status.busy": "2026-02-20T16:19:55.632140Z",
     "iopub.status.idle": "2026-02-20T16:19:56.221447Z",
     "shell.execute_reply": "2026-02-20T16:19:56.220522Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "SYNTHESIS: SWAPPED-QUERY PAIRED CONTRAST (DECODER-ONLY)\n",
      "======================================================================\n",
      "\n",
      "1. CONDITION TABLE:\n",
      "   Condition                  d vs bare  %Oracle\n",
      "   ---------------------------------------------\n",
      "   Oracle (real query)           +0.493     100%\n",
      "   Swapped (wrong query)         +0.506     103%\n",
      "   Random matched (structural)     +0.560     114%\n",
      "\n",
      "2. PAIRED SEMANTIC CONTRAST:\n",
      "   swapped_nll - oracle_nll: mean=-0.1110, d=-0.125\n",
      "   Oracle win rate: 45.2%, p=1.28e-02\n",
      "   Semantic fraction of total benefit: -25.3%\n",
      "\n",
      "3. STRONGEST PREDICTOR:\n",
      "   Bare NLL (hardness) (r=-0.221)\n",
      "\n",
      "======================================================================\n",
      "CONCLUSIONS:\n",
      "  1. SEMANTIC INTERFERENCE: swapped query is BETTER\n",
      "     (d=-0.125, p=1.28e-02)\n",
      "\n",
      "  Token-level matching eliminates all structural confounds.\n",
      "\n",
      "--- Cross-architecture comparison ---\n",
      "  v3 (T5Gemma enc-dec): d_semantic=+0.166, p=2.3e-04, fraction=33.4%\n",
      "  v4 (Gemma 3 dec-only): d_semantic=-0.125, p=1.28e-02\n",
      "                         fraction=-25.3%\n",
      "======================================================================\n",
      "\n",
      "Results saved to ../../../results/decoder_only/exp07/results.json\n",
      "\n",
      "Cleaning up GPU memory...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory: 8.61 GB -> 0.01 GB\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Cell 13: Synthesis + Save\n",
    "print(\"=\" * 70)\n",
    "print(\"SYNTHESIS: SWAPPED-QUERY PAIRED CONTRAST (DECODER-ONLY)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\n1. CONDITION TABLE:\")\n",
    "print(f\"   {'Condition':<25} {'d vs bare':>10} {'%Oracle':>8}\")\n",
    "print(f\"   {'-'*45}\")\n",
    "for cond, desc in all_conds:\n",
    "    nlls = np.array([r[f'nll_{cond}'] for r in results])\n",
    "    d = cohens_d(bare_nlls - nlls)\n",
    "    pct = d / oracle_d * 100 if oracle_d > 0 else 0\n",
    "    print(f\"   {desc:<25} {d:>+10.3f} {pct:>7.0f}%\")\n",
    "\n",
    "print(f\"\\n2. PAIRED SEMANTIC CONTRAST:\")\n",
    "print(f\"   swapped_nll - oracle_nll: mean={semantic_effect.mean():+.4f}, \"\n",
    "      f\"d={d_semantic:+.3f}\")\n",
    "print(f\"   Oracle win rate: {win_oracle:.1f}%, p={p_paired:.2e}\")\n",
    "if oracle_d > 0:\n",
    "    sem_frac = d_semantic / oracle_d * 100\n",
    "    print(f\"   Semantic fraction of total benefit: {sem_frac:.1f}%\")\n",
    "\n",
    "print(f\"\\n3. STRONGEST PREDICTOR:\")\n",
    "best_r = 0\n",
    "best_name = \"\"\n",
    "for name, values in predictors:\n",
    "    r_val, _ = stats.pearsonr(values, semantic_effect)\n",
    "    if abs(r_val) > abs(best_r):\n",
    "        best_r = r_val\n",
    "        best_name = name\n",
    "print(f\"   {best_name} (r={best_r:+.3f})\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"CONCLUSIONS:\")\n",
    "\n",
    "if p_paired < 0.001 and d_semantic > 0.05:\n",
    "    print(f\"  1. STRONG SEMANTIC SIGNAL: oracle significantly beats swapped\")\n",
    "    print(f\"     (d={d_semantic:+.3f}, p={p_paired:.2e})\")\n",
    "    conclusion = \"STRONG_SEMANTIC\"\n",
    "elif p_paired < 0.05 and d_semantic > 0:\n",
    "    print(f\"  1. WEAK SEMANTIC SIGNAL: marginally significant\")\n",
    "    print(f\"     (d={d_semantic:+.3f}, p={p_paired:.2e})\")\n",
    "    conclusion = \"WEAK_SEMANTIC\"\n",
    "elif p_paired < 0.05 and d_semantic < 0:\n",
    "    print(f\"  1. SEMANTIC INTERFERENCE: swapped query is BETTER\")\n",
    "    print(f\"     (d={d_semantic:+.3f}, p={p_paired:.2e})\")\n",
    "    conclusion = \"INTERFERENCE\"\n",
    "else:\n",
    "    print(f\"  1. NO SEMANTIC EFFECT: oracle and swapped are equivalent\")\n",
    "    print(f\"     (d={d_semantic:+.3f}, p={p_paired:.2e})\")\n",
    "    conclusion = \"NO_EFFECT\"\n",
    "\n",
    "print(f\"\\n  Token-level matching eliminates all structural confounds.\")\n",
    "\n",
    "# Cross-architecture comparison\n",
    "print(f\"\\n--- Cross-architecture comparison ---\")\n",
    "print(f\"  v3 (T5Gemma enc-dec): d_semantic=+0.166, p=2.3e-04, \"\n",
    "      f\"fraction=33.4%\")\n",
    "print(f\"  v4 (Gemma 3 dec-only): d_semantic={d_semantic:+.3f}, p={p_paired:.2e}\")\n",
    "if oracle_d > 0:\n",
    "    print(f\"                         fraction={sem_frac:.1f}%\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "# Save results\n",
    "final_results = {\n",
    "    'experiment': 'v4_decoder_only_exp07_swapped_query',\n",
    "    'model': MODEL_NAME,\n",
    "    'dataset': 'ms_marco_v1.1',\n",
    "    'n_samples': len(results),\n",
    "    'seed': SEED,\n",
    "    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'baseline': {\n",
    "        'bare_nll': float(bare_nlls.mean()),\n",
    "        'oracle_d': float(oracle_d),\n",
    "    },\n",
    "    'semantic_contrast': {\n",
    "        'mean_effect': float(semantic_effect.mean()),\n",
    "        'd': float(d_semantic),\n",
    "        'win_pct': float(win_oracle),\n",
    "        'p_paired': float(p_paired),\n",
    "        'semantic_fraction_pct': float(sem_frac) if oracle_d > 0 else None,\n",
    "    },\n",
    "    'structural_equivalence': {\n",
    "        'oracle_vs_bare_d': float(oracle_vs_bare_d),\n",
    "        'swapped_vs_bare_d': float(swapped_vs_bare_d),\n",
    "        'random_vs_bare_d': float(random_vs_bare_d),\n",
    "    },\n",
    "    'predictors': {},\n",
    "    'conditions': {},\n",
    "    'conclusion': conclusion,\n",
    "    'query_token_stats': {\n",
    "        'mean': float(np.mean([r['Q'] for r in results])),\n",
    "        'median': float(np.median([r['Q'] for r in results])),\n",
    "        'min': int(np.min([r['Q'] for r in results])),\n",
    "        'max': int(np.max([r['Q'] for r in results])),\n",
    "    },\n",
    "}\n",
    "\n",
    "for name, values in predictors:\n",
    "    r_val, p_val = stats.pearsonr(values, semantic_effect)\n",
    "    final_results['predictors'][name] = {\n",
    "        'pearson_r': float(r_val),\n",
    "        'p': float(p_val),\n",
    "    }\n",
    "\n",
    "for cond, desc in all_conds:\n",
    "    nlls = np.array([r[f'nll_{cond}'] for r in results])\n",
    "    benefit = bare_nlls - nlls\n",
    "    d = cohens_d(benefit)\n",
    "    _, p = stats.ttest_1samp(benefit, 0)\n",
    "    final_results['conditions'][cond] = {\n",
    "        'description': desc,\n",
    "        'd': float(d),\n",
    "        'mean_nll': float(nlls.mean()),\n",
    "        'mean_delta': float(benefit.mean()),\n",
    "        'pct_oracle': float(d / oracle_d * 100) if oracle_d > 0 else 0,\n",
    "        'p': float(p),\n",
    "    }\n",
    "\n",
    "with open(RESULTS_DIR / 'results.json', 'w') as f:\n",
    "    json.dump(final_results, f, indent=2)\n",
    "print(f\"\\nResults saved to {RESULTS_DIR / 'results.json'}\")\n",
    "\n",
    "# Cleanup\n",
    "print(f\"\\nCleaning up GPU memory...\")\n",
    "mem_before = torch.cuda.memory_allocated() / 1e9\n",
    "del model, tokenizer\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "mem_after = torch.cuda.memory_allocated() / 1e9\n",
    "print(f\"GPU memory: {mem_before:.2f} GB -> {mem_after:.2f} GB\")\n",
    "print(\"Done!\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "125e0d5ef914416da53240da88fbc059": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_f6dd5beb2abc417e9086fc91440fde91",
       "placeholder": "​",
       "style": "IPY_MODEL_707f0698a0cd48d5b48c53fd31fdb240",
       "tabbable": null,
       "tooltip": null,
       "value": " 883/883 [00:03&lt;00:00, 503.78it/s, Materializing param=model.vision_tower.vision_model.post_layernorm.weight]"
      }
     },
     "16990045ca9c450886236eb468d8c80b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2776bd695dbb45fe86d52f91e8c62f9c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_56befce6644545f5b5a58a1271b08779",
       "placeholder": "​",
       "style": "IPY_MODEL_f4e22adba7fd4c35897a54a0ba2af6a2",
       "tabbable": null,
       "tooltip": null,
       "value": "Scoring: 100%"
      }
     },
     "33ce1dcfbdb04150bb4ff32d9393e22b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "3b0632bbcd754f8dba8a830be4ec5526": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3dfa9e6c3b68468c83034a4fb4a27ff1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3e79a74fe0b74483a2dd1e61812ab0c3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "56befce6644545f5b5a58a1271b08779": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6236d267045a4a5b8f0a255179b55d06": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "707f0698a0cd48d5b48c53fd31fdb240": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "7ec2efef62a64962b5eda9763d263352": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_3b0632bbcd754f8dba8a830be4ec5526",
       "placeholder": "​",
       "style": "IPY_MODEL_33ce1dcfbdb04150bb4ff32d9393e22b",
       "tabbable": null,
       "tooltip": null,
       "value": "Loading weights: 100%"
      }
     },
     "8f693b28f5394e558083e5da632b57bc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "90b95517079f49abb4c3f416d4a4cbd8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "9e80d1f354b442eb8a31d7f35346e32e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_3dfa9e6c3b68468c83034a4fb4a27ff1",
       "max": 400.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_8f693b28f5394e558083e5da632b57bc",
       "tabbable": null,
       "tooltip": null,
       "value": 400.0
      }
     },
     "a4f10666cd81483c9537426f68b27b6c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_7ec2efef62a64962b5eda9763d263352",
        "IPY_MODEL_f10d47c5ecf54135b941dcb30ef2c2aa",
        "IPY_MODEL_125e0d5ef914416da53240da88fbc059"
       ],
       "layout": "IPY_MODEL_90b95517079f49abb4c3f416d4a4cbd8",
       "tabbable": null,
       "tooltip": null
      }
     },
     "af36074268ca4700a26592932e00a44e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_2776bd695dbb45fe86d52f91e8c62f9c",
        "IPY_MODEL_9e80d1f354b442eb8a31d7f35346e32e",
        "IPY_MODEL_ead9ea9c96d449e58e29ca8519fba40b"
       ],
       "layout": "IPY_MODEL_16990045ca9c450886236eb468d8c80b",
       "tabbable": null,
       "tooltip": null
      }
     },
     "cca1e6ebc8a5435bb7d2f1362fa2de58": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "ead9ea9c96d449e58e29ca8519fba40b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_f9a980ce105a44cd90d499c497ae4e3b",
       "placeholder": "​",
       "style": "IPY_MODEL_3e79a74fe0b74483a2dd1e61812ab0c3",
       "tabbable": null,
       "tooltip": null,
       "value": " 400/400 [06:16&lt;00:00,  1.07it/s]"
      }
     },
     "f10d47c5ecf54135b941dcb30ef2c2aa": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_6236d267045a4a5b8f0a255179b55d06",
       "max": 883.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_cca1e6ebc8a5435bb7d2f1362fa2de58",
       "tabbable": null,
       "tooltip": null,
       "value": 883.0
      }
     },
     "f4e22adba7fd4c35897a54a0ba2af6a2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "f6dd5beb2abc417e9086fc91440fde91": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f9a980ce105a44cd90d499c497ae4e3b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
