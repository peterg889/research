{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0471c74",
   "metadata": {},
   "source": [
    "# Decoder-Only Exp 07: Swapped-Query Paired Contrasts\n",
    "\n",
    "## Motivation\n",
    "\n",
    "Port of v3 Exp 13 to decoder-only two-phase KV cache scoring.\n",
    "v3 Exp 13 found a significant semantic signal (d=+0.166, p=2.3e-04)\n",
    "with the encoder-decoder T5Gemma. Does the same paired semantic contrast\n",
    "appear with decoder-only Gemma 3 4B-PT using KV cache priming?\n",
    "\n",
    "## Design\n",
    "\n",
    "For each (query, document, answer) triple, we score the answer NLL under two\n",
    "prefix conditions that are structurally identical (same Q prefix token IDs):\n",
    "- `oracle`: the real query tokens (semantically relevant)\n",
    "- `swapped`: query tokens from a completely different sample (semantically irrelevant)\n",
    "\n",
    "The per-document paired contrast `swapped_nll - oracle_nll` isolates the semantic\n",
    "component with maximum statistical power.\n",
    "\n",
    "All prefixed conditions use exactly Q token IDs (token-level matching).\n",
    "\n",
    "## Conditions (4)\n",
    "\n",
    "| # | Condition | Prefix content |\n",
    "|---|-----------|---------------|\n",
    "| 1 | bare | (none) |\n",
    "| 2 | oracle | real query tokens |\n",
    "| 3 | swapped | query tokens from sample (i + N//2) % N |\n",
    "| 4 | random_matched | random passage word tokens |\n",
    "\n",
    "## Analysis\n",
    "\n",
    "1. Standard condition table\n",
    "2. Paired semantic contrast (the key test)\n",
    "3. Effect distribution\n",
    "4. Predictors of semantic benefit\n",
    "5. Structural equivalence check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef3274a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Setup\n",
    "import os\n",
    "os.umask(0o000)\n",
    "\n",
    "import sys, json, time, re, gc, random as pyrandom\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from scipy import stats\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "sys.path.insert(0, \"../../..\")\n",
    "from lib.analysis import cohens_d\n",
    "\n",
    "SEED = 43  # Different seed from Exp 06 (42) for independent samples\n",
    "N_SAMPLES = 400\n",
    "MODEL_NAME = \"google/gemma-3-4b-it\"\n",
    "\n",
    "RESULTS_DIR = Path(\"../../../results/decoder_only/exp07\")\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CHECKPOINT_PATH = RESULTS_DIR / \"checkpoint.json\"\n",
    "\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "pyrandom.seed(SEED)\n",
    "\n",
    "print(\"Exp 07: Swapped-Query Paired Contrasts (Decoder-Only)\")\n",
    "print(f\"N: {N_SAMPLES}, SEED: {SEED}\")\n",
    "print(f\"Model: {MODEL_NAME}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b8729b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Load MS MARCO and select samples\n",
    "from lib.data import count_words\n",
    "from datasets import load_dataset\n",
    "\n",
    "print(\"Loading MS MARCO...\")\n",
    "ds = load_dataset(\"microsoft/ms_marco\", \"v1.1\", split=\"validation\")\n",
    "\n",
    "samples = []\n",
    "for item in ds:\n",
    "    if len(samples) >= N_SAMPLES * 3:\n",
    "        break\n",
    "    passages = item.get('passages', {})\n",
    "    ptexts = passages.get('passage_text', [])\n",
    "    is_sel = passages.get('is_selected', [])\n",
    "    query = item.get('query', '')\n",
    "    answers = item.get('answers', [])\n",
    "    well_formed = item.get('wellFormedAnswers', [])\n",
    "    answer = None\n",
    "    if well_formed and len(well_formed) > 0 and well_formed[0] not in ('[]', ''):\n",
    "        answer = well_formed[0]\n",
    "    elif answers and len(answers) > 0 and answers[0] != 'No Answer Present.':\n",
    "        answer = answers[0]\n",
    "    if not answer:\n",
    "        continue\n",
    "    for pt, sel in zip(ptexts, is_sel):\n",
    "        wc = count_words(pt)\n",
    "        if sel == 1 and 30 <= wc <= 300:\n",
    "            samples.append({\n",
    "                'passage': pt, 'query': query, 'answer': answer,\n",
    "                'word_count': wc,\n",
    "            })\n",
    "            break\n",
    "\n",
    "np.random.seed(SEED)\n",
    "np.random.shuffle(samples)\n",
    "samples = samples[:N_SAMPLES]\n",
    "del ds\n",
    "gc.collect()\n",
    "\n",
    "passage_words = np.array([s['word_count'] for s in samples])\n",
    "query_words = np.array([len(s['query'].split()) for s in samples])\n",
    "print(f\"Selected {N_SAMPLES} samples\")\n",
    "print(f\"Document lengths: {passage_words.min()}-{passage_words.max()} words, \"\n",
    "      f\"mean={passage_words.mean():.0f}\")\n",
    "print(f\"Query lengths: {query_words.min()}-{query_words.max()} words, \"\n",
    "      f\"mean={query_words.mean():.1f}\")\n",
    "\n",
    "# Verify swapped queries are from different topics\n",
    "print(f\"\\nSwapped query assignment:\")\n",
    "for i in range(5):\n",
    "    j = (i + N_SAMPLES // 2) % N_SAMPLES\n",
    "    print(f\"  Sample {i}: '{samples[i]['query'][:50]}...'\")\n",
    "    print(f\"    Swapped: '{samples[j]['query'][:50]}...'\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5ec089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Load model and define scoring helpers\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv())\n",
    "HF_TOKEN = os.environ.get(\"HF_TOKEN\")\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "print(f\"Loading {MODEL_NAME}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, token=HF_TOKEN)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME, device_map=\"auto\", torch_dtype=torch.bfloat16, token=HF_TOKEN,\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "DEVICE = next(model.parameters()).device\n",
    "\n",
    "print(f\"Model loaded. dtype={next(model.parameters()).dtype}\")\n",
    "print(f\"GPU memory: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "text_cfg = getattr(model.config, 'text_config', model.config)\n",
    "VOCAB_SIZE = getattr(text_cfg, 'vocab_size', 262208)\n",
    "print(f\"Vocab size: {VOCAB_SIZE}\")\n",
    "\n",
    "NEWLINE_IDS = tokenizer(\"\\n\", add_special_tokens=False).input_ids\n",
    "BOS_ID = tokenizer.bos_token_id\n",
    "print(f\"BOS token ID: {BOS_ID}\")\n",
    "print(f\"Newline token IDs: {NEWLINE_IDS} ({len(NEWLINE_IDS)} tokens)\")\n",
    "\n",
    "STOP_WORDS = {\n",
    "    'a', 'an', 'the', 'is', 'are', 'was', 'were', 'be', 'been', 'being',\n",
    "    'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could',\n",
    "    'should', 'may', 'might', 'can', 'shall', 'to', 'of', 'in', 'for',\n",
    "    'on', 'with', 'at', 'by', 'from', 'as', 'into', 'through', 'during',\n",
    "    'before', 'after', 'above', 'below', 'between', 'and', 'but', 'or',\n",
    "    'not', 'no', 'if', 'then', 'than', 'so', 'up', 'out', 'about',\n",
    "    'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those',\n",
    "    'it', 'its', 'i', 'me', 'my', 'we', 'our', 'you', 'your', 'he',\n",
    "    'him', 'his', 'she', 'her', 'they', 'them', 'their', 'how', 'when',\n",
    "    'where', 'why', 'much', 'many', 'some', 'any', 'all', 'each',\n",
    "    'does', 'also', 'just', 'more', 'most', 'very', 'too', 'only',\n",
    "}\n",
    "\n",
    "\n",
    "def slice_kv_cache(cache, start_idx):\n",
    "    # Remove first start_idx entries from KV cache.\n",
    "    from transformers import DynamicCache\n",
    "    if isinstance(cache, DynamicCache):\n",
    "        sliced = DynamicCache()\n",
    "        for i in range(len(cache.layers)):\n",
    "            k = cache.layers[i].keys[:, :, start_idx:, :]\n",
    "            v = cache.layers[i].values[:, :, start_idx:, :]\n",
    "            sliced.update(k, v, i)\n",
    "        return sliced\n",
    "    else:\n",
    "        return tuple(\n",
    "            (k[:, :, start_idx:, :], v[:, :, start_idx:, :])\n",
    "            for k, v in cache\n",
    "        )\n",
    "\n",
    "\n",
    "def score(doc_text, query_text, answer_text, prefix_token_ids=None):\n",
    "    # Score NLL of answer tokens using two-phase KV cache.\n",
    "    #\n",
    "    # If prefix_token_ids is provided:\n",
    "    #   Phase A: [BOS] + prefix_ids + [\\n] + doc_ids\n",
    "    #   Slice first 1+len(prefix_ids)+len(NEWLINE_IDS) entries\n",
    "    # Otherwise (bare):\n",
    "    #   Phase A: [BOS] + doc_ids (nothing sliced)\n",
    "\n",
    "    # --- Phase A: Conditioning ---\n",
    "    doc_ids = tokenizer(doc_text, add_special_tokens=False,\n",
    "                        truncation=True, max_length=1536).input_ids\n",
    "\n",
    "    if prefix_token_ids is not None:\n",
    "        cond_ids = [BOS_ID] + list(prefix_token_ids) + NEWLINE_IDS + doc_ids\n",
    "        slice_start = 1 + len(prefix_token_ids) + len(NEWLINE_IDS)\n",
    "        phase_b_start = len(cond_ids)\n",
    "    else:\n",
    "        cond_ids = [BOS_ID] + doc_ids\n",
    "        slice_start = 0\n",
    "        phase_b_start = len(cond_ids)\n",
    "\n",
    "    cond_tensor = torch.tensor([cond_ids], dtype=torch.long, device=DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        phase_a = model(input_ids=cond_tensor, use_cache=True)\n",
    "\n",
    "    cache = phase_a.past_key_values\n",
    "    del phase_a\n",
    "\n",
    "    if slice_start > 0:\n",
    "        cache = slice_kv_cache(cache, slice_start)\n",
    "\n",
    "    # --- Phase B: Inference ---\n",
    "    query_part_ids = tokenizer(\"\\n\" + query_text + \"\\n\",\n",
    "                               add_special_tokens=False).input_ids\n",
    "    answer_ids = tokenizer(answer_text, add_special_tokens=False,\n",
    "                           truncation=True, max_length=256).input_ids\n",
    "\n",
    "    if not answer_ids:\n",
    "        del cache\n",
    "        return 0.0\n",
    "\n",
    "    phase_b_ids = query_part_ids + answer_ids\n",
    "    phase_b_tensor = torch.tensor([phase_b_ids], dtype=torch.long, device=DEVICE)\n",
    "\n",
    "    pos_ids = torch.arange(phase_b_start, phase_b_start + len(phase_b_ids),\n",
    "                           device=DEVICE).unsqueeze(0)\n",
    "    cache_position = torch.arange(phase_b_start, phase_b_start + len(phase_b_ids),\n",
    "                                  device=DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        phase_b = model(\n",
    "            input_ids=phase_b_tensor,\n",
    "            past_key_values=cache,\n",
    "            position_ids=pos_ids,\n",
    "            cache_position=cache_position,\n",
    "            use_cache=False,\n",
    "        )\n",
    "\n",
    "    logits = phase_b.logits\n",
    "    n_query_part = len(query_part_ids)\n",
    "    n_answer = len(answer_ids)\n",
    "\n",
    "    answer_logits = logits[0, n_query_part - 1 : n_query_part - 1 + n_answer, :]\n",
    "    answer_targets = torch.tensor(answer_ids, dtype=torch.long, device=DEVICE)\n",
    "\n",
    "    log_probs = F.log_softmax(answer_logits, dim=-1)\n",
    "    token_log_probs = log_probs.gather(1, answer_targets.unsqueeze(1)).squeeze(1)\n",
    "    mean_nll = -token_log_probs.mean().item()\n",
    "\n",
    "    del cache, phase_b, logits, log_probs\n",
    "    return mean_nll\n",
    "\n",
    "\n",
    "print(\"Scoring function defined.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51e21d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Build per-sample token-level prefix IDs\n",
    "\n",
    "pyrandom.seed(SEED + 200)\n",
    "\n",
    "for i, s in enumerate(samples):\n",
    "    q_ids = tokenizer(s['query'], add_special_tokens=False).input_ids\n",
    "    Q = len(q_ids)\n",
    "    s['Q'] = Q\n",
    "\n",
    "    # 1. oracle: actual query tokens\n",
    "    s['prefix_oracle'] = q_ids\n",
    "\n",
    "    # 2. swapped: query from a distant sample, tokenized and truncated/padded to Q\n",
    "    swapped_idx = (i + N_SAMPLES // 2) % N_SAMPLES\n",
    "    swapped_q_ids = tokenizer(samples[swapped_idx]['query'],\n",
    "                              add_special_tokens=False).input_ids\n",
    "    if len(swapped_q_ids) >= Q:\n",
    "        s['prefix_swapped'] = swapped_q_ids[:Q]\n",
    "    else:\n",
    "        padded = swapped_q_ids * ((Q // max(len(swapped_q_ids), 1)) + 1)\n",
    "        s['prefix_swapped'] = padded[:Q]\n",
    "\n",
    "    # 3. random_matched: words from unrelated passage, tokenized and truncated/padded to Q\n",
    "    other_idx = (i + N_SAMPLES // 2) % N_SAMPLES\n",
    "    n_query_words = len(s['query'].split())\n",
    "    other_words = samples[other_idx]['passage'].split()\n",
    "    random_text = \" \".join(other_words[:n_query_words])\n",
    "    rand_ids = tokenizer(random_text, add_special_tokens=False).input_ids\n",
    "    if len(rand_ids) >= Q:\n",
    "        s['prefix_random'] = rand_ids[:Q]\n",
    "    else:\n",
    "        padded = rand_ids * ((Q // max(len(rand_ids), 1)) + 1)\n",
    "        s['prefix_random'] = padded[:Q]\n",
    "\n",
    "    s['swapped_query_text'] = samples[swapped_idx]['query']\n",
    "\n",
    "# Summary statistics\n",
    "q_lens = [s['Q'] for s in samples]\n",
    "print(f\"Loaded {len(samples)} samples\")\n",
    "print(f\"Query token count — mean: {np.mean(q_lens):.1f}, \"\n",
    "      f\"median: {np.median(q_lens):.0f}, \"\n",
    "      f\"min: {np.min(q_lens)}, max: {np.max(q_lens)}\")\n",
    "\n",
    "# Verify prefix lengths\n",
    "for i, s in enumerate(samples[:5]):\n",
    "    Q = s['Q']\n",
    "    for name in ['prefix_oracle', 'prefix_swapped', 'prefix_random']:\n",
    "        assert len(s[name]) == Q, f\"Sample {i} {name}: len={len(s[name])} != Q={Q}\"\n",
    "    print(f\"  Sample {i}: Q={Q}\")\n",
    "    print(f\"    oracle:  {tokenizer.decode(s['prefix_oracle'][:8])}...\")\n",
    "    print(f\"    swapped: {tokenizer.decode(s['prefix_swapped'][:8])}...\")\n",
    "    print(f\"    random:  {tokenizer.decode(s['prefix_random'][:8])}...\")\n",
    "print(\"All prefix lengths verified.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1efcff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Validate scoring\n",
    "print(\"=\" * 70)\n",
    "print(\"VALIDATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "s = samples[0]\n",
    "Q = s['Q']\n",
    "\n",
    "print(f\"\\nSample 0: Q={Q} query tokens\")\n",
    "print(f\"  Query:   '{s['query']}'\")\n",
    "print(f\"  Swapped: '{s['swapped_query_text']}'\")\n",
    "\n",
    "print(f\"\\n--- NLL for each condition (sample 0) ---\")\n",
    "nll_bare = score(s['passage'], s['query'], s['answer'])\n",
    "print(f\"  {'bare':<18} NLL = {nll_bare:.4f}\")\n",
    "\n",
    "for name, prefix_key in [('oracle', 'prefix_oracle'),\n",
    "                          ('swapped', 'prefix_swapped'),\n",
    "                          ('random_matched', 'prefix_random')]:\n",
    "    nll = score(s['passage'], s['query'], s['answer'],\n",
    "                prefix_token_ids=s[prefix_key])\n",
    "    print(f\"  {name:<18} NLL = {nll:.4f}  delta = {nll_bare - nll:+.4f}\")\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ad2278",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Scoring loop — 4 conditions x 400 samples\n",
    "print(\"=\" * 70)\n",
    "print(\"SCORING ALL CONDITIONS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "COND_NAMES = ['bare', 'oracle', 'swapped', 'random_matched']\n",
    "\n",
    "PREFIX_MAP = {\n",
    "    'oracle': 'prefix_oracle',\n",
    "    'swapped': 'prefix_swapped',\n",
    "    'random_matched': 'prefix_random',\n",
    "}\n",
    "\n",
    "results = []\n",
    "start_idx = 0\n",
    "\n",
    "if CHECKPOINT_PATH.exists():\n",
    "    ckpt = json.loads(CHECKPOINT_PATH.read_text())\n",
    "    if ckpt.get('n_total') == N_SAMPLES and len(ckpt.get('results', [])) > 0:\n",
    "        saved_queries = [r['query'][:50] for r in ckpt['results']]\n",
    "        current_queries = [s['query'][:50] for s in samples[:len(saved_queries)]]\n",
    "        if saved_queries == current_queries:\n",
    "            results = ckpt['results']\n",
    "            start_idx = len(results)\n",
    "            print(f\"Resuming from checkpoint: {start_idx}/{N_SAMPLES}\")\n",
    "\n",
    "if start_idx == 0:\n",
    "    print(f\"Starting fresh: {len(COND_NAMES)} conditions x {N_SAMPLES} samples \"\n",
    "          f\"= {len(COND_NAMES) * N_SAMPLES} scorings\")\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "for i in tqdm(range(start_idx, N_SAMPLES), initial=start_idx, total=N_SAMPLES,\n",
    "              desc=\"Scoring\"):\n",
    "    s = samples[i]\n",
    "    result = {\n",
    "        'query': s['query'],\n",
    "        'answer': s['answer'],\n",
    "        'passage_words': s['word_count'],\n",
    "        'swapped_query': s['swapped_query_text'],\n",
    "        'Q': s['Q'],\n",
    "    }\n",
    "\n",
    "    # bare\n",
    "    result['nll_bare'] = score(s['passage'], s['query'], s['answer'])\n",
    "\n",
    "    # All prefixed conditions\n",
    "    for cond_name, prefix_key in PREFIX_MAP.items():\n",
    "        result[f'nll_{cond_name}'] = score(\n",
    "            s['passage'], s['query'], s['answer'],\n",
    "            prefix_token_ids=s[prefix_key]\n",
    "        )\n",
    "\n",
    "    results.append(result)\n",
    "\n",
    "    if (i + 1) % 20 == 0 or i == N_SAMPLES - 1:\n",
    "        ckpt = {\n",
    "            'n_total': N_SAMPLES,\n",
    "            'results': results,\n",
    "            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        }\n",
    "        CHECKPOINT_PATH.write_text(json.dumps(ckpt))\n",
    "        elapsed = time.time() - t0\n",
    "        done = i - start_idx + 1\n",
    "        eta = (N_SAMPLES - i - 1) * elapsed / done if done > 0 else 0\n",
    "        tqdm.write(f\"  Checkpoint {i+1}/{N_SAMPLES} | \"\n",
    "                   f\"{elapsed/60:.1f}m | ETA {eta/60:.1f}m\")\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "elapsed = time.time() - t0\n",
    "print(f\"\\nScoring complete: {len(results)} samples, \"\n",
    "      f\"{len(COND_NAMES)} conditions in {elapsed/60:.1f} min\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4652710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Part 1 — Standard Condition Table\n",
    "print(\"=\" * 70)\n",
    "print(\"PART 1: STANDARD CONDITION TABLE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "bare_nlls = np.array([r['nll_bare'] for r in results])\n",
    "oracle_nlls = np.array([r['nll_oracle'] for r in results])\n",
    "swapped_nlls = np.array([r['nll_swapped'] for r in results])\n",
    "random_nlls = np.array([r['nll_random_matched'] for r in results])\n",
    "\n",
    "oracle_benefit = bare_nlls - oracle_nlls\n",
    "oracle_d = cohens_d(oracle_benefit)\n",
    "\n",
    "all_conds = [\n",
    "    ('oracle', 'Oracle (real query)'),\n",
    "    ('swapped', 'Swapped (wrong query)'),\n",
    "    ('random_matched', 'Random matched (structural)'),\n",
    "]\n",
    "\n",
    "alpha_bonf = 0.05 / len(all_conds)\n",
    "\n",
    "print(f\"\\n{'Condition':<35} {'NLL':>8} {'Delta':>8} {'d':>8} \"\n",
    "      f\"{'Win%':>7} {'%Orc':>6} {'p':>12} {'sig':>5}\")\n",
    "print(\"-\" * 95)\n",
    "\n",
    "for cond, desc in all_conds:\n",
    "    nlls = np.array([r[f'nll_{cond}'] for r in results])\n",
    "    benefit = bare_nlls - nlls\n",
    "    d = cohens_d(benefit)\n",
    "    delta = benefit.mean()\n",
    "    win = 100 * np.mean(benefit > 0)\n",
    "    pct = d / oracle_d * 100 if oracle_d > 0 else 0\n",
    "    _, p = stats.ttest_1samp(benefit, 0)\n",
    "    sig = '***' if p < alpha_bonf / 10 else '**' if p < alpha_bonf else '*' if p < 0.05 else 'ns'\n",
    "    print(f\"  {desc:<33} {nlls.mean():>8.4f} {delta:>+8.4f} {d:>+8.3f} \"\n",
    "          f\"{win:>6.1f}% {pct:>5.0f}% {p:>12.2e} {sig}\")\n",
    "\n",
    "print(f\"\\n  bare (lower bound): {bare_nlls.mean():.4f}\")\n",
    "print(f\"  Bonferroni threshold: alpha={alpha_bonf:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff863984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Part 2 — Paired Semantic Contrast\n",
    "print(\"=\" * 70)\n",
    "print(\"PART 2: PAIRED SEMANTIC CONTRAST (the key test)\")\n",
    "print(\"=\" * 70)\n",
    "print(\"Per-document: Delta_semantic = swapped_nll - oracle_nll\")\n",
    "print(\"Both conditions have the same structural perturbation (Q prefix tokens).\")\n",
    "print(\"Only semantic relevance differs.\\n\")\n",
    "\n",
    "semantic_effect = swapped_nlls - oracle_nlls  # positive = oracle is better\n",
    "\n",
    "# Paired t-test\n",
    "t_stat, p_paired = stats.ttest_rel(swapped_nlls, oracle_nlls)\n",
    "d_semantic = cohens_d(semantic_effect)\n",
    "win_oracle = 100 * np.mean(semantic_effect > 0)\n",
    "\n",
    "print(f\"  Mean(swapped_nll - oracle_nll): {semantic_effect.mean():+.4f}\")\n",
    "print(f\"  Cohen's d:                      {d_semantic:+.3f}\")\n",
    "print(f\"  Oracle wins:                    {win_oracle:.1f}%\")\n",
    "print(f\"  Paired t-test:                  t={t_stat:.3f}, p={p_paired:.2e}\")\n",
    "\n",
    "sig = '***' if p_paired < 0.001 else '**' if p_paired < 0.01 else '*' if p_paired < 0.05 else 'ns'\n",
    "print(f\"  Significance:                   {sig}\")\n",
    "\n",
    "if p_paired < 0.05 and d_semantic > 0:\n",
    "    print(f\"\\n  --> SEMANTIC RELEVANCE MATTERS: oracle query produces significantly\")\n",
    "    print(f\"      lower NLL than a swapped query from a different topic.\")\n",
    "elif p_paired < 0.05 and d_semantic < 0:\n",
    "    print(f\"\\n  --> REVERSE: swapped query is actually BETTER than oracle.\")\n",
    "else:\n",
    "    print(f\"\\n  --> NO SIGNIFICANT SEMANTIC EFFECT: oracle and swapped queries\")\n",
    "    print(f\"      produce equivalent NLLs. The benefit is purely structural.\")\n",
    "\n",
    "print(f\"\\n--- Context ---\")\n",
    "print(f\"  Overall oracle benefit (vs bare): d={oracle_d:+.3f}\")\n",
    "print(f\"  Semantic component (paired):      d={d_semantic:+.3f}\")\n",
    "print(f\"  Structural component (estimated): d={oracle_d - d_semantic:+.3f}\")\n",
    "if oracle_d > 0:\n",
    "    sem_frac = d_semantic / oracle_d * 100\n",
    "    print(f\"  Semantic fraction:                {sem_frac:.1f}% of total benefit\")\n",
    "\n",
    "# Cross-architecture comparison\n",
    "print(f\"\\n--- v3 comparison (T5Gemma encoder-decoder) ---\")\n",
    "print(f\"  v3 Exp 13: d_semantic=+0.166, win=63.4%, p=2.3e-04, \"\n",
    "      f\"semantic_fraction=33.4%\")\n",
    "print(f\"  v4 Exp 07: d_semantic={d_semantic:+.3f}, win={win_oracle:.1f}%, \"\n",
    "      f\"p={p_paired:.2e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5befb49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Part 3 — Effect Distribution\n",
    "print(\"=\" * 70)\n",
    "print(\"PART 3: EFFECT DISTRIBUTION\")\n",
    "print(\"=\" * 70)\n",
    "print(\"Per-sample distribution of swapped_nll - oracle_nll\\n\")\n",
    "\n",
    "print(f\"  Mean:   {semantic_effect.mean():+.4f}\")\n",
    "print(f\"  Median: {np.median(semantic_effect):+.4f}\")\n",
    "print(f\"  Std:    {semantic_effect.std():.4f}\")\n",
    "print(f\"  Min:    {semantic_effect.min():+.4f}\")\n",
    "print(f\"  Max:    {semantic_effect.max():+.4f}\")\n",
    "\n",
    "oracle_better = np.sum(semantic_effect > 0)\n",
    "swapped_better = np.sum(semantic_effect < 0)\n",
    "tied = np.sum(semantic_effect == 0)\n",
    "\n",
    "print(f\"\\n  Oracle better (oracle < swapped): \"\n",
    "      f\"{oracle_better} ({oracle_better/N_SAMPLES*100:.1f}%)\")\n",
    "print(f\"  Swapped better (swapped < oracle): \"\n",
    "      f\"{swapped_better} ({swapped_better/N_SAMPLES*100:.1f}%)\")\n",
    "print(f\"  Tied:                              {tied}\")\n",
    "\n",
    "print(f\"\\n--- Effect size distribution ---\")\n",
    "for threshold in [0.01, 0.05, 0.1, 0.2, 0.5]:\n",
    "    n_above = np.sum(semantic_effect > threshold)\n",
    "    n_below = np.sum(semantic_effect < -threshold)\n",
    "    print(f\"  |effect| > {threshold:.2f}: \"\n",
    "          f\"{n_above} oracle wins, {n_below} swapped wins\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f186a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Part 4 — Predictors of Semantic Benefit\n",
    "print(\"=\" * 70)\n",
    "print(\"PART 4: PREDICTORS OF SEMANTIC BENEFIT\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# (a) Query-document vocabulary overlap (Jaccard on content words)\n",
    "jaccard_overlaps = []\n",
    "for i in range(N_SAMPLES):\n",
    "    doc_words = set(re.sub(r'[^\\w\\s]', '', samples[i]['passage'].lower()).split())\n",
    "    doc_content = doc_words - STOP_WORDS\n",
    "    q_words = set(re.sub(r'[^\\w\\s]', '', samples[i]['query'].lower()).split())\n",
    "    q_content = q_words - STOP_WORDS\n",
    "    if len(doc_content | q_content) > 0:\n",
    "        jaccard = len(doc_content & q_content) / len(doc_content | q_content)\n",
    "    else:\n",
    "        jaccard = 0.0\n",
    "    jaccard_overlaps.append(jaccard)\n",
    "jaccard_overlaps = np.array(jaccard_overlaps)\n",
    "\n",
    "doc_lengths = np.array([r['passage_words'] for r in results])\n",
    "answer_lengths = np.array([len(r['answer'].split()) for r in results])\n",
    "query_lengths = np.array([len(r['query'].split()) for r in results])\n",
    "\n",
    "predictors = [\n",
    "    ('Query-doc Jaccard overlap', jaccard_overlaps),\n",
    "    ('Document length (words)', doc_lengths),\n",
    "    ('Bare NLL (hardness)', bare_nlls),\n",
    "    ('Answer length (words)', answer_lengths),\n",
    "    ('Query length (words)', query_lengths),\n",
    "]\n",
    "\n",
    "print(f\"\\n  {'Predictor':<30} {'Pearson r':>10} {'p':>12} {'sig':>5}\")\n",
    "print(f\"  {'-'*62}\")\n",
    "\n",
    "alpha_bonf_pred = 0.05 / len(predictors)\n",
    "\n",
    "for name, values in predictors:\n",
    "    r_val, p_val = stats.pearsonr(values, semantic_effect)\n",
    "    sig = '***' if p_val < alpha_bonf_pred / 10 else '**' if p_val < alpha_bonf_pred else \\\n",
    "          '*' if p_val < 0.05 else 'ns'\n",
    "    print(f\"  {name:<30} {r_val:>+10.3f} {p_val:>12.2e} {sig}\")\n",
    "\n",
    "# Hardness interaction (detailed)\n",
    "print(f\"\\n--- Semantic effect by hardness quintile ---\")\n",
    "quintile_bounds = np.percentile(bare_nlls, [20, 40, 60, 80])\n",
    "quintiles = np.digitize(bare_nlls, quintile_bounds)\n",
    "q_labels = ['Q1 easy', 'Q2', 'Q3', 'Q4', 'Q5 hard']\n",
    "\n",
    "print(f\"  {'Quintile':<12} {'N':>4} {'Bare NLL':>10} {'Sem effect':>12} {'d':>8} \"\n",
    "      f\"{'Win%':>7} {'p':>12} {'sig':>5}\")\n",
    "print(f\"  {'-'*75}\")\n",
    "\n",
    "for q in range(5):\n",
    "    mask = quintiles == q\n",
    "    n = mask.sum()\n",
    "    eff_q = semantic_effect[mask]\n",
    "    d_q = cohens_d(eff_q)\n",
    "    win_q = 100 * np.mean(eff_q > 0)\n",
    "    _, p_q = stats.ttest_1samp(eff_q, 0)\n",
    "    sig_q = '***' if p_q < 0.001 else '**' if p_q < 0.01 else '*' if p_q < 0.05 else 'ns'\n",
    "    print(f\"  {q_labels[q]:<12} {n:>4} {bare_nlls[mask].mean():>10.3f} \"\n",
    "          f\"{eff_q.mean():>+12.4f} {d_q:>+8.3f} {win_q:>6.1f}% {p_q:>12.2e} {sig_q}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d88bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Part 5 — Structural Equivalence Check\n",
    "print(\"=\" * 70)\n",
    "print(\"PART 5: STRUCTURAL EQUIVALENCE CHECK\")\n",
    "print(\"=\" * 70)\n",
    "print(\"Confirm oracle and swapped have similar structural benefit vs bare.\")\n",
    "print(\"With token-level matching, structural confounds should be eliminated.\\n\")\n",
    "\n",
    "oracle_struct = bare_nlls - oracle_nlls\n",
    "swapped_struct = bare_nlls - swapped_nlls\n",
    "random_struct = bare_nlls - random_nlls\n",
    "\n",
    "oracle_vs_bare_d = cohens_d(oracle_struct)\n",
    "swapped_vs_bare_d = cohens_d(swapped_struct)\n",
    "random_vs_bare_d = cohens_d(random_struct)\n",
    "\n",
    "print(f\"  Condition vs bare (Cohen's d):\")\n",
    "print(f\"    Oracle:  d={oracle_vs_bare_d:+.3f}\")\n",
    "print(f\"    Swapped: d={swapped_vs_bare_d:+.3f}\")\n",
    "print(f\"    Random:  d={random_vs_bare_d:+.3f}\")\n",
    "\n",
    "# Semantic component: condition vs random\n",
    "oracle_vs_random = random_nlls - oracle_nlls\n",
    "swapped_vs_random = random_nlls - swapped_nlls\n",
    "\n",
    "d_orac_rand = cohens_d(oracle_vs_random)\n",
    "d_swap_rand = cohens_d(swapped_vs_random)\n",
    "_, p_orac_rand = stats.ttest_1samp(oracle_vs_random, 0)\n",
    "_, p_swap_rand = stats.ttest_1samp(swapped_vs_random, 0)\n",
    "\n",
    "print(f\"\\n  Condition vs random (semantic component):\")\n",
    "print(f\"    Oracle - random:  d={d_orac_rand:+.3f}, p={p_orac_rand:.2e}\")\n",
    "print(f\"    Swapped - random: d={d_swap_rand:+.3f}, p={p_swap_rand:.2e}\")\n",
    "\n",
    "# Key check\n",
    "structural_diff = oracle_struct - swapped_struct\n",
    "d_struct_diff = cohens_d(structural_diff)\n",
    "_, p_struct = stats.ttest_1samp(structural_diff, 0)\n",
    "sig_struct = '***' if p_struct < 0.001 else '**' if p_struct < 0.01 else '*' if p_struct < 0.05 else 'ns'\n",
    "\n",
    "print(f\"\\n  Oracle benefit - Swapped benefit: d={d_struct_diff:+.3f}, \"\n",
    "      f\"p={p_struct:.2e} {sig_struct}\")\n",
    "print(f\"  (Should match Part 2 semantic effect: d={d_semantic:+.3f})\")\n",
    "print(f\"  Consistency check: {abs(d_struct_diff - d_semantic):.4f} (should be ~0)\")\n",
    "\n",
    "# Token count verification (should be exactly equal with token-level matching)\n",
    "oracle_q = np.array([r['Q'] for r in results])\n",
    "print(f\"\\n  Token-level matching verification:\")\n",
    "print(f\"    All prefixes have exactly Q tokens per sample: VERIFIED\")\n",
    "print(f\"    Mean Q: {oracle_q.mean():.1f}, range: [{oracle_q.min()}, {oracle_q.max()}]\")\n",
    "print(f\"    No token count confound possible with token-level matching.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c988740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13: Synthesis + Save\n",
    "print(\"=\" * 70)\n",
    "print(\"SYNTHESIS: SWAPPED-QUERY PAIRED CONTRAST (DECODER-ONLY)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\n1. CONDITION TABLE:\")\n",
    "print(f\"   {'Condition':<25} {'d vs bare':>10} {'%Oracle':>8}\")\n",
    "print(f\"   {'-'*45}\")\n",
    "for cond, desc in all_conds:\n",
    "    nlls = np.array([r[f'nll_{cond}'] for r in results])\n",
    "    d = cohens_d(bare_nlls - nlls)\n",
    "    pct = d / oracle_d * 100 if oracle_d > 0 else 0\n",
    "    print(f\"   {desc:<25} {d:>+10.3f} {pct:>7.0f}%\")\n",
    "\n",
    "print(f\"\\n2. PAIRED SEMANTIC CONTRAST:\")\n",
    "print(f\"   swapped_nll - oracle_nll: mean={semantic_effect.mean():+.4f}, \"\n",
    "      f\"d={d_semantic:+.3f}\")\n",
    "print(f\"   Oracle win rate: {win_oracle:.1f}%, p={p_paired:.2e}\")\n",
    "if oracle_d > 0:\n",
    "    sem_frac = d_semantic / oracle_d * 100\n",
    "    print(f\"   Semantic fraction of total benefit: {sem_frac:.1f}%\")\n",
    "\n",
    "print(f\"\\n3. STRONGEST PREDICTOR:\")\n",
    "best_r = 0\n",
    "best_name = \"\"\n",
    "for name, values in predictors:\n",
    "    r_val, _ = stats.pearsonr(values, semantic_effect)\n",
    "    if abs(r_val) > abs(best_r):\n",
    "        best_r = r_val\n",
    "        best_name = name\n",
    "print(f\"   {best_name} (r={best_r:+.3f})\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"CONCLUSIONS:\")\n",
    "\n",
    "if p_paired < 0.001 and d_semantic > 0.05:\n",
    "    print(f\"  1. STRONG SEMANTIC SIGNAL: oracle significantly beats swapped\")\n",
    "    print(f\"     (d={d_semantic:+.3f}, p={p_paired:.2e})\")\n",
    "    conclusion = \"STRONG_SEMANTIC\"\n",
    "elif p_paired < 0.05 and d_semantic > 0:\n",
    "    print(f\"  1. WEAK SEMANTIC SIGNAL: marginally significant\")\n",
    "    print(f\"     (d={d_semantic:+.3f}, p={p_paired:.2e})\")\n",
    "    conclusion = \"WEAK_SEMANTIC\"\n",
    "elif p_paired < 0.05 and d_semantic < 0:\n",
    "    print(f\"  1. SEMANTIC INTERFERENCE: swapped query is BETTER\")\n",
    "    print(f\"     (d={d_semantic:+.3f}, p={p_paired:.2e})\")\n",
    "    conclusion = \"INTERFERENCE\"\n",
    "else:\n",
    "    print(f\"  1. NO SEMANTIC EFFECT: oracle and swapped are equivalent\")\n",
    "    print(f\"     (d={d_semantic:+.3f}, p={p_paired:.2e})\")\n",
    "    conclusion = \"NO_EFFECT\"\n",
    "\n",
    "print(f\"\\n  Token-level matching eliminates all structural confounds.\")\n",
    "\n",
    "# Cross-architecture comparison\n",
    "print(f\"\\n--- Cross-architecture comparison ---\")\n",
    "print(f\"  v3 (T5Gemma enc-dec): d_semantic=+0.166, p=2.3e-04, \"\n",
    "      f\"fraction=33.4%\")\n",
    "print(f\"  v4 (Gemma 3 dec-only): d_semantic={d_semantic:+.3f}, p={p_paired:.2e}\")\n",
    "if oracle_d > 0:\n",
    "    print(f\"                         fraction={sem_frac:.1f}%\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "# Save results\n",
    "final_results = {\n",
    "    'experiment': 'v4_decoder_only_exp07_swapped_query',\n",
    "    'model': MODEL_NAME,\n",
    "    'dataset': 'ms_marco_v1.1',\n",
    "    'n_samples': len(results),\n",
    "    'seed': SEED,\n",
    "    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'baseline': {\n",
    "        'bare_nll': float(bare_nlls.mean()),\n",
    "        'oracle_d': float(oracle_d),\n",
    "    },\n",
    "    'semantic_contrast': {\n",
    "        'mean_effect': float(semantic_effect.mean()),\n",
    "        'd': float(d_semantic),\n",
    "        'win_pct': float(win_oracle),\n",
    "        'p_paired': float(p_paired),\n",
    "        'semantic_fraction_pct': float(sem_frac) if oracle_d > 0 else None,\n",
    "    },\n",
    "    'structural_equivalence': {\n",
    "        'oracle_vs_bare_d': float(oracle_vs_bare_d),\n",
    "        'swapped_vs_bare_d': float(swapped_vs_bare_d),\n",
    "        'random_vs_bare_d': float(random_vs_bare_d),\n",
    "    },\n",
    "    'predictors': {},\n",
    "    'conditions': {},\n",
    "    'conclusion': conclusion,\n",
    "    'query_token_stats': {\n",
    "        'mean': float(np.mean([r['Q'] for r in results])),\n",
    "        'median': float(np.median([r['Q'] for r in results])),\n",
    "        'min': int(np.min([r['Q'] for r in results])),\n",
    "        'max': int(np.max([r['Q'] for r in results])),\n",
    "    },\n",
    "}\n",
    "\n",
    "for name, values in predictors:\n",
    "    r_val, p_val = stats.pearsonr(values, semantic_effect)\n",
    "    final_results['predictors'][name] = {\n",
    "        'pearson_r': float(r_val),\n",
    "        'p': float(p_val),\n",
    "    }\n",
    "\n",
    "for cond, desc in all_conds:\n",
    "    nlls = np.array([r[f'nll_{cond}'] for r in results])\n",
    "    benefit = bare_nlls - nlls\n",
    "    d = cohens_d(benefit)\n",
    "    _, p = stats.ttest_1samp(benefit, 0)\n",
    "    final_results['conditions'][cond] = {\n",
    "        'description': desc,\n",
    "        'd': float(d),\n",
    "        'mean_nll': float(nlls.mean()),\n",
    "        'mean_delta': float(benefit.mean()),\n",
    "        'pct_oracle': float(d / oracle_d * 100) if oracle_d > 0 else 0,\n",
    "        'p': float(p),\n",
    "    }\n",
    "\n",
    "with open(RESULTS_DIR / 'results.json', 'w') as f:\n",
    "    json.dump(final_results, f, indent=2)\n",
    "print(f\"\\nResults saved to {RESULTS_DIR / 'results.json'}\")\n",
    "\n",
    "# Cleanup\n",
    "print(f\"\\nCleaning up GPU memory...\")\n",
    "mem_before = torch.cuda.memory_allocated() / 1e9\n",
    "del model, tokenizer\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "mem_after = torch.cuda.memory_allocated() / 1e9\n",
    "print(f\"GPU memory: {mem_before:.2f} GB -> {mem_after:.2f} GB\")\n",
    "print(\"Done!\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
