{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33df4107",
   "metadata": {},
   "source": [
    "# Decoder-Only Exp 06: Graded Semantic Relevance\n",
    "\n",
    "## Motivation\n",
    "\n",
    "Port of v3 Exp 12 to decoder-only two-phase KV cache scoring.\n",
    "v3 Exp 12 found a monotonic semantic gradient (Spearman rho=+0.94, p=0.005)\n",
    "in the encoder-decoder T5Gemma. Does the same gradient appear with decoder-only\n",
    "Gemma 3 4B-PT using KV cache priming?\n",
    "\n",
    "## Design\n",
    "\n",
    "All structural confounds (BOS removal, position offset, cache length) are\n",
    "equalized by using token-level prefix matching: every prefixed condition\n",
    "constructs exactly Q prefix token IDs (Q = number of real query tokens).\n",
    "\n",
    "Phase A input: [BOS] + prefix_ids(Q) + [\\n] + doc_ids\n",
    "Slice from cache: first Q+2 entries (BOS + prefix + newline)\n",
    "Result: only doc KV entries remain, at identical positions across all conditions.\n",
    "\n",
    "## Conditions (7)\n",
    "\n",
    "| # | Condition | Prefix content | Semantic relevance |\n",
    "|---|-----------|---------------|--------------------|\n",
    "| 1 | bare | (none) | N/A (lower bound) |\n",
    "| 2 | oracle | real query tokens | maximal (exact query) |\n",
    "| 3 | paraphrase | LLM paraphrase tokens | high (same meaning, diff words) |\n",
    "| 4 | same_topic | LLM same-topic question | medium (right topic, wrong question) |\n",
    "| 5 | unrelated_query | different sample's query | low (real syntax, wrong topic) |\n",
    "| 6 | scrambled_oracle | query tokens shuffled | vocabulary only |\n",
    "| 7 | random_matched | random passage word tokens | none (structural baseline) |\n",
    "\n",
    "## Analysis\n",
    "\n",
    "- Part 1: Standard condition table\n",
    "- Part 2: Semantic gradient with monotonicity test\n",
    "- Part 3: Fine-grained decomposition chain\n",
    "- Part 4: Hardness interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940f45a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Setup\n",
    "import os\n",
    "os.umask(0o000)\n",
    "\n",
    "import sys, json, time, re, gc, random as pyrandom\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from scipy import stats\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "sys.path.insert(0, \"../../..\")\n",
    "from lib.analysis import cohens_d\n",
    "\n",
    "SEED = 42\n",
    "N_SAMPLES = 400\n",
    "MODEL_NAME = \"google/gemma-3-4b-it\"\n",
    "\n",
    "RESULTS_DIR = Path(\"../../../results/decoder_only/exp06\")\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "SURROGATES_PATH = RESULTS_DIR / \"surrogates.json\"\n",
    "CHECKPOINT_PATH = RESULTS_DIR / \"checkpoint.json\"\n",
    "\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "pyrandom.seed(SEED)\n",
    "\n",
    "# Prompt templates for LLM generation\n",
    "PROMPT_PARAPHRASE = (\n",
    "    \"Rephrase this search query using completely different words but keeping \"\n",
    "    \"the same meaning. Keep it to 5-8 words. Output only the rephrased query.\"\n",
    ")\n",
    "PROMPT_SAME_TOPIC = (\n",
    "    \"Write a question about the same topic as this document but asking for \"\n",
    "    \"DIFFERENT information. Keep it to 5-8 words. Output only the question.\"\n",
    ")\n",
    "\n",
    "print(\"Exp 06: Graded Semantic Relevance (Decoder-Only)\")\n",
    "print(f\"N: {N_SAMPLES}\")\n",
    "print(f\"Model: {MODEL_NAME} (generation + scoring)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a20931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Load MS MARCO and select samples\n",
    "from lib.data import count_words\n",
    "from datasets import load_dataset\n",
    "\n",
    "print(\"Loading MS MARCO...\")\n",
    "ds = load_dataset(\"microsoft/ms_marco\", \"v1.1\", split=\"validation\")\n",
    "\n",
    "samples = []\n",
    "for item in ds:\n",
    "    if len(samples) >= N_SAMPLES * 3:\n",
    "        break\n",
    "    passages = item.get('passages', {})\n",
    "    ptexts = passages.get('passage_text', [])\n",
    "    is_sel = passages.get('is_selected', [])\n",
    "    query = item.get('query', '')\n",
    "    answers = item.get('answers', [])\n",
    "    well_formed = item.get('wellFormedAnswers', [])\n",
    "    answer = None\n",
    "    if well_formed and len(well_formed) > 0 and well_formed[0] not in ('[]', ''):\n",
    "        answer = well_formed[0]\n",
    "    elif answers and len(answers) > 0 and answers[0] != 'No Answer Present.':\n",
    "        answer = answers[0]\n",
    "    if not answer:\n",
    "        continue\n",
    "    for pt, sel in zip(ptexts, is_sel):\n",
    "        wc = count_words(pt)\n",
    "        if sel == 1 and 30 <= wc <= 300:\n",
    "            samples.append({\n",
    "                'passage': pt, 'query': query, 'answer': answer,\n",
    "                'word_count': wc,\n",
    "            })\n",
    "            break\n",
    "\n",
    "np.random.seed(SEED)\n",
    "np.random.shuffle(samples)\n",
    "samples = samples[:N_SAMPLES]\n",
    "del ds\n",
    "gc.collect()\n",
    "\n",
    "passage_words = np.array([s['word_count'] for s in samples])\n",
    "query_words = np.array([len(s['query'].split()) for s in samples])\n",
    "print(f\"Selected {N_SAMPLES} samples\")\n",
    "print(f\"Document lengths: {passage_words.min()}-{passage_words.max()} words, \"\n",
    "      f\"mean={passage_words.mean():.0f}\")\n",
    "print(f\"Query lengths: {query_words.min()}-{query_words.max()} words, \"\n",
    "      f\"mean={query_words.mean():.1f}\")\n",
    "\n",
    "for i in range(5):\n",
    "    s = samples[i]\n",
    "    print(f\"\\nExample {i}:\")\n",
    "    print(f\"  Q: {s['query']}\")\n",
    "    print(f\"  A: {s['answer'][:80]}\")\n",
    "    print(f\"  P ({s['word_count']}w): {s['passage'][:100]}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ee0766",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Phase 1 — Generate surrogates with Gemma 3 12B-IT\n",
    "# Skip if surrogates already cached\n",
    "\n",
    "if SURROGATES_PATH.exists():\n",
    "    print(\"Loading cached surrogates...\")\n",
    "    surrogates = json.loads(SURROGATES_PATH.read_text())\n",
    "    assert len(surrogates) == N_SAMPLES, f\"Expected {N_SAMPLES}, got {len(surrogates)}\"\n",
    "    for i in range(min(10, N_SAMPLES)):\n",
    "        assert surrogates[i]['query'][:50] == samples[i]['query'][:50], \\\n",
    "            f\"Sample {i} query mismatch\"\n",
    "    print(f\"Loaded {len(surrogates)} cached surrogates\")\n",
    "    print(f\"Keys per sample: {list(surrogates[0].keys())}\")\n",
    "else:\n",
    "    print(f\"Loading {MODEL_NAME} for surrogate generation...\")\n",
    "    from dotenv import load_dotenv, find_dotenv\n",
    "    load_dotenv(find_dotenv())\n",
    "    HF_TOKEN = os.environ.get(\"HF_TOKEN\")\n",
    "\n",
    "    from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "    gen_tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, token=HF_TOKEN)\n",
    "    gen_model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME, device_map=\"auto\", torch_dtype=torch.bfloat16, token=HF_TOKEN,\n",
    "    )\n",
    "    gen_model.eval()\n",
    "    GEN_DEVICE = next(gen_model.parameters()).device\n",
    "    print(f\"Model loaded. GPU memory: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "\n",
    "    def generate_text(input_text, prompt_text):\n",
    "        # Generate text from a prompt + input using Gemma IT.\n",
    "        messages = [\n",
    "            {\"role\": \"user\",\n",
    "             \"content\": f\"{prompt_text}\\n\\n{input_text}\"}\n",
    "        ]\n",
    "        chat_text = gen_tokenizer.apply_chat_template(\n",
    "            messages, tokenize=False, add_generation_prompt=True\n",
    "        )\n",
    "        inputs = gen_tokenizer(chat_text, return_tensors=\"pt\",\n",
    "                               truncation=True, max_length=1024).to(GEN_DEVICE)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output_ids = gen_model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=30,\n",
    "                temperature=0.7,\n",
    "                top_p=0.9,\n",
    "                do_sample=True,\n",
    "            )\n",
    "\n",
    "        new_tokens = output_ids[0, inputs['input_ids'].shape[1]:]\n",
    "        raw_text = gen_tokenizer.decode(new_tokens, skip_special_tokens=True)\n",
    "\n",
    "        # Post-process: strip, take first line, remove quotes, truncate to 15 words\n",
    "        cleaned = raw_text.strip().split(\"\\n\")[0].strip()\n",
    "        cleaned = cleaned.strip('\"').strip(\"'\").strip()\n",
    "        cleaned = \" \".join(cleaned.split()[:15])\n",
    "        return cleaned\n",
    "\n",
    "    # Generate with checkpointing\n",
    "    surrogates = []\n",
    "    gen_ckpt_path = RESULTS_DIR / \"gen_checkpoint.json\"\n",
    "\n",
    "    if gen_ckpt_path.exists():\n",
    "        gen_ckpt = json.loads(gen_ckpt_path.read_text())\n",
    "        if gen_ckpt.get('n_total') == N_SAMPLES:\n",
    "            surrogates = gen_ckpt['surrogates']\n",
    "            print(f\"Resuming generation from {len(surrogates)}/{N_SAMPLES}\")\n",
    "\n",
    "    start_gen = len(surrogates)\n",
    "    t0 = time.time()\n",
    "\n",
    "    for i in tqdm(range(start_gen, N_SAMPLES), initial=start_gen, total=N_SAMPLES,\n",
    "                  desc=\"Generating\"):\n",
    "        s = samples[i]\n",
    "        entry = {'query': s['query']}\n",
    "\n",
    "        # Paraphrase: rephrase the query\n",
    "        torch.manual_seed(SEED + i * 10)\n",
    "        entry['paraphrase'] = generate_text(\n",
    "            f\"Query: {s['query']}\", PROMPT_PARAPHRASE\n",
    "        )\n",
    "\n",
    "        # Same-topic: question about same topic but different info\n",
    "        torch.manual_seed(SEED + i * 10 + 1)\n",
    "        words = s['passage'].split()[:150]\n",
    "        entry['same_topic'] = generate_text(\n",
    "            f\"Document:\\n{' '.join(words)}\", PROMPT_SAME_TOPIC\n",
    "        )\n",
    "\n",
    "        surrogates.append(entry)\n",
    "\n",
    "        if (i + 1) % 50 == 0 or i == N_SAMPLES - 1:\n",
    "            gen_ckpt = {'n_total': N_SAMPLES, 'surrogates': surrogates,\n",
    "                        'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')}\n",
    "            gen_ckpt_path.write_text(json.dumps(gen_ckpt))\n",
    "            elapsed = time.time() - t0\n",
    "            done = i - start_gen + 1\n",
    "            eta = (N_SAMPLES - i - 1) * elapsed / done if done > 0 else 0\n",
    "            tqdm.write(f\"  Gen checkpoint {i+1}/{N_SAMPLES} | \"\n",
    "                       f\"{elapsed/60:.1f}m | ETA {eta/60:.1f}m\")\n",
    "\n",
    "    elapsed = time.time() - t0\n",
    "    print(f\"\\nGeneration complete: {len(surrogates)} samples in {elapsed/60:.1f} min\")\n",
    "\n",
    "    # Save final surrogates\n",
    "    SURROGATES_PATH.write_text(json.dumps(surrogates, indent=2))\n",
    "    print(f\"Saved surrogates to {SURROGATES_PATH}\")\n",
    "\n",
    "    # Free VRAM\n",
    "    print(\"Freeing generation model VRAM...\")\n",
    "    mem_before = torch.cuda.memory_allocated() / 1e9\n",
    "    del gen_model, gen_tokenizer\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    mem_after = torch.cuda.memory_allocated() / 1e9\n",
    "    print(f\"GPU memory: {mem_before:.2f} GB -> {mem_after:.2f} GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c988bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Load scoring model and define helpers\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv())\n",
    "HF_TOKEN = os.environ.get(\"HF_TOKEN\")\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "print(f\"Loading {MODEL_NAME}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, token=HF_TOKEN)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME, device_map=\"auto\", torch_dtype=torch.bfloat16, token=HF_TOKEN,\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "DEVICE = next(model.parameters()).device\n",
    "\n",
    "print(f\"Model loaded. dtype={next(model.parameters()).dtype}\")\n",
    "print(f\"GPU memory: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "text_cfg = getattr(model.config, 'text_config', model.config)\n",
    "VOCAB_SIZE = getattr(text_cfg, 'vocab_size', 262208)\n",
    "print(f\"Vocab size: {VOCAB_SIZE}\")\n",
    "\n",
    "NEWLINE_IDS = tokenizer(\"\\n\", add_special_tokens=False).input_ids\n",
    "BOS_ID = tokenizer.bos_token_id\n",
    "print(f\"BOS token ID: {BOS_ID}\")\n",
    "print(f\"Newline token IDs: {NEWLINE_IDS} ({len(NEWLINE_IDS)} tokens)\")\n",
    "\n",
    "\n",
    "def slice_kv_cache(cache, start_idx):\n",
    "    # Remove first start_idx entries from KV cache.\n",
    "    from transformers import DynamicCache\n",
    "    if isinstance(cache, DynamicCache):\n",
    "        sliced = DynamicCache()\n",
    "        for i in range(len(cache.layers)):\n",
    "            k = cache.layers[i].keys[:, :, start_idx:, :]\n",
    "            v = cache.layers[i].values[:, :, start_idx:, :]\n",
    "            sliced.update(k, v, i)\n",
    "        return sliced\n",
    "    else:\n",
    "        return tuple(\n",
    "            (k[:, :, start_idx:, :], v[:, :, start_idx:, :])\n",
    "            for k, v in cache\n",
    "        )\n",
    "\n",
    "\n",
    "def score(doc_text, query_text, answer_text, prefix_token_ids=None):\n",
    "    # Score NLL of answer tokens using two-phase KV cache.\n",
    "    #\n",
    "    # If prefix_token_ids is provided:\n",
    "    #   Phase A: [BOS] + prefix_ids + [\\n] + doc_ids\n",
    "    #   Slice first 1+len(prefix_ids)+len(NEWLINE_IDS) entries\n",
    "    # Otherwise (bare):\n",
    "    #   Phase A: [BOS] + doc_ids (nothing sliced)\n",
    "\n",
    "    # --- Phase A: Conditioning ---\n",
    "    doc_ids = tokenizer(doc_text, add_special_tokens=False,\n",
    "                        truncation=True, max_length=1536).input_ids\n",
    "\n",
    "    if prefix_token_ids is not None:\n",
    "        cond_ids = [BOS_ID] + list(prefix_token_ids) + NEWLINE_IDS + doc_ids\n",
    "        slice_start = 1 + len(prefix_token_ids) + len(NEWLINE_IDS)\n",
    "        phase_b_start = len(cond_ids)\n",
    "    else:\n",
    "        cond_ids = [BOS_ID] + doc_ids\n",
    "        slice_start = 0\n",
    "        phase_b_start = len(cond_ids)\n",
    "\n",
    "    cond_tensor = torch.tensor([cond_ids], dtype=torch.long, device=DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        phase_a = model(input_ids=cond_tensor, use_cache=True)\n",
    "\n",
    "    cache = phase_a.past_key_values\n",
    "    del phase_a\n",
    "\n",
    "    if slice_start > 0:\n",
    "        cache = slice_kv_cache(cache, slice_start)\n",
    "\n",
    "    # --- Phase B: Inference ---\n",
    "    query_part_ids = tokenizer(\"\\n\" + query_text + \"\\n\",\n",
    "                               add_special_tokens=False).input_ids\n",
    "    answer_ids = tokenizer(answer_text, add_special_tokens=False,\n",
    "                           truncation=True, max_length=256).input_ids\n",
    "\n",
    "    if not answer_ids:\n",
    "        del cache\n",
    "        return 0.0\n",
    "\n",
    "    phase_b_ids = query_part_ids + answer_ids\n",
    "    phase_b_tensor = torch.tensor([phase_b_ids], dtype=torch.long, device=DEVICE)\n",
    "\n",
    "    pos_ids = torch.arange(phase_b_start, phase_b_start + len(phase_b_ids),\n",
    "                           device=DEVICE).unsqueeze(0)\n",
    "    cache_position = torch.arange(phase_b_start, phase_b_start + len(phase_b_ids),\n",
    "                                  device=DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        phase_b = model(\n",
    "            input_ids=phase_b_tensor,\n",
    "            past_key_values=cache,\n",
    "            position_ids=pos_ids,\n",
    "            cache_position=cache_position,\n",
    "            use_cache=False,\n",
    "        )\n",
    "\n",
    "    logits = phase_b.logits\n",
    "    n_query_part = len(query_part_ids)\n",
    "    n_answer = len(answer_ids)\n",
    "\n",
    "    answer_logits = logits[0, n_query_part - 1 : n_query_part - 1 + n_answer, :]\n",
    "    answer_targets = torch.tensor(answer_ids, dtype=torch.long, device=DEVICE)\n",
    "\n",
    "    log_probs = F.log_softmax(answer_logits, dim=-1)\n",
    "    token_log_probs = log_probs.gather(1, answer_targets.unsqueeze(1)).squeeze(1)\n",
    "    mean_nll = -token_log_probs.mean().item()\n",
    "\n",
    "    del cache, phase_b, logits, log_probs\n",
    "    return mean_nll\n",
    "\n",
    "\n",
    "print(\"Scoring function defined.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb54ba48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Build per-sample token-level prefix IDs\n",
    "\n",
    "# Collect all query token IDs (for random pool)\n",
    "all_query_token_ids = []\n",
    "for s in samples:\n",
    "    ids = tokenizer(s['query'], add_special_tokens=False).input_ids\n",
    "    all_query_token_ids.extend(ids)\n",
    "query_vocab_pool = list(set(all_query_token_ids))\n",
    "print(f\"Query vocabulary pool: {len(query_vocab_pool)} unique token IDs\")\n",
    "\n",
    "pyrandom.seed(SEED + 200)\n",
    "\n",
    "for i, s in enumerate(samples):\n",
    "    surr = surrogates[i]\n",
    "    q_ids = tokenizer(s['query'], add_special_tokens=False).input_ids\n",
    "    Q = len(q_ids)\n",
    "    s['Q'] = Q\n",
    "\n",
    "    # 1. oracle: actual query tokens\n",
    "    s['prefix_oracle'] = q_ids\n",
    "\n",
    "    # 2. paraphrase: LLM paraphrase, tokenized and truncated/padded to Q\n",
    "    para_ids = tokenizer(surr['paraphrase'], add_special_tokens=False).input_ids\n",
    "    if len(para_ids) >= Q:\n",
    "        s['prefix_paraphrase'] = para_ids[:Q]\n",
    "    else:\n",
    "        padded = para_ids * ((Q // max(len(para_ids), 1)) + 1)\n",
    "        s['prefix_paraphrase'] = padded[:Q]\n",
    "\n",
    "    # 3. same_topic: LLM same-topic question, tokenized and truncated/padded to Q\n",
    "    topic_ids = tokenizer(surr['same_topic'], add_special_tokens=False).input_ids\n",
    "    if len(topic_ids) >= Q:\n",
    "        s['prefix_same_topic'] = topic_ids[:Q]\n",
    "    else:\n",
    "        padded = topic_ids * ((Q // max(len(topic_ids), 1)) + 1)\n",
    "        s['prefix_same_topic'] = padded[:Q]\n",
    "\n",
    "    # 4. unrelated_query: different sample's query tokens, truncated/padded to Q\n",
    "    other_idx = (i + N_SAMPLES // 2) % len(samples)\n",
    "    other_q_ids = tokenizer(samples[other_idx]['query'],\n",
    "                            add_special_tokens=False).input_ids\n",
    "    if len(other_q_ids) >= Q:\n",
    "        s['prefix_unrelated'] = other_q_ids[:Q]\n",
    "    else:\n",
    "        padded = other_q_ids * ((Q // max(len(other_q_ids), 1)) + 1)\n",
    "        s['prefix_unrelated'] = padded[:Q]\n",
    "\n",
    "    # 5. scrambled_oracle: query tokens randomly permuted\n",
    "    shuffled = list(q_ids)\n",
    "    pyrandom.shuffle(shuffled)\n",
    "    s['prefix_scrambled'] = shuffled\n",
    "\n",
    "    # 6. random_matched: words from unrelated passage, tokenized and truncated/padded to Q\n",
    "    other_words = samples[other_idx]['passage'].split()\n",
    "    random_text = \" \".join(other_words[:len(s['query'].split())])\n",
    "    rand_ids = tokenizer(random_text, add_special_tokens=False).input_ids\n",
    "    if len(rand_ids) >= Q:\n",
    "        s['prefix_random'] = rand_ids[:Q]\n",
    "    else:\n",
    "        padded = rand_ids * ((Q // max(len(rand_ids), 1)) + 1)\n",
    "        s['prefix_random'] = padded[:Q]\n",
    "\n",
    "# Summary statistics\n",
    "q_lens = [s['Q'] for s in samples]\n",
    "print(f\"\\nLoaded {len(samples)} samples\")\n",
    "print(f\"Mean passage words: {np.mean([s['word_count'] for s in samples]):.0f}\")\n",
    "print(f\"Query token count — mean: {np.mean(q_lens):.1f}, \"\n",
    "      f\"median: {np.median(q_lens):.0f}, \"\n",
    "      f\"min: {np.min(q_lens)}, max: {np.max(q_lens)}\")\n",
    "\n",
    "# Verify all prefixes have exactly Q tokens\n",
    "prefix_names = ['prefix_oracle', 'prefix_paraphrase', 'prefix_same_topic',\n",
    "                'prefix_unrelated', 'prefix_scrambled', 'prefix_random']\n",
    "for i, s in enumerate(samples[:5]):\n",
    "    Q = s['Q']\n",
    "    for name in prefix_names:\n",
    "        assert len(s[name]) == Q, f\"Sample {i} {name}: len={len(s[name])} != Q={Q}\"\n",
    "    print(f\"  Sample {i}: Q={Q}, query='{s['query'][:50]}...'\")\n",
    "    for name in prefix_names:\n",
    "        label = name.replace('prefix_', '')\n",
    "        print(f\"    {label:<15}: {tokenizer.decode(s[name][:8])}...\")\n",
    "print(\"All prefix lengths verified.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443a82e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Validate scoring\n",
    "print(\"=\" * 70)\n",
    "print(\"VALIDATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "s = samples[0]\n",
    "Q = s['Q']\n",
    "\n",
    "print(f\"\\nSample 0: Q={Q} query tokens\")\n",
    "print(f\"  Query: '{s['query']}'\")\n",
    "print(f\"  Doc position start (all prefixed): {Q + 2}\")\n",
    "print(f\"  (BOS=1 + prefix={Q} + newline={len(NEWLINE_IDS)})\")\n",
    "\n",
    "print(f\"\\n--- NLL for each condition (sample 0) ---\")\n",
    "nll_bare = score(s['passage'], s['query'], s['answer'])\n",
    "print(f\"  {'bare':<20} NLL = {nll_bare:.4f}\")\n",
    "\n",
    "for name, prefix_key in [('oracle', 'prefix_oracle'),\n",
    "                          ('paraphrase', 'prefix_paraphrase'),\n",
    "                          ('same_topic', 'prefix_same_topic'),\n",
    "                          ('unrelated_query', 'prefix_unrelated'),\n",
    "                          ('scrambled_oracle', 'prefix_scrambled'),\n",
    "                          ('random_matched', 'prefix_random')]:\n",
    "    nll = score(s['passage'], s['query'], s['answer'],\n",
    "                prefix_token_ids=s[prefix_key])\n",
    "    print(f\"  {name:<20} NLL = {nll:.4f}  delta = {nll_bare - nll:+.4f}\")\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b63225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Scoring loop — 7 conditions x 400 samples\n",
    "print(\"=\" * 70)\n",
    "print(\"SCORING ALL CONDITIONS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "COND_NAMES = [\n",
    "    'bare', 'oracle', 'paraphrase', 'same_topic',\n",
    "    'unrelated_query', 'scrambled_oracle', 'random_matched',\n",
    "]\n",
    "\n",
    "# Semantic relevance ordering (for gradient analysis)\n",
    "RELEVANCE_ORDER = [\n",
    "    ('random_matched', 'Random matched', 0, 'none (structural baseline)'),\n",
    "    ('scrambled_oracle', 'Scrambled oracle', 1, 'vocabulary only'),\n",
    "    ('unrelated_query', 'Unrelated query', 2, 'low (wrong topic)'),\n",
    "    ('same_topic', 'Same topic', 3, 'medium (right topic)'),\n",
    "    ('paraphrase', 'Paraphrase', 4, 'high (same meaning)'),\n",
    "    ('oracle', 'Oracle', 5, 'maximal (exact query)'),\n",
    "]\n",
    "\n",
    "PREFIX_MAP = {\n",
    "    'oracle': 'prefix_oracle',\n",
    "    'paraphrase': 'prefix_paraphrase',\n",
    "    'same_topic': 'prefix_same_topic',\n",
    "    'unrelated_query': 'prefix_unrelated',\n",
    "    'scrambled_oracle': 'prefix_scrambled',\n",
    "    'random_matched': 'prefix_random',\n",
    "}\n",
    "\n",
    "results = []\n",
    "start_idx = 0\n",
    "\n",
    "if CHECKPOINT_PATH.exists():\n",
    "    ckpt = json.loads(CHECKPOINT_PATH.read_text())\n",
    "    if ckpt.get('n_total') == N_SAMPLES and len(ckpt.get('results', [])) > 0:\n",
    "        saved_queries = [r['query'][:50] for r in ckpt['results']]\n",
    "        current_queries = [s['query'][:50] for s in samples[:len(saved_queries)]]\n",
    "        if saved_queries == current_queries:\n",
    "            results = ckpt['results']\n",
    "            start_idx = len(results)\n",
    "            print(f\"Resuming from checkpoint: {start_idx}/{N_SAMPLES}\")\n",
    "\n",
    "if start_idx == 0:\n",
    "    print(f\"Starting fresh: {len(COND_NAMES)} conditions x {N_SAMPLES} samples \"\n",
    "          f\"= {len(COND_NAMES) * N_SAMPLES} scorings\")\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "for i in tqdm(range(start_idx, N_SAMPLES), initial=start_idx, total=N_SAMPLES,\n",
    "              desc=\"Scoring\"):\n",
    "    s = samples[i]\n",
    "    result = {\n",
    "        'query': s['query'],\n",
    "        'answer': s['answer'],\n",
    "        'passage_words': s['word_count'],\n",
    "        'Q': s['Q'],\n",
    "    }\n",
    "\n",
    "    # bare\n",
    "    result['nll_bare'] = score(s['passage'], s['query'], s['answer'])\n",
    "\n",
    "    # All prefixed conditions\n",
    "    for cond_name, prefix_key in PREFIX_MAP.items():\n",
    "        result[f'nll_{cond_name}'] = score(\n",
    "            s['passage'], s['query'], s['answer'],\n",
    "            prefix_token_ids=s[prefix_key]\n",
    "        )\n",
    "\n",
    "    results.append(result)\n",
    "\n",
    "    if (i + 1) % 20 == 0 or i == N_SAMPLES - 1:\n",
    "        ckpt = {\n",
    "            'n_total': N_SAMPLES,\n",
    "            'results': results,\n",
    "            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        }\n",
    "        CHECKPOINT_PATH.write_text(json.dumps(ckpt))\n",
    "        elapsed = time.time() - t0\n",
    "        done = i - start_idx + 1\n",
    "        eta = (N_SAMPLES - i - 1) * elapsed / done if done > 0 else 0\n",
    "        tqdm.write(f\"  Checkpoint {i+1}/{N_SAMPLES} | \"\n",
    "                   f\"{elapsed/60:.1f}m | ETA {eta/60:.1f}m\")\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "elapsed = time.time() - t0\n",
    "print(f\"\\nScoring complete: {len(results)} samples, \"\n",
    "      f\"{len(COND_NAMES)} conditions in {elapsed/60:.1f} min\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d1f1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Part 1 — Standard Condition Table\n",
    "print(\"=\" * 70)\n",
    "print(\"PART 1: STANDARD CONDITION TABLE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "bare_nlls = np.array([r['nll_bare'] for r in results])\n",
    "oracle_nlls = np.array([r['nll_oracle'] for r in results])\n",
    "oracle_benefit = bare_nlls - oracle_nlls\n",
    "oracle_d = cohens_d(oracle_benefit)\n",
    "\n",
    "all_conds = [\n",
    "    ('oracle', 'Oracle (real query)'),\n",
    "    ('paraphrase', 'Paraphrase (same meaning)'),\n",
    "    ('same_topic', 'Same topic (diff question)'),\n",
    "    ('unrelated_query', 'Unrelated query (wrong topic)'),\n",
    "    ('scrambled_oracle', 'Scrambled oracle (vocab only)'),\n",
    "    ('random_matched', 'Random matched (structural)'),\n",
    "]\n",
    "\n",
    "alpha_bonf = 0.05 / len(all_conds)\n",
    "\n",
    "print(f\"\\n{'Condition':<38} {'NLL':>8} {'Delta':>8} {'d':>8} \"\n",
    "      f\"{'Win%':>7} {'%Orc':>6} {'p':>12} {'sig':>5}\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "for cond, desc in all_conds:\n",
    "    nlls = np.array([r[f'nll_{cond}'] for r in results])\n",
    "    benefit = bare_nlls - nlls\n",
    "    d = cohens_d(benefit)\n",
    "    delta = benefit.mean()\n",
    "    win = 100 * np.mean(benefit > 0)\n",
    "    pct = d / oracle_d * 100 if oracle_d > 0 else 0\n",
    "    _, p = stats.ttest_1samp(benefit, 0)\n",
    "    sig = '***' if p < alpha_bonf / 10 else '**' if p < alpha_bonf else '*' if p < 0.05 else 'ns'\n",
    "    print(f\"  {desc:<36} {nlls.mean():>8.4f} {delta:>+8.4f} {d:>+8.3f} \"\n",
    "          f\"{win:>6.1f}% {pct:>5.0f}% {p:>12.2e} {sig}\")\n",
    "\n",
    "print(f\"\\n  bare (lower bound): {bare_nlls.mean():.4f}\")\n",
    "print(f\"  Bonferroni threshold: alpha={alpha_bonf:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2097bf81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Part 2 — Semantic Gradient\n",
    "print(\"=\" * 70)\n",
    "print(\"PART 2: SEMANTIC GRADIENT\")\n",
    "print(\"=\" * 70)\n",
    "print(\"Does NLL improvement increase monotonically with semantic relevance?\\n\")\n",
    "\n",
    "random_nlls = np.array([r['nll_random_matched'] for r in results])\n",
    "random_benefit = bare_nlls - random_nlls\n",
    "\n",
    "print(\"--- Raw delta (benefit over bare) ---\")\n",
    "print(f\"  {'Condition':<30} {'Relevance':>10} {'Mean delta':>12} {'d':>8} {'%Oracle':>8}\")\n",
    "print(f\"  {'-'*75}\")\n",
    "\n",
    "gradient_ds = []\n",
    "gradient_labels = []\n",
    "\n",
    "for cond, desc, rank, rel_desc in RELEVANCE_ORDER:\n",
    "    nlls = np.array([r[f'nll_{cond}'] for r in results])\n",
    "    benefit = bare_nlls - nlls\n",
    "    d = cohens_d(benefit)\n",
    "    pct = d / oracle_d * 100 if oracle_d > 0 else 0\n",
    "    print(f\"  {desc:<30} {rank:>10} {benefit.mean():>+12.4f} {d:>+8.3f} {pct:>7.0f}%\")\n",
    "    gradient_ds.append(d)\n",
    "    gradient_labels.append(desc)\n",
    "\n",
    "# Semantic delta (above structural baseline)\n",
    "print(f\"\\n--- Semantic delta (above random_matched baseline) ---\")\n",
    "print(f\"  {'Condition':<30} {'Semantic d':>12} {'p vs random':>14} {'sig':>5}\")\n",
    "print(f\"  {'-'*65}\")\n",
    "\n",
    "semantic_ds = []\n",
    "\n",
    "for cond, desc, rank, rel_desc in RELEVANCE_ORDER:\n",
    "    nlls = np.array([r[f'nll_{cond}'] for r in results])\n",
    "    diff = random_nlls - nlls  # positive = condition is better than random\n",
    "    d = cohens_d(diff)\n",
    "    _, p = stats.ttest_1samp(diff, 0)\n",
    "    sig = '***' if p < 0.001 else '**' if p < 0.01 else '*' if p < 0.05 else 'ns'\n",
    "    print(f\"  {desc:<30} {d:>+12.3f} {p:>14.2e} {sig}\")\n",
    "    semantic_ds.append(d)\n",
    "\n",
    "# Monotonicity test (Spearman)\n",
    "ranks = [rank for _, _, rank, _ in RELEVANCE_ORDER]\n",
    "rho, p_mono = stats.spearmanr(ranks, gradient_ds)\n",
    "sig_mono = '***' if p_mono < 0.001 else '**' if p_mono < 0.01 else '*' if p_mono < 0.05 else 'ns'\n",
    "print(f\"\\n--- Monotonicity test ---\")\n",
    "print(f\"  Spearman rho (relevance rank vs Cohen's d): rho={rho:+.3f}, \"\n",
    "      f\"p={p_mono:.4f} {sig_mono}\")\n",
    "\n",
    "rho_sem, p_sem = stats.spearmanr(ranks, semantic_ds)\n",
    "sig_sem = '***' if p_sem < 0.001 else '**' if p_sem < 0.01 else '*' if p_sem < 0.05 else 'ns'\n",
    "print(f\"  Spearman rho (relevance rank vs semantic d): rho={rho_sem:+.3f}, \"\n",
    "      f\"p={p_sem:.4f} {sig_sem}\")\n",
    "\n",
    "if rho > 0.8 and p_mono < 0.05:\n",
    "    print(f\"  --> MONOTONIC: clear semantic gradient (rho={rho:+.3f})\")\n",
    "elif rho > 0.5:\n",
    "    print(f\"  --> PARTIAL: imperfect gradient (rho={rho:+.3f})\")\n",
    "else:\n",
    "    print(f\"  --> FLAT: no clear gradient (rho={rho:+.3f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4d4f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Part 3 — Decomposition Chain\n",
    "print(\"=\" * 70)\n",
    "print(\"PART 3: FINE-GRAINED DECOMPOSITION CHAIN\")\n",
    "print(\"=\" * 70)\n",
    "print(\"bare -> random_matched -> scrambled_oracle -> unrelated_query -> \"\n",
    "      \"same_topic -> paraphrase -> oracle\\n\")\n",
    "\n",
    "scrambled_nlls = np.array([r['nll_scrambled_oracle'] for r in results])\n",
    "unrelated_nlls = np.array([r['nll_unrelated_query'] for r in results])\n",
    "same_topic_nlls = np.array([r['nll_same_topic'] for r in results])\n",
    "paraphrase_nlls = np.array([r['nll_paraphrase'] for r in results])\n",
    "\n",
    "chain = [\n",
    "    ('Structure', bare_nlls - random_nlls),\n",
    "    ('Vocabulary', random_nlls - scrambled_nlls),\n",
    "    ('Query syntax', scrambled_nlls - unrelated_nlls),\n",
    "    ('Topic relevance', unrelated_nlls - same_topic_nlls),\n",
    "    ('Semantic precision', same_topic_nlls - paraphrase_nlls),\n",
    "    ('Exact match', paraphrase_nlls - oracle_nlls),\n",
    "]\n",
    "\n",
    "total = bare_nlls - oracle_nlls\n",
    "total_mean = total.mean()\n",
    "\n",
    "print(f\"  {'Component':<22} {'Delta':>10} {'%total':>8} {'d':>8} {'p':>12} {'sig':>5}\")\n",
    "print(f\"  {'-'*70}\")\n",
    "\n",
    "chain_pcts = {}\n",
    "for label, comp in chain:\n",
    "    mu = comp.mean()\n",
    "    pct = mu / total_mean * 100 if total_mean != 0 else 0\n",
    "    d = cohens_d(comp)\n",
    "    _, p = stats.ttest_1samp(comp, 0)\n",
    "    sig = '***' if p < 0.001 else '**' if p < 0.01 else '*' if p < 0.05 else 'ns'\n",
    "    print(f\"  {label:<22} {mu:>+10.4f} {pct:>7.1f}% {d:>+8.3f} {p:>12.2e} {sig}\")\n",
    "    chain_pcts[label] = pct\n",
    "\n",
    "print(f\"  {'TOTAL':<22} {total_mean:>+10.4f} {'100.0%':>8}\")\n",
    "residual = total_mean - sum(comp.mean() for _, comp in chain)\n",
    "print(f\"\\n  Decomposition residual: {residual:.6f} (should be ~0)\")\n",
    "\n",
    "struct_pct = chain_pcts['Structure']\n",
    "print(f\"\\n--- Grouped Summary ---\")\n",
    "print(f\"  Structure:                {struct_pct:>6.1f}%\")\n",
    "print(f\"  All semantic components:  {100 - struct_pct:>6.1f}%\")\n",
    "for label in ['Vocabulary', 'Query syntax', 'Topic relevance',\n",
    "              'Semantic precision', 'Exact match']:\n",
    "    print(f\"    {label}:{'':>{20-len(label)}} {chain_pcts[label]:>6.1f}%\")\n",
    "\n",
    "# Comparison with v3 Exp 12\n",
    "print(f\"\\n--- v3 Exp 12 comparison (T5Gemma encoder-decoder) ---\")\n",
    "print(f\"  v3: Structure=86.5%, Vocab=4.3%, Syntax=-1.6%, \"\n",
    "      f\"Topic=-15.8%, Precision=7.4%, Exact=19.2%\")\n",
    "print(f\"  v4: Structure={struct_pct:.1f}%, Vocab={chain_pcts['Vocabulary']:.1f}%, \"\n",
    "      f\"Syntax={chain_pcts['Query syntax']:.1f}%, \"\n",
    "      f\"Topic={chain_pcts['Topic relevance']:.1f}%, \"\n",
    "      f\"Precision={chain_pcts['Semantic precision']:.1f}%, \"\n",
    "      f\"Exact={chain_pcts['Exact match']:.1f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1bbfac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Part 4 — Hardness Interaction\n",
    "print(\"=\" * 70)\n",
    "print(\"PART 4: HARDNESS INTERACTION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "quintile_bounds = np.percentile(bare_nlls, [20, 40, 60, 80])\n",
    "quintiles = np.digitize(bare_nlls, quintile_bounds)\n",
    "q_labels = ['Q1 easy', 'Q2', 'Q3', 'Q4', 'Q5 hard']\n",
    "\n",
    "print(\"--- Semantic delta by quintile (above random baseline) ---\")\n",
    "print(f\"  {'Quintile':<12} {'Bare NLL':>10}\", end=\"\")\n",
    "for _, desc, _, _ in RELEVANCE_ORDER[1:]:\n",
    "    print(f\"  {desc:>12}\", end=\"\")\n",
    "print()\n",
    "print(f\"  {'-'*(14 + 14 * (len(RELEVANCE_ORDER) - 1))}\")\n",
    "\n",
    "for q in range(5):\n",
    "    mask = quintiles == q\n",
    "    row = f\"  {q_labels[q]:<12} {bare_nlls[mask].mean():>10.3f}\"\n",
    "    for cond, desc, _, _ in RELEVANCE_ORDER[1:]:\n",
    "        nlls_c = np.array([r[f'nll_{cond}'] for r in results])[mask]\n",
    "        rand_c = random_nlls[mask]\n",
    "        sem_delta = cohens_d(rand_c - nlls_c)\n",
    "        row += f\"  {sem_delta:>+12.3f}\"\n",
    "    print(row)\n",
    "\n",
    "print(f\"\\n--- Structure vs semantic % by quintile ---\")\n",
    "print(f\"  {'Quintile':<12} {'Struct%':>9} {'Vocab%':>8} {'Syntax%':>9} \"\n",
    "      f\"{'Topic%':>8} {'Precis%':>9} {'Exact%':>8}\")\n",
    "print(f\"  {'-'*65}\")\n",
    "\n",
    "for q in range(5):\n",
    "    mask = quintiles == q\n",
    "    total_q = (bare_nlls[mask] - oracle_nlls[mask]).mean()\n",
    "    if total_q > 0:\n",
    "        s_pct = (bare_nlls[mask] - random_nlls[mask]).mean() / total_q * 100\n",
    "        v_pct = (random_nlls[mask] - scrambled_nlls[mask]).mean() / total_q * 100\n",
    "        syn_pct = (scrambled_nlls[mask] - unrelated_nlls[mask]).mean() / total_q * 100\n",
    "        top_pct = (unrelated_nlls[mask] - same_topic_nlls[mask]).mean() / total_q * 100\n",
    "        pre_pct = (same_topic_nlls[mask] - paraphrase_nlls[mask]).mean() / total_q * 100\n",
    "        ex_pct = (paraphrase_nlls[mask] - oracle_nlls[mask]).mean() / total_q * 100\n",
    "    else:\n",
    "        s_pct = v_pct = syn_pct = top_pct = pre_pct = ex_pct = 0\n",
    "    print(f\"  {q_labels[q]:<12} {s_pct:>8.1f}% {v_pct:>7.1f}% {syn_pct:>8.1f}% \"\n",
    "          f\"{top_pct:>7.1f}% {pre_pct:>8.1f}% {ex_pct:>7.1f}%\")\n",
    "\n",
    "# Correlation: hardness vs semantic delta\n",
    "print(f\"\\n--- Correlations: hardness vs semantic delta ---\")\n",
    "for cond, desc, rank, _ in RELEVANCE_ORDER[1:]:\n",
    "    nlls_c = np.array([r[f'nll_{cond}'] for r in results])\n",
    "    sem_delta = random_nlls - nlls_c\n",
    "    r_val, p_val = stats.pearsonr(bare_nlls, sem_delta)\n",
    "    sig = '***' if p_val < 0.001 else '**' if p_val < 0.01 else '*' if p_val < 0.05 else 'ns'\n",
    "    print(f\"  {desc:<25} r={r_val:+.3f} (p={p_val:.2e}) {sig}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12652b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13: Synthesis + Save\n",
    "print(\"=\" * 70)\n",
    "print(\"SYNTHESIS: GRADED SEMANTIC RELEVANCE (DECODER-ONLY)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\n1. SEMANTIC GRADIENT (d, % oracle):\")\n",
    "for cond, desc, rank, rel_desc in RELEVANCE_ORDER:\n",
    "    nlls = np.array([r[f'nll_{cond}'] for r in results])\n",
    "    d = cohens_d(bare_nlls - nlls)\n",
    "    pct = d / oracle_d * 100 if oracle_d > 0 else 0\n",
    "    print(f\"   [{rank}] {desc:<25} d={d:>+.3f} ({pct:>5.1f}% oracle) — {rel_desc}\")\n",
    "\n",
    "print(f\"\\n2. MONOTONICITY:\")\n",
    "print(f\"   Spearman rho (raw d): {rho:+.3f} (p={p_mono:.4f})\")\n",
    "print(f\"   Spearman rho (semantic d): {rho_sem:+.3f} (p={p_sem:.4f})\")\n",
    "\n",
    "print(f\"\\n3. DECOMPOSITION CHAIN:\")\n",
    "for label, pct in chain_pcts.items():\n",
    "    print(f\"   {label:<22} {pct:>6.1f}%\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"CONCLUSIONS:\")\n",
    "\n",
    "if rho > 0.8 and p_mono < 0.05:\n",
    "    print(f\"  1. CLEAR SEMANTIC GRADIENT: monotonic (rho={rho:+.3f}, p={p_mono:.4f})\")\n",
    "elif rho > 0.5:\n",
    "    print(f\"  1. PARTIAL GRADIENT: imperfect (rho={rho:+.3f}, p={p_mono:.4f})\")\n",
    "else:\n",
    "    print(f\"  1. NO CLEAR GRADIENT (rho={rho:+.3f}, p={p_mono:.4f})\")\n",
    "\n",
    "if struct_pct > 75:\n",
    "    print(f\"  2. Structure dominates ({struct_pct:.0f}%)\")\n",
    "elif struct_pct > 50:\n",
    "    print(f\"  2. Structure largest ({struct_pct:.0f}%) but semantics substantial\")\n",
    "else:\n",
    "    print(f\"  2. Semantics dominate ({100-struct_pct:.0f}%)\")\n",
    "\n",
    "sem_components = [(k, v) for k, v in chain_pcts.items() if k != 'Structure']\n",
    "largest_sem = max(sem_components, key=lambda x: abs(x[1]))\n",
    "print(f\"  3. Largest semantic component: {largest_sem[0]} ({largest_sem[1]:+.1f}%)\")\n",
    "\n",
    "# Cross-architecture comparison\n",
    "print(f\"\\n--- Cross-architecture comparison ---\")\n",
    "print(f\"  v3 (T5Gemma encoder-decoder): monotonic rho=+0.943, structure=86.5%\")\n",
    "print(f\"  v4 (Gemma 3 decoder-only):    rho={rho:+.3f}, structure={struct_pct:.1f}%\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "# Save results\n",
    "final_results = {\n",
    "    'experiment': 'v4_decoder_only_exp06_semantic_gradient',\n",
    "    'scoring_model': MODEL_NAME,\n",
    "    'generation_model': MODEL_NAME,\n",
    "    'dataset': 'ms_marco_v1.1',\n",
    "    'n_samples': len(results),\n",
    "    'seed': SEED,\n",
    "    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'conditions': {},\n",
    "    'gradient': {\n",
    "        'spearman_rho_raw': float(rho),\n",
    "        'spearman_p_raw': float(p_mono),\n",
    "        'spearman_rho_semantic': float(rho_sem),\n",
    "        'spearman_p_semantic': float(p_sem),\n",
    "    },\n",
    "    'decomposition_chain': {k: float(v) for k, v in chain_pcts.items()},\n",
    "    'structure_pct': float(struct_pct),\n",
    "    'query_token_stats': {\n",
    "        'mean': float(np.mean([r['Q'] for r in results])),\n",
    "        'median': float(np.median([r['Q'] for r in results])),\n",
    "        'min': int(np.min([r['Q'] for r in results])),\n",
    "        'max': int(np.max([r['Q'] for r in results])),\n",
    "    },\n",
    "}\n",
    "\n",
    "for cond, desc, rank, rel_desc in RELEVANCE_ORDER:\n",
    "    nlls = np.array([r[f'nll_{cond}'] for r in results])\n",
    "    benefit = bare_nlls - nlls\n",
    "    d = cohens_d(benefit)\n",
    "    _, p = stats.ttest_1samp(benefit, 0)\n",
    "    sem_diff = random_nlls - nlls\n",
    "    sem_d = cohens_d(sem_diff)\n",
    "    _, sem_p = stats.ttest_1samp(sem_diff, 0)\n",
    "    final_results['conditions'][cond] = {\n",
    "        'description': desc,\n",
    "        'relevance_rank': rank,\n",
    "        'relevance_label': rel_desc,\n",
    "        'd': float(d),\n",
    "        'mean_nll': float(nlls.mean()),\n",
    "        'mean_delta': float(benefit.mean()),\n",
    "        'pct_oracle': float(d / oracle_d * 100) if oracle_d > 0 else 0,\n",
    "        'p': float(p),\n",
    "        'semantic_d': float(sem_d),\n",
    "        'semantic_p': float(sem_p),\n",
    "    }\n",
    "\n",
    "with open(RESULTS_DIR / 'results.json', 'w') as f:\n",
    "    json.dump(final_results, f, indent=2)\n",
    "print(f\"\\nResults saved to {RESULTS_DIR / 'results.json'}\")\n",
    "\n",
    "# Cleanup\n",
    "print(f\"\\nCleaning up GPU memory...\")\n",
    "mem_before = torch.cuda.memory_allocated() / 1e9\n",
    "del model, tokenizer\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "mem_after = torch.cuda.memory_allocated() / 1e9\n",
    "print(f\"GPU memory: {mem_before:.2f} GB -> {mem_after:.2f} GB\")\n",
    "print(\"Done!\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
