{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33df4107",
   "metadata": {},
   "source": [
    "# Decoder-Only Exp 06: Graded Semantic Relevance\n",
    "\n",
    "## Motivation\n",
    "\n",
    "Port of v3 Exp 12 to decoder-only two-phase KV cache scoring.\n",
    "v3 Exp 12 found a monotonic semantic gradient (Spearman rho=+0.94, p=0.005)\n",
    "in the encoder-decoder T5Gemma. Does the same gradient appear with decoder-only\n",
    "Gemma 3 4B-PT using KV cache priming?\n",
    "\n",
    "## Design\n",
    "\n",
    "All structural confounds (BOS removal, position offset, cache length) are\n",
    "equalized by using token-level prefix matching: every prefixed condition\n",
    "constructs exactly Q prefix token IDs (Q = number of real query tokens).\n",
    "\n",
    "Phase A input: [BOS] + prefix_ids(Q) + [\\n] + doc_ids\n",
    "Slice from cache: first Q+2 entries (BOS + prefix + newline)\n",
    "Result: only doc KV entries remain, at identical positions across all conditions.\n",
    "\n",
    "## Conditions (7)\n",
    "\n",
    "| # | Condition | Prefix content | Semantic relevance |\n",
    "|---|-----------|---------------|--------------------|\n",
    "| 1 | bare | (none) | N/A (lower bound) |\n",
    "| 2 | oracle | real query tokens | maximal (exact query) |\n",
    "| 3 | paraphrase | LLM paraphrase tokens | high (same meaning, diff words) |\n",
    "| 4 | same_topic | LLM same-topic question | medium (right topic, wrong question) |\n",
    "| 5 | unrelated_query | different sample's query | low (real syntax, wrong topic) |\n",
    "| 6 | scrambled_oracle | query tokens shuffled | vocabulary only |\n",
    "| 7 | random_matched | random passage word tokens | none (structural baseline) |\n",
    "\n",
    "## Analysis\n",
    "\n",
    "- Part 1: Standard condition table\n",
    "- Part 2: Semantic gradient with monotonicity test\n",
    "- Part 3: Fine-grained decomposition chain\n",
    "- Part 4: Hardness interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "940f45a9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-20T15:54:01.986463Z",
     "iopub.status.busy": "2026-02-20T15:54:01.983680Z",
     "iopub.status.idle": "2026-02-20T15:54:04.427856Z",
     "shell.execute_reply": "2026-02-20T15:54:04.426884Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exp 06: Graded Semantic Relevance (Decoder-Only)\n",
      "N: 400\n",
      "Model: google/gemma-3-4b-it (generation + scoring)\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Setup\n",
    "import os\n",
    "os.umask(0o000)\n",
    "\n",
    "import sys, json, time, re, gc, random as pyrandom\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from scipy import stats\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "sys.path.insert(0, \"../../..\")\n",
    "from lib.analysis import cohens_d\n",
    "\n",
    "SEED = 42\n",
    "N_SAMPLES = 400\n",
    "MODEL_NAME = \"google/gemma-3-4b-it\"\n",
    "\n",
    "RESULTS_DIR = Path(\"../../../results/decoder_only/exp06\")\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "SURROGATES_PATH = RESULTS_DIR / \"surrogates.json\"\n",
    "CHECKPOINT_PATH = RESULTS_DIR / \"checkpoint.json\"\n",
    "\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "pyrandom.seed(SEED)\n",
    "\n",
    "# Prompt templates for LLM generation\n",
    "PROMPT_PARAPHRASE = (\n",
    "    \"Rephrase this search query using completely different words but keeping \"\n",
    "    \"the same meaning. Keep it to 5-8 words. Output only the rephrased query.\"\n",
    ")\n",
    "PROMPT_SAME_TOPIC = (\n",
    "    \"Write a question about the same topic as this document but asking for \"\n",
    "    \"DIFFERENT information. Keep it to 5-8 words. Output only the question.\"\n",
    ")\n",
    "\n",
    "print(\"Exp 06: Graded Semantic Relevance (Decoder-Only)\")\n",
    "print(f\"N: {N_SAMPLES}\")\n",
    "print(f\"Model: {MODEL_NAME} (generation + scoring)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3a20931",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-20T15:54:04.432020Z",
     "iopub.status.busy": "2026-02-20T15:54:04.431636Z",
     "iopub.status.idle": "2026-02-20T15:54:06.300689Z",
     "shell.execute_reply": "2026-02-20T15:54:06.299644Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MS MARCO...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 400 samples\n",
      "Document lengths: 30-146 words, mean=73\n",
      "Query lengths: 2-15 words, mean=5.9\n",
      "\n",
      "Example 0:\n",
      "  Q: average annual temperature of Uruguay\n",
      "  A: Very mild at 15.8 degrees Celsius (60.4 degrees Fahrenheit).\n",
      "  P (76w): Average Temperatures in Montevideo, Uruguay. 1  The average annual temperature in Montevideo, Urugua...\n",
      "\n",
      "Example 1:\n",
      "  Q: average cost for an acre of land in arizona\n",
      "  A: $4,300 per acre.\n",
      "  P (102w): Arizona. With more than 72 million acres, Arizona was one of the largest states in the country. Howe...\n",
      "\n",
      "Example 2:\n",
      "  Q: where can i buy nematodes\n",
      "  A: Here to buy THE GOOD BUGS Supplier beneficial insects, mites and nematodes for c\n",
      "  P (42w): Where to buy THE GOOD BUGS Supplier beneficial insects, mites and nematodes for commercial growers B...\n",
      "\n",
      "Example 3:\n",
      "  Q: how much does it cost for varicose vein removal\n",
      "  A: From $600 to as much as $2,000 per leg.\n",
      "  P (76w): 1 According to the website LawPublish.com, the cost of varicose vein removal can cost anywhere from ...\n",
      "\n",
      "Example 4:\n",
      "  Q: where is popocatepetl located\n",
      "  A: Mexico\n",
      "  P (45w): Confidence votes 25.5K. Mexico The volcano is located in the Puebla state, southeast-central Mexico,...\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Load MS MARCO and select samples\n",
    "from lib.data import count_words\n",
    "from datasets import load_dataset\n",
    "\n",
    "print(\"Loading MS MARCO...\")\n",
    "ds = load_dataset(\"microsoft/ms_marco\", \"v1.1\", split=\"validation\")\n",
    "\n",
    "samples = []\n",
    "for item in ds:\n",
    "    if len(samples) >= N_SAMPLES * 3:\n",
    "        break\n",
    "    passages = item.get('passages', {})\n",
    "    ptexts = passages.get('passage_text', [])\n",
    "    is_sel = passages.get('is_selected', [])\n",
    "    query = item.get('query', '')\n",
    "    answers = item.get('answers', [])\n",
    "    well_formed = item.get('wellFormedAnswers', [])\n",
    "    answer = None\n",
    "    if well_formed and len(well_formed) > 0 and well_formed[0] not in ('[]', ''):\n",
    "        answer = well_formed[0]\n",
    "    elif answers and len(answers) > 0 and answers[0] != 'No Answer Present.':\n",
    "        answer = answers[0]\n",
    "    if not answer:\n",
    "        continue\n",
    "    for pt, sel in zip(ptexts, is_sel):\n",
    "        wc = count_words(pt)\n",
    "        if sel == 1 and 30 <= wc <= 300:\n",
    "            samples.append({\n",
    "                'passage': pt, 'query': query, 'answer': answer,\n",
    "                'word_count': wc,\n",
    "            })\n",
    "            break\n",
    "\n",
    "np.random.seed(SEED)\n",
    "np.random.shuffle(samples)\n",
    "samples = samples[:N_SAMPLES]\n",
    "del ds\n",
    "gc.collect()\n",
    "\n",
    "passage_words = np.array([s['word_count'] for s in samples])\n",
    "query_words = np.array([len(s['query'].split()) for s in samples])\n",
    "print(f\"Selected {N_SAMPLES} samples\")\n",
    "print(f\"Document lengths: {passage_words.min()}-{passage_words.max()} words, \"\n",
    "      f\"mean={passage_words.mean():.0f}\")\n",
    "print(f\"Query lengths: {query_words.min()}-{query_words.max()} words, \"\n",
    "      f\"mean={query_words.mean():.1f}\")\n",
    "\n",
    "for i in range(5):\n",
    "    s = samples[i]\n",
    "    print(f\"\\nExample {i}:\")\n",
    "    print(f\"  Q: {s['query']}\")\n",
    "    print(f\"  A: {s['answer'][:80]}\")\n",
    "    print(f\"  P ({s['word_count']}w): {s['passage'][:100]}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12ee0766",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-20T15:54:06.304828Z",
     "iopub.status.busy": "2026-02-20T15:54:06.304340Z",
     "iopub.status.idle": "2026-02-20T16:03:22.195789Z",
     "shell.execute_reply": "2026-02-20T16:03:22.194780Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading google/gemma-3-4b-it for surrogate generation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "033b7faa349145e6a2939cd724e10abc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/883 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded. GPU memory: 8.60 GB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82c63216b86049199e9ba60fd6fb9946",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Gen checkpoint 50/400 | 1.1m | ETA 8.0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Gen checkpoint 100/400 | 2.3m | ETA 6.8m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Gen checkpoint 150/400 | 3.4m | ETA 5.7m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Gen checkpoint 200/400 | 4.6m | ETA 4.6m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Gen checkpoint 250/400 | 5.7m | ETA 3.4m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Gen checkpoint 300/400 | 6.8m | ETA 2.3m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Gen checkpoint 350/400 | 7.9m | ETA 1.1m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Gen checkpoint 400/400 | 9.0m | ETA 0.0m\n",
      "\n",
      "Generation complete: 400 samples in 9.0 min\n",
      "Saved surrogates to ../../../results/decoder_only/exp06/surrogates.json\n",
      "Freeing generation model VRAM...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory: 8.61 GB -> 0.01 GB\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Phase 1 — Generate surrogates with Gemma 3 12B-IT\n",
    "# Skip if surrogates already cached\n",
    "\n",
    "if SURROGATES_PATH.exists():\n",
    "    print(\"Loading cached surrogates...\")\n",
    "    surrogates = json.loads(SURROGATES_PATH.read_text())\n",
    "    assert len(surrogates) == N_SAMPLES, f\"Expected {N_SAMPLES}, got {len(surrogates)}\"\n",
    "    for i in range(min(10, N_SAMPLES)):\n",
    "        assert surrogates[i]['query'][:50] == samples[i]['query'][:50], \\\n",
    "            f\"Sample {i} query mismatch\"\n",
    "    print(f\"Loaded {len(surrogates)} cached surrogates\")\n",
    "    print(f\"Keys per sample: {list(surrogates[0].keys())}\")\n",
    "else:\n",
    "    print(f\"Loading {MODEL_NAME} for surrogate generation...\")\n",
    "    from dotenv import load_dotenv, find_dotenv\n",
    "    load_dotenv(find_dotenv())\n",
    "    HF_TOKEN = os.environ.get(\"HF_TOKEN\")\n",
    "\n",
    "    from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "    gen_tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, token=HF_TOKEN)\n",
    "    gen_model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME, device_map=\"auto\", torch_dtype=torch.bfloat16, token=HF_TOKEN,\n",
    "    )\n",
    "    gen_model.eval()\n",
    "    GEN_DEVICE = next(gen_model.parameters()).device\n",
    "    print(f\"Model loaded. GPU memory: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "\n",
    "    def generate_text(input_text, prompt_text):\n",
    "        # Generate text from a prompt + input using Gemma IT.\n",
    "        messages = [\n",
    "            {\"role\": \"user\",\n",
    "             \"content\": f\"{prompt_text}\\n\\n{input_text}\"}\n",
    "        ]\n",
    "        chat_text = gen_tokenizer.apply_chat_template(\n",
    "            messages, tokenize=False, add_generation_prompt=True\n",
    "        )\n",
    "        inputs = gen_tokenizer(chat_text, return_tensors=\"pt\",\n",
    "                               truncation=True, max_length=1024).to(GEN_DEVICE)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output_ids = gen_model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=30,\n",
    "                temperature=0.7,\n",
    "                top_p=0.9,\n",
    "                do_sample=True,\n",
    "            )\n",
    "\n",
    "        new_tokens = output_ids[0, inputs['input_ids'].shape[1]:]\n",
    "        raw_text = gen_tokenizer.decode(new_tokens, skip_special_tokens=True)\n",
    "\n",
    "        # Post-process: strip, take first line, remove quotes, truncate to 15 words\n",
    "        cleaned = raw_text.strip().split(\"\\n\")[0].strip()\n",
    "        cleaned = cleaned.strip('\"').strip(\"'\").strip()\n",
    "        cleaned = \" \".join(cleaned.split()[:15])\n",
    "        return cleaned\n",
    "\n",
    "    # Generate with checkpointing\n",
    "    surrogates = []\n",
    "    gen_ckpt_path = RESULTS_DIR / \"gen_checkpoint.json\"\n",
    "\n",
    "    if gen_ckpt_path.exists():\n",
    "        gen_ckpt = json.loads(gen_ckpt_path.read_text())\n",
    "        if gen_ckpt.get('n_total') == N_SAMPLES:\n",
    "            surrogates = gen_ckpt['surrogates']\n",
    "            print(f\"Resuming generation from {len(surrogates)}/{N_SAMPLES}\")\n",
    "\n",
    "    start_gen = len(surrogates)\n",
    "    t0 = time.time()\n",
    "\n",
    "    for i in tqdm(range(start_gen, N_SAMPLES), initial=start_gen, total=N_SAMPLES,\n",
    "                  desc=\"Generating\"):\n",
    "        s = samples[i]\n",
    "        entry = {'query': s['query']}\n",
    "\n",
    "        # Paraphrase: rephrase the query\n",
    "        torch.manual_seed(SEED + i * 10)\n",
    "        entry['paraphrase'] = generate_text(\n",
    "            f\"Query: {s['query']}\", PROMPT_PARAPHRASE\n",
    "        )\n",
    "\n",
    "        # Same-topic: question about same topic but different info\n",
    "        torch.manual_seed(SEED + i * 10 + 1)\n",
    "        words = s['passage'].split()[:150]\n",
    "        entry['same_topic'] = generate_text(\n",
    "            f\"Document:\\n{' '.join(words)}\", PROMPT_SAME_TOPIC\n",
    "        )\n",
    "\n",
    "        surrogates.append(entry)\n",
    "\n",
    "        if (i + 1) % 50 == 0 or i == N_SAMPLES - 1:\n",
    "            gen_ckpt = {'n_total': N_SAMPLES, 'surrogates': surrogates,\n",
    "                        'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')}\n",
    "            gen_ckpt_path.write_text(json.dumps(gen_ckpt))\n",
    "            elapsed = time.time() - t0\n",
    "            done = i - start_gen + 1\n",
    "            eta = (N_SAMPLES - i - 1) * elapsed / done if done > 0 else 0\n",
    "            tqdm.write(f\"  Gen checkpoint {i+1}/{N_SAMPLES} | \"\n",
    "                       f\"{elapsed/60:.1f}m | ETA {eta/60:.1f}m\")\n",
    "\n",
    "    elapsed = time.time() - t0\n",
    "    print(f\"\\nGeneration complete: {len(surrogates)} samples in {elapsed/60:.1f} min\")\n",
    "\n",
    "    # Save final surrogates\n",
    "    SURROGATES_PATH.write_text(json.dumps(surrogates, indent=2))\n",
    "    print(f\"Saved surrogates to {SURROGATES_PATH}\")\n",
    "\n",
    "    # Free VRAM\n",
    "    print(\"Freeing generation model VRAM...\")\n",
    "    mem_before = torch.cuda.memory_allocated() / 1e9\n",
    "    del gen_model, gen_tokenizer\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    mem_after = torch.cuda.memory_allocated() / 1e9\n",
    "    print(f\"GPU memory: {mem_before:.2f} GB -> {mem_after:.2f} GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c988bf0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-20T16:03:22.200062Z",
     "iopub.status.busy": "2026-02-20T16:03:22.199287Z",
     "iopub.status.idle": "2026-02-20T16:03:30.428647Z",
     "shell.execute_reply": "2026-02-20T16:03:30.427644Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading google/gemma-3-4b-it...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae1fb35370d74debb41317b898629a1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/883 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded. dtype=torch.bfloat16\n",
      "GPU memory: 8.61 GB\n",
      "Vocab size: 262208\n",
      "BOS token ID: 2\n",
      "Newline token IDs: [107] (1 tokens)\n",
      "Scoring function defined.\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Load scoring model and define helpers\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv())\n",
    "HF_TOKEN = os.environ.get(\"HF_TOKEN\")\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "print(f\"Loading {MODEL_NAME}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, token=HF_TOKEN)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME, device_map=\"auto\", torch_dtype=torch.bfloat16, token=HF_TOKEN,\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "DEVICE = next(model.parameters()).device\n",
    "\n",
    "print(f\"Model loaded. dtype={next(model.parameters()).dtype}\")\n",
    "print(f\"GPU memory: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "text_cfg = getattr(model.config, 'text_config', model.config)\n",
    "VOCAB_SIZE = getattr(text_cfg, 'vocab_size', 262208)\n",
    "print(f\"Vocab size: {VOCAB_SIZE}\")\n",
    "\n",
    "NEWLINE_IDS = tokenizer(\"\\n\", add_special_tokens=False).input_ids\n",
    "BOS_ID = tokenizer.bos_token_id\n",
    "print(f\"BOS token ID: {BOS_ID}\")\n",
    "print(f\"Newline token IDs: {NEWLINE_IDS} ({len(NEWLINE_IDS)} tokens)\")\n",
    "\n",
    "\n",
    "def slice_kv_cache(cache, start_idx):\n",
    "    # Remove first start_idx entries from KV cache.\n",
    "    from transformers import DynamicCache\n",
    "    if isinstance(cache, DynamicCache):\n",
    "        sliced = DynamicCache()\n",
    "        for i in range(len(cache.layers)):\n",
    "            k = cache.layers[i].keys[:, :, start_idx:, :]\n",
    "            v = cache.layers[i].values[:, :, start_idx:, :]\n",
    "            sliced.update(k, v, i)\n",
    "        return sliced\n",
    "    else:\n",
    "        return tuple(\n",
    "            (k[:, :, start_idx:, :], v[:, :, start_idx:, :])\n",
    "            for k, v in cache\n",
    "        )\n",
    "\n",
    "\n",
    "def score(doc_text, query_text, answer_text, prefix_token_ids=None):\n",
    "    # Score NLL of answer tokens using two-phase KV cache.\n",
    "    #\n",
    "    # If prefix_token_ids is provided:\n",
    "    #   Phase A: [BOS] + prefix_ids + [\\n] + doc_ids\n",
    "    #   Slice first 1+len(prefix_ids)+len(NEWLINE_IDS) entries\n",
    "    # Otherwise (bare):\n",
    "    #   Phase A: [BOS] + doc_ids (nothing sliced)\n",
    "\n",
    "    # --- Phase A: Conditioning ---\n",
    "    doc_ids = tokenizer(doc_text, add_special_tokens=False,\n",
    "                        truncation=True, max_length=1536).input_ids\n",
    "\n",
    "    if prefix_token_ids is not None:\n",
    "        cond_ids = [BOS_ID] + list(prefix_token_ids) + NEWLINE_IDS + doc_ids\n",
    "        slice_start = 1 + len(prefix_token_ids) + len(NEWLINE_IDS)\n",
    "        phase_b_start = len(cond_ids)\n",
    "    else:\n",
    "        cond_ids = [BOS_ID] + doc_ids\n",
    "        slice_start = 0\n",
    "        phase_b_start = len(cond_ids)\n",
    "\n",
    "    cond_tensor = torch.tensor([cond_ids], dtype=torch.long, device=DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        phase_a = model(input_ids=cond_tensor, use_cache=True)\n",
    "\n",
    "    cache = phase_a.past_key_values\n",
    "    del phase_a\n",
    "\n",
    "    if slice_start > 0:\n",
    "        cache = slice_kv_cache(cache, slice_start)\n",
    "\n",
    "    # --- Phase B: Inference ---\n",
    "    query_part_ids = tokenizer(\"\\n\" + query_text + \"\\n\",\n",
    "                               add_special_tokens=False).input_ids\n",
    "    answer_ids = tokenizer(answer_text, add_special_tokens=False,\n",
    "                           truncation=True, max_length=256).input_ids\n",
    "\n",
    "    if not answer_ids:\n",
    "        del cache\n",
    "        return 0.0\n",
    "\n",
    "    phase_b_ids = query_part_ids + answer_ids\n",
    "    phase_b_tensor = torch.tensor([phase_b_ids], dtype=torch.long, device=DEVICE)\n",
    "\n",
    "    pos_ids = torch.arange(phase_b_start, phase_b_start + len(phase_b_ids),\n",
    "                           device=DEVICE).unsqueeze(0)\n",
    "    cache_position = torch.arange(phase_b_start, phase_b_start + len(phase_b_ids),\n",
    "                                  device=DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        phase_b = model(\n",
    "            input_ids=phase_b_tensor,\n",
    "            past_key_values=cache,\n",
    "            position_ids=pos_ids,\n",
    "            cache_position=cache_position,\n",
    "            use_cache=False,\n",
    "        )\n",
    "\n",
    "    logits = phase_b.logits\n",
    "    n_query_part = len(query_part_ids)\n",
    "    n_answer = len(answer_ids)\n",
    "\n",
    "    answer_logits = logits[0, n_query_part - 1 : n_query_part - 1 + n_answer, :]\n",
    "    answer_targets = torch.tensor(answer_ids, dtype=torch.long, device=DEVICE)\n",
    "\n",
    "    log_probs = F.log_softmax(answer_logits, dim=-1)\n",
    "    token_log_probs = log_probs.gather(1, answer_targets.unsqueeze(1)).squeeze(1)\n",
    "    mean_nll = -token_log_probs.mean().item()\n",
    "\n",
    "    del cache, phase_b, logits, log_probs\n",
    "    return mean_nll\n",
    "\n",
    "\n",
    "print(\"Scoring function defined.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb54ba48",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-20T16:03:30.432552Z",
     "iopub.status.busy": "2026-02-20T16:03:30.432237Z",
     "iopub.status.idle": "2026-02-20T16:03:30.610348Z",
     "shell.execute_reply": "2026-02-20T16:03:30.609233Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query vocabulary pool: 1214 unique token IDs\n",
      "\n",
      "Loaded 400 samples\n",
      "Mean passage words: 73\n",
      "Query token count — mean: 6.5, median: 6, min: 2, max: 15\n",
      "  Sample 0: Q=5, query='average annual temperature of Uruguay...'\n",
      "    oracle         : average annual temperature of Uruguay...\n",
      "    paraphrase     : Uruguay’s...\n",
      "    same_topic     : What is Montevideo's...\n",
      "    unrelated      : gestation period for chickens...\n",
      "    scrambled      :  of Uruguay annualaverage temperature...\n",
      "    random         : Technically chickens do not...\n",
      "  Sample 1: Q=10, query='average cost for an acre of land in arizona...'\n",
      "    oracle         : average cost for an acre of land in...\n",
      "    paraphrase     : Arizona land price per acre average.Arizona...\n",
      "    same_topic     : What was Mississippi's land value?...\n",
      "    unrelated      : does seatac have special first class tsa...\n",
      "    scrambled      :  of an in land acrerizona costaverage...\n",
      "    random         : Anonymous said... When I get to the...\n",
      "  Sample 2: Q=5, query='where can i buy nematodes...'\n",
      "    oracle         : where can i buy nematodes...\n",
      "    paraphrase     : Where to purchase beneficial nematodes...\n",
      "    same_topic     : Which suppliers offer nematodes?...\n",
      "    unrelated      : could chlamydia be cured...\n",
      "    scrambled      :  nematodes buy can iwhere...\n",
      "    random         : If your partner(s...\n",
      "  Sample 3: Q=9, query='how much does it cost for varicose vein removal...'\n",
      "    oracle         : how much does it cost for varicose vein...\n",
      "    paraphrase     : Varicose vein removal price estimates please...\n",
      "    same_topic     : How many sessions are typical?How many...\n",
      "    unrelated      : how long does it take to cook a...\n",
      "    scrambled      :  varicose does much for removal cost veinhow...\n",
      "    random         : Step 5- Put The Turkey in...\n",
      "  Sample 4: Q=7, query='where is popocatepetl located...'\n",
      "    oracle         : where is popocatepetl located...\n",
      "    paraphrase     : What are popocatepetl'...\n",
      "    same_topic     : What is Popocatepetl'...\n",
      "    unrelated      : how quickly can you get pregnant after...\n",
      "    scrambled      : ocate locatedlpet iswhere pop...\n",
      "    random         : Implantation is the processImplantation...\n",
      "All prefix lengths verified.\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Build per-sample token-level prefix IDs\n",
    "\n",
    "# Collect all query token IDs (for random pool)\n",
    "all_query_token_ids = []\n",
    "for s in samples:\n",
    "    ids = tokenizer(s['query'], add_special_tokens=False).input_ids\n",
    "    all_query_token_ids.extend(ids)\n",
    "query_vocab_pool = list(set(all_query_token_ids))\n",
    "print(f\"Query vocabulary pool: {len(query_vocab_pool)} unique token IDs\")\n",
    "\n",
    "pyrandom.seed(SEED + 200)\n",
    "\n",
    "for i, s in enumerate(samples):\n",
    "    surr = surrogates[i]\n",
    "    q_ids = tokenizer(s['query'], add_special_tokens=False).input_ids\n",
    "    Q = len(q_ids)\n",
    "    s['Q'] = Q\n",
    "\n",
    "    # 1. oracle: actual query tokens\n",
    "    s['prefix_oracle'] = q_ids\n",
    "\n",
    "    # 2. paraphrase: LLM paraphrase, tokenized and truncated/padded to Q\n",
    "    para_ids = tokenizer(surr['paraphrase'], add_special_tokens=False).input_ids\n",
    "    if len(para_ids) >= Q:\n",
    "        s['prefix_paraphrase'] = para_ids[:Q]\n",
    "    else:\n",
    "        padded = para_ids * ((Q // max(len(para_ids), 1)) + 1)\n",
    "        s['prefix_paraphrase'] = padded[:Q]\n",
    "\n",
    "    # 3. same_topic: LLM same-topic question, tokenized and truncated/padded to Q\n",
    "    topic_ids = tokenizer(surr['same_topic'], add_special_tokens=False).input_ids\n",
    "    if len(topic_ids) >= Q:\n",
    "        s['prefix_same_topic'] = topic_ids[:Q]\n",
    "    else:\n",
    "        padded = topic_ids * ((Q // max(len(topic_ids), 1)) + 1)\n",
    "        s['prefix_same_topic'] = padded[:Q]\n",
    "\n",
    "    # 4. unrelated_query: different sample's query tokens, truncated/padded to Q\n",
    "    other_idx = (i + N_SAMPLES // 2) % len(samples)\n",
    "    other_q_ids = tokenizer(samples[other_idx]['query'],\n",
    "                            add_special_tokens=False).input_ids\n",
    "    if len(other_q_ids) >= Q:\n",
    "        s['prefix_unrelated'] = other_q_ids[:Q]\n",
    "    else:\n",
    "        padded = other_q_ids * ((Q // max(len(other_q_ids), 1)) + 1)\n",
    "        s['prefix_unrelated'] = padded[:Q]\n",
    "\n",
    "    # 5. scrambled_oracle: query tokens randomly permuted\n",
    "    shuffled = list(q_ids)\n",
    "    pyrandom.shuffle(shuffled)\n",
    "    s['prefix_scrambled'] = shuffled\n",
    "\n",
    "    # 6. random_matched: words from unrelated passage, tokenized and truncated/padded to Q\n",
    "    other_words = samples[other_idx]['passage'].split()\n",
    "    random_text = \" \".join(other_words[:len(s['query'].split())])\n",
    "    rand_ids = tokenizer(random_text, add_special_tokens=False).input_ids\n",
    "    if len(rand_ids) >= Q:\n",
    "        s['prefix_random'] = rand_ids[:Q]\n",
    "    else:\n",
    "        padded = rand_ids * ((Q // max(len(rand_ids), 1)) + 1)\n",
    "        s['prefix_random'] = padded[:Q]\n",
    "\n",
    "# Summary statistics\n",
    "q_lens = [s['Q'] for s in samples]\n",
    "print(f\"\\nLoaded {len(samples)} samples\")\n",
    "print(f\"Mean passage words: {np.mean([s['word_count'] for s in samples]):.0f}\")\n",
    "print(f\"Query token count — mean: {np.mean(q_lens):.1f}, \"\n",
    "      f\"median: {np.median(q_lens):.0f}, \"\n",
    "      f\"min: {np.min(q_lens)}, max: {np.max(q_lens)}\")\n",
    "\n",
    "# Verify all prefixes have exactly Q tokens\n",
    "prefix_names = ['prefix_oracle', 'prefix_paraphrase', 'prefix_same_topic',\n",
    "                'prefix_unrelated', 'prefix_scrambled', 'prefix_random']\n",
    "for i, s in enumerate(samples[:5]):\n",
    "    Q = s['Q']\n",
    "    for name in prefix_names:\n",
    "        assert len(s[name]) == Q, f\"Sample {i} {name}: len={len(s[name])} != Q={Q}\"\n",
    "    print(f\"  Sample {i}: Q={Q}, query='{s['query'][:50]}...'\")\n",
    "    for name in prefix_names:\n",
    "        label = name.replace('prefix_', '')\n",
    "        print(f\"    {label:<15}: {tokenizer.decode(s[name][:8])}...\")\n",
    "print(\"All prefix lengths verified.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "443a82e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-20T16:03:30.614802Z",
     "iopub.status.busy": "2026-02-20T16:03:30.613958Z",
     "iopub.status.idle": "2026-02-20T16:03:32.058030Z",
     "shell.execute_reply": "2026-02-20T16:03:32.057253Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "VALIDATION\n",
      "======================================================================\n",
      "\n",
      "Sample 0: Q=5 query tokens\n",
      "  Query: 'average annual temperature of Uruguay'\n",
      "  Doc position start (all prefixed): 7\n",
      "  (BOS=1 + prefix=5 + newline=1)\n",
      "\n",
      "--- NLL for each condition (sample 0) ---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  bare                 NLL = 0.7383\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  oracle               NLL = 0.8594  delta = -0.1211\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  paraphrase           NLL = 0.8047  delta = -0.0664\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  same_topic           NLL = 0.7344  delta = +0.0039\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  unrelated_query      NLL = 1.1875  delta = -0.4492\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  scrambled_oracle     NLL = 0.8438  delta = -0.1055\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  random_matched       NLL = 1.0078  delta = -0.2695\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Validate scoring\n",
    "print(\"=\" * 70)\n",
    "print(\"VALIDATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "s = samples[0]\n",
    "Q = s['Q']\n",
    "\n",
    "print(f\"\\nSample 0: Q={Q} query tokens\")\n",
    "print(f\"  Query: '{s['query']}'\")\n",
    "print(f\"  Doc position start (all prefixed): {Q + 2}\")\n",
    "print(f\"  (BOS=1 + prefix={Q} + newline={len(NEWLINE_IDS)})\")\n",
    "\n",
    "print(f\"\\n--- NLL for each condition (sample 0) ---\")\n",
    "nll_bare = score(s['passage'], s['query'], s['answer'])\n",
    "print(f\"  {'bare':<20} NLL = {nll_bare:.4f}\")\n",
    "\n",
    "for name, prefix_key in [('oracle', 'prefix_oracle'),\n",
    "                          ('paraphrase', 'prefix_paraphrase'),\n",
    "                          ('same_topic', 'prefix_same_topic'),\n",
    "                          ('unrelated_query', 'prefix_unrelated'),\n",
    "                          ('scrambled_oracle', 'prefix_scrambled'),\n",
    "                          ('random_matched', 'prefix_random')]:\n",
    "    nll = score(s['passage'], s['query'], s['answer'],\n",
    "                prefix_token_ids=s[prefix_key])\n",
    "    print(f\"  {name:<20} NLL = {nll:.4f}  delta = {nll_bare - nll:+.4f}\")\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62b63225",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-20T16:03:32.062119Z",
     "iopub.status.busy": "2026-02-20T16:03:32.061815Z",
     "iopub.status.idle": "2026-02-20T16:13:14.156886Z",
     "shell.execute_reply": "2026-02-20T16:13:14.155966Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "SCORING ALL CONDITIONS\n",
      "======================================================================\n",
      "Starting fresh: 7 conditions x 400 samples = 2800 scorings\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff0ab51fff5b44f29fca1badb4b1b474",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Scoring:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 20/400 | 0.5m | ETA 9.1m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 40/400 | 1.0m | ETA 8.7m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 60/400 | 1.4m | ETA 8.2m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 80/400 | 1.9m | ETA 7.7m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 100/400 | 2.4m | ETA 7.2m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 120/400 | 2.9m | ETA 6.8m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 140/400 | 3.4m | ETA 6.3m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 160/400 | 3.9m | ETA 5.8m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 180/400 | 4.3m | ETA 5.3m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 200/400 | 4.8m | ETA 4.8m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 220/400 | 5.3m | ETA 4.3m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 240/400 | 5.8m | ETA 3.9m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 260/400 | 6.3m | ETA 3.4m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 280/400 | 6.8m | ETA 2.9m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 300/400 | 7.3m | ETA 2.4m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 320/400 | 7.8m | ETA 1.9m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 340/400 | 8.3m | ETA 1.5m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 360/400 | 8.7m | ETA 1.0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 380/400 | 9.2m | ETA 0.5m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 400/400 | 9.7m | ETA 0.0m\n",
      "\n",
      "Scoring complete: 400 samples, 7 conditions in 9.7 min\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Scoring loop — 7 conditions x 400 samples\n",
    "print(\"=\" * 70)\n",
    "print(\"SCORING ALL CONDITIONS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "COND_NAMES = [\n",
    "    'bare', 'oracle', 'paraphrase', 'same_topic',\n",
    "    'unrelated_query', 'scrambled_oracle', 'random_matched',\n",
    "]\n",
    "\n",
    "# Semantic relevance ordering (for gradient analysis)\n",
    "RELEVANCE_ORDER = [\n",
    "    ('random_matched', 'Random matched', 0, 'none (structural baseline)'),\n",
    "    ('scrambled_oracle', 'Scrambled oracle', 1, 'vocabulary only'),\n",
    "    ('unrelated_query', 'Unrelated query', 2, 'low (wrong topic)'),\n",
    "    ('same_topic', 'Same topic', 3, 'medium (right topic)'),\n",
    "    ('paraphrase', 'Paraphrase', 4, 'high (same meaning)'),\n",
    "    ('oracle', 'Oracle', 5, 'maximal (exact query)'),\n",
    "]\n",
    "\n",
    "PREFIX_MAP = {\n",
    "    'oracle': 'prefix_oracle',\n",
    "    'paraphrase': 'prefix_paraphrase',\n",
    "    'same_topic': 'prefix_same_topic',\n",
    "    'unrelated_query': 'prefix_unrelated',\n",
    "    'scrambled_oracle': 'prefix_scrambled',\n",
    "    'random_matched': 'prefix_random',\n",
    "}\n",
    "\n",
    "results = []\n",
    "start_idx = 0\n",
    "\n",
    "if CHECKPOINT_PATH.exists():\n",
    "    ckpt = json.loads(CHECKPOINT_PATH.read_text())\n",
    "    if ckpt.get('n_total') == N_SAMPLES and len(ckpt.get('results', [])) > 0:\n",
    "        saved_queries = [r['query'][:50] for r in ckpt['results']]\n",
    "        current_queries = [s['query'][:50] for s in samples[:len(saved_queries)]]\n",
    "        if saved_queries == current_queries:\n",
    "            results = ckpt['results']\n",
    "            start_idx = len(results)\n",
    "            print(f\"Resuming from checkpoint: {start_idx}/{N_SAMPLES}\")\n",
    "\n",
    "if start_idx == 0:\n",
    "    print(f\"Starting fresh: {len(COND_NAMES)} conditions x {N_SAMPLES} samples \"\n",
    "          f\"= {len(COND_NAMES) * N_SAMPLES} scorings\")\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "for i in tqdm(range(start_idx, N_SAMPLES), initial=start_idx, total=N_SAMPLES,\n",
    "              desc=\"Scoring\"):\n",
    "    s = samples[i]\n",
    "    result = {\n",
    "        'query': s['query'],\n",
    "        'answer': s['answer'],\n",
    "        'passage_words': s['word_count'],\n",
    "        'Q': s['Q'],\n",
    "    }\n",
    "\n",
    "    # bare\n",
    "    result['nll_bare'] = score(s['passage'], s['query'], s['answer'])\n",
    "\n",
    "    # All prefixed conditions\n",
    "    for cond_name, prefix_key in PREFIX_MAP.items():\n",
    "        result[f'nll_{cond_name}'] = score(\n",
    "            s['passage'], s['query'], s['answer'],\n",
    "            prefix_token_ids=s[prefix_key]\n",
    "        )\n",
    "\n",
    "    results.append(result)\n",
    "\n",
    "    if (i + 1) % 20 == 0 or i == N_SAMPLES - 1:\n",
    "        ckpt = {\n",
    "            'n_total': N_SAMPLES,\n",
    "            'results': results,\n",
    "            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        }\n",
    "        CHECKPOINT_PATH.write_text(json.dumps(ckpt))\n",
    "        elapsed = time.time() - t0\n",
    "        done = i - start_idx + 1\n",
    "        eta = (N_SAMPLES - i - 1) * elapsed / done if done > 0 else 0\n",
    "        tqdm.write(f\"  Checkpoint {i+1}/{N_SAMPLES} | \"\n",
    "                   f\"{elapsed/60:.1f}m | ETA {eta/60:.1f}m\")\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "elapsed = time.time() - t0\n",
    "print(f\"\\nScoring complete: {len(results)} samples, \"\n",
    "      f\"{len(COND_NAMES)} conditions in {elapsed/60:.1f} min\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03d1f1c1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-20T16:13:14.161198Z",
     "iopub.status.busy": "2026-02-20T16:13:14.160379Z",
     "iopub.status.idle": "2026-02-20T16:13:14.180504Z",
     "shell.execute_reply": "2026-02-20T16:13:14.179799Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PART 1: STANDARD CONDITION TABLE\n",
      "======================================================================\n",
      "\n",
      "Condition                                   NLL    Delta        d    Win%   %Orc            p   sig\n",
      "----------------------------------------------------------------------------------------------------\n",
      "  Oracle (real query)                    0.9044  +0.6977   +0.428   73.8%   100%     2.32e-16 ***\n",
      "  Paraphrase (same meaning)              0.7443  +0.8578   +0.558   77.8%   130%     2.24e-25 ***\n",
      "  Same topic (diff question)             0.7848  +0.8174   +0.513   75.2%   120%     4.40e-22 ***\n",
      "  Unrelated query (wrong topic)          0.7360  +0.8661   +0.536   77.8%   125%     1.00e-23 ***\n",
      "  Scrambled oracle (vocab only)          0.8132  +0.7890   +0.518   79.0%   121%     2.01e-22 ***\n",
      "  Random matched (structural)            0.7161  +0.8861   +0.520   78.0%   121%     1.53e-22 ***\n",
      "\n",
      "  bare (lower bound): 1.6022\n",
      "  Bonferroni threshold: alpha=0.0083\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Part 1 — Standard Condition Table\n",
    "print(\"=\" * 70)\n",
    "print(\"PART 1: STANDARD CONDITION TABLE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "bare_nlls = np.array([r['nll_bare'] for r in results])\n",
    "oracle_nlls = np.array([r['nll_oracle'] for r in results])\n",
    "oracle_benefit = bare_nlls - oracle_nlls\n",
    "oracle_d = cohens_d(oracle_benefit)\n",
    "\n",
    "all_conds = [\n",
    "    ('oracle', 'Oracle (real query)'),\n",
    "    ('paraphrase', 'Paraphrase (same meaning)'),\n",
    "    ('same_topic', 'Same topic (diff question)'),\n",
    "    ('unrelated_query', 'Unrelated query (wrong topic)'),\n",
    "    ('scrambled_oracle', 'Scrambled oracle (vocab only)'),\n",
    "    ('random_matched', 'Random matched (structural)'),\n",
    "]\n",
    "\n",
    "alpha_bonf = 0.05 / len(all_conds)\n",
    "\n",
    "print(f\"\\n{'Condition':<38} {'NLL':>8} {'Delta':>8} {'d':>8} \"\n",
    "      f\"{'Win%':>7} {'%Orc':>6} {'p':>12} {'sig':>5}\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "for cond, desc in all_conds:\n",
    "    nlls = np.array([r[f'nll_{cond}'] for r in results])\n",
    "    benefit = bare_nlls - nlls\n",
    "    d = cohens_d(benefit)\n",
    "    delta = benefit.mean()\n",
    "    win = 100 * np.mean(benefit > 0)\n",
    "    pct = d / oracle_d * 100 if oracle_d > 0 else 0\n",
    "    _, p = stats.ttest_1samp(benefit, 0)\n",
    "    sig = '***' if p < alpha_bonf / 10 else '**' if p < alpha_bonf else '*' if p < 0.05 else 'ns'\n",
    "    print(f\"  {desc:<36} {nlls.mean():>8.4f} {delta:>+8.4f} {d:>+8.3f} \"\n",
    "          f\"{win:>6.1f}% {pct:>5.0f}% {p:>12.2e} {sig}\")\n",
    "\n",
    "print(f\"\\n  bare (lower bound): {bare_nlls.mean():.4f}\")\n",
    "print(f\"  Bonferroni threshold: alpha={alpha_bonf:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2097bf81",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-20T16:13:14.183698Z",
     "iopub.status.busy": "2026-02-20T16:13:14.183440Z",
     "iopub.status.idle": "2026-02-20T16:13:14.204027Z",
     "shell.execute_reply": "2026-02-20T16:13:14.203296Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PART 2: SEMANTIC GRADIENT\n",
      "======================================================================\n",
      "Does NLL improvement increase monotonically with semantic relevance?\n",
      "\n",
      "--- Raw delta (benefit over bare) ---\n",
      "  Condition                       Relevance   Mean delta        d  %Oracle\n",
      "  ---------------------------------------------------------------------------\n",
      "  Random matched                          0      +0.8861   +0.520     121%\n",
      "  Scrambled oracle                        1      +0.7890   +0.518     121%\n",
      "  Unrelated query                         2      +0.8661   +0.536     125%\n",
      "  Same topic                              3      +0.8174   +0.513     120%\n",
      "  Paraphrase                              4      +0.8578   +0.558     130%\n",
      "  Oracle                                  5      +0.6977   +0.428     100%\n",
      "\n",
      "--- Semantic delta (above random_matched baseline) ---\n",
      "  Condition                        Semantic d    p vs random   sig\n",
      "  -----------------------------------------------------------------\n",
      "  Random matched                       +0.000            nan ns\n",
      "  Scrambled oracle                     -0.129       1.01e-02 *\n",
      "  Unrelated query                      -0.031       5.30e-01 ns\n",
      "  Same topic                           -0.083       9.65e-02 ns\n",
      "  Paraphrase                           -0.036       4.73e-01 ns\n",
      "  Oracle                               -0.200       7.62e-05 ***\n",
      "\n",
      "--- Monotonicity test ---\n",
      "  Spearman rho (relevance rank vs Cohen's d): rho=-0.257, p=0.6228 ns\n",
      "  Spearman rho (relevance rank vs semantic d): rho=-0.600, p=0.2080 ns\n",
      "  --> FLAT: no clear gradient (rho=-0.257)\n"
     ]
    }
   ],
   "source": [
    "# Cell 10: Part 2 — Semantic Gradient\n",
    "print(\"=\" * 70)\n",
    "print(\"PART 2: SEMANTIC GRADIENT\")\n",
    "print(\"=\" * 70)\n",
    "print(\"Does NLL improvement increase monotonically with semantic relevance?\\n\")\n",
    "\n",
    "random_nlls = np.array([r['nll_random_matched'] for r in results])\n",
    "random_benefit = bare_nlls - random_nlls\n",
    "\n",
    "print(\"--- Raw delta (benefit over bare) ---\")\n",
    "print(f\"  {'Condition':<30} {'Relevance':>10} {'Mean delta':>12} {'d':>8} {'%Oracle':>8}\")\n",
    "print(f\"  {'-'*75}\")\n",
    "\n",
    "gradient_ds = []\n",
    "gradient_labels = []\n",
    "\n",
    "for cond, desc, rank, rel_desc in RELEVANCE_ORDER:\n",
    "    nlls = np.array([r[f'nll_{cond}'] for r in results])\n",
    "    benefit = bare_nlls - nlls\n",
    "    d = cohens_d(benefit)\n",
    "    pct = d / oracle_d * 100 if oracle_d > 0 else 0\n",
    "    print(f\"  {desc:<30} {rank:>10} {benefit.mean():>+12.4f} {d:>+8.3f} {pct:>7.0f}%\")\n",
    "    gradient_ds.append(d)\n",
    "    gradient_labels.append(desc)\n",
    "\n",
    "# Semantic delta (above structural baseline)\n",
    "print(f\"\\n--- Semantic delta (above random_matched baseline) ---\")\n",
    "print(f\"  {'Condition':<30} {'Semantic d':>12} {'p vs random':>14} {'sig':>5}\")\n",
    "print(f\"  {'-'*65}\")\n",
    "\n",
    "semantic_ds = []\n",
    "\n",
    "for cond, desc, rank, rel_desc in RELEVANCE_ORDER:\n",
    "    nlls = np.array([r[f'nll_{cond}'] for r in results])\n",
    "    diff = random_nlls - nlls  # positive = condition is better than random\n",
    "    d = cohens_d(diff)\n",
    "    _, p = stats.ttest_1samp(diff, 0)\n",
    "    sig = '***' if p < 0.001 else '**' if p < 0.01 else '*' if p < 0.05 else 'ns'\n",
    "    print(f\"  {desc:<30} {d:>+12.3f} {p:>14.2e} {sig}\")\n",
    "    semantic_ds.append(d)\n",
    "\n",
    "# Monotonicity test (Spearman)\n",
    "ranks = [rank for _, _, rank, _ in RELEVANCE_ORDER]\n",
    "rho, p_mono = stats.spearmanr(ranks, gradient_ds)\n",
    "sig_mono = '***' if p_mono < 0.001 else '**' if p_mono < 0.01 else '*' if p_mono < 0.05 else 'ns'\n",
    "print(f\"\\n--- Monotonicity test ---\")\n",
    "print(f\"  Spearman rho (relevance rank vs Cohen's d): rho={rho:+.3f}, \"\n",
    "      f\"p={p_mono:.4f} {sig_mono}\")\n",
    "\n",
    "rho_sem, p_sem = stats.spearmanr(ranks, semantic_ds)\n",
    "sig_sem = '***' if p_sem < 0.001 else '**' if p_sem < 0.01 else '*' if p_sem < 0.05 else 'ns'\n",
    "print(f\"  Spearman rho (relevance rank vs semantic d): rho={rho_sem:+.3f}, \"\n",
    "      f\"p={p_sem:.4f} {sig_sem}\")\n",
    "\n",
    "if rho > 0.8 and p_mono < 0.05:\n",
    "    print(f\"  --> MONOTONIC: clear semantic gradient (rho={rho:+.3f})\")\n",
    "elif rho > 0.5:\n",
    "    print(f\"  --> PARTIAL: imperfect gradient (rho={rho:+.3f})\")\n",
    "else:\n",
    "    print(f\"  --> FLAT: no clear gradient (rho={rho:+.3f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b4d4f10",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-20T16:13:14.207566Z",
     "iopub.status.busy": "2026-02-20T16:13:14.207202Z",
     "iopub.status.idle": "2026-02-20T16:13:14.224534Z",
     "shell.execute_reply": "2026-02-20T16:13:14.223641Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PART 3: FINE-GRAINED DECOMPOSITION CHAIN\n",
      "======================================================================\n",
      "bare -> random_matched -> scrambled_oracle -> unrelated_query -> same_topic -> paraphrase -> oracle\n",
      "\n",
      "  Component                   Delta   %total        d            p   sig\n",
      "  ----------------------------------------------------------------------\n",
      "  Structure                 +0.8861   127.0%   +0.520     1.53e-22 ***\n",
      "  Vocabulary                -0.0971   -13.9%   -0.129     1.01e-02 *\n",
      "  Query syntax              +0.0772    11.1%   +0.096     5.55e-02 ns\n",
      "  Topic relevance           -0.0488    -7.0%   -0.058     2.47e-01 ns\n",
      "  Semantic precision        +0.0405     5.8%   +0.052     3.03e-01 ns\n",
      "  Exact match               -0.1601   -22.9%   -0.218     1.70e-05 ***\n",
      "  TOTAL                     +0.6977   100.0%\n",
      "\n",
      "  Decomposition residual: 0.000000 (should be ~0)\n",
      "\n",
      "--- Grouped Summary ---\n",
      "  Structure:                 127.0%\n",
      "  All semantic components:   -27.0%\n",
      "    Vocabulary:            -13.9%\n",
      "    Query syntax:           11.1%\n",
      "    Topic relevance:        -7.0%\n",
      "    Semantic precision:      5.8%\n",
      "    Exact match:           -22.9%\n",
      "\n",
      "--- v3 Exp 12 comparison (T5Gemma encoder-decoder) ---\n",
      "  v3: Structure=86.5%, Vocab=4.3%, Syntax=-1.6%, Topic=-15.8%, Precision=7.4%, Exact=19.2%\n",
      "  v4: Structure=127.0%, Vocab=-13.9%, Syntax=11.1%, Topic=-7.0%, Precision=5.8%, Exact=-22.9%\n"
     ]
    }
   ],
   "source": [
    "# Cell 11: Part 3 — Decomposition Chain\n",
    "print(\"=\" * 70)\n",
    "print(\"PART 3: FINE-GRAINED DECOMPOSITION CHAIN\")\n",
    "print(\"=\" * 70)\n",
    "print(\"bare -> random_matched -> scrambled_oracle -> unrelated_query -> \"\n",
    "      \"same_topic -> paraphrase -> oracle\\n\")\n",
    "\n",
    "scrambled_nlls = np.array([r['nll_scrambled_oracle'] for r in results])\n",
    "unrelated_nlls = np.array([r['nll_unrelated_query'] for r in results])\n",
    "same_topic_nlls = np.array([r['nll_same_topic'] for r in results])\n",
    "paraphrase_nlls = np.array([r['nll_paraphrase'] for r in results])\n",
    "\n",
    "chain = [\n",
    "    ('Structure', bare_nlls - random_nlls),\n",
    "    ('Vocabulary', random_nlls - scrambled_nlls),\n",
    "    ('Query syntax', scrambled_nlls - unrelated_nlls),\n",
    "    ('Topic relevance', unrelated_nlls - same_topic_nlls),\n",
    "    ('Semantic precision', same_topic_nlls - paraphrase_nlls),\n",
    "    ('Exact match', paraphrase_nlls - oracle_nlls),\n",
    "]\n",
    "\n",
    "total = bare_nlls - oracle_nlls\n",
    "total_mean = total.mean()\n",
    "\n",
    "print(f\"  {'Component':<22} {'Delta':>10} {'%total':>8} {'d':>8} {'p':>12} {'sig':>5}\")\n",
    "print(f\"  {'-'*70}\")\n",
    "\n",
    "chain_pcts = {}\n",
    "for label, comp in chain:\n",
    "    mu = comp.mean()\n",
    "    pct = mu / total_mean * 100 if total_mean != 0 else 0\n",
    "    d = cohens_d(comp)\n",
    "    _, p = stats.ttest_1samp(comp, 0)\n",
    "    sig = '***' if p < 0.001 else '**' if p < 0.01 else '*' if p < 0.05 else 'ns'\n",
    "    print(f\"  {label:<22} {mu:>+10.4f} {pct:>7.1f}% {d:>+8.3f} {p:>12.2e} {sig}\")\n",
    "    chain_pcts[label] = pct\n",
    "\n",
    "print(f\"  {'TOTAL':<22} {total_mean:>+10.4f} {'100.0%':>8}\")\n",
    "residual = total_mean - sum(comp.mean() for _, comp in chain)\n",
    "print(f\"\\n  Decomposition residual: {residual:.6f} (should be ~0)\")\n",
    "\n",
    "struct_pct = chain_pcts['Structure']\n",
    "print(f\"\\n--- Grouped Summary ---\")\n",
    "print(f\"  Structure:                {struct_pct:>6.1f}%\")\n",
    "print(f\"  All semantic components:  {100 - struct_pct:>6.1f}%\")\n",
    "for label in ['Vocabulary', 'Query syntax', 'Topic relevance',\n",
    "              'Semantic precision', 'Exact match']:\n",
    "    print(f\"    {label}:{'':>{20-len(label)}} {chain_pcts[label]:>6.1f}%\")\n",
    "\n",
    "# Comparison with v3 Exp 12\n",
    "print(f\"\\n--- v3 Exp 12 comparison (T5Gemma encoder-decoder) ---\")\n",
    "print(f\"  v3: Structure=86.5%, Vocab=4.3%, Syntax=-1.6%, \"\n",
    "      f\"Topic=-15.8%, Precision=7.4%, Exact=19.2%\")\n",
    "print(f\"  v4: Structure={struct_pct:.1f}%, Vocab={chain_pcts['Vocabulary']:.1f}%, \"\n",
    "      f\"Syntax={chain_pcts['Query syntax']:.1f}%, \"\n",
    "      f\"Topic={chain_pcts['Topic relevance']:.1f}%, \"\n",
    "      f\"Precision={chain_pcts['Semantic precision']:.1f}%, \"\n",
    "      f\"Exact={chain_pcts['Exact match']:.1f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a1bbfac2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-20T16:13:14.228471Z",
     "iopub.status.busy": "2026-02-20T16:13:14.228061Z",
     "iopub.status.idle": "2026-02-20T16:13:14.255919Z",
     "shell.execute_reply": "2026-02-20T16:13:14.255033Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PART 4: HARDNESS INTERACTION\n",
      "======================================================================\n",
      "--- Semantic delta by quintile (above random baseline) ---\n",
      "  Quintile       Bare NLL  Scrambled oracle  Unrelated query    Same topic    Paraphrase        Oracle\n",
      "  ------------------------------------------------------------------------------------\n",
      "  Q1 easy           0.261        -0.139        -0.091        -0.129        -0.159        -0.261\n",
      "  Q2                0.586        +0.113        +0.159        -0.023        +0.139        +0.003\n",
      "  Q3                0.940        +0.157        +0.089        +0.013        +0.210        -0.099\n",
      "  Q4                1.690        -0.099        -0.138        -0.253        +0.030        -0.317\n",
      "  Q5 hard           4.520        -0.353        -0.099        -0.108        -0.135        -0.309\n",
      "\n",
      "--- Structure vs semantic % by quintile ---\n",
      "  Quintile       Struct%   Vocab%   Syntax%   Topic%   Precis%   Exact%\n",
      "  -----------------------------------------------------------------\n",
      "  Q1 easy           0.0%     0.0%      0.0%     0.0%      0.0%     0.0%\n",
      "  Q2               98.9%    55.2%      4.6%   -65.8%     59.4%   -52.3%\n",
      "  Q3              106.2%     8.7%     -3.0%    -4.9%     11.7%   -18.8%\n",
      "  Q4              136.0%    -5.7%     -0.9%    -7.6%     15.5%   -37.4%\n",
      "  Q5 hard         122.7%   -20.5%     15.2%    -2.6%     -1.3%   -13.6%\n",
      "\n",
      "--- Correlations: hardness vs semantic delta ---\n",
      "  Scrambled oracle          r=-0.376 (p=7.55e-15) ***\n",
      "  Unrelated query           r=-0.107 (p=3.26e-02) *\n",
      "  Same topic                r=-0.265 (p=7.89e-08) ***\n",
      "  Paraphrase                r=-0.285 (p=6.13e-09) ***\n",
      "  Oracle                    r=-0.308 (p=3.01e-10) ***\n"
     ]
    }
   ],
   "source": [
    "# Cell 12: Part 4 — Hardness Interaction\n",
    "print(\"=\" * 70)\n",
    "print(\"PART 4: HARDNESS INTERACTION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "quintile_bounds = np.percentile(bare_nlls, [20, 40, 60, 80])\n",
    "quintiles = np.digitize(bare_nlls, quintile_bounds)\n",
    "q_labels = ['Q1 easy', 'Q2', 'Q3', 'Q4', 'Q5 hard']\n",
    "\n",
    "print(\"--- Semantic delta by quintile (above random baseline) ---\")\n",
    "print(f\"  {'Quintile':<12} {'Bare NLL':>10}\", end=\"\")\n",
    "for _, desc, _, _ in RELEVANCE_ORDER[1:]:\n",
    "    print(f\"  {desc:>12}\", end=\"\")\n",
    "print()\n",
    "print(f\"  {'-'*(14 + 14 * (len(RELEVANCE_ORDER) - 1))}\")\n",
    "\n",
    "for q in range(5):\n",
    "    mask = quintiles == q\n",
    "    row = f\"  {q_labels[q]:<12} {bare_nlls[mask].mean():>10.3f}\"\n",
    "    for cond, desc, _, _ in RELEVANCE_ORDER[1:]:\n",
    "        nlls_c = np.array([r[f'nll_{cond}'] for r in results])[mask]\n",
    "        rand_c = random_nlls[mask]\n",
    "        sem_delta = cohens_d(rand_c - nlls_c)\n",
    "        row += f\"  {sem_delta:>+12.3f}\"\n",
    "    print(row)\n",
    "\n",
    "print(f\"\\n--- Structure vs semantic % by quintile ---\")\n",
    "print(f\"  {'Quintile':<12} {'Struct%':>9} {'Vocab%':>8} {'Syntax%':>9} \"\n",
    "      f\"{'Topic%':>8} {'Precis%':>9} {'Exact%':>8}\")\n",
    "print(f\"  {'-'*65}\")\n",
    "\n",
    "for q in range(5):\n",
    "    mask = quintiles == q\n",
    "    total_q = (bare_nlls[mask] - oracle_nlls[mask]).mean()\n",
    "    if total_q > 0:\n",
    "        s_pct = (bare_nlls[mask] - random_nlls[mask]).mean() / total_q * 100\n",
    "        v_pct = (random_nlls[mask] - scrambled_nlls[mask]).mean() / total_q * 100\n",
    "        syn_pct = (scrambled_nlls[mask] - unrelated_nlls[mask]).mean() / total_q * 100\n",
    "        top_pct = (unrelated_nlls[mask] - same_topic_nlls[mask]).mean() / total_q * 100\n",
    "        pre_pct = (same_topic_nlls[mask] - paraphrase_nlls[mask]).mean() / total_q * 100\n",
    "        ex_pct = (paraphrase_nlls[mask] - oracle_nlls[mask]).mean() / total_q * 100\n",
    "    else:\n",
    "        s_pct = v_pct = syn_pct = top_pct = pre_pct = ex_pct = 0\n",
    "    print(f\"  {q_labels[q]:<12} {s_pct:>8.1f}% {v_pct:>7.1f}% {syn_pct:>8.1f}% \"\n",
    "          f\"{top_pct:>7.1f}% {pre_pct:>8.1f}% {ex_pct:>7.1f}%\")\n",
    "\n",
    "# Correlation: hardness vs semantic delta\n",
    "print(f\"\\n--- Correlations: hardness vs semantic delta ---\")\n",
    "for cond, desc, rank, _ in RELEVANCE_ORDER[1:]:\n",
    "    nlls_c = np.array([r[f'nll_{cond}'] for r in results])\n",
    "    sem_delta = random_nlls - nlls_c\n",
    "    r_val, p_val = stats.pearsonr(bare_nlls, sem_delta)\n",
    "    sig = '***' if p_val < 0.001 else '**' if p_val < 0.01 else '*' if p_val < 0.05 else 'ns'\n",
    "    print(f\"  {desc:<25} r={r_val:+.3f} (p={p_val:.2e}) {sig}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c12652b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-20T16:13:14.259270Z",
     "iopub.status.busy": "2026-02-20T16:13:14.258945Z",
     "iopub.status.idle": "2026-02-20T16:13:14.836320Z",
     "shell.execute_reply": "2026-02-20T16:13:14.835399Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "SYNTHESIS: GRADED SEMANTIC RELEVANCE (DECODER-ONLY)\n",
      "======================================================================\n",
      "\n",
      "1. SEMANTIC GRADIENT (d, % oracle):\n",
      "   [0] Random matched            d=+0.520 (121.3% oracle) — none (structural baseline)\n",
      "   [1] Scrambled oracle          d=+0.518 (120.9% oracle) — vocabulary only\n",
      "   [2] Unrelated query           d=+0.536 (125.1% oracle) — low (wrong topic)\n",
      "   [3] Same topic                d=+0.513 (119.8% oracle) — medium (right topic)\n",
      "   [4] Paraphrase                d=+0.558 (130.3% oracle) — high (same meaning)\n",
      "   [5] Oracle                    d=+0.428 (100.0% oracle) — maximal (exact query)\n",
      "\n",
      "2. MONOTONICITY:\n",
      "   Spearman rho (raw d): -0.257 (p=0.6228)\n",
      "   Spearman rho (semantic d): -0.600 (p=0.2080)\n",
      "\n",
      "3. DECOMPOSITION CHAIN:\n",
      "   Structure               127.0%\n",
      "   Vocabulary              -13.9%\n",
      "   Query syntax             11.1%\n",
      "   Topic relevance          -7.0%\n",
      "   Semantic precision        5.8%\n",
      "   Exact match             -22.9%\n",
      "\n",
      "======================================================================\n",
      "CONCLUSIONS:\n",
      "  1. NO CLEAR GRADIENT (rho=-0.257, p=0.6228)\n",
      "  2. Structure dominates (127%)\n",
      "  3. Largest semantic component: Exact match (-22.9%)\n",
      "\n",
      "--- Cross-architecture comparison ---\n",
      "  v3 (T5Gemma encoder-decoder): monotonic rho=+0.943, structure=86.5%\n",
      "  v4 (Gemma 3 decoder-only):    rho=-0.257, structure=127.0%\n",
      "======================================================================\n",
      "\n",
      "Results saved to ../../../results/decoder_only/exp06/results.json\n",
      "\n",
      "Cleaning up GPU memory...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory: 8.61 GB -> 0.01 GB\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Cell 13: Synthesis + Save\n",
    "print(\"=\" * 70)\n",
    "print(\"SYNTHESIS: GRADED SEMANTIC RELEVANCE (DECODER-ONLY)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\n1. SEMANTIC GRADIENT (d, % oracle):\")\n",
    "for cond, desc, rank, rel_desc in RELEVANCE_ORDER:\n",
    "    nlls = np.array([r[f'nll_{cond}'] for r in results])\n",
    "    d = cohens_d(bare_nlls - nlls)\n",
    "    pct = d / oracle_d * 100 if oracle_d > 0 else 0\n",
    "    print(f\"   [{rank}] {desc:<25} d={d:>+.3f} ({pct:>5.1f}% oracle) — {rel_desc}\")\n",
    "\n",
    "print(f\"\\n2. MONOTONICITY:\")\n",
    "print(f\"   Spearman rho (raw d): {rho:+.3f} (p={p_mono:.4f})\")\n",
    "print(f\"   Spearman rho (semantic d): {rho_sem:+.3f} (p={p_sem:.4f})\")\n",
    "\n",
    "print(f\"\\n3. DECOMPOSITION CHAIN:\")\n",
    "for label, pct in chain_pcts.items():\n",
    "    print(f\"   {label:<22} {pct:>6.1f}%\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"CONCLUSIONS:\")\n",
    "\n",
    "if rho > 0.8 and p_mono < 0.05:\n",
    "    print(f\"  1. CLEAR SEMANTIC GRADIENT: monotonic (rho={rho:+.3f}, p={p_mono:.4f})\")\n",
    "elif rho > 0.5:\n",
    "    print(f\"  1. PARTIAL GRADIENT: imperfect (rho={rho:+.3f}, p={p_mono:.4f})\")\n",
    "else:\n",
    "    print(f\"  1. NO CLEAR GRADIENT (rho={rho:+.3f}, p={p_mono:.4f})\")\n",
    "\n",
    "if struct_pct > 75:\n",
    "    print(f\"  2. Structure dominates ({struct_pct:.0f}%)\")\n",
    "elif struct_pct > 50:\n",
    "    print(f\"  2. Structure largest ({struct_pct:.0f}%) but semantics substantial\")\n",
    "else:\n",
    "    print(f\"  2. Semantics dominate ({100-struct_pct:.0f}%)\")\n",
    "\n",
    "sem_components = [(k, v) for k, v in chain_pcts.items() if k != 'Structure']\n",
    "largest_sem = max(sem_components, key=lambda x: abs(x[1]))\n",
    "print(f\"  3. Largest semantic component: {largest_sem[0]} ({largest_sem[1]:+.1f}%)\")\n",
    "\n",
    "# Cross-architecture comparison\n",
    "print(f\"\\n--- Cross-architecture comparison ---\")\n",
    "print(f\"  v3 (T5Gemma encoder-decoder): monotonic rho=+0.943, structure=86.5%\")\n",
    "print(f\"  v4 (Gemma 3 decoder-only):    rho={rho:+.3f}, structure={struct_pct:.1f}%\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "# Save results\n",
    "final_results = {\n",
    "    'experiment': 'v4_decoder_only_exp06_semantic_gradient',\n",
    "    'scoring_model': MODEL_NAME,\n",
    "    'generation_model': MODEL_NAME,\n",
    "    'dataset': 'ms_marco_v1.1',\n",
    "    'n_samples': len(results),\n",
    "    'seed': SEED,\n",
    "    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'conditions': {},\n",
    "    'gradient': {\n",
    "        'spearman_rho_raw': float(rho),\n",
    "        'spearman_p_raw': float(p_mono),\n",
    "        'spearman_rho_semantic': float(rho_sem),\n",
    "        'spearman_p_semantic': float(p_sem),\n",
    "    },\n",
    "    'decomposition_chain': {k: float(v) for k, v in chain_pcts.items()},\n",
    "    'structure_pct': float(struct_pct),\n",
    "    'query_token_stats': {\n",
    "        'mean': float(np.mean([r['Q'] for r in results])),\n",
    "        'median': float(np.median([r['Q'] for r in results])),\n",
    "        'min': int(np.min([r['Q'] for r in results])),\n",
    "        'max': int(np.max([r['Q'] for r in results])),\n",
    "    },\n",
    "}\n",
    "\n",
    "for cond, desc, rank, rel_desc in RELEVANCE_ORDER:\n",
    "    nlls = np.array([r[f'nll_{cond}'] for r in results])\n",
    "    benefit = bare_nlls - nlls\n",
    "    d = cohens_d(benefit)\n",
    "    _, p = stats.ttest_1samp(benefit, 0)\n",
    "    sem_diff = random_nlls - nlls\n",
    "    sem_d = cohens_d(sem_diff)\n",
    "    _, sem_p = stats.ttest_1samp(sem_diff, 0)\n",
    "    final_results['conditions'][cond] = {\n",
    "        'description': desc,\n",
    "        'relevance_rank': rank,\n",
    "        'relevance_label': rel_desc,\n",
    "        'd': float(d),\n",
    "        'mean_nll': float(nlls.mean()),\n",
    "        'mean_delta': float(benefit.mean()),\n",
    "        'pct_oracle': float(d / oracle_d * 100) if oracle_d > 0 else 0,\n",
    "        'p': float(p),\n",
    "        'semantic_d': float(sem_d),\n",
    "        'semantic_p': float(sem_p),\n",
    "    }\n",
    "\n",
    "with open(RESULTS_DIR / 'results.json', 'w') as f:\n",
    "    json.dump(final_results, f, indent=2)\n",
    "print(f\"\\nResults saved to {RESULTS_DIR / 'results.json'}\")\n",
    "\n",
    "# Cleanup\n",
    "print(f\"\\nCleaning up GPU memory...\")\n",
    "mem_before = torch.cuda.memory_allocated() / 1e9\n",
    "del model, tokenizer\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "mem_after = torch.cuda.memory_allocated() / 1e9\n",
    "print(f\"GPU memory: {mem_before:.2f} GB -> {mem_after:.2f} GB\")\n",
    "print(\"Done!\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "033b7faa349145e6a2939cd724e10abc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_f5691cfede7d409280e0dceec6d1995c",
        "IPY_MODEL_8154d3d63f7540af86651bd9fc29bed8",
        "IPY_MODEL_97551981574e4c44a199bec83c6aba22"
       ],
       "layout": "IPY_MODEL_063cc965849045a0978b48e463489b50",
       "tabbable": null,
       "tooltip": null
      }
     },
     "036e82cbfcfd420db1239994ada6ff8c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_e2372d4afb6e483a83867fe3892b1efc",
       "max": 400.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_6cbe04dfcf9d4d3d8f3c7e8a90f8a6bc",
       "tabbable": null,
       "tooltip": null,
       "value": 400.0
      }
     },
     "05ba660272d446e8a1e2335f9acd486b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "063cc965849045a0978b48e463489b50": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "180711794fc948a2abda942fc1521e9d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "19b49eeb18df46e899ef9f329aa69323": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "30f1ba5f824a40e6bb296e103e476d63": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "4543c419c2f74014a33f093a67fe9413": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "4651805a3fac4ea29a32b7ac1622a082": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_19b49eeb18df46e899ef9f329aa69323",
       "placeholder": "​",
       "style": "IPY_MODEL_8ffdbf394ca242d986e7b00efeca8506",
       "tabbable": null,
       "tooltip": null,
       "value": " 400/400 [09:42&lt;00:00,  1.45s/it]"
      }
     },
     "479e5a410e9844c29ab7bef81445d67b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_180711794fc948a2abda942fc1521e9d",
       "max": 883.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_93d2164ae6744c41896f6e530a063f32",
       "tabbable": null,
       "tooltip": null,
       "value": 883.0
      }
     },
     "4a61bf7c95d14fe880f3aaee45dcecb8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "4a643cd11d4d4120a2ab2a435c499ffe": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "4d5a9b9b7bf94d4caec720903bff5b64": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "4f9e627f39d44a13a190d436d0c21d5c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "55160b2b883b4ef3a2fe6e83922102ba": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_4d5a9b9b7bf94d4caec720903bff5b64",
       "max": 400.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_8f7b0f2dd889492e98dbfdc08e36ce0e",
       "tabbable": null,
       "tooltip": null,
       "value": 400.0
      }
     },
     "593f83ff09ed435298b331e7080c3c93": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6cbe04dfcf9d4d3d8f3c7e8a90f8a6bc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "7434cc496f754e3999d3914793e41910": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "775ce6579d1746ec8eb276b69b2ef6e8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "77ca6e11a28740ad985365654d70538a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "7f645bcdc37349e1bea0f8f6f7257f31": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "8154d3d63f7540af86651bd9fc29bed8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_d58e37508141410594e0f9c652540711",
       "max": 883.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_05ba660272d446e8a1e2335f9acd486b",
       "tabbable": null,
       "tooltip": null,
       "value": 883.0
      }
     },
     "82c63216b86049199e9ba60fd6fb9946": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_94f2fff7f34449d3836e54f2820a0c10",
        "IPY_MODEL_55160b2b883b4ef3a2fe6e83922102ba",
        "IPY_MODEL_d9ced05949e74c6f8231e8af3f400d71"
       ],
       "layout": "IPY_MODEL_9da961596fcf4d29b6a36e88862d1ac4",
       "tabbable": null,
       "tooltip": null
      }
     },
     "8f7b0f2dd889492e98dbfdc08e36ce0e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "8ffdbf394ca242d986e7b00efeca8506": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "910437ffb0c34c95bc880f729189ad04": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_593f83ff09ed435298b331e7080c3c93",
       "placeholder": "​",
       "style": "IPY_MODEL_4a643cd11d4d4120a2ab2a435c499ffe",
       "tabbable": null,
       "tooltip": null,
       "value": "Scoring: 100%"
      }
     },
     "93d2164ae6744c41896f6e530a063f32": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "94f2fff7f34449d3836e54f2820a0c10": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_77ca6e11a28740ad985365654d70538a",
       "placeholder": "​",
       "style": "IPY_MODEL_ab6b81098d564a948348ab7bb68de880",
       "tabbable": null,
       "tooltip": null,
       "value": "Generating: 100%"
      }
     },
     "97551981574e4c44a199bec83c6aba22": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_4543c419c2f74014a33f093a67fe9413",
       "placeholder": "​",
       "style": "IPY_MODEL_30f1ba5f824a40e6bb296e103e476d63",
       "tabbable": null,
       "tooltip": null,
       "value": " 883/883 [00:03&lt;00:00, 508.05it/s, Materializing param=model.vision_tower.vision_model.post_layernorm.weight]"
      }
     },
     "989bd264f2dd4e19aa85fb319b7828a1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "9da961596fcf4d29b6a36e88862d1ac4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ab6b81098d564a948348ab7bb68de880": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "ae1fb35370d74debb41317b898629a1b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_e1ae164fb52546e7891ffdba6c0217a7",
        "IPY_MODEL_479e5a410e9844c29ab7bef81445d67b",
        "IPY_MODEL_eceac5b2d26a46bfa38a166cd3c3a328"
       ],
       "layout": "IPY_MODEL_aee8de5ef5a846229b37494f232de2ff",
       "tabbable": null,
       "tooltip": null
      }
     },
     "aee8de5ef5a846229b37494f232de2ff": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d0868f41d059467aabe1676cedfc6f0b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d58e37508141410594e0f9c652540711": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d5f864a8da4d43109ee0d8707bf0a519": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d9ced05949e74c6f8231e8af3f400d71": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_d5f864a8da4d43109ee0d8707bf0a519",
       "placeholder": "​",
       "style": "IPY_MODEL_775ce6579d1746ec8eb276b69b2ef6e8",
       "tabbable": null,
       "tooltip": null,
       "value": " 400/400 [09:02&lt;00:00,  1.37s/it]"
      }
     },
     "e1ae164fb52546e7891ffdba6c0217a7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_fa38005d35784cedaee02d0518ddd832",
       "placeholder": "​",
       "style": "IPY_MODEL_7f645bcdc37349e1bea0f8f6f7257f31",
       "tabbable": null,
       "tooltip": null,
       "value": "Loading weights: 100%"
      }
     },
     "e2372d4afb6e483a83867fe3892b1efc": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "eceac5b2d26a46bfa38a166cd3c3a328": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_d0868f41d059467aabe1676cedfc6f0b",
       "placeholder": "​",
       "style": "IPY_MODEL_4f9e627f39d44a13a190d436d0c21d5c",
       "tabbable": null,
       "tooltip": null,
       "value": " 883/883 [00:03&lt;00:00, 521.63it/s, Materializing param=model.vision_tower.vision_model.post_layernorm.weight]"
      }
     },
     "f5691cfede7d409280e0dceec6d1995c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_7434cc496f754e3999d3914793e41910",
       "placeholder": "​",
       "style": "IPY_MODEL_4a61bf7c95d14fe880f3aaee45dcecb8",
       "tabbable": null,
       "tooltip": null,
       "value": "Loading weights: 100%"
      }
     },
     "fa38005d35784cedaee02d0518ddd832": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ff0ab51fff5b44f29fca1badb4b1b474": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_910437ffb0c34c95bc880f729189ad04",
        "IPY_MODEL_036e82cbfcfd420db1239994ada6ff8c",
        "IPY_MODEL_4651805a3fac4ea29a32b7ac1622a082"
       ],
       "layout": "IPY_MODEL_989bd264f2dd4e19aa85fb319b7828a1",
       "tabbable": null,
       "tooltip": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
