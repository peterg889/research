{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb971e52",
   "metadata": {},
   "source": [
    "# Decoder-Only Exp 04: Position Sweep + Cache Surgery\n",
    "\n",
    "## Motivation\n",
    "\n",
    "Exp 03 revealed that the \"structural effect\" was actually BOS removal (87%) +\n",
    "position offset (53%) minus attention-enrichment harm (-40%). But two mysteries remain:\n",
    "\n",
    "1. **Position sweet spot**: `pos_offset_4` (d=+0.78) vastly outperformed `pos_offset_20`\n",
    "   (d=+0.43) and `bare_no_bos` (d=+0.46). Why? Where exactly is the peak?\n",
    "\n",
    "2. **BOS mechanism**: Removing BOS from the Phase B cache was the biggest single factor.\n",
    "   Can we exploit this further? Does BOS matter during Phase A (shaping doc representations)\n",
    "   or only during Phase B (distracting query attention)?\n",
    "\n",
    "## Conditions (14 total)\n",
    "\n",
    "### Baselines\n",
    "| # | Condition | Description |\n",
    "|---|-----------|-------------|\n",
    "| 1 | bare | Standard causal, BOS in cache |\n",
    "| 2 | oracle | Real query as prefix — upper bound |\n",
    "\n",
    "### Position sweep (BOS removed from cache, no prefix)\n",
    "| # | S | Doc RoPE positions | BOS-to-doc distance |\n",
    "|---|---|-------------------|-------------------|\n",
    "| 3 | 1 | 1..D (= bare_no_bos) | 1 |\n",
    "| 4 | 2 | 2..2+D | 2 |\n",
    "| 5 | 4 | 4..4+D | 4 |\n",
    "| 6 | 8 | 8..8+D | 8 |\n",
    "| 7 | 16 | 16..16+D | 16 |\n",
    "| 8 | 32 | 32..32+D | 32 |\n",
    "\n",
    "### BOS mechanism isolation\n",
    "| # | Condition | Phase A BOS? | Phase B BOS? | Doc positions |\n",
    "|---|-----------|-------------|-------------|--------------|\n",
    "| 9 | no_bos_at_all | NO | NO | 0..D |\n",
    "| 10 | keep_bos_offset_4 | yes | **YES** | 4..4+D |\n",
    "\n",
    "### Cache pruning (natural positions, selective removal)\n",
    "| # | Condition | What's removed from cache | Purpose |\n",
    "|---|-----------|--------------------------|---------|\n",
    "| 11 | prune_first_1 | BOS + doc[0] | Are early doc tokens sinks too? |\n",
    "| 12 | prune_first_3 | BOS + doc[0:3] | Deeper pruning |\n",
    "| 13 | prune_first_5 | BOS + doc[0:5] | How far can we go? |\n",
    "| 14 | prune_last_3 | BOS + doc[-3:] | Control (end tokens shouldn't be sinks) |\n",
    "\n",
    "## Key comparisons\n",
    "\n",
    "**Position sweep**: S=1 → 2 → 4 → 8 → 16 → 32 traces the full curve.\n",
    "\n",
    "**BOS in Phase A vs Phase B**:\n",
    "- bare vs bare_no_bos → Phase B BOS effect (removing from cache)\n",
    "- bare_no_bos vs no_bos_at_all → Phase A BOS effect (removing from input)\n",
    "- pos_offset_4 vs keep_bos_offset_4 → Phase B BOS effect at optimal offset\n",
    "\n",
    "**Cache pruning depth**:\n",
    "- prune_first_{1,3,5} vs bare_no_bos → do early doc tokens act as sinks?\n",
    "- prune_last_3 → control (removing informative tokens should hurt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c6d9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Setup and model loading\n",
    "import os\n",
    "os.umask(0o000)\n",
    "\n",
    "import sys, json, time, gc, re\n",
    "import random as pyrandom\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from scipy import stats\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "sys.path.insert(0, \"../../..\")\n",
    "from lib.analysis import cohens_d\n",
    "\n",
    "SEED = 42\n",
    "N_SAMPLES = 400\n",
    "MODEL_NAME = \"google/gemma-3-4b-it\"\n",
    "\n",
    "RESULTS_DIR = Path(\"../../../results/decoder_only/exp04\")\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CHECKPOINT_PATH = RESULTS_DIR / \"checkpoint.json\"\n",
    "\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "pyrandom.seed(SEED)\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv())\n",
    "HF_TOKEN = os.environ.get(\"HF_TOKEN\")\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "print(f\"Loading {MODEL_NAME}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, token=HF_TOKEN)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME, device_map=\"auto\", torch_dtype=torch.bfloat16, token=HF_TOKEN,\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "DEVICE = next(model.parameters()).device\n",
    "\n",
    "print(f\"Exp 04: Position Sweep + Cache Surgery\")\n",
    "print(f\"N: {N_SAMPLES}, Model: {MODEL_NAME}\")\n",
    "print(f\"DEVICE: {DEVICE}, dtype: {next(model.parameters()).dtype}\")\n",
    "print(f\"GPU memory: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "text_cfg = getattr(model.config, 'text_config', model.config)\n",
    "print(f\"Vocab size: {getattr(text_cfg, 'vocab_size', 'N/A')}\")\n",
    "print(f\"Num layers: {getattr(text_cfg, 'num_hidden_layers', 'N/A')}\")\n",
    "print(f\"Num KV heads: {getattr(text_cfg, 'num_key_value_heads', 'N/A')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fbd836",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: KV cache helpers and scoring functions\n",
    "\n",
    "def slice_kv_cache(cache, start_idx):\n",
    "    # Remove first start_idx entries from KV cache.\n",
    "    from transformers import DynamicCache\n",
    "    if isinstance(cache, DynamicCache):\n",
    "        sliced = DynamicCache()\n",
    "        for i in range(len(cache.layers)):\n",
    "            k = cache.layers[i].keys[:, :, start_idx:, :]\n",
    "            v = cache.layers[i].values[:, :, start_idx:, :]\n",
    "            sliced.update(k, v, i)\n",
    "        return sliced\n",
    "    else:\n",
    "        return tuple(\n",
    "            (k[:, :, start_idx:, :], v[:, :, start_idx:, :])\n",
    "            for k, v in cache\n",
    "        )\n",
    "\n",
    "\n",
    "def prune_kv_cache_end(cache, n):\n",
    "    # Remove last n entries from KV cache.\n",
    "    from transformers import DynamicCache\n",
    "    if isinstance(cache, DynamicCache):\n",
    "        pruned = DynamicCache()\n",
    "        for i in range(len(cache.layers)):\n",
    "            k = cache.layers[i].keys[:, :, :-n, :]\n",
    "            v = cache.layers[i].values[:, :, :-n, :]\n",
    "            pruned.update(k, v, i)\n",
    "        return pruned\n",
    "    else:\n",
    "        return tuple(\n",
    "            (k[:, :, :-n, :], v[:, :, :-n, :])\n",
    "            for k, v in cache\n",
    "        )\n",
    "\n",
    "\n",
    "def score(doc_text, query_text, answer_text, prefix_text=None,\n",
    "          position_offset=0, remove_bos=False,\n",
    "          keep_bos_in_offset=False, no_bos_input=False,\n",
    "          prune_first=0, prune_last=0):\n",
    "    # Score NLL of answer tokens using two-phase KV cache.\n",
    "    #\n",
    "    # Modes:\n",
    "    #   prefix_text: [BOS + prefix + \\n + doc], prefix sliced from cache\n",
    "    #   position_offset > 0: BOS at pos 0, doc at offset..offset+D\n",
    "    #     BOS removed from cache unless keep_bos_in_offset=True\n",
    "    #   no_bos_input: forward doc WITHOUT BOS token, no BOS in cache\n",
    "    #   remove_bos: bare but BOS removed from cache\n",
    "    #   prune_first/prune_last: additional doc token pruning from cache\n",
    "\n",
    "    # --- Phase A: Conditioning ---\n",
    "    if prefix_text is not None:\n",
    "        prefix_ids = tokenizer(prefix_text + \"\\n\", add_special_tokens=True,\n",
    "                               truncation=True, max_length=512).input_ids\n",
    "        doc_ids = tokenizer(doc_text, add_special_tokens=False,\n",
    "                            truncation=True, max_length=1536).input_ids\n",
    "        cond_ids = prefix_ids + doc_ids\n",
    "        slice_start = len(prefix_ids)\n",
    "        custom_pos = None\n",
    "        phase_b_start = len(cond_ids)\n",
    "\n",
    "    elif no_bos_input:\n",
    "        # No BOS in input at all\n",
    "        cond_ids = tokenizer(doc_text, add_special_tokens=False,\n",
    "                             truncation=True, max_length=2048).input_ids\n",
    "        slice_start = 0\n",
    "        custom_pos = None\n",
    "        phase_b_start = len(cond_ids)\n",
    "\n",
    "    elif position_offset > 0:\n",
    "        cond_ids = tokenizer(doc_text, add_special_tokens=True,\n",
    "                             truncation=True, max_length=2048).input_ids\n",
    "        n_doc = len(cond_ids) - 1\n",
    "        pos_list = [0] + list(range(position_offset, position_offset + n_doc))\n",
    "        custom_pos = torch.tensor([pos_list], dtype=torch.long, device=DEVICE)\n",
    "        if keep_bos_in_offset:\n",
    "            slice_start = 0\n",
    "        else:\n",
    "            slice_start = 1\n",
    "        phase_b_start = position_offset + n_doc\n",
    "\n",
    "    else:\n",
    "        cond_ids = tokenizer(doc_text, add_special_tokens=True,\n",
    "                             truncation=True, max_length=2048).input_ids\n",
    "        slice_start = 1 if remove_bos else 0\n",
    "        custom_pos = None\n",
    "        phase_b_start = len(cond_ids)\n",
    "\n",
    "    cond_tensor = torch.tensor([cond_ids], dtype=torch.long, device=DEVICE)\n",
    "\n",
    "    fwd_kwargs = {'input_ids': cond_tensor, 'use_cache': True}\n",
    "    if custom_pos is not None:\n",
    "        fwd_kwargs['position_ids'] = custom_pos\n",
    "\n",
    "    with torch.no_grad():\n",
    "        phase_a = model(**fwd_kwargs)\n",
    "\n",
    "    cache = phase_a.past_key_values\n",
    "    del phase_a\n",
    "\n",
    "    if slice_start > 0:\n",
    "        cache = slice_kv_cache(cache, slice_start)\n",
    "\n",
    "    # Additional pruning\n",
    "    if prune_first > 0:\n",
    "        cache = slice_kv_cache(cache, prune_first)\n",
    "    if prune_last > 0:\n",
    "        cache = prune_kv_cache_end(cache, prune_last)\n",
    "\n",
    "    # --- Phase B: Inference ---\n",
    "    query_part_ids = tokenizer(\"\\n\" + query_text + \"\\n\",\n",
    "                               add_special_tokens=False).input_ids\n",
    "    answer_ids = tokenizer(answer_text, add_special_tokens=False,\n",
    "                           truncation=True, max_length=256).input_ids\n",
    "\n",
    "    if not answer_ids:\n",
    "        del cache\n",
    "        return 0.0\n",
    "\n",
    "    phase_b_ids = query_part_ids + answer_ids\n",
    "    phase_b_tensor = torch.tensor([phase_b_ids], dtype=torch.long, device=DEVICE)\n",
    "\n",
    "    pos_ids = torch.arange(phase_b_start, phase_b_start + len(phase_b_ids),\n",
    "                           device=DEVICE).unsqueeze(0)\n",
    "    cache_position = torch.arange(phase_b_start, phase_b_start + len(phase_b_ids),\n",
    "                                  device=DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        phase_b = model(\n",
    "            input_ids=phase_b_tensor,\n",
    "            past_key_values=cache,\n",
    "            position_ids=pos_ids,\n",
    "            cache_position=cache_position,\n",
    "            use_cache=False,\n",
    "        )\n",
    "\n",
    "    logits = phase_b.logits\n",
    "    n_query_part = len(query_part_ids)\n",
    "    n_answer = len(answer_ids)\n",
    "\n",
    "    answer_logits = logits[0, n_query_part - 1 : n_query_part - 1 + n_answer, :]\n",
    "    answer_targets = torch.tensor(answer_ids, dtype=torch.long, device=DEVICE)\n",
    "\n",
    "    log_probs = F.log_softmax(answer_logits, dim=-1)\n",
    "    token_log_probs = log_probs.gather(1, answer_targets.unsqueeze(1)).squeeze(1)\n",
    "    mean_nll = -token_log_probs.mean().item()\n",
    "\n",
    "    del cache, phase_b, logits, log_probs\n",
    "    return mean_nll\n",
    "\n",
    "\n",
    "print(\"Scoring function defined with extended modes.\")\n",
    "print(f\"  position_offset: RoPE shift for doc tokens\")\n",
    "print(f\"  keep_bos_in_offset: keep BOS in Phase B cache\")\n",
    "print(f\"  no_bos_input: skip BOS token entirely\")\n",
    "print(f\"  prune_first/prune_last: remove doc tokens from cache\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef18f4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Load MS MARCO data\n",
    "from lib.data import count_words\n",
    "from datasets import load_dataset\n",
    "\n",
    "print(\"Loading MS MARCO v1.1 validation...\")\n",
    "ds = load_dataset(\"microsoft/ms_marco\", \"v1.1\", split=\"validation\")\n",
    "\n",
    "all_candidates = []\n",
    "for item in ds:\n",
    "    if len(all_candidates) >= 3 * N_SAMPLES:\n",
    "        break\n",
    "    passages = item.get('passages', {})\n",
    "    ptexts = passages.get('passage_text', [])\n",
    "    is_sel = passages.get('is_selected', [])\n",
    "    query = item.get('query', '')\n",
    "    answers = item.get('answers', [])\n",
    "    well_formed = item.get('wellFormedAnswers', [])\n",
    "    answer = None\n",
    "    if well_formed and len(well_formed) > 0 and well_formed[0] not in ('[]', ''):\n",
    "        answer = well_formed[0]\n",
    "    elif answers and len(answers) > 0 and answers[0] != 'No Answer Present.':\n",
    "        answer = answers[0]\n",
    "    if not answer:\n",
    "        continue\n",
    "    for pt, sel in zip(ptexts, is_sel):\n",
    "        wc = count_words(pt)\n",
    "        if sel == 1 and 30 <= wc <= 300:\n",
    "            all_candidates.append({\n",
    "                'passage': pt, 'query': query, 'answer': answer,\n",
    "                'word_count': wc,\n",
    "            })\n",
    "            break\n",
    "\n",
    "print(f\"Total candidates: {len(all_candidates)}\")\n",
    "np.random.seed(SEED)\n",
    "indices = np.random.permutation(len(all_candidates))\n",
    "samples = [all_candidates[i] for i in indices[:N_SAMPLES]]\n",
    "del ds, all_candidates\n",
    "gc.collect()\n",
    "\n",
    "print(f\"Loaded {len(samples)} samples\")\n",
    "print(f\"Mean passage words: {np.mean([s['word_count'] for s in samples]):.0f}\")\n",
    "print(f\"Mean query words: {np.mean([len(s['query'].split()) for s in samples]):.1f}\")\n",
    "mean_doc_tokens = np.mean([len(tokenizer(s['passage'], add_special_tokens=False).input_ids)\n",
    "                           for s in samples[:50]])\n",
    "print(f\"Mean passage tokens (first 50): {mean_doc_tokens:.0f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17fa8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Validate all scoring modes\n",
    "print(\"=\" * 70)\n",
    "print(\"VALIDATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "s = samples[0]\n",
    "\n",
    "# All modes run without error\n",
    "print(f\"\\n--- All modes on sample 0 ---\")\n",
    "modes = [\n",
    "    (\"bare\",              dict()),\n",
    "    (\"oracle\",            dict(prefix_text=s['query'])),\n",
    "    (\"S=1 (bare_no_bos)\", dict(position_offset=1)),\n",
    "    (\"S=4\",               dict(position_offset=4)),\n",
    "    (\"S=8\",               dict(position_offset=8)),\n",
    "    (\"S=32\",              dict(position_offset=32)),\n",
    "    (\"no_bos_at_all\",     dict(no_bos_input=True)),\n",
    "    (\"keep_bos_S=4\",      dict(position_offset=4, keep_bos_in_offset=True)),\n",
    "    (\"prune_first_3\",     dict(remove_bos=True, prune_first=3)),\n",
    "    (\"prune_last_3\",      dict(remove_bos=True, prune_last=3)),\n",
    "]\n",
    "\n",
    "for name, kwargs in modes:\n",
    "    nll = score(s['passage'], s['query'], s['answer'], **kwargs)\n",
    "    delta = score(s['passage'], s['query'], s['answer']) - nll\n",
    "    print(f\"  {name:<20} NLL = {nll:.4f}  delta vs bare = {delta:+.4f}\")\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fb0dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Scoring loop — 14 conditions x 400 samples\n",
    "print(\"=\" * 70)\n",
    "print(\"SCORING ALL CONDITIONS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "COND_NAMES = [\n",
    "    'bare', 'oracle',\n",
    "    'pos_1', 'pos_2', 'pos_4', 'pos_8', 'pos_16', 'pos_32',\n",
    "    'no_bos_at_all', 'keep_bos_offset_4',\n",
    "    'prune_first_1', 'prune_first_3', 'prune_first_5', 'prune_last_3',\n",
    "]\n",
    "\n",
    "results = []\n",
    "start_idx = 0\n",
    "\n",
    "if CHECKPOINT_PATH.exists():\n",
    "    ckpt = json.loads(CHECKPOINT_PATH.read_text())\n",
    "    if ckpt.get('n_total') == N_SAMPLES and len(ckpt.get('results', [])) > 0:\n",
    "        saved_queries = [r['query'][:50] for r in ckpt['results']]\n",
    "        current_queries = [s['query'][:50] for s in samples[:len(saved_queries)]]\n",
    "        if saved_queries == current_queries:\n",
    "            results = ckpt['results']\n",
    "            start_idx = len(results)\n",
    "            print(f\"Resuming from checkpoint: {start_idx}/{N_SAMPLES}\")\n",
    "\n",
    "if start_idx == 0:\n",
    "    print(f\"Starting fresh: {len(COND_NAMES)} conditions x {N_SAMPLES} samples\")\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "for i in tqdm(range(start_idx, N_SAMPLES), initial=start_idx, total=N_SAMPLES,\n",
    "              desc=\"Scoring\"):\n",
    "    s = samples[i]\n",
    "    query = s['query']\n",
    "    passage = s['passage']\n",
    "    answer = s['answer']\n",
    "\n",
    "    result = {\n",
    "        'query': query,\n",
    "        'answer': answer,\n",
    "        'passage_words': s['word_count'],\n",
    "        'query_words': len(query.split()),\n",
    "    }\n",
    "\n",
    "    # Baselines\n",
    "    result['nll_bare'] = score(passage, query, answer)\n",
    "    result['nll_oracle'] = score(passage, query, answer, prefix_text=query)\n",
    "\n",
    "    # Position sweep (BOS removed)\n",
    "    for S in [1, 2, 4, 8, 16, 32]:\n",
    "        result[f'nll_pos_{S}'] = score(passage, query, answer, position_offset=S)\n",
    "\n",
    "    # BOS mechanism\n",
    "    result['nll_no_bos_at_all'] = score(passage, query, answer, no_bos_input=True)\n",
    "    result['nll_keep_bos_offset_4'] = score(passage, query, answer,\n",
    "                                             position_offset=4,\n",
    "                                             keep_bos_in_offset=True)\n",
    "\n",
    "    # Cache pruning (natural positions, BOS removed)\n",
    "    result['nll_prune_first_1'] = score(passage, query, answer,\n",
    "                                         remove_bos=True, prune_first=1)\n",
    "    result['nll_prune_first_3'] = score(passage, query, answer,\n",
    "                                         remove_bos=True, prune_first=3)\n",
    "    result['nll_prune_first_5'] = score(passage, query, answer,\n",
    "                                         remove_bos=True, prune_first=5)\n",
    "    result['nll_prune_last_3'] = score(passage, query, answer,\n",
    "                                        remove_bos=True, prune_last=3)\n",
    "\n",
    "    results.append(result)\n",
    "\n",
    "    if (i + 1) % 20 == 0 or i == N_SAMPLES - 1:\n",
    "        ckpt = {\n",
    "            'n_total': N_SAMPLES,\n",
    "            'results': results,\n",
    "            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        }\n",
    "        CHECKPOINT_PATH.write_text(json.dumps(ckpt))\n",
    "        elapsed = time.time() - t0\n",
    "        done = i - start_idx + 1\n",
    "        eta = (N_SAMPLES - i - 1) * elapsed / done if done > 0 else 0\n",
    "        tqdm.write(f\"  Checkpoint {i+1}/{N_SAMPLES} | {elapsed/60:.1f}m | ETA {eta/60:.1f}m\")\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "elapsed = time.time() - t0\n",
    "print(f\"\\nScoring complete: {len(results)} samples, \"\n",
    "      f\"{len(COND_NAMES)} conditions in {elapsed/60:.1f} min\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d7178c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Results table\n",
    "print(\"=\" * 70)\n",
    "print(f\"RESULTS (N={len(results)})\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "arrays = {}\n",
    "for name in COND_NAMES:\n",
    "    arrays[name] = np.array([r[f'nll_{name}'] for r in results])\n",
    "\n",
    "bare = arrays['bare']\n",
    "oracle = arrays['oracle']\n",
    "oracle_delta_mean = (bare - oracle).mean()\n",
    "\n",
    "print(f\"\\n  {'Condition':<22} {'NLL':>8} {'vs bare':>10} {'d':>8} \"\n",
    "      f\"{'Win%':>8} {'p':>12} {'sig':>5} {'Recovery':>10}\")\n",
    "print(f\"  {'-'*92}\")\n",
    "\n",
    "analysis = {}\n",
    "for name in COND_NAMES:\n",
    "    nlls = arrays[name]\n",
    "    mean_nll = nlls.mean()\n",
    "\n",
    "    if name == 'bare':\n",
    "        print(f\"  {name:<22} {mean_nll:>8.4f} {'--':>10} {'--':>8} \"\n",
    "              f\"{'--':>8} {'--':>12} {'--':>5} {'--':>10}\")\n",
    "        analysis[name] = {'mean_nll': float(mean_nll)}\n",
    "    else:\n",
    "        diff = bare - nlls\n",
    "        d = cohens_d(diff)\n",
    "        win_pct = 100 * np.mean(diff > 0)\n",
    "        _, p_val = stats.ttest_1samp(diff, 0)\n",
    "        sig = '***' if p_val < 0.001 else '**' if p_val < 0.01 else '*' if p_val < 0.05 else 'ns'\n",
    "        rec = diff.mean() / oracle_delta_mean * 100 if oracle_delta_mean > 0 else 0\n",
    "        rec_str = f\"{rec:>9.1f}%\"\n",
    "\n",
    "        print(f\"  {name:<22} {mean_nll:>8.4f} {diff.mean():>+10.4f} {d:>+8.3f} \"\n",
    "              f\"{win_pct:>7.1f}% {p_val:>12.2e} {sig:>5} {rec_str:>10}\")\n",
    "        analysis[name] = {\n",
    "            'mean_nll': float(mean_nll), 'delta': float(diff.mean()),\n",
    "            'd': float(d), 'win_pct': float(win_pct), 'p': float(p_val),\n",
    "            'recovery': float(rec),\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ed287a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Position sweep analysis\n",
    "print(\"=\" * 70)\n",
    "print(\"POSITION SWEEP\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\n  All conditions: BOS removed from cache, no prefix tokens.\")\n",
    "print(f\"  S = position offset (BOS at 0, doc at S..S+D)\")\n",
    "\n",
    "print(f\"\\n  {'S':>4} {'NLL':>8} {'d vs bare':>10} {'recovery':>10} \"\n",
    "      f\"{'d vs S=1':>10} {'p vs S=1':>12}\")\n",
    "print(f\"  {'-'*62}\")\n",
    "\n",
    "sweep_s = [1, 2, 4, 8, 16, 32]\n",
    "sweep_d = []\n",
    "for S in sweep_s:\n",
    "    name = f'pos_{S}'\n",
    "    nlls = arrays[name]\n",
    "    d_b = cohens_d(bare - nlls)\n",
    "    rec = (bare - nlls).mean() / oracle_delta_mean * 100\n",
    "    diff_vs_1 = arrays['pos_1'] - nlls\n",
    "    d_1 = cohens_d(diff_vs_1)\n",
    "    _, p_1 = stats.ttest_1samp(diff_vs_1, 0)\n",
    "    sig = '***' if p_1 < 0.001 else '**' if p_1 < 0.01 else '*' if p_1 < 0.05 else 'ns'\n",
    "    print(f\"  {S:>4} {nlls.mean():>8.4f} {d_b:>+10.4f} {rec:>9.1f}% \"\n",
    "          f\"{d_1:>+10.4f} {p_1:>12.2e} {sig}\")\n",
    "    sweep_d.append(d_b)\n",
    "\n",
    "# Find peak\n",
    "peak_idx = np.argmax(sweep_d)\n",
    "peak_s = sweep_s[peak_idx]\n",
    "print(f\"\\n  Peak: S={peak_s} (d = {sweep_d[peak_idx]:+.4f})\")\n",
    "\n",
    "# Shape characterization\n",
    "print(f\"\\n  Curve shape:\")\n",
    "if peak_idx == 0:\n",
    "    print(f\"  -> MONOTONICALLY DECREASING from S=1. Offset always hurts.\")\n",
    "elif peak_idx == len(sweep_s) - 1:\n",
    "    print(f\"  -> MONOTONICALLY INCREASING to S=32. Larger offset always better.\")\n",
    "else:\n",
    "    print(f\"  -> INVERTED-U with peak at S={peak_s}.\")\n",
    "    print(f\"  -> Below peak: benefit increases with offset\")\n",
    "    print(f\"  -> Above peak: benefit decreases (over-shifting)\")\n",
    "\n",
    "# Adjacent pairwise significance\n",
    "print(f\"\\n  Adjacent pairwise tests (does increasing S help?):\")\n",
    "for i in range(len(sweep_s) - 1):\n",
    "    s_a, s_b = sweep_s[i], sweep_s[i + 1]\n",
    "    diff = arrays[f'pos_{s_a}'] - arrays[f'pos_{s_b}']\n",
    "    d = cohens_d(diff)\n",
    "    _, p = stats.ttest_1samp(diff, 0)\n",
    "    sig = '***' if p < 0.001 else '**' if p < 0.01 else '*' if p < 0.05 else 'ns'\n",
    "    direction = \"better\" if d > 0 else \"worse\" if d < 0 else \"same\"\n",
    "    print(f\"  S={s_a:>2} → S={s_b:>2}: d = {d:+.4f} ({sig}) — S={s_b} is {direction}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a5359a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: BOS mechanism isolation and cache surgery\n",
    "print(\"=\" * 70)\n",
    "print(\"BOS MECHANISM ISOLATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# --- Phase A vs Phase B BOS effects ---\n",
    "print(f\"\\n--- Where does BOS matter? ---\\n\")\n",
    "\n",
    "# Phase B effect: bare → bare_no_bos (pos_1)\n",
    "f_phase_b = bare - arrays['pos_1']\n",
    "d_pb = cohens_d(f_phase_b)\n",
    "_, p_pb = stats.ttest_1samp(f_phase_b, 0)\n",
    "print(f\"  Phase B (removing BOS from cache):\")\n",
    "print(f\"    bare → pos_1 (bare_no_bos): d = {d_pb:+.4f}, p = {p_pb:.2e}\")\n",
    "print(f\"    NLL: {bare.mean():.4f} → {arrays['pos_1'].mean():.4f}\")\n",
    "\n",
    "# Phase A effect: bare_no_bos → no_bos_at_all\n",
    "f_phase_a = arrays['pos_1'] - arrays['no_bos_at_all']\n",
    "d_pa = cohens_d(f_phase_a)\n",
    "_, p_pa = stats.ttest_1samp(f_phase_a, 0)\n",
    "sig_pa = '***' if p_pa < 0.001 else '**' if p_pa < 0.01 else '*' if p_pa < 0.05 else 'ns'\n",
    "print(f\"\\n  Phase A (removing BOS from input):\")\n",
    "print(f\"    pos_1 → no_bos_at_all: d = {d_pa:+.4f} ({sig_pa}), p = {p_pa:.2e}\")\n",
    "print(f\"    NLL: {arrays['pos_1'].mean():.4f} → {arrays['no_bos_at_all'].mean():.4f}\")\n",
    "if d_pa > 0.05:\n",
    "    print(f\"    → BOS in Phase A HURTS doc representations\")\n",
    "elif d_pa < -0.05:\n",
    "    print(f\"    → BOS in Phase A HELPS doc representations\")\n",
    "else:\n",
    "    print(f\"    → BOS in Phase A has minimal effect on doc representations\")\n",
    "\n",
    "# Phase B BOS effect at optimal offset\n",
    "print(f\"\\n  Phase B BOS effect at S=4:\")\n",
    "diff_keep = arrays['keep_bos_offset_4'] - arrays['pos_4']\n",
    "d_keep = cohens_d(diff_keep)\n",
    "_, p_keep = stats.ttest_1samp(diff_keep, 0)\n",
    "sig_k = '***' if p_keep < 0.001 else '**' if p_keep < 0.01 else '*' if p_keep < 0.05 else 'ns'\n",
    "print(f\"    keep_bos_offset_4 NLL = {arrays['keep_bos_offset_4'].mean():.4f}\")\n",
    "print(f\"    pos_4 (BOS removed) NLL = {arrays['pos_4'].mean():.4f}\")\n",
    "print(f\"    Keeping BOS: d = {d_keep:+.4f} ({sig_k})\")\n",
    "if d_keep > 0.05:\n",
    "    print(f\"    → Even at S=4, BOS in cache HURTS Phase B\")\n",
    "elif d_keep < -0.05:\n",
    "    print(f\"    → At S=4, BOS in cache actually helps Phase B\")\n",
    "else:\n",
    "    print(f\"    → At S=4, BOS in cache makes no difference\")\n",
    "\n",
    "# -----------------------------------------------------------------------\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"CACHE PRUNING\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "print(f\"\\n  All conditions: BOS removed, natural positions, doc tokens pruned.\")\n",
    "print(f\"\\n  {'Condition':<22} {'NLL':>8} {'d vs bare':>10} {'d vs pos_1':>12} {'p':>12}\")\n",
    "print(f\"  {'-'*70}\")\n",
    "for name in ['pos_1', 'prune_first_1', 'prune_first_3', 'prune_first_5', 'prune_last_3']:\n",
    "    nlls = arrays[name]\n",
    "    d_b = cohens_d(bare - nlls)\n",
    "    diff_vs_1 = arrays['pos_1'] - nlls\n",
    "    d_1 = cohens_d(diff_vs_1)\n",
    "    _, p_1 = stats.ttest_1samp(diff_vs_1, 0)\n",
    "    sig = '***' if p_1 < 0.001 else '**' if p_1 < 0.01 else '*' if p_1 < 0.05 else 'ns'\n",
    "    print(f\"  {name:<22} {nlls.mean():>8.4f} {d_b:>+10.4f} {d_1:>+12.4f} {p_1:>12.2e} {sig}\")\n",
    "\n",
    "# Interpretation\n",
    "d_pf1 = cohens_d(arrays['pos_1'] - arrays['prune_first_1'])\n",
    "d_pf3 = cohens_d(arrays['pos_1'] - arrays['prune_first_3'])\n",
    "d_pf5 = cohens_d(arrays['pos_1'] - arrays['prune_first_5'])\n",
    "d_pl3 = cohens_d(arrays['pos_1'] - arrays['prune_last_3'])\n",
    "\n",
    "print(f\"\\n  Interpretation:\")\n",
    "if d_pf1 > 0.05:\n",
    "    print(f\"  -> Removing doc[0] helps (d = {d_pf1:+.4f}): early tokens ARE sinks\")\n",
    "elif d_pf1 < -0.05:\n",
    "    print(f\"  -> Removing doc[0] hurts (d = {d_pf1:+.4f}): doc[0] is informative\")\n",
    "else:\n",
    "    print(f\"  -> Removing doc[0] neutral (d = {d_pf1:+.4f})\")\n",
    "\n",
    "if d_pl3 < -0.05:\n",
    "    print(f\"  -> Removing last 3 hurts (d = {d_pl3:+.4f}): confirms end tokens are informative\")\n",
    "elif d_pl3 > 0.05:\n",
    "    print(f\"  -> Removing last 3 helps?! (d = {d_pl3:+.4f}): unexpected\")\n",
    "else:\n",
    "    print(f\"  -> Removing last 3 neutral (d = {d_pl3:+.4f})\")\n",
    "\n",
    "# Best overall condition\n",
    "print(f\"\\n  Best non-oracle conditions:\")\n",
    "non_oracle = {k: v for k, v in analysis.items()\n",
    "              if k not in ('bare', 'oracle') and 'd' in v}\n",
    "sorted_conds = sorted(non_oracle.items(), key=lambda x: x[1]['d'], reverse=True)\n",
    "for name, info in sorted_conds[:5]:\n",
    "    print(f\"    {name:<22} d = {info['d']:+.4f} ({info['recovery']:.0f}% recovery)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dce2c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Verdict\n",
    "print(\"=\" * 70)\n",
    "print(\"VERDICT — Exp 04: Position Sweep + Cache Surgery\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\nModel: {MODEL_NAME}\")\n",
    "print(f\"N: {len(results)} samples (MS MARCO v1.1)\")\n",
    "\n",
    "# Best condition\n",
    "non_oracle = {k: v for k, v in analysis.items()\n",
    "              if k not in ('bare', 'oracle') and 'd' in v}\n",
    "best_name = max(non_oracle, key=lambda k: non_oracle[k]['d'])\n",
    "best_d = non_oracle[best_name]['d']\n",
    "best_rec = non_oracle[best_name]['recovery']\n",
    "\n",
    "print(f\"\\n--- Key findings ---\")\n",
    "print(f\"  Best condition: {best_name} (d = {best_d:+.4f}, {best_rec:.0f}% of oracle)\")\n",
    "print(f\"  Oracle: d = {cohens_d(bare - oracle):+.4f}\")\n",
    "\n",
    "# Position peak\n",
    "sweep_ds = [cohens_d(bare - arrays[f'pos_{S}']) for S in [1, 2, 4, 8, 16, 32]]\n",
    "peak_s = [1, 2, 4, 8, 16, 32][np.argmax(sweep_ds)]\n",
    "print(f\"  Position peak: S={peak_s}\")\n",
    "\n",
    "# BOS summary\n",
    "d_phase_b = cohens_d(bare - arrays['pos_1'])\n",
    "d_phase_a = cohens_d(arrays['pos_1'] - arrays['no_bos_at_all'])\n",
    "print(f\"\\n  BOS effects:\")\n",
    "print(f\"    Phase B (cache removal): d = {d_phase_b:+.4f}\")\n",
    "print(f\"    Phase A (input removal): d = {d_phase_a:+.4f}\")\n",
    "\n",
    "# All conditions ordered\n",
    "print(f\"\\n--- All conditions (ranked by d) ---\")\n",
    "all_ranked = sorted(analysis.items(),\n",
    "                    key=lambda x: x[1].get('d', -999), reverse=True)\n",
    "for name, info in all_ranked:\n",
    "    if name == 'bare':\n",
    "        print(f\"  {name:<22} NLL = {info['mean_nll']:.4f}  (baseline)\")\n",
    "    else:\n",
    "        print(f\"  {name:<22} NLL = {info['mean_nll']:.4f}  d = {info['d']:+.4f}\")\n",
    "\n",
    "# Save\n",
    "final_results = {\n",
    "    'experiment': 'v4_decoder_only_exp04_position_sweep_cache_surgery',\n",
    "    'model': MODEL_NAME,\n",
    "    'dataset': 'ms_marco_v1.1',\n",
    "    'n_samples': len(results),\n",
    "    'seed': SEED,\n",
    "    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'conditions': {k: v for k, v in analysis.items()},\n",
    "}\n",
    "\n",
    "with open(RESULTS_DIR / 'results.json', 'w') as f:\n",
    "    json.dump(final_results, f, indent=2)\n",
    "print(f\"\\nResults saved to {RESULTS_DIR / 'results.json'}\")\n",
    "\n",
    "# Cleanup\n",
    "print(f\"\\nCleaning up GPU memory...\")\n",
    "mem_before = torch.cuda.memory_allocated() / 1e9\n",
    "del model, tokenizer\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "mem_after = torch.cuda.memory_allocated() / 1e9\n",
    "print(f\"GPU memory: {mem_before:.2f} GB -> {mem_after:.2f} GB\")\n",
    "print(\"Done!\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
