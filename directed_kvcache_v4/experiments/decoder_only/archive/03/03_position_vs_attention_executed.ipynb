{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "279130b8",
   "metadata": {},
   "source": [
    "# Decoder-Only Exp 03: Position vs Attention Isolation\n",
    "\n",
    "## Motivation\n",
    "\n",
    "Exp 02 found that the **structural effect** (any prefix helps, regardless of content)\n",
    "accounts for 82-88% of the benefit on Gemma 3 4B. But what drives this structural\n",
    "component? Three candidate mechanisms:\n",
    "\n",
    "1. **BOS removal**: In all prefixed conditions, BOS is sliced from the Phase B cache\n",
    "   (because BOS is part of the prefix). In bare, BOS stays. If BOS acts as an attention\n",
    "   sink that wastes capacity, removing it from the cache could improve query attention.\n",
    "\n",
    "2. **RoPE position offset**: With a prefix of S tokens, doc tokens sit at RoPE positions\n",
    "   S..S+D instead of 1..D. Even though relative doc-query distances are preserved,\n",
    "   the absolute positions differ. The model may have learned position-dependent behaviors.\n",
    "   Also, BOS-to-doc distance increases from 1 to S, weakening the BOS attention sink.\n",
    "\n",
    "3. **Attention enrichment**: During Phase A, doc tokens attend to prefix tokens in\n",
    "   addition to BOS and prior doc tokens. This changes hidden states and thus KV values.\n",
    "   Even random words provide alternative attention targets beyond BOS, yielding a richer\n",
    "   weighted sum.\n",
    "\n",
    "## Conditions (10 total)\n",
    "\n",
    "### Diagnostic controls\n",
    "| # | Condition | Prefix tokens? | BOS in cache? | Doc RoPE positions | Tests |\n",
    "|---|-----------|---------------|---------------|-------------------|-------|\n",
    "| 1 | bare | no | YES | 1..D | Baseline |\n",
    "| 2 | oracle | query | no | S..S+D | Upper bound |\n",
    "| 3 | bare_no_bos | no | **NO** | 1..D | Factor 1: BOS removal |\n",
    "| 4 | pos_offset_4 | no | no | 4..4+D | Factor 2: small offset |\n",
    "| 5 | pos_offset_20 | no | no | 20..20+D | Factor 2: large offset |\n",
    "\n",
    "### Saturation curve (real prefix tokens)\n",
    "| # | Condition | Prefix | BOS in cache? | Doc positions (approx) |\n",
    "|---|-----------|--------|---------------|----------------------|\n",
    "| 6 | newline_only | just `\\n` | no | 2..2+D |\n",
    "| 7 | single_word | 1 random word | no | ~4..4+D |\n",
    "| 8 | random_3w | 3 random words | no | ~6..6+D |\n",
    "| 9 | random_5w | 5 random words | no | ~8..8+D |\n",
    "| 10 | random_15w | 15 random words | no | ~20..20+D |\n",
    "\n",
    "## Key diagnostic comparisons\n",
    "\n",
    "1. **BOS removal**: bare → bare_no_bos\n",
    "2. **Position offset**: bare_no_bos → pos_offset_20\n",
    "3. **Attention enrichment**: pos_offset_20 → random_15w\n",
    "4. **Saturation curve**: newline → 1w → 3w → 5w → 15w\n",
    "5. **Offset dose-response**: pos_offset_4 vs pos_offset_20\n",
    "\n",
    "These three factors should sum to the total structural effect:\n",
    "`(bare − random_15w) = (bare − bare_no_bos) + (bare_no_bos − pos_offset_20) + (pos_offset_20 − random_15w)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ddcb764b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-20T15:09:35.249301Z",
     "iopub.status.busy": "2026-02-20T15:09:35.248682Z",
     "iopub.status.idle": "2026-02-20T15:09:50.373536Z",
     "shell.execute_reply": "2026-02-20T15:09:50.372552Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading google/gemma-3-4b-it...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4b5f02e99e343a0b7ec0a7c8f3fa752",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/883 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exp 03: Position vs Attention Isolation\n",
      "N: 400, Model: google/gemma-3-4b-it\n",
      "DEVICE: cuda:0, dtype: torch.bfloat16\n",
      "GPU memory: 8.60 GB\n",
      "Vocab size: 262208\n",
      "Num layers: 34\n",
      "Num KV heads: 4\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Setup and model loading\n",
    "import os\n",
    "os.umask(0o000)\n",
    "\n",
    "import sys, json, time, gc, re\n",
    "import random as pyrandom\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from scipy import stats\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "sys.path.insert(0, \"../../..\")\n",
    "from lib.analysis import cohens_d\n",
    "\n",
    "SEED = 42\n",
    "N_SAMPLES = 400\n",
    "MODEL_NAME = \"google/gemma-3-4b-it\"\n",
    "\n",
    "RESULTS_DIR = Path(\"../../../results/decoder_only/exp03\")\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CHECKPOINT_PATH = RESULTS_DIR / \"checkpoint.json\"\n",
    "\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "pyrandom.seed(SEED)\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv())\n",
    "HF_TOKEN = os.environ.get(\"HF_TOKEN\")\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "print(f\"Loading {MODEL_NAME}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, token=HF_TOKEN)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME, device_map=\"auto\", torch_dtype=torch.bfloat16, token=HF_TOKEN,\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "DEVICE = next(model.parameters()).device\n",
    "\n",
    "print(f\"Exp 03: Position vs Attention Isolation\")\n",
    "print(f\"N: {N_SAMPLES}, Model: {MODEL_NAME}\")\n",
    "print(f\"DEVICE: {DEVICE}, dtype: {next(model.parameters()).dtype}\")\n",
    "print(f\"GPU memory: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "text_cfg = getattr(model.config, 'text_config', model.config)\n",
    "print(f\"Vocab size: {getattr(text_cfg, 'vocab_size', 'N/A')}\")\n",
    "print(f\"Num layers: {getattr(text_cfg, 'num_hidden_layers', 'N/A')}\")\n",
    "print(f\"Num KV heads: {getattr(text_cfg, 'num_key_value_heads', 'N/A')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65ed5862",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-20T15:09:50.378179Z",
     "iopub.status.busy": "2026-02-20T15:09:50.377347Z",
     "iopub.status.idle": "2026-02-20T15:09:50.402257Z",
     "shell.execute_reply": "2026-02-20T15:09:50.401411Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scoring functions defined.\n",
      "\n",
      "Mode verification:\n",
      "  score(doc, q, a)                     → bare (BOS in cache)\n",
      "  score(doc, q, a, remove_bos=True)    → bare_no_bos\n",
      "  score(doc, q, a, position_offset=20) → pos_offset (BOS removed)\n",
      "  score(doc, q, a, prefix_text='...')   → prefixed (BOS+prefix sliced)\n",
      "\n",
      "  newline_only prefix_ids: [2, 107] (2 tokens)\n",
      "  Token names: ['<bos>', '\\n']\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: KV cache helpers and unified scoring function\n",
    "\n",
    "def slice_kv_cache(cache, start_idx):\n",
    "    # Remove first start_idx entries from KV cache.\n",
    "    from transformers import DynamicCache\n",
    "    if isinstance(cache, DynamicCache):\n",
    "        sliced = DynamicCache()\n",
    "        for i in range(len(cache.layers)):\n",
    "            k = cache.layers[i].keys[:, :, start_idx:, :]\n",
    "            v = cache.layers[i].values[:, :, start_idx:, :]\n",
    "            sliced.update(k, v, i)\n",
    "        return sliced\n",
    "    else:\n",
    "        return tuple(\n",
    "            (k[:, :, start_idx:, :], v[:, :, start_idx:, :])\n",
    "            for k, v in cache\n",
    "        )\n",
    "\n",
    "\n",
    "def score(doc_text, query_text, answer_text, prefix_text=None,\n",
    "          position_offset=0, remove_bos=False):\n",
    "    # Score NLL of answer tokens using two-phase KV cache approach.\n",
    "    #\n",
    "    # Three modes:\n",
    "    #   1. prefix_text given: [BOS + prefix + \\n + doc], slice prefix+BOS from cache.\n",
    "    #   2. position_offset > 0: doc at offset RoPE positions, no prefix tokens.\n",
    "    #      BOS at position 0, doc at offset..offset+D. BOS always removed.\n",
    "    #   3. remove_bos=True: like bare but BOS sliced from cache.\n",
    "    # Default (all False/0): bare with BOS in cache.\n",
    "\n",
    "    # --- Phase A: Conditioning ---\n",
    "    if prefix_text is not None:\n",
    "        # Standard prefixed mode\n",
    "        prefix_ids = tokenizer(prefix_text + \"\\n\", add_special_tokens=True,\n",
    "                               truncation=True, max_length=512).input_ids\n",
    "        doc_ids = tokenizer(doc_text, add_special_tokens=False,\n",
    "                            truncation=True, max_length=1536).input_ids\n",
    "        cond_ids = prefix_ids + doc_ids\n",
    "        slice_start = len(prefix_ids)\n",
    "        custom_pos = None\n",
    "        # Last token at position len(cond_ids)-1, Phase B starts after\n",
    "        phase_b_start = len(cond_ids)\n",
    "\n",
    "    elif position_offset > 0:\n",
    "        # Position offset mode: no prefix, doc at offset positions\n",
    "        cond_ids = tokenizer(doc_text, add_special_tokens=True,\n",
    "                             truncation=True, max_length=2048).input_ids\n",
    "        slice_start = 1  # always remove BOS in this mode\n",
    "        # BOS at position 0, doc tokens at offset..offset+D\n",
    "        n_doc = len(cond_ids) - 1  # exclude BOS\n",
    "        pos_list = [0] + list(range(position_offset, position_offset + n_doc))\n",
    "        custom_pos = torch.tensor([pos_list], dtype=torch.long, device=DEVICE)\n",
    "        # Last doc token at position offset + n_doc - 1\n",
    "        phase_b_start = position_offset + n_doc\n",
    "\n",
    "    else:\n",
    "        # Bare mode (optionally remove BOS)\n",
    "        cond_ids = tokenizer(doc_text, add_special_tokens=True,\n",
    "                             truncation=True, max_length=2048).input_ids\n",
    "        slice_start = 1 if remove_bos else 0\n",
    "        custom_pos = None\n",
    "        phase_b_start = len(cond_ids)\n",
    "\n",
    "    cond_tensor = torch.tensor([cond_ids], dtype=torch.long, device=DEVICE)\n",
    "\n",
    "    fwd_kwargs = {'input_ids': cond_tensor, 'use_cache': True}\n",
    "    if custom_pos is not None:\n",
    "        fwd_kwargs['position_ids'] = custom_pos\n",
    "\n",
    "    with torch.no_grad():\n",
    "        phase_a = model(**fwd_kwargs)\n",
    "\n",
    "    cache = phase_a.past_key_values\n",
    "    del phase_a\n",
    "\n",
    "    if slice_start > 0:\n",
    "        cache = slice_kv_cache(cache, slice_start)\n",
    "\n",
    "    # --- Phase B: Inference with query + answer ---\n",
    "    query_part_ids = tokenizer(\"\\n\" + query_text + \"\\n\",\n",
    "                               add_special_tokens=False).input_ids\n",
    "    answer_ids = tokenizer(answer_text, add_special_tokens=False,\n",
    "                           truncation=True, max_length=256).input_ids\n",
    "\n",
    "    if not answer_ids:\n",
    "        del cache\n",
    "        return 0.0\n",
    "\n",
    "    phase_b_ids = query_part_ids + answer_ids\n",
    "    phase_b_tensor = torch.tensor([phase_b_ids], dtype=torch.long, device=DEVICE)\n",
    "\n",
    "    pos_ids = torch.arange(phase_b_start, phase_b_start + len(phase_b_ids),\n",
    "                           device=DEVICE).unsqueeze(0)\n",
    "    cache_position = torch.arange(phase_b_start, phase_b_start + len(phase_b_ids),\n",
    "                                  device=DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        phase_b = model(\n",
    "            input_ids=phase_b_tensor,\n",
    "            past_key_values=cache,\n",
    "            position_ids=pos_ids,\n",
    "            cache_position=cache_position,\n",
    "            use_cache=False,\n",
    "        )\n",
    "\n",
    "    logits = phase_b.logits\n",
    "    n_query_part = len(query_part_ids)\n",
    "    n_answer = len(answer_ids)\n",
    "\n",
    "    answer_logits = logits[0, n_query_part - 1 : n_query_part - 1 + n_answer, :]\n",
    "    answer_targets = torch.tensor(answer_ids, dtype=torch.long, device=DEVICE)\n",
    "\n",
    "    log_probs = F.log_softmax(answer_logits, dim=-1)\n",
    "    token_log_probs = log_probs.gather(1, answer_targets.unsqueeze(1)).squeeze(1)\n",
    "    mean_nll = -token_log_probs.mean().item()\n",
    "\n",
    "    del cache, phase_b, logits, log_probs\n",
    "    return mean_nll\n",
    "\n",
    "\n",
    "def score_full_sequence(doc_text, query_text, answer_text):\n",
    "    # Single-pass scoring for validation (bare equivalent).\n",
    "    doc_ids = tokenizer(doc_text, add_special_tokens=True,\n",
    "                        truncation=True, max_length=2048).input_ids\n",
    "    query_part_ids = tokenizer(\"\\n\" + query_text + \"\\n\",\n",
    "                               add_special_tokens=False).input_ids\n",
    "    answer_ids = tokenizer(answer_text, add_special_tokens=False,\n",
    "                           truncation=True, max_length=256).input_ids\n",
    "\n",
    "    if not answer_ids:\n",
    "        return 0.0\n",
    "\n",
    "    all_ids = doc_ids + query_part_ids + answer_ids\n",
    "    input_tensor = torch.tensor([all_ids], dtype=torch.long, device=DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_tensor, use_cache=False)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    n_doc = len(doc_ids)\n",
    "    n_query = len(query_part_ids)\n",
    "    n_answer = len(answer_ids)\n",
    "\n",
    "    start = n_doc + n_query - 1\n",
    "    answer_logits = logits[0, start : start + n_answer, :]\n",
    "    answer_targets = torch.tensor(answer_ids, dtype=torch.long, device=DEVICE)\n",
    "\n",
    "    log_probs = F.log_softmax(answer_logits, dim=-1)\n",
    "    token_log_probs = log_probs.gather(1, answer_targets.unsqueeze(1)).squeeze(1)\n",
    "    mean_nll = -token_log_probs.mean().item()\n",
    "\n",
    "    del outputs, logits, log_probs\n",
    "    return mean_nll\n",
    "\n",
    "\n",
    "print(\"Scoring functions defined.\")\n",
    "print(f\"\\nMode verification:\")\n",
    "print(f\"  score(doc, q, a)                     → bare (BOS in cache)\")\n",
    "print(f\"  score(doc, q, a, remove_bos=True)    → bare_no_bos\")\n",
    "print(f\"  score(doc, q, a, position_offset=20) → pos_offset (BOS removed)\")\n",
    "print(f\"  score(doc, q, a, prefix_text='...')   → prefixed (BOS+prefix sliced)\")\n",
    "\n",
    "# Show how newline_only tokenizes\n",
    "nl_ids = tokenizer(\"\\n\", add_special_tokens=True).input_ids\n",
    "print(f\"\\n  newline_only prefix_ids: {nl_ids} ({len(nl_ids)} tokens)\")\n",
    "print(f\"  Token names: {[tokenizer.decode([t]) for t in nl_ids]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2d37467",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-20T15:09:50.405687Z",
     "iopub.status.busy": "2026-02-20T15:09:50.405405Z",
     "iopub.status.idle": "2026-02-20T15:09:51.728468Z",
     "shell.execute_reply": "2026-02-20T15:09:51.727513Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MS MARCO v1.1 validation...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total candidates: 1200\n",
      "Loaded 400 samples\n",
      "Mean passage words: 73\n",
      "Mean query words: 5.9\n",
      "\n",
      "Prefix token counts (sample 0):\n",
      "  newline_only       2 tok  S=2     text: ''\n",
      "  single_word        3 tok  S=3     text: 'an'\n",
      "  random_3w          7 tok  S=7     text: 'Chronic value dermis.'\n",
      "  random_5w          8 tok  S=8     text: 'hide systems. agreement is the'\n",
      "  random_15w        20 tok  S=20    text: 'as security phone into or amplitude most'\n",
      "\n",
      "  pos_offset_4:   S=4   (matches ~single_word)\n",
      "  pos_offset_20:  S=20  (matches ~random_15w)\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Load MS MARCO data and generate per-sample prefixes\n",
    "from lib.data import count_words\n",
    "from datasets import load_dataset\n",
    "\n",
    "print(\"Loading MS MARCO v1.1 validation...\")\n",
    "ds = load_dataset(\"microsoft/ms_marco\", \"v1.1\", split=\"validation\")\n",
    "\n",
    "all_candidates = []\n",
    "for item in ds:\n",
    "    if len(all_candidates) >= 3 * N_SAMPLES:\n",
    "        break\n",
    "    passages = item.get('passages', {})\n",
    "    ptexts = passages.get('passage_text', [])\n",
    "    is_sel = passages.get('is_selected', [])\n",
    "    query = item.get('query', '')\n",
    "    answers = item.get('answers', [])\n",
    "    well_formed = item.get('wellFormedAnswers', [])\n",
    "    answer = None\n",
    "    if well_formed and len(well_formed) > 0 and well_formed[0] not in ('[]', ''):\n",
    "        answer = well_formed[0]\n",
    "    elif answers and len(answers) > 0 and answers[0] != 'No Answer Present.':\n",
    "        answer = answers[0]\n",
    "    if not answer:\n",
    "        continue\n",
    "    for pt, sel in zip(ptexts, is_sel):\n",
    "        wc = count_words(pt)\n",
    "        if sel == 1 and 30 <= wc <= 300:\n",
    "            all_candidates.append({\n",
    "                'passage': pt, 'query': query, 'answer': answer,\n",
    "                'word_count': wc,\n",
    "            })\n",
    "            break\n",
    "\n",
    "print(f\"Total candidates: {len(all_candidates)}\")\n",
    "np.random.seed(SEED)\n",
    "indices = np.random.permutation(len(all_candidates))\n",
    "samples = [all_candidates[i] for i in indices[:N_SAMPLES]]\n",
    "del ds, all_candidates\n",
    "gc.collect()\n",
    "\n",
    "# Build word pool from passage text\n",
    "all_words = []\n",
    "for s in samples:\n",
    "    all_words.extend(s['passage'].split())\n",
    "pyrandom.seed(SEED + 99)\n",
    "pyrandom.shuffle(all_words)\n",
    "word_pool = all_words\n",
    "\n",
    "# Generate per-sample prefixes at different lengths\n",
    "for i, s in enumerate(samples):\n",
    "    pool_offset = i * 50\n",
    "\n",
    "    # single_word: 1 word from pool\n",
    "    s['single_word'] = word_pool[pool_offset]\n",
    "\n",
    "    # random_3w: 3 words\n",
    "    s['random_3w'] = \" \".join(word_pool[pool_offset + 1:pool_offset + 4])\n",
    "\n",
    "    # random_5w: 5 words\n",
    "    s['random_5w'] = \" \".join(word_pool[pool_offset + 4:pool_offset + 9])\n",
    "\n",
    "    # random_15w: 15 words\n",
    "    s['random_15w'] = \" \".join(word_pool[pool_offset + 9:pool_offset + 24])\n",
    "\n",
    "print(f\"Loaded {len(samples)} samples\")\n",
    "print(f\"Mean passage words: {np.mean([s['word_count'] for s in samples]):.0f}\")\n",
    "print(f\"Mean query words: {np.mean([len(s['query'].split()) for s in samples]):.1f}\")\n",
    "\n",
    "# Show actual token counts for each prefix type\n",
    "print(f\"\\nPrefix token counts (sample 0):\")\n",
    "for name, text in [('newline_only', ''),\n",
    "                    ('single_word', samples[0]['single_word']),\n",
    "                    ('random_3w', samples[0]['random_3w']),\n",
    "                    ('random_5w', samples[0]['random_5w']),\n",
    "                    ('random_15w', samples[0]['random_15w'])]:\n",
    "    prefix_ids = tokenizer(text + \"\\n\", add_special_tokens=True).input_ids\n",
    "    print(f\"  {name:<16} {len(prefix_ids):>3} tok  S={len(prefix_ids):<4}  \"\n",
    "          f\"text: {repr(text[:40])}\")\n",
    "\n",
    "print(f\"\\n  pos_offset_4:   S=4   (matches ~single_word)\")\n",
    "print(f\"  pos_offset_20:  S=20  (matches ~random_15w)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5803e8ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-20T15:09:51.732108Z",
     "iopub.status.busy": "2026-02-20T15:09:51.731634Z",
     "iopub.status.idle": "2026-02-20T15:09:55.626602Z",
     "shell.execute_reply": "2026-02-20T15:09:55.625675Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "VALIDATION\n",
      "======================================================================\n",
      "\n",
      "--- Bare: cached vs full-sequence ---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Sample 0: cached=0.738281, full=0.746094, diff=0.007812 [OK]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Sample 1: cached=0.882812, full=0.878906, diff=0.003906 [OK]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Sample 2: cached=1.851562, full=1.859375, diff=0.007812 [OK]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Sample 3: cached=0.691406, full=0.699219, diff=0.007812 [OK]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Sample 4: cached=4.843750, full=4.781250, diff=0.062500 [~]\n",
      "  PASSED (max diff = 0.062500, bf16 rounding)\n",
      "\n",
      "--- All modes on sample 0 ---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  bare                 NLL = 0.738281\n",
      "  bare_no_bos          NLL = 0.632812  delta = +0.1055\n",
      "  pos_offset_4         NLL = 0.425781  delta = +0.3125\n",
      "  pos_offset_20        NLL = 0.664062  delta = +0.0742\n",
      "  newline_only         NLL = 0.394531  delta = +0.3438\n",
      "  single_word          NLL = 0.332031  delta = +0.4062\n",
      "  random_3w            NLL = 0.726562  delta = +0.0117\n",
      "  random_5w            NLL = 0.726562  delta = +0.0117\n",
      "  random_15w           NLL = 1.023438  delta = -0.2852\n",
      "  oracle               NLL = 0.859375  delta = -0.1211\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Validate all scoring modes\n",
    "print(\"=\" * 70)\n",
    "print(\"VALIDATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# 1. Bare cached vs full-sequence\n",
    "print(\"\\n--- Bare: cached vs full-sequence ---\")\n",
    "max_diff = 0.0\n",
    "for i in range(5):\n",
    "    s = samples[i]\n",
    "    nll_cached = score(s['passage'], s['query'], s['answer'])\n",
    "    nll_full = score_full_sequence(s['passage'], s['query'], s['answer'])\n",
    "    diff = abs(nll_cached - nll_full)\n",
    "    max_diff = max(max_diff, diff)\n",
    "    status = \"OK\" if diff < 0.01 else \"~\"\n",
    "    print(f\"  Sample {i}: cached={nll_cached:.6f}, full={nll_full:.6f}, \"\n",
    "          f\"diff={diff:.6f} [{status}]\")\n",
    "if max_diff < 0.1:\n",
    "    print(f\"  PASSED (max diff = {max_diff:.6f}, bf16 rounding)\")\n",
    "else:\n",
    "    print(f\"  WARNING: max diff = {max_diff:.6f}\")\n",
    "\n",
    "# 2. All modes run without error on sample 0\n",
    "print(f\"\\n--- All modes on sample 0 ---\")\n",
    "s = samples[0]\n",
    "nll_bare = score(s['passage'], s['query'], s['answer'])\n",
    "nll_no_bos = score(s['passage'], s['query'], s['answer'], remove_bos=True)\n",
    "nll_pos4 = score(s['passage'], s['query'], s['answer'], position_offset=4)\n",
    "nll_pos20 = score(s['passage'], s['query'], s['answer'], position_offset=20)\n",
    "nll_nl = score(s['passage'], s['query'], s['answer'], prefix_text=\"\")\n",
    "nll_1w = score(s['passage'], s['query'], s['answer'], prefix_text=s['single_word'])\n",
    "nll_3w = score(s['passage'], s['query'], s['answer'], prefix_text=s['random_3w'])\n",
    "nll_5w = score(s['passage'], s['query'], s['answer'], prefix_text=s['random_5w'])\n",
    "nll_15w = score(s['passage'], s['query'], s['answer'], prefix_text=s['random_15w'])\n",
    "nll_oracle = score(s['passage'], s['query'], s['answer'], prefix_text=s['query'])\n",
    "\n",
    "print(f\"  {'bare':<20} NLL = {nll_bare:.6f}\")\n",
    "print(f\"  {'bare_no_bos':<20} NLL = {nll_no_bos:.6f}  delta = {nll_bare - nll_no_bos:+.4f}\")\n",
    "print(f\"  {'pos_offset_4':<20} NLL = {nll_pos4:.6f}  delta = {nll_bare - nll_pos4:+.4f}\")\n",
    "print(f\"  {'pos_offset_20':<20} NLL = {nll_pos20:.6f}  delta = {nll_bare - nll_pos20:+.4f}\")\n",
    "print(f\"  {'newline_only':<20} NLL = {nll_nl:.6f}  delta = {nll_bare - nll_nl:+.4f}\")\n",
    "print(f\"  {'single_word':<20} NLL = {nll_1w:.6f}  delta = {nll_bare - nll_1w:+.4f}\")\n",
    "print(f\"  {'random_3w':<20} NLL = {nll_3w:.6f}  delta = {nll_bare - nll_3w:+.4f}\")\n",
    "print(f\"  {'random_5w':<20} NLL = {nll_5w:.6f}  delta = {nll_bare - nll_5w:+.4f}\")\n",
    "print(f\"  {'random_15w':<20} NLL = {nll_15w:.6f}  delta = {nll_bare - nll_15w:+.4f}\")\n",
    "print(f\"  {'oracle':<20} NLL = {nll_oracle:.6f}  delta = {nll_bare - nll_oracle:+.4f}\")\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8efd01ac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-20T15:09:55.630411Z",
     "iopub.status.busy": "2026-02-20T15:09:55.630136Z",
     "iopub.status.idle": "2026-02-20T15:23:03.307068Z",
     "shell.execute_reply": "2026-02-20T15:23:03.305922Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "SCORING ALL CONDITIONS\n",
      "======================================================================\n",
      "Starting fresh: 10 conditions x 400 samples\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bfa4e3be8704b809fc3ff255e9c58a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Scoring:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 20/400 | 0.7m | ETA 12.4m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 40/400 | 1.3m | ETA 11.8m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 60/400 | 2.0m | ETA 11.1m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 80/400 | 2.6m | ETA 10.5m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 100/400 | 3.3m | ETA 9.8m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 120/400 | 3.9m | ETA 9.2m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 140/400 | 4.6m | ETA 8.5m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 160/400 | 5.2m | ETA 7.9m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 180/400 | 5.9m | ETA 7.2m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 200/400 | 6.5m | ETA 6.5m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 220/400 | 7.2m | ETA 5.9m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 240/400 | 7.9m | ETA 5.2m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 260/400 | 8.5m | ETA 4.6m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 280/400 | 9.2m | ETA 3.9m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 300/400 | 9.8m | ETA 3.3m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 320/400 | 10.5m | ETA 2.6m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 340/400 | 11.2m | ETA 2.0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 360/400 | 11.8m | ETA 1.3m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 380/400 | 12.5m | ETA 0.7m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 400/400 | 13.1m | ETA 0.0m\n",
      "\n",
      "Scoring complete: 400 samples, 10 conditions in 13.1 min\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Scoring loop — 10 conditions x 400 samples\n",
    "print(\"=\" * 70)\n",
    "print(\"SCORING ALL CONDITIONS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "COND_NAMES = [\n",
    "    'bare', 'oracle', 'bare_no_bos',\n",
    "    'pos_offset_4', 'pos_offset_20',\n",
    "    'newline_only', 'single_word', 'random_3w', 'random_5w', 'random_15w',\n",
    "]\n",
    "\n",
    "results = []\n",
    "start_idx = 0\n",
    "\n",
    "if CHECKPOINT_PATH.exists():\n",
    "    ckpt = json.loads(CHECKPOINT_PATH.read_text())\n",
    "    if ckpt.get('n_total') == N_SAMPLES and len(ckpt.get('results', [])) > 0:\n",
    "        saved_queries = [r['query'][:50] for r in ckpt['results']]\n",
    "        current_queries = [s['query'][:50] for s in samples[:len(saved_queries)]]\n",
    "        if saved_queries == current_queries:\n",
    "            results = ckpt['results']\n",
    "            start_idx = len(results)\n",
    "            print(f\"Resuming from checkpoint: {start_idx}/{N_SAMPLES}\")\n",
    "\n",
    "if start_idx == 0:\n",
    "    print(f\"Starting fresh: {len(COND_NAMES)} conditions x {N_SAMPLES} samples\")\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "for i in tqdm(range(start_idx, N_SAMPLES), initial=start_idx, total=N_SAMPLES,\n",
    "              desc=\"Scoring\"):\n",
    "    s = samples[i]\n",
    "    query = s['query']\n",
    "    passage = s['passage']\n",
    "    answer = s['answer']\n",
    "\n",
    "    result = {\n",
    "        'query': query,\n",
    "        'answer': answer,\n",
    "        'passage_words': s['word_count'],\n",
    "        'query_words': len(query.split()),\n",
    "    }\n",
    "\n",
    "    # 1. bare\n",
    "    result['nll_bare'] = score(passage, query, answer)\n",
    "\n",
    "    # 2. oracle\n",
    "    result['nll_oracle'] = score(passage, query, answer, prefix_text=query)\n",
    "\n",
    "    # 3. bare_no_bos\n",
    "    result['nll_bare_no_bos'] = score(passage, query, answer, remove_bos=True)\n",
    "\n",
    "    # 4. pos_offset_4 (S=4, matches ~single_word prefix length)\n",
    "    result['nll_pos_offset_4'] = score(passage, query, answer, position_offset=4)\n",
    "\n",
    "    # 5. pos_offset_20 (S=20, matches ~random_15w prefix length)\n",
    "    result['nll_pos_offset_20'] = score(passage, query, answer, position_offset=20)\n",
    "\n",
    "    # 6. newline_only (prefix_text=\"\" → BOS + \\n only)\n",
    "    result['nll_newline_only'] = score(passage, query, answer, prefix_text=\"\")\n",
    "\n",
    "    # 7. single_word\n",
    "    result['nll_single_word'] = score(passage, query, answer,\n",
    "                                      prefix_text=s['single_word'])\n",
    "\n",
    "    # 8. random_3w\n",
    "    result['nll_random_3w'] = score(passage, query, answer,\n",
    "                                    prefix_text=s['random_3w'])\n",
    "\n",
    "    # 9. random_5w\n",
    "    result['nll_random_5w'] = score(passage, query, answer,\n",
    "                                    prefix_text=s['random_5w'])\n",
    "\n",
    "    # 10. random_15w\n",
    "    result['nll_random_15w'] = score(passage, query, answer,\n",
    "                                     prefix_text=s['random_15w'])\n",
    "\n",
    "    results.append(result)\n",
    "\n",
    "    if (i + 1) % 20 == 0 or i == N_SAMPLES - 1:\n",
    "        ckpt = {\n",
    "            'n_total': N_SAMPLES,\n",
    "            'results': results,\n",
    "            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        }\n",
    "        CHECKPOINT_PATH.write_text(json.dumps(ckpt))\n",
    "        elapsed = time.time() - t0\n",
    "        done = i - start_idx + 1\n",
    "        eta = (N_SAMPLES - i - 1) * elapsed / done if done > 0 else 0\n",
    "        tqdm.write(f\"  Checkpoint {i+1}/{N_SAMPLES} | {elapsed/60:.1f}m | ETA {eta/60:.1f}m\")\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "elapsed = time.time() - t0\n",
    "print(f\"\\nScoring complete: {len(results)} samples, \"\n",
    "      f\"{len(COND_NAMES)} conditions in {elapsed/60:.1f} min\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "beb1b437",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-20T15:23:03.310741Z",
     "iopub.status.busy": "2026-02-20T15:23:03.310438Z",
     "iopub.status.idle": "2026-02-20T15:23:03.332500Z",
     "shell.execute_reply": "2026-02-20T15:23:03.331637Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "RESULTS (N=400)\n",
      "======================================================================\n",
      "\n",
      "  Condition                 NLL    vs bare        d     Win%            p   sig   Recovery\n",
      "  ------------------------------------------------------------------------------------------\n",
      "  bare                   1.6022         --       --       --           --    --         --\n",
      "  oracle                 0.9044    +0.6977   +0.428    73.8%     2.32e-16   ***     100.0%\n",
      "  bare_no_bos            1.5026    +0.0995   +0.069    52.0%     1.66e-01    ns      14.3%\n",
      "  pos_offset_4           0.6406    +0.9616   +0.517    75.5%     2.28e-22   ***     137.8%\n",
      "  pos_offset_20          0.9794    +0.6228   +0.300    61.3%     4.37e-09   ***      89.3%\n",
      "  newline_only           0.6538    +0.9484   +0.551    80.8%     7.28e-25   ***     135.9%\n",
      "  single_word            0.5647    +1.0375   +0.615    82.5%     1.02e-29   ***     148.7%\n",
      "  random_3w              0.6411    +0.9611   +0.565    83.2%     7.51e-26   ***     137.7%\n",
      "  random_5w              0.7031    +0.8991   +0.544    79.5%     2.39e-24   ***     128.9%\n",
      "  random_15w             0.9680    +0.6342   +0.344    64.0%     2.47e-11   ***      90.9%\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Results table\n",
    "print(\"=\" * 70)\n",
    "print(f\"RESULTS (N={len(results)})\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Extract arrays\n",
    "arrays = {}\n",
    "for name in COND_NAMES:\n",
    "    arrays[name] = np.array([r[f'nll_{name}'] for r in results])\n",
    "\n",
    "bare = arrays['bare']\n",
    "oracle = arrays['oracle']\n",
    "oracle_delta_mean = (bare - oracle).mean()\n",
    "\n",
    "print(f\"\\n  {'Condition':<20} {'NLL':>8} {'vs bare':>10} {'d':>8} \"\n",
    "      f\"{'Win%':>8} {'p':>12} {'sig':>5} {'Recovery':>10}\")\n",
    "print(f\"  {'-'*90}\")\n",
    "\n",
    "analysis = {}\n",
    "for name in COND_NAMES:\n",
    "    nlls = arrays[name]\n",
    "    mean_nll = nlls.mean()\n",
    "\n",
    "    if name == 'bare':\n",
    "        print(f\"  {name:<20} {mean_nll:>8.4f} {'--':>10} {'--':>8} \"\n",
    "              f\"{'--':>8} {'--':>12} {'--':>5} {'--':>10}\")\n",
    "        analysis[name] = {'mean_nll': float(mean_nll)}\n",
    "    else:\n",
    "        diff = bare - nlls\n",
    "        d = cohens_d(diff)\n",
    "        win_pct = 100 * np.mean(diff > 0)\n",
    "        _, p_val = stats.ttest_1samp(diff, 0)\n",
    "        sig = '***' if p_val < 0.001 else '**' if p_val < 0.01 else '*' if p_val < 0.05 else 'ns'\n",
    "\n",
    "        if oracle_delta_mean > 0:\n",
    "            recovery = diff.mean() / oracle_delta_mean * 100\n",
    "            rec_str = f\"{recovery:>9.1f}%\"\n",
    "        else:\n",
    "            recovery = float('nan')\n",
    "            rec_str = \"n/a\"\n",
    "\n",
    "        print(f\"  {name:<20} {mean_nll:>8.4f} {diff.mean():>+10.4f} {d:>+8.3f} \"\n",
    "              f\"{win_pct:>7.1f}% {p_val:>12.2e} {sig:>5} {rec_str:>10}\")\n",
    "        analysis[name] = {\n",
    "            'mean_nll': float(mean_nll), 'delta': float(diff.mean()),\n",
    "            'd': float(d), 'win_pct': float(win_pct), 'p': float(p_val),\n",
    "            'recovery': float(recovery) if not np.isnan(recovery) else None,\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12fd392a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-20T15:23:03.336976Z",
     "iopub.status.busy": "2026-02-20T15:23:03.336690Z",
     "iopub.status.idle": "2026-02-20T15:23:03.376811Z",
     "shell.execute_reply": "2026-02-20T15:23:03.375887Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "MECHANISM ISOLATION\n",
      "======================================================================\n",
      "\n",
      "============================================================\n",
      "  THREE-FACTOR DECOMPOSITION\n",
      "  Total structural effect: bare → random_15w\n",
      "============================================================\n",
      "\n",
      "  TOTAL: NLL delta = +0.6342, d = +0.3435, p = 2.47e-11\n",
      "\n",
      "  Factor 1: BOS removal  (bare → bare_no_bos)\n",
      "    NLL delta = +0.0995  ( 15.7% of total)\n",
      "    d = +0.0693, p = 1.66e-01 (ns)\n",
      "\n",
      "  Factor 2: Position offset  (bare_no_bos → pos_offset_20)\n",
      "    NLL delta = +0.5232  ( 82.5% of total)\n",
      "    d = +0.3035, p = 2.98e-09 (***)\n",
      "\n",
      "  Factor 3: Attention enrichment  (pos_offset_20 → random_15w)\n",
      "    NLL delta = +0.0114  (  1.8% of total)\n",
      "    d = +0.0093, p = 8.52e-01 (ns)\n",
      "\n",
      "  Sum check: 0.0995 + 0.5232 + 0.0114 = 0.6342 vs total 0.6342\n",
      "\n",
      "============================================================\n",
      "  POSITION OFFSET DOSE-RESPONSE\n",
      "  (all with BOS removed, no prefix tokens)\n",
      "============================================================\n",
      "\n",
      "  Condition               S  d vs bare   d vs bare_no_bos            p\n",
      "  ----------------------------------------------------------------------\n",
      "  bare_no_bos             0    +0.0693            +0.0000          nan ns\n",
      "  pos_offset_4            4    +0.5171            +0.5921     6.21e-28 ***\n",
      "  pos_offset_20          20    +0.3001            +0.3035     2.98e-09 ***\n",
      "\n",
      "============================================================\n",
      "  SATURATION CURVE\n",
      "  (real prefix tokens: when does benefit plateau?)\n",
      "============================================================\n",
      "\n",
      "  Condition          ~words  d vs bare   recovery            p\n",
      "  --------------------------------------------------------------\n",
      "  newline_only            0    +0.5514     135.9%     7.28e-25 ***\n",
      "  single_word             1    +0.6152     148.7%     1.02e-29 ***\n",
      "  random_3w               3    +0.5646     137.7%     7.51e-26 ***\n",
      "  random_5w               5    +0.5444     128.9%     2.39e-24 ***\n",
      "  random_15w             15    +0.3435      90.9%     2.47e-11 ***\n",
      "\n",
      "  Pairwise (does adding more words help beyond previous level?):\n",
      "  newline_only     → single_word      d = +0.1391 (**)\n",
      "  single_word      → random_3w        d = -0.0964 (ns)\n",
      "  random_3w        → random_5w        d = -0.0815 (ns)\n",
      "  random_5w        → random_15w       d = -0.2971 (***)\n",
      "\n",
      "============================================================\n",
      "  POSITION-MATCHED COMPARISONS\n",
      "  (same approximate offset, with vs without prefix tokens)\n",
      "============================================================\n",
      "\n",
      "  S≈~4: pos_offset_4 vs single_word\n",
      "    pos_offset NLL = 0.6406\n",
      "    prefix     NLL = 0.5647\n",
      "    diff d = +0.0660 (ns)\n",
      "    → Prefix tokens ADD value beyond position offset alone\n",
      "\n",
      "  S≈~20: pos_offset_20 vs random_15w\n",
      "    pos_offset NLL = 0.9794\n",
      "    prefix     NLL = 0.9680\n",
      "    diff d = +0.0093 (ns)\n",
      "    → No significant difference — position alone explains it\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Mechanism isolation — three-factor decomposition\n",
    "print(\"=\" * 70)\n",
    "print(\"MECHANISM ISOLATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# -----------------------------------------------------------------------\n",
    "# Three-factor decomposition of the total structural effect\n",
    "# Total: bare → random_15w (the effect of a 15-word random prefix)\n",
    "# Factor 1: BOS removal           (bare → bare_no_bos)\n",
    "# Factor 2: Position offset       (bare_no_bos → pos_offset_20)\n",
    "# Factor 3: Attention enrichment  (pos_offset_20 → random_15w)\n",
    "# -----------------------------------------------------------------------\n",
    "\n",
    "total = bare - arrays['random_15w']\n",
    "f1 = bare - arrays['bare_no_bos']\n",
    "f2 = arrays['bare_no_bos'] - arrays['pos_offset_20']\n",
    "f3 = arrays['pos_offset_20'] - arrays['random_15w']\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"  THREE-FACTOR DECOMPOSITION\")\n",
    "print(f\"  Total structural effect: bare → random_15w\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "total_mean = total.mean()\n",
    "total_d = cohens_d(total)\n",
    "_, total_p = stats.ttest_1samp(total, 0)\n",
    "print(f\"\\n  TOTAL: NLL delta = {total_mean:+.4f}, d = {total_d:+.4f}, \"\n",
    "      f\"p = {total_p:.2e}\")\n",
    "\n",
    "for label, factor, explanation in [\n",
    "    (\"Factor 1: BOS removal\", f1, \"bare → bare_no_bos\"),\n",
    "    (\"Factor 2: Position offset\", f2, \"bare_no_bos → pos_offset_20\"),\n",
    "    (\"Factor 3: Attention enrichment\", f3, \"pos_offset_20 → random_15w\"),\n",
    "]:\n",
    "    f_mean = factor.mean()\n",
    "    f_d = cohens_d(factor)\n",
    "    _, f_p = stats.ttest_1samp(factor, 0)\n",
    "    f_sig = '***' if f_p < 0.001 else '**' if f_p < 0.01 else '*' if f_p < 0.05 else 'ns'\n",
    "    if total_mean > 0:\n",
    "        pct = f_mean / total_mean * 100\n",
    "    else:\n",
    "        pct = 0\n",
    "    print(f\"\\n  {label}  ({explanation})\")\n",
    "    print(f\"    NLL delta = {f_mean:+.4f}  ({pct:>5.1f}% of total)\")\n",
    "    print(f\"    d = {f_d:+.4f}, p = {f_p:.2e} ({f_sig})\")\n",
    "\n",
    "# Sum check\n",
    "sum_factors = f1.mean() + f2.mean() + f3.mean()\n",
    "print(f\"\\n  Sum check: {f1.mean():.4f} + {f2.mean():.4f} + {f3.mean():.4f} \"\n",
    "      f\"= {sum_factors:.4f} vs total {total_mean:.4f}\")\n",
    "\n",
    "# -----------------------------------------------------------------------\n",
    "# Position offset dose-response\n",
    "# -----------------------------------------------------------------------\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"  POSITION OFFSET DOSE-RESPONSE\")\n",
    "print(f\"  (all with BOS removed, no prefix tokens)\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "print(f\"\\n  {'Condition':<20} {'S':>4} {'d vs bare':>10} {'d vs bare_no_bos':>18} {'p':>12}\")\n",
    "print(f\"  {'-'*70}\")\n",
    "for name, S in [('bare_no_bos', 0), ('pos_offset_4', 4), ('pos_offset_20', 20)]:\n",
    "    diff_vs_bare = bare - arrays[name]\n",
    "    diff_vs_nobos = arrays['bare_no_bos'] - arrays[name]\n",
    "    d_b = cohens_d(diff_vs_bare)\n",
    "    d_nb = cohens_d(diff_vs_nobos)\n",
    "    _, p_nb = stats.ttest_1samp(diff_vs_nobos, 0)\n",
    "    sig = '***' if p_nb < 0.001 else '**' if p_nb < 0.01 else '*' if p_nb < 0.05 else 'ns'\n",
    "    print(f\"  {name:<20} {S:>4} {d_b:>+10.4f} {d_nb:>+18.4f} {p_nb:>12.2e} {sig}\")\n",
    "\n",
    "# -----------------------------------------------------------------------\n",
    "# Saturation curve — where does the effect plateau?\n",
    "# -----------------------------------------------------------------------\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"  SATURATION CURVE\")\n",
    "print(f\"  (real prefix tokens: when does benefit plateau?)\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "print(f\"\\n  {'Condition':<16} {'~words':>8} {'d vs bare':>10} {'recovery':>10} {'p':>12}\")\n",
    "print(f\"  {'-'*62}\")\n",
    "for name, nw in [('newline_only', '0'),\n",
    "                  ('single_word', '1'),\n",
    "                  ('random_3w', '3'),\n",
    "                  ('random_5w', '5'),\n",
    "                  ('random_15w', '15')]:\n",
    "    diff = bare - arrays[name]\n",
    "    d = cohens_d(diff)\n",
    "    _, p = stats.ttest_1samp(diff, 0)\n",
    "    sig = '***' if p < 0.001 else '**' if p < 0.01 else '*' if p < 0.05 else 'ns'\n",
    "    rec = diff.mean() / oracle_delta_mean * 100 if oracle_delta_mean > 0 else 0\n",
    "    print(f\"  {name:<16} {nw:>8} {d:>+10.4f} {rec:>9.1f}% {p:>12.2e} {sig}\")\n",
    "\n",
    "# Pairwise: does adding more words help significantly?\n",
    "print(f\"\\n  Pairwise (does adding more words help beyond previous level?):\")\n",
    "pairs = [('newline_only', 'single_word'),\n",
    "         ('single_word', 'random_3w'),\n",
    "         ('random_3w', 'random_5w'),\n",
    "         ('random_5w', 'random_15w')]\n",
    "for a, b in pairs:\n",
    "    diff_ab = arrays[a] - arrays[b]  # positive = b better\n",
    "    d_ab = cohens_d(diff_ab)\n",
    "    _, p_ab = stats.ttest_1samp(diff_ab, 0)\n",
    "    sig = '***' if p_ab < 0.001 else '**' if p_ab < 0.01 else '*' if p_ab < 0.05 else 'ns'\n",
    "    print(f\"  {a:<16} → {b:<16} d = {d_ab:+.4f} ({sig})\")\n",
    "\n",
    "# -----------------------------------------------------------------------\n",
    "# Matched comparison: pos_offset vs prefix at same approximate S\n",
    "# -----------------------------------------------------------------------\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"  POSITION-MATCHED COMPARISONS\")\n",
    "print(f\"  (same approximate offset, with vs without prefix tokens)\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "for pos_name, prefix_name, approx_s in [\n",
    "    ('pos_offset_4', 'single_word', '~4'),\n",
    "    ('pos_offset_20', 'random_15w', '~20'),\n",
    "]:\n",
    "    diff = arrays[pos_name] - arrays[prefix_name]  # positive = prefix better\n",
    "    d = cohens_d(diff)\n",
    "    _, p = stats.ttest_1samp(diff, 0)\n",
    "    sig = '***' if p < 0.001 else '**' if p < 0.01 else '*' if p < 0.05 else 'ns'\n",
    "    print(f\"\\n  S≈{approx_s}: {pos_name} vs {prefix_name}\")\n",
    "    print(f\"    pos_offset NLL = {arrays[pos_name].mean():.4f}\")\n",
    "    print(f\"    prefix     NLL = {arrays[prefix_name].mean():.4f}\")\n",
    "    print(f\"    diff d = {d:+.4f} ({sig})\")\n",
    "    if d > 0.05:\n",
    "        print(f\"    → Prefix tokens ADD value beyond position offset alone\")\n",
    "    elif d < -0.05:\n",
    "        print(f\"    → Prefix tokens HURT — position alone is better?!\")\n",
    "    else:\n",
    "        print(f\"    → No significant difference — position alone explains it\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ffd410e2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-20T15:23:03.381058Z",
     "iopub.status.busy": "2026-02-20T15:23:03.380700Z",
     "iopub.status.idle": "2026-02-20T15:23:04.018700Z",
     "shell.execute_reply": "2026-02-20T15:23:04.017636Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "VERDICT — Exp 03: Position vs Attention Isolation\n",
      "======================================================================\n",
      "\n",
      "Model: google/gemma-3-4b-it\n",
      "N: 400 samples (MS MARCO v1.1)\n",
      "\n",
      "--- Structural effect decomposition ---\n",
      "  Total (bare → random_15w): +0.6342\n",
      "  Factor 1 — BOS removal:     +0.0995 ( 15.7%)\n",
      "  Factor 2 — Position offset:  +0.5232 ( 82.5%)\n",
      "  Factor 3 — Attention enrich: +0.0114 (  1.8%)\n",
      "\n",
      "--- Interpretation ---\n",
      "  BOS removal               15.7%  (not significant, p=1.66e-01)\n",
      "  Position offset           82.5%  (SIGNIFICANT, p=2.98e-09)\n",
      "  Attention enrichment       1.8%  (not significant, p=8.52e-01)\n",
      "\n",
      "  Saturation: 1 word achieves 179% of 15-word effect\n",
      "\n",
      "--- All conditions ---\n",
      "  bare                 NLL = 1.6022\n",
      "  oracle               NLL = 0.9044  d = +0.4284 (***)\n",
      "  bare_no_bos          NLL = 1.5026  d = +0.0693 (ns)\n",
      "  pos_offset_4         NLL = 0.6406  d = +0.5171 (***)\n",
      "  pos_offset_20        NLL = 0.9794  d = +0.3001 (***)\n",
      "  newline_only         NLL = 0.6538  d = +0.5514 (***)\n",
      "  single_word          NLL = 0.5647  d = +0.6152 (***)\n",
      "  random_3w            NLL = 0.6411  d = +0.5646 (***)\n",
      "  random_5w            NLL = 0.7031  d = +0.5444 (***)\n",
      "  random_15w           NLL = 0.9680  d = +0.3435 (***)\n",
      "\n",
      "Results saved to ../../../results/decoder_only/exp03/results.json\n",
      "\n",
      "Cleaning up GPU memory...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory: 8.61 GB -> 0.01 GB\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Verdict and interpretation\n",
    "print(\"=\" * 70)\n",
    "print(\"VERDICT — Exp 03: Position vs Attention Isolation\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\nModel: {MODEL_NAME}\")\n",
    "print(f\"N: {len(results)} samples (MS MARCO v1.1)\")\n",
    "\n",
    "# Recall the three factors\n",
    "total_mean = (bare - arrays['random_15w']).mean()\n",
    "f1_mean = (bare - arrays['bare_no_bos']).mean()\n",
    "f2_mean = (arrays['bare_no_bos'] - arrays['pos_offset_20']).mean()\n",
    "f3_mean = (arrays['pos_offset_20'] - arrays['random_15w']).mean()\n",
    "\n",
    "print(f\"\\n--- Structural effect decomposition ---\")\n",
    "print(f\"  Total (bare → random_15w): {total_mean:+.4f}\")\n",
    "if total_mean > 0:\n",
    "    print(f\"  Factor 1 — BOS removal:     {f1_mean:+.4f} ({f1_mean/total_mean*100:>5.1f}%)\")\n",
    "    print(f\"  Factor 2 — Position offset:  {f2_mean:+.4f} ({f2_mean/total_mean*100:>5.1f}%)\")\n",
    "    print(f\"  Factor 3 — Attention enrich: {f3_mean:+.4f} ({f3_mean/total_mean*100:>5.1f}%)\")\n",
    "\n",
    "# Interpretation\n",
    "print(f\"\\n--- Interpretation ---\")\n",
    "_, f1_p = stats.ttest_1samp(bare - arrays['bare_no_bos'], 0)\n",
    "_, f2_p = stats.ttest_1samp(arrays['bare_no_bos'] - arrays['pos_offset_20'], 0)\n",
    "_, f3_p = stats.ttest_1samp(arrays['pos_offset_20'] - arrays['random_15w'], 0)\n",
    "\n",
    "for label, pct, p in [(\"BOS removal\", f1_mean/total_mean*100 if total_mean > 0 else 0, f1_p),\n",
    "                        (\"Position offset\", f2_mean/total_mean*100 if total_mean > 0 else 0, f2_p),\n",
    "                        (\"Attention enrichment\", f3_mean/total_mean*100 if total_mean > 0 else 0, f3_p)]:\n",
    "    sig = \"SIGNIFICANT\" if p < 0.05 else \"not significant\"\n",
    "    print(f\"  {label:<24} {pct:>5.1f}%  ({sig}, p={p:.2e})\")\n",
    "\n",
    "# Saturation\n",
    "d_1w = cohens_d(bare - arrays['single_word'])\n",
    "d_15w = cohens_d(bare - arrays['random_15w'])\n",
    "if d_15w > 0:\n",
    "    print(f\"\\n  Saturation: 1 word achieves {d_1w/d_15w*100:.0f}% of 15-word effect\")\n",
    "\n",
    "# All conditions summary\n",
    "print(f\"\\n--- All conditions ---\")\n",
    "for name in COND_NAMES:\n",
    "    if name == 'bare':\n",
    "        print(f\"  {name:<20} NLL = {arrays[name].mean():.4f}\")\n",
    "    else:\n",
    "        d = cohens_d(bare - arrays[name])\n",
    "        _, p = stats.ttest_1samp(bare - arrays[name], 0)\n",
    "        sig = '***' if p < 0.001 else '**' if p < 0.01 else '*' if p < 0.05 else 'ns'\n",
    "        print(f\"  {name:<20} NLL = {arrays[name].mean():.4f}  d = {d:+.4f} ({sig})\")\n",
    "\n",
    "# Save\n",
    "final_results = {\n",
    "    'experiment': 'v4_decoder_only_exp03_position_vs_attention',\n",
    "    'model': MODEL_NAME,\n",
    "    'dataset': 'ms_marco_v1.1',\n",
    "    'n_samples': len(results),\n",
    "    'seed': SEED,\n",
    "    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'conditions': {k: v for k, v in analysis.items()},\n",
    "}\n",
    "\n",
    "with open(RESULTS_DIR / 'results.json', 'w') as f:\n",
    "    json.dump(final_results, f, indent=2)\n",
    "print(f\"\\nResults saved to {RESULTS_DIR / 'results.json'}\")\n",
    "\n",
    "# Cleanup\n",
    "print(f\"\\nCleaning up GPU memory...\")\n",
    "mem_before = torch.cuda.memory_allocated() / 1e9\n",
    "del model, tokenizer\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "mem_after = torch.cuda.memory_allocated() / 1e9\n",
    "print(f\"GPU memory: {mem_before:.2f} GB -> {mem_after:.2f} GB\")\n",
    "print(\"Done!\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "15c6a80961ae48e6b0fb3c50eb18b8d0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "219e71f662eb4e5ea7864c61fcf55f27": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "33dbf74455414c02a258c12abe8f2cbd": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "353d60c994434164b153e02541449988": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_33dbf74455414c02a258c12abe8f2cbd",
       "placeholder": "​",
       "style": "IPY_MODEL_7738c73620674dacb5e0bc69ed6d78fb",
       "tabbable": null,
       "tooltip": null,
       "value": " 400/400 [13:07&lt;00:00,  1.99s/it]"
      }
     },
     "3c8624e2240346d4bf34505ecb84fb5d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "52ca60b55e1242abaa31ab6d6cfeab98": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6cc1ed8b0285413e9fc6aa1b32589e07": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "7738c73620674dacb5e0bc69ed6d78fb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "8bfa4e3be8704b809fc3ff255e9c58a5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_fea1b4cbd1404743b2148b3aa0442b6f",
        "IPY_MODEL_b7ffe304b08b4e70acc290f0f02eb608",
        "IPY_MODEL_353d60c994434164b153e02541449988"
       ],
       "layout": "IPY_MODEL_a77c5223f3cc4de9a614ebcc546c9b18",
       "tabbable": null,
       "tooltip": null
      }
     },
     "8fb873b13f964a3b8a555ceb2ad24dae": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "9138c91d8e2a42c592ea80f516737634": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "9e5ed22a4e3641bc875d4b511026f6bd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "a77c5223f3cc4de9a614ebcc546c9b18": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b4b5f02e99e343a0b7ec0a7c8f3fa752": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_c1c37d16e96842d88ea6be5ea6b888cf",
        "IPY_MODEL_d575c02b0c5a4726bcae6a7837aa8bc4",
        "IPY_MODEL_bfbc3dbe597b4a07b7c574f7bc55da2f"
       ],
       "layout": "IPY_MODEL_e514bd6533e84226b0ac9ae9f13a855e",
       "tabbable": null,
       "tooltip": null
      }
     },
     "b7ffe304b08b4e70acc290f0f02eb608": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_219e71f662eb4e5ea7864c61fcf55f27",
       "max": 400.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_9138c91d8e2a42c592ea80f516737634",
       "tabbable": null,
       "tooltip": null,
       "value": 400.0
      }
     },
     "bfbc3dbe597b4a07b7c574f7bc55da2f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_6cc1ed8b0285413e9fc6aa1b32589e07",
       "placeholder": "​",
       "style": "IPY_MODEL_3c8624e2240346d4bf34505ecb84fb5d",
       "tabbable": null,
       "tooltip": null,
       "value": " 883/883 [00:02&lt;00:00, 512.54it/s, Materializing param=model.vision_tower.vision_model.post_layernorm.weight]"
      }
     },
     "c1b277b11551470b95c992fc41b48881": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "c1c37d16e96842d88ea6be5ea6b888cf": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_8fb873b13f964a3b8a555ceb2ad24dae",
       "placeholder": "​",
       "style": "IPY_MODEL_c1b277b11551470b95c992fc41b48881",
       "tabbable": null,
       "tooltip": null,
       "value": "Loading weights: 100%"
      }
     },
     "d575c02b0c5a4726bcae6a7837aa8bc4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_f86a42373919408d9395cfefe167f205",
       "max": 883.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_15c6a80961ae48e6b0fb3c50eb18b8d0",
       "tabbable": null,
       "tooltip": null,
       "value": 883.0
      }
     },
     "e514bd6533e84226b0ac9ae9f13a855e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f86a42373919408d9395cfefe167f205": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "fea1b4cbd1404743b2148b3aa0442b6f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_52ca60b55e1242abaa31ab6d6cfeab98",
       "placeholder": "​",
       "style": "IPY_MODEL_9e5ed22a4e3641bc875d4b511026f6bd",
       "tabbable": null,
       "tooltip": null,
       "value": "Scoring: 100%"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
