{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f733e31",
   "metadata": {
    "papermill": {
     "duration": 0.003654,
     "end_time": "2026-02-22T17:31:43.798366",
     "exception": false,
     "start_time": "2026-02-22T17:31:43.794712",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Experiment 04: Instruction Framing Decomposition\n",
    "\n",
    "## Motivation\n",
    "\n",
    "Exp 03 found that `extractor_matched` — a generic extraction instruction\n",
    "(\"Extract all key data points, facts, entities, and specific attributes from the\n",
    "following text.\") token-matched to Q tokens — is the **best** KV cache prefix\n",
    "across all 4 datasets (pooled semantic delta d=+0.357, \\*\\*\\*). Oracle HURTS\n",
    "(d=-0.253), LLM doc-specific surrogates lose to the generic instruction, and\n",
    "there is no semantic gradient.\n",
    "\n",
    "**Why does the extraction instruction work?** Five hypotheses:\n",
    "- **H1**: Extraction framing is specifically beneficial (vs other task framings)\n",
    "- **H2**: Any coherent task-relevant instruction helps equally\n",
    "- **H3**: Instruction structure (coherent syntax) matters, content doesn't\n",
    "- **H4**: Extraction-related token vocabulary activates relevant representations\n",
    "- **H5**: Short repeated instruction phrases combine structural repetition with semantic content\n",
    "\n",
    "## Design\n",
    "\n",
    "### Three-Level Decomposition (key metric)\n",
    "\n",
    "For each instruction I:\n",
    "- **Structural** = NLL(bare) - NLL(random_tokens) — effect of any tokens in cache\n",
    "- **Vocabulary** = NLL(random_tokens) - NLL(scrambled_I) — effect of instruction token vocabulary\n",
    "- **Meaning** = NLL(scrambled_I) - NLL(coherent_I) — effect of coherent instruction order\n",
    "- **Total** = structural + vocabulary + meaning = NLL(bare) - NLL(coherent_I)\n",
    "\n",
    "### Conditions (20 total: 15 scored fresh + 5 loaded from Exp 03)\n",
    "\n",
    "**8 coherent instructions** (4 extraction + 3 non-extraction + 1 special):\n",
    "\n",
    "| # | Name | Category | Text |\n",
    "|---|------|----------|------|\n",
    "| 1 | `extract_general` | extraction | \"Extract all key data points, facts, entities, and specific attributes from the following text.\" (**LOADED** as `extractor_matched` from Exp 03) |\n",
    "| 2 | `extract_entities` | extraction | \"Identify and list every named entity including people, locations, organizations, and dates mentioned in this text.\" |\n",
    "| 3 | `extract_claims` | extraction | \"Extract all factual claims, statistics, numerical data, and specific assertions made in this passage.\" |\n",
    "| 4 | `extract_qa` | extraction | \"Extract information from this text that would help answer questions about its content and meaning.\" |\n",
    "| 5 | `comprehend` | non-extraction | \"Read and understand the main ideas, arguments, and supporting details presented in the following text.\" |\n",
    "| 6 | `generate_qa` | non-extraction | \"Generate questions that could be answered using the specific facts and details in this passage.\" |\n",
    "| 7 | `classify` | non-extraction | \"Determine the subject matter, text type, writing style, and intended audience of this passage.\" |\n",
    "| 8 | `extract_minimal` | special | \"Extract key facts from the text.\" (~7 tokens, heavily repeated in Q-token prefix) |\n",
    "\n",
    "**8 scrambled controls**: For each coherent instruction, permute the Q-length\n",
    "token-matched prefix (after `make_prefix`) with a per-condition+sample seed.\n",
    "Same token multiset, destroyed meaning.\n",
    "\n",
    "**5 loaded from Exp 03**: `bare`, `random_tokens`, `repeat_token`,\n",
    "`extractor_matched` (= `extract_general`), `adversarial_matched`\n",
    "\n",
    "### Key Analyses\n",
    "\n",
    "1. Three-level decomposition for each of 8 instructions, pooled across datasets\n",
    "2. Extraction vs non-extraction (H1 vs H2): paired test\n",
    "3. Coherent vs scrambled (H3): per-instruction paired t-test\n",
    "4. Vocabulary by category (H4): scrambled extraction vs scrambled non-extraction\n",
    "5. Repetition effect (H5): extract_minimal vs extract_general\n",
    "6. Cross-dataset consistency and instruction ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "403afe0e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-22T17:31:43.806393Z",
     "iopub.status.busy": "2026-02-22T17:31:43.805979Z",
     "iopub.status.idle": "2026-02-22T17:32:03.210775Z",
     "shell.execute_reply": "2026-02-22T17:32:03.209977Z"
    },
    "papermill": {
     "duration": 19.410957,
     "end_time": "2026-02-22T17:32:03.212360",
     "exception": false,
     "start_time": "2026-02-22T17:31:43.801403",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading google/gemma-3-12b-it...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "332045618be84ddcbd1f04f843428f67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/1065 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exp 04: Instruction Framing Decomposition\n",
      "Scoring: BOS-retained repositioning + token-level prefix matching\n",
      "N_SAMPLES: 400 per dataset, HARD_FRAC: 0.4\n",
      "Model: google/gemma-3-12b-it\n",
      "DEVICE: cuda:0, dtype: torch.bfloat16\n",
      "GPU memory: 24.38 GB\n",
      "Vocab size: 262208\n",
      "Sliding window: 1024, cache limit: 1023\n",
      "Num layers: 48\n",
      "  extract_general     : 18 tokens -> 'Extract all key data points, facts, entities, and specific a...'\n",
      "  extract_entities    : 20 tokens -> 'Identify and list every named entity including people, locat...'\n",
      "  extract_claims      : 18 tokens -> 'Extract all factual claims, statistics, numerical data, and ...'\n",
      "  extract_qa          : 16 tokens -> 'Extract information from this text that would help answer qu...'\n",
      "  comprehend          : 18 tokens -> 'Read and understand the main ideas, arguments, and supportin...'\n",
      "  generate_qa         : 16 tokens -> 'Generate questions that could be answered using the specific...'\n",
      "  classify            : 18 tokens -> 'Determine the subject matter, text type, writing style, and ...'\n",
      "  extract_minimal     : 7 tokens -> 'Extract key facts from the text....'\n",
      "\n",
      "Setup complete. Functions defined: score, make_prefix, scramble_prefix\n",
      "Fresh conditions to score: 15 (7 coherent + 8 scrambled)\n",
      "Loaded from Exp 03: 5\n",
      "Total conditions: 21\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Setup, model loading, and scoring functions\n",
    "import os\n",
    "os.umask(0o000)\n",
    "import sys, json, time, gc, re\n",
    "import random as pyrandom\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from scipy import stats\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "sys.path.insert(0, \"../../..\")\n",
    "from lib.analysis import cohens_d\n",
    "from lib.data import count_words\n",
    "\n",
    "SEED = 42\n",
    "N_SAMPLES = 400      # per dataset\n",
    "HARD_FRAC = 0.40     # top 40% by bare NLL\n",
    "MODEL_NAME = \"google/gemma-3-12b-it\"\n",
    "\n",
    "RESULTS_DIR = Path(\"../../../results/decoder_only/exp04\")\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "EXP02_DIR = Path(\"../../../results/decoder_only/exp02\")\n",
    "EXP03_DIR = Path(\"../../../results/decoder_only/exp03\")\n",
    "\n",
    "DATASET_NAMES = ['ms_marco', 'squad_v2', 'triviaqa', 'hotpotqa']\n",
    "NEW_DATASETS = ['squad_v2', 'triviaqa', 'hotpotqa']\n",
    "\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "pyrandom.seed(SEED)\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv())\n",
    "HF_TOKEN = os.environ.get(\"HF_TOKEN\")\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, DynamicCache\n",
    "\n",
    "print(f\"Loading {MODEL_NAME}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, token=HF_TOKEN)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME, device_map=\"auto\", dtype=torch.bfloat16, token=HF_TOKEN,\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "DEVICE = next(model.parameters()).device\n",
    "BOS_ID = tokenizer.bos_token_id\n",
    "NEWLINE_IDS = tokenizer(\"\\n\", add_special_tokens=False).input_ids\n",
    "text_cfg = getattr(model.config, 'text_config', model.config)\n",
    "# Use actual embedding table size (config vocab_size may include padding rows)\n",
    "VOCAB_SIZE = model.get_input_embeddings().num_embeddings\n",
    "cfg_vocab = getattr(text_cfg, 'vocab_size', None)\n",
    "if cfg_vocab != VOCAB_SIZE:\n",
    "    print(f\"WARNING: config vocab_size={cfg_vocab} != embedding size={VOCAB_SIZE}\")\n",
    "    print(f\"Using embedding size {VOCAB_SIZE} for random token generation\")\n",
    "rope_params = getattr(text_cfg, 'rope_parameters', {})\n",
    "layer_types = getattr(text_cfg, 'layer_types', [])\n",
    "# Sliding attention layers cache only (sliding_window - 1) entries.\n",
    "# select_kv_cache uses uniform indices across all layers, so total Phase A\n",
    "# tokens must not exceed this limit when a prefix is used.\n",
    "SLIDING_WINDOW = getattr(text_cfg, 'sliding_window', 4096)\n",
    "SLIDING_CACHE_LIMIT = SLIDING_WINDOW - 1  # observed: 1024-1 = 1023 for Gemma 3\n",
    "\n",
    "print(f\"Exp 04: Instruction Framing Decomposition\")\n",
    "print(f\"Scoring: BOS-retained repositioning + token-level prefix matching\")\n",
    "print(f\"N_SAMPLES: {N_SAMPLES} per dataset, HARD_FRAC: {HARD_FRAC}\")\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"DEVICE: {DEVICE}, dtype: {next(model.parameters()).dtype}\")\n",
    "print(f\"GPU memory: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "print(f\"Vocab size: {VOCAB_SIZE}\")\n",
    "print(f\"Sliding window: {SLIDING_WINDOW}, cache limit: {SLIDING_CACHE_LIMIT}\")\n",
    "print(f\"Num layers: {getattr(text_cfg, 'num_hidden_layers', 'N/A')}\")\n",
    "\n",
    "# --- RoPE repositioning helpers ---\n",
    "def build_layer_inv_freqs():\n",
    "    inv_freqs = {}\n",
    "    for lt, params in rope_params.items():\n",
    "        theta = params.get('rope_theta', 10000.0)\n",
    "        dim = text_cfg.head_dim\n",
    "        inv_freq = 1.0 / (theta ** (torch.arange(0, dim, 2, dtype=torch.float32, device=DEVICE) / dim))\n",
    "        inv_freqs[lt] = inv_freq\n",
    "    return inv_freqs\n",
    "\n",
    "LAYER_INV_FREQS = build_layer_inv_freqs()\n",
    "\n",
    "\n",
    "def rotate_half(x):\n",
    "    x1, x2 = x[..., :x.shape[-1]//2], x[..., x.shape[-1]//2:]\n",
    "    return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "\n",
    "def select_kv_cache(cache, indices):\n",
    "    selected = DynamicCache()\n",
    "    idx_tensor = torch.tensor(indices, dtype=torch.long, device=DEVICE)\n",
    "    for i in range(len(cache.layers)):\n",
    "        k = cache.layers[i].keys[:, :, idx_tensor, :]\n",
    "        v = cache.layers[i].values[:, :, idx_tensor, :]\n",
    "        selected.update(k, v, i)\n",
    "    return selected\n",
    "\n",
    "\n",
    "def reposition_kv_cache(cache, old_positions, new_positions, bos_start=0):\n",
    "    delta = new_positions - old_positions\n",
    "    for L in range(len(cache.layers)):\n",
    "        lt = layer_types[L]\n",
    "        inv_freq = LAYER_INV_FREQS[lt]\n",
    "        k = cache.layers[L].keys\n",
    "        doc_keys = k[:, :, bos_start + 1:, :]\n",
    "        freqs = torch.einsum('i,j->ij', delta.float(), inv_freq)\n",
    "        emb = torch.cat([freqs, freqs], dim=-1)\n",
    "        cos_delta = emb.cos().to(k.dtype).unsqueeze(0).unsqueeze(0)\n",
    "        sin_delta = emb.sin().to(k.dtype).unsqueeze(0).unsqueeze(0)\n",
    "        doc_keys_new = doc_keys * cos_delta + rotate_half(doc_keys) * sin_delta\n",
    "        cache.layers[L].keys = torch.cat([\n",
    "            k[:, :, :bos_start + 1, :],\n",
    "            doc_keys_new,\n",
    "        ], dim=2)\n",
    "    return cache\n",
    "\n",
    "\n",
    "def score(doc_text, query_text, answer_text, prefix_token_ids=None):\n",
    "    # BOS-retained repositioning.\n",
    "    doc_ids = tokenizer(doc_text, add_special_tokens=False,\n",
    "                        truncation=True, max_length=1024).input_ids\n",
    "\n",
    "    if prefix_token_ids is not None:\n",
    "        P = len(prefix_token_ids)\n",
    "        NL = len(NEWLINE_IDS)\n",
    "        # Truncate doc so total Phase A tokens fit in sliding window cache.\n",
    "        max_doc = SLIDING_CACHE_LIMIT - 1 - P - NL  # 1 for BOS\n",
    "        if len(doc_ids) > max_doc:\n",
    "            doc_ids = doc_ids[:max_doc]\n",
    "        D = len(doc_ids)\n",
    "        cond_ids = [BOS_ID] + list(prefix_token_ids) + NEWLINE_IDS + doc_ids\n",
    "        with torch.no_grad():\n",
    "            pa = model(input_ids=torch.tensor([cond_ids], device=DEVICE),\n",
    "                       use_cache=True)\n",
    "        cache = pa.past_key_values\n",
    "        del pa\n",
    "        keep_indices = [0] + list(range(1 + P + NL, len(cond_ids)))\n",
    "        cache = select_kv_cache(cache, keep_indices)\n",
    "        old_pos = torch.arange(1 + P + NL, 1 + P + NL + D, device=DEVICE)\n",
    "        new_pos = torch.arange(1, D + 1, device=DEVICE)\n",
    "        cache = reposition_kv_cache(cache, old_pos, new_pos, bos_start=0)\n",
    "    else:\n",
    "        D = len(doc_ids)\n",
    "        with torch.no_grad():\n",
    "            pa = model(input_ids=torch.tensor([[BOS_ID] + doc_ids], device=DEVICE),\n",
    "                       use_cache=True)\n",
    "        cache = pa.past_key_values\n",
    "        del pa\n",
    "\n",
    "    phase_b_start = D + 1\n",
    "    query_ids = tokenizer(\"\\n\" + query_text + \"\\n\",\n",
    "                          add_special_tokens=False).input_ids\n",
    "    answer_ids = tokenizer(answer_text, add_special_tokens=False,\n",
    "                           truncation=True, max_length=256).input_ids\n",
    "    if not answer_ids:\n",
    "        del cache\n",
    "        return 0.0\n",
    "\n",
    "    pb_ids = query_ids + answer_ids\n",
    "    pos = torch.arange(phase_b_start, phase_b_start + len(pb_ids), device=DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pb = model(\n",
    "            input_ids=torch.tensor([pb_ids], device=DEVICE),\n",
    "            past_key_values=cache,\n",
    "            position_ids=pos.unsqueeze(0),\n",
    "            use_cache=False,\n",
    "        )\n",
    "\n",
    "    n_q = len(query_ids)\n",
    "    logits = pb.logits[0, n_q - 1:n_q - 1 + len(answer_ids), :].float()\n",
    "    targets = torch.tensor(answer_ids, device=DEVICE)\n",
    "    nll = -F.log_softmax(logits, dim=-1).gather(\n",
    "        1, targets.unsqueeze(1)).squeeze(1).mean().item()\n",
    "    del cache, pb\n",
    "    return nll\n",
    "\n",
    "\n",
    "def make_prefix(token_ids, Q):\n",
    "    if len(token_ids) >= Q:\n",
    "        return token_ids[:Q]\n",
    "    else:\n",
    "        padded = token_ids * ((Q // max(len(token_ids), 1)) + 1)\n",
    "        return padded[:Q]\n",
    "\n",
    "\n",
    "def scramble_prefix(prefix_ids, seed):\n",
    "    # Permute Q-length prefix token IDs (after make_prefix).\n",
    "    # Same token multiset, destroyed meaning.\n",
    "    rng = pyrandom.Random(seed)\n",
    "    shuffled = list(prefix_ids)\n",
    "    rng.shuffle(shuffled)\n",
    "    return shuffled\n",
    "\n",
    "\n",
    "# --- Instruction definitions ---\n",
    "INSTRUCTIONS = {\n",
    "    # 4 extraction instructions\n",
    "    'extract_general': \"Extract all key data points, facts, entities, and specific attributes from the following text.\",\n",
    "    'extract_entities': \"Identify and list every named entity including people, locations, organizations, and dates mentioned in this text.\",\n",
    "    'extract_claims': \"Extract all factual claims, statistics, numerical data, and specific assertions made in this passage.\",\n",
    "    'extract_qa': \"Extract information from this text that would help answer questions about its content and meaning.\",\n",
    "    # 3 non-extraction instructions\n",
    "    'comprehend': \"Read and understand the main ideas, arguments, and supporting details presented in the following text.\",\n",
    "    'generate_qa': \"Generate questions that could be answered using the specific facts and details in this passage.\",\n",
    "    'classify': \"Determine the subject matter, text type, writing style, and intended audience of this passage.\",\n",
    "    # 1 special (short, heavily repeated)\n",
    "    'extract_minimal': \"Extract key facts from the text.\",\n",
    "}\n",
    "\n",
    "EXTRACTION_CONDS = ['extract_general', 'extract_entities', 'extract_claims', 'extract_qa']\n",
    "NON_EXTRACTION_CONDS = ['comprehend', 'generate_qa', 'classify']\n",
    "ALL_INSTRUCTION_CONDS = list(INSTRUCTIONS.keys())\n",
    "\n",
    "# Conditions scored fresh in this experiment (exclude extract_general = loaded from Exp 03)\n",
    "FRESH_COHERENT_CONDS = [c for c in ALL_INSTRUCTION_CONDS if c != 'extract_general']\n",
    "FRESH_SCRAMBLED_CONDS = [f'scrambled_{c}' for c in ALL_INSTRUCTION_CONDS]\n",
    "FRESH_COND_NAMES = FRESH_COHERENT_CONDS + FRESH_SCRAMBLED_CONDS\n",
    "\n",
    "# Conditions loaded from Exp 03\n",
    "LOADED_COND_NAMES = ['bare', 'random_tokens', 'repeat_token',\n",
    "                     'extractor_matched', 'adversarial_matched']\n",
    "\n",
    "# All 20 conditions (for analysis)\n",
    "ALL_COND_NAMES = LOADED_COND_NAMES + ['extract_general'] + FRESH_COHERENT_CONDS + FRESH_SCRAMBLED_CONDS\n",
    "\n",
    "SCORING_KEY = 'bos_retained_token_matched_v04'\n",
    "\n",
    "# Pre-tokenize all instructions\n",
    "INSTRUCTION_IDS = {}\n",
    "for name, text in INSTRUCTIONS.items():\n",
    "    ids = tokenizer(text, add_special_tokens=False).input_ids\n",
    "    INSTRUCTION_IDS[name] = ids\n",
    "    print(f\"  {name:<20}: {len(ids)} tokens -> '{text[:60]}...'\")\n",
    "\n",
    "print(f\"\\nSetup complete. Functions defined: score, make_prefix, scramble_prefix\")\n",
    "print(f\"Fresh conditions to score: {len(FRESH_COND_NAMES)} \"\n",
    "      f\"({len(FRESH_COHERENT_CONDS)} coherent + {len(FRESH_SCRAMBLED_CONDS)} scrambled)\")\n",
    "print(f\"Loaded from Exp 03: {len(LOADED_COND_NAMES)}\")\n",
    "print(f\"Total conditions: {len(ALL_COND_NAMES)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6bee1709",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-22T17:32:03.221088Z",
     "iopub.status.busy": "2026-02-22T17:32:03.220500Z",
     "iopub.status.idle": "2026-02-22T17:32:14.609690Z",
     "shell.execute_reply": "2026-02-22T17:32:14.608813Z"
    },
    "papermill": {
     "duration": 11.395642,
     "end_time": "2026-02-22T17:32:14.611482",
     "exception": false,
     "start_time": "2026-02-22T17:32:03.215840",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "LOADING MS MARCO from Exp 02 + Exp 03 baselines\n",
      "======================================================================\n",
      "MS MARCO: 400 total, top 40% = 160 hard samples\n",
      "\n",
      "Reloading MS MARCO v1.1 for passage text...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MS MARCO alignment verified (first 20 queries match Exp 02)\n",
      "\n",
      "======================================================================\n",
      "Loading SQuAD 2.0 validation...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SQuAD 2.0 candidates: 5906\n",
      "\n",
      "Loading TriviaQA rc.wikipedia validation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbf50c2809324117a2fc382f09d8ecab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TriviaQA candidates: 5407\n",
      "\n",
      "Loading HotpotQA distractor validation...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HotpotQA candidates: 7004\n",
      "\n",
      "======================================================================\n",
      "LOADING EXP 03 BASELINES for new datasets\n",
      "======================================================================\n",
      "\n",
      "squad_v2: query alignment verified with Exp 03 bare checkpoint\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Loaded 160 hard samples with 5 baseline conditions\n",
      "  Bare NLL range: 2.0078 - 21.9133\n",
      "\n",
      "triviaqa: query alignment verified with Exp 03 bare checkpoint\n",
      "  Loaded 160 hard samples with 5 baseline conditions\n",
      "  Bare NLL range: 2.4382 - 16.2727\n",
      "\n",
      "hotpotqa: query alignment verified with Exp 03 bare checkpoint\n",
      "  Loaded 160 hard samples with 5 baseline conditions\n",
      "  Bare NLL range: 1.3281 - 20.5008\n",
      "\n",
      "======================================================================\n",
      "Dataset + baseline loading summary:\n",
      "  ms_marco: 160 hard samples, loaded conditions: ['bare', 'random_tokens', 'repeat_token', 'extractor_matched', 'adversarial_matched', 'extract_general']\n",
      "  squad_v2: 160 hard samples, loaded conditions: ['bare', 'random_tokens', 'repeat_token', 'extractor_matched', 'adversarial_matched', 'extract_general']\n",
      "  triviaqa: 160 hard samples, loaded conditions: ['bare', 'random_tokens', 'repeat_token', 'extractor_matched', 'adversarial_matched', 'extract_general']\n",
      "  hotpotqa: 160 hard samples, loaded conditions: ['bare', 'random_tokens', 'repeat_token', 'extractor_matched', 'adversarial_matched', 'extract_general']\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Reload datasets (for passage text) + load Exp 03 baselines\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Per-dataset seeds (same as Exp 03)\n",
    "DS_SEEDS = {\n",
    "    'squad_v2': SEED + 100,\n",
    "    'triviaqa': SEED + 200,\n",
    "    'hotpotqa': SEED + 300,\n",
    "}\n",
    "\n",
    "# ================================================================\n",
    "# PART 1: Load MS MARCO from Exp 02 checkpoint\n",
    "# ================================================================\n",
    "print(\"=\" * 70)\n",
    "print(\"LOADING MS MARCO from Exp 02 + Exp 03 baselines\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "assert EXP02_DIR.exists(), f\"Exp 02 results not found at {EXP02_DIR}\"\n",
    "exp02_ckpt = json.loads((EXP02_DIR / \"checkpoint.json\").read_text())\n",
    "assert exp02_ckpt.get('scoring') == 'bos_retained_token_matched_v02', \\\n",
    "    f\"Unexpected scoring key: {exp02_ckpt.get('scoring')}\"\n",
    "exp02_results = exp02_ckpt['results']\n",
    "assert len(exp02_results) == N_SAMPLES, \\\n",
    "    f\"Expected {N_SAMPLES} results, got {len(exp02_results)}\"\n",
    "\n",
    "# Extract bare NLLs and select hard 40% (same selection as Exp 03)\n",
    "msmarco_bare = np.array([r['nll_bare'] for r in exp02_results])\n",
    "N_HARD = int(N_SAMPLES * HARD_FRAC)\n",
    "sorted_idx = np.argsort(msmarco_bare)[::-1]\n",
    "msmarco_hard_idx = np.sort(sorted_idx[:N_HARD])\n",
    "\n",
    "print(f\"MS MARCO: {N_SAMPLES} total, top {HARD_FRAC*100:.0f}% = {N_HARD} hard samples\")\n",
    "\n",
    "# Build hard_nlls with loaded baselines\n",
    "hard_nlls = {}  # ds_name -> {cond_name: np.array}\n",
    "hard_metadata = {}\n",
    "\n",
    "hard_nlls['ms_marco'] = {}\n",
    "for cond in LOADED_COND_NAMES:\n",
    "    arr = np.array([exp02_results[i][f'nll_{cond}'] for i in msmarco_hard_idx])\n",
    "    hard_nlls['ms_marco'][cond] = arr\n",
    "\n",
    "# Map extract_general = extractor_matched (same instruction text)\n",
    "hard_nlls['ms_marco']['extract_general'] = hard_nlls['ms_marco']['extractor_matched'].copy()\n",
    "\n",
    "# Get passage text for MS MARCO hard samples (need to reload dataset)\n",
    "print(\"\\nReloading MS MARCO v1.1 for passage text...\")\n",
    "ds_msmarco = load_dataset(\"microsoft/ms_marco\", \"v1.1\", split=\"validation\")\n",
    "msmarco_candidates = []\n",
    "for item in ds_msmarco:\n",
    "    if len(msmarco_candidates) >= 3 * N_SAMPLES:\n",
    "        break\n",
    "    passages = item.get('passages', {})\n",
    "    ptexts = passages.get('passage_text', [])\n",
    "    is_sel = passages.get('is_selected', [])\n",
    "    query = item.get('query', '')\n",
    "    answers = item.get('answers', [])\n",
    "    well_formed = item.get('wellFormedAnswers', [])\n",
    "    answer = None\n",
    "    if well_formed and len(well_formed) > 0 and well_formed[0] not in ('[]', ''):\n",
    "        answer = well_formed[0]\n",
    "    elif answers and len(answers) > 0 and answers[0] != 'No Answer Present.':\n",
    "        answer = answers[0]\n",
    "    if not answer:\n",
    "        continue\n",
    "    for pt, sel in zip(ptexts, is_sel):\n",
    "        wc = count_words(pt)\n",
    "        if sel == 1 and 30 <= wc <= 300:\n",
    "            msmarco_candidates.append({\n",
    "                'passage': pt, 'query': query, 'answer': answer,\n",
    "                'word_count': wc,\n",
    "            })\n",
    "            break\n",
    "\n",
    "np.random.seed(SEED)\n",
    "indices = np.random.permutation(len(msmarco_candidates))[:N_SAMPLES]\n",
    "msmarco_all = [msmarco_candidates[i] for i in indices]\n",
    "del ds_msmarco, msmarco_candidates\n",
    "gc.collect()\n",
    "\n",
    "# Verify alignment with Exp 02 results\n",
    "for i in range(min(20, N_SAMPLES)):\n",
    "    assert msmarco_all[i]['query'][:50] == exp02_results[i]['query'][:50], \\\n",
    "        f\"MS MARCO query mismatch at sample {i}\"\n",
    "print(\"MS MARCO alignment verified (first 20 queries match Exp 02)\")\n",
    "\n",
    "# Build hard_samples for MS MARCO\n",
    "hard_samples = {}\n",
    "hs_msmarco = []\n",
    "for idx in msmarco_hard_idx:\n",
    "    s = dict(msmarco_all[idx])\n",
    "    s['nll_bare'] = float(msmarco_bare[idx])\n",
    "    s['original_idx'] = int(idx)\n",
    "    hs_msmarco.append(s)\n",
    "hard_samples['ms_marco'] = hs_msmarco\n",
    "\n",
    "hard_metadata['ms_marco'] = {\n",
    "    'n_total': N_SAMPLES,\n",
    "    'n_hard': N_HARD,\n",
    "    'source': 'exp02_reuse',\n",
    "    'mean_passage_words': float(np.mean([s['word_count'] for s in hs_msmarco])),\n",
    "    'mean_query_tokens': float(np.mean([exp02_results[i]['Q'] for i in msmarco_hard_idx])),\n",
    "    'mean_answer_words': float(np.mean([count_words(s['answer']) for s in hs_msmarco])),\n",
    "}\n",
    "\n",
    "del exp02_ckpt, exp02_results\n",
    "gc.collect()\n",
    "\n",
    "# ================================================================\n",
    "# PART 2: Reload 3 new datasets (same seeds as Exp 03)\n",
    "# ================================================================\n",
    "all_samples = {}  # ds_name -> list of N_SAMPLES dicts (for query alignment check)\n",
    "\n",
    "# ---- SQuAD 2.0 ----\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Loading SQuAD 2.0 validation...\")\n",
    "ds_squad = load_dataset(\"rajpurkar/squad_v2\", split=\"validation\")\n",
    "\n",
    "squad_candidates = []\n",
    "for item in ds_squad:\n",
    "    answers = item.get('answers', {})\n",
    "    answer_texts = answers.get('text', [])\n",
    "    if not answer_texts:\n",
    "        continue\n",
    "    passage = item['context']\n",
    "    query = item['question']\n",
    "    answer = answer_texts[0]\n",
    "    wc = count_words(passage)\n",
    "    if 30 <= wc <= 500 and count_words(answer) >= 1:\n",
    "        squad_candidates.append({\n",
    "            'passage': passage, 'query': query, 'answer': answer,\n",
    "            'word_count': wc,\n",
    "        })\n",
    "\n",
    "print(f\"SQuAD 2.0 candidates: {len(squad_candidates)}\")\n",
    "np.random.seed(DS_SEEDS['squad_v2'])\n",
    "sq_indices = np.random.permutation(len(squad_candidates))[:N_SAMPLES]\n",
    "all_samples['squad_v2'] = [squad_candidates[i] for i in sq_indices]\n",
    "del ds_squad, squad_candidates\n",
    "gc.collect()\n",
    "\n",
    "# ---- TriviaQA ----\n",
    "print(\"\\nLoading TriviaQA rc.wikipedia validation...\")\n",
    "ds_trivia = load_dataset(\"mandarjoshi/trivia_qa\", \"rc.wikipedia\", split=\"validation\")\n",
    "\n",
    "trivia_candidates = []\n",
    "for item in ds_trivia:\n",
    "    entity_pages = item.get('entity_pages', {})\n",
    "    wiki_contexts = entity_pages.get('wiki_context', [])\n",
    "    if not wiki_contexts or not wiki_contexts[0]:\n",
    "        continue\n",
    "    words = wiki_contexts[0].split()[:500]\n",
    "    passage = ' '.join(words)\n",
    "    query = item['question']\n",
    "    answer_val = item['answer']['value']\n",
    "    aliases = item['answer'].get('aliases', [])\n",
    "    passage_lower = passage.lower()\n",
    "    found = answer_val.lower() in passage_lower\n",
    "    if not found:\n",
    "        for alias in aliases:\n",
    "            if alias.lower() in passage_lower:\n",
    "                found = True\n",
    "                break\n",
    "    if not found:\n",
    "        continue\n",
    "    wc = count_words(passage)\n",
    "    if 30 <= wc <= 500 and count_words(answer_val) >= 1:\n",
    "        trivia_candidates.append({\n",
    "            'passage': passage, 'query': query, 'answer': answer_val,\n",
    "            'word_count': wc,\n",
    "        })\n",
    "\n",
    "print(f\"TriviaQA candidates: {len(trivia_candidates)}\")\n",
    "np.random.seed(DS_SEEDS['triviaqa'])\n",
    "tr_indices = np.random.permutation(len(trivia_candidates))[:N_SAMPLES]\n",
    "all_samples['triviaqa'] = [trivia_candidates[i] for i in tr_indices]\n",
    "del ds_trivia, trivia_candidates\n",
    "gc.collect()\n",
    "\n",
    "# ---- HotpotQA ----\n",
    "print(\"\\nLoading HotpotQA distractor validation...\")\n",
    "ds_hotpot = load_dataset(\"hotpotqa/hotpot_qa\", \"distractor\", split=\"validation\")\n",
    "\n",
    "hotpot_candidates = []\n",
    "for item in ds_hotpot:\n",
    "    context = item.get('context', {})\n",
    "    sf = item.get('supporting_facts', {})\n",
    "    ctx_titles = context.get('title', [])\n",
    "    ctx_sentences = context.get('sentences', [])\n",
    "    sf_titles = sf.get('title', [])\n",
    "    sf_sent_ids = sf.get('sent_id', [])\n",
    "    title_to_sents = {}\n",
    "    for title, sents in zip(ctx_titles, ctx_sentences):\n",
    "        title_to_sents[title] = sents\n",
    "    passage_parts = []\n",
    "    for title, sid in zip(sf_titles, sf_sent_ids):\n",
    "        if title in title_to_sents and sid < len(title_to_sents[title]):\n",
    "            passage_parts.append(title_to_sents[title][sid])\n",
    "    if not passage_parts:\n",
    "        continue\n",
    "    passage = ' '.join(passage_parts)\n",
    "    query = item['question']\n",
    "    answer = item['answer']\n",
    "    wc = count_words(passage)\n",
    "    if 30 <= wc <= 500 and count_words(answer) >= 1:\n",
    "        hotpot_candidates.append({\n",
    "            'passage': passage, 'query': query, 'answer': answer,\n",
    "            'word_count': wc,\n",
    "        })\n",
    "\n",
    "print(f\"HotpotQA candidates: {len(hotpot_candidates)}\")\n",
    "np.random.seed(DS_SEEDS['hotpotqa'])\n",
    "hp_indices = np.random.permutation(len(hotpot_candidates))[:N_SAMPLES]\n",
    "all_samples['hotpotqa'] = [hotpot_candidates[i] for i in hp_indices]\n",
    "del ds_hotpot, hotpot_candidates\n",
    "gc.collect()\n",
    "\n",
    "# ================================================================\n",
    "# PART 3: Load Exp 03 baselines for 3 new datasets\n",
    "# ================================================================\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"LOADING EXP 03 BASELINES for new datasets\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "assert EXP03_DIR.exists(), f\"Exp 03 results not found at {EXP03_DIR}\"\n",
    "\n",
    "for ds_name in NEW_DATASETS:\n",
    "    samples_ds = all_samples[ds_name]\n",
    "\n",
    "    # Load bare NLLs to replicate hard selection\n",
    "    bare_path = EXP03_DIR / f\"bare_{ds_name}.json\"\n",
    "    assert bare_path.exists(), f\"Exp 03 bare checkpoint not found: {bare_path}\"\n",
    "    bare_ckpt = json.loads(bare_path.read_text())\n",
    "    assert bare_ckpt.get('n_total') == N_SAMPLES\n",
    "    bare_nlls_all = bare_ckpt['bare_nlls']\n",
    "    assert len(bare_nlls_all) == N_SAMPLES\n",
    "\n",
    "    # Verify query alignment\n",
    "    saved_queries = bare_ckpt.get('queries_first50', [])\n",
    "    current_queries = [s['query'][:50] for s in samples_ds[:len(saved_queries)]]\n",
    "    assert saved_queries == current_queries, \\\n",
    "        f\"{ds_name}: query alignment mismatch with Exp 03 bare checkpoint\"\n",
    "    print(f\"\\n{ds_name}: query alignment verified with Exp 03 bare checkpoint\")\n",
    "\n",
    "    # Replicate hard selection (same as Exp 03)\n",
    "    bare_arr = np.array(bare_nlls_all)\n",
    "    sorted_idx = np.argsort(bare_arr)[::-1]\n",
    "    h_idx = np.sort(sorted_idx[:N_HARD])\n",
    "\n",
    "    # Build hard_samples\n",
    "    hs = []\n",
    "    for idx in h_idx:\n",
    "        s = dict(samples_ds[idx])\n",
    "        s['nll_bare'] = float(bare_arr[idx])\n",
    "        s['original_idx'] = int(idx)\n",
    "        hs.append(s)\n",
    "    hard_samples[ds_name] = hs\n",
    "\n",
    "    # Load Exp 03 scored conditions for hard samples\n",
    "    ckpt_path = EXP03_DIR / f\"checkpoint_{ds_name}.json\"\n",
    "    assert ckpt_path.exists(), f\"Exp 03 checkpoint not found: {ckpt_path}\"\n",
    "    ckpt = json.loads(ckpt_path.read_text())\n",
    "    exp03_results = ckpt['results']\n",
    "    assert len(exp03_results) == N_HARD, \\\n",
    "        f\"{ds_name}: expected {N_HARD} results, got {len(exp03_results)}\"\n",
    "\n",
    "    # Verify alignment: Exp 03 checkpoint queries match our hard samples\n",
    "    for j in range(min(10, N_HARD)):\n",
    "        assert exp03_results[j]['query'][:50] == hs[j]['query'][:50], \\\n",
    "            f\"{ds_name}: query mismatch at hard sample {j}\"\n",
    "\n",
    "    # Extract baseline NLLs\n",
    "    hard_nlls[ds_name] = {}\n",
    "    hard_nlls[ds_name]['bare'] = bare_arr[h_idx]\n",
    "    for cond in ['random_tokens', 'repeat_token', 'extractor_matched',\n",
    "                 'adversarial_matched']:\n",
    "        hard_nlls[ds_name][cond] = np.array(\n",
    "            [r[f'nll_{cond}'] for r in exp03_results])\n",
    "\n",
    "    # Map extract_general = extractor_matched\n",
    "    hard_nlls[ds_name]['extract_general'] = hard_nlls[ds_name]['extractor_matched'].copy()\n",
    "\n",
    "    hard_metadata[ds_name] = {\n",
    "        'n_total': N_SAMPLES,\n",
    "        'n_hard': N_HARD,\n",
    "        'source': 'exp03_reuse',\n",
    "        'mean_passage_words': float(np.mean([s['word_count'] for s in hs])),\n",
    "        'mean_query_tokens': 0.0,  # filled after tokenization\n",
    "        'mean_answer_words': float(np.mean([count_words(s['answer']) for s in hs])),\n",
    "    }\n",
    "\n",
    "    print(f\"  Loaded {N_HARD} hard samples with 5 baseline conditions\")\n",
    "    print(f\"  Bare NLL range: {bare_arr[h_idx].min():.4f} - {bare_arr[h_idx].max():.4f}\")\n",
    "\n",
    "del bare_ckpt, ckpt, exp03_results\n",
    "gc.collect()\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Dataset + baseline loading summary:\")\n",
    "for ds_name in DATASET_NAMES:\n",
    "    n_h = len(hard_samples[ds_name])\n",
    "    loaded_conds = [c for c in hard_nlls[ds_name].keys()]\n",
    "    print(f\"  {ds_name}: {n_h} hard samples, loaded conditions: {loaded_conds}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b19b8fea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-22T17:32:14.621355Z",
     "iopub.status.busy": "2026-02-22T17:32:14.620717Z",
     "iopub.status.idle": "2026-02-22T17:32:16.516373Z",
     "shell.execute_reply": "2026-02-22T17:32:16.515412Z"
    },
    "papermill": {
     "duration": 1.902536,
     "end_time": "2026-02-22T17:32:16.517994",
     "exception": false,
     "start_time": "2026-02-22T17:32:14.615458",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PREFIX CONSTRUCTION + VALIDATION\n",
      "======================================================================\n",
      "\n",
      "--- ms_marco (160 hard samples) ---\n",
      "  Q tokens -- mean: 7.1, median: 7, min: 3, max: 15\n",
      "  All 15 fresh prefix types verified for 160 samples\n",
      "  Token multiset invariant verified (first 5 samples)\n",
      "\n",
      "--- squad_v2 (160 hard samples) ---\n",
      "  Q tokens -- mean: 12.8, median: 12, min: 5, max: 33\n",
      "  All 15 fresh prefix types verified for 160 samples\n",
      "  Token multiset invariant verified (first 5 samples)\n",
      "\n",
      "--- triviaqa (160 hard samples) ---\n",
      "  Q tokens -- mean: 17.5, median: 16, min: 6, max: 60\n",
      "  All 15 fresh prefix types verified for 160 samples\n",
      "  Token multiset invariant verified (first 5 samples)\n",
      "\n",
      "--- hotpotqa (160 hard samples) ---\n",
      "  Q tokens -- mean: 19.2, median: 19, min: 9, max: 48\n",
      "  All 15 fresh prefix types verified for 160 samples\n",
      "  Token multiset invariant verified (first 5 samples)\n",
      "\n",
      "======================================================================\n",
      "VALIDATION TESTS\n",
      "======================================================================\n",
      "\n",
      "--- Test 1: Bare two-phase matches single-pass ---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Single-pass NLL: 1.942079\n",
      "  Two-phase bare:  1.945658 (diff: 0.18%)\n",
      "  PASSED\n",
      "\n",
      "--- Test 2: Prefixed scoring runs correctly ---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [ms_marco] Bare:              1.5033\n",
      "  [ms_marco] extract_entities:  1.4378  delta=+0.0655\n",
      "  [ms_marco] scrambled:         1.3559  delta=+0.1474\n",
      "  PASSED\n",
      "\n",
      "ALL VALIDATION TESTS PASSED\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Build prefixes for 15 fresh conditions + validation\n",
    "print(\"=\" * 70)\n",
    "print(\"PREFIX CONSTRUCTION + VALIDATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "special_ids = set(tokenizer.all_special_ids)\n",
    "\n",
    "for ds_name in DATASET_NAMES:\n",
    "    hs = hard_samples[ds_name]\n",
    "    n_hard = len(hs)\n",
    "\n",
    "    print(f\"\\n--- {ds_name} ({n_hard} hard samples) ---\")\n",
    "\n",
    "    for i, s in enumerate(hs):\n",
    "        q_ids = tokenizer(s['query'], add_special_tokens=False).input_ids\n",
    "        Q = len(q_ids)\n",
    "        s['Q'] = Q\n",
    "\n",
    "        # Build coherent prefixes (7 fresh + extract_general already loaded)\n",
    "        for cond_name in FRESH_COHERENT_CONDS:\n",
    "            instr_ids = INSTRUCTION_IDS[cond_name]\n",
    "            s[f'prefix_{cond_name}'] = make_prefix(instr_ids, Q)\n",
    "\n",
    "        # Build scrambled prefixes (8 total, including scrambled_extract_general)\n",
    "        for cond_name in ALL_INSTRUCTION_CONDS:\n",
    "            instr_ids = INSTRUCTION_IDS[cond_name]\n",
    "            coherent_prefix = make_prefix(instr_ids, Q)\n",
    "            # Seed = hash(condition_name) % 2**31 + sample_index\n",
    "            seed = hash(cond_name) % (2**31) + i\n",
    "            s[f'prefix_scrambled_{cond_name}'] = scramble_prefix(coherent_prefix, seed)\n",
    "\n",
    "    # Update metadata with query token stats\n",
    "    q_lens = [s['Q'] for s in hs]\n",
    "    hard_metadata[ds_name]['mean_query_tokens'] = float(np.mean(q_lens))\n",
    "    print(f\"  Q tokens -- mean: {np.mean(q_lens):.1f}, \"\n",
    "          f\"median: {np.median(q_lens):.0f}, \"\n",
    "          f\"min: {np.min(q_lens)}, max: {np.max(q_lens)}\")\n",
    "\n",
    "    # Verify all fresh prefixes have exactly Q tokens\n",
    "    errors = 0\n",
    "    for i, s in enumerate(hs):\n",
    "        Q = s['Q']\n",
    "        for cond in FRESH_COND_NAMES:\n",
    "            prefix = s[f'prefix_{cond}']\n",
    "            if len(prefix) != Q:\n",
    "                print(f\"  ERROR: Sample {i} {cond}: len={len(prefix)} != Q={Q}\")\n",
    "                errors += 1\n",
    "    assert errors == 0, f\"{ds_name}: {errors} prefix length mismatches!\"\n",
    "    print(f\"  All {len(FRESH_COND_NAMES)} fresh prefix types verified for {n_hard} samples\")\n",
    "\n",
    "    # Verify scrambled prefixes have same token multiset as coherent\n",
    "    multiset_errors = 0\n",
    "    for i in range(min(5, n_hard)):\n",
    "        s = hs[i]\n",
    "        for cond_name in ALL_INSTRUCTION_CONDS:\n",
    "            coherent = make_prefix(INSTRUCTION_IDS[cond_name], s['Q'])\n",
    "            scrambled = s[f'prefix_scrambled_{cond_name}']\n",
    "            if sorted(coherent) != sorted(scrambled):\n",
    "                print(f\"  ERROR: Sample {i} {cond_name}: multiset mismatch\")\n",
    "                multiset_errors += 1\n",
    "    assert multiset_errors == 0, f\"{ds_name}: {multiset_errors} multiset mismatches!\"\n",
    "    print(f\"  Token multiset invariant verified (first 5 samples)\")\n",
    "\n",
    "# ================================================================\n",
    "# VALIDATION TESTS\n",
    "# ================================================================\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"VALIDATION TESTS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Test 1: Bare two-phase matches single-pass\n",
    "print(\"\\n--- Test 1: Bare two-phase matches single-pass ---\")\n",
    "doc_text_t = \"The cat sat on the mat near the door of the house by the lake\"\n",
    "query_text_t = \"Where did the cat sit?\"\n",
    "answer_text_t = \"on the mat\"\n",
    "doc_ids_t = tokenizer(doc_text_t, add_special_tokens=False).input_ids\n",
    "D_t = len(doc_ids_t)\n",
    "query_ids_t = tokenizer(\"\\n\" + query_text_t + \"\\n\", add_special_tokens=False).input_ids\n",
    "answer_ids_t = tokenizer(answer_text_t, add_special_tokens=False).input_ids\n",
    "\n",
    "full_ids = [BOS_ID] + doc_ids_t + query_ids_t + answer_ids_t\n",
    "with torch.no_grad():\n",
    "    out_full = model(input_ids=torch.tensor([full_ids], device=DEVICE))\n",
    "n_ctx = 1 + D_t + len(query_ids_t)\n",
    "logits_full = out_full.logits[0, n_ctx - 1:n_ctx - 1 + len(answer_ids_t), :].float()\n",
    "targets_t = torch.tensor(answer_ids_t, device=DEVICE)\n",
    "nll_single = -F.log_softmax(logits_full, dim=-1).gather(\n",
    "    1, targets_t.unsqueeze(1)).squeeze(1).mean().item()\n",
    "del out_full\n",
    "\n",
    "nll_bare = score(doc_text_t, query_text_t, answer_text_t)\n",
    "diff_pct = abs(nll_single - nll_bare) / nll_single * 100\n",
    "print(f\"  Single-pass NLL: {nll_single:.6f}\")\n",
    "print(f\"  Two-phase bare:  {nll_bare:.6f} (diff: {diff_pct:.2f}%)\")\n",
    "assert diff_pct < 1.0, f\"Bare doesn't match single-pass: {diff_pct}%\"\n",
    "print(f\"  PASSED\")\n",
    "\n",
    "# Test 2: Prefixed scoring on first hard sample\n",
    "print(\"\\n--- Test 2: Prefixed scoring runs correctly ---\")\n",
    "test_ds = DATASET_NAMES[0]\n",
    "ts = hard_samples[test_ds][0]\n",
    "nll_b = score(ts['passage'], ts['query'], ts['answer'])\n",
    "nll_c = score(ts['passage'], ts['query'], ts['answer'],\n",
    "              prefix_token_ids=ts['prefix_extract_entities'])\n",
    "nll_s = score(ts['passage'], ts['query'], ts['answer'],\n",
    "              prefix_token_ids=ts['prefix_scrambled_extract_entities'])\n",
    "print(f\"  [{test_ds}] Bare:              {nll_b:.4f}\")\n",
    "print(f\"  [{test_ds}] extract_entities:  {nll_c:.4f}  delta={nll_b - nll_c:+.4f}\")\n",
    "print(f\"  [{test_ds}] scrambled:         {nll_s:.4f}  delta={nll_b - nll_s:+.4f}\")\n",
    "assert 0 < nll_b < 20 and 0 < nll_c < 20 and 0 < nll_s < 20\n",
    "print(\"  PASSED\")\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print(\"\\nALL VALIDATION TESTS PASSED\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24d9d2e7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-22T17:32:16.527349Z",
     "iopub.status.busy": "2026-02-22T17:32:16.527043Z",
     "iopub.status.idle": "2026-02-22T18:13:11.583064Z",
     "shell.execute_reply": "2026-02-22T18:13:11.582349Z"
    },
    "papermill": {
     "duration": 2455.069097,
     "end_time": "2026-02-22T18:13:11.591101",
     "exception": false,
     "start_time": "2026-02-22T17:32:16.522004",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "SCORING — 15 fresh conditions x ~160 hard x 4 datasets\n",
      "======================================================================\n",
      "\n",
      "--- ms_marco (160 hard samples x 15 conditions) ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fd59aa37fe04c8c8c85ee45143fd19f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Score ms_marco:   0%|          | 0/160 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 20/160 | 1.2m | ETA 8.6m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 40/160 | 2.4m | ETA 7.3m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 60/160 | 3.7m | ETA 6.1m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 80/160 | 4.9m | ETA 4.9m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 100/160 | 6.1m | ETA 3.7m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 120/160 | 7.4m | ETA 2.5m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 140/160 | 8.6m | ETA 1.2m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 160/160 | 9.9m | ETA 0.0m\n",
      "  Scoring complete in 9.9 min\n",
      "\n",
      "--- squad_v2 (160 hard samples x 15 conditions) ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46c1288b10254021908aef18627649ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Score squad_v2:   0%|          | 0/160 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 20/160 | 1.2m | ETA 8.7m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 40/160 | 2.5m | ETA 7.5m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 60/160 | 3.7m | ETA 6.2m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 80/160 | 5.0m | ETA 5.0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 100/160 | 6.2m | ETA 3.7m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 120/160 | 7.5m | ETA 2.5m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 140/160 | 8.7m | ETA 1.2m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 160/160 | 10.0m | ETA 0.0m\n",
      "  Scoring complete in 10.0 min\n",
      "\n",
      "--- triviaqa (160 hard samples x 15 conditions) ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f12adcc29bf4c828f6388e9ffa94c68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Score triviaqa:   0%|          | 0/160 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 20/160 | 1.4m | ETA 9.8m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 40/160 | 2.8m | ETA 8.3m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 60/160 | 4.2m | ETA 6.9m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 80/160 | 5.6m | ETA 5.6m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 100/160 | 6.9m | ETA 4.2m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 120/160 | 8.3m | ETA 2.8m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 140/160 | 9.8m | ETA 1.4m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 160/160 | 11.1m | ETA 0.0m\n",
      "  Scoring complete in 11.2 min\n",
      "\n",
      "--- hotpotqa (160 hard samples x 15 conditions) ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c987a75aaa445e09d6dda0d310cb89d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Score hotpotqa:   0%|          | 0/160 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 20/160 | 1.2m | ETA 8.7m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 40/160 | 2.5m | ETA 7.5m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 60/160 | 3.7m | ETA 6.2m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 80/160 | 5.0m | ETA 5.0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 100/160 | 6.2m | ETA 3.7m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 120/160 | 7.4m | ETA 2.5m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 140/160 | 8.7m | ETA 1.2m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 160/160 | 9.9m | ETA 0.0m\n",
      "  Scoring complete in 9.9 min\n",
      "\n",
      "======================================================================\n",
      "VALIDATION: extract_general loaded vs extractor_matched loaded\n",
      "  ms_marco: max_diff=0.000000, mean_diff=0.000000\n",
      "  squad_v2: max_diff=0.000000, mean_diff=0.000000\n",
      "  triviaqa: max_diff=0.000000, mean_diff=0.000000\n",
      "  hotpotqa: max_diff=0.000000, mean_diff=0.000000\n",
      "\n",
      "All scoring complete. Datasets in hard_nlls:\n",
      "  ms_marco: 160 hard samples x 21 conditions\n",
      "  squad_v2: 160 hard samples x 21 conditions\n",
      "  triviaqa: 160 hard samples x 21 conditions\n",
      "  hotpotqa: 160 hard samples x 21 conditions\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Score 15 fresh conditions x ~160 hard samples x 4 datasets\n",
    "print(\"=\" * 70)\n",
    "print(f\"SCORING — {len(FRESH_COND_NAMES)} fresh conditions x ~{N_HARD} hard x \"\n",
    "      f\"{len(DATASET_NAMES)} datasets\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for ds_name in DATASET_NAMES:\n",
    "    hs = hard_samples[ds_name]\n",
    "    n_hard = len(hs)\n",
    "    ckpt_path = RESULTS_DIR / f\"checkpoint_{ds_name}.json\"\n",
    "\n",
    "    print(f\"\\n--- {ds_name} ({n_hard} hard samples x {len(FRESH_COND_NAMES)} conditions) ---\")\n",
    "\n",
    "    ds_results = []\n",
    "    start_idx = 0\n",
    "\n",
    "    if ckpt_path.exists():\n",
    "        ckpt = json.loads(ckpt_path.read_text())\n",
    "        if (ckpt.get('dataset') == ds_name and\n",
    "            ckpt.get('scoring') == SCORING_KEY and\n",
    "            ckpt.get('n_hard') == n_hard):\n",
    "            saved_queries = [r['query'][:50] for r in ckpt.get('results', [])]\n",
    "            current_queries = [s['query'][:50] for s in hs[:len(saved_queries)]]\n",
    "            if saved_queries == current_queries:\n",
    "                ds_results = ckpt['results']\n",
    "                start_idx = len(ds_results)\n",
    "                print(f\"  Resuming from checkpoint: {start_idx}/{n_hard}\")\n",
    "\n",
    "    if start_idx < n_hard:\n",
    "        t0 = time.time()\n",
    "\n",
    "        for i in tqdm(range(start_idx, n_hard), initial=start_idx,\n",
    "                      total=n_hard, desc=f\"Score {ds_name}\"):\n",
    "            s = hs[i]\n",
    "            result = {\n",
    "                'query': s['query'],\n",
    "                'answer': s['answer'],\n",
    "                'passage_words': s['word_count'],\n",
    "                'Q': s['Q'],\n",
    "            }\n",
    "\n",
    "            # Score all fresh conditions\n",
    "            for cond_name in FRESH_COND_NAMES:\n",
    "                result[f'nll_{cond_name}'] = score(\n",
    "                    s['passage'], s['query'], s['answer'],\n",
    "                    prefix_token_ids=s[f'prefix_{cond_name}']\n",
    "                )\n",
    "\n",
    "            ds_results.append(result)\n",
    "\n",
    "            if (i + 1) % 20 == 0 or i == n_hard - 1:\n",
    "                ckpt = {\n",
    "                    'dataset': ds_name,\n",
    "                    'n_hard': n_hard,\n",
    "                    'scoring': SCORING_KEY,\n",
    "                    'results': ds_results,\n",
    "                    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                }\n",
    "                ckpt_path.write_text(json.dumps(ckpt))\n",
    "                elapsed = time.time() - t0\n",
    "                done = i - start_idx + 1\n",
    "                eta = (n_hard - i - 1) * elapsed / done if done > 0 else 0\n",
    "                tqdm.write(f\"  Checkpoint {i+1}/{n_hard} | \"\n",
    "                           f\"{elapsed/60:.1f}m | ETA {eta/60:.1f}m\")\n",
    "\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        elapsed = time.time() - t0\n",
    "        print(f\"  Scoring complete in {elapsed/60:.1f} min\")\n",
    "    else:\n",
    "        print(f\"  Loaded {len(ds_results)} cached results\")\n",
    "\n",
    "    # Populate hard_nlls with fresh conditions\n",
    "    for cond in FRESH_COND_NAMES:\n",
    "        hard_nlls[ds_name][cond] = np.array(\n",
    "            [r[f'nll_{cond}'] for r in ds_results])\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Validation: extract_general (loaded) should be very close to extractor_matched\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"VALIDATION: extract_general loaded vs extractor_matched loaded\")\n",
    "for ds_name in DATASET_NAMES:\n",
    "    eg = hard_nlls[ds_name]['extract_general']\n",
    "    em = hard_nlls[ds_name]['extractor_matched']\n",
    "    max_diff = np.abs(eg - em).max()\n",
    "    mean_diff = np.abs(eg - em).mean()\n",
    "    print(f\"  {ds_name}: max_diff={max_diff:.6f}, mean_diff={mean_diff:.6f}\")\n",
    "    assert np.allclose(eg, em, atol=0.01), \\\n",
    "        f\"{ds_name}: extract_general != extractor_matched (max diff {max_diff:.6f})\"\n",
    "\n",
    "print(\"\\nAll scoring complete. Datasets in hard_nlls:\")\n",
    "for ds_name in DATASET_NAMES:\n",
    "    n = len(hard_nlls[ds_name]['bare'])\n",
    "    n_conds = len(hard_nlls[ds_name])\n",
    "    print(f\"  {ds_name}: {n} hard samples x {n_conds} conditions\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0313d14",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-22T18:13:11.605384Z",
     "iopub.status.busy": "2026-02-22T18:13:11.605061Z",
     "iopub.status.idle": "2026-02-22T18:13:11.961702Z",
     "shell.execute_reply": "2026-02-22T18:13:11.960921Z"
    },
    "papermill": {
     "duration": 0.365742,
     "end_time": "2026-02-22T18:13:11.963294",
     "exception": false,
     "start_time": "2026-02-22T18:13:11.597552",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PER-DATASET ANALYSIS\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "  MS_MARCO -- 160 hard samples\n",
      "======================================================================\n",
      "\n",
      "  Cond                             NLL   d bare    sem d   win%          p  sig\n",
      "  ----------------------------------------------------------------------------\n",
      "  bare                           2.885       --       --     --         --   --\n",
      "  random_tokens                  2.760   +0.137    (ref)  65.0%   8.55e-02   ns\n",
      "  repeat_token                   2.691   +0.221   +0.098  47.5%   2.19e-01   ns\n",
      "  extractor_matched              2.632   +0.194   +0.098  61.9%   2.15e-01   ns\n",
      "  adversarial_matched            2.721   +0.152   +0.036  50.0%   6.48e-01   ns\n",
      "  extract_general                2.632   +0.194   +0.098  61.9%   2.15e-01   ns\n",
      "  extract_entities               2.832   +0.040   -0.067  47.5%   4.01e-01   ns\n",
      "  extract_claims                 2.669   +0.156   +0.073  56.9%   3.57e-01   ns\n",
      "  extract_qa                     2.625   +0.195   +0.107  60.0%   1.79e-01   ns\n",
      "  comprehend                     2.516   +0.329   +0.230  62.5%   4.22e-03   **\n",
      "  generate_qa                    2.950   -0.035   -0.116  46.9%   1.45e-01   ns\n",
      "  classify                       2.729   +0.116   +0.025  56.9%   7.56e-01   ns\n",
      "  extract_minimal                2.594   +0.253   +0.146  62.5%   6.75e-02   ns\n",
      "  scrambled_extract_general      2.523   +0.307   +0.249  62.5%   1.97e-03   **\n",
      "  scrambled_extract_entities     2.690   +0.142   +0.067  60.0%   3.96e-01   ns\n",
      "  scrambled_extract_claims       2.629   +0.179   +0.110  63.1%   1.67e-01   ns\n",
      "  scrambled_extract_qa           2.577   +0.234   +0.152  60.6%   5.60e-02   ns\n",
      "  scrambled_comprehend           2.523   +0.346   +0.269  67.5%   8.58e-04  ***\n",
      "  scrambled_generate_qa          2.635   +0.268   +0.154  53.1%   5.28e-02   ns\n",
      "  scrambled_classify             2.774   +0.090   -0.011  52.5%   8.85e-01   ns\n",
      "  scrambled_extract_minimal      2.510   +0.306   +0.221  66.9%   5.92e-03   **\n",
      "\n",
      "  Three-Level Decomposition (NLL deltas, positive = better):\n",
      "  Instruction            Structural      Vocab    Meaning      Total\n",
      "  ------------------------------------------------------------------\n",
      "  extract_general           +0.1249    +0.2369    -0.1085    +0.2533\n",
      "  extract_entities          +0.1249    +0.0694    -0.1415    +0.0529\n",
      "  extract_claims            +0.1249    +0.1312    -0.0403    +0.2158\n",
      "  extract_qa                +0.1249    +0.1827    -0.0481    +0.2595\n",
      "  comprehend                +0.1249    +0.2367    +0.0076    +0.3692\n",
      "  generate_qa               +0.1249    +0.1253    -0.3150    -0.0648\n",
      "  classify                  +0.1249    -0.0137    +0.0444    +0.1557\n",
      "  extract_minimal           +0.1249    +0.2500    -0.0841    +0.2908\n",
      "\n",
      "  Coherent vs Scrambled (paired t-test, positive = coherent better):\n",
      "  Instruction                   d          p  sig  coherent wins\n",
      "  --------------------------------------------------------------\n",
      "  extract_general          -0.101   2.02e-01   ns          46.9%\n",
      "  extract_entities         -0.133   9.56e-02   ns          35.0%\n",
      "  extract_claims           -0.041   6.07e-01   ns          48.1%\n",
      "  extract_qa               -0.049   5.35e-01   ns          48.8%\n",
      "  comprehend               +0.012   8.83e-01   ns          51.9%\n",
      "  generate_qa              -0.207   9.62e-03   **          46.2%\n",
      "  classify                 +0.044   5.81e-01   ns          56.9%\n",
      "  extract_minimal          -0.097   2.24e-01   ns          38.1%\n",
      "\n",
      "======================================================================\n",
      "  SQUAD_V2 -- 160 hard samples\n",
      "======================================================================\n",
      "\n",
      "  Cond                             NLL   d bare    sem d   win%          p  sig\n",
      "  ----------------------------------------------------------------------------\n",
      "  bare                           5.274       --       --     --         --   --\n",
      "  random_tokens                  4.493   +0.552    (ref)  73.1%   7.69e-11  ***\n",
      "  repeat_token                   4.244   +0.576   +0.198  61.9%   1.31e-02    *\n",
      "  extractor_matched              3.895   +0.680   +0.359  70.0%   1.07e-05  ***\n",
      "  adversarial_matched            4.165   +0.709   +0.251  61.9%   1.79e-03   **\n",
      "  extract_general                3.895   +0.680   +0.359  70.0%   1.07e-05  ***\n",
      "  extract_entities               4.594   +0.291   -0.051  54.4%   5.17e-01   ns\n",
      "  extract_claims                 4.037   +0.651   +0.315  68.1%   1.05e-04  ***\n",
      "  extract_qa                     3.765   +0.678   +0.378  68.8%   4.01e-06  ***\n",
      "  comprehend                     3.387   +0.865   +0.614  79.4%   9.21e-13  ***\n",
      "  generate_qa                    4.682   +0.270   -0.095  50.6%   2.30e-01   ns\n",
      "  classify                       3.931   +0.603   +0.301  68.1%   2.04e-04  ***\n",
      "  extract_minimal                4.048   +0.621   +0.260  66.2%   1.24e-03   **\n",
      "  scrambled_extract_general      3.646   +0.642   +0.425  74.4%   2.65e-07  ***\n",
      "  scrambled_extract_entities     3.770   +0.701   +0.429  70.0%   2.06e-07  ***\n",
      "  scrambled_extract_claims       3.777   +0.647   +0.389  70.6%   2.12e-06  ***\n",
      "  scrambled_extract_qa           3.708   +0.712   +0.439  71.2%   1.13e-07  ***\n",
      "  scrambled_comprehend           3.832   +0.728   +0.396  73.1%   1.42e-06  ***\n",
      "  scrambled_generate_qa          4.554   +0.363   -0.039  42.5%   6.26e-01   ns\n",
      "  scrambled_classify             3.657   +0.712   +0.490  72.5%   4.64e-09  ***\n",
      "  scrambled_extract_minimal      3.879   +0.581   +0.294  69.4%   2.76e-04  ***\n",
      "\n",
      "  Three-Level Decomposition (NLL deltas, positive = better):\n",
      "  Instruction            Structural      Vocab    Meaning      Total\n",
      "  ------------------------------------------------------------------\n",
      "  extract_general           +0.7807    +0.8472    -0.2496    +1.3783\n",
      "  extract_entities          +0.7807    +0.7225    -0.8242    +0.6790\n",
      "  extract_claims            +0.7807    +0.7159    -0.2596    +1.2370\n",
      "  extract_qa                +0.7807    +0.7846    -0.0571    +1.5082\n",
      "  comprehend                +0.7807    +0.6612    +0.4447    +1.8866\n",
      "  generate_qa               +0.7807    -0.0608    -0.1283    +0.5916\n",
      "  classify                  +0.7807    +0.8360    -0.2741    +1.3426\n",
      "  extract_minimal           +0.7807    +0.6133    -0.1688    +1.2252\n",
      "\n",
      "  Coherent vs Scrambled (paired t-test, positive = coherent better):\n",
      "  Instruction                   d          p  sig  coherent wins\n",
      "  --------------------------------------------------------------\n",
      "  extract_general          -0.140   7.78e-02   ns          38.8%\n",
      "  extract_entities         -0.448   6.61e-08  ***          28.7%\n",
      "  extract_claims           -0.152   5.69e-02   ns          51.2%\n",
      "  extract_qa               -0.032   6.86e-01   ns          47.5%\n",
      "  comprehend               +0.305   1.66e-04  ***          66.9%\n",
      "  generate_qa              -0.067   3.95e-01   ns          56.2%\n",
      "  classify                 -0.174   2.95e-02    *          43.1%\n",
      "  extract_minimal          -0.104   1.89e-01   ns          44.4%\n",
      "\n",
      "======================================================================\n",
      "  TRIVIAQA -- 160 hard samples\n",
      "======================================================================\n",
      "\n",
      "  Cond                             NLL   d bare    sem d   win%          p  sig\n",
      "  ----------------------------------------------------------------------------\n",
      "  bare                           4.796       --       --     --         --   --\n",
      "  random_tokens                  4.935   -0.107    (ref)  40.0%   1.76e-01   ns\n",
      "  repeat_token                   5.165   -0.217   -0.155  37.5%   5.15e-02   ns\n",
      "  extractor_matched              4.067   +0.471   +0.536  73.1%   2.17e-10  ***\n",
      "  adversarial_matched            4.548   +0.138   +0.218  62.5%   6.56e-03   **\n",
      "  extract_general                4.067   +0.471   +0.536  73.1%   2.17e-10  ***\n",
      "  extract_entities               4.456   +0.208   +0.312  64.4%   1.20e-04  ***\n",
      "  extract_claims                 4.360   +0.241   +0.312  64.4%   1.21e-04  ***\n",
      "  extract_qa                     4.691   +0.057   +0.134  57.5%   9.26e-02   ns\n",
      "  comprehend                     4.214   +0.354   +0.440  70.0%   1.07e-07  ***\n",
      "  generate_qa                    4.969   -0.094   -0.019  49.4%   8.10e-01   ns\n",
      "  classify                       4.398   +0.229   +0.311  66.2%   1.23e-04  ***\n",
      "  extract_minimal                4.633   +0.100   +0.180  58.8%   2.43e-02    *\n",
      "  scrambled_extract_general      4.252   +0.326   +0.392  68.1%   1.80e-06  ***\n",
      "  scrambled_extract_entities     4.140   +0.385   +0.541  73.8%   1.62e-10  ***\n",
      "  scrambled_extract_claims       4.429   +0.224   +0.346  69.4%   2.21e-05  ***\n",
      "  scrambled_extract_qa           4.690   +0.050   +0.123  48.8%   1.22e-01   ns\n",
      "  scrambled_comprehend           4.450   +0.200   +0.298  69.4%   2.27e-04  ***\n",
      "  scrambled_generate_qa          4.835   -0.022   +0.059  54.4%   4.59e-01   ns\n",
      "  scrambled_classify             4.338   +0.283   +0.370  66.9%   6.20e-06  ***\n",
      "  scrambled_extract_minimal      4.508   +0.159   +0.247  54.4%   2.14e-03   **\n",
      "\n",
      "  Three-Level Decomposition (NLL deltas, positive = better):\n",
      "  Instruction            Structural      Vocab    Meaning      Total\n",
      "  ------------------------------------------------------------------\n",
      "  extract_general           -0.1384    +0.6828    +0.1853    +0.7296\n",
      "  extract_entities          -0.1384    +0.7952    -0.3160    +0.3407\n",
      "  extract_claims            -0.1384    +0.5064    +0.0682    +0.4362\n",
      "  extract_qa                -0.1384    +0.2447    -0.0011    +0.1052\n",
      "  comprehend                -0.1384    +0.4846    +0.2359    +0.5821\n",
      "  generate_qa               -0.1384    +0.1002    -0.1346    -0.1728\n",
      "  classify                  -0.1384    +0.5972    -0.0606    +0.3982\n",
      "  extract_minimal           -0.1384    +0.4271    -0.1251    +0.1636\n",
      "\n",
      "  Coherent vs Scrambled (paired t-test, positive = coherent better):\n",
      "  Instruction                   d          p  sig  coherent wins\n",
      "  --------------------------------------------------------------\n",
      "  extract_general          +0.150   5.90e-02   ns          57.5%\n",
      "  extract_entities         -0.247   2.08e-03   **          37.5%\n",
      "  extract_claims           +0.057   4.74e-01   ns          52.5%\n",
      "  extract_qa               -0.001   9.91e-01   ns          55.6%\n",
      "  comprehend               +0.217   6.87e-03   **          54.4%\n",
      "  generate_qa              -0.084   2.89e-01   ns          46.2%\n",
      "  classify                 -0.043   5.88e-01   ns          52.5%\n",
      "  extract_minimal          -0.121   1.28e-01   ns          47.5%\n",
      "\n",
      "======================================================================\n",
      "  HOTPOTQA -- 160 hard samples\n",
      "======================================================================\n",
      "\n",
      "  Cond                             NLL   d bare    sem d   win%          p  sig\n",
      "  ----------------------------------------------------------------------------\n",
      "  bare                           4.411       --       --     --         --   --\n",
      "  random_tokens                  4.170   +0.180    (ref)  57.5%   2.41e-02    *\n",
      "  repeat_token                   3.688   +0.453   +0.370  69.4%   5.98e-06  ***\n",
      "  extractor_matched              3.397   +0.619   +0.467  73.8%   2.06e-08  ***\n",
      "  adversarial_matched            3.913   +0.347   +0.176  60.0%   2.78e-02    *\n",
      "  extract_general                3.397   +0.619   +0.467  73.8%   2.06e-08  ***\n",
      "  extract_entities               3.857   +0.346   +0.196  58.1%   1.41e-02    *\n",
      "  extract_claims                 3.590   +0.559   +0.388  65.6%   2.26e-06  ***\n",
      "  extract_qa                     3.363   +0.624   +0.519  69.4%   6.85e-10  ***\n",
      "  comprehend                     3.144   +0.764   +0.639  77.5%   1.46e-13  ***\n",
      "  generate_qa                    3.678   +0.436   +0.299  63.1%   2.23e-04  ***\n",
      "  classify                       3.731   +0.353   +0.234  59.4%   3.55e-03   **\n",
      "  extract_minimal                3.616   +0.477   +0.389  65.0%   2.17e-06  ***\n",
      "  scrambled_extract_general      3.416   +0.508   +0.394  61.3%   1.58e-06  ***\n",
      "  scrambled_extract_entities     3.580   +0.474   +0.352  69.4%   1.61e-05  ***\n",
      "  scrambled_extract_claims       3.565   +0.498   +0.391  66.2%   1.88e-06  ***\n",
      "  scrambled_extract_qa           3.451   +0.554   +0.457  70.0%   3.74e-08  ***\n",
      "  scrambled_comprehend           3.515   +0.590   +0.469  67.5%   1.77e-08  ***\n",
      "  scrambled_generate_qa          3.825   +0.403   +0.231  65.0%   3.91e-03   **\n",
      "  scrambled_classify             3.487   +0.629   +0.471  66.9%   1.58e-08  ***\n",
      "  scrambled_extract_minimal      3.492   +0.525   +0.412  64.4%   5.63e-07  ***\n",
      "\n",
      "  Three-Level Decomposition (NLL deltas, positive = better):\n",
      "  Instruction            Structural      Vocab    Meaning      Total\n",
      "  ------------------------------------------------------------------\n",
      "  extract_general           +0.2401    +0.7548    +0.0184    +1.0133\n",
      "  extract_entities          +0.2401    +0.5905    -0.2770    +0.5535\n",
      "  extract_claims            +0.2401    +0.6056    -0.0255    +0.8202\n",
      "  extract_qa                +0.2401    +0.7195    +0.0876    +1.0472\n",
      "  comprehend                +0.2401    +0.6559    +0.3710    +1.2670\n",
      "  generate_qa               +0.2401    +0.3453    +0.1475    +0.7328\n",
      "  classify                  +0.2401    +0.6835    -0.2439    +0.6796\n",
      "  extract_minimal           +0.2401    +0.6786    -0.1241    +0.7946\n",
      "\n",
      "  Coherent vs Scrambled (paired t-test, positive = coherent better):\n",
      "  Instruction                   d          p  sig  coherent wins\n",
      "  --------------------------------------------------------------\n",
      "  extract_general          +0.011   8.94e-01   ns          48.8%\n",
      "  extract_entities         -0.196   1.41e-02    *          41.9%\n",
      "  extract_claims           -0.021   7.87e-01   ns          52.5%\n",
      "  extract_qa               +0.065   4.15e-01   ns          50.0%\n",
      "  comprehend               +0.330   4.80e-05  ***          59.4%\n",
      "  generate_qa              +0.095   2.32e-01   ns          55.6%\n",
      "  classify                 -0.157   4.84e-02    *          38.8%\n",
      "  extract_minimal          -0.107   1.78e-01   ns          42.5%\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Per-dataset analysis — condition tables, three-level decomposition\n",
    "print(\"=\" * 70)\n",
    "print(\"PER-DATASET ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "per_dataset_analysis = {}\n",
    "\n",
    "for ds_name in DATASET_NAMES:\n",
    "    nlls = hard_nlls[ds_name]\n",
    "    n_hard = len(nlls['bare'])\n",
    "    bare = nlls['bare']\n",
    "    random_base = nlls['random_tokens']\n",
    "\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"  {ds_name.upper()} -- {n_hard} hard samples\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "    analysis = {}\n",
    "\n",
    "    # ---- Part A: Condition table (all 20 conditions) ----\n",
    "    print(f\"\\n  {'Cond':<28} {'NLL':>7} {'d bare':>8} {'sem d':>8} \"\n",
    "          f\"{'win%':>6} {'p':>10} {'sig':>4}\")\n",
    "    print(f\"  {'-'*76}\")\n",
    "\n",
    "    for cond in ALL_COND_NAMES:\n",
    "        if cond not in nlls:\n",
    "            continue\n",
    "        c_nlls = nlls[cond]\n",
    "        mean_nll = c_nlls.mean()\n",
    "\n",
    "        if cond == 'bare':\n",
    "            print(f\"  {cond:<28} {mean_nll:>7.3f} {'--':>8} {'--':>8} \"\n",
    "                  f\"{'--':>6} {'--':>10} {'--':>4}\")\n",
    "            analysis[cond] = {'mean_nll': float(mean_nll)}\n",
    "            continue\n",
    "\n",
    "        diff_bare = bare - c_nlls\n",
    "        d_bare = cohens_d(diff_bare)\n",
    "        _, p_bare = stats.ttest_1samp(diff_bare, 0)\n",
    "\n",
    "        if cond == 'random_tokens':\n",
    "            win_pct = 100 * np.mean(diff_bare > 0)\n",
    "            sig = ('***' if p_bare < 0.001 else '**' if p_bare < 0.01\n",
    "                   else '*' if p_bare < 0.05 else 'ns')\n",
    "            print(f\"  {cond:<28} {mean_nll:>7.3f} {d_bare:>+8.3f} {'(ref)':>8} \"\n",
    "                  f\"{win_pct:>5.1f}% {p_bare:>10.2e} {sig:>4}\")\n",
    "            analysis[cond] = {\n",
    "                'mean_nll': float(mean_nll), 'd_bare': float(d_bare),\n",
    "                'semantic_delta_d': 0.0, 'p_bare': float(p_bare),\n",
    "            }\n",
    "        else:\n",
    "            sem_delta = random_base - c_nlls\n",
    "            d_sem = cohens_d(sem_delta)\n",
    "            _, p_sem = stats.ttest_1samp(sem_delta, 0)\n",
    "            win_pct = 100 * np.mean(sem_delta > 0)\n",
    "            sig = ('***' if p_sem < 0.001 else '**' if p_sem < 0.01\n",
    "                   else '*' if p_sem < 0.05 else 'ns')\n",
    "            print(f\"  {cond:<28} {mean_nll:>7.3f} {d_bare:>+8.3f} {d_sem:>+8.3f} \"\n",
    "                  f\"{win_pct:>5.1f}% {p_sem:>10.2e} {sig:>4}\")\n",
    "            analysis[cond] = {\n",
    "                'mean_nll': float(mean_nll), 'd_bare': float(d_bare),\n",
    "                'semantic_delta_d': float(d_sem), 'p_semantic': float(p_sem),\n",
    "            }\n",
    "\n",
    "    # ---- Part B: Three-level decomposition ----\n",
    "    print(f\"\\n  Three-Level Decomposition (NLL deltas, positive = better):\")\n",
    "    print(f\"  {'Instruction':<22} {'Structural':>10} {'Vocab':>10} \"\n",
    "          f\"{'Meaning':>10} {'Total':>10}\")\n",
    "    print(f\"  {'-'*66}\")\n",
    "\n",
    "    structural = (bare - random_base).mean()\n",
    "\n",
    "    decomp = {}\n",
    "    for cond_name in ALL_INSTRUCTION_CONDS:\n",
    "        coherent_key = cond_name\n",
    "        scrambled_key = f'scrambled_{cond_name}'\n",
    "\n",
    "        if coherent_key not in nlls or scrambled_key not in nlls:\n",
    "            continue\n",
    "\n",
    "        vocab = (random_base - nlls[scrambled_key]).mean()\n",
    "        meaning = (nlls[scrambled_key] - nlls[coherent_key]).mean()\n",
    "        total = structural + vocab + meaning\n",
    "\n",
    "        print(f\"  {cond_name:<22} {structural:>+10.4f} {vocab:>+10.4f} \"\n",
    "              f\"{meaning:>+10.4f} {total:>+10.4f}\")\n",
    "\n",
    "        decomp[cond_name] = {\n",
    "            'structural': float(structural),\n",
    "            'vocabulary': float(vocab),\n",
    "            'meaning': float(meaning),\n",
    "            'total': float(total),\n",
    "        }\n",
    "\n",
    "    analysis['decomposition'] = decomp\n",
    "\n",
    "    # ---- Part C: Coherent vs scrambled paired tests ----\n",
    "    print(f\"\\n  Coherent vs Scrambled (paired t-test, positive = coherent better):\")\n",
    "    print(f\"  {'Instruction':<22} {'d':>8} {'p':>10} {'sig':>4} {'coherent wins':>14}\")\n",
    "    print(f\"  {'-'*62}\")\n",
    "\n",
    "    for cond_name in ALL_INSTRUCTION_CONDS:\n",
    "        coherent_key = cond_name\n",
    "        scrambled_key = f'scrambled_{cond_name}'\n",
    "        if coherent_key not in nlls or scrambled_key not in nlls:\n",
    "            continue\n",
    "        diff = nlls[scrambled_key] - nlls[coherent_key]  # pos = coherent better\n",
    "        d = cohens_d(diff)\n",
    "        _, p = stats.ttest_1samp(diff, 0)\n",
    "        win_pct = 100 * np.mean(diff > 0)\n",
    "        sig = ('***' if p < 0.001 else '**' if p < 0.01\n",
    "               else '*' if p < 0.05 else 'ns')\n",
    "        print(f\"  {cond_name:<22} {d:>+8.3f} {p:>10.2e} {sig:>4} {win_pct:>13.1f}%\")\n",
    "\n",
    "    per_dataset_analysis[ds_name] = analysis\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44fdccfa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-22T18:13:11.977940Z",
     "iopub.status.busy": "2026-02-22T18:13:11.977529Z",
     "iopub.status.idle": "2026-02-22T18:13:12.050397Z",
     "shell.execute_reply": "2026-02-22T18:13:12.049690Z"
    },
    "papermill": {
     "duration": 0.081864,
     "end_time": "2026-02-22T18:13:12.051880",
     "exception": false,
     "start_time": "2026-02-22T18:13:11.970016",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "CROSS-DATASET META-ANALYSIS\n",
      "======================================================================\n",
      "\n",
      "--- PART 1: Pooled Three-Level Decomposition ---\n",
      "\n",
      "  Instruction            Structural      Vocab    Meaning      Total\n",
      "  ------------------------------------------------------------------\n",
      "  extract_general           +0.2518    +0.6304    -0.0386    +0.8436\n",
      "  extract_entities          +0.2518    +0.5444    -0.3897    +0.4065\n",
      "  extract_claims            +0.2518    +0.4898    -0.0643    +0.6773\n",
      "  extract_qa                +0.2518    +0.4829    -0.0047    +0.7300\n",
      "  comprehend                +0.2518    +0.5096    +0.2648    +1.0262\n",
      "  generate_qa               +0.2518    +0.1275    -0.1076    +0.2717\n",
      "  classify                  +0.2518    +0.5258    -0.1336    +0.6440\n",
      "  extract_minimal           +0.2518    +0.4923    -0.1255    +0.6185\n",
      "\n",
      "--- PART 2: Fixed-Effects Meta-Analysis (semantic delta d) ---\n",
      "\n",
      "  Condition                     pooled_d       SE        z          p           95% CI  sig\n",
      "  --------------------------------------------------------------------------------------\n",
      "  extract_general                +0.3573   0.0410    +8.70   3.18e-18 [+0.277, +0.438]  ***\n",
      "  extract_entities               +0.0947   0.0399    +2.38   1.75e-02 [+0.017, +0.173]    *\n",
      "  extract_claims                 +0.2688   0.0404    +6.66   2.77e-11 [+0.190, +0.348]  ***\n",
      "  extract_qa                     +0.2760   0.0406    +6.81   1.00e-11 [+0.197, +0.356]  ***\n",
      "  comprehend                     +0.4702   0.0419   +11.22   3.35e-29 [+0.388, +0.552]  ***\n",
      "  generate_qa                    +0.0144   0.0398    +0.36   7.17e-01 [-0.064, +0.092]   ns\n",
      "  classify                       +0.2155   0.0401    +5.37   7.80e-08 [+0.137, +0.294]  ***\n",
      "  extract_minimal                +0.2413   0.0402    +6.00   1.93e-09 [+0.162, +0.320]  ***\n",
      "  scrambled_extract_general      +0.3636   0.0409    +8.90   5.60e-19 [+0.284, +0.444]  ***\n",
      "  scrambled_extract_entities     +0.3389   0.0409    +8.28   1.26e-16 [+0.259, +0.419]  ***\n",
      "  scrambled_extract_claims       +0.3057   0.0406    +7.53   4.91e-14 [+0.226, +0.385]  ***\n",
      "  scrambled_extract_qa           +0.2862   0.0406    +7.06   1.72e-12 [+0.207, +0.366]  ***\n",
      "  scrambled_comprehend           +0.3560   0.0408    +8.72   2.78e-18 [+0.276, +0.436]  ***\n",
      "  scrambled_generate_qa          +0.1005   0.0397    +2.53   1.14e-02 [+0.023, +0.178]    *\n",
      "  scrambled_classify             +0.3208   0.0409    +7.84   4.64e-15 [+0.241, +0.401]  ***\n",
      "  scrambled_extract_minimal      +0.2918   0.0404    +7.22   5.17e-13 [+0.213, +0.371]  ***\n",
      "  repeat_token                   +0.1244   0.0400    +3.11   1.89e-03 [+0.046, +0.203]   **\n",
      "  adversarial_matched            +0.1692   0.0399    +4.24   2.20e-05 [+0.091, +0.247]  ***\n",
      "\n",
      "--- PART 3: Extraction vs Non-Extraction (H1 vs H2) ---\n",
      "  Extraction (n=4):     mean pooled_d = +0.2492\n",
      "  Non-extraction (n=3): mean pooled_d = +0.2334\n",
      "  Paired (extraction - non-extraction): d=+0.0212, p=5.91e-01 ns\n",
      "  --> H1 NOT SUPPORTED: extraction framing is not uniquely better\n",
      "\n",
      "--- PART 4: Coherent vs Scrambled -- Meaning Effect (H3) ---\n",
      "  Instruction              d(meaning)          p  sig consistent\n",
      "  --------------------------------------------------------------\n",
      "  extract_general             -0.0258   5.14e-01   ns         NO\n",
      "  extract_entities            -0.2693   2.23e-11  ***        YES\n",
      "  extract_claims              -0.0493   2.13e-01   ns         NO\n",
      "  extract_qa                  -0.0034   9.31e-01   ns         NO\n",
      "  comprehend                  +0.2348   4.68e-09  ***        YES\n",
      "  generate_qa                 -0.0650   1.01e-01   ns         NO\n",
      "  classify                    -0.0947   1.69e-02    *         NO\n",
      "  extract_minimal             -0.1045   8.41e-03   **        YES\n",
      "\n",
      "  1/8 instructions have significant meaning effect\n",
      "  --> H3 PARTIALLY SUPPORTED: meaning effect is weak/inconsistent\n",
      "\n",
      "--- PART 5: Scrambled Extraction vs Scrambled Non-Extraction (H4) ---\n",
      "  Scrambled extraction mean sem delta: +0.5369\n",
      "  Scrambled non-extraction mean sem delta: +0.3876\n",
      "  Paired diff: d=+0.1771, p=8.82e-06 ***\n",
      "  --> H4 SUPPORTED: extraction vocabulary alone activates relevant representations\n",
      "\n",
      "--- PART 6: Repetition Effect (H5) ---\n",
      "  extract_minimal (~7 tokens, heavily repeated) vs extract_general (~18 tokens)\n",
      "  ms_marco        : extract_minimal wins 46.9%, d=+0.0405, p=6.09e-01 ns\n",
      "  squad_v2        : extract_minimal wins 45.0%, d=-0.1060, p=1.82e-01 ns\n",
      "  triviaqa        : extract_minimal wins 24.4%, d=-0.4552, p=4.26e-08 ***\n",
      "  hotpotqa        : extract_minimal wins 43.8%, d=-0.1574, p=4.81e-02 *\n",
      "\n",
      "  Pooled: extract_minimal vs extract_general: d=-0.1755, p=1.06e-05 ***\n",
      "  --> H5 REFUTED: longer instruction outperforms short repeated one\n",
      "\n",
      "--- PART 7: Cross-Dataset Consistency ---\n",
      "\n",
      "  Condition                      ms_marco   squad_v2   triviaqa   hotpotqa      mean consistent\n",
      "  ------------------------------------------------------------------------------------------\n",
      "  extract_general                  +0.098     +0.359     +0.536     +0.467    +0.365        YES\n",
      "  extract_entities                 -0.067     -0.051     +0.312     +0.196    +0.098         NO\n",
      "  extract_claims                   +0.073     +0.315     +0.312     +0.388    +0.272        YES\n",
      "  extract_qa                       +0.107     +0.378     +0.134     +0.519    +0.284        YES\n",
      "  comprehend                       +0.230     +0.614     +0.440     +0.639    +0.481        YES\n",
      "  generate_qa                      -0.116     -0.095     -0.019     +0.299    +0.017         NO\n",
      "  classify                         +0.025     +0.301     +0.311     +0.234    +0.218        YES\n",
      "  extract_minimal                  +0.146     +0.260     +0.180     +0.389    +0.244        YES\n",
      "  scrambled_extract_general        +0.249     +0.425     +0.392     +0.394    +0.365        YES\n",
      "  scrambled_extract_entities       +0.067     +0.429     +0.541     +0.352    +0.347        YES\n",
      "  scrambled_extract_claims         +0.110     +0.389     +0.346     +0.391    +0.309        YES\n",
      "  scrambled_extract_qa             +0.152     +0.439     +0.123     +0.457    +0.293        YES\n",
      "  scrambled_comprehend             +0.269     +0.396     +0.298     +0.469    +0.358        YES\n",
      "  scrambled_generate_qa            +0.154     -0.039     +0.059     +0.231    +0.101         NO\n",
      "  scrambled_classify               -0.011     +0.490     +0.370     +0.471    +0.330         NO\n",
      "  scrambled_extract_minimal        +0.221     +0.294     +0.247     +0.412    +0.293        YES\n",
      "  repeat_token                     +0.098     +0.198     -0.155     +0.370    +0.128         NO\n",
      "  adversarial_matched              +0.036     +0.251     +0.218     +0.176    +0.170        YES\n",
      "\n",
      "--- PART 8: Instruction Ranking (pooled semantic delta d) ---\n",
      "\n",
      "  Rank Condition                     pooled_d  sig\n",
      "  --------------------------------------------------\n",
      "     1 comprehend                     +0.4702  ***\n",
      "     2 scrambled_extract_general      +0.3636  ***\n",
      "     3 extract_general                +0.3573  ***\n",
      "     4 scrambled_comprehend           +0.3560  ***\n",
      "     5 scrambled_extract_entities     +0.3389  ***\n",
      "     6 scrambled_classify             +0.3208  ***\n",
      "     7 scrambled_extract_claims       +0.3057  ***\n",
      "     8 scrambled_extract_minimal      +0.2918  ***\n",
      "     9 scrambled_extract_qa           +0.2862  ***\n",
      "    10 extract_qa                     +0.2760  ***\n",
      "    11 extract_claims                 +0.2688  ***\n",
      "    12 extract_minimal                +0.2413  ***\n",
      "    13 classify                       +0.2155  ***\n",
      "    14 adversarial_matched            +0.1692  ***\n",
      "    15 repeat_token                   +0.1244   **\n",
      "    16 scrambled_generate_qa          +0.1005    *\n",
      "    17 extract_entities               +0.0947    *\n",
      "    18 generate_qa                    +0.0144   ns\n",
      "\n",
      "======================================================================\n",
      "VERDICT -- Exp 04: Instruction Framing Decomposition\n",
      "======================================================================\n",
      "\n",
      "Model: google/gemma-3-12b-it\n",
      "Scoring: BOS-retained repositioning + token-level prefix matching\n",
      "Datasets: 4 (ms_marco, squad_v2, triviaqa, hotpotqa)\n",
      "Hard selection: top 40% by bare NLL\n",
      "\n",
      "--- Hypothesis Verdicts ---\n",
      "\n",
      "  H1 (Extraction framing is specifically beneficial):\n",
      "    Extraction mean pooled_d: +0.2492\n",
      "    Non-extraction mean pooled_d: +0.2334\n",
      "    Paired diff d=+0.0212, p=5.91e-01\n",
      "\n",
      "  H2 (Any coherent task-relevant instruction helps equally):\n",
      "    All coherent instructions mean pooled_d: +0.2423\n",
      "    Range: [+0.0144, +0.4702]\n",
      "    7/8 significantly positive\n",
      "\n",
      "  H3 (Structure matters, content doesn't):\n",
      "    1/8 instructions have significant meaning effect\n",
      "\n",
      "  H4 (Extraction vocabulary activates relevant representations):\n",
      "    Scrambled ext vs non-ext: d=+0.1771, p=8.82e-06\n",
      "\n",
      "  H5 (Short repeated instruction combines repetition + semantics):\n",
      "    extract_minimal vs extract_general: d=-0.1755, p=1.06e-05\n",
      "\n",
      "--- Key decomposition finding ---\n",
      "  Mean across all 8 instructions (pooled NLL delta):\n",
      "    Structural: +0.2518\n",
      "    Vocabulary:  +0.4753\n",
      "    Meaning:     -0.0749\n",
      "    Total:       +0.6522\n",
      "    Decomposition: 39% structural, 73% vocabulary, -11% meaning\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Cross-dataset meta-analysis, hypothesis verdicts\n",
    "print(\"=\" * 70)\n",
    "print(\"CROSS-DATASET META-ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# ================================================================\n",
    "# PART 1: Pooled three-level decomposition\n",
    "# ================================================================\n",
    "print(f\"\\n--- PART 1: Pooled Three-Level Decomposition ---\")\n",
    "print(f\"\\n  {'Instruction':<22} {'Structural':>10} {'Vocab':>10} \"\n",
    "      f\"{'Meaning':>10} {'Total':>10}\")\n",
    "print(f\"  {'-'*66}\")\n",
    "\n",
    "pooled_decomp = {}\n",
    "for cond_name in ALL_INSTRUCTION_CONDS:\n",
    "    struct_vals, vocab_vals, meaning_vals, total_vals = [], [], [], []\n",
    "    for ds_name in DATASET_NAMES:\n",
    "        nlls = hard_nlls[ds_name]\n",
    "        bare = nlls['bare']\n",
    "        random_base = nlls['random_tokens']\n",
    "        coherent_key = cond_name\n",
    "        scrambled_key = f'scrambled_{cond_name}'\n",
    "        if coherent_key not in nlls or scrambled_key not in nlls:\n",
    "            continue\n",
    "        struct_vals.append((bare - random_base).mean())\n",
    "        vocab_vals.append((random_base - nlls[scrambled_key]).mean())\n",
    "        meaning_vals.append((nlls[scrambled_key] - nlls[coherent_key]).mean())\n",
    "        total_vals.append((bare - nlls[coherent_key]).mean())\n",
    "\n",
    "    s_mean = np.mean(struct_vals)\n",
    "    v_mean = np.mean(vocab_vals)\n",
    "    m_mean = np.mean(meaning_vals)\n",
    "    t_mean = np.mean(total_vals)\n",
    "    print(f\"  {cond_name:<22} {s_mean:>+10.4f} {v_mean:>+10.4f} \"\n",
    "          f\"{m_mean:>+10.4f} {t_mean:>+10.4f}\")\n",
    "    pooled_decomp[cond_name] = {\n",
    "        'structural': float(s_mean), 'vocabulary': float(v_mean),\n",
    "        'meaning': float(m_mean), 'total': float(t_mean),\n",
    "    }\n",
    "\n",
    "# ================================================================\n",
    "# PART 2: Fixed-effects meta-analysis (semantic delta d)\n",
    "# ================================================================\n",
    "print(f\"\\n--- PART 2: Fixed-Effects Meta-Analysis (semantic delta d) ---\")\n",
    "\n",
    "# All scorable conditions (coherent + scrambled, excluding bare/random_tokens)\n",
    "META_CONDS = (\n",
    "    ALL_INSTRUCTION_CONDS +\n",
    "    FRESH_SCRAMBLED_CONDS +\n",
    "    ['repeat_token', 'adversarial_matched']\n",
    ")\n",
    "\n",
    "print(f\"\\n  {'Condition':<28} {'pooled_d':>9} {'SE':>8} {'z':>8} \"\n",
    "      f\"{'p':>10} {'95% CI':>16} {'sig':>4}\")\n",
    "print(f\"  {'-'*86}\")\n",
    "\n",
    "meta_results = {}\n",
    "for cond in META_CONDS:\n",
    "    ds_effects = []\n",
    "    for ds_name in DATASET_NAMES:\n",
    "        nlls = hard_nlls[ds_name]\n",
    "        if cond not in nlls:\n",
    "            continue\n",
    "        sem_delta = nlls['random_tokens'] - nlls[cond]\n",
    "        n = len(sem_delta)\n",
    "        d = cohens_d(sem_delta)\n",
    "        se = np.sqrt(1.0/n + d**2 / (2.0*n))\n",
    "        ds_effects.append((d, se, n))\n",
    "\n",
    "    if not ds_effects:\n",
    "        continue\n",
    "\n",
    "    weights = [1.0 / (se**2) for _, se, _ in ds_effects]\n",
    "    w_sum = sum(weights)\n",
    "    pooled_d = sum(w * d for (d, _, _), w in zip(ds_effects, weights)) / w_sum\n",
    "    pooled_se = 1.0 / np.sqrt(w_sum)\n",
    "    z = pooled_d / pooled_se if pooled_se > 0 else 0.0\n",
    "    p = 2 * stats.norm.sf(abs(z))\n",
    "    ci_lo = pooled_d - 1.96 * pooled_se\n",
    "    ci_hi = pooled_d + 1.96 * pooled_se\n",
    "    sig = ('***' if p < 0.001 else '**' if p < 0.01\n",
    "           else '*' if p < 0.05 else 'ns')\n",
    "\n",
    "    print(f\"  {cond:<28} {pooled_d:>+9.4f} {pooled_se:>8.4f} {z:>+8.2f} \"\n",
    "          f\"{p:>10.2e} [{ci_lo:>+.3f}, {ci_hi:>+.3f}] {sig:>4}\")\n",
    "    meta_results[cond] = {\n",
    "        'pooled_d': float(pooled_d), 'se': float(pooled_se),\n",
    "        'z': float(z), 'p': float(p),\n",
    "        'ci_lo': float(ci_lo), 'ci_hi': float(ci_hi),\n",
    "    }\n",
    "\n",
    "# ================================================================\n",
    "# PART 3: H1 vs H2 — Extraction vs Non-Extraction\n",
    "# ================================================================\n",
    "print(f\"\\n--- PART 3: Extraction vs Non-Extraction (H1 vs H2) ---\")\n",
    "\n",
    "extraction_ds = [meta_results[c]['pooled_d'] for c in EXTRACTION_CONDS if c in meta_results]\n",
    "non_extraction_ds = [meta_results[c]['pooled_d'] for c in NON_EXTRACTION_CONDS if c in meta_results]\n",
    "print(f\"  Extraction (n={len(extraction_ds)}):     mean pooled_d = {np.mean(extraction_ds):+.4f}\")\n",
    "print(f\"  Non-extraction (n={len(non_extraction_ds)}): mean pooled_d = {np.mean(non_extraction_ds):+.4f}\")\n",
    "\n",
    "# Per-sample paired test across all datasets\n",
    "ext_nlls_pooled = []\n",
    "non_ext_nlls_pooled = []\n",
    "for ds_name in DATASET_NAMES:\n",
    "    nlls = hard_nlls[ds_name]\n",
    "    random_base = nlls['random_tokens']\n",
    "    ext_mean = np.mean([random_base - nlls[c] for c in EXTRACTION_CONDS if c in nlls], axis=0)\n",
    "    non_ext_mean = np.mean([random_base - nlls[c] for c in NON_EXTRACTION_CONDS if c in nlls], axis=0)\n",
    "    ext_nlls_pooled.append(ext_mean)\n",
    "    non_ext_nlls_pooled.append(non_ext_mean)\n",
    "\n",
    "ext_pooled = np.concatenate(ext_nlls_pooled)\n",
    "non_ext_pooled = np.concatenate(non_ext_nlls_pooled)\n",
    "diff_ext_vs_non = ext_pooled - non_ext_pooled\n",
    "d_ext_vs_non = cohens_d(diff_ext_vs_non)\n",
    "_, p_ext_vs_non = stats.ttest_1samp(diff_ext_vs_non, 0)\n",
    "sig_ext = ('***' if p_ext_vs_non < 0.001 else '**' if p_ext_vs_non < 0.01\n",
    "           else '*' if p_ext_vs_non < 0.05 else 'ns')\n",
    "print(f\"  Paired (extraction - non-extraction): d={d_ext_vs_non:+.4f}, \"\n",
    "      f\"p={p_ext_vs_non:.2e} {sig_ext}\")\n",
    "if d_ext_vs_non > 0.05 and p_ext_vs_non < 0.05:\n",
    "    print(f\"  --> H1 SUPPORTED: extraction framing is specifically beneficial\")\n",
    "else:\n",
    "    print(f\"  --> H1 NOT SUPPORTED: extraction framing is not uniquely better\")\n",
    "\n",
    "# ================================================================\n",
    "# PART 4: H3 — Coherent vs Scrambled (meaning effect)\n",
    "# ================================================================\n",
    "print(f\"\\n--- PART 4: Coherent vs Scrambled -- Meaning Effect (H3) ---\")\n",
    "print(f\"  {'Instruction':<22} {'d(meaning)':>12} {'p':>10} {'sig':>4} {'consistent':>10}\")\n",
    "print(f\"  {'-'*62}\")\n",
    "\n",
    "meaning_tests = {}\n",
    "for cond_name in ALL_INSTRUCTION_CONDS:\n",
    "    per_ds_d = []\n",
    "    all_diffs = []\n",
    "    for ds_name in DATASET_NAMES:\n",
    "        nlls = hard_nlls[ds_name]\n",
    "        coherent_key = cond_name\n",
    "        scrambled_key = f'scrambled_{cond_name}'\n",
    "        if coherent_key not in nlls or scrambled_key not in nlls:\n",
    "            continue\n",
    "        diff = nlls[scrambled_key] - nlls[coherent_key]\n",
    "        per_ds_d.append(cohens_d(diff))\n",
    "        all_diffs.append(diff)\n",
    "\n",
    "    pooled_diff = np.concatenate(all_diffs)\n",
    "    d = cohens_d(pooled_diff)\n",
    "    _, p = stats.ttest_1samp(pooled_diff, 0)\n",
    "    sig = ('***' if p < 0.001 else '**' if p < 0.01\n",
    "           else '*' if p < 0.05 else 'ns')\n",
    "    consistent = all(x > 0 for x in per_ds_d) or all(x <= 0 for x in per_ds_d)\n",
    "    print(f\"  {cond_name:<22} {d:>+12.4f} {p:>10.2e} {sig:>4} \"\n",
    "          f\"{'YES' if consistent else 'NO':>10}\")\n",
    "    meaning_tests[cond_name] = {\n",
    "        'd': float(d), 'p': float(p), 'consistent': consistent,\n",
    "        'per_ds_d': [float(x) for x in per_ds_d],\n",
    "    }\n",
    "\n",
    "n_sig_meaning = sum(1 for v in meaning_tests.values() if v['p'] < 0.05 and v['d'] > 0)\n",
    "print(f\"\\n  {n_sig_meaning}/{len(meaning_tests)} instructions have significant meaning effect\")\n",
    "if n_sig_meaning == 0:\n",
    "    print(f\"  --> H3 SUPPORTED: instruction structure matters, content doesn't\")\n",
    "elif n_sig_meaning <= 2:\n",
    "    print(f\"  --> H3 PARTIALLY SUPPORTED: meaning effect is weak/inconsistent\")\n",
    "else:\n",
    "    print(f\"  --> H3 NOT SUPPORTED: meaning (word order) matters for coherent instructions\")\n",
    "\n",
    "# ================================================================\n",
    "# PART 5: H4 — Vocabulary by Category\n",
    "# ================================================================\n",
    "print(f\"\\n--- PART 5: Scrambled Extraction vs Scrambled Non-Extraction (H4) ---\")\n",
    "\n",
    "scr_ext_nlls = []\n",
    "scr_non_nlls = []\n",
    "for ds_name in DATASET_NAMES:\n",
    "    nlls = hard_nlls[ds_name]\n",
    "    random_base = nlls['random_tokens']\n",
    "    ext_scr = [random_base - nlls[f'scrambled_{c}'] for c in EXTRACTION_CONDS\n",
    "               if f'scrambled_{c}' in nlls]\n",
    "    non_scr = [random_base - nlls[f'scrambled_{c}'] for c in NON_EXTRACTION_CONDS\n",
    "               if f'scrambled_{c}' in nlls]\n",
    "    if ext_scr and non_scr:\n",
    "        scr_ext_nlls.append(np.mean(ext_scr, axis=0))\n",
    "        scr_non_nlls.append(np.mean(non_scr, axis=0))\n",
    "\n",
    "scr_ext_pooled = np.concatenate(scr_ext_nlls)\n",
    "scr_non_pooled = np.concatenate(scr_non_nlls)\n",
    "diff_scr = scr_ext_pooled - scr_non_pooled\n",
    "d_scr = cohens_d(diff_scr)\n",
    "_, p_scr = stats.ttest_1samp(diff_scr, 0)\n",
    "sig_scr = ('***' if p_scr < 0.001 else '**' if p_scr < 0.01\n",
    "           else '*' if p_scr < 0.05 else 'ns')\n",
    "\n",
    "print(f\"  Scrambled extraction mean sem delta: {np.mean(scr_ext_pooled):+.4f}\")\n",
    "print(f\"  Scrambled non-extraction mean sem delta: {np.mean(scr_non_pooled):+.4f}\")\n",
    "print(f\"  Paired diff: d={d_scr:+.4f}, p={p_scr:.2e} {sig_scr}\")\n",
    "if d_scr > 0.05 and p_scr < 0.05:\n",
    "    print(f\"  --> H4 SUPPORTED: extraction vocabulary alone activates relevant representations\")\n",
    "else:\n",
    "    print(f\"  --> H4 NOT SUPPORTED: vocabulary category doesn't matter when scrambled\")\n",
    "\n",
    "# ================================================================\n",
    "# PART 6: H5 — Repetition effect\n",
    "# ================================================================\n",
    "print(f\"\\n--- PART 6: Repetition Effect (H5) ---\")\n",
    "print(f\"  extract_minimal (~7 tokens, heavily repeated) vs extract_general (~18 tokens)\")\n",
    "\n",
    "for ds_name in DATASET_NAMES:\n",
    "    nlls = hard_nlls[ds_name]\n",
    "    if 'extract_minimal' in nlls and 'extract_general' in nlls:\n",
    "        diff = nlls['extract_general'] - nlls['extract_minimal']\n",
    "        d = cohens_d(diff)\n",
    "        _, p = stats.ttest_1samp(diff, 0)\n",
    "        sig = ('***' if p < 0.001 else '**' if p < 0.01\n",
    "               else '*' if p < 0.05 else 'ns')\n",
    "        win = 100 * np.mean(diff > 0)\n",
    "        print(f\"  {ds_name:<16}: extract_minimal wins {win:.1f}%, d={d:+.4f}, p={p:.2e} {sig}\")\n",
    "\n",
    "# Pooled\n",
    "min_diffs = []\n",
    "for ds_name in DATASET_NAMES:\n",
    "    nlls = hard_nlls[ds_name]\n",
    "    if 'extract_minimal' in nlls and 'extract_general' in nlls:\n",
    "        min_diffs.append(nlls['extract_general'] - nlls['extract_minimal'])\n",
    "min_pooled = np.concatenate(min_diffs)\n",
    "d_min = cohens_d(min_pooled)\n",
    "_, p_min = stats.ttest_1samp(min_pooled, 0)\n",
    "sig_min = ('***' if p_min < 0.001 else '**' if p_min < 0.01\n",
    "           else '*' if p_min < 0.05 else 'ns')\n",
    "print(f\"\\n  Pooled: extract_minimal vs extract_general: d={d_min:+.4f}, p={p_min:.2e} {sig_min}\")\n",
    "if d_min > 0.05 and p_min < 0.05:\n",
    "    print(f\"  --> H5 SUPPORTED: short repeated instruction outperforms longer one\")\n",
    "elif d_min < -0.05 and p_min < 0.05:\n",
    "    print(f\"  --> H5 REFUTED: longer instruction outperforms short repeated one\")\n",
    "else:\n",
    "    print(f\"  --> H5 INCONCLUSIVE: no significant difference\")\n",
    "\n",
    "# ================================================================\n",
    "# PART 7: Cross-dataset consistency\n",
    "# ================================================================\n",
    "print(f\"\\n--- PART 7: Cross-Dataset Consistency ---\")\n",
    "print(f\"\\n  {'Condition':<28}\", end=\"\")\n",
    "for ds_name in DATASET_NAMES:\n",
    "    print(f\" {ds_name[:10]:>10}\", end=\"\")\n",
    "print(f\"  {'mean':>8} {'consistent':>10}\")\n",
    "print(f\"  {'-'*90}\")\n",
    "\n",
    "cross_dataset = {}\n",
    "for cond in META_CONDS:\n",
    "    ds_vals = []\n",
    "    for ds_name in DATASET_NAMES:\n",
    "        nlls = hard_nlls[ds_name]\n",
    "        if cond not in nlls:\n",
    "            ds_vals.append(float('nan'))\n",
    "            continue\n",
    "        sem_delta = nlls['random_tokens'] - nlls[cond]\n",
    "        d = cohens_d(sem_delta)\n",
    "        ds_vals.append(d)\n",
    "\n",
    "    valid = [v for v in ds_vals if not np.isnan(v)]\n",
    "    if not valid:\n",
    "        continue\n",
    "    mean_d = np.mean(valid)\n",
    "    same_sign = all(v >= 0 for v in valid) or all(v <= 0 for v in valid)\n",
    "    row = f\"  {cond:<28}\"\n",
    "    for v in ds_vals:\n",
    "        row += f\" {v:>+10.3f}\" if not np.isnan(v) else f\" {'N/A':>10}\"\n",
    "    row += f\"  {mean_d:>+8.3f} {'YES' if same_sign else 'NO':>10}\"\n",
    "    print(row)\n",
    "    cross_dataset[cond] = {\n",
    "        ds: float(d) for ds, d in zip(DATASET_NAMES, ds_vals)\n",
    "    }\n",
    "    cross_dataset[cond]['mean'] = float(mean_d)\n",
    "    cross_dataset[cond]['consistent_sign'] = same_sign\n",
    "\n",
    "# ================================================================\n",
    "# PART 8: Instruction ranking\n",
    "# ================================================================\n",
    "print(f\"\\n--- PART 8: Instruction Ranking (pooled semantic delta d) ---\")\n",
    "ranked = sorted(meta_results.items(), key=lambda x: x[1]['pooled_d'], reverse=True)\n",
    "print(f\"\\n  {'Rank':>4} {'Condition':<28} {'pooled_d':>9} {'sig':>4}\")\n",
    "print(f\"  {'-'*50}\")\n",
    "for rank, (cond, m) in enumerate(ranked, 1):\n",
    "    sig = ('***' if m['p'] < 0.001 else '**' if m['p'] < 0.01\n",
    "           else '*' if m['p'] < 0.05 else 'ns')\n",
    "    print(f\"  {rank:>4} {cond:<28} {m['pooled_d']:>+9.4f} {sig:>4}\")\n",
    "\n",
    "# ================================================================\n",
    "# VERDICT\n",
    "# ================================================================\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"VERDICT -- Exp 04: Instruction Framing Decomposition\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\nModel: {MODEL_NAME}\")\n",
    "print(f\"Scoring: BOS-retained repositioning + token-level prefix matching\")\n",
    "print(f\"Datasets: {len(DATASET_NAMES)} ({', '.join(DATASET_NAMES)})\")\n",
    "print(f\"Hard selection: top {HARD_FRAC*100:.0f}% by bare NLL\")\n",
    "\n",
    "print(f\"\\n--- Hypothesis Verdicts ---\")\n",
    "\n",
    "# H1\n",
    "print(f\"\\n  H1 (Extraction framing is specifically beneficial):\")\n",
    "print(f\"    Extraction mean pooled_d: {np.mean(extraction_ds):+.4f}\")\n",
    "print(f\"    Non-extraction mean pooled_d: {np.mean(non_extraction_ds):+.4f}\")\n",
    "print(f\"    Paired diff d={d_ext_vs_non:+.4f}, p={p_ext_vs_non:.2e}\")\n",
    "\n",
    "# H2\n",
    "print(f\"\\n  H2 (Any coherent task-relevant instruction helps equally):\")\n",
    "all_coherent_ds = [meta_results[c]['pooled_d'] for c in ALL_INSTRUCTION_CONDS if c in meta_results]\n",
    "print(f\"    All coherent instructions mean pooled_d: {np.mean(all_coherent_ds):+.4f}\")\n",
    "print(f\"    Range: [{min(all_coherent_ds):+.4f}, {max(all_coherent_ds):+.4f}]\")\n",
    "n_sig_pos = sum(1 for c in ALL_INSTRUCTION_CONDS\n",
    "                if c in meta_results and meta_results[c]['pooled_d'] > 0 and meta_results[c]['p'] < 0.05)\n",
    "print(f\"    {n_sig_pos}/{len(ALL_INSTRUCTION_CONDS)} significantly positive\")\n",
    "\n",
    "# H3\n",
    "print(f\"\\n  H3 (Structure matters, content doesn't):\")\n",
    "print(f\"    {n_sig_meaning}/{len(meaning_tests)} instructions have significant meaning effect\")\n",
    "\n",
    "# H4\n",
    "print(f\"\\n  H4 (Extraction vocabulary activates relevant representations):\")\n",
    "print(f\"    Scrambled ext vs non-ext: d={d_scr:+.4f}, p={p_scr:.2e}\")\n",
    "\n",
    "# H5\n",
    "print(f\"\\n  H5 (Short repeated instruction combines repetition + semantics):\")\n",
    "print(f\"    extract_minimal vs extract_general: d={d_min:+.4f}, p={p_min:.2e}\")\n",
    "\n",
    "# Summary\n",
    "print(f\"\\n--- Key decomposition finding ---\")\n",
    "mean_struct = np.mean([pooled_decomp[c]['structural'] for c in ALL_INSTRUCTION_CONDS])\n",
    "mean_vocab = np.mean([pooled_decomp[c]['vocabulary'] for c in ALL_INSTRUCTION_CONDS])\n",
    "mean_meaning = np.mean([pooled_decomp[c]['meaning'] for c in ALL_INSTRUCTION_CONDS])\n",
    "mean_total = np.mean([pooled_decomp[c]['total'] for c in ALL_INSTRUCTION_CONDS])\n",
    "print(f\"  Mean across all 8 instructions (pooled NLL delta):\")\n",
    "print(f\"    Structural: {mean_struct:+.4f}\")\n",
    "print(f\"    Vocabulary:  {mean_vocab:+.4f}\")\n",
    "print(f\"    Meaning:     {mean_meaning:+.4f}\")\n",
    "print(f\"    Total:       {mean_total:+.4f}\")\n",
    "if abs(mean_total) > 0.001:\n",
    "    pct_struct = mean_struct / mean_total * 100\n",
    "    pct_vocab = mean_vocab / mean_total * 100\n",
    "    pct_meaning = mean_meaning / mean_total * 100\n",
    "    print(f\"    Decomposition: {pct_struct:.0f}% structural, \"\n",
    "          f\"{pct_vocab:.0f}% vocabulary, {pct_meaning:.0f}% meaning\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a493eef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-22T18:13:12.066096Z",
     "iopub.status.busy": "2026-02-22T18:13:12.065458Z",
     "iopub.status.idle": "2026-02-22T18:13:12.605074Z",
     "shell.execute_reply": "2026-02-22T18:13:12.604381Z"
    },
    "papermill": {
     "duration": 0.548421,
     "end_time": "2026-02-22T18:13:12.606739",
     "exception": false,
     "start_time": "2026-02-22T18:13:12.058318",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "SAVING RESULTS\n",
      "======================================================================\n",
      "Results saved to ../../../results/decoder_only/exp04/results.json\n",
      "\n",
      "Cleaning up GPU memory...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory: 24.39 GB -> 0.01 GB\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Save results\n",
    "print(\"=\" * 70)\n",
    "print(\"SAVING RESULTS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "final_results = {\n",
    "    'experiment': 'v4_exp04_instruction_framing_decomposition',\n",
    "    'model': MODEL_NAME,\n",
    "    'scoring': 'bos_retained_repositioning_token_matched',\n",
    "    'hard_fraction': HARD_FRAC,\n",
    "    'datasets': DATASET_NAMES,\n",
    "    'n_samples_per_dataset': N_SAMPLES,\n",
    "    'n_hard_per_dataset': N_HARD,\n",
    "    'seed': SEED,\n",
    "    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'instructions': {name: text for name, text in INSTRUCTIONS.items()},\n",
    "    'conditions': {\n",
    "        'loaded_from_exp03': LOADED_COND_NAMES,\n",
    "        'fresh_coherent': FRESH_COHERENT_CONDS,\n",
    "        'fresh_scrambled': FRESH_SCRAMBLED_CONDS,\n",
    "        'all': ALL_COND_NAMES,\n",
    "        'extraction': EXTRACTION_CONDS,\n",
    "        'non_extraction': NON_EXTRACTION_CONDS,\n",
    "    },\n",
    "    'per_dataset': {ds: per_dataset_analysis.get(ds, {}) for ds in DATASET_NAMES},\n",
    "    'meta_analysis': meta_results,\n",
    "    'pooled_decomposition': pooled_decomp,\n",
    "    'cross_dataset': cross_dataset,\n",
    "    'meaning_tests': meaning_tests,\n",
    "    'hypotheses': {\n",
    "        'H1_extraction_vs_non': {\n",
    "            'd': float(d_ext_vs_non),\n",
    "            'p': float(p_ext_vs_non),\n",
    "        },\n",
    "        'H3_meaning_n_sig': n_sig_meaning,\n",
    "        'H4_vocab_category': {\n",
    "            'd': float(d_scr),\n",
    "            'p': float(p_scr),\n",
    "        },\n",
    "        'H5_repetition': {\n",
    "            'd': float(d_min),\n",
    "            'p': float(p_min),\n",
    "        },\n",
    "    },\n",
    "    'hard_metadata': {ds: hard_metadata[ds] for ds in DATASET_NAMES},\n",
    "}\n",
    "\n",
    "with open(RESULTS_DIR / 'results.json', 'w') as f:\n",
    "    json.dump(final_results, f, indent=2)\n",
    "print(f\"Results saved to {RESULTS_DIR / 'results.json'}\")\n",
    "\n",
    "# Cleanup\n",
    "print(f\"\\nCleaning up GPU memory...\")\n",
    "mem_before = torch.cuda.memory_allocated() / 1e9\n",
    "del model, tokenizer\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "mem_after = torch.cuda.memory_allocated() / 1e9\n",
    "print(f\"GPU memory: {mem_before:.2f} GB -> {mem_after:.2f} GB\")\n",
    "print(\"Done!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2493.055061,
   "end_time": "2026-02-22T18:13:15.919548",
   "environment_variables": {},
   "exception": null,
   "input_path": "04_instruction_framing.ipynb",
   "output_path": "04_instruction_framing_executed.ipynb",
   "parameters": {},
   "start_time": "2026-02-22T17:31:42.864487",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "018443c514274a5ab7b138abcc453378": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "068dbd6de5284353a0282cbf270152b2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_14a2684833aa4c4595e9406c590a6d5b",
       "max": 1065.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_018443c514274a5ab7b138abcc453378",
       "tabbable": null,
       "tooltip": null,
       "value": 1065.0
      }
     },
     "0c3686a1a2384265ac751f287ba31bbe": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "0d8887704b694986b65a076dc80b6aa4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "14a2684833aa4c4595e9406c590a6d5b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "1aef776f414541728ccf721802e9915c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_d435334a25664cb39ce6ace55d25a4b8",
       "max": 160.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_28834d0126f04a9fa8ea9ad40e8e4043",
       "tabbable": null,
       "tooltip": null,
       "value": 160.0
      }
     },
     "1bd35d59675c414fb58d2c2d9036109f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_43374c837cdd4a26921e2c121024a3dc",
       "max": 26.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_58074bc197f140518d73d379dff0b09b",
       "tabbable": null,
       "tooltip": null,
       "value": 26.0
      }
     },
     "1c987a75aaa445e09d6dda0d310cb89d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_39f071d9162f4f09b2bd6cf5c0032255",
        "IPY_MODEL_fba152ed46db46b0bb544fca4f78ca0d",
        "IPY_MODEL_feded02971714a6e8391e430e2588f51"
       ],
       "layout": "IPY_MODEL_a954125a384947a5a885bc6223f5f16a",
       "tabbable": null,
       "tooltip": null
      }
     },
     "2547c8b0ea7e476bae79e845c3fbb94f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_4f62b3fe0b374f778366380b1afba489",
       "placeholder": "​",
       "style": "IPY_MODEL_7db8ff12ab6f45a59195ac532554b3a9",
       "tabbable": null,
       "tooltip": null,
       "value": "Score triviaqa: 100%"
      }
     },
     "28316d06e5e642d3875f2c21a0b0ec99": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "28834d0126f04a9fa8ea9ad40e8e4043": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "2db9a30067414883999a5a19f0b6ebc0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_6c35c05370fa45bebabbf8f966ba84ae",
       "placeholder": "​",
       "style": "IPY_MODEL_8e1f6a0e478c4104bbd5188946af23b0",
       "tabbable": null,
       "tooltip": null,
       "value": "Loading weights: 100%"
      }
     },
     "332045618be84ddcbd1f04f843428f67": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_2db9a30067414883999a5a19f0b6ebc0",
        "IPY_MODEL_068dbd6de5284353a0282cbf270152b2",
        "IPY_MODEL_c504e6825c8745589b248e91b22870ac"
       ],
       "layout": "IPY_MODEL_6bbc5847beb34b698b4923bd24d3847b",
       "tabbable": null,
       "tooltip": null
      }
     },
     "39f071d9162f4f09b2bd6cf5c0032255": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_d87c6cc0ed874e458c2d01101308e922",
       "placeholder": "​",
       "style": "IPY_MODEL_7a6bc584051c4a9fabb45a643bad8886",
       "tabbable": null,
       "tooltip": null,
       "value": "Score hotpotqa: 100%"
      }
     },
     "3c6d9d80f3a443e0a4f62f419e8dcde0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "416b9becc82d4e28a1874c9206274e13": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "43374c837cdd4a26921e2c121024a3dc": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "44799baf35654d45a88fe8311f7ed6f1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "46c1288b10254021908aef18627649ca": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_dfb4dd7b1c254927a074e183282e0a80",
        "IPY_MODEL_1aef776f414541728ccf721802e9915c",
        "IPY_MODEL_a1b20da28cd24e5cbe1fdb1829e0a283"
       ],
       "layout": "IPY_MODEL_9f09042389ff4a1981a6975ee8088565",
       "tabbable": null,
       "tooltip": null
      }
     },
     "46fc3af6ecb347cbbc37808cfd90d877": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_be41dde6fb7e49ab885f959743d7252f",
       "placeholder": "​",
       "style": "IPY_MODEL_9d1cc4ec74284dfd8d3422c15fbe7c0c",
       "tabbable": null,
       "tooltip": null,
       "value": " 160/160 [11:09&lt;00:00,  4.12s/it]"
      }
     },
     "4c8553778b174717a7628e21a9aff54f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_28316d06e5e642d3875f2c21a0b0ec99",
       "placeholder": "​",
       "style": "IPY_MODEL_68682cbe5214401aa524555e3e8fb5da",
       "tabbable": null,
       "tooltip": null,
       "value": " 26/26 [00:00&lt;00:00, 2753.14it/s]"
      }
     },
     "4f62b3fe0b374f778366380b1afba489": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "53c8f09493ff4df89f4ed82b8a633a97": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "54dcf6cb1a5f42459702f02649474eac": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "58074bc197f140518d73d379dff0b09b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "5906bb5664bb4b91971a4865bcc08785": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "6204aa900c28494fad6c41c35183cd3b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6588df1225db4ade823c1835fc988c4d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "68682cbe5214401aa524555e3e8fb5da": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "6bbc5847beb34b698b4923bd24d3847b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6c35c05370fa45bebabbf8f966ba84ae": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6d2cdf945a3b4fec88808338aa14129f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "775ae3cd8e724d87961cd92a62e6195d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "7a6bc584051c4a9fabb45a643bad8886": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "7db8ff12ab6f45a59195ac532554b3a9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "86e7327022a1476c92eb212d2fd6ffb7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "8c50693769cf452a8b92ee9c515a4144": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "8e1f6a0e478c4104bbd5188946af23b0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "94bc62e97c484faeaa048462aca2bccb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_e2a067b6418f432eaa21057403ad8306",
       "max": 160.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_0d8887704b694986b65a076dc80b6aa4",
       "tabbable": null,
       "tooltip": null,
       "value": 160.0
      }
     },
     "979f6821202145c782386b1861a0048f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_e067a6387e814f4eaff4aa0386431742",
       "placeholder": "​",
       "style": "IPY_MODEL_775ae3cd8e724d87961cd92a62e6195d",
       "tabbable": null,
       "tooltip": null,
       "value": "Resolving data files: 100%"
      }
     },
     "9d1cc4ec74284dfd8d3422c15fbe7c0c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "9f09042389ff4a1981a6975ee8088565": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "9f12adcc29bf4c828f6388e9ffa94c68": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_2547c8b0ea7e476bae79e845c3fbb94f",
        "IPY_MODEL_d44495d461264eb6a40e95a2c01852f8",
        "IPY_MODEL_46fc3af6ecb347cbbc37808cfd90d877"
       ],
       "layout": "IPY_MODEL_54dcf6cb1a5f42459702f02649474eac",
       "tabbable": null,
       "tooltip": null
      }
     },
     "9fd59aa37fe04c8c8c85ee45143fd19f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_b90251d152c84c3193543a6b8e3b95e7",
        "IPY_MODEL_94bc62e97c484faeaa048462aca2bccb",
        "IPY_MODEL_fb144a795a8b4b31844fda22c6330b65"
       ],
       "layout": "IPY_MODEL_86e7327022a1476c92eb212d2fd6ffb7",
       "tabbable": null,
       "tooltip": null
      }
     },
     "a1b20da28cd24e5cbe1fdb1829e0a283": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_d9e36cc2bee74768839d470c6169f0f1",
       "placeholder": "​",
       "style": "IPY_MODEL_44799baf35654d45a88fe8311f7ed6f1",
       "tabbable": null,
       "tooltip": null,
       "value": " 160/160 [09:57&lt;00:00,  3.77s/it]"
      }
     },
     "a954125a384947a5a885bc6223f5f16a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ac1bff26f06b49df976c2dce935bc1d7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b90251d152c84c3193543a6b8e3b95e7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_6204aa900c28494fad6c41c35183cd3b",
       "placeholder": "​",
       "style": "IPY_MODEL_5906bb5664bb4b91971a4865bcc08785",
       "tabbable": null,
       "tooltip": null,
       "value": "Score ms_marco: 100%"
      }
     },
     "be41dde6fb7e49ab885f959743d7252f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "bff615ff7a7a433daa18c075226d1d58": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "c504e6825c8745589b248e91b22870ac": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_c7cb7216c5e24b4892c8d1fe3c5f9322",
       "placeholder": "​",
       "style": "IPY_MODEL_f3827ab4e9094668817eb8213869a6e3",
       "tabbable": null,
       "tooltip": null,
       "value": " 1065/1065 [00:06&lt;00:00, 643.49it/s, Materializing param=model.vision_tower.vision_model.post_layernorm.weight]"
      }
     },
     "c7cb7216c5e24b4892c8d1fe3c5f9322": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "cbf50c2809324117a2fc382f09d8ecab": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_979f6821202145c782386b1861a0048f",
        "IPY_MODEL_1bd35d59675c414fb58d2c2d9036109f",
        "IPY_MODEL_4c8553778b174717a7628e21a9aff54f"
       ],
       "layout": "IPY_MODEL_dd2dc5f01e404e46bfaa2ecde1b498a5",
       "tabbable": null,
       "tooltip": null
      }
     },
     "d435334a25664cb39ce6ace55d25a4b8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d44495d461264eb6a40e95a2c01852f8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_6588df1225db4ade823c1835fc988c4d",
       "max": 160.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_dca92528bcb1487992947725c23aaef1",
       "tabbable": null,
       "tooltip": null,
       "value": 160.0
      }
     },
     "d87c6cc0ed874e458c2d01101308e922": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d9e36cc2bee74768839d470c6169f0f1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "dca92528bcb1487992947725c23aaef1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "dd2dc5f01e404e46bfaa2ecde1b498a5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "dfb4dd7b1c254927a074e183282e0a80": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_8c50693769cf452a8b92ee9c515a4144",
       "placeholder": "​",
       "style": "IPY_MODEL_0c3686a1a2384265ac751f287ba31bbe",
       "tabbable": null,
       "tooltip": null,
       "value": "Score squad_v2: 100%"
      }
     },
     "e067a6387e814f4eaff4aa0386431742": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e2a067b6418f432eaa21057403ad8306": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f3827ab4e9094668817eb8213869a6e3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "fb144a795a8b4b31844fda22c6330b65": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_6d2cdf945a3b4fec88808338aa14129f",
       "placeholder": "​",
       "style": "IPY_MODEL_bff615ff7a7a433daa18c075226d1d58",
       "tabbable": null,
       "tooltip": null,
       "value": " 160/160 [09:51&lt;00:00,  3.75s/it]"
      }
     },
     "fba152ed46db46b0bb544fca4f78ca0d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_416b9becc82d4e28a1874c9206274e13",
       "max": 160.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_3c6d9d80f3a443e0a4f62f419e8dcde0",
       "tabbable": null,
       "tooltip": null,
       "value": 160.0
      }
     },
     "feded02971714a6e8391e430e2588f51": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_ac1bff26f06b49df976c2dce935bc1d7",
       "placeholder": "​",
       "style": "IPY_MODEL_53c8f09493ff4df89f4ed82b8a633a97",
       "tabbable": null,
       "tooltip": null,
       "value": " 160/160 [09:56&lt;00:00,  3.75s/it]"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}