{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f733e31",
   "metadata": {},
   "source": [
    "# Experiment 04: Instruction Framing Decomposition\n",
    "\n",
    "## Motivation\n",
    "\n",
    "Exp 03 found that `extractor_matched` — a generic extraction instruction\n",
    "(\"Extract all key data points, facts, entities, and specific attributes from the\n",
    "following text.\") token-matched to Q tokens — is the **best** KV cache prefix\n",
    "across all 4 datasets (pooled semantic delta d=+0.357, \\*\\*\\*). Oracle HURTS\n",
    "(d=-0.253), LLM doc-specific surrogates lose to the generic instruction, and\n",
    "there is no semantic gradient.\n",
    "\n",
    "**Why does the extraction instruction work?** Five hypotheses:\n",
    "- **H1**: Extraction framing is specifically beneficial (vs other task framings)\n",
    "- **H2**: Any coherent task-relevant instruction helps equally\n",
    "- **H3**: Instruction structure (coherent syntax) matters, content doesn't\n",
    "- **H4**: Extraction-related token vocabulary activates relevant representations\n",
    "- **H5**: Short repeated instruction phrases combine structural repetition with semantic content\n",
    "\n",
    "## Design\n",
    "\n",
    "### Three-Level Decomposition (key metric)\n",
    "\n",
    "For each instruction I:\n",
    "- **Structural** = NLL(bare) - NLL(random_tokens) — effect of any tokens in cache\n",
    "- **Vocabulary** = NLL(random_tokens) - NLL(scrambled_I) — effect of instruction token vocabulary\n",
    "- **Meaning** = NLL(scrambled_I) - NLL(coherent_I) — effect of coherent instruction order\n",
    "- **Total** = structural + vocabulary + meaning = NLL(bare) - NLL(coherent_I)\n",
    "\n",
    "### Conditions (20 total: 15 scored fresh + 5 loaded from Exp 03)\n",
    "\n",
    "**8 coherent instructions** (4 extraction + 3 non-extraction + 1 special):\n",
    "\n",
    "| # | Name | Category | Text |\n",
    "|---|------|----------|------|\n",
    "| 1 | `extract_general` | extraction | \"Extract all key data points, facts, entities, and specific attributes from the following text.\" (**LOADED** as `extractor_matched` from Exp 03) |\n",
    "| 2 | `extract_entities` | extraction | \"Identify and list every named entity including people, locations, organizations, and dates mentioned in this text.\" |\n",
    "| 3 | `extract_claims` | extraction | \"Extract all factual claims, statistics, numerical data, and specific assertions made in this passage.\" |\n",
    "| 4 | `extract_qa` | extraction | \"Extract information from this text that would help answer questions about its content and meaning.\" |\n",
    "| 5 | `comprehend` | non-extraction | \"Read and understand the main ideas, arguments, and supporting details presented in the following text.\" |\n",
    "| 6 | `generate_qa` | non-extraction | \"Generate questions that could be answered using the specific facts and details in this passage.\" |\n",
    "| 7 | `classify` | non-extraction | \"Determine the subject matter, text type, writing style, and intended audience of this passage.\" |\n",
    "| 8 | `extract_minimal` | special | \"Extract key facts from the text.\" (~7 tokens, heavily repeated in Q-token prefix) |\n",
    "\n",
    "**8 scrambled controls**: For each coherent instruction, permute the Q-length\n",
    "token-matched prefix (after `make_prefix`) with a per-condition+sample seed.\n",
    "Same token multiset, destroyed meaning.\n",
    "\n",
    "**5 loaded from Exp 03**: `bare`, `random_tokens`, `repeat_token`,\n",
    "`extractor_matched` (= `extract_general`), `adversarial_matched`\n",
    "\n",
    "### Key Analyses\n",
    "\n",
    "1. Three-level decomposition for each of 8 instructions, pooled across datasets\n",
    "2. Extraction vs non-extraction (H1 vs H2): paired test\n",
    "3. Coherent vs scrambled (H3): per-instruction paired t-test\n",
    "4. Vocabulary by category (H4): scrambled extraction vs scrambled non-extraction\n",
    "5. Repetition effect (H5): extract_minimal vs extract_general\n",
    "6. Cross-dataset consistency and instruction ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403afe0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Setup, model loading, and scoring functions\n",
    "import os\n",
    "os.umask(0o000)\n",
    "import sys, json, time, gc, re\n",
    "import random as pyrandom\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from scipy import stats\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "sys.path.insert(0, \"../../..\")\n",
    "from lib.analysis import cohens_d\n",
    "from lib.data import count_words\n",
    "\n",
    "SEED = 42\n",
    "N_SAMPLES = 400      # per dataset\n",
    "HARD_FRAC = 0.40     # top 40% by bare NLL\n",
    "MODEL_NAME = \"google/gemma-3-12b-it\"\n",
    "\n",
    "RESULTS_DIR = Path(\"../../../results/decoder_only/exp04\")\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "EXP02_DIR = Path(\"../../../results/decoder_only/exp02\")\n",
    "EXP03_DIR = Path(\"../../../results/decoder_only/exp03\")\n",
    "\n",
    "DATASET_NAMES = ['ms_marco', 'squad_v2', 'triviaqa', 'hotpotqa']\n",
    "NEW_DATASETS = ['squad_v2', 'triviaqa', 'hotpotqa']\n",
    "\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "pyrandom.seed(SEED)\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv())\n",
    "HF_TOKEN = os.environ.get(\"HF_TOKEN\")\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, DynamicCache\n",
    "\n",
    "print(f\"Loading {MODEL_NAME}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, token=HF_TOKEN)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME, device_map=\"auto\", dtype=torch.bfloat16, token=HF_TOKEN,\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "DEVICE = next(model.parameters()).device\n",
    "BOS_ID = tokenizer.bos_token_id\n",
    "NEWLINE_IDS = tokenizer(\"\\n\", add_special_tokens=False).input_ids\n",
    "text_cfg = getattr(model.config, 'text_config', model.config)\n",
    "# Use actual embedding table size (config vocab_size may include padding rows)\n",
    "VOCAB_SIZE = model.get_input_embeddings().num_embeddings\n",
    "cfg_vocab = getattr(text_cfg, 'vocab_size', None)\n",
    "if cfg_vocab != VOCAB_SIZE:\n",
    "    print(f\"WARNING: config vocab_size={cfg_vocab} != embedding size={VOCAB_SIZE}\")\n",
    "    print(f\"Using embedding size {VOCAB_SIZE} for random token generation\")\n",
    "rope_params = getattr(text_cfg, 'rope_parameters', {})\n",
    "layer_types = getattr(text_cfg, 'layer_types', [])\n",
    "# Sliding attention layers cache only (sliding_window - 1) entries.\n",
    "# select_kv_cache uses uniform indices across all layers, so total Phase A\n",
    "# tokens must not exceed this limit when a prefix is used.\n",
    "SLIDING_WINDOW = getattr(text_cfg, 'sliding_window', 4096)\n",
    "SLIDING_CACHE_LIMIT = SLIDING_WINDOW - 1  # observed: 1024-1 = 1023 for Gemma 3\n",
    "\n",
    "print(f\"Exp 04: Instruction Framing Decomposition\")\n",
    "print(f\"Scoring: BOS-retained repositioning + token-level prefix matching\")\n",
    "print(f\"N_SAMPLES: {N_SAMPLES} per dataset, HARD_FRAC: {HARD_FRAC}\")\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"DEVICE: {DEVICE}, dtype: {next(model.parameters()).dtype}\")\n",
    "print(f\"GPU memory: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "print(f\"Vocab size: {VOCAB_SIZE}\")\n",
    "print(f\"Sliding window: {SLIDING_WINDOW}, cache limit: {SLIDING_CACHE_LIMIT}\")\n",
    "print(f\"Num layers: {getattr(text_cfg, 'num_hidden_layers', 'N/A')}\")\n",
    "\n",
    "# --- RoPE repositioning helpers ---\n",
    "def build_layer_inv_freqs():\n",
    "    inv_freqs = {}\n",
    "    for lt, params in rope_params.items():\n",
    "        theta = params.get('rope_theta', 10000.0)\n",
    "        dim = text_cfg.head_dim\n",
    "        inv_freq = 1.0 / (theta ** (torch.arange(0, dim, 2, dtype=torch.float32, device=DEVICE) / dim))\n",
    "        inv_freqs[lt] = inv_freq\n",
    "    return inv_freqs\n",
    "\n",
    "LAYER_INV_FREQS = build_layer_inv_freqs()\n",
    "\n",
    "\n",
    "def rotate_half(x):\n",
    "    x1, x2 = x[..., :x.shape[-1]//2], x[..., x.shape[-1]//2:]\n",
    "    return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "\n",
    "def select_kv_cache(cache, indices):\n",
    "    selected = DynamicCache()\n",
    "    idx_tensor = torch.tensor(indices, dtype=torch.long, device=DEVICE)\n",
    "    for i in range(len(cache.layers)):\n",
    "        k = cache.layers[i].keys[:, :, idx_tensor, :]\n",
    "        v = cache.layers[i].values[:, :, idx_tensor, :]\n",
    "        selected.update(k, v, i)\n",
    "    return selected\n",
    "\n",
    "\n",
    "def reposition_kv_cache(cache, old_positions, new_positions, bos_start=0):\n",
    "    delta = new_positions - old_positions\n",
    "    for L in range(len(cache.layers)):\n",
    "        lt = layer_types[L]\n",
    "        inv_freq = LAYER_INV_FREQS[lt]\n",
    "        k = cache.layers[L].keys\n",
    "        doc_keys = k[:, :, bos_start + 1:, :]\n",
    "        freqs = torch.einsum('i,j->ij', delta.float(), inv_freq)\n",
    "        emb = torch.cat([freqs, freqs], dim=-1)\n",
    "        cos_delta = emb.cos().to(k.dtype).unsqueeze(0).unsqueeze(0)\n",
    "        sin_delta = emb.sin().to(k.dtype).unsqueeze(0).unsqueeze(0)\n",
    "        doc_keys_new = doc_keys * cos_delta + rotate_half(doc_keys) * sin_delta\n",
    "        cache.layers[L].keys = torch.cat([\n",
    "            k[:, :, :bos_start + 1, :],\n",
    "            doc_keys_new,\n",
    "        ], dim=2)\n",
    "    return cache\n",
    "\n",
    "\n",
    "def score(doc_text, query_text, answer_text, prefix_token_ids=None):\n",
    "    # BOS-retained repositioning.\n",
    "    doc_ids = tokenizer(doc_text, add_special_tokens=False,\n",
    "                        truncation=True, max_length=1024).input_ids\n",
    "\n",
    "    if prefix_token_ids is not None:\n",
    "        P = len(prefix_token_ids)\n",
    "        NL = len(NEWLINE_IDS)\n",
    "        # Truncate doc so total Phase A tokens fit in sliding window cache.\n",
    "        max_doc = SLIDING_CACHE_LIMIT - 1 - P - NL  # 1 for BOS\n",
    "        if len(doc_ids) > max_doc:\n",
    "            doc_ids = doc_ids[:max_doc]\n",
    "        D = len(doc_ids)\n",
    "        cond_ids = [BOS_ID] + list(prefix_token_ids) + NEWLINE_IDS + doc_ids\n",
    "        with torch.no_grad():\n",
    "            pa = model(input_ids=torch.tensor([cond_ids], device=DEVICE),\n",
    "                       use_cache=True)\n",
    "        cache = pa.past_key_values\n",
    "        del pa\n",
    "        keep_indices = [0] + list(range(1 + P + NL, len(cond_ids)))\n",
    "        cache = select_kv_cache(cache, keep_indices)\n",
    "        old_pos = torch.arange(1 + P + NL, 1 + P + NL + D, device=DEVICE)\n",
    "        new_pos = torch.arange(1, D + 1, device=DEVICE)\n",
    "        cache = reposition_kv_cache(cache, old_pos, new_pos, bos_start=0)\n",
    "    else:\n",
    "        D = len(doc_ids)\n",
    "        with torch.no_grad():\n",
    "            pa = model(input_ids=torch.tensor([[BOS_ID] + doc_ids], device=DEVICE),\n",
    "                       use_cache=True)\n",
    "        cache = pa.past_key_values\n",
    "        del pa\n",
    "\n",
    "    phase_b_start = D + 1\n",
    "    query_ids = tokenizer(\"\\n\" + query_text + \"\\n\",\n",
    "                          add_special_tokens=False).input_ids\n",
    "    answer_ids = tokenizer(answer_text, add_special_tokens=False,\n",
    "                           truncation=True, max_length=256).input_ids\n",
    "    if not answer_ids:\n",
    "        del cache\n",
    "        return 0.0\n",
    "\n",
    "    pb_ids = query_ids + answer_ids\n",
    "    pos = torch.arange(phase_b_start, phase_b_start + len(pb_ids), device=DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pb = model(\n",
    "            input_ids=torch.tensor([pb_ids], device=DEVICE),\n",
    "            past_key_values=cache,\n",
    "            position_ids=pos.unsqueeze(0),\n",
    "            use_cache=False,\n",
    "        )\n",
    "\n",
    "    n_q = len(query_ids)\n",
    "    logits = pb.logits[0, n_q - 1:n_q - 1 + len(answer_ids), :].float()\n",
    "    targets = torch.tensor(answer_ids, device=DEVICE)\n",
    "    nll = -F.log_softmax(logits, dim=-1).gather(\n",
    "        1, targets.unsqueeze(1)).squeeze(1).mean().item()\n",
    "    del cache, pb\n",
    "    return nll\n",
    "\n",
    "\n",
    "def make_prefix(token_ids, Q):\n",
    "    if len(token_ids) >= Q:\n",
    "        return token_ids[:Q]\n",
    "    else:\n",
    "        padded = token_ids * ((Q // max(len(token_ids), 1)) + 1)\n",
    "        return padded[:Q]\n",
    "\n",
    "\n",
    "def scramble_prefix(prefix_ids, seed):\n",
    "    # Permute Q-length prefix token IDs (after make_prefix).\n",
    "    # Same token multiset, destroyed meaning.\n",
    "    rng = pyrandom.Random(seed)\n",
    "    shuffled = list(prefix_ids)\n",
    "    rng.shuffle(shuffled)\n",
    "    return shuffled\n",
    "\n",
    "\n",
    "# --- Instruction definitions ---\n",
    "INSTRUCTIONS = {\n",
    "    # 4 extraction instructions\n",
    "    'extract_general': \"Extract all key data points, facts, entities, and specific attributes from the following text.\",\n",
    "    'extract_entities': \"Identify and list every named entity including people, locations, organizations, and dates mentioned in this text.\",\n",
    "    'extract_claims': \"Extract all factual claims, statistics, numerical data, and specific assertions made in this passage.\",\n",
    "    'extract_qa': \"Extract information from this text that would help answer questions about its content and meaning.\",\n",
    "    # 3 non-extraction instructions\n",
    "    'comprehend': \"Read and understand the main ideas, arguments, and supporting details presented in the following text.\",\n",
    "    'generate_qa': \"Generate questions that could be answered using the specific facts and details in this passage.\",\n",
    "    'classify': \"Determine the subject matter, text type, writing style, and intended audience of this passage.\",\n",
    "    # 1 special (short, heavily repeated)\n",
    "    'extract_minimal': \"Extract key facts from the text.\",\n",
    "}\n",
    "\n",
    "EXTRACTION_CONDS = ['extract_general', 'extract_entities', 'extract_claims', 'extract_qa']\n",
    "NON_EXTRACTION_CONDS = ['comprehend', 'generate_qa', 'classify']\n",
    "ALL_INSTRUCTION_CONDS = list(INSTRUCTIONS.keys())\n",
    "\n",
    "# Conditions scored fresh in this experiment (exclude extract_general = loaded from Exp 03)\n",
    "FRESH_COHERENT_CONDS = [c for c in ALL_INSTRUCTION_CONDS if c != 'extract_general']\n",
    "FRESH_SCRAMBLED_CONDS = [f'scrambled_{c}' for c in ALL_INSTRUCTION_CONDS]\n",
    "FRESH_COND_NAMES = FRESH_COHERENT_CONDS + FRESH_SCRAMBLED_CONDS\n",
    "\n",
    "# Conditions loaded from Exp 03\n",
    "LOADED_COND_NAMES = ['bare', 'random_tokens', 'repeat_token',\n",
    "                     'extractor_matched', 'adversarial_matched']\n",
    "\n",
    "# All 20 conditions (for analysis)\n",
    "ALL_COND_NAMES = LOADED_COND_NAMES + ['extract_general'] + FRESH_COHERENT_CONDS + FRESH_SCRAMBLED_CONDS\n",
    "\n",
    "SCORING_KEY = 'bos_retained_token_matched_v04'\n",
    "\n",
    "# Pre-tokenize all instructions\n",
    "INSTRUCTION_IDS = {}\n",
    "for name, text in INSTRUCTIONS.items():\n",
    "    ids = tokenizer(text, add_special_tokens=False).input_ids\n",
    "    INSTRUCTION_IDS[name] = ids\n",
    "    print(f\"  {name:<20}: {len(ids)} tokens -> '{text[:60]}...'\")\n",
    "\n",
    "print(f\"\\nSetup complete. Functions defined: score, make_prefix, scramble_prefix\")\n",
    "print(f\"Fresh conditions to score: {len(FRESH_COND_NAMES)} \"\n",
    "      f\"({len(FRESH_COHERENT_CONDS)} coherent + {len(FRESH_SCRAMBLED_CONDS)} scrambled)\")\n",
    "print(f\"Loaded from Exp 03: {len(LOADED_COND_NAMES)}\")\n",
    "print(f\"Total conditions: {len(ALL_COND_NAMES)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bee1709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Reload datasets (for passage text) + load Exp 03 baselines\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Per-dataset seeds (same as Exp 03)\n",
    "DS_SEEDS = {\n",
    "    'squad_v2': SEED + 100,\n",
    "    'triviaqa': SEED + 200,\n",
    "    'hotpotqa': SEED + 300,\n",
    "}\n",
    "\n",
    "# ================================================================\n",
    "# PART 1: Load MS MARCO from Exp 02 checkpoint\n",
    "# ================================================================\n",
    "print(\"=\" * 70)\n",
    "print(\"LOADING MS MARCO from Exp 02 + Exp 03 baselines\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "assert EXP02_DIR.exists(), f\"Exp 02 results not found at {EXP02_DIR}\"\n",
    "exp02_ckpt = json.loads((EXP02_DIR / \"checkpoint.json\").read_text())\n",
    "assert exp02_ckpt.get('scoring') == 'bos_retained_token_matched_v02', \\\n",
    "    f\"Unexpected scoring key: {exp02_ckpt.get('scoring')}\"\n",
    "exp02_results = exp02_ckpt['results']\n",
    "assert len(exp02_results) == N_SAMPLES, \\\n",
    "    f\"Expected {N_SAMPLES} results, got {len(exp02_results)}\"\n",
    "\n",
    "# Extract bare NLLs and select hard 40% (same selection as Exp 03)\n",
    "msmarco_bare = np.array([r['nll_bare'] for r in exp02_results])\n",
    "N_HARD = int(N_SAMPLES * HARD_FRAC)\n",
    "sorted_idx = np.argsort(msmarco_bare)[::-1]\n",
    "msmarco_hard_idx = np.sort(sorted_idx[:N_HARD])\n",
    "\n",
    "print(f\"MS MARCO: {N_SAMPLES} total, top {HARD_FRAC*100:.0f}% = {N_HARD} hard samples\")\n",
    "\n",
    "# Build hard_nlls with loaded baselines\n",
    "hard_nlls = {}  # ds_name -> {cond_name: np.array}\n",
    "hard_metadata = {}\n",
    "\n",
    "hard_nlls['ms_marco'] = {}\n",
    "for cond in LOADED_COND_NAMES:\n",
    "    arr = np.array([exp02_results[i][f'nll_{cond}'] for i in msmarco_hard_idx])\n",
    "    hard_nlls['ms_marco'][cond] = arr\n",
    "\n",
    "# Map extract_general = extractor_matched (same instruction text)\n",
    "hard_nlls['ms_marco']['extract_general'] = hard_nlls['ms_marco']['extractor_matched'].copy()\n",
    "\n",
    "# Get passage text for MS MARCO hard samples (need to reload dataset)\n",
    "print(\"\\nReloading MS MARCO v1.1 for passage text...\")\n",
    "ds_msmarco = load_dataset(\"microsoft/ms_marco\", \"v1.1\", split=\"validation\")\n",
    "msmarco_candidates = []\n",
    "for item in ds_msmarco:\n",
    "    if len(msmarco_candidates) >= 3 * N_SAMPLES:\n",
    "        break\n",
    "    passages = item.get('passages', {})\n",
    "    ptexts = passages.get('passage_text', [])\n",
    "    is_sel = passages.get('is_selected', [])\n",
    "    query = item.get('query', '')\n",
    "    answers = item.get('answers', [])\n",
    "    well_formed = item.get('wellFormedAnswers', [])\n",
    "    answer = None\n",
    "    if well_formed and len(well_formed) > 0 and well_formed[0] not in ('[]', ''):\n",
    "        answer = well_formed[0]\n",
    "    elif answers and len(answers) > 0 and answers[0] != 'No Answer Present.':\n",
    "        answer = answers[0]\n",
    "    if not answer:\n",
    "        continue\n",
    "    for pt, sel in zip(ptexts, is_sel):\n",
    "        wc = count_words(pt)\n",
    "        if sel == 1 and 30 <= wc <= 300:\n",
    "            msmarco_candidates.append({\n",
    "                'passage': pt, 'query': query, 'answer': answer,\n",
    "                'word_count': wc,\n",
    "            })\n",
    "            break\n",
    "\n",
    "np.random.seed(SEED)\n",
    "indices = np.random.permutation(len(msmarco_candidates))[:N_SAMPLES]\n",
    "msmarco_all = [msmarco_candidates[i] for i in indices]\n",
    "del ds_msmarco, msmarco_candidates\n",
    "gc.collect()\n",
    "\n",
    "# Verify alignment with Exp 02 results\n",
    "for i in range(min(20, N_SAMPLES)):\n",
    "    assert msmarco_all[i]['query'][:50] == exp02_results[i]['query'][:50], \\\n",
    "        f\"MS MARCO query mismatch at sample {i}\"\n",
    "print(\"MS MARCO alignment verified (first 20 queries match Exp 02)\")\n",
    "\n",
    "# Build hard_samples for MS MARCO\n",
    "hard_samples = {}\n",
    "hs_msmarco = []\n",
    "for idx in msmarco_hard_idx:\n",
    "    s = dict(msmarco_all[idx])\n",
    "    s['nll_bare'] = float(msmarco_bare[idx])\n",
    "    s['original_idx'] = int(idx)\n",
    "    hs_msmarco.append(s)\n",
    "hard_samples['ms_marco'] = hs_msmarco\n",
    "\n",
    "hard_metadata['ms_marco'] = {\n",
    "    'n_total': N_SAMPLES,\n",
    "    'n_hard': N_HARD,\n",
    "    'source': 'exp02_reuse',\n",
    "    'mean_passage_words': float(np.mean([s['word_count'] for s in hs_msmarco])),\n",
    "    'mean_query_tokens': float(np.mean([exp02_results[i]['Q'] for i in msmarco_hard_idx])),\n",
    "    'mean_answer_words': float(np.mean([count_words(s['answer']) for s in hs_msmarco])),\n",
    "}\n",
    "\n",
    "del exp02_ckpt, exp02_results\n",
    "gc.collect()\n",
    "\n",
    "# ================================================================\n",
    "# PART 2: Reload 3 new datasets (same seeds as Exp 03)\n",
    "# ================================================================\n",
    "all_samples = {}  # ds_name -> list of N_SAMPLES dicts (for query alignment check)\n",
    "\n",
    "# ---- SQuAD 2.0 ----\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Loading SQuAD 2.0 validation...\")\n",
    "ds_squad = load_dataset(\"rajpurkar/squad_v2\", split=\"validation\")\n",
    "\n",
    "squad_candidates = []\n",
    "for item in ds_squad:\n",
    "    answers = item.get('answers', {})\n",
    "    answer_texts = answers.get('text', [])\n",
    "    if not answer_texts:\n",
    "        continue\n",
    "    passage = item['context']\n",
    "    query = item['question']\n",
    "    answer = answer_texts[0]\n",
    "    wc = count_words(passage)\n",
    "    if 30 <= wc <= 500 and count_words(answer) >= 1:\n",
    "        squad_candidates.append({\n",
    "            'passage': passage, 'query': query, 'answer': answer,\n",
    "            'word_count': wc,\n",
    "        })\n",
    "\n",
    "print(f\"SQuAD 2.0 candidates: {len(squad_candidates)}\")\n",
    "np.random.seed(DS_SEEDS['squad_v2'])\n",
    "sq_indices = np.random.permutation(len(squad_candidates))[:N_SAMPLES]\n",
    "all_samples['squad_v2'] = [squad_candidates[i] for i in sq_indices]\n",
    "del ds_squad, squad_candidates\n",
    "gc.collect()\n",
    "\n",
    "# ---- TriviaQA ----\n",
    "print(\"\\nLoading TriviaQA rc.wikipedia validation...\")\n",
    "ds_trivia = load_dataset(\"mandarjoshi/trivia_qa\", \"rc.wikipedia\", split=\"validation\")\n",
    "\n",
    "trivia_candidates = []\n",
    "for item in ds_trivia:\n",
    "    entity_pages = item.get('entity_pages', {})\n",
    "    wiki_contexts = entity_pages.get('wiki_context', [])\n",
    "    if not wiki_contexts or not wiki_contexts[0]:\n",
    "        continue\n",
    "    words = wiki_contexts[0].split()[:500]\n",
    "    passage = ' '.join(words)\n",
    "    query = item['question']\n",
    "    answer_val = item['answer']['value']\n",
    "    aliases = item['answer'].get('aliases', [])\n",
    "    passage_lower = passage.lower()\n",
    "    found = answer_val.lower() in passage_lower\n",
    "    if not found:\n",
    "        for alias in aliases:\n",
    "            if alias.lower() in passage_lower:\n",
    "                found = True\n",
    "                break\n",
    "    if not found:\n",
    "        continue\n",
    "    wc = count_words(passage)\n",
    "    if 30 <= wc <= 500 and count_words(answer_val) >= 1:\n",
    "        trivia_candidates.append({\n",
    "            'passage': passage, 'query': query, 'answer': answer_val,\n",
    "            'word_count': wc,\n",
    "        })\n",
    "\n",
    "print(f\"TriviaQA candidates: {len(trivia_candidates)}\")\n",
    "np.random.seed(DS_SEEDS['triviaqa'])\n",
    "tr_indices = np.random.permutation(len(trivia_candidates))[:N_SAMPLES]\n",
    "all_samples['triviaqa'] = [trivia_candidates[i] for i in tr_indices]\n",
    "del ds_trivia, trivia_candidates\n",
    "gc.collect()\n",
    "\n",
    "# ---- HotpotQA ----\n",
    "print(\"\\nLoading HotpotQA distractor validation...\")\n",
    "ds_hotpot = load_dataset(\"hotpotqa/hotpot_qa\", \"distractor\", split=\"validation\")\n",
    "\n",
    "hotpot_candidates = []\n",
    "for item in ds_hotpot:\n",
    "    context = item.get('context', {})\n",
    "    sf = item.get('supporting_facts', {})\n",
    "    ctx_titles = context.get('title', [])\n",
    "    ctx_sentences = context.get('sentences', [])\n",
    "    sf_titles = sf.get('title', [])\n",
    "    sf_sent_ids = sf.get('sent_id', [])\n",
    "    title_to_sents = {}\n",
    "    for title, sents in zip(ctx_titles, ctx_sentences):\n",
    "        title_to_sents[title] = sents\n",
    "    passage_parts = []\n",
    "    for title, sid in zip(sf_titles, sf_sent_ids):\n",
    "        if title in title_to_sents and sid < len(title_to_sents[title]):\n",
    "            passage_parts.append(title_to_sents[title][sid])\n",
    "    if not passage_parts:\n",
    "        continue\n",
    "    passage = ' '.join(passage_parts)\n",
    "    query = item['question']\n",
    "    answer = item['answer']\n",
    "    wc = count_words(passage)\n",
    "    if 30 <= wc <= 500 and count_words(answer) >= 1:\n",
    "        hotpot_candidates.append({\n",
    "            'passage': passage, 'query': query, 'answer': answer,\n",
    "            'word_count': wc,\n",
    "        })\n",
    "\n",
    "print(f\"HotpotQA candidates: {len(hotpot_candidates)}\")\n",
    "np.random.seed(DS_SEEDS['hotpotqa'])\n",
    "hp_indices = np.random.permutation(len(hotpot_candidates))[:N_SAMPLES]\n",
    "all_samples['hotpotqa'] = [hotpot_candidates[i] for i in hp_indices]\n",
    "del ds_hotpot, hotpot_candidates\n",
    "gc.collect()\n",
    "\n",
    "# ================================================================\n",
    "# PART 3: Load Exp 03 baselines for 3 new datasets\n",
    "# ================================================================\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"LOADING EXP 03 BASELINES for new datasets\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "assert EXP03_DIR.exists(), f\"Exp 03 results not found at {EXP03_DIR}\"\n",
    "\n",
    "for ds_name in NEW_DATASETS:\n",
    "    samples_ds = all_samples[ds_name]\n",
    "\n",
    "    # Load bare NLLs to replicate hard selection\n",
    "    bare_path = EXP03_DIR / f\"bare_{ds_name}.json\"\n",
    "    assert bare_path.exists(), f\"Exp 03 bare checkpoint not found: {bare_path}\"\n",
    "    bare_ckpt = json.loads(bare_path.read_text())\n",
    "    assert bare_ckpt.get('n_total') == N_SAMPLES\n",
    "    bare_nlls_all = bare_ckpt['bare_nlls']\n",
    "    assert len(bare_nlls_all) == N_SAMPLES\n",
    "\n",
    "    # Verify query alignment\n",
    "    saved_queries = bare_ckpt.get('queries_first50', [])\n",
    "    current_queries = [s['query'][:50] for s in samples_ds[:len(saved_queries)]]\n",
    "    assert saved_queries == current_queries, \\\n",
    "        f\"{ds_name}: query alignment mismatch with Exp 03 bare checkpoint\"\n",
    "    print(f\"\\n{ds_name}: query alignment verified with Exp 03 bare checkpoint\")\n",
    "\n",
    "    # Replicate hard selection (same as Exp 03)\n",
    "    bare_arr = np.array(bare_nlls_all)\n",
    "    sorted_idx = np.argsort(bare_arr)[::-1]\n",
    "    h_idx = np.sort(sorted_idx[:N_HARD])\n",
    "\n",
    "    # Build hard_samples\n",
    "    hs = []\n",
    "    for idx in h_idx:\n",
    "        s = dict(samples_ds[idx])\n",
    "        s['nll_bare'] = float(bare_arr[idx])\n",
    "        s['original_idx'] = int(idx)\n",
    "        hs.append(s)\n",
    "    hard_samples[ds_name] = hs\n",
    "\n",
    "    # Load Exp 03 scored conditions for hard samples\n",
    "    ckpt_path = EXP03_DIR / f\"checkpoint_{ds_name}.json\"\n",
    "    assert ckpt_path.exists(), f\"Exp 03 checkpoint not found: {ckpt_path}\"\n",
    "    ckpt = json.loads(ckpt_path.read_text())\n",
    "    exp03_results = ckpt['results']\n",
    "    assert len(exp03_results) == N_HARD, \\\n",
    "        f\"{ds_name}: expected {N_HARD} results, got {len(exp03_results)}\"\n",
    "\n",
    "    # Verify alignment: Exp 03 checkpoint queries match our hard samples\n",
    "    for j in range(min(10, N_HARD)):\n",
    "        assert exp03_results[j]['query'][:50] == hs[j]['query'][:50], \\\n",
    "            f\"{ds_name}: query mismatch at hard sample {j}\"\n",
    "\n",
    "    # Extract baseline NLLs\n",
    "    hard_nlls[ds_name] = {}\n",
    "    hard_nlls[ds_name]['bare'] = bare_arr[h_idx]\n",
    "    for cond in ['random_tokens', 'repeat_token', 'extractor_matched',\n",
    "                 'adversarial_matched']:\n",
    "        hard_nlls[ds_name][cond] = np.array(\n",
    "            [r[f'nll_{cond}'] for r in exp03_results])\n",
    "\n",
    "    # Map extract_general = extractor_matched\n",
    "    hard_nlls[ds_name]['extract_general'] = hard_nlls[ds_name]['extractor_matched'].copy()\n",
    "\n",
    "    hard_metadata[ds_name] = {\n",
    "        'n_total': N_SAMPLES,\n",
    "        'n_hard': N_HARD,\n",
    "        'source': 'exp03_reuse',\n",
    "        'mean_passage_words': float(np.mean([s['word_count'] for s in hs])),\n",
    "        'mean_query_tokens': 0.0,  # filled after tokenization\n",
    "        'mean_answer_words': float(np.mean([count_words(s['answer']) for s in hs])),\n",
    "    }\n",
    "\n",
    "    print(f\"  Loaded {N_HARD} hard samples with 5 baseline conditions\")\n",
    "    print(f\"  Bare NLL range: {bare_arr[h_idx].min():.4f} - {bare_arr[h_idx].max():.4f}\")\n",
    "\n",
    "del bare_ckpt, ckpt, exp03_results\n",
    "gc.collect()\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Dataset + baseline loading summary:\")\n",
    "for ds_name in DATASET_NAMES:\n",
    "    n_h = len(hard_samples[ds_name])\n",
    "    loaded_conds = [c for c in hard_nlls[ds_name].keys()]\n",
    "    print(f\"  {ds_name}: {n_h} hard samples, loaded conditions: {loaded_conds}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19b8fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Build prefixes for 15 fresh conditions + validation\n",
    "print(\"=\" * 70)\n",
    "print(\"PREFIX CONSTRUCTION + VALIDATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "special_ids = set(tokenizer.all_special_ids)\n",
    "\n",
    "for ds_name in DATASET_NAMES:\n",
    "    hs = hard_samples[ds_name]\n",
    "    n_hard = len(hs)\n",
    "\n",
    "    print(f\"\\n--- {ds_name} ({n_hard} hard samples) ---\")\n",
    "\n",
    "    for i, s in enumerate(hs):\n",
    "        q_ids = tokenizer(s['query'], add_special_tokens=False).input_ids\n",
    "        Q = len(q_ids)\n",
    "        s['Q'] = Q\n",
    "\n",
    "        # Build coherent prefixes (7 fresh + extract_general already loaded)\n",
    "        for cond_name in FRESH_COHERENT_CONDS:\n",
    "            instr_ids = INSTRUCTION_IDS[cond_name]\n",
    "            s[f'prefix_{cond_name}'] = make_prefix(instr_ids, Q)\n",
    "\n",
    "        # Build scrambled prefixes (8 total, including scrambled_extract_general)\n",
    "        for cond_name in ALL_INSTRUCTION_CONDS:\n",
    "            instr_ids = INSTRUCTION_IDS[cond_name]\n",
    "            coherent_prefix = make_prefix(instr_ids, Q)\n",
    "            # Seed = hash(condition_name) % 2**31 + sample_index\n",
    "            seed = hash(cond_name) % (2**31) + i\n",
    "            s[f'prefix_scrambled_{cond_name}'] = scramble_prefix(coherent_prefix, seed)\n",
    "\n",
    "    # Update metadata with query token stats\n",
    "    q_lens = [s['Q'] for s in hs]\n",
    "    hard_metadata[ds_name]['mean_query_tokens'] = float(np.mean(q_lens))\n",
    "    print(f\"  Q tokens -- mean: {np.mean(q_lens):.1f}, \"\n",
    "          f\"median: {np.median(q_lens):.0f}, \"\n",
    "          f\"min: {np.min(q_lens)}, max: {np.max(q_lens)}\")\n",
    "\n",
    "    # Verify all fresh prefixes have exactly Q tokens\n",
    "    errors = 0\n",
    "    for i, s in enumerate(hs):\n",
    "        Q = s['Q']\n",
    "        for cond in FRESH_COND_NAMES:\n",
    "            prefix = s[f'prefix_{cond}']\n",
    "            if len(prefix) != Q:\n",
    "                print(f\"  ERROR: Sample {i} {cond}: len={len(prefix)} != Q={Q}\")\n",
    "                errors += 1\n",
    "    assert errors == 0, f\"{ds_name}: {errors} prefix length mismatches!\"\n",
    "    print(f\"  All {len(FRESH_COND_NAMES)} fresh prefix types verified for {n_hard} samples\")\n",
    "\n",
    "    # Verify scrambled prefixes have same token multiset as coherent\n",
    "    multiset_errors = 0\n",
    "    for i in range(min(5, n_hard)):\n",
    "        s = hs[i]\n",
    "        for cond_name in ALL_INSTRUCTION_CONDS:\n",
    "            coherent = make_prefix(INSTRUCTION_IDS[cond_name], s['Q'])\n",
    "            scrambled = s[f'prefix_scrambled_{cond_name}']\n",
    "            if sorted(coherent) != sorted(scrambled):\n",
    "                print(f\"  ERROR: Sample {i} {cond_name}: multiset mismatch\")\n",
    "                multiset_errors += 1\n",
    "    assert multiset_errors == 0, f\"{ds_name}: {multiset_errors} multiset mismatches!\"\n",
    "    print(f\"  Token multiset invariant verified (first 5 samples)\")\n",
    "\n",
    "# ================================================================\n",
    "# VALIDATION TESTS\n",
    "# ================================================================\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"VALIDATION TESTS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Test 1: Bare two-phase matches single-pass\n",
    "print(\"\\n--- Test 1: Bare two-phase matches single-pass ---\")\n",
    "doc_text_t = \"The cat sat on the mat near the door of the house by the lake\"\n",
    "query_text_t = \"Where did the cat sit?\"\n",
    "answer_text_t = \"on the mat\"\n",
    "doc_ids_t = tokenizer(doc_text_t, add_special_tokens=False).input_ids\n",
    "D_t = len(doc_ids_t)\n",
    "query_ids_t = tokenizer(\"\\n\" + query_text_t + \"\\n\", add_special_tokens=False).input_ids\n",
    "answer_ids_t = tokenizer(answer_text_t, add_special_tokens=False).input_ids\n",
    "\n",
    "full_ids = [BOS_ID] + doc_ids_t + query_ids_t + answer_ids_t\n",
    "with torch.no_grad():\n",
    "    out_full = model(input_ids=torch.tensor([full_ids], device=DEVICE))\n",
    "n_ctx = 1 + D_t + len(query_ids_t)\n",
    "logits_full = out_full.logits[0, n_ctx - 1:n_ctx - 1 + len(answer_ids_t), :].float()\n",
    "targets_t = torch.tensor(answer_ids_t, device=DEVICE)\n",
    "nll_single = -F.log_softmax(logits_full, dim=-1).gather(\n",
    "    1, targets_t.unsqueeze(1)).squeeze(1).mean().item()\n",
    "del out_full\n",
    "\n",
    "nll_bare = score(doc_text_t, query_text_t, answer_text_t)\n",
    "diff_pct = abs(nll_single - nll_bare) / nll_single * 100\n",
    "print(f\"  Single-pass NLL: {nll_single:.6f}\")\n",
    "print(f\"  Two-phase bare:  {nll_bare:.6f} (diff: {diff_pct:.2f}%)\")\n",
    "assert diff_pct < 1.0, f\"Bare doesn't match single-pass: {diff_pct}%\"\n",
    "print(f\"  PASSED\")\n",
    "\n",
    "# Test 2: Prefixed scoring on first hard sample\n",
    "print(\"\\n--- Test 2: Prefixed scoring runs correctly ---\")\n",
    "test_ds = DATASET_NAMES[0]\n",
    "ts = hard_samples[test_ds][0]\n",
    "nll_b = score(ts['passage'], ts['query'], ts['answer'])\n",
    "nll_c = score(ts['passage'], ts['query'], ts['answer'],\n",
    "              prefix_token_ids=ts['prefix_extract_entities'])\n",
    "nll_s = score(ts['passage'], ts['query'], ts['answer'],\n",
    "              prefix_token_ids=ts['prefix_scrambled_extract_entities'])\n",
    "print(f\"  [{test_ds}] Bare:              {nll_b:.4f}\")\n",
    "print(f\"  [{test_ds}] extract_entities:  {nll_c:.4f}  delta={nll_b - nll_c:+.4f}\")\n",
    "print(f\"  [{test_ds}] scrambled:         {nll_s:.4f}  delta={nll_b - nll_s:+.4f}\")\n",
    "assert 0 < nll_b < 20 and 0 < nll_c < 20 and 0 < nll_s < 20\n",
    "print(\"  PASSED\")\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print(\"\\nALL VALIDATION TESTS PASSED\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d9d2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Score 15 fresh conditions x ~160 hard samples x 4 datasets\n",
    "print(\"=\" * 70)\n",
    "print(f\"SCORING — {len(FRESH_COND_NAMES)} fresh conditions x ~{N_HARD} hard x \"\n",
    "      f\"{len(DATASET_NAMES)} datasets\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for ds_name in DATASET_NAMES:\n",
    "    hs = hard_samples[ds_name]\n",
    "    n_hard = len(hs)\n",
    "    ckpt_path = RESULTS_DIR / f\"checkpoint_{ds_name}.json\"\n",
    "\n",
    "    print(f\"\\n--- {ds_name} ({n_hard} hard samples x {len(FRESH_COND_NAMES)} conditions) ---\")\n",
    "\n",
    "    ds_results = []\n",
    "    start_idx = 0\n",
    "\n",
    "    if ckpt_path.exists():\n",
    "        ckpt = json.loads(ckpt_path.read_text())\n",
    "        if (ckpt.get('dataset') == ds_name and\n",
    "            ckpt.get('scoring') == SCORING_KEY and\n",
    "            ckpt.get('n_hard') == n_hard):\n",
    "            saved_queries = [r['query'][:50] for r in ckpt.get('results', [])]\n",
    "            current_queries = [s['query'][:50] for s in hs[:len(saved_queries)]]\n",
    "            if saved_queries == current_queries:\n",
    "                ds_results = ckpt['results']\n",
    "                start_idx = len(ds_results)\n",
    "                print(f\"  Resuming from checkpoint: {start_idx}/{n_hard}\")\n",
    "\n",
    "    if start_idx < n_hard:\n",
    "        t0 = time.time()\n",
    "\n",
    "        for i in tqdm(range(start_idx, n_hard), initial=start_idx,\n",
    "                      total=n_hard, desc=f\"Score {ds_name}\"):\n",
    "            s = hs[i]\n",
    "            result = {\n",
    "                'query': s['query'],\n",
    "                'answer': s['answer'],\n",
    "                'passage_words': s['word_count'],\n",
    "                'Q': s['Q'],\n",
    "            }\n",
    "\n",
    "            # Score all fresh conditions\n",
    "            for cond_name in FRESH_COND_NAMES:\n",
    "                result[f'nll_{cond_name}'] = score(\n",
    "                    s['passage'], s['query'], s['answer'],\n",
    "                    prefix_token_ids=s[f'prefix_{cond_name}']\n",
    "                )\n",
    "\n",
    "            ds_results.append(result)\n",
    "\n",
    "            if (i + 1) % 20 == 0 or i == n_hard - 1:\n",
    "                ckpt = {\n",
    "                    'dataset': ds_name,\n",
    "                    'n_hard': n_hard,\n",
    "                    'scoring': SCORING_KEY,\n",
    "                    'results': ds_results,\n",
    "                    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                }\n",
    "                ckpt_path.write_text(json.dumps(ckpt))\n",
    "                elapsed = time.time() - t0\n",
    "                done = i - start_idx + 1\n",
    "                eta = (n_hard - i - 1) * elapsed / done if done > 0 else 0\n",
    "                tqdm.write(f\"  Checkpoint {i+1}/{n_hard} | \"\n",
    "                           f\"{elapsed/60:.1f}m | ETA {eta/60:.1f}m\")\n",
    "\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        elapsed = time.time() - t0\n",
    "        print(f\"  Scoring complete in {elapsed/60:.1f} min\")\n",
    "    else:\n",
    "        print(f\"  Loaded {len(ds_results)} cached results\")\n",
    "\n",
    "    # Populate hard_nlls with fresh conditions\n",
    "    for cond in FRESH_COND_NAMES:\n",
    "        hard_nlls[ds_name][cond] = np.array(\n",
    "            [r[f'nll_{cond}'] for r in ds_results])\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Validation: extract_general (loaded) should be very close to extractor_matched\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"VALIDATION: extract_general loaded vs extractor_matched loaded\")\n",
    "for ds_name in DATASET_NAMES:\n",
    "    eg = hard_nlls[ds_name]['extract_general']\n",
    "    em = hard_nlls[ds_name]['extractor_matched']\n",
    "    max_diff = np.abs(eg - em).max()\n",
    "    mean_diff = np.abs(eg - em).mean()\n",
    "    print(f\"  {ds_name}: max_diff={max_diff:.6f}, mean_diff={mean_diff:.6f}\")\n",
    "    assert np.allclose(eg, em, atol=0.01), \\\n",
    "        f\"{ds_name}: extract_general != extractor_matched (max diff {max_diff:.6f})\"\n",
    "\n",
    "print(\"\\nAll scoring complete. Datasets in hard_nlls:\")\n",
    "for ds_name in DATASET_NAMES:\n",
    "    n = len(hard_nlls[ds_name]['bare'])\n",
    "    n_conds = len(hard_nlls[ds_name])\n",
    "    print(f\"  {ds_name}: {n} hard samples x {n_conds} conditions\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0313d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Per-dataset analysis — condition tables, three-level decomposition\n",
    "print(\"=\" * 70)\n",
    "print(\"PER-DATASET ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "per_dataset_analysis = {}\n",
    "\n",
    "for ds_name in DATASET_NAMES:\n",
    "    nlls = hard_nlls[ds_name]\n",
    "    n_hard = len(nlls['bare'])\n",
    "    bare = nlls['bare']\n",
    "    random_base = nlls['random_tokens']\n",
    "\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"  {ds_name.upper()} -- {n_hard} hard samples\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "    analysis = {}\n",
    "\n",
    "    # ---- Part A: Condition table (all 20 conditions) ----\n",
    "    print(f\"\\n  {'Cond':<28} {'NLL':>7} {'d bare':>8} {'sem d':>8} \"\n",
    "          f\"{'win%':>6} {'p':>10} {'sig':>4}\")\n",
    "    print(f\"  {'-'*76}\")\n",
    "\n",
    "    for cond in ALL_COND_NAMES:\n",
    "        if cond not in nlls:\n",
    "            continue\n",
    "        c_nlls = nlls[cond]\n",
    "        mean_nll = c_nlls.mean()\n",
    "\n",
    "        if cond == 'bare':\n",
    "            print(f\"  {cond:<28} {mean_nll:>7.3f} {'--':>8} {'--':>8} \"\n",
    "                  f\"{'--':>6} {'--':>10} {'--':>4}\")\n",
    "            analysis[cond] = {'mean_nll': float(mean_nll)}\n",
    "            continue\n",
    "\n",
    "        diff_bare = bare - c_nlls\n",
    "        d_bare = cohens_d(diff_bare)\n",
    "        _, p_bare = stats.ttest_1samp(diff_bare, 0)\n",
    "\n",
    "        if cond == 'random_tokens':\n",
    "            win_pct = 100 * np.mean(diff_bare > 0)\n",
    "            sig = ('***' if p_bare < 0.001 else '**' if p_bare < 0.01\n",
    "                   else '*' if p_bare < 0.05 else 'ns')\n",
    "            print(f\"  {cond:<28} {mean_nll:>7.3f} {d_bare:>+8.3f} {'(ref)':>8} \"\n",
    "                  f\"{win_pct:>5.1f}% {p_bare:>10.2e} {sig:>4}\")\n",
    "            analysis[cond] = {\n",
    "                'mean_nll': float(mean_nll), 'd_bare': float(d_bare),\n",
    "                'semantic_delta_d': 0.0, 'p_bare': float(p_bare),\n",
    "            }\n",
    "        else:\n",
    "            sem_delta = random_base - c_nlls\n",
    "            d_sem = cohens_d(sem_delta)\n",
    "            _, p_sem = stats.ttest_1samp(sem_delta, 0)\n",
    "            win_pct = 100 * np.mean(sem_delta > 0)\n",
    "            sig = ('***' if p_sem < 0.001 else '**' if p_sem < 0.01\n",
    "                   else '*' if p_sem < 0.05 else 'ns')\n",
    "            print(f\"  {cond:<28} {mean_nll:>7.3f} {d_bare:>+8.3f} {d_sem:>+8.3f} \"\n",
    "                  f\"{win_pct:>5.1f}% {p_sem:>10.2e} {sig:>4}\")\n",
    "            analysis[cond] = {\n",
    "                'mean_nll': float(mean_nll), 'd_bare': float(d_bare),\n",
    "                'semantic_delta_d': float(d_sem), 'p_semantic': float(p_sem),\n",
    "            }\n",
    "\n",
    "    # ---- Part B: Three-level decomposition ----\n",
    "    print(f\"\\n  Three-Level Decomposition (NLL deltas, positive = better):\")\n",
    "    print(f\"  {'Instruction':<22} {'Structural':>10} {'Vocab':>10} \"\n",
    "          f\"{'Meaning':>10} {'Total':>10}\")\n",
    "    print(f\"  {'-'*66}\")\n",
    "\n",
    "    structural = (bare - random_base).mean()\n",
    "\n",
    "    decomp = {}\n",
    "    for cond_name in ALL_INSTRUCTION_CONDS:\n",
    "        coherent_key = cond_name\n",
    "        scrambled_key = f'scrambled_{cond_name}'\n",
    "\n",
    "        if coherent_key not in nlls or scrambled_key not in nlls:\n",
    "            continue\n",
    "\n",
    "        vocab = (random_base - nlls[scrambled_key]).mean()\n",
    "        meaning = (nlls[scrambled_key] - nlls[coherent_key]).mean()\n",
    "        total = structural + vocab + meaning\n",
    "\n",
    "        print(f\"  {cond_name:<22} {structural:>+10.4f} {vocab:>+10.4f} \"\n",
    "              f\"{meaning:>+10.4f} {total:>+10.4f}\")\n",
    "\n",
    "        decomp[cond_name] = {\n",
    "            'structural': float(structural),\n",
    "            'vocabulary': float(vocab),\n",
    "            'meaning': float(meaning),\n",
    "            'total': float(total),\n",
    "        }\n",
    "\n",
    "    analysis['decomposition'] = decomp\n",
    "\n",
    "    # ---- Part C: Coherent vs scrambled paired tests ----\n",
    "    print(f\"\\n  Coherent vs Scrambled (paired t-test, positive = coherent better):\")\n",
    "    print(f\"  {'Instruction':<22} {'d':>8} {'p':>10} {'sig':>4} {'coherent wins':>14}\")\n",
    "    print(f\"  {'-'*62}\")\n",
    "\n",
    "    for cond_name in ALL_INSTRUCTION_CONDS:\n",
    "        coherent_key = cond_name\n",
    "        scrambled_key = f'scrambled_{cond_name}'\n",
    "        if coherent_key not in nlls or scrambled_key not in nlls:\n",
    "            continue\n",
    "        diff = nlls[scrambled_key] - nlls[coherent_key]  # pos = coherent better\n",
    "        d = cohens_d(diff)\n",
    "        _, p = stats.ttest_1samp(diff, 0)\n",
    "        win_pct = 100 * np.mean(diff > 0)\n",
    "        sig = ('***' if p < 0.001 else '**' if p < 0.01\n",
    "               else '*' if p < 0.05 else 'ns')\n",
    "        print(f\"  {cond_name:<22} {d:>+8.3f} {p:>10.2e} {sig:>4} {win_pct:>13.1f}%\")\n",
    "\n",
    "    per_dataset_analysis[ds_name] = analysis\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44fdccfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Cross-dataset meta-analysis, hypothesis verdicts\n",
    "print(\"=\" * 70)\n",
    "print(\"CROSS-DATASET META-ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# ================================================================\n",
    "# PART 1: Pooled three-level decomposition\n",
    "# ================================================================\n",
    "print(f\"\\n--- PART 1: Pooled Three-Level Decomposition ---\")\n",
    "print(f\"\\n  {'Instruction':<22} {'Structural':>10} {'Vocab':>10} \"\n",
    "      f\"{'Meaning':>10} {'Total':>10}\")\n",
    "print(f\"  {'-'*66}\")\n",
    "\n",
    "pooled_decomp = {}\n",
    "for cond_name in ALL_INSTRUCTION_CONDS:\n",
    "    struct_vals, vocab_vals, meaning_vals, total_vals = [], [], [], []\n",
    "    for ds_name in DATASET_NAMES:\n",
    "        nlls = hard_nlls[ds_name]\n",
    "        bare = nlls['bare']\n",
    "        random_base = nlls['random_tokens']\n",
    "        coherent_key = cond_name\n",
    "        scrambled_key = f'scrambled_{cond_name}'\n",
    "        if coherent_key not in nlls or scrambled_key not in nlls:\n",
    "            continue\n",
    "        struct_vals.append((bare - random_base).mean())\n",
    "        vocab_vals.append((random_base - nlls[scrambled_key]).mean())\n",
    "        meaning_vals.append((nlls[scrambled_key] - nlls[coherent_key]).mean())\n",
    "        total_vals.append((bare - nlls[coherent_key]).mean())\n",
    "\n",
    "    s_mean = np.mean(struct_vals)\n",
    "    v_mean = np.mean(vocab_vals)\n",
    "    m_mean = np.mean(meaning_vals)\n",
    "    t_mean = np.mean(total_vals)\n",
    "    print(f\"  {cond_name:<22} {s_mean:>+10.4f} {v_mean:>+10.4f} \"\n",
    "          f\"{m_mean:>+10.4f} {t_mean:>+10.4f}\")\n",
    "    pooled_decomp[cond_name] = {\n",
    "        'structural': float(s_mean), 'vocabulary': float(v_mean),\n",
    "        'meaning': float(m_mean), 'total': float(t_mean),\n",
    "    }\n",
    "\n",
    "# ================================================================\n",
    "# PART 2: Fixed-effects meta-analysis (semantic delta d)\n",
    "# ================================================================\n",
    "print(f\"\\n--- PART 2: Fixed-Effects Meta-Analysis (semantic delta d) ---\")\n",
    "\n",
    "# All scorable conditions (coherent + scrambled, excluding bare/random_tokens)\n",
    "META_CONDS = (\n",
    "    ALL_INSTRUCTION_CONDS +\n",
    "    FRESH_SCRAMBLED_CONDS +\n",
    "    ['repeat_token', 'adversarial_matched']\n",
    ")\n",
    "\n",
    "print(f\"\\n  {'Condition':<28} {'pooled_d':>9} {'SE':>8} {'z':>8} \"\n",
    "      f\"{'p':>10} {'95% CI':>16} {'sig':>4}\")\n",
    "print(f\"  {'-'*86}\")\n",
    "\n",
    "meta_results = {}\n",
    "for cond in META_CONDS:\n",
    "    ds_effects = []\n",
    "    for ds_name in DATASET_NAMES:\n",
    "        nlls = hard_nlls[ds_name]\n",
    "        if cond not in nlls:\n",
    "            continue\n",
    "        sem_delta = nlls['random_tokens'] - nlls[cond]\n",
    "        n = len(sem_delta)\n",
    "        d = cohens_d(sem_delta)\n",
    "        se = np.sqrt(1.0/n + d**2 / (2.0*n))\n",
    "        ds_effects.append((d, se, n))\n",
    "\n",
    "    if not ds_effects:\n",
    "        continue\n",
    "\n",
    "    weights = [1.0 / (se**2) for _, se, _ in ds_effects]\n",
    "    w_sum = sum(weights)\n",
    "    pooled_d = sum(w * d for (d, _, _), w in zip(ds_effects, weights)) / w_sum\n",
    "    pooled_se = 1.0 / np.sqrt(w_sum)\n",
    "    z = pooled_d / pooled_se if pooled_se > 0 else 0.0\n",
    "    p = 2 * stats.norm.sf(abs(z))\n",
    "    ci_lo = pooled_d - 1.96 * pooled_se\n",
    "    ci_hi = pooled_d + 1.96 * pooled_se\n",
    "    sig = ('***' if p < 0.001 else '**' if p < 0.01\n",
    "           else '*' if p < 0.05 else 'ns')\n",
    "\n",
    "    print(f\"  {cond:<28} {pooled_d:>+9.4f} {pooled_se:>8.4f} {z:>+8.2f} \"\n",
    "          f\"{p:>10.2e} [{ci_lo:>+.3f}, {ci_hi:>+.3f}] {sig:>4}\")\n",
    "    meta_results[cond] = {\n",
    "        'pooled_d': float(pooled_d), 'se': float(pooled_se),\n",
    "        'z': float(z), 'p': float(p),\n",
    "        'ci_lo': float(ci_lo), 'ci_hi': float(ci_hi),\n",
    "    }\n",
    "\n",
    "# ================================================================\n",
    "# PART 3: H1 vs H2 — Extraction vs Non-Extraction\n",
    "# ================================================================\n",
    "print(f\"\\n--- PART 3: Extraction vs Non-Extraction (H1 vs H2) ---\")\n",
    "\n",
    "extraction_ds = [meta_results[c]['pooled_d'] for c in EXTRACTION_CONDS if c in meta_results]\n",
    "non_extraction_ds = [meta_results[c]['pooled_d'] for c in NON_EXTRACTION_CONDS if c in meta_results]\n",
    "print(f\"  Extraction (n={len(extraction_ds)}):     mean pooled_d = {np.mean(extraction_ds):+.4f}\")\n",
    "print(f\"  Non-extraction (n={len(non_extraction_ds)}): mean pooled_d = {np.mean(non_extraction_ds):+.4f}\")\n",
    "\n",
    "# Per-sample paired test across all datasets\n",
    "ext_nlls_pooled = []\n",
    "non_ext_nlls_pooled = []\n",
    "for ds_name in DATASET_NAMES:\n",
    "    nlls = hard_nlls[ds_name]\n",
    "    random_base = nlls['random_tokens']\n",
    "    ext_mean = np.mean([random_base - nlls[c] for c in EXTRACTION_CONDS if c in nlls], axis=0)\n",
    "    non_ext_mean = np.mean([random_base - nlls[c] for c in NON_EXTRACTION_CONDS if c in nlls], axis=0)\n",
    "    ext_nlls_pooled.append(ext_mean)\n",
    "    non_ext_nlls_pooled.append(non_ext_mean)\n",
    "\n",
    "ext_pooled = np.concatenate(ext_nlls_pooled)\n",
    "non_ext_pooled = np.concatenate(non_ext_nlls_pooled)\n",
    "diff_ext_vs_non = ext_pooled - non_ext_pooled\n",
    "d_ext_vs_non = cohens_d(diff_ext_vs_non)\n",
    "_, p_ext_vs_non = stats.ttest_1samp(diff_ext_vs_non, 0)\n",
    "sig_ext = ('***' if p_ext_vs_non < 0.001 else '**' if p_ext_vs_non < 0.01\n",
    "           else '*' if p_ext_vs_non < 0.05 else 'ns')\n",
    "print(f\"  Paired (extraction - non-extraction): d={d_ext_vs_non:+.4f}, \"\n",
    "      f\"p={p_ext_vs_non:.2e} {sig_ext}\")\n",
    "if d_ext_vs_non > 0.05 and p_ext_vs_non < 0.05:\n",
    "    print(f\"  --> H1 SUPPORTED: extraction framing is specifically beneficial\")\n",
    "else:\n",
    "    print(f\"  --> H1 NOT SUPPORTED: extraction framing is not uniquely better\")\n",
    "\n",
    "# ================================================================\n",
    "# PART 4: H3 — Coherent vs Scrambled (meaning effect)\n",
    "# ================================================================\n",
    "print(f\"\\n--- PART 4: Coherent vs Scrambled -- Meaning Effect (H3) ---\")\n",
    "print(f\"  {'Instruction':<22} {'d(meaning)':>12} {'p':>10} {'sig':>4} {'consistent':>10}\")\n",
    "print(f\"  {'-'*62}\")\n",
    "\n",
    "meaning_tests = {}\n",
    "for cond_name in ALL_INSTRUCTION_CONDS:\n",
    "    per_ds_d = []\n",
    "    all_diffs = []\n",
    "    for ds_name in DATASET_NAMES:\n",
    "        nlls = hard_nlls[ds_name]\n",
    "        coherent_key = cond_name\n",
    "        scrambled_key = f'scrambled_{cond_name}'\n",
    "        if coherent_key not in nlls or scrambled_key not in nlls:\n",
    "            continue\n",
    "        diff = nlls[scrambled_key] - nlls[coherent_key]\n",
    "        per_ds_d.append(cohens_d(diff))\n",
    "        all_diffs.append(diff)\n",
    "\n",
    "    pooled_diff = np.concatenate(all_diffs)\n",
    "    d = cohens_d(pooled_diff)\n",
    "    _, p = stats.ttest_1samp(pooled_diff, 0)\n",
    "    sig = ('***' if p < 0.001 else '**' if p < 0.01\n",
    "           else '*' if p < 0.05 else 'ns')\n",
    "    consistent = all(x > 0 for x in per_ds_d) or all(x <= 0 for x in per_ds_d)\n",
    "    print(f\"  {cond_name:<22} {d:>+12.4f} {p:>10.2e} {sig:>4} \"\n",
    "          f\"{'YES' if consistent else 'NO':>10}\")\n",
    "    meaning_tests[cond_name] = {\n",
    "        'd': float(d), 'p': float(p), 'consistent': consistent,\n",
    "        'per_ds_d': [float(x) for x in per_ds_d],\n",
    "    }\n",
    "\n",
    "n_sig_meaning = sum(1 for v in meaning_tests.values() if v['p'] < 0.05 and v['d'] > 0)\n",
    "print(f\"\\n  {n_sig_meaning}/{len(meaning_tests)} instructions have significant meaning effect\")\n",
    "if n_sig_meaning == 0:\n",
    "    print(f\"  --> H3 SUPPORTED: instruction structure matters, content doesn't\")\n",
    "elif n_sig_meaning <= 2:\n",
    "    print(f\"  --> H3 PARTIALLY SUPPORTED: meaning effect is weak/inconsistent\")\n",
    "else:\n",
    "    print(f\"  --> H3 NOT SUPPORTED: meaning (word order) matters for coherent instructions\")\n",
    "\n",
    "# ================================================================\n",
    "# PART 5: H4 — Vocabulary by Category\n",
    "# ================================================================\n",
    "print(f\"\\n--- PART 5: Scrambled Extraction vs Scrambled Non-Extraction (H4) ---\")\n",
    "\n",
    "scr_ext_nlls = []\n",
    "scr_non_nlls = []\n",
    "for ds_name in DATASET_NAMES:\n",
    "    nlls = hard_nlls[ds_name]\n",
    "    random_base = nlls['random_tokens']\n",
    "    ext_scr = [random_base - nlls[f'scrambled_{c}'] for c in EXTRACTION_CONDS\n",
    "               if f'scrambled_{c}' in nlls]\n",
    "    non_scr = [random_base - nlls[f'scrambled_{c}'] for c in NON_EXTRACTION_CONDS\n",
    "               if f'scrambled_{c}' in nlls]\n",
    "    if ext_scr and non_scr:\n",
    "        scr_ext_nlls.append(np.mean(ext_scr, axis=0))\n",
    "        scr_non_nlls.append(np.mean(non_scr, axis=0))\n",
    "\n",
    "scr_ext_pooled = np.concatenate(scr_ext_nlls)\n",
    "scr_non_pooled = np.concatenate(scr_non_nlls)\n",
    "diff_scr = scr_ext_pooled - scr_non_pooled\n",
    "d_scr = cohens_d(diff_scr)\n",
    "_, p_scr = stats.ttest_1samp(diff_scr, 0)\n",
    "sig_scr = ('***' if p_scr < 0.001 else '**' if p_scr < 0.01\n",
    "           else '*' if p_scr < 0.05 else 'ns')\n",
    "\n",
    "print(f\"  Scrambled extraction mean sem delta: {np.mean(scr_ext_pooled):+.4f}\")\n",
    "print(f\"  Scrambled non-extraction mean sem delta: {np.mean(scr_non_pooled):+.4f}\")\n",
    "print(f\"  Paired diff: d={d_scr:+.4f}, p={p_scr:.2e} {sig_scr}\")\n",
    "if d_scr > 0.05 and p_scr < 0.05:\n",
    "    print(f\"  --> H4 SUPPORTED: extraction vocabulary alone activates relevant representations\")\n",
    "else:\n",
    "    print(f\"  --> H4 NOT SUPPORTED: vocabulary category doesn't matter when scrambled\")\n",
    "\n",
    "# ================================================================\n",
    "# PART 6: H5 — Repetition effect\n",
    "# ================================================================\n",
    "print(f\"\\n--- PART 6: Repetition Effect (H5) ---\")\n",
    "print(f\"  extract_minimal (~7 tokens, heavily repeated) vs extract_general (~18 tokens)\")\n",
    "\n",
    "for ds_name in DATASET_NAMES:\n",
    "    nlls = hard_nlls[ds_name]\n",
    "    if 'extract_minimal' in nlls and 'extract_general' in nlls:\n",
    "        diff = nlls['extract_general'] - nlls['extract_minimal']\n",
    "        d = cohens_d(diff)\n",
    "        _, p = stats.ttest_1samp(diff, 0)\n",
    "        sig = ('***' if p < 0.001 else '**' if p < 0.01\n",
    "               else '*' if p < 0.05 else 'ns')\n",
    "        win = 100 * np.mean(diff > 0)\n",
    "        print(f\"  {ds_name:<16}: extract_minimal wins {win:.1f}%, d={d:+.4f}, p={p:.2e} {sig}\")\n",
    "\n",
    "# Pooled\n",
    "min_diffs = []\n",
    "for ds_name in DATASET_NAMES:\n",
    "    nlls = hard_nlls[ds_name]\n",
    "    if 'extract_minimal' in nlls and 'extract_general' in nlls:\n",
    "        min_diffs.append(nlls['extract_general'] - nlls['extract_minimal'])\n",
    "min_pooled = np.concatenate(min_diffs)\n",
    "d_min = cohens_d(min_pooled)\n",
    "_, p_min = stats.ttest_1samp(min_pooled, 0)\n",
    "sig_min = ('***' if p_min < 0.001 else '**' if p_min < 0.01\n",
    "           else '*' if p_min < 0.05 else 'ns')\n",
    "print(f\"\\n  Pooled: extract_minimal vs extract_general: d={d_min:+.4f}, p={p_min:.2e} {sig_min}\")\n",
    "if d_min > 0.05 and p_min < 0.05:\n",
    "    print(f\"  --> H5 SUPPORTED: short repeated instruction outperforms longer one\")\n",
    "elif d_min < -0.05 and p_min < 0.05:\n",
    "    print(f\"  --> H5 REFUTED: longer instruction outperforms short repeated one\")\n",
    "else:\n",
    "    print(f\"  --> H5 INCONCLUSIVE: no significant difference\")\n",
    "\n",
    "# ================================================================\n",
    "# PART 7: Cross-dataset consistency\n",
    "# ================================================================\n",
    "print(f\"\\n--- PART 7: Cross-Dataset Consistency ---\")\n",
    "print(f\"\\n  {'Condition':<28}\", end=\"\")\n",
    "for ds_name in DATASET_NAMES:\n",
    "    print(f\" {ds_name[:10]:>10}\", end=\"\")\n",
    "print(f\"  {'mean':>8} {'consistent':>10}\")\n",
    "print(f\"  {'-'*90}\")\n",
    "\n",
    "cross_dataset = {}\n",
    "for cond in META_CONDS:\n",
    "    ds_vals = []\n",
    "    for ds_name in DATASET_NAMES:\n",
    "        nlls = hard_nlls[ds_name]\n",
    "        if cond not in nlls:\n",
    "            ds_vals.append(float('nan'))\n",
    "            continue\n",
    "        sem_delta = nlls['random_tokens'] - nlls[cond]\n",
    "        d = cohens_d(sem_delta)\n",
    "        ds_vals.append(d)\n",
    "\n",
    "    valid = [v for v in ds_vals if not np.isnan(v)]\n",
    "    if not valid:\n",
    "        continue\n",
    "    mean_d = np.mean(valid)\n",
    "    same_sign = all(v >= 0 for v in valid) or all(v <= 0 for v in valid)\n",
    "    row = f\"  {cond:<28}\"\n",
    "    for v in ds_vals:\n",
    "        row += f\" {v:>+10.3f}\" if not np.isnan(v) else f\" {'N/A':>10}\"\n",
    "    row += f\"  {mean_d:>+8.3f} {'YES' if same_sign else 'NO':>10}\"\n",
    "    print(row)\n",
    "    cross_dataset[cond] = {\n",
    "        ds: float(d) for ds, d in zip(DATASET_NAMES, ds_vals)\n",
    "    }\n",
    "    cross_dataset[cond]['mean'] = float(mean_d)\n",
    "    cross_dataset[cond]['consistent_sign'] = same_sign\n",
    "\n",
    "# ================================================================\n",
    "# PART 8: Instruction ranking\n",
    "# ================================================================\n",
    "print(f\"\\n--- PART 8: Instruction Ranking (pooled semantic delta d) ---\")\n",
    "ranked = sorted(meta_results.items(), key=lambda x: x[1]['pooled_d'], reverse=True)\n",
    "print(f\"\\n  {'Rank':>4} {'Condition':<28} {'pooled_d':>9} {'sig':>4}\")\n",
    "print(f\"  {'-'*50}\")\n",
    "for rank, (cond, m) in enumerate(ranked, 1):\n",
    "    sig = ('***' if m['p'] < 0.001 else '**' if m['p'] < 0.01\n",
    "           else '*' if m['p'] < 0.05 else 'ns')\n",
    "    print(f\"  {rank:>4} {cond:<28} {m['pooled_d']:>+9.4f} {sig:>4}\")\n",
    "\n",
    "# ================================================================\n",
    "# VERDICT\n",
    "# ================================================================\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"VERDICT -- Exp 04: Instruction Framing Decomposition\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\nModel: {MODEL_NAME}\")\n",
    "print(f\"Scoring: BOS-retained repositioning + token-level prefix matching\")\n",
    "print(f\"Datasets: {len(DATASET_NAMES)} ({', '.join(DATASET_NAMES)})\")\n",
    "print(f\"Hard selection: top {HARD_FRAC*100:.0f}% by bare NLL\")\n",
    "\n",
    "print(f\"\\n--- Hypothesis Verdicts ---\")\n",
    "\n",
    "# H1\n",
    "print(f\"\\n  H1 (Extraction framing is specifically beneficial):\")\n",
    "print(f\"    Extraction mean pooled_d: {np.mean(extraction_ds):+.4f}\")\n",
    "print(f\"    Non-extraction mean pooled_d: {np.mean(non_extraction_ds):+.4f}\")\n",
    "print(f\"    Paired diff d={d_ext_vs_non:+.4f}, p={p_ext_vs_non:.2e}\")\n",
    "\n",
    "# H2\n",
    "print(f\"\\n  H2 (Any coherent task-relevant instruction helps equally):\")\n",
    "all_coherent_ds = [meta_results[c]['pooled_d'] for c in ALL_INSTRUCTION_CONDS if c in meta_results]\n",
    "print(f\"    All coherent instructions mean pooled_d: {np.mean(all_coherent_ds):+.4f}\")\n",
    "print(f\"    Range: [{min(all_coherent_ds):+.4f}, {max(all_coherent_ds):+.4f}]\")\n",
    "n_sig_pos = sum(1 for c in ALL_INSTRUCTION_CONDS\n",
    "                if c in meta_results and meta_results[c]['pooled_d'] > 0 and meta_results[c]['p'] < 0.05)\n",
    "print(f\"    {n_sig_pos}/{len(ALL_INSTRUCTION_CONDS)} significantly positive\")\n",
    "\n",
    "# H3\n",
    "print(f\"\\n  H3 (Structure matters, content doesn't):\")\n",
    "print(f\"    {n_sig_meaning}/{len(meaning_tests)} instructions have significant meaning effect\")\n",
    "\n",
    "# H4\n",
    "print(f\"\\n  H4 (Extraction vocabulary activates relevant representations):\")\n",
    "print(f\"    Scrambled ext vs non-ext: d={d_scr:+.4f}, p={p_scr:.2e}\")\n",
    "\n",
    "# H5\n",
    "print(f\"\\n  H5 (Short repeated instruction combines repetition + semantics):\")\n",
    "print(f\"    extract_minimal vs extract_general: d={d_min:+.4f}, p={p_min:.2e}\")\n",
    "\n",
    "# Summary\n",
    "print(f\"\\n--- Key decomposition finding ---\")\n",
    "mean_struct = np.mean([pooled_decomp[c]['structural'] for c in ALL_INSTRUCTION_CONDS])\n",
    "mean_vocab = np.mean([pooled_decomp[c]['vocabulary'] for c in ALL_INSTRUCTION_CONDS])\n",
    "mean_meaning = np.mean([pooled_decomp[c]['meaning'] for c in ALL_INSTRUCTION_CONDS])\n",
    "mean_total = np.mean([pooled_decomp[c]['total'] for c in ALL_INSTRUCTION_CONDS])\n",
    "print(f\"  Mean across all 8 instructions (pooled NLL delta):\")\n",
    "print(f\"    Structural: {mean_struct:+.4f}\")\n",
    "print(f\"    Vocabulary:  {mean_vocab:+.4f}\")\n",
    "print(f\"    Meaning:     {mean_meaning:+.4f}\")\n",
    "print(f\"    Total:       {mean_total:+.4f}\")\n",
    "if abs(mean_total) > 0.001:\n",
    "    pct_struct = mean_struct / mean_total * 100\n",
    "    pct_vocab = mean_vocab / mean_total * 100\n",
    "    pct_meaning = mean_meaning / mean_total * 100\n",
    "    print(f\"    Decomposition: {pct_struct:.0f}% structural, \"\n",
    "          f\"{pct_vocab:.0f}% vocabulary, {pct_meaning:.0f}% meaning\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a493eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Save results\n",
    "print(\"=\" * 70)\n",
    "print(\"SAVING RESULTS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "final_results = {\n",
    "    'experiment': 'v4_exp04_instruction_framing_decomposition',\n",
    "    'model': MODEL_NAME,\n",
    "    'scoring': 'bos_retained_repositioning_token_matched',\n",
    "    'hard_fraction': HARD_FRAC,\n",
    "    'datasets': DATASET_NAMES,\n",
    "    'n_samples_per_dataset': N_SAMPLES,\n",
    "    'n_hard_per_dataset': N_HARD,\n",
    "    'seed': SEED,\n",
    "    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'instructions': {name: text for name, text in INSTRUCTIONS.items()},\n",
    "    'conditions': {\n",
    "        'loaded_from_exp03': LOADED_COND_NAMES,\n",
    "        'fresh_coherent': FRESH_COHERENT_CONDS,\n",
    "        'fresh_scrambled': FRESH_SCRAMBLED_CONDS,\n",
    "        'all': ALL_COND_NAMES,\n",
    "        'extraction': EXTRACTION_CONDS,\n",
    "        'non_extraction': NON_EXTRACTION_CONDS,\n",
    "    },\n",
    "    'per_dataset': {ds: per_dataset_analysis.get(ds, {}) for ds in DATASET_NAMES},\n",
    "    'meta_analysis': meta_results,\n",
    "    'pooled_decomposition': pooled_decomp,\n",
    "    'cross_dataset': cross_dataset,\n",
    "    'meaning_tests': meaning_tests,\n",
    "    'hypotheses': {\n",
    "        'H1_extraction_vs_non': {\n",
    "            'd': float(d_ext_vs_non),\n",
    "            'p': float(p_ext_vs_non),\n",
    "        },\n",
    "        'H3_meaning_n_sig': n_sig_meaning,\n",
    "        'H4_vocab_category': {\n",
    "            'd': float(d_scr),\n",
    "            'p': float(p_scr),\n",
    "        },\n",
    "        'H5_repetition': {\n",
    "            'd': float(d_min),\n",
    "            'p': float(p_min),\n",
    "        },\n",
    "    },\n",
    "    'hard_metadata': {ds: hard_metadata[ds] for ds in DATASET_NAMES},\n",
    "}\n",
    "\n",
    "with open(RESULTS_DIR / 'results.json', 'w') as f:\n",
    "    json.dump(final_results, f, indent=2)\n",
    "print(f\"Results saved to {RESULTS_DIR / 'results.json'}\")\n",
    "\n",
    "# Cleanup\n",
    "print(f\"\\nCleaning up GPU memory...\")\n",
    "mem_before = torch.cuda.memory_allocated() / 1e9\n",
    "del model, tokenizer\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "mem_after = torch.cuda.memory_allocated() / 1e9\n",
    "print(f\"GPU memory: {mem_before:.2f} GB -> {mem_after:.2f} GB\")\n",
    "print(\"Done!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
