{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb971e52",
   "metadata": {},
   "source": [
    "# Decoder-Only Exp 04: Position Sweep + Cache Surgery\n",
    "\n",
    "## Motivation\n",
    "\n",
    "Exp 03 revealed that the \"structural effect\" was actually BOS removal (87%) +\n",
    "position offset (53%) minus attention-enrichment harm (-40%). But two mysteries remain:\n",
    "\n",
    "1. **Position sweet spot**: `pos_offset_4` (d=+0.78) vastly outperformed `pos_offset_20`\n",
    "   (d=+0.43) and `bare_no_bos` (d=+0.46). Why? Where exactly is the peak?\n",
    "\n",
    "2. **BOS mechanism**: Removing BOS from the Phase B cache was the biggest single factor.\n",
    "   Can we exploit this further? Does BOS matter during Phase A (shaping doc representations)\n",
    "   or only during Phase B (distracting query attention)?\n",
    "\n",
    "## Conditions (14 total)\n",
    "\n",
    "### Baselines\n",
    "| # | Condition | Description |\n",
    "|---|-----------|-------------|\n",
    "| 1 | bare | Standard causal, BOS in cache |\n",
    "| 2 | oracle | Real query as prefix — upper bound |\n",
    "\n",
    "### Position sweep (BOS removed from cache, no prefix)\n",
    "| # | S | Doc RoPE positions | BOS-to-doc distance |\n",
    "|---|---|-------------------|-------------------|\n",
    "| 3 | 1 | 1..D (= bare_no_bos) | 1 |\n",
    "| 4 | 2 | 2..2+D | 2 |\n",
    "| 5 | 4 | 4..4+D | 4 |\n",
    "| 6 | 8 | 8..8+D | 8 |\n",
    "| 7 | 16 | 16..16+D | 16 |\n",
    "| 8 | 32 | 32..32+D | 32 |\n",
    "\n",
    "### BOS mechanism isolation\n",
    "| # | Condition | Phase A BOS? | Phase B BOS? | Doc positions |\n",
    "|---|-----------|-------------|-------------|--------------|\n",
    "| 9 | no_bos_at_all | NO | NO | 0..D |\n",
    "| 10 | keep_bos_offset_4 | yes | **YES** | 4..4+D |\n",
    "\n",
    "### Cache pruning (natural positions, selective removal)\n",
    "| # | Condition | What's removed from cache | Purpose |\n",
    "|---|-----------|--------------------------|---------|\n",
    "| 11 | prune_first_1 | BOS + doc[0] | Are early doc tokens sinks too? |\n",
    "| 12 | prune_first_3 | BOS + doc[0:3] | Deeper pruning |\n",
    "| 13 | prune_first_5 | BOS + doc[0:5] | How far can we go? |\n",
    "| 14 | prune_last_3 | BOS + doc[-3:] | Control (end tokens shouldn't be sinks) |\n",
    "\n",
    "## Key comparisons\n",
    "\n",
    "**Position sweep**: S=1 → 2 → 4 → 8 → 16 → 32 traces the full curve.\n",
    "\n",
    "**BOS in Phase A vs Phase B**:\n",
    "- bare vs bare_no_bos → Phase B BOS effect (removing from cache)\n",
    "- bare_no_bos vs no_bos_at_all → Phase A BOS effect (removing from input)\n",
    "- pos_offset_4 vs keep_bos_offset_4 → Phase B BOS effect at optimal offset\n",
    "\n",
    "**Cache pruning depth**:\n",
    "- prune_first_{1,3,5} vs bare_no_bos → do early doc tokens act as sinks?\n",
    "- prune_last_3 → control (removing informative tokens should hurt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23c6d9e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-20T15:23:09.538141Z",
     "iopub.status.busy": "2026-02-20T15:23:09.537845Z",
     "iopub.status.idle": "2026-02-20T15:23:24.763537Z",
     "shell.execute_reply": "2026-02-20T15:23:24.762507Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading google/gemma-3-4b-it...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "931dc7e95da34cee930639fd54593151",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/883 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exp 04: Position Sweep + Cache Surgery\n",
      "N: 400, Model: google/gemma-3-4b-it\n",
      "DEVICE: cuda:0, dtype: torch.bfloat16\n",
      "GPU memory: 8.60 GB\n",
      "Vocab size: 262208\n",
      "Num layers: 34\n",
      "Num KV heads: 4\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Setup and model loading\n",
    "import os\n",
    "os.umask(0o000)\n",
    "\n",
    "import sys, json, time, gc, re\n",
    "import random as pyrandom\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from scipy import stats\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "sys.path.insert(0, \"../../..\")\n",
    "from lib.analysis import cohens_d\n",
    "\n",
    "SEED = 42\n",
    "N_SAMPLES = 400\n",
    "MODEL_NAME = \"google/gemma-3-4b-it\"\n",
    "\n",
    "RESULTS_DIR = Path(\"../../../results/decoder_only/exp04\")\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CHECKPOINT_PATH = RESULTS_DIR / \"checkpoint.json\"\n",
    "\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "pyrandom.seed(SEED)\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv())\n",
    "HF_TOKEN = os.environ.get(\"HF_TOKEN\")\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "print(f\"Loading {MODEL_NAME}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, token=HF_TOKEN)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME, device_map=\"auto\", torch_dtype=torch.bfloat16, token=HF_TOKEN,\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "DEVICE = next(model.parameters()).device\n",
    "\n",
    "print(f\"Exp 04: Position Sweep + Cache Surgery\")\n",
    "print(f\"N: {N_SAMPLES}, Model: {MODEL_NAME}\")\n",
    "print(f\"DEVICE: {DEVICE}, dtype: {next(model.parameters()).dtype}\")\n",
    "print(f\"GPU memory: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "text_cfg = getattr(model.config, 'text_config', model.config)\n",
    "print(f\"Vocab size: {getattr(text_cfg, 'vocab_size', 'N/A')}\")\n",
    "print(f\"Num layers: {getattr(text_cfg, 'num_hidden_layers', 'N/A')}\")\n",
    "print(f\"Num KV heads: {getattr(text_cfg, 'num_key_value_heads', 'N/A')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11fbd836",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-20T15:23:24.768140Z",
     "iopub.status.busy": "2026-02-20T15:23:24.767406Z",
     "iopub.status.idle": "2026-02-20T15:23:24.792429Z",
     "shell.execute_reply": "2026-02-20T15:23:24.791480Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scoring function defined with extended modes.\n",
      "  position_offset: RoPE shift for doc tokens\n",
      "  keep_bos_in_offset: keep BOS in Phase B cache\n",
      "  no_bos_input: skip BOS token entirely\n",
      "  prune_first/prune_last: remove doc tokens from cache\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: KV cache helpers and scoring functions\n",
    "\n",
    "def slice_kv_cache(cache, start_idx):\n",
    "    # Remove first start_idx entries from KV cache.\n",
    "    from transformers import DynamicCache\n",
    "    if isinstance(cache, DynamicCache):\n",
    "        sliced = DynamicCache()\n",
    "        for i in range(len(cache.layers)):\n",
    "            k = cache.layers[i].keys[:, :, start_idx:, :]\n",
    "            v = cache.layers[i].values[:, :, start_idx:, :]\n",
    "            sliced.update(k, v, i)\n",
    "        return sliced\n",
    "    else:\n",
    "        return tuple(\n",
    "            (k[:, :, start_idx:, :], v[:, :, start_idx:, :])\n",
    "            for k, v in cache\n",
    "        )\n",
    "\n",
    "\n",
    "def prune_kv_cache_end(cache, n):\n",
    "    # Remove last n entries from KV cache.\n",
    "    from transformers import DynamicCache\n",
    "    if isinstance(cache, DynamicCache):\n",
    "        pruned = DynamicCache()\n",
    "        for i in range(len(cache.layers)):\n",
    "            k = cache.layers[i].keys[:, :, :-n, :]\n",
    "            v = cache.layers[i].values[:, :, :-n, :]\n",
    "            pruned.update(k, v, i)\n",
    "        return pruned\n",
    "    else:\n",
    "        return tuple(\n",
    "            (k[:, :, :-n, :], v[:, :, :-n, :])\n",
    "            for k, v in cache\n",
    "        )\n",
    "\n",
    "\n",
    "def score(doc_text, query_text, answer_text, prefix_text=None,\n",
    "          position_offset=0, remove_bos=False,\n",
    "          keep_bos_in_offset=False, no_bos_input=False,\n",
    "          prune_first=0, prune_last=0):\n",
    "    # Score NLL of answer tokens using two-phase KV cache.\n",
    "    #\n",
    "    # Modes:\n",
    "    #   prefix_text: [BOS + prefix + \\n + doc], prefix sliced from cache\n",
    "    #   position_offset > 0: BOS at pos 0, doc at offset..offset+D\n",
    "    #     BOS removed from cache unless keep_bos_in_offset=True\n",
    "    #   no_bos_input: forward doc WITHOUT BOS token, no BOS in cache\n",
    "    #   remove_bos: bare but BOS removed from cache\n",
    "    #   prune_first/prune_last: additional doc token pruning from cache\n",
    "\n",
    "    # --- Phase A: Conditioning ---\n",
    "    if prefix_text is not None:\n",
    "        prefix_ids = tokenizer(prefix_text + \"\\n\", add_special_tokens=True,\n",
    "                               truncation=True, max_length=512).input_ids\n",
    "        doc_ids = tokenizer(doc_text, add_special_tokens=False,\n",
    "                            truncation=True, max_length=1536).input_ids\n",
    "        cond_ids = prefix_ids + doc_ids\n",
    "        slice_start = len(prefix_ids)\n",
    "        custom_pos = None\n",
    "        phase_b_start = len(cond_ids)\n",
    "\n",
    "    elif no_bos_input:\n",
    "        # No BOS in input at all\n",
    "        cond_ids = tokenizer(doc_text, add_special_tokens=False,\n",
    "                             truncation=True, max_length=2048).input_ids\n",
    "        slice_start = 0\n",
    "        custom_pos = None\n",
    "        phase_b_start = len(cond_ids)\n",
    "\n",
    "    elif position_offset > 0:\n",
    "        cond_ids = tokenizer(doc_text, add_special_tokens=True,\n",
    "                             truncation=True, max_length=2048).input_ids\n",
    "        n_doc = len(cond_ids) - 1\n",
    "        pos_list = [0] + list(range(position_offset, position_offset + n_doc))\n",
    "        custom_pos = torch.tensor([pos_list], dtype=torch.long, device=DEVICE)\n",
    "        if keep_bos_in_offset:\n",
    "            slice_start = 0\n",
    "        else:\n",
    "            slice_start = 1\n",
    "        phase_b_start = position_offset + n_doc\n",
    "\n",
    "    else:\n",
    "        cond_ids = tokenizer(doc_text, add_special_tokens=True,\n",
    "                             truncation=True, max_length=2048).input_ids\n",
    "        slice_start = 1 if remove_bos else 0\n",
    "        custom_pos = None\n",
    "        phase_b_start = len(cond_ids)\n",
    "\n",
    "    cond_tensor = torch.tensor([cond_ids], dtype=torch.long, device=DEVICE)\n",
    "\n",
    "    fwd_kwargs = {'input_ids': cond_tensor, 'use_cache': True}\n",
    "    if custom_pos is not None:\n",
    "        fwd_kwargs['position_ids'] = custom_pos\n",
    "\n",
    "    with torch.no_grad():\n",
    "        phase_a = model(**fwd_kwargs)\n",
    "\n",
    "    cache = phase_a.past_key_values\n",
    "    del phase_a\n",
    "\n",
    "    if slice_start > 0:\n",
    "        cache = slice_kv_cache(cache, slice_start)\n",
    "\n",
    "    # Additional pruning\n",
    "    if prune_first > 0:\n",
    "        cache = slice_kv_cache(cache, prune_first)\n",
    "    if prune_last > 0:\n",
    "        cache = prune_kv_cache_end(cache, prune_last)\n",
    "\n",
    "    # --- Phase B: Inference ---\n",
    "    query_part_ids = tokenizer(\"\\n\" + query_text + \"\\n\",\n",
    "                               add_special_tokens=False).input_ids\n",
    "    answer_ids = tokenizer(answer_text, add_special_tokens=False,\n",
    "                           truncation=True, max_length=256).input_ids\n",
    "\n",
    "    if not answer_ids:\n",
    "        del cache\n",
    "        return 0.0\n",
    "\n",
    "    phase_b_ids = query_part_ids + answer_ids\n",
    "    phase_b_tensor = torch.tensor([phase_b_ids], dtype=torch.long, device=DEVICE)\n",
    "\n",
    "    pos_ids = torch.arange(phase_b_start, phase_b_start + len(phase_b_ids),\n",
    "                           device=DEVICE).unsqueeze(0)\n",
    "    cache_position = torch.arange(phase_b_start, phase_b_start + len(phase_b_ids),\n",
    "                                  device=DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        phase_b = model(\n",
    "            input_ids=phase_b_tensor,\n",
    "            past_key_values=cache,\n",
    "            position_ids=pos_ids,\n",
    "            cache_position=cache_position,\n",
    "            use_cache=False,\n",
    "        )\n",
    "\n",
    "    logits = phase_b.logits\n",
    "    n_query_part = len(query_part_ids)\n",
    "    n_answer = len(answer_ids)\n",
    "\n",
    "    answer_logits = logits[0, n_query_part - 1 : n_query_part - 1 + n_answer, :]\n",
    "    answer_targets = torch.tensor(answer_ids, dtype=torch.long, device=DEVICE)\n",
    "\n",
    "    log_probs = F.log_softmax(answer_logits, dim=-1)\n",
    "    token_log_probs = log_probs.gather(1, answer_targets.unsqueeze(1)).squeeze(1)\n",
    "    mean_nll = -token_log_probs.mean().item()\n",
    "\n",
    "    del cache, phase_b, logits, log_probs\n",
    "    return mean_nll\n",
    "\n",
    "\n",
    "print(\"Scoring function defined with extended modes.\")\n",
    "print(f\"  position_offset: RoPE shift for doc tokens\")\n",
    "print(f\"  keep_bos_in_offset: keep BOS in Phase B cache\")\n",
    "print(f\"  no_bos_input: skip BOS token entirely\")\n",
    "print(f\"  prune_first/prune_last: remove doc tokens from cache\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef18f4c2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-20T15:23:24.796022Z",
     "iopub.status.busy": "2026-02-20T15:23:24.795725Z",
     "iopub.status.idle": "2026-02-20T15:23:26.170717Z",
     "shell.execute_reply": "2026-02-20T15:23:26.169798Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MS MARCO v1.1 validation...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total candidates: 1200\n",
      "Loaded 400 samples\n",
      "Mean passage words: 73\n",
      "Mean query words: 5.9\n",
      "Mean passage tokens (first 50): 104\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Load MS MARCO data\n",
    "from lib.data import count_words\n",
    "from datasets import load_dataset\n",
    "\n",
    "print(\"Loading MS MARCO v1.1 validation...\")\n",
    "ds = load_dataset(\"microsoft/ms_marco\", \"v1.1\", split=\"validation\")\n",
    "\n",
    "all_candidates = []\n",
    "for item in ds:\n",
    "    if len(all_candidates) >= 3 * N_SAMPLES:\n",
    "        break\n",
    "    passages = item.get('passages', {})\n",
    "    ptexts = passages.get('passage_text', [])\n",
    "    is_sel = passages.get('is_selected', [])\n",
    "    query = item.get('query', '')\n",
    "    answers = item.get('answers', [])\n",
    "    well_formed = item.get('wellFormedAnswers', [])\n",
    "    answer = None\n",
    "    if well_formed and len(well_formed) > 0 and well_formed[0] not in ('[]', ''):\n",
    "        answer = well_formed[0]\n",
    "    elif answers and len(answers) > 0 and answers[0] != 'No Answer Present.':\n",
    "        answer = answers[0]\n",
    "    if not answer:\n",
    "        continue\n",
    "    for pt, sel in zip(ptexts, is_sel):\n",
    "        wc = count_words(pt)\n",
    "        if sel == 1 and 30 <= wc <= 300:\n",
    "            all_candidates.append({\n",
    "                'passage': pt, 'query': query, 'answer': answer,\n",
    "                'word_count': wc,\n",
    "            })\n",
    "            break\n",
    "\n",
    "print(f\"Total candidates: {len(all_candidates)}\")\n",
    "np.random.seed(SEED)\n",
    "indices = np.random.permutation(len(all_candidates))\n",
    "samples = [all_candidates[i] for i in indices[:N_SAMPLES]]\n",
    "del ds, all_candidates\n",
    "gc.collect()\n",
    "\n",
    "print(f\"Loaded {len(samples)} samples\")\n",
    "print(f\"Mean passage words: {np.mean([s['word_count'] for s in samples]):.0f}\")\n",
    "print(f\"Mean query words: {np.mean([len(s['query'].split()) for s in samples]):.1f}\")\n",
    "mean_doc_tokens = np.mean([len(tokenizer(s['passage'], add_special_tokens=False).input_ids)\n",
    "                           for s in samples[:50]])\n",
    "print(f\"Mean passage tokens (first 50): {mean_doc_tokens:.0f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e17fa8dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-20T15:23:26.174695Z",
     "iopub.status.busy": "2026-02-20T15:23:26.173825Z",
     "iopub.status.idle": "2026-02-20T15:23:30.368648Z",
     "shell.execute_reply": "2026-02-20T15:23:30.367653Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "VALIDATION\n",
      "======================================================================\n",
      "\n",
      "--- All modes on sample 0 ---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  bare                 NLL = 0.7383  delta vs bare = +0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  oracle               NLL = 0.8594  delta vs bare = -0.1211\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  S=1 (bare_no_bos)    NLL = 0.6328  delta vs bare = +0.1055\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  S=4                  NLL = 0.4258  delta vs bare = +0.3125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  S=8                  NLL = 0.3867  delta vs bare = +0.3516\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  S=32                 NLL = 0.6914  delta vs bare = +0.0469\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  no_bos_at_all        NLL = 0.6406  delta vs bare = +0.0977\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  keep_bos_S=4         NLL = 0.6836  delta vs bare = +0.0547\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  prune_first_3        NLL = 0.5273  delta vs bare = +0.2109\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  prune_last_3         NLL = 0.8242  delta vs bare = -0.0859\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Validate all scoring modes\n",
    "print(\"=\" * 70)\n",
    "print(\"VALIDATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "s = samples[0]\n",
    "\n",
    "# All modes run without error\n",
    "print(f\"\\n--- All modes on sample 0 ---\")\n",
    "modes = [\n",
    "    (\"bare\",              dict()),\n",
    "    (\"oracle\",            dict(prefix_text=s['query'])),\n",
    "    (\"S=1 (bare_no_bos)\", dict(position_offset=1)),\n",
    "    (\"S=4\",               dict(position_offset=4)),\n",
    "    (\"S=8\",               dict(position_offset=8)),\n",
    "    (\"S=32\",              dict(position_offset=32)),\n",
    "    (\"no_bos_at_all\",     dict(no_bos_input=True)),\n",
    "    (\"keep_bos_S=4\",      dict(position_offset=4, keep_bos_in_offset=True)),\n",
    "    (\"prune_first_3\",     dict(remove_bos=True, prune_first=3)),\n",
    "    (\"prune_last_3\",      dict(remove_bos=True, prune_last=3)),\n",
    "]\n",
    "\n",
    "for name, kwargs in modes:\n",
    "    nll = score(s['passage'], s['query'], s['answer'], **kwargs)\n",
    "    delta = score(s['passage'], s['query'], s['answer']) - nll\n",
    "    print(f\"  {name:<20} NLL = {nll:.4f}  delta vs bare = {delta:+.4f}\")\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20fb0dd1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-20T15:23:30.372693Z",
     "iopub.status.busy": "2026-02-20T15:23:30.372387Z",
     "iopub.status.idle": "2026-02-20T15:41:34.918428Z",
     "shell.execute_reply": "2026-02-20T15:41:34.917518Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "SCORING ALL CONDITIONS\n",
      "======================================================================\n",
      "Starting fresh: 14 conditions x 400 samples\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0efbba3869648bead635906300f2db2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Scoring:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 20/400 | 0.9m | ETA 17.0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 40/400 | 1.8m | ETA 16.1m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 60/400 | 2.7m | ETA 15.2m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 80/400 | 3.6m | ETA 14.3m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 100/400 | 4.5m | ETA 13.4m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 120/400 | 5.4m | ETA 12.6m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 140/400 | 6.3m | ETA 11.7m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 160/400 | 7.2m | ETA 10.8m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 180/400 | 8.1m | ETA 9.9m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 200/400 | 9.0m | ETA 9.0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 220/400 | 9.9m | ETA 8.1m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 240/400 | 10.8m | ETA 7.2m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 260/400 | 11.7m | ETA 6.3m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 280/400 | 12.6m | ETA 5.4m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 300/400 | 13.5m | ETA 4.5m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 320/400 | 14.4m | ETA 3.6m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 340/400 | 15.3m | ETA 2.7m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 360/400 | 16.3m | ETA 1.8m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 380/400 | 17.2m | ETA 0.9m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 400/400 | 18.1m | ETA 0.0m\n",
      "\n",
      "Scoring complete: 400 samples, 14 conditions in 18.1 min\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Scoring loop — 14 conditions x 400 samples\n",
    "print(\"=\" * 70)\n",
    "print(\"SCORING ALL CONDITIONS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "COND_NAMES = [\n",
    "    'bare', 'oracle',\n",
    "    'pos_1', 'pos_2', 'pos_4', 'pos_8', 'pos_16', 'pos_32',\n",
    "    'no_bos_at_all', 'keep_bos_offset_4',\n",
    "    'prune_first_1', 'prune_first_3', 'prune_first_5', 'prune_last_3',\n",
    "]\n",
    "\n",
    "results = []\n",
    "start_idx = 0\n",
    "\n",
    "if CHECKPOINT_PATH.exists():\n",
    "    ckpt = json.loads(CHECKPOINT_PATH.read_text())\n",
    "    if ckpt.get('n_total') == N_SAMPLES and len(ckpt.get('results', [])) > 0:\n",
    "        saved_queries = [r['query'][:50] for r in ckpt['results']]\n",
    "        current_queries = [s['query'][:50] for s in samples[:len(saved_queries)]]\n",
    "        if saved_queries == current_queries:\n",
    "            results = ckpt['results']\n",
    "            start_idx = len(results)\n",
    "            print(f\"Resuming from checkpoint: {start_idx}/{N_SAMPLES}\")\n",
    "\n",
    "if start_idx == 0:\n",
    "    print(f\"Starting fresh: {len(COND_NAMES)} conditions x {N_SAMPLES} samples\")\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "for i in tqdm(range(start_idx, N_SAMPLES), initial=start_idx, total=N_SAMPLES,\n",
    "              desc=\"Scoring\"):\n",
    "    s = samples[i]\n",
    "    query = s['query']\n",
    "    passage = s['passage']\n",
    "    answer = s['answer']\n",
    "\n",
    "    result = {\n",
    "        'query': query,\n",
    "        'answer': answer,\n",
    "        'passage_words': s['word_count'],\n",
    "        'query_words': len(query.split()),\n",
    "    }\n",
    "\n",
    "    # Baselines\n",
    "    result['nll_bare'] = score(passage, query, answer)\n",
    "    result['nll_oracle'] = score(passage, query, answer, prefix_text=query)\n",
    "\n",
    "    # Position sweep (BOS removed)\n",
    "    for S in [1, 2, 4, 8, 16, 32]:\n",
    "        result[f'nll_pos_{S}'] = score(passage, query, answer, position_offset=S)\n",
    "\n",
    "    # BOS mechanism\n",
    "    result['nll_no_bos_at_all'] = score(passage, query, answer, no_bos_input=True)\n",
    "    result['nll_keep_bos_offset_4'] = score(passage, query, answer,\n",
    "                                             position_offset=4,\n",
    "                                             keep_bos_in_offset=True)\n",
    "\n",
    "    # Cache pruning (natural positions, BOS removed)\n",
    "    result['nll_prune_first_1'] = score(passage, query, answer,\n",
    "                                         remove_bos=True, prune_first=1)\n",
    "    result['nll_prune_first_3'] = score(passage, query, answer,\n",
    "                                         remove_bos=True, prune_first=3)\n",
    "    result['nll_prune_first_5'] = score(passage, query, answer,\n",
    "                                         remove_bos=True, prune_first=5)\n",
    "    result['nll_prune_last_3'] = score(passage, query, answer,\n",
    "                                        remove_bos=True, prune_last=3)\n",
    "\n",
    "    results.append(result)\n",
    "\n",
    "    if (i + 1) % 20 == 0 or i == N_SAMPLES - 1:\n",
    "        ckpt = {\n",
    "            'n_total': N_SAMPLES,\n",
    "            'results': results,\n",
    "            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        }\n",
    "        CHECKPOINT_PATH.write_text(json.dumps(ckpt))\n",
    "        elapsed = time.time() - t0\n",
    "        done = i - start_idx + 1\n",
    "        eta = (N_SAMPLES - i - 1) * elapsed / done if done > 0 else 0\n",
    "        tqdm.write(f\"  Checkpoint {i+1}/{N_SAMPLES} | {elapsed/60:.1f}m | ETA {eta/60:.1f}m\")\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "elapsed = time.time() - t0\n",
    "print(f\"\\nScoring complete: {len(results)} samples, \"\n",
    "      f\"{len(COND_NAMES)} conditions in {elapsed/60:.1f} min\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5d7178c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-20T15:41:34.922513Z",
     "iopub.status.busy": "2026-02-20T15:41:34.921984Z",
     "iopub.status.idle": "2026-02-20T15:41:34.948442Z",
     "shell.execute_reply": "2026-02-20T15:41:34.947491Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "RESULTS (N=400)\n",
      "======================================================================\n",
      "\n",
      "  Condition                   NLL    vs bare        d     Win%            p   sig   Recovery\n",
      "  --------------------------------------------------------------------------------------------\n",
      "  bare                     1.6022         --       --       --           --    --         --\n",
      "  oracle                   0.9044    +0.6977   +0.428    73.8%     2.32e-16   ***     100.0%\n",
      "  pos_1                    1.5026    +0.0995   +0.069    52.0%     1.66e-01    ns      14.3%\n",
      "  pos_2                    0.7355    +0.8667   +0.449    70.2%     1.15e-17   ***     124.2%\n",
      "  pos_4                    0.6406    +0.9616   +0.517    75.5%     2.28e-22   ***     137.8%\n",
      "  pos_8                    0.7465    +0.8556   +0.452    73.0%     6.87e-18   ***     122.6%\n",
      "  pos_16                   0.9219    +0.6803   +0.329    62.5%     1.50e-10   ***      97.5%\n",
      "  pos_32                   1.1103    +0.4918   +0.224    58.0%     9.54e-06   ***      70.5%\n",
      "  no_bos_at_all            2.2453    -0.6431   -0.374    29.2%     4.61e-13   ***     -92.2%\n",
      "  keep_bos_offset_4        1.4987    +0.1034   +0.055    50.5%     2.68e-01    ns      14.8%\n",
      "  prune_first_1            0.7263    +0.8758   +0.494    76.8%     9.38e-21   ***     125.5%\n",
      "  prune_first_3            0.6658    +0.9363   +0.536    82.0%     1.06e-23   ***     134.2%\n",
      "  prune_first_5            0.7480    +0.8541   +0.536    78.2%     9.23e-24   ***     122.4%\n",
      "  prune_last_3             1.3157    +0.2865   +0.170    61.3%     7.30e-04   ***      41.1%\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Results table\n",
    "print(\"=\" * 70)\n",
    "print(f\"RESULTS (N={len(results)})\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "arrays = {}\n",
    "for name in COND_NAMES:\n",
    "    arrays[name] = np.array([r[f'nll_{name}'] for r in results])\n",
    "\n",
    "bare = arrays['bare']\n",
    "oracle = arrays['oracle']\n",
    "oracle_delta_mean = (bare - oracle).mean()\n",
    "\n",
    "print(f\"\\n  {'Condition':<22} {'NLL':>8} {'vs bare':>10} {'d':>8} \"\n",
    "      f\"{'Win%':>8} {'p':>12} {'sig':>5} {'Recovery':>10}\")\n",
    "print(f\"  {'-'*92}\")\n",
    "\n",
    "analysis = {}\n",
    "for name in COND_NAMES:\n",
    "    nlls = arrays[name]\n",
    "    mean_nll = nlls.mean()\n",
    "\n",
    "    if name == 'bare':\n",
    "        print(f\"  {name:<22} {mean_nll:>8.4f} {'--':>10} {'--':>8} \"\n",
    "              f\"{'--':>8} {'--':>12} {'--':>5} {'--':>10}\")\n",
    "        analysis[name] = {'mean_nll': float(mean_nll)}\n",
    "    else:\n",
    "        diff = bare - nlls\n",
    "        d = cohens_d(diff)\n",
    "        win_pct = 100 * np.mean(diff > 0)\n",
    "        _, p_val = stats.ttest_1samp(diff, 0)\n",
    "        sig = '***' if p_val < 0.001 else '**' if p_val < 0.01 else '*' if p_val < 0.05 else 'ns'\n",
    "        rec = diff.mean() / oracle_delta_mean * 100 if oracle_delta_mean > 0 else 0\n",
    "        rec_str = f\"{rec:>9.1f}%\"\n",
    "\n",
    "        print(f\"  {name:<22} {mean_nll:>8.4f} {diff.mean():>+10.4f} {d:>+8.3f} \"\n",
    "              f\"{win_pct:>7.1f}% {p_val:>12.2e} {sig:>5} {rec_str:>10}\")\n",
    "        analysis[name] = {\n",
    "            'mean_nll': float(mean_nll), 'delta': float(diff.mean()),\n",
    "            'd': float(d), 'win_pct': float(win_pct), 'p': float(p_val),\n",
    "            'recovery': float(rec),\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9ed287a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-20T15:41:34.951800Z",
     "iopub.status.busy": "2026-02-20T15:41:34.951506Z",
     "iopub.status.idle": "2026-02-20T15:41:34.973427Z",
     "shell.execute_reply": "2026-02-20T15:41:34.972638Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "POSITION SWEEP\n",
      "======================================================================\n",
      "\n",
      "  All conditions: BOS removed from cache, no prefix tokens.\n",
      "  S = position offset (BOS at 0, doc at S..S+D)\n",
      "\n",
      "     S      NLL  d vs bare   recovery   d vs S=1     p vs S=1\n",
      "  --------------------------------------------------------------\n",
      "     1   1.5026    +0.0693      14.3%    +0.0000          nan ns\n",
      "     2   0.7355    +0.4486     124.2%    +0.5414     3.96e-24 ***\n",
      "     4   0.6406    +0.5171     137.8%    +0.5921     6.21e-28 ***\n",
      "     8   0.7465    +0.4520     122.6%    +0.4840     4.76e-20 ***\n",
      "    16   0.9219    +0.3289      97.5%    +0.3367     5.80e-11 ***\n",
      "    32   1.1103    +0.2243      70.5%    +0.2113     2.95e-05 ***\n",
      "\n",
      "  Peak: S=4 (d = +0.5171)\n",
      "\n",
      "  Curve shape:\n",
      "  -> INVERTED-U with peak at S=4.\n",
      "  -> Below peak: benefit increases with offset\n",
      "  -> Above peak: benefit decreases (over-shifting)\n",
      "\n",
      "  Adjacent pairwise tests (does increasing S help?):\n",
      "  S= 1 → S= 2: d = +0.5414 (***) — S=2 is better\n",
      "  S= 2 → S= 4: d = +0.1428 (**) — S=4 is better\n",
      "  S= 4 → S= 8: d = -0.1297 (**) — S=8 is worse\n",
      "  S= 8 → S=16: d = -0.2398 (***) — S=16 is worse\n",
      "  S=16 → S=32: d = -0.2391 (***) — S=32 is worse\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Position sweep analysis\n",
    "print(\"=\" * 70)\n",
    "print(\"POSITION SWEEP\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\n  All conditions: BOS removed from cache, no prefix tokens.\")\n",
    "print(f\"  S = position offset (BOS at 0, doc at S..S+D)\")\n",
    "\n",
    "print(f\"\\n  {'S':>4} {'NLL':>8} {'d vs bare':>10} {'recovery':>10} \"\n",
    "      f\"{'d vs S=1':>10} {'p vs S=1':>12}\")\n",
    "print(f\"  {'-'*62}\")\n",
    "\n",
    "sweep_s = [1, 2, 4, 8, 16, 32]\n",
    "sweep_d = []\n",
    "for S in sweep_s:\n",
    "    name = f'pos_{S}'\n",
    "    nlls = arrays[name]\n",
    "    d_b = cohens_d(bare - nlls)\n",
    "    rec = (bare - nlls).mean() / oracle_delta_mean * 100\n",
    "    diff_vs_1 = arrays['pos_1'] - nlls\n",
    "    d_1 = cohens_d(diff_vs_1)\n",
    "    _, p_1 = stats.ttest_1samp(diff_vs_1, 0)\n",
    "    sig = '***' if p_1 < 0.001 else '**' if p_1 < 0.01 else '*' if p_1 < 0.05 else 'ns'\n",
    "    print(f\"  {S:>4} {nlls.mean():>8.4f} {d_b:>+10.4f} {rec:>9.1f}% \"\n",
    "          f\"{d_1:>+10.4f} {p_1:>12.2e} {sig}\")\n",
    "    sweep_d.append(d_b)\n",
    "\n",
    "# Find peak\n",
    "peak_idx = np.argmax(sweep_d)\n",
    "peak_s = sweep_s[peak_idx]\n",
    "print(f\"\\n  Peak: S={peak_s} (d = {sweep_d[peak_idx]:+.4f})\")\n",
    "\n",
    "# Shape characterization\n",
    "print(f\"\\n  Curve shape:\")\n",
    "if peak_idx == 0:\n",
    "    print(f\"  -> MONOTONICALLY DECREASING from S=1. Offset always hurts.\")\n",
    "elif peak_idx == len(sweep_s) - 1:\n",
    "    print(f\"  -> MONOTONICALLY INCREASING to S=32. Larger offset always better.\")\n",
    "else:\n",
    "    print(f\"  -> INVERTED-U with peak at S={peak_s}.\")\n",
    "    print(f\"  -> Below peak: benefit increases with offset\")\n",
    "    print(f\"  -> Above peak: benefit decreases (over-shifting)\")\n",
    "\n",
    "# Adjacent pairwise significance\n",
    "print(f\"\\n  Adjacent pairwise tests (does increasing S help?):\")\n",
    "for i in range(len(sweep_s) - 1):\n",
    "    s_a, s_b = sweep_s[i], sweep_s[i + 1]\n",
    "    diff = arrays[f'pos_{s_a}'] - arrays[f'pos_{s_b}']\n",
    "    d = cohens_d(diff)\n",
    "    _, p = stats.ttest_1samp(diff, 0)\n",
    "    sig = '***' if p < 0.001 else '**' if p < 0.01 else '*' if p < 0.05 else 'ns'\n",
    "    direction = \"better\" if d > 0 else \"worse\" if d < 0 else \"same\"\n",
    "    print(f\"  S={s_a:>2} → S={s_b:>2}: d = {d:+.4f} ({sig}) — S={s_b} is {direction}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4a5359a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-20T15:41:34.977326Z",
     "iopub.status.busy": "2026-02-20T15:41:34.976960Z",
     "iopub.status.idle": "2026-02-20T15:41:35.004062Z",
     "shell.execute_reply": "2026-02-20T15:41:35.003283Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "BOS MECHANISM ISOLATION\n",
      "======================================================================\n",
      "\n",
      "--- Where does BOS matter? ---\n",
      "\n",
      "  Phase B (removing BOS from cache):\n",
      "    bare → pos_1 (bare_no_bos): d = +0.0693, p = 1.66e-01\n",
      "    NLL: 1.6022 → 1.5026\n",
      "\n",
      "  Phase A (removing BOS from input):\n",
      "    pos_1 → no_bos_at_all: d = -0.4204 (***), p = 7.51e-16\n",
      "    NLL: 1.5026 → 2.2453\n",
      "    → BOS in Phase A HELPS doc representations\n",
      "\n",
      "  Phase B BOS effect at S=4:\n",
      "    keep_bos_offset_4 NLL = 1.4987\n",
      "    pos_4 (BOS removed) NLL = 0.6406\n",
      "    Keeping BOS: d = +0.6187 (***)\n",
      "    → Even at S=4, BOS in cache HURTS Phase B\n",
      "\n",
      "======================================================================\n",
      "CACHE PRUNING\n",
      "======================================================================\n",
      "\n",
      "  All conditions: BOS removed, natural positions, doc tokens pruned.\n",
      "\n",
      "  Condition                   NLL  d vs bare   d vs pos_1            p\n",
      "  ----------------------------------------------------------------------\n",
      "  pos_1                    1.5026    +0.0693      +0.0000          nan ns\n",
      "  prune_first_1            0.7263    +0.4942      +0.5146     3.46e-22 ***\n",
      "  prune_first_3            0.6658    +0.5356      +0.5546     4.22e-25 ***\n",
      "  prune_first_5            0.7480    +0.5364      +0.5614     1.32e-25 ***\n",
      "  prune_last_3             1.3157    +0.1702      +0.2157     2.02e-05 ***\n",
      "\n",
      "  Interpretation:\n",
      "  -> Removing doc[0] helps (d = +0.5146): early tokens ARE sinks\n",
      "  -> Removing last 3 helps?! (d = +0.2157): unexpected\n",
      "\n",
      "  Best non-oracle conditions:\n",
      "    prune_first_5          d = +0.5364 (122% recovery)\n",
      "    prune_first_3          d = +0.5356 (134% recovery)\n",
      "    pos_4                  d = +0.5171 (138% recovery)\n",
      "    prune_first_1          d = +0.4942 (126% recovery)\n",
      "    pos_8                  d = +0.4520 (123% recovery)\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: BOS mechanism isolation and cache surgery\n",
    "print(\"=\" * 70)\n",
    "print(\"BOS MECHANISM ISOLATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# --- Phase A vs Phase B BOS effects ---\n",
    "print(f\"\\n--- Where does BOS matter? ---\\n\")\n",
    "\n",
    "# Phase B effect: bare → bare_no_bos (pos_1)\n",
    "f_phase_b = bare - arrays['pos_1']\n",
    "d_pb = cohens_d(f_phase_b)\n",
    "_, p_pb = stats.ttest_1samp(f_phase_b, 0)\n",
    "print(f\"  Phase B (removing BOS from cache):\")\n",
    "print(f\"    bare → pos_1 (bare_no_bos): d = {d_pb:+.4f}, p = {p_pb:.2e}\")\n",
    "print(f\"    NLL: {bare.mean():.4f} → {arrays['pos_1'].mean():.4f}\")\n",
    "\n",
    "# Phase A effect: bare_no_bos → no_bos_at_all\n",
    "f_phase_a = arrays['pos_1'] - arrays['no_bos_at_all']\n",
    "d_pa = cohens_d(f_phase_a)\n",
    "_, p_pa = stats.ttest_1samp(f_phase_a, 0)\n",
    "sig_pa = '***' if p_pa < 0.001 else '**' if p_pa < 0.01 else '*' if p_pa < 0.05 else 'ns'\n",
    "print(f\"\\n  Phase A (removing BOS from input):\")\n",
    "print(f\"    pos_1 → no_bos_at_all: d = {d_pa:+.4f} ({sig_pa}), p = {p_pa:.2e}\")\n",
    "print(f\"    NLL: {arrays['pos_1'].mean():.4f} → {arrays['no_bos_at_all'].mean():.4f}\")\n",
    "if d_pa > 0.05:\n",
    "    print(f\"    → BOS in Phase A HURTS doc representations\")\n",
    "elif d_pa < -0.05:\n",
    "    print(f\"    → BOS in Phase A HELPS doc representations\")\n",
    "else:\n",
    "    print(f\"    → BOS in Phase A has minimal effect on doc representations\")\n",
    "\n",
    "# Phase B BOS effect at optimal offset\n",
    "print(f\"\\n  Phase B BOS effect at S=4:\")\n",
    "diff_keep = arrays['keep_bos_offset_4'] - arrays['pos_4']\n",
    "d_keep = cohens_d(diff_keep)\n",
    "_, p_keep = stats.ttest_1samp(diff_keep, 0)\n",
    "sig_k = '***' if p_keep < 0.001 else '**' if p_keep < 0.01 else '*' if p_keep < 0.05 else 'ns'\n",
    "print(f\"    keep_bos_offset_4 NLL = {arrays['keep_bos_offset_4'].mean():.4f}\")\n",
    "print(f\"    pos_4 (BOS removed) NLL = {arrays['pos_4'].mean():.4f}\")\n",
    "print(f\"    Keeping BOS: d = {d_keep:+.4f} ({sig_k})\")\n",
    "if d_keep > 0.05:\n",
    "    print(f\"    → Even at S=4, BOS in cache HURTS Phase B\")\n",
    "elif d_keep < -0.05:\n",
    "    print(f\"    → At S=4, BOS in cache actually helps Phase B\")\n",
    "else:\n",
    "    print(f\"    → At S=4, BOS in cache makes no difference\")\n",
    "\n",
    "# -----------------------------------------------------------------------\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"CACHE PRUNING\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "print(f\"\\n  All conditions: BOS removed, natural positions, doc tokens pruned.\")\n",
    "print(f\"\\n  {'Condition':<22} {'NLL':>8} {'d vs bare':>10} {'d vs pos_1':>12} {'p':>12}\")\n",
    "print(f\"  {'-'*70}\")\n",
    "for name in ['pos_1', 'prune_first_1', 'prune_first_3', 'prune_first_5', 'prune_last_3']:\n",
    "    nlls = arrays[name]\n",
    "    d_b = cohens_d(bare - nlls)\n",
    "    diff_vs_1 = arrays['pos_1'] - nlls\n",
    "    d_1 = cohens_d(diff_vs_1)\n",
    "    _, p_1 = stats.ttest_1samp(diff_vs_1, 0)\n",
    "    sig = '***' if p_1 < 0.001 else '**' if p_1 < 0.01 else '*' if p_1 < 0.05 else 'ns'\n",
    "    print(f\"  {name:<22} {nlls.mean():>8.4f} {d_b:>+10.4f} {d_1:>+12.4f} {p_1:>12.2e} {sig}\")\n",
    "\n",
    "# Interpretation\n",
    "d_pf1 = cohens_d(arrays['pos_1'] - arrays['prune_first_1'])\n",
    "d_pf3 = cohens_d(arrays['pos_1'] - arrays['prune_first_3'])\n",
    "d_pf5 = cohens_d(arrays['pos_1'] - arrays['prune_first_5'])\n",
    "d_pl3 = cohens_d(arrays['pos_1'] - arrays['prune_last_3'])\n",
    "\n",
    "print(f\"\\n  Interpretation:\")\n",
    "if d_pf1 > 0.05:\n",
    "    print(f\"  -> Removing doc[0] helps (d = {d_pf1:+.4f}): early tokens ARE sinks\")\n",
    "elif d_pf1 < -0.05:\n",
    "    print(f\"  -> Removing doc[0] hurts (d = {d_pf1:+.4f}): doc[0] is informative\")\n",
    "else:\n",
    "    print(f\"  -> Removing doc[0] neutral (d = {d_pf1:+.4f})\")\n",
    "\n",
    "if d_pl3 < -0.05:\n",
    "    print(f\"  -> Removing last 3 hurts (d = {d_pl3:+.4f}): confirms end tokens are informative\")\n",
    "elif d_pl3 > 0.05:\n",
    "    print(f\"  -> Removing last 3 helps?! (d = {d_pl3:+.4f}): unexpected\")\n",
    "else:\n",
    "    print(f\"  -> Removing last 3 neutral (d = {d_pl3:+.4f})\")\n",
    "\n",
    "# Best overall condition\n",
    "print(f\"\\n  Best non-oracle conditions:\")\n",
    "non_oracle = {k: v for k, v in analysis.items()\n",
    "              if k not in ('bare', 'oracle') and 'd' in v}\n",
    "sorted_conds = sorted(non_oracle.items(), key=lambda x: x[1]['d'], reverse=True)\n",
    "for name, info in sorted_conds[:5]:\n",
    "    print(f\"    {name:<22} d = {info['d']:+.4f} ({info['recovery']:.0f}% recovery)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4dce2c20",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-20T15:41:35.007853Z",
     "iopub.status.busy": "2026-02-20T15:41:35.007158Z",
     "iopub.status.idle": "2026-02-20T15:41:35.602118Z",
     "shell.execute_reply": "2026-02-20T15:41:35.601126Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "VERDICT — Exp 04: Position Sweep + Cache Surgery\n",
      "======================================================================\n",
      "\n",
      "Model: google/gemma-3-4b-it\n",
      "N: 400 samples (MS MARCO v1.1)\n",
      "\n",
      "--- Key findings ---\n",
      "  Best condition: prune_first_5 (d = +0.5364, 122% of oracle)\n",
      "  Oracle: d = +0.4284\n",
      "  Position peak: S=4\n",
      "\n",
      "  BOS effects:\n",
      "    Phase B (cache removal): d = +0.0693\n",
      "    Phase A (input removal): d = -0.4204\n",
      "\n",
      "--- All conditions (ranked by d) ---\n",
      "  prune_first_5          NLL = 0.7480  d = +0.5364\n",
      "  prune_first_3          NLL = 0.6658  d = +0.5356\n",
      "  pos_4                  NLL = 0.6406  d = +0.5171\n",
      "  prune_first_1          NLL = 0.7263  d = +0.4942\n",
      "  pos_8                  NLL = 0.7465  d = +0.4520\n",
      "  pos_2                  NLL = 0.7355  d = +0.4486\n",
      "  oracle                 NLL = 0.9044  d = +0.4284\n",
      "  pos_16                 NLL = 0.9219  d = +0.3289\n",
      "  pos_32                 NLL = 1.1103  d = +0.2243\n",
      "  prune_last_3           NLL = 1.3157  d = +0.1702\n",
      "  pos_1                  NLL = 1.5026  d = +0.0693\n",
      "  keep_bos_offset_4      NLL = 1.4987  d = +0.0554\n",
      "  no_bos_at_all          NLL = 2.2453  d = -0.3743\n",
      "  bare                   NLL = 1.6022  (baseline)\n",
      "\n",
      "Results saved to ../../../results/decoder_only/exp04/results.json\n",
      "\n",
      "Cleaning up GPU memory...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory: 8.61 GB -> 0.01 GB\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Cell 10: Verdict\n",
    "print(\"=\" * 70)\n",
    "print(\"VERDICT — Exp 04: Position Sweep + Cache Surgery\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\nModel: {MODEL_NAME}\")\n",
    "print(f\"N: {len(results)} samples (MS MARCO v1.1)\")\n",
    "\n",
    "# Best condition\n",
    "non_oracle = {k: v for k, v in analysis.items()\n",
    "              if k not in ('bare', 'oracle') and 'd' in v}\n",
    "best_name = max(non_oracle, key=lambda k: non_oracle[k]['d'])\n",
    "best_d = non_oracle[best_name]['d']\n",
    "best_rec = non_oracle[best_name]['recovery']\n",
    "\n",
    "print(f\"\\n--- Key findings ---\")\n",
    "print(f\"  Best condition: {best_name} (d = {best_d:+.4f}, {best_rec:.0f}% of oracle)\")\n",
    "print(f\"  Oracle: d = {cohens_d(bare - oracle):+.4f}\")\n",
    "\n",
    "# Position peak\n",
    "sweep_ds = [cohens_d(bare - arrays[f'pos_{S}']) for S in [1, 2, 4, 8, 16, 32]]\n",
    "peak_s = [1, 2, 4, 8, 16, 32][np.argmax(sweep_ds)]\n",
    "print(f\"  Position peak: S={peak_s}\")\n",
    "\n",
    "# BOS summary\n",
    "d_phase_b = cohens_d(bare - arrays['pos_1'])\n",
    "d_phase_a = cohens_d(arrays['pos_1'] - arrays['no_bos_at_all'])\n",
    "print(f\"\\n  BOS effects:\")\n",
    "print(f\"    Phase B (cache removal): d = {d_phase_b:+.4f}\")\n",
    "print(f\"    Phase A (input removal): d = {d_phase_a:+.4f}\")\n",
    "\n",
    "# All conditions ordered\n",
    "print(f\"\\n--- All conditions (ranked by d) ---\")\n",
    "all_ranked = sorted(analysis.items(),\n",
    "                    key=lambda x: x[1].get('d', -999), reverse=True)\n",
    "for name, info in all_ranked:\n",
    "    if name == 'bare':\n",
    "        print(f\"  {name:<22} NLL = {info['mean_nll']:.4f}  (baseline)\")\n",
    "    else:\n",
    "        print(f\"  {name:<22} NLL = {info['mean_nll']:.4f}  d = {info['d']:+.4f}\")\n",
    "\n",
    "# Save\n",
    "final_results = {\n",
    "    'experiment': 'v4_decoder_only_exp04_position_sweep_cache_surgery',\n",
    "    'model': MODEL_NAME,\n",
    "    'dataset': 'ms_marco_v1.1',\n",
    "    'n_samples': len(results),\n",
    "    'seed': SEED,\n",
    "    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'conditions': {k: v for k, v in analysis.items()},\n",
    "}\n",
    "\n",
    "with open(RESULTS_DIR / 'results.json', 'w') as f:\n",
    "    json.dump(final_results, f, indent=2)\n",
    "print(f\"\\nResults saved to {RESULTS_DIR / 'results.json'}\")\n",
    "\n",
    "# Cleanup\n",
    "print(f\"\\nCleaning up GPU memory...\")\n",
    "mem_before = torch.cuda.memory_allocated() / 1e9\n",
    "del model, tokenizer\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "mem_after = torch.cuda.memory_allocated() / 1e9\n",
    "print(f\"GPU memory: {mem_before:.2f} GB -> {mem_after:.2f} GB\")\n",
    "print(\"Done!\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "03cf6bef8fcc4c7fb62f8a687da273a4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_5cc5fdd17d4943c4b1175077c125a7dd",
       "max": 883.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_ab73e7ad93644972b41bbc0acca5f36a",
       "tabbable": null,
       "tooltip": null,
       "value": 883.0
      }
     },
     "23e17b76369e4789849cdbd03c989857": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2600745cb4784a1b9556fad962d29c90": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "28f7cb27fa2e400487dbb598c7a9a300": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "4bb1971461b74f58976b5348d177d59c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "5cc5fdd17d4943c4b1175077c125a7dd": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6493c18a4dbf417ab654426b14e34b0f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "67348cfa71d64e33a7086a82ce75678b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_28f7cb27fa2e400487dbb598c7a9a300",
       "placeholder": "​",
       "style": "IPY_MODEL_a53320af44e24b73869a3779dddd43d8",
       "tabbable": null,
       "tooltip": null,
       "value": "Loading weights: 100%"
      }
     },
     "703a5db29a654eb09808ad9cbb3a1b73": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "8f49bba2245347d38401f99739a5e883": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_4bb1971461b74f58976b5348d177d59c",
       "placeholder": "​",
       "style": "IPY_MODEL_703a5db29a654eb09808ad9cbb3a1b73",
       "tabbable": null,
       "tooltip": null,
       "value": " 400/400 [18:04&lt;00:00,  2.65s/it]"
      }
     },
     "931dc7e95da34cee930639fd54593151": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_67348cfa71d64e33a7086a82ce75678b",
        "IPY_MODEL_03cf6bef8fcc4c7fb62f8a687da273a4",
        "IPY_MODEL_bead063026124165a80c1c9ddbee91e9"
       ],
       "layout": "IPY_MODEL_6493c18a4dbf417ab654426b14e34b0f",
       "tabbable": null,
       "tooltip": null
      }
     },
     "9ca5238cf56043febb60632d196c8176": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "a53320af44e24b73869a3779dddd43d8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "ab73e7ad93644972b41bbc0acca5f36a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "bead063026124165a80c1c9ddbee91e9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_2600745cb4784a1b9556fad962d29c90",
       "placeholder": "​",
       "style": "IPY_MODEL_cf8b41f73fa744a6930b0eb8646e2d6a",
       "tabbable": null,
       "tooltip": null,
       "value": " 883/883 [00:03&lt;00:00, 519.20it/s, Materializing param=model.vision_tower.vision_model.post_layernorm.weight]"
      }
     },
     "c0efbba3869648bead635906300f2db2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_f3422bce5dd84de9b5f4aea9e8271a42",
        "IPY_MODEL_d07c8e7d917c40eda859c3816eaafc60",
        "IPY_MODEL_8f49bba2245347d38401f99739a5e883"
       ],
       "layout": "IPY_MODEL_fa919e08ac624c36b28f6c5a79786084",
       "tabbable": null,
       "tooltip": null
      }
     },
     "cba32eec82b64de09aa250c5d2706be6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "cf8b41f73fa744a6930b0eb8646e2d6a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "d07c8e7d917c40eda859c3816eaafc60": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_cba32eec82b64de09aa250c5d2706be6",
       "max": 400.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_9ca5238cf56043febb60632d196c8176",
       "tabbable": null,
       "tooltip": null,
       "value": 400.0
      }
     },
     "f3422bce5dd84de9b5f4aea9e8271a42": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_23e17b76369e4789849cdbd03c989857",
       "placeholder": "​",
       "style": "IPY_MODEL_f55b3ae1b8ad4c5e8248d29c21fe591f",
       "tabbable": null,
       "tooltip": null,
       "value": "Scoring: 100%"
      }
     },
     "f55b3ae1b8ad4c5e8248d29c21fe591f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "fa919e08ac624c36b28f6c5a79786084": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
