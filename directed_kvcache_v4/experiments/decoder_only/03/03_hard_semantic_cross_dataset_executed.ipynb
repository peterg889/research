{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "699b7b65",
   "metadata": {
    "papermill": {
     "duration": 0.003752,
     "end_time": "2026-02-22T13:00:53.395790",
     "exception": false,
     "start_time": "2026-02-22T13:00:53.392038",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Experiment 03: Hard-Example Semantic Isolation Across Datasets\n",
    "\n",
    "## Motivation\n",
    "\n",
    "Exp 02 (token-matched, 13 conditions, N=400, Gemma 3 12B-IT) confirmed:\n",
    "- **Oracle HURTS overall** (d=-0.151, p=0.003)\n",
    "- **No semantic gradient overall** (Spearman rho=-0.43, p=0.40)\n",
    "- **But in Q5 (hardest 20%)**, ALL conditions help, and there IS a semantic gradient:\n",
    "  llm_summarize d=+0.493 > llm_extract +0.356 > same_topic +0.334 > paraphrase +0.291\n",
    "  > random_tokens +0.218 > oracle +0.078\n",
    "- The semantic effect is **real but masked** by the dominant structural effect in easy samples\n",
    "\n",
    "## Goal\n",
    "\n",
    "Isolate the semantic effect by:\n",
    "1. Restricting to hard examples (top 40% by bare NLL) where conditioning helps\n",
    "2. Measuring the **semantic delta** above the structural baseline (condition - random_tokens)\n",
    "3. Testing whether this pattern generalizes across 4 diverse QA datasets\n",
    "\n",
    "## Method — BOS-Retained Repositioning with Token-Level Matching\n",
    "\n",
    "Identical to Exp 02. Phase A builds KV cache with `[BOS] + prefix_ids(Q) + [\\n] + doc_ids(D)`,\n",
    "selects BOS + doc, repositions doc keys. Phase B scores `[\\n + query + \\n + answer]` with\n",
    "cache_position auto-generated from cache length. No look-ahead.\n",
    "\n",
    "## Datasets (4 total)\n",
    "\n",
    "| Dataset | Source | Question type | Passage type |\n",
    "|---------|--------|--------------|--------------|\n",
    "| MS MARCO | `microsoft/ms_marco` v1.1 | Web search queries | Selected passages (30-300w) |\n",
    "| SQuAD 2.0 | `rajpurkar/squad_v2` | Factoid questions | Wikipedia paragraphs (30-500w) |\n",
    "| TriviaQA | `mandarjoshi/trivia_qa` rc.wikipedia | Trivia questions | Wikipedia articles (first 500w) |\n",
    "| HotpotQA | `hotpotqa/hotpot_qa` distractor | Multi-hop questions | Supporting fact sentences (30-500w) |\n",
    "\n",
    "## Conditions (13 total, same as Exp 02)\n",
    "\n",
    "| # | Key | Semantic relevance | Token construction |\n",
    "|---|-----|-------------------|--------------------|\n",
    "| 1 | `bare` | baseline | No prefix |\n",
    "| 2 | `random_tokens` | none | Q random IDs from vocab |\n",
    "| 3 | `repeat_token` | none (structural) | Token ID 1000 repeated Q times |\n",
    "| 4 | `scrambled_oracle` | vocab match only | Random permutation of oracle IDs |\n",
    "| 5 | `unrelated_query` | low | Other sample's query, pad/trunc to Q |\n",
    "| 6 | `same_topic` | medium | LLM: \"Write a question about same topic...\" |\n",
    "| 7 | `paraphrase` | high | LLM: \"Rephrase this query differently...\" |\n",
    "| 8 | `oracle` | maximal | Exact query token IDs |\n",
    "| 9 | `llm_extract` | task-framing (doc) | LLM: \"List key facts from this document\" |\n",
    "| 10 | `llm_question` | query-like (doc) | LLM: \"What question does this doc answer?\" |\n",
    "| 11 | `llm_summarize` | summary (doc) | LLM: \"Summarize in one sentence\" |\n",
    "| 12 | `extractor_matched` | task-framing (generic) | Fixed extraction text |\n",
    "| 13 | `adversarial_matched` | adversarial | Fixed adversarial text |\n",
    "\n",
    "## Key metric: Semantic delta\n",
    "\n",
    "For each condition C: `semantic_delta(C) = NLL(random_tokens) - NLL(C)`\n",
    "\n",
    "Positive = semantic content helps beyond structural baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e22d287f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-22T13:00:53.404133Z",
     "iopub.status.busy": "2026-02-22T13:00:53.403792Z",
     "iopub.status.idle": "2026-02-22T13:01:08.473270Z",
     "shell.execute_reply": "2026-02-22T13:01:08.472353Z"
    },
    "papermill": {
     "duration": 15.076046,
     "end_time": "2026-02-22T13:01:08.474987",
     "exception": false,
     "start_time": "2026-02-22T13:00:53.398941",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading google/gemma-3-12b-it...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef070973cffd45238d7daee94b3422f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/1065 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exp 03: Hard-Example Semantic Isolation Across Datasets\n",
      "Scoring: BOS-retained repositioning + token-level prefix matching\n",
      "N_SAMPLES: 400 per dataset, HARD_FRAC: 0.4\n",
      "Model: google/gemma-3-12b-it\n",
      "DEVICE: cuda:0, dtype: torch.bfloat16\n",
      "GPU memory: 14.51 GB\n",
      "Vocab size: 262208\n",
      "Sliding window: 1024, cache limit: 1023\n",
      "Num layers: 48\n",
      "\n",
      "Setup complete. Functions defined: score, generate_text, make_prefix\n",
      "Conditions: 13 (12 prefixed + bare)\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Setup, model loading, and scoring functions\n",
    "import os\n",
    "os.umask(0o000)\n",
    "import sys, json, time, gc, re\n",
    "import random as pyrandom\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from scipy import stats\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "sys.path.insert(0, \"../../..\")\n",
    "from lib.analysis import cohens_d\n",
    "from lib.data import count_words\n",
    "\n",
    "SEED = 42\n",
    "N_SAMPLES = 400      # per dataset\n",
    "HARD_FRAC = 0.40     # top 40% by bare NLL\n",
    "MODEL_NAME = \"google/gemma-3-12b-it\"\n",
    "\n",
    "RESULTS_DIR = Path(\"../../../results/decoder_only/exp03\")\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "EXP02_DIR = Path(\"../../../results/decoder_only/exp02\")\n",
    "\n",
    "DATASET_NAMES = ['ms_marco', 'squad_v2', 'triviaqa', 'hotpotqa']\n",
    "NEW_DATASETS = ['squad_v2', 'triviaqa', 'hotpotqa']\n",
    "\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "pyrandom.seed(SEED)\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv())\n",
    "HF_TOKEN = os.environ.get(\"HF_TOKEN\")\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, DynamicCache\n",
    "\n",
    "print(f\"Loading {MODEL_NAME}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, token=HF_TOKEN)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME, device_map=\"auto\", dtype=torch.bfloat16, token=HF_TOKEN,\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "DEVICE = next(model.parameters()).device\n",
    "BOS_ID = tokenizer.bos_token_id\n",
    "NEWLINE_IDS = tokenizer(\"\\n\", add_special_tokens=False).input_ids\n",
    "text_cfg = getattr(model.config, 'text_config', model.config)\n",
    "# Use actual embedding table size (config vocab_size may include padding rows)\n",
    "VOCAB_SIZE = model.get_input_embeddings().num_embeddings\n",
    "cfg_vocab = getattr(text_cfg, 'vocab_size', None)\n",
    "if cfg_vocab != VOCAB_SIZE:\n",
    "    print(f\"WARNING: config vocab_size={cfg_vocab} != embedding size={VOCAB_SIZE}\")\n",
    "    print(f\"Using embedding size {VOCAB_SIZE} for random token generation\")\n",
    "rope_params = getattr(text_cfg, 'rope_parameters', {})\n",
    "layer_types = getattr(text_cfg, 'layer_types', [])\n",
    "# Sliding attention layers cache only (sliding_window - 1) entries.\n",
    "# select_kv_cache uses uniform indices across all layers, so total Phase A\n",
    "# tokens must not exceed this limit when a prefix is used.\n",
    "SLIDING_WINDOW = getattr(text_cfg, 'sliding_window', 4096)\n",
    "SLIDING_CACHE_LIMIT = SLIDING_WINDOW - 1  # observed: 1024-1 = 1023 for Gemma 3\n",
    "\n",
    "print(f\"Exp 03: Hard-Example Semantic Isolation Across Datasets\")\n",
    "print(f\"Scoring: BOS-retained repositioning + token-level prefix matching\")\n",
    "print(f\"N_SAMPLES: {N_SAMPLES} per dataset, HARD_FRAC: {HARD_FRAC}\")\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"DEVICE: {DEVICE}, dtype: {next(model.parameters()).dtype}\")\n",
    "print(f\"GPU memory: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "print(f\"Vocab size: {VOCAB_SIZE}\")\n",
    "print(f\"Sliding window: {SLIDING_WINDOW}, cache limit: {SLIDING_CACHE_LIMIT}\")\n",
    "print(f\"Num layers: {getattr(text_cfg, 'num_hidden_layers', 'N/A')}\")\n",
    "\n",
    "# --- RoPE repositioning helpers ---\n",
    "def build_layer_inv_freqs():\n",
    "    inv_freqs = {}\n",
    "    for lt, params in rope_params.items():\n",
    "        theta = params.get('rope_theta', 10000.0)\n",
    "        dim = text_cfg.head_dim\n",
    "        inv_freq = 1.0 / (theta ** (torch.arange(0, dim, 2, dtype=torch.float32, device=DEVICE) / dim))\n",
    "        inv_freqs[lt] = inv_freq\n",
    "    return inv_freqs\n",
    "\n",
    "LAYER_INV_FREQS = build_layer_inv_freqs()\n",
    "\n",
    "\n",
    "def rotate_half(x):\n",
    "    x1, x2 = x[..., :x.shape[-1]//2], x[..., x.shape[-1]//2:]\n",
    "    return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "\n",
    "def select_kv_cache(cache, indices):\n",
    "    selected = DynamicCache()\n",
    "    idx_tensor = torch.tensor(indices, dtype=torch.long, device=DEVICE)\n",
    "    for i in range(len(cache.layers)):\n",
    "        k = cache.layers[i].keys[:, :, idx_tensor, :]\n",
    "        v = cache.layers[i].values[:, :, idx_tensor, :]\n",
    "        selected.update(k, v, i)\n",
    "    return selected\n",
    "\n",
    "\n",
    "def reposition_kv_cache(cache, old_positions, new_positions, bos_start=0):\n",
    "    delta = new_positions - old_positions\n",
    "    for L in range(len(cache.layers)):\n",
    "        lt = layer_types[L]\n",
    "        inv_freq = LAYER_INV_FREQS[lt]\n",
    "        k = cache.layers[L].keys\n",
    "        doc_keys = k[:, :, bos_start + 1:, :]\n",
    "        freqs = torch.einsum('i,j->ij', delta.float(), inv_freq)\n",
    "        emb = torch.cat([freqs, freqs], dim=-1)\n",
    "        cos_delta = emb.cos().to(k.dtype).unsqueeze(0).unsqueeze(0)\n",
    "        sin_delta = emb.sin().to(k.dtype).unsqueeze(0).unsqueeze(0)\n",
    "        doc_keys_new = doc_keys * cos_delta + rotate_half(doc_keys) * sin_delta\n",
    "        cache.layers[L].keys = torch.cat([\n",
    "            k[:, :, :bos_start + 1, :],\n",
    "            doc_keys_new,\n",
    "        ], dim=2)\n",
    "    return cache\n",
    "\n",
    "\n",
    "def score(doc_text, query_text, answer_text, prefix_token_ids=None):\n",
    "    # BOS-retained repositioning.\n",
    "    doc_ids = tokenizer(doc_text, add_special_tokens=False,\n",
    "                        truncation=True, max_length=1024).input_ids\n",
    "\n",
    "    if prefix_token_ids is not None:\n",
    "        P = len(prefix_token_ids)\n",
    "        NL = len(NEWLINE_IDS)\n",
    "        # Truncate doc so total Phase A tokens fit in sliding window cache.\n",
    "        # Sliding attention layers store only (sliding_window - 1) KV entries;\n",
    "        # select_kv_cache needs uniform indexing across all layers.\n",
    "        max_doc = SLIDING_CACHE_LIMIT - 1 - P - NL  # 1 for BOS\n",
    "        if len(doc_ids) > max_doc:\n",
    "            doc_ids = doc_ids[:max_doc]\n",
    "        D = len(doc_ids)\n",
    "        cond_ids = [BOS_ID] + list(prefix_token_ids) + NEWLINE_IDS + doc_ids\n",
    "        with torch.no_grad():\n",
    "            pa = model(input_ids=torch.tensor([cond_ids], device=DEVICE),\n",
    "                       use_cache=True)\n",
    "        cache = pa.past_key_values\n",
    "        del pa\n",
    "        keep_indices = [0] + list(range(1 + P + NL, len(cond_ids)))\n",
    "        cache = select_kv_cache(cache, keep_indices)\n",
    "        old_pos = torch.arange(1 + P + NL, 1 + P + NL + D, device=DEVICE)\n",
    "        new_pos = torch.arange(1, D + 1, device=DEVICE)\n",
    "        cache = reposition_kv_cache(cache, old_pos, new_pos, bos_start=0)\n",
    "    else:\n",
    "        D = len(doc_ids)\n",
    "        with torch.no_grad():\n",
    "            pa = model(input_ids=torch.tensor([[BOS_ID] + doc_ids], device=DEVICE),\n",
    "                       use_cache=True)\n",
    "        cache = pa.past_key_values\n",
    "        del pa\n",
    "\n",
    "    phase_b_start = D + 1\n",
    "    query_ids = tokenizer(\"\\n\" + query_text + \"\\n\",\n",
    "                          add_special_tokens=False).input_ids\n",
    "    answer_ids = tokenizer(answer_text, add_special_tokens=False,\n",
    "                           truncation=True, max_length=256).input_ids\n",
    "    if not answer_ids:\n",
    "        del cache\n",
    "        return 0.0\n",
    "\n",
    "    pb_ids = query_ids + answer_ids\n",
    "    pos = torch.arange(phase_b_start, phase_b_start + len(pb_ids), device=DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pb = model(\n",
    "            input_ids=torch.tensor([pb_ids], device=DEVICE),\n",
    "            past_key_values=cache,\n",
    "            position_ids=pos.unsqueeze(0),\n",
    "            use_cache=False,\n",
    "        )\n",
    "\n",
    "    n_q = len(query_ids)\n",
    "    logits = pb.logits[0, n_q - 1:n_q - 1 + len(answer_ids), :].float()\n",
    "    targets = torch.tensor(answer_ids, device=DEVICE)\n",
    "    nll = -F.log_softmax(logits, dim=-1).gather(\n",
    "        1, targets.unsqueeze(1)).squeeze(1).mean().item()\n",
    "    del cache, pb\n",
    "    return nll\n",
    "\n",
    "\n",
    "def generate_text(input_text, prompt_text, max_new_tokens=50):\n",
    "    messages = [\n",
    "        {\"role\": \"user\",\n",
    "         \"content\": f\"{prompt_text}\\n\\n{input_text}\"}\n",
    "    ]\n",
    "    chat_text = tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    inputs = tokenizer(chat_text, return_tensors=\"pt\",\n",
    "                       truncation=True, max_length=1024).to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "        )\n",
    "    new_tokens = output_ids[0, inputs['input_ids'].shape[1]:]\n",
    "    raw_text = tokenizer.decode(new_tokens, skip_special_tokens=True)\n",
    "    cleaned = raw_text.strip().split(\"\\n\")[0].strip()\n",
    "    cleaned = cleaned.strip('\"').strip(\"'\").strip()\n",
    "    cleaned = \" \".join(cleaned.split()[:20])\n",
    "    return cleaned\n",
    "\n",
    "\n",
    "def make_prefix(token_ids, Q):\n",
    "    if len(token_ids) >= Q:\n",
    "        return token_ids[:Q]\n",
    "    else:\n",
    "        padded = token_ids * ((Q // max(len(token_ids), 1)) + 1)\n",
    "        return padded[:Q]\n",
    "\n",
    "\n",
    "# LLM surrogate prompts\n",
    "PROMPT_PARAPHRASE = (\n",
    "    \"Rephrase this search query using completely different words but keeping \"\n",
    "    \"the same meaning. Keep it to 5-8 words. Output only the rephrased query.\"\n",
    ")\n",
    "PROMPT_SAME_TOPIC = (\n",
    "    \"Write a question about the same topic as this document but asking for \"\n",
    "    \"DIFFERENT information. Keep it to 5-8 words. Output only the question.\"\n",
    ")\n",
    "PROMPT_EXTRACT = (\n",
    "    \"List the key facts from this document as a brief comma-separated list. \"\n",
    "    \"Output only the fact list, nothing else.\"\n",
    ")\n",
    "PROMPT_QUESTION = (\n",
    "    \"What question does this document answer? Write only the question, \"\n",
    "    \"nothing else. Keep it to 5-10 words.\"\n",
    ")\n",
    "PROMPT_SUMMARIZE = (\n",
    "    \"Summarize this document in one sentence. Output only the summary, nothing else.\"\n",
    ")\n",
    "\n",
    "# Condition definitions\n",
    "COND_NAMES = [\n",
    "    'bare', 'random_tokens', 'repeat_token', 'scrambled_oracle',\n",
    "    'unrelated_query', 'same_topic', 'paraphrase', 'oracle',\n",
    "    'llm_extract', 'llm_question', 'llm_summarize',\n",
    "    'extractor_matched', 'adversarial_matched',\n",
    "]\n",
    "\n",
    "COND_PREFIX_MAP = {\n",
    "    'random_tokens': 'prefix_random_tokens',\n",
    "    'repeat_token': 'prefix_repeat_token',\n",
    "    'scrambled_oracle': 'prefix_scrambled_oracle',\n",
    "    'unrelated_query': 'prefix_unrelated_query',\n",
    "    'same_topic': 'prefix_same_topic',\n",
    "    'paraphrase': 'prefix_paraphrase',\n",
    "    'oracle': 'prefix_oracle',\n",
    "    'llm_extract': 'prefix_llm_extract',\n",
    "    'llm_question': 'prefix_llm_question',\n",
    "    'llm_summarize': 'prefix_llm_summarize',\n",
    "    'extractor_matched': 'prefix_extractor_matched',\n",
    "    'adversarial_matched': 'prefix_adversarial_matched',\n",
    "}\n",
    "\n",
    "PREFIX_KEYS = list(COND_PREFIX_MAP.values())\n",
    "\n",
    "# Fixed-text prefixes\n",
    "EXTRACTOR_TEXT = \"Extract all key data points, facts, entities, and specific attributes from the following text.\"\n",
    "ADVERSARIAL_TEXT = \"The recipe calls for two cups of flour, one cup of sugar, and a pinch of salt mixed together.\"\n",
    "\n",
    "SCORING_KEY = 'bos_retained_token_matched_v03'\n",
    "\n",
    "print(f\"\\nSetup complete. Functions defined: score, generate_text, make_prefix\")\n",
    "print(f\"Conditions: {len(COND_NAMES)} ({len(COND_PREFIX_MAP)} prefixed + bare)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21ace9e4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-22T13:01:08.484421Z",
     "iopub.status.busy": "2026-02-22T13:01:08.483514Z",
     "iopub.status.idle": "2026-02-22T13:01:18.508550Z",
     "shell.execute_reply": "2026-02-22T13:01:18.507824Z"
    },
    "papermill": {
     "duration": 10.031699,
     "end_time": "2026-02-22T13:01:18.510184",
     "exception": false,
     "start_time": "2026-02-22T13:01:08.478485",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Loading SQuAD 2.0 validation...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SQuAD 2.0 candidates: 5906\n",
      "\n",
      "Loading TriviaQA rc.wikipedia validation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00709842e9124af1a27d81d7dcdca455",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TriviaQA candidates: 5407\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading HotpotQA distractor validation...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HotpotQA candidates: 7004\n",
      "\n",
      "======================================================================\n",
      "Dataset loading summary:\n",
      "\n",
      "  squad_v2: 400 samples\n",
      "    Mean passage words: 128\n",
      "    Mean answer words: 3\n",
      "    Mean query words: 10\n",
      "    Example query: What percentage of Victorians are Christian?...\n",
      "    Example answer: 61.1%...\n",
      "\n",
      "  triviaqa: 400 samples\n",
      "    Mean passage words: 482\n",
      "    Mean answer words: 2\n",
      "    Mean query words: 14\n",
      "    Example query: Which Rugby League team plays home games at Derwent Park?...\n",
      "    Example answer: WORKINGTON TOWN...\n",
      "\n",
      "  hotpotqa: 400 samples\n",
      "    Mean passage words: 60\n",
      "    Mean answer words: 2\n",
      "    Mean query words: 16\n",
      "    Example query: Are Jeff Ragsdale and Millard Webb both authors?...\n",
      "    Example answer: no...\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Load SQuAD 2.0, TriviaQA, HotpotQA (MS MARCO reused from Exp 02)\n",
    "from datasets import load_dataset\n",
    "\n",
    "all_samples = {}  # ds_name -> list of 400 sample dicts\n",
    "\n",
    "# Per-dataset seeds for reproducible sampling\n",
    "DS_SEEDS = {\n",
    "    'squad_v2': SEED + 100,\n",
    "    'triviaqa': SEED + 200,\n",
    "    'hotpotqa': SEED + 300,\n",
    "}\n",
    "\n",
    "# ---- SQuAD 2.0 ----\n",
    "print(\"=\" * 70)\n",
    "print(\"Loading SQuAD 2.0 validation...\")\n",
    "ds_squad = load_dataset(\"rajpurkar/squad_v2\", split=\"validation\")\n",
    "\n",
    "squad_candidates = []\n",
    "for item in ds_squad:\n",
    "    answers = item.get('answers', {})\n",
    "    answer_texts = answers.get('text', [])\n",
    "    if not answer_texts:\n",
    "        continue\n",
    "    passage = item['context']\n",
    "    query = item['question']\n",
    "    answer = answer_texts[0]\n",
    "    wc = count_words(passage)\n",
    "    if 30 <= wc <= 500 and count_words(answer) >= 1:\n",
    "        squad_candidates.append({\n",
    "            'passage': passage, 'query': query, 'answer': answer,\n",
    "            'word_count': wc,\n",
    "        })\n",
    "\n",
    "print(f\"SQuAD 2.0 candidates: {len(squad_candidates)}\")\n",
    "np.random.seed(DS_SEEDS['squad_v2'])\n",
    "indices = np.random.permutation(len(squad_candidates))[:N_SAMPLES]\n",
    "all_samples['squad_v2'] = [squad_candidates[i] for i in indices]\n",
    "del ds_squad, squad_candidates\n",
    "gc.collect()\n",
    "\n",
    "# ---- TriviaQA ----\n",
    "print(\"\\nLoading TriviaQA rc.wikipedia validation...\")\n",
    "ds_trivia = load_dataset(\"mandarjoshi/trivia_qa\", \"rc.wikipedia\", split=\"validation\")\n",
    "\n",
    "trivia_candidates = []\n",
    "for item in ds_trivia:\n",
    "    entity_pages = item.get('entity_pages', {})\n",
    "    wiki_contexts = entity_pages.get('wiki_context', [])\n",
    "    if not wiki_contexts or not wiki_contexts[0]:\n",
    "        continue\n",
    "    # Take first 500 words of first wiki context\n",
    "    words = wiki_contexts[0].split()[:500]\n",
    "    passage = ' '.join(words)\n",
    "    query = item['question']\n",
    "    answer_val = item['answer']['value']\n",
    "    aliases = item['answer'].get('aliases', [])\n",
    "\n",
    "    # Check if answer or any alias appears in passage (case-insensitive)\n",
    "    passage_lower = passage.lower()\n",
    "    found = answer_val.lower() in passage_lower\n",
    "    if not found:\n",
    "        for alias in aliases:\n",
    "            if alias.lower() in passage_lower:\n",
    "                found = True\n",
    "                break\n",
    "    if not found:\n",
    "        continue\n",
    "\n",
    "    wc = count_words(passage)\n",
    "    if 30 <= wc <= 500 and count_words(answer_val) >= 1:\n",
    "        trivia_candidates.append({\n",
    "            'passage': passage, 'query': query, 'answer': answer_val,\n",
    "            'word_count': wc,\n",
    "        })\n",
    "\n",
    "print(f\"TriviaQA candidates: {len(trivia_candidates)}\")\n",
    "np.random.seed(DS_SEEDS['triviaqa'])\n",
    "indices = np.random.permutation(len(trivia_candidates))[:N_SAMPLES]\n",
    "all_samples['triviaqa'] = [trivia_candidates[i] for i in indices]\n",
    "del ds_trivia, trivia_candidates\n",
    "gc.collect()\n",
    "\n",
    "# ---- HotpotQA ----\n",
    "print(\"\\nLoading HotpotQA distractor validation...\")\n",
    "ds_hotpot = load_dataset(\"hotpotqa/hotpot_qa\", \"distractor\", split=\"validation\")\n",
    "\n",
    "hotpot_candidates = []\n",
    "for item in ds_hotpot:\n",
    "    context = item.get('context', {})\n",
    "    sf = item.get('supporting_facts', {})\n",
    "    ctx_titles = context.get('title', [])\n",
    "    ctx_sentences = context.get('sentences', [])\n",
    "    sf_titles = sf.get('title', [])\n",
    "    sf_sent_ids = sf.get('sent_id', [])\n",
    "\n",
    "    # Build title -> sentences mapping\n",
    "    title_to_sents = {}\n",
    "    for title, sents in zip(ctx_titles, ctx_sentences):\n",
    "        title_to_sents[title] = sents\n",
    "\n",
    "    # Extract supporting fact sentences\n",
    "    passage_parts = []\n",
    "    for title, sid in zip(sf_titles, sf_sent_ids):\n",
    "        if title in title_to_sents and sid < len(title_to_sents[title]):\n",
    "            passage_parts.append(title_to_sents[title][sid])\n",
    "\n",
    "    if not passage_parts:\n",
    "        continue\n",
    "    passage = ' '.join(passage_parts)\n",
    "    query = item['question']\n",
    "    answer = item['answer']\n",
    "    wc = count_words(passage)\n",
    "    if 30 <= wc <= 500 and count_words(answer) >= 1:\n",
    "        hotpot_candidates.append({\n",
    "            'passage': passage, 'query': query, 'answer': answer,\n",
    "            'word_count': wc,\n",
    "        })\n",
    "\n",
    "print(f\"HotpotQA candidates: {len(hotpot_candidates)}\")\n",
    "np.random.seed(DS_SEEDS['hotpotqa'])\n",
    "indices = np.random.permutation(len(hotpot_candidates))[:N_SAMPLES]\n",
    "all_samples['hotpotqa'] = [hotpot_candidates[i] for i in indices]\n",
    "del ds_hotpot, hotpot_candidates\n",
    "gc.collect()\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Dataset loading summary:\")\n",
    "for ds_name in NEW_DATASETS:\n",
    "    samps = all_samples[ds_name]\n",
    "    print(f\"\\n  {ds_name}: {len(samps)} samples\")\n",
    "    print(f\"    Mean passage words: {np.mean([s['word_count'] for s in samps]):.0f}\")\n",
    "    print(f\"    Mean answer words: {np.mean([count_words(s['answer']) for s in samps]):.0f}\")\n",
    "    print(f\"    Mean query words: {np.mean([count_words(s['query']) for s in samps]):.0f}\")\n",
    "    print(f\"    Example query: {samps[0]['query'][:70]}...\")\n",
    "    print(f\"    Example answer: {samps[0]['answer'][:70]}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2f3e953",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-22T13:01:18.519805Z",
     "iopub.status.busy": "2026-02-22T13:01:18.519305Z",
     "iopub.status.idle": "2026-02-22T13:01:18.742394Z",
     "shell.execute_reply": "2026-02-22T13:01:18.741706Z"
    },
    "papermill": {
     "duration": 0.229746,
     "end_time": "2026-02-22T13:01:18.743875",
     "exception": false,
     "start_time": "2026-02-22T13:01:18.514129",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Loading MS MARCO results from Exp 02\n",
      "======================================================================\n",
      "MS MARCO: 400 total, selecting top 40% = 160 hard samples\n",
      "Bare NLL range: 0.0113 - 12.3487\n",
      "Hard cutoff (min NLL in hard set): 1.1876\n",
      "Hard samples mean bare NLL: 2.8848\n",
      "\n",
      "MS MARCO hard samples loaded:\n",
      "  N_hard: 160\n",
      "  Mean passage words: 72\n",
      "  Mean query tokens: 7\n",
      "  Conditions: 13\n",
      "\n",
      "  Condition                     NLL  d vs bare  sem delta d\n",
      "  ----------------------------------------------------------\n",
      "  bare                       2.8848         --           --\n",
      "  random_tokens              2.7599     +0.137        (ref)\n",
      "  repeat_token               2.6911     +0.221       +0.098\n",
      "  scrambled_oracle           2.8638     +0.025       -0.112\n",
      "  unrelated_query            2.8547     +0.026       -0.084\n",
      "  same_topic                 2.8226     +0.064       -0.058\n",
      "  paraphrase                 2.7221     +0.162       +0.033\n",
      "  oracle                     2.9474     -0.051       -0.141\n",
      "  llm_extract                2.7340     +0.200       +0.028\n",
      "  llm_question               2.8898     -0.005       -0.124\n",
      "  llm_summarize              2.6554     +0.265       +0.106\n",
      "  extractor_matched          2.6315     +0.194       +0.098\n",
      "  adversarial_matched        2.7214     +0.152       +0.036\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 4: Reuse MS MARCO results from Exp 02\n",
    "print(\"=\" * 70)\n",
    "print(\"Loading MS MARCO results from Exp 02\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "assert EXP02_DIR.exists(), f\"Exp 02 results not found at {EXP02_DIR}\"\n",
    "exp02_ckpt = json.loads((EXP02_DIR / \"checkpoint.json\").read_text())\n",
    "assert exp02_ckpt.get('scoring') == 'bos_retained_token_matched_v02', \\\n",
    "    f\"Unexpected scoring key: {exp02_ckpt.get('scoring')}\"\n",
    "exp02_results = exp02_ckpt['results']\n",
    "assert len(exp02_results) == N_SAMPLES, \\\n",
    "    f\"Expected {N_SAMPLES} results, got {len(exp02_results)}\"\n",
    "\n",
    "# Extract bare NLLs and select hard 40%\n",
    "msmarco_bare = np.array([r['nll_bare'] for r in exp02_results])\n",
    "N_HARD = int(N_SAMPLES * HARD_FRAC)\n",
    "sorted_idx = np.argsort(msmarco_bare)[::-1]  # descending (hardest first)\n",
    "msmarco_hard_idx = np.sort(sorted_idx[:N_HARD])  # restore original order\n",
    "\n",
    "print(f\"MS MARCO: {N_SAMPLES} total, selecting top {HARD_FRAC*100:.0f}% = {N_HARD} hard samples\")\n",
    "print(f\"Bare NLL range: {msmarco_bare.min():.4f} - {msmarco_bare.max():.4f}\")\n",
    "print(f\"Hard cutoff (min NLL in hard set): {msmarco_bare[msmarco_hard_idx].min():.4f}\")\n",
    "print(f\"Hard samples mean bare NLL: {msmarco_bare[msmarco_hard_idx].mean():.4f}\")\n",
    "\n",
    "# Build hard_nlls for MS MARCO\n",
    "hard_nlls = {}  # ds_name -> {cond_name: np.array}\n",
    "hard_metadata = {}  # ds_name -> {n_total, n_hard, ...}\n",
    "\n",
    "hard_nlls['ms_marco'] = {}\n",
    "for cond in COND_NAMES:\n",
    "    arr = np.array([exp02_results[i][f'nll_{cond}'] for i in msmarco_hard_idx])\n",
    "    hard_nlls['ms_marco'][cond] = arr\n",
    "\n",
    "hard_metadata['ms_marco'] = {\n",
    "    'n_total': N_SAMPLES,\n",
    "    'n_hard': N_HARD,\n",
    "    'source': 'exp02_reuse',\n",
    "    'mean_passage_words': float(np.mean([exp02_results[i]['passage_words']\n",
    "                                          for i in msmarco_hard_idx])),\n",
    "    'mean_query_tokens': float(np.mean([exp02_results[i]['Q']\n",
    "                                         for i in msmarco_hard_idx])),\n",
    "    'mean_answer_words': float(np.mean([count_words(exp02_results[i]['answer'])\n",
    "                                         for i in msmarco_hard_idx])),\n",
    "}\n",
    "\n",
    "# Quick sanity check: bare should match\n",
    "assert np.allclose(hard_nlls['ms_marco']['bare'],\n",
    "                   msmarco_bare[msmarco_hard_idx]), \"Bare NLL mismatch\"\n",
    "\n",
    "print(f\"\\nMS MARCO hard samples loaded:\")\n",
    "print(f\"  N_hard: {N_HARD}\")\n",
    "print(f\"  Mean passage words: {hard_metadata['ms_marco']['mean_passage_words']:.0f}\")\n",
    "print(f\"  Mean query tokens: {hard_metadata['ms_marco']['mean_query_tokens']:.0f}\")\n",
    "print(f\"  Conditions: {len(COND_NAMES)}\")\n",
    "\n",
    "# Show condition summary for MS MARCO hard set\n",
    "bare_h = hard_nlls['ms_marco']['bare']\n",
    "print(f\"\\n  {'Condition':<24} {'NLL':>8} {'d vs bare':>10} {'sem delta d':>12}\")\n",
    "print(f\"  {'-'*58}\")\n",
    "for cond in COND_NAMES:\n",
    "    nlls_h = hard_nlls['ms_marco'][cond]\n",
    "    mean_nll = nlls_h.mean()\n",
    "    if cond == 'bare':\n",
    "        print(f\"  {cond:<24} {mean_nll:>8.4f} {'--':>10} {'--':>12}\")\n",
    "    else:\n",
    "        d_bare = cohens_d(bare_h - nlls_h)\n",
    "        if cond == 'random_tokens':\n",
    "            print(f\"  {cond:<24} {mean_nll:>8.4f} {d_bare:>+10.3f} {'(ref)':>12}\")\n",
    "        else:\n",
    "            sem_delta = hard_nlls['ms_marco']['random_tokens'] - nlls_h\n",
    "            d_sem = cohens_d(sem_delta)\n",
    "            print(f\"  {cond:<24} {mean_nll:>8.4f} {d_bare:>+10.3f} {d_sem:>+12.3f}\")\n",
    "\n",
    "del exp02_ckpt, exp02_results\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10cce544",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-22T13:01:18.753595Z",
     "iopub.status.busy": "2026-02-22T13:01:18.753241Z",
     "iopub.status.idle": "2026-02-22T13:01:18.972757Z",
     "shell.execute_reply": "2026-02-22T13:01:18.972005Z"
    },
    "papermill": {
     "duration": 0.226549,
     "end_time": "2026-02-22T13:01:18.974415",
     "exception": false,
     "start_time": "2026-02-22T13:01:18.747866",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "BARE SCORING — 3 new datasets x 400 samples\n",
      "======================================================================\n",
      "\n",
      "--- squad_v2 (400 samples) ---\n",
      "  Resuming from checkpoint: 400/400\n",
      "  Hard cutoff: 2.0078\n",
      "  Hard mean bare NLL: 5.2735\n",
      "  Easy mean bare NLL: 0.8615\n",
      "\n",
      "--- triviaqa (400 samples) ---\n",
      "  Resuming from checkpoint: 400/400\n",
      "  Hard cutoff: 2.4382\n",
      "  Hard mean bare NLL: 4.7965\n",
      "  Easy mean bare NLL: 0.7739\n",
      "\n",
      "--- hotpotqa (400 samples) ---\n",
      "  Resuming from checkpoint: 400/400\n",
      "  Hard cutoff: 1.3281\n",
      "  Hard mean bare NLL: 4.4105\n",
      "  Easy mean bare NLL: 0.4266\n",
      "\n",
      "======================================================================\n",
      "Hard sample selection complete:\n",
      "  squad_v2: 160 hard samples (mean bare NLL: 5.2735)\n",
      "  triviaqa: 160 hard samples (mean bare NLL: 4.7965)\n",
      "  hotpotqa: 160 hard samples (mean bare NLL: 4.4105)\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Bare NLL scoring for SQuAD, TriviaQA, HotpotQA — select hard 40%\n",
    "print(\"=\" * 70)\n",
    "print(\"BARE SCORING — 3 new datasets x 400 samples\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "hard_indices = {}   # ds_name -> np.array of indices into all_samples\n",
    "hard_samples = {}   # ds_name -> list of hard sample dicts\n",
    "\n",
    "for ds_name in NEW_DATASETS:\n",
    "    print(f\"\\n--- {ds_name} ({N_SAMPLES} samples) ---\")\n",
    "    samples = all_samples[ds_name]\n",
    "    bare_ckpt_path = RESULTS_DIR / f\"bare_{ds_name}.json\"\n",
    "\n",
    "    bare_nlls = []\n",
    "    start_idx = 0\n",
    "\n",
    "    # Try to resume from checkpoint\n",
    "    if bare_ckpt_path.exists():\n",
    "        ckpt = json.loads(bare_ckpt_path.read_text())\n",
    "        if (ckpt.get('n_total') == N_SAMPLES and\n",
    "            ckpt.get('scoring') == SCORING_KEY and\n",
    "            ckpt.get('dataset') == ds_name):\n",
    "            saved_queries = ckpt.get('queries_first50', [])\n",
    "            current_queries = [s['query'][:50] for s in samples[:len(saved_queries)]]\n",
    "            if saved_queries == current_queries:\n",
    "                bare_nlls = ckpt['bare_nlls']\n",
    "                start_idx = len(bare_nlls)\n",
    "                print(f\"  Resuming from checkpoint: {start_idx}/{N_SAMPLES}\")\n",
    "\n",
    "    if start_idx < N_SAMPLES:\n",
    "        t0 = time.time()\n",
    "        for i in tqdm(range(start_idx, N_SAMPLES), initial=start_idx,\n",
    "                      total=N_SAMPLES, desc=f\"Bare {ds_name}\"):\n",
    "            s = samples[i]\n",
    "            nll = score(s['passage'], s['query'], s['answer'])\n",
    "            bare_nlls.append(nll)\n",
    "\n",
    "            if (i + 1) % 20 == 0 or i == N_SAMPLES - 1:\n",
    "                ckpt = {\n",
    "                    'dataset': ds_name,\n",
    "                    'n_total': N_SAMPLES,\n",
    "                    'scoring': SCORING_KEY,\n",
    "                    'bare_nlls': bare_nlls,\n",
    "                    'queries_first50': [s['query'][:50]\n",
    "                                        for s in samples[:len(bare_nlls)]],\n",
    "                    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                }\n",
    "                bare_ckpt_path.write_text(json.dumps(ckpt))\n",
    "                elapsed = time.time() - t0\n",
    "                done = i - start_idx + 1\n",
    "                eta = (N_SAMPLES - i - 1) * elapsed / done if done > 0 else 0\n",
    "                tqdm.write(f\"  Checkpoint {i+1}/{N_SAMPLES} | \"\n",
    "                           f\"{elapsed/60:.1f}m | ETA {eta/60:.1f}m\")\n",
    "\n",
    "        elapsed = time.time() - t0\n",
    "        print(f\"  Bare scoring complete in {elapsed/60:.1f} min\")\n",
    "\n",
    "    bare_arr = np.array(bare_nlls)\n",
    "\n",
    "    # Select hard 40%\n",
    "    N_HARD = int(N_SAMPLES * HARD_FRAC)\n",
    "    sorted_idx = np.argsort(bare_arr)[::-1]\n",
    "    h_idx = np.sort(sorted_idx[:N_HARD])\n",
    "    hard_indices[ds_name] = h_idx\n",
    "\n",
    "    # Build hard_samples with bare NLL attached\n",
    "    hs = []\n",
    "    for idx in h_idx:\n",
    "        s = dict(samples[idx])  # copy\n",
    "        s['nll_bare'] = bare_arr[idx]\n",
    "        s['original_idx'] = int(idx)\n",
    "        hs.append(s)\n",
    "    hard_samples[ds_name] = hs\n",
    "\n",
    "    # Initialize hard_nlls with bare\n",
    "    hard_nlls[ds_name] = {'bare': bare_arr[h_idx]}\n",
    "\n",
    "    hard_metadata[ds_name] = {\n",
    "        'n_total': N_SAMPLES,\n",
    "        'n_hard': N_HARD,\n",
    "        'source': 'scored',\n",
    "        'mean_passage_words': float(np.mean([s['word_count'] for s in hs])),\n",
    "        'mean_query_tokens': 0.0,  # filled in after tokenization\n",
    "        'mean_answer_words': float(np.mean([count_words(s['answer']) for s in hs])),\n",
    "    }\n",
    "\n",
    "    print(f\"  Hard cutoff: {bare_arr[h_idx].min():.4f}\")\n",
    "    print(f\"  Hard mean bare NLL: {bare_arr[h_idx].mean():.4f}\")\n",
    "    print(f\"  Easy mean bare NLL: {np.delete(bare_arr, h_idx).mean():.4f}\")\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Hard sample selection complete:\")\n",
    "for ds_name in NEW_DATASETS:\n",
    "    n_h = len(hard_samples[ds_name])\n",
    "    print(f\"  {ds_name}: {n_h} hard samples (mean bare NLL: \"\n",
    "          f\"{hard_nlls[ds_name]['bare'].mean():.4f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c16828ec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-22T13:01:18.984581Z",
     "iopub.status.busy": "2026-02-22T13:01:18.984131Z",
     "iopub.status.idle": "2026-02-22T13:01:19.201313Z",
     "shell.execute_reply": "2026-02-22T13:01:19.200581Z"
    },
    "papermill": {
     "duration": 0.224871,
     "end_time": "2026-02-22T13:01:19.203442",
     "exception": false,
     "start_time": "2026-02-22T13:01:18.978571",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "LLM SURROGATE GENERATION — 3 datasets x ~160 hard samples x 5 surrogates\n",
      "======================================================================\n",
      "\n",
      "--- squad_v2 (160 hard samples) ---\n",
      "  Resuming from 160/160\n",
      "  Loaded 160 cached surrogates\n",
      "\n",
      "  Sample 0: query='What else was used by pharmas?'\n",
      "    paraphrase     : Alternative ingredients employed by pharmaceutical\n",
      "    same_topic     : What services did early pharmas provide besides me\n",
      "    llm_extract    : Pharmacy derived from \"pharma,\" root used since 15\n",
      "\n",
      "  Sample 1: query='What does the utilitarian principle seek for the g'\n",
      "    paraphrase     : Maximize happiness for the most individuals.\n",
      "    same_topic     : How does inequality impact overall societal happin\n",
      "    llm_extract    : Economic inequality is problematic, distributive e\n",
      "\n",
      "--- triviaqa (160 hard samples) ---\n",
      "  Resuming from 160/160\n",
      "  Loaded 160 cached surrogates\n",
      "\n",
      "  Sample 0: query='Which Rugby League team plays home games at Derwen'\n",
      "    paraphrase     : Which club uses Derwent Park for matches?\n",
      "    same_topic     : When were floodlights first used at the stadium?\n",
      "    llm_extract    : Workington, England, Cumbrian River Derwent, rugby\n",
      "\n",
      "  Sample 1: query='In which present day African nation are the Amhara'\n",
      "    paraphrase     : Where do Amhara and Oromo people reside now, Afric\n",
      "    same_topic     : What languages are related to Amharic?\n",
      "    llm_extract    : Ethnic group inhabiting northern/central Ethiopia,\n",
      "\n",
      "--- hotpotqa (160 hard samples) ---\n",
      "  Resuming from 160/160\n",
      "  Loaded 160 cached surrogates\n",
      "\n",
      "  Sample 0: query='Are Jeff Ragsdale and Millard Webb both authors?'\n",
      "    paraphrase     : Did Ragsdale and Webb write books?\n",
      "    same_topic     : What were Millard Webb's birth and death dates?\n",
      "    llm_extract    : Jeff Ragsdale is an American author, documentary f\n",
      "\n",
      "  Sample 1: query='Why did the Japanese occupy the westernmost and la'\n",
      "    paraphrase     : Japanese control of Hokkaido: reasons and history?\n",
      "    same_topic     : Why did Japan target the Aleutian Islands?\n",
      "    llm_extract    : Japanese occupied Kiska and Attu Islands, protecti\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Generate LLM surrogates for hard samples of 3 new datasets\n",
    "print(\"=\" * 70)\n",
    "print(\"LLM SURROGATE GENERATION — 3 datasets x ~160 hard samples x 5 surrogates\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "surrogates_all = {}  # ds_name -> list of surrogates for hard samples\n",
    "\n",
    "for ds_name in NEW_DATASETS:\n",
    "    hs = hard_samples[ds_name]\n",
    "    n_hard = len(hs)\n",
    "    surr_path = RESULTS_DIR / f\"surrogates_{ds_name}.json\"\n",
    "\n",
    "    print(f\"\\n--- {ds_name} ({n_hard} hard samples) ---\")\n",
    "\n",
    "    surrogates = []\n",
    "    start_idx = 0\n",
    "\n",
    "    if surr_path.exists():\n",
    "        surr_ckpt = json.loads(surr_path.read_text())\n",
    "        if (surr_ckpt.get('dataset') == ds_name and\n",
    "            surr_ckpt.get('n_hard') == n_hard):\n",
    "            saved_queries = [s.get('query', '')[:50]\n",
    "                             for s in surr_ckpt.get('surrogates', [])]\n",
    "            current_queries = [s['query'][:50] for s in hs[:len(saved_queries)]]\n",
    "            if saved_queries == current_queries:\n",
    "                surrogates = surr_ckpt['surrogates']\n",
    "                start_idx = len(surrogates)\n",
    "                print(f\"  Resuming from {start_idx}/{n_hard}\")\n",
    "\n",
    "    if start_idx < n_hard:\n",
    "        t0 = time.time()\n",
    "        ds_seed_offset = DS_SEEDS[ds_name]\n",
    "\n",
    "        for i in tqdm(range(start_idx, n_hard), initial=start_idx,\n",
    "                      total=n_hard, desc=f\"Gen {ds_name}\"):\n",
    "            s = hs[i]\n",
    "            entry = {'query': s['query']}\n",
    "\n",
    "            doc_words = s['passage'].split()[:200]\n",
    "            doc_input = f\"Document:\\n{' '.join(doc_words)}\"\n",
    "\n",
    "            torch.manual_seed(ds_seed_offset + i * 10)\n",
    "            entry['paraphrase'] = generate_text(\n",
    "                f\"Query: {s['query']}\", PROMPT_PARAPHRASE\n",
    "            )\n",
    "\n",
    "            torch.manual_seed(ds_seed_offset + i * 10 + 1)\n",
    "            entry['same_topic'] = generate_text(doc_input, PROMPT_SAME_TOPIC)\n",
    "\n",
    "            torch.manual_seed(ds_seed_offset + i * 10 + 2)\n",
    "            entry['llm_extract'] = generate_text(doc_input, PROMPT_EXTRACT)\n",
    "\n",
    "            torch.manual_seed(ds_seed_offset + i * 10 + 3)\n",
    "            entry['llm_question'] = generate_text(doc_input, PROMPT_QUESTION)\n",
    "\n",
    "            torch.manual_seed(ds_seed_offset + i * 10 + 4)\n",
    "            entry['llm_summarize'] = generate_text(doc_input, PROMPT_SUMMARIZE)\n",
    "\n",
    "            surrogates.append(entry)\n",
    "\n",
    "            if (i + 1) % 20 == 0 or i == n_hard - 1:\n",
    "                surr_ckpt = {\n",
    "                    'dataset': ds_name,\n",
    "                    'n_hard': n_hard,\n",
    "                    'surrogates': surrogates,\n",
    "                    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                }\n",
    "                surr_path.write_text(json.dumps(surr_ckpt))\n",
    "                elapsed = time.time() - t0\n",
    "                done = i - start_idx + 1\n",
    "                eta = (n_hard - i - 1) * elapsed / done if done > 0 else 0\n",
    "                tqdm.write(f\"  Gen checkpoint {i+1}/{n_hard} | \"\n",
    "                           f\"{elapsed/60:.1f}m | ETA {eta/60:.1f}m\")\n",
    "\n",
    "        elapsed = time.time() - t0\n",
    "        print(f\"  Generation complete in {elapsed/60:.1f} min\")\n",
    "    else:\n",
    "        print(f\"  Loaded {len(surrogates)} cached surrogates\")\n",
    "\n",
    "    surrogates_all[ds_name] = surrogates\n",
    "\n",
    "    # Show examples\n",
    "    for j in range(min(2, n_hard)):\n",
    "        print(f\"\\n  Sample {j}: query='{surrogates[j]['query'][:50]}'\")\n",
    "        for key in ['paraphrase', 'same_topic', 'llm_extract']:\n",
    "            print(f\"    {key:<15}: {surrogates[j].get(key, 'N/A')[:50]}\")\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f1b5fed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-22T13:01:19.217596Z",
     "iopub.status.busy": "2026-02-22T13:01:19.217254Z",
     "iopub.status.idle": "2026-02-22T13:01:43.261464Z",
     "shell.execute_reply": "2026-02-22T13:01:43.260714Z"
    },
    "papermill": {
     "duration": 24.053002,
     "end_time": "2026-02-22T13:01:43.263111",
     "exception": false,
     "start_time": "2026-02-22T13:01:19.210109",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PREFIX CONSTRUCTION + VALIDATION\n",
      "======================================================================\n",
      "\n",
      "--- squad_v2 (160 hard samples) ---\n",
      "  Q tokens — mean: 12.8, median: 12, min: 5, max: 33\n",
      "  All 12 prefix types verified for 160 samples\n",
      "\n",
      "--- triviaqa (160 hard samples) ---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Q tokens — mean: 17.5, median: 16, min: 6, max: 60\n",
      "  All 12 prefix types verified for 160 samples\n",
      "\n",
      "--- hotpotqa (160 hard samples) ---\n",
      "  Q tokens — mean: 19.2, median: 19, min: 9, max: 48\n",
      "  All 12 prefix types verified for 160 samples\n",
      "\n",
      "======================================================================\n",
      "VALIDATION TESTS\n",
      "======================================================================\n",
      "\n",
      "--- Test 1: Bare two-phase matches single-pass ---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Single-pass NLL: 1.942079\n",
      "  Two-phase bare:  1.945658 (diff: 0.18%)\n",
      "  PASSED\n",
      "\n",
      "--- Test 2: Prefixed scoring runs correctly ---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [squad_v2] Bare:          3.4452\n",
      "  [squad_v2] Oracle:        4.0485  delta=-0.6033\n",
      "  [squad_v2] Random tokens: 2.6113  delta=+0.8339\n",
      "  PASSED\n",
      "\n",
      "--- Test 3: Token-matching invariant ---\n",
      "  All 12 prefixed conditions have Q=8 tokens\n",
      "  PASSED\n",
      "\n",
      "ALL VALIDATION TESTS PASSED\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Build per-sample token-level prefix IDs for hard samples + validation\n",
    "print(\"=\" * 70)\n",
    "print(\"PREFIX CONSTRUCTION + VALIDATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Pre-tokenize fixed texts\n",
    "extractor_ids = tokenizer(EXTRACTOR_TEXT, add_special_tokens=False).input_ids\n",
    "adversarial_ids = tokenizer(ADVERSARIAL_TEXT, add_special_tokens=False).input_ids\n",
    "special_ids = set(tokenizer.all_special_ids)\n",
    "\n",
    "for ds_name in NEW_DATASETS:\n",
    "    hs = hard_samples[ds_name]\n",
    "    surrs = surrogates_all[ds_name]\n",
    "    n_hard = len(hs)\n",
    "\n",
    "    print(f\"\\n--- {ds_name} ({n_hard} hard samples) ---\")\n",
    "\n",
    "    pyrandom.seed(DS_SEEDS[ds_name] + 200)\n",
    "    np.random.seed(DS_SEEDS[ds_name] + 300)\n",
    "\n",
    "    for i, s in enumerate(hs):\n",
    "        surr = surrs[i]\n",
    "        q_ids = tokenizer(s['query'], add_special_tokens=False).input_ids\n",
    "        Q = len(q_ids)\n",
    "        s['Q'] = Q\n",
    "\n",
    "        # 1. oracle: exact query token IDs\n",
    "        s['prefix_oracle'] = q_ids\n",
    "\n",
    "        # 2. random_tokens: random vocab IDs (excluding special)\n",
    "        rand_ids = []\n",
    "        while len(rand_ids) < Q:\n",
    "            tid = np.random.randint(0, VOCAB_SIZE)\n",
    "            if tid not in special_ids:\n",
    "                rand_ids.append(int(tid))\n",
    "        s['prefix_random_tokens'] = rand_ids[:Q]\n",
    "\n",
    "        # 3. repeat_token\n",
    "        s['prefix_repeat_token'] = [1000] * Q\n",
    "\n",
    "        # 4. scrambled_oracle\n",
    "        shuffled = list(q_ids)\n",
    "        pyrandom.shuffle(shuffled)\n",
    "        s['prefix_scrambled_oracle'] = shuffled\n",
    "\n",
    "        # 5. unrelated_query: other hard sample's query\n",
    "        other_idx = (i + n_hard // 2) % n_hard\n",
    "        other_q_ids = tokenizer(hs[other_idx]['query'],\n",
    "                                add_special_tokens=False).input_ids\n",
    "        s['prefix_unrelated_query'] = make_prefix(other_q_ids, Q)\n",
    "\n",
    "        # 6. same_topic\n",
    "        topic_ids = tokenizer(surr['same_topic'],\n",
    "                              add_special_tokens=False).input_ids\n",
    "        s['prefix_same_topic'] = make_prefix(topic_ids, Q)\n",
    "\n",
    "        # 7. paraphrase\n",
    "        para_ids = tokenizer(surr['paraphrase'],\n",
    "                             add_special_tokens=False).input_ids\n",
    "        s['prefix_paraphrase'] = make_prefix(para_ids, Q)\n",
    "\n",
    "        # 8. llm_extract\n",
    "        extract_ids = tokenizer(surr['llm_extract'],\n",
    "                                add_special_tokens=False).input_ids\n",
    "        s['prefix_llm_extract'] = make_prefix(extract_ids, Q)\n",
    "\n",
    "        # 9. llm_question\n",
    "        question_ids = tokenizer(surr['llm_question'],\n",
    "                                 add_special_tokens=False).input_ids\n",
    "        s['prefix_llm_question'] = make_prefix(question_ids, Q)\n",
    "\n",
    "        # 10. llm_summarize\n",
    "        summarize_ids = tokenizer(surr['llm_summarize'],\n",
    "                                  add_special_tokens=False).input_ids\n",
    "        s['prefix_llm_summarize'] = make_prefix(summarize_ids, Q)\n",
    "\n",
    "        # 11. extractor_matched\n",
    "        s['prefix_extractor_matched'] = make_prefix(extractor_ids, Q)\n",
    "\n",
    "        # 12. adversarial_matched\n",
    "        s['prefix_adversarial_matched'] = make_prefix(adversarial_ids, Q)\n",
    "\n",
    "    # Update metadata with query token stats\n",
    "    q_lens = [s['Q'] for s in hs]\n",
    "    hard_metadata[ds_name]['mean_query_tokens'] = float(np.mean(q_lens))\n",
    "    print(f\"  Q tokens — mean: {np.mean(q_lens):.1f}, \"\n",
    "          f\"median: {np.median(q_lens):.0f}, \"\n",
    "          f\"min: {np.min(q_lens)}, max: {np.max(q_lens)}\")\n",
    "\n",
    "    # Verify all prefixes have exactly Q tokens\n",
    "    errors = 0\n",
    "    for i, s in enumerate(hs):\n",
    "        Q = s['Q']\n",
    "        for key in PREFIX_KEYS:\n",
    "            if len(s[key]) != Q:\n",
    "                print(f\"  ERROR: Sample {i} {key}: len={len(s[key])} != Q={Q}\")\n",
    "                errors += 1\n",
    "    assert errors == 0, f\"{ds_name}: {errors} prefix length mismatches!\"\n",
    "    print(f\"  All {len(PREFIX_KEYS)} prefix types verified for {n_hard} samples\")\n",
    "\n",
    "# ================================================================\n",
    "# VALIDATION TESTS\n",
    "# ================================================================\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"VALIDATION TESTS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Test 1: Bare two-phase matches single-pass\n",
    "print(\"\\n--- Test 1: Bare two-phase matches single-pass ---\")\n",
    "doc_text_t = \"The cat sat on the mat near the door of the house by the lake\"\n",
    "query_text_t = \"Where did the cat sit?\"\n",
    "answer_text_t = \"on the mat\"\n",
    "doc_ids_t = tokenizer(doc_text_t, add_special_tokens=False).input_ids\n",
    "D_t = len(doc_ids_t)\n",
    "query_ids_t = tokenizer(\"\\n\" + query_text_t + \"\\n\", add_special_tokens=False).input_ids\n",
    "answer_ids_t = tokenizer(answer_text_t, add_special_tokens=False).input_ids\n",
    "\n",
    "full_ids = [BOS_ID] + doc_ids_t + query_ids_t + answer_ids_t\n",
    "with torch.no_grad():\n",
    "    out_full = model(input_ids=torch.tensor([full_ids], device=DEVICE))\n",
    "n_ctx = 1 + D_t + len(query_ids_t)\n",
    "logits_full = out_full.logits[0, n_ctx - 1:n_ctx - 1 + len(answer_ids_t), :].float()\n",
    "targets_t = torch.tensor(answer_ids_t, device=DEVICE)\n",
    "nll_single = -F.log_softmax(logits_full, dim=-1).gather(\n",
    "    1, targets_t.unsqueeze(1)).squeeze(1).mean().item()\n",
    "del out_full\n",
    "\n",
    "nll_bare = score(doc_text_t, query_text_t, answer_text_t)\n",
    "diff_pct = abs(nll_single - nll_bare) / nll_single * 100\n",
    "print(f\"  Single-pass NLL: {nll_single:.6f}\")\n",
    "print(f\"  Two-phase bare:  {nll_bare:.6f} (diff: {diff_pct:.2f}%)\")\n",
    "assert diff_pct < 1.0, f\"Bare doesn't match single-pass: {diff_pct}%\"\n",
    "print(f\"  PASSED\")\n",
    "\n",
    "# Test 2: Prefixed scoring on first hard sample from first new dataset\n",
    "print(\"\\n--- Test 2: Prefixed scoring runs correctly ---\")\n",
    "test_ds = NEW_DATASETS[0]\n",
    "ts = hard_samples[test_ds][0]\n",
    "nll_b = score(ts['passage'], ts['query'], ts['answer'])\n",
    "nll_o = score(ts['passage'], ts['query'], ts['answer'],\n",
    "              prefix_token_ids=ts['prefix_oracle'])\n",
    "nll_r = score(ts['passage'], ts['query'], ts['answer'],\n",
    "              prefix_token_ids=ts['prefix_random_tokens'])\n",
    "print(f\"  [{test_ds}] Bare:          {nll_b:.4f}\")\n",
    "print(f\"  [{test_ds}] Oracle:        {nll_o:.4f}  delta={nll_b - nll_o:+.4f}\")\n",
    "print(f\"  [{test_ds}] Random tokens: {nll_r:.4f}  delta={nll_b - nll_r:+.4f}\")\n",
    "assert 0 < nll_b < 20 and 0 < nll_o < 20 and 0 < nll_r < 20\n",
    "print(\"  PASSED\")\n",
    "\n",
    "# Test 3: Token-matching invariant\n",
    "print(\"\\n--- Test 3: Token-matching invariant ---\")\n",
    "Q = ts['Q']\n",
    "for key in PREFIX_KEYS:\n",
    "    assert len(ts[key]) == Q, f\"{key}: {len(ts[key])} != Q={Q}\"\n",
    "print(f\"  All 12 prefixed conditions have Q={Q} tokens\")\n",
    "print(\"  PASSED\")\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print(\"\\nALL VALIDATION TESTS PASSED\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68b700e5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-22T13:01:43.274211Z",
     "iopub.status.busy": "2026-02-22T13:01:43.273627Z",
     "iopub.status.idle": "2026-02-22T15:38:53.427964Z",
     "shell.execute_reply": "2026-02-22T15:38:53.427091Z"
    },
    "papermill": {
     "duration": 9430.162518,
     "end_time": "2026-02-22T15:38:53.430298",
     "exception": false,
     "start_time": "2026-02-22T13:01:43.267780",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "FULL SCORING — 12 prefixed conditions x ~160 hard samples x 3 datasets\n",
      "======================================================================\n",
      "\n",
      "--- squad_v2 (160 hard samples x 12 conditions) ---\n",
      "  Resuming from checkpoint: 160/160\n",
      "  Loaded 160 cached results\n",
      "\n",
      "--- triviaqa (160 hard samples x 12 conditions) ---\n",
      "  Resuming from checkpoint: 160/160\n",
      "  Loaded 160 cached results\n",
      "\n",
      "--- hotpotqa (160 hard samples x 12 conditions) ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "437dbf1175384ae189e0b329ab2801bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Score hotpotqa:   0%|          | 0/160 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 20/160 | 19.9m | ETA 139.6m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 40/160 | 39.6m | ETA 118.9m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 60/160 | 59.3m | ETA 98.8m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 80/160 | 79.0m | ETA 79.0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 100/160 | 98.7m | ETA 59.2m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 120/160 | 118.4m | ETA 39.5m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 140/160 | 137.8m | ETA 19.7m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 160/160 | 157.2m | ETA 0.0m\n",
      "  Scoring complete in 157.2 min\n",
      "\n",
      "======================================================================\n",
      "All scoring complete. Datasets in hard_nlls:\n",
      "  ms_marco: 160 hard samples x 13 conditions\n",
      "  squad_v2: 160 hard samples x 13 conditions\n",
      "  triviaqa: 160 hard samples x 13 conditions\n",
      "  hotpotqa: 160 hard samples x 13 conditions\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Full 12-condition scoring for hard samples of 3 new datasets\n",
    "print(\"=\" * 70)\n",
    "print(\"FULL SCORING — 12 prefixed conditions x ~160 hard samples x 3 datasets\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for ds_name in NEW_DATASETS:\n",
    "    hs = hard_samples[ds_name]\n",
    "    n_hard = len(hs)\n",
    "    ckpt_path = RESULTS_DIR / f\"checkpoint_{ds_name}.json\"\n",
    "\n",
    "    print(f\"\\n--- {ds_name} ({n_hard} hard samples x 12 conditions) ---\")\n",
    "\n",
    "    ds_results = []\n",
    "    start_idx = 0\n",
    "\n",
    "    if ckpt_path.exists():\n",
    "        ckpt = json.loads(ckpt_path.read_text())\n",
    "        if (ckpt.get('dataset') == ds_name and\n",
    "            ckpt.get('scoring') == SCORING_KEY and\n",
    "            ckpt.get('n_hard') == n_hard):\n",
    "            saved_queries = [r['query'][:50] for r in ckpt.get('results', [])]\n",
    "            current_queries = [s['query'][:50] for s in hs[:len(saved_queries)]]\n",
    "            if saved_queries == current_queries:\n",
    "                ds_results = ckpt['results']\n",
    "                start_idx = len(ds_results)\n",
    "                print(f\"  Resuming from checkpoint: {start_idx}/{n_hard}\")\n",
    "\n",
    "    if start_idx < n_hard:\n",
    "        t0 = time.time()\n",
    "\n",
    "        for i in tqdm(range(start_idx, n_hard), initial=start_idx,\n",
    "                      total=n_hard, desc=f\"Score {ds_name}\"):\n",
    "            s = hs[i]\n",
    "            result = {\n",
    "                'query': s['query'],\n",
    "                'answer': s['answer'],\n",
    "                'passage_words': s['word_count'],\n",
    "                'Q': s['Q'],\n",
    "                'nll_bare': float(s['nll_bare']),\n",
    "            }\n",
    "\n",
    "            for cond_name, prefix_key in COND_PREFIX_MAP.items():\n",
    "                result[f'nll_{cond_name}'] = score(\n",
    "                    s['passage'], s['query'], s['answer'],\n",
    "                    prefix_token_ids=s[prefix_key]\n",
    "                )\n",
    "\n",
    "            ds_results.append(result)\n",
    "\n",
    "            if (i + 1) % 20 == 0 or i == n_hard - 1:\n",
    "                ckpt = {\n",
    "                    'dataset': ds_name,\n",
    "                    'n_hard': n_hard,\n",
    "                    'scoring': SCORING_KEY,\n",
    "                    'results': ds_results,\n",
    "                    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                }\n",
    "                ckpt_path.write_text(json.dumps(ckpt))\n",
    "                elapsed = time.time() - t0\n",
    "                done = i - start_idx + 1\n",
    "                eta = (n_hard - i - 1) * elapsed / done if done > 0 else 0\n",
    "                tqdm.write(f\"  Checkpoint {i+1}/{n_hard} | \"\n",
    "                           f\"{elapsed/60:.1f}m | ETA {eta/60:.1f}m\")\n",
    "\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        elapsed = time.time() - t0\n",
    "        print(f\"  Scoring complete in {elapsed/60:.1f} min\")\n",
    "    else:\n",
    "        print(f\"  Loaded {len(ds_results)} cached results\")\n",
    "\n",
    "    # Populate hard_nlls for this dataset\n",
    "    for cond in COND_NAMES:\n",
    "        hard_nlls[ds_name][cond] = np.array(\n",
    "            [r[f'nll_{cond}'] for r in ds_results])\n",
    "\n",
    "    # Sanity check: bare NLLs should match\n",
    "    bare_from_results = np.array([r['nll_bare'] for r in ds_results])\n",
    "    bare_from_cell5 = hard_nlls[ds_name]['bare']\n",
    "    assert np.allclose(bare_from_results, bare_from_cell5, atol=0.01), \\\n",
    "        f\"{ds_name}: bare NLL mismatch between Cell 5 and Cell 8\"\n",
    "    # Use the fresh bare from Cell 5 (scored directly)\n",
    "    hard_nlls[ds_name]['bare'] = bare_from_cell5\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"All scoring complete. Datasets in hard_nlls:\")\n",
    "for ds_name in DATASET_NAMES:\n",
    "    n = len(hard_nlls[ds_name]['bare'])\n",
    "    print(f\"  {ds_name}: {n} hard samples x {len(COND_NAMES)} conditions\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f515e828",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-22T15:38:53.442031Z",
     "iopub.status.busy": "2026-02-22T15:38:53.441730Z",
     "iopub.status.idle": "2026-02-22T15:38:53.731911Z",
     "shell.execute_reply": "2026-02-22T15:38:53.731193Z"
    },
    "papermill": {
     "duration": 0.298124,
     "end_time": "2026-02-22T15:38:53.733546",
     "exception": false,
     "start_time": "2026-02-22T15:38:53.435422",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PER-DATASET ANALYSIS\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "  MS_MARCO — 160 hard samples\n",
      "======================================================================\n",
      "\n",
      "  Cond                         NLL   d bare    sem d   win%          p  sig\n",
      "  ------------------------------------------------------------------------\n",
      "  bare                       2.885       --       --     --         --   --\n",
      "  random_tokens              2.760   +0.137    (ref)  65.0%   8.55e-02   ns\n",
      "  repeat_token               2.691   +0.221   +0.098  47.5%   2.19e-01   ns\n",
      "  scrambled_oracle           2.864   +0.025   -0.112  41.9%   1.59e-01   ns\n",
      "  unrelated_query            2.855   +0.026   -0.084  43.8%   2.88e-01   ns\n",
      "  same_topic                 2.823   +0.064   -0.058  43.1%   4.65e-01   ns\n",
      "  paraphrase                 2.722   +0.162   +0.033  45.0%   6.75e-01   ns\n",
      "  oracle                     2.947   -0.051   -0.141  36.9%   7.71e-02   ns\n",
      "  llm_extract                2.734   +0.200   +0.028  50.0%   7.28e-01   ns\n",
      "  llm_question               2.890   -0.005   -0.124  36.2%   1.20e-01   ns\n",
      "  llm_summarize              2.655   +0.265   +0.106  51.9%   1.83e-01   ns\n",
      "  extractor_matched          2.632   +0.194   +0.098  61.9%   2.15e-01   ns\n",
      "  adversarial_matched        2.721   +0.152   +0.036  50.0%   6.48e-01   ns\n",
      "\n",
      "  Semantic gradient (within hard examples):\n",
      "    [1] scrambled_oracle       sem_delta_d=-0.1118\n",
      "    [2] unrelated_query        sem_delta_d=-0.0843\n",
      "    [3] same_topic             sem_delta_d=-0.0579\n",
      "    [4] paraphrase             sem_delta_d=+0.0332\n",
      "    [5] oracle                 sem_delta_d=-0.1407\n",
      "    Spearman rho: +0.000 (p=1.0000) ns\n",
      "    --> NO gradient\n",
      "\n",
      "  LLM doc-specific vs generic task-framing:\n",
      "    llm_extract vs extractor_matched: d=-0.074, LLM wins 40.6%, p=3.53e-01 ns\n",
      "    llm_question vs extractor_matched: d=-0.159, LLM wins 33.1%, p=4.57e-02 *\n",
      "    llm_summarize vs extractor_matched: d=-0.019, LLM wins 45.0%, p=8.12e-01 ns\n",
      "\n",
      "======================================================================\n",
      "  SQUAD_V2 — 160 hard samples\n",
      "======================================================================\n",
      "\n",
      "  Cond                         NLL   d bare    sem d   win%          p  sig\n",
      "  ------------------------------------------------------------------------\n",
      "  bare                       5.274       --       --     --         --   --\n",
      "  random_tokens              4.493   +0.552    (ref)  73.1%   7.69e-11  ***\n",
      "  repeat_token               4.244   +0.576   +0.198  61.9%   1.31e-02    *\n",
      "  scrambled_oracle           5.128   +0.086   -0.419  33.8%   3.77e-07  ***\n",
      "  unrelated_query            5.653   -0.180   -0.538  29.4%   1.92e-10  ***\n",
      "  same_topic                 5.560   -0.134   -0.525  27.5%   4.56e-10  ***\n",
      "  paraphrase                 5.058   +0.120   -0.348  35.0%   1.95e-05  ***\n",
      "  oracle                     5.552   -0.138   -0.515  30.6%   8.94e-10  ***\n",
      "  llm_extract                4.858   +0.241   -0.251  45.0%   1.82e-03   **\n",
      "  llm_question               5.664   -0.190   -0.561  27.5%   3.91e-11  ***\n",
      "  llm_summarize              4.624   +0.396   -0.095  54.4%   2.33e-01   ns\n",
      "  extractor_matched          3.895   +0.680   +0.359  70.0%   1.07e-05  ***\n",
      "  adversarial_matched        4.165   +0.709   +0.251  61.9%   1.79e-03   **\n",
      "\n",
      "  Semantic gradient (within hard examples):\n",
      "    [1] scrambled_oracle       sem_delta_d=-0.4192\n",
      "    [2] unrelated_query        sem_delta_d=-0.5382\n",
      "    [3] same_topic             sem_delta_d=-0.5254\n",
      "    [4] paraphrase             sem_delta_d=-0.3481\n",
      "    [5] oracle                 sem_delta_d=-0.5154\n",
      "    Spearman rho: +0.200 (p=0.7471) ns\n",
      "    --> WEAK positive trend\n",
      "\n",
      "  LLM doc-specific vs generic task-framing:\n",
      "    llm_extract vs extractor_matched: d=-0.466, LLM wins 31.9%, p=2.24e-08 ***\n",
      "    llm_question vs extractor_matched: d=-0.698, LLM wins 18.1%, p=1.85e-15 ***\n",
      "    llm_summarize vs extractor_matched: d=-0.378, LLM wins 35.0%, p=3.93e-06 ***\n",
      "\n",
      "======================================================================\n",
      "  TRIVIAQA — 160 hard samples\n",
      "======================================================================\n",
      "\n",
      "  Cond                         NLL   d bare    sem d   win%          p  sig\n",
      "  ------------------------------------------------------------------------\n",
      "  bare                       4.796       --       --     --         --   --\n",
      "  random_tokens              4.935   -0.107    (ref)  40.0%   1.76e-01   ns\n",
      "  repeat_token               5.165   -0.217   -0.155  37.5%   5.15e-02   ns\n",
      "  scrambled_oracle           5.404   -0.390   -0.276  45.6%   6.36e-04  ***\n",
      "  unrelated_query            4.964   -0.097   -0.016  49.4%   8.36e-01   ns\n",
      "  same_topic                 5.439   -0.401   -0.253  50.6%   1.67e-03   **\n",
      "  paraphrase                 5.069   -0.172   -0.067  55.0%   3.98e-01   ns\n",
      "  oracle                     5.088   -0.237   -0.097  50.6%   2.24e-01   ns\n",
      "  llm_extract                4.749   +0.039   +0.105  63.7%   1.86e-01   ns\n",
      "  llm_question               5.823   -0.570   -0.394  32.5%   1.57e-06  ***\n",
      "  llm_summarize              4.682   +0.103   +0.155  65.6%   5.15e-02   ns\n",
      "  extractor_matched          4.067   +0.471   +0.536  73.1%   2.17e-10  ***\n",
      "  adversarial_matched        4.548   +0.138   +0.218  62.5%   6.56e-03   **\n",
      "\n",
      "  Semantic gradient (within hard examples):\n",
      "    [1] scrambled_oracle       sem_delta_d=-0.2755\n",
      "    [2] unrelated_query        sem_delta_d=-0.0163\n",
      "    [3] same_topic             sem_delta_d=-0.2529\n",
      "    [4] paraphrase             sem_delta_d=-0.0671\n",
      "    [5] oracle                 sem_delta_d=-0.0966\n",
      "    Spearman rho: +0.300 (p=0.6238) ns\n",
      "    --> WEAK positive trend\n",
      "\n",
      "  LLM doc-specific vs generic task-framing:\n",
      "    llm_extract vs extractor_matched: d=-0.372, LLM wins 36.2%, p=5.51e-06 ***\n",
      "    llm_question vs extractor_matched: d=-0.722, LLM wins 14.4%, p=2.90e-16 ***\n",
      "    llm_summarize vs extractor_matched: d=-0.389, LLM wins 38.8%, p=2.14e-06 ***\n",
      "\n",
      "======================================================================\n",
      "  HOTPOTQA — 160 hard samples\n",
      "======================================================================\n",
      "\n",
      "  Cond                         NLL   d bare    sem d   win%          p  sig\n",
      "  ------------------------------------------------------------------------\n",
      "  bare                       4.411       --       --     --         --   --\n",
      "  random_tokens              4.170   +0.180    (ref)  57.5%   2.41e-02    *\n",
      "  repeat_token               3.688   +0.453   +0.370  69.4%   5.98e-06  ***\n",
      "  scrambled_oracle           4.548   -0.083   -0.272  40.0%   7.57e-04  ***\n",
      "  unrelated_query            4.884   -0.237   -0.374  36.9%   5.00e-06  ***\n",
      "  same_topic                 4.357   +0.034   -0.116  47.5%   1.44e-01   ns\n",
      "  paraphrase                 4.364   +0.026   -0.117  46.9%   1.39e-01   ns\n",
      "  oracle                     4.633   -0.114   -0.288  41.2%   3.67e-04  ***\n",
      "  llm_extract                4.143   +0.178   +0.019  51.9%   8.07e-01   ns\n",
      "  llm_question               4.672   -0.152   -0.322  39.4%   7.17e-05  ***\n",
      "  llm_summarize              4.111   +0.197   +0.041  56.2%   6.08e-01   ns\n",
      "  extractor_matched          3.397   +0.619   +0.467  73.8%   2.06e-08  ***\n",
      "  adversarial_matched        3.913   +0.347   +0.176  60.0%   2.78e-02    *\n",
      "\n",
      "  Semantic gradient (within hard examples):\n",
      "    [1] scrambled_oracle       sem_delta_d=-0.2715\n",
      "    [2] unrelated_query        sem_delta_d=-0.3737\n",
      "    [3] same_topic             sem_delta_d=-0.1161\n",
      "    [4] paraphrase             sem_delta_d=-0.1175\n",
      "    [5] oracle                 sem_delta_d=-0.2879\n",
      "    Spearman rho: +0.100 (p=0.8729) ns\n",
      "    --> WEAK positive trend\n",
      "\n",
      "  LLM doc-specific vs generic task-framing:\n",
      "    llm_extract vs extractor_matched: d=-0.453, LLM wins 31.9%, p=4.79e-08 ***\n",
      "    llm_question vs extractor_matched: d=-0.593, LLM wins 30.6%, p=4.31e-12 ***\n",
      "    llm_summarize vs extractor_matched: d=-0.405, LLM wins 33.1%, p=8.46e-07 ***\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Per-dataset analysis — condition tables, semantic gradient, LLM vs generic\n",
    "print(\"=\" * 70)\n",
    "print(\"PER-DATASET ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Relevance ordering for gradient test (excluding random_tokens = reference)\n",
    "GRADIENT_CONDS = [\n",
    "    ('scrambled_oracle', 1),\n",
    "    ('unrelated_query', 2),\n",
    "    ('same_topic', 3),\n",
    "    ('paraphrase', 4),\n",
    "    ('oracle', 5),\n",
    "]\n",
    "\n",
    "per_dataset_analysis = {}\n",
    "\n",
    "for ds_name in DATASET_NAMES:\n",
    "    nlls = hard_nlls[ds_name]\n",
    "    n_hard = len(nlls['bare'])\n",
    "    bare = nlls['bare']\n",
    "    random_base = nlls['random_tokens']\n",
    "\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"  {ds_name.upper()} — {n_hard} hard samples\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "    analysis = {}\n",
    "\n",
    "    # ---- Part A: Condition table ----\n",
    "    print(f\"\\n  {'Cond':<24} {'NLL':>7} {'d bare':>8} {'sem d':>8} \"\n",
    "          f\"{'win%':>6} {'p':>10} {'sig':>4}\")\n",
    "    print(f\"  {'-'*72}\")\n",
    "\n",
    "    for cond in COND_NAMES:\n",
    "        c_nlls = nlls[cond]\n",
    "        mean_nll = c_nlls.mean()\n",
    "\n",
    "        if cond == 'bare':\n",
    "            print(f\"  {cond:<24} {mean_nll:>7.3f} {'--':>8} {'--':>8} \"\n",
    "                  f\"{'--':>6} {'--':>10} {'--':>4}\")\n",
    "            analysis[cond] = {'mean_nll': float(mean_nll)}\n",
    "            continue\n",
    "\n",
    "        diff_bare = bare - c_nlls\n",
    "        d_bare = cohens_d(diff_bare)\n",
    "        _, p_bare = stats.ttest_1samp(diff_bare, 0)\n",
    "\n",
    "        if cond == 'random_tokens':\n",
    "            win_pct = 100 * np.mean(diff_bare > 0)\n",
    "            sig = ('***' if p_bare < 0.001 else '**' if p_bare < 0.01\n",
    "                   else '*' if p_bare < 0.05 else 'ns')\n",
    "            print(f\"  {cond:<24} {mean_nll:>7.3f} {d_bare:>+8.3f} {'(ref)':>8} \"\n",
    "                  f\"{win_pct:>5.1f}% {p_bare:>10.2e} {sig:>4}\")\n",
    "            analysis[cond] = {\n",
    "                'mean_nll': float(mean_nll), 'd_bare': float(d_bare),\n",
    "                'semantic_delta_d': 0.0, 'p_bare': float(p_bare),\n",
    "            }\n",
    "        else:\n",
    "            sem_delta = random_base - c_nlls\n",
    "            d_sem = cohens_d(sem_delta)\n",
    "            _, p_sem = stats.ttest_1samp(sem_delta, 0)\n",
    "            win_pct = 100 * np.mean(sem_delta > 0)\n",
    "            sig = ('***' if p_sem < 0.001 else '**' if p_sem < 0.01\n",
    "                   else '*' if p_sem < 0.05 else 'ns')\n",
    "            print(f\"  {cond:<24} {mean_nll:>7.3f} {d_bare:>+8.3f} {d_sem:>+8.3f} \"\n",
    "                  f\"{win_pct:>5.1f}% {p_sem:>10.2e} {sig:>4}\")\n",
    "            analysis[cond] = {\n",
    "                'mean_nll': float(mean_nll), 'd_bare': float(d_bare),\n",
    "                'semantic_delta_d': float(d_sem), 'p_semantic': float(p_sem),\n",
    "            }\n",
    "\n",
    "    # ---- Part B: Semantic gradient test ----\n",
    "    print(f\"\\n  Semantic gradient (within hard examples):\")\n",
    "    grad_ranks = []\n",
    "    grad_ds = []\n",
    "    for cond, rank in GRADIENT_CONDS:\n",
    "        sem_d = cohens_d(random_base - nlls[cond])\n",
    "        grad_ranks.append(rank)\n",
    "        grad_ds.append(sem_d)\n",
    "        print(f\"    [{rank}] {cond:<22} sem_delta_d={sem_d:+.4f}\")\n",
    "\n",
    "    rho, p_grad = stats.spearmanr(grad_ranks, grad_ds)\n",
    "    sig = ('***' if p_grad < 0.001 else '**' if p_grad < 0.01\n",
    "           else '*' if p_grad < 0.05 else 'ns')\n",
    "    print(f\"    Spearman rho: {rho:+.3f} (p={p_grad:.4f}) {sig}\")\n",
    "\n",
    "    if rho > 0.8 and p_grad < 0.10:\n",
    "        verdict = \"MONOTONIC gradient\"\n",
    "    elif rho > 0.5:\n",
    "        verdict = \"PARTIAL gradient\"\n",
    "    elif rho > 0:\n",
    "        verdict = \"WEAK positive trend\"\n",
    "    else:\n",
    "        verdict = \"NO gradient\"\n",
    "    print(f\"    --> {verdict}\")\n",
    "    analysis['gradient'] = {\n",
    "        'rho': float(rho), 'p': float(p_grad), 'verdict': verdict,\n",
    "    }\n",
    "\n",
    "    # ---- Part C: LLM doc-specific vs generic ----\n",
    "    print(f\"\\n  LLM doc-specific vs generic task-framing:\")\n",
    "    llm_pairs = [\n",
    "        ('llm_extract', 'extractor_matched'),\n",
    "        ('llm_question', 'extractor_matched'),\n",
    "        ('llm_summarize', 'extractor_matched'),\n",
    "    ]\n",
    "    for llm_cond, gen_cond in llm_pairs:\n",
    "        diff = nlls[gen_cond] - nlls[llm_cond]  # pos = LLM better\n",
    "        d = cohens_d(diff)\n",
    "        _, p = stats.ttest_1samp(diff, 0)\n",
    "        win = 100 * np.mean(diff > 0)\n",
    "        sig = ('***' if p < 0.001 else '**' if p < 0.01\n",
    "               else '*' if p < 0.05 else 'ns')\n",
    "        print(f\"    {llm_cond} vs {gen_cond}: d={d:+.3f}, \"\n",
    "              f\"LLM wins {win:.1f}%, p={p:.2e} {sig}\")\n",
    "\n",
    "    per_dataset_analysis[ds_name] = analysis\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "872900b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-22T15:38:53.746550Z",
     "iopub.status.busy": "2026-02-22T15:38:53.746233Z",
     "iopub.status.idle": "2026-02-22T15:38:54.592156Z",
     "shell.execute_reply": "2026-02-22T15:38:54.591241Z"
    },
    "papermill": {
     "duration": 0.854643,
     "end_time": "2026-02-22T15:38:54.593991",
     "exception": false,
     "start_time": "2026-02-22T15:38:53.739348",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "CROSS-DATASET ANALYSIS\n",
      "======================================================================\n",
      "\n",
      "--- PART 1: Semantic delta d across datasets ---\n",
      "\n",
      "  Condition                  ms_marco   squad_v2   triviaqa   hotpotqa      mean consistent\n",
      "  ------------------------------------------------------------------------------------------\n",
      "  repeat_token                 +0.098     +0.198     -0.155     +0.370    +0.128         NO\n",
      "  scrambled_oracle             -0.112     -0.419     -0.276     -0.272    -0.270        YES\n",
      "  unrelated_query              -0.084     -0.538     -0.016     -0.374    -0.253        YES\n",
      "  same_topic                   -0.058     -0.525     -0.253     -0.116    -0.238        YES\n",
      "  paraphrase                   +0.033     -0.348     -0.067     -0.117    -0.125         NO\n",
      "  oracle                       -0.141     -0.515     -0.097     -0.288    -0.260        YES\n",
      "  llm_extract                  +0.028     -0.251     +0.105     +0.019    -0.025         NO\n",
      "  llm_question                 -0.124     -0.561     -0.394     -0.322    -0.350        YES\n",
      "  llm_summarize                +0.106     -0.095     +0.155     +0.041    +0.052         NO\n",
      "  extractor_matched            +0.098     +0.359     +0.536     +0.467    +0.365        YES\n",
      "  adversarial_matched          +0.036     +0.251     +0.218     +0.176    +0.170        YES\n",
      "\n",
      "--- PART 2: Fixed-Effects Meta-Analysis ---\n",
      "\n",
      "  Condition                 pooled_d       SE        z          p           95% CI  sig\n",
      "  ----------------------------------------------------------------------------------\n",
      "  repeat_token               +0.1244   0.0400    +3.11   1.89e-03 [+0.046, +0.203]   **\n",
      "  scrambled_oracle           -0.2665   0.0403    -6.61   3.90e-11 [-0.346, -0.187]  ***\n",
      "  unrelated_query            -0.2418   0.0405    -5.96   2.45e-09 [-0.321, -0.162]  ***\n",
      "  same_topic                 -0.2292   0.0403    -5.68   1.35e-08 [-0.308, -0.150]  ***\n",
      "  paraphrase                 -0.1217   0.0399    -3.05   2.27e-03 [-0.200, -0.044]   **\n",
      "  oracle                     -0.2525   0.0404    -6.25   4.11e-10 [-0.332, -0.173]  ***\n",
      "  llm_extract                -0.0232   0.0397    -0.58   5.59e-01 [-0.101, +0.055]   ns\n",
      "  llm_question               -0.3427   0.0409    -8.38   5.37e-17 [-0.423, -0.263]  ***\n",
      "  llm_summarize              +0.0515   0.0396    +1.30   1.94e-01 [-0.026, +0.129]   ns\n",
      "  extractor_matched          +0.3573   0.0410    +8.70   3.18e-18 [+0.277, +0.438]  ***\n",
      "  adversarial_matched        +0.1692   0.0399    +4.24   2.20e-05 [+0.091, +0.247]  ***\n",
      "\n",
      "--- PART 3: Semantic Gradient Per Dataset ---\n",
      "\n",
      "  Dataset               rho          p              verdict\n",
      "  ----------------------------------------------------------\n",
      "  ms_marco           +0.000     1.0000                 NONE\n",
      "  squad_v2           +0.200     0.7471                 WEAK\n",
      "  triviaqa           +0.300     0.6238                 WEAK\n",
      "  hotpotqa           +0.100     0.8729                 WEAK\n",
      "\n",
      "  Mean rho across datasets: +0.150\n",
      "  Datasets with positive gradient: 3/4\n",
      "\n",
      "--- PART 4: Dataset Properties ---\n",
      "\n",
      "  Dataset          N_hard   pass_w    ans_w    Q_tok   bare NLL best_sem_d\n",
      "  ----------------------------------------------------------------------\n",
      "  ms_marco            160       72      5.8      7.1      2.885     +0.106\n",
      "  squad_v2            160      129      2.4     12.8      5.274     +0.359\n",
      "  triviaqa            160      479      1.5     17.5      4.796     +0.536\n",
      "  hotpotqa            160       60      2.0     19.2      4.411     +0.467\n",
      "\n",
      "======================================================================\n",
      "VERDICT — Exp 03: Hard-Example Semantic Isolation Across Datasets\n",
      "======================================================================\n",
      "\n",
      "Model: google/gemma-3-12b-it\n",
      "Scoring: BOS-retained repositioning + token-level prefix matching\n",
      "Datasets: 4 (ms_marco, squad_v2, triviaqa, hotpotqa)\n",
      "Hard selection: top 40% by bare NLL\n",
      "\n",
      "--- Key findings ---\n",
      "\n",
      "  1. Semantic gradient in hard examples:\n",
      "     0/4 datasets show partial/monotonic gradient\n",
      "     Mean Spearman rho: +0.150\n",
      "     --> NO: gradient does not consistently emerge\n",
      "\n",
      "  2. Conditions with significant semantic benefit (pooled p<0.05):\n",
      "     extractor_matched        pooled_d=+0.357 [+0.277, +0.438]\n",
      "     adversarial_matched      pooled_d=+0.169 [+0.091, +0.247]\n",
      "     repeat_token             pooled_d=+0.124 [+0.046, +0.203]\n",
      "     paraphrase               pooled_d=-0.122 [-0.200, -0.044]\n",
      "     same_topic               pooled_d=-0.229 [-0.308, -0.150]\n",
      "     unrelated_query          pooled_d=-0.242 [-0.321, -0.162]\n",
      "     oracle                   pooled_d=-0.253 [-0.332, -0.173]\n",
      "     scrambled_oracle         pooled_d=-0.267 [-0.346, -0.187]\n",
      "     llm_question             pooled_d=-0.343 [-0.423, -0.263]\n",
      "\n",
      "  3. Cross-dataset sign consistency:\n",
      "     7/11 conditions have same sign across all 4 datasets\n",
      "\n",
      "  4. LLM doc-specific vs generic (pooled):\n",
      "     llm_extract            pooled_d=-0.023  extractor_matched pooled_d=+0.357\n",
      "     llm_question           pooled_d=-0.343  extractor_matched pooled_d=+0.357\n",
      "     llm_summarize          pooled_d=+0.051  extractor_matched pooled_d=+0.357\n",
      "\n",
      "  5. All conditions ranked by pooled semantic delta d:\n",
      "     extractor_matched        d=+0.3573 (***)\n",
      "     adversarial_matched      d=+0.1692 (***)\n",
      "     repeat_token             d=+0.1244 (**)\n",
      "     llm_summarize            d=+0.0515 (ns)\n",
      "     llm_extract              d=-0.0232 (ns)\n",
      "     paraphrase               d=-0.1217 (**)\n",
      "     same_topic               d=-0.2292 (***)\n",
      "     unrelated_query          d=-0.2418 (***)\n",
      "     oracle                   d=-0.2525 (***)\n",
      "     scrambled_oracle         d=-0.2665 (***)\n",
      "     llm_question             d=-0.3427 (***)\n",
      "\n",
      "Results saved to ../../../results/decoder_only/exp03/results.json\n",
      "\n",
      "Cleaning up GPU memory...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory: 14.52 GB -> 0.01 GB\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Cell 10: Cross-dataset meta-analysis, consistency, and verdict\n",
    "print(\"=\" * 70)\n",
    "print(\"CROSS-DATASET ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# ================================================================\n",
    "# PART 1: Cross-dataset condition comparison\n",
    "# ================================================================\n",
    "print(f\"\\n--- PART 1: Semantic delta d across datasets ---\")\n",
    "print(f\"\\n  {'Condition':<24}\", end=\"\")\n",
    "for ds_name in DATASET_NAMES:\n",
    "    print(f\" {ds_name[:10]:>10}\", end=\"\")\n",
    "print(f\"  {'mean':>8} {'consistent':>10}\")\n",
    "print(f\"  {'-'*90}\")\n",
    "\n",
    "cross_dataset = {}  # cond -> {ds_name: d, ...}\n",
    "for cond in COND_NAMES:\n",
    "    if cond in ('bare', 'random_tokens'):\n",
    "        continue\n",
    "    row = f\"  {cond:<24}\"\n",
    "    ds_vals = []\n",
    "    for ds_name in DATASET_NAMES:\n",
    "        sem_delta = hard_nlls[ds_name]['random_tokens'] - hard_nlls[ds_name][cond]\n",
    "        d = cohens_d(sem_delta)\n",
    "        row += f\" {d:>+10.3f}\"\n",
    "        ds_vals.append(d)\n",
    "    mean_d = np.mean(ds_vals)\n",
    "    same_sign = all(v >= 0 for v in ds_vals) or all(v <= 0 for v in ds_vals)\n",
    "    row += f\"  {mean_d:>+8.3f} {'YES' if same_sign else 'NO':>10}\"\n",
    "    print(row)\n",
    "    cross_dataset[cond] = {ds: float(d) for ds, d in zip(DATASET_NAMES, ds_vals)}\n",
    "    cross_dataset[cond]['mean'] = float(mean_d)\n",
    "    cross_dataset[cond]['consistent_sign'] = same_sign\n",
    "\n",
    "# ================================================================\n",
    "# PART 2: Fixed-effects meta-analysis\n",
    "# ================================================================\n",
    "print(f\"\\n--- PART 2: Fixed-Effects Meta-Analysis ---\")\n",
    "print(f\"\\n  {'Condition':<24} {'pooled_d':>9} {'SE':>8} {'z':>8} \"\n",
    "      f\"{'p':>10} {'95% CI':>16} {'sig':>4}\")\n",
    "print(f\"  {'-'*82}\")\n",
    "\n",
    "meta_results = {}\n",
    "for cond in COND_NAMES:\n",
    "    if cond in ('bare', 'random_tokens'):\n",
    "        continue\n",
    "\n",
    "    # Per-dataset estimates\n",
    "    ds_effects = []\n",
    "    for ds_name in DATASET_NAMES:\n",
    "        sem_delta = hard_nlls[ds_name]['random_tokens'] - hard_nlls[ds_name][cond]\n",
    "        n = len(sem_delta)\n",
    "        d = cohens_d(sem_delta)\n",
    "        se = np.sqrt(1.0/n + d**2 / (2.0*n))\n",
    "        ds_effects.append((d, se, n))\n",
    "\n",
    "    # Fixed-effects pooling\n",
    "    weights = [1.0 / (se**2) for _, se, _ in ds_effects]\n",
    "    w_sum = sum(weights)\n",
    "    pooled_d = sum(w * d for (d, _, _), w in zip(ds_effects, weights)) / w_sum\n",
    "    pooled_se = 1.0 / np.sqrt(w_sum)\n",
    "    z = pooled_d / pooled_se if pooled_se > 0 else 0.0\n",
    "    p = 2 * stats.norm.sf(abs(z))\n",
    "    ci_lo = pooled_d - 1.96 * pooled_se\n",
    "    ci_hi = pooled_d + 1.96 * pooled_se\n",
    "    sig = ('***' if p < 0.001 else '**' if p < 0.01\n",
    "           else '*' if p < 0.05 else 'ns')\n",
    "\n",
    "    print(f\"  {cond:<24} {pooled_d:>+9.4f} {pooled_se:>8.4f} {z:>+8.2f} \"\n",
    "          f\"{p:>10.2e} [{ci_lo:>+.3f}, {ci_hi:>+.3f}] {sig:>4}\")\n",
    "    meta_results[cond] = {\n",
    "        'pooled_d': float(pooled_d), 'se': float(pooled_se),\n",
    "        'z': float(z), 'p': float(p),\n",
    "        'ci_lo': float(ci_lo), 'ci_hi': float(ci_hi),\n",
    "    }\n",
    "\n",
    "# ================================================================\n",
    "# PART 3: Semantic gradient across datasets\n",
    "# ================================================================\n",
    "print(f\"\\n--- PART 3: Semantic Gradient Per Dataset ---\")\n",
    "print(f\"\\n  {'Dataset':<16} {'rho':>8} {'p':>10} {'verdict':>20}\")\n",
    "print(f\"  {'-'*58}\")\n",
    "\n",
    "all_rhos = []\n",
    "for ds_name in DATASET_NAMES:\n",
    "    grad_ranks = []\n",
    "    grad_ds = []\n",
    "    for cond, rank in GRADIENT_CONDS:\n",
    "        sem_d = cohens_d(hard_nlls[ds_name]['random_tokens'] -\n",
    "                         hard_nlls[ds_name][cond])\n",
    "        grad_ranks.append(rank)\n",
    "        grad_ds.append(sem_d)\n",
    "    rho, p = stats.spearmanr(grad_ranks, grad_ds)\n",
    "    all_rhos.append(rho)\n",
    "    if rho > 0.8 and p < 0.10:\n",
    "        verdict = \"MONOTONIC\"\n",
    "    elif rho > 0.5:\n",
    "        verdict = \"PARTIAL\"\n",
    "    elif rho > 0:\n",
    "        verdict = \"WEAK\"\n",
    "    else:\n",
    "        verdict = \"NONE\"\n",
    "    print(f\"  {ds_name:<16} {rho:>+8.3f} {p:>10.4f} {verdict:>20}\")\n",
    "\n",
    "mean_rho = np.mean(all_rhos)\n",
    "print(f\"\\n  Mean rho across datasets: {mean_rho:+.3f}\")\n",
    "n_positive = sum(1 for r in all_rhos if r > 0)\n",
    "print(f\"  Datasets with positive gradient: {n_positive}/{len(DATASET_NAMES)}\")\n",
    "\n",
    "# ================================================================\n",
    "# PART 4: Dataset properties\n",
    "# ================================================================\n",
    "print(f\"\\n--- PART 4: Dataset Properties ---\")\n",
    "print(f\"\\n  {'Dataset':<16} {'N_hard':>6} {'pass_w':>8} {'ans_w':>8} \"\n",
    "      f\"{'Q_tok':>8} {'bare NLL':>10} {'best_sem_d':>10}\")\n",
    "print(f\"  {'-'*70}\")\n",
    "\n",
    "for ds_name in DATASET_NAMES:\n",
    "    meta = hard_metadata[ds_name]\n",
    "    bare_mean = hard_nlls[ds_name]['bare'].mean()\n",
    "\n",
    "    # Find condition with highest semantic delta d\n",
    "    best_cond = None\n",
    "    best_d = -999\n",
    "    for cond in COND_NAMES:\n",
    "        if cond in ('bare', 'random_tokens'):\n",
    "            continue\n",
    "        sem_d = cohens_d(hard_nlls[ds_name]['random_tokens'] -\n",
    "                         hard_nlls[ds_name][cond])\n",
    "        if sem_d > best_d:\n",
    "            best_d = sem_d\n",
    "            best_cond = cond\n",
    "\n",
    "    print(f\"  {ds_name:<16} {meta['n_hard']:>6} \"\n",
    "          f\"{meta['mean_passage_words']:>8.0f} \"\n",
    "          f\"{meta['mean_answer_words']:>8.1f} \"\n",
    "          f\"{meta['mean_query_tokens']:>8.1f} \"\n",
    "          f\"{bare_mean:>10.3f} {best_d:>+10.3f}\")\n",
    "\n",
    "# ================================================================\n",
    "# VERDICT\n",
    "# ================================================================\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"VERDICT — Exp 03: Hard-Example Semantic Isolation Across Datasets\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\nModel: {MODEL_NAME}\")\n",
    "print(f\"Scoring: BOS-retained repositioning + token-level prefix matching\")\n",
    "print(f\"Datasets: {len(DATASET_NAMES)} ({', '.join(DATASET_NAMES)})\")\n",
    "print(f\"Hard selection: top {HARD_FRAC*100:.0f}% by bare NLL\")\n",
    "\n",
    "# Key findings\n",
    "print(f\"\\n--- Key findings ---\")\n",
    "\n",
    "# 1. Does the semantic gradient emerge in hard examples?\n",
    "n_gradient = sum(1 for r in all_rhos if r > 0.5)\n",
    "print(f\"\\n  1. Semantic gradient in hard examples:\")\n",
    "print(f\"     {n_gradient}/{len(DATASET_NAMES)} datasets show partial/monotonic gradient\")\n",
    "print(f\"     Mean Spearman rho: {mean_rho:+.3f}\")\n",
    "if n_gradient >= 3:\n",
    "    print(f\"     --> YES: gradient generalizes across datasets\")\n",
    "elif n_gradient >= 2:\n",
    "    print(f\"     --> PARTIAL: gradient in {n_gradient}/{len(DATASET_NAMES)} datasets\")\n",
    "else:\n",
    "    print(f\"     --> NO: gradient does not consistently emerge\")\n",
    "\n",
    "# 2. Which conditions have significant semantic benefit?\n",
    "print(f\"\\n  2. Conditions with significant semantic benefit (pooled p<0.05):\")\n",
    "sig_conds = [(c, m) for c, m in meta_results.items() if m['p'] < 0.05]\n",
    "sig_conds.sort(key=lambda x: x[1]['pooled_d'], reverse=True)\n",
    "for cond, m in sig_conds:\n",
    "    print(f\"     {cond:<24} pooled_d={m['pooled_d']:+.3f} \"\n",
    "          f\"[{m['ci_lo']:+.3f}, {m['ci_hi']:+.3f}]\")\n",
    "if not sig_conds:\n",
    "    print(f\"     (none)\")\n",
    "\n",
    "# 3. Cross-dataset consistency\n",
    "n_consistent = sum(1 for v in cross_dataset.values() if v.get('consistent_sign'))\n",
    "print(f\"\\n  3. Cross-dataset sign consistency:\")\n",
    "print(f\"     {n_consistent}/{len(cross_dataset)} conditions have same sign across all 4 datasets\")\n",
    "\n",
    "# 4. LLM doc-specific vs generic\n",
    "print(f\"\\n  4. LLM doc-specific vs generic (pooled):\")\n",
    "for cond in ['llm_extract', 'llm_question', 'llm_summarize']:\n",
    "    if cond in meta_results:\n",
    "        m = meta_results[cond]\n",
    "        gen_cond = 'extractor_matched'\n",
    "        gm = meta_results.get(gen_cond, {})\n",
    "        print(f\"     {cond:<22} pooled_d={m['pooled_d']:+.3f}  \"\n",
    "              f\"{gen_cond} pooled_d={gm.get('pooled_d', 0):+.3f}\")\n",
    "\n",
    "# Ranked conditions by pooled d\n",
    "print(f\"\\n  5. All conditions ranked by pooled semantic delta d:\")\n",
    "ranked = sorted(meta_results.items(), key=lambda x: x[1]['pooled_d'], reverse=True)\n",
    "for cond, m in ranked:\n",
    "    sig = ('***' if m['p'] < 0.001 else '**' if m['p'] < 0.01\n",
    "           else '*' if m['p'] < 0.05 else 'ns')\n",
    "    print(f\"     {cond:<24} d={m['pooled_d']:+.4f} ({sig})\")\n",
    "\n",
    "# ================================================================\n",
    "# SAVE RESULTS\n",
    "# ================================================================\n",
    "final_results = {\n",
    "    'experiment': 'v4_exp03_hard_semantic_cross_dataset',\n",
    "    'model': MODEL_NAME,\n",
    "    'scoring': 'bos_retained_repositioning_token_matched',\n",
    "    'hard_fraction': HARD_FRAC,\n",
    "    'datasets': DATASET_NAMES,\n",
    "    'n_samples_per_dataset': N_SAMPLES,\n",
    "    'seed': SEED,\n",
    "    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'per_dataset': {},\n",
    "    'meta_analysis': meta_results,\n",
    "    'cross_dataset': cross_dataset,\n",
    "    'gradient': {\n",
    "        'per_dataset_rho': {ds: float(r) for ds, r in zip(DATASET_NAMES, all_rhos)},\n",
    "        'mean_rho': float(mean_rho),\n",
    "    },\n",
    "    'hard_metadata': {ds: hard_metadata[ds] for ds in DATASET_NAMES},\n",
    "}\n",
    "\n",
    "for ds_name in DATASET_NAMES:\n",
    "    final_results['per_dataset'][ds_name] = per_dataset_analysis.get(ds_name, {})\n",
    "\n",
    "with open(RESULTS_DIR / 'results.json', 'w') as f:\n",
    "    json.dump(final_results, f, indent=2)\n",
    "print(f\"\\nResults saved to {RESULTS_DIR / 'results.json'}\")\n",
    "\n",
    "# Cleanup\n",
    "print(f\"\\nCleaning up GPU memory...\")\n",
    "mem_before = torch.cuda.memory_allocated() / 1e9\n",
    "del model, tokenizer\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "mem_after = torch.cuda.memory_allocated() / 1e9\n",
    "print(f\"GPU memory: {mem_before:.2f} GB -> {mem_after:.2f} GB\")\n",
    "print(\"Done!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 9485.400319,
   "end_time": "2026-02-22T15:38:57.822822",
   "environment_variables": {},
   "exception": null,
   "input_path": "03_hard_semantic_cross_dataset.ipynb",
   "output_path": "03_hard_semantic_cross_dataset_executed.ipynb",
   "parameters": {},
   "start_time": "2026-02-22T13:00:52.422503",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "001e7a864c48478fbe12c7266103aa74": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_979d8a5ef73a4d3193b7edcc70411783",
       "placeholder": "​",
       "style": "IPY_MODEL_09e9c894efde4ff8a3bcb91a50c139fa",
       "tabbable": null,
       "tooltip": null,
       "value": "Score hotpotqa: 100%"
      }
     },
     "00709842e9124af1a27d81d7dcdca455": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_a63da337f70d485185f9d9ea39fbb48c",
        "IPY_MODEL_ae54aafe001d4ae2a0d075c15ea53e82",
        "IPY_MODEL_22f06fe081984e6d9094d75773aad706"
       ],
       "layout": "IPY_MODEL_09110344e58a41ff8cb53b49f93213af",
       "tabbable": null,
       "tooltip": null
      }
     },
     "09110344e58a41ff8cb53b49f93213af": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "09e9c894efde4ff8a3bcb91a50c139fa": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "1a4e4b0108274bb7a2f163388cb7fb63": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "1df26de423534aab83182f9887a84218": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_7e2f729da59542a4b0d586871f928456",
       "placeholder": "​",
       "style": "IPY_MODEL_b8b539330d0f430b98b2c00a4b7df59e",
       "tabbable": null,
       "tooltip": null,
       "value": " 1065/1065 [00:04&lt;00:00, 686.38it/s, Materializing param=model.vision_tower.vision_model.post_layernorm.weight]"
      }
     },
     "22f06fe081984e6d9094d75773aad706": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_9468a3aee1a8498fa7e0a1fad9a2c756",
       "placeholder": "​",
       "style": "IPY_MODEL_1a4e4b0108274bb7a2f163388cb7fb63",
       "tabbable": null,
       "tooltip": null,
       "value": " 26/26 [00:00&lt;00:00, 2586.31it/s]"
      }
     },
     "2ad0f0fa5f794047ba09b4ec28872b4c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_4a2412896a844a958f54934f2e1cb842",
       "max": 160.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_edbc86abe4a146b9883284df3eef3520",
       "tabbable": null,
       "tooltip": null,
       "value": 160.0
      }
     },
     "3e79e71c49a0441fb6fc2cc609eb88e3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_b1c9eb1c11974d989aad4229041daefd",
       "placeholder": "​",
       "style": "IPY_MODEL_e29d0941cfc7467c9f104a9509a72954",
       "tabbable": null,
       "tooltip": null,
       "value": "Loading weights: 100%"
      }
     },
     "437dbf1175384ae189e0b329ab2801bc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_001e7a864c48478fbe12c7266103aa74",
        "IPY_MODEL_2ad0f0fa5f794047ba09b4ec28872b4c",
        "IPY_MODEL_ac37636b61c3405c894ddf13e6567f52"
       ],
       "layout": "IPY_MODEL_a8387ee9fa474bc5a82cfc9db56777c7",
       "tabbable": null,
       "tooltip": null
      }
     },
     "43dfb0df0f06437ca6c9cdbd28fec452": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_76a37cccd36e43aa835fe277bc82d80b",
       "max": 1065.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_d2377fb11cb94972a8de3b7035c3ec9b",
       "tabbable": null,
       "tooltip": null,
       "value": 1065.0
      }
     },
     "4a2412896a844a958f54934f2e1cb842": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6151d02b9c2e42d5bcc11db9b9292c2a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "66ab1f16697144a3ad1e838ee27d745c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "76a37cccd36e43aa835fe277bc82d80b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "7e2f729da59542a4b0d586871f928456": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "9468a3aee1a8498fa7e0a1fad9a2c756": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "979d8a5ef73a4d3193b7edcc70411783": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "9821bb87e2b84df2ad64824214a8da2b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "a09f3257e04b46de8839f0e9e871ddef": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "a4afea049a8040aaa18b27cbdf0ac5cd": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a63da337f70d485185f9d9ea39fbb48c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_a4afea049a8040aaa18b27cbdf0ac5cd",
       "placeholder": "​",
       "style": "IPY_MODEL_d3a48d9dbc9c4e40a96b93d78dfb4b1d",
       "tabbable": null,
       "tooltip": null,
       "value": "Resolving data files: 100%"
      }
     },
     "a8387ee9fa474bc5a82cfc9db56777c7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ac37636b61c3405c894ddf13e6567f52": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_e813a290b1d94ae5b16396c6b6b9e774",
       "placeholder": "​",
       "style": "IPY_MODEL_a09f3257e04b46de8839f0e9e871ddef",
       "tabbable": null,
       "tooltip": null,
       "value": " 160/160 [2:37:09&lt;00:00, 58.27s/it]"
      }
     },
     "ae54aafe001d4ae2a0d075c15ea53e82": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_6151d02b9c2e42d5bcc11db9b9292c2a",
       "max": 26.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_9821bb87e2b84df2ad64824214a8da2b",
       "tabbable": null,
       "tooltip": null,
       "value": 26.0
      }
     },
     "b1c9eb1c11974d989aad4229041daefd": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b8b539330d0f430b98b2c00a4b7df59e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "d2377fb11cb94972a8de3b7035c3ec9b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "d3a48d9dbc9c4e40a96b93d78dfb4b1d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "e29d0941cfc7467c9f104a9509a72954": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "e813a290b1d94ae5b16396c6b6b9e774": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "edbc86abe4a146b9883284df3eef3520": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "ef070973cffd45238d7daee94b3422f8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_3e79e71c49a0441fb6fc2cc609eb88e3",
        "IPY_MODEL_43dfb0df0f06437ca6c9cdbd28fec452",
        "IPY_MODEL_1df26de423534aab83182f9887a84218"
       ],
       "layout": "IPY_MODEL_66ab1f16697144a3ad1e838ee27d745c",
       "tabbable": null,
       "tooltip": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}