{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "699b7b65",
   "metadata": {},
   "source": [
    "# Experiment 03: Hard-Example Semantic Isolation Across Datasets\n",
    "\n",
    "## Motivation\n",
    "\n",
    "Exp 02 (token-matched, 13 conditions, N=400, Gemma 3 12B-IT) confirmed:\n",
    "- **Oracle HURTS overall** (d=-0.151, p=0.003)\n",
    "- **No semantic gradient overall** (Spearman rho=-0.43, p=0.40)\n",
    "- **But in Q5 (hardest 20%)**, ALL conditions help, and there IS a semantic gradient:\n",
    "  llm_summarize d=+0.493 > llm_extract +0.356 > same_topic +0.334 > paraphrase +0.291\n",
    "  > random_tokens +0.218 > oracle +0.078\n",
    "- The semantic effect is **real but masked** by the dominant structural effect in easy samples\n",
    "\n",
    "## Goal\n",
    "\n",
    "Isolate the semantic effect by:\n",
    "1. Restricting to hard examples (top 40% by bare NLL) where conditioning helps\n",
    "2. Measuring the **semantic delta** above the structural baseline (condition - random_tokens)\n",
    "3. Testing whether this pattern generalizes across 4 diverse QA datasets\n",
    "\n",
    "## Method — BOS-Retained Repositioning with Token-Level Matching\n",
    "\n",
    "Identical to Exp 02. Phase A builds KV cache with `[BOS] + prefix_ids(Q) + [\\n] + doc_ids(D)`,\n",
    "selects BOS + doc, repositions doc keys. Phase B scores `[\\n + query + \\n + answer]` with\n",
    "cache_position auto-generated from cache length. No look-ahead.\n",
    "\n",
    "## Datasets (4 total)\n",
    "\n",
    "| Dataset | Source | Question type | Passage type |\n",
    "|---------|--------|--------------|--------------|\n",
    "| MS MARCO | `microsoft/ms_marco` v1.1 | Web search queries | Selected passages (30-300w) |\n",
    "| SQuAD 2.0 | `rajpurkar/squad_v2` | Factoid questions | Wikipedia paragraphs (30-500w) |\n",
    "| TriviaQA | `mandarjoshi/trivia_qa` rc.wikipedia | Trivia questions | Wikipedia articles (first 500w) |\n",
    "| HotpotQA | `hotpotqa/hotpot_qa` distractor | Multi-hop questions | Supporting fact sentences (30-500w) |\n",
    "\n",
    "## Conditions (13 total, same as Exp 02)\n",
    "\n",
    "| # | Key | Semantic relevance | Token construction |\n",
    "|---|-----|-------------------|--------------------|\n",
    "| 1 | `bare` | baseline | No prefix |\n",
    "| 2 | `random_tokens` | none | Q random IDs from vocab |\n",
    "| 3 | `repeat_token` | none (structural) | Token ID 1000 repeated Q times |\n",
    "| 4 | `scrambled_oracle` | vocab match only | Random permutation of oracle IDs |\n",
    "| 5 | `unrelated_query` | low | Other sample's query, pad/trunc to Q |\n",
    "| 6 | `same_topic` | medium | LLM: \"Write a question about same topic...\" |\n",
    "| 7 | `paraphrase` | high | LLM: \"Rephrase this query differently...\" |\n",
    "| 8 | `oracle` | maximal | Exact query token IDs |\n",
    "| 9 | `llm_extract` | task-framing (doc) | LLM: \"List key facts from this document\" |\n",
    "| 10 | `llm_question` | query-like (doc) | LLM: \"What question does this doc answer?\" |\n",
    "| 11 | `llm_summarize` | summary (doc) | LLM: \"Summarize in one sentence\" |\n",
    "| 12 | `extractor_matched` | task-framing (generic) | Fixed extraction text |\n",
    "| 13 | `adversarial_matched` | adversarial | Fixed adversarial text |\n",
    "\n",
    "## Key metric: Semantic delta\n",
    "\n",
    "For each condition C: `semantic_delta(C) = NLL(random_tokens) - NLL(C)`\n",
    "\n",
    "Positive = semantic content helps beyond structural baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22d287f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Setup, model loading, and scoring functions\n",
    "import os\n",
    "os.umask(0o000)\n",
    "import sys, json, time, gc, re\n",
    "import random as pyrandom\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from scipy import stats\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "sys.path.insert(0, \"../../..\")\n",
    "from lib.analysis import cohens_d\n",
    "from lib.data import count_words\n",
    "\n",
    "SEED = 42\n",
    "N_SAMPLES = 400      # per dataset\n",
    "HARD_FRAC = 0.40     # top 40% by bare NLL\n",
    "MODEL_NAME = \"google/gemma-3-12b-it\"\n",
    "\n",
    "RESULTS_DIR = Path(\"../../../results/decoder_only/exp03\")\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "EXP02_DIR = Path(\"../../../results/decoder_only/exp02\")\n",
    "\n",
    "DATASET_NAMES = ['ms_marco', 'squad_v2', 'triviaqa', 'hotpotqa']\n",
    "NEW_DATASETS = ['squad_v2', 'triviaqa', 'hotpotqa']\n",
    "\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "pyrandom.seed(SEED)\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv())\n",
    "HF_TOKEN = os.environ.get(\"HF_TOKEN\")\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, DynamicCache\n",
    "\n",
    "print(f\"Loading {MODEL_NAME}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, token=HF_TOKEN)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME, device_map=\"auto\", dtype=torch.bfloat16, token=HF_TOKEN,\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "DEVICE = next(model.parameters()).device\n",
    "BOS_ID = tokenizer.bos_token_id\n",
    "NEWLINE_IDS = tokenizer(\"\\n\", add_special_tokens=False).input_ids\n",
    "text_cfg = getattr(model.config, 'text_config', model.config)\n",
    "# Use actual embedding table size (config vocab_size may include padding rows)\n",
    "VOCAB_SIZE = model.get_input_embeddings().num_embeddings\n",
    "cfg_vocab = getattr(text_cfg, 'vocab_size', None)\n",
    "if cfg_vocab != VOCAB_SIZE:\n",
    "    print(f\"WARNING: config vocab_size={cfg_vocab} != embedding size={VOCAB_SIZE}\")\n",
    "    print(f\"Using embedding size {VOCAB_SIZE} for random token generation\")\n",
    "rope_params = getattr(text_cfg, 'rope_parameters', {})\n",
    "layer_types = getattr(text_cfg, 'layer_types', [])\n",
    "# Sliding attention layers cache only (sliding_window - 1) entries.\n",
    "# select_kv_cache uses uniform indices across all layers, so total Phase A\n",
    "# tokens must not exceed this limit when a prefix is used.\n",
    "SLIDING_WINDOW = getattr(text_cfg, 'sliding_window', 4096)\n",
    "SLIDING_CACHE_LIMIT = SLIDING_WINDOW - 1  # observed: 1024-1 = 1023 for Gemma 3\n",
    "\n",
    "print(f\"Exp 03: Hard-Example Semantic Isolation Across Datasets\")\n",
    "print(f\"Scoring: BOS-retained repositioning + token-level prefix matching\")\n",
    "print(f\"N_SAMPLES: {N_SAMPLES} per dataset, HARD_FRAC: {HARD_FRAC}\")\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"DEVICE: {DEVICE}, dtype: {next(model.parameters()).dtype}\")\n",
    "print(f\"GPU memory: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "print(f\"Vocab size: {VOCAB_SIZE}\")\n",
    "print(f\"Sliding window: {SLIDING_WINDOW}, cache limit: {SLIDING_CACHE_LIMIT}\")\n",
    "print(f\"Num layers: {getattr(text_cfg, 'num_hidden_layers', 'N/A')}\")\n",
    "\n",
    "# --- RoPE repositioning helpers ---\n",
    "def build_layer_inv_freqs():\n",
    "    inv_freqs = {}\n",
    "    for lt, params in rope_params.items():\n",
    "        theta = params.get('rope_theta', 10000.0)\n",
    "        dim = text_cfg.head_dim\n",
    "        inv_freq = 1.0 / (theta ** (torch.arange(0, dim, 2, dtype=torch.float32, device=DEVICE) / dim))\n",
    "        inv_freqs[lt] = inv_freq\n",
    "    return inv_freqs\n",
    "\n",
    "LAYER_INV_FREQS = build_layer_inv_freqs()\n",
    "\n",
    "\n",
    "def rotate_half(x):\n",
    "    x1, x2 = x[..., :x.shape[-1]//2], x[..., x.shape[-1]//2:]\n",
    "    return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "\n",
    "def select_kv_cache(cache, indices):\n",
    "    selected = DynamicCache()\n",
    "    idx_tensor = torch.tensor(indices, dtype=torch.long, device=DEVICE)\n",
    "    for i in range(len(cache.layers)):\n",
    "        k = cache.layers[i].keys[:, :, idx_tensor, :]\n",
    "        v = cache.layers[i].values[:, :, idx_tensor, :]\n",
    "        selected.update(k, v, i)\n",
    "    return selected\n",
    "\n",
    "\n",
    "def reposition_kv_cache(cache, old_positions, new_positions, bos_start=0):\n",
    "    delta = new_positions - old_positions\n",
    "    for L in range(len(cache.layers)):\n",
    "        lt = layer_types[L]\n",
    "        inv_freq = LAYER_INV_FREQS[lt]\n",
    "        k = cache.layers[L].keys\n",
    "        doc_keys = k[:, :, bos_start + 1:, :]\n",
    "        freqs = torch.einsum('i,j->ij', delta.float(), inv_freq)\n",
    "        emb = torch.cat([freqs, freqs], dim=-1)\n",
    "        cos_delta = emb.cos().to(k.dtype).unsqueeze(0).unsqueeze(0)\n",
    "        sin_delta = emb.sin().to(k.dtype).unsqueeze(0).unsqueeze(0)\n",
    "        doc_keys_new = doc_keys * cos_delta + rotate_half(doc_keys) * sin_delta\n",
    "        cache.layers[L].keys = torch.cat([\n",
    "            k[:, :, :bos_start + 1, :],\n",
    "            doc_keys_new,\n",
    "        ], dim=2)\n",
    "    return cache\n",
    "\n",
    "\n",
    "def score(doc_text, query_text, answer_text, prefix_token_ids=None):\n",
    "    # BOS-retained repositioning.\n",
    "    doc_ids = tokenizer(doc_text, add_special_tokens=False,\n",
    "                        truncation=True, max_length=1024).input_ids\n",
    "\n",
    "    if prefix_token_ids is not None:\n",
    "        P = len(prefix_token_ids)\n",
    "        NL = len(NEWLINE_IDS)\n",
    "        # Truncate doc so total Phase A tokens fit in sliding window cache.\n",
    "        # Sliding attention layers store only (sliding_window - 1) KV entries;\n",
    "        # select_kv_cache needs uniform indexing across all layers.\n",
    "        max_doc = SLIDING_CACHE_LIMIT - 1 - P - NL  # 1 for BOS\n",
    "        if len(doc_ids) > max_doc:\n",
    "            doc_ids = doc_ids[:max_doc]\n",
    "        D = len(doc_ids)\n",
    "        cond_ids = [BOS_ID] + list(prefix_token_ids) + NEWLINE_IDS + doc_ids\n",
    "        with torch.no_grad():\n",
    "            pa = model(input_ids=torch.tensor([cond_ids], device=DEVICE),\n",
    "                       use_cache=True)\n",
    "        cache = pa.past_key_values\n",
    "        del pa\n",
    "        keep_indices = [0] + list(range(1 + P + NL, len(cond_ids)))\n",
    "        cache = select_kv_cache(cache, keep_indices)\n",
    "        old_pos = torch.arange(1 + P + NL, 1 + P + NL + D, device=DEVICE)\n",
    "        new_pos = torch.arange(1, D + 1, device=DEVICE)\n",
    "        cache = reposition_kv_cache(cache, old_pos, new_pos, bos_start=0)\n",
    "    else:\n",
    "        D = len(doc_ids)\n",
    "        with torch.no_grad():\n",
    "            pa = model(input_ids=torch.tensor([[BOS_ID] + doc_ids], device=DEVICE),\n",
    "                       use_cache=True)\n",
    "        cache = pa.past_key_values\n",
    "        del pa\n",
    "\n",
    "    phase_b_start = D + 1\n",
    "    query_ids = tokenizer(\"\\n\" + query_text + \"\\n\",\n",
    "                          add_special_tokens=False).input_ids\n",
    "    answer_ids = tokenizer(answer_text, add_special_tokens=False,\n",
    "                           truncation=True, max_length=256).input_ids\n",
    "    if not answer_ids:\n",
    "        del cache\n",
    "        return 0.0\n",
    "\n",
    "    pb_ids = query_ids + answer_ids\n",
    "    pos = torch.arange(phase_b_start, phase_b_start + len(pb_ids), device=DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pb = model(\n",
    "            input_ids=torch.tensor([pb_ids], device=DEVICE),\n",
    "            past_key_values=cache,\n",
    "            position_ids=pos.unsqueeze(0),\n",
    "            use_cache=False,\n",
    "        )\n",
    "\n",
    "    n_q = len(query_ids)\n",
    "    logits = pb.logits[0, n_q - 1:n_q - 1 + len(answer_ids), :].float()\n",
    "    targets = torch.tensor(answer_ids, device=DEVICE)\n",
    "    nll = -F.log_softmax(logits, dim=-1).gather(\n",
    "        1, targets.unsqueeze(1)).squeeze(1).mean().item()\n",
    "    del cache, pb\n",
    "    return nll\n",
    "\n",
    "\n",
    "def generate_text(input_text, prompt_text, max_new_tokens=50):\n",
    "    messages = [\n",
    "        {\"role\": \"user\",\n",
    "         \"content\": f\"{prompt_text}\\n\\n{input_text}\"}\n",
    "    ]\n",
    "    chat_text = tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    inputs = tokenizer(chat_text, return_tensors=\"pt\",\n",
    "                       truncation=True, max_length=1024).to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "        )\n",
    "    new_tokens = output_ids[0, inputs['input_ids'].shape[1]:]\n",
    "    raw_text = tokenizer.decode(new_tokens, skip_special_tokens=True)\n",
    "    cleaned = raw_text.strip().split(\"\\n\")[0].strip()\n",
    "    cleaned = cleaned.strip('\"').strip(\"'\").strip()\n",
    "    cleaned = \" \".join(cleaned.split()[:20])\n",
    "    return cleaned\n",
    "\n",
    "\n",
    "def make_prefix(token_ids, Q):\n",
    "    if len(token_ids) >= Q:\n",
    "        return token_ids[:Q]\n",
    "    else:\n",
    "        padded = token_ids * ((Q // max(len(token_ids), 1)) + 1)\n",
    "        return padded[:Q]\n",
    "\n",
    "\n",
    "# LLM surrogate prompts\n",
    "PROMPT_PARAPHRASE = (\n",
    "    \"Rephrase this search query using completely different words but keeping \"\n",
    "    \"the same meaning. Keep it to 5-8 words. Output only the rephrased query.\"\n",
    ")\n",
    "PROMPT_SAME_TOPIC = (\n",
    "    \"Write a question about the same topic as this document but asking for \"\n",
    "    \"DIFFERENT information. Keep it to 5-8 words. Output only the question.\"\n",
    ")\n",
    "PROMPT_EXTRACT = (\n",
    "    \"List the key facts from this document as a brief comma-separated list. \"\n",
    "    \"Output only the fact list, nothing else.\"\n",
    ")\n",
    "PROMPT_QUESTION = (\n",
    "    \"What question does this document answer? Write only the question, \"\n",
    "    \"nothing else. Keep it to 5-10 words.\"\n",
    ")\n",
    "PROMPT_SUMMARIZE = (\n",
    "    \"Summarize this document in one sentence. Output only the summary, nothing else.\"\n",
    ")\n",
    "\n",
    "# Condition definitions\n",
    "COND_NAMES = [\n",
    "    'bare', 'random_tokens', 'repeat_token', 'scrambled_oracle',\n",
    "    'unrelated_query', 'same_topic', 'paraphrase', 'oracle',\n",
    "    'llm_extract', 'llm_question', 'llm_summarize',\n",
    "    'extractor_matched', 'adversarial_matched',\n",
    "]\n",
    "\n",
    "COND_PREFIX_MAP = {\n",
    "    'random_tokens': 'prefix_random_tokens',\n",
    "    'repeat_token': 'prefix_repeat_token',\n",
    "    'scrambled_oracle': 'prefix_scrambled_oracle',\n",
    "    'unrelated_query': 'prefix_unrelated_query',\n",
    "    'same_topic': 'prefix_same_topic',\n",
    "    'paraphrase': 'prefix_paraphrase',\n",
    "    'oracle': 'prefix_oracle',\n",
    "    'llm_extract': 'prefix_llm_extract',\n",
    "    'llm_question': 'prefix_llm_question',\n",
    "    'llm_summarize': 'prefix_llm_summarize',\n",
    "    'extractor_matched': 'prefix_extractor_matched',\n",
    "    'adversarial_matched': 'prefix_adversarial_matched',\n",
    "}\n",
    "\n",
    "PREFIX_KEYS = list(COND_PREFIX_MAP.values())\n",
    "\n",
    "# Fixed-text prefixes\n",
    "EXTRACTOR_TEXT = \"Extract all key data points, facts, entities, and specific attributes from the following text.\"\n",
    "ADVERSARIAL_TEXT = \"The recipe calls for two cups of flour, one cup of sugar, and a pinch of salt mixed together.\"\n",
    "\n",
    "SCORING_KEY = 'bos_retained_token_matched_v03'\n",
    "\n",
    "print(f\"\\nSetup complete. Functions defined: score, generate_text, make_prefix\")\n",
    "print(f\"Conditions: {len(COND_NAMES)} ({len(COND_PREFIX_MAP)} prefixed + bare)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ace9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Load SQuAD 2.0, TriviaQA, HotpotQA (MS MARCO reused from Exp 02)\n",
    "from datasets import load_dataset\n",
    "\n",
    "all_samples = {}  # ds_name -> list of 400 sample dicts\n",
    "\n",
    "# Per-dataset seeds for reproducible sampling\n",
    "DS_SEEDS = {\n",
    "    'squad_v2': SEED + 100,\n",
    "    'triviaqa': SEED + 200,\n",
    "    'hotpotqa': SEED + 300,\n",
    "}\n",
    "\n",
    "# ---- SQuAD 2.0 ----\n",
    "print(\"=\" * 70)\n",
    "print(\"Loading SQuAD 2.0 validation...\")\n",
    "ds_squad = load_dataset(\"rajpurkar/squad_v2\", split=\"validation\")\n",
    "\n",
    "squad_candidates = []\n",
    "for item in ds_squad:\n",
    "    answers = item.get('answers', {})\n",
    "    answer_texts = answers.get('text', [])\n",
    "    if not answer_texts:\n",
    "        continue\n",
    "    passage = item['context']\n",
    "    query = item['question']\n",
    "    answer = answer_texts[0]\n",
    "    wc = count_words(passage)\n",
    "    if 30 <= wc <= 500 and count_words(answer) >= 1:\n",
    "        squad_candidates.append({\n",
    "            'passage': passage, 'query': query, 'answer': answer,\n",
    "            'word_count': wc,\n",
    "        })\n",
    "\n",
    "print(f\"SQuAD 2.0 candidates: {len(squad_candidates)}\")\n",
    "np.random.seed(DS_SEEDS['squad_v2'])\n",
    "indices = np.random.permutation(len(squad_candidates))[:N_SAMPLES]\n",
    "all_samples['squad_v2'] = [squad_candidates[i] for i in indices]\n",
    "del ds_squad, squad_candidates\n",
    "gc.collect()\n",
    "\n",
    "# ---- TriviaQA ----\n",
    "print(\"\\nLoading TriviaQA rc.wikipedia validation...\")\n",
    "ds_trivia = load_dataset(\"mandarjoshi/trivia_qa\", \"rc.wikipedia\", split=\"validation\")\n",
    "\n",
    "trivia_candidates = []\n",
    "for item in ds_trivia:\n",
    "    entity_pages = item.get('entity_pages', {})\n",
    "    wiki_contexts = entity_pages.get('wiki_context', [])\n",
    "    if not wiki_contexts or not wiki_contexts[0]:\n",
    "        continue\n",
    "    # Take first 500 words of first wiki context\n",
    "    words = wiki_contexts[0].split()[:500]\n",
    "    passage = ' '.join(words)\n",
    "    query = item['question']\n",
    "    answer_val = item['answer']['value']\n",
    "    aliases = item['answer'].get('aliases', [])\n",
    "\n",
    "    # Check if answer or any alias appears in passage (case-insensitive)\n",
    "    passage_lower = passage.lower()\n",
    "    found = answer_val.lower() in passage_lower\n",
    "    if not found:\n",
    "        for alias in aliases:\n",
    "            if alias.lower() in passage_lower:\n",
    "                found = True\n",
    "                break\n",
    "    if not found:\n",
    "        continue\n",
    "\n",
    "    wc = count_words(passage)\n",
    "    if 30 <= wc <= 500 and count_words(answer_val) >= 1:\n",
    "        trivia_candidates.append({\n",
    "            'passage': passage, 'query': query, 'answer': answer_val,\n",
    "            'word_count': wc,\n",
    "        })\n",
    "\n",
    "print(f\"TriviaQA candidates: {len(trivia_candidates)}\")\n",
    "np.random.seed(DS_SEEDS['triviaqa'])\n",
    "indices = np.random.permutation(len(trivia_candidates))[:N_SAMPLES]\n",
    "all_samples['triviaqa'] = [trivia_candidates[i] for i in indices]\n",
    "del ds_trivia, trivia_candidates\n",
    "gc.collect()\n",
    "\n",
    "# ---- HotpotQA ----\n",
    "print(\"\\nLoading HotpotQA distractor validation...\")\n",
    "ds_hotpot = load_dataset(\"hotpotqa/hotpot_qa\", \"distractor\", split=\"validation\")\n",
    "\n",
    "hotpot_candidates = []\n",
    "for item in ds_hotpot:\n",
    "    context = item.get('context', {})\n",
    "    sf = item.get('supporting_facts', {})\n",
    "    ctx_titles = context.get('title', [])\n",
    "    ctx_sentences = context.get('sentences', [])\n",
    "    sf_titles = sf.get('title', [])\n",
    "    sf_sent_ids = sf.get('sent_id', [])\n",
    "\n",
    "    # Build title -> sentences mapping\n",
    "    title_to_sents = {}\n",
    "    for title, sents in zip(ctx_titles, ctx_sentences):\n",
    "        title_to_sents[title] = sents\n",
    "\n",
    "    # Extract supporting fact sentences\n",
    "    passage_parts = []\n",
    "    for title, sid in zip(sf_titles, sf_sent_ids):\n",
    "        if title in title_to_sents and sid < len(title_to_sents[title]):\n",
    "            passage_parts.append(title_to_sents[title][sid])\n",
    "\n",
    "    if not passage_parts:\n",
    "        continue\n",
    "    passage = ' '.join(passage_parts)\n",
    "    query = item['question']\n",
    "    answer = item['answer']\n",
    "    wc = count_words(passage)\n",
    "    if 30 <= wc <= 500 and count_words(answer) >= 1:\n",
    "        hotpot_candidates.append({\n",
    "            'passage': passage, 'query': query, 'answer': answer,\n",
    "            'word_count': wc,\n",
    "        })\n",
    "\n",
    "print(f\"HotpotQA candidates: {len(hotpot_candidates)}\")\n",
    "np.random.seed(DS_SEEDS['hotpotqa'])\n",
    "indices = np.random.permutation(len(hotpot_candidates))[:N_SAMPLES]\n",
    "all_samples['hotpotqa'] = [hotpot_candidates[i] for i in indices]\n",
    "del ds_hotpot, hotpot_candidates\n",
    "gc.collect()\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Dataset loading summary:\")\n",
    "for ds_name in NEW_DATASETS:\n",
    "    samps = all_samples[ds_name]\n",
    "    print(f\"\\n  {ds_name}: {len(samps)} samples\")\n",
    "    print(f\"    Mean passage words: {np.mean([s['word_count'] for s in samps]):.0f}\")\n",
    "    print(f\"    Mean answer words: {np.mean([count_words(s['answer']) for s in samps]):.0f}\")\n",
    "    print(f\"    Mean query words: {np.mean([count_words(s['query']) for s in samps]):.0f}\")\n",
    "    print(f\"    Example query: {samps[0]['query'][:70]}...\")\n",
    "    print(f\"    Example answer: {samps[0]['answer'][:70]}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f3e953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Reuse MS MARCO results from Exp 02\n",
    "print(\"=\" * 70)\n",
    "print(\"Loading MS MARCO results from Exp 02\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "assert EXP02_DIR.exists(), f\"Exp 02 results not found at {EXP02_DIR}\"\n",
    "exp02_ckpt = json.loads((EXP02_DIR / \"checkpoint.json\").read_text())\n",
    "assert exp02_ckpt.get('scoring') == 'bos_retained_token_matched_v02', \\\n",
    "    f\"Unexpected scoring key: {exp02_ckpt.get('scoring')}\"\n",
    "exp02_results = exp02_ckpt['results']\n",
    "assert len(exp02_results) == N_SAMPLES, \\\n",
    "    f\"Expected {N_SAMPLES} results, got {len(exp02_results)}\"\n",
    "\n",
    "# Extract bare NLLs and select hard 40%\n",
    "msmarco_bare = np.array([r['nll_bare'] for r in exp02_results])\n",
    "N_HARD = int(N_SAMPLES * HARD_FRAC)\n",
    "sorted_idx = np.argsort(msmarco_bare)[::-1]  # descending (hardest first)\n",
    "msmarco_hard_idx = np.sort(sorted_idx[:N_HARD])  # restore original order\n",
    "\n",
    "print(f\"MS MARCO: {N_SAMPLES} total, selecting top {HARD_FRAC*100:.0f}% = {N_HARD} hard samples\")\n",
    "print(f\"Bare NLL range: {msmarco_bare.min():.4f} - {msmarco_bare.max():.4f}\")\n",
    "print(f\"Hard cutoff (min NLL in hard set): {msmarco_bare[msmarco_hard_idx].min():.4f}\")\n",
    "print(f\"Hard samples mean bare NLL: {msmarco_bare[msmarco_hard_idx].mean():.4f}\")\n",
    "\n",
    "# Build hard_nlls for MS MARCO\n",
    "hard_nlls = {}  # ds_name -> {cond_name: np.array}\n",
    "hard_metadata = {}  # ds_name -> {n_total, n_hard, ...}\n",
    "\n",
    "hard_nlls['ms_marco'] = {}\n",
    "for cond in COND_NAMES:\n",
    "    arr = np.array([exp02_results[i][f'nll_{cond}'] for i in msmarco_hard_idx])\n",
    "    hard_nlls['ms_marco'][cond] = arr\n",
    "\n",
    "hard_metadata['ms_marco'] = {\n",
    "    'n_total': N_SAMPLES,\n",
    "    'n_hard': N_HARD,\n",
    "    'source': 'exp02_reuse',\n",
    "    'mean_passage_words': float(np.mean([exp02_results[i]['passage_words']\n",
    "                                          for i in msmarco_hard_idx])),\n",
    "    'mean_query_tokens': float(np.mean([exp02_results[i]['Q']\n",
    "                                         for i in msmarco_hard_idx])),\n",
    "    'mean_answer_words': float(np.mean([count_words(exp02_results[i]['answer'])\n",
    "                                         for i in msmarco_hard_idx])),\n",
    "}\n",
    "\n",
    "# Quick sanity check: bare should match\n",
    "assert np.allclose(hard_nlls['ms_marco']['bare'],\n",
    "                   msmarco_bare[msmarco_hard_idx]), \"Bare NLL mismatch\"\n",
    "\n",
    "print(f\"\\nMS MARCO hard samples loaded:\")\n",
    "print(f\"  N_hard: {N_HARD}\")\n",
    "print(f\"  Mean passage words: {hard_metadata['ms_marco']['mean_passage_words']:.0f}\")\n",
    "print(f\"  Mean query tokens: {hard_metadata['ms_marco']['mean_query_tokens']:.0f}\")\n",
    "print(f\"  Conditions: {len(COND_NAMES)}\")\n",
    "\n",
    "# Show condition summary for MS MARCO hard set\n",
    "bare_h = hard_nlls['ms_marco']['bare']\n",
    "print(f\"\\n  {'Condition':<24} {'NLL':>8} {'d vs bare':>10} {'sem delta d':>12}\")\n",
    "print(f\"  {'-'*58}\")\n",
    "for cond in COND_NAMES:\n",
    "    nlls_h = hard_nlls['ms_marco'][cond]\n",
    "    mean_nll = nlls_h.mean()\n",
    "    if cond == 'bare':\n",
    "        print(f\"  {cond:<24} {mean_nll:>8.4f} {'--':>10} {'--':>12}\")\n",
    "    else:\n",
    "        d_bare = cohens_d(bare_h - nlls_h)\n",
    "        if cond == 'random_tokens':\n",
    "            print(f\"  {cond:<24} {mean_nll:>8.4f} {d_bare:>+10.3f} {'(ref)':>12}\")\n",
    "        else:\n",
    "            sem_delta = hard_nlls['ms_marco']['random_tokens'] - nlls_h\n",
    "            d_sem = cohens_d(sem_delta)\n",
    "            print(f\"  {cond:<24} {mean_nll:>8.4f} {d_bare:>+10.3f} {d_sem:>+12.3f}\")\n",
    "\n",
    "del exp02_ckpt, exp02_results\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10cce544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Bare NLL scoring for SQuAD, TriviaQA, HotpotQA — select hard 40%\n",
    "print(\"=\" * 70)\n",
    "print(\"BARE SCORING — 3 new datasets x 400 samples\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "hard_indices = {}   # ds_name -> np.array of indices into all_samples\n",
    "hard_samples = {}   # ds_name -> list of hard sample dicts\n",
    "\n",
    "for ds_name in NEW_DATASETS:\n",
    "    print(f\"\\n--- {ds_name} ({N_SAMPLES} samples) ---\")\n",
    "    samples = all_samples[ds_name]\n",
    "    bare_ckpt_path = RESULTS_DIR / f\"bare_{ds_name}.json\"\n",
    "\n",
    "    bare_nlls = []\n",
    "    start_idx = 0\n",
    "\n",
    "    # Try to resume from checkpoint\n",
    "    if bare_ckpt_path.exists():\n",
    "        ckpt = json.loads(bare_ckpt_path.read_text())\n",
    "        if (ckpt.get('n_total') == N_SAMPLES and\n",
    "            ckpt.get('scoring') == SCORING_KEY and\n",
    "            ckpt.get('dataset') == ds_name):\n",
    "            saved_queries = ckpt.get('queries_first50', [])\n",
    "            current_queries = [s['query'][:50] for s in samples[:len(saved_queries)]]\n",
    "            if saved_queries == current_queries:\n",
    "                bare_nlls = ckpt['bare_nlls']\n",
    "                start_idx = len(bare_nlls)\n",
    "                print(f\"  Resuming from checkpoint: {start_idx}/{N_SAMPLES}\")\n",
    "\n",
    "    if start_idx < N_SAMPLES:\n",
    "        t0 = time.time()\n",
    "        for i in tqdm(range(start_idx, N_SAMPLES), initial=start_idx,\n",
    "                      total=N_SAMPLES, desc=f\"Bare {ds_name}\"):\n",
    "            s = samples[i]\n",
    "            nll = score(s['passage'], s['query'], s['answer'])\n",
    "            bare_nlls.append(nll)\n",
    "\n",
    "            if (i + 1) % 20 == 0 or i == N_SAMPLES - 1:\n",
    "                ckpt = {\n",
    "                    'dataset': ds_name,\n",
    "                    'n_total': N_SAMPLES,\n",
    "                    'scoring': SCORING_KEY,\n",
    "                    'bare_nlls': bare_nlls,\n",
    "                    'queries_first50': [s['query'][:50]\n",
    "                                        for s in samples[:len(bare_nlls)]],\n",
    "                    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                }\n",
    "                bare_ckpt_path.write_text(json.dumps(ckpt))\n",
    "                elapsed = time.time() - t0\n",
    "                done = i - start_idx + 1\n",
    "                eta = (N_SAMPLES - i - 1) * elapsed / done if done > 0 else 0\n",
    "                tqdm.write(f\"  Checkpoint {i+1}/{N_SAMPLES} | \"\n",
    "                           f\"{elapsed/60:.1f}m | ETA {eta/60:.1f}m\")\n",
    "\n",
    "        elapsed = time.time() - t0\n",
    "        print(f\"  Bare scoring complete in {elapsed/60:.1f} min\")\n",
    "\n",
    "    bare_arr = np.array(bare_nlls)\n",
    "\n",
    "    # Select hard 40%\n",
    "    N_HARD = int(N_SAMPLES * HARD_FRAC)\n",
    "    sorted_idx = np.argsort(bare_arr)[::-1]\n",
    "    h_idx = np.sort(sorted_idx[:N_HARD])\n",
    "    hard_indices[ds_name] = h_idx\n",
    "\n",
    "    # Build hard_samples with bare NLL attached\n",
    "    hs = []\n",
    "    for idx in h_idx:\n",
    "        s = dict(samples[idx])  # copy\n",
    "        s['nll_bare'] = bare_arr[idx]\n",
    "        s['original_idx'] = int(idx)\n",
    "        hs.append(s)\n",
    "    hard_samples[ds_name] = hs\n",
    "\n",
    "    # Initialize hard_nlls with bare\n",
    "    hard_nlls[ds_name] = {'bare': bare_arr[h_idx]}\n",
    "\n",
    "    hard_metadata[ds_name] = {\n",
    "        'n_total': N_SAMPLES,\n",
    "        'n_hard': N_HARD,\n",
    "        'source': 'scored',\n",
    "        'mean_passage_words': float(np.mean([s['word_count'] for s in hs])),\n",
    "        'mean_query_tokens': 0.0,  # filled in after tokenization\n",
    "        'mean_answer_words': float(np.mean([count_words(s['answer']) for s in hs])),\n",
    "    }\n",
    "\n",
    "    print(f\"  Hard cutoff: {bare_arr[h_idx].min():.4f}\")\n",
    "    print(f\"  Hard mean bare NLL: {bare_arr[h_idx].mean():.4f}\")\n",
    "    print(f\"  Easy mean bare NLL: {np.delete(bare_arr, h_idx).mean():.4f}\")\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Hard sample selection complete:\")\n",
    "for ds_name in NEW_DATASETS:\n",
    "    n_h = len(hard_samples[ds_name])\n",
    "    print(f\"  {ds_name}: {n_h} hard samples (mean bare NLL: \"\n",
    "          f\"{hard_nlls[ds_name]['bare'].mean():.4f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16828ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Generate LLM surrogates for hard samples of 3 new datasets\n",
    "print(\"=\" * 70)\n",
    "print(\"LLM SURROGATE GENERATION — 3 datasets x ~160 hard samples x 5 surrogates\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "surrogates_all = {}  # ds_name -> list of surrogates for hard samples\n",
    "\n",
    "for ds_name in NEW_DATASETS:\n",
    "    hs = hard_samples[ds_name]\n",
    "    n_hard = len(hs)\n",
    "    surr_path = RESULTS_DIR / f\"surrogates_{ds_name}.json\"\n",
    "\n",
    "    print(f\"\\n--- {ds_name} ({n_hard} hard samples) ---\")\n",
    "\n",
    "    surrogates = []\n",
    "    start_idx = 0\n",
    "\n",
    "    if surr_path.exists():\n",
    "        surr_ckpt = json.loads(surr_path.read_text())\n",
    "        if (surr_ckpt.get('dataset') == ds_name and\n",
    "            surr_ckpt.get('n_hard') == n_hard):\n",
    "            saved_queries = [s.get('query', '')[:50]\n",
    "                             for s in surr_ckpt.get('surrogates', [])]\n",
    "            current_queries = [s['query'][:50] for s in hs[:len(saved_queries)]]\n",
    "            if saved_queries == current_queries:\n",
    "                surrogates = surr_ckpt['surrogates']\n",
    "                start_idx = len(surrogates)\n",
    "                print(f\"  Resuming from {start_idx}/{n_hard}\")\n",
    "\n",
    "    if start_idx < n_hard:\n",
    "        t0 = time.time()\n",
    "        ds_seed_offset = DS_SEEDS[ds_name]\n",
    "\n",
    "        for i in tqdm(range(start_idx, n_hard), initial=start_idx,\n",
    "                      total=n_hard, desc=f\"Gen {ds_name}\"):\n",
    "            s = hs[i]\n",
    "            entry = {'query': s['query']}\n",
    "\n",
    "            doc_words = s['passage'].split()[:200]\n",
    "            doc_input = f\"Document:\\n{' '.join(doc_words)}\"\n",
    "\n",
    "            torch.manual_seed(ds_seed_offset + i * 10)\n",
    "            entry['paraphrase'] = generate_text(\n",
    "                f\"Query: {s['query']}\", PROMPT_PARAPHRASE\n",
    "            )\n",
    "\n",
    "            torch.manual_seed(ds_seed_offset + i * 10 + 1)\n",
    "            entry['same_topic'] = generate_text(doc_input, PROMPT_SAME_TOPIC)\n",
    "\n",
    "            torch.manual_seed(ds_seed_offset + i * 10 + 2)\n",
    "            entry['llm_extract'] = generate_text(doc_input, PROMPT_EXTRACT)\n",
    "\n",
    "            torch.manual_seed(ds_seed_offset + i * 10 + 3)\n",
    "            entry['llm_question'] = generate_text(doc_input, PROMPT_QUESTION)\n",
    "\n",
    "            torch.manual_seed(ds_seed_offset + i * 10 + 4)\n",
    "            entry['llm_summarize'] = generate_text(doc_input, PROMPT_SUMMARIZE)\n",
    "\n",
    "            surrogates.append(entry)\n",
    "\n",
    "            if (i + 1) % 20 == 0 or i == n_hard - 1:\n",
    "                surr_ckpt = {\n",
    "                    'dataset': ds_name,\n",
    "                    'n_hard': n_hard,\n",
    "                    'surrogates': surrogates,\n",
    "                    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                }\n",
    "                surr_path.write_text(json.dumps(surr_ckpt))\n",
    "                elapsed = time.time() - t0\n",
    "                done = i - start_idx + 1\n",
    "                eta = (n_hard - i - 1) * elapsed / done if done > 0 else 0\n",
    "                tqdm.write(f\"  Gen checkpoint {i+1}/{n_hard} | \"\n",
    "                           f\"{elapsed/60:.1f}m | ETA {eta/60:.1f}m\")\n",
    "\n",
    "        elapsed = time.time() - t0\n",
    "        print(f\"  Generation complete in {elapsed/60:.1f} min\")\n",
    "    else:\n",
    "        print(f\"  Loaded {len(surrogates)} cached surrogates\")\n",
    "\n",
    "    surrogates_all[ds_name] = surrogates\n",
    "\n",
    "    # Show examples\n",
    "    for j in range(min(2, n_hard)):\n",
    "        print(f\"\\n  Sample {j}: query='{surrogates[j]['query'][:50]}'\")\n",
    "        for key in ['paraphrase', 'same_topic', 'llm_extract']:\n",
    "            print(f\"    {key:<15}: {surrogates[j].get(key, 'N/A')[:50]}\")\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1b5fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Build per-sample token-level prefix IDs for hard samples + validation\n",
    "print(\"=\" * 70)\n",
    "print(\"PREFIX CONSTRUCTION + VALIDATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Pre-tokenize fixed texts\n",
    "extractor_ids = tokenizer(EXTRACTOR_TEXT, add_special_tokens=False).input_ids\n",
    "adversarial_ids = tokenizer(ADVERSARIAL_TEXT, add_special_tokens=False).input_ids\n",
    "special_ids = set(tokenizer.all_special_ids)\n",
    "\n",
    "for ds_name in NEW_DATASETS:\n",
    "    hs = hard_samples[ds_name]\n",
    "    surrs = surrogates_all[ds_name]\n",
    "    n_hard = len(hs)\n",
    "\n",
    "    print(f\"\\n--- {ds_name} ({n_hard} hard samples) ---\")\n",
    "\n",
    "    pyrandom.seed(DS_SEEDS[ds_name] + 200)\n",
    "    np.random.seed(DS_SEEDS[ds_name] + 300)\n",
    "\n",
    "    for i, s in enumerate(hs):\n",
    "        surr = surrs[i]\n",
    "        q_ids = tokenizer(s['query'], add_special_tokens=False).input_ids\n",
    "        Q = len(q_ids)\n",
    "        s['Q'] = Q\n",
    "\n",
    "        # 1. oracle: exact query token IDs\n",
    "        s['prefix_oracle'] = q_ids\n",
    "\n",
    "        # 2. random_tokens: random vocab IDs (excluding special)\n",
    "        rand_ids = []\n",
    "        while len(rand_ids) < Q:\n",
    "            tid = np.random.randint(0, VOCAB_SIZE)\n",
    "            if tid not in special_ids:\n",
    "                rand_ids.append(int(tid))\n",
    "        s['prefix_random_tokens'] = rand_ids[:Q]\n",
    "\n",
    "        # 3. repeat_token\n",
    "        s['prefix_repeat_token'] = [1000] * Q\n",
    "\n",
    "        # 4. scrambled_oracle\n",
    "        shuffled = list(q_ids)\n",
    "        pyrandom.shuffle(shuffled)\n",
    "        s['prefix_scrambled_oracle'] = shuffled\n",
    "\n",
    "        # 5. unrelated_query: other hard sample's query\n",
    "        other_idx = (i + n_hard // 2) % n_hard\n",
    "        other_q_ids = tokenizer(hs[other_idx]['query'],\n",
    "                                add_special_tokens=False).input_ids\n",
    "        s['prefix_unrelated_query'] = make_prefix(other_q_ids, Q)\n",
    "\n",
    "        # 6. same_topic\n",
    "        topic_ids = tokenizer(surr['same_topic'],\n",
    "                              add_special_tokens=False).input_ids\n",
    "        s['prefix_same_topic'] = make_prefix(topic_ids, Q)\n",
    "\n",
    "        # 7. paraphrase\n",
    "        para_ids = tokenizer(surr['paraphrase'],\n",
    "                             add_special_tokens=False).input_ids\n",
    "        s['prefix_paraphrase'] = make_prefix(para_ids, Q)\n",
    "\n",
    "        # 8. llm_extract\n",
    "        extract_ids = tokenizer(surr['llm_extract'],\n",
    "                                add_special_tokens=False).input_ids\n",
    "        s['prefix_llm_extract'] = make_prefix(extract_ids, Q)\n",
    "\n",
    "        # 9. llm_question\n",
    "        question_ids = tokenizer(surr['llm_question'],\n",
    "                                 add_special_tokens=False).input_ids\n",
    "        s['prefix_llm_question'] = make_prefix(question_ids, Q)\n",
    "\n",
    "        # 10. llm_summarize\n",
    "        summarize_ids = tokenizer(surr['llm_summarize'],\n",
    "                                  add_special_tokens=False).input_ids\n",
    "        s['prefix_llm_summarize'] = make_prefix(summarize_ids, Q)\n",
    "\n",
    "        # 11. extractor_matched\n",
    "        s['prefix_extractor_matched'] = make_prefix(extractor_ids, Q)\n",
    "\n",
    "        # 12. adversarial_matched\n",
    "        s['prefix_adversarial_matched'] = make_prefix(adversarial_ids, Q)\n",
    "\n",
    "    # Update metadata with query token stats\n",
    "    q_lens = [s['Q'] for s in hs]\n",
    "    hard_metadata[ds_name]['mean_query_tokens'] = float(np.mean(q_lens))\n",
    "    print(f\"  Q tokens — mean: {np.mean(q_lens):.1f}, \"\n",
    "          f\"median: {np.median(q_lens):.0f}, \"\n",
    "          f\"min: {np.min(q_lens)}, max: {np.max(q_lens)}\")\n",
    "\n",
    "    # Verify all prefixes have exactly Q tokens\n",
    "    errors = 0\n",
    "    for i, s in enumerate(hs):\n",
    "        Q = s['Q']\n",
    "        for key in PREFIX_KEYS:\n",
    "            if len(s[key]) != Q:\n",
    "                print(f\"  ERROR: Sample {i} {key}: len={len(s[key])} != Q={Q}\")\n",
    "                errors += 1\n",
    "    assert errors == 0, f\"{ds_name}: {errors} prefix length mismatches!\"\n",
    "    print(f\"  All {len(PREFIX_KEYS)} prefix types verified for {n_hard} samples\")\n",
    "\n",
    "# ================================================================\n",
    "# VALIDATION TESTS\n",
    "# ================================================================\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"VALIDATION TESTS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Test 1: Bare two-phase matches single-pass\n",
    "print(\"\\n--- Test 1: Bare two-phase matches single-pass ---\")\n",
    "doc_text_t = \"The cat sat on the mat near the door of the house by the lake\"\n",
    "query_text_t = \"Where did the cat sit?\"\n",
    "answer_text_t = \"on the mat\"\n",
    "doc_ids_t = tokenizer(doc_text_t, add_special_tokens=False).input_ids\n",
    "D_t = len(doc_ids_t)\n",
    "query_ids_t = tokenizer(\"\\n\" + query_text_t + \"\\n\", add_special_tokens=False).input_ids\n",
    "answer_ids_t = tokenizer(answer_text_t, add_special_tokens=False).input_ids\n",
    "\n",
    "full_ids = [BOS_ID] + doc_ids_t + query_ids_t + answer_ids_t\n",
    "with torch.no_grad():\n",
    "    out_full = model(input_ids=torch.tensor([full_ids], device=DEVICE))\n",
    "n_ctx = 1 + D_t + len(query_ids_t)\n",
    "logits_full = out_full.logits[0, n_ctx - 1:n_ctx - 1 + len(answer_ids_t), :].float()\n",
    "targets_t = torch.tensor(answer_ids_t, device=DEVICE)\n",
    "nll_single = -F.log_softmax(logits_full, dim=-1).gather(\n",
    "    1, targets_t.unsqueeze(1)).squeeze(1).mean().item()\n",
    "del out_full\n",
    "\n",
    "nll_bare = score(doc_text_t, query_text_t, answer_text_t)\n",
    "diff_pct = abs(nll_single - nll_bare) / nll_single * 100\n",
    "print(f\"  Single-pass NLL: {nll_single:.6f}\")\n",
    "print(f\"  Two-phase bare:  {nll_bare:.6f} (diff: {diff_pct:.2f}%)\")\n",
    "assert diff_pct < 1.0, f\"Bare doesn't match single-pass: {diff_pct}%\"\n",
    "print(f\"  PASSED\")\n",
    "\n",
    "# Test 2: Prefixed scoring on first hard sample from first new dataset\n",
    "print(\"\\n--- Test 2: Prefixed scoring runs correctly ---\")\n",
    "test_ds = NEW_DATASETS[0]\n",
    "ts = hard_samples[test_ds][0]\n",
    "nll_b = score(ts['passage'], ts['query'], ts['answer'])\n",
    "nll_o = score(ts['passage'], ts['query'], ts['answer'],\n",
    "              prefix_token_ids=ts['prefix_oracle'])\n",
    "nll_r = score(ts['passage'], ts['query'], ts['answer'],\n",
    "              prefix_token_ids=ts['prefix_random_tokens'])\n",
    "print(f\"  [{test_ds}] Bare:          {nll_b:.4f}\")\n",
    "print(f\"  [{test_ds}] Oracle:        {nll_o:.4f}  delta={nll_b - nll_o:+.4f}\")\n",
    "print(f\"  [{test_ds}] Random tokens: {nll_r:.4f}  delta={nll_b - nll_r:+.4f}\")\n",
    "assert 0 < nll_b < 20 and 0 < nll_o < 20 and 0 < nll_r < 20\n",
    "print(\"  PASSED\")\n",
    "\n",
    "# Test 3: Token-matching invariant\n",
    "print(\"\\n--- Test 3: Token-matching invariant ---\")\n",
    "Q = ts['Q']\n",
    "for key in PREFIX_KEYS:\n",
    "    assert len(ts[key]) == Q, f\"{key}: {len(ts[key])} != Q={Q}\"\n",
    "print(f\"  All 12 prefixed conditions have Q={Q} tokens\")\n",
    "print(\"  PASSED\")\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print(\"\\nALL VALIDATION TESTS PASSED\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b700e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Full 12-condition scoring for hard samples of 3 new datasets\n",
    "print(\"=\" * 70)\n",
    "print(\"FULL SCORING — 12 prefixed conditions x ~160 hard samples x 3 datasets\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for ds_name in NEW_DATASETS:\n",
    "    hs = hard_samples[ds_name]\n",
    "    n_hard = len(hs)\n",
    "    ckpt_path = RESULTS_DIR / f\"checkpoint_{ds_name}.json\"\n",
    "\n",
    "    print(f\"\\n--- {ds_name} ({n_hard} hard samples x 12 conditions) ---\")\n",
    "\n",
    "    ds_results = []\n",
    "    start_idx = 0\n",
    "\n",
    "    if ckpt_path.exists():\n",
    "        ckpt = json.loads(ckpt_path.read_text())\n",
    "        if (ckpt.get('dataset') == ds_name and\n",
    "            ckpt.get('scoring') == SCORING_KEY and\n",
    "            ckpt.get('n_hard') == n_hard):\n",
    "            saved_queries = [r['query'][:50] for r in ckpt.get('results', [])]\n",
    "            current_queries = [s['query'][:50] for s in hs[:len(saved_queries)]]\n",
    "            if saved_queries == current_queries:\n",
    "                ds_results = ckpt['results']\n",
    "                start_idx = len(ds_results)\n",
    "                print(f\"  Resuming from checkpoint: {start_idx}/{n_hard}\")\n",
    "\n",
    "    if start_idx < n_hard:\n",
    "        t0 = time.time()\n",
    "\n",
    "        for i in tqdm(range(start_idx, n_hard), initial=start_idx,\n",
    "                      total=n_hard, desc=f\"Score {ds_name}\"):\n",
    "            s = hs[i]\n",
    "            result = {\n",
    "                'query': s['query'],\n",
    "                'answer': s['answer'],\n",
    "                'passage_words': s['word_count'],\n",
    "                'Q': s['Q'],\n",
    "                'nll_bare': float(s['nll_bare']),\n",
    "            }\n",
    "\n",
    "            for cond_name, prefix_key in COND_PREFIX_MAP.items():\n",
    "                result[f'nll_{cond_name}'] = score(\n",
    "                    s['passage'], s['query'], s['answer'],\n",
    "                    prefix_token_ids=s[prefix_key]\n",
    "                )\n",
    "\n",
    "            ds_results.append(result)\n",
    "\n",
    "            if (i + 1) % 20 == 0 or i == n_hard - 1:\n",
    "                ckpt = {\n",
    "                    'dataset': ds_name,\n",
    "                    'n_hard': n_hard,\n",
    "                    'scoring': SCORING_KEY,\n",
    "                    'results': ds_results,\n",
    "                    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                }\n",
    "                ckpt_path.write_text(json.dumps(ckpt))\n",
    "                elapsed = time.time() - t0\n",
    "                done = i - start_idx + 1\n",
    "                eta = (n_hard - i - 1) * elapsed / done if done > 0 else 0\n",
    "                tqdm.write(f\"  Checkpoint {i+1}/{n_hard} | \"\n",
    "                           f\"{elapsed/60:.1f}m | ETA {eta/60:.1f}m\")\n",
    "\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        elapsed = time.time() - t0\n",
    "        print(f\"  Scoring complete in {elapsed/60:.1f} min\")\n",
    "    else:\n",
    "        print(f\"  Loaded {len(ds_results)} cached results\")\n",
    "\n",
    "    # Populate hard_nlls for this dataset\n",
    "    for cond in COND_NAMES:\n",
    "        hard_nlls[ds_name][cond] = np.array(\n",
    "            [r[f'nll_{cond}'] for r in ds_results])\n",
    "\n",
    "    # Sanity check: bare NLLs should match\n",
    "    bare_from_results = np.array([r['nll_bare'] for r in ds_results])\n",
    "    bare_from_cell5 = hard_nlls[ds_name]['bare']\n",
    "    assert np.allclose(bare_from_results, bare_from_cell5, atol=0.01), \\\n",
    "        f\"{ds_name}: bare NLL mismatch between Cell 5 and Cell 8\"\n",
    "    # Use the fresh bare from Cell 5 (scored directly)\n",
    "    hard_nlls[ds_name]['bare'] = bare_from_cell5\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"All scoring complete. Datasets in hard_nlls:\")\n",
    "for ds_name in DATASET_NAMES:\n",
    "    n = len(hard_nlls[ds_name]['bare'])\n",
    "    print(f\"  {ds_name}: {n} hard samples x {len(COND_NAMES)} conditions\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f515e828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Per-dataset analysis — condition tables, semantic gradient, LLM vs generic\n",
    "print(\"=\" * 70)\n",
    "print(\"PER-DATASET ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Relevance ordering for gradient test (excluding random_tokens = reference)\n",
    "GRADIENT_CONDS = [\n",
    "    ('scrambled_oracle', 1),\n",
    "    ('unrelated_query', 2),\n",
    "    ('same_topic', 3),\n",
    "    ('paraphrase', 4),\n",
    "    ('oracle', 5),\n",
    "]\n",
    "\n",
    "per_dataset_analysis = {}\n",
    "\n",
    "for ds_name in DATASET_NAMES:\n",
    "    nlls = hard_nlls[ds_name]\n",
    "    n_hard = len(nlls['bare'])\n",
    "    bare = nlls['bare']\n",
    "    random_base = nlls['random_tokens']\n",
    "\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"  {ds_name.upper()} — {n_hard} hard samples\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "    analysis = {}\n",
    "\n",
    "    # ---- Part A: Condition table ----\n",
    "    print(f\"\\n  {'Cond':<24} {'NLL':>7} {'d bare':>8} {'sem d':>8} \"\n",
    "          f\"{'win%':>6} {'p':>10} {'sig':>4}\")\n",
    "    print(f\"  {'-'*72}\")\n",
    "\n",
    "    for cond in COND_NAMES:\n",
    "        c_nlls = nlls[cond]\n",
    "        mean_nll = c_nlls.mean()\n",
    "\n",
    "        if cond == 'bare':\n",
    "            print(f\"  {cond:<24} {mean_nll:>7.3f} {'--':>8} {'--':>8} \"\n",
    "                  f\"{'--':>6} {'--':>10} {'--':>4}\")\n",
    "            analysis[cond] = {'mean_nll': float(mean_nll)}\n",
    "            continue\n",
    "\n",
    "        diff_bare = bare - c_nlls\n",
    "        d_bare = cohens_d(diff_bare)\n",
    "        _, p_bare = stats.ttest_1samp(diff_bare, 0)\n",
    "\n",
    "        if cond == 'random_tokens':\n",
    "            win_pct = 100 * np.mean(diff_bare > 0)\n",
    "            sig = ('***' if p_bare < 0.001 else '**' if p_bare < 0.01\n",
    "                   else '*' if p_bare < 0.05 else 'ns')\n",
    "            print(f\"  {cond:<24} {mean_nll:>7.3f} {d_bare:>+8.3f} {'(ref)':>8} \"\n",
    "                  f\"{win_pct:>5.1f}% {p_bare:>10.2e} {sig:>4}\")\n",
    "            analysis[cond] = {\n",
    "                'mean_nll': float(mean_nll), 'd_bare': float(d_bare),\n",
    "                'semantic_delta_d': 0.0, 'p_bare': float(p_bare),\n",
    "            }\n",
    "        else:\n",
    "            sem_delta = random_base - c_nlls\n",
    "            d_sem = cohens_d(sem_delta)\n",
    "            _, p_sem = stats.ttest_1samp(sem_delta, 0)\n",
    "            win_pct = 100 * np.mean(sem_delta > 0)\n",
    "            sig = ('***' if p_sem < 0.001 else '**' if p_sem < 0.01\n",
    "                   else '*' if p_sem < 0.05 else 'ns')\n",
    "            print(f\"  {cond:<24} {mean_nll:>7.3f} {d_bare:>+8.3f} {d_sem:>+8.3f} \"\n",
    "                  f\"{win_pct:>5.1f}% {p_sem:>10.2e} {sig:>4}\")\n",
    "            analysis[cond] = {\n",
    "                'mean_nll': float(mean_nll), 'd_bare': float(d_bare),\n",
    "                'semantic_delta_d': float(d_sem), 'p_semantic': float(p_sem),\n",
    "            }\n",
    "\n",
    "    # ---- Part B: Semantic gradient test ----\n",
    "    print(f\"\\n  Semantic gradient (within hard examples):\")\n",
    "    grad_ranks = []\n",
    "    grad_ds = []\n",
    "    for cond, rank in GRADIENT_CONDS:\n",
    "        sem_d = cohens_d(random_base - nlls[cond])\n",
    "        grad_ranks.append(rank)\n",
    "        grad_ds.append(sem_d)\n",
    "        print(f\"    [{rank}] {cond:<22} sem_delta_d={sem_d:+.4f}\")\n",
    "\n",
    "    rho, p_grad = stats.spearmanr(grad_ranks, grad_ds)\n",
    "    sig = ('***' if p_grad < 0.001 else '**' if p_grad < 0.01\n",
    "           else '*' if p_grad < 0.05 else 'ns')\n",
    "    print(f\"    Spearman rho: {rho:+.3f} (p={p_grad:.4f}) {sig}\")\n",
    "\n",
    "    if rho > 0.8 and p_grad < 0.10:\n",
    "        verdict = \"MONOTONIC gradient\"\n",
    "    elif rho > 0.5:\n",
    "        verdict = \"PARTIAL gradient\"\n",
    "    elif rho > 0:\n",
    "        verdict = \"WEAK positive trend\"\n",
    "    else:\n",
    "        verdict = \"NO gradient\"\n",
    "    print(f\"    --> {verdict}\")\n",
    "    analysis['gradient'] = {\n",
    "        'rho': float(rho), 'p': float(p_grad), 'verdict': verdict,\n",
    "    }\n",
    "\n",
    "    # ---- Part C: LLM doc-specific vs generic ----\n",
    "    print(f\"\\n  LLM doc-specific vs generic task-framing:\")\n",
    "    llm_pairs = [\n",
    "        ('llm_extract', 'extractor_matched'),\n",
    "        ('llm_question', 'extractor_matched'),\n",
    "        ('llm_summarize', 'extractor_matched'),\n",
    "    ]\n",
    "    for llm_cond, gen_cond in llm_pairs:\n",
    "        diff = nlls[gen_cond] - nlls[llm_cond]  # pos = LLM better\n",
    "        d = cohens_d(diff)\n",
    "        _, p = stats.ttest_1samp(diff, 0)\n",
    "        win = 100 * np.mean(diff > 0)\n",
    "        sig = ('***' if p < 0.001 else '**' if p < 0.01\n",
    "               else '*' if p < 0.05 else 'ns')\n",
    "        print(f\"    {llm_cond} vs {gen_cond}: d={d:+.3f}, \"\n",
    "              f\"LLM wins {win:.1f}%, p={p:.2e} {sig}\")\n",
    "\n",
    "    per_dataset_analysis[ds_name] = analysis\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872900b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Cross-dataset meta-analysis, consistency, and verdict\n",
    "print(\"=\" * 70)\n",
    "print(\"CROSS-DATASET ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# ================================================================\n",
    "# PART 1: Cross-dataset condition comparison\n",
    "# ================================================================\n",
    "print(f\"\\n--- PART 1: Semantic delta d across datasets ---\")\n",
    "print(f\"\\n  {'Condition':<24}\", end=\"\")\n",
    "for ds_name in DATASET_NAMES:\n",
    "    print(f\" {ds_name[:10]:>10}\", end=\"\")\n",
    "print(f\"  {'mean':>8} {'consistent':>10}\")\n",
    "print(f\"  {'-'*90}\")\n",
    "\n",
    "cross_dataset = {}  # cond -> {ds_name: d, ...}\n",
    "for cond in COND_NAMES:\n",
    "    if cond in ('bare', 'random_tokens'):\n",
    "        continue\n",
    "    row = f\"  {cond:<24}\"\n",
    "    ds_vals = []\n",
    "    for ds_name in DATASET_NAMES:\n",
    "        sem_delta = hard_nlls[ds_name]['random_tokens'] - hard_nlls[ds_name][cond]\n",
    "        d = cohens_d(sem_delta)\n",
    "        row += f\" {d:>+10.3f}\"\n",
    "        ds_vals.append(d)\n",
    "    mean_d = np.mean(ds_vals)\n",
    "    same_sign = all(v >= 0 for v in ds_vals) or all(v <= 0 for v in ds_vals)\n",
    "    row += f\"  {mean_d:>+8.3f} {'YES' if same_sign else 'NO':>10}\"\n",
    "    print(row)\n",
    "    cross_dataset[cond] = {ds: float(d) for ds, d in zip(DATASET_NAMES, ds_vals)}\n",
    "    cross_dataset[cond]['mean'] = float(mean_d)\n",
    "    cross_dataset[cond]['consistent_sign'] = same_sign\n",
    "\n",
    "# ================================================================\n",
    "# PART 2: Fixed-effects meta-analysis\n",
    "# ================================================================\n",
    "print(f\"\\n--- PART 2: Fixed-Effects Meta-Analysis ---\")\n",
    "print(f\"\\n  {'Condition':<24} {'pooled_d':>9} {'SE':>8} {'z':>8} \"\n",
    "      f\"{'p':>10} {'95% CI':>16} {'sig':>4}\")\n",
    "print(f\"  {'-'*82}\")\n",
    "\n",
    "meta_results = {}\n",
    "for cond in COND_NAMES:\n",
    "    if cond in ('bare', 'random_tokens'):\n",
    "        continue\n",
    "\n",
    "    # Per-dataset estimates\n",
    "    ds_effects = []\n",
    "    for ds_name in DATASET_NAMES:\n",
    "        sem_delta = hard_nlls[ds_name]['random_tokens'] - hard_nlls[ds_name][cond]\n",
    "        n = len(sem_delta)\n",
    "        d = cohens_d(sem_delta)\n",
    "        se = np.sqrt(1.0/n + d**2 / (2.0*n))\n",
    "        ds_effects.append((d, se, n))\n",
    "\n",
    "    # Fixed-effects pooling\n",
    "    weights = [1.0 / (se**2) for _, se, _ in ds_effects]\n",
    "    w_sum = sum(weights)\n",
    "    pooled_d = sum(w * d for (d, _, _), w in zip(ds_effects, weights)) / w_sum\n",
    "    pooled_se = 1.0 / np.sqrt(w_sum)\n",
    "    z = pooled_d / pooled_se if pooled_se > 0 else 0.0\n",
    "    p = 2 * stats.norm.sf(abs(z))\n",
    "    ci_lo = pooled_d - 1.96 * pooled_se\n",
    "    ci_hi = pooled_d + 1.96 * pooled_se\n",
    "    sig = ('***' if p < 0.001 else '**' if p < 0.01\n",
    "           else '*' if p < 0.05 else 'ns')\n",
    "\n",
    "    print(f\"  {cond:<24} {pooled_d:>+9.4f} {pooled_se:>8.4f} {z:>+8.2f} \"\n",
    "          f\"{p:>10.2e} [{ci_lo:>+.3f}, {ci_hi:>+.3f}] {sig:>4}\")\n",
    "    meta_results[cond] = {\n",
    "        'pooled_d': float(pooled_d), 'se': float(pooled_se),\n",
    "        'z': float(z), 'p': float(p),\n",
    "        'ci_lo': float(ci_lo), 'ci_hi': float(ci_hi),\n",
    "    }\n",
    "\n",
    "# ================================================================\n",
    "# PART 3: Semantic gradient across datasets\n",
    "# ================================================================\n",
    "print(f\"\\n--- PART 3: Semantic Gradient Per Dataset ---\")\n",
    "print(f\"\\n  {'Dataset':<16} {'rho':>8} {'p':>10} {'verdict':>20}\")\n",
    "print(f\"  {'-'*58}\")\n",
    "\n",
    "all_rhos = []\n",
    "for ds_name in DATASET_NAMES:\n",
    "    grad_ranks = []\n",
    "    grad_ds = []\n",
    "    for cond, rank in GRADIENT_CONDS:\n",
    "        sem_d = cohens_d(hard_nlls[ds_name]['random_tokens'] -\n",
    "                         hard_nlls[ds_name][cond])\n",
    "        grad_ranks.append(rank)\n",
    "        grad_ds.append(sem_d)\n",
    "    rho, p = stats.spearmanr(grad_ranks, grad_ds)\n",
    "    all_rhos.append(rho)\n",
    "    if rho > 0.8 and p < 0.10:\n",
    "        verdict = \"MONOTONIC\"\n",
    "    elif rho > 0.5:\n",
    "        verdict = \"PARTIAL\"\n",
    "    elif rho > 0:\n",
    "        verdict = \"WEAK\"\n",
    "    else:\n",
    "        verdict = \"NONE\"\n",
    "    print(f\"  {ds_name:<16} {rho:>+8.3f} {p:>10.4f} {verdict:>20}\")\n",
    "\n",
    "mean_rho = np.mean(all_rhos)\n",
    "print(f\"\\n  Mean rho across datasets: {mean_rho:+.3f}\")\n",
    "n_positive = sum(1 for r in all_rhos if r > 0)\n",
    "print(f\"  Datasets with positive gradient: {n_positive}/{len(DATASET_NAMES)}\")\n",
    "\n",
    "# ================================================================\n",
    "# PART 4: Dataset properties\n",
    "# ================================================================\n",
    "print(f\"\\n--- PART 4: Dataset Properties ---\")\n",
    "print(f\"\\n  {'Dataset':<16} {'N_hard':>6} {'pass_w':>8} {'ans_w':>8} \"\n",
    "      f\"{'Q_tok':>8} {'bare NLL':>10} {'best_sem_d':>10}\")\n",
    "print(f\"  {'-'*70}\")\n",
    "\n",
    "for ds_name in DATASET_NAMES:\n",
    "    meta = hard_metadata[ds_name]\n",
    "    bare_mean = hard_nlls[ds_name]['bare'].mean()\n",
    "\n",
    "    # Find condition with highest semantic delta d\n",
    "    best_cond = None\n",
    "    best_d = -999\n",
    "    for cond in COND_NAMES:\n",
    "        if cond in ('bare', 'random_tokens'):\n",
    "            continue\n",
    "        sem_d = cohens_d(hard_nlls[ds_name]['random_tokens'] -\n",
    "                         hard_nlls[ds_name][cond])\n",
    "        if sem_d > best_d:\n",
    "            best_d = sem_d\n",
    "            best_cond = cond\n",
    "\n",
    "    print(f\"  {ds_name:<16} {meta['n_hard']:>6} \"\n",
    "          f\"{meta['mean_passage_words']:>8.0f} \"\n",
    "          f\"{meta['mean_answer_words']:>8.1f} \"\n",
    "          f\"{meta['mean_query_tokens']:>8.1f} \"\n",
    "          f\"{bare_mean:>10.3f} {best_d:>+10.3f}\")\n",
    "\n",
    "# ================================================================\n",
    "# VERDICT\n",
    "# ================================================================\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"VERDICT — Exp 03: Hard-Example Semantic Isolation Across Datasets\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\nModel: {MODEL_NAME}\")\n",
    "print(f\"Scoring: BOS-retained repositioning + token-level prefix matching\")\n",
    "print(f\"Datasets: {len(DATASET_NAMES)} ({', '.join(DATASET_NAMES)})\")\n",
    "print(f\"Hard selection: top {HARD_FRAC*100:.0f}% by bare NLL\")\n",
    "\n",
    "# Key findings\n",
    "print(f\"\\n--- Key findings ---\")\n",
    "\n",
    "# 1. Does the semantic gradient emerge in hard examples?\n",
    "n_gradient = sum(1 for r in all_rhos if r > 0.5)\n",
    "print(f\"\\n  1. Semantic gradient in hard examples:\")\n",
    "print(f\"     {n_gradient}/{len(DATASET_NAMES)} datasets show partial/monotonic gradient\")\n",
    "print(f\"     Mean Spearman rho: {mean_rho:+.3f}\")\n",
    "if n_gradient >= 3:\n",
    "    print(f\"     --> YES: gradient generalizes across datasets\")\n",
    "elif n_gradient >= 2:\n",
    "    print(f\"     --> PARTIAL: gradient in {n_gradient}/{len(DATASET_NAMES)} datasets\")\n",
    "else:\n",
    "    print(f\"     --> NO: gradient does not consistently emerge\")\n",
    "\n",
    "# 2. Which conditions have significant semantic benefit?\n",
    "print(f\"\\n  2. Conditions with significant semantic benefit (pooled p<0.05):\")\n",
    "sig_conds = [(c, m) for c, m in meta_results.items() if m['p'] < 0.05]\n",
    "sig_conds.sort(key=lambda x: x[1]['pooled_d'], reverse=True)\n",
    "for cond, m in sig_conds:\n",
    "    print(f\"     {cond:<24} pooled_d={m['pooled_d']:+.3f} \"\n",
    "          f\"[{m['ci_lo']:+.3f}, {m['ci_hi']:+.3f}]\")\n",
    "if not sig_conds:\n",
    "    print(f\"     (none)\")\n",
    "\n",
    "# 3. Cross-dataset consistency\n",
    "n_consistent = sum(1 for v in cross_dataset.values() if v.get('consistent_sign'))\n",
    "print(f\"\\n  3. Cross-dataset sign consistency:\")\n",
    "print(f\"     {n_consistent}/{len(cross_dataset)} conditions have same sign across all 4 datasets\")\n",
    "\n",
    "# 4. LLM doc-specific vs generic\n",
    "print(f\"\\n  4. LLM doc-specific vs generic (pooled):\")\n",
    "for cond in ['llm_extract', 'llm_question', 'llm_summarize']:\n",
    "    if cond in meta_results:\n",
    "        m = meta_results[cond]\n",
    "        gen_cond = 'extractor_matched'\n",
    "        gm = meta_results.get(gen_cond, {})\n",
    "        print(f\"     {cond:<22} pooled_d={m['pooled_d']:+.3f}  \"\n",
    "              f\"{gen_cond} pooled_d={gm.get('pooled_d', 0):+.3f}\")\n",
    "\n",
    "# Ranked conditions by pooled d\n",
    "print(f\"\\n  5. All conditions ranked by pooled semantic delta d:\")\n",
    "ranked = sorted(meta_results.items(), key=lambda x: x[1]['pooled_d'], reverse=True)\n",
    "for cond, m in ranked:\n",
    "    sig = ('***' if m['p'] < 0.001 else '**' if m['p'] < 0.01\n",
    "           else '*' if m['p'] < 0.05 else 'ns')\n",
    "    print(f\"     {cond:<24} d={m['pooled_d']:+.4f} ({sig})\")\n",
    "\n",
    "# ================================================================\n",
    "# SAVE RESULTS\n",
    "# ================================================================\n",
    "final_results = {\n",
    "    'experiment': 'v4_exp03_hard_semantic_cross_dataset',\n",
    "    'model': MODEL_NAME,\n",
    "    'scoring': 'bos_retained_repositioning_token_matched',\n",
    "    'hard_fraction': HARD_FRAC,\n",
    "    'datasets': DATASET_NAMES,\n",
    "    'n_samples_per_dataset': N_SAMPLES,\n",
    "    'seed': SEED,\n",
    "    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'per_dataset': {},\n",
    "    'meta_analysis': meta_results,\n",
    "    'cross_dataset': cross_dataset,\n",
    "    'gradient': {\n",
    "        'per_dataset_rho': {ds: float(r) for ds, r in zip(DATASET_NAMES, all_rhos)},\n",
    "        'mean_rho': float(mean_rho),\n",
    "    },\n",
    "    'hard_metadata': {ds: hard_metadata[ds] for ds in DATASET_NAMES},\n",
    "}\n",
    "\n",
    "for ds_name in DATASET_NAMES:\n",
    "    final_results['per_dataset'][ds_name] = per_dataset_analysis.get(ds_name, {})\n",
    "\n",
    "with open(RESULTS_DIR / 'results.json', 'w') as f:\n",
    "    json.dump(final_results, f, indent=2)\n",
    "print(f\"\\nResults saved to {RESULTS_DIR / 'results.json'}\")\n",
    "\n",
    "# Cleanup\n",
    "print(f\"\\nCleaning up GPU memory...\")\n",
    "mem_before = torch.cuda.memory_allocated() / 1e9\n",
    "del model, tokenizer\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "mem_after = torch.cuda.memory_allocated() / 1e9\n",
    "print(f\"GPU memory: {mem_before:.2f} GB -> {mem_after:.2f} GB\")\n",
    "print(\"Done!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
