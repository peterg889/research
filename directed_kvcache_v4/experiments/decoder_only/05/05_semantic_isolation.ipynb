{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f57e603b",
   "metadata": {},
   "source": [
    "# Decoder-Only Exp 05: Semantic Priming in Isolation\n",
    "\n",
    "## Motivation\n",
    "\n",
    "Exp 03-04 showed that the \"structural effect\" is actually BOS removal + position\n",
    "offset + attention sink pruning. These structural factors are so large that\n",
    "`prune_first_3` (d=+0.80) and `pos_4` (d=+0.78) beat the oracle (d=+0.64).\n",
    "\n",
    "But **we cannot conclude that semantic priming is zero**, because in all prior\n",
    "experiments the oracle differs from controls in BOTH structure AND content\n",
    "simultaneously. To isolate semantics, we need an experiment where **all structural\n",
    "confounds are equalized** and only prefix content varies.\n",
    "\n",
    "## Design principle\n",
    "\n",
    "For each sample, compute Q = number of query tokens (without BOS). ALL prefixed\n",
    "conditions (4-9) use a prefix of **exactly Q token IDs**. This ensures:\n",
    "- Same BOS removal (always sliced)\n",
    "- Same position offset (doc starts at position Q+2 in all conditions)\n",
    "- Same number of Phase A attention targets (Q prefix tokens)\n",
    "- Same cache length (only doc KV entries)\n",
    "\n",
    "The **only** thing that varies across prefixed conditions is the semantic content\n",
    "of the Q prefix tokens.\n",
    "\n",
    "## Conditions (9)\n",
    "\n",
    "### Reference baselines (different structure, for context)\n",
    "| # | Condition | Description |\n",
    "|---|-----------|-------------|\n",
    "| 1 | bare | Standard: [BOS + doc], nothing sliced |\n",
    "| 2 | best_structural | BOS + first 3 doc tokens removed from cache (exp04 champion) |\n",
    "| 3 | no_prefix_posmatched | position_offset = per-sample Q+2, BOS removed, no prefix |\n",
    "\n",
    "### Length-matched semantic gradient (all have exactly Q prefix tokens)\n",
    "| # | Condition | Prefix content | Semantic level |\n",
    "|---|-----------|---------------|----------------|\n",
    "| 4 | repeat_token | token(\"the\") × Q | Zero content variation |\n",
    "| 5 | random_tokens | Q random IDs from query vocab | Varied embeddings, no meaning |\n",
    "| 6 | unrelated_query | Different sample's query tokens (truncated/padded to Q) | Coherent text, wrong content |\n",
    "| 7 | shuffled_query | Query tokens randomly permuted | Right vocabulary, wrong syntax |\n",
    "| 8 | doc_keywords | First Q non-stopword doc tokens | Doc-relevant, not query-specific |\n",
    "| 9 | oracle | Actual query tokens | Full semantic match |\n",
    "\n",
    "## Planned contrasts\n",
    "\n",
    "| Contrast | Isolates |\n",
    "|----------|----------|\n",
    "| no_prefix_posmatched vs repeat_token | Attention enrichment effect |\n",
    "| repeat_token vs random_tokens | Embedding variation |\n",
    "| random_tokens vs unrelated_query | Natural language structure |\n",
    "| unrelated_query vs shuffled_query | Vocabulary match (right words, wrong order) |\n",
    "| shuffled_query vs oracle | Word order / syntax |\n",
    "| doc_keywords vs oracle | Query vs document relevance |\n",
    "| best_structural vs oracle | Pure structure vs semantic priming |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f012c1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Setup and model loading\n",
    "import os\n",
    "os.umask(0o000)\n",
    "\n",
    "import sys, json, time, gc, re\n",
    "import random as pyrandom\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from scipy import stats\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "sys.path.insert(0, \"../../..\")\n",
    "from lib.analysis import cohens_d\n",
    "\n",
    "SEED = 42\n",
    "N_SAMPLES = 400\n",
    "MODEL_NAME = \"google/gemma-3-4b-it\"\n",
    "\n",
    "RESULTS_DIR = Path(\"../../../results/decoder_only/exp05\")\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CHECKPOINT_PATH = RESULTS_DIR / \"checkpoint.json\"\n",
    "\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "pyrandom.seed(SEED)\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv())\n",
    "HF_TOKEN = os.environ.get(\"HF_TOKEN\")\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "print(f\"Loading {MODEL_NAME}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, token=HF_TOKEN)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME, device_map=\"auto\", torch_dtype=torch.bfloat16, token=HF_TOKEN,\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "DEVICE = next(model.parameters()).device\n",
    "\n",
    "print(f\"Exp 05: Semantic Priming in Isolation\")\n",
    "print(f\"N: {N_SAMPLES}, Model: {MODEL_NAME}\")\n",
    "print(f\"DEVICE: {DEVICE}, dtype: {next(model.parameters()).dtype}\")\n",
    "print(f\"GPU memory: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "text_cfg = getattr(model.config, 'text_config', model.config)\n",
    "VOCAB_SIZE = getattr(text_cfg, 'vocab_size', 262208)\n",
    "print(f\"Vocab size: {VOCAB_SIZE}\")\n",
    "print(f\"Num layers: {getattr(text_cfg, 'num_hidden_layers', 'N/A')}\")\n",
    "print(f\"Num KV heads: {getattr(text_cfg, 'num_key_value_heads', 'N/A')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7304a711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: KV cache helpers and scoring function\n",
    "\n",
    "def slice_kv_cache(cache, start_idx):\n",
    "    # Remove first start_idx entries from KV cache.\n",
    "    from transformers import DynamicCache\n",
    "    if isinstance(cache, DynamicCache):\n",
    "        sliced = DynamicCache()\n",
    "        for i in range(len(cache.layers)):\n",
    "            k = cache.layers[i].keys[:, :, start_idx:, :]\n",
    "            v = cache.layers[i].values[:, :, start_idx:, :]\n",
    "            sliced.update(k, v, i)\n",
    "        return sliced\n",
    "    else:\n",
    "        return tuple(\n",
    "            (k[:, :, start_idx:, :], v[:, :, start_idx:, :])\n",
    "            for k, v in cache\n",
    "        )\n",
    "\n",
    "\n",
    "NEWLINE_IDS = tokenizer(\"\\n\", add_special_tokens=False).input_ids\n",
    "BOS_ID = tokenizer.bos_token_id\n",
    "print(f\"BOS token ID: {BOS_ID}\")\n",
    "print(f\"Newline token IDs: {NEWLINE_IDS} ({len(NEWLINE_IDS)} tokens)\")\n",
    "\n",
    "\n",
    "def score(doc_text, query_text, answer_text,\n",
    "          prefix_token_ids=None,\n",
    "          position_offset=0, remove_bos=False,\n",
    "          prune_first=0):\n",
    "    # Score NLL of answer tokens using two-phase KV cache.\n",
    "    #\n",
    "    # Modes:\n",
    "    #   prefix_token_ids: [BOS] + prefix_ids + [\\n] + doc_ids\n",
    "    #     Slices first 1+len(prefix_ids)+len(NEWLINE_IDS) entries (BOS+prefix+\\n)\n",
    "    #   position_offset > 0: BOS at pos 0, doc at offset..offset+D, BOS removed\n",
    "    #   remove_bos + prune_first: bare with BOS + first N doc tokens removed\n",
    "    #   Default: bare (BOS in cache)\n",
    "\n",
    "    # --- Phase A: Conditioning ---\n",
    "    if prefix_token_ids is not None:\n",
    "        doc_ids = tokenizer(doc_text, add_special_tokens=False,\n",
    "                            truncation=True, max_length=1536).input_ids\n",
    "        cond_ids = [BOS_ID] + list(prefix_token_ids) + NEWLINE_IDS + doc_ids\n",
    "        slice_start = 1 + len(prefix_token_ids) + len(NEWLINE_IDS)\n",
    "        custom_pos = None\n",
    "        phase_b_start = len(cond_ids)\n",
    "\n",
    "    elif position_offset > 0:\n",
    "        cond_ids = tokenizer(doc_text, add_special_tokens=True,\n",
    "                             truncation=True, max_length=2048).input_ids\n",
    "        n_doc = len(cond_ids) - 1\n",
    "        pos_list = [0] + list(range(position_offset, position_offset + n_doc))\n",
    "        custom_pos = torch.tensor([pos_list], dtype=torch.long, device=DEVICE)\n",
    "        slice_start = 1  # remove BOS\n",
    "        phase_b_start = position_offset + n_doc\n",
    "\n",
    "    else:\n",
    "        cond_ids = tokenizer(doc_text, add_special_tokens=True,\n",
    "                             truncation=True, max_length=2048).input_ids\n",
    "        slice_start = 1 if remove_bos else 0\n",
    "        custom_pos = None\n",
    "        phase_b_start = len(cond_ids)\n",
    "\n",
    "    cond_tensor = torch.tensor([cond_ids], dtype=torch.long, device=DEVICE)\n",
    "\n",
    "    fwd_kwargs = {'input_ids': cond_tensor, 'use_cache': True}\n",
    "    if custom_pos is not None:\n",
    "        fwd_kwargs['position_ids'] = custom_pos\n",
    "\n",
    "    with torch.no_grad():\n",
    "        phase_a = model(**fwd_kwargs)\n",
    "\n",
    "    cache = phase_a.past_key_values\n",
    "    del phase_a\n",
    "\n",
    "    if slice_start > 0:\n",
    "        cache = slice_kv_cache(cache, slice_start)\n",
    "\n",
    "    if prune_first > 0:\n",
    "        cache = slice_kv_cache(cache, prune_first)\n",
    "\n",
    "    # --- Phase B: Inference ---\n",
    "    query_part_ids = tokenizer(\"\\n\" + query_text + \"\\n\",\n",
    "                               add_special_tokens=False).input_ids\n",
    "    answer_ids = tokenizer(answer_text, add_special_tokens=False,\n",
    "                           truncation=True, max_length=256).input_ids\n",
    "\n",
    "    if not answer_ids:\n",
    "        del cache\n",
    "        return 0.0\n",
    "\n",
    "    phase_b_ids = query_part_ids + answer_ids\n",
    "    phase_b_tensor = torch.tensor([phase_b_ids], dtype=torch.long, device=DEVICE)\n",
    "\n",
    "    pos_ids = torch.arange(phase_b_start, phase_b_start + len(phase_b_ids),\n",
    "                           device=DEVICE).unsqueeze(0)\n",
    "    cache_position = torch.arange(phase_b_start, phase_b_start + len(phase_b_ids),\n",
    "                                  device=DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        phase_b = model(\n",
    "            input_ids=phase_b_tensor,\n",
    "            past_key_values=cache,\n",
    "            position_ids=pos_ids,\n",
    "            cache_position=cache_position,\n",
    "            use_cache=False,\n",
    "        )\n",
    "\n",
    "    logits = phase_b.logits\n",
    "    n_query_part = len(query_part_ids)\n",
    "    n_answer = len(answer_ids)\n",
    "\n",
    "    answer_logits = logits[0, n_query_part - 1 : n_query_part - 1 + n_answer, :]\n",
    "    answer_targets = torch.tensor(answer_ids, dtype=torch.long, device=DEVICE)\n",
    "\n",
    "    log_probs = F.log_softmax(answer_logits, dim=-1)\n",
    "    token_log_probs = log_probs.gather(1, answer_targets.unsqueeze(1)).squeeze(1)\n",
    "    mean_nll = -token_log_probs.mean().item()\n",
    "\n",
    "    del cache, phase_b, logits, log_probs\n",
    "    return mean_nll\n",
    "\n",
    "\n",
    "print(\"Scoring function defined with token-level prefix support.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5913d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Load MS MARCO data and build per-sample prefix token IDs\n",
    "from lib.data import count_words\n",
    "from datasets import load_dataset\n",
    "\n",
    "print(\"Loading MS MARCO v1.1 validation...\")\n",
    "ds = load_dataset(\"microsoft/ms_marco\", \"v1.1\", split=\"validation\")\n",
    "\n",
    "all_candidates = []\n",
    "for item in ds:\n",
    "    if len(all_candidates) >= 3 * N_SAMPLES:\n",
    "        break\n",
    "    passages = item.get('passages', {})\n",
    "    ptexts = passages.get('passage_text', [])\n",
    "    is_sel = passages.get('is_selected', [])\n",
    "    query = item.get('query', '')\n",
    "    answers = item.get('answers', [])\n",
    "    well_formed = item.get('wellFormedAnswers', [])\n",
    "    answer = None\n",
    "    if well_formed and len(well_formed) > 0 and well_formed[0] not in ('[]', ''):\n",
    "        answer = well_formed[0]\n",
    "    elif answers and len(answers) > 0 and answers[0] != 'No Answer Present.':\n",
    "        answer = answers[0]\n",
    "    if not answer:\n",
    "        continue\n",
    "    for pt, sel in zip(ptexts, is_sel):\n",
    "        wc = count_words(pt)\n",
    "        if sel == 1 and 30 <= wc <= 300:\n",
    "            all_candidates.append({\n",
    "                'passage': pt, 'query': query, 'answer': answer,\n",
    "                'word_count': wc,\n",
    "            })\n",
    "            break\n",
    "\n",
    "print(f\"Total candidates: {len(all_candidates)}\")\n",
    "np.random.seed(SEED)\n",
    "indices = np.random.permutation(len(all_candidates))\n",
    "samples = [all_candidates[i] for i in indices[:N_SAMPLES]]\n",
    "del ds, all_candidates\n",
    "gc.collect()\n",
    "\n",
    "# --- Build per-sample token-level prefix data ---\n",
    "\n",
    "# Collect all query token IDs (for random_tokens pool)\n",
    "all_query_token_ids = []\n",
    "for s in samples:\n",
    "    ids = tokenizer(s['query'], add_special_tokens=False).input_ids\n",
    "    all_query_token_ids.extend(ids)\n",
    "query_vocab_pool = list(set(all_query_token_ids))\n",
    "print(f\"Query vocabulary pool: {len(query_vocab_pool)} unique token IDs\")\n",
    "\n",
    "# Stopword token IDs for doc_keywords filtering\n",
    "STOPWORDS = set(\"the a an is are was were be been being have has had do does did \"\n",
    "                \"will would shall should may might can could of in to for on with \"\n",
    "                \"at by from as into through during before after above below between \"\n",
    "                \"and or but not no nor so yet both either neither each every all any \"\n",
    "                \"few more most other some such than too very it its this that these \"\n",
    "                \"those i me my we our you your he him his she her they them their \"\n",
    "                \"what which who whom whose when where how why if then else\".split())\n",
    "\n",
    "# The token ID for \"the\" (for repeat_token condition)\n",
    "THE_TOKEN_ID = tokenizer(\"the\", add_special_tokens=False).input_ids[0]\n",
    "print(f\"Token ID for 'the': {THE_TOKEN_ID}\")\n",
    "\n",
    "pyrandom.seed(SEED + 200)\n",
    "\n",
    "for i, s in enumerate(samples):\n",
    "    # Tokenize query\n",
    "    q_ids = tokenizer(s['query'], add_special_tokens=False).input_ids\n",
    "    Q = len(q_ids)\n",
    "    s['query_token_ids'] = q_ids\n",
    "    s['Q'] = Q\n",
    "\n",
    "    # 1. repeat_token: \"the\" repeated Q times\n",
    "    s['prefix_repeat'] = [THE_TOKEN_ID] * Q\n",
    "\n",
    "    # 2. random_tokens: Q random IDs from query vocabulary pool\n",
    "    s['prefix_random'] = [pyrandom.choice(query_vocab_pool) for _ in range(Q)]\n",
    "\n",
    "    # 3. unrelated_query: next sample's query tokens, truncated/padded to Q\n",
    "    other_idx = (i + 1) % len(samples)\n",
    "    other_q_ids = tokenizer(samples[other_idx]['query'],\n",
    "                            add_special_tokens=False).input_ids\n",
    "    if len(other_q_ids) >= Q:\n",
    "        s['prefix_unrelated'] = other_q_ids[:Q]\n",
    "    else:\n",
    "        # Pad by repeating the other query's tokens\n",
    "        padded = other_q_ids * ((Q // len(other_q_ids)) + 1)\n",
    "        s['prefix_unrelated'] = padded[:Q]\n",
    "\n",
    "    # 4. shuffled_query: query tokens permuted\n",
    "    shuffled = list(q_ids)\n",
    "    pyrandom.shuffle(shuffled)\n",
    "    s['prefix_shuffled'] = shuffled\n",
    "\n",
    "    # 5. doc_keywords: first Q non-stopword tokens from document\n",
    "    doc_tokens = tokenizer(s['passage'], add_special_tokens=False).input_ids\n",
    "    doc_words = s['passage'].split()\n",
    "    keyword_ids = []\n",
    "    for word in doc_words:\n",
    "        if word.lower().strip(\".,;:!?()[]{}\\\"'\") not in STOPWORDS:\n",
    "            w_ids = tokenizer(word, add_special_tokens=False).input_ids\n",
    "            keyword_ids.extend(w_ids)\n",
    "            if len(keyword_ids) >= Q:\n",
    "                break\n",
    "    # Pad with first doc tokens if not enough keywords\n",
    "    if len(keyword_ids) < Q:\n",
    "        keyword_ids.extend(doc_tokens[:Q - len(keyword_ids)])\n",
    "    s['prefix_doc_kw'] = keyword_ids[:Q]\n",
    "\n",
    "    # 6. oracle: actual query tokens (already have them)\n",
    "    s['prefix_oracle'] = q_ids\n",
    "\n",
    "# Summary statistics\n",
    "q_lens = [s['Q'] for s in samples]\n",
    "print(f\"\\nLoaded {len(samples)} samples\")\n",
    "print(f\"Mean passage words: {np.mean([s['word_count'] for s in samples]):.0f}\")\n",
    "print(f\"Mean query words: {np.mean([len(s['query'].split()) for s in samples]):.1f}\")\n",
    "print(f\"Query token count — mean: {np.mean(q_lens):.1f}, \"\n",
    "      f\"median: {np.median(q_lens):.0f}, \"\n",
    "      f\"min: {np.min(q_lens)}, max: {np.max(q_lens)}\")\n",
    "\n",
    "# Verify all prefixes have exactly Q tokens\n",
    "for i, s in enumerate(samples[:5]):\n",
    "    Q = s['Q']\n",
    "    for name in ['prefix_repeat', 'prefix_random', 'prefix_unrelated',\n",
    "                  'prefix_shuffled', 'prefix_doc_kw', 'prefix_oracle']:\n",
    "        assert len(s[name]) == Q, f\"Sample {i} {name}: len={len(s[name])} != Q={Q}\"\n",
    "    print(f\"  Sample {i}: Q={Q}, query='{s['query'][:50]}...'\")\n",
    "    print(f\"    repeat:    {tokenizer.decode(s['prefix_repeat'][:8])}...\")\n",
    "    print(f\"    random:    {tokenizer.decode(s['prefix_random'][:8])}...\")\n",
    "    print(f\"    unrelated: {tokenizer.decode(s['prefix_unrelated'][:8])}...\")\n",
    "    print(f\"    shuffled:  {tokenizer.decode(s['prefix_shuffled'][:8])}...\")\n",
    "    print(f\"    doc_kw:    {tokenizer.decode(s['prefix_doc_kw'][:8])}...\")\n",
    "    print(f\"    oracle:    {tokenizer.decode(s['prefix_oracle'][:8])}...\")\n",
    "print(\"All prefix lengths verified.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a7cd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Validate scoring modes\n",
    "print(\"=\" * 70)\n",
    "print(\"VALIDATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "s = samples[0]\n",
    "Q = s['Q']\n",
    "\n",
    "print(f\"\\nSample 0: Q={Q} query tokens\")\n",
    "print(f\"  Query: '{s['query']}'\")\n",
    "print(f\"  Position of doc start in all prefixed conditions: {Q + 2}\")\n",
    "print(f\"  (BOS=1 + prefix={Q} + newline={len(NEWLINE_IDS)})\")\n",
    "\n",
    "# Verify position matching: all prefixed conditions should put doc at same position\n",
    "print(f\"\\n--- Position verification ---\")\n",
    "doc_ids = tokenizer(s['passage'], add_special_tokens=False,\n",
    "                    truncation=True, max_length=1536).input_ids\n",
    "for name in ['prefix_repeat', 'prefix_random', 'prefix_unrelated',\n",
    "             'prefix_shuffled', 'prefix_doc_kw', 'prefix_oracle']:\n",
    "    prefix_ids = s[name]\n",
    "    cond_ids = [BOS_ID] + prefix_ids + NEWLINE_IDS + doc_ids\n",
    "    slice_start = 1 + len(prefix_ids) + len(NEWLINE_IDS)\n",
    "    doc_start_pos = slice_start  # doc[0] is at this position in the sequence\n",
    "    print(f\"  {name:<20} prefix_len={len(prefix_ids):>3}, \"\n",
    "          f\"slice_start={slice_start:>3}, doc_start_pos={doc_start_pos:>3}, \"\n",
    "          f\"total_len={len(cond_ids):>4}\")\n",
    "\n",
    "# Verify no_prefix_posmatched uses same offset\n",
    "pos_offset = Q + 1 + len(NEWLINE_IDS)\n",
    "print(f\"\\n  no_prefix_posmatched: position_offset={pos_offset} \"\n",
    "      f\"(matching prefixed doc start)\")\n",
    "\n",
    "# Score all modes\n",
    "print(f\"\\n--- NLL for each condition (sample 0) ---\")\n",
    "nll_bare = score(s['passage'], s['query'], s['answer'])\n",
    "print(f\"  {'bare':<24} NLL = {nll_bare:.4f}\")\n",
    "\n",
    "nll_struct = score(s['passage'], s['query'], s['answer'],\n",
    "                   remove_bos=True, prune_first=3)\n",
    "print(f\"  {'best_structural':<24} NLL = {nll_struct:.4f}  \"\n",
    "      f\"delta = {nll_bare - nll_struct:+.4f}\")\n",
    "\n",
    "nll_posmatched = score(s['passage'], s['query'], s['answer'],\n",
    "                       position_offset=pos_offset)\n",
    "print(f\"  {'no_prefix_posmatched':<24} NLL = {nll_posmatched:.4f}  \"\n",
    "      f\"delta = {nll_bare - nll_posmatched:+.4f}\")\n",
    "\n",
    "for name, prefix_key in [('repeat_token', 'prefix_repeat'),\n",
    "                          ('random_tokens', 'prefix_random'),\n",
    "                          ('unrelated_query', 'prefix_unrelated'),\n",
    "                          ('shuffled_query', 'prefix_shuffled'),\n",
    "                          ('doc_keywords', 'prefix_doc_kw'),\n",
    "                          ('oracle', 'prefix_oracle')]:\n",
    "    nll = score(s['passage'], s['query'], s['answer'],\n",
    "                prefix_token_ids=s[prefix_key])\n",
    "    print(f\"  {name:<24} NLL = {nll:.4f}  delta = {nll_bare - nll:+.4f}\")\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bde2f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Scoring loop — 9 conditions x 400 samples\n",
    "print(\"=\" * 70)\n",
    "print(\"SCORING ALL CONDITIONS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "COND_NAMES = [\n",
    "    'bare', 'best_structural', 'no_prefix_posmatched',\n",
    "    'repeat_token', 'random_tokens', 'unrelated_query',\n",
    "    'shuffled_query', 'doc_keywords', 'oracle',\n",
    "]\n",
    "\n",
    "results = []\n",
    "start_idx = 0\n",
    "\n",
    "if CHECKPOINT_PATH.exists():\n",
    "    ckpt = json.loads(CHECKPOINT_PATH.read_text())\n",
    "    if ckpt.get('n_total') == N_SAMPLES and len(ckpt.get('results', [])) > 0:\n",
    "        saved_queries = [r['query'][:50] for r in ckpt['results']]\n",
    "        current_queries = [s['query'][:50] for s in samples[:len(saved_queries)]]\n",
    "        if saved_queries == current_queries:\n",
    "            results = ckpt['results']\n",
    "            start_idx = len(results)\n",
    "            print(f\"Resuming from checkpoint: {start_idx}/{N_SAMPLES}\")\n",
    "\n",
    "if start_idx == 0:\n",
    "    print(f\"Starting fresh: {len(COND_NAMES)} conditions x {N_SAMPLES} samples\")\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "for i in tqdm(range(start_idx, N_SAMPLES), initial=start_idx, total=N_SAMPLES,\n",
    "              desc=\"Scoring\"):\n",
    "    s = samples[i]\n",
    "    query = s['query']\n",
    "    passage = s['passage']\n",
    "    answer = s['answer']\n",
    "    Q = s['Q']\n",
    "\n",
    "    result = {\n",
    "        'query': query,\n",
    "        'answer': answer,\n",
    "        'passage_words': s['word_count'],\n",
    "        'query_words': len(query.split()),\n",
    "        'Q': Q,\n",
    "    }\n",
    "\n",
    "    # 1. bare\n",
    "    result['nll_bare'] = score(passage, query, answer)\n",
    "\n",
    "    # 2. best_structural (BOS + first 3 doc tokens removed)\n",
    "    result['nll_best_structural'] = score(passage, query, answer,\n",
    "                                          remove_bos=True, prune_first=3)\n",
    "\n",
    "    # 3. no_prefix_posmatched (position offset = Q + 1 + len(NEWLINE_IDS))\n",
    "    pos_offset = Q + 1 + len(NEWLINE_IDS)\n",
    "    result['nll_no_prefix_posmatched'] = score(passage, query, answer,\n",
    "                                                position_offset=pos_offset)\n",
    "\n",
    "    # 4-9. Length-matched prefixed conditions\n",
    "    for cond_name, prefix_key in [\n",
    "        ('repeat_token', 'prefix_repeat'),\n",
    "        ('random_tokens', 'prefix_random'),\n",
    "        ('unrelated_query', 'prefix_unrelated'),\n",
    "        ('shuffled_query', 'prefix_shuffled'),\n",
    "        ('doc_keywords', 'prefix_doc_kw'),\n",
    "        ('oracle', 'prefix_oracle'),\n",
    "    ]:\n",
    "        result[f'nll_{cond_name}'] = score(passage, query, answer,\n",
    "                                            prefix_token_ids=s[prefix_key])\n",
    "\n",
    "    results.append(result)\n",
    "\n",
    "    if (i + 1) % 20 == 0 or i == N_SAMPLES - 1:\n",
    "        ckpt = {\n",
    "            'n_total': N_SAMPLES,\n",
    "            'results': results,\n",
    "            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        }\n",
    "        CHECKPOINT_PATH.write_text(json.dumps(ckpt))\n",
    "        elapsed = time.time() - t0\n",
    "        done = i - start_idx + 1\n",
    "        eta = (N_SAMPLES - i - 1) * elapsed / done if done > 0 else 0\n",
    "        tqdm.write(f\"  Checkpoint {i+1}/{N_SAMPLES} | {elapsed/60:.1f}m | ETA {eta/60:.1f}m\")\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "elapsed = time.time() - t0\n",
    "print(f\"\\nScoring complete: {len(results)} samples, \"\n",
    "      f\"{len(COND_NAMES)} conditions in {elapsed/60:.1f} min\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61f249b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Results table\n",
    "print(\"=\" * 70)\n",
    "print(f\"RESULTS (N={len(results)})\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "arrays = {}\n",
    "for name in COND_NAMES:\n",
    "    arrays[name] = np.array([r[f'nll_{name}'] for r in results])\n",
    "\n",
    "bare = arrays['bare']\n",
    "oracle = arrays['oracle']\n",
    "oracle_delta_mean = (bare - oracle).mean()\n",
    "\n",
    "print(f\"\\n  Oracle delta (bare - oracle): {oracle_delta_mean:+.4f}\")\n",
    "\n",
    "print(f\"\\n  {'Condition':<24} {'NLL':>8} {'vs bare':>10} {'d':>8} \"\n",
    "      f\"{'Win%':>8} {'p':>12} {'sig':>5} {'Recovery':>10}\")\n",
    "print(f\"  {'-'*96}\")\n",
    "\n",
    "analysis = {}\n",
    "for name in COND_NAMES:\n",
    "    nlls = arrays[name]\n",
    "    mean_nll = nlls.mean()\n",
    "\n",
    "    if name == 'bare':\n",
    "        print(f\"  {name:<24} {mean_nll:>8.4f} {'--':>10} {'--':>8} \"\n",
    "              f\"{'--':>8} {'--':>12} {'--':>5} {'--':>10}\")\n",
    "        analysis[name] = {'mean_nll': float(mean_nll)}\n",
    "    else:\n",
    "        diff = bare - nlls\n",
    "        d = cohens_d(diff)\n",
    "        win_pct = 100 * np.mean(diff > 0)\n",
    "        _, p_val = stats.ttest_1samp(diff, 0)\n",
    "        sig = '***' if p_val < 0.001 else '**' if p_val < 0.01 else '*' if p_val < 0.05 else 'ns'\n",
    "        rec = diff.mean() / oracle_delta_mean * 100 if oracle_delta_mean > 0 else 0\n",
    "\n",
    "        print(f\"  {name:<24} {mean_nll:>8.4f} {diff.mean():>+10.4f} {d:>+8.3f} \"\n",
    "              f\"{win_pct:>7.1f}% {p_val:>12.2e} {sig:>5} {rec:>9.1f}%\")\n",
    "        analysis[name] = {\n",
    "            'mean_nll': float(mean_nll), 'delta': float(diff.mean()),\n",
    "            'd': float(d), 'win_pct': float(win_pct), 'p': float(p_val),\n",
    "            'recovery': float(rec),\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8880353f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Semantic gradient — pairwise contrasts\n",
    "print(\"=\" * 70)\n",
    "print(\"SEMANTIC GRADIENT ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# --- Attention enrichment: does having prefix tokens matter? ---\n",
    "print(f\"\\n--- Attention enrichment effect ---\")\n",
    "print(f\"  no_prefix_posmatched vs repeat_token\")\n",
    "print(f\"  (same position offset, no prefix tokens vs Q uniform prefix tokens)\")\n",
    "diff_enrich = arrays['no_prefix_posmatched'] - arrays['repeat_token']\n",
    "d_enrich = cohens_d(diff_enrich)\n",
    "_, p_enrich = stats.ttest_1samp(diff_enrich, 0)\n",
    "sig_e = '***' if p_enrich < 0.001 else '**' if p_enrich < 0.01 else '*' if p_enrich < 0.05 else 'ns'\n",
    "print(f\"  d = {d_enrich:+.4f} ({sig_e}), p = {p_enrich:.2e}\")\n",
    "if d_enrich > 0.05:\n",
    "    print(f\"  -> Having prefix tokens to attend to HELPS (repeat_token < no_prefix)\")\n",
    "elif d_enrich < -0.05:\n",
    "    print(f\"  -> Having prefix tokens to attend to HURTS (repeat_token > no_prefix)\")\n",
    "else:\n",
    "    print(f\"  -> Attention enrichment is negligible\")\n",
    "\n",
    "# --- Semantic gradient: pairwise steps ---\n",
    "print(f\"\\n--- Semantic gradient (pairwise contrasts) ---\")\n",
    "print(f\"  Each step adds one dimension of semantic content.\")\n",
    "print(f\"  Positive d = later condition is better.\\n\")\n",
    "\n",
    "gradient_pairs = [\n",
    "    ('repeat_token', 'random_tokens', 'Embedding variation'),\n",
    "    ('random_tokens', 'unrelated_query', 'Natural language structure'),\n",
    "    ('unrelated_query', 'shuffled_query', 'Vocabulary match (right words)'),\n",
    "    ('shuffled_query', 'oracle', 'Word order / syntax'),\n",
    "]\n",
    "\n",
    "print(f\"  {'Step':<40} {'d':>8} {'p':>12} {'sig':>5}\")\n",
    "print(f\"  {'-'*70}\")\n",
    "for cond_a, cond_b, label in gradient_pairs:\n",
    "    diff = arrays[cond_a] - arrays[cond_b]  # positive = cond_b better\n",
    "    d = cohens_d(diff)\n",
    "    _, p = stats.ttest_1samp(diff, 0)\n",
    "    sig = '***' if p < 0.001 else '**' if p < 0.01 else '*' if p < 0.05 else 'ns'\n",
    "    print(f\"  {label:<40} {d:>+8.4f} {p:>12.2e} {sig:>5}\")\n",
    "\n",
    "# --- Doc keywords vs oracle ---\n",
    "print(f\"\\n--- Doc relevance vs query relevance ---\")\n",
    "diff_dq = arrays['doc_keywords'] - arrays['oracle']\n",
    "d_dq = cohens_d(diff_dq)\n",
    "_, p_dq = stats.ttest_1samp(diff_dq, 0)\n",
    "sig_dq = '***' if p_dq < 0.001 else '**' if p_dq < 0.01 else '*' if p_dq < 0.05 else 'ns'\n",
    "print(f\"  doc_keywords vs oracle: d = {d_dq:+.4f} ({sig_dq})\")\n",
    "if d_dq > 0.05:\n",
    "    print(f\"  -> Oracle is better: query-specific semantics matter\")\n",
    "elif d_dq < -0.05:\n",
    "    print(f\"  -> Doc keywords are better: doc-derived priming is stronger\")\n",
    "else:\n",
    "    print(f\"  -> No significant difference\")\n",
    "\n",
    "# --- Structure vs semantics ---\n",
    "print(f\"\\n--- Structure vs semantics ---\")\n",
    "diff_ss = arrays['best_structural'] - arrays['oracle']\n",
    "d_ss = cohens_d(diff_ss)\n",
    "_, p_ss = stats.ttest_1samp(diff_ss, 0)\n",
    "sig_ss = '***' if p_ss < 0.001 else '**' if p_ss < 0.01 else '*' if p_ss < 0.05 else 'ns'\n",
    "print(f\"  best_structural (prune_first_3) vs oracle: d = {d_ss:+.4f} ({sig_ss})\")\n",
    "if d_ss > 0.05:\n",
    "    print(f\"  -> Oracle beats best structural: semantic priming adds value\")\n",
    "elif d_ss < -0.05:\n",
    "    print(f\"  -> Best structural beats oracle: structure > semantics\")\n",
    "else:\n",
    "    print(f\"  -> No significant difference\")\n",
    "\n",
    "# --- Is there ANY semantic gradient at all? ---\n",
    "print(f\"\\n--- Overall semantic test ---\")\n",
    "print(f\"  ANOVA-like: do the 6 prefixed conditions differ significantly?\")\n",
    "prefixed_names = ['repeat_token', 'random_tokens', 'unrelated_query',\n",
    "                  'shuffled_query', 'doc_keywords', 'oracle']\n",
    "prefixed_arrays = [arrays[n] for n in prefixed_names]\n",
    "F_stat, p_anova = stats.f_oneway(*prefixed_arrays)\n",
    "sig_anova = '***' if p_anova < 0.001 else '**' if p_anova < 0.01 else '*' if p_anova < 0.05 else 'ns'\n",
    "print(f\"  F = {F_stat:.2f}, p = {p_anova:.2e} ({sig_anova})\")\n",
    "if p_anova < 0.05:\n",
    "    print(f\"  -> YES: prefix content significantly affects NLL\")\n",
    "else:\n",
    "    print(f\"  -> NO: prefix content does not significantly affect NLL\")\n",
    "    print(f\"  -> The structural effect is EVERYTHING; semantics are zero\")\n",
    "\n",
    "# --- Strongest single semantic contrast: repeat_token vs oracle ---\n",
    "print(f\"\\n--- Strongest semantic contrast: repeat_token vs oracle ---\")\n",
    "diff_max = arrays['repeat_token'] - arrays['oracle']\n",
    "d_max = cohens_d(diff_max)\n",
    "_, p_max = stats.ttest_1samp(diff_max, 0)\n",
    "sig_max = '***' if p_max < 0.001 else '**' if p_max < 0.01 else '*' if p_max < 0.05 else 'ns'\n",
    "print(f\"  d = {d_max:+.4f} ({sig_max}), p = {p_max:.2e}\")\n",
    "print(f\"  repeat_token NLL = {arrays['repeat_token'].mean():.4f}\")\n",
    "print(f\"  oracle NLL       = {arrays['oracle'].mean():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf564d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Verdict\n",
    "print(\"=\" * 70)\n",
    "print(\"VERDICT — Exp 05: Semantic Priming in Isolation\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\nModel: {MODEL_NAME}\")\n",
    "print(f\"N: {len(results)} samples (MS MARCO v1.1)\")\n",
    "print(f\"Mean query tokens: {np.mean([r['Q'] for r in results]):.1f}\")\n",
    "\n",
    "# All conditions ranked\n",
    "print(f\"\\n--- All conditions (ranked by d vs bare) ---\")\n",
    "all_ranked = sorted(analysis.items(),\n",
    "                    key=lambda x: x[1].get('d', -999), reverse=True)\n",
    "for name, info in all_ranked:\n",
    "    if name == 'bare':\n",
    "        print(f\"  {name:<24} NLL = {info['mean_nll']:.4f}  (baseline)\")\n",
    "    else:\n",
    "        print(f\"  {name:<24} NLL = {info['mean_nll']:.4f}  \"\n",
    "              f\"d = {info['d']:+.4f}  ({info['recovery']:.0f}% recovery)\")\n",
    "\n",
    "# Classification of result\n",
    "print(f\"\\n--- Classification ---\")\n",
    "d_repeat = analysis.get('repeat_token', {}).get('d', 0)\n",
    "d_oracle = analysis.get('oracle', {}).get('d', 0)\n",
    "d_random = analysis.get('random_tokens', {}).get('d', 0)\n",
    "d_shuffled = analysis.get('shuffled_query', {}).get('d', 0)\n",
    "\n",
    "# Test repeat vs oracle\n",
    "diff_ro = arrays['repeat_token'] - arrays['oracle']\n",
    "_, p_ro = stats.ttest_1samp(diff_ro, 0)\n",
    "\n",
    "if p_ro < 0.01 and d_oracle > d_repeat + 0.05:\n",
    "    print(f\"  SEMANTIC PRIMING IS REAL\")\n",
    "    print(f\"  Oracle significantly outperforms matched-structure repeat_token\")\n",
    "    print(f\"  Effect size of pure semantics: d = {cohens_d(diff_ro):+.4f}\")\n",
    "\n",
    "    # Where does the semantic benefit come from?\n",
    "    diff_rv = arrays['repeat_token'] - arrays['random_tokens']\n",
    "    diff_ru = arrays['random_tokens'] - arrays['unrelated_query']\n",
    "    diff_us = arrays['unrelated_query'] - arrays['shuffled_query']\n",
    "    diff_so = arrays['shuffled_query'] - arrays['oracle']\n",
    "\n",
    "    total_sem = diff_ro.mean()\n",
    "    print(f\"\\n  Semantic decomposition (total = {total_sem:+.4f}):\")\n",
    "    for label, diff_step in [\n",
    "        (\"Embedding variation (repeat→random)\", diff_rv),\n",
    "        (\"Natural language (random→unrelated)\", diff_ru),\n",
    "        (\"Vocabulary match (unrelated→shuffled)\", diff_us),\n",
    "        (\"Word order (shuffled→oracle)\", diff_so),\n",
    "    ]:\n",
    "        step_mean = diff_step.mean()\n",
    "        _, step_p = stats.ttest_1samp(diff_step, 0)\n",
    "        sig = '***' if step_p < 0.001 else '**' if step_p < 0.01 else '*' if step_p < 0.05 else 'ns'\n",
    "        pct = step_mean / total_sem * 100 if total_sem != 0 else 0\n",
    "        print(f\"    {label}: {step_mean:+.4f} ({pct:>5.1f}%) ({sig})\")\n",
    "else:\n",
    "    print(f\"  SEMANTIC PRIMING IS NEGLIGIBLE\")\n",
    "    print(f\"  Oracle does NOT significantly outperform repeat_token\")\n",
    "    print(f\"  The entire benefit of prefix co-encoding is structural\")\n",
    "    print(f\"  (BOS removal + position offset + attention enrichment)\")\n",
    "\n",
    "# Save\n",
    "final_results = {\n",
    "    'experiment': 'v4_decoder_only_exp05_semantic_isolation',\n",
    "    'model': MODEL_NAME,\n",
    "    'dataset': 'ms_marco_v1.1',\n",
    "    'n_samples': len(results),\n",
    "    'seed': SEED,\n",
    "    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'conditions': {k: v for k, v in analysis.items()},\n",
    "    'query_token_stats': {\n",
    "        'mean': float(np.mean([r['Q'] for r in results])),\n",
    "        'median': float(np.median([r['Q'] for r in results])),\n",
    "        'min': int(np.min([r['Q'] for r in results])),\n",
    "        'max': int(np.max([r['Q'] for r in results])),\n",
    "    },\n",
    "}\n",
    "\n",
    "with open(RESULTS_DIR / 'results.json', 'w') as f:\n",
    "    json.dump(final_results, f, indent=2)\n",
    "print(f\"\\nResults saved to {RESULTS_DIR / 'results.json'}\")\n",
    "\n",
    "# Cleanup\n",
    "print(f\"\\nCleaning up GPU memory...\")\n",
    "mem_before = torch.cuda.memory_allocated() / 1e9\n",
    "del model, tokenizer\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "mem_after = torch.cuda.memory_allocated() / 1e9\n",
    "print(f\"GPU memory: {mem_before:.2f} GB -> {mem_after:.2f} GB\")\n",
    "print(\"Done!\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
