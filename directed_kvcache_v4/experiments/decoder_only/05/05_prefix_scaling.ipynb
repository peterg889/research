{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27e600de",
   "metadata": {},
   "source": [
    "# Experiment 05: Scaling the Prefix Effect — New Benchmarks + Length Optimization\n",
    "\n",
    "## Motivation\n",
    "\n",
    "Exp 04 decomposed the extractor\\_matched effect into three components:\n",
    "- **Structural** (any tokens in cache): 39% of total effect\n",
    "- **Vocabulary** (instruction tokens vs random): **73%** of total — the dominant factor\n",
    "- **Meaning** (coherent order vs scrambled): **-11%** — meaning is negligible or slightly harmful\n",
    "\n",
    "Key Exp 04 findings:\n",
    "- `comprehend` is the best prefix (pooled d=+0.470 \\*\\*\\*), beating extract\\_general (d=+0.357)\n",
    "- `comprehend` is the ONLY instruction with a significant positive meaning effect (d=+0.235)\n",
    "- Scrambled instructions often match or beat coherent ones\n",
    "- Extraction framing is NOT specifically better than other framings (H1 rejected)\n",
    "- Longer instruction > shorter repeated (H5 refuted, d=-0.176)\n",
    "\n",
    "**What we don't know:**\n",
    "1. All experiments use Q-matched prefix length (mean 6-19 tokens) — prefix length has never been varied independently\n",
    "2. All experiments test extractive QA only — does the effect generalize to reasoning, boolean QA, exam comprehension?\n",
    "3. Does lower NLL translate to better generated answers?\n",
    "\n",
    "## Design\n",
    "\n",
    "### Two Experimental Phases\n",
    "\n",
    "**Phase A: Q-matched scoring on new benchmarks** (comparison with Exp 04)\n",
    "- Score 3 new datasets (DROP, BoolQ, RACE-high) with 5 conditions\n",
    "- Tests: Does the vocabulary-dominated effect generalize to reasoning tasks?\n",
    "\n",
    "**Phase B: Fixed-length prefix scaling across all 7 datasets**\n",
    "- Fixed prefix lengths: L = 32, 64, 128, 256 tokens\n",
    "- Conditions at each L: bare\\_trunc, random\\_tokens\\_L, comprehend\\_L, extract\\_general\\_L, scrambled\\_comprehend\\_L\n",
    "- All docs truncated to common max\\_doc for consistent cross-length comparison\n",
    "\n",
    "### Conditions Summary\n",
    "\n",
    "**Phase A (Q-matched, 3 new datasets only):** 5 conditions\n",
    "1. `bare` — no prefix\n",
    "2. `random_tokens_Q` — Q random tokens\n",
    "3. `comprehend_Q` — comprehend instruction at Q tokens\n",
    "4. `extract_general_Q` — extract instruction at Q tokens\n",
    "5. `scrambled_comprehend_Q` — scrambled comprehend at Q tokens\n",
    "\n",
    "**Phase B (fixed-length, all 7 datasets):** 4 lengths x 4 conditions + 1 bare = 17 per sample\n",
    "- `bare_trunc` — bare on truncated doc (scored once)\n",
    "- At each L in {32, 64, 128, 256}: random\\_tokens\\_L, comprehend\\_L, extract\\_general\\_L, scrambled\\_comprehend\\_L\n",
    "\n",
    "**Loaded from Exp 03/04 (existing 4 datasets):** Q-matched bare, random\\_tokens, comprehend, extract\\_general, scrambled\\_comprehend\n",
    "\n",
    "### New Benchmarks\n",
    "\n",
    "| Dataset | Task type | Answer format | Why it's harder |\n",
    "|---------|-----------|---------------|-----------------|\n",
    "| **DROP** | Discrete reasoning | Numbers, dates, spans | Counting, arithmetic, sorting over text |\n",
    "| **BoolQ** | Boolean reasoning | \"Yes\" / \"No\" | Requires inference, not just extraction |\n",
    "| **RACE-high** | Exam comprehension | MC text (4 options) | English exam questions, deeper comprehension |\n",
    "\n",
    "### Three-Level Decomposition at Each Length\n",
    "\n",
    "For each length L:\n",
    "- **Structural\\_L** = NLL(bare\\_trunc) - NLL(random\\_L)\n",
    "- **Vocabulary\\_L** = NLL(random\\_L) - NLL(scrambled\\_comprehend\\_L)\n",
    "- **Meaning\\_L** = NLL(scrambled\\_comprehend\\_L) - NLL(comprehend\\_L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed35f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Setup, model loading, and scoring functions\n",
    "import os\n",
    "os.umask(0o000)\n",
    "import sys, json, time, gc, re\n",
    "import random as pyrandom\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from scipy import stats\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "sys.path.insert(0, \"../../..\")\n",
    "from lib.analysis import cohens_d\n",
    "from lib.data import count_words\n",
    "\n",
    "SEED = 42\n",
    "N_SAMPLES = 400      # per dataset\n",
    "HARD_FRAC = 0.40     # top 40% by bare NLL\n",
    "MODEL_NAME = \"google/gemma-3-12b-it\"\n",
    "\n",
    "RESULTS_DIR = Path(\"../../../results/decoder_only/exp05\")\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "EXP02_DIR = Path(\"../../../results/decoder_only/exp02\")\n",
    "EXP03_DIR = Path(\"../../../results/decoder_only/exp03\")\n",
    "EXP04_DIR = Path(\"../../../results/decoder_only/exp04\")\n",
    "\n",
    "# All 7 datasets\n",
    "OLD_DATASETS = ['ms_marco', 'squad_v2', 'triviaqa', 'hotpotqa']\n",
    "NEW_DATASETS = ['drop', 'boolq', 'race_high']\n",
    "ALL_DATASETS = OLD_DATASETS + NEW_DATASETS\n",
    "\n",
    "# Phase B prefix lengths\n",
    "PREFIX_LENGTHS = [32, 64, 128, 256]\n",
    "\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "pyrandom.seed(SEED)\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv())\n",
    "HF_TOKEN = os.environ.get(\"HF_TOKEN\")\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, DynamicCache\n",
    "\n",
    "print(f\"Loading {MODEL_NAME}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, token=HF_TOKEN)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME, device_map=\"auto\", dtype=torch.bfloat16, token=HF_TOKEN,\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "DEVICE = next(model.parameters()).device\n",
    "BOS_ID = tokenizer.bos_token_id\n",
    "NEWLINE_IDS = tokenizer(\"\\n\", add_special_tokens=False).input_ids\n",
    "text_cfg = getattr(model.config, 'text_config', model.config)\n",
    "VOCAB_SIZE = model.get_input_embeddings().num_embeddings\n",
    "cfg_vocab = getattr(text_cfg, 'vocab_size', None)\n",
    "if cfg_vocab != VOCAB_SIZE:\n",
    "    print(f\"WARNING: config vocab_size={cfg_vocab} != embedding size={VOCAB_SIZE}\")\n",
    "    print(f\"Using embedding size {VOCAB_SIZE} for random token generation\")\n",
    "rope_params = getattr(text_cfg, 'rope_parameters', {})\n",
    "layer_types = getattr(text_cfg, 'layer_types', [])\n",
    "SLIDING_WINDOW = getattr(text_cfg, 'sliding_window', 4096)\n",
    "SLIDING_CACHE_LIMIT = SLIDING_WINDOW - 1  # 1023 for Gemma 3\n",
    "\n",
    "# Common max doc length for Phase B: use L=256 (max prefix) to ensure all fit\n",
    "NL = len(NEWLINE_IDS)\n",
    "COMMON_MAX_DOC = SLIDING_CACHE_LIMIT - 1 - max(PREFIX_LENGTHS) - NL  # 765 for Gemma 3\n",
    "\n",
    "N_HARD = int(N_SAMPLES * HARD_FRAC)\n",
    "\n",
    "print(f\"Exp 05: Scaling the Prefix Effect — New Benchmarks + Length Optimization\")\n",
    "print(f\"Scoring: BOS-retained repositioning + token-level prefix matching\")\n",
    "print(f\"N_SAMPLES: {N_SAMPLES} per dataset, HARD_FRAC: {HARD_FRAC}, N_HARD: {N_HARD}\")\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"DEVICE: {DEVICE}, dtype: {next(model.parameters()).dtype}\")\n",
    "print(f\"GPU memory: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "print(f\"Vocab size: {VOCAB_SIZE}\")\n",
    "print(f\"Sliding window: {SLIDING_WINDOW}, cache limit: {SLIDING_CACHE_LIMIT}\")\n",
    "print(f\"COMMON_MAX_DOC: {COMMON_MAX_DOC} tokens (for Phase B)\")\n",
    "print(f\"Prefix lengths: {PREFIX_LENGTHS}\")\n",
    "\n",
    "# --- RoPE repositioning helpers ---\n",
    "def build_layer_inv_freqs():\n",
    "    inv_freqs = {}\n",
    "    for lt, params in rope_params.items():\n",
    "        theta = params.get('rope_theta', 10000.0)\n",
    "        dim = text_cfg.head_dim\n",
    "        inv_freq = 1.0 / (theta ** (torch.arange(0, dim, 2, dtype=torch.float32, device=DEVICE) / dim))\n",
    "        inv_freqs[lt] = inv_freq\n",
    "    return inv_freqs\n",
    "\n",
    "LAYER_INV_FREQS = build_layer_inv_freqs()\n",
    "\n",
    "\n",
    "def rotate_half(x):\n",
    "    x1, x2 = x[..., :x.shape[-1]//2], x[..., x.shape[-1]//2:]\n",
    "    return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "\n",
    "def select_kv_cache(cache, indices):\n",
    "    selected = DynamicCache()\n",
    "    idx_tensor = torch.tensor(indices, dtype=torch.long, device=DEVICE)\n",
    "    for i in range(len(cache.layers)):\n",
    "        k = cache.layers[i].keys[:, :, idx_tensor, :]\n",
    "        v = cache.layers[i].values[:, :, idx_tensor, :]\n",
    "        selected.update(k, v, i)\n",
    "    return selected\n",
    "\n",
    "\n",
    "def reposition_kv_cache(cache, old_positions, new_positions, bos_start=0):\n",
    "    delta = new_positions - old_positions\n",
    "    for L in range(len(cache.layers)):\n",
    "        lt = layer_types[L]\n",
    "        inv_freq = LAYER_INV_FREQS[lt]\n",
    "        k = cache.layers[L].keys\n",
    "        doc_keys = k[:, :, bos_start + 1:, :]\n",
    "        freqs = torch.einsum('i,j->ij', delta.float(), inv_freq)\n",
    "        emb = torch.cat([freqs, freqs], dim=-1)\n",
    "        cos_delta = emb.cos().to(k.dtype).unsqueeze(0).unsqueeze(0)\n",
    "        sin_delta = emb.sin().to(k.dtype).unsqueeze(0).unsqueeze(0)\n",
    "        doc_keys_new = doc_keys * cos_delta + rotate_half(doc_keys) * sin_delta\n",
    "        cache.layers[L].keys = torch.cat([\n",
    "            k[:, :, :bos_start + 1, :],\n",
    "            doc_keys_new,\n",
    "        ], dim=2)\n",
    "    return cache\n",
    "\n",
    "\n",
    "def score(doc_text, query_text, answer_text, prefix_token_ids=None,\n",
    "          max_doc_override=None):\n",
    "    # BOS-retained repositioning.\n",
    "    doc_ids = tokenizer(doc_text, add_special_tokens=False,\n",
    "                        truncation=True, max_length=1024).input_ids\n",
    "\n",
    "    # Apply max_doc_override for Phase B consistent truncation\n",
    "    if max_doc_override is not None and len(doc_ids) > max_doc_override:\n",
    "        doc_ids = doc_ids[:max_doc_override]\n",
    "\n",
    "    if prefix_token_ids is not None:\n",
    "        P = len(prefix_token_ids)\n",
    "        _NL = len(NEWLINE_IDS)\n",
    "        max_doc = SLIDING_CACHE_LIMIT - 1 - P - _NL  # 1 for BOS\n",
    "        if len(doc_ids) > max_doc:\n",
    "            doc_ids = doc_ids[:max_doc]\n",
    "        D = len(doc_ids)\n",
    "        cond_ids = [BOS_ID] + list(prefix_token_ids) + NEWLINE_IDS + doc_ids\n",
    "        with torch.no_grad():\n",
    "            pa = model(input_ids=torch.tensor([cond_ids], device=DEVICE),\n",
    "                       use_cache=True)\n",
    "        cache = pa.past_key_values\n",
    "        del pa\n",
    "        keep_indices = [0] + list(range(1 + P + _NL, len(cond_ids)))\n",
    "        cache = select_kv_cache(cache, keep_indices)\n",
    "        old_pos = torch.arange(1 + P + _NL, 1 + P + _NL + D, device=DEVICE)\n",
    "        new_pos = torch.arange(1, D + 1, device=DEVICE)\n",
    "        cache = reposition_kv_cache(cache, old_pos, new_pos, bos_start=0)\n",
    "    else:\n",
    "        D = len(doc_ids)\n",
    "        with torch.no_grad():\n",
    "            pa = model(input_ids=torch.tensor([[BOS_ID] + doc_ids], device=DEVICE),\n",
    "                       use_cache=True)\n",
    "        cache = pa.past_key_values\n",
    "        del pa\n",
    "\n",
    "    phase_b_start = D + 1\n",
    "    query_ids = tokenizer(\"\\n\" + query_text + \"\\n\",\n",
    "                          add_special_tokens=False).input_ids\n",
    "    answer_ids = tokenizer(answer_text, add_special_tokens=False,\n",
    "                           truncation=True, max_length=256).input_ids\n",
    "    if not answer_ids:\n",
    "        del cache\n",
    "        return 0.0\n",
    "\n",
    "    pb_ids = query_ids + answer_ids\n",
    "    pos = torch.arange(phase_b_start, phase_b_start + len(pb_ids), device=DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pb = model(\n",
    "            input_ids=torch.tensor([pb_ids], device=DEVICE),\n",
    "            past_key_values=cache,\n",
    "            position_ids=pos.unsqueeze(0),\n",
    "            use_cache=False,\n",
    "        )\n",
    "\n",
    "    n_q = len(query_ids)\n",
    "    logits = pb.logits[0, n_q - 1:n_q - 1 + len(answer_ids), :].float()\n",
    "    targets = torch.tensor(answer_ids, device=DEVICE)\n",
    "    nll = -F.log_softmax(logits, dim=-1).gather(\n",
    "        1, targets.unsqueeze(1)).squeeze(1).mean().item()\n",
    "    del cache, pb\n",
    "    return nll\n",
    "\n",
    "\n",
    "def make_prefix(token_ids, L):\n",
    "    # Pad/truncate instruction to exactly L tokens.\n",
    "    if len(token_ids) >= L:\n",
    "        return token_ids[:L]\n",
    "    padded = token_ids * ((L // max(len(token_ids), 1)) + 1)\n",
    "    return padded[:L]\n",
    "\n",
    "\n",
    "def scramble_prefix(prefix_ids, seed):\n",
    "    rng = pyrandom.Random(seed)\n",
    "    shuffled = list(prefix_ids)\n",
    "    rng.shuffle(shuffled)\n",
    "    return shuffled\n",
    "\n",
    "\n",
    "# --- Instruction definitions (from Exp 04) ---\n",
    "INSTRUCTIONS = {\n",
    "    'extract_general': \"Extract all key data points, facts, entities, and specific attributes from the following text.\",\n",
    "    'comprehend': \"Read and understand the main ideas, arguments, and supporting details presented in the following text.\",\n",
    "}\n",
    "\n",
    "# Pre-tokenize instructions\n",
    "INSTRUCTION_IDS = {}\n",
    "for name, text in INSTRUCTIONS.items():\n",
    "    ids = tokenizer(text, add_special_tokens=False).input_ids\n",
    "    INSTRUCTION_IDS[name] = ids\n",
    "    print(f\"  {name:<20}: {len(ids)} tokens -> '{text[:60]}...'\")\n",
    "\n",
    "# Phase A Q-matched conditions (for new datasets)\n",
    "PHASE_A_PREFIX_CONDS = ['random_tokens', 'comprehend', 'extract_general',\n",
    "                        'scrambled_comprehend']\n",
    "\n",
    "# Phase B fixed-length conditions (for all datasets)\n",
    "PHASE_B_PREFIX_CONDS = ['random_tokens', 'comprehend', 'extract_general',\n",
    "                        'scrambled_comprehend']\n",
    "\n",
    "# Per-dataset seeds (extending Exp 03 pattern)\n",
    "DS_SEEDS = {\n",
    "    'squad_v2': SEED + 100,\n",
    "    'triviaqa': SEED + 200,\n",
    "    'hotpotqa': SEED + 300,\n",
    "    'drop': SEED + 400,\n",
    "    'boolq': SEED + 500,\n",
    "    'race_high': SEED + 600,\n",
    "}\n",
    "\n",
    "SCORING_KEY = 'bos_retained_token_matched_v05'\n",
    "\n",
    "special_ids = set(tokenizer.all_special_ids)\n",
    "\n",
    "print(f\"\\nSetup complete. Functions: score, make_prefix, scramble_prefix\")\n",
    "print(f\"Phase A conditions: bare + {PHASE_A_PREFIX_CONDS}\")\n",
    "print(f\"Phase B conditions: bare_trunc + {len(PREFIX_LENGTHS)} lengths x {len(PHASE_B_PREFIX_CONDS)} prefixes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34337ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Load existing 4 datasets + Exp 03/04 Q-matched baselines\n",
    "from datasets import load_dataset\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"LOADING EXISTING 4 DATASETS + EXP 03/04 BASELINES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "hard_nlls = {}      # ds_name -> {cond_name: np.array}\n",
    "hard_metadata = {}  # ds_name -> dict\n",
    "hard_samples = {}   # ds_name -> list of hard sample dicts\n",
    "all_samples = {}    # ds_name -> list of N_SAMPLES sample dicts\n",
    "\n",
    "# ================================================================\n",
    "# PART 1: Load MS MARCO from Exp 02\n",
    "# ================================================================\n",
    "print(\"\\n--- MS MARCO from Exp 02 + Exp 03 baselines ---\")\n",
    "assert EXP02_DIR.exists(), f\"Exp 02 results not found at {EXP02_DIR}\"\n",
    "exp02_ckpt = json.loads((EXP02_DIR / \"checkpoint.json\").read_text())\n",
    "exp02_results = exp02_ckpt['results']\n",
    "assert len(exp02_results) == N_SAMPLES\n",
    "\n",
    "msmarco_bare = np.array([r['nll_bare'] for r in exp02_results])\n",
    "sorted_idx = np.argsort(msmarco_bare)[::-1]\n",
    "msmarco_hard_idx = np.sort(sorted_idx[:N_HARD])\n",
    "\n",
    "# Load passage text\n",
    "print(\"  Reloading MS MARCO v1.1 for passage text...\")\n",
    "ds_msmarco = load_dataset(\"microsoft/ms_marco\", \"v1.1\", split=\"validation\")\n",
    "msmarco_candidates = []\n",
    "for item in ds_msmarco:\n",
    "    if len(msmarco_candidates) >= 3 * N_SAMPLES:\n",
    "        break\n",
    "    passages = item.get('passages', {})\n",
    "    ptexts = passages.get('passage_text', [])\n",
    "    is_sel = passages.get('is_selected', [])\n",
    "    query = item.get('query', '')\n",
    "    answers = item.get('answers', [])\n",
    "    well_formed = item.get('wellFormedAnswers', [])\n",
    "    answer = None\n",
    "    if well_formed and len(well_formed) > 0 and well_formed[0] not in ('[]', ''):\n",
    "        answer = well_formed[0]\n",
    "    elif answers and len(answers) > 0 and answers[0] != 'No Answer Present.':\n",
    "        answer = answers[0]\n",
    "    if not answer:\n",
    "        continue\n",
    "    for pt, sel in zip(ptexts, is_sel):\n",
    "        wc = count_words(pt)\n",
    "        if sel == 1 and 30 <= wc <= 300:\n",
    "            msmarco_candidates.append({\n",
    "                'passage': pt, 'query': query, 'answer': answer,\n",
    "                'word_count': wc,\n",
    "            })\n",
    "            break\n",
    "\n",
    "np.random.seed(SEED)\n",
    "indices = np.random.permutation(len(msmarco_candidates))[:N_SAMPLES]\n",
    "msmarco_all = [msmarco_candidates[i] for i in indices]\n",
    "del ds_msmarco, msmarco_candidates\n",
    "gc.collect()\n",
    "\n",
    "# Verify alignment\n",
    "for i in range(min(20, N_SAMPLES)):\n",
    "    assert msmarco_all[i]['query'][:50] == exp02_results[i]['query'][:50], \\\n",
    "        f\"MS MARCO query mismatch at sample {i}\"\n",
    "print(\"  MS MARCO alignment verified\")\n",
    "\n",
    "hs_msmarco = []\n",
    "for idx in msmarco_hard_idx:\n",
    "    s = dict(msmarco_all[idx])\n",
    "    s['nll_bare'] = float(msmarco_bare[idx])\n",
    "    s['original_idx'] = int(idx)\n",
    "    hs_msmarco.append(s)\n",
    "hard_samples['ms_marco'] = hs_msmarco\n",
    "all_samples['ms_marco'] = msmarco_all\n",
    "\n",
    "# Load baselines: bare, random_tokens from Exp 03\n",
    "hard_nlls['ms_marco'] = {}\n",
    "hard_nlls['ms_marco']['bare'] = msmarco_bare[msmarco_hard_idx]\n",
    "for cond in ['random_tokens']:\n",
    "    hard_nlls['ms_marco'][cond] = np.array(\n",
    "        [exp02_results[i][f'nll_{cond}'] for i in msmarco_hard_idx])\n",
    "# extract_general = extractor_matched from Exp 03\n",
    "hard_nlls['ms_marco']['extract_general'] = np.array(\n",
    "    [exp02_results[i]['nll_extractor_matched'] for i in msmarco_hard_idx])\n",
    "\n",
    "# Load comprehend + scrambled_comprehend from Exp 04\n",
    "exp04_ckpt_msmarco = json.loads(\n",
    "    (EXP04_DIR / \"checkpoint_ms_marco.json\").read_text())\n",
    "exp04_results_msmarco = exp04_ckpt_msmarco['results']\n",
    "hard_nlls['ms_marco']['comprehend'] = np.array(\n",
    "    [r['nll_comprehend'] for r in exp04_results_msmarco])\n",
    "hard_nlls['ms_marco']['scrambled_comprehend'] = np.array(\n",
    "    [r['nll_scrambled_comprehend'] for r in exp04_results_msmarco])\n",
    "hard_nlls['ms_marco']['scrambled_extract_general'] = np.array(\n",
    "    [r['nll_scrambled_extract_general'] for r in exp04_results_msmarco])\n",
    "\n",
    "hard_metadata['ms_marco'] = {\n",
    "    'n_total': N_SAMPLES, 'n_hard': N_HARD, 'source': 'exp02_reuse',\n",
    "    'mean_passage_words': float(np.mean([s['word_count'] for s in hs_msmarco])),\n",
    "}\n",
    "print(f\"  MS MARCO: {N_HARD} hard, loaded bare/random_tokens/extract_general/\"\n",
    "      f\"comprehend/scrambled_comprehend\")\n",
    "\n",
    "del exp02_ckpt, exp02_results, exp04_ckpt_msmarco, exp04_results_msmarco\n",
    "gc.collect()\n",
    "\n",
    "# ================================================================\n",
    "# PART 2: Load 3 existing datasets (SQuAD, TriviaQA, HotpotQA)\n",
    "# ================================================================\n",
    "# --- SQuAD 2.0 ---\n",
    "print(\"\\n--- SQuAD 2.0 ---\")\n",
    "ds_squad = load_dataset(\"rajpurkar/squad_v2\", split=\"validation\")\n",
    "squad_candidates = []\n",
    "for item in ds_squad:\n",
    "    answers = item.get('answers', {})\n",
    "    answer_texts = answers.get('text', [])\n",
    "    if not answer_texts:\n",
    "        continue\n",
    "    passage = item['context']\n",
    "    query = item['question']\n",
    "    answer = answer_texts[0]\n",
    "    wc = count_words(passage)\n",
    "    if 30 <= wc <= 500 and count_words(answer) >= 1:\n",
    "        squad_candidates.append({\n",
    "            'passage': passage, 'query': query, 'answer': answer,\n",
    "            'word_count': wc,\n",
    "        })\n",
    "np.random.seed(DS_SEEDS['squad_v2'])\n",
    "sq_indices = np.random.permutation(len(squad_candidates))[:N_SAMPLES]\n",
    "all_samples['squad_v2'] = [squad_candidates[i] for i in sq_indices]\n",
    "del ds_squad, squad_candidates\n",
    "gc.collect()\n",
    "\n",
    "# --- TriviaQA ---\n",
    "print(\"--- TriviaQA ---\")\n",
    "ds_trivia = load_dataset(\"mandarjoshi/trivia_qa\", \"rc.wikipedia\", split=\"validation\")\n",
    "trivia_candidates = []\n",
    "for item in ds_trivia:\n",
    "    entity_pages = item.get('entity_pages', {})\n",
    "    wiki_contexts = entity_pages.get('wiki_context', [])\n",
    "    if not wiki_contexts or not wiki_contexts[0]:\n",
    "        continue\n",
    "    words = wiki_contexts[0].split()[:500]\n",
    "    passage = ' '.join(words)\n",
    "    query = item['question']\n",
    "    answer_val = item['answer']['value']\n",
    "    aliases = item['answer'].get('aliases', [])\n",
    "    passage_lower = passage.lower()\n",
    "    found = answer_val.lower() in passage_lower\n",
    "    if not found:\n",
    "        for alias in aliases:\n",
    "            if alias.lower() in passage_lower:\n",
    "                found = True\n",
    "                break\n",
    "    if not found:\n",
    "        continue\n",
    "    wc = count_words(passage)\n",
    "    if 30 <= wc <= 500 and count_words(answer_val) >= 1:\n",
    "        trivia_candidates.append({\n",
    "            'passage': passage, 'query': query, 'answer': answer_val,\n",
    "            'word_count': wc,\n",
    "        })\n",
    "np.random.seed(DS_SEEDS['triviaqa'])\n",
    "tr_indices = np.random.permutation(len(trivia_candidates))[:N_SAMPLES]\n",
    "all_samples['triviaqa'] = [trivia_candidates[i] for i in tr_indices]\n",
    "del ds_trivia, trivia_candidates\n",
    "gc.collect()\n",
    "\n",
    "# --- HotpotQA ---\n",
    "print(\"--- HotpotQA ---\")\n",
    "ds_hotpot = load_dataset(\"hotpotqa/hotpot_qa\", \"distractor\", split=\"validation\")\n",
    "hotpot_candidates = []\n",
    "for item in ds_hotpot:\n",
    "    context = item.get('context', {})\n",
    "    sf = item.get('supporting_facts', {})\n",
    "    ctx_titles = context.get('title', [])\n",
    "    ctx_sentences = context.get('sentences', [])\n",
    "    sf_titles = sf.get('title', [])\n",
    "    sf_sent_ids = sf.get('sent_id', [])\n",
    "    title_to_sents = {}\n",
    "    for title, sents in zip(ctx_titles, ctx_sentences):\n",
    "        title_to_sents[title] = sents\n",
    "    passage_parts = []\n",
    "    for title, sid in zip(sf_titles, sf_sent_ids):\n",
    "        if title in title_to_sents and sid < len(title_to_sents[title]):\n",
    "            passage_parts.append(title_to_sents[title][sid])\n",
    "    if not passage_parts:\n",
    "        continue\n",
    "    passage = ' '.join(passage_parts)\n",
    "    query = item['question']\n",
    "    answer = item['answer']\n",
    "    wc = count_words(passage)\n",
    "    if 30 <= wc <= 500 and count_words(answer) >= 1:\n",
    "        hotpot_candidates.append({\n",
    "            'passage': passage, 'query': query, 'answer': answer,\n",
    "            'word_count': wc,\n",
    "        })\n",
    "np.random.seed(DS_SEEDS['hotpotqa'])\n",
    "hp_indices = np.random.permutation(len(hotpot_candidates))[:N_SAMPLES]\n",
    "all_samples['hotpotqa'] = [hotpot_candidates[i] for i in hp_indices]\n",
    "del ds_hotpot, hotpot_candidates\n",
    "gc.collect()\n",
    "\n",
    "# ================================================================\n",
    "# PART 3: Load Exp 03/04 baselines for SQuAD, TriviaQA, HotpotQA\n",
    "# ================================================================\n",
    "print(\"\\n--- Loading Exp 03/04 baselines ---\")\n",
    "for ds_name in ['squad_v2', 'triviaqa', 'hotpotqa']:\n",
    "    samples_ds = all_samples[ds_name]\n",
    "\n",
    "    # Bare NLLs from Exp 03\n",
    "    bare_path = EXP03_DIR / f\"bare_{ds_name}.json\"\n",
    "    bare_ckpt = json.loads(bare_path.read_text())\n",
    "    assert bare_ckpt.get('n_total') == N_SAMPLES\n",
    "    bare_nlls_all = bare_ckpt['bare_nlls']\n",
    "\n",
    "    # Verify alignment\n",
    "    saved_queries = bare_ckpt.get('queries_first50', [])\n",
    "    current_queries = [s['query'][:50] for s in samples_ds[:len(saved_queries)]]\n",
    "    assert saved_queries == current_queries, \\\n",
    "        f\"{ds_name}: query alignment mismatch with Exp 03\"\n",
    "\n",
    "    bare_arr = np.array(bare_nlls_all)\n",
    "    sorted_idx = np.argsort(bare_arr)[::-1]\n",
    "    h_idx = np.sort(sorted_idx[:N_HARD])\n",
    "\n",
    "    hs = []\n",
    "    for idx in h_idx:\n",
    "        s = dict(samples_ds[idx])\n",
    "        s['nll_bare'] = float(bare_arr[idx])\n",
    "        s['original_idx'] = int(idx)\n",
    "        hs.append(s)\n",
    "    hard_samples[ds_name] = hs\n",
    "\n",
    "    # Exp 03 baselines\n",
    "    ckpt03 = json.loads((EXP03_DIR / f\"checkpoint_{ds_name}.json\").read_text())\n",
    "    exp03_results = ckpt03['results']\n",
    "    assert len(exp03_results) == N_HARD\n",
    "\n",
    "    hard_nlls[ds_name] = {}\n",
    "    hard_nlls[ds_name]['bare'] = bare_arr[h_idx]\n",
    "    hard_nlls[ds_name]['random_tokens'] = np.array(\n",
    "        [r['nll_random_tokens'] for r in exp03_results])\n",
    "    hard_nlls[ds_name]['extract_general'] = np.array(\n",
    "        [r['nll_extractor_matched'] for r in exp03_results])\n",
    "\n",
    "    # Exp 04 baselines (comprehend + scrambled_comprehend)\n",
    "    ckpt04 = json.loads((EXP04_DIR / f\"checkpoint_{ds_name}.json\").read_text())\n",
    "    exp04_results = ckpt04['results']\n",
    "    assert len(exp04_results) == N_HARD\n",
    "    hard_nlls[ds_name]['comprehend'] = np.array(\n",
    "        [r['nll_comprehend'] for r in exp04_results])\n",
    "    hard_nlls[ds_name]['scrambled_comprehend'] = np.array(\n",
    "        [r['nll_scrambled_comprehend'] for r in exp04_results])\n",
    "    hard_nlls[ds_name]['scrambled_extract_general'] = np.array(\n",
    "        [r['nll_scrambled_extract_general'] for r in exp04_results])\n",
    "\n",
    "    hard_metadata[ds_name] = {\n",
    "        'n_total': N_SAMPLES, 'n_hard': N_HARD, 'source': 'exp03_04_reuse',\n",
    "        'mean_passage_words': float(np.mean([s['word_count'] for s in hs])),\n",
    "    }\n",
    "\n",
    "    print(f\"  {ds_name}: {N_HARD} hard, loaded 6 Q-matched baselines\")\n",
    "\n",
    "del bare_ckpt, ckpt03, exp03_results, ckpt04, exp04_results\n",
    "gc.collect()\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Existing dataset loading summary:\")\n",
    "for ds_name in OLD_DATASETS:\n",
    "    n_h = len(hard_samples[ds_name])\n",
    "    conds = sorted(hard_nlls[ds_name].keys())\n",
    "    print(f\"  {ds_name}: {n_h} hard, conditions: {conds}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123ce7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Load DROP, BoolQ, RACE-high\n",
    "from datasets import load_dataset\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"LOADING 3 NEW DATASETS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# ---- DROP ----\n",
    "print(\"\\n--- DROP (ucinlp/drop, validation) ---\")\n",
    "ds_drop = load_dataset(\"ucinlp/drop\", split=\"validation\")\n",
    "\n",
    "drop_candidates = []\n",
    "for item in ds_drop:\n",
    "    passage = item['passage']\n",
    "    question = item['question']\n",
    "    answers_spans = item.get('answers_spans', {})\n",
    "    spans = answers_spans.get('spans', [])\n",
    "    if not spans or not spans[0]:\n",
    "        continue\n",
    "    answer = spans[0]\n",
    "    wc = count_words(passage)\n",
    "    if 30 <= wc <= 500 and count_words(answer) >= 1:\n",
    "        drop_candidates.append({\n",
    "            'passage': passage, 'query': question, 'answer': answer,\n",
    "            'word_count': wc,\n",
    "        })\n",
    "\n",
    "print(f\"  DROP candidates: {len(drop_candidates)}\")\n",
    "np.random.seed(DS_SEEDS['drop'])\n",
    "drop_indices = np.random.permutation(len(drop_candidates))[:N_SAMPLES]\n",
    "all_samples['drop'] = [drop_candidates[i] for i in drop_indices]\n",
    "del ds_drop, drop_candidates\n",
    "gc.collect()\n",
    "\n",
    "# ---- BoolQ ----\n",
    "print(\"\\n--- BoolQ (google/boolq, validation) ---\")\n",
    "ds_boolq = load_dataset(\"google/boolq\", split=\"validation\")\n",
    "\n",
    "boolq_candidates = []\n",
    "for item in ds_boolq:\n",
    "    passage = item['passage']\n",
    "    question = item['question']\n",
    "    answer = \"Yes\" if item['answer'] else \"No\"\n",
    "    wc = count_words(passage)\n",
    "    if 30 <= wc <= 500:\n",
    "        boolq_candidates.append({\n",
    "            'passage': passage, 'query': question, 'answer': answer,\n",
    "            'word_count': wc,\n",
    "        })\n",
    "\n",
    "print(f\"  BoolQ candidates: {len(boolq_candidates)}\")\n",
    "np.random.seed(DS_SEEDS['boolq'])\n",
    "boolq_indices = np.random.permutation(len(boolq_candidates))[:N_SAMPLES]\n",
    "all_samples['boolq'] = [boolq_candidates[i] for i in boolq_indices]\n",
    "del ds_boolq, boolq_candidates\n",
    "gc.collect()\n",
    "\n",
    "# ---- RACE-high ----\n",
    "print(\"\\n--- RACE (race, high, test) ---\")\n",
    "ds_race = load_dataset(\"race\", \"high\", split=\"test\")\n",
    "\n",
    "race_candidates = []\n",
    "for item in ds_race:\n",
    "    passage = item['article']\n",
    "    question = item['question']\n",
    "    correct_idx = ord(item['answer']) - ord('A')\n",
    "    options = item['options']\n",
    "    if correct_idx < 0 or correct_idx >= len(options):\n",
    "        continue\n",
    "    answer = options[correct_idx]\n",
    "    wc = count_words(passage)\n",
    "    if 30 <= wc <= 500 and count_words(answer) >= 1:\n",
    "        race_candidates.append({\n",
    "            'passage': passage, 'query': question, 'answer': answer,\n",
    "            'word_count': wc,\n",
    "            'all_options': options,\n",
    "            'correct_idx': correct_idx,\n",
    "        })\n",
    "\n",
    "print(f\"  RACE-high candidates: {len(race_candidates)}\")\n",
    "np.random.seed(DS_SEEDS['race_high'])\n",
    "race_indices = np.random.permutation(len(race_candidates))[:N_SAMPLES]\n",
    "all_samples['race_high'] = [race_candidates[i] for i in race_indices]\n",
    "del ds_race, race_candidates\n",
    "gc.collect()\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"New dataset loading summary:\")\n",
    "for ds_name in NEW_DATASETS:\n",
    "    samps = all_samples[ds_name]\n",
    "    print(f\"\\n  {ds_name}: {len(samps)} samples\")\n",
    "    print(f\"    Mean passage words: {np.mean([s['word_count'] for s in samps]):.0f}\")\n",
    "    print(f\"    Mean answer words: {np.mean([count_words(s['answer']) for s in samps]):.0f}\")\n",
    "    print(f\"    Example query: {samps[0]['query'][:70]}...\")\n",
    "    print(f\"    Example answer: {samps[0]['answer'][:70]}...\")\n",
    "    if ds_name == 'race_high':\n",
    "        print(f\"    Example options: {[o[:30] for o in samps[0]['all_options']]}\")\n",
    "        print(f\"    Correct idx: {samps[0]['correct_idx']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50488a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Bare NLL scoring for 3 new datasets + hard 40% selection\n",
    "print(\"=\" * 70)\n",
    "print(\"BARE SCORING — 3 new datasets x 400 samples\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for ds_name in NEW_DATASETS:\n",
    "    print(f\"\\n--- {ds_name} ({N_SAMPLES} samples) ---\")\n",
    "    samples = all_samples[ds_name]\n",
    "    bare_ckpt_path = RESULTS_DIR / f\"bare_{ds_name}.json\"\n",
    "\n",
    "    bare_nlls = []\n",
    "    start_idx = 0\n",
    "\n",
    "    if bare_ckpt_path.exists():\n",
    "        ckpt = json.loads(bare_ckpt_path.read_text())\n",
    "        if (ckpt.get('n_total') == N_SAMPLES and\n",
    "            ckpt.get('scoring') == SCORING_KEY and\n",
    "            ckpt.get('dataset') == ds_name):\n",
    "            saved_queries = ckpt.get('queries_first50', [])\n",
    "            current_queries = [s['query'][:50] for s in samples[:len(saved_queries)]]\n",
    "            if saved_queries == current_queries:\n",
    "                bare_nlls = ckpt['bare_nlls']\n",
    "                start_idx = len(bare_nlls)\n",
    "                print(f\"  Resuming from checkpoint: {start_idx}/{N_SAMPLES}\")\n",
    "\n",
    "    if start_idx < N_SAMPLES:\n",
    "        t0 = time.time()\n",
    "        for i in tqdm(range(start_idx, N_SAMPLES), initial=start_idx,\n",
    "                      total=N_SAMPLES, desc=f\"Bare {ds_name}\"):\n",
    "            s = samples[i]\n",
    "            nll = score(s['passage'], s['query'], s['answer'])\n",
    "            bare_nlls.append(nll)\n",
    "\n",
    "            if (i + 1) % 20 == 0 or i == N_SAMPLES - 1:\n",
    "                ckpt = {\n",
    "                    'dataset': ds_name,\n",
    "                    'n_total': N_SAMPLES,\n",
    "                    'scoring': SCORING_KEY,\n",
    "                    'bare_nlls': bare_nlls,\n",
    "                    'queries_first50': [s['query'][:50]\n",
    "                                        for s in samples[:len(bare_nlls)]],\n",
    "                    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                }\n",
    "                bare_ckpt_path.write_text(json.dumps(ckpt))\n",
    "                elapsed = time.time() - t0\n",
    "                done = i - start_idx + 1\n",
    "                eta = (N_SAMPLES - i - 1) * elapsed / done if done > 0 else 0\n",
    "                tqdm.write(f\"  Checkpoint {i+1}/{N_SAMPLES} | \"\n",
    "                           f\"{elapsed/60:.1f}m | ETA {eta/60:.1f}m\")\n",
    "\n",
    "        elapsed = time.time() - t0\n",
    "        print(f\"  Bare scoring complete in {elapsed/60:.1f} min\")\n",
    "\n",
    "    bare_arr = np.array(bare_nlls)\n",
    "    sorted_idx = np.argsort(bare_arr)[::-1]\n",
    "    h_idx = np.sort(sorted_idx[:N_HARD])\n",
    "\n",
    "    hs = []\n",
    "    for idx in h_idx:\n",
    "        s = dict(samples[idx])\n",
    "        s['nll_bare'] = float(bare_arr[idx])\n",
    "        s['original_idx'] = int(idx)\n",
    "        hs.append(s)\n",
    "    hard_samples[ds_name] = hs\n",
    "\n",
    "    hard_nlls[ds_name] = {'bare': bare_arr[h_idx]}\n",
    "\n",
    "    hard_metadata[ds_name] = {\n",
    "        'n_total': N_SAMPLES, 'n_hard': N_HARD, 'source': 'scored',\n",
    "        'mean_passage_words': float(np.mean([s['word_count'] for s in hs])),\n",
    "        'mean_answer_words': float(np.mean([count_words(s['answer']) for s in hs])),\n",
    "    }\n",
    "\n",
    "    print(f\"  Hard cutoff: {bare_arr[h_idx].min():.4f}\")\n",
    "    print(f\"  Hard mean bare NLL: {bare_arr[h_idx].mean():.4f}\")\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Hard sample selection complete:\")\n",
    "for ds_name in NEW_DATASETS:\n",
    "    n_h = len(hard_samples[ds_name])\n",
    "    print(f\"  {ds_name}: {n_h} hard samples (mean bare NLL: \"\n",
    "          f\"{hard_nlls[ds_name]['bare'].mean():.4f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd7c408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Phase A — Q-matched scoring for 3 new datasets\n",
    "# Conditions: random_tokens, comprehend, extract_general, scrambled_comprehend\n",
    "print(\"=\" * 70)\n",
    "print(\"PHASE A: Q-MATCHED SCORING — 3 new datasets x ~160 hard x 4 prefix conditions\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# --- Validation tests ---\n",
    "print(\"\\n--- Validation: bare two-phase matches single-pass ---\")\n",
    "doc_text_t = \"The cat sat on the mat near the door of the house by the lake\"\n",
    "query_text_t = \"Where did the cat sit?\"\n",
    "answer_text_t = \"on the mat\"\n",
    "doc_ids_t = tokenizer(doc_text_t, add_special_tokens=False).input_ids\n",
    "D_t = len(doc_ids_t)\n",
    "query_ids_t = tokenizer(\"\\n\" + query_text_t + \"\\n\", add_special_tokens=False).input_ids\n",
    "answer_ids_t = tokenizer(answer_text_t, add_special_tokens=False).input_ids\n",
    "\n",
    "full_ids = [BOS_ID] + doc_ids_t + query_ids_t + answer_ids_t\n",
    "with torch.no_grad():\n",
    "    out_full = model(input_ids=torch.tensor([full_ids], device=DEVICE))\n",
    "n_ctx = 1 + D_t + len(query_ids_t)\n",
    "logits_full = out_full.logits[0, n_ctx - 1:n_ctx - 1 + len(answer_ids_t), :].float()\n",
    "targets_t = torch.tensor(answer_ids_t, device=DEVICE)\n",
    "nll_single = -F.log_softmax(logits_full, dim=-1).gather(\n",
    "    1, targets_t.unsqueeze(1)).squeeze(1).mean().item()\n",
    "del out_full\n",
    "\n",
    "nll_bare = score(doc_text_t, query_text_t, answer_text_t)\n",
    "diff_pct = abs(nll_single - nll_bare) / nll_single * 100\n",
    "print(f\"  Single-pass: {nll_single:.6f}, Two-phase: {nll_bare:.6f} (diff: {diff_pct:.2f}%)\")\n",
    "assert diff_pct < 1.0, f\"Bare doesn't match single-pass: {diff_pct}%\"\n",
    "print(\"  PASSED\")\n",
    "\n",
    "# --- Score Phase A ---\n",
    "for ds_name in NEW_DATASETS:\n",
    "    hs = hard_samples[ds_name]\n",
    "    n_hard = len(hs)\n",
    "    ckpt_path = RESULTS_DIR / f\"phaseA_{ds_name}.json\"\n",
    "\n",
    "    print(f\"\\n--- Phase A: {ds_name} ({n_hard} hard x 4 conditions) ---\")\n",
    "\n",
    "    ds_results = []\n",
    "    start_idx = 0\n",
    "\n",
    "    if ckpt_path.exists():\n",
    "        ckpt = json.loads(ckpt_path.read_text())\n",
    "        if (ckpt.get('dataset') == ds_name and\n",
    "            ckpt.get('scoring') == SCORING_KEY and\n",
    "            ckpt.get('phase') == 'A' and\n",
    "            ckpt.get('n_hard') == n_hard):\n",
    "            saved_queries = [r['query'][:50] for r in ckpt.get('results', [])]\n",
    "            current_queries = [s['query'][:50] for s in hs[:len(saved_queries)]]\n",
    "            if saved_queries == current_queries:\n",
    "                ds_results = ckpt['results']\n",
    "                start_idx = len(ds_results)\n",
    "                print(f\"  Resuming from checkpoint: {start_idx}/{n_hard}\")\n",
    "\n",
    "    if start_idx < n_hard:\n",
    "        t0 = time.time()\n",
    "\n",
    "        # Reset RNG for prefix generation\n",
    "        np.random.seed(DS_SEEDS[ds_name] + 1000)\n",
    "        pyrandom.seed(DS_SEEDS[ds_name] + 1000)\n",
    "\n",
    "        for i in tqdm(range(start_idx, n_hard), initial=start_idx,\n",
    "                      total=n_hard, desc=f\"PhaseA {ds_name}\"):\n",
    "            s = hs[i]\n",
    "            q_ids = tokenizer(s['query'], add_special_tokens=False).input_ids\n",
    "            Q = len(q_ids)\n",
    "\n",
    "            result = {\n",
    "                'query': s['query'],\n",
    "                'answer': s['answer'],\n",
    "                'passage_words': s['word_count'],\n",
    "                'Q': Q,\n",
    "            }\n",
    "\n",
    "            # random_tokens: Q random IDs\n",
    "            rand_ids = []\n",
    "            while len(rand_ids) < Q:\n",
    "                tid = np.random.randint(0, VOCAB_SIZE)\n",
    "                if tid not in special_ids:\n",
    "                    rand_ids.append(int(tid))\n",
    "            result['nll_random_tokens'] = score(\n",
    "                s['passage'], s['query'], s['answer'],\n",
    "                prefix_token_ids=rand_ids[:Q])\n",
    "\n",
    "            # comprehend: Q tokens\n",
    "            comp_prefix = make_prefix(INSTRUCTION_IDS['comprehend'], Q)\n",
    "            result['nll_comprehend'] = score(\n",
    "                s['passage'], s['query'], s['answer'],\n",
    "                prefix_token_ids=comp_prefix)\n",
    "\n",
    "            # extract_general: Q tokens\n",
    "            ext_prefix = make_prefix(INSTRUCTION_IDS['extract_general'], Q)\n",
    "            result['nll_extract_general'] = score(\n",
    "                s['passage'], s['query'], s['answer'],\n",
    "                prefix_token_ids=ext_prefix)\n",
    "\n",
    "            # scrambled_comprehend: Q tokens\n",
    "            scr_seed = hash('scrambled_comprehend') % (2**31) + i\n",
    "            scr_prefix = scramble_prefix(comp_prefix, scr_seed)\n",
    "            result['nll_scrambled_comprehend'] = score(\n",
    "                s['passage'], s['query'], s['answer'],\n",
    "                prefix_token_ids=scr_prefix)\n",
    "\n",
    "            ds_results.append(result)\n",
    "\n",
    "            if (i + 1) % 20 == 0 or i == n_hard - 1:\n",
    "                ckpt = {\n",
    "                    'dataset': ds_name,\n",
    "                    'n_hard': n_hard,\n",
    "                    'scoring': SCORING_KEY,\n",
    "                    'phase': 'A',\n",
    "                    'results': ds_results,\n",
    "                    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                }\n",
    "                ckpt_path.write_text(json.dumps(ckpt))\n",
    "                elapsed = time.time() - t0\n",
    "                done = i - start_idx + 1\n",
    "                eta = (n_hard - i - 1) * elapsed / done if done > 0 else 0\n",
    "                tqdm.write(f\"  Checkpoint {i+1}/{n_hard} | \"\n",
    "                           f\"{elapsed/60:.1f}m | ETA {eta/60:.1f}m\")\n",
    "\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        elapsed = time.time() - t0\n",
    "        print(f\"  Phase A scoring complete in {elapsed/60:.1f} min\")\n",
    "    else:\n",
    "        print(f\"  Loaded {len(ds_results)} cached results\")\n",
    "\n",
    "    # Populate hard_nlls\n",
    "    for cond in PHASE_A_PREFIX_CONDS:\n",
    "        hard_nlls[ds_name][cond] = np.array(\n",
    "            [r[f'nll_{cond}'] for r in ds_results])\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Quick Phase A summary\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Phase A summary (Q-matched, new datasets):\")\n",
    "for ds_name in NEW_DATASETS:\n",
    "    bare = hard_nlls[ds_name]['bare']\n",
    "    rand = hard_nlls[ds_name]['random_tokens']\n",
    "    comp = hard_nlls[ds_name]['comprehend']\n",
    "    ext = hard_nlls[ds_name]['extract_general']\n",
    "    print(f\"\\n  {ds_name}:\")\n",
    "    print(f\"    bare:            {bare.mean():.4f}\")\n",
    "    print(f\"    random_tokens:   {rand.mean():.4f}  d_bare={cohens_d(bare - rand):+.3f}\")\n",
    "    print(f\"    comprehend:      {comp.mean():.4f}  sem_d={cohens_d(rand - comp):+.3f}\")\n",
    "    print(f\"    extract_general: {ext.mean():.4f}  sem_d={cohens_d(rand - ext):+.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fcf0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Phase B — Fixed-length prefix scoring across all 7 datasets\n",
    "# L = 32, 64, 128, 256; conditions: bare_trunc, random_tokens, comprehend,\n",
    "# extract_general, scrambled_comprehend at each L\n",
    "print(\"=\" * 70)\n",
    "print(\"PHASE B: FIXED-LENGTH PREFIX SCORING\")\n",
    "print(f\"  Lengths: {PREFIX_LENGTHS}\")\n",
    "print(f\"  Datasets: {ALL_DATASETS}\")\n",
    "print(f\"  COMMON_MAX_DOC: {COMMON_MAX_DOC} tokens\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Storage for Phase B NLLs\n",
    "# phase_b_nlls[ds_name][cond_key] = np.array of N_HARD values\n",
    "# cond_key examples: 'bare_trunc', 'random_tokens_32', 'comprehend_128', etc.\n",
    "phase_b_nlls = {ds: {} for ds in ALL_DATASETS}\n",
    "\n",
    "for ds_name in ALL_DATASETS:\n",
    "    hs = hard_samples[ds_name]\n",
    "    n_hard = len(hs)\n",
    "    ckpt_path = RESULTS_DIR / f\"phaseB_{ds_name}.json\"\n",
    "\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Phase B: {ds_name} ({n_hard} hard samples)\")\n",
    "\n",
    "    ds_results = []\n",
    "    start_idx = 0\n",
    "\n",
    "    if ckpt_path.exists():\n",
    "        ckpt = json.loads(ckpt_path.read_text())\n",
    "        if (ckpt.get('dataset') == ds_name and\n",
    "            ckpt.get('scoring') == SCORING_KEY and\n",
    "            ckpt.get('phase') == 'B' and\n",
    "            ckpt.get('n_hard') == n_hard):\n",
    "            saved_queries = [r['query'][:50] for r in ckpt.get('results', [])]\n",
    "            current_queries = [s['query'][:50] for s in hs[:len(saved_queries)]]\n",
    "            if saved_queries == current_queries:\n",
    "                ds_results = ckpt['results']\n",
    "                start_idx = len(ds_results)\n",
    "                print(f\"  Resuming from checkpoint: {start_idx}/{n_hard}\")\n",
    "\n",
    "    if start_idx < n_hard:\n",
    "        t0 = time.time()\n",
    "\n",
    "        # Reset RNG for this dataset's Phase B\n",
    "        np.random.seed(DS_SEEDS.get(ds_name, SEED) + 2000)\n",
    "        pyrandom.seed(DS_SEEDS.get(ds_name, SEED) + 2000)\n",
    "\n",
    "        for i in tqdm(range(start_idx, n_hard), initial=start_idx,\n",
    "                      total=n_hard, desc=f\"PhaseB {ds_name}\"):\n",
    "            s = hs[i]\n",
    "            result = {\n",
    "                'query': s['query'],\n",
    "                'answer': s['answer'],\n",
    "                'passage_words': s['word_count'],\n",
    "            }\n",
    "\n",
    "            # bare_trunc: bare with COMMON_MAX_DOC truncation\n",
    "            result['nll_bare_trunc'] = score(\n",
    "                s['passage'], s['query'], s['answer'],\n",
    "                max_doc_override=COMMON_MAX_DOC)\n",
    "\n",
    "            # For each prefix length\n",
    "            for L in PREFIX_LENGTHS:\n",
    "                # random_tokens_L\n",
    "                rand_ids = []\n",
    "                while len(rand_ids) < L:\n",
    "                    tid = np.random.randint(0, VOCAB_SIZE)\n",
    "                    if tid not in special_ids:\n",
    "                        rand_ids.append(int(tid))\n",
    "                result[f'nll_random_tokens_{L}'] = score(\n",
    "                    s['passage'], s['query'], s['answer'],\n",
    "                    prefix_token_ids=rand_ids[:L],\n",
    "                    max_doc_override=COMMON_MAX_DOC)\n",
    "\n",
    "                # comprehend_L\n",
    "                comp_prefix = make_prefix(INSTRUCTION_IDS['comprehend'], L)\n",
    "                result[f'nll_comprehend_{L}'] = score(\n",
    "                    s['passage'], s['query'], s['answer'],\n",
    "                    prefix_token_ids=comp_prefix,\n",
    "                    max_doc_override=COMMON_MAX_DOC)\n",
    "\n",
    "                # extract_general_L\n",
    "                ext_prefix = make_prefix(INSTRUCTION_IDS['extract_general'], L)\n",
    "                result[f'nll_extract_general_{L}'] = score(\n",
    "                    s['passage'], s['query'], s['answer'],\n",
    "                    prefix_token_ids=ext_prefix,\n",
    "                    max_doc_override=COMMON_MAX_DOC)\n",
    "\n",
    "                # scrambled_comprehend_L\n",
    "                scr_seed = hash(f'scrambled_comprehend_{L}') % (2**31) + i\n",
    "                scr_prefix = scramble_prefix(comp_prefix, scr_seed)\n",
    "                result[f'nll_scrambled_comprehend_{L}'] = score(\n",
    "                    s['passage'], s['query'], s['answer'],\n",
    "                    prefix_token_ids=scr_prefix,\n",
    "                    max_doc_override=COMMON_MAX_DOC)\n",
    "\n",
    "            ds_results.append(result)\n",
    "\n",
    "            if (i + 1) % 20 == 0 or i == n_hard - 1:\n",
    "                ckpt = {\n",
    "                    'dataset': ds_name,\n",
    "                    'n_hard': n_hard,\n",
    "                    'scoring': SCORING_KEY,\n",
    "                    'phase': 'B',\n",
    "                    'common_max_doc': COMMON_MAX_DOC,\n",
    "                    'prefix_lengths': PREFIX_LENGTHS,\n",
    "                    'results': ds_results,\n",
    "                    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                }\n",
    "                ckpt_path.write_text(json.dumps(ckpt))\n",
    "                elapsed = time.time() - t0\n",
    "                done = i - start_idx + 1\n",
    "                eta = (n_hard - i - 1) * elapsed / done if done > 0 else 0\n",
    "                tqdm.write(f\"  Checkpoint {i+1}/{n_hard} | \"\n",
    "                           f\"{elapsed/60:.1f}m | ETA {eta/60:.1f}m\")\n",
    "\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        elapsed = time.time() - t0\n",
    "        print(f\"  Phase B scoring complete in {elapsed/60:.1f} min\")\n",
    "    else:\n",
    "        print(f\"  Loaded {len(ds_results)} cached results\")\n",
    "\n",
    "    # Populate phase_b_nlls\n",
    "    phase_b_nlls[ds_name]['bare_trunc'] = np.array(\n",
    "        [r['nll_bare_trunc'] for r in ds_results])\n",
    "    for L in PREFIX_LENGTHS:\n",
    "        for cond in PHASE_B_PREFIX_CONDS:\n",
    "            key = f'{cond}_{L}'\n",
    "            phase_b_nlls[ds_name][key] = np.array(\n",
    "                [r[f'nll_{key}'] for r in ds_results])\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# ================================================================\n",
    "# RACE MC accuracy scoring\n",
    "# ================================================================\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"RACE MC ACCURACY SCORING\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "race_mc_results = []  # list of dicts with NLLs for all 4 options under each condition\n",
    "\n",
    "hs_race = hard_samples['race_high']\n",
    "n_hard_race = len(hs_race)\n",
    "race_mc_ckpt_path = RESULTS_DIR / \"race_mc.json\"\n",
    "\n",
    "start_idx_mc = 0\n",
    "if race_mc_ckpt_path.exists():\n",
    "    mc_ckpt = json.loads(race_mc_ckpt_path.read_text())\n",
    "    if (mc_ckpt.get('scoring') == SCORING_KEY and\n",
    "        mc_ckpt.get('n_hard') == n_hard_race):\n",
    "        saved_queries = [r['query'][:50] for r in mc_ckpt.get('results', [])]\n",
    "        current_queries = [s['query'][:50] for s in hs_race[:len(saved_queries)]]\n",
    "        if saved_queries == current_queries:\n",
    "            race_mc_results = mc_ckpt['results']\n",
    "            start_idx_mc = len(race_mc_results)\n",
    "            print(f\"  Resuming from checkpoint: {start_idx_mc}/{n_hard_race}\")\n",
    "\n",
    "if start_idx_mc < n_hard_race:\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Define conditions to score MC for\n",
    "    # Q-matched + Phase B fixed-length\n",
    "    mc_conditions = ['bare']  # bare (no prefix, no truncation)\n",
    "    mc_conditions += ['bare_trunc']  # bare with truncation\n",
    "    for cond in PHASE_A_PREFIX_CONDS:\n",
    "        mc_conditions.append(f'qmatched_{cond}')\n",
    "    for L in PREFIX_LENGTHS:\n",
    "        for cond in PHASE_B_PREFIX_CONDS:\n",
    "            mc_conditions.append(f'{cond}_{L}')\n",
    "\n",
    "    np.random.seed(DS_SEEDS['race_high'] + 3000)\n",
    "    pyrandom.seed(DS_SEEDS['race_high'] + 3000)\n",
    "\n",
    "    for i in tqdm(range(start_idx_mc, n_hard_race), initial=start_idx_mc,\n",
    "                  total=n_hard_race, desc=\"RACE MC\"):\n",
    "        s = hs_race[i]\n",
    "        options = s['all_options']\n",
    "        correct_idx = s['correct_idx']\n",
    "        q_ids = tokenizer(s['query'], add_special_tokens=False).input_ids\n",
    "        Q = len(q_ids)\n",
    "\n",
    "        result = {\n",
    "            'query': s['query'],\n",
    "            'correct_idx': correct_idx,\n",
    "            'n_options': len(options),\n",
    "        }\n",
    "\n",
    "        # Score all 4 options under each condition\n",
    "        for opt_i, opt_text in enumerate(options):\n",
    "            # bare\n",
    "            result[f'nll_bare_opt{opt_i}'] = score(\n",
    "                s['passage'], s['query'], opt_text)\n",
    "\n",
    "            # bare_trunc\n",
    "            result[f'nll_bare_trunc_opt{opt_i}'] = score(\n",
    "                s['passage'], s['query'], opt_text,\n",
    "                max_doc_override=COMMON_MAX_DOC)\n",
    "\n",
    "            # Q-matched conditions\n",
    "            rand_ids = []\n",
    "            while len(rand_ids) < Q:\n",
    "                tid = np.random.randint(0, VOCAB_SIZE)\n",
    "                if tid not in special_ids:\n",
    "                    rand_ids.append(int(tid))\n",
    "\n",
    "            result[f'nll_qmatched_random_tokens_opt{opt_i}'] = score(\n",
    "                s['passage'], s['query'], opt_text,\n",
    "                prefix_token_ids=rand_ids[:Q])\n",
    "\n",
    "            comp_prefix_q = make_prefix(INSTRUCTION_IDS['comprehend'], Q)\n",
    "            result[f'nll_qmatched_comprehend_opt{opt_i}'] = score(\n",
    "                s['passage'], s['query'], opt_text,\n",
    "                prefix_token_ids=comp_prefix_q)\n",
    "\n",
    "            ext_prefix_q = make_prefix(INSTRUCTION_IDS['extract_general'], Q)\n",
    "            result[f'nll_qmatched_extract_general_opt{opt_i}'] = score(\n",
    "                s['passage'], s['query'], opt_text,\n",
    "                prefix_token_ids=ext_prefix_q)\n",
    "\n",
    "            scr_seed_q = hash('scrambled_comprehend') % (2**31) + i\n",
    "            scr_prefix_q = scramble_prefix(comp_prefix_q, scr_seed_q)\n",
    "            result[f'nll_qmatched_scrambled_comprehend_opt{opt_i}'] = score(\n",
    "                s['passage'], s['query'], opt_text,\n",
    "                prefix_token_ids=scr_prefix_q)\n",
    "\n",
    "            # Fixed-length conditions\n",
    "            for L in PREFIX_LENGTHS:\n",
    "                rand_ids_L = []\n",
    "                while len(rand_ids_L) < L:\n",
    "                    tid = np.random.randint(0, VOCAB_SIZE)\n",
    "                    if tid not in special_ids:\n",
    "                        rand_ids_L.append(int(tid))\n",
    "                result[f'nll_random_tokens_{L}_opt{opt_i}'] = score(\n",
    "                    s['passage'], s['query'], opt_text,\n",
    "                    prefix_token_ids=rand_ids_L[:L],\n",
    "                    max_doc_override=COMMON_MAX_DOC)\n",
    "\n",
    "                comp_prefix_L = make_prefix(INSTRUCTION_IDS['comprehend'], L)\n",
    "                result[f'nll_comprehend_{L}_opt{opt_i}'] = score(\n",
    "                    s['passage'], s['query'], opt_text,\n",
    "                    prefix_token_ids=comp_prefix_L,\n",
    "                    max_doc_override=COMMON_MAX_DOC)\n",
    "\n",
    "                ext_prefix_L = make_prefix(INSTRUCTION_IDS['extract_general'], L)\n",
    "                result[f'nll_extract_general_{L}_opt{opt_i}'] = score(\n",
    "                    s['passage'], s['query'], opt_text,\n",
    "                    prefix_token_ids=ext_prefix_L,\n",
    "                    max_doc_override=COMMON_MAX_DOC)\n",
    "\n",
    "                scr_seed_L = hash(f'scrambled_comprehend_{L}') % (2**31) + i\n",
    "                scr_prefix_L = scramble_prefix(comp_prefix_L, scr_seed_L)\n",
    "                result[f'nll_scrambled_comprehend_{L}_opt{opt_i}'] = score(\n",
    "                    s['passage'], s['query'], opt_text,\n",
    "                    prefix_token_ids=scr_prefix_L,\n",
    "                    max_doc_override=COMMON_MAX_DOC)\n",
    "\n",
    "        race_mc_results.append(result)\n",
    "\n",
    "        if (i + 1) % 20 == 0 or i == n_hard_race - 1:\n",
    "            mc_ckpt = {\n",
    "                'scoring': SCORING_KEY,\n",
    "                'n_hard': n_hard_race,\n",
    "                'results': race_mc_results,\n",
    "                'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            }\n",
    "            race_mc_ckpt_path.write_text(json.dumps(mc_ckpt))\n",
    "            elapsed = time.time() - t0\n",
    "            done = i - start_idx_mc + 1\n",
    "            eta = (n_hard_race - i - 1) * elapsed / done if done > 0 else 0\n",
    "            tqdm.write(f\"  MC Checkpoint {i+1}/{n_hard_race} | \"\n",
    "                       f\"{elapsed/60:.1f}m | ETA {eta/60:.1f}m\")\n",
    "\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    elapsed = time.time() - t0\n",
    "    print(f\"  RACE MC scoring complete in {elapsed/60:.1f} min\")\n",
    "else:\n",
    "    print(f\"  Loaded {len(race_mc_results)} cached MC results\")\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print(f\"\\nPhase B + RACE MC scoring complete for all datasets.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d1d6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Per-dataset results — Phase A + Phase B tables\n",
    "print(\"=\" * 70)\n",
    "print(\"PER-DATASET RESULTS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "per_dataset_analysis = {}\n",
    "\n",
    "# ================================================================\n",
    "# PART 1: Phase A — Q-matched results (all 7 datasets)\n",
    "# ================================================================\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"PHASE A: Q-MATCHED RESULTS (all 7 datasets)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "PHASE_A_CONDS = ['bare', 'random_tokens', 'comprehend', 'extract_general',\n",
    "                 'scrambled_comprehend']\n",
    "\n",
    "for ds_name in ALL_DATASETS:\n",
    "    nlls = hard_nlls[ds_name]\n",
    "    n_hard = len(nlls['bare'])\n",
    "    bare = nlls['bare']\n",
    "    rand = nlls.get('random_tokens')\n",
    "\n",
    "    print(f\"\\n--- {ds_name.upper()} ({n_hard} hard samples, Q-matched) ---\")\n",
    "    print(f\"  {'Condition':<28} {'NLL':>7} {'d_bare':>8} {'sem_d':>8} \"\n",
    "          f\"{'win%':>6} {'p':>10} {'sig':>4}\")\n",
    "    print(f\"  {'-'*76}\")\n",
    "\n",
    "    analysis = {}\n",
    "    for cond in PHASE_A_CONDS:\n",
    "        if cond not in nlls:\n",
    "            continue\n",
    "        c = nlls[cond]\n",
    "        mean_nll = c.mean()\n",
    "\n",
    "        if cond == 'bare':\n",
    "            print(f\"  {cond:<28} {mean_nll:>7.3f} {'--':>8} {'--':>8} \"\n",
    "                  f\"{'--':>6} {'--':>10} {'--':>4}\")\n",
    "            analysis[cond] = {'mean_nll': float(mean_nll)}\n",
    "            continue\n",
    "\n",
    "        diff_bare = bare - c\n",
    "        d_bare = cohens_d(diff_bare)\n",
    "        _, p_bare = stats.ttest_1samp(diff_bare, 0)\n",
    "\n",
    "        if cond == 'random_tokens':\n",
    "            win_pct = 100 * np.mean(diff_bare > 0)\n",
    "            sig = ('***' if p_bare < 0.001 else '**' if p_bare < 0.01\n",
    "                   else '*' if p_bare < 0.05 else 'ns')\n",
    "            print(f\"  {cond:<28} {mean_nll:>7.3f} {d_bare:>+8.3f} {'(ref)':>8} \"\n",
    "                  f\"{win_pct:>5.1f}% {p_bare:>10.2e} {sig:>4}\")\n",
    "            analysis[cond] = {\n",
    "                'mean_nll': float(mean_nll), 'd_bare': float(d_bare),\n",
    "                'semantic_delta_d': 0.0, 'p_bare': float(p_bare),\n",
    "            }\n",
    "        else:\n",
    "            sem_delta = rand - c\n",
    "            d_sem = cohens_d(sem_delta)\n",
    "            _, p_sem = stats.ttest_1samp(sem_delta, 0)\n",
    "            win_pct = 100 * np.mean(sem_delta > 0)\n",
    "            sig = ('***' if p_sem < 0.001 else '**' if p_sem < 0.01\n",
    "                   else '*' if p_sem < 0.05 else 'ns')\n",
    "            print(f\"  {cond:<28} {mean_nll:>7.3f} {d_bare:>+8.3f} {d_sem:>+8.3f} \"\n",
    "                  f\"{win_pct:>5.1f}% {p_sem:>10.2e} {sig:>4}\")\n",
    "            analysis[cond] = {\n",
    "                'mean_nll': float(mean_nll), 'd_bare': float(d_bare),\n",
    "                'semantic_delta_d': float(d_sem), 'p_semantic': float(p_sem),\n",
    "            }\n",
    "\n",
    "    # Three-level decomposition (Q-matched)\n",
    "    if 'random_tokens' in nlls and 'comprehend' in nlls and 'scrambled_comprehend' in nlls:\n",
    "        structural = (bare - rand).mean()\n",
    "        vocab = (rand - nlls['scrambled_comprehend']).mean()\n",
    "        meaning = (nlls['scrambled_comprehend'] - nlls['comprehend']).mean()\n",
    "        total = structural + vocab + meaning\n",
    "        print(f\"\\n  Q-matched decomposition (comprehend):\")\n",
    "        print(f\"    Structural: {structural:+.4f}, Vocabulary: {vocab:+.4f}, \"\n",
    "              f\"Meaning: {meaning:+.4f}, Total: {total:+.4f}\")\n",
    "        analysis['decomposition_qmatched'] = {\n",
    "            'structural': float(structural), 'vocabulary': float(vocab),\n",
    "            'meaning': float(meaning), 'total': float(total),\n",
    "        }\n",
    "\n",
    "    per_dataset_analysis[ds_name] = analysis\n",
    "\n",
    "# ================================================================\n",
    "# PART 2: Phase B — Fixed-length results (all 7 datasets)\n",
    "# ================================================================\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"PHASE B: FIXED-LENGTH RESULTS (all 7 datasets)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for ds_name in ALL_DATASETS:\n",
    "    pb = phase_b_nlls[ds_name]\n",
    "    bare_t = pb['bare_trunc']\n",
    "    n_hard = len(bare_t)\n",
    "\n",
    "    print(f\"\\n--- {ds_name.upper()} ({n_hard} hard samples, fixed-length) ---\")\n",
    "    print(f\"  bare_trunc NLL: {bare_t.mean():.4f}\")\n",
    "\n",
    "    print(f\"\\n  {'L':>5} {'rand NLL':>9} {'comp NLL':>9} {'ext NLL':>9} \"\n",
    "          f\"{'scr NLL':>9} {'d_comp':>8} {'d_ext':>8} {'d_scr':>8}\")\n",
    "    print(f\"  {'-'*72}\")\n",
    "\n",
    "    phase_b_analysis = {}\n",
    "    for L in PREFIX_LENGTHS:\n",
    "        rand_L = pb[f'random_tokens_{L}']\n",
    "        comp_L = pb[f'comprehend_{L}']\n",
    "        ext_L = pb[f'extract_general_{L}']\n",
    "        scr_L = pb[f'scrambled_comprehend_{L}']\n",
    "\n",
    "        d_comp = cohens_d(rand_L - comp_L)\n",
    "        d_ext = cohens_d(rand_L - ext_L)\n",
    "        d_scr = cohens_d(rand_L - scr_L)\n",
    "\n",
    "        print(f\"  {L:>5} {rand_L.mean():>9.4f} {comp_L.mean():>9.4f} \"\n",
    "              f\"{ext_L.mean():>9.4f} {scr_L.mean():>9.4f} \"\n",
    "              f\"{d_comp:>+8.3f} {d_ext:>+8.3f} {d_scr:>+8.3f}\")\n",
    "\n",
    "        # Three-level decomposition at this length\n",
    "        structural = (bare_t - rand_L).mean()\n",
    "        vocab = (rand_L - scr_L).mean()\n",
    "        meaning = (scr_L - comp_L).mean()\n",
    "        total = structural + vocab + meaning\n",
    "\n",
    "        phase_b_analysis[L] = {\n",
    "            'structural': float(structural), 'vocabulary': float(vocab),\n",
    "            'meaning': float(meaning), 'total': float(total),\n",
    "            'd_comprehend': float(d_comp), 'd_extract_general': float(d_ext),\n",
    "            'd_scrambled_comprehend': float(d_scr),\n",
    "            'mean_nll': {\n",
    "                'bare_trunc': float(bare_t.mean()),\n",
    "                'random_tokens': float(rand_L.mean()),\n",
    "                'comprehend': float(comp_L.mean()),\n",
    "                'extract_general': float(ext_L.mean()),\n",
    "                'scrambled_comprehend': float(scr_L.mean()),\n",
    "            }\n",
    "        }\n",
    "\n",
    "    per_dataset_analysis[ds_name]['phase_b'] = phase_b_analysis\n",
    "\n",
    "    # Decomposition table\n",
    "    print(f\"\\n  Three-Level Decomposition x Length:\")\n",
    "    print(f\"  {'L':>5} {'Structural':>12} {'Vocabulary':>12} \"\n",
    "          f\"{'Meaning':>12} {'Total':>12}\")\n",
    "    print(f\"  {'-'*56}\")\n",
    "    for L in PREFIX_LENGTHS:\n",
    "        d = phase_b_analysis[L]\n",
    "        print(f\"  {L:>5} {d['structural']:>+12.4f} {d['vocabulary']:>+12.4f} \"\n",
    "              f\"{d['meaning']:>+12.4f} {d['total']:>+12.4f}\")\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695a5e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Cross-dataset analysis — scaling curves, decomposition, RACE accuracy\n",
    "print(\"=\" * 70)\n",
    "print(\"CROSS-DATASET ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# ================================================================\n",
    "# PART 1: Phase A — Q-matched meta-analysis (all 7 datasets)\n",
    "# ================================================================\n",
    "print(\"\\n--- PART 1: Phase A Q-Matched Meta-Analysis ---\")\n",
    "\n",
    "PHASE_A_META_CONDS = ['comprehend', 'extract_general', 'scrambled_comprehend']\n",
    "\n",
    "print(f\"\\n  {'Condition':<28} {'pooled_d':>9} {'SE':>8} {'z':>8} \"\n",
    "      f\"{'p':>10} {'95% CI':>16} {'sig':>4}\")\n",
    "print(f\"  {'-'*86}\")\n",
    "\n",
    "meta_results_A = {}\n",
    "for cond in PHASE_A_META_CONDS:\n",
    "    ds_effects = []\n",
    "    for ds_name in ALL_DATASETS:\n",
    "        nlls = hard_nlls[ds_name]\n",
    "        if cond not in nlls or 'random_tokens' not in nlls:\n",
    "            continue\n",
    "        sem_delta = nlls['random_tokens'] - nlls[cond]\n",
    "        n = len(sem_delta)\n",
    "        d = cohens_d(sem_delta)\n",
    "        se = np.sqrt(1.0/n + d**2 / (2.0*n))\n",
    "        ds_effects.append((d, se, n))\n",
    "\n",
    "    if not ds_effects:\n",
    "        continue\n",
    "\n",
    "    weights = [1.0 / (se**2) for _, se, _ in ds_effects]\n",
    "    w_sum = sum(weights)\n",
    "    pooled_d = sum(w * d for (d, _, _), w in zip(ds_effects, weights)) / w_sum\n",
    "    pooled_se = 1.0 / np.sqrt(w_sum)\n",
    "    z = pooled_d / pooled_se if pooled_se > 0 else 0.0\n",
    "    p = 2 * stats.norm.sf(abs(z))\n",
    "    ci_lo = pooled_d - 1.96 * pooled_se\n",
    "    ci_hi = pooled_d + 1.96 * pooled_se\n",
    "    sig = ('***' if p < 0.001 else '**' if p < 0.01\n",
    "           else '*' if p < 0.05 else 'ns')\n",
    "\n",
    "    print(f\"  {cond:<28} {pooled_d:>+9.4f} {pooled_se:>8.4f} {z:>+8.2f} \"\n",
    "          f\"{p:>10.2e} [{ci_lo:>+.3f}, {ci_hi:>+.3f}] {sig:>4}\")\n",
    "    meta_results_A[cond] = {\n",
    "        'pooled_d': float(pooled_d), 'se': float(pooled_se),\n",
    "        'z': float(z), 'p': float(p),\n",
    "        'ci_lo': float(ci_lo), 'ci_hi': float(ci_hi),\n",
    "        'n_datasets': len(ds_effects),\n",
    "    }\n",
    "\n",
    "# Per-dataset d for cross-dataset consistency\n",
    "print(f\"\\n  {'Condition':<28}\", end=\"\")\n",
    "for ds_name in ALL_DATASETS:\n",
    "    print(f\" {ds_name[:8]:>8}\", end=\"\")\n",
    "print(f\"  {'mean':>8}\")\n",
    "print(f\"  {'-'*100}\")\n",
    "\n",
    "for cond in PHASE_A_META_CONDS:\n",
    "    row = f\"  {cond:<28}\"\n",
    "    ds_vals = []\n",
    "    for ds_name in ALL_DATASETS:\n",
    "        nlls = hard_nlls[ds_name]\n",
    "        if cond not in nlls or 'random_tokens' not in nlls:\n",
    "            row += f\" {'N/A':>8}\"\n",
    "            continue\n",
    "        d = cohens_d(nlls['random_tokens'] - nlls[cond])\n",
    "        row += f\" {d:>+8.3f}\"\n",
    "        ds_vals.append(d)\n",
    "    if ds_vals:\n",
    "        row += f\"  {np.mean(ds_vals):>+8.3f}\"\n",
    "    print(row)\n",
    "\n",
    "# ================================================================\n",
    "# PART 2: Phase B — Length Scaling Curves\n",
    "# ================================================================\n",
    "print(f\"\\n--- PART 2: Length Scaling Curves (pooled across datasets) ---\")\n",
    "\n",
    "scaling_results = {}\n",
    "for cond_base in ['comprehend', 'extract_general', 'scrambled_comprehend']:\n",
    "    print(f\"\\n  {cond_base}:\")\n",
    "    print(f\"  {'L':>5} {'pooled_d':>9} {'SE':>8} {'z':>8} \"\n",
    "          f\"{'p':>10} {'sig':>4}\")\n",
    "    print(f\"  {'-'*48}\")\n",
    "\n",
    "    scaling_results[cond_base] = {}\n",
    "    for L in PREFIX_LENGTHS:\n",
    "        ds_effects = []\n",
    "        for ds_name in ALL_DATASETS:\n",
    "            pb = phase_b_nlls[ds_name]\n",
    "            rand_key = f'random_tokens_{L}'\n",
    "            cond_key = f'{cond_base}_{L}'\n",
    "            if rand_key not in pb or cond_key not in pb:\n",
    "                continue\n",
    "            sem_delta = pb[rand_key] - pb[cond_key]\n",
    "            n = len(sem_delta)\n",
    "            d = cohens_d(sem_delta)\n",
    "            se = np.sqrt(1.0/n + d**2 / (2.0*n))\n",
    "            ds_effects.append((d, se, n))\n",
    "\n",
    "        if not ds_effects:\n",
    "            continue\n",
    "\n",
    "        weights = [1.0 / (se**2) for _, se, _ in ds_effects]\n",
    "        w_sum = sum(weights)\n",
    "        pooled_d = sum(w * d for (d, _, _), w in zip(ds_effects, weights)) / w_sum\n",
    "        pooled_se = 1.0 / np.sqrt(w_sum)\n",
    "        z = pooled_d / pooled_se if pooled_se > 0 else 0.0\n",
    "        p = 2 * stats.norm.sf(abs(z))\n",
    "        sig = ('***' if p < 0.001 else '**' if p < 0.01\n",
    "               else '*' if p < 0.05 else 'ns')\n",
    "\n",
    "        print(f\"  {L:>5} {pooled_d:>+9.4f} {pooled_se:>8.4f} {z:>+8.2f} \"\n",
    "              f\"{p:>10.2e} {sig:>4}\")\n",
    "        scaling_results[cond_base][L] = {\n",
    "            'pooled_d': float(pooled_d), 'se': float(pooled_se),\n",
    "            'z': float(z), 'p': float(p),\n",
    "        }\n",
    "\n",
    "# ================================================================\n",
    "# PART 3: Three-Level Decomposition x Length (pooled)\n",
    "# ================================================================\n",
    "print(f\"\\n--- PART 3: Three-Level Decomposition x Length (pooled) ---\")\n",
    "print(f\"  {'L':>5} {'Structural':>12} {'Vocabulary':>12} \"\n",
    "      f\"{'Meaning':>12} {'Total':>12}\")\n",
    "print(f\"  {'-'*56}\")\n",
    "\n",
    "pooled_decomp_by_length = {}\n",
    "for L in PREFIX_LENGTHS:\n",
    "    structs, vocabs, meanings, totals = [], [], [], []\n",
    "    for ds_name in ALL_DATASETS:\n",
    "        pb = phase_b_nlls[ds_name]\n",
    "        bare_t = pb['bare_trunc']\n",
    "        rand_L = pb[f'random_tokens_{L}']\n",
    "        scr_L = pb[f'scrambled_comprehend_{L}']\n",
    "        comp_L = pb[f'comprehend_{L}']\n",
    "\n",
    "        structs.append((bare_t - rand_L).mean())\n",
    "        vocabs.append((rand_L - scr_L).mean())\n",
    "        meanings.append((scr_L - comp_L).mean())\n",
    "        totals.append((bare_t - comp_L).mean())\n",
    "\n",
    "    s_m, v_m, m_m, t_m = np.mean(structs), np.mean(vocabs), np.mean(meanings), np.mean(totals)\n",
    "    print(f\"  {L:>5} {s_m:>+12.4f} {v_m:>+12.4f} {m_m:>+12.4f} {t_m:>+12.4f}\")\n",
    "    pooled_decomp_by_length[L] = {\n",
    "        'structural': float(s_m), 'vocabulary': float(v_m),\n",
    "        'meaning': float(m_m), 'total': float(t_m),\n",
    "    }\n",
    "\n",
    "# Percentage breakdown\n",
    "print(f\"\\n  Percentage breakdown:\")\n",
    "print(f\"  {'L':>5} {'Struct%':>9} {'Vocab%':>9} {'Meaning%':>9}\")\n",
    "print(f\"  {'-'*36}\")\n",
    "for L in PREFIX_LENGTHS:\n",
    "    d = pooled_decomp_by_length[L]\n",
    "    t = d['total']\n",
    "    if abs(t) > 0.001:\n",
    "        print(f\"  {L:>5} {d['structural']/t*100:>8.0f}% \"\n",
    "              f\"{d['vocabulary']/t*100:>8.0f}% {d['meaning']/t*100:>8.0f}%\")\n",
    "\n",
    "# ================================================================\n",
    "# PART 4: Task x Length Interaction\n",
    "# ================================================================\n",
    "print(f\"\\n--- PART 4: Task x Length Interaction (comprehend sem_d) ---\")\n",
    "print(f\"  {'Dataset':<16}\", end=\"\")\n",
    "for L in PREFIX_LENGTHS:\n",
    "    print(f\" {'L='+str(L):>8}\", end=\"\")\n",
    "print(f\"  {'trend':>8}\")\n",
    "print(f\"  {'-'*60}\")\n",
    "\n",
    "task_length = {}\n",
    "for ds_name in ALL_DATASETS:\n",
    "    pb = phase_b_nlls[ds_name]\n",
    "    row = f\"  {ds_name:<16}\"\n",
    "    ds_vals = []\n",
    "    for L in PREFIX_LENGTHS:\n",
    "        d = cohens_d(pb[f'random_tokens_{L}'] - pb[f'comprehend_{L}'])\n",
    "        row += f\" {d:>+8.3f}\"\n",
    "        ds_vals.append(d)\n",
    "    # Simple trend: increasing or decreasing?\n",
    "    if len(ds_vals) >= 2:\n",
    "        rho, _ = stats.spearmanr(PREFIX_LENGTHS, ds_vals)\n",
    "        trend = \"UP\" if rho > 0.5 else \"DOWN\" if rho < -0.5 else \"FLAT\"\n",
    "    else:\n",
    "        trend = \"N/A\"\n",
    "    row += f\"  {trend:>8}\"\n",
    "    print(row)\n",
    "    task_length[ds_name] = {L: float(d) for L, d in zip(PREFIX_LENGTHS, ds_vals)}\n",
    "\n",
    "# ================================================================\n",
    "# PART 5: RACE MC Accuracy\n",
    "# ================================================================\n",
    "print(f\"\\n--- PART 5: RACE MC Accuracy ---\")\n",
    "\n",
    "def compute_accuracy(results, cond_prefix, n_options=4):\n",
    "    correct = 0\n",
    "    for r in results:\n",
    "        correct_idx = r['correct_idx']\n",
    "        nlls = []\n",
    "        for opt_i in range(n_options):\n",
    "            key = f'nll_{cond_prefix}_opt{opt_i}'\n",
    "            if key not in r:\n",
    "                break\n",
    "            nlls.append(r[key])\n",
    "        if len(nlls) == n_options:\n",
    "            pred = np.argmin(nlls)\n",
    "            if pred == correct_idx:\n",
    "                correct += 1\n",
    "    return correct / len(results) if results else 0.0\n",
    "\n",
    "print(f\"\\n  {'Condition':<36} {'Accuracy':>8} {'N':>5}\")\n",
    "print(f\"  {'-'*52}\")\n",
    "\n",
    "race_accuracy = {}\n",
    "# Bare\n",
    "acc_bare = compute_accuracy(race_mc_results, 'bare')\n",
    "print(f\"  {'bare':<36} {acc_bare:>7.1%} {len(race_mc_results):>5}\")\n",
    "race_accuracy['bare'] = float(acc_bare)\n",
    "\n",
    "# Bare truncated\n",
    "acc_bare_t = compute_accuracy(race_mc_results, 'bare_trunc')\n",
    "print(f\"  {'bare_trunc':<36} {acc_bare_t:>7.1%} {len(race_mc_results):>5}\")\n",
    "race_accuracy['bare_trunc'] = float(acc_bare_t)\n",
    "\n",
    "# Q-matched\n",
    "for cond in PHASE_A_PREFIX_CONDS:\n",
    "    acc = compute_accuracy(race_mc_results, f'qmatched_{cond}')\n",
    "    print(f\"  {'qmatched_' + cond:<36} {acc:>7.1%} {len(race_mc_results):>5}\")\n",
    "    race_accuracy[f'qmatched_{cond}'] = float(acc)\n",
    "\n",
    "# Fixed-length\n",
    "for L in PREFIX_LENGTHS:\n",
    "    for cond in PHASE_B_PREFIX_CONDS:\n",
    "        acc = compute_accuracy(race_mc_results, f'{cond}_{L}')\n",
    "        print(f\"  {f'{cond}_{L}':<36} {acc:>7.1%} {len(race_mc_results):>5}\")\n",
    "        race_accuracy[f'{cond}_{L}'] = float(acc)\n",
    "\n",
    "# ================================================================\n",
    "# PART 6: Cross-Benchmark Ranking\n",
    "# ================================================================\n",
    "print(f\"\\n--- PART 6: Cross-Benchmark Ranking (best prefix x length) ---\")\n",
    "\n",
    "# Find the condition+length with highest pooled semantic delta d\n",
    "all_combos = []\n",
    "for cond_base in ['comprehend', 'extract_general']:\n",
    "    for L in PREFIX_LENGTHS:\n",
    "        ds_effects = []\n",
    "        for ds_name in ALL_DATASETS:\n",
    "            pb = phase_b_nlls[ds_name]\n",
    "            sem_delta = pb[f'random_tokens_{L}'] - pb[f'{cond_base}_{L}']\n",
    "            n = len(sem_delta)\n",
    "            d = cohens_d(sem_delta)\n",
    "            se = np.sqrt(1.0/n + d**2 / (2.0*n))\n",
    "            ds_effects.append((d, se, n))\n",
    "        weights = [1.0 / (se**2) for _, se, _ in ds_effects]\n",
    "        w_sum = sum(weights)\n",
    "        pooled_d = sum(w * d for (d, _, _), w in zip(ds_effects, weights)) / w_sum\n",
    "        pooled_se = 1.0 / np.sqrt(w_sum)\n",
    "        all_combos.append((cond_base, L, pooled_d, pooled_se))\n",
    "\n",
    "all_combos.sort(key=lambda x: x[2], reverse=True)\n",
    "print(f\"\\n  {'Rank':>4} {'Condition':<28} {'L':>5} {'pooled_d':>9} {'SE':>8}\")\n",
    "print(f\"  {'-'*58}\")\n",
    "for rank, (cond, L, d, se) in enumerate(all_combos, 1):\n",
    "    print(f\"  {rank:>4} {cond:<28} {L:>5} {d:>+9.4f} {se:>8.4f}\")\n",
    "\n",
    "# ================================================================\n",
    "# PART 7: Comprehend Meaning at Length\n",
    "# ================================================================\n",
    "print(f\"\\n--- PART 7: Comprehend Meaning Effect at Each Length ---\")\n",
    "print(f\"  {'L':>5} {'pooled_d':>9} {'SE':>8} {'z':>8} {'p':>10} {'sig':>4}\")\n",
    "print(f\"  {'-'*48}\")\n",
    "\n",
    "meaning_by_length = {}\n",
    "for L in PREFIX_LENGTHS:\n",
    "    ds_diffs = []\n",
    "    for ds_name in ALL_DATASETS:\n",
    "        pb = phase_b_nlls[ds_name]\n",
    "        diff = pb[f'scrambled_comprehend_{L}'] - pb[f'comprehend_{L}']\n",
    "        ds_diffs.append(diff)\n",
    "    pooled_diff = np.concatenate(ds_diffs)\n",
    "    d = cohens_d(pooled_diff)\n",
    "    _, p = stats.ttest_1samp(pooled_diff, 0)\n",
    "    se = np.sqrt(1.0/len(pooled_diff) + d**2/(2.0*len(pooled_diff)))\n",
    "    z = d / se if se > 0 else 0.0\n",
    "    sig = ('***' if p < 0.001 else '**' if p < 0.01\n",
    "           else '*' if p < 0.05 else 'ns')\n",
    "    print(f\"  {L:>5} {d:>+9.4f} {se:>8.4f} {z:>+8.2f} {p:>10.2e} {sig:>4}\")\n",
    "    meaning_by_length[L] = {'d': float(d), 'p': float(p), 'se': float(se)}\n",
    "\n",
    "# ================================================================\n",
    "# PART 8: Ceiling Analysis\n",
    "# ================================================================\n",
    "print(f\"\\n--- PART 8: Ceiling Analysis ---\")\n",
    "print(f\"  Maximum achievable total effect (bare_trunc - comprehend_L):\")\n",
    "print(f\"  {'L':>5} {'pooled mean delta':>18} {'max ds delta':>14} {'as % of bare':>14}\")\n",
    "print(f\"  {'-'*56}\")\n",
    "\n",
    "ceiling = {}\n",
    "for L in PREFIX_LENGTHS:\n",
    "    ds_deltas = []\n",
    "    ds_pcts = []\n",
    "    for ds_name in ALL_DATASETS:\n",
    "        pb = phase_b_nlls[ds_name]\n",
    "        delta = (pb['bare_trunc'] - pb[f'comprehend_{L}']).mean()\n",
    "        bare_mean = pb['bare_trunc'].mean()\n",
    "        pct = delta / bare_mean * 100 if bare_mean > 0 else 0\n",
    "        ds_deltas.append(delta)\n",
    "        ds_pcts.append(pct)\n",
    "    mean_delta = np.mean(ds_deltas)\n",
    "    max_delta = np.max(ds_deltas)\n",
    "    mean_pct = np.mean(ds_pcts)\n",
    "    print(f\"  {L:>5} {mean_delta:>+18.4f} {max_delta:>+14.4f} {mean_pct:>13.1f}%\")\n",
    "    ceiling[L] = {\n",
    "        'mean_delta': float(mean_delta), 'max_delta': float(max_delta),\n",
    "        'mean_pct': float(mean_pct),\n",
    "    }\n",
    "\n",
    "# ================================================================\n",
    "# VERDICT\n",
    "# ================================================================\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"VERDICT — Exp 05: Scaling the Prefix Effect\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\nModel: {MODEL_NAME}\")\n",
    "print(f\"Scoring: BOS-retained repositioning + token-level prefix matching\")\n",
    "print(f\"Datasets: {len(ALL_DATASETS)} ({', '.join(ALL_DATASETS)})\")\n",
    "print(f\"Hard selection: top {HARD_FRAC*100:.0f}% by bare NLL\")\n",
    "\n",
    "print(f\"\\n--- Key Findings ---\")\n",
    "\n",
    "# 1. Task generalization\n",
    "print(f\"\\n  1. Task generalization (Phase A, Q-matched):\")\n",
    "for cond in PHASE_A_META_CONDS:\n",
    "    if cond in meta_results_A:\n",
    "        m = meta_results_A[cond]\n",
    "        sig = ('***' if m['p'] < 0.001 else '**' if m['p'] < 0.01\n",
    "               else '*' if m['p'] < 0.05 else 'ns')\n",
    "        print(f\"     {cond:<28} pooled_d={m['pooled_d']:+.4f} ({sig}) \"\n",
    "              f\"across {m['n_datasets']} datasets\")\n",
    "\n",
    "# 2. Optimal length\n",
    "print(f\"\\n  2. Optimal prefix length (Phase B, comprehend):\")\n",
    "if scaling_results.get('comprehend'):\n",
    "    best_L = max(scaling_results['comprehend'].items(),\n",
    "                 key=lambda x: x[1]['pooled_d'])\n",
    "    print(f\"     Best L={best_L[0]}: pooled_d={best_L[1]['pooled_d']:+.4f}\")\n",
    "\n",
    "# 3. Meaning growth\n",
    "print(f\"\\n  3. Comprehend meaning effect by length:\")\n",
    "for L in PREFIX_LENGTHS:\n",
    "    if L in meaning_by_length:\n",
    "        m = meaning_by_length[L]\n",
    "        sig = ('***' if m['p'] < 0.001 else '**' if m['p'] < 0.01\n",
    "               else '*' if m['p'] < 0.05 else 'ns')\n",
    "        print(f\"     L={L}: d={m['d']:+.4f} ({sig})\")\n",
    "\n",
    "# 4. RACE accuracy\n",
    "print(f\"\\n  4. RACE MC accuracy:\")\n",
    "print(f\"     bare: {race_accuracy.get('bare', 0):.1%}\")\n",
    "best_race = max(race_accuracy.items(), key=lambda x: x[1])\n",
    "print(f\"     best: {best_race[0]} = {best_race[1]:.1%}\")\n",
    "\n",
    "# 5. Best overall combo\n",
    "if all_combos:\n",
    "    best = all_combos[0]\n",
    "    print(f\"\\n  5. Best overall prefix x length:\")\n",
    "    print(f\"     {best[0]} @ L={best[1]}: pooled_d={best[2]:+.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c11d7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Save results\n",
    "print(\"=\" * 70)\n",
    "print(\"SAVING RESULTS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "final_results = {\n",
    "    'experiment': 'v4_exp05_prefix_scaling',\n",
    "    'model': MODEL_NAME,\n",
    "    'scoring': 'bos_retained_repositioning_token_matched',\n",
    "    'hard_fraction': HARD_FRAC,\n",
    "    'datasets': ALL_DATASETS,\n",
    "    'old_datasets': OLD_DATASETS,\n",
    "    'new_datasets': NEW_DATASETS,\n",
    "    'n_samples_per_dataset': N_SAMPLES,\n",
    "    'n_hard_per_dataset': N_HARD,\n",
    "    'prefix_lengths': PREFIX_LENGTHS,\n",
    "    'common_max_doc': COMMON_MAX_DOC,\n",
    "    'seed': SEED,\n",
    "    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'instructions': {name: text for name, text in INSTRUCTIONS.items()},\n",
    "    'phase_a_meta_analysis': meta_results_A,\n",
    "    'phase_b_scaling': scaling_results,\n",
    "    'pooled_decomposition_by_length': pooled_decomp_by_length,\n",
    "    'task_length_interaction': task_length,\n",
    "    'meaning_by_length': meaning_by_length,\n",
    "    'ceiling_analysis': ceiling,\n",
    "    'race_accuracy': race_accuracy,\n",
    "    'cross_benchmark_ranking': [\n",
    "        {'condition': c, 'length': l, 'pooled_d': float(d), 'se': float(se)}\n",
    "        for c, l, d, se in all_combos\n",
    "    ],\n",
    "    'per_dataset': per_dataset_analysis,\n",
    "    'hard_metadata': {ds: hard_metadata[ds] for ds in ALL_DATASETS},\n",
    "}\n",
    "\n",
    "with open(RESULTS_DIR / 'results.json', 'w') as f:\n",
    "    json.dump(final_results, f, indent=2)\n",
    "print(f\"Results saved to {RESULTS_DIR / 'results.json'}\")\n",
    "\n",
    "# Cleanup\n",
    "print(f\"\\nCleaning up GPU memory...\")\n",
    "mem_before = torch.cuda.memory_allocated() / 1e9\n",
    "del model, tokenizer\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "mem_after = torch.cuda.memory_allocated() / 1e9\n",
    "print(f\"GPU memory: {mem_before:.2f} GB -> {mem_after:.2f} GB\")\n",
    "print(\"Done!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
