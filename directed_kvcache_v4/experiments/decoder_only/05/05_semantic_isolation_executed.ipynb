{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f57e603b",
   "metadata": {},
   "source": [
    "# Decoder-Only Exp 05: Semantic Priming in Isolation\n",
    "\n",
    "## Motivation\n",
    "\n",
    "Exp 03-04 showed that the \"structural effect\" is actually BOS removal + position\n",
    "offset + attention sink pruning. These structural factors are so large that\n",
    "`prune_first_3` (d=+0.80) and `pos_4` (d=+0.78) beat the oracle (d=+0.64).\n",
    "\n",
    "But **we cannot conclude that semantic priming is zero**, because in all prior\n",
    "experiments the oracle differs from controls in BOTH structure AND content\n",
    "simultaneously. To isolate semantics, we need an experiment where **all structural\n",
    "confounds are equalized** and only prefix content varies.\n",
    "\n",
    "## Design principle\n",
    "\n",
    "For each sample, compute Q = number of query tokens (without BOS). ALL prefixed\n",
    "conditions (4-9) use a prefix of **exactly Q token IDs**. This ensures:\n",
    "- Same BOS removal (always sliced)\n",
    "- Same position offset (doc starts at position Q+2 in all conditions)\n",
    "- Same number of Phase A attention targets (Q prefix tokens)\n",
    "- Same cache length (only doc KV entries)\n",
    "\n",
    "The **only** thing that varies across prefixed conditions is the semantic content\n",
    "of the Q prefix tokens.\n",
    "\n",
    "## Conditions (9)\n",
    "\n",
    "### Reference baselines (different structure, for context)\n",
    "| # | Condition | Description |\n",
    "|---|-----------|-------------|\n",
    "| 1 | bare | Standard: [BOS + doc], nothing sliced |\n",
    "| 2 | best_structural | BOS + first 3 doc tokens removed from cache (exp04 champion) |\n",
    "| 3 | no_prefix_posmatched | position_offset = per-sample Q+2, BOS removed, no prefix |\n",
    "\n",
    "### Length-matched semantic gradient (all have exactly Q prefix tokens)\n",
    "| # | Condition | Prefix content | Semantic level |\n",
    "|---|-----------|---------------|----------------|\n",
    "| 4 | repeat_token | token(\"the\") × Q | Zero content variation |\n",
    "| 5 | random_tokens | Q random IDs from query vocab | Varied embeddings, no meaning |\n",
    "| 6 | unrelated_query | Different sample's query tokens (truncated/padded to Q) | Coherent text, wrong content |\n",
    "| 7 | shuffled_query | Query tokens randomly permuted | Right vocabulary, wrong syntax |\n",
    "| 8 | doc_keywords | First Q non-stopword doc tokens | Doc-relevant, not query-specific |\n",
    "| 9 | oracle | Actual query tokens | Full semantic match |\n",
    "\n",
    "## Planned contrasts\n",
    "\n",
    "| Contrast | Isolates |\n",
    "|----------|----------|\n",
    "| no_prefix_posmatched vs repeat_token | Attention enrichment effect |\n",
    "| repeat_token vs random_tokens | Embedding variation |\n",
    "| random_tokens vs unrelated_query | Natural language structure |\n",
    "| unrelated_query vs shuffled_query | Vocabulary match (right words, wrong order) |\n",
    "| shuffled_query vs oracle | Word order / syntax |\n",
    "| doc_keywords vs oracle | Query vs document relevance |\n",
    "| best_structural vs oracle | Pure structure vs semantic priming |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f012c1cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-20T15:41:40.635255Z",
     "iopub.status.busy": "2026-02-20T15:41:40.634957Z",
     "iopub.status.idle": "2026-02-20T15:41:55.825057Z",
     "shell.execute_reply": "2026-02-20T15:41:55.824081Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading google/gemma-3-4b-it...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bb70defda6549aab2a16b32c6a4fe6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/883 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exp 05: Semantic Priming in Isolation\n",
      "N: 400, Model: google/gemma-3-4b-it\n",
      "DEVICE: cuda:0, dtype: torch.bfloat16\n",
      "GPU memory: 8.60 GB\n",
      "Vocab size: 262208\n",
      "Num layers: 34\n",
      "Num KV heads: 4\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Setup and model loading\n",
    "import os\n",
    "os.umask(0o000)\n",
    "\n",
    "import sys, json, time, gc, re\n",
    "import random as pyrandom\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from scipy import stats\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "sys.path.insert(0, \"../../..\")\n",
    "from lib.analysis import cohens_d\n",
    "\n",
    "SEED = 42\n",
    "N_SAMPLES = 400\n",
    "MODEL_NAME = \"google/gemma-3-4b-it\"\n",
    "\n",
    "RESULTS_DIR = Path(\"../../../results/decoder_only/exp05\")\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CHECKPOINT_PATH = RESULTS_DIR / \"checkpoint.json\"\n",
    "\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "pyrandom.seed(SEED)\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv())\n",
    "HF_TOKEN = os.environ.get(\"HF_TOKEN\")\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "print(f\"Loading {MODEL_NAME}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, token=HF_TOKEN)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME, device_map=\"auto\", torch_dtype=torch.bfloat16, token=HF_TOKEN,\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "DEVICE = next(model.parameters()).device\n",
    "\n",
    "print(f\"Exp 05: Semantic Priming in Isolation\")\n",
    "print(f\"N: {N_SAMPLES}, Model: {MODEL_NAME}\")\n",
    "print(f\"DEVICE: {DEVICE}, dtype: {next(model.parameters()).dtype}\")\n",
    "print(f\"GPU memory: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "text_cfg = getattr(model.config, 'text_config', model.config)\n",
    "VOCAB_SIZE = getattr(text_cfg, 'vocab_size', 262208)\n",
    "print(f\"Vocab size: {VOCAB_SIZE}\")\n",
    "print(f\"Num layers: {getattr(text_cfg, 'num_hidden_layers', 'N/A')}\")\n",
    "print(f\"Num KV heads: {getattr(text_cfg, 'num_key_value_heads', 'N/A')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7304a711",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-20T15:41:55.829318Z",
     "iopub.status.busy": "2026-02-20T15:41:55.828712Z",
     "iopub.status.idle": "2026-02-20T15:41:55.851611Z",
     "shell.execute_reply": "2026-02-20T15:41:55.850852Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOS token ID: 2\n",
      "Newline token IDs: [107] (1 tokens)\n",
      "Scoring function defined with token-level prefix support.\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: KV cache helpers and scoring function\n",
    "\n",
    "def slice_kv_cache(cache, start_idx):\n",
    "    # Remove first start_idx entries from KV cache.\n",
    "    from transformers import DynamicCache\n",
    "    if isinstance(cache, DynamicCache):\n",
    "        sliced = DynamicCache()\n",
    "        for i in range(len(cache.layers)):\n",
    "            k = cache.layers[i].keys[:, :, start_idx:, :]\n",
    "            v = cache.layers[i].values[:, :, start_idx:, :]\n",
    "            sliced.update(k, v, i)\n",
    "        return sliced\n",
    "    else:\n",
    "        return tuple(\n",
    "            (k[:, :, start_idx:, :], v[:, :, start_idx:, :])\n",
    "            for k, v in cache\n",
    "        )\n",
    "\n",
    "\n",
    "NEWLINE_IDS = tokenizer(\"\\n\", add_special_tokens=False).input_ids\n",
    "BOS_ID = tokenizer.bos_token_id\n",
    "print(f\"BOS token ID: {BOS_ID}\")\n",
    "print(f\"Newline token IDs: {NEWLINE_IDS} ({len(NEWLINE_IDS)} tokens)\")\n",
    "\n",
    "\n",
    "def score(doc_text, query_text, answer_text,\n",
    "          prefix_token_ids=None,\n",
    "          position_offset=0, remove_bos=False,\n",
    "          prune_first=0):\n",
    "    # Score NLL of answer tokens using two-phase KV cache.\n",
    "    #\n",
    "    # Modes:\n",
    "    #   prefix_token_ids: [BOS] + prefix_ids + [\\n] + doc_ids\n",
    "    #     Slices first 1+len(prefix_ids)+len(NEWLINE_IDS) entries (BOS+prefix+\\n)\n",
    "    #   position_offset > 0: BOS at pos 0, doc at offset..offset+D, BOS removed\n",
    "    #   remove_bos + prune_first: bare with BOS + first N doc tokens removed\n",
    "    #   Default: bare (BOS in cache)\n",
    "\n",
    "    # --- Phase A: Conditioning ---\n",
    "    if prefix_token_ids is not None:\n",
    "        doc_ids = tokenizer(doc_text, add_special_tokens=False,\n",
    "                            truncation=True, max_length=1536).input_ids\n",
    "        cond_ids = [BOS_ID] + list(prefix_token_ids) + NEWLINE_IDS + doc_ids\n",
    "        slice_start = 1 + len(prefix_token_ids) + len(NEWLINE_IDS)\n",
    "        custom_pos = None\n",
    "        phase_b_start = len(cond_ids)\n",
    "\n",
    "    elif position_offset > 0:\n",
    "        cond_ids = tokenizer(doc_text, add_special_tokens=True,\n",
    "                             truncation=True, max_length=2048).input_ids\n",
    "        n_doc = len(cond_ids) - 1\n",
    "        pos_list = [0] + list(range(position_offset, position_offset + n_doc))\n",
    "        custom_pos = torch.tensor([pos_list], dtype=torch.long, device=DEVICE)\n",
    "        slice_start = 1  # remove BOS\n",
    "        phase_b_start = position_offset + n_doc\n",
    "\n",
    "    else:\n",
    "        cond_ids = tokenizer(doc_text, add_special_tokens=True,\n",
    "                             truncation=True, max_length=2048).input_ids\n",
    "        slice_start = 1 if remove_bos else 0\n",
    "        custom_pos = None\n",
    "        phase_b_start = len(cond_ids)\n",
    "\n",
    "    cond_tensor = torch.tensor([cond_ids], dtype=torch.long, device=DEVICE)\n",
    "\n",
    "    fwd_kwargs = {'input_ids': cond_tensor, 'use_cache': True}\n",
    "    if custom_pos is not None:\n",
    "        fwd_kwargs['position_ids'] = custom_pos\n",
    "\n",
    "    with torch.no_grad():\n",
    "        phase_a = model(**fwd_kwargs)\n",
    "\n",
    "    cache = phase_a.past_key_values\n",
    "    del phase_a\n",
    "\n",
    "    if slice_start > 0:\n",
    "        cache = slice_kv_cache(cache, slice_start)\n",
    "\n",
    "    if prune_first > 0:\n",
    "        cache = slice_kv_cache(cache, prune_first)\n",
    "\n",
    "    # --- Phase B: Inference ---\n",
    "    query_part_ids = tokenizer(\"\\n\" + query_text + \"\\n\",\n",
    "                               add_special_tokens=False).input_ids\n",
    "    answer_ids = tokenizer(answer_text, add_special_tokens=False,\n",
    "                           truncation=True, max_length=256).input_ids\n",
    "\n",
    "    if not answer_ids:\n",
    "        del cache\n",
    "        return 0.0\n",
    "\n",
    "    phase_b_ids = query_part_ids + answer_ids\n",
    "    phase_b_tensor = torch.tensor([phase_b_ids], dtype=torch.long, device=DEVICE)\n",
    "\n",
    "    pos_ids = torch.arange(phase_b_start, phase_b_start + len(phase_b_ids),\n",
    "                           device=DEVICE).unsqueeze(0)\n",
    "    cache_position = torch.arange(phase_b_start, phase_b_start + len(phase_b_ids),\n",
    "                                  device=DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        phase_b = model(\n",
    "            input_ids=phase_b_tensor,\n",
    "            past_key_values=cache,\n",
    "            position_ids=pos_ids,\n",
    "            cache_position=cache_position,\n",
    "            use_cache=False,\n",
    "        )\n",
    "\n",
    "    logits = phase_b.logits\n",
    "    n_query_part = len(query_part_ids)\n",
    "    n_answer = len(answer_ids)\n",
    "\n",
    "    answer_logits = logits[0, n_query_part - 1 : n_query_part - 1 + n_answer, :]\n",
    "    answer_targets = torch.tensor(answer_ids, dtype=torch.long, device=DEVICE)\n",
    "\n",
    "    log_probs = F.log_softmax(answer_logits, dim=-1)\n",
    "    token_log_probs = log_probs.gather(1, answer_targets.unsqueeze(1)).squeeze(1)\n",
    "    mean_nll = -token_log_probs.mean().item()\n",
    "\n",
    "    del cache, phase_b, logits, log_probs\n",
    "    return mean_nll\n",
    "\n",
    "\n",
    "print(\"Scoring function defined with token-level prefix support.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef5913d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-20T15:41:55.854936Z",
     "iopub.status.busy": "2026-02-20T15:41:55.854645Z",
     "iopub.status.idle": "2026-02-20T15:41:57.480244Z",
     "shell.execute_reply": "2026-02-20T15:41:57.479222Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MS MARCO v1.1 validation...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total candidates: 1200\n",
      "Query vocabulary pool: 1214 unique token IDs\n",
      "Token ID for 'the': 1437\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loaded 400 samples\n",
      "Mean passage words: 73\n",
      "Mean query words: 5.9\n",
      "Query token count — mean: 6.5, median: 6, min: 2, max: 15\n",
      "  Sample 0: Q=5, query='average annual temperature of Uruguay...'\n",
      "    repeat:    thethethethethe...\n",
      "    random:     treatyUT observed electric les...\n",
      "    unrelated: average cost for an acre...\n",
      "    shuffled:   of temperatureaverage annual Uruguay...\n",
      "    doc_kw:    AverageTemperaturesMontevideo...\n",
      "    oracle:    average annual temperature of Uruguay...\n",
      "  Sample 1: Q=10, query='average cost for an acre of land in arizona...'\n",
      "    repeat:    thethethethethethethethe...\n",
      "    random:     dollars water represent downstreamx sunscreen virginiaaket...\n",
      "    unrelated: where can i buy nematodeswhere can i...\n",
      "    shuffled:   cost of a acre for in an land...\n",
      "    doc_kw:    Arizona.72millionacres,Arizona...\n",
      "    oracle:    average cost for an acre of land in...\n",
      "  Sample 2: Q=5, query='where can i buy nematodes...'\n",
      "    repeat:    thethethethethe...\n",
      "    random:     disease special plateau mountain squirrel...\n",
      "    unrelated: how much does it cost...\n",
      "    shuffled:   nematodeswhere buy i can...\n",
      "    doc_kw:    buyGOODBUGSSupplier...\n",
      "    oracle:    where can i buy nematodes...\n",
      "  Sample 3: Q=9, query='how much does it cost for varicose vein removal...'\n",
      "    repeat:    thethethethethethethethe...\n",
      "    random:    tu eye electric chasscar rhin monop...\n",
      "    unrelated: where is popocatepetl locatedwhere...\n",
      "    shuffled:  how it cost vein for varicose removal much...\n",
      "    doc_kw:    1AccordingwebsiteLawPublish.com,...\n",
      "    oracle:    how much does it cost for varicose vein...\n",
      "  Sample 4: Q=7, query='where is popocatepetl located...'\n",
      "    repeat:    thethethethethethethe...\n",
      "    random:    hevik married Get qualificationsgest rabbits club...\n",
      "    unrelated: what is function of glutamatewhat is...\n",
      "    shuffled:  ocatewherel poppet is located...\n",
      "    doc_kw:    Confidencevotes25.5K...\n",
      "    oracle:    where is popocatepetl located...\n",
      "All prefix lengths verified.\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Load MS MARCO data and build per-sample prefix token IDs\n",
    "from lib.data import count_words\n",
    "from datasets import load_dataset\n",
    "\n",
    "print(\"Loading MS MARCO v1.1 validation...\")\n",
    "ds = load_dataset(\"microsoft/ms_marco\", \"v1.1\", split=\"validation\")\n",
    "\n",
    "all_candidates = []\n",
    "for item in ds:\n",
    "    if len(all_candidates) >= 3 * N_SAMPLES:\n",
    "        break\n",
    "    passages = item.get('passages', {})\n",
    "    ptexts = passages.get('passage_text', [])\n",
    "    is_sel = passages.get('is_selected', [])\n",
    "    query = item.get('query', '')\n",
    "    answers = item.get('answers', [])\n",
    "    well_formed = item.get('wellFormedAnswers', [])\n",
    "    answer = None\n",
    "    if well_formed and len(well_formed) > 0 and well_formed[0] not in ('[]', ''):\n",
    "        answer = well_formed[0]\n",
    "    elif answers and len(answers) > 0 and answers[0] != 'No Answer Present.':\n",
    "        answer = answers[0]\n",
    "    if not answer:\n",
    "        continue\n",
    "    for pt, sel in zip(ptexts, is_sel):\n",
    "        wc = count_words(pt)\n",
    "        if sel == 1 and 30 <= wc <= 300:\n",
    "            all_candidates.append({\n",
    "                'passage': pt, 'query': query, 'answer': answer,\n",
    "                'word_count': wc,\n",
    "            })\n",
    "            break\n",
    "\n",
    "print(f\"Total candidates: {len(all_candidates)}\")\n",
    "np.random.seed(SEED)\n",
    "indices = np.random.permutation(len(all_candidates))\n",
    "samples = [all_candidates[i] for i in indices[:N_SAMPLES]]\n",
    "del ds, all_candidates\n",
    "gc.collect()\n",
    "\n",
    "# --- Build per-sample token-level prefix data ---\n",
    "\n",
    "# Collect all query token IDs (for random_tokens pool)\n",
    "all_query_token_ids = []\n",
    "for s in samples:\n",
    "    ids = tokenizer(s['query'], add_special_tokens=False).input_ids\n",
    "    all_query_token_ids.extend(ids)\n",
    "query_vocab_pool = list(set(all_query_token_ids))\n",
    "print(f\"Query vocabulary pool: {len(query_vocab_pool)} unique token IDs\")\n",
    "\n",
    "# Stopword token IDs for doc_keywords filtering\n",
    "STOPWORDS = set(\"the a an is are was were be been being have has had do does did \"\n",
    "                \"will would shall should may might can could of in to for on with \"\n",
    "                \"at by from as into through during before after above below between \"\n",
    "                \"and or but not no nor so yet both either neither each every all any \"\n",
    "                \"few more most other some such than too very it its this that these \"\n",
    "                \"those i me my we our you your he him his she her they them their \"\n",
    "                \"what which who whom whose when where how why if then else\".split())\n",
    "\n",
    "# The token ID for \"the\" (for repeat_token condition)\n",
    "THE_TOKEN_ID = tokenizer(\"the\", add_special_tokens=False).input_ids[0]\n",
    "print(f\"Token ID for 'the': {THE_TOKEN_ID}\")\n",
    "\n",
    "pyrandom.seed(SEED + 200)\n",
    "\n",
    "for i, s in enumerate(samples):\n",
    "    # Tokenize query\n",
    "    q_ids = tokenizer(s['query'], add_special_tokens=False).input_ids\n",
    "    Q = len(q_ids)\n",
    "    s['query_token_ids'] = q_ids\n",
    "    s['Q'] = Q\n",
    "\n",
    "    # 1. repeat_token: \"the\" repeated Q times\n",
    "    s['prefix_repeat'] = [THE_TOKEN_ID] * Q\n",
    "\n",
    "    # 2. random_tokens: Q random IDs from query vocabulary pool\n",
    "    s['prefix_random'] = [pyrandom.choice(query_vocab_pool) for _ in range(Q)]\n",
    "\n",
    "    # 3. unrelated_query: next sample's query tokens, truncated/padded to Q\n",
    "    other_idx = (i + 1) % len(samples)\n",
    "    other_q_ids = tokenizer(samples[other_idx]['query'],\n",
    "                            add_special_tokens=False).input_ids\n",
    "    if len(other_q_ids) >= Q:\n",
    "        s['prefix_unrelated'] = other_q_ids[:Q]\n",
    "    else:\n",
    "        # Pad by repeating the other query's tokens\n",
    "        padded = other_q_ids * ((Q // len(other_q_ids)) + 1)\n",
    "        s['prefix_unrelated'] = padded[:Q]\n",
    "\n",
    "    # 4. shuffled_query: query tokens permuted\n",
    "    shuffled = list(q_ids)\n",
    "    pyrandom.shuffle(shuffled)\n",
    "    s['prefix_shuffled'] = shuffled\n",
    "\n",
    "    # 5. doc_keywords: first Q non-stopword tokens from document\n",
    "    doc_tokens = tokenizer(s['passage'], add_special_tokens=False).input_ids\n",
    "    doc_words = s['passage'].split()\n",
    "    keyword_ids = []\n",
    "    for word in doc_words:\n",
    "        if word.lower().strip(\".,;:!?()[]{}\\\"'\") not in STOPWORDS:\n",
    "            w_ids = tokenizer(word, add_special_tokens=False).input_ids\n",
    "            keyword_ids.extend(w_ids)\n",
    "            if len(keyword_ids) >= Q:\n",
    "                break\n",
    "    # Pad with first doc tokens if not enough keywords\n",
    "    if len(keyword_ids) < Q:\n",
    "        keyword_ids.extend(doc_tokens[:Q - len(keyword_ids)])\n",
    "    s['prefix_doc_kw'] = keyword_ids[:Q]\n",
    "\n",
    "    # 6. oracle: actual query tokens (already have them)\n",
    "    s['prefix_oracle'] = q_ids\n",
    "\n",
    "# Summary statistics\n",
    "q_lens = [s['Q'] for s in samples]\n",
    "print(f\"\\nLoaded {len(samples)} samples\")\n",
    "print(f\"Mean passage words: {np.mean([s['word_count'] for s in samples]):.0f}\")\n",
    "print(f\"Mean query words: {np.mean([len(s['query'].split()) for s in samples]):.1f}\")\n",
    "print(f\"Query token count — mean: {np.mean(q_lens):.1f}, \"\n",
    "      f\"median: {np.median(q_lens):.0f}, \"\n",
    "      f\"min: {np.min(q_lens)}, max: {np.max(q_lens)}\")\n",
    "\n",
    "# Verify all prefixes have exactly Q tokens\n",
    "for i, s in enumerate(samples[:5]):\n",
    "    Q = s['Q']\n",
    "    for name in ['prefix_repeat', 'prefix_random', 'prefix_unrelated',\n",
    "                  'prefix_shuffled', 'prefix_doc_kw', 'prefix_oracle']:\n",
    "        assert len(s[name]) == Q, f\"Sample {i} {name}: len={len(s[name])} != Q={Q}\"\n",
    "    print(f\"  Sample {i}: Q={Q}, query='{s['query'][:50]}...'\")\n",
    "    print(f\"    repeat:    {tokenizer.decode(s['prefix_repeat'][:8])}...\")\n",
    "    print(f\"    random:    {tokenizer.decode(s['prefix_random'][:8])}...\")\n",
    "    print(f\"    unrelated: {tokenizer.decode(s['prefix_unrelated'][:8])}...\")\n",
    "    print(f\"    shuffled:  {tokenizer.decode(s['prefix_shuffled'][:8])}...\")\n",
    "    print(f\"    doc_kw:    {tokenizer.decode(s['prefix_doc_kw'][:8])}...\")\n",
    "    print(f\"    oracle:    {tokenizer.decode(s['prefix_oracle'][:8])}...\")\n",
    "print(\"All prefix lengths verified.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05a7cd96",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-20T15:41:57.484397Z",
     "iopub.status.busy": "2026-02-20T15:41:57.483479Z",
     "iopub.status.idle": "2026-02-20T15:41:59.836835Z",
     "shell.execute_reply": "2026-02-20T15:41:59.835828Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "VALIDATION\n",
      "======================================================================\n",
      "\n",
      "Sample 0: Q=5 query tokens\n",
      "  Query: 'average annual temperature of Uruguay'\n",
      "  Position of doc start in all prefixed conditions: 7\n",
      "  (BOS=1 + prefix=5 + newline=1)\n",
      "\n",
      "--- Position verification ---\n",
      "  prefix_repeat        prefix_len=  5, slice_start=  7, doc_start_pos=  7, total_len= 128\n",
      "  prefix_random        prefix_len=  5, slice_start=  7, doc_start_pos=  7, total_len= 128\n",
      "  prefix_unrelated     prefix_len=  5, slice_start=  7, doc_start_pos=  7, total_len= 128\n",
      "  prefix_shuffled      prefix_len=  5, slice_start=  7, doc_start_pos=  7, total_len= 128\n",
      "  prefix_doc_kw        prefix_len=  5, slice_start=  7, doc_start_pos=  7, total_len= 128\n",
      "  prefix_oracle        prefix_len=  5, slice_start=  7, doc_start_pos=  7, total_len= 128\n",
      "\n",
      "  no_prefix_posmatched: position_offset=7 (matching prefixed doc start)\n",
      "\n",
      "--- NLL for each condition (sample 0) ---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  bare                     NLL = 0.7383\n",
      "  best_structural          NLL = 0.5273  delta = +0.2109\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  no_prefix_posmatched     NLL = 0.4434  delta = +0.2949\n",
      "  repeat_token             NLL = 0.7266  delta = +0.0117\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  random_tokens            NLL = 0.7930  delta = -0.0547\n",
      "  unrelated_query          NLL = 0.8633  delta = -0.1250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  shuffled_query           NLL = 0.9727  delta = -0.2344\n",
      "  doc_keywords             NLL = 0.8164  delta = -0.0781\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  oracle                   NLL = 0.8594  delta = -0.1211\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Validate scoring modes\n",
    "print(\"=\" * 70)\n",
    "print(\"VALIDATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "s = samples[0]\n",
    "Q = s['Q']\n",
    "\n",
    "print(f\"\\nSample 0: Q={Q} query tokens\")\n",
    "print(f\"  Query: '{s['query']}'\")\n",
    "print(f\"  Position of doc start in all prefixed conditions: {Q + 2}\")\n",
    "print(f\"  (BOS=1 + prefix={Q} + newline={len(NEWLINE_IDS)})\")\n",
    "\n",
    "# Verify position matching: all prefixed conditions should put doc at same position\n",
    "print(f\"\\n--- Position verification ---\")\n",
    "doc_ids = tokenizer(s['passage'], add_special_tokens=False,\n",
    "                    truncation=True, max_length=1536).input_ids\n",
    "for name in ['prefix_repeat', 'prefix_random', 'prefix_unrelated',\n",
    "             'prefix_shuffled', 'prefix_doc_kw', 'prefix_oracle']:\n",
    "    prefix_ids = s[name]\n",
    "    cond_ids = [BOS_ID] + prefix_ids + NEWLINE_IDS + doc_ids\n",
    "    slice_start = 1 + len(prefix_ids) + len(NEWLINE_IDS)\n",
    "    doc_start_pos = slice_start  # doc[0] is at this position in the sequence\n",
    "    print(f\"  {name:<20} prefix_len={len(prefix_ids):>3}, \"\n",
    "          f\"slice_start={slice_start:>3}, doc_start_pos={doc_start_pos:>3}, \"\n",
    "          f\"total_len={len(cond_ids):>4}\")\n",
    "\n",
    "# Verify no_prefix_posmatched uses same offset\n",
    "pos_offset = Q + 1 + len(NEWLINE_IDS)\n",
    "print(f\"\\n  no_prefix_posmatched: position_offset={pos_offset} \"\n",
    "      f\"(matching prefixed doc start)\")\n",
    "\n",
    "# Score all modes\n",
    "print(f\"\\n--- NLL for each condition (sample 0) ---\")\n",
    "nll_bare = score(s['passage'], s['query'], s['answer'])\n",
    "print(f\"  {'bare':<24} NLL = {nll_bare:.4f}\")\n",
    "\n",
    "nll_struct = score(s['passage'], s['query'], s['answer'],\n",
    "                   remove_bos=True, prune_first=3)\n",
    "print(f\"  {'best_structural':<24} NLL = {nll_struct:.4f}  \"\n",
    "      f\"delta = {nll_bare - nll_struct:+.4f}\")\n",
    "\n",
    "nll_posmatched = score(s['passage'], s['query'], s['answer'],\n",
    "                       position_offset=pos_offset)\n",
    "print(f\"  {'no_prefix_posmatched':<24} NLL = {nll_posmatched:.4f}  \"\n",
    "      f\"delta = {nll_bare - nll_posmatched:+.4f}\")\n",
    "\n",
    "for name, prefix_key in [('repeat_token', 'prefix_repeat'),\n",
    "                          ('random_tokens', 'prefix_random'),\n",
    "                          ('unrelated_query', 'prefix_unrelated'),\n",
    "                          ('shuffled_query', 'prefix_shuffled'),\n",
    "                          ('doc_keywords', 'prefix_doc_kw'),\n",
    "                          ('oracle', 'prefix_oracle')]:\n",
    "    nll = score(s['passage'], s['query'], s['answer'],\n",
    "                prefix_token_ids=s[prefix_key])\n",
    "    print(f\"  {name:<24} NLL = {nll:.4f}  delta = {nll_bare - nll:+.4f}\")\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1bde2f6a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-20T15:41:59.840578Z",
     "iopub.status.busy": "2026-02-20T15:41:59.840243Z",
     "iopub.status.idle": "2026-02-20T15:53:55.692604Z",
     "shell.execute_reply": "2026-02-20T15:53:55.691729Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "SCORING ALL CONDITIONS\n",
      "======================================================================\n",
      "Starting fresh: 9 conditions x 400 samples\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68d796bf0c054977b03a0120342290ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Scoring:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 20/400 | 0.6m | ETA 11.6m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 40/400 | 1.2m | ETA 10.8m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 60/400 | 1.8m | ETA 10.2m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 80/400 | 2.4m | ETA 9.6m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 100/400 | 3.0m | ETA 8.9m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 120/400 | 3.6m | ETA 8.3m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 140/400 | 4.2m | ETA 7.7m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 160/400 | 4.7m | ETA 7.1m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 180/400 | 5.3m | ETA 6.5m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 200/400 | 5.9m | ETA 5.9m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 220/400 | 6.5m | ETA 5.3m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 240/400 | 7.1m | ETA 4.8m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 260/400 | 7.7m | ETA 4.2m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 280/400 | 8.3m | ETA 3.6m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 300/400 | 8.9m | ETA 3.0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 320/400 | 9.5m | ETA 2.4m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 340/400 | 10.1m | ETA 1.8m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 360/400 | 10.7m | ETA 1.2m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 380/400 | 11.3m | ETA 0.6m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 400/400 | 11.9m | ETA 0.0m\n",
      "\n",
      "Scoring complete: 400 samples, 9 conditions in 11.9 min\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Scoring loop — 9 conditions x 400 samples\n",
    "print(\"=\" * 70)\n",
    "print(\"SCORING ALL CONDITIONS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "COND_NAMES = [\n",
    "    'bare', 'best_structural', 'no_prefix_posmatched',\n",
    "    'repeat_token', 'random_tokens', 'unrelated_query',\n",
    "    'shuffled_query', 'doc_keywords', 'oracle',\n",
    "]\n",
    "\n",
    "results = []\n",
    "start_idx = 0\n",
    "\n",
    "if CHECKPOINT_PATH.exists():\n",
    "    ckpt = json.loads(CHECKPOINT_PATH.read_text())\n",
    "    if ckpt.get('n_total') == N_SAMPLES and len(ckpt.get('results', [])) > 0:\n",
    "        saved_queries = [r['query'][:50] for r in ckpt['results']]\n",
    "        current_queries = [s['query'][:50] for s in samples[:len(saved_queries)]]\n",
    "        if saved_queries == current_queries:\n",
    "            results = ckpt['results']\n",
    "            start_idx = len(results)\n",
    "            print(f\"Resuming from checkpoint: {start_idx}/{N_SAMPLES}\")\n",
    "\n",
    "if start_idx == 0:\n",
    "    print(f\"Starting fresh: {len(COND_NAMES)} conditions x {N_SAMPLES} samples\")\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "for i in tqdm(range(start_idx, N_SAMPLES), initial=start_idx, total=N_SAMPLES,\n",
    "              desc=\"Scoring\"):\n",
    "    s = samples[i]\n",
    "    query = s['query']\n",
    "    passage = s['passage']\n",
    "    answer = s['answer']\n",
    "    Q = s['Q']\n",
    "\n",
    "    result = {\n",
    "        'query': query,\n",
    "        'answer': answer,\n",
    "        'passage_words': s['word_count'],\n",
    "        'query_words': len(query.split()),\n",
    "        'Q': Q,\n",
    "    }\n",
    "\n",
    "    # 1. bare\n",
    "    result['nll_bare'] = score(passage, query, answer)\n",
    "\n",
    "    # 2. best_structural (BOS + first 3 doc tokens removed)\n",
    "    result['nll_best_structural'] = score(passage, query, answer,\n",
    "                                          remove_bos=True, prune_first=3)\n",
    "\n",
    "    # 3. no_prefix_posmatched (position offset = Q + 1 + len(NEWLINE_IDS))\n",
    "    pos_offset = Q + 1 + len(NEWLINE_IDS)\n",
    "    result['nll_no_prefix_posmatched'] = score(passage, query, answer,\n",
    "                                                position_offset=pos_offset)\n",
    "\n",
    "    # 4-9. Length-matched prefixed conditions\n",
    "    for cond_name, prefix_key in [\n",
    "        ('repeat_token', 'prefix_repeat'),\n",
    "        ('random_tokens', 'prefix_random'),\n",
    "        ('unrelated_query', 'prefix_unrelated'),\n",
    "        ('shuffled_query', 'prefix_shuffled'),\n",
    "        ('doc_keywords', 'prefix_doc_kw'),\n",
    "        ('oracle', 'prefix_oracle'),\n",
    "    ]:\n",
    "        result[f'nll_{cond_name}'] = score(passage, query, answer,\n",
    "                                            prefix_token_ids=s[prefix_key])\n",
    "\n",
    "    results.append(result)\n",
    "\n",
    "    if (i + 1) % 20 == 0 or i == N_SAMPLES - 1:\n",
    "        ckpt = {\n",
    "            'n_total': N_SAMPLES,\n",
    "            'results': results,\n",
    "            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        }\n",
    "        CHECKPOINT_PATH.write_text(json.dumps(ckpt))\n",
    "        elapsed = time.time() - t0\n",
    "        done = i - start_idx + 1\n",
    "        eta = (N_SAMPLES - i - 1) * elapsed / done if done > 0 else 0\n",
    "        tqdm.write(f\"  Checkpoint {i+1}/{N_SAMPLES} | {elapsed/60:.1f}m | ETA {eta/60:.1f}m\")\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "elapsed = time.time() - t0\n",
    "print(f\"\\nScoring complete: {len(results)} samples, \"\n",
    "      f\"{len(COND_NAMES)} conditions in {elapsed/60:.1f} min\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a61f249b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-20T15:53:55.696407Z",
     "iopub.status.busy": "2026-02-20T15:53:55.695682Z",
     "iopub.status.idle": "2026-02-20T15:53:55.716139Z",
     "shell.execute_reply": "2026-02-20T15:53:55.715293Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "RESULTS (N=400)\n",
      "======================================================================\n",
      "\n",
      "  Oracle delta (bare - oracle): +0.6977\n",
      "\n",
      "  Condition                     NLL    vs bare        d     Win%            p   sig   Recovery\n",
      "  ------------------------------------------------------------------------------------------------\n",
      "  bare                       1.6022         --       --       --           --    --         --\n",
      "  best_structural            0.6658    +0.9363   +0.536    82.0%     1.06e-23   ***     134.2%\n",
      "  no_prefix_posmatched       0.7259    +0.8762   +0.463    73.5%     1.24e-18   ***     125.6%\n",
      "  repeat_token               0.6796    +0.9226   +0.569    80.0%     3.67e-26   ***     132.2%\n",
      "  random_tokens              0.6631    +0.9391   +0.572    80.8%     1.97e-26   ***     134.6%\n",
      "  unrelated_query            0.7688    +0.8334   +0.541    77.2%     4.05e-24   ***     119.4%\n",
      "  shuffled_query             0.8308    +0.7713   +0.521    78.0%     1.20e-22   ***     110.5%\n",
      "  doc_keywords               0.5865    +1.0157   +0.621    82.5%     3.69e-30   ***     145.6%\n",
      "  oracle                     0.9044    +0.6977   +0.428    73.8%     2.32e-16   ***     100.0%\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Results table\n",
    "print(\"=\" * 70)\n",
    "print(f\"RESULTS (N={len(results)})\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "arrays = {}\n",
    "for name in COND_NAMES:\n",
    "    arrays[name] = np.array([r[f'nll_{name}'] for r in results])\n",
    "\n",
    "bare = arrays['bare']\n",
    "oracle = arrays['oracle']\n",
    "oracle_delta_mean = (bare - oracle).mean()\n",
    "\n",
    "print(f\"\\n  Oracle delta (bare - oracle): {oracle_delta_mean:+.4f}\")\n",
    "\n",
    "print(f\"\\n  {'Condition':<24} {'NLL':>8} {'vs bare':>10} {'d':>8} \"\n",
    "      f\"{'Win%':>8} {'p':>12} {'sig':>5} {'Recovery':>10}\")\n",
    "print(f\"  {'-'*96}\")\n",
    "\n",
    "analysis = {}\n",
    "for name in COND_NAMES:\n",
    "    nlls = arrays[name]\n",
    "    mean_nll = nlls.mean()\n",
    "\n",
    "    if name == 'bare':\n",
    "        print(f\"  {name:<24} {mean_nll:>8.4f} {'--':>10} {'--':>8} \"\n",
    "              f\"{'--':>8} {'--':>12} {'--':>5} {'--':>10}\")\n",
    "        analysis[name] = {'mean_nll': float(mean_nll)}\n",
    "    else:\n",
    "        diff = bare - nlls\n",
    "        d = cohens_d(diff)\n",
    "        win_pct = 100 * np.mean(diff > 0)\n",
    "        _, p_val = stats.ttest_1samp(diff, 0)\n",
    "        sig = '***' if p_val < 0.001 else '**' if p_val < 0.01 else '*' if p_val < 0.05 else 'ns'\n",
    "        rec = diff.mean() / oracle_delta_mean * 100 if oracle_delta_mean > 0 else 0\n",
    "\n",
    "        print(f\"  {name:<24} {mean_nll:>8.4f} {diff.mean():>+10.4f} {d:>+8.3f} \"\n",
    "              f\"{win_pct:>7.1f}% {p_val:>12.2e} {sig:>5} {rec:>9.1f}%\")\n",
    "        analysis[name] = {\n",
    "            'mean_nll': float(mean_nll), 'delta': float(diff.mean()),\n",
    "            'd': float(d), 'win_pct': float(win_pct), 'p': float(p_val),\n",
    "            'recovery': float(rec),\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8880353f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-20T15:53:55.719777Z",
     "iopub.status.busy": "2026-02-20T15:53:55.719509Z",
     "iopub.status.idle": "2026-02-20T15:53:55.745203Z",
     "shell.execute_reply": "2026-02-20T15:53:55.744281Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "SEMANTIC GRADIENT ANALYSIS\n",
      "======================================================================\n",
      "\n",
      "--- Attention enrichment effect ---\n",
      "  no_prefix_posmatched vs repeat_token\n",
      "  (same position offset, no prefix tokens vs Q uniform prefix tokens)\n",
      "  d = +0.0412 (ns), p = 4.11e-01\n",
      "  -> Attention enrichment is negligible\n",
      "\n",
      "--- Semantic gradient (pairwise contrasts) ---\n",
      "  Each step adds one dimension of semantic content.\n",
      "  Positive d = later condition is better.\n",
      "\n",
      "  Step                                            d            p   sig\n",
      "  ----------------------------------------------------------------------\n",
      "  Embedding variation                       +0.0307     5.39e-01    ns\n",
      "  Natural language structure                -0.1283     1.06e-02     *\n",
      "  Vocabulary match (right words)            -0.0688     1.69e-01    ns\n",
      "  Word order / syntax                       -0.1104     2.78e-02     *\n",
      "\n",
      "--- Doc relevance vs query relevance ---\n",
      "  doc_keywords vs oracle: d = -0.3382 (***)\n",
      "  -> Doc keywords are better: doc-derived priming is stronger\n",
      "\n",
      "--- Structure vs semantics ---\n",
      "  best_structural (prune_first_3) vs oracle: d = -0.2203 (***)\n",
      "  -> Best structural beats oracle: structure > semantics\n",
      "\n",
      "--- Overall semantic test ---\n",
      "  ANOVA-like: do the 6 prefixed conditions differ significantly?\n",
      "  F = 4.60, p = 3.50e-04 (***)\n",
      "  -> YES: prefix content significantly affects NLL\n",
      "\n",
      "--- Strongest semantic contrast: repeat_token vs oracle ---\n",
      "  d = -0.2397 (***), p = 2.32e-06\n",
      "  repeat_token NLL = 0.6796\n",
      "  oracle NLL       = 0.9044\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Semantic gradient — pairwise contrasts\n",
    "print(\"=\" * 70)\n",
    "print(\"SEMANTIC GRADIENT ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# --- Attention enrichment: does having prefix tokens matter? ---\n",
    "print(f\"\\n--- Attention enrichment effect ---\")\n",
    "print(f\"  no_prefix_posmatched vs repeat_token\")\n",
    "print(f\"  (same position offset, no prefix tokens vs Q uniform prefix tokens)\")\n",
    "diff_enrich = arrays['no_prefix_posmatched'] - arrays['repeat_token']\n",
    "d_enrich = cohens_d(diff_enrich)\n",
    "_, p_enrich = stats.ttest_1samp(diff_enrich, 0)\n",
    "sig_e = '***' if p_enrich < 0.001 else '**' if p_enrich < 0.01 else '*' if p_enrich < 0.05 else 'ns'\n",
    "print(f\"  d = {d_enrich:+.4f} ({sig_e}), p = {p_enrich:.2e}\")\n",
    "if d_enrich > 0.05:\n",
    "    print(f\"  -> Having prefix tokens to attend to HELPS (repeat_token < no_prefix)\")\n",
    "elif d_enrich < -0.05:\n",
    "    print(f\"  -> Having prefix tokens to attend to HURTS (repeat_token > no_prefix)\")\n",
    "else:\n",
    "    print(f\"  -> Attention enrichment is negligible\")\n",
    "\n",
    "# --- Semantic gradient: pairwise steps ---\n",
    "print(f\"\\n--- Semantic gradient (pairwise contrasts) ---\")\n",
    "print(f\"  Each step adds one dimension of semantic content.\")\n",
    "print(f\"  Positive d = later condition is better.\\n\")\n",
    "\n",
    "gradient_pairs = [\n",
    "    ('repeat_token', 'random_tokens', 'Embedding variation'),\n",
    "    ('random_tokens', 'unrelated_query', 'Natural language structure'),\n",
    "    ('unrelated_query', 'shuffled_query', 'Vocabulary match (right words)'),\n",
    "    ('shuffled_query', 'oracle', 'Word order / syntax'),\n",
    "]\n",
    "\n",
    "print(f\"  {'Step':<40} {'d':>8} {'p':>12} {'sig':>5}\")\n",
    "print(f\"  {'-'*70}\")\n",
    "for cond_a, cond_b, label in gradient_pairs:\n",
    "    diff = arrays[cond_a] - arrays[cond_b]  # positive = cond_b better\n",
    "    d = cohens_d(diff)\n",
    "    _, p = stats.ttest_1samp(diff, 0)\n",
    "    sig = '***' if p < 0.001 else '**' if p < 0.01 else '*' if p < 0.05 else 'ns'\n",
    "    print(f\"  {label:<40} {d:>+8.4f} {p:>12.2e} {sig:>5}\")\n",
    "\n",
    "# --- Doc keywords vs oracle ---\n",
    "print(f\"\\n--- Doc relevance vs query relevance ---\")\n",
    "diff_dq = arrays['doc_keywords'] - arrays['oracle']\n",
    "d_dq = cohens_d(diff_dq)\n",
    "_, p_dq = stats.ttest_1samp(diff_dq, 0)\n",
    "sig_dq = '***' if p_dq < 0.001 else '**' if p_dq < 0.01 else '*' if p_dq < 0.05 else 'ns'\n",
    "print(f\"  doc_keywords vs oracle: d = {d_dq:+.4f} ({sig_dq})\")\n",
    "if d_dq > 0.05:\n",
    "    print(f\"  -> Oracle is better: query-specific semantics matter\")\n",
    "elif d_dq < -0.05:\n",
    "    print(f\"  -> Doc keywords are better: doc-derived priming is stronger\")\n",
    "else:\n",
    "    print(f\"  -> No significant difference\")\n",
    "\n",
    "# --- Structure vs semantics ---\n",
    "print(f\"\\n--- Structure vs semantics ---\")\n",
    "diff_ss = arrays['best_structural'] - arrays['oracle']\n",
    "d_ss = cohens_d(diff_ss)\n",
    "_, p_ss = stats.ttest_1samp(diff_ss, 0)\n",
    "sig_ss = '***' if p_ss < 0.001 else '**' if p_ss < 0.01 else '*' if p_ss < 0.05 else 'ns'\n",
    "print(f\"  best_structural (prune_first_3) vs oracle: d = {d_ss:+.4f} ({sig_ss})\")\n",
    "if d_ss > 0.05:\n",
    "    print(f\"  -> Oracle beats best structural: semantic priming adds value\")\n",
    "elif d_ss < -0.05:\n",
    "    print(f\"  -> Best structural beats oracle: structure > semantics\")\n",
    "else:\n",
    "    print(f\"  -> No significant difference\")\n",
    "\n",
    "# --- Is there ANY semantic gradient at all? ---\n",
    "print(f\"\\n--- Overall semantic test ---\")\n",
    "print(f\"  ANOVA-like: do the 6 prefixed conditions differ significantly?\")\n",
    "prefixed_names = ['repeat_token', 'random_tokens', 'unrelated_query',\n",
    "                  'shuffled_query', 'doc_keywords', 'oracle']\n",
    "prefixed_arrays = [arrays[n] for n in prefixed_names]\n",
    "F_stat, p_anova = stats.f_oneway(*prefixed_arrays)\n",
    "sig_anova = '***' if p_anova < 0.001 else '**' if p_anova < 0.01 else '*' if p_anova < 0.05 else 'ns'\n",
    "print(f\"  F = {F_stat:.2f}, p = {p_anova:.2e} ({sig_anova})\")\n",
    "if p_anova < 0.05:\n",
    "    print(f\"  -> YES: prefix content significantly affects NLL\")\n",
    "else:\n",
    "    print(f\"  -> NO: prefix content does not significantly affect NLL\")\n",
    "    print(f\"  -> The structural effect is EVERYTHING; semantics are zero\")\n",
    "\n",
    "# --- Strongest single semantic contrast: repeat_token vs oracle ---\n",
    "print(f\"\\n--- Strongest semantic contrast: repeat_token vs oracle ---\")\n",
    "diff_max = arrays['repeat_token'] - arrays['oracle']\n",
    "d_max = cohens_d(diff_max)\n",
    "_, p_max = stats.ttest_1samp(diff_max, 0)\n",
    "sig_max = '***' if p_max < 0.001 else '**' if p_max < 0.01 else '*' if p_max < 0.05 else 'ns'\n",
    "print(f\"  d = {d_max:+.4f} ({sig_max}), p = {p_max:.2e}\")\n",
    "print(f\"  repeat_token NLL = {arrays['repeat_token'].mean():.4f}\")\n",
    "print(f\"  oracle NLL       = {arrays['oracle'].mean():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dbf564d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-20T15:53:55.748669Z",
     "iopub.status.busy": "2026-02-20T15:53:55.748400Z",
     "iopub.status.idle": "2026-02-20T15:53:56.346041Z",
     "shell.execute_reply": "2026-02-20T15:53:56.345014Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "VERDICT — Exp 05: Semantic Priming in Isolation\n",
      "======================================================================\n",
      "\n",
      "Model: google/gemma-3-4b-it\n",
      "N: 400 samples (MS MARCO v1.1)\n",
      "Mean query tokens: 6.5\n",
      "\n",
      "--- All conditions (ranked by d vs bare) ---\n",
      "  doc_keywords             NLL = 0.5865  d = +0.6209  (146% recovery)\n",
      "  random_tokens            NLL = 0.6631  d = +0.5724  (135% recovery)\n",
      "  repeat_token             NLL = 0.6796  d = +0.5688  (132% recovery)\n",
      "  unrelated_query          NLL = 0.7688  d = +0.5413  (119% recovery)\n",
      "  best_structural          NLL = 0.6658  d = +0.5356  (134% recovery)\n",
      "  shuffled_query           NLL = 0.8308  d = +0.5210  (111% recovery)\n",
      "  no_prefix_posmatched     NLL = 0.7259  d = +0.4632  (126% recovery)\n",
      "  oracle                   NLL = 0.9044  d = +0.4284  (100% recovery)\n",
      "  bare                     NLL = 1.6022  (baseline)\n",
      "\n",
      "--- Classification ---\n",
      "  SEMANTIC PRIMING IS NEGLIGIBLE\n",
      "  Oracle does NOT significantly outperform repeat_token\n",
      "  The entire benefit of prefix co-encoding is structural\n",
      "  (BOS removal + position offset + attention enrichment)\n",
      "\n",
      "Results saved to ../../../results/decoder_only/exp05/results.json\n",
      "\n",
      "Cleaning up GPU memory...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory: 8.61 GB -> 0.01 GB\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Verdict\n",
    "print(\"=\" * 70)\n",
    "print(\"VERDICT — Exp 05: Semantic Priming in Isolation\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\nModel: {MODEL_NAME}\")\n",
    "print(f\"N: {len(results)} samples (MS MARCO v1.1)\")\n",
    "print(f\"Mean query tokens: {np.mean([r['Q'] for r in results]):.1f}\")\n",
    "\n",
    "# All conditions ranked\n",
    "print(f\"\\n--- All conditions (ranked by d vs bare) ---\")\n",
    "all_ranked = sorted(analysis.items(),\n",
    "                    key=lambda x: x[1].get('d', -999), reverse=True)\n",
    "for name, info in all_ranked:\n",
    "    if name == 'bare':\n",
    "        print(f\"  {name:<24} NLL = {info['mean_nll']:.4f}  (baseline)\")\n",
    "    else:\n",
    "        print(f\"  {name:<24} NLL = {info['mean_nll']:.4f}  \"\n",
    "              f\"d = {info['d']:+.4f}  ({info['recovery']:.0f}% recovery)\")\n",
    "\n",
    "# Classification of result\n",
    "print(f\"\\n--- Classification ---\")\n",
    "d_repeat = analysis.get('repeat_token', {}).get('d', 0)\n",
    "d_oracle = analysis.get('oracle', {}).get('d', 0)\n",
    "d_random = analysis.get('random_tokens', {}).get('d', 0)\n",
    "d_shuffled = analysis.get('shuffled_query', {}).get('d', 0)\n",
    "\n",
    "# Test repeat vs oracle\n",
    "diff_ro = arrays['repeat_token'] - arrays['oracle']\n",
    "_, p_ro = stats.ttest_1samp(diff_ro, 0)\n",
    "\n",
    "if p_ro < 0.01 and d_oracle > d_repeat + 0.05:\n",
    "    print(f\"  SEMANTIC PRIMING IS REAL\")\n",
    "    print(f\"  Oracle significantly outperforms matched-structure repeat_token\")\n",
    "    print(f\"  Effect size of pure semantics: d = {cohens_d(diff_ro):+.4f}\")\n",
    "\n",
    "    # Where does the semantic benefit come from?\n",
    "    diff_rv = arrays['repeat_token'] - arrays['random_tokens']\n",
    "    diff_ru = arrays['random_tokens'] - arrays['unrelated_query']\n",
    "    diff_us = arrays['unrelated_query'] - arrays['shuffled_query']\n",
    "    diff_so = arrays['shuffled_query'] - arrays['oracle']\n",
    "\n",
    "    total_sem = diff_ro.mean()\n",
    "    print(f\"\\n  Semantic decomposition (total = {total_sem:+.4f}):\")\n",
    "    for label, diff_step in [\n",
    "        (\"Embedding variation (repeat→random)\", diff_rv),\n",
    "        (\"Natural language (random→unrelated)\", diff_ru),\n",
    "        (\"Vocabulary match (unrelated→shuffled)\", diff_us),\n",
    "        (\"Word order (shuffled→oracle)\", diff_so),\n",
    "    ]:\n",
    "        step_mean = diff_step.mean()\n",
    "        _, step_p = stats.ttest_1samp(diff_step, 0)\n",
    "        sig = '***' if step_p < 0.001 else '**' if step_p < 0.01 else '*' if step_p < 0.05 else 'ns'\n",
    "        pct = step_mean / total_sem * 100 if total_sem != 0 else 0\n",
    "        print(f\"    {label}: {step_mean:+.4f} ({pct:>5.1f}%) ({sig})\")\n",
    "else:\n",
    "    print(f\"  SEMANTIC PRIMING IS NEGLIGIBLE\")\n",
    "    print(f\"  Oracle does NOT significantly outperform repeat_token\")\n",
    "    print(f\"  The entire benefit of prefix co-encoding is structural\")\n",
    "    print(f\"  (BOS removal + position offset + attention enrichment)\")\n",
    "\n",
    "# Save\n",
    "final_results = {\n",
    "    'experiment': 'v4_decoder_only_exp05_semantic_isolation',\n",
    "    'model': MODEL_NAME,\n",
    "    'dataset': 'ms_marco_v1.1',\n",
    "    'n_samples': len(results),\n",
    "    'seed': SEED,\n",
    "    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'conditions': {k: v for k, v in analysis.items()},\n",
    "    'query_token_stats': {\n",
    "        'mean': float(np.mean([r['Q'] for r in results])),\n",
    "        'median': float(np.median([r['Q'] for r in results])),\n",
    "        'min': int(np.min([r['Q'] for r in results])),\n",
    "        'max': int(np.max([r['Q'] for r in results])),\n",
    "    },\n",
    "}\n",
    "\n",
    "with open(RESULTS_DIR / 'results.json', 'w') as f:\n",
    "    json.dump(final_results, f, indent=2)\n",
    "print(f\"\\nResults saved to {RESULTS_DIR / 'results.json'}\")\n",
    "\n",
    "# Cleanup\n",
    "print(f\"\\nCleaning up GPU memory...\")\n",
    "mem_before = torch.cuda.memory_allocated() / 1e9\n",
    "del model, tokenizer\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "mem_after = torch.cuda.memory_allocated() / 1e9\n",
    "print(f\"GPU memory: {mem_before:.2f} GB -> {mem_after:.2f} GB\")\n",
    "print(\"Done!\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "021ecf32d5ae45c5878b337ec89e068e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "05b7ba960932426c95a14f1d46089003": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "0ef90f70abe94965a3436e27b04771eb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "18b7a23e3bf64cf0b68e5aa2a012d1a3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "1a120b0160024d40a65757c9c47f5b1e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "247c71f2623541a3988a7af859e34a69": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_05b7ba960932426c95a14f1d46089003",
       "max": 883.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_3279797af29945b48926df9cdbec57e9",
       "tabbable": null,
       "tooltip": null,
       "value": 883.0
      }
     },
     "29ee414e27d04dc8afff45c154976b7f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_696f3d926a9249e892f8f726d5bcd049",
       "placeholder": "​",
       "style": "IPY_MODEL_0ef90f70abe94965a3436e27b04771eb",
       "tabbable": null,
       "tooltip": null,
       "value": "Loading weights: 100%"
      }
     },
     "3279797af29945b48926df9cdbec57e9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "3bb70defda6549aab2a16b32c6a4fe6c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_29ee414e27d04dc8afff45c154976b7f",
        "IPY_MODEL_247c71f2623541a3988a7af859e34a69",
        "IPY_MODEL_70c76dc03b134aafa09a40e0648f57b3"
       ],
       "layout": "IPY_MODEL_7272b8e83d5c483a97f4779fe6a3ea8a",
       "tabbable": null,
       "tooltip": null
      }
     },
     "473c0e130d7b498988c96462b1547681": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_cd54197c1b5d4d9c9c14f6713f685ce1",
       "placeholder": "​",
       "style": "IPY_MODEL_18b7a23e3bf64cf0b68e5aa2a012d1a3",
       "tabbable": null,
       "tooltip": null,
       "value": " 400/400 [11:55&lt;00:00,  1.80s/it]"
      }
     },
     "482ed5917daa428b97a2d57e546ed12a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_87074420c026456caf91afc999ec1778",
       "placeholder": "​",
       "style": "IPY_MODEL_021ecf32d5ae45c5878b337ec89e068e",
       "tabbable": null,
       "tooltip": null,
       "value": "Scoring: 100%"
      }
     },
     "500d58b88b0e418f8147b0ee0d5bdf4c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_d77a7dba25d54f9796575ad4eb14e120",
       "max": 400.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_80d7cd28347043b2bcafa6ad368d8535",
       "tabbable": null,
       "tooltip": null,
       "value": 400.0
      }
     },
     "68d796bf0c054977b03a0120342290ad": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_482ed5917daa428b97a2d57e546ed12a",
        "IPY_MODEL_500d58b88b0e418f8147b0ee0d5bdf4c",
        "IPY_MODEL_473c0e130d7b498988c96462b1547681"
       ],
       "layout": "IPY_MODEL_bbafa13448ea4184a00cf6dbbd7203ca",
       "tabbable": null,
       "tooltip": null
      }
     },
     "696f3d926a9249e892f8f726d5bcd049": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "70c76dc03b134aafa09a40e0648f57b3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_ed360586855c4aacb8324e343f61eff6",
       "placeholder": "​",
       "style": "IPY_MODEL_1a120b0160024d40a65757c9c47f5b1e",
       "tabbable": null,
       "tooltip": null,
       "value": " 883/883 [00:02&lt;00:00, 550.66it/s, Materializing param=model.vision_tower.vision_model.post_layernorm.weight]"
      }
     },
     "7272b8e83d5c483a97f4779fe6a3ea8a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "80d7cd28347043b2bcafa6ad368d8535": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "87074420c026456caf91afc999ec1778": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "bbafa13448ea4184a00cf6dbbd7203ca": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "cd54197c1b5d4d9c9c14f6713f685ce1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d77a7dba25d54f9796575ad4eb14e120": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ed360586855c4aacb8324e343f61eff6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
