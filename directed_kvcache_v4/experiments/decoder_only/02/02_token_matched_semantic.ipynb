{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aea0611d",
   "metadata": {},
   "source": [
    "# Experiment 02: Token-Matched Semantic Probing with LLM Surrogates\n",
    "\n",
    "## Motivation\n",
    "\n",
    "Exp 01 rerun (Gemma 3 12B-IT, N=400, BOS-retained repositioning) established that\n",
    "the semantic effect is real but **reversed from expectations**: oracle conditioning\n",
    "HURTS (d=-0.151), while the data-extraction task-framing prefix HELPS (d=+0.264).\n",
    "However, Exp 01 has a **prefix-length confound** — different conditions have different\n",
    "token counts (P=14-20), producing different RoPE repositioning deltas.\n",
    "\n",
    "Exps 02-05 were invalidated by the 1-token look-ahead bug. Exps 06-07 used\n",
    "`slice_kv_cache` without BOS retention on a different model (4B-IT) and are not\n",
    "directly comparable.\n",
    "\n",
    "## Goal\n",
    "\n",
    "Design and run a definitive experiment that:\n",
    "1. Eliminates all structural confounds via token-level prefix matching\n",
    "2. Spans the full semantic gradient\n",
    "3. Introduces LLM-generated document-specific surrogates\n",
    "4. Deeply probes where conditioning helps vs hurts\n",
    "\n",
    "## Method — BOS-Retained Repositioning with Token-Level Matching\n",
    "\n",
    "**Phase A:** `[BOS] + prefix_ids(Q) + [\\n] + doc_ids(D)` at natural positions.\n",
    "Select BOS + doc from cache (skip prefix + newline).\n",
    "Reposition doc keys from `[1+Q+NL, ..., Q+NL+D]` to `[1, ..., D]`.\n",
    "Cache has `1+D` entries (BOS at 0, doc at 1..D).\n",
    "\n",
    "**Phase B:** `[\\n + query + \\n + answer]` at positions `[D+1, ...]`.\n",
    "`cache_position` auto-generated from cache length (= 1+D = D+1). No look-ahead.\n",
    "\n",
    "**Key invariant:** For ALL prefixed conditions, repositioning delta = -(Q+NL) is\n",
    "**identical** per sample. Every condition uses exactly Q prefix token IDs.\n",
    "\n",
    "## Conditions (13 total)\n",
    "\n",
    "| # | Key | Source | Semantic relevance | Token construction |\n",
    "|---|-----|--------|-------------------|--------------------|\n",
    "| 1 | `bare` | — | baseline | No prefix |\n",
    "| 2 | `random_tokens` | Random vocab IDs | none | Q random IDs from vocab |\n",
    "| 3 | `repeat_token` | Single token x Q | none (structural) | Token ID 1000 repeated Q times |\n",
    "| 4 | `scrambled_oracle` | Shuffled query | vocab match only | Random permutation of oracle IDs |\n",
    "| 5 | `unrelated_query` | Other sample's query | low | Sample (i+N/2)%N query, pad/trunc to Q |\n",
    "| 6 | `same_topic` | LLM-generated | medium | \"Write a question about same topic...\" pad/trunc to Q |\n",
    "| 7 | `paraphrase` | LLM-generated | high | \"Rephrase this query differently...\" pad/trunc to Q |\n",
    "| 8 | `oracle` | Real query | maximal | Exact query token IDs (already Q) |\n",
    "| 9 | `llm_extract` | LLM doc-specific | task-framing (doc) | \"List key facts from this document\" pad/trunc to Q |\n",
    "| 10 | `llm_question` | LLM doc-specific | query-like (doc) | \"What question does this doc answer?\" pad/trunc to Q |\n",
    "| 11 | `llm_summarize` | LLM doc-specific | summary (doc) | \"Summarize in one sentence\" pad/trunc to Q |\n",
    "| 12 | `extractor_matched` | Fixed text | task-framing (generic) | \"Extract:\" text tokenized, pad/trunc to Q |\n",
    "| 13 | `adversarial_matched` | Fixed text | adversarial | Adversarial text tokenized, pad/trunc to Q |\n",
    "\n",
    "## Key analyses\n",
    "\n",
    "1. **Semantic gradient test**: Spearman rho of relevance rank vs delta_NLL\n",
    "2. **Structural decomposition**: delta(random_tokens) / delta(oracle)\n",
    "3. **LLM doc-specific vs generic**: paired test llm_extract vs extractor_matched\n",
    "4. **Hardness interaction**: 5 quintile bins x 13 conditions\n",
    "5. **Per-sample ranking**: which condition gives lowest NLL per sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c85fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Setup and model loading\n",
    "import os\n",
    "os.umask(0o000)\n",
    "\n",
    "import sys, json, time, gc, re\n",
    "import random as pyrandom\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from scipy import stats\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "sys.path.insert(0, \"../../..\")\n",
    "from lib.analysis import cohens_d\n",
    "\n",
    "SEED = 42\n",
    "N_SAMPLES = 400\n",
    "MODEL_NAME = \"google/gemma-3-12b-it\"\n",
    "\n",
    "RESULTS_DIR = Path(\"../../../results/decoder_only/exp02\")\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "SURROGATES_PATH = RESULTS_DIR / \"surrogates.json\"\n",
    "CHECKPOINT_PATH = RESULTS_DIR / \"checkpoint.json\"\n",
    "\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "pyrandom.seed(SEED)\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv())\n",
    "HF_TOKEN = os.environ.get(\"HF_TOKEN\")\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, DynamicCache\n",
    "\n",
    "print(f\"Loading {MODEL_NAME}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, token=HF_TOKEN)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME, device_map=\"auto\", dtype=torch.bfloat16, token=HF_TOKEN,\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "DEVICE = next(model.parameters()).device\n",
    "BOS_ID = tokenizer.bos_token_id\n",
    "NEWLINE_IDS = tokenizer(\"\\n\", add_special_tokens=False).input_ids\n",
    "text_cfg = getattr(model.config, 'text_config', model.config)\n",
    "VOCAB_SIZE = getattr(text_cfg, 'vocab_size', 262208)\n",
    "\n",
    "print(f\"Exp 02: Token-Matched Semantic Probing with LLM Surrogates\")\n",
    "print(f\"Scoring: BOS-retained repositioning (look-ahead fix)\")\n",
    "print(f\"N: {N_SAMPLES}, Model: {MODEL_NAME}\")\n",
    "print(f\"DEVICE: {DEVICE}, dtype: {next(model.parameters()).dtype}\")\n",
    "print(f\"GPU memory: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "print(f\"Vocab size: {VOCAB_SIZE}\")\n",
    "print(f\"Num layers: {getattr(text_cfg, 'num_hidden_layers', 'N/A')}\")\n",
    "print(f\"Num KV heads: {getattr(text_cfg, 'num_key_value_heads', 'N/A')}\")\n",
    "rope_params = getattr(text_cfg, 'rope_parameters', {})\n",
    "layer_types_list = getattr(text_cfg, 'layer_types', [])\n",
    "print(f\"Layer types: {set(layer_types_list)} ({len(layer_types_list)} layers)\")\n",
    "for ltype, params in rope_params.items():\n",
    "    print(f\"  {ltype}: theta={params.get('rope_theta')}, \"\n",
    "          f\"type={params.get('rope_type')}, factor={params.get('factor', 'N/A')}\")\n",
    "n_global = sum(1 for t in layer_types_list if t == 'full_attention')\n",
    "print(f\"  Global layers: {n_global}/{len(layer_types_list)} \"\n",
    "      f\"(indices: {[i for i, t in enumerate(layer_types_list) if t == 'full_attention']})\")\n",
    "\n",
    "# Load MS MARCO\n",
    "from lib.data import count_words\n",
    "from datasets import load_dataset\n",
    "\n",
    "print(\"\\nLoading MS MARCO v1.1 validation...\")\n",
    "ds = load_dataset(\"microsoft/ms_marco\", \"v1.1\", split=\"validation\")\n",
    "\n",
    "all_candidates = []\n",
    "for item in ds:\n",
    "    if len(all_candidates) >= 3 * N_SAMPLES:\n",
    "        break\n",
    "    passages = item.get('passages', {})\n",
    "    ptexts = passages.get('passage_text', [])\n",
    "    is_sel = passages.get('is_selected', [])\n",
    "    query = item.get('query', '')\n",
    "    answers = item.get('answers', [])\n",
    "    well_formed = item.get('wellFormedAnswers', [])\n",
    "    answer = None\n",
    "    if well_formed and len(well_formed) > 0 and well_formed[0] not in ('[]', ''):\n",
    "        answer = well_formed[0]\n",
    "    elif answers and len(answers) > 0 and answers[0] != 'No Answer Present.':\n",
    "        answer = answers[0]\n",
    "    if not answer:\n",
    "        continue\n",
    "    for pt, sel in zip(ptexts, is_sel):\n",
    "        wc = count_words(pt)\n",
    "        if sel == 1 and 30 <= wc <= 300:\n",
    "            all_candidates.append({\n",
    "                'passage': pt, 'query': query, 'answer': answer,\n",
    "                'word_count': wc,\n",
    "            })\n",
    "            break\n",
    "\n",
    "print(f\"Total candidates: {len(all_candidates)}\")\n",
    "np.random.seed(SEED)\n",
    "indices = np.random.permutation(len(all_candidates))\n",
    "samples = [all_candidates[i] for i in indices[:N_SAMPLES]]\n",
    "del ds, all_candidates\n",
    "gc.collect()\n",
    "\n",
    "print(f\"Loaded {len(samples)} samples\")\n",
    "print(f\"Mean passage words: {np.mean([s['word_count'] for s in samples]):.0f}\")\n",
    "print(f\"Mean answer words: {np.mean([count_words(s['answer']) for s in samples]):.0f}\")\n",
    "print(f\"Mean query words: {np.mean([count_words(s['query']) for s in samples]):.0f}\")\n",
    "print(f\"\\nFirst sample:\")\n",
    "print(f\"  Query:  {samples[0]['query'][:70]}...\")\n",
    "print(f\"  Answer: {samples[0]['answer'][:70]}...\")\n",
    "print(f\"  Passage ({samples[0]['word_count']}w): {samples[0]['passage'][:70]}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35fff73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Generate LLM surrogates (5 per sample)\n",
    "\n",
    "PROMPT_PARAPHRASE = (\n",
    "    \"Rephrase this search query using completely different words but keeping \"\n",
    "    \"the same meaning. Keep it to 5-8 words. Output only the rephrased query.\"\n",
    ")\n",
    "PROMPT_SAME_TOPIC = (\n",
    "    \"Write a question about the same topic as this document but asking for \"\n",
    "    \"DIFFERENT information. Keep it to 5-8 words. Output only the question.\"\n",
    ")\n",
    "PROMPT_EXTRACT = (\n",
    "    \"List the key facts from this document as a brief comma-separated list. \"\n",
    "    \"Output only the fact list, nothing else.\"\n",
    ")\n",
    "PROMPT_QUESTION = (\n",
    "    \"What question does this document answer? Write only the question, \"\n",
    "    \"nothing else. Keep it to 5-10 words.\"\n",
    ")\n",
    "PROMPT_SUMMARIZE = (\n",
    "    \"Summarize this document in one sentence. Output only the summary, nothing else.\"\n",
    ")\n",
    "\n",
    "def generate_text(input_text, prompt_text, max_new_tokens=50):\n",
    "    # Generate text from a prompt + input using Gemma IT chat template.\n",
    "    messages = [\n",
    "        {\"role\": \"user\",\n",
    "         \"content\": f\"{prompt_text}\\n\\n{input_text}\"}\n",
    "    ]\n",
    "    chat_text = tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    inputs = tokenizer(chat_text, return_tensors=\"pt\",\n",
    "                       truncation=True, max_length=1024).to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "        )\n",
    "\n",
    "    new_tokens = output_ids[0, inputs['input_ids'].shape[1]:]\n",
    "    raw_text = tokenizer.decode(new_tokens, skip_special_tokens=True)\n",
    "\n",
    "    # Post-process: strip, take first line, remove quotes, truncate to 20 words\n",
    "    cleaned = raw_text.strip().split(\"\\n\")[0].strip()\n",
    "    cleaned = cleaned.strip('\"').strip(\"'\").strip()\n",
    "    cleaned = \" \".join(cleaned.split()[:20])\n",
    "    return cleaned\n",
    "\n",
    "\n",
    "if SURROGATES_PATH.exists():\n",
    "    print(\"Loading cached surrogates...\")\n",
    "    surrogates = json.loads(SURROGATES_PATH.read_text())\n",
    "    assert len(surrogates) == N_SAMPLES, f\"Expected {N_SAMPLES}, got {len(surrogates)}\"\n",
    "    for i in range(min(10, N_SAMPLES)):\n",
    "        assert surrogates[i]['query'][:50] == samples[i]['query'][:50], \\\n",
    "            f\"Sample {i} query mismatch\"\n",
    "    print(f\"Loaded {len(surrogates)} cached surrogates\")\n",
    "    print(f\"Keys per sample: {list(surrogates[0].keys())}\")\n",
    "else:\n",
    "    # Generate with checkpointing\n",
    "    surrogates = []\n",
    "    gen_ckpt_path = RESULTS_DIR / \"gen_checkpoint.json\"\n",
    "\n",
    "    if gen_ckpt_path.exists():\n",
    "        gen_ckpt = json.loads(gen_ckpt_path.read_text())\n",
    "        if gen_ckpt.get('n_total') == N_SAMPLES:\n",
    "            surrogates = gen_ckpt['surrogates']\n",
    "            print(f\"Resuming generation from {len(surrogates)}/{N_SAMPLES}\")\n",
    "\n",
    "    start_gen = len(surrogates)\n",
    "    t0 = time.time()\n",
    "\n",
    "    for i in tqdm(range(start_gen, N_SAMPLES), initial=start_gen, total=N_SAMPLES,\n",
    "                  desc=\"Generating surrogates\"):\n",
    "        s = samples[i]\n",
    "        entry = {'query': s['query']}\n",
    "\n",
    "        # First 200 words of passage for doc-specific prompts\n",
    "        doc_words = s['passage'].split()[:200]\n",
    "        doc_input = f\"Document:\\n{' '.join(doc_words)}\"\n",
    "\n",
    "        # 1. Paraphrase: rephrase the query\n",
    "        torch.manual_seed(SEED + i * 10)\n",
    "        entry['paraphrase'] = generate_text(\n",
    "            f\"Query: {s['query']}\", PROMPT_PARAPHRASE\n",
    "        )\n",
    "\n",
    "        # 2. Same-topic: question about same topic but different info\n",
    "        torch.manual_seed(SEED + i * 10 + 1)\n",
    "        entry['same_topic'] = generate_text(doc_input, PROMPT_SAME_TOPIC)\n",
    "\n",
    "        # 3. LLM extract: key facts from document\n",
    "        torch.manual_seed(SEED + i * 10 + 2)\n",
    "        entry['llm_extract'] = generate_text(doc_input, PROMPT_EXTRACT)\n",
    "\n",
    "        # 4. LLM question: what question does the doc answer?\n",
    "        torch.manual_seed(SEED + i * 10 + 3)\n",
    "        entry['llm_question'] = generate_text(doc_input, PROMPT_QUESTION)\n",
    "\n",
    "        # 5. LLM summarize: one-sentence summary\n",
    "        torch.manual_seed(SEED + i * 10 + 4)\n",
    "        entry['llm_summarize'] = generate_text(doc_input, PROMPT_SUMMARIZE)\n",
    "\n",
    "        surrogates.append(entry)\n",
    "\n",
    "        if (i + 1) % 20 == 0 or i == N_SAMPLES - 1:\n",
    "            gen_ckpt = {'n_total': N_SAMPLES, 'surrogates': surrogates,\n",
    "                        'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')}\n",
    "            gen_ckpt_path.write_text(json.dumps(gen_ckpt))\n",
    "            elapsed = time.time() - t0\n",
    "            done = i - start_gen + 1\n",
    "            eta = (N_SAMPLES - i - 1) * elapsed / done if done > 0 else 0\n",
    "            tqdm.write(f\"  Gen checkpoint {i+1}/{N_SAMPLES} | \"\n",
    "                       f\"{elapsed/60:.1f}m | ETA {eta/60:.1f}m\")\n",
    "\n",
    "    elapsed = time.time() - t0\n",
    "    print(f\"\\nGeneration complete: {len(surrogates)} samples in {elapsed/60:.1f} min\")\n",
    "\n",
    "    # Save final surrogates\n",
    "    SURROGATES_PATH.write_text(json.dumps(surrogates, indent=2))\n",
    "    print(f\"Saved surrogates to {SURROGATES_PATH}\")\n",
    "\n",
    "# Show examples\n",
    "for i in range(3):\n",
    "    s = surrogates[i]\n",
    "    print(f\"\\nSample {i}: query='{s['query'][:60]}'\")\n",
    "    for key in ['paraphrase', 'same_topic', 'llm_extract', 'llm_question', 'llm_summarize']:\n",
    "        print(f\"  {key:<15}: {s.get(key, 'N/A')[:60]}\")\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb1c886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Build per-sample token-level prefix IDs (13 conditions)\n",
    "\n",
    "# Fixed-text prefixes\n",
    "EXTRACTOR_TEXT = \"Extract all key data points, facts, entities, and specific attributes from the following text.\"\n",
    "ADVERSARIAL_TEXT = \"The recipe calls for two cups of flour, one cup of sugar, and a pinch of salt mixed together.\"\n",
    "\n",
    "def make_prefix(token_ids, Q):\n",
    "    # Pad or truncate token_ids to exactly Q tokens.\n",
    "    if len(token_ids) >= Q:\n",
    "        return token_ids[:Q]\n",
    "    else:\n",
    "        padded = token_ids * ((Q // max(len(token_ids), 1)) + 1)\n",
    "        return padded[:Q]\n",
    "\n",
    "\n",
    "pyrandom.seed(SEED + 200)\n",
    "np.random.seed(SEED + 300)\n",
    "\n",
    "# Pre-tokenize fixed texts\n",
    "extractor_ids = tokenizer(EXTRACTOR_TEXT, add_special_tokens=False).input_ids\n",
    "adversarial_ids = tokenizer(ADVERSARIAL_TEXT, add_special_tokens=False).input_ids\n",
    "\n",
    "# Special token IDs to exclude from random sampling\n",
    "special_ids = set(tokenizer.all_special_ids)\n",
    "\n",
    "for i, s in enumerate(samples):\n",
    "    surr = surrogates[i]\n",
    "    q_ids = tokenizer(s['query'], add_special_tokens=False).input_ids\n",
    "    Q = len(q_ids)\n",
    "    s['Q'] = Q\n",
    "\n",
    "    # 1. oracle: exact query token IDs (already Q tokens)\n",
    "    s['prefix_oracle'] = q_ids\n",
    "\n",
    "    # 2. random_tokens: random vocab IDs (excluding special tokens)\n",
    "    rand_ids = []\n",
    "    while len(rand_ids) < Q:\n",
    "        tid = np.random.randint(0, VOCAB_SIZE)\n",
    "        if tid not in special_ids:\n",
    "            rand_ids.append(int(tid))\n",
    "    s['prefix_random_tokens'] = rand_ids[:Q]\n",
    "\n",
    "    # 3. repeat_token: single token repeated Q times\n",
    "    s['prefix_repeat_token'] = [1000] * Q\n",
    "\n",
    "    # 4. scrambled_oracle: random permutation of query IDs\n",
    "    shuffled = list(q_ids)\n",
    "    pyrandom.shuffle(shuffled)\n",
    "    s['prefix_scrambled_oracle'] = shuffled\n",
    "\n",
    "    # 5. unrelated_query: other sample's query, pad/trunc to Q\n",
    "    other_idx = (i + N_SAMPLES // 2) % len(samples)\n",
    "    other_q_ids = tokenizer(samples[other_idx]['query'],\n",
    "                            add_special_tokens=False).input_ids\n",
    "    s['prefix_unrelated_query'] = make_prefix(other_q_ids, Q)\n",
    "\n",
    "    # 6. same_topic: LLM-generated, pad/trunc to Q\n",
    "    topic_ids = tokenizer(surr['same_topic'], add_special_tokens=False).input_ids\n",
    "    s['prefix_same_topic'] = make_prefix(topic_ids, Q)\n",
    "\n",
    "    # 7. paraphrase: LLM-generated, pad/trunc to Q\n",
    "    para_ids = tokenizer(surr['paraphrase'], add_special_tokens=False).input_ids\n",
    "    s['prefix_paraphrase'] = make_prefix(para_ids, Q)\n",
    "\n",
    "    # 8. llm_extract: LLM doc-specific fact list, pad/trunc to Q\n",
    "    extract_ids = tokenizer(surr['llm_extract'], add_special_tokens=False).input_ids\n",
    "    s['prefix_llm_extract'] = make_prefix(extract_ids, Q)\n",
    "\n",
    "    # 9. llm_question: LLM doc-specific question, pad/trunc to Q\n",
    "    question_ids = tokenizer(surr['llm_question'], add_special_tokens=False).input_ids\n",
    "    s['prefix_llm_question'] = make_prefix(question_ids, Q)\n",
    "\n",
    "    # 10. llm_summarize: LLM doc-specific summary, pad/trunc to Q\n",
    "    summarize_ids = tokenizer(surr['llm_summarize'], add_special_tokens=False).input_ids\n",
    "    s['prefix_llm_summarize'] = make_prefix(summarize_ids, Q)\n",
    "\n",
    "    # 11. extractor_matched: fixed extraction text, pad/trunc to Q\n",
    "    s['prefix_extractor_matched'] = make_prefix(extractor_ids, Q)\n",
    "\n",
    "    # 12. adversarial_matched: fixed adversarial text, pad/trunc to Q\n",
    "    s['prefix_adversarial_matched'] = make_prefix(adversarial_ids, Q)\n",
    "\n",
    "# Verify all prefixes have exactly Q tokens\n",
    "PREFIX_KEYS = [\n",
    "    'prefix_oracle', 'prefix_random_tokens', 'prefix_repeat_token',\n",
    "    'prefix_scrambled_oracle', 'prefix_unrelated_query', 'prefix_same_topic',\n",
    "    'prefix_paraphrase', 'prefix_llm_extract', 'prefix_llm_question',\n",
    "    'prefix_llm_summarize', 'prefix_extractor_matched', 'prefix_adversarial_matched',\n",
    "]\n",
    "\n",
    "q_lens = [s['Q'] for s in samples]\n",
    "print(f\"Query token count — mean: {np.mean(q_lens):.1f}, \"\n",
    "      f\"median: {np.median(q_lens):.0f}, \"\n",
    "      f\"min: {np.min(q_lens)}, max: {np.max(q_lens)}\")\n",
    "\n",
    "errors = 0\n",
    "for i, s in enumerate(samples):\n",
    "    Q = s['Q']\n",
    "    for key in PREFIX_KEYS:\n",
    "        if len(s[key]) != Q:\n",
    "            print(f\"  ERROR: Sample {i} {key}: len={len(s[key])} != Q={Q}\")\n",
    "            errors += 1\n",
    "assert errors == 0, f\"{errors} prefix length mismatches!\"\n",
    "\n",
    "# Show examples\n",
    "for i in range(3):\n",
    "    Q = samples[i]['Q']\n",
    "    print(f\"\\nSample {i}: Q={Q}, query='{samples[i]['query'][:50]}...'\")\n",
    "    for key in PREFIX_KEYS:\n",
    "        label = key.replace('prefix_', '')\n",
    "        decoded = tokenizer.decode(samples[i][key][:8])\n",
    "        print(f\"  {label:<22}: {decoded}...\")\n",
    "\n",
    "print(f\"\\nAll {len(PREFIX_KEYS)} prefix types verified for {len(samples)} samples.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736c8a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Scoring functions with BOS-retained repositioning + validation\n",
    "\n",
    "# --- RoPE repositioning helpers ---\n",
    "layer_types = getattr(text_cfg, 'layer_types', [])\n",
    "\n",
    "def build_layer_inv_freqs():\n",
    "    # Build per-layer-type inverse frequency tensors for RoPE rotation.\n",
    "    inv_freqs = {}\n",
    "    for lt, params in rope_params.items():\n",
    "        theta = params.get('rope_theta', 10000.0)\n",
    "        dim = text_cfg.head_dim\n",
    "        inv_freq = 1.0 / (theta ** (torch.arange(0, dim, 2, dtype=torch.float32, device=DEVICE) / dim))\n",
    "        inv_freqs[lt] = inv_freq\n",
    "    return inv_freqs\n",
    "\n",
    "LAYER_INV_FREQS = build_layer_inv_freqs()\n",
    "\n",
    "\n",
    "def rotate_half(x):\n",
    "    x1, x2 = x[..., :x.shape[-1]//2], x[..., x.shape[-1]//2:]\n",
    "    return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "\n",
    "def select_kv_cache(cache, indices):\n",
    "    # Select specific cache indices (e.g., BOS + doc, skipping prefix).\n",
    "    selected = DynamicCache()\n",
    "    idx_tensor = torch.tensor(indices, dtype=torch.long, device=DEVICE)\n",
    "    for i in range(len(cache.layers)):\n",
    "        k = cache.layers[i].keys[:, :, idx_tensor, :]\n",
    "        v = cache.layers[i].values[:, :, idx_tensor, :]\n",
    "        selected.update(k, v, i)\n",
    "    return selected\n",
    "\n",
    "\n",
    "def reposition_kv_cache(cache, old_positions, new_positions, bos_start=0):\n",
    "    # Reposition doc keys from old_positions to new_positions via RoPE rotation.\n",
    "    # BOS entry at bos_start is left untouched. Doc entries start at bos_start+1.\n",
    "    delta = new_positions - old_positions\n",
    "    for L in range(len(cache.layers)):\n",
    "        lt = layer_types[L]\n",
    "        inv_freq = LAYER_INV_FREQS[lt]\n",
    "        k = cache.layers[L].keys\n",
    "        doc_keys = k[:, :, bos_start + 1:, :]\n",
    "        freqs = torch.einsum('i,j->ij', delta.float(), inv_freq)\n",
    "        emb = torch.cat([freqs, freqs], dim=-1)\n",
    "        cos_delta = emb.cos().to(k.dtype).unsqueeze(0).unsqueeze(0)\n",
    "        sin_delta = emb.sin().to(k.dtype).unsqueeze(0).unsqueeze(0)\n",
    "        doc_keys_new = doc_keys * cos_delta + rotate_half(doc_keys) * sin_delta\n",
    "        cache.layers[L].keys = torch.cat([\n",
    "            k[:, :, :bos_start + 1, :],\n",
    "            doc_keys_new,\n",
    "        ], dim=2)\n",
    "    return cache\n",
    "\n",
    "\n",
    "def score(doc_text, query_text, answer_text, prefix_token_ids=None):\n",
    "    # BOS-retained repositioning.\n",
    "    #\n",
    "    # If prefix_token_ids provided:\n",
    "    #   Phase A: [BOS] + prefix_ids(Q) + [\\n] + doc_ids(D) at natural positions.\n",
    "    #   Select BOS(0) + doc(1+Q+NL .. end) from cache.\n",
    "    #   Reposition doc keys from [1+Q+NL, ..., Q+NL+D] to [1, ..., D].\n",
    "    #   Cache: 1+D entries (BOS at 0, doc at 1..D).\n",
    "    #\n",
    "    # Bare: [BOS + doc] with default positions. Cache: 1+D entries.\n",
    "    #\n",
    "    # Phase B: score [\\n + query + \\n + answer] at positions [D+1, ...]\n",
    "    #   cache_position auto-generated from cache length (= 1+D = D+1).\n",
    "\n",
    "    doc_ids = tokenizer(doc_text, add_special_tokens=False,\n",
    "                        truncation=True, max_length=1536).input_ids\n",
    "    D = len(doc_ids)\n",
    "\n",
    "    if prefix_token_ids is not None:\n",
    "        P = len(prefix_token_ids)\n",
    "        NL = len(NEWLINE_IDS)\n",
    "\n",
    "        cond_ids = [BOS_ID] + list(prefix_token_ids) + NEWLINE_IDS + doc_ids\n",
    "        with torch.no_grad():\n",
    "            pa = model(input_ids=torch.tensor([cond_ids], device=DEVICE),\n",
    "                       use_cache=True)\n",
    "        cache = pa.past_key_values\n",
    "        del pa\n",
    "\n",
    "        # Select BOS (index 0) + doc (indices 1+P+NL .. end)\n",
    "        keep_indices = [0] + list(range(1 + P + NL, len(cond_ids)))\n",
    "        cache = select_kv_cache(cache, keep_indices)\n",
    "\n",
    "        # Reposition doc keys from natural positions to bare positions\n",
    "        old_pos = torch.arange(1 + P + NL, 1 + P + NL + D, device=DEVICE)\n",
    "        new_pos = torch.arange(1, D + 1, device=DEVICE)\n",
    "        cache = reposition_kv_cache(cache, old_pos, new_pos, bos_start=0)\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            pa = model(input_ids=torch.tensor([[BOS_ID] + doc_ids], device=DEVICE),\n",
    "                       use_cache=True)\n",
    "        cache = pa.past_key_values\n",
    "        del pa\n",
    "\n",
    "    # Cache has 1+D entries. Phase B at D+1.\n",
    "    phase_b_start = D + 1\n",
    "\n",
    "    query_ids = tokenizer(\"\\n\" + query_text + \"\\n\",\n",
    "                          add_special_tokens=False).input_ids\n",
    "    answer_ids = tokenizer(answer_text, add_special_tokens=False,\n",
    "                           truncation=True, max_length=256).input_ids\n",
    "    if not answer_ids:\n",
    "        del cache\n",
    "        return 0.0\n",
    "\n",
    "    pb_ids = query_ids + answer_ids\n",
    "    pos = torch.arange(phase_b_start, phase_b_start + len(pb_ids), device=DEVICE)\n",
    "\n",
    "    # Phase B: NO explicit cache_position — auto-generated from cache length\n",
    "    with torch.no_grad():\n",
    "        pb = model(\n",
    "            input_ids=torch.tensor([pb_ids], device=DEVICE),\n",
    "            past_key_values=cache,\n",
    "            position_ids=pos.unsqueeze(0),\n",
    "            use_cache=False,\n",
    "        )\n",
    "\n",
    "    n_q = len(query_ids)\n",
    "    logits = pb.logits[0, n_q - 1:n_q - 1 + len(answer_ids), :].float()\n",
    "    targets = torch.tensor(answer_ids, device=DEVICE)\n",
    "    nll = -F.log_softmax(logits, dim=-1).gather(\n",
    "        1, targets.unsqueeze(1)).squeeze(1).mean().item()\n",
    "    del cache, pb\n",
    "    return nll\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# VALIDATION TESTS\n",
    "# ================================================================\n",
    "print(\"=\" * 70)\n",
    "print(\"VALIDATION: BOS-Retained Repositioning with Token-Level Matching\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# TEST 1: Bare two-phase matches single-pass\n",
    "print(\"\\n--- Test 1: Bare two-phase matches single-pass ---\")\n",
    "doc_text_t = \"The cat sat on the mat near the door of the house by the lake\"\n",
    "query_text_t = \"Where did the cat sit?\"\n",
    "answer_text_t = \"on the mat\"\n",
    "doc_ids_t = tokenizer(doc_text_t, add_special_tokens=False).input_ids\n",
    "D_t = len(doc_ids_t)\n",
    "query_ids_t = tokenizer(\"\\n\" + query_text_t + \"\\n\", add_special_tokens=False).input_ids\n",
    "answer_ids_t = tokenizer(answer_text_t, add_special_tokens=False).input_ids\n",
    "\n",
    "# Single-pass reference\n",
    "full_ids = [BOS_ID] + doc_ids_t + query_ids_t + answer_ids_t\n",
    "with torch.no_grad():\n",
    "    out_full = model(input_ids=torch.tensor([full_ids], device=DEVICE))\n",
    "n_ctx = 1 + D_t + len(query_ids_t)\n",
    "logits_full = out_full.logits[0, n_ctx - 1:n_ctx - 1 + len(answer_ids_t), :].float()\n",
    "targets_t = torch.tensor(answer_ids_t, device=DEVICE)\n",
    "nll_single = -F.log_softmax(logits_full, dim=-1).gather(\n",
    "    1, targets_t.unsqueeze(1)).squeeze(1).mean().item()\n",
    "del out_full\n",
    "\n",
    "# Two-phase bare\n",
    "nll_bare = score(doc_text_t, query_text_t, answer_text_t)\n",
    "\n",
    "diff_pct = abs(nll_single - nll_bare) / nll_single * 100\n",
    "print(f\"  Single-pass NLL: {nll_single:.6f}\")\n",
    "print(f\"  Two-phase bare:  {nll_bare:.6f} (diff: {diff_pct:.2f}%)\")\n",
    "assert diff_pct < 1.0, f\"Bare doesn't match single-pass: {diff_pct}%\"\n",
    "print(f\"  PASSED — bare matches single-pass within {diff_pct:.2f}%\")\n",
    "\n",
    "# TEST 2: Prefixed scoring runs without error\n",
    "print(\"\\n--- Test 2: Prefixed scoring runs correctly ---\")\n",
    "s = samples[0]\n",
    "nll_b = score(s['passage'], s['query'], s['answer'])\n",
    "nll_o = score(s['passage'], s['query'], s['answer'],\n",
    "              prefix_token_ids=s['prefix_oracle'])\n",
    "nll_r = score(s['passage'], s['query'], s['answer'],\n",
    "              prefix_token_ids=s['prefix_random_tokens'])\n",
    "print(f\"  Bare:          {nll_b:.4f}\")\n",
    "print(f\"  Oracle:        {nll_o:.4f}  delta={nll_b - nll_o:+.4f}\")\n",
    "print(f\"  Random tokens: {nll_r:.4f}  delta={nll_b - nll_r:+.4f}\")\n",
    "assert 0 < nll_b < 20, f\"Bare NLL out of range: {nll_b}\"\n",
    "assert 0 < nll_o < 20, f\"Oracle NLL out of range: {nll_o}\"\n",
    "assert 0 < nll_r < 20, f\"Random NLL out of range: {nll_r}\"\n",
    "print(\"  PASSED — all NLLs in valid range\")\n",
    "\n",
    "# TEST 3: Token-matching invariant (all prefixed conds have same Q)\n",
    "print(\"\\n--- Test 3: Token-matching invariant ---\")\n",
    "Q = s['Q']\n",
    "for key in PREFIX_KEYS:\n",
    "    assert len(s[key]) == Q, f\"{key}: {len(s[key])} != Q={Q}\"\n",
    "print(f\"  All 12 prefixed conditions have Q={Q} tokens for sample 0\")\n",
    "print(\"  PASSED\")\n",
    "\n",
    "# TEST 4: 5-sample quick check\n",
    "print(\"\\n--- Test 4: 5-sample bare vs oracle vs random_tokens ---\")\n",
    "for i in range(5):\n",
    "    st = samples[i]\n",
    "    nb_ = score(st['passage'], st['query'], st['answer'])\n",
    "    no_ = score(st['passage'], st['query'], st['answer'],\n",
    "                prefix_token_ids=st['prefix_oracle'])\n",
    "    nr_ = score(st['passage'], st['query'], st['answer'],\n",
    "                prefix_token_ids=st['prefix_random_tokens'])\n",
    "    print(f\"  Sample {i}: bare={nb_:.4f}, oracle={no_:.4f} ({nb_-no_:+.4f}), \"\n",
    "          f\"random={nr_:.4f} ({nb_-nr_:+.4f})\")\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ALL VALIDATION TESTS PASSED\")\n",
    "print(\"=\" * 70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e869066b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Scoring loop — 13 conditions x 400 samples\n",
    "print(\"=\" * 70)\n",
    "print(\"SCORING ALL CONDITIONS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "COND_NAMES = [\n",
    "    'bare', 'random_tokens', 'repeat_token', 'scrambled_oracle',\n",
    "    'unrelated_query', 'same_topic', 'paraphrase', 'oracle',\n",
    "    'llm_extract', 'llm_question', 'llm_summarize',\n",
    "    'extractor_matched', 'adversarial_matched',\n",
    "]\n",
    "\n",
    "# Map condition name -> prefix key in samples dict\n",
    "COND_PREFIX_MAP = {\n",
    "    'random_tokens': 'prefix_random_tokens',\n",
    "    'repeat_token': 'prefix_repeat_token',\n",
    "    'scrambled_oracle': 'prefix_scrambled_oracle',\n",
    "    'unrelated_query': 'prefix_unrelated_query',\n",
    "    'same_topic': 'prefix_same_topic',\n",
    "    'paraphrase': 'prefix_paraphrase',\n",
    "    'oracle': 'prefix_oracle',\n",
    "    'llm_extract': 'prefix_llm_extract',\n",
    "    'llm_question': 'prefix_llm_question',\n",
    "    'llm_summarize': 'prefix_llm_summarize',\n",
    "    'extractor_matched': 'prefix_extractor_matched',\n",
    "    'adversarial_matched': 'prefix_adversarial_matched',\n",
    "}\n",
    "\n",
    "SCORING_KEY = 'bos_retained_token_matched_v02'\n",
    "\n",
    "results = []\n",
    "start_idx = 0\n",
    "\n",
    "if CHECKPOINT_PATH.exists():\n",
    "    ckpt = json.loads(CHECKPOINT_PATH.read_text())\n",
    "    if ckpt.get('n_total') == N_SAMPLES and ckpt.get('scoring') == SCORING_KEY:\n",
    "        if len(ckpt.get('results', [])) > 0:\n",
    "            saved_queries = [r['query'][:50] for r in ckpt['results']]\n",
    "            current_queries = [s['query'][:50] for s in samples[:len(saved_queries)]]\n",
    "            if saved_queries == current_queries:\n",
    "                results = ckpt['results']\n",
    "                start_idx = len(results)\n",
    "                print(f\"Resuming from checkpoint: {start_idx}/{N_SAMPLES}\")\n",
    "\n",
    "if start_idx == 0:\n",
    "    print(f\"Starting fresh: {len(COND_NAMES)} conditions x {N_SAMPLES} samples \"\n",
    "          f\"= {len(COND_NAMES) * N_SAMPLES} scorings\")\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "for i in tqdm(range(start_idx, N_SAMPLES), initial=start_idx, total=N_SAMPLES,\n",
    "              desc=\"Scoring\"):\n",
    "    s = samples[i]\n",
    "    query = s['query']\n",
    "    passage = s['passage']\n",
    "    answer = s['answer']\n",
    "\n",
    "    result = {\n",
    "        'query': query,\n",
    "        'answer': answer,\n",
    "        'passage_words': s['word_count'],\n",
    "        'Q': s['Q'],\n",
    "    }\n",
    "\n",
    "    # bare — no prefix\n",
    "    result['nll_bare'] = score(passage, query, answer)\n",
    "\n",
    "    # All prefixed conditions\n",
    "    for cond_name, prefix_key in COND_PREFIX_MAP.items():\n",
    "        result[f'nll_{cond_name}'] = score(\n",
    "            passage, query, answer,\n",
    "            prefix_token_ids=s[prefix_key]\n",
    "        )\n",
    "\n",
    "    results.append(result)\n",
    "\n",
    "    if (i + 1) % 20 == 0 or i == N_SAMPLES - 1:\n",
    "        ckpt = {\n",
    "            'n_total': N_SAMPLES,\n",
    "            'scoring': SCORING_KEY,\n",
    "            'results': results,\n",
    "            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        }\n",
    "        CHECKPOINT_PATH.write_text(json.dumps(ckpt))\n",
    "        elapsed = time.time() - t0\n",
    "        done = i - start_idx + 1\n",
    "        eta = (N_SAMPLES - i - 1) * elapsed / done if done > 0 else 0\n",
    "        tqdm.write(f\"  Checkpoint {i+1}/{N_SAMPLES} | {elapsed/60:.1f}m | ETA {eta/60:.1f}m\")\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "elapsed = time.time() - t0\n",
    "print(f\"\\nScoring complete: {len(results)} samples, \"\n",
    "      f\"{len(COND_NAMES)} conditions in {elapsed/60:.1f} min\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507183a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Results & analysis\n",
    "print(\"=\" * 70)\n",
    "print(f\"RESULTS (N={len(results)})\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Build arrays for all conditions\n",
    "cond_arrays = {}\n",
    "for cond in COND_NAMES:\n",
    "    cond_arrays[cond] = np.array([r[f'nll_{cond}'] for r in results])\n",
    "\n",
    "bare = cond_arrays['bare']\n",
    "\n",
    "# ================================================================\n",
    "# PART 1: Basic condition table\n",
    "# ================================================================\n",
    "print(f\"\\n--- PART 1: Condition Table ---\")\n",
    "print(f\"\\n  {'Condition':<24} {'NLL':>8} {'Delta':>8} {'d':>8} {'Win%':>7} \"\n",
    "      f\"{'p':>12} {'sig':>5}\")\n",
    "print(f\"  {'-'*78}\")\n",
    "\n",
    "analysis = {}\n",
    "for cond in COND_NAMES:\n",
    "    nlls = cond_arrays[cond]\n",
    "    mean_nll = nlls.mean()\n",
    "    if cond == 'bare':\n",
    "        print(f\"  {cond:<24} {mean_nll:>8.4f} {'--':>8} {'--':>8} {'--':>7} \"\n",
    "              f\"{'--':>12} {'--':>5}\")\n",
    "        analysis[cond] = {'mean_nll': float(mean_nll)}\n",
    "    else:\n",
    "        diff = bare - nlls  # positive = condition has lower NLL (better)\n",
    "        d = cohens_d(diff)\n",
    "        win_pct = 100 * np.mean(diff > 0)\n",
    "        _, p_val = stats.ttest_1samp(diff, 0)\n",
    "        sig = '***' if p_val < 0.001 else '**' if p_val < 0.01 else '*' if p_val < 0.05 else 'ns'\n",
    "        print(f\"  {cond:<24} {mean_nll:>8.4f} {diff.mean():>+8.4f} {d:>+8.3f} \"\n",
    "              f\"{win_pct:>6.1f}% {p_val:>12.2e} {sig:>5}\")\n",
    "        analysis[cond] = {\n",
    "            'mean_nll': float(mean_nll), 'delta': float(diff.mean()),\n",
    "            'd': float(d), 'win_pct': float(win_pct), 'p': float(p_val),\n",
    "        }\n",
    "\n",
    "# ================================================================\n",
    "# PART 2: Semantic gradient test\n",
    "# ================================================================\n",
    "print(f\"\\n--- PART 2: Semantic Gradient Test ---\")\n",
    "print(\"Relevance ordering: random_tokens(0) < scrambled(1) < unrelated(2) \"\n",
    "      \"< same_topic(3) < paraphrase(4) < oracle(5)\")\n",
    "\n",
    "GRADIENT_CONDS = [\n",
    "    ('random_tokens', 0),\n",
    "    ('scrambled_oracle', 1),\n",
    "    ('unrelated_query', 2),\n",
    "    ('same_topic', 3),\n",
    "    ('paraphrase', 4),\n",
    "    ('oracle', 5),\n",
    "]\n",
    "\n",
    "gradient_ranks = []\n",
    "gradient_ds = []\n",
    "for cond, rank in GRADIENT_CONDS:\n",
    "    d = cohens_d(bare - cond_arrays[cond])\n",
    "    gradient_ranks.append(rank)\n",
    "    gradient_ds.append(d)\n",
    "    print(f\"  [{rank}] {cond:<22} d={d:+.4f}\")\n",
    "\n",
    "rho, p_mono = stats.spearmanr(gradient_ranks, gradient_ds)\n",
    "sig_mono = '***' if p_mono < 0.001 else '**' if p_mono < 0.01 else '*' if p_mono < 0.05 else 'ns'\n",
    "print(f\"\\n  Spearman rho (relevance rank vs d): rho={rho:+.3f}, p={p_mono:.4f} {sig_mono}\")\n",
    "\n",
    "if rho > 0.8 and p_mono < 0.05:\n",
    "    print(f\"  --> MONOTONIC: clear semantic gradient\")\n",
    "elif rho > 0.5:\n",
    "    print(f\"  --> PARTIAL: imperfect gradient\")\n",
    "else:\n",
    "    print(f\"  --> FLAT: no clear gradient\")\n",
    "\n",
    "# ================================================================\n",
    "# PART 3: Structural decomposition\n",
    "# ================================================================\n",
    "print(f\"\\n--- PART 3: Structural Decomposition ---\")\n",
    "oracle_d = cohens_d(bare - cond_arrays['oracle'])\n",
    "random_d = cohens_d(bare - cond_arrays['random_tokens'])\n",
    "repeat_d = cohens_d(bare - cond_arrays['repeat_token'])\n",
    "\n",
    "if oracle_d != 0:\n",
    "    structural_frac_random = random_d / oracle_d * 100\n",
    "    structural_frac_repeat = repeat_d / oracle_d * 100\n",
    "else:\n",
    "    structural_frac_random = structural_frac_repeat = float('nan')\n",
    "\n",
    "print(f\"  Oracle d:         {oracle_d:+.4f}\")\n",
    "print(f\"  Random tokens d:  {random_d:+.4f} ({structural_frac_random:.1f}% of oracle)\")\n",
    "print(f\"  Repeat token d:   {repeat_d:+.4f} ({structural_frac_repeat:.1f}% of oracle)\")\n",
    "\n",
    "if abs(structural_frac_random) > 80:\n",
    "    print(f\"  --> Structure dominates: random_tokens recovers {structural_frac_random:.0f}% of oracle\")\n",
    "elif abs(structural_frac_random) > 40:\n",
    "    print(f\"  --> Mixed: structure accounts for {structural_frac_random:.0f}%\")\n",
    "else:\n",
    "    print(f\"  --> Semantics dominate: structure only {structural_frac_random:.0f}%\")\n",
    "\n",
    "# ================================================================\n",
    "# PART 4: LLM surrogates vs fixed task-framing\n",
    "# ================================================================\n",
    "print(f\"\\n--- PART 4: LLM Document-Specific vs Generic Task-Framing ---\")\n",
    "\n",
    "# Paired comparisons\n",
    "pairs = [\n",
    "    ('llm_extract', 'extractor_matched', 'doc-specific vs generic extraction'),\n",
    "    ('llm_question', 'extractor_matched', 'doc question vs generic extraction'),\n",
    "    ('llm_summarize', 'extractor_matched', 'doc summary vs generic extraction'),\n",
    "]\n",
    "\n",
    "for llm_cond, generic_cond, desc in pairs:\n",
    "    diff = cond_arrays[generic_cond] - cond_arrays[llm_cond]\n",
    "    d = cohens_d(diff)\n",
    "    _, p = stats.ttest_1samp(diff, 0)\n",
    "    sig = '***' if p < 0.001 else '**' if p < 0.01 else '*' if p < 0.05 else 'ns'\n",
    "    win = 100 * np.mean(diff > 0)\n",
    "    print(f\"  {desc}:\")\n",
    "    print(f\"    {llm_cond:<22} d={cohens_d(bare - cond_arrays[llm_cond]):+.4f}\")\n",
    "    print(f\"    {generic_cond:<22} d={cohens_d(bare - cond_arrays[generic_cond]):+.4f}\")\n",
    "    print(f\"    Paired diff: d={d:+.4f}, p={p:.2e} {sig}, \"\n",
    "          f\"LLM wins {win:.1f}%\")\n",
    "\n",
    "# ================================================================\n",
    "# PART 5: Hardness interaction\n",
    "# ================================================================\n",
    "print(f\"\\n--- PART 5: Hardness Interaction (5 quintiles x 13 conditions) ---\")\n",
    "quintile_bounds = np.percentile(bare, [20, 40, 60, 80])\n",
    "quintiles = np.digitize(bare, quintile_bounds)\n",
    "q_labels = ['Q1 easy', 'Q2', 'Q3', 'Q4', 'Q5 hard']\n",
    "\n",
    "# Print header\n",
    "header = f\"  {'Quintile':<12} {'N':>4} {'bare':>8}\"\n",
    "for cond in COND_NAMES[1:]:\n",
    "    header += f\"  {cond[:8]:>8}\"\n",
    "print(header)\n",
    "print(f\"  {'-'*(16 + 10 * len(COND_NAMES))}\")\n",
    "\n",
    "hardness_data = {}\n",
    "for q in range(5):\n",
    "    mask = quintiles == q\n",
    "    n_q = mask.sum()\n",
    "    row = f\"  {q_labels[q]:<12} {n_q:>4} {bare[mask].mean():>8.3f}\"\n",
    "    hardness_data[q_labels[q]] = {}\n",
    "    for cond in COND_NAMES[1:]:\n",
    "        delta = (bare[mask] - cond_arrays[cond][mask]).mean()\n",
    "        d = cohens_d(bare[mask] - cond_arrays[cond][mask])\n",
    "        row += f\"  {d:>+8.3f}\"\n",
    "        hardness_data[q_labels[q]][cond] = float(d)\n",
    "    print(row)\n",
    "\n",
    "# Correlation: hardness vs benefit for key conditions\n",
    "print(f\"\\n  Hardness-benefit correlations:\")\n",
    "for cond in ['oracle', 'random_tokens', 'extractor_matched', 'llm_extract', 'paraphrase']:\n",
    "    diff = bare - cond_arrays[cond]\n",
    "    r_val, p_val = stats.spearmanr(bare, diff)\n",
    "    sig = '***' if p_val < 0.001 else '**' if p_val < 0.01 else '*' if p_val < 0.05 else 'ns'\n",
    "    print(f\"    {cond:<22} rho={r_val:+.3f} (p={p_val:.2e}) {sig}\")\n",
    "\n",
    "# ================================================================\n",
    "# PART 6: Per-sample ranking\n",
    "# ================================================================\n",
    "print(f\"\\n--- PART 6: Per-Sample Ranking ---\")\n",
    "\n",
    "stacked = np.stack([cond_arrays[c] for c in COND_NAMES], axis=1)\n",
    "best_idx = stacked.argmin(axis=1)\n",
    "\n",
    "print(f\"  {'Condition':<24} {'Best count':>12} {'Best %':>8} {'Mean rank':>10}\")\n",
    "print(f\"  {'-'*58}\")\n",
    "\n",
    "ranks = stacked.argsort(axis=1).argsort(axis=1) + 1\n",
    "mean_ranks = ranks.mean(axis=0)\n",
    "for ci, cname in enumerate(COND_NAMES):\n",
    "    count = (best_idx == ci).sum()\n",
    "    pct = 100 * count / len(best_idx)\n",
    "    print(f\"  {cname:<24} {count:>12} {pct:>7.1f}% {mean_ranks[ci]:>10.2f}\")\n",
    "\n",
    "# ================================================================\n",
    "# PART 7: Document-specific vs generic — deep dive\n",
    "# ================================================================\n",
    "print(f\"\\n--- PART 7: Document-Specific vs Generic Deep Dive ---\")\n",
    "print(\"For each sample: is llm_extract better than extractor_matched?\")\n",
    "\n",
    "llm_better = cond_arrays['extractor_matched'] - cond_arrays['llm_extract']\n",
    "print(f\"  llm_extract wins: {100 * np.mean(llm_better > 0):.1f}%\")\n",
    "print(f\"  Mean advantage: {llm_better.mean():+.4f}\")\n",
    "print(f\"  Cohen's d: {cohens_d(llm_better):+.4f}\")\n",
    "\n",
    "# By quintile\n",
    "print(f\"\\n  LLM advantage by hardness quintile:\")\n",
    "for q in range(5):\n",
    "    mask = quintiles == q\n",
    "    diff_q = llm_better[mask]\n",
    "    d_q = cohens_d(diff_q)\n",
    "    win_q = 100 * np.mean(diff_q > 0)\n",
    "    print(f\"    {q_labels[q]:<12} d={d_q:+.4f}, LLM wins {win_q:.1f}%\")\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9bfc4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Verdict and save\n",
    "print(\"=\" * 70)\n",
    "print(\"VERDICT — Exp 02: Token-Matched Semantic Probing with LLM Surrogates\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\nModel: {MODEL_NAME}\")\n",
    "print(f\"Scoring: BOS-retained repositioning + token-level prefix matching\")\n",
    "print(f\"N: {len(results)} samples (MS MARCO v1.1)\")\n",
    "print(f\"Conditions: {len(COND_NAMES)}\")\n",
    "\n",
    "# Key results\n",
    "oracle_d = cohens_d(bare - cond_arrays['oracle'])\n",
    "_, p_oracle = stats.ttest_1samp(bare - cond_arrays['oracle'], 0)\n",
    "\n",
    "print(f\"\\n--- Key findings ---\")\n",
    "print(f\"  1. Oracle (token-matched): d={oracle_d:+.4f} \"\n",
    "      f\"({'***' if p_oracle < 0.001 else '**' if p_oracle < 0.01 else '*' if p_oracle < 0.05 else 'ns'})\")\n",
    "\n",
    "print(f\"\\n  2. Semantic gradient: rho={rho:+.3f} (p={p_mono:.4f})\")\n",
    "if rho > 0.8 and p_mono < 0.05:\n",
    "    print(f\"     -> MONOTONIC: semantic content drives the effect\")\n",
    "elif abs(rho) < 0.3:\n",
    "    print(f\"     -> FLAT: no semantic gradient\")\n",
    "else:\n",
    "    print(f\"     -> PARTIAL: imperfect gradient\")\n",
    "\n",
    "print(f\"\\n  3. Structural fraction: {structural_frac_random:.1f}% (random_tokens / oracle)\")\n",
    "\n",
    "print(f\"\\n  4. All conditions ranked by d:\")\n",
    "sorted_conds = sorted(\n",
    "    [(c, cohens_d(bare - cond_arrays[c])) for c in COND_NAMES if c != 'bare'],\n",
    "    key=lambda x: x[1], reverse=True\n",
    ")\n",
    "for cond, d in sorted_conds:\n",
    "    _, p = stats.ttest_1samp(bare - cond_arrays[cond], 0)\n",
    "    sig = '***' if p < 0.001 else '**' if p < 0.01 else '*' if p < 0.05 else 'ns'\n",
    "    print(f\"     {cond:<24} d={d:+.4f} ({sig})\")\n",
    "\n",
    "# Conclusions\n",
    "print(f\"\\n--- Conclusions ---\")\n",
    "if abs(oracle_d) < 0.05:\n",
    "    print(f\"  Oracle conditioning has negligible effect (d={oracle_d:+.3f}).\")\n",
    "elif oracle_d < -0.1:\n",
    "    print(f\"  Oracle conditioning HURTS (d={oracle_d:+.3f}).\")\n",
    "elif oracle_d > 0.1:\n",
    "    print(f\"  Oracle conditioning HELPS (d={oracle_d:+.3f}).\")\n",
    "else:\n",
    "    print(f\"  Oracle conditioning has weak effect (d={oracle_d:+.3f}).\")\n",
    "\n",
    "best_cond, best_d = sorted_conds[0]\n",
    "worst_cond, worst_d = sorted_conds[-1]\n",
    "print(f\"  Best condition: {best_cond} (d={best_d:+.3f})\")\n",
    "print(f\"  Worst condition: {worst_cond} (d={worst_d:+.3f})\")\n",
    "\n",
    "# Save\n",
    "final_results = {\n",
    "    'experiment': 'v4_exp02_token_matched_semantic_probing',\n",
    "    'model': MODEL_NAME,\n",
    "    'scoring': 'bos_retained_repositioning_token_matched',\n",
    "    'dataset': 'ms_marco_v1.1',\n",
    "    'n_samples': len(results),\n",
    "    'n_conditions': len(COND_NAMES),\n",
    "    'seed': SEED,\n",
    "    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'conditions': analysis,\n",
    "    'gradient': {\n",
    "        'spearman_rho': float(rho),\n",
    "        'spearman_p': float(p_mono),\n",
    "    },\n",
    "    'structural_decomposition': {\n",
    "        'oracle_d': float(oracle_d),\n",
    "        'random_tokens_d': float(random_d),\n",
    "        'repeat_token_d': float(repeat_d),\n",
    "        'structural_frac_random': float(structural_frac_random),\n",
    "        'structural_frac_repeat': float(structural_frac_repeat),\n",
    "    },\n",
    "    'hardness_interaction': hardness_data,\n",
    "    'per_sample_mean_ranks': {\n",
    "        cond: float(mean_ranks[ci]) for ci, cond in enumerate(COND_NAMES)\n",
    "    },\n",
    "    'query_token_stats': {\n",
    "        'mean': float(np.mean([r['Q'] for r in results])),\n",
    "        'median': float(np.median([r['Q'] for r in results])),\n",
    "        'min': int(np.min([r['Q'] for r in results])),\n",
    "        'max': int(np.max([r['Q'] for r in results])),\n",
    "    },\n",
    "}\n",
    "\n",
    "with open(RESULTS_DIR / 'results.json', 'w') as f:\n",
    "    json.dump(final_results, f, indent=2)\n",
    "print(f\"\\nResults saved to {RESULTS_DIR / 'results.json'}\")\n",
    "\n",
    "# Cleanup\n",
    "print(f\"\\nCleaning up GPU memory...\")\n",
    "mem_before = torch.cuda.memory_allocated() / 1e9\n",
    "del model, tokenizer\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "mem_after = torch.cuda.memory_allocated() / 1e9\n",
    "print(f\"GPU memory: {mem_before:.2f} GB -> {mem_after:.2f} GB\")\n",
    "print(\"Done!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
