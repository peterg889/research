{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aea0611d",
   "metadata": {
    "papermill": {
     "duration": 0.003291,
     "end_time": "2026-02-21T11:30:54.387631",
     "exception": false,
     "start_time": "2026-02-21T11:30:54.384340",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Experiment 02: Token-Matched Semantic Probing with LLM Surrogates\n",
    "\n",
    "## Motivation\n",
    "\n",
    "Exp 01 rerun (Gemma 3 12B-IT, N=400, BOS-retained repositioning) established that\n",
    "the semantic effect is real but **reversed from expectations**: oracle conditioning\n",
    "HURTS (d=-0.151), while the data-extraction task-framing prefix HELPS (d=+0.264).\n",
    "However, Exp 01 has a **prefix-length confound** — different conditions have different\n",
    "token counts (P=14-20), producing different RoPE repositioning deltas.\n",
    "\n",
    "Exps 02-05 were invalidated by the 1-token look-ahead bug. Exps 06-07 used\n",
    "`slice_kv_cache` without BOS retention on a different model (4B-IT) and are not\n",
    "directly comparable.\n",
    "\n",
    "## Goal\n",
    "\n",
    "Design and run a definitive experiment that:\n",
    "1. Eliminates all structural confounds via token-level prefix matching\n",
    "2. Spans the full semantic gradient\n",
    "3. Introduces LLM-generated document-specific surrogates\n",
    "4. Deeply probes where conditioning helps vs hurts\n",
    "\n",
    "## Method — BOS-Retained Repositioning with Token-Level Matching\n",
    "\n",
    "**Phase A:** `[BOS] + prefix_ids(Q) + [\\n] + doc_ids(D)` at natural positions.\n",
    "Select BOS + doc from cache (skip prefix + newline).\n",
    "Reposition doc keys from `[1+Q+NL, ..., Q+NL+D]` to `[1, ..., D]`.\n",
    "Cache has `1+D` entries (BOS at 0, doc at 1..D).\n",
    "\n",
    "**Phase B:** `[\\n + query + \\n + answer]` at positions `[D+1, ...]`.\n",
    "`cache_position` auto-generated from cache length (= 1+D = D+1). No look-ahead.\n",
    "\n",
    "**Key invariant:** For ALL prefixed conditions, repositioning delta = -(Q+NL) is\n",
    "**identical** per sample. Every condition uses exactly Q prefix token IDs.\n",
    "\n",
    "## Conditions (13 total)\n",
    "\n",
    "| # | Key | Source | Semantic relevance | Token construction |\n",
    "|---|-----|--------|-------------------|--------------------|\n",
    "| 1 | `bare` | — | baseline | No prefix |\n",
    "| 2 | `random_tokens` | Random vocab IDs | none | Q random IDs from vocab |\n",
    "| 3 | `repeat_token` | Single token x Q | none (structural) | Token ID 1000 repeated Q times |\n",
    "| 4 | `scrambled_oracle` | Shuffled query | vocab match only | Random permutation of oracle IDs |\n",
    "| 5 | `unrelated_query` | Other sample's query | low | Sample (i+N/2)%N query, pad/trunc to Q |\n",
    "| 6 | `same_topic` | LLM-generated | medium | \"Write a question about same topic...\" pad/trunc to Q |\n",
    "| 7 | `paraphrase` | LLM-generated | high | \"Rephrase this query differently...\" pad/trunc to Q |\n",
    "| 8 | `oracle` | Real query | maximal | Exact query token IDs (already Q) |\n",
    "| 9 | `llm_extract` | LLM doc-specific | task-framing (doc) | \"List key facts from this document\" pad/trunc to Q |\n",
    "| 10 | `llm_question` | LLM doc-specific | query-like (doc) | \"What question does this doc answer?\" pad/trunc to Q |\n",
    "| 11 | `llm_summarize` | LLM doc-specific | summary (doc) | \"Summarize in one sentence\" pad/trunc to Q |\n",
    "| 12 | `extractor_matched` | Fixed text | task-framing (generic) | \"Extract:\" text tokenized, pad/trunc to Q |\n",
    "| 13 | `adversarial_matched` | Fixed text | adversarial | Adversarial text tokenized, pad/trunc to Q |\n",
    "\n",
    "## Key analyses\n",
    "\n",
    "1. **Semantic gradient test**: Spearman rho of relevance rank vs delta_NLL\n",
    "2. **Structural decomposition**: delta(random_tokens) / delta(oracle)\n",
    "3. **LLM doc-specific vs generic**: paired test llm_extract vs extractor_matched\n",
    "4. **Hardness interaction**: 5 quintile bins x 13 conditions\n",
    "5. **Per-sample ranking**: which condition gives lowest NLL per sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25c85fa9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-21T11:30:54.395233Z",
     "iopub.status.busy": "2026-02-21T11:30:54.394911Z",
     "iopub.status.idle": "2026-02-21T11:31:15.522194Z",
     "shell.execute_reply": "2026-02-21T11:31:15.521243Z"
    },
    "papermill": {
     "duration": 21.133351,
     "end_time": "2026-02-21T11:31:15.523882",
     "exception": false,
     "start_time": "2026-02-21T11:30:54.390531",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading google/gemma-3-12b-it...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3544f7def5b44447b749ee46147aafac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/1065 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exp 02: Token-Matched Semantic Probing with LLM Surrogates\n",
      "Scoring: BOS-retained repositioning (look-ahead fix)\n",
      "N: 400, Model: google/gemma-3-12b-it\n",
      "DEVICE: cuda:0, dtype: torch.bfloat16\n",
      "GPU memory: 24.38 GB\n",
      "Vocab size: 262208\n",
      "Num layers: 48\n",
      "Num KV heads: 8\n",
      "Layer types: {'full_attention', 'sliding_attention'} (48 layers)\n",
      "  sliding_attention: theta=10000.0, type=default, factor=N/A\n",
      "  full_attention: theta=1000000.0, type=linear, factor=8.0\n",
      "  Global layers: 8/48 (indices: [5, 11, 17, 23, 29, 35, 41, 47])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading MS MARCO v1.1 validation...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total candidates: 1200\n",
      "Loaded 400 samples\n",
      "Mean passage words: 73\n",
      "Mean answer words: 14\n",
      "Mean query words: 6\n",
      "\n",
      "First sample:\n",
      "  Query:  average annual temperature of Uruguay...\n",
      "  Answer: Very mild at 15.8 degrees Celsius (60.4 degrees Fahrenheit)....\n",
      "  Passage (76w): Average Temperatures in Montevideo, Uruguay. 1  The average annual tem...\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Setup and model loading\n",
    "import os\n",
    "os.umask(0o000)\n",
    "\n",
    "import sys, json, time, gc, re\n",
    "import random as pyrandom\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from scipy import stats\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "sys.path.insert(0, \"../../..\")\n",
    "from lib.analysis import cohens_d\n",
    "\n",
    "SEED = 42\n",
    "N_SAMPLES = 400\n",
    "MODEL_NAME = \"google/gemma-3-12b-it\"\n",
    "\n",
    "RESULTS_DIR = Path(\"../../../results/decoder_only/exp02\")\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "SURROGATES_PATH = RESULTS_DIR / \"surrogates.json\"\n",
    "CHECKPOINT_PATH = RESULTS_DIR / \"checkpoint.json\"\n",
    "\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "pyrandom.seed(SEED)\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv())\n",
    "HF_TOKEN = os.environ.get(\"HF_TOKEN\")\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, DynamicCache\n",
    "\n",
    "print(f\"Loading {MODEL_NAME}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, token=HF_TOKEN)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME, device_map=\"auto\", dtype=torch.bfloat16, token=HF_TOKEN,\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "DEVICE = next(model.parameters()).device\n",
    "BOS_ID = tokenizer.bos_token_id\n",
    "NEWLINE_IDS = tokenizer(\"\\n\", add_special_tokens=False).input_ids\n",
    "text_cfg = getattr(model.config, 'text_config', model.config)\n",
    "VOCAB_SIZE = getattr(text_cfg, 'vocab_size', 262208)\n",
    "\n",
    "print(f\"Exp 02: Token-Matched Semantic Probing with LLM Surrogates\")\n",
    "print(f\"Scoring: BOS-retained repositioning (look-ahead fix)\")\n",
    "print(f\"N: {N_SAMPLES}, Model: {MODEL_NAME}\")\n",
    "print(f\"DEVICE: {DEVICE}, dtype: {next(model.parameters()).dtype}\")\n",
    "print(f\"GPU memory: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "print(f\"Vocab size: {VOCAB_SIZE}\")\n",
    "print(f\"Num layers: {getattr(text_cfg, 'num_hidden_layers', 'N/A')}\")\n",
    "print(f\"Num KV heads: {getattr(text_cfg, 'num_key_value_heads', 'N/A')}\")\n",
    "rope_params = getattr(text_cfg, 'rope_parameters', {})\n",
    "layer_types_list = getattr(text_cfg, 'layer_types', [])\n",
    "print(f\"Layer types: {set(layer_types_list)} ({len(layer_types_list)} layers)\")\n",
    "for ltype, params in rope_params.items():\n",
    "    print(f\"  {ltype}: theta={params.get('rope_theta')}, \"\n",
    "          f\"type={params.get('rope_type')}, factor={params.get('factor', 'N/A')}\")\n",
    "n_global = sum(1 for t in layer_types_list if t == 'full_attention')\n",
    "print(f\"  Global layers: {n_global}/{len(layer_types_list)} \"\n",
    "      f\"(indices: {[i for i, t in enumerate(layer_types_list) if t == 'full_attention']})\")\n",
    "\n",
    "# Load MS MARCO\n",
    "from lib.data import count_words\n",
    "from datasets import load_dataset\n",
    "\n",
    "print(\"\\nLoading MS MARCO v1.1 validation...\")\n",
    "ds = load_dataset(\"microsoft/ms_marco\", \"v1.1\", split=\"validation\")\n",
    "\n",
    "all_candidates = []\n",
    "for item in ds:\n",
    "    if len(all_candidates) >= 3 * N_SAMPLES:\n",
    "        break\n",
    "    passages = item.get('passages', {})\n",
    "    ptexts = passages.get('passage_text', [])\n",
    "    is_sel = passages.get('is_selected', [])\n",
    "    query = item.get('query', '')\n",
    "    answers = item.get('answers', [])\n",
    "    well_formed = item.get('wellFormedAnswers', [])\n",
    "    answer = None\n",
    "    if well_formed and len(well_formed) > 0 and well_formed[0] not in ('[]', ''):\n",
    "        answer = well_formed[0]\n",
    "    elif answers and len(answers) > 0 and answers[0] != 'No Answer Present.':\n",
    "        answer = answers[0]\n",
    "    if not answer:\n",
    "        continue\n",
    "    for pt, sel in zip(ptexts, is_sel):\n",
    "        wc = count_words(pt)\n",
    "        if sel == 1 and 30 <= wc <= 300:\n",
    "            all_candidates.append({\n",
    "                'passage': pt, 'query': query, 'answer': answer,\n",
    "                'word_count': wc,\n",
    "            })\n",
    "            break\n",
    "\n",
    "print(f\"Total candidates: {len(all_candidates)}\")\n",
    "np.random.seed(SEED)\n",
    "indices = np.random.permutation(len(all_candidates))\n",
    "samples = [all_candidates[i] for i in indices[:N_SAMPLES]]\n",
    "del ds, all_candidates\n",
    "gc.collect()\n",
    "\n",
    "print(f\"Loaded {len(samples)} samples\")\n",
    "print(f\"Mean passage words: {np.mean([s['word_count'] for s in samples]):.0f}\")\n",
    "print(f\"Mean answer words: {np.mean([count_words(s['answer']) for s in samples]):.0f}\")\n",
    "print(f\"Mean query words: {np.mean([count_words(s['query']) for s in samples]):.0f}\")\n",
    "print(f\"\\nFirst sample:\")\n",
    "print(f\"  Query:  {samples[0]['query'][:70]}...\")\n",
    "print(f\"  Answer: {samples[0]['answer'][:70]}...\")\n",
    "print(f\"  Passage ({samples[0]['word_count']}w): {samples[0]['passage'][:70]}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c35fff73",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-21T11:31:15.532206Z",
     "iopub.status.busy": "2026-02-21T11:31:15.531479Z",
     "iopub.status.idle": "2026-02-21T12:41:45.789785Z",
     "shell.execute_reply": "2026-02-21T12:41:45.789020Z"
    },
    "papermill": {
     "duration": 4230.264123,
     "end_time": "2026-02-21T12:41:45.791396",
     "exception": false,
     "start_time": "2026-02-21T11:31:15.527273",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2a64dcff9a24379a654d2e1a6c17079",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating surrogates:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Gen checkpoint 20/400 | 3.5m | ETA 66.5m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Gen checkpoint 40/400 | 7.4m | ETA 66.6m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Gen checkpoint 60/400 | 10.9m | ETA 61.5m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Gen checkpoint 80/400 | 14.4m | ETA 57.6m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Gen checkpoint 100/400 | 18.1m | ETA 54.3m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Gen checkpoint 120/400 | 21.8m | ETA 50.8m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Gen checkpoint 140/400 | 25.1m | ETA 46.7m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Gen checkpoint 160/400 | 28.6m | ETA 42.9m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Gen checkpoint 180/400 | 32.2m | ETA 39.3m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Gen checkpoint 200/400 | 35.6m | ETA 35.6m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Gen checkpoint 220/400 | 39.1m | ETA 32.0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Gen checkpoint 240/400 | 42.6m | ETA 28.4m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Gen checkpoint 260/400 | 45.9m | ETA 24.7m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Gen checkpoint 280/400 | 49.6m | ETA 21.3m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Gen checkpoint 300/400 | 53.0m | ETA 17.7m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Gen checkpoint 320/400 | 56.5m | ETA 14.1m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Gen checkpoint 340/400 | 59.9m | ETA 10.6m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Gen checkpoint 360/400 | 63.3m | ETA 7.0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Gen checkpoint 380/400 | 66.8m | ETA 3.5m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Gen checkpoint 400/400 | 70.5m | ETA 0.0m\n",
      "\n",
      "Generation complete: 400 samples in 70.5 min\n",
      "Saved surrogates to ../../../results/decoder_only/exp02/surrogates.json\n",
      "\n",
      "Sample 0: query='average annual temperature of Uruguay'\n",
      "  paraphrase     : Typical yearly temperatures in Uruguay.\n",
      "  same_topic     : What is Montevideo's coldest average monthly temperature?\n",
      "  llm_extract    : 15.8°C (60.4°F) average annual temperature, 12°C (21.6°F) mo\n",
      "  llm_question   : What are the average temperatures in Montevideo, Uruguay?\n",
      "  llm_summarize  : Montevideo, Uruguay experiences a mild climate with a relati\n",
      "\n",
      "Sample 1: query='average cost for an acre of land in arizona'\n",
      "  paraphrase     : Arizona land price per acre typically?\n",
      "  same_topic     : What percentage of Mississippi land is federally owned?\n",
      "  llm_extract    : 72 million acres, Arizona, $315 billion valuation, $4,300/ac\n",
      "  llm_question   : How does Arizona's land value and ownership compare?\n",
      "  llm_summarize  : Arizona, despite being a large state, has relatively low lan\n",
      "\n",
      "Sample 2: query='where can i buy nematodes'\n",
      "  paraphrase     : Retailers selling microscopic worms for pest control?\n",
      "  same_topic     : What are reliable beneficial insect suppliers?\n",
      "  llm_extract    : Beneficial insects, mites, and nematodes, commercial growers\n",
      "  llm_question   : Where can commercial growers buy beneficial insects?\n",
      "  llm_summarize  : This document recommends several trusted suppliers of benefi\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Generate LLM surrogates (5 per sample)\n",
    "\n",
    "PROMPT_PARAPHRASE = (\n",
    "    \"Rephrase this search query using completely different words but keeping \"\n",
    "    \"the same meaning. Keep it to 5-8 words. Output only the rephrased query.\"\n",
    ")\n",
    "PROMPT_SAME_TOPIC = (\n",
    "    \"Write a question about the same topic as this document but asking for \"\n",
    "    \"DIFFERENT information. Keep it to 5-8 words. Output only the question.\"\n",
    ")\n",
    "PROMPT_EXTRACT = (\n",
    "    \"List the key facts from this document as a brief comma-separated list. \"\n",
    "    \"Output only the fact list, nothing else.\"\n",
    ")\n",
    "PROMPT_QUESTION = (\n",
    "    \"What question does this document answer? Write only the question, \"\n",
    "    \"nothing else. Keep it to 5-10 words.\"\n",
    ")\n",
    "PROMPT_SUMMARIZE = (\n",
    "    \"Summarize this document in one sentence. Output only the summary, nothing else.\"\n",
    ")\n",
    "\n",
    "def generate_text(input_text, prompt_text, max_new_tokens=50):\n",
    "    # Generate text from a prompt + input using Gemma IT chat template.\n",
    "    messages = [\n",
    "        {\"role\": \"user\",\n",
    "         \"content\": f\"{prompt_text}\\n\\n{input_text}\"}\n",
    "    ]\n",
    "    chat_text = tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    inputs = tokenizer(chat_text, return_tensors=\"pt\",\n",
    "                       truncation=True, max_length=1024).to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "        )\n",
    "\n",
    "    new_tokens = output_ids[0, inputs['input_ids'].shape[1]:]\n",
    "    raw_text = tokenizer.decode(new_tokens, skip_special_tokens=True)\n",
    "\n",
    "    # Post-process: strip, take first line, remove quotes, truncate to 20 words\n",
    "    cleaned = raw_text.strip().split(\"\\n\")[0].strip()\n",
    "    cleaned = cleaned.strip('\"').strip(\"'\").strip()\n",
    "    cleaned = \" \".join(cleaned.split()[:20])\n",
    "    return cleaned\n",
    "\n",
    "\n",
    "if SURROGATES_PATH.exists():\n",
    "    print(\"Loading cached surrogates...\")\n",
    "    surrogates = json.loads(SURROGATES_PATH.read_text())\n",
    "    assert len(surrogates) == N_SAMPLES, f\"Expected {N_SAMPLES}, got {len(surrogates)}\"\n",
    "    for i in range(min(10, N_SAMPLES)):\n",
    "        assert surrogates[i]['query'][:50] == samples[i]['query'][:50], \\\n",
    "            f\"Sample {i} query mismatch\"\n",
    "    print(f\"Loaded {len(surrogates)} cached surrogates\")\n",
    "    print(f\"Keys per sample: {list(surrogates[0].keys())}\")\n",
    "else:\n",
    "    # Generate with checkpointing\n",
    "    surrogates = []\n",
    "    gen_ckpt_path = RESULTS_DIR / \"gen_checkpoint.json\"\n",
    "\n",
    "    if gen_ckpt_path.exists():\n",
    "        gen_ckpt = json.loads(gen_ckpt_path.read_text())\n",
    "        if gen_ckpt.get('n_total') == N_SAMPLES:\n",
    "            surrogates = gen_ckpt['surrogates']\n",
    "            print(f\"Resuming generation from {len(surrogates)}/{N_SAMPLES}\")\n",
    "\n",
    "    start_gen = len(surrogates)\n",
    "    t0 = time.time()\n",
    "\n",
    "    for i in tqdm(range(start_gen, N_SAMPLES), initial=start_gen, total=N_SAMPLES,\n",
    "                  desc=\"Generating surrogates\"):\n",
    "        s = samples[i]\n",
    "        entry = {'query': s['query']}\n",
    "\n",
    "        # First 200 words of passage for doc-specific prompts\n",
    "        doc_words = s['passage'].split()[:200]\n",
    "        doc_input = f\"Document:\\n{' '.join(doc_words)}\"\n",
    "\n",
    "        # 1. Paraphrase: rephrase the query\n",
    "        torch.manual_seed(SEED + i * 10)\n",
    "        entry['paraphrase'] = generate_text(\n",
    "            f\"Query: {s['query']}\", PROMPT_PARAPHRASE\n",
    "        )\n",
    "\n",
    "        # 2. Same-topic: question about same topic but different info\n",
    "        torch.manual_seed(SEED + i * 10 + 1)\n",
    "        entry['same_topic'] = generate_text(doc_input, PROMPT_SAME_TOPIC)\n",
    "\n",
    "        # 3. LLM extract: key facts from document\n",
    "        torch.manual_seed(SEED + i * 10 + 2)\n",
    "        entry['llm_extract'] = generate_text(doc_input, PROMPT_EXTRACT)\n",
    "\n",
    "        # 4. LLM question: what question does the doc answer?\n",
    "        torch.manual_seed(SEED + i * 10 + 3)\n",
    "        entry['llm_question'] = generate_text(doc_input, PROMPT_QUESTION)\n",
    "\n",
    "        # 5. LLM summarize: one-sentence summary\n",
    "        torch.manual_seed(SEED + i * 10 + 4)\n",
    "        entry['llm_summarize'] = generate_text(doc_input, PROMPT_SUMMARIZE)\n",
    "\n",
    "        surrogates.append(entry)\n",
    "\n",
    "        if (i + 1) % 20 == 0 or i == N_SAMPLES - 1:\n",
    "            gen_ckpt = {'n_total': N_SAMPLES, 'surrogates': surrogates,\n",
    "                        'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')}\n",
    "            gen_ckpt_path.write_text(json.dumps(gen_ckpt))\n",
    "            elapsed = time.time() - t0\n",
    "            done = i - start_gen + 1\n",
    "            eta = (N_SAMPLES - i - 1) * elapsed / done if done > 0 else 0\n",
    "            tqdm.write(f\"  Gen checkpoint {i+1}/{N_SAMPLES} | \"\n",
    "                       f\"{elapsed/60:.1f}m | ETA {eta/60:.1f}m\")\n",
    "\n",
    "    elapsed = time.time() - t0\n",
    "    print(f\"\\nGeneration complete: {len(surrogates)} samples in {elapsed/60:.1f} min\")\n",
    "\n",
    "    # Save final surrogates\n",
    "    SURROGATES_PATH.write_text(json.dumps(surrogates, indent=2))\n",
    "    print(f\"Saved surrogates to {SURROGATES_PATH}\")\n",
    "\n",
    "# Show examples\n",
    "for i in range(3):\n",
    "    s = surrogates[i]\n",
    "    print(f\"\\nSample {i}: query='{s['query'][:60]}'\")\n",
    "    for key in ['paraphrase', 'same_topic', 'llm_extract', 'llm_question', 'llm_summarize']:\n",
    "        print(f\"  {key:<15}: {s.get(key, 'N/A')[:60]}\")\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ecb1c886",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-21T12:41:45.801352Z",
     "iopub.status.busy": "2026-02-21T12:41:45.801059Z",
     "iopub.status.idle": "2026-02-21T12:41:46.075306Z",
     "shell.execute_reply": "2026-02-21T12:41:46.074629Z"
    },
    "papermill": {
     "duration": 0.28122,
     "end_time": "2026-02-21T12:41:46.076859",
     "exception": false,
     "start_time": "2026-02-21T12:41:45.795639",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query token count — mean: 6.5, median: 6, min: 2, max: 15\n",
      "\n",
      "Sample 0: Q=5, query='average annual temperature of Uruguay...'\n",
      "  oracle                : average annual temperature of Uruguay...\n",
      "  random_tokens         : σταAreaDataIndex grownCry salarié...\n",
      "  repeat_token          : putputputputput...\n",
      "  scrambled_oracle      :  of Uruguay annualaverage temperature...\n",
      "  unrelated_query       : gestation period for chickens...\n",
      "  same_topic            : What is Montevideo's...\n",
      "  paraphrase            : Typical yearly temperatures in Uruguay...\n",
      "  llm_extract           : 15.8°...\n",
      "  llm_question          : What are the average temperatures...\n",
      "  llm_summarize         : Montevideo, Uruguay experiences...\n",
      "  extractor_matched     : Extract all key data points...\n",
      "  adversarial_matched   : The recipe calls for two...\n",
      "\n",
      "Sample 1: Q=10, query='average cost for an acre of land in arizona...'\n",
      "  oracle                : average cost for an acre of land in...\n",
      "  random_tokens         : <unused3821> Observqosებაშიnoticedirah<unused3561>кистон...\n",
      "  repeat_token          : putputputputputputputput...\n",
      "  scrambled_oracle      :  of an in land acrerizona costaverage...\n",
      "  unrelated_query       : does seatac have special first class tsa...\n",
      "  same_topic            : What percentage of Mississippi land is federally owned...\n",
      "  paraphrase            : Arizona land price per acre typically?Arizona...\n",
      "  llm_extract           : 72 million acres, Arizona, $...\n",
      "  llm_question          : How does Arizona's land value and...\n",
      "  llm_summarize         : Arizona, despite being a large state,...\n",
      "  extractor_matched     : Extract all key data points, facts,...\n",
      "  adversarial_matched   : The recipe calls for two cups of flour...\n",
      "\n",
      "Sample 2: Q=5, query='where can i buy nematodes...'\n",
      "  oracle                : where can i buy nematodes...\n",
      "  random_tokens         : branded Екатери Chees обна сопро...\n",
      "  repeat_token          : putputputputput...\n",
      "  scrambled_oracle      :  nematodes buy can iwhere...\n",
      "  unrelated_query       : could chlamydia be cured...\n",
      "  same_topic            : What are reliable beneficial insect...\n",
      "  paraphrase            : Retailers selling microscopic worms...\n",
      "  llm_extract           : Beneficial insects, mites...\n",
      "  llm_question          : Where can commercial growers buy...\n",
      "  llm_summarize         : This document recommends several trusted...\n",
      "  extractor_matched     : Extract all key data points...\n",
      "  adversarial_matched   : The recipe calls for two...\n",
      "\n",
      "All 12 prefix types verified for 400 samples.\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Build per-sample token-level prefix IDs (13 conditions)\n",
    "\n",
    "# Fixed-text prefixes\n",
    "EXTRACTOR_TEXT = \"Extract all key data points, facts, entities, and specific attributes from the following text.\"\n",
    "ADVERSARIAL_TEXT = \"The recipe calls for two cups of flour, one cup of sugar, and a pinch of salt mixed together.\"\n",
    "\n",
    "def make_prefix(token_ids, Q):\n",
    "    # Pad or truncate token_ids to exactly Q tokens.\n",
    "    if len(token_ids) >= Q:\n",
    "        return token_ids[:Q]\n",
    "    else:\n",
    "        padded = token_ids * ((Q // max(len(token_ids), 1)) + 1)\n",
    "        return padded[:Q]\n",
    "\n",
    "\n",
    "pyrandom.seed(SEED + 200)\n",
    "np.random.seed(SEED + 300)\n",
    "\n",
    "# Pre-tokenize fixed texts\n",
    "extractor_ids = tokenizer(EXTRACTOR_TEXT, add_special_tokens=False).input_ids\n",
    "adversarial_ids = tokenizer(ADVERSARIAL_TEXT, add_special_tokens=False).input_ids\n",
    "\n",
    "# Special token IDs to exclude from random sampling\n",
    "special_ids = set(tokenizer.all_special_ids)\n",
    "\n",
    "for i, s in enumerate(samples):\n",
    "    surr = surrogates[i]\n",
    "    q_ids = tokenizer(s['query'], add_special_tokens=False).input_ids\n",
    "    Q = len(q_ids)\n",
    "    s['Q'] = Q\n",
    "\n",
    "    # 1. oracle: exact query token IDs (already Q tokens)\n",
    "    s['prefix_oracle'] = q_ids\n",
    "\n",
    "    # 2. random_tokens: random vocab IDs (excluding special tokens)\n",
    "    rand_ids = []\n",
    "    while len(rand_ids) < Q:\n",
    "        tid = np.random.randint(0, VOCAB_SIZE)\n",
    "        if tid not in special_ids:\n",
    "            rand_ids.append(int(tid))\n",
    "    s['prefix_random_tokens'] = rand_ids[:Q]\n",
    "\n",
    "    # 3. repeat_token: single token repeated Q times\n",
    "    s['prefix_repeat_token'] = [1000] * Q\n",
    "\n",
    "    # 4. scrambled_oracle: random permutation of query IDs\n",
    "    shuffled = list(q_ids)\n",
    "    pyrandom.shuffle(shuffled)\n",
    "    s['prefix_scrambled_oracle'] = shuffled\n",
    "\n",
    "    # 5. unrelated_query: other sample's query, pad/trunc to Q\n",
    "    other_idx = (i + N_SAMPLES // 2) % len(samples)\n",
    "    other_q_ids = tokenizer(samples[other_idx]['query'],\n",
    "                            add_special_tokens=False).input_ids\n",
    "    s['prefix_unrelated_query'] = make_prefix(other_q_ids, Q)\n",
    "\n",
    "    # 6. same_topic: LLM-generated, pad/trunc to Q\n",
    "    topic_ids = tokenizer(surr['same_topic'], add_special_tokens=False).input_ids\n",
    "    s['prefix_same_topic'] = make_prefix(topic_ids, Q)\n",
    "\n",
    "    # 7. paraphrase: LLM-generated, pad/trunc to Q\n",
    "    para_ids = tokenizer(surr['paraphrase'], add_special_tokens=False).input_ids\n",
    "    s['prefix_paraphrase'] = make_prefix(para_ids, Q)\n",
    "\n",
    "    # 8. llm_extract: LLM doc-specific fact list, pad/trunc to Q\n",
    "    extract_ids = tokenizer(surr['llm_extract'], add_special_tokens=False).input_ids\n",
    "    s['prefix_llm_extract'] = make_prefix(extract_ids, Q)\n",
    "\n",
    "    # 9. llm_question: LLM doc-specific question, pad/trunc to Q\n",
    "    question_ids = tokenizer(surr['llm_question'], add_special_tokens=False).input_ids\n",
    "    s['prefix_llm_question'] = make_prefix(question_ids, Q)\n",
    "\n",
    "    # 10. llm_summarize: LLM doc-specific summary, pad/trunc to Q\n",
    "    summarize_ids = tokenizer(surr['llm_summarize'], add_special_tokens=False).input_ids\n",
    "    s['prefix_llm_summarize'] = make_prefix(summarize_ids, Q)\n",
    "\n",
    "    # 11. extractor_matched: fixed extraction text, pad/trunc to Q\n",
    "    s['prefix_extractor_matched'] = make_prefix(extractor_ids, Q)\n",
    "\n",
    "    # 12. adversarial_matched: fixed adversarial text, pad/trunc to Q\n",
    "    s['prefix_adversarial_matched'] = make_prefix(adversarial_ids, Q)\n",
    "\n",
    "# Verify all prefixes have exactly Q tokens\n",
    "PREFIX_KEYS = [\n",
    "    'prefix_oracle', 'prefix_random_tokens', 'prefix_repeat_token',\n",
    "    'prefix_scrambled_oracle', 'prefix_unrelated_query', 'prefix_same_topic',\n",
    "    'prefix_paraphrase', 'prefix_llm_extract', 'prefix_llm_question',\n",
    "    'prefix_llm_summarize', 'prefix_extractor_matched', 'prefix_adversarial_matched',\n",
    "]\n",
    "\n",
    "q_lens = [s['Q'] for s in samples]\n",
    "print(f\"Query token count — mean: {np.mean(q_lens):.1f}, \"\n",
    "      f\"median: {np.median(q_lens):.0f}, \"\n",
    "      f\"min: {np.min(q_lens)}, max: {np.max(q_lens)}\")\n",
    "\n",
    "errors = 0\n",
    "for i, s in enumerate(samples):\n",
    "    Q = s['Q']\n",
    "    for key in PREFIX_KEYS:\n",
    "        if len(s[key]) != Q:\n",
    "            print(f\"  ERROR: Sample {i} {key}: len={len(s[key])} != Q={Q}\")\n",
    "            errors += 1\n",
    "assert errors == 0, f\"{errors} prefix length mismatches!\"\n",
    "\n",
    "# Show examples\n",
    "for i in range(3):\n",
    "    Q = samples[i]['Q']\n",
    "    print(f\"\\nSample {i}: Q={Q}, query='{samples[i]['query'][:50]}...'\")\n",
    "    for key in PREFIX_KEYS:\n",
    "        label = key.replace('prefix_', '')\n",
    "        decoded = tokenizer.decode(samples[i][key][:8])\n",
    "        print(f\"  {label:<22}: {decoded}...\")\n",
    "\n",
    "print(f\"\\nAll {len(PREFIX_KEYS)} prefix types verified for {len(samples)} samples.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "736c8a1f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-21T12:41:46.087141Z",
     "iopub.status.busy": "2026-02-21T12:41:46.086862Z",
     "iopub.status.idle": "2026-02-21T12:41:50.623117Z",
     "shell.execute_reply": "2026-02-21T12:41:50.622399Z"
    },
    "papermill": {
     "duration": 4.543318,
     "end_time": "2026-02-21T12:41:50.624669",
     "exception": false,
     "start_time": "2026-02-21T12:41:46.081351",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "VALIDATION: BOS-Retained Repositioning with Token-Level Matching\n",
      "======================================================================\n",
      "\n",
      "--- Test 1: Bare two-phase matches single-pass ---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Single-pass NLL: 1.942079\n",
      "  Two-phase bare:  1.945658 (diff: 0.18%)\n",
      "  PASSED — bare matches single-pass within 0.18%\n",
      "\n",
      "--- Test 2: Prefixed scoring runs correctly ---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Bare:          0.7034\n",
      "  Oracle:        0.8882  delta=-0.1848\n",
      "  Random tokens: 0.5484  delta=+0.1551\n",
      "  PASSED — all NLLs in valid range\n",
      "\n",
      "--- Test 3: Token-matching invariant ---\n",
      "  All 12 prefixed conditions have Q=5 tokens for sample 0\n",
      "  PASSED\n",
      "\n",
      "--- Test 4: 5-sample bare vs oracle vs random_tokens ---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Sample 0: bare=0.7034, oracle=0.8882 (-0.1848), random=0.5484 (+0.1551)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Sample 1: bare=0.9199, oracle=1.2878 (-0.3679), random=0.8777 (+0.0422)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Sample 2: bare=1.5033, oracle=1.4933 (+0.0099), random=1.3840 (+0.1193)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Sample 3: bare=0.4759, oracle=0.6682 (-0.1923), random=0.4248 (+0.0511)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Sample 4: bare=5.4449, oracle=3.4125 (+2.0323), random=7.4476 (-2.0027)\n",
      "\n",
      "======================================================================\n",
      "ALL VALIDATION TESTS PASSED\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Scoring functions with BOS-retained repositioning + validation\n",
    "\n",
    "# --- RoPE repositioning helpers ---\n",
    "layer_types = getattr(text_cfg, 'layer_types', [])\n",
    "\n",
    "def build_layer_inv_freqs():\n",
    "    # Build per-layer-type inverse frequency tensors for RoPE rotation.\n",
    "    inv_freqs = {}\n",
    "    for lt, params in rope_params.items():\n",
    "        theta = params.get('rope_theta', 10000.0)\n",
    "        dim = text_cfg.head_dim\n",
    "        inv_freq = 1.0 / (theta ** (torch.arange(0, dim, 2, dtype=torch.float32, device=DEVICE) / dim))\n",
    "        inv_freqs[lt] = inv_freq\n",
    "    return inv_freqs\n",
    "\n",
    "LAYER_INV_FREQS = build_layer_inv_freqs()\n",
    "\n",
    "\n",
    "def rotate_half(x):\n",
    "    x1, x2 = x[..., :x.shape[-1]//2], x[..., x.shape[-1]//2:]\n",
    "    return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "\n",
    "def select_kv_cache(cache, indices):\n",
    "    # Select specific cache indices (e.g., BOS + doc, skipping prefix).\n",
    "    selected = DynamicCache()\n",
    "    idx_tensor = torch.tensor(indices, dtype=torch.long, device=DEVICE)\n",
    "    for i in range(len(cache.layers)):\n",
    "        k = cache.layers[i].keys[:, :, idx_tensor, :]\n",
    "        v = cache.layers[i].values[:, :, idx_tensor, :]\n",
    "        selected.update(k, v, i)\n",
    "    return selected\n",
    "\n",
    "\n",
    "def reposition_kv_cache(cache, old_positions, new_positions, bos_start=0):\n",
    "    # Reposition doc keys from old_positions to new_positions via RoPE rotation.\n",
    "    # BOS entry at bos_start is left untouched. Doc entries start at bos_start+1.\n",
    "    delta = new_positions - old_positions\n",
    "    for L in range(len(cache.layers)):\n",
    "        lt = layer_types[L]\n",
    "        inv_freq = LAYER_INV_FREQS[lt]\n",
    "        k = cache.layers[L].keys\n",
    "        doc_keys = k[:, :, bos_start + 1:, :]\n",
    "        freqs = torch.einsum('i,j->ij', delta.float(), inv_freq)\n",
    "        emb = torch.cat([freqs, freqs], dim=-1)\n",
    "        cos_delta = emb.cos().to(k.dtype).unsqueeze(0).unsqueeze(0)\n",
    "        sin_delta = emb.sin().to(k.dtype).unsqueeze(0).unsqueeze(0)\n",
    "        doc_keys_new = doc_keys * cos_delta + rotate_half(doc_keys) * sin_delta\n",
    "        cache.layers[L].keys = torch.cat([\n",
    "            k[:, :, :bos_start + 1, :],\n",
    "            doc_keys_new,\n",
    "        ], dim=2)\n",
    "    return cache\n",
    "\n",
    "\n",
    "def score(doc_text, query_text, answer_text, prefix_token_ids=None):\n",
    "    # BOS-retained repositioning.\n",
    "    #\n",
    "    # If prefix_token_ids provided:\n",
    "    #   Phase A: [BOS] + prefix_ids(Q) + [\\n] + doc_ids(D) at natural positions.\n",
    "    #   Select BOS(0) + doc(1+Q+NL .. end) from cache.\n",
    "    #   Reposition doc keys from [1+Q+NL, ..., Q+NL+D] to [1, ..., D].\n",
    "    #   Cache: 1+D entries (BOS at 0, doc at 1..D).\n",
    "    #\n",
    "    # Bare: [BOS + doc] with default positions. Cache: 1+D entries.\n",
    "    #\n",
    "    # Phase B: score [\\n + query + \\n + answer] at positions [D+1, ...]\n",
    "    #   cache_position auto-generated from cache length (= 1+D = D+1).\n",
    "\n",
    "    doc_ids = tokenizer(doc_text, add_special_tokens=False,\n",
    "                        truncation=True, max_length=1536).input_ids\n",
    "    D = len(doc_ids)\n",
    "\n",
    "    if prefix_token_ids is not None:\n",
    "        P = len(prefix_token_ids)\n",
    "        NL = len(NEWLINE_IDS)\n",
    "\n",
    "        cond_ids = [BOS_ID] + list(prefix_token_ids) + NEWLINE_IDS + doc_ids\n",
    "        with torch.no_grad():\n",
    "            pa = model(input_ids=torch.tensor([cond_ids], device=DEVICE),\n",
    "                       use_cache=True)\n",
    "        cache = pa.past_key_values\n",
    "        del pa\n",
    "\n",
    "        # Select BOS (index 0) + doc (indices 1+P+NL .. end)\n",
    "        keep_indices = [0] + list(range(1 + P + NL, len(cond_ids)))\n",
    "        cache = select_kv_cache(cache, keep_indices)\n",
    "\n",
    "        # Reposition doc keys from natural positions to bare positions\n",
    "        old_pos = torch.arange(1 + P + NL, 1 + P + NL + D, device=DEVICE)\n",
    "        new_pos = torch.arange(1, D + 1, device=DEVICE)\n",
    "        cache = reposition_kv_cache(cache, old_pos, new_pos, bos_start=0)\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            pa = model(input_ids=torch.tensor([[BOS_ID] + doc_ids], device=DEVICE),\n",
    "                       use_cache=True)\n",
    "        cache = pa.past_key_values\n",
    "        del pa\n",
    "\n",
    "    # Cache has 1+D entries. Phase B at D+1.\n",
    "    phase_b_start = D + 1\n",
    "\n",
    "    query_ids = tokenizer(\"\\n\" + query_text + \"\\n\",\n",
    "                          add_special_tokens=False).input_ids\n",
    "    answer_ids = tokenizer(answer_text, add_special_tokens=False,\n",
    "                           truncation=True, max_length=256).input_ids\n",
    "    if not answer_ids:\n",
    "        del cache\n",
    "        return 0.0\n",
    "\n",
    "    pb_ids = query_ids + answer_ids\n",
    "    pos = torch.arange(phase_b_start, phase_b_start + len(pb_ids), device=DEVICE)\n",
    "\n",
    "    # Phase B: NO explicit cache_position — auto-generated from cache length\n",
    "    with torch.no_grad():\n",
    "        pb = model(\n",
    "            input_ids=torch.tensor([pb_ids], device=DEVICE),\n",
    "            past_key_values=cache,\n",
    "            position_ids=pos.unsqueeze(0),\n",
    "            use_cache=False,\n",
    "        )\n",
    "\n",
    "    n_q = len(query_ids)\n",
    "    logits = pb.logits[0, n_q - 1:n_q - 1 + len(answer_ids), :].float()\n",
    "    targets = torch.tensor(answer_ids, device=DEVICE)\n",
    "    nll = -F.log_softmax(logits, dim=-1).gather(\n",
    "        1, targets.unsqueeze(1)).squeeze(1).mean().item()\n",
    "    del cache, pb\n",
    "    return nll\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# VALIDATION TESTS\n",
    "# ================================================================\n",
    "print(\"=\" * 70)\n",
    "print(\"VALIDATION: BOS-Retained Repositioning with Token-Level Matching\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# TEST 1: Bare two-phase matches single-pass\n",
    "print(\"\\n--- Test 1: Bare two-phase matches single-pass ---\")\n",
    "doc_text_t = \"The cat sat on the mat near the door of the house by the lake\"\n",
    "query_text_t = \"Where did the cat sit?\"\n",
    "answer_text_t = \"on the mat\"\n",
    "doc_ids_t = tokenizer(doc_text_t, add_special_tokens=False).input_ids\n",
    "D_t = len(doc_ids_t)\n",
    "query_ids_t = tokenizer(\"\\n\" + query_text_t + \"\\n\", add_special_tokens=False).input_ids\n",
    "answer_ids_t = tokenizer(answer_text_t, add_special_tokens=False).input_ids\n",
    "\n",
    "# Single-pass reference\n",
    "full_ids = [BOS_ID] + doc_ids_t + query_ids_t + answer_ids_t\n",
    "with torch.no_grad():\n",
    "    out_full = model(input_ids=torch.tensor([full_ids], device=DEVICE))\n",
    "n_ctx = 1 + D_t + len(query_ids_t)\n",
    "logits_full = out_full.logits[0, n_ctx - 1:n_ctx - 1 + len(answer_ids_t), :].float()\n",
    "targets_t = torch.tensor(answer_ids_t, device=DEVICE)\n",
    "nll_single = -F.log_softmax(logits_full, dim=-1).gather(\n",
    "    1, targets_t.unsqueeze(1)).squeeze(1).mean().item()\n",
    "del out_full\n",
    "\n",
    "# Two-phase bare\n",
    "nll_bare = score(doc_text_t, query_text_t, answer_text_t)\n",
    "\n",
    "diff_pct = abs(nll_single - nll_bare) / nll_single * 100\n",
    "print(f\"  Single-pass NLL: {nll_single:.6f}\")\n",
    "print(f\"  Two-phase bare:  {nll_bare:.6f} (diff: {diff_pct:.2f}%)\")\n",
    "assert diff_pct < 1.0, f\"Bare doesn't match single-pass: {diff_pct}%\"\n",
    "print(f\"  PASSED — bare matches single-pass within {diff_pct:.2f}%\")\n",
    "\n",
    "# TEST 2: Prefixed scoring runs without error\n",
    "print(\"\\n--- Test 2: Prefixed scoring runs correctly ---\")\n",
    "s = samples[0]\n",
    "nll_b = score(s['passage'], s['query'], s['answer'])\n",
    "nll_o = score(s['passage'], s['query'], s['answer'],\n",
    "              prefix_token_ids=s['prefix_oracle'])\n",
    "nll_r = score(s['passage'], s['query'], s['answer'],\n",
    "              prefix_token_ids=s['prefix_random_tokens'])\n",
    "print(f\"  Bare:          {nll_b:.4f}\")\n",
    "print(f\"  Oracle:        {nll_o:.4f}  delta={nll_b - nll_o:+.4f}\")\n",
    "print(f\"  Random tokens: {nll_r:.4f}  delta={nll_b - nll_r:+.4f}\")\n",
    "assert 0 < nll_b < 20, f\"Bare NLL out of range: {nll_b}\"\n",
    "assert 0 < nll_o < 20, f\"Oracle NLL out of range: {nll_o}\"\n",
    "assert 0 < nll_r < 20, f\"Random NLL out of range: {nll_r}\"\n",
    "print(\"  PASSED — all NLLs in valid range\")\n",
    "\n",
    "# TEST 3: Token-matching invariant (all prefixed conds have same Q)\n",
    "print(\"\\n--- Test 3: Token-matching invariant ---\")\n",
    "Q = s['Q']\n",
    "for key in PREFIX_KEYS:\n",
    "    assert len(s[key]) == Q, f\"{key}: {len(s[key])} != Q={Q}\"\n",
    "print(f\"  All 12 prefixed conditions have Q={Q} tokens for sample 0\")\n",
    "print(\"  PASSED\")\n",
    "\n",
    "# TEST 4: 5-sample quick check\n",
    "print(\"\\n--- Test 4: 5-sample bare vs oracle vs random_tokens ---\")\n",
    "for i in range(5):\n",
    "    st = samples[i]\n",
    "    nb_ = score(st['passage'], st['query'], st['answer'])\n",
    "    no_ = score(st['passage'], st['query'], st['answer'],\n",
    "                prefix_token_ids=st['prefix_oracle'])\n",
    "    nr_ = score(st['passage'], st['query'], st['answer'],\n",
    "                prefix_token_ids=st['prefix_random_tokens'])\n",
    "    print(f\"  Sample {i}: bare={nb_:.4f}, oracle={no_:.4f} ({nb_-no_:+.4f}), \"\n",
    "          f\"random={nr_:.4f} ({nb_-nr_:+.4f})\")\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ALL VALIDATION TESTS PASSED\")\n",
    "print(\"=\" * 70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e869066b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-21T12:41:50.636628Z",
     "iopub.status.busy": "2026-02-21T12:41:50.636320Z",
     "iopub.status.idle": "2026-02-21T13:02:46.581967Z",
     "shell.execute_reply": "2026-02-21T13:02:46.581119Z"
    },
    "papermill": {
     "duration": 1255.954825,
     "end_time": "2026-02-21T13:02:46.585220",
     "exception": false,
     "start_time": "2026-02-21T12:41:50.630395",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "SCORING ALL CONDITIONS\n",
      "======================================================================\n",
      "Starting fresh: 13 conditions x 400 samples = 5200 scorings\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fa503fc9d714b318d3b7689c5b6c85b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Scoring:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 20/400 | 1.0m | ETA 19.7m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 40/400 | 2.1m | ETA 18.8m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 60/400 | 3.1m | ETA 17.7m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 80/400 | 4.2m | ETA 16.7m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 100/400 | 5.2m | ETA 15.6m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 120/400 | 6.3m | ETA 14.6m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 140/400 | 7.3m | ETA 13.5m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 160/400 | 8.3m | ETA 12.5m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 180/400 | 9.4m | ETA 11.5m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 200/400 | 10.5m | ETA 10.5m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 220/400 | 11.5m | ETA 9.4m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 240/400 | 12.5m | ETA 8.4m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 260/400 | 13.6m | ETA 7.3m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 280/400 | 14.6m | ETA 6.3m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 300/400 | 15.7m | ETA 5.2m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 320/400 | 16.7m | ETA 4.2m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 340/400 | 17.8m | ETA 3.1m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 360/400 | 18.8m | ETA 2.1m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 380/400 | 19.9m | ETA 1.0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 400/400 | 20.9m | ETA 0.0m\n",
      "\n",
      "Scoring complete: 400 samples, 13 conditions in 20.9 min\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Scoring loop — 13 conditions x 400 samples\n",
    "print(\"=\" * 70)\n",
    "print(\"SCORING ALL CONDITIONS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "COND_NAMES = [\n",
    "    'bare', 'random_tokens', 'repeat_token', 'scrambled_oracle',\n",
    "    'unrelated_query', 'same_topic', 'paraphrase', 'oracle',\n",
    "    'llm_extract', 'llm_question', 'llm_summarize',\n",
    "    'extractor_matched', 'adversarial_matched',\n",
    "]\n",
    "\n",
    "# Map condition name -> prefix key in samples dict\n",
    "COND_PREFIX_MAP = {\n",
    "    'random_tokens': 'prefix_random_tokens',\n",
    "    'repeat_token': 'prefix_repeat_token',\n",
    "    'scrambled_oracle': 'prefix_scrambled_oracle',\n",
    "    'unrelated_query': 'prefix_unrelated_query',\n",
    "    'same_topic': 'prefix_same_topic',\n",
    "    'paraphrase': 'prefix_paraphrase',\n",
    "    'oracle': 'prefix_oracle',\n",
    "    'llm_extract': 'prefix_llm_extract',\n",
    "    'llm_question': 'prefix_llm_question',\n",
    "    'llm_summarize': 'prefix_llm_summarize',\n",
    "    'extractor_matched': 'prefix_extractor_matched',\n",
    "    'adversarial_matched': 'prefix_adversarial_matched',\n",
    "}\n",
    "\n",
    "SCORING_KEY = 'bos_retained_token_matched_v02'\n",
    "\n",
    "results = []\n",
    "start_idx = 0\n",
    "\n",
    "if CHECKPOINT_PATH.exists():\n",
    "    ckpt = json.loads(CHECKPOINT_PATH.read_text())\n",
    "    if ckpt.get('n_total') == N_SAMPLES and ckpt.get('scoring') == SCORING_KEY:\n",
    "        if len(ckpt.get('results', [])) > 0:\n",
    "            saved_queries = [r['query'][:50] for r in ckpt['results']]\n",
    "            current_queries = [s['query'][:50] for s in samples[:len(saved_queries)]]\n",
    "            if saved_queries == current_queries:\n",
    "                results = ckpt['results']\n",
    "                start_idx = len(results)\n",
    "                print(f\"Resuming from checkpoint: {start_idx}/{N_SAMPLES}\")\n",
    "\n",
    "if start_idx == 0:\n",
    "    print(f\"Starting fresh: {len(COND_NAMES)} conditions x {N_SAMPLES} samples \"\n",
    "          f\"= {len(COND_NAMES) * N_SAMPLES} scorings\")\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "for i in tqdm(range(start_idx, N_SAMPLES), initial=start_idx, total=N_SAMPLES,\n",
    "              desc=\"Scoring\"):\n",
    "    s = samples[i]\n",
    "    query = s['query']\n",
    "    passage = s['passage']\n",
    "    answer = s['answer']\n",
    "\n",
    "    result = {\n",
    "        'query': query,\n",
    "        'answer': answer,\n",
    "        'passage_words': s['word_count'],\n",
    "        'Q': s['Q'],\n",
    "    }\n",
    "\n",
    "    # bare — no prefix\n",
    "    result['nll_bare'] = score(passage, query, answer)\n",
    "\n",
    "    # All prefixed conditions\n",
    "    for cond_name, prefix_key in COND_PREFIX_MAP.items():\n",
    "        result[f'nll_{cond_name}'] = score(\n",
    "            passage, query, answer,\n",
    "            prefix_token_ids=s[prefix_key]\n",
    "        )\n",
    "\n",
    "    results.append(result)\n",
    "\n",
    "    if (i + 1) % 20 == 0 or i == N_SAMPLES - 1:\n",
    "        ckpt = {\n",
    "            'n_total': N_SAMPLES,\n",
    "            'scoring': SCORING_KEY,\n",
    "            'results': results,\n",
    "            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        }\n",
    "        CHECKPOINT_PATH.write_text(json.dumps(ckpt))\n",
    "        elapsed = time.time() - t0\n",
    "        done = i - start_idx + 1\n",
    "        eta = (N_SAMPLES - i - 1) * elapsed / done if done > 0 else 0\n",
    "        tqdm.write(f\"  Checkpoint {i+1}/{N_SAMPLES} | {elapsed/60:.1f}m | ETA {eta/60:.1f}m\")\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "elapsed = time.time() - t0\n",
    "print(f\"\\nScoring complete: {len(results)} samples, \"\n",
    "      f\"{len(COND_NAMES)} conditions in {elapsed/60:.1f} min\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "507183a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-21T13:02:46.598484Z",
     "iopub.status.busy": "2026-02-21T13:02:46.598198Z",
     "iopub.status.idle": "2026-02-21T13:02:46.831241Z",
     "shell.execute_reply": "2026-02-21T13:02:46.830538Z"
    },
    "papermill": {
     "duration": 0.241874,
     "end_time": "2026-02-21T13:02:46.832837",
     "exception": false,
     "start_time": "2026-02-21T13:02:46.590963",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "RESULTS (N=400)\n",
      "======================================================================\n",
      "\n",
      "--- PART 1: Condition Table ---\n",
      "\n",
      "  Condition                     NLL    Delta        d    Win%            p   sig\n",
      "  ------------------------------------------------------------------------------\n",
      "  bare                       1.4989       --       --      --           --    --\n",
      "  random_tokens              1.4503  +0.0486   +0.081   56.8%     1.08e-01    ns\n",
      "  repeat_token               1.4120  +0.0869   +0.147   59.5%     3.53e-03    **\n",
      "  scrambled_oracle           1.5505  -0.0516   -0.088   41.5%     7.87e-02    ns\n",
      "  unrelated_query            1.5519  -0.0530   -0.067   46.0%     1.84e-01    ns\n",
      "  same_topic                 1.5329  -0.0340   -0.050   48.2%     3.14e-01    ns\n",
      "  paraphrase                 1.4780  +0.0209   +0.029   51.0%     5.58e-01    ns\n",
      "  oracle                     1.6212  -0.1223   -0.151   32.0%     2.72e-03    **\n",
      "  llm_extract                1.4616  +0.0373   +0.070   56.2%     1.62e-01    ns\n",
      "  llm_question               1.5703  -0.0714   -0.098   42.2%     5.03e-02    ns\n",
      "  llm_summarize              1.4403  +0.0586   +0.091   61.0%     6.93e-02    ns\n",
      "  extractor_matched          1.3795  +0.1194   +0.132   63.5%     8.44e-03    **\n",
      "  adversarial_matched        1.4663  +0.0326   +0.041   62.7%     4.13e-01    ns\n",
      "\n",
      "--- PART 2: Semantic Gradient Test ---\n",
      "Relevance ordering: random_tokens(0) < scrambled(1) < unrelated(2) < same_topic(3) < paraphrase(4) < oracle(5)\n",
      "  [0] random_tokens          d=+0.0807\n",
      "  [1] scrambled_oracle       d=-0.0881\n",
      "  [2] unrelated_query        d=-0.0665\n",
      "  [3] same_topic             d=-0.0504\n",
      "  [4] paraphrase             d=+0.0293\n",
      "  [5] oracle                 d=-0.1509\n",
      "\n",
      "  Spearman rho (relevance rank vs d): rho=-0.429, p=0.3965 ns\n",
      "  --> FLAT: no clear gradient\n",
      "\n",
      "--- PART 3: Structural Decomposition ---\n",
      "  Oracle d:         -0.1509\n",
      "  Random tokens d:  +0.0807 (-53.5% of oracle)\n",
      "  Repeat token d:   +0.1468 (-97.3% of oracle)\n",
      "  --> Mixed: structure accounts for -53%\n",
      "\n",
      "--- PART 4: LLM Document-Specific vs Generic Task-Framing ---\n",
      "  doc-specific vs generic extraction:\n",
      "    llm_extract            d=+0.0700\n",
      "    extractor_matched      d=+0.1324\n",
      "    Paired diff: d=-0.0877, p=8.03e-02 ns, LLM wins 39.0%\n",
      "  doc question vs generic extraction:\n",
      "    llm_question           d=-0.0982\n",
      "    extractor_matched      d=+0.1324\n",
      "    Paired diff: d=-0.1765, p=4.65e-04 ***, LLM wins 30.8%\n",
      "  doc summary vs generic extraction:\n",
      "    llm_summarize          d=+0.0911\n",
      "    extractor_matched      d=+0.1324\n",
      "    Paired diff: d=-0.0715, p=1.53e-01 ns, LLM wins 40.5%\n",
      "\n",
      "--- PART 5: Hardness Interaction (5 quintiles x 13 conditions) ---\n",
      "  Quintile        N     bare  random_t  repeat_t  scramble  unrelate  same_top  paraphra    oracle  llm_extr  llm_ques  llm_summ  extracto  adversar\n",
      "  --------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "  Q1 easy        80    0.232    -0.318    -0.286    -0.404    -0.361    -0.448    -0.304    -0.499    -0.301    -0.406    -0.318    -0.236    -0.340\n",
      "  Q2             80    0.547    +0.032    +0.119    -0.415    -0.259    -0.288    -0.214    -0.559    -0.104    -0.221    -0.101    +0.121    -0.144\n",
      "  Q3             80    0.946    +0.127    +0.323    -0.288    -0.051    -0.012    +0.066    -0.600    +0.070    -0.255    +0.112    +0.407    +0.291\n",
      "  Q4             80    1.565    +0.023    +0.333    -0.184    -0.298    -0.250    -0.062    -0.376    -0.032    -0.282    -0.030    +0.060    -0.115\n",
      "  Q5 hard        80    4.204    +0.218    +0.195    +0.108    +0.244    +0.334    +0.291    +0.078    +0.356    +0.164    +0.493    +0.294    +0.347\n",
      "\n",
      "  Hardness-benefit correlations:\n",
      "    oracle                 rho=+0.110 (p=2.85e-02) *\n",
      "    random_tokens          rho=+0.306 (p=4.18e-10) ***\n",
      "    extractor_matched      rho=+0.280 (p=1.17e-08) ***\n",
      "    llm_extract            rho=+0.268 (p=5.51e-08) ***\n",
      "    paraphrase             rho=+0.331 (p=1.19e-11) ***\n",
      "\n",
      "--- PART 6: Per-Sample Ranking ---\n",
      "  Condition                  Best count   Best %  Mean rank\n",
      "  ----------------------------------------------------------\n",
      "  bare                               21     5.2%       7.21\n",
      "  random_tokens                      23     5.8%       6.32\n",
      "  repeat_token                       39     9.8%       6.18\n",
      "  scrambled_oracle                    8     2.0%       8.03\n",
      "  unrelated_query                    21     5.2%       7.49\n",
      "  same_topic                         21     5.2%       7.54\n",
      "  paraphrase                         23     5.8%       6.99\n",
      "  oracle                             11     2.8%       9.29\n",
      "  llm_extract                        22     5.5%       6.49\n",
      "  llm_question                       14     3.5%       7.95\n",
      "  llm_summarize                      27     6.8%       6.29\n",
      "  extractor_matched                 131    32.8%       5.21\n",
      "  adversarial_matched                39     9.8%       6.03\n",
      "\n",
      "--- PART 7: Document-Specific vs Generic Deep Dive ---\n",
      "For each sample: is llm_extract better than extractor_matched?\n",
      "  llm_extract wins: 39.0%\n",
      "  Mean advantage: -0.0821\n",
      "  Cohen's d: -0.0877\n",
      "\n",
      "  LLM advantage by hardness quintile:\n",
      "    Q1 easy      d=+0.0316, LLM wins 42.5%\n",
      "    Q2           d=-0.2470, LLM wins 40.0%\n",
      "    Q3           d=-0.3277, LLM wins 31.2%\n",
      "    Q4           d=-0.0967, LLM wins 36.2%\n",
      "    Q5 hard      d=-0.0703, LLM wins 45.0%\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Results & analysis\n",
    "print(\"=\" * 70)\n",
    "print(f\"RESULTS (N={len(results)})\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Build arrays for all conditions\n",
    "cond_arrays = {}\n",
    "for cond in COND_NAMES:\n",
    "    cond_arrays[cond] = np.array([r[f'nll_{cond}'] for r in results])\n",
    "\n",
    "bare = cond_arrays['bare']\n",
    "\n",
    "# ================================================================\n",
    "# PART 1: Basic condition table\n",
    "# ================================================================\n",
    "print(f\"\\n--- PART 1: Condition Table ---\")\n",
    "print(f\"\\n  {'Condition':<24} {'NLL':>8} {'Delta':>8} {'d':>8} {'Win%':>7} \"\n",
    "      f\"{'p':>12} {'sig':>5}\")\n",
    "print(f\"  {'-'*78}\")\n",
    "\n",
    "analysis = {}\n",
    "for cond in COND_NAMES:\n",
    "    nlls = cond_arrays[cond]\n",
    "    mean_nll = nlls.mean()\n",
    "    if cond == 'bare':\n",
    "        print(f\"  {cond:<24} {mean_nll:>8.4f} {'--':>8} {'--':>8} {'--':>7} \"\n",
    "              f\"{'--':>12} {'--':>5}\")\n",
    "        analysis[cond] = {'mean_nll': float(mean_nll)}\n",
    "    else:\n",
    "        diff = bare - nlls  # positive = condition has lower NLL (better)\n",
    "        d = cohens_d(diff)\n",
    "        win_pct = 100 * np.mean(diff > 0)\n",
    "        _, p_val = stats.ttest_1samp(diff, 0)\n",
    "        sig = '***' if p_val < 0.001 else '**' if p_val < 0.01 else '*' if p_val < 0.05 else 'ns'\n",
    "        print(f\"  {cond:<24} {mean_nll:>8.4f} {diff.mean():>+8.4f} {d:>+8.3f} \"\n",
    "              f\"{win_pct:>6.1f}% {p_val:>12.2e} {sig:>5}\")\n",
    "        analysis[cond] = {\n",
    "            'mean_nll': float(mean_nll), 'delta': float(diff.mean()),\n",
    "            'd': float(d), 'win_pct': float(win_pct), 'p': float(p_val),\n",
    "        }\n",
    "\n",
    "# ================================================================\n",
    "# PART 2: Semantic gradient test\n",
    "# ================================================================\n",
    "print(f\"\\n--- PART 2: Semantic Gradient Test ---\")\n",
    "print(\"Relevance ordering: random_tokens(0) < scrambled(1) < unrelated(2) \"\n",
    "      \"< same_topic(3) < paraphrase(4) < oracle(5)\")\n",
    "\n",
    "GRADIENT_CONDS = [\n",
    "    ('random_tokens', 0),\n",
    "    ('scrambled_oracle', 1),\n",
    "    ('unrelated_query', 2),\n",
    "    ('same_topic', 3),\n",
    "    ('paraphrase', 4),\n",
    "    ('oracle', 5),\n",
    "]\n",
    "\n",
    "gradient_ranks = []\n",
    "gradient_ds = []\n",
    "for cond, rank in GRADIENT_CONDS:\n",
    "    d = cohens_d(bare - cond_arrays[cond])\n",
    "    gradient_ranks.append(rank)\n",
    "    gradient_ds.append(d)\n",
    "    print(f\"  [{rank}] {cond:<22} d={d:+.4f}\")\n",
    "\n",
    "rho, p_mono = stats.spearmanr(gradient_ranks, gradient_ds)\n",
    "sig_mono = '***' if p_mono < 0.001 else '**' if p_mono < 0.01 else '*' if p_mono < 0.05 else 'ns'\n",
    "print(f\"\\n  Spearman rho (relevance rank vs d): rho={rho:+.3f}, p={p_mono:.4f} {sig_mono}\")\n",
    "\n",
    "if rho > 0.8 and p_mono < 0.05:\n",
    "    print(f\"  --> MONOTONIC: clear semantic gradient\")\n",
    "elif rho > 0.5:\n",
    "    print(f\"  --> PARTIAL: imperfect gradient\")\n",
    "else:\n",
    "    print(f\"  --> FLAT: no clear gradient\")\n",
    "\n",
    "# ================================================================\n",
    "# PART 3: Structural decomposition\n",
    "# ================================================================\n",
    "print(f\"\\n--- PART 3: Structural Decomposition ---\")\n",
    "oracle_d = cohens_d(bare - cond_arrays['oracle'])\n",
    "random_d = cohens_d(bare - cond_arrays['random_tokens'])\n",
    "repeat_d = cohens_d(bare - cond_arrays['repeat_token'])\n",
    "\n",
    "if oracle_d != 0:\n",
    "    structural_frac_random = random_d / oracle_d * 100\n",
    "    structural_frac_repeat = repeat_d / oracle_d * 100\n",
    "else:\n",
    "    structural_frac_random = structural_frac_repeat = float('nan')\n",
    "\n",
    "print(f\"  Oracle d:         {oracle_d:+.4f}\")\n",
    "print(f\"  Random tokens d:  {random_d:+.4f} ({structural_frac_random:.1f}% of oracle)\")\n",
    "print(f\"  Repeat token d:   {repeat_d:+.4f} ({structural_frac_repeat:.1f}% of oracle)\")\n",
    "\n",
    "if abs(structural_frac_random) > 80:\n",
    "    print(f\"  --> Structure dominates: random_tokens recovers {structural_frac_random:.0f}% of oracle\")\n",
    "elif abs(structural_frac_random) > 40:\n",
    "    print(f\"  --> Mixed: structure accounts for {structural_frac_random:.0f}%\")\n",
    "else:\n",
    "    print(f\"  --> Semantics dominate: structure only {structural_frac_random:.0f}%\")\n",
    "\n",
    "# ================================================================\n",
    "# PART 4: LLM surrogates vs fixed task-framing\n",
    "# ================================================================\n",
    "print(f\"\\n--- PART 4: LLM Document-Specific vs Generic Task-Framing ---\")\n",
    "\n",
    "# Paired comparisons\n",
    "pairs = [\n",
    "    ('llm_extract', 'extractor_matched', 'doc-specific vs generic extraction'),\n",
    "    ('llm_question', 'extractor_matched', 'doc question vs generic extraction'),\n",
    "    ('llm_summarize', 'extractor_matched', 'doc summary vs generic extraction'),\n",
    "]\n",
    "\n",
    "for llm_cond, generic_cond, desc in pairs:\n",
    "    diff = cond_arrays[generic_cond] - cond_arrays[llm_cond]\n",
    "    d = cohens_d(diff)\n",
    "    _, p = stats.ttest_1samp(diff, 0)\n",
    "    sig = '***' if p < 0.001 else '**' if p < 0.01 else '*' if p < 0.05 else 'ns'\n",
    "    win = 100 * np.mean(diff > 0)\n",
    "    print(f\"  {desc}:\")\n",
    "    print(f\"    {llm_cond:<22} d={cohens_d(bare - cond_arrays[llm_cond]):+.4f}\")\n",
    "    print(f\"    {generic_cond:<22} d={cohens_d(bare - cond_arrays[generic_cond]):+.4f}\")\n",
    "    print(f\"    Paired diff: d={d:+.4f}, p={p:.2e} {sig}, \"\n",
    "          f\"LLM wins {win:.1f}%\")\n",
    "\n",
    "# ================================================================\n",
    "# PART 5: Hardness interaction\n",
    "# ================================================================\n",
    "print(f\"\\n--- PART 5: Hardness Interaction (5 quintiles x 13 conditions) ---\")\n",
    "quintile_bounds = np.percentile(bare, [20, 40, 60, 80])\n",
    "quintiles = np.digitize(bare, quintile_bounds)\n",
    "q_labels = ['Q1 easy', 'Q2', 'Q3', 'Q4', 'Q5 hard']\n",
    "\n",
    "# Print header\n",
    "header = f\"  {'Quintile':<12} {'N':>4} {'bare':>8}\"\n",
    "for cond in COND_NAMES[1:]:\n",
    "    header += f\"  {cond[:8]:>8}\"\n",
    "print(header)\n",
    "print(f\"  {'-'*(16 + 10 * len(COND_NAMES))}\")\n",
    "\n",
    "hardness_data = {}\n",
    "for q in range(5):\n",
    "    mask = quintiles == q\n",
    "    n_q = mask.sum()\n",
    "    row = f\"  {q_labels[q]:<12} {n_q:>4} {bare[mask].mean():>8.3f}\"\n",
    "    hardness_data[q_labels[q]] = {}\n",
    "    for cond in COND_NAMES[1:]:\n",
    "        delta = (bare[mask] - cond_arrays[cond][mask]).mean()\n",
    "        d = cohens_d(bare[mask] - cond_arrays[cond][mask])\n",
    "        row += f\"  {d:>+8.3f}\"\n",
    "        hardness_data[q_labels[q]][cond] = float(d)\n",
    "    print(row)\n",
    "\n",
    "# Correlation: hardness vs benefit for key conditions\n",
    "print(f\"\\n  Hardness-benefit correlations:\")\n",
    "for cond in ['oracle', 'random_tokens', 'extractor_matched', 'llm_extract', 'paraphrase']:\n",
    "    diff = bare - cond_arrays[cond]\n",
    "    r_val, p_val = stats.spearmanr(bare, diff)\n",
    "    sig = '***' if p_val < 0.001 else '**' if p_val < 0.01 else '*' if p_val < 0.05 else 'ns'\n",
    "    print(f\"    {cond:<22} rho={r_val:+.3f} (p={p_val:.2e}) {sig}\")\n",
    "\n",
    "# ================================================================\n",
    "# PART 6: Per-sample ranking\n",
    "# ================================================================\n",
    "print(f\"\\n--- PART 6: Per-Sample Ranking ---\")\n",
    "\n",
    "stacked = np.stack([cond_arrays[c] for c in COND_NAMES], axis=1)\n",
    "best_idx = stacked.argmin(axis=1)\n",
    "\n",
    "print(f\"  {'Condition':<24} {'Best count':>12} {'Best %':>8} {'Mean rank':>10}\")\n",
    "print(f\"  {'-'*58}\")\n",
    "\n",
    "ranks = stacked.argsort(axis=1).argsort(axis=1) + 1\n",
    "mean_ranks = ranks.mean(axis=0)\n",
    "for ci, cname in enumerate(COND_NAMES):\n",
    "    count = (best_idx == ci).sum()\n",
    "    pct = 100 * count / len(best_idx)\n",
    "    print(f\"  {cname:<24} {count:>12} {pct:>7.1f}% {mean_ranks[ci]:>10.2f}\")\n",
    "\n",
    "# ================================================================\n",
    "# PART 7: Document-specific vs generic — deep dive\n",
    "# ================================================================\n",
    "print(f\"\\n--- PART 7: Document-Specific vs Generic Deep Dive ---\")\n",
    "print(\"For each sample: is llm_extract better than extractor_matched?\")\n",
    "\n",
    "llm_better = cond_arrays['extractor_matched'] - cond_arrays['llm_extract']\n",
    "print(f\"  llm_extract wins: {100 * np.mean(llm_better > 0):.1f}%\")\n",
    "print(f\"  Mean advantage: {llm_better.mean():+.4f}\")\n",
    "print(f\"  Cohen's d: {cohens_d(llm_better):+.4f}\")\n",
    "\n",
    "# By quintile\n",
    "print(f\"\\n  LLM advantage by hardness quintile:\")\n",
    "for q in range(5):\n",
    "    mask = quintiles == q\n",
    "    diff_q = llm_better[mask]\n",
    "    d_q = cohens_d(diff_q)\n",
    "    win_q = 100 * np.mean(diff_q > 0)\n",
    "    print(f\"    {q_labels[q]:<12} d={d_q:+.4f}, LLM wins {win_q:.1f}%\")\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9bfc4b4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-21T13:02:46.846409Z",
     "iopub.status.busy": "2026-02-21T13:02:46.846140Z",
     "iopub.status.idle": "2026-02-21T13:02:47.369190Z",
     "shell.execute_reply": "2026-02-21T13:02:47.368512Z"
    },
    "papermill": {
     "duration": 0.531807,
     "end_time": "2026-02-21T13:02:47.370820",
     "exception": false,
     "start_time": "2026-02-21T13:02:46.839013",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "VERDICT — Exp 02: Token-Matched Semantic Probing with LLM Surrogates\n",
      "======================================================================\n",
      "\n",
      "Model: google/gemma-3-12b-it\n",
      "Scoring: BOS-retained repositioning + token-level prefix matching\n",
      "N: 400 samples (MS MARCO v1.1)\n",
      "Conditions: 13\n",
      "\n",
      "--- Key findings ---\n",
      "  1. Oracle (token-matched): d=-0.1509 (**)\n",
      "\n",
      "  2. Semantic gradient: rho=-0.429 (p=0.3965)\n",
      "     -> PARTIAL: imperfect gradient\n",
      "\n",
      "  3. Structural fraction: -53.5% (random_tokens / oracle)\n",
      "\n",
      "  4. All conditions ranked by d:\n",
      "     repeat_token             d=+0.1468 (**)\n",
      "     extractor_matched        d=+0.1324 (**)\n",
      "     llm_summarize            d=+0.0911 (ns)\n",
      "     random_tokens            d=+0.0807 (ns)\n",
      "     llm_extract              d=+0.0700 (ns)\n",
      "     adversarial_matched      d=+0.0410 (ns)\n",
      "     paraphrase               d=+0.0293 (ns)\n",
      "     same_topic               d=-0.0504 (ns)\n",
      "     unrelated_query          d=-0.0665 (ns)\n",
      "     scrambled_oracle         d=-0.0881 (ns)\n",
      "     llm_question             d=-0.0982 (ns)\n",
      "     oracle                   d=-0.1509 (**)\n",
      "\n",
      "--- Conclusions ---\n",
      "  Oracle conditioning HURTS (d=-0.151).\n",
      "  Best condition: repeat_token (d=+0.147)\n",
      "  Worst condition: oracle (d=-0.151)\n",
      "\n",
      "Results saved to ../../../results/decoder_only/exp02/results.json\n",
      "\n",
      "Cleaning up GPU memory...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory: 24.39 GB -> 0.01 GB\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Verdict and save\n",
    "print(\"=\" * 70)\n",
    "print(\"VERDICT — Exp 02: Token-Matched Semantic Probing with LLM Surrogates\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\nModel: {MODEL_NAME}\")\n",
    "print(f\"Scoring: BOS-retained repositioning + token-level prefix matching\")\n",
    "print(f\"N: {len(results)} samples (MS MARCO v1.1)\")\n",
    "print(f\"Conditions: {len(COND_NAMES)}\")\n",
    "\n",
    "# Key results\n",
    "oracle_d = cohens_d(bare - cond_arrays['oracle'])\n",
    "_, p_oracle = stats.ttest_1samp(bare - cond_arrays['oracle'], 0)\n",
    "\n",
    "print(f\"\\n--- Key findings ---\")\n",
    "print(f\"  1. Oracle (token-matched): d={oracle_d:+.4f} \"\n",
    "      f\"({'***' if p_oracle < 0.001 else '**' if p_oracle < 0.01 else '*' if p_oracle < 0.05 else 'ns'})\")\n",
    "\n",
    "print(f\"\\n  2. Semantic gradient: rho={rho:+.3f} (p={p_mono:.4f})\")\n",
    "if rho > 0.8 and p_mono < 0.05:\n",
    "    print(f\"     -> MONOTONIC: semantic content drives the effect\")\n",
    "elif abs(rho) < 0.3:\n",
    "    print(f\"     -> FLAT: no semantic gradient\")\n",
    "else:\n",
    "    print(f\"     -> PARTIAL: imperfect gradient\")\n",
    "\n",
    "print(f\"\\n  3. Structural fraction: {structural_frac_random:.1f}% (random_tokens / oracle)\")\n",
    "\n",
    "print(f\"\\n  4. All conditions ranked by d:\")\n",
    "sorted_conds = sorted(\n",
    "    [(c, cohens_d(bare - cond_arrays[c])) for c in COND_NAMES if c != 'bare'],\n",
    "    key=lambda x: x[1], reverse=True\n",
    ")\n",
    "for cond, d in sorted_conds:\n",
    "    _, p = stats.ttest_1samp(bare - cond_arrays[cond], 0)\n",
    "    sig = '***' if p < 0.001 else '**' if p < 0.01 else '*' if p < 0.05 else 'ns'\n",
    "    print(f\"     {cond:<24} d={d:+.4f} ({sig})\")\n",
    "\n",
    "# Conclusions\n",
    "print(f\"\\n--- Conclusions ---\")\n",
    "if abs(oracle_d) < 0.05:\n",
    "    print(f\"  Oracle conditioning has negligible effect (d={oracle_d:+.3f}).\")\n",
    "elif oracle_d < -0.1:\n",
    "    print(f\"  Oracle conditioning HURTS (d={oracle_d:+.3f}).\")\n",
    "elif oracle_d > 0.1:\n",
    "    print(f\"  Oracle conditioning HELPS (d={oracle_d:+.3f}).\")\n",
    "else:\n",
    "    print(f\"  Oracle conditioning has weak effect (d={oracle_d:+.3f}).\")\n",
    "\n",
    "best_cond, best_d = sorted_conds[0]\n",
    "worst_cond, worst_d = sorted_conds[-1]\n",
    "print(f\"  Best condition: {best_cond} (d={best_d:+.3f})\")\n",
    "print(f\"  Worst condition: {worst_cond} (d={worst_d:+.3f})\")\n",
    "\n",
    "# Save\n",
    "final_results = {\n",
    "    'experiment': 'v4_exp02_token_matched_semantic_probing',\n",
    "    'model': MODEL_NAME,\n",
    "    'scoring': 'bos_retained_repositioning_token_matched',\n",
    "    'dataset': 'ms_marco_v1.1',\n",
    "    'n_samples': len(results),\n",
    "    'n_conditions': len(COND_NAMES),\n",
    "    'seed': SEED,\n",
    "    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'conditions': analysis,\n",
    "    'gradient': {\n",
    "        'spearman_rho': float(rho),\n",
    "        'spearman_p': float(p_mono),\n",
    "    },\n",
    "    'structural_decomposition': {\n",
    "        'oracle_d': float(oracle_d),\n",
    "        'random_tokens_d': float(random_d),\n",
    "        'repeat_token_d': float(repeat_d),\n",
    "        'structural_frac_random': float(structural_frac_random),\n",
    "        'structural_frac_repeat': float(structural_frac_repeat),\n",
    "    },\n",
    "    'hardness_interaction': hardness_data,\n",
    "    'per_sample_mean_ranks': {\n",
    "        cond: float(mean_ranks[ci]) for ci, cond in enumerate(COND_NAMES)\n",
    "    },\n",
    "    'query_token_stats': {\n",
    "        'mean': float(np.mean([r['Q'] for r in results])),\n",
    "        'median': float(np.median([r['Q'] for r in results])),\n",
    "        'min': int(np.min([r['Q'] for r in results])),\n",
    "        'max': int(np.max([r['Q'] for r in results])),\n",
    "    },\n",
    "}\n",
    "\n",
    "with open(RESULTS_DIR / 'results.json', 'w') as f:\n",
    "    json.dump(final_results, f, indent=2)\n",
    "print(f\"\\nResults saved to {RESULTS_DIR / 'results.json'}\")\n",
    "\n",
    "# Cleanup\n",
    "print(f\"\\nCleaning up GPU memory...\")\n",
    "mem_before = torch.cuda.memory_allocated() / 1e9\n",
    "del model, tokenizer\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "mem_after = torch.cuda.memory_allocated() / 1e9\n",
    "print(f\"GPU memory: {mem_before:.2f} GB -> {mem_after:.2f} GB\")\n",
    "print(\"Done!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 5517.107281,
   "end_time": "2026-02-21T13:02:50.525976",
   "environment_variables": {},
   "exception": null,
   "input_path": "02_token_matched_semantic.ipynb",
   "output_path": "02_token_matched_semantic_executed.ipynb",
   "parameters": {},
   "start_time": "2026-02-21T11:30:53.418695",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0dfe8b17a0824c548c2afcedb106b513": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "133e1d9132484da18c090d570826eca7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "188aac60bb73493b8da0e33894b39555": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_78701e959eb94188948a321a7b1d3902",
       "placeholder": "​",
       "style": "IPY_MODEL_5ac21e961378444fbd926e39d700e3ea",
       "tabbable": null,
       "tooltip": null,
       "value": " 400/400 [20:55&lt;00:00,  3.16s/it]"
      }
     },
     "2d0940fab6aa441e94ded9c8b7dfc0da": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3544f7def5b44447b749ee46147aafac": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_ba22355aa478404f83a7ffdcf9f85287",
        "IPY_MODEL_36ad80555bb14a77b7a11846ecdc92f5",
        "IPY_MODEL_645dfdcd60864749a2f65e04d9eece14"
       ],
       "layout": "IPY_MODEL_ca938ca16d7b45cf9b6b10c2004b6a98",
       "tabbable": null,
       "tooltip": null
      }
     },
     "36ad80555bb14a77b7a11846ecdc92f5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_fbd44a797fd2408db8d5cf70df696e6a",
       "max": 1065.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_5c46fa1361df43d4b21e45a4e96ed118",
       "tabbable": null,
       "tooltip": null,
       "value": 1065.0
      }
     },
     "3ce422b755a64cccada3a684dfb33ad3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_e088b443e62a4ca2980e10f67ba0fe5c",
       "placeholder": "​",
       "style": "IPY_MODEL_c43e963d081c456d9072487b5a539f04",
       "tabbable": null,
       "tooltip": null,
       "value": "Scoring: 100%"
      }
     },
     "5ac21e961378444fbd926e39d700e3ea": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "5c46fa1361df43d4b21e45a4e96ed118": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "61c041e0db1446bfad08a95fea3cfac9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "645dfdcd60864749a2f65e04d9eece14": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_ab48492e98f1457699c69ab4c8079115",
       "placeholder": "​",
       "style": "IPY_MODEL_133e1d9132484da18c090d570826eca7",
       "tabbable": null,
       "tooltip": null,
       "value": " 1065/1065 [00:06&lt;00:00, 637.76it/s, Materializing param=model.vision_tower.vision_model.post_layernorm.weight]"
      }
     },
     "6aef7832566341cc93f6314a87a6ee71": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_e01e7df4bd404ecab13eb093931461ec",
       "placeholder": "​",
       "style": "IPY_MODEL_0dfe8b17a0824c548c2afcedb106b513",
       "tabbable": null,
       "tooltip": null,
       "value": "Generating surrogates: 100%"
      }
     },
     "74be43fc3daa48d0a74c095f858c3cbb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "78701e959eb94188948a321a7b1d3902": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "8609923957564050bbd3a79d79d63359": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "884b3789b8e545e39bfc269db0cf2a76": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "8ca98cfdf5aa4115b2d0a2a3c03a8410": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "914aafae058f46378365a5c6fda5e18a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "9a4c742e1ba747949eb5a162631f4934": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_c999a689d78149869c27ea757add51b7",
       "placeholder": "​",
       "style": "IPY_MODEL_8609923957564050bbd3a79d79d63359",
       "tabbable": null,
       "tooltip": null,
       "value": " 400/400 [1:10:30&lt;00:00, 10.44s/it]"
      }
     },
     "9fa503fc9d714b318d3b7689c5b6c85b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_3ce422b755a64cccada3a684dfb33ad3",
        "IPY_MODEL_e8c5b90998584fe186c688ea9503f01b",
        "IPY_MODEL_188aac60bb73493b8da0e33894b39555"
       ],
       "layout": "IPY_MODEL_f08f7f1c70194936bf763df914563126",
       "tabbable": null,
       "tooltip": null
      }
     },
     "ab48492e98f1457699c69ab4c8079115": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b3e0d4f7ad5e475da2989d8882c7ea74": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "ba22355aa478404f83a7ffdcf9f85287": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_884b3789b8e545e39bfc269db0cf2a76",
       "placeholder": "​",
       "style": "IPY_MODEL_74be43fc3daa48d0a74c095f858c3cbb",
       "tabbable": null,
       "tooltip": null,
       "value": "Loading weights: 100%"
      }
     },
     "c2a64dcff9a24379a654d2e1a6c17079": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_6aef7832566341cc93f6314a87a6ee71",
        "IPY_MODEL_da2f87588ed74c399ce45231bbfd812b",
        "IPY_MODEL_9a4c742e1ba747949eb5a162631f4934"
       ],
       "layout": "IPY_MODEL_61c041e0db1446bfad08a95fea3cfac9",
       "tabbable": null,
       "tooltip": null
      }
     },
     "c43e963d081c456d9072487b5a539f04": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "c999a689d78149869c27ea757add51b7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ca938ca16d7b45cf9b6b10c2004b6a98": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "da2f87588ed74c399ce45231bbfd812b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_2d0940fab6aa441e94ded9c8b7dfc0da",
       "max": 400.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_b3e0d4f7ad5e475da2989d8882c7ea74",
       "tabbable": null,
       "tooltip": null,
       "value": 400.0
      }
     },
     "e01e7df4bd404ecab13eb093931461ec": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e088b443e62a4ca2980e10f67ba0fe5c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e8c5b90998584fe186c688ea9503f01b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_8ca98cfdf5aa4115b2d0a2a3c03a8410",
       "max": 400.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_914aafae058f46378365a5c6fda5e18a",
       "tabbable": null,
       "tooltip": null,
       "value": 400.0
      }
     },
     "f08f7f1c70194936bf763df914563126": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "fbd44a797fd2408db8d5cf70df696e6a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}