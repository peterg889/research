{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ebb61db",
   "metadata": {},
   "source": [
    "# Decoder-Only Exp 02: Length vs Content Decomposition\n",
    "\n",
    "## Motivation\n",
    "\n",
    "Exp 01 (Gemma 2 2B) found that **all surrogates beat the oracle** (recovery 115-146%),\n",
    "including a completely off-topic adversarial prefix (124%). This suggests the effect\n",
    "is primarily structural — any prefix text enriches document KV representations.\n",
    "\n",
    "This experiment disentangles **prefix length** from **prefix content** using a larger\n",
    "model (Gemma 3 4B-PT) with 10 conditions designed as a controlled factorial.\n",
    "\n",
    "## Conditions (10 total)\n",
    "\n",
    "### Baselines\n",
    "| # | Condition | Description |\n",
    "|---|-----------|-------------|\n",
    "| 1 | bare | No prefix — lower bound |\n",
    "| 2 | oracle | Real query as prefix — upper bound |\n",
    "\n",
    "### Content at surrogate length (~15 words)\n",
    "| # | Condition | Description |\n",
    "|---|-----------|-------------|\n",
    "| 3 | surr_reasonant | \"Evaluate the underlying arguments...\" (best from exp01) |\n",
    "| 4 | surr_universal | \"Analyze the following text...\" |\n",
    "| 5 | adversarial | Off-topic text (~15 words) |\n",
    "\n",
    "### Length sweep (random words, varying count)\n",
    "| # | Condition | Description |\n",
    "|---|-----------|-------------|\n",
    "| 6 | random_matched | Random words, same count as query (per-sample) |\n",
    "| 7 | random_15w | 15 random words |\n",
    "| 8 | random_30w | 30 random words |\n",
    "\n",
    "### Controls\n",
    "| # | Condition | Description |\n",
    "|---|-----------|-------------|\n",
    "| 9 | repeat_15w | \"the\" × 15 — minimal content, same length |\n",
    "| 10 | oracle_padded | Query + \"the\" padding to 15 words |\n",
    "\n",
    "## Key comparisons\n",
    "\n",
    "1. **Pure length**: random_matched → random_15w → random_30w\n",
    "2. **Content at oracle's length**: oracle vs random_matched\n",
    "3. **Content at surrogate length**: surr_reasonant vs random_15w vs adversarial\n",
    "4. **Token diversity**: repeat_15w vs random_15w (both 15 words)\n",
    "5. **Oracle + length boost**: oracle_padded vs oracle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a775cf7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Setup and model loading\n",
    "import os\n",
    "os.umask(0o000)\n",
    "\n",
    "import sys, json, time, gc, re\n",
    "import random as pyrandom\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from scipy import stats\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "sys.path.insert(0, \"../../..\")\n",
    "from lib.analysis import cohens_d\n",
    "\n",
    "SEED = 42\n",
    "N_SAMPLES = 400\n",
    "MODEL_NAME = \"google/gemma-3-4b-it\"\n",
    "\n",
    "RESULTS_DIR = Path(\"../../../results/decoder_only/exp02\")\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CHECKPOINT_PATH = RESULTS_DIR / \"checkpoint.json\"\n",
    "\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "pyrandom.seed(SEED)\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv())\n",
    "HF_TOKEN = os.environ.get(\"HF_TOKEN\")\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "print(f\"Loading {MODEL_NAME}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, token=HF_TOKEN)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME, device_map=\"auto\", torch_dtype=torch.bfloat16, token=HF_TOKEN,\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "DEVICE = next(model.parameters()).device\n",
    "\n",
    "print(f\"Exp 02: Length vs Content Decomposition\")\n",
    "print(f\"N: {N_SAMPLES}, Model: {MODEL_NAME}\")\n",
    "print(f\"DEVICE: {DEVICE}, dtype: {next(model.parameters()).dtype}\")\n",
    "print(f\"GPU memory: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "print(f\"Config: {type(model.config).__name__}\")\n",
    "text_cfg = getattr(model.config, 'text_config', model.config)\n",
    "print(f\"Vocab size: {getattr(text_cfg, 'vocab_size', 'N/A')}\")\n",
    "print(f\"Num layers: {getattr(text_cfg, 'num_hidden_layers', 'N/A')}\")\n",
    "print(f\"Num KV heads: {getattr(text_cfg, 'num_key_value_heads', 'N/A')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2d5428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: KV cache helpers and scoring function\n",
    "\n",
    "def slice_kv_cache(cache, start_idx):\n",
    "    # Remove first start_idx entries from KV cache.\n",
    "    from transformers import DynamicCache\n",
    "\n",
    "    if isinstance(cache, DynamicCache):\n",
    "        sliced = DynamicCache()\n",
    "        for i in range(len(cache.layers)):\n",
    "            k = cache.layers[i].keys[:, :, start_idx:, :]\n",
    "            v = cache.layers[i].values[:, :, start_idx:, :]\n",
    "            sliced.update(k, v, i)\n",
    "        return sliced\n",
    "    else:\n",
    "        return tuple(\n",
    "            (k[:, :, start_idx:, :], v[:, :, start_idx:, :])\n",
    "            for k, v in cache\n",
    "        )\n",
    "\n",
    "\n",
    "def score(doc_text, query_text, answer_text, prefix_text=None):\n",
    "    # Score NLL of answer tokens using two-phase KV cache approach.\n",
    "    #\n",
    "    # Phase A: Forward [prefix + doc] (or just [doc]) -> KV cache.\n",
    "    # Phase B: Forward [query + answer] using cached doc KV.\n",
    "    # If prefix_text is provided, prefix KV entries are sliced off.\n",
    "\n",
    "    # --- Phase A: Conditioning ---\n",
    "    if prefix_text:\n",
    "        prefix_ids = tokenizer(prefix_text + \"\\n\", add_special_tokens=True,\n",
    "                               truncation=True, max_length=512).input_ids\n",
    "        doc_ids = tokenizer(doc_text, add_special_tokens=False,\n",
    "                            truncation=True, max_length=1536).input_ids\n",
    "        cond_ids = prefix_ids + doc_ids\n",
    "        slice_start = len(prefix_ids)\n",
    "    else:\n",
    "        cond_ids = tokenizer(doc_text, add_special_tokens=True,\n",
    "                             truncation=True, max_length=2048).input_ids\n",
    "        slice_start = 0\n",
    "\n",
    "    cond_tensor = torch.tensor([cond_ids], dtype=torch.long, device=DEVICE)\n",
    "    total_cond_len = len(cond_ids)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        phase_a = model(input_ids=cond_tensor, use_cache=True)\n",
    "\n",
    "    cache = phase_a.past_key_values\n",
    "    del phase_a\n",
    "\n",
    "    if slice_start > 0:\n",
    "        cache = slice_kv_cache(cache, slice_start)\n",
    "\n",
    "    # --- Phase B: Inference with query + answer ---\n",
    "    query_part_ids = tokenizer(\"\\n\" + query_text + \"\\n\", add_special_tokens=False).input_ids\n",
    "    answer_ids = tokenizer(answer_text, add_special_tokens=False,\n",
    "                           truncation=True, max_length=256).input_ids\n",
    "\n",
    "    if not answer_ids:\n",
    "        del cache\n",
    "        return 0.0\n",
    "\n",
    "    phase_b_ids = query_part_ids + answer_ids\n",
    "    phase_b_tensor = torch.tensor([phase_b_ids], dtype=torch.long, device=DEVICE)\n",
    "\n",
    "    pos_ids = torch.arange(total_cond_len, total_cond_len + len(phase_b_ids),\n",
    "                           device=DEVICE).unsqueeze(0)\n",
    "\n",
    "    cache_position = torch.arange(total_cond_len, total_cond_len + len(phase_b_ids),\n",
    "                                  device=DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        phase_b = model(\n",
    "            input_ids=phase_b_tensor,\n",
    "            past_key_values=cache,\n",
    "            position_ids=pos_ids,\n",
    "            cache_position=cache_position,\n",
    "            use_cache=False,\n",
    "        )\n",
    "\n",
    "    logits = phase_b.logits\n",
    "    n_query_part = len(query_part_ids)\n",
    "    n_answer = len(answer_ids)\n",
    "\n",
    "    answer_logits = logits[0, n_query_part - 1 : n_query_part - 1 + n_answer, :]\n",
    "    answer_targets = torch.tensor(answer_ids, dtype=torch.long, device=DEVICE)\n",
    "\n",
    "    log_probs = F.log_softmax(answer_logits, dim=-1)\n",
    "    token_log_probs = log_probs.gather(1, answer_targets.unsqueeze(1)).squeeze(1)\n",
    "    mean_nll = -token_log_probs.mean().item()\n",
    "\n",
    "    del cache, phase_b, logits, log_probs\n",
    "    return mean_nll\n",
    "\n",
    "\n",
    "def score_full_sequence(doc_text, query_text, answer_text):\n",
    "    # Single-pass scoring for validation.\n",
    "    doc_ids = tokenizer(doc_text, add_special_tokens=True,\n",
    "                        truncation=True, max_length=2048).input_ids\n",
    "    query_part_ids = tokenizer(\"\\n\" + query_text + \"\\n\", add_special_tokens=False).input_ids\n",
    "    answer_ids = tokenizer(answer_text, add_special_tokens=False,\n",
    "                           truncation=True, max_length=256).input_ids\n",
    "\n",
    "    if not answer_ids:\n",
    "        return 0.0\n",
    "\n",
    "    all_ids = doc_ids + query_part_ids + answer_ids\n",
    "    input_tensor = torch.tensor([all_ids], dtype=torch.long, device=DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_tensor, use_cache=False)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    n_doc = len(doc_ids)\n",
    "    n_query = len(query_part_ids)\n",
    "    n_answer = len(answer_ids)\n",
    "\n",
    "    start = n_doc + n_query - 1\n",
    "    answer_logits = logits[0, start : start + n_answer, :]\n",
    "    answer_targets = torch.tensor(answer_ids, dtype=torch.long, device=DEVICE)\n",
    "\n",
    "    log_probs = F.log_softmax(answer_logits, dim=-1)\n",
    "    token_log_probs = log_probs.gather(1, answer_targets.unsqueeze(1)).squeeze(1)\n",
    "    mean_nll = -token_log_probs.mean().item()\n",
    "\n",
    "    del outputs, logits, log_probs\n",
    "    return mean_nll\n",
    "\n",
    "\n",
    "# === Surrogate definitions ===\n",
    "SURR_REASONANT = \"Evaluate the underlying arguments, sentiment, and intent of the following passage.\"\n",
    "SURR_UNIVERSAL = \"Analyze the following text for all key entities, factual claims, and logical relationships.\"\n",
    "ADVERSARIAL_15W = \"The recipe calls for two cups of flour, one cup of sugar, a pinch of salt, and some butter.\"\n",
    "\n",
    "STOP_WORDS = {\n",
    "    'a', 'an', 'the', 'is', 'are', 'was', 'were', 'be', 'been', 'being',\n",
    "    'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could',\n",
    "    'should', 'may', 'might', 'can', 'shall', 'to', 'of', 'in', 'for',\n",
    "    'on', 'with', 'at', 'by', 'from', 'as', 'into', 'through', 'during',\n",
    "    'before', 'after', 'above', 'below', 'between', 'and', 'but', 'or',\n",
    "    'not', 'no', 'if', 'then', 'than', 'so', 'up', 'out', 'about',\n",
    "    'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those',\n",
    "    'it', 'its', 'i', 'me', 'my', 'we', 'our', 'you', 'your', 'he',\n",
    "    'him', 'his', 'she', 'her', 'they', 'them', 'their', 'how', 'when',\n",
    "    'where', 'why', 'much', 'many', 'some', 'any', 'all', 'each',\n",
    "    'does', 'also', 'just', 'more', 'most', 'very', 'too', 'only',\n",
    "}\n",
    "\n",
    "def extract_keywords(text):\n",
    "    words = re.sub(r'[^\\w\\s]', '', text.lower()).split()\n",
    "    return [w for w in words if w not in STOP_WORDS and len(w) > 2]\n",
    "\n",
    "print(\"Scoring functions defined.\")\n",
    "print(f\"\\nPrefix token counts:\")\n",
    "for name, text in [('surr_reasonant', SURR_REASONANT),\n",
    "                    ('surr_universal', SURR_UNIVERSAL),\n",
    "                    ('adversarial_15w', ADVERSARIAL_15W)]:\n",
    "    n_tok = len(tokenizer(text, add_special_tokens=False).input_ids)\n",
    "    n_words = len(text.split())\n",
    "    print(f\"  {name:<20} {n_words:>3}w {n_tok:>3}tok: {text[:50]}...\")\n",
    "print(f\"  {'repeat_15w':<20} {'15':>3}w {'?':>3}tok: the the the the the ...\")\n",
    "the_15 = \" \".join([\"the\"] * 15)\n",
    "n_tok_the = len(tokenizer(the_15, add_special_tokens=False).input_ids)\n",
    "print(f\"  {'':>24} -> {n_tok_the} tokens\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e6d2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Load MS MARCO data and generate per-sample prefixes\n",
    "from lib.data import count_words\n",
    "from datasets import load_dataset\n",
    "\n",
    "print(\"Loading MS MARCO v1.1 validation...\")\n",
    "ds = load_dataset(\"microsoft/ms_marco\", \"v1.1\", split=\"validation\")\n",
    "\n",
    "all_candidates = []\n",
    "for item in ds:\n",
    "    if len(all_candidates) >= 3 * N_SAMPLES:\n",
    "        break\n",
    "    passages = item.get('passages', {})\n",
    "    ptexts = passages.get('passage_text', [])\n",
    "    is_sel = passages.get('is_selected', [])\n",
    "    query = item.get('query', '')\n",
    "    answers = item.get('answers', [])\n",
    "    well_formed = item.get('wellFormedAnswers', [])\n",
    "    answer = None\n",
    "    if well_formed and len(well_formed) > 0 and well_formed[0] not in ('[]', ''):\n",
    "        answer = well_formed[0]\n",
    "    elif answers and len(answers) > 0 and answers[0] != 'No Answer Present.':\n",
    "        answer = answers[0]\n",
    "    if not answer:\n",
    "        continue\n",
    "    for pt, sel in zip(ptexts, is_sel):\n",
    "        wc = count_words(pt)\n",
    "        if sel == 1 and 30 <= wc <= 300:\n",
    "            all_candidates.append({\n",
    "                'passage': pt, 'query': query, 'answer': answer,\n",
    "                'word_count': wc,\n",
    "            })\n",
    "            break\n",
    "\n",
    "print(f\"Total candidates: {len(all_candidates)}\")\n",
    "np.random.seed(SEED)\n",
    "indices = np.random.permutation(len(all_candidates))\n",
    "samples = [all_candidates[i] for i in indices[:N_SAMPLES]]\n",
    "del ds, all_candidates\n",
    "gc.collect()\n",
    "\n",
    "# Build a pool of random English words from OTHER passages for random prefixes.\n",
    "# Use the second half of the shuffled data as the word pool (no overlap).\n",
    "word_pool = []\n",
    "for i in indices[N_SAMPLES:N_SAMPLES + 500]:\n",
    "    words = samples[0]['passage'].split() if i >= len(samples) else []\n",
    "    # Use all_candidates — but it's deleted. Rebuild from samples neighbors.\n",
    "    pass\n",
    "\n",
    "# Simpler: collect words from all passages, shuffle once\n",
    "all_words = []\n",
    "for s in samples:\n",
    "    all_words.extend(s['passage'].split())\n",
    "pyrandom.seed(SEED + 99)\n",
    "pyrandom.shuffle(all_words)\n",
    "word_pool = all_words  # ~30k words, plenty for random prefixes\n",
    "\n",
    "# Generate per-sample prefixes\n",
    "for i, s in enumerate(samples):\n",
    "    query_wc = len(s['query'].split())\n",
    "\n",
    "    # random_matched: same word count as query, from word pool\n",
    "    pool_offset = i * 50  # each sample gets a different slice\n",
    "    s['random_matched'] = \" \".join(word_pool[pool_offset:pool_offset + query_wc])\n",
    "\n",
    "    # random_15w: 15 words from pool\n",
    "    s['random_15w'] = \" \".join(word_pool[pool_offset + 50:pool_offset + 65])\n",
    "\n",
    "    # random_30w: 30 words from pool\n",
    "    s['random_30w'] = \" \".join(word_pool[pool_offset + 65:pool_offset + 95])\n",
    "\n",
    "    # repeat_15w: \"the\" x 15\n",
    "    s['repeat_15w'] = \" \".join([\"the\"] * 15)\n",
    "\n",
    "    # oracle_padded: query + \"the\" to reach 15 words\n",
    "    pad_count = max(0, 15 - query_wc)\n",
    "    s['oracle_padded'] = s['query'] + \" \" + \" \".join([\"the\"] * pad_count)\n",
    "\n",
    "print(f\"Loaded {len(samples)} samples\")\n",
    "print(f\"Mean passage words: {np.mean([s['word_count'] for s in samples]):.0f}\")\n",
    "print(f\"Mean query words: {np.mean([len(s['query'].split()) for s in samples]):.1f}\")\n",
    "\n",
    "# Show prefix word count stats\n",
    "print(f\"\\nPrefix word counts:\")\n",
    "for key in ['random_matched', 'random_15w', 'random_30w', 'repeat_15w', 'oracle_padded']:\n",
    "    wcs = [len(s[key].split()) for s in samples]\n",
    "    print(f\"  {key:<20} mean={np.mean(wcs):.1f}, range=[{min(wcs)}, {max(wcs)}]\")\n",
    "\n",
    "print(f\"\\nSample 0:\")\n",
    "print(f\"  Query ({len(samples[0]['query'].split())}w): {samples[0]['query']}\")\n",
    "print(f\"  random_matched:     {samples[0]['random_matched']}\")\n",
    "print(f\"  random_15w:         {samples[0]['random_15w'][:60]}...\")\n",
    "print(f\"  random_30w:         {samples[0]['random_30w'][:60]}...\")\n",
    "print(f\"  repeat_15w:         {samples[0]['repeat_15w']}\")\n",
    "print(f\"  oracle_padded:      {samples[0]['oracle_padded']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04bc5a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Validate two-phase caching vs single-pass\n",
    "print(\"=\" * 70)\n",
    "print(\"VALIDATION: Two-phase caching vs single-pass\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nComparing bare score (cached) vs full-sequence for 5 samples...\")\n",
    "max_diff = 0.0\n",
    "for i in range(5):\n",
    "    s = samples[i]\n",
    "    nll_cached = score(s['passage'], s['query'], s['answer'], prefix_text=None)\n",
    "    nll_full = score_full_sequence(s['passage'], s['query'], s['answer'])\n",
    "    diff = abs(nll_cached - nll_full)\n",
    "    max_diff = max(max_diff, diff)\n",
    "    status = \"OK\" if diff < 0.01 else \"MISMATCH\"\n",
    "    print(f\"  Sample {i}: cached={nll_cached:.6f}, full={nll_full:.6f}, \"\n",
    "          f\"diff={diff:.8f} [{status}]\")\n",
    "\n",
    "if max_diff < 0.01:\n",
    "    print(f\"\\nVALIDATION PASSED (max diff = {max_diff:.8f})\")\n",
    "else:\n",
    "    print(f\"\\nWARNING: max diff = {max_diff:.8f}\")\n",
    "\n",
    "# Quick sanity: conditioned vs bare\n",
    "print(f\"\\nQuick sanity check (sample 0):\")\n",
    "nll_bare = score(samples[0]['passage'], samples[0]['query'], samples[0]['answer'])\n",
    "nll_oracle = score(samples[0]['passage'], samples[0]['query'], samples[0]['answer'],\n",
    "                   prefix_text=samples[0]['query'])\n",
    "nll_random = score(samples[0]['passage'], samples[0]['query'], samples[0]['answer'],\n",
    "                   prefix_text=samples[0]['random_15w'])\n",
    "print(f\"  bare:      {nll_bare:.6f}\")\n",
    "print(f\"  oracle:    {nll_oracle:.6f} (delta: {nll_bare - nll_oracle:+.4f})\")\n",
    "print(f\"  random_15w:{nll_random:.6f} (delta: {nll_bare - nll_random:+.4f})\")\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41dcf65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Scoring loop — 10 conditions x 400 samples\n",
    "print(\"=\" * 70)\n",
    "print(\"SCORING ALL CONDITIONS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "COND_NAMES = [\n",
    "    'bare', 'oracle',\n",
    "    'surr_reasonant', 'surr_universal', 'adversarial',\n",
    "    'random_matched', 'random_15w', 'random_30w',\n",
    "    'repeat_15w', 'oracle_padded',\n",
    "]\n",
    "\n",
    "results = []\n",
    "start_idx = 0\n",
    "\n",
    "if CHECKPOINT_PATH.exists():\n",
    "    ckpt = json.loads(CHECKPOINT_PATH.read_text())\n",
    "    if ckpt.get('n_total') == N_SAMPLES and len(ckpt.get('results', [])) > 0:\n",
    "        saved_queries = [r['query'][:50] for r in ckpt['results']]\n",
    "        current_queries = [s['query'][:50] for s in samples[:len(saved_queries)]]\n",
    "        if saved_queries == current_queries:\n",
    "            results = ckpt['results']\n",
    "            start_idx = len(results)\n",
    "            print(f\"Resuming from checkpoint: {start_idx}/{N_SAMPLES}\")\n",
    "\n",
    "if start_idx == 0:\n",
    "    print(f\"Starting fresh: {len(COND_NAMES)} conditions x {N_SAMPLES} samples\")\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "for i in tqdm(range(start_idx, N_SAMPLES), initial=start_idx, total=N_SAMPLES,\n",
    "              desc=\"Scoring\"):\n",
    "    s = samples[i]\n",
    "    query = s['query']\n",
    "    passage = s['passage']\n",
    "    answer = s['answer']\n",
    "\n",
    "    result = {\n",
    "        'query': query,\n",
    "        'answer': answer,\n",
    "        'passage_words': s['word_count'],\n",
    "        'query_words': len(query.split()),\n",
    "    }\n",
    "\n",
    "    # 1. bare\n",
    "    result['nll_bare'] = score(passage, query, answer)\n",
    "\n",
    "    # 2. oracle\n",
    "    result['nll_oracle'] = score(passage, query, answer, prefix_text=query)\n",
    "\n",
    "    # 3. surr_reasonant\n",
    "    result['nll_surr_reasonant'] = score(passage, query, answer,\n",
    "                                         prefix_text=SURR_REASONANT)\n",
    "\n",
    "    # 4. surr_universal\n",
    "    result['nll_surr_universal'] = score(passage, query, answer,\n",
    "                                          prefix_text=SURR_UNIVERSAL)\n",
    "\n",
    "    # 5. adversarial\n",
    "    result['nll_adversarial'] = score(passage, query, answer,\n",
    "                                      prefix_text=ADVERSARIAL_15W)\n",
    "\n",
    "    # 6. random_matched (same word count as query)\n",
    "    result['nll_random_matched'] = score(passage, query, answer,\n",
    "                                          prefix_text=s['random_matched'])\n",
    "\n",
    "    # 7. random_15w\n",
    "    result['nll_random_15w'] = score(passage, query, answer,\n",
    "                                      prefix_text=s['random_15w'])\n",
    "\n",
    "    # 8. random_30w\n",
    "    result['nll_random_30w'] = score(passage, query, answer,\n",
    "                                      prefix_text=s['random_30w'])\n",
    "\n",
    "    # 9. repeat_15w (\"the\" x 15)\n",
    "    result['nll_repeat_15w'] = score(passage, query, answer,\n",
    "                                      prefix_text=s['repeat_15w'])\n",
    "\n",
    "    # 10. oracle_padded (query + \"the\" to 15 words)\n",
    "    result['nll_oracle_padded'] = score(passage, query, answer,\n",
    "                                         prefix_text=s['oracle_padded'])\n",
    "\n",
    "    results.append(result)\n",
    "\n",
    "    if (i + 1) % 20 == 0 or i == N_SAMPLES - 1:\n",
    "        ckpt = {\n",
    "            'n_total': N_SAMPLES,\n",
    "            'results': results,\n",
    "            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        }\n",
    "        CHECKPOINT_PATH.write_text(json.dumps(ckpt))\n",
    "        elapsed = time.time() - t0\n",
    "        done = i - start_idx + 1\n",
    "        eta = (N_SAMPLES - i - 1) * elapsed / done if done > 0 else 0\n",
    "        tqdm.write(f\"  Checkpoint {i+1}/{N_SAMPLES} | {elapsed/60:.1f}m | ETA {eta/60:.1f}m\")\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "elapsed = time.time() - t0\n",
    "print(f\"\\nScoring complete: {len(results)} samples, \"\n",
    "      f\"{len(COND_NAMES)} conditions in {elapsed/60:.1f} min\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9194e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Results table\n",
    "print(\"=\" * 70)\n",
    "print(f\"RESULTS (N={len(results)})\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Extract arrays\n",
    "arrays = {}\n",
    "for name in ['bare', 'oracle', 'surr_reasonant', 'surr_universal', 'adversarial',\n",
    "             'random_matched', 'random_15w', 'random_30w', 'repeat_15w', 'oracle_padded']:\n",
    "    arrays[name] = np.array([r[f'nll_{name}'] for r in results])\n",
    "\n",
    "bare = arrays['bare']\n",
    "oracle = arrays['oracle']\n",
    "oracle_delta_mean = (bare - oracle).mean()\n",
    "\n",
    "print(f\"\\n  {'Condition':<20} {'~words':>6} {'NLL':>8} {'vs bare':>10} {'d':>8} \"\n",
    "      f\"{'Win%':>8} {'p':>12} {'sig':>5} {'Recovery':>10}\")\n",
    "print(f\"  {'-'*98}\")\n",
    "\n",
    "# Approximate word counts for display\n",
    "approx_words = {\n",
    "    'bare': 0, 'oracle': 6, 'surr_reasonant': 11, 'surr_universal': 13,\n",
    "    'adversarial': 15, 'random_matched': 6, 'random_15w': 15, 'random_30w': 30,\n",
    "    'repeat_15w': 15, 'oracle_padded': 15,\n",
    "}\n",
    "\n",
    "analysis = {}\n",
    "for name in ['bare', 'oracle', 'surr_reasonant', 'surr_universal', 'adversarial',\n",
    "             'random_matched', 'random_15w', 'random_30w', 'repeat_15w', 'oracle_padded']:\n",
    "    nlls = arrays[name]\n",
    "    mean_nll = nlls.mean()\n",
    "    aw = approx_words[name]\n",
    "\n",
    "    if name == 'bare':\n",
    "        print(f\"  {name:<20} {aw:>6} {mean_nll:>8.4f} {'--':>10} {'--':>8} \"\n",
    "              f\"{'--':>8} {'--':>12} {'--':>5} {'--':>10}\")\n",
    "        analysis[name] = {'mean_nll': float(mean_nll)}\n",
    "    else:\n",
    "        diff = bare - nlls\n",
    "        d = cohens_d(diff)\n",
    "        win_pct = 100 * np.mean(diff > 0)\n",
    "        _, p_val = stats.ttest_1samp(diff, 0)\n",
    "        sig = '***' if p_val < 0.001 else '**' if p_val < 0.01 else '*' if p_val < 0.05 else 'ns'\n",
    "\n",
    "        if oracle_delta_mean > 0:\n",
    "            recovery = diff.mean() / oracle_delta_mean * 100\n",
    "            rec_str = f\"{recovery:>9.1f}%\"\n",
    "        else:\n",
    "            recovery = float('nan')\n",
    "            rec_str = \"n/a\"\n",
    "\n",
    "        print(f\"  {name:<20} {aw:>6} {mean_nll:>8.4f} {diff.mean():>+10.4f} {d:>+8.3f} \"\n",
    "              f\"{win_pct:>7.1f}% {p_val:>12.2e} {sig:>5} {rec_str:>10}\")\n",
    "        analysis[name] = {\n",
    "            'mean_nll': float(mean_nll), 'delta': float(diff.mean()),\n",
    "            'd': float(d), 'win_pct': float(win_pct), 'p': float(p_val),\n",
    "            'recovery': float(recovery) if not np.isnan(recovery) else None,\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c6ad20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Length vs Content decomposition\n",
    "print(\"=\" * 70)\n",
    "print(\"DECOMPOSITION: LENGTH vs CONTENT\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# --- 1. Pure length effect (random words at different counts) ---\n",
    "print(f\"\\n--- 1. PURE LENGTH EFFECT (random words, varying count) ---\")\n",
    "print(f\"  {'Condition':<20} {'~words':>6} {'d vs bare':>10} {'p':>12}\")\n",
    "print(f\"  {'-'*55}\")\n",
    "for name, nw in [('random_matched', '~6'), ('random_15w', '15'), ('random_30w', '30')]:\n",
    "    diff = bare - arrays[name]\n",
    "    d = cohens_d(diff)\n",
    "    _, p = stats.ttest_1samp(diff, 0)\n",
    "    sig = '***' if p < 0.001 else '**' if p < 0.01 else '*' if p < 0.05 else 'ns'\n",
    "    print(f\"  {name:<20} {nw:>6} {d:>+10.4f} {p:>12.2e} {sig}\")\n",
    "\n",
    "# Length gradient\n",
    "d_matched = cohens_d(bare - arrays['random_matched'])\n",
    "d_15 = cohens_d(bare - arrays['random_15w'])\n",
    "d_30 = cohens_d(bare - arrays['random_30w'])\n",
    "if d_matched > 0:\n",
    "    print(f\"\\n  Length scaling: 15w/matched = {d_15/d_matched:.2f}x, \"\n",
    "          f\"30w/matched = {d_30/d_matched:.2f}x\")\n",
    "    if d_30 > d_15 * 1.2:\n",
    "        print(f\"  -> Benefit INCREASES with length (not saturated)\")\n",
    "    elif d_30 > d_15 * 0.9:\n",
    "        print(f\"  -> Benefit roughly FLAT from 15w to 30w (saturated)\")\n",
    "    else:\n",
    "        print(f\"  -> Benefit DECREASES at 30w (diminishing returns)\")\n",
    "\n",
    "# --- 2. Content effect at oracle's length (~6 words) ---\n",
    "print(f\"\\n--- 2. CONTENT EFFECT at oracle's length (~6 words) ---\")\n",
    "diff_oracle_vs_random = arrays['random_matched'] - arrays['oracle']\n",
    "d_content_oracle = cohens_d(diff_oracle_vs_random)\n",
    "_, p_content_oracle = stats.ttest_1samp(diff_oracle_vs_random, 0)\n",
    "sig_co = '***' if p_content_oracle < 0.001 else '**' if p_content_oracle < 0.01 else '*' if p_content_oracle < 0.05 else 'ns'\n",
    "\n",
    "d_oracle = cohens_d(bare - arrays['oracle'])\n",
    "d_random_m = cohens_d(bare - arrays['random_matched'])\n",
    "\n",
    "print(f\"  oracle        d vs bare = {d_oracle:+.4f}\")\n",
    "print(f\"  random_matched d vs bare = {d_random_m:+.4f}\")\n",
    "print(f\"  oracle vs random_matched: d = {d_content_oracle:+.4f} ({sig_co})\")\n",
    "if d_oracle > 0 and d_random_m > 0:\n",
    "    content_pct_oracle = (d_oracle - d_random_m) / d_oracle * 100\n",
    "    print(f\"  -> Content accounts for {content_pct_oracle:.0f}% of oracle benefit at this length\")\n",
    "    print(f\"  -> Structure accounts for {100 - content_pct_oracle:.0f}%\")\n",
    "\n",
    "# --- 3. Content effect at surrogate length (~15 words) ---\n",
    "print(f\"\\n--- 3. CONTENT EFFECT at surrogate length (~15 words) ---\")\n",
    "print(f\"  {'Condition':<20} {'Content':>12} {'d vs bare':>10}\")\n",
    "print(f\"  {'-'*48}\")\n",
    "for name, content_type in [('repeat_15w', 'none'),\n",
    "                            ('random_15w', 'random'),\n",
    "                            ('adversarial', 'off-topic'),\n",
    "                            ('surr_universal', 'generic'),\n",
    "                            ('surr_reasonant', 'targeted')]:\n",
    "    d = cohens_d(bare - arrays[name])\n",
    "    print(f\"  {name:<20} {content_type:>12} {d:>+10.4f}\")\n",
    "\n",
    "# Pairwise: surr_reasonant vs random_15w\n",
    "diff_sr = arrays['random_15w'] - arrays['surr_reasonant']\n",
    "d_sr = cohens_d(diff_sr)\n",
    "_, p_sr = stats.ttest_1samp(diff_sr, 0)\n",
    "sig_sr = '***' if p_sr < 0.001 else '**' if p_sr < 0.01 else '*' if p_sr < 0.05 else 'ns'\n",
    "print(f\"\\n  surr_reasonant vs random_15w: d = {d_sr:+.4f} ({sig_sr})\")\n",
    "print(f\"  -> {'Content matters' if p_sr < 0.05 else 'No significant content effect'} \"\n",
    "      f\"at matched length\")\n",
    "\n",
    "# Pairwise: random_15w vs repeat_15w\n",
    "diff_rr = arrays['repeat_15w'] - arrays['random_15w']\n",
    "d_rr = cohens_d(diff_rr)\n",
    "_, p_rr = stats.ttest_1samp(diff_rr, 0)\n",
    "sig_rr = '***' if p_rr < 0.001 else '**' if p_rr < 0.01 else '*' if p_rr < 0.05 else 'ns'\n",
    "print(f\"  random_15w vs repeat_15w: d = {d_rr:+.4f} ({sig_rr})\")\n",
    "print(f\"  -> {'Token diversity matters' if p_rr < 0.05 else 'Token diversity does NOT matter'}\")\n",
    "\n",
    "# --- 4. Oracle + length boost ---\n",
    "print(f\"\\n--- 4. ORACLE + LENGTH BOOST ---\")\n",
    "d_oracle_plain = cohens_d(bare - arrays['oracle'])\n",
    "d_oracle_pad = cohens_d(bare - arrays['oracle_padded'])\n",
    "diff_pad = arrays['oracle'] - arrays['oracle_padded']\n",
    "d_pad_boost = cohens_d(diff_pad)\n",
    "_, p_pad = stats.ttest_1samp(diff_pad, 0)\n",
    "sig_pad = '***' if p_pad < 0.001 else '**' if p_pad < 0.01 else '*' if p_pad < 0.05 else 'ns'\n",
    "\n",
    "print(f\"  oracle (natural):  d = {d_oracle_plain:+.4f}\")\n",
    "print(f\"  oracle_padded(15w): d = {d_oracle_pad:+.4f}\")\n",
    "print(f\"  padding boost: d = {d_pad_boost:+.4f} ({sig_pad})\")\n",
    "if d_pad_boost > 0.05:\n",
    "    print(f\"  -> Padding oracle to 15w IMPROVES it — length matters even with real query\")\n",
    "elif d_pad_boost < -0.05:\n",
    "    print(f\"  -> Padding oracle HURTS — the extra 'the' tokens add noise\")\n",
    "else:\n",
    "    print(f\"  -> Padding has no significant effect on oracle\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934fa18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Summary — how much is length vs content?\n",
    "print(\"=\" * 70)\n",
    "print(\"SUMMARY DECOMPOSITION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "d_bare = 0  # reference\n",
    "d_oracle = cohens_d(bare - arrays['oracle'])\n",
    "d_random_m = cohens_d(bare - arrays['random_matched'])\n",
    "d_random_15 = cohens_d(bare - arrays['random_15w'])\n",
    "d_random_30 = cohens_d(bare - arrays['random_30w'])\n",
    "d_repeat_15 = cohens_d(bare - arrays['repeat_15w'])\n",
    "d_surr_r = cohens_d(bare - arrays['surr_reasonant'])\n",
    "d_surr_u = cohens_d(bare - arrays['surr_universal'])\n",
    "d_adv = cohens_d(bare - arrays['adversarial'])\n",
    "d_oracle_pad = cohens_d(bare - arrays['oracle_padded'])\n",
    "\n",
    "print(f\"\\n  Effect sizes (Cohen's d vs bare):\")\n",
    "print(f\"  {'':>30} {'d':>8}\")\n",
    "print(f\"  {'-'*42}\")\n",
    "print(f\"  {'bare (reference)':>30} {0:>+8.4f}\")\n",
    "print(f\"  {'--- ~6 words ---':>30}\")\n",
    "print(f\"  {'random_matched (~6w)':>30} {d_random_m:>+8.4f}\")\n",
    "print(f\"  {'oracle (~6w)':>30} {d_oracle:>+8.4f}\")\n",
    "print(f\"  {'--- ~15 words ---':>30}\")\n",
    "print(f\"  {'repeat_15w':>30} {d_repeat_15:>+8.4f}\")\n",
    "print(f\"  {'random_15w':>30} {d_random_15:>+8.4f}\")\n",
    "print(f\"  {'adversarial (~15w)':>30} {d_adv:>+8.4f}\")\n",
    "print(f\"  {'oracle_padded (15w)':>30} {d_oracle_pad:>+8.4f}\")\n",
    "print(f\"  {'surr_universal (~13w)':>30} {d_surr_u:>+8.4f}\")\n",
    "print(f\"  {'surr_reasonant (~11w)':>30} {d_surr_r:>+8.4f}\")\n",
    "print(f\"  {'--- ~30 words ---':>30}\")\n",
    "print(f\"  {'random_30w':>30} {d_random_30:>+8.4f}\")\n",
    "\n",
    "# Decompose the best surrogate's effect\n",
    "print(f\"\\n  Decomposition of surr_reasonant (d = {d_surr_r:+.4f}):\")\n",
    "length_component = d_random_15  # effect of having ~15 random words\n",
    "content_component = d_surr_r - d_random_15\n",
    "total = d_surr_r\n",
    "if total > 0:\n",
    "    print(f\"    Length (~15 random words):  {length_component:+.4f} ({length_component/total*100:.0f}%)\")\n",
    "    print(f\"    Content (reasonant vs random): {content_component:+.4f} ({content_component/total*100:.0f}%)\")\n",
    "\n",
    "# Decompose oracle's effect\n",
    "print(f\"\\n  Decomposition of oracle (d = {d_oracle:+.4f}):\")\n",
    "length_component_o = d_random_m  # effect of having ~6 random words\n",
    "content_component_o = d_oracle - d_random_m\n",
    "if d_oracle > 0:\n",
    "    print(f\"    Length (~6 random words):   {length_component_o:+.4f} ({length_component_o/d_oracle*100:.0f}%)\")\n",
    "    print(f\"    Content (real query vs random): {content_component_o:+.4f} ({content_component_o/d_oracle*100:.0f}%)\")\n",
    "\n",
    "# Interpret\n",
    "print(f\"\\n  INTERPRETATION:\")\n",
    "if d_random_15 > d_surr_r * 0.8:\n",
    "    print(f\"  -> STRUCTURAL DOMINANCE: random words at 15w achieve {d_random_15/d_surr_r*100:.0f}% of best surrogate\")\n",
    "    print(f\"     Content adds only marginal value. The effect is primarily about\")\n",
    "    print(f\"     giving doc tokens additional causal context to attend to.\")\n",
    "elif d_random_15 > d_surr_r * 0.5:\n",
    "    print(f\"  -> MIXED: length provides {d_random_15/d_surr_r*100:.0f}% of surrogate benefit,\")\n",
    "    print(f\"     but content contributes a meaningful additional {(d_surr_r - d_random_15)/d_surr_r*100:.0f}%.\")\n",
    "else:\n",
    "    print(f\"  -> CONTENT DOMINATES: random words at 15w only achieve {d_random_15/d_surr_r*100:.0f}%.\")\n",
    "    print(f\"     The semantic content of the prefix is the primary driver.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81519cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Verdict and save\n",
    "print(\"=\" * 70)\n",
    "print(\"VERDICT — Decoder-Only Exp 02: Length vs Content\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\nModel: {MODEL_NAME}\")\n",
    "print(f\"N: {len(results)} samples (MS MARCO v1.1)\")\n",
    "\n",
    "d_oracle = cohens_d(bare - arrays['oracle'])\n",
    "d_surr_r = cohens_d(bare - arrays['surr_reasonant'])\n",
    "d_random_15 = cohens_d(bare - arrays['random_15w'])\n",
    "d_random_m = cohens_d(bare - arrays['random_matched'])\n",
    "\n",
    "print(f\"\\n--- Key findings ---\")\n",
    "print(f\"  1. Oracle (real query): d = {d_oracle:+.4f}\")\n",
    "print(f\"  2. Best surrogate:     d = {d_surr_r:+.4f}\")\n",
    "print(f\"  3. Random 15w:         d = {d_random_15:+.4f}\")\n",
    "print(f\"  4. Random matched:     d = {d_random_m:+.4f}\")\n",
    "\n",
    "# Compare to exp01 (Gemma 2 2B)\n",
    "print(f\"\\n--- Model comparison (vs exp01 Gemma 2 2B) ---\")\n",
    "print(f\"  Exp01 oracle d = +0.440, surr_reasonant d = +0.647\")\n",
    "print(f\"  Exp02 oracle d = {d_oracle:+.4f}, surr_reasonant d = {d_surr_r:+.4f}\")\n",
    "if d_oracle > 0.3:\n",
    "    print(f\"  -> Effect REPLICATES on larger model\")\n",
    "elif d_oracle > 0.1:\n",
    "    print(f\"  -> Weaker but present on larger model\")\n",
    "else:\n",
    "    print(f\"  -> Effect FAILS to replicate on larger model\")\n",
    "\n",
    "# All conditions summary\n",
    "print(f\"\\n--- All conditions ---\")\n",
    "for name in ['bare', 'oracle', 'surr_reasonant', 'surr_universal', 'adversarial',\n",
    "             'random_matched', 'random_15w', 'random_30w', 'repeat_15w', 'oracle_padded']:\n",
    "    if name == 'bare':\n",
    "        print(f\"  {name:<20} NLL = {arrays[name].mean():.4f}\")\n",
    "    else:\n",
    "        d = cohens_d(bare - arrays[name])\n",
    "        _, p = stats.ttest_1samp(bare - arrays[name], 0)\n",
    "        sig = '***' if p < 0.001 else '**' if p < 0.01 else '*' if p < 0.05 else 'ns'\n",
    "        print(f\"  {name:<20} NLL = {arrays[name].mean():.4f}  d = {d:+.4f} ({sig})\")\n",
    "\n",
    "# Save\n",
    "final_results = {\n",
    "    'experiment': 'v4_decoder_only_exp02_length_vs_content',\n",
    "    'model': MODEL_NAME,\n",
    "    'dataset': 'ms_marco_v1.1',\n",
    "    'n_samples': len(results),\n",
    "    'seed': SEED,\n",
    "    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'conditions': {k: v for k, v in analysis.items()},\n",
    "}\n",
    "\n",
    "with open(RESULTS_DIR / 'results.json', 'w') as f:\n",
    "    json.dump(final_results, f, indent=2)\n",
    "print(f\"\\nResults saved to {RESULTS_DIR / 'results.json'}\")\n",
    "\n",
    "# Cleanup\n",
    "print(f\"\\nCleaning up GPU memory...\")\n",
    "mem_before = torch.cuda.memory_allocated() / 1e9\n",
    "del model, tokenizer\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "mem_after = torch.cuda.memory_allocated() / 1e9\n",
    "print(f\"GPU memory: {mem_before:.2f} GB -> {mem_after:.2f} GB\")\n",
    "print(\"Done!\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
