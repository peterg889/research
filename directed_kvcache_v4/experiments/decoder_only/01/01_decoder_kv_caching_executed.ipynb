{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c55b365",
   "metadata": {},
   "source": [
    "# Experiment 02: Decoder-Only Surrogate KV Caching\n",
    "\n",
    "## Motivation\n",
    "\n",
    "v4 Exp 01 tested surrogate enrichment with T5Gemma (encoder-decoder).\n",
    "This experiment tests the same concept with a **decoder-only** model using\n",
    "actual KV cache manipulation — the production deployment approach.\n",
    "\n",
    "In a causal (decoder-only) model, document tokens D cannot attend to a query Q\n",
    "that comes after them. This creates a performance gap vs. bidirectional models.\n",
    "We address this by using a **surrogate query** during offline cache generation\n",
    "to \"condition\" D's KV representations.\n",
    "\n",
    "## Method\n",
    "\n",
    "**Phase A — Offline Cache Generation:**\n",
    "1. Construct `[surrogate + document]`\n",
    "2. Forward pass with `use_cache=True` → full KV cache\n",
    "3. Slice: remove surrogate entries, keep only document KV\n",
    "4. The document KV now encodes features influenced by the surrogate\n",
    "\n",
    "**Phase B — Online Inference:**\n",
    "1. Load sliced document KV cache\n",
    "2. Set `position_ids` for query to start after the document's original positions\n",
    "3. Forward pass: query + answer attend to conditioned document KV\n",
    "4. Compute NLL on answer tokens\n",
    "\n",
    "**Position alignment**: If the surrogate was S tokens and document is D tokens,\n",
    "the cached document occupies positions S through S+D-1. New query tokens start\n",
    "at position S+D. This preserves correct RoPE relative distances.\n",
    "\n",
    "**Key insight from v3**: RoPE-based attention depends on relative positions.\n",
    "Doc-to-query relative distance is identical across conditions, so the position\n",
    "offset from the surrogate does NOT confound the comparison.\n",
    "\n",
    "## Conditions (8 total)\n",
    "\n",
    "| # | Condition | Prefix | Slice? | Description |\n",
    "|---|-----------|--------|--------|-------------|\n",
    "| 1 | bare | (none) | no | Standard causal — lower bound |\n",
    "| 2 | oracle | real query | yes | Real query conditions doc — upper bound |\n",
    "| 3 | surr_universal | generic analysis | yes | \"Analyze for entities, facts, relationships\" |\n",
    "| 4 | surr_extractor | data extraction | yes | \"Examine for data points, dates, attributes\" |\n",
    "| 5 | surr_reasonant | reasoning | yes | \"Evaluate arguments, sentiment, intent\" |\n",
    "| 6 | surr_analytic | technical | yes | \"Technical breakdown of systems/processes\" |\n",
    "| 7 | surr_doc_kw | doc keywords | yes | Top-5 document keywords (v3's best) |\n",
    "| 8 | adversarial | off-topic | yes | Off-topic text — negative control |\n",
    "\n",
    "## Key metrics\n",
    "- **Recovery rate**: (surrogate − bare) / (oracle − bare) × 100%\n",
    "- Cohen's d, win%, paired t-test\n",
    "- Hardness gradient analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db0acd0e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-20T14:44:17.366166Z",
     "iopub.status.busy": "2026-02-20T14:44:17.365372Z",
     "iopub.status.idle": "2026-02-20T14:45:06.006639Z",
     "shell.execute_reply": "2026-02-20T14:45:06.005579Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading google/gemma-3-4b-it...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87e21d75a2c94a37a2c73b66cc82a2d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/883 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exp 02: Decoder-Only Surrogate KV Caching\n",
      "N: 400, Model: google/gemma-3-4b-it\n",
      "DEVICE: cuda:0, dtype: torch.bfloat16\n",
      "GPU memory: 8.60 GB\n",
      "Vocab size: 262208\n",
      "Num layers: 34\n",
      "Num KV heads: 4\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Setup and model loading\n",
    "import os\n",
    "os.umask(0o000)\n",
    "\n",
    "import sys, json, time, gc, re\n",
    "import random as pyrandom\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from scipy import stats\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "sys.path.insert(0, \"../../..\")\n",
    "from lib.analysis import cohens_d\n",
    "\n",
    "SEED = 42\n",
    "N_SAMPLES = 400\n",
    "MODEL_NAME = \"google/gemma-3-4b-it\"\n",
    "\n",
    "RESULTS_DIR = Path(\"../../../results/decoder_only/exp01\")\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CHECKPOINT_PATH = RESULTS_DIR / \"checkpoint.json\"\n",
    "\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "pyrandom.seed(SEED)\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv())\n",
    "HF_TOKEN = os.environ.get(\"HF_TOKEN\")\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "print(f\"Loading {MODEL_NAME}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, token=HF_TOKEN)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME, device_map=\"auto\", torch_dtype=torch.bfloat16, token=HF_TOKEN,\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "DEVICE = next(model.parameters()).device\n",
    "\n",
    "print(f\"Exp 02: Decoder-Only Surrogate KV Caching\")\n",
    "print(f\"N: {N_SAMPLES}, Model: {MODEL_NAME}\")\n",
    "print(f\"DEVICE: {DEVICE}, dtype: {next(model.parameters()).dtype}\")\n",
    "print(f\"GPU memory: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "text_cfg = getattr(model.config, 'text_config', model.config)\n",
    "print(f\"Vocab size: {getattr(text_cfg, 'vocab_size', 'N/A')}\")\n",
    "print(f\"Num layers: {getattr(text_cfg, 'num_hidden_layers', 'N/A')}\")\n",
    "print(f\"Num KV heads: {getattr(text_cfg, 'num_key_value_heads', 'N/A')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57571aa2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-20T14:45:06.011211Z",
     "iopub.status.busy": "2026-02-20T14:45:06.010502Z",
     "iopub.status.idle": "2026-02-20T14:45:06.040450Z",
     "shell.execute_reply": "2026-02-20T14:45:06.039585Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scoring functions defined.\n",
      "\n",
      "Surrogate prompts:\n",
      "  universal    (16 tok): Analyze the following text for all key entities, factual cla...\n",
      "  extractor    (19 tok): Examine this document specifically for data points, dates, n...\n",
      "  reasonant    (14 tok): Evaluate the underlying arguments, sentiment, and intent of ...\n",
      "  analytic     (14 tok): Provide a technical breakdown of the systems and processes d...\n",
      "  adversarial  (20 tok): The recipe calls for two cups of flour, one cup of sugar, an...\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: KV cache helpers and scoring function\n",
    "\n",
    "def slice_kv_cache(cache, start_idx):\n",
    "    # Remove first start_idx entries from KV cache.\n",
    "    # DynamicCache API: cache.layers[i].keys / .values are tensors.\n",
    "    from transformers import DynamicCache\n",
    "\n",
    "    if isinstance(cache, DynamicCache):\n",
    "        sliced = DynamicCache()\n",
    "        for i in range(len(cache.layers)):\n",
    "            k = cache.layers[i].keys[:, :, start_idx:, :]\n",
    "            v = cache.layers[i].values[:, :, start_idx:, :]\n",
    "            sliced.update(k, v, i)\n",
    "        return sliced\n",
    "    else:\n",
    "        # Tuple-of-tuples fallback\n",
    "        return tuple(\n",
    "            (k[:, :, start_idx:, :], v[:, :, start_idx:, :])\n",
    "            for k, v in cache\n",
    "        )\n",
    "\n",
    "\n",
    "def score(doc_text, query_text, answer_text, prefix_text=None):\n",
    "    # Score NLL of answer tokens using two-phase KV cache approach.\n",
    "    #\n",
    "    # Phase A: Forward [prefix + doc] (or just [doc]) -> KV cache.\n",
    "    # Phase B: Forward [query + answer] using cached doc KV.\n",
    "    # If prefix_text is provided, prefix KV entries are sliced off.\n",
    "    #\n",
    "    # Returns: mean NLL over answer tokens.\n",
    "\n",
    "    # --- Phase A: Conditioning ---\n",
    "    if prefix_text:\n",
    "        # Tokenize prefix and doc separately to know exact split point\n",
    "        prefix_ids = tokenizer(prefix_text + \"\\n\", add_special_tokens=True,\n",
    "                               truncation=True, max_length=512).input_ids\n",
    "        doc_ids = tokenizer(doc_text, add_special_tokens=False,\n",
    "                            truncation=True, max_length=1536).input_ids\n",
    "        cond_ids = prefix_ids + doc_ids\n",
    "        slice_start = len(prefix_ids)\n",
    "    else:\n",
    "        cond_ids = tokenizer(doc_text, add_special_tokens=True,\n",
    "                             truncation=True, max_length=2048).input_ids\n",
    "        slice_start = 0\n",
    "\n",
    "    cond_tensor = torch.tensor([cond_ids], dtype=torch.long, device=DEVICE)\n",
    "    total_cond_len = len(cond_ids)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        phase_a = model(input_ids=cond_tensor, use_cache=True)\n",
    "\n",
    "    cache = phase_a.past_key_values\n",
    "    del phase_a\n",
    "\n",
    "    # Slice prefix from cache\n",
    "    if slice_start > 0:\n",
    "        cache = slice_kv_cache(cache, slice_start)\n",
    "\n",
    "    # --- Phase B: Inference with query + answer ---\n",
    "    query_part_ids = tokenizer(\"\\n\" + query_text + \"\\n\", add_special_tokens=False).input_ids\n",
    "    answer_ids = tokenizer(answer_text, add_special_tokens=False,\n",
    "                           truncation=True, max_length=256).input_ids\n",
    "\n",
    "    if not answer_ids:\n",
    "        del cache\n",
    "        return 0.0\n",
    "\n",
    "    phase_b_ids = query_part_ids + answer_ids\n",
    "    phase_b_tensor = torch.tensor([phase_b_ids], dtype=torch.long, device=DEVICE)\n",
    "\n",
    "    # Position IDs: new tokens start after the original conditioning sequence\n",
    "    pos_ids = torch.arange(total_cond_len, total_cond_len + len(phase_b_ids),\n",
    "                           device=DEVICE).unsqueeze(0)\n",
    "\n",
    "    # Cache position for correct causal mask computation\n",
    "    cache_position = torch.arange(total_cond_len, total_cond_len + len(phase_b_ids),\n",
    "                                  device=DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        phase_b = model(\n",
    "            input_ids=phase_b_tensor,\n",
    "            past_key_values=cache,\n",
    "            position_ids=pos_ids,\n",
    "            cache_position=cache_position,\n",
    "            use_cache=False,\n",
    "        )\n",
    "\n",
    "    # NLL on answer tokens only\n",
    "    logits = phase_b.logits  # [1, n_phase_b, vocab]\n",
    "    n_query_part = len(query_part_ids)\n",
    "    n_answer = len(answer_ids)\n",
    "\n",
    "    # logits[0, t, :] predicts token at position t+1 in the new sequence\n",
    "    # Answer starts at index n_query_part in phase_b sequence\n",
    "    # To predict answer[0], need logits at index n_query_part - 1\n",
    "    answer_logits = logits[0, n_query_part - 1 : n_query_part - 1 + n_answer, :]\n",
    "    answer_targets = torch.tensor(answer_ids, dtype=torch.long, device=DEVICE)\n",
    "\n",
    "    log_probs = F.log_softmax(answer_logits, dim=-1)\n",
    "    token_log_probs = log_probs.gather(1, answer_targets.unsqueeze(1)).squeeze(1)\n",
    "    mean_nll = -token_log_probs.mean().item()\n",
    "\n",
    "    del cache, phase_b, logits, log_probs\n",
    "    return mean_nll\n",
    "\n",
    "\n",
    "def score_full_sequence(doc_text, query_text, answer_text):\n",
    "    # Score NLL with a single forward pass: [doc + query + answer].\n",
    "    # Used for validation against the two-phase approach.\n",
    "    doc_ids = tokenizer(doc_text, add_special_tokens=True,\n",
    "                        truncation=True, max_length=2048).input_ids\n",
    "    query_part_ids = tokenizer(\"\\n\" + query_text + \"\\n\", add_special_tokens=False).input_ids\n",
    "    answer_ids = tokenizer(answer_text, add_special_tokens=False,\n",
    "                           truncation=True, max_length=256).input_ids\n",
    "\n",
    "    if not answer_ids:\n",
    "        return 0.0\n",
    "\n",
    "    all_ids = doc_ids + query_part_ids + answer_ids\n",
    "    input_tensor = torch.tensor([all_ids], dtype=torch.long, device=DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_tensor, use_cache=False)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    n_doc = len(doc_ids)\n",
    "    n_query = len(query_part_ids)\n",
    "    n_answer = len(answer_ids)\n",
    "\n",
    "    # Answer starts at index n_doc + n_query in the full sequence\n",
    "    # To predict answer[0], need logits at index n_doc + n_query - 1\n",
    "    start = n_doc + n_query - 1\n",
    "    answer_logits = logits[0, start : start + n_answer, :]\n",
    "    answer_targets = torch.tensor(answer_ids, dtype=torch.long, device=DEVICE)\n",
    "\n",
    "    log_probs = F.log_softmax(answer_logits, dim=-1)\n",
    "    token_log_probs = log_probs.gather(1, answer_targets.unsqueeze(1)).squeeze(1)\n",
    "    mean_nll = -token_log_probs.mean().item()\n",
    "\n",
    "    del outputs, logits, log_probs\n",
    "    return mean_nll\n",
    "\n",
    "\n",
    "# === Surrogate definitions ===\n",
    "SURROGATES = {\n",
    "    'universal': \"Analyze the following text for all key entities, factual claims, and logical relationships.\",\n",
    "    'extractor': \"Examine this document specifically for data points, dates, numerical values, and specific named attributes.\",\n",
    "    'reasonant': \"Evaluate the underlying arguments, sentiment, and intent of the following passage.\",\n",
    "    'analytic': \"Provide a technical breakdown of the systems and processes described in this text.\",\n",
    "}\n",
    "\n",
    "ADVERSARIAL_PREFIX = \"The recipe calls for two cups of flour, one cup of sugar, and a pinch of salt.\"\n",
    "\n",
    "STOP_WORDS = {\n",
    "    'a', 'an', 'the', 'is', 'are', 'was', 'were', 'be', 'been', 'being',\n",
    "    'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could',\n",
    "    'should', 'may', 'might', 'can', 'shall', 'to', 'of', 'in', 'for',\n",
    "    'on', 'with', 'at', 'by', 'from', 'as', 'into', 'through', 'during',\n",
    "    'before', 'after', 'above', 'below', 'between', 'and', 'but', 'or',\n",
    "    'not', 'no', 'if', 'then', 'than', 'so', 'up', 'out', 'about',\n",
    "    'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those',\n",
    "    'it', 'its', 'i', 'me', 'my', 'we', 'our', 'you', 'your', 'he',\n",
    "    'him', 'his', 'she', 'her', 'they', 'them', 'their', 'how', 'when',\n",
    "    'where', 'why', 'much', 'many', 'some', 'any', 'all', 'each',\n",
    "    'does', 'also', 'just', 'more', 'most', 'very', 'too', 'only',\n",
    "}\n",
    "\n",
    "def extract_keywords(text):\n",
    "    words = re.sub(r'[^\\w\\s]', '', text.lower()).split()\n",
    "    return [w for w in words if w not in STOP_WORDS and len(w) > 2]\n",
    "\n",
    "def make_doc_keywords(passage):\n",
    "    content_words = extract_keywords(passage)\n",
    "    if not content_words:\n",
    "        return \"information\"\n",
    "    counts = Counter(content_words)\n",
    "    return \" \".join(w for w, _ in counts.most_common(5))\n",
    "\n",
    "\n",
    "print(\"Scoring functions defined.\")\n",
    "print(f\"\\nSurrogate prompts:\")\n",
    "for name, prompt in SURROGATES.items():\n",
    "    n_tok = len(tokenizer(prompt, add_special_tokens=False).input_ids)\n",
    "    print(f\"  {name:<12} ({n_tok:>2} tok): {prompt[:60]}...\")\n",
    "adv_tok = len(tokenizer(ADVERSARIAL_PREFIX, add_special_tokens=False).input_ids)\n",
    "print(f\"  {'adversarial':<12} ({adv_tok:>2} tok): {ADVERSARIAL_PREFIX[:60]}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4fe1f4bb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-20T14:45:06.044249Z",
     "iopub.status.busy": "2026-02-20T14:45:06.043571Z",
     "iopub.status.idle": "2026-02-20T14:45:07.828435Z",
     "shell.execute_reply": "2026-02-20T14:45:07.827496Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MS MARCO v1.1 validation...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total candidates: 1200\n",
      "Loaded 400 samples\n",
      "Mean passage words: 73\n",
      "Mean answer words: 14\n",
      "Mean query words: 6\n",
      "\n",
      "First sample:\n",
      "  Query:  average annual temperature of Uruguay...\n",
      "  Answer: Very mild at 15.8 degrees Celsius (60.4 degrees Fahrenheit)....\n",
      "  Passage (76w): Average Temperatures in Montevideo, Uruguay. 1  The average annual tem...\n",
      "  Doc keywords: average degrees temperatures montevideo uruguay\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Load MS MARCO data and generate surrogates\n",
    "from lib.data import count_words\n",
    "from datasets import load_dataset\n",
    "\n",
    "print(\"Loading MS MARCO v1.1 validation...\")\n",
    "ds = load_dataset(\"microsoft/ms_marco\", \"v1.1\", split=\"validation\")\n",
    "\n",
    "all_candidates = []\n",
    "for item in ds:\n",
    "    if len(all_candidates) >= 3 * N_SAMPLES:\n",
    "        break\n",
    "    passages = item.get('passages', {})\n",
    "    ptexts = passages.get('passage_text', [])\n",
    "    is_sel = passages.get('is_selected', [])\n",
    "    query = item.get('query', '')\n",
    "    answers = item.get('answers', [])\n",
    "    well_formed = item.get('wellFormedAnswers', [])\n",
    "    answer = None\n",
    "    if well_formed and len(well_formed) > 0 and well_formed[0] not in ('[]', ''):\n",
    "        answer = well_formed[0]\n",
    "    elif answers and len(answers) > 0 and answers[0] != 'No Answer Present.':\n",
    "        answer = answers[0]\n",
    "    if not answer:\n",
    "        continue\n",
    "    for pt, sel in zip(ptexts, is_sel):\n",
    "        wc = count_words(pt)\n",
    "        if sel == 1 and 30 <= wc <= 300:\n",
    "            all_candidates.append({\n",
    "                'passage': pt, 'query': query, 'answer': answer,\n",
    "                'word_count': wc,\n",
    "            })\n",
    "            break\n",
    "\n",
    "print(f\"Total candidates: {len(all_candidates)}\")\n",
    "np.random.seed(SEED)\n",
    "indices = np.random.permutation(len(all_candidates))\n",
    "samples = [all_candidates[i] for i in indices[:N_SAMPLES]]\n",
    "del ds, all_candidates\n",
    "gc.collect()\n",
    "\n",
    "# Generate surrogates\n",
    "for s in samples:\n",
    "    s['surr_doc_kw'] = make_doc_keywords(s['passage'])\n",
    "\n",
    "print(f\"Loaded {len(samples)} samples\")\n",
    "print(f\"Mean passage words: {np.mean([s['word_count'] for s in samples]):.0f}\")\n",
    "print(f\"Mean answer words: {np.mean([count_words(s['answer']) for s in samples]):.0f}\")\n",
    "print(f\"Mean query words: {np.mean([count_words(s['query']) for s in samples]):.0f}\")\n",
    "print(f\"\\nFirst sample:\")\n",
    "print(f\"  Query:  {samples[0]['query'][:70]}...\")\n",
    "print(f\"  Answer: {samples[0]['answer'][:70]}...\")\n",
    "print(f\"  Passage ({samples[0]['word_count']}w): {samples[0]['passage'][:70]}...\")\n",
    "print(f\"  Doc keywords: {samples[0]['surr_doc_kw']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30d95146",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-20T14:45:07.832334Z",
     "iopub.status.busy": "2026-02-20T14:45:07.831785Z",
     "iopub.status.idle": "2026-02-20T14:45:10.565831Z",
     "shell.execute_reply": "2026-02-20T14:45:10.564615Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "VALIDATION: Two-phase caching vs single-pass\n",
      "======================================================================\n",
      "\n",
      "Comparing bare score (cached) vs full-sequence score for 5 samples...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Sample 0: cached=0.738281, full=0.746094, diff=0.00781250 [MISMATCH]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Sample 1: cached=0.882812, full=0.878906, diff=0.00390625 [MISMATCH]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Sample 2: cached=1.851562, full=1.859375, diff=0.00781250 [MISMATCH]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Sample 3: cached=0.691406, full=0.699219, diff=0.00781250 [MISMATCH]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Sample 4: cached=4.843750, full=4.781250, diff=0.06250000 [MISMATCH]\n",
      "\n",
      "WARNING: max diff = 0.06250000 — investigate before proceeding\n",
      "\n",
      "Quick test: oracle score for sample 0...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  bare:   0.738281\n",
      "  oracle: 0.859375\n",
      "  delta:  -0.121094 (positive = oracle better)\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Validate two-phase caching matches single-pass (bare condition)\n",
    "print(\"=\" * 70)\n",
    "print(\"VALIDATION: Two-phase caching vs single-pass\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nComparing bare score (cached) vs full-sequence score for 5 samples...\")\n",
    "max_diff = 0.0\n",
    "for i in range(5):\n",
    "    s = samples[i]\n",
    "    nll_cached = score(s['passage'], s['query'], s['answer'], prefix_text=None)\n",
    "    nll_full = score_full_sequence(s['passage'], s['query'], s['answer'])\n",
    "    diff = abs(nll_cached - nll_full)\n",
    "    max_diff = max(max_diff, diff)\n",
    "    status = \"OK\" if diff < 0.001 else \"MISMATCH\"\n",
    "    print(f\"  Sample {i}: cached={nll_cached:.6f}, full={nll_full:.6f}, \"\n",
    "          f\"diff={diff:.8f} [{status}]\")\n",
    "\n",
    "if max_diff < 0.001:\n",
    "    print(f\"\\nVALIDATION PASSED: max diff = {max_diff:.8f}\")\n",
    "else:\n",
    "    print(f\"\\nWARNING: max diff = {max_diff:.8f} — investigate before proceeding\")\n",
    "\n",
    "# Quick test: conditioned score runs without error\n",
    "print(f\"\\nQuick test: oracle score for sample 0...\")\n",
    "nll_oracle = score(samples[0]['passage'], samples[0]['query'], samples[0]['answer'],\n",
    "                   prefix_text=samples[0]['query'])\n",
    "nll_bare = score(samples[0]['passage'], samples[0]['query'], samples[0]['answer'])\n",
    "print(f\"  bare:   {nll_bare:.6f}\")\n",
    "print(f\"  oracle: {nll_oracle:.6f}\")\n",
    "print(f\"  delta:  {nll_bare - nll_oracle:+.6f} (positive = oracle better)\")\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22610e8f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-20T14:45:10.570061Z",
     "iopub.status.busy": "2026-02-20T14:45:10.569426Z",
     "iopub.status.idle": "2026-02-20T14:55:57.350585Z",
     "shell.execute_reply": "2026-02-20T14:55:57.349598Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "SCORING ALL CONDITIONS\n",
      "======================================================================\n",
      "Starting fresh: 8 conditions x 400 samples\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a314d3c93f942ec8cceadbe0956ca72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Scoring:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 20/400 | 0.5m | ETA 10.2m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 40/400 | 1.1m | ETA 9.7m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 60/400 | 1.6m | ETA 9.1m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 80/400 | 2.1m | ETA 8.6m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 100/400 | 2.7m | ETA 8.0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 120/400 | 3.2m | ETA 7.5m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 140/400 | 3.8m | ETA 7.0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 160/400 | 4.3m | ETA 6.5m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 180/400 | 4.8m | ETA 5.9m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 200/400 | 5.4m | ETA 5.4m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 220/400 | 5.9m | ETA 4.8m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 240/400 | 6.5m | ETA 4.3m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 260/400 | 7.0m | ETA 3.8m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 280/400 | 7.5m | ETA 3.2m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 300/400 | 8.1m | ETA 2.7m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 320/400 | 8.6m | ETA 2.2m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 340/400 | 9.1m | ETA 1.6m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 360/400 | 9.7m | ETA 1.1m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 380/400 | 10.2m | ETA 0.5m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 400/400 | 10.8m | ETA 0.0m\n",
      "\n",
      "Scoring complete: 400 samples, 8 conditions in 10.8 min\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Scoring loop — 8 conditions x 400 samples\n",
    "print(\"=\" * 70)\n",
    "print(\"SCORING ALL CONDITIONS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "COND_NAMES = [\n",
    "    'bare', 'oracle',\n",
    "    'surr_universal', 'surr_extractor', 'surr_reasonant', 'surr_analytic',\n",
    "    'surr_doc_kw', 'adversarial',\n",
    "]\n",
    "\n",
    "results = []\n",
    "start_idx = 0\n",
    "\n",
    "if CHECKPOINT_PATH.exists():\n",
    "    ckpt = json.loads(CHECKPOINT_PATH.read_text())\n",
    "    if ckpt.get('n_total') == N_SAMPLES and len(ckpt.get('results', [])) > 0:\n",
    "        saved_queries = [r['query'][:50] for r in ckpt['results']]\n",
    "        current_queries = [s['query'][:50] for s in samples[:len(saved_queries)]]\n",
    "        if saved_queries == current_queries:\n",
    "            results = ckpt['results']\n",
    "            start_idx = len(results)\n",
    "            print(f\"Resuming from checkpoint: {start_idx}/{N_SAMPLES}\")\n",
    "\n",
    "if start_idx == 0:\n",
    "    print(f\"Starting fresh: {len(COND_NAMES)} conditions x {N_SAMPLES} samples\")\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "for i in tqdm(range(start_idx, N_SAMPLES), initial=start_idx, total=N_SAMPLES,\n",
    "              desc=\"Scoring\"):\n",
    "    s = samples[i]\n",
    "    query = s['query']\n",
    "    passage = s['passage']\n",
    "    answer = s['answer']\n",
    "\n",
    "    result = {\n",
    "        'query': query,\n",
    "        'answer': answer,\n",
    "        'passage_words': s['word_count'],\n",
    "    }\n",
    "\n",
    "    # 1. bare — no prefix\n",
    "    result['nll_bare'] = score(passage, query, answer)\n",
    "\n",
    "    # 2. oracle — real query as prefix\n",
    "    result['nll_oracle'] = score(passage, query, answer, prefix_text=query)\n",
    "\n",
    "    # 3-6. Surrogate prompts\n",
    "    for surr_name, surr_prompt in SURROGATES.items():\n",
    "        result[f'nll_surr_{surr_name}'] = score(\n",
    "            passage, query, answer, prefix_text=surr_prompt)\n",
    "\n",
    "    # 7. doc keywords\n",
    "    result['nll_surr_doc_kw'] = score(\n",
    "        passage, query, answer, prefix_text=s['surr_doc_kw'])\n",
    "\n",
    "    # 8. adversarial\n",
    "    result['nll_adversarial'] = score(\n",
    "        passage, query, answer, prefix_text=ADVERSARIAL_PREFIX)\n",
    "\n",
    "    results.append(result)\n",
    "\n",
    "    if (i + 1) % 20 == 0 or i == N_SAMPLES - 1:\n",
    "        ckpt = {\n",
    "            'n_total': N_SAMPLES,\n",
    "            'results': results,\n",
    "            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        }\n",
    "        CHECKPOINT_PATH.write_text(json.dumps(ckpt))\n",
    "        elapsed = time.time() - t0\n",
    "        done = i - start_idx + 1\n",
    "        eta = (N_SAMPLES - i - 1) * elapsed / done if done > 0 else 0\n",
    "        tqdm.write(f\"  Checkpoint {i+1}/{N_SAMPLES} | {elapsed/60:.1f}m | ETA {eta/60:.1f}m\")\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "elapsed = time.time() - t0\n",
    "print(f\"\\nScoring complete: {len(results)} samples, \"\n",
    "      f\"{len(COND_NAMES)} conditions in {elapsed/60:.1f} min\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13919e00",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-20T14:55:57.354620Z",
     "iopub.status.busy": "2026-02-20T14:55:57.354310Z",
     "iopub.status.idle": "2026-02-20T14:55:57.382494Z",
     "shell.execute_reply": "2026-02-20T14:55:57.381729Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "RESULTS (N=400)\n",
      "======================================================================\n",
      "\n",
      "  Condition                 NLL    vs bare        d     Win%            p   sig   Recovery\n",
      "  -------------------------------------------------------------------------------------\n",
      "  bare                   1.6022         --       --       --           --    --         --\n",
      "  oracle                 0.9044    +0.6977   +0.428    73.8%     2.32e-16   ***     100.0%\n",
      "  surr_universal         0.8527    +0.7495   +0.400    65.5%     1.39e-14   ***     107.4%\n",
      "  surr_extractor         1.0374    +0.5648   +0.308    60.8%     1.82e-09   ***      80.9%\n",
      "  surr_reasonant         0.8076    +0.7946   +0.435    64.5%     9.27e-17   ***     113.9%\n",
      "  surr_analytic          0.8440    +0.7581   +0.429    68.0%     2.03e-16   ***     108.7%\n",
      "  surr_doc_kw            0.7136    +0.8886   +0.547    77.8%     1.67e-24   ***     127.3%\n",
      "  adversarial            0.9835    +0.6187   +0.340    62.0%     3.89e-11   ***      88.7%\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Results table\n",
    "print(\"=\" * 70)\n",
    "print(f\"RESULTS (N={len(results)})\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "bare = np.array([r['nll_bare'] for r in results])\n",
    "oracle = np.array([r['nll_oracle'] for r in results])\n",
    "surr_universal = np.array([r['nll_surr_universal'] for r in results])\n",
    "surr_extractor = np.array([r['nll_surr_extractor'] for r in results])\n",
    "surr_reasonant = np.array([r['nll_surr_reasonant'] for r in results])\n",
    "surr_analytic = np.array([r['nll_surr_analytic'] for r in results])\n",
    "surr_doc_kw = np.array([r['nll_surr_doc_kw'] for r in results])\n",
    "adversarial = np.array([r['nll_adversarial'] for r in results])\n",
    "\n",
    "print(f\"\\n  {'Condition':<20} {'NLL':>8} {'vs bare':>10} {'d':>8} {'Win%':>8} \"\n",
    "      f\"{'p':>12} {'sig':>5} {'Recovery':>10}\")\n",
    "print(f\"  {'-'*85}\")\n",
    "\n",
    "# Oracle delta for recovery calculation\n",
    "oracle_delta_mean = (bare - oracle).mean()\n",
    "oracle_d = cohens_d(bare - oracle)\n",
    "\n",
    "all_conds = [\n",
    "    ('bare', bare),\n",
    "    ('oracle', oracle),\n",
    "    ('surr_universal', surr_universal),\n",
    "    ('surr_extractor', surr_extractor),\n",
    "    ('surr_reasonant', surr_reasonant),\n",
    "    ('surr_analytic', surr_analytic),\n",
    "    ('surr_doc_kw', surr_doc_kw),\n",
    "    ('adversarial', adversarial),\n",
    "]\n",
    "\n",
    "analysis = {}\n",
    "for name, nlls in all_conds:\n",
    "    mean_nll = nlls.mean()\n",
    "    if name == 'bare':\n",
    "        print(f\"  {name:<20} {mean_nll:>8.4f} {'--':>10} {'--':>8} {'--':>8} \"\n",
    "              f\"{'--':>12} {'--':>5} {'--':>10}\")\n",
    "        analysis[name] = {'mean_nll': float(mean_nll)}\n",
    "    else:\n",
    "        diff = bare - nlls  # positive = condition has lower NLL (better)\n",
    "        d = cohens_d(diff)\n",
    "        win_pct = 100 * np.mean(diff > 0)\n",
    "        _, p_val = stats.ttest_1samp(diff, 0)\n",
    "        sig = '***' if p_val < 0.001 else '**' if p_val < 0.01 else '*' if p_val < 0.05 else 'ns'\n",
    "\n",
    "        if oracle_delta_mean > 0:\n",
    "            recovery = diff.mean() / oracle_delta_mean * 100\n",
    "            rec_str = f\"{recovery:>9.1f}%\"\n",
    "        else:\n",
    "            recovery = float('nan')\n",
    "            rec_str = \"n/a\"\n",
    "\n",
    "        print(f\"  {name:<20} {mean_nll:>8.4f} {diff.mean():>+10.4f} {d:>+8.3f} \"\n",
    "              f\"{win_pct:>7.1f}% {p_val:>12.2e} {sig:>5} {rec_str:>10}\")\n",
    "        analysis[name] = {\n",
    "            'mean_nll': float(mean_nll), 'delta': float(diff.mean()),\n",
    "            'd': float(d), 'win_pct': float(win_pct), 'p': float(p_val),\n",
    "            'recovery': float(recovery) if not np.isnan(recovery) else None,\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f4cd63f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-20T14:55:57.385941Z",
     "iopub.status.busy": "2026-02-20T14:55:57.385641Z",
     "iopub.status.idle": "2026-02-20T14:55:57.406414Z",
     "shell.execute_reply": "2026-02-20T14:55:57.405522Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "KEY COMPARISONS\n",
      "======================================================================\n",
      "\n",
      "1. Oracle conditioning (upper bound):\n",
      "   d=+0.4284 (***), mean delta=+0.6977\n",
      "\n",
      "2. Adversarial (negative control):\n",
      "   d=+0.3399 (***)\n",
      "   -> Off-topic prefix helps?! Suggests structural (not semantic) effect\n",
      "\n",
      "3. Best surrogate: surr_doc_kw (d=+0.5465)\n",
      "\n",
      "4. Surrogate type ranking:\n",
      "   surr_doc_kw          d=+0.5465 (***) recovery=127%\n",
      "   surr_reasonant       d=+0.4346 (***) recovery=114%\n",
      "   surr_analytic        d=+0.4293 (***) recovery=109%\n",
      "   surr_universal       d=+0.3999 (***) recovery=107%\n",
      "   adversarial          d=+0.3399 (***) recovery=89%\n",
      "   surr_extractor       d=+0.3078 (***) recovery=81%\n",
      "\n",
      "--- Hardness gradient (oracle conditioning by difficulty) ---\n",
      "  Quintile        N     bare   oracle    delta        d\n",
      "  ----------------------------------------------------\n",
      "  Q1 easy        80   0.2613   0.4579  -0.1966   -0.402\n",
      "  Q2             79   0.5858   0.4653  +0.1205   +0.274\n",
      "  Q3             80   0.9398   0.5243  +0.4156   +0.979\n",
      "  Q4             81   1.6901   0.9153  +0.7748   +0.754\n",
      "  Q5 hard        80   4.5199   2.1538  +2.3662   +0.854\n",
      "\n",
      "  Spearman (hardness vs oracle benefit): rho=0.700 (p=4.23e-60)\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Key comparisons and hardness gradient\n",
    "print(\"=\" * 70)\n",
    "print(\"KEY COMPARISONS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# 1. Does conditioning help at all?\n",
    "d_oracle = cohens_d(bare - oracle)\n",
    "_, p_oracle = stats.ttest_1samp(bare - oracle, 0)\n",
    "sig_oracle = '***' if p_oracle < 0.001 else '**' if p_oracle < 0.01 else '*' if p_oracle < 0.05 else 'ns'\n",
    "print(f\"\\n1. Oracle conditioning (upper bound):\")\n",
    "print(f\"   d={d_oracle:+.4f} ({sig_oracle}), mean delta={bare.mean() - oracle.mean():+.4f}\")\n",
    "\n",
    "# 2. Adversarial vs bare (semantic sensitivity test)\n",
    "d_adv = cohens_d(bare - adversarial)\n",
    "_, p_adv = stats.ttest_1samp(bare - adversarial, 0)\n",
    "sig_adv = '***' if p_adv < 0.001 else '**' if p_adv < 0.01 else '*' if p_adv < 0.05 else 'ns'\n",
    "print(f\"\\n2. Adversarial (negative control):\")\n",
    "print(f\"   d={d_adv:+.4f} ({sig_adv})\")\n",
    "if d_adv < -0.05:\n",
    "    print(f\"   -> Off-topic prefix HURTS: conditioning is semantically sensitive\")\n",
    "elif d_adv > 0.05:\n",
    "    print(f\"   -> Off-topic prefix helps?! Suggests structural (not semantic) effect\")\n",
    "else:\n",
    "    print(f\"   -> Off-topic prefix neutral: conditioning effect is structural\")\n",
    "\n",
    "# 3. Best surrogate\n",
    "surr_results = {k: v for k, v in analysis.items()\n",
    "                if k.startswith('surr_') or k == 'adversarial'}\n",
    "best_surr = max(surr_results.items(), key=lambda x: x[1].get('d', -999))\n",
    "print(f\"\\n3. Best surrogate: {best_surr[0]} (d={best_surr[1]['d']:+.4f})\")\n",
    "\n",
    "# 4. Surrogate type comparison\n",
    "print(f\"\\n4. Surrogate type ranking:\")\n",
    "sorted_surrs = sorted(surr_results.items(), key=lambda x: x[1].get('d', -999), reverse=True)\n",
    "for name, info in sorted_surrs:\n",
    "    sig = '***' if info['p'] < 0.001 else '**' if info['p'] < 0.01 else '*' if info['p'] < 0.05 else 'ns'\n",
    "    rec = f\"{info['recovery']:.0f}%\" if info.get('recovery') is not None else \"n/a\"\n",
    "    print(f\"   {name:<20} d={info['d']:+.4f} ({sig}) recovery={rec}\")\n",
    "\n",
    "# 5. Hardness gradient\n",
    "print(f\"\\n--- Hardness gradient (oracle conditioning by difficulty) ---\")\n",
    "quintile_bounds = np.percentile(bare, [20, 40, 60, 80])\n",
    "quintiles = np.digitize(bare, quintile_bounds)\n",
    "\n",
    "print(f\"  {'Quintile':<12} {'N':>4} {'bare':>8} {'oracle':>8} {'delta':>8} {'d':>8}\")\n",
    "print(f\"  {'-'*52}\")\n",
    "for q in range(5):\n",
    "    mask = quintiles == q\n",
    "    n_q = mask.sum()\n",
    "    if n_q < 5:\n",
    "        continue\n",
    "    qlabel = ['Q1 easy', 'Q2', 'Q3', 'Q4', 'Q5 hard'][q]\n",
    "    b = bare[mask].mean()\n",
    "    o = oracle[mask].mean()\n",
    "    delta = (bare[mask] - oracle[mask]).mean()\n",
    "    d = cohens_d(bare[mask] - oracle[mask])\n",
    "    print(f\"  {qlabel:<12} {n_q:>4} {b:>8.4f} {o:>8.4f} {delta:>+8.4f} {d:>+8.3f}\")\n",
    "\n",
    "r_hard, p_hard = stats.spearmanr(bare, bare - oracle)\n",
    "print(f\"\\n  Spearman (hardness vs oracle benefit): rho={r_hard:.3f} (p={p_hard:.2e})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63b64db3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-20T14:55:57.410579Z",
     "iopub.status.busy": "2026-02-20T14:55:57.410282Z",
     "iopub.status.idle": "2026-02-20T14:55:58.027980Z",
     "shell.execute_reply": "2026-02-20T14:55:58.026912Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "VERDICT — Exp 02: Decoder-Only Surrogate KV Caching\n",
      "======================================================================\n",
      "\n",
      "Model: google/gemma-3-4b-it\n",
      "N: 400 samples (MS MARCO v1.1)\n",
      "\n",
      "--- Key result ---\n",
      "  Oracle conditioning: d=+0.4284 (***)\n",
      "\n",
      "  CONDITIONING WORKS in decoder-only KV cache manipulation.\n",
      "  Surrogate prompts that saw the document improve answer NLL.\n",
      "\n",
      "--- Surrogate comparison ---\n",
      "  surr_universal       d=+0.3999 (***)\n",
      "  surr_extractor       d=+0.3078 (***)\n",
      "  surr_reasonant       d=+0.4346 (***)\n",
      "  surr_analytic        d=+0.4293 (***)\n",
      "  surr_doc_kw          d=+0.5465 (***)\n",
      "  adversarial          d=+0.3399 (***)\n",
      "\n",
      "Results saved to ../../../results/decoder_only/exp01/results.json\n",
      "\n",
      "Cleaning up GPU memory...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory: 8.61 GB -> 0.01 GB\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Verdict and save\n",
    "print(\"=\" * 70)\n",
    "print(\"VERDICT — Exp 02: Decoder-Only Surrogate KV Caching\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "d_oracle = cohens_d(bare - oracle)\n",
    "_, p_oracle = stats.ttest_1samp(bare - oracle, 0)\n",
    "\n",
    "print(f\"\\nModel: {MODEL_NAME}\")\n",
    "print(f\"N: {len(results)} samples (MS MARCO v1.1)\")\n",
    "\n",
    "print(f\"\\n--- Key result ---\")\n",
    "print(f\"  Oracle conditioning: d={d_oracle:+.4f} \"\n",
    "      f\"({'***' if p_oracle < 0.001 else '**' if p_oracle < 0.01 else '*' if p_oracle < 0.05 else 'ns'})\")\n",
    "\n",
    "if d_oracle > 0.1:\n",
    "    print(f\"\\n  CONDITIONING WORKS in decoder-only KV cache manipulation.\")\n",
    "    print(f\"  Surrogate prompts that saw the document improve answer NLL.\")\n",
    "elif d_oracle > 0.05:\n",
    "    print(f\"\\n  WEAK conditioning effect. Some benefit from KV cache manipulation\")\n",
    "    print(f\"  but the effect is small.\")\n",
    "else:\n",
    "    print(f\"\\n  NO significant conditioning effect detected.\")\n",
    "    print(f\"  The causal mask position trick doesn't transfer sufficient\")\n",
    "    print(f\"  information through the KV cache to help answer generation.\")\n",
    "\n",
    "# Compare surrogates vs v3 findings\n",
    "print(f\"\\n--- Surrogate comparison ---\")\n",
    "for name in ['surr_universal', 'surr_extractor', 'surr_reasonant',\n",
    "             'surr_analytic', 'surr_doc_kw', 'adversarial']:\n",
    "    nlls = np.array([r[f'nll_{name}'] for r in results])\n",
    "    d = cohens_d(bare - nlls)\n",
    "    _, p = stats.ttest_1samp(bare - nlls, 0)\n",
    "    sig = '***' if p < 0.001 else '**' if p < 0.01 else '*' if p < 0.05 else 'ns'\n",
    "    print(f\"  {name:<20} d={d:+.4f} ({sig})\")\n",
    "\n",
    "# Save\n",
    "final_results = {\n",
    "    'experiment': 'v4_exp02_decoder_kv_caching',\n",
    "    'model': MODEL_NAME,\n",
    "    'dataset': 'ms_marco_v1.1',\n",
    "    'n_samples': len(results),\n",
    "    'seed': SEED,\n",
    "    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'conditions': {k: v for k, v in analysis.items()},\n",
    "}\n",
    "\n",
    "with open(RESULTS_DIR / 'results.json', 'w') as f:\n",
    "    json.dump(final_results, f, indent=2)\n",
    "print(f\"\\nResults saved to {RESULTS_DIR / 'results.json'}\")\n",
    "\n",
    "# Cleanup\n",
    "print(f\"\\nCleaning up GPU memory...\")\n",
    "mem_before = torch.cuda.memory_allocated() / 1e9\n",
    "del model, tokenizer\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "mem_after = torch.cuda.memory_allocated() / 1e9\n",
    "print(f\"GPU memory: {mem_before:.2f} GB -> {mem_after:.2f} GB\")\n",
    "print(\"Done!\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "08f0559f7d8e45f6ac8b681fd9f0a1f1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "12e4acd26262410f814c30eaa8a91127": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_1caade82270a4d8787846bd6bf900f13",
       "placeholder": "​",
       "style": "IPY_MODEL_5a3d8b7a04fe44e7b5c2b7cd0b36eaf3",
       "tabbable": null,
       "tooltip": null,
       "value": " 883/883 [00:33&lt;00:00, 131.80it/s, Materializing param=model.vision_tower.vision_model.post_layernorm.weight]"
      }
     },
     "1caade82270a4d8787846bd6bf900f13": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "5256910dccf440b4874e520f01cb0828": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "5a314d3c93f942ec8cceadbe0956ca72": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_f5184cd452fd444389bf3084a3af62f5",
        "IPY_MODEL_e6a917d1f6cf48be825fba139bb4257f",
        "IPY_MODEL_72b765e46ef04398ba023d46837598b8"
       ],
       "layout": "IPY_MODEL_63ed5d2c2b094bf48282bedecb0b24fe",
       "tabbable": null,
       "tooltip": null
      }
     },
     "5a3d8b7a04fe44e7b5c2b7cd0b36eaf3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "63ed5d2c2b094bf48282bedecb0b24fe": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "69af80baa9eb4031ab227145717852f4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "72b765e46ef04398ba023d46837598b8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_e45ee1300b5d438ebd9dc881bc02cb3d",
       "placeholder": "​",
       "style": "IPY_MODEL_8a288a8d6c714b408ea914dc124e2362",
       "tabbable": null,
       "tooltip": null,
       "value": " 400/400 [10:46&lt;00:00,  1.62s/it]"
      }
     },
     "87e21d75a2c94a37a2c73b66cc82a2d7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_99712aeedaf24060962e71364cbd9923",
        "IPY_MODEL_c3402fbb197b47f3b7af1b26531fbfb7",
        "IPY_MODEL_12e4acd26262410f814c30eaa8a91127"
       ],
       "layout": "IPY_MODEL_5256910dccf440b4874e520f01cb0828",
       "tabbable": null,
       "tooltip": null
      }
     },
     "8a288a8d6c714b408ea914dc124e2362": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "99712aeedaf24060962e71364cbd9923": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_69af80baa9eb4031ab227145717852f4",
       "placeholder": "​",
       "style": "IPY_MODEL_a4684c66e53b48a7b1d7a806842ff21f",
       "tabbable": null,
       "tooltip": null,
       "value": "Loading weights: 100%"
      }
     },
     "a4684c66e53b48a7b1d7a806842ff21f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "afdcdf36c65d45aba2b0ba46816b8ddc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "c3402fbb197b47f3b7af1b26531fbfb7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_08f0559f7d8e45f6ac8b681fd9f0a1f1",
       "max": 883.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_d87e51746ef74a76af286c94d2bd42a2",
       "tabbable": null,
       "tooltip": null,
       "value": 883.0
      }
     },
     "d87e51746ef74a76af286c94d2bd42a2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "e45ee1300b5d438ebd9dc881bc02cb3d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e6a917d1f6cf48be825fba139bb4257f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_f3b521fdeb924782afa869d731ffa643",
       "max": 400.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_afdcdf36c65d45aba2b0ba46816b8ddc",
       "tabbable": null,
       "tooltip": null,
       "value": 400.0
      }
     },
     "ed31268fa1a94e0f99ce9fddd8e5b26d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f100e6dda18a4492bcf9738c9a95ab71": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "f3b521fdeb924782afa869d731ffa643": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f5184cd452fd444389bf3084a3af62f5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_ed31268fa1a94e0f99ce9fddd8e5b26d",
       "placeholder": "​",
       "style": "IPY_MODEL_f100e6dda18a4492bcf9738c9a95ab71",
       "tabbable": null,
       "tooltip": null,
       "value": "Scoring: 100%"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
