{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder-Only Exp 01: Surrogate KV Caching — Condition Examples\n",
    "\n",
    "This notebook shows the actual inputs for each experimental condition using real data.\n",
    "No GPU needed — just data loading and text display.\n",
    "\n",
    "The experiment tests whether conditioning a **decoder-only** model's KV cache with\n",
    "surrogate prompts improves answer quality, using actual KV cache slicing and position\n",
    "ID alignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint verification: MATCH\n",
      "Loaded 400 MS MARCO samples (SEED=42)\n",
      "Sample 0 query: average annual temperature of Uruguay\n"
     ]
    }
   ],
   "source": [
    "import os, sys, json, re\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "sys.path.insert(0, \"../../..\")\n",
    "from lib.data import count_words\n",
    "\n",
    "SEED = 42\n",
    "N_SAMPLES = 400\n",
    "\n",
    "# ---- Load MS MARCO (same reconstruction as scoring notebook) ----\n",
    "from datasets import load_dataset\n",
    "ds = load_dataset(\"microsoft/ms_marco\", \"v1.1\", split=\"validation\")\n",
    "\n",
    "all_candidates = []\n",
    "for item in ds:\n",
    "    if len(all_candidates) >= 3 * N_SAMPLES:\n",
    "        break\n",
    "    passages = item.get('passages', {})\n",
    "    ptexts = passages.get('passage_text', [])\n",
    "    is_sel = passages.get('is_selected', [])\n",
    "    query = item.get('query', '')\n",
    "    answers = item.get('answers', [])\n",
    "    well_formed = item.get('wellFormedAnswers', [])\n",
    "    answer = None\n",
    "    if well_formed and len(well_formed) > 0 and well_formed[0] not in ('[]', ''):\n",
    "        answer = well_formed[0]\n",
    "    elif answers and len(answers) > 0 and answers[0] != 'No Answer Present.':\n",
    "        answer = answers[0]\n",
    "    if not answer:\n",
    "        continue\n",
    "    for pt, sel in zip(ptexts, is_sel):\n",
    "        wc = count_words(pt)\n",
    "        if sel == 1 and 30 <= wc <= 300:\n",
    "            all_candidates.append({\n",
    "                'passage': pt, 'query': query, 'answer': answer,\n",
    "                'word_count': wc,\n",
    "            })\n",
    "            break\n",
    "\n",
    "np.random.seed(SEED)\n",
    "indices = np.random.permutation(len(all_candidates))\n",
    "samples = [all_candidates[i] for i in indices[:N_SAMPLES]]\n",
    "del ds, all_candidates\n",
    "\n",
    "STOP_WORDS = {\n",
    "    'a', 'an', 'the', 'is', 'are', 'was', 'were', 'be', 'been', 'being',\n",
    "    'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could',\n",
    "    'should', 'may', 'might', 'can', 'shall', 'to', 'of', 'in', 'for',\n",
    "    'on', 'with', 'at', 'by', 'from', 'as', 'into', 'through', 'during',\n",
    "    'before', 'after', 'above', 'below', 'between', 'and', 'but', 'or',\n",
    "    'not', 'no', 'if', 'then', 'than', 'so', 'up', 'out', 'about',\n",
    "    'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those',\n",
    "    'it', 'its', 'i', 'me', 'my', 'we', 'our', 'you', 'your', 'he',\n",
    "    'him', 'his', 'she', 'her', 'they', 'them', 'their', 'how', 'when',\n",
    "    'where', 'why', 'much', 'many', 'some', 'any', 'all', 'each',\n",
    "    'does', 'also', 'just', 'more', 'most', 'very', 'too', 'only',\n",
    "}\n",
    "\n",
    "def extract_keywords(text):\n",
    "    words = re.sub(r'[^\\w\\s]', '', text.lower()).split()\n",
    "    return [w for w in words if w not in STOP_WORDS and len(w) > 2]\n",
    "\n",
    "def make_doc_keywords(passage):\n",
    "    content_words = extract_keywords(passage)\n",
    "    if not content_words:\n",
    "        return \"information\"\n",
    "    counts = Counter(content_words)\n",
    "    return \" \".join(w for w, _ in counts.most_common(5))\n",
    "\n",
    "SURROGATES = {\n",
    "    'universal': \"Analyze the following text for all key entities, factual claims, and logical relationships.\",\n",
    "    'extractor': \"Examine this document specifically for data points, dates, numerical values, and specific named attributes.\",\n",
    "    'reasonant': \"Evaluate the underlying arguments, sentiment, and intent of the following passage.\",\n",
    "    'analytic': \"Provide a technical breakdown of the systems and processes described in this text.\",\n",
    "}\n",
    "\n",
    "ADVERSARIAL_PREFIX = \"The recipe calls for two cups of flour, one cup of sugar, and a pinch of salt.\"\n",
    "\n",
    "# Verify against checkpoint\n",
    "ckpt_path = Path(\"../../../results/decoder_only/exp01/checkpoint.json\")\n",
    "if ckpt_path.exists():\n",
    "    ckpt = json.loads(ckpt_path.read_text())\n",
    "    results = ckpt.get('results', [])\n",
    "    if results and results[0].get('query', '')[:50] == samples[0]['query'][:50]:\n",
    "        print(f\"Checkpoint verification: MATCH\")\n",
    "    elif results:\n",
    "        print(f\"Checkpoint verification: MISMATCH\")\n",
    "        print(f\"  Checkpoint: {results[0].get('query', '')[:50]}\")\n",
    "        print(f\"  Samples:    {samples[0]['query'][:50]}\")\n",
    "else:\n",
    "    print(\"No checkpoint found\")\n",
    "\n",
    "print(f\"Loaded {len(samples)} MS MARCO samples (SEED={SEED})\")\n",
    "print(f\"Sample 0 query: {samples[0]['query'][:70]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SAMPLE 0\n",
      "================================================================================\n",
      "  Query:        average annual temperature of Uruguay\n",
      "  Answer:       Very mild at 15.8 degrees Celsius (60.4 degrees Fahrenheit).\n",
      "  Document:     Average Temperatures in Montevideo, Uruguay. 1  The average annual temperature in Montevideo, Urugua...\n",
      "  Doc words:    76\n",
      "  Doc keywords: average degrees temperatures montevideo uruguay\n",
      "\n",
      "================================================================================\n",
      "SAMPLE 1\n",
      "================================================================================\n",
      "  Query:        average cost for an acre of land in arizona\n",
      "  Answer:       $4,300 per acre.\n",
      "  Document:     Arizona. With more than 72 million acres, Arizona was one of the largest states in the country. Howe...\n",
      "  Doc words:    102\n",
      "  Doc keywords: acre land average less arizona\n",
      "\n",
      "================================================================================\n",
      "SAMPLE 2\n",
      "================================================================================\n",
      "  Query:        where can i buy nematodes\n",
      "  Answer:       Here to buy THE GOOD BUGS Supplier beneficial insects, mites and nematodes for commercial growers Buying insects from a reliable source is a very important step. yes\n",
      "  Document:     Where to buy THE GOOD BUGS Supplier beneficial insects, mites and nematodes for commercial growers B...\n",
      "  Doc words:    42\n",
      "  Doc keywords: insects buy good bugs supplier\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show 3 representative samples\n",
    "for idx in [0, 1, 2]:\n",
    "    ex = samples[idx]\n",
    "    doc_kw = make_doc_keywords(ex['passage'])\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"SAMPLE {idx}\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"  Query:        {ex['query']}\")\n",
    "    print(f\"  Answer:       {ex['answer']}\")\n",
    "    print(f\"  Document:     {ex['passage'][:100]}...\")\n",
    "    print(f\"  Doc words:    {ex['word_count']}\")\n",
    "    print(f\"  Doc keywords: {doc_kw}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "HOW THIS EXPERIMENT WORKS\n",
      "================================================================================\n",
      "\n",
      "  Gemma 2 2B is a DECODER-ONLY model with causal attention.\n",
      "  In standard left-to-right processing [doc, query, answer]:\n",
      "    - Doc tokens can only see other doc tokens (causal mask)\n",
      "    - Query tokens can see doc + query\n",
      "    - Answer tokens can see everything\n",
      "\n",
      "  KEY LIMITATION: Doc tokens NEVER see the query. Their KV\n",
      "  representations are query-blind.\n",
      "\n",
      "  THE TRICK: Put a surrogate prompt BEFORE the document:\n",
      "    [surrogate, doc, query, answer]\n",
      "  Now doc tokens can see the surrogate via causal attention,\n",
      "  and their KV representations encode surrogate-aware features.\n",
      "\n",
      "  PRODUCTION DEPLOYMENT:\n",
      "    Phase A (offline): Forward [surrogate + doc] -> KV cache\n",
      "                       Slice off surrogate entries -> store doc KV\n",
      "    Phase B (online):  Load cached doc KV\n",
      "                       Forward [query + answer] with cached KV\n",
      "                       Set position_ids to continue after doc\n",
      "\n",
      "  POSITION ALIGNMENT:\n",
      "    If surrogate = S tokens, doc = D tokens:\n",
      "      Conditioning: [BOS, s1, ..., sS, sep, d1, ..., dD]\n",
      "      Positions:    [0,   1,  ..., S,  S+1, S+2, ..., S+1+D]\n",
      "      After slice:  keep [d1, ..., dD] at positions [S+2, ..., S+1+D]\n",
      "      Query starts: position S+2+D\n",
      "\n",
      "    RoPE relative positions are IDENTICAL across conditions:\n",
      "      last_doc - first_query distance = 2 (always)\n",
      "      doc internal distances = unchanged\n",
      "    So the position offset does NOT confound the comparison.\n"
     ]
    }
   ],
   "source": [
    "ex = samples[0]\n",
    "doc = ex['passage']\n",
    "query = ex['query']\n",
    "answer = ex['answer']\n",
    "doc_kw = make_doc_keywords(doc)\n",
    "doc_short = doc[:80]\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"HOW THIS EXPERIMENT WORKS\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "print(\"  Gemma 2 2B is a DECODER-ONLY model with causal attention.\")\n",
    "print(\"  In standard left-to-right processing [doc, query, answer]:\")\n",
    "print(\"    - Doc tokens can only see other doc tokens (causal mask)\")\n",
    "print(\"    - Query tokens can see doc + query\")\n",
    "print(\"    - Answer tokens can see everything\")\n",
    "print()\n",
    "print(\"  KEY LIMITATION: Doc tokens NEVER see the query. Their KV\")\n",
    "print(\"  representations are query-blind.\")\n",
    "print()\n",
    "print(\"  THE TRICK: Put a surrogate prompt BEFORE the document:\")\n",
    "print(\"    [surrogate, doc, query, answer]\")\n",
    "print(\"  Now doc tokens can see the surrogate via causal attention,\")\n",
    "print(\"  and their KV representations encode surrogate-aware features.\")\n",
    "print()\n",
    "print(\"  PRODUCTION DEPLOYMENT:\")\n",
    "print(\"    Phase A (offline): Forward [surrogate + doc] -> KV cache\")\n",
    "print(\"                       Slice off surrogate entries -> store doc KV\")\n",
    "print(\"    Phase B (online):  Load cached doc KV\")\n",
    "print(\"                       Forward [query + answer] with cached KV\")\n",
    "print(\"                       Set position_ids to continue after doc\")\n",
    "print()\n",
    "print(\"  POSITION ALIGNMENT:\")\n",
    "print(\"    If surrogate = S tokens, doc = D tokens:\")\n",
    "print(\"      Conditioning: [BOS, s1, ..., sS, sep, d1, ..., dD]\")\n",
    "print(\"      Positions:    [0,   1,  ..., S,  S+1, S+2, ..., S+1+D]\")\n",
    "print(\"      After slice:  keep [d1, ..., dD] at positions [S+2, ..., S+1+D]\")\n",
    "print(\"      Query starts: position S+2+D\")\n",
    "print()\n",
    "print(\"    RoPE relative positions are IDENTICAL across conditions:\")\n",
    "print(\"      last_doc - first_query distance = 2 (always)\")\n",
    "print(\"      doc internal distances = unchanged\")\n",
    "print(\"    So the position offset does NOT confound the comparison.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CONDITIONS (8 total) — Sample 0\n",
      "================================================================================\n",
      "\n",
      "  Query:   average annual temperature of Uruguay\n",
      "  Answer:  Very mild at 15.8 degrees Celsius (60.4 degrees Fahrenheit).\n",
      "  Doc:     Average Temperatures in Montevideo, Uruguay. 1  The average annual temperature i...\n",
      "\n",
      "--- CONDITION 1: bare (LOWER BOUND) ---\n",
      "\n",
      "  Phase A cache: [doc]\n",
      "  Slicing:       none\n",
      "  Phase B input: [query + answer]\n",
      "\n",
      "  Standard causal LM. Doc tokens are query-blind.\n",
      "  Equivalent to a single forward pass of [doc, query, answer].\n",
      "\n",
      "  Phase A sees: \"Average Temperatures in Montevideo, Uruguay. 1  The average annual temperature i...\"\n",
      "  Phase B sees: \"\\naverage annual temperature of Uruguay\\nVery mild at 15.8 degrees Celsius (60.4 degrees Fahrenheit).\"\n",
      "\n",
      "--- CONDITION 2: oracle (UPPER BOUND) ---\n",
      "\n",
      "  Prefix:        \"average annual temperature of Uruguay\"\n",
      "  Phase A cache: [prefix + \\n + doc]\n",
      "  Slicing:       remove prefix entries\n",
      "  Phase B input: [query + answer] (same query again)\n",
      "\n",
      "  The real query conditions the doc KV via causal attention.\n",
      "  Doc tokens now \"know\" what the query is about.\n",
      "  This is the ceiling -- but uses future knowledge (cheating).\n",
      "\n",
      "  Phase A sees: \"average annual temperature of Uruguay\\nAverage Temperatures in Montevideo, Uruguay. 1  The average annual temperature i...\"\n",
      "  After slice:  doc KV only (at positions 7+)\n",
      "  Phase B sees: \"\\naverage annual temperature of Uruguay\\nVery mild at 15.8 degrees Celsius (60.4 degrees Fahrenheit).\"\n",
      "\n",
      "--- CONDITION 3: surr_universal ---\n",
      "\n",
      "  Prefix:        \"Analyze the following text for all key entities, factual claims, and logical relationships.\"\n",
      "  Phase A cache: [prefix + \\n + doc]\n",
      "  Slicing:       remove prefix entries\n",
      "  Phase B input: [query + answer]\n",
      "\n",
      "  Generic analysis prompt. Forces the model to activate entity,\n",
      "  factual, and relational features in the document KV.\n",
      "\n",
      "  Phase A sees: \"Analyze the following text for all key entities, factual claims, and logical relationships.\\nAverage Temperatures in Montevideo, Uruguay. 1  The average annual temperature i...\"\n",
      "\n",
      "--- CONDITION 4: surr_extractor ---\n",
      "\n",
      "  Prefix:        \"Examine this document specifically for data points, dates, numerical values, and specific named attributes.\"\n",
      "  Phase A cache: [prefix + \\n + doc]\n",
      "  Slicing:       remove prefix entries\n",
      "  Phase B input: [query + answer]\n",
      "\n",
      "  Targets numerical/factual extraction. Best for data-point queries.\n",
      "\n",
      "  Phase A sees: \"Examine this document specifically for data points, dates, numerical values, and specific named attributes.\\nAverage Temperatures in Montevideo, Uruguay. 1  The average annual temperature i...\"\n",
      "\n",
      "--- CONDITION 5: surr_reasonant ---\n",
      "\n",
      "  Prefix:        \"Evaluate the underlying arguments, sentiment, and intent of the following passage.\"\n",
      "  Phase A cache: [prefix + \\n + doc]\n",
      "  Slicing:       remove prefix entries\n",
      "  Phase B input: [query + answer]\n",
      "\n",
      "  Targets reasoning/sentiment. Best for \"how\" or \"why\" queries.\n",
      "\n",
      "  Phase A sees: \"Evaluate the underlying arguments, sentiment, and intent of the following passage.\\nAverage Temperatures in Montevideo, Uruguay. 1  The average annual temperature i...\"\n",
      "\n",
      "--- CONDITION 6: surr_analytic ---\n",
      "\n",
      "  Prefix:        \"Provide a technical breakdown of the systems and processes described in this text.\"\n",
      "  Phase A cache: [prefix + \\n + doc]\n",
      "  Slicing:       remove prefix entries\n",
      "  Phase B input: [query + answer]\n",
      "\n",
      "  Targets technical/process analysis. Best for engineering docs.\n",
      "\n",
      "  Phase A sees: \"Provide a technical breakdown of the systems and processes described in this text.\\nAverage Temperatures in Montevideo, Uruguay. 1  The average annual temperature i...\"\n",
      "\n",
      "--- CONDITION 7: surr_doc_kw ---\n",
      "\n",
      "  Prefix:        \"average degrees temperatures montevideo uruguay\"  (top-5 TF keywords from the document)\n",
      "  Phase A cache: [prefix + \\n + doc]\n",
      "  Slicing:       remove prefix entries\n",
      "  Phase B input: [query + answer]\n",
      "\n",
      "  Document-derived surrogate. In v3 (encoder-decoder), this was the\n",
      "  best non-oracle condition, capturing 89% of oracle benefit.\n",
      "\n",
      "  Phase A sees: \"average degrees temperatures montevideo uruguay\\nAverage Temperatures in Montevideo, Uruguay. 1  The average annual temperature i...\"\n",
      "\n",
      "--- CONDITION 8: adversarial (NEGATIVE CONTROL) ---\n",
      "\n",
      "  Prefix:        \"The recipe calls for two cups of flour, one cup of sugar, and a pinch of salt.\"\n",
      "  Phase A cache: [prefix + \\n + doc]\n",
      "  Slicing:       remove prefix entries\n",
      "  Phase B input: [query + answer]\n",
      "\n",
      "  Completely off-topic prefix. Tests semantic sensitivity:\n",
      "  - If adversarial HURTS vs bare: conditioning is semantic\n",
      "  - If adversarial HELPS like others: effect is structural\n",
      "  - If adversarial = bare: prefix content is ignored\n",
      "\n",
      "  Phase A sees: \"The recipe calls for two cups of flour, one cup of sugar, and a pinch of salt.\\nAverage Temperatures in Montevideo, Uruguay. 1  The average annual temperature i...\"\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(f\"CONDITIONS (8 total) — Sample 0\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "print(f\"  Query:   {query}\")\n",
    "print(f\"  Answer:  {answer}\")\n",
    "print(f\"  Doc:     {doc_short}...\")\n",
    "\n",
    "print()\n",
    "print(\"--- CONDITION 1: bare (LOWER BOUND) ---\")\n",
    "print()\n",
    "print(\"  Phase A cache: [doc]\")\n",
    "print(\"  Slicing:       none\")\n",
    "print(\"  Phase B input: [query + answer]\")\n",
    "print()\n",
    "print(\"  Standard causal LM. Doc tokens are query-blind.\")\n",
    "print(\"  Equivalent to a single forward pass of [doc, query, answer].\")\n",
    "print()\n",
    "print(f\"  Phase A sees: \\\"{doc_short}...\\\"\")\n",
    "print(f\"  Phase B sees: \\\"\\\\n{query}\\\\n{answer}\\\"\")\n",
    "\n",
    "print()\n",
    "print(\"--- CONDITION 2: oracle (UPPER BOUND) ---\")\n",
    "print()\n",
    "print(f\"  Prefix:        \\\"{query}\\\"\")\n",
    "print(f\"  Phase A cache: [prefix + \\\\n + doc]\")\n",
    "print(f\"  Slicing:       remove prefix entries\")\n",
    "print(f\"  Phase B input: [query + answer] (same query again)\")\n",
    "print()\n",
    "print(\"  The real query conditions the doc KV via causal attention.\")\n",
    "print(\"  Doc tokens now \\\"know\\\" what the query is about.\")\n",
    "print(\"  This is the ceiling -- but uses future knowledge (cheating).\")\n",
    "print()\n",
    "print(f\"  Phase A sees: \\\"{query}\\\\n{doc_short}...\\\"\")\n",
    "print(f\"  After slice:  doc KV only (at positions {len(query.split())+2}+)\")\n",
    "print(f\"  Phase B sees: \\\"\\\\n{query}\\\\n{answer}\\\"\")\n",
    "\n",
    "print()\n",
    "print(\"--- CONDITION 3: surr_universal ---\")\n",
    "print()\n",
    "surr = SURROGATES['universal']\n",
    "print(f\"  Prefix:        \\\"{surr}\\\"\")\n",
    "print(f\"  Phase A cache: [prefix + \\\\n + doc]\")\n",
    "print(f\"  Slicing:       remove prefix entries\")\n",
    "print(f\"  Phase B input: [query + answer]\")\n",
    "print()\n",
    "print(\"  Generic analysis prompt. Forces the model to activate entity,\")\n",
    "print(\"  factual, and relational features in the document KV.\")\n",
    "print()\n",
    "print(f\"  Phase A sees: \\\"{surr}\\\\n{doc_short}...\\\"\")\n",
    "\n",
    "print()\n",
    "print(\"--- CONDITION 4: surr_extractor ---\")\n",
    "print()\n",
    "surr = SURROGATES['extractor']\n",
    "print(f\"  Prefix:        \\\"{surr}\\\"\")\n",
    "print(f\"  Phase A cache: [prefix + \\\\n + doc]\")\n",
    "print(f\"  Slicing:       remove prefix entries\")\n",
    "print(f\"  Phase B input: [query + answer]\")\n",
    "print()\n",
    "print(\"  Targets numerical/factual extraction. Best for data-point queries.\")\n",
    "print()\n",
    "print(f\"  Phase A sees: \\\"{surr}\\\\n{doc_short}...\\\"\")\n",
    "\n",
    "print()\n",
    "print(\"--- CONDITION 5: surr_reasonant ---\")\n",
    "print()\n",
    "surr = SURROGATES['reasonant']\n",
    "print(f\"  Prefix:        \\\"{surr}\\\"\")\n",
    "print(f\"  Phase A cache: [prefix + \\\\n + doc]\")\n",
    "print(f\"  Slicing:       remove prefix entries\")\n",
    "print(f\"  Phase B input: [query + answer]\")\n",
    "print()\n",
    "print(\"  Targets reasoning/sentiment. Best for \\\"how\\\" or \\\"why\\\" queries.\")\n",
    "print()\n",
    "print(f\"  Phase A sees: \\\"{surr}\\\\n{doc_short}...\\\"\")\n",
    "\n",
    "print()\n",
    "print(\"--- CONDITION 6: surr_analytic ---\")\n",
    "print()\n",
    "surr = SURROGATES['analytic']\n",
    "print(f\"  Prefix:        \\\"{surr}\\\"\")\n",
    "print(f\"  Phase A cache: [prefix + \\\\n + doc]\")\n",
    "print(f\"  Slicing:       remove prefix entries\")\n",
    "print(f\"  Phase B input: [query + answer]\")\n",
    "print()\n",
    "print(\"  Targets technical/process analysis. Best for engineering docs.\")\n",
    "print()\n",
    "print(f\"  Phase A sees: \\\"{surr}\\\\n{doc_short}...\\\"\")\n",
    "\n",
    "print()\n",
    "print(\"--- CONDITION 7: surr_doc_kw ---\")\n",
    "print()\n",
    "print(f\"  Prefix:        \\\"{doc_kw}\\\"  (top-5 TF keywords from the document)\")\n",
    "print(f\"  Phase A cache: [prefix + \\\\n + doc]\")\n",
    "print(f\"  Slicing:       remove prefix entries\")\n",
    "print(f\"  Phase B input: [query + answer]\")\n",
    "print()\n",
    "print(\"  Document-derived surrogate. In v3 (encoder-decoder), this was the\")\n",
    "print(\"  best non-oracle condition, capturing 89% of oracle benefit.\")\n",
    "print()\n",
    "print(f\"  Phase A sees: \\\"{doc_kw}\\\\n{doc_short}...\\\"\")\n",
    "\n",
    "print()\n",
    "print(\"--- CONDITION 8: adversarial (NEGATIVE CONTROL) ---\")\n",
    "print()\n",
    "print(f\"  Prefix:        \\\"{ADVERSARIAL_PREFIX}\\\"\")\n",
    "print(f\"  Phase A cache: [prefix + \\\\n + doc]\")\n",
    "print(f\"  Slicing:       remove prefix entries\")\n",
    "print(f\"  Phase B input: [query + answer]\")\n",
    "print()\n",
    "print(\"  Completely off-topic prefix. Tests semantic sensitivity:\")\n",
    "print(\"  - If adversarial HURTS vs bare: conditioning is semantic\")\n",
    "print(\"  - If adversarial HELPS like others: effect is structural\")\n",
    "print(\"  - If adversarial = bare: prefix content is ignored\")\n",
    "print()\n",
    "print(f\"  Phase A sees: \\\"{ADVERSARIAL_PREFIX}\\\\n{doc_short}...\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SUMMARY TABLE — All conditions for Sample 0\n",
      "================================================================================\n",
      "\n",
      "  #   Condition             Prefix words Phase A: conditioning input (first 70 chars)\n",
      "  --------------------------------------------------------------------------------------------------------------\n",
      "  1   bare                        (none)   Average Temperatures in Montevideo, Uruguay. 1  The average annual tem...\n",
      "  2   oracle                          5w   average annual temperature of Uruguay\\nAverage Temperatures in Montevi...\n",
      "  3   surr_universal                 13w   Analyze the following text for all key entities, factual claims, and l...\n",
      "  4   surr_extractor                 14w   Examine this document specifically for data points, dates, numerical v...\n",
      "  5   surr_reasonant                 11w   Evaluate the underlying arguments, sentiment, and intent of the follow...\n",
      "  6   surr_analytic                  13w   Provide a technical breakdown of the systems and processes described i...\n",
      "  7   surr_doc_kw                     5w   average degrees temperatures montevideo uruguay\\nAverage Temperatures ...\n",
      "  8   adversarial                    17w   The recipe calls for two cups of flour, one cup of sugar, and a pinch ...\n",
      "\n",
      "  Phase B (same for ALL conditions): \"\\naverage annual temperature of Uruguay\\nVery mild at 15.8 degrees Celsius (60.4 degrees Fahrenheit).\"\n",
      "  NLL measured on: answer tokens only (9 words)\n",
      "\n",
      "  For conditions 2-8, Phase A output is sliced: prefix KV entries\n",
      "  are removed, keeping only the conditioned document KV.\n",
      "  Phase B position_ids are set to continue after the original\n",
      "  conditioning sequence (preserving correct RoPE distances).\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"SUMMARY TABLE — All conditions for Sample 0\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "conditions = [\n",
    "    ('bare',           None),\n",
    "    ('oracle',         query),\n",
    "    ('surr_universal',  SURROGATES['universal']),\n",
    "    ('surr_extractor',  SURROGATES['extractor']),\n",
    "    ('surr_reasonant',  SURROGATES['reasonant']),\n",
    "    ('surr_analytic',   SURROGATES['analytic']),\n",
    "    ('surr_doc_kw',     doc_kw),\n",
    "    ('adversarial',     ADVERSARIAL_PREFIX),\n",
    "]\n",
    "\n",
    "print(f\"  {'#':<3} {'Condition':<20} {'Prefix words':>13} {'Phase A: conditioning input (first 70 chars)'}\")\n",
    "print(f\"  {'-'*110}\")\n",
    "for i, (name, prefix) in enumerate(conditions, 1):\n",
    "    if prefix is None:\n",
    "        pw = '(none)'\n",
    "        phase_a = doc[:70]\n",
    "    else:\n",
    "        pw = f\"{len(prefix.split())}w\"\n",
    "        phase_a = (prefix + '\\\\n' + doc)[:70]\n",
    "    print(f\"  {i:<3} {name:<20} {pw:>13}   {phase_a}...\")\n",
    "\n",
    "print()\n",
    "print(f\"  Phase B (same for ALL conditions): \\\"\\\\n{query}\\\\n{answer}\\\"\")\n",
    "print(f\"  NLL measured on: answer tokens only ({len(answer.split())} words)\")\n",
    "print()\n",
    "print(\"  For conditions 2-8, Phase A output is sliced: prefix KV entries\")\n",
    "print(\"  are removed, keeping only the conditioned document KV.\")\n",
    "print(\"  Phase B position_ids are set to continue after the original\")\n",
    "print(\"  conditioning sequence (preserving correct RoPE distances).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SAMPLE 1\n",
      "================================================================================\n",
      "  Query:        average cost for an acre of land in arizona\n",
      "  Answer:       $4,300 per acre.\n",
      "  Document:     Arizona. With more than 72 million acres, Arizona was one of the largest states in the country. Howe...\n",
      "  Doc words:    102\n",
      "  Doc keywords: acre land average less arizona\n",
      "\n",
      "  #   Condition             Prefix words Phase A input (first 70 chars)\n",
      "  --------------------------------------------------------------------------------------------------------------\n",
      "  1   bare                        (none)   Arizona. With more than 72 million acres, Arizona was one of the large...\n",
      "  2   oracle                          9w   average cost for an acre of land in arizona\\nArizona. With more than 7...\n",
      "  3   surr_universal                 13w   Analyze the following text for all key entities, factual claims, and l...\n",
      "  4   surr_extractor                 14w   Examine this document specifically for data points, dates, numerical v...\n",
      "  5   surr_reasonant                 11w   Evaluate the underlying arguments, sentiment, and intent of the follow...\n",
      "  6   surr_analytic                  13w   Provide a technical breakdown of the systems and processes described i...\n",
      "  7   surr_doc_kw                     5w   acre land average less arizona\\nArizona. With more than 72 million acr...\n",
      "  8   adversarial                    17w   The recipe calls for two cups of flour, one cup of sugar, and a pinch ...\n",
      "\n",
      "  Phase B: \"\\naverage cost for an acre of land in arizona\\n$4,300 per acre.\"\n",
      "\n",
      "================================================================================\n",
      "SAMPLE 2\n",
      "================================================================================\n",
      "  Query:        where can i buy nematodes\n",
      "  Answer:       Here to buy THE GOOD BUGS Supplier beneficial insects, mites and nematodes for commercial growers Buying insects from a reliable source is a very important step. yes\n",
      "  Document:     Where to buy THE GOOD BUGS Supplier beneficial insects, mites and nematodes for commercial growers B...\n",
      "  Doc words:    42\n",
      "  Doc keywords: insects buy good bugs supplier\n",
      "\n",
      "  #   Condition             Prefix words Phase A input (first 70 chars)\n",
      "  --------------------------------------------------------------------------------------------------------------\n",
      "  1   bare                        (none)   Where to buy THE GOOD BUGS Supplier beneficial insects, mites and nema...\n",
      "  2   oracle                          5w   where can i buy nematodes\\nWhere to buy THE GOOD BUGS Supplier benefic...\n",
      "  3   surr_universal                 13w   Analyze the following text for all key entities, factual claims, and l...\n",
      "  4   surr_extractor                 14w   Examine this document specifically for data points, dates, numerical v...\n",
      "  5   surr_reasonant                 11w   Evaluate the underlying arguments, sentiment, and intent of the follow...\n",
      "  6   surr_analytic                  13w   Provide a technical breakdown of the systems and processes described i...\n",
      "  7   surr_doc_kw                     5w   insects buy good bugs supplier\\nWhere to buy THE GOOD BUGS Supplier be...\n",
      "  8   adversarial                    17w   The recipe calls for two cups of flour, one cup of sugar, and a pinch ...\n",
      "\n",
      "  Phase B: \"\\nwhere can i buy nematodes\\nHere to buy THE GOOD BUGS Supplier beneficial insects, mites and nematodes for commercial growers Buying insects from a reliable source is a very important step. yes\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show full condition details for 2 more samples\n",
    "for idx in [1, 2]:\n",
    "    ex = samples[idx]\n",
    "    dkw = make_doc_keywords(ex['passage'])\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"SAMPLE {idx}\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"  Query:        {ex['query']}\")\n",
    "    print(f\"  Answer:       {ex['answer']}\")\n",
    "    print(f\"  Document:     {ex['passage'][:100]}...\")\n",
    "    print(f\"  Doc words:    {ex['word_count']}\")\n",
    "    print(f\"  Doc keywords: {dkw}\")\n",
    "    print()\n",
    "\n",
    "    conds = [\n",
    "        ('bare',           None),\n",
    "        ('oracle',         ex['query']),\n",
    "        ('surr_universal',  SURROGATES['universal']),\n",
    "        ('surr_extractor',  SURROGATES['extractor']),\n",
    "        ('surr_reasonant',  SURROGATES['reasonant']),\n",
    "        ('surr_analytic',   SURROGATES['analytic']),\n",
    "        ('surr_doc_kw',     dkw),\n",
    "        ('adversarial',     ADVERSARIAL_PREFIX),\n",
    "    ]\n",
    "\n",
    "    print(f\"  {'#':<3} {'Condition':<20} {'Prefix words':>13} {'Phase A input (first 70 chars)'}\")\n",
    "    print(f\"  {'-'*110}\")\n",
    "    for i, (name, prefix) in enumerate(conds, 1):\n",
    "        if prefix is None:\n",
    "            pw = '(none)'\n",
    "            phase_a = ex['passage'][:70]\n",
    "        else:\n",
    "            pw = f\"{len(prefix.split())}w\"\n",
    "            phase_a = (prefix + '\\\\n' + ex['passage'])[:70]\n",
    "        print(f\"  {i:<3} {name:<20} {pw:>13}   {phase_a}...\")\n",
    "    print()\n",
    "    print(f\"  Phase B: \\\"\\\\n{ex['query']}\\\\n{ex['answer']}\\\"\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "WHAT TO LOOK FOR IN RESULTS\n",
      "================================================================================\n",
      "\n",
      "  1. oracle >> bare?\n",
      "     Does conditioning with the real query improve answer NLL?\n",
      "     Expected: yes (d~+0.4 based on v3 findings).\n",
      "\n",
      "  2. surrogates > bare?\n",
      "     Do generic prompt surrogates help without knowing the query?\n",
      "     This is the practical question -- surrogates are free at offline time.\n",
      "\n",
      "  3. adversarial vs bare?\n",
      "     The semantic sensitivity test:\n",
      "     - adversarial < bare: conditioning is content-sensitive (good)\n",
      "     - adversarial ~ surrogates: effect is purely structural (interesting)\n",
      "     - adversarial = bare: prefix is ignored entirely\n",
      "\n",
      "  4. Recovery rate: (surr - bare) / (oracle - bare) x 100%\n",
      "     How much of the oracle ceiling does each surrogate capture?\n",
      "     v3 found surrogates often EXCEED oracle (>100% recovery)\n",
      "     because the real query can create semantic interference.\n",
      "\n",
      "  5. Surrogate type ranking:\n",
      "     Do task-specific prompts (extractor, reasonant, analytic)\n",
      "     outperform generic ones (universal)? Or is the effect\n",
      "     mostly structural (v3 found 85% structural)?\n",
      "\n",
      "  6. Hardness gradient:\n",
      "     Does conditioning help more for harder queries (higher bare NLL)?\n",
      "     v3 found huge gains for hard queries, slight degradation for easy.\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"WHAT TO LOOK FOR IN RESULTS\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "print(\"  1. oracle >> bare?\")\n",
    "print(\"     Does conditioning with the real query improve answer NLL?\")\n",
    "print(\"     Expected: yes (d~+0.4 based on v3 findings).\")\n",
    "print()\n",
    "print(\"  2. surrogates > bare?\")\n",
    "print(\"     Do generic prompt surrogates help without knowing the query?\")\n",
    "print(\"     This is the practical question -- surrogates are free at offline time.\")\n",
    "print()\n",
    "print(\"  3. adversarial vs bare?\")\n",
    "print(\"     The semantic sensitivity test:\")\n",
    "print(\"     - adversarial < bare: conditioning is content-sensitive (good)\")\n",
    "print(\"     - adversarial ~ surrogates: effect is purely structural (interesting)\")\n",
    "print(\"     - adversarial = bare: prefix is ignored entirely\")\n",
    "print()\n",
    "print(\"  4. Recovery rate: (surr - bare) / (oracle - bare) x 100%\")\n",
    "print(\"     How much of the oracle ceiling does each surrogate capture?\")\n",
    "print(\"     v3 found surrogates often EXCEED oracle (>100% recovery)\")\n",
    "print(\"     because the real query can create semantic interference.\")\n",
    "print()\n",
    "print(\"  5. Surrogate type ranking:\")\n",
    "print(\"     Do task-specific prompts (extractor, reasonant, analytic)\")\n",
    "print(\"     outperform generic ones (universal)? Or is the effect\")\n",
    "print(\"     mostly structural (v3 found 85% structural)?\")\n",
    "print()\n",
    "print(\"  6. Hardness gradient:\")\n",
    "print(\"     Does conditioning help more for harder queries (higher bare NLL)?\")\n",
    "print(\"     v3 found huge gains for hard queries, slight degradation for easy.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cu124.m127",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/base-cu124:m127"
  },
  "kernelspec": {
   "display_name": "Python 3 (Local)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
