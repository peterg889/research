{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f25a901b",
   "metadata": {},
   "source": [
    "# Experiment 01: Decoder-Only Surrogate Prefix Conditioning\n",
    "\n",
    "## Motivation\n",
    "\n",
    "In a causal (decoder-only) model, document tokens D cannot attend to a query Q\n",
    "that comes after them. We test whether prepending a **surrogate query** before the\n",
    "document allows D to encode query-relevant features via causal attention, improving\n",
    "downstream answer NLL.\n",
    "\n",
    "## Method — Two-Phase KV Cache with Natural Encoding + RoPE Repositioning\n",
    "\n",
    "We use Gemma 3 12B-IT with a two-phase scoring approach that preserves semantic\n",
    "signal during encoding while eliminating position confounds during inference.\n",
    "\n",
    "**Phase A (conditioning):** Encode `[BOS] + prefix + \\n + doc` with **natural**\n",
    "(sequential) positions. Prefix at `[1, ..., P]`, doc at `[1+P+NL, ..., P+NL+D]`.\n",
    "Doc tokens attend to prefix via normal causal attention at in-distribution RoPE\n",
    "relative distances — the semantic content of the prefix can genuinely influence\n",
    "doc representations.\n",
    "\n",
    "**Slice:** Remove the first `1 + len(prefix) + len(\\n)` KV entries, keeping only\n",
    "the doc KV cache.\n",
    "\n",
    "**Reposition:** Rotate all doc **keys** back by `-(P+NL)` RoPE positions, so they\n",
    "appear at positions `[1, ..., D]` — identical to bare. Values are not affected\n",
    "by RoPE, so the semantic enrichment from Phase A is preserved in values while\n",
    "positional geometry matches bare exactly.\n",
    "\n",
    "**Phase B (inference):** Score `[\\n + query + \\n + answer]` using the repositioned\n",
    "doc-only cache, with position_ids starting at `D + 1` (same as bare condition).\n",
    "\n",
    "**Why this works:** The semantic enrichment from prefix attention is baked into\n",
    "both keys (content projection) and values during Phase A. Repositioning only changes\n",
    "the RoPE component of keys, not their content. Phase B then sees doc cache at\n",
    "bare-identical positions, so any NLL difference comes purely from how the prefix\n",
    "**content** altered doc representations — not from position offset.\n",
    "\n",
    "## Conditions (8 total)\n",
    "\n",
    "| # | Condition | Prefix | Description |\n",
    "|---|-----------|--------|-------------|\n",
    "| 1 | bare | (none) | Standard causal — lower bound |\n",
    "| 2 | oracle | real query | Real query conditions doc — upper bound |\n",
    "| 3 | surr_universal | generic analysis | \"Analyze for entities, facts, relationships\" |\n",
    "| 4 | surr_extractor | data extraction | \"Examine for data points, dates, attributes\" |\n",
    "| 5 | surr_reasonant | reasoning | \"Evaluate arguments, sentiment, intent\" |\n",
    "| 6 | surr_analytic | technical | \"Technical breakdown of systems/processes\" |\n",
    "| 7 | surr_doc_kw | doc keywords | Top-5 document keywords (v3's best) |\n",
    "| 8 | adversarial | off-topic | Off-topic text — negative control |\n",
    "\n",
    "## Key metrics\n",
    "- **Recovery rate**: (surrogate − bare) / (oracle − bare) × 100%\n",
    "- Cohen's d, win%, paired t-test\n",
    "- Hardness gradient analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16abaaf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Setup and model loading\n",
    "import os\n",
    "os.umask(0o000)\n",
    "\n",
    "import sys, json, time, gc, re\n",
    "import random as pyrandom\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from scipy import stats\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "sys.path.insert(0, \"../../..\")\n",
    "from lib.analysis import cohens_d\n",
    "\n",
    "SEED = 42\n",
    "N_SAMPLES = 400\n",
    "MODEL_NAME = \"google/gemma-3-12b-it\"\n",
    "\n",
    "RESULTS_DIR = Path(\"../../../results/decoder_only/exp01\")\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CHECKPOINT_PATH = RESULTS_DIR / \"checkpoint.json\"\n",
    "\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "pyrandom.seed(SEED)\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv())\n",
    "HF_TOKEN = os.environ.get(\"HF_TOKEN\")\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, DynamicCache\n",
    "\n",
    "print(f\"Loading {MODEL_NAME}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, token=HF_TOKEN)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME, device_map=\"auto\", torch_dtype=torch.bfloat16, token=HF_TOKEN,\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "DEVICE = next(model.parameters()).device\n",
    "BOS_ID = tokenizer.bos_token_id\n",
    "NEWLINE_IDS = tokenizer(\"\\n\", add_special_tokens=False).input_ids\n",
    "\n",
    "print(f\"Exp 01: Decoder-Only Surrogate Prefix Conditioning\")\n",
    "print(f\"Scoring: Two-phase KV cache + natural encoding + RoPE repositioning\")\n",
    "print(f\"N: {N_SAMPLES}, Model: {MODEL_NAME}\")\n",
    "print(f\"DEVICE: {DEVICE}, dtype: {next(model.parameters()).dtype}\")\n",
    "print(f\"GPU memory: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "text_cfg = getattr(model.config, 'text_config', model.config)\n",
    "print(f\"Vocab size: {getattr(text_cfg, 'vocab_size', 'N/A')}\")\n",
    "print(f\"Num layers: {getattr(text_cfg, 'num_hidden_layers', 'N/A')}\")\n",
    "print(f\"Num KV heads: {getattr(text_cfg, 'num_key_value_heads', 'N/A')}\")\n",
    "rope_params = getattr(text_cfg, 'rope_parameters', {})\n",
    "layer_types_list = getattr(text_cfg, 'layer_types', [])\n",
    "print(f\"Layer types: {set(layer_types_list)} ({len(layer_types_list)} layers)\")\n",
    "for ltype, params in rope_params.items():\n",
    "    print(f\"  {ltype}: theta={params.get('rope_theta')}, \"\n",
    "          f\"type={params.get('rope_type')}, factor={params.get('factor', 'N/A')}\")\n",
    "n_global = sum(1 for t in layer_types_list if t == 'full_attention')\n",
    "print(f\"  Global layers: {n_global}/{len(layer_types_list)} \"\n",
    "      f\"(indices: {[i for i, t in enumerate(layer_types_list) if t == 'full_attention']})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067f878a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Two-phase scoring with natural encoding + RoPE repositioning\n",
    "\n",
    "def slice_kv_cache(cache, start_idx):\n",
    "    sliced = DynamicCache()\n",
    "    for i in range(len(cache.layers)):\n",
    "        k = cache.layers[i].keys[:, :, start_idx:, :]\n",
    "        v = cache.layers[i].values[:, :, start_idx:, :]\n",
    "        sliced.update(k, v, i)\n",
    "    return sliced\n",
    "\n",
    "\n",
    "def rotate_half(x):\n",
    "    \"\"\"Rotates half the hidden dims (HuggingFace RoPE convention).\"\"\"\n",
    "    x1 = x[..., : x.shape[-1] // 2]\n",
    "    x2 = x[..., x.shape[-1] // 2 :]\n",
    "    return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "\n",
    "def build_layer_inv_freqs(config, head_dim, device):\n",
    "    \"\"\"Build per-layer inv_freq tensors matching Gemma 3's hybrid RoPE.\n",
    "\n",
    "    Gemma 3 uses different RoPE parameters for local (sliding) vs global\n",
    "    (full) attention layers:\n",
    "      - sliding_attention: theta=10,000, standard RoPE\n",
    "      - full_attention: theta=1,000,000, linear scaling (÷8.0)\n",
    "\n",
    "    Returns a list of inv_freq tensors, one per layer.\n",
    "    \"\"\"\n",
    "    text_cfg = getattr(config, 'text_config', config)\n",
    "    layer_types = text_cfg.layer_types\n",
    "    rope_params = text_cfg.rope_parameters\n",
    "\n",
    "    # Pre-compute inv_freq for each layer type\n",
    "    type_inv_freqs = {}\n",
    "    for ltype, params in rope_params.items():\n",
    "        theta = float(params['rope_theta'])\n",
    "        inv_freq = 1.0 / (theta ** (\n",
    "            torch.arange(0, head_dim, 2, dtype=torch.float32, device=device)\n",
    "            / head_dim\n",
    "        ))\n",
    "        if params.get('rope_type') == 'linear':\n",
    "            inv_freq = inv_freq / float(params['factor'])\n",
    "        type_inv_freqs[ltype] = inv_freq\n",
    "\n",
    "    # Map each layer to its inv_freq\n",
    "    return [type_inv_freqs[layer_types[i]] for i in range(len(layer_types))]\n",
    "\n",
    "\n",
    "def reposition_kv_cache(cache, delta, layer_inv_freqs):\n",
    "    \"\"\"Rotate all keys in cache by `delta` RoPE positions.\n",
    "\n",
    "    Uses per-layer RoPE parameters to handle Gemma 3's hybrid attention:\n",
    "    local (sliding) and global (full) layers use different theta values.\n",
    "\n",
    "    Computation done in float32, cast back to original dtype.\n",
    "    \"\"\"\n",
    "    # Pre-compute rotation embeddings per unique inv_freq\n",
    "    rotation_cache = {}\n",
    "    repositioned = DynamicCache()\n",
    "\n",
    "    for i in range(len(cache.layers)):\n",
    "        k = cache.layers[i].keys\n",
    "        v = cache.layers[i].values\n",
    "\n",
    "        # Use id() to cache rotation matrices for layers sharing the same inv_freq\n",
    "        freq_id = id(layer_inv_freqs[i])\n",
    "        if freq_id not in rotation_cache:\n",
    "            inv_freq = layer_inv_freqs[i]\n",
    "            angles = delta * inv_freq\n",
    "            emb = torch.cat([angles, angles])\n",
    "            rotation_cache[freq_id] = (\n",
    "                emb.cos().view(1, 1, 1, -1),\n",
    "                emb.sin().view(1, 1, 1, -1),\n",
    "            )\n",
    "\n",
    "        cos_d, sin_d = rotation_cache[freq_id]\n",
    "        k_f = k.float()\n",
    "        k_new = (k_f * cos_d + rotate_half(k_f) * sin_d).to(k.dtype)\n",
    "        repositioned.update(k_new, v, i)\n",
    "\n",
    "    return repositioned\n",
    "\n",
    "\n",
    "# Pre-compute layer inv_freqs (used by reposition_kv_cache and validation)\n",
    "head_dim_check = model.config.text_config.head_dim\n",
    "LAYER_INV_FREQS = build_layer_inv_freqs(model.config, head_dim_check, DEVICE)\n",
    "print(f\"Built per-layer inv_freqs for {len(LAYER_INV_FREQS)} layers\")\n",
    "# Verify the two types have different frequencies\n",
    "local_freq = LAYER_INV_FREQS[0]  # layer 0 = sliding\n",
    "global_idx = [i for i, t in enumerate(text_cfg.layer_types) if t == 'full_attention'][0]\n",
    "global_freq = LAYER_INV_FREQS[global_idx]\n",
    "print(f\"  Local  inv_freq[0]: {local_freq[0].item():.6e}\")\n",
    "print(f\"  Global inv_freq[0]: {global_freq[0].item():.6e}\")\n",
    "print(f\"  Ratio (local/global): {local_freq[0].item() / global_freq[0].item():.1f}x\")\n",
    "\n",
    "\n",
    "def score(doc_text, query_text, answer_text, prefix_text=None):\n",
    "    # Two-phase KV cache scoring with natural encoding + RoPE repositioning.\n",
    "    #\n",
    "    # Phase A: encode with NATURAL positions so prefix-doc attention uses\n",
    "    # in-distribution RoPE relative distances (semantic signal preserved).\n",
    "    #\n",
    "    # Conditioned: [BOS + prefix + \\n + doc] → natural positions [0..P+NL+D]\n",
    "    #   - Doc tokens attend to prefix at normal positive relative distances\n",
    "    #   - Slice removes BOS + prefix + \\n\n",
    "    #   - Reposition: rotate doc keys back by -(P+NL) to positions [1..D]\n",
    "    #\n",
    "    # Bare: [BOS + doc] → natural positions [0..D]\n",
    "    #   - Slice removes BOS only\n",
    "    #   - No repositioning needed (doc already at [1..D])\n",
    "    #\n",
    "    # Phase B: score [\\n + query + \\n + answer] at positions [D+1, ...]\n",
    "    # Position geometry is IDENTICAL for bare and conditioned.\n",
    "    # Any NLL difference comes from semantic enrichment of doc representations.\n",
    "\n",
    "    doc_ids = tokenizer(doc_text, add_special_tokens=False,\n",
    "                        truncation=True, max_length=1536).input_ids\n",
    "    D = len(doc_ids)\n",
    "\n",
    "    if prefix_text:\n",
    "        prefix_ids = tokenizer(prefix_text, add_special_tokens=False,\n",
    "                               truncation=True, max_length=512).input_ids\n",
    "        P = len(prefix_ids)\n",
    "        NL = len(NEWLINE_IDS)\n",
    "\n",
    "        # Build Phase A token sequence: [BOS, prefix, \\n, doc]\n",
    "        cond_ids = [BOS_ID] + prefix_ids + NEWLINE_IDS + doc_ids\n",
    "        slice_start = 1 + P + NL  # remove BOS + prefix + \\n\n",
    "        reposition_delta = -(P + NL)  # rotate doc keys back to [1..D]\n",
    "    else:\n",
    "        # Bare: [BOS + doc], natural positions, slice BOS\n",
    "        cond_ids = [BOS_ID] + doc_ids\n",
    "        slice_start = 1  # remove BOS\n",
    "        reposition_delta = 0\n",
    "\n",
    "    # Phase B always starts at D+1 (after repositioning, doc is at [1..D])\n",
    "    phase_b_start = D + 1\n",
    "\n",
    "    # --- Phase A: build KV cache with natural positions ---\n",
    "    with torch.no_grad():\n",
    "        pa = model(\n",
    "            input_ids=torch.tensor([cond_ids], device=DEVICE),\n",
    "            use_cache=True,\n",
    "        )\n",
    "    cache = pa.past_key_values\n",
    "    del pa\n",
    "\n",
    "    # Slice prefix entries\n",
    "    cache = slice_kv_cache(cache, slice_start)\n",
    "\n",
    "    # Reposition doc keys to [1..D] (no-op for bare)\n",
    "    if reposition_delta != 0:\n",
    "        cache = reposition_kv_cache(cache, reposition_delta, LAYER_INV_FREQS)\n",
    "\n",
    "    # --- Phase B: score query + answer ---\n",
    "    query_ids = tokenizer(\"\\n\" + query_text + \"\\n\", add_special_tokens=False).input_ids\n",
    "    answer_ids = tokenizer(answer_text, add_special_tokens=False,\n",
    "                           truncation=True, max_length=256).input_ids\n",
    "    if not answer_ids:\n",
    "        del cache\n",
    "        return 0.0\n",
    "\n",
    "    pb_ids = query_ids + answer_ids\n",
    "    pos = torch.arange(phase_b_start, phase_b_start + len(pb_ids), device=DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pb = model(\n",
    "            input_ids=torch.tensor([pb_ids], device=DEVICE),\n",
    "            past_key_values=cache,\n",
    "            position_ids=pos.unsqueeze(0),\n",
    "            cache_position=pos,\n",
    "            use_cache=False,\n",
    "        )\n",
    "\n",
    "    # Score answer tokens only\n",
    "    n_q = len(query_ids)\n",
    "    logits = pb.logits[0, n_q - 1:n_q - 1 + len(answer_ids), :].float()\n",
    "    targets = torch.tensor(answer_ids, device=DEVICE)\n",
    "    nll = -F.log_softmax(logits, dim=-1).gather(1, targets.unsqueeze(1)).squeeze(1).mean().item()\n",
    "    del cache, pb\n",
    "    return nll\n",
    "\n",
    "\n",
    "# === Surrogate definitions ===\n",
    "SURROGATES = {\n",
    "    'universal': \"Analyze the following text for all key entities, factual claims, and logical relationships.\",\n",
    "    'extractor': \"Examine this document specifically for data points, dates, numerical values, and specific named attributes.\",\n",
    "    'reasonant': \"Evaluate the underlying arguments, sentiment, and intent of the following passage.\",\n",
    "    'analytic': \"Provide a technical breakdown of the systems and processes described in this text.\",\n",
    "}\n",
    "\n",
    "ADVERSARIAL_PREFIX = \"The recipe calls for two cups of flour, one cup of sugar, and a pinch of salt.\"\n",
    "\n",
    "STOP_WORDS = {\n",
    "    'a', 'an', 'the', 'is', 'are', 'was', 'were', 'be', 'been', 'being',\n",
    "    'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could',\n",
    "    'should', 'may', 'might', 'can', 'shall', 'to', 'of', 'in', 'for',\n",
    "    'on', 'with', 'at', 'by', 'from', 'as', 'into', 'through', 'during',\n",
    "    'before', 'after', 'above', 'below', 'between', 'and', 'but', 'or',\n",
    "    'not', 'no', 'if', 'then', 'than', 'so', 'up', 'out', 'about',\n",
    "    'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those',\n",
    "    'it', 'its', 'i', 'me', 'my', 'we', 'our', 'you', 'your', 'he',\n",
    "    'him', 'his', 'she', 'her', 'they', 'them', 'their', 'how', 'when',\n",
    "    'where', 'why', 'much', 'many', 'some', 'any', 'all', 'each',\n",
    "    'does', 'also', 'just', 'more', 'most', 'very', 'too', 'only',\n",
    "}\n",
    "\n",
    "def extract_keywords(text):\n",
    "    words = re.sub(r'[^\\w\\s]', '', text.lower()).split()\n",
    "    return [w for w in words if w not in STOP_WORDS and len(w) > 2]\n",
    "\n",
    "def make_doc_keywords(passage):\n",
    "    content_words = extract_keywords(passage)\n",
    "    if not content_words:\n",
    "        return \"information\"\n",
    "    counts = Counter(content_words)\n",
    "    return \" \".join(w for w, _ in counts.most_common(5))\n",
    "\n",
    "\n",
    "print(\"\\nScoring function defined (natural encoding + per-layer RoPE repositioning).\")\n",
    "print(f\"\\nSurrogate prompts:\")\n",
    "for name, prompt in SURROGATES.items():\n",
    "    n_tok = len(tokenizer(prompt, add_special_tokens=False).input_ids)\n",
    "    print(f\"  {name:<12} ({n_tok:>2} tok): {prompt[:60]}...\")\n",
    "adv_tok = len(tokenizer(ADVERSARIAL_PREFIX, add_special_tokens=False).input_ids)\n",
    "print(f\"  {'adversarial':<12} ({adv_tok:>2} tok): {ADVERSARIAL_PREFIX[:60]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40c16c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Load MS MARCO data and generate surrogates\n",
    "from lib.data import count_words\n",
    "from datasets import load_dataset\n",
    "\n",
    "print(\"Loading MS MARCO v1.1 validation...\")\n",
    "ds = load_dataset(\"microsoft/ms_marco\", \"v1.1\", split=\"validation\")\n",
    "\n",
    "all_candidates = []\n",
    "for item in ds:\n",
    "    if len(all_candidates) >= 3 * N_SAMPLES:\n",
    "        break\n",
    "    passages = item.get('passages', {})\n",
    "    ptexts = passages.get('passage_text', [])\n",
    "    is_sel = passages.get('is_selected', [])\n",
    "    query = item.get('query', '')\n",
    "    answers = item.get('answers', [])\n",
    "    well_formed = item.get('wellFormedAnswers', [])\n",
    "    answer = None\n",
    "    if well_formed and len(well_formed) > 0 and well_formed[0] not in ('[]', ''):\n",
    "        answer = well_formed[0]\n",
    "    elif answers and len(answers) > 0 and answers[0] != 'No Answer Present.':\n",
    "        answer = answers[0]\n",
    "    if not answer:\n",
    "        continue\n",
    "    for pt, sel in zip(ptexts, is_sel):\n",
    "        wc = count_words(pt)\n",
    "        if sel == 1 and 30 <= wc <= 300:\n",
    "            all_candidates.append({\n",
    "                'passage': pt, 'query': query, 'answer': answer,\n",
    "                'word_count': wc,\n",
    "            })\n",
    "            break\n",
    "\n",
    "print(f\"Total candidates: {len(all_candidates)}\")\n",
    "np.random.seed(SEED)\n",
    "indices = np.random.permutation(len(all_candidates))\n",
    "samples = [all_candidates[i] for i in indices[:N_SAMPLES]]\n",
    "del ds, all_candidates\n",
    "gc.collect()\n",
    "\n",
    "# Generate surrogates\n",
    "for s in samples:\n",
    "    s['surr_doc_kw'] = make_doc_keywords(s['passage'])\n",
    "\n",
    "print(f\"Loaded {len(samples)} samples\")\n",
    "print(f\"Mean passage words: {np.mean([s['word_count'] for s in samples]):.0f}\")\n",
    "print(f\"Mean answer words: {np.mean([count_words(s['answer']) for s in samples]):.0f}\")\n",
    "print(f\"Mean query words: {np.mean([count_words(s['query']) for s in samples]):.0f}\")\n",
    "print(f\"\\nFirst sample:\")\n",
    "print(f\"  Query:  {samples[0]['query'][:70]}...\")\n",
    "print(f\"  Answer: {samples[0]['answer'][:70]}...\")\n",
    "print(f\"  Passage ({samples[0]['word_count']}w): {samples[0]['passage'][:70]}...\")\n",
    "print(f\"  Doc keywords: {samples[0]['surr_doc_kw']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae22973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: DEEP VALIDATION — test repositioning against native Gemma 3 RoPE\n",
    "print(\"=\" * 70)\n",
    "print(\"DEEP VALIDATION: RoPE repositioning vs native Gemma 3 implementation\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "s = samples[0]\n",
    "doc_ids = tokenizer(s['passage'], add_special_tokens=False,\n",
    "                    truncation=True, max_length=1536).input_ids\n",
    "D_test = len(doc_ids)\n",
    "\n",
    "# ================================================================\n",
    "# TEST 1: inv_freq comparison against model's internal buffers\n",
    "# ================================================================\n",
    "print(\"\\n--- Test 1: inv_freq matches model's rotary_emb buffers ---\")\n",
    "rotary = model.model.rotary_emb\n",
    "for ltype in ['sliding_attention', 'full_attention']:\n",
    "    model_inv = getattr(rotary, f'{ltype}_inv_freq').float().to(DEVICE)\n",
    "    layer_idx_for_type = [i for i, t in enumerate(text_cfg.layer_types)\n",
    "                          if t == ltype][0]\n",
    "    our_inv = LAYER_INV_FREQS[layer_idx_for_type]\n",
    "    diff = (model_inv - our_inv).abs().max().item()\n",
    "    print(f\"  {ltype}: max diff = {diff:.2e} \"\n",
    "          f\"(model shape={model_inv.shape}, ours={our_inv.shape})\")\n",
    "    assert diff < 1e-6, f\"inv_freq mismatch for {ltype}: diff={diff}\"\n",
    "print(\"  PASSED\")\n",
    "\n",
    "# ================================================================\n",
    "# TEST 2: attention_scaling is 1.0 for both layer types\n",
    "# ================================================================\n",
    "print(\"\\n--- Test 2: attention_scaling values ---\")\n",
    "for ltype in ['sliding_attention', 'full_attention']:\n",
    "    scaling = getattr(rotary, f'{ltype}_attention_scaling')\n",
    "    print(f\"  {ltype}: attention_scaling = {scaling}\")\n",
    "    assert scaling == 1.0, f\"attention_scaling != 1.0 for {ltype}: {scaling}\"\n",
    "print(\"  PASSED\")\n",
    "\n",
    "# ================================================================\n",
    "# TEST 3: Layer-0 repositioning matches native model computation\n",
    "# ================================================================\n",
    "# At layer 0, keys = RoPE(k_norm(k_proj(embed(token))), position).\n",
    "# The pre-RoPE content depends ONLY on token identity, not position.\n",
    "# So: reposition(key_at_pos_p, delta) should EXACTLY match key_at_pos_{p+delta}\n",
    "# (up to bf16 quantization from storing the original key).\n",
    "print(\"\\n--- Test 3: Layer-0 key repositioning vs native computation ---\")\n",
    "test_tokens = tokenizer(\"The quick brown fox jumps over the lazy dog\",\n",
    "                        add_special_tokens=False).input_ids\n",
    "delta_test = 23\n",
    "\n",
    "# Encode at natural positions [0, 1, ..., N]\n",
    "ids_test = [BOS_ID] + test_tokens\n",
    "with torch.no_grad():\n",
    "    out_nat = model(input_ids=torch.tensor([ids_test], device=DEVICE),\n",
    "                    use_cache=True)\n",
    "cache_nat = out_nat.past_key_values\n",
    "del out_nat\n",
    "\n",
    "# Encode at shifted positions [delta, delta+1, ..., delta+N]\n",
    "pos_shifted = torch.arange(delta_test, delta_test + len(ids_test), device=DEVICE)\n",
    "with torch.no_grad():\n",
    "    out_shifted = model(\n",
    "        input_ids=torch.tensor([ids_test], device=DEVICE),\n",
    "        position_ids=pos_shifted.unsqueeze(0),\n",
    "        cache_position=pos_shifted,\n",
    "        use_cache=True,\n",
    "    )\n",
    "cache_shifted = out_shifted.past_key_values\n",
    "del out_shifted\n",
    "\n",
    "# Test layer 0 (sliding) and first global layer\n",
    "for test_layer in [0, global_idx]:\n",
    "    ltype = text_cfg.layer_types[test_layer]\n",
    "    k_nat = cache_nat.layers[test_layer].keys  # at positions 0..N\n",
    "    k_shifted = cache_shifted.layers[test_layer].keys  # at positions delta..delta+N\n",
    "\n",
    "    # Reposition k_nat by delta_test using our function\n",
    "    inv_freq = LAYER_INV_FREQS[test_layer]\n",
    "    angles = delta_test * inv_freq\n",
    "    emb = torch.cat([angles, angles])\n",
    "    cos_d = emb.cos().view(1, 1, 1, -1)\n",
    "    sin_d = emb.sin().view(1, 1, 1, -1)\n",
    "    k_nat_f = k_nat.float()\n",
    "    k_repositioned = (k_nat_f * cos_d + rotate_half(k_nat_f) * sin_d)\n",
    "\n",
    "    # Compare repositioned vs natively shifted\n",
    "    k_shifted_f = k_shifted.float()\n",
    "    abs_diff = (k_repositioned - k_shifted_f).abs().max().item()\n",
    "    rel_diff = abs_diff / k_shifted_f.abs().max().item()\n",
    "\n",
    "    # Also compare values (no RoPE on values, so should be identical)\n",
    "    v_nat = cache_nat.layers[test_layer].values\n",
    "    v_shifted = cache_shifted.layers[test_layer].values\n",
    "    val_diff = (v_nat.float() - v_shifted.float()).abs().max().item()\n",
    "\n",
    "    print(f\"  Layer {test_layer:>2} ({ltype[:7]}): \"\n",
    "          f\"key rel diff = {rel_diff:.2e}, \"\n",
    "          f\"val abs diff = {val_diff:.2e}\")\n",
    "    assert rel_diff < 1e-4, (\n",
    "        f\"Layer-0 repositioning mismatch for {ltype}: rel_diff={rel_diff}\")\n",
    "\n",
    "# Show higher layers diverge (expected — attention-dependent hidden states)\n",
    "print(\"  Higher layers (expected to diverge — different attention contexts):\")\n",
    "for test_layer in [1, 2, 3]:\n",
    "    k_nat = cache_nat.layers[test_layer].keys\n",
    "    k_shifted = cache_shifted.layers[test_layer].keys\n",
    "    inv_freq = LAYER_INV_FREQS[test_layer]\n",
    "    angles = delta_test * inv_freq\n",
    "    emb = torch.cat([angles, angles])\n",
    "    cos_d = emb.cos().view(1, 1, 1, -1)\n",
    "    sin_d = emb.sin().view(1, 1, 1, -1)\n",
    "    k_repositioned = (k_nat.float() * cos_d + rotate_half(k_nat.float()) * sin_d)\n",
    "    rel_diff = ((k_repositioned - k_shifted.float()).abs().max().item()\n",
    "                / k_shifted.float().abs().max().item())\n",
    "    print(f\"    Layer {test_layer}: key rel diff = {rel_diff:.2e}\")\n",
    "\n",
    "del cache_nat, cache_shifted\n",
    "print(\"  PASSED — layer-0 repositioning matches native computation exactly\")\n",
    "\n",
    "# ================================================================\n",
    "# TEST 4: Full pipeline test — conditioned + slice + reposition vs bare\n",
    "# ================================================================\n",
    "# Encode [BOS + prefix + \\n + doc], slice prefix, reposition doc keys.\n",
    "# At layer 0, the repositioned doc keys should MATCH bare [BOS + doc] keys\n",
    "# because pre-RoPE keys depend only on token identity.\n",
    "print(\"\\n--- Test 4: Full pipeline (cond+slice+reposition) vs bare at layer 0 ---\")\n",
    "prefix_ids = tokenizer(s['query'], add_special_tokens=False,\n",
    "                       truncation=True, max_length=512).input_ids\n",
    "P = len(prefix_ids)\n",
    "NL = len(NEWLINE_IDS)\n",
    "D = len(doc_ids)\n",
    "print(f\"  Prefix tokens: {P}, Newline tokens: {NL}, Doc tokens: {D}\")\n",
    "print(f\"  Reposition delta: {-(P + NL)}\")\n",
    "\n",
    "# Bare: [BOS + doc]\n",
    "with torch.no_grad():\n",
    "    out_bare = model(\n",
    "        input_ids=torch.tensor([[BOS_ID] + doc_ids], device=DEVICE),\n",
    "        use_cache=True)\n",
    "cache_bare = out_bare.past_key_values\n",
    "del out_bare\n",
    "\n",
    "# Conditioned: [BOS + prefix + \\n + doc]\n",
    "cond_ids = [BOS_ID] + prefix_ids + NEWLINE_IDS + doc_ids\n",
    "with torch.no_grad():\n",
    "    out_cond = model(\n",
    "        input_ids=torch.tensor([cond_ids], device=DEVICE),\n",
    "        use_cache=True)\n",
    "cache_cond = out_cond.past_key_values\n",
    "del out_cond\n",
    "\n",
    "# Slice prefix from conditioned cache\n",
    "slice_start = 1 + P + NL\n",
    "cache_cond_sliced = slice_kv_cache(cache_cond, slice_start)\n",
    "\n",
    "# Reposition doc keys\n",
    "cache_cond_repo = reposition_kv_cache(cache_cond_sliced, -(P + NL),\n",
    "                                      LAYER_INV_FREQS)\n",
    "\n",
    "# Compare at layer 0 (keys and values)\n",
    "# Bare keys: skip BOS (index 0), take doc entries (indices 1..D)\n",
    "bare_k0 = cache_bare.layers[0].keys[:, :, 1:, :].float()\n",
    "repo_k0 = cache_cond_repo.layers[0].keys.float()\n",
    "bare_v0 = cache_bare.layers[0].values[:, :, 1:, :].float()\n",
    "repo_v0 = cache_cond_repo.layers[0].values.float()\n",
    "\n",
    "key_rel_diff = (bare_k0 - repo_k0).abs().max().item() / bare_k0.abs().max().item()\n",
    "val_abs_diff = (bare_v0 - repo_v0).abs().max().item()\n",
    "print(f\"  Layer 0 keys:   rel diff = {key_rel_diff:.2e}\")\n",
    "print(f\"  Layer 0 values: abs diff = {val_abs_diff:.2e}\")\n",
    "assert key_rel_diff < 0.02, f\"Layer 0 key mismatch after full pipeline: {key_rel_diff}\"\n",
    "assert val_abs_diff < 1e-6, f\"Layer 0 value mismatch: {val_abs_diff}\"\n",
    "\n",
    "# Do the same for the first global layer\n",
    "bare_kg = cache_bare.layers[global_idx].keys[:, :, 1:, :].float()\n",
    "repo_kg = cache_cond_repo.layers[global_idx].keys.float()\n",
    "bare_vg = cache_bare.layers[global_idx].values[:, :, 1:, :].float()\n",
    "repo_vg = cache_cond_repo.layers[global_idx].values.float()\n",
    "key_rel_diff_g = (bare_kg - repo_kg).abs().max().item() / bare_kg.abs().max().item()\n",
    "val_abs_diff_g = (bare_vg - repo_vg).abs().max().item()\n",
    "print(f\"  Layer {global_idx} keys:   rel diff = {key_rel_diff_g:.2e}\")\n",
    "print(f\"  Layer {global_idx} values: abs diff = {val_abs_diff_g:.2e}\")\n",
    "\n",
    "# Show per-layer comparison (layer 0 should match, higher layers diverge)\n",
    "print(f\"\\n  Per-layer key comparison (first 15 layers):\")\n",
    "print(f\"  {'Layer':>5} {'Type':>4} {'Key RelDiff':>12} {'Val AbsDiff':>12} {'Match':>6}\")\n",
    "for L in range(min(15, len(cache_bare.layers))):\n",
    "    bare_k = cache_bare.layers[L].keys[:, :, 1:, :].float()\n",
    "    repo_k = cache_cond_repo.layers[L].keys.float()\n",
    "    bare_v = cache_bare.layers[L].values[:, :, 1:, :].float()\n",
    "    repo_v = cache_cond_repo.layers[L].values.float()\n",
    "    krd = (bare_k - repo_k).abs().max().item() / bare_k.abs().max().item()\n",
    "    vad = (bare_v - repo_v).abs().max().item() / bare_v.abs().max().item()\n",
    "    ltype = 'G' if text_cfg.layer_types[L] == 'full_attention' else 'L'\n",
    "    match = 'Y' if krd < 0.02 and vad < 0.02 else 'N'\n",
    "    print(f\"  {L:>5} {ltype:>4} {krd:>12.4e} {vad:>12.4e} {match:>6}\")\n",
    "\n",
    "del cache_bare, cache_cond, cache_cond_sliced, cache_cond_repo\n",
    "\n",
    "print(\"  PASSED — layer-0 keys and values match between bare and conditioned\")\n",
    "\n",
    "# ================================================================\n",
    "# TEST 5: Phase B NLL comparison (the key end-to-end test)\n",
    "# ================================================================\n",
    "# If repositioning is correct, the ONLY difference between bare NLL\n",
    "# and conditioned NLL should come from the semantic content change\n",
    "# in doc representations (values and key content at higher layers).\n",
    "#\n",
    "# To isolate repositioning artifacts, test with a \"neutral\" prefix that\n",
    "# the model should essentially ignore (just padding tokens).\n",
    "# If repositioning introduces artifacts, even a neutral prefix will\n",
    "# change NLL significantly.\n",
    "print(\"\\n--- Test 5: End-to-end NLL comparison ---\")\n",
    "nll_bare = score(s['passage'], s['query'], s['answer'])\n",
    "nll_oracle = score(s['passage'], s['query'], s['answer'], prefix_text=s['query'])\n",
    "nll_adv = score(s['passage'], s['query'], s['answer'],\n",
    "                prefix_text=ADVERSARIAL_PREFIX)\n",
    "print(f\"  Bare:        {nll_bare:.6f}\")\n",
    "print(f\"  Oracle:      {nll_oracle:.6f} (delta: {nll_bare - nll_oracle:+.6f})\")\n",
    "print(f\"  Adversarial: {nll_adv:.6f} (delta: {nll_bare - nll_adv:+.6f})\")\n",
    "\n",
    "# ================================================================\n",
    "# TEST 6: Verify the model's cos/sin at specific positions\n",
    "# ================================================================\n",
    "# Generate cos/sin through the model's rotary embedding, and compare\n",
    "# what we'd compute for the rotation delta.\n",
    "print(\"\\n--- Test 6: cos/sin verification against model's rotary_emb ---\")\n",
    "dummy_hidden = torch.zeros(1, 1, head_dim_check, device=DEVICE,\n",
    "                           dtype=torch.bfloat16)\n",
    "pos_ids = torch.tensor([[5, 22]], device=DEVICE)  # positions 5 and 22\n",
    "for ltype in ['sliding_attention', 'full_attention']:\n",
    "    cos_model, sin_model = rotary(dummy_hidden, pos_ids, layer_type=ltype)\n",
    "    # cos_model shape: [1, 2, head_dim]\n",
    "    # Extract cos/sin at position 5 and position 22\n",
    "    cos_5 = cos_model[0, 0, :].float()  # [head_dim]\n",
    "    cos_22 = cos_model[0, 1, :].float()\n",
    "    sin_5 = sin_model[0, 0, :].float()\n",
    "    sin_22 = sin_model[0, 1, :].float()\n",
    "\n",
    "    # Our rotation delta = 17 (from pos 5 to pos 22)\n",
    "    inv_freq_type = LAYER_INV_FREQS[\n",
    "        [i for i, t in enumerate(text_cfg.layer_types) if t == ltype][0]]\n",
    "    angles_17 = 17.0 * inv_freq_type\n",
    "    emb_17 = torch.cat([angles_17, angles_17])\n",
    "    our_cos_17 = emb_17.cos()\n",
    "    our_sin_17 = emb_17.sin()\n",
    "\n",
    "    # Verify: cos(22*theta) = cos(5*theta)*cos(17*theta) - sin(5*theta)*sin(17*theta)\n",
    "    cos_22_from_composition = cos_5 * our_cos_17 - sin_5 * our_sin_17\n",
    "    sin_22_from_composition = sin_5 * our_cos_17 + cos_5 * our_sin_17\n",
    "\n",
    "    cos_err = (cos_22 - cos_22_from_composition).abs().max().item()\n",
    "    sin_err = (sin_22 - sin_22_from_composition).abs().max().item()\n",
    "    print(f\"  {ltype}: cos composition err = {cos_err:.2e}, \"\n",
    "          f\"sin composition err = {sin_err:.2e}\")\n",
    "    assert cos_err < 1e-5, f\"cos composition failed for {ltype}: {cos_err}\"\n",
    "    assert sin_err < 1e-5, f\"sin composition failed for {ltype}: {sin_err}\"\n",
    "print(\"  PASSED — rotation composition identity verified against model\")\n",
    "\n",
    "# ================================================================\n",
    "# TEST 7: Bare NLL consistency\n",
    "# ================================================================\n",
    "print(\"\\n--- Test 7: Bare NLL consistency ---\")\n",
    "nll_1 = score(s['passage'], s['query'], s['answer'])\n",
    "nll_2 = score(s['passage'], s['query'], s['answer'])\n",
    "print(f\"  Call 1: {nll_1:.6f}, Call 2: {nll_2:.6f}, diff: {abs(nll_1 - nll_2):.2e}\")\n",
    "assert abs(nll_1 - nll_2) < 1e-5, \"Bare NLL not consistent\"\n",
    "print(\"  PASSED\")\n",
    "\n",
    "# ================================================================\n",
    "# TEST 8: Multi-sample oracle check\n",
    "# ================================================================\n",
    "print(\"\\n--- Test 8: 5-sample bare vs oracle ---\")\n",
    "oracle_wins = 0\n",
    "for i in range(5):\n",
    "    s_test = samples[i]\n",
    "    nll_b = score(s_test['passage'], s_test['query'], s_test['answer'])\n",
    "    nll_o = score(s_test['passage'], s_test['query'], s_test['answer'],\n",
    "                  prefix_text=s_test['query'])\n",
    "    delta = nll_b - nll_o\n",
    "    win = delta > 0\n",
    "    oracle_wins += win\n",
    "    print(f\"  Sample {i}: bare={nll_b:.6f}, oracle={nll_o:.6f}, \"\n",
    "          f\"delta={delta:+.6f} {'(oracle wins)' if win else '(bare wins)'}\")\n",
    "print(f\"  Oracle wins: {oracle_wins}/5\")\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ALL DEEP VALIDATION TESTS PASSED\")\n",
    "print(\"=\" * 70)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
