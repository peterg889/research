{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7a56da8",
   "metadata": {},
   "source": [
    "# Prefix LM Exp 01: Prefix LM Enrichment with Gemma 3 12B IT\n",
    "\n",
    "## Motivation\n",
    "\n",
    "Experiments 01-10 tested surrogate-enriched encoder caching in encoder-decoder models\n",
    "(T5Gemma, flan-T5, standard T5, BART). Core finding: prepending a short surrogate\n",
    "prefix with bidirectional attention improves answer NLL (d=+0.228 to +0.531).\n",
    "\n",
    "**Next question**: Does this transfer to a **decoder-only** model if we retrofit it\n",
    "with prefix LM attention (bidirectional on [surrogate+doc], causal on [query+answer])?\n",
    "\n",
    "## Model\n",
    "\n",
    "`google/gemma-3-12b-it` in bfloat16 (~24GB on 40GB GPU). Single model for Exp 01.\n",
    "\n",
    "## Two-Pass Design\n",
    "\n",
    "Mirrors the encoder-decoder experiments' production scenario:\n",
    "\n",
    "- **Phase A (offline)**: Process `[BOS, surrogate, doc]` with custom attention mask\n",
    "  and `use_cache=True` → cache KV states as `past_key_values`\n",
    "- **Phase B (online)**: Process `[query, answer]` using cached KVs via\n",
    "  `past_key_values` → compute NLL on answer tokens\n",
    "\n",
    "Position IDs are sequential: Phase A uses `0..n_prefix-1`, Phase B continues from\n",
    "`n_prefix..n_prefix+n_cont-1`. This ensures correct RoPE encoding of token distances.\n",
    "\n",
    "For `_nq` conditions (no query), Phase B input is just `[answer]`. The first answer\n",
    "token's logit comes from Phase A's last position; remaining answer logits from Phase B.\n",
    "\n",
    "## Token Layout\n",
    "\n",
    "```\n",
    "Phase A: [<bos>] [surrogate_tokens] [doc_tokens]\n",
    "Phase B: [query_tokens] [answer_tokens]\n",
    "```\n",
    "\n",
    "No chat template — raw concatenation for clean experimental control. Conditions\n",
    "differ only in attention pattern and surrogate content. NLL computed only on answer tokens.\n",
    "\n",
    "## Conditions (2 x 4 + 2 = 10)\n",
    "\n",
    "**Attention modes** (2):\n",
    "- `causal`: Standard lower-triangular throughout (model's native mode)\n",
    "- `prefix_lm`: Bidirectional on [surrogate+doc], causal on [query+answer]\n",
    "\n",
    "**Prefix conditions** (4):\n",
    "- `bare`: No surrogate (just [bos | doc] cached, then [query | answer])\n",
    "- `oracle_trunc`: Real query as surrogate, masked from Phase B continuation\n",
    "- `random_trunc`: Random tokens as surrogate, masked from Phase B continuation\n",
    "- `surr_doc_trunc`: Document keywords (10 keywords) as surrogate, masked from Phase B\n",
    "\n",
    "**Extra controls** (2):\n",
    "- `causal_bare_nq`: Causal, no surrogate, no query ([bos | doc] -> [answer]) — floor\n",
    "- `prefix_lm_oracle_trunc_nq`: Prefix LM with oracle, no query — isolates bidirectional benefit\n",
    "\n",
    "## Key Metrics\n",
    "\n",
    "- `d_bidirectional`: prefix_lm_bare - causal_bare → pure bidirectional encoding benefit\n",
    "- `d_oracle`: prefix_lm_oracle_trunc - prefix_lm_bare → surrogate enrichment under prefix LM\n",
    "- `d_surr_doc`: prefix_lm_surr_doc_trunc - prefix_lm_bare → keyword surrogate enrichment\n",
    "- `d_random`: prefix_lm_random_trunc - prefix_lm_bare → structural component\n",
    "- `structural_fraction`: d_random / d_oracle\n",
    "- `d_causal_oracle`: causal_oracle_trunc - causal_bare → enrichment under causal attention\n",
    "  (Note: under causal, doc tokens CAN attend to surrogate causally, so this may be > 0.\n",
    "  Unlike encoder-decoder where cross-attention mask directly blocks surrogate access.)\n",
    "- `d_dec_q`: compare _nq vs with-query conditions\n",
    "\n",
    "## 4D Attention Mask\n",
    "\n",
    "Gemma 3's `forward()` accepts `attention_mask` as a dict:\n",
    "```python\n",
    "attention_mask = {\n",
    "    \"full_attention\": mask_4d,       # for global attention layers\n",
    "    \"sliding_attention\": mask_4d,    # for sliding window layers\n",
    "}\n",
    "```\n",
    "Float dtype. `0.0` = attend, `torch.finfo(dtype).min` = mask.\n",
    "Phase A mask: `(1, 1, n_prefix, n_prefix)`.\n",
    "Phase B mask: `(1, 1, n_cont, n_prefix + n_cont)` — accounts for cached prefix + new tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befadb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Setup\n",
    "import os\n",
    "os.umask(0o000)\n",
    "\n",
    "import sys, json, time, gc, re\n",
    "import random as pyrandom\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from scipy import stats\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "sys.path.insert(0, \"../../..\")\n",
    "from lib.analysis import cohens_d\n",
    "\n",
    "SEED = 42\n",
    "N_SAMPLES = 500\n",
    "\n",
    "MODEL_NAME = \"google/gemma-3-12b-it\"\n",
    "\n",
    "RESULTS_DIR = Path(\"../../../results/prefix_lm_exp01\")\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "pyrandom.seed(SEED)\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv())\n",
    "HF_TOKEN = os.environ.get(\"HF_TOKEN\")\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 10 conditions: (attention_mode, prefix_type, has_query)\n",
    "CONDITIONS = [\n",
    "    (\"causal\",    \"bare\",          True),   # causal_bare\n",
    "    (\"causal\",    \"oracle_trunc\",  True),   # causal_oracle_trunc\n",
    "    (\"causal\",    \"random_trunc\",  True),   # causal_random_trunc\n",
    "    (\"causal\",    \"surr_doc_trunc\", True),  # causal_surr_doc_trunc\n",
    "    (\"prefix_lm\", \"bare\",          True),   # prefix_lm_bare\n",
    "    (\"prefix_lm\", \"oracle_trunc\",  True),   # prefix_lm_oracle_trunc\n",
    "    (\"prefix_lm\", \"random_trunc\",  True),   # prefix_lm_random_trunc\n",
    "    (\"prefix_lm\", \"surr_doc_trunc\", True),  # prefix_lm_surr_doc_trunc\n",
    "    (\"causal\",    \"bare\",          False),  # causal_bare_nq\n",
    "    (\"prefix_lm\", \"oracle_trunc\",  False),  # prefix_lm_oracle_trunc_nq\n",
    "]\n",
    "\n",
    "def condition_name(mode, prefix, has_query):\n",
    "    name = f\"{mode}_{prefix}\" if prefix != \"bare\" else f\"{mode}_bare\"\n",
    "    if not has_query:\n",
    "        name += \"_nq\"\n",
    "    return name\n",
    "\n",
    "COND_NAMES = [condition_name(m, p, q) for m, p, q in CONDITIONS]\n",
    "\n",
    "print(f\"Prefix LM Exp 01: Enrichment with Gemma 3 12B IT\")\n",
    "print(f\"N: {N_SAMPLES}, Conditions: {len(CONDITIONS)}\")\n",
    "print(f\"DEVICE: {DEVICE}\")\n",
    "print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "print(f\"\\nConditions:\")\n",
    "for cn in COND_NAMES:\n",
    "    print(f\"  {cn}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b6205e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Load model + tokenizer\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import transformers\n",
    "\n",
    "print(f\"transformers version: {transformers.__version__}\")\n",
    "\n",
    "print(f\"Loading {MODEL_NAME}...\")\n",
    "t0 = time.time()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, token=HF_TOKEN)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    attn_implementation=\"eager\",\n",
    "    token=HF_TOKEN,\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "n_params = sum(p.numel() for p in model.parameters()) / 1e9\n",
    "gpu_mem = torch.cuda.memory_allocated() / 1e9\n",
    "print(f\"Loaded: {n_params:.1f}B params, {gpu_mem:.1f} GB GPU, {time.time()-t0:.0f}s\")\n",
    "print(f\"BOS token id: {tokenizer.bos_token_id}\")\n",
    "print(f\"Model dtype: {model.dtype}\")\n",
    "print(f\"Attn implementation: {model.config._attn_implementation}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8713c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Phase A/B attention masks + sanity check\n",
    "#\n",
    "# Two-pass design:\n",
    "#   Phase A: Process [BOS, surrogate, doc] -> cache KV states\n",
    "#   Phase B: Process [query, answer] using cached KVs -> NLL\n",
    "#\n",
    "# Each phase gets its own attention mask. Phase A mask controls how prefix\n",
    "# tokens attend to each other. Phase B mask controls how continuation tokens\n",
    "# attend to cached prefix (with truncation) and to each other (causally).\n",
    "\n",
    "def make_phase_a_mask(n_s, n_d, mode=\"prefix_lm\", dtype=torch.bfloat16):\n",
    "    # Phase A mask for prefix [BOS, surrogate, doc].\n",
    "    # Returns (1, 1, n_prefix, n_prefix).\n",
    "    # prefix_lm: fully bidirectional within prefix.\n",
    "    # causal: standard lower-triangular.\n",
    "    n_prefix = 1 + n_s + n_d\n",
    "    min_val = torch.finfo(dtype).min\n",
    "    if mode == \"prefix_lm\":\n",
    "        mask = torch.zeros((n_prefix, n_prefix), dtype=dtype)\n",
    "    else:\n",
    "        mask = torch.triu(torch.full((n_prefix, n_prefix), min_val, dtype=dtype),\n",
    "                          diagonal=1)\n",
    "    return mask.unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "\n",
    "def make_phase_b_mask(n_s, n_d, n_q, n_a, truncate=True, dtype=torch.bfloat16):\n",
    "    # Phase B mask for continuation [query, answer] attending to cached prefix.\n",
    "    # Returns (1, 1, n_cont, n_prefix + n_cont).\n",
    "    # Left block: attend to cached BOS + doc; mask surrogate if truncate.\n",
    "    # Right block: causal self-attention among continuation tokens.\n",
    "    n_prefix = 1 + n_s + n_d\n",
    "    n_cont = n_q + n_a\n",
    "    min_val = torch.finfo(dtype).min\n",
    "\n",
    "    mask = torch.full((n_cont, n_prefix + n_cont), min_val, dtype=dtype)\n",
    "\n",
    "    # Attend to all cached prefix positions\n",
    "    mask[:, :n_prefix] = 0.0\n",
    "\n",
    "    # Truncation: mask surrogate positions (1..n_s) from continuation\n",
    "    if truncate and n_s > 0:\n",
    "        mask[:, 1:1 + n_s] = min_val\n",
    "\n",
    "    # Causal self-attention among continuation tokens\n",
    "    mask[:, n_prefix:] = torch.triu(\n",
    "        torch.full((n_cont, n_cont), min_val, dtype=dtype), diagonal=1\n",
    "    )\n",
    "\n",
    "    return mask.unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "\n",
    "def make_mask_dict(mask_4d):\n",
    "    # Wrap 4D mask in Gemma 3's dict format (bypasses internal mask creation).\n",
    "    # Both full and sliding attention layers get the same mask (seq < 1024).\n",
    "    return {\"full_attention\": mask_4d, \"sliding_attention\": mask_4d}\n",
    "\n",
    "\n",
    "# --- Sanity check: custom causal mask matches default forward ---\n",
    "print(\"Mask sanity check: custom causal mask vs default forward...\")\n",
    "test_text = \"The quick brown fox jumps over the lazy dog.\"\n",
    "test_ids = tokenizer(test_text, return_tensors=\"pt\",\n",
    "                     add_special_tokens=True).input_ids.to(DEVICE)\n",
    "Lt = test_ids.shape[1]\n",
    "\n",
    "with torch.no_grad():\n",
    "    out_default = model(input_ids=test_ids)\n",
    "\n",
    "# Build custom causal mask (treat entire sequence as bare prefix, no continuation)\n",
    "causal_mask = make_phase_a_mask(0, Lt - 1, mode=\"causal\")\n",
    "causal_dict = make_mask_dict(causal_mask.to(DEVICE))\n",
    "causal_pos = torch.arange(Lt, device=DEVICE).unsqueeze(0)\n",
    "\n",
    "with torch.no_grad():\n",
    "    out_custom = model(input_ids=test_ids, attention_mask=causal_dict,\n",
    "                       position_ids=causal_pos)\n",
    "\n",
    "max_diff = (out_default.logits - out_custom.logits).abs().max().item()\n",
    "print(f\"  Max logit diff: {max_diff:.6f}\")\n",
    "assert max_diff < 0.1, (\n",
    "    f\"FAIL: Custom causal mask doesn't match default (max_diff={max_diff:.4f}). \"\n",
    "    f\"Dict-based mask API may not work with this model/version.\")\n",
    "print(f\"  PASS: Dict-based mask API verified.\")\n",
    "print(f\"  (Run test_attention_masks.py --model for comprehensive tests)\")\n",
    "\n",
    "del out_default, out_custom\n",
    "gc.collect(); torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7788b2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Load MS MARCO data + generate surrogates and random prefixes\n",
    "from lib.data import count_words\n",
    "from datasets import load_dataset\n",
    "\n",
    "STOP_WORDS = {\n",
    "    'a', 'an', 'the', 'is', 'are', 'was', 'were', 'be', 'been', 'being',\n",
    "    'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could',\n",
    "    'should', 'may', 'might', 'can', 'shall', 'to', 'of', 'in', 'for',\n",
    "    'on', 'with', 'at', 'by', 'from', 'as', 'into', 'through', 'during',\n",
    "    'before', 'after', 'above', 'below', 'between', 'and', 'but', 'or',\n",
    "    'not', 'no', 'if', 'then', 'than', 'so', 'up', 'out', 'about',\n",
    "    'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those',\n",
    "    'it', 'its', 'i', 'me', 'my', 'we', 'our', 'you', 'your', 'he',\n",
    "    'him', 'his', 'she', 'her', 'they', 'them', 'their', 'how', 'when',\n",
    "    'where', 'why', 'much', 'many', 'some', 'any', 'all', 'each',\n",
    "    'does', 'also', 'just', 'more', 'most', 'very', 'too', 'only',\n",
    "}\n",
    "\n",
    "def extract_keywords(text, top_k=10):\n",
    "    words = re.sub(r'[^\\w\\s]', '', text.lower()).split()\n",
    "    content = [w for w in words if w not in STOP_WORDS and len(w) > 2]\n",
    "    if not content:\n",
    "        return [\"information\"]\n",
    "    counts = Counter(content)\n",
    "    return [w for w, _ in counts.most_common(top_k)]\n",
    "\n",
    "WORD_POOL = [\n",
    "    \"computer\", \"mountain\", \"hospital\", \"children\", \"building\", \"national\",\n",
    "    \"business\", \"research\", \"students\", \"american\", \"possible\", \"economic\",\n",
    "    \"personal\", \"together\", \"products\", \"services\", \"actually\", \"remember\",\n",
    "    \"practice\", \"training\", \"industry\", \"complete\", \"critical\", \"function\",\n",
    "    \"language\", \"standard\", \"material\", \"original\", \"physical\", \"security\",\n",
    "    \"interest\", \"problems\", \"consider\", \"response\", \"pressure\", \"politics\",\n",
    "    \"movement\", \"evidence\", \"southern\", \"northern\", \"exchange\", \"decision\",\n",
    "    \"position\", \"increase\", \"describe\", \"military\", \"required\", \"approach\",\n",
    "    \"strategy\", \"customer\", \"resource\", \"employee\", \"audience\", \"location\",\n",
    "    \"property\", \"cultural\", \"activity\", \"strength\", \"analysis\", \"powerful\",\n",
    "    \"election\", \"argument\", \"campaign\", \"maintain\", \"question\", \"behavior\",\n",
    "    \"majority\", \"solution\", \"software\", \"consumer\", \"creative\", \"reaction\",\n",
    "    \"european\", \"delivery\", \"organize\", \"involved\", \"relative\", \"learning\",\n",
    "    \"positive\", \"numerous\", \"familiar\", \"engineer\", \"platform\", \"indicate\",\n",
    "    \"previous\", \"pleasure\", \"opposite\", \"magazine\", \"document\", \"religion\",\n",
    "    \"scenario\", \"workshop\", \"minority\", \"guidance\", \"estimate\", \"recently\",\n",
    "    \"surprise\", \"champion\", \"pleasant\", \"grateful\", \"moderate\", \"boundary\",\n",
    "]\n",
    "\n",
    "print(\"Loading MS MARCO v1.1 validation...\")\n",
    "ds = load_dataset(\"microsoft/ms_marco\", \"v1.1\", split=\"validation\")\n",
    "\n",
    "all_candidates = []\n",
    "for item in ds:\n",
    "    if len(all_candidates) >= 3 * N_SAMPLES:\n",
    "        break\n",
    "    passages = item.get('passages', {})\n",
    "    ptexts = passages.get('passage_text', [])\n",
    "    is_sel = passages.get('is_selected', [])\n",
    "    query = item.get('query', '')\n",
    "    answers = item.get('answers', [])\n",
    "    well_formed = item.get('wellFormedAnswers', [])\n",
    "    answer = None\n",
    "    if well_formed and len(well_formed) > 0 and well_formed[0] not in ('[]', ''):\n",
    "        answer = well_formed[0]\n",
    "    elif answers and len(answers) > 0 and answers[0] != 'No Answer Present.':\n",
    "        answer = answers[0]\n",
    "    if not answer:\n",
    "        continue\n",
    "    for pt, sel in zip(ptexts, is_sel):\n",
    "        wc = count_words(pt)\n",
    "        if sel == 1 and 30 <= wc <= 300:\n",
    "            all_candidates.append({\n",
    "                'passage': pt, 'query': query, 'answer': answer,\n",
    "                'word_count': wc,\n",
    "            })\n",
    "            break\n",
    "\n",
    "print(f\"Total candidates: {len(all_candidates)}\")\n",
    "np.random.seed(SEED)\n",
    "indices = np.random.permutation(len(all_candidates))\n",
    "samples = [all_candidates[i] for i in indices[:N_SAMPLES]]\n",
    "del ds, all_candidates\n",
    "gc.collect()\n",
    "\n",
    "# Generate surrogates\n",
    "for i, s in enumerate(samples):\n",
    "    s['surr_doc'] = \" \".join(extract_keywords(s['passage'], top_k=10))\n",
    "    rng = np.random.RandomState(SEED + i + 20000)\n",
    "    words = rng.choice(WORD_POOL, size=8, replace=False)\n",
    "    s['random_prefix'] = \" \".join(words)\n",
    "\n",
    "print(f\"Loaded {len(samples)} samples\")\n",
    "print(f\"Mean passage words: {np.mean([s['word_count'] for s in samples]):.0f}\")\n",
    "print(f\"Mean query words: {np.mean([count_words(s['query']) for s in samples]):.0f}\")\n",
    "print(f\"Mean answer words: {np.mean([count_words(s['answer']) for s in samples]):.0f}\")\n",
    "print(f\"Example surr_doc: '{samples[0]['surr_doc']}'\")\n",
    "print(f\"Example random prefix: '{samples[0]['random_prefix']}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dceeabed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: score_sample() — two-pass scoring\n",
    "#\n",
    "# Phase A (offline): Forward [BOS, surr, doc] with use_cache=True -> past_key_values\n",
    "# Phase B (online):  Forward [query, answer] using cached KVs -> compute NLL\n",
    "#\n",
    "# For _nq conditions: Phase B input is [answer] only. The first answer token's\n",
    "# logit comes from Phase A's last position; remaining from Phase B.\n",
    "\n",
    "def score_sample(model, tokenizer, sample, device, conditions):\n",
    "    # Score one MS MARCO sample under all conditions using two-pass design.\n",
    "    # Returns dict mapping condition_name -> mean NLL on answer tokens.\n",
    "    passage = sample['passage']\n",
    "    query = sample['query']\n",
    "    answer = sample['answer']\n",
    "    surr_doc = sample['surr_doc']\n",
    "    random_prefix = sample['random_prefix']\n",
    "\n",
    "    bos_id = tokenizer.bos_token_id\n",
    "\n",
    "    # Tokenize segments (no special tokens — we add BOS manually)\n",
    "    doc_ids = tokenizer(passage, add_special_tokens=False, truncation=True,\n",
    "                        max_length=1024).input_ids\n",
    "    query_ids = tokenizer(query, add_special_tokens=False, truncation=True,\n",
    "                          max_length=512).input_ids\n",
    "    answer_ids = tokenizer(answer, add_special_tokens=False, truncation=True,\n",
    "                           max_length=256).input_ids\n",
    "\n",
    "    if len(answer_ids) == 0:\n",
    "        return None\n",
    "\n",
    "    # Surrogate token IDs for each prefix type\n",
    "    oracle_ids = tokenizer(query, add_special_tokens=False, truncation=True,\n",
    "                           max_length=256).input_ids\n",
    "    random_ids = tokenizer(random_prefix, add_special_tokens=False, truncation=True,\n",
    "                           max_length=256).input_ids\n",
    "    surr_doc_ids = tokenizer(surr_doc, add_special_tokens=False, truncation=True,\n",
    "                             max_length=256).input_ids\n",
    "\n",
    "    prefix_map = {\n",
    "        \"bare\": [],\n",
    "        \"oracle_trunc\": oracle_ids,\n",
    "        \"random_trunc\": random_ids,\n",
    "        \"surr_doc_trunc\": surr_doc_ids,\n",
    "    }\n",
    "\n",
    "    targets = torch.tensor(answer_ids, dtype=torch.long, device=device)\n",
    "    result = {}\n",
    "\n",
    "    for attn_mode, prefix_type, has_query in conditions:\n",
    "        cond_name = condition_name(attn_mode, prefix_type, has_query)\n",
    "\n",
    "        surr_ids = prefix_map[prefix_type]\n",
    "        n_s = len(surr_ids)\n",
    "        n_d = len(doc_ids)\n",
    "        n_q = len(query_ids) if has_query else 0\n",
    "        n_a = len(answer_ids)\n",
    "        n_prefix = 1 + n_s + n_d\n",
    "\n",
    "        # === Phase A: Cache generation ===\n",
    "        prefix_tokens = [bos_id] + surr_ids + doc_ids\n",
    "        prefix_input = torch.tensor([prefix_tokens], dtype=torch.long, device=device)\n",
    "\n",
    "        phase_a_mask = make_phase_a_mask(n_s, n_d, mode=attn_mode)\n",
    "        phase_a_dict = make_mask_dict(phase_a_mask.to(device))\n",
    "        phase_a_pos = torch.arange(n_prefix, device=device).unsqueeze(0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            out_a = model(input_ids=prefix_input, attention_mask=phase_a_dict,\n",
    "                          position_ids=phase_a_pos, use_cache=True)\n",
    "        past_kv = out_a.past_key_values\n",
    "\n",
    "        # === Phase B: Evaluation with cached KVs ===\n",
    "        if has_query:\n",
    "            cont_tokens = query_ids + answer_ids\n",
    "        else:\n",
    "            cont_tokens = list(answer_ids)\n",
    "        n_cont = len(cont_tokens)\n",
    "\n",
    "        cont_input = torch.tensor([cont_tokens], dtype=torch.long, device=device)\n",
    "\n",
    "        truncate = (prefix_type != \"bare\")\n",
    "        phase_b_mask = make_phase_b_mask(n_s, n_d, n_q, n_a, truncate=truncate)\n",
    "        phase_b_dict = make_mask_dict(phase_b_mask.to(device))\n",
    "        phase_b_pos = torch.arange(n_prefix, n_prefix + n_cont,\n",
    "                                    device=device).unsqueeze(0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            out_b = model(input_ids=cont_input, attention_mask=phase_b_dict,\n",
    "                          position_ids=phase_b_pos, past_key_values=past_kv)\n",
    "\n",
    "        # === Compute NLL on answer tokens ===\n",
    "        if has_query:\n",
    "            # Position n_q-1 in Phase B predicts first answer token (a0)\n",
    "            # Position n_q+n_a-2 predicts last answer token (a_{n_a-1})\n",
    "            answer_logits = out_b.logits[0, n_q - 1 : n_q + n_a - 1, :]\n",
    "        else:\n",
    "            # First answer token predicted by Phase A's last position\n",
    "            logit_first = out_a.logits[0, -1:, :]\n",
    "            if n_a > 1:\n",
    "                # Remaining answer tokens predicted by Phase B positions 0..n_a-2\n",
    "                logit_rest = out_b.logits[0, :n_a - 1, :]\n",
    "                answer_logits = torch.cat([logit_first, logit_rest], dim=0)\n",
    "            else:\n",
    "                answer_logits = logit_first\n",
    "\n",
    "        log_probs = F.log_softmax(answer_logits, dim=-1)\n",
    "        token_nlls = -log_probs.gather(1, targets.unsqueeze(1)).squeeze(1)\n",
    "        mean_nll = token_nlls.mean().item()\n",
    "\n",
    "        result[f'nll_{cond_name}'] = mean_nll\n",
    "\n",
    "        del out_a, out_b, past_kv, prefix_input, cont_input\n",
    "        del phase_a_mask, phase_b_mask, phase_a_dict, phase_b_dict\n",
    "        del answer_logits, log_probs, token_nlls\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "print(\"Scoring function defined (two-pass, 10 conditions per sample).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce99514b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Main scoring loop\n",
    "print(\"=\" * 70)\n",
    "print(\"MAIN SCORING LOOP\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "CKPT_PATH = RESULTS_DIR / \"checkpoint.json\"\n",
    "\n",
    "# Resume from checkpoint\n",
    "all_results = []\n",
    "start_idx = 0\n",
    "if CKPT_PATH.exists():\n",
    "    ckpt = json.loads(CKPT_PATH.read_text())\n",
    "    if len(ckpt.get('results', [])) > 0:\n",
    "        saved_queries = [r['query'][:50] for r in ckpt['results']]\n",
    "        current_queries = [s['query'][:50] for s in samples[:len(saved_queries)]]\n",
    "        if saved_queries == current_queries:\n",
    "            all_results = ckpt['results']\n",
    "            start_idx = len(all_results)\n",
    "            print(f\"Resuming from checkpoint: {start_idx}/{N_SAMPLES}\")\n",
    "\n",
    "if start_idx == 0:\n",
    "    print(f\"Starting fresh: {N_SAMPLES} samples x {len(CONDITIONS)} conditions\")\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "for i in tqdm(range(start_idx, N_SAMPLES), initial=start_idx, total=N_SAMPLES,\n",
    "              desc=\"Scoring\"):\n",
    "    s = samples[i]\n",
    "    try:\n",
    "        result = score_sample(model, tokenizer, s, DEVICE, CONDITIONS)\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR at sample {i}: {e}\")\n",
    "        result = None\n",
    "\n",
    "    if result is None:\n",
    "        continue\n",
    "    result['query'] = s['query'][:50]\n",
    "    all_results.append(result)\n",
    "\n",
    "    if (i + 1) % 25 == 0 or i == N_SAMPLES - 1:\n",
    "        ckpt = {\n",
    "            'model': MODEL_NAME,\n",
    "            'n_total': N_SAMPLES,\n",
    "            'n_conditions': len(CONDITIONS),\n",
    "            'condition_names': COND_NAMES,\n",
    "            'results': all_results,\n",
    "            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        }\n",
    "        CKPT_PATH.write_text(json.dumps(ckpt))\n",
    "\n",
    "    if (i + 1) % 100 == 0:\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "elapsed = time.time() - t0\n",
    "print(f\"\\nDone: {len(all_results)} samples in {elapsed/60:.1f} min\")\n",
    "print(f\"\\nQuick summary:\")\n",
    "for cn in COND_NAMES:\n",
    "    vals = [r[f'nll_{cn}'] for r in all_results]\n",
    "    print(f\"  {cn:<35} NLL={np.mean(vals):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828104c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Effect sizes and significance tests\n",
    "print(\"=\" * 70)\n",
    "print(\"RESULTS: EFFECT SIZES AND SIGNIFICANCE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Extract NLL arrays\n",
    "nll = {}\n",
    "for cn in COND_NAMES:\n",
    "    nll[cn] = np.array([r[f'nll_{cn}'] for r in all_results])\n",
    "\n",
    "N = len(all_results)\n",
    "\n",
    "# --- Mean NLL table ---\n",
    "print(f\"\\n--- Mean NLL ({N} samples) ---\\n\")\n",
    "print(f\"  {'Condition':<35} {'Mean NLL':>10} {'Std':>8}\")\n",
    "print(f\"  {'-'*55}\")\n",
    "for cn in COND_NAMES:\n",
    "    print(f\"  {cn:<35} {nll[cn].mean():>10.4f} {nll[cn].std():>8.4f}\")\n",
    "\n",
    "# --- Key comparisons ---\n",
    "print(f\"\\n--- Key Comparisons (positive d = condition helps) ---\\n\")\n",
    "print(f\"  {'Comparison':<50} {'d':>8} {'win%':>7} {'p':>12} {'sig':>5}\")\n",
    "print(f\"  {'-'*85}\")\n",
    "\n",
    "comparisons = [\n",
    "    # Pure bidirectional benefit (no surrogate)\n",
    "    (\"d_bidirectional: plm_bare vs c_bare\",\n",
    "     nll['causal_bare'] - nll['prefix_lm_bare']),\n",
    "\n",
    "    # Oracle enrichment under prefix LM\n",
    "    (\"d_oracle: plm_oracle vs plm_bare\",\n",
    "     nll['prefix_lm_bare'] - nll['prefix_lm_oracle_trunc']),\n",
    "\n",
    "    # Surrogate enrichment under prefix LM\n",
    "    (\"d_surr_doc: plm_surr_doc vs plm_bare\",\n",
    "     nll['prefix_lm_bare'] - nll['prefix_lm_surr_doc_trunc']),\n",
    "\n",
    "    # Structural (random) under prefix LM\n",
    "    (\"d_random: plm_random vs plm_bare\",\n",
    "     nll['prefix_lm_bare'] - nll['prefix_lm_random_trunc']),\n",
    "\n",
    "    # Oracle under causal (doc attends causally to surrogate)\n",
    "    (\"d_causal_oracle: c_oracle vs c_bare (causal pfx)\",\n",
    "     nll['causal_bare'] - nll['causal_oracle_trunc']),\n",
    "\n",
    "    # Random under causal\n",
    "    (\"d_causal_random: c_random vs c_bare (causal pfx)\",\n",
    "     nll['causal_bare'] - nll['causal_random_trunc']),\n",
    "\n",
    "    # Surr_doc under causal\n",
    "    (\"d_causal_surr: c_surr_doc vs c_bare (causal pfx)\",\n",
    "     nll['causal_bare'] - nll['causal_surr_doc_trunc']),\n",
    "\n",
    "    # Decoder query effect (causal)\n",
    "    (\"d_dec_q: c_bare vs c_bare_nq (query helps?)\",\n",
    "     nll['causal_bare_nq'] - nll['causal_bare']),\n",
    "\n",
    "    # Oracle bidirectional without query\n",
    "    (\"d_plm_oracle_nq: plm_orc_nq vs c_bare_nq\",\n",
    "     nll['causal_bare_nq'] - nll['prefix_lm_oracle_trunc_nq']),\n",
    "\n",
    "    # Interaction: bidirectional with vs without surrogate\n",
    "    (\"d_bidir_with_oracle: plm_orc vs c_orc\",\n",
    "     nll['causal_oracle_trunc'] - nll['prefix_lm_oracle_trunc']),\n",
    "]\n",
    "\n",
    "for label, diff in comparisons:\n",
    "    d = cohens_d(diff)\n",
    "    _, p = stats.ttest_1samp(diff, 0)\n",
    "    sig = '***' if p < 0.001 else '**' if p < 0.01 else '*' if p < 0.05 else 'ns'\n",
    "    win = (diff > 0).mean() * 100\n",
    "    print(f\"  {label:<50} {d:>+8.3f} {win:>6.1f}% {p:>12.2e} {sig:>5}\")\n",
    "\n",
    "# Structural fraction\n",
    "d_oracle_val = cohens_d(nll['prefix_lm_bare'] - nll['prefix_lm_oracle_trunc'])\n",
    "d_random_val = cohens_d(nll['prefix_lm_bare'] - nll['prefix_lm_random_trunc'])\n",
    "struct_frac = d_random_val / d_oracle_val if d_oracle_val != 0 else float('nan')\n",
    "print(f\"\\n  Structural fraction (d_random / d_oracle): {struct_frac:.1%}\")\n",
    "\n",
    "# --- 2x4 table: attention mode x prefix type ---\n",
    "print(f\"\\n--- 2x4 Table: Mean NLL by (attention mode x prefix type) ---\\n\")\n",
    "print(f\"  {'Prefix':<20} {'Causal':>10} {'Prefix LM':>10} {'Diff (C-P)':>12} {'d':>8}\")\n",
    "print(f\"  {'-'*65}\")\n",
    "for prefix in ['bare', 'oracle_trunc', 'random_trunc', 'surr_doc_trunc']:\n",
    "    c_name = f\"causal_{prefix}\"\n",
    "    p_name = f\"prefix_lm_{prefix}\"\n",
    "    diff = nll[c_name] - nll[p_name]\n",
    "    d = cohens_d(diff)\n",
    "    print(f\"  {prefix:<20} {nll[c_name].mean():>10.4f} {nll[p_name].mean():>10.4f} \"\n",
    "          f\"{diff.mean():>+12.4f} {d:>+8.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae64bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Save final results\n",
    "print(\"=\" * 70)\n",
    "print(\"SUMMARY -- Prefix LM Exp 01\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Collect summary\n",
    "summary = {\n",
    "    'n_samples': N,\n",
    "    'model': MODEL_NAME,\n",
    "}\n",
    "\n",
    "# NLL means\n",
    "for cn in COND_NAMES:\n",
    "    summary[f'nll_{cn}'] = float(nll[cn].mean())\n",
    "\n",
    "# Key effect sizes\n",
    "key_effects = {\n",
    "    'd_bidirectional': nll['causal_bare'] - nll['prefix_lm_bare'],\n",
    "    'd_oracle': nll['prefix_lm_bare'] - nll['prefix_lm_oracle_trunc'],\n",
    "    'd_surr_doc': nll['prefix_lm_bare'] - nll['prefix_lm_surr_doc_trunc'],\n",
    "    'd_random': nll['prefix_lm_bare'] - nll['prefix_lm_random_trunc'],\n",
    "    'd_causal_oracle': nll['causal_bare'] - nll['causal_oracle_trunc'],\n",
    "    'd_dec_q': nll['causal_bare_nq'] - nll['causal_bare'],\n",
    "    'd_plm_oracle_nq': nll['causal_bare_nq'] - nll['prefix_lm_oracle_trunc_nq'],\n",
    "}\n",
    "\n",
    "for name, diff in key_effects.items():\n",
    "    d = cohens_d(diff)\n",
    "    _, p = stats.ttest_1samp(diff, 0)\n",
    "    summary[name] = float(d)\n",
    "    summary[f'{name}_p'] = float(p)\n",
    "\n",
    "summary['structural_fraction'] = float(struct_frac)\n",
    "\n",
    "# Verdict (use significance, not arbitrary d threshold)\n",
    "d_bidir = cohens_d(nll['causal_bare'] - nll['prefix_lm_bare'])\n",
    "_, p_bidir = stats.ttest_1samp(nll['causal_bare'] - nll['prefix_lm_bare'], 0)\n",
    "d_oracle = cohens_d(nll['prefix_lm_bare'] - nll['prefix_lm_oracle_trunc'])\n",
    "_, p_oracle = stats.ttest_1samp(nll['prefix_lm_bare'] - nll['prefix_lm_oracle_trunc'], 0)\n",
    "d_causal_orc = cohens_d(nll['causal_bare'] - nll['causal_oracle_trunc'])\n",
    "_, p_causal_orc = stats.ttest_1samp(nll['causal_bare'] - nll['causal_oracle_trunc'], 0)\n",
    "\n",
    "print(f\"\\n  d_bidirectional (prefix LM bare vs causal bare): {d_bidir:+.3f} (p={p_bidir:.2e})\")\n",
    "print(f\"  d_oracle (prefix LM: oracle vs bare):             {d_oracle:+.3f} (p={p_oracle:.2e})\")\n",
    "print(f\"  d_causal_oracle (causal: oracle vs bare):          {d_causal_orc:+.3f} (p={p_causal_orc:.2e})\")\n",
    "print(f\"  structural_fraction:                               {struct_frac:.1%}\")\n",
    "\n",
    "print(f\"\\n  VERDICT:\")\n",
    "if p_bidir < 0.05 and d_bidir > 0:\n",
    "    print(f\"  Bidirectional attention HELPS decoder-only models (d={d_bidir:+.3f}, ***).\")\n",
    "elif p_bidir < 0.05 and d_bidir < 0:\n",
    "    print(f\"  Bidirectional attention HURTS decoder-only models (d={d_bidir:+.3f}, ***).\")\n",
    "    print(f\"  Model was trained with causal attention -- bidirectional disrupts representations.\")\n",
    "else:\n",
    "    print(f\"  Bidirectional attention has no significant effect (d={d_bidir:+.3f}, ns).\")\n",
    "\n",
    "if p_oracle < 0.05 and d_oracle > 0:\n",
    "    print(f\"  Surrogate enrichment transfers under prefix LM (d={d_oracle:+.3f}, sig).\")\n",
    "else:\n",
    "    print(f\"  Surrogate enrichment does NOT transfer under prefix LM (d={d_oracle:+.3f}, ns).\")\n",
    "\n",
    "if p_causal_orc < 0.05 and d_causal_orc > 0:\n",
    "    print(f\"  Causal prefix WORKS (d={d_causal_orc:+.3f}, ***). Doc attends causally to surrogate.\")\n",
    "    print(f\"  The enrichment effect transfers via CAUSAL attention, not bidirectionality.\")\n",
    "else:\n",
    "    print(f\"  Causal prefix has no effect (d={d_causal_orc:+.3f}).\")\n",
    "\n",
    "# Save\n",
    "final_results = {\n",
    "    'experiment': 'prefix_lm_exp01',\n",
    "    'dataset': 'ms_marco_v1.1',\n",
    "    'model': MODEL_NAME,\n",
    "    'n_samples': N,\n",
    "    'seed': SEED,\n",
    "    'conditions': COND_NAMES,\n",
    "    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'summary': summary,\n",
    "    'enc_dec_references': {\n",
    "        'T5Gemma_exp01': {\n",
    "            'd_oracle': 0.228, 'd_random': 0.080, 'structural_frac': 0.35,\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "with open(RESULTS_DIR / 'results.json', 'w') as f:\n",
    "    json.dump(final_results, f, indent=2)\n",
    "print(f\"\\nResults saved to {RESULTS_DIR / 'results.json'}\")\n",
    "print(\"Done!\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
