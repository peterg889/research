{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "305259ff",
   "metadata": {},
   "source": [
    "# Prefix LM Exp 02: Semantic Isolation via Truncation x Content Factorial\n",
    "\n",
    "## Motivation\n",
    "\n",
    "Exp 01 showed causal prefixes help Gemma 3 12B IT: d_oracle=+0.452, d_random=+0.475,\n",
    "structural fraction 140%. Random tokens work AS WELL as oracle under truncation.\n",
    "\n",
    "**Key unused lever**: Exp 01 only tested `truncate=True` (Phase B cannot attend to\n",
    "surrogate positions). With `truncate=False`, query/answer tokens attend DIRECTLY to\n",
    "cached surrogate KVs -- a **direct semantic channel** that doesn't exist under truncation.\n",
    "\n",
    "If semantic content matters, oracle should benefit MORE from direct access than\n",
    "wrong-query or random.\n",
    "\n",
    "## Conditions (3 x 2 + 2 = 8)\n",
    "\n",
    "| # | Condition | Content | Truncate | Channel |\n",
    "|---|-----------|---------|----------|---------|\n",
    "| 1 | bare | none | n/a | Baseline |\n",
    "| 2 | oracle_trunc | correct query | yes | Indirect only |\n",
    "| 3 | wrong_query_trunc | wrong query | yes | Indirect, wrong semantics |\n",
    "| 4 | random_trunc | random words | yes | Indirect, no semantics |\n",
    "| 5 | oracle_full | correct query | no | Direct + indirect |\n",
    "| 6 | wrong_query_full | wrong query | no | Direct wrong + indirect |\n",
    "| 7 | random_full | random words | no | Direct noise + indirect |\n",
    "| 8 | answer_leak_trunc | first 5 answer tokens | yes | Positive control |\n",
    "\n",
    "**Wrong query**: query from sample `(i+1) % N` -- matched style/length, wrong content.\n",
    "\n",
    "## Key Analyses\n",
    "\n",
    "- **A**: Structural replication (bare vs random_trunc, expect d~+0.475)\n",
    "- **B**: Semantic under truncation (oracle_trunc vs wrong_query_trunc)\n",
    "- **C**: Semantic under full access (oracle_full vs wrong_query_full)\n",
    "- **D**: Truncation x content interaction (THE critical test)\n",
    "- **E**: Truncation main effect per content type\n",
    "- **F**: Positive control (answer_leak_trunc vs random_trunc)\n",
    "- **G**: Per-sample heterogeneity (correlate with overlap, length)\n",
    "- **H**: Length-controlled regression (oracle vs wrong_query may differ in length)\n",
    "\n",
    "## Two-Pass Design\n",
    "\n",
    "Same as Exp 01. All conditions use causal attention for Phase A.\n",
    "\n",
    "- **Phase A (offline)**: Process `[BOS, surrogate, doc]` with causal mask, `use_cache=True`\n",
    "- **Phase B (online)**: Process `[query, answer]` with cached KVs\n",
    "  - `_trunc`: surrogate positions masked from continuation\n",
    "  - `_full`: ALL cached positions accessible (direct semantic channel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1201cea4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-21T14:20:25.687534Z",
     "iopub.status.busy": "2026-02-21T14:20:25.686729Z",
     "iopub.status.idle": "2026-02-21T14:20:30.209150Z",
     "shell.execute_reply": "2026-02-21T14:20:30.208098Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prefix LM Exp 02: Semantic Isolation (Truncation x Content)\n",
      "N: 500, Conditions: 8\n",
      "DEVICE: cuda\n",
      "GPU: NVIDIA A100-SXM4-40GB\n",
      "GPU memory: 42.3 GB\n",
      "\n",
      "Conditions:\n",
      "  bare\n",
      "  oracle_trunc\n",
      "  wrong_query_trunc\n",
      "  random_trunc\n",
      "  oracle_full\n",
      "  wrong_query_full\n",
      "  random_full\n",
      "  answer_leak_trunc\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Setup\n",
    "import os\n",
    "os.umask(0o000)\n",
    "\n",
    "import sys, json, time, gc, re\n",
    "import random as pyrandom\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from scipy import stats\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "sys.path.insert(0, \"../../..\")\n",
    "from lib.analysis import cohens_d\n",
    "\n",
    "SEED = 42\n",
    "N_SAMPLES = 500\n",
    "\n",
    "MODEL_NAME = \"google/gemma-3-12b-it\"\n",
    "\n",
    "RESULTS_DIR = Path(\"../../../results/prefix_lm_exp02\")\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "pyrandom.seed(SEED)\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv())\n",
    "HF_TOKEN = os.environ.get(\"HF_TOKEN\")\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 8 conditions: (prefix_type, truncate)\n",
    "CONDITIONS = [\n",
    "    (\"bare\",         True),   # bare -- baseline\n",
    "    (\"oracle\",       True),   # oracle_trunc\n",
    "    (\"wrong_query\",  True),   # wrong_query_trunc\n",
    "    (\"random\",       True),   # random_trunc\n",
    "    (\"oracle\",       False),  # oracle_full\n",
    "    (\"wrong_query\",  False),  # wrong_query_full\n",
    "    (\"random\",       False),  # random_full\n",
    "    (\"answer_leak\",  True),   # answer_leak_trunc -- positive control\n",
    "]\n",
    "\n",
    "def condition_name(prefix_type, truncate):\n",
    "    if prefix_type == \"bare\":\n",
    "        return \"bare\"\n",
    "    suffix = \"trunc\" if truncate else \"full\"\n",
    "    return f\"{prefix_type}_{suffix}\"\n",
    "\n",
    "COND_NAMES = [condition_name(p, t) for p, t in CONDITIONS]\n",
    "\n",
    "print(f\"Prefix LM Exp 02: Semantic Isolation (Truncation x Content)\")\n",
    "print(f\"N: {N_SAMPLES}, Conditions: {len(CONDITIONS)}\")\n",
    "print(f\"DEVICE: {DEVICE}\")\n",
    "print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "print(f\"\\nConditions:\")\n",
    "for cn in COND_NAMES:\n",
    "    print(f\"  {cn}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36fb03bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-21T14:20:30.212974Z",
     "iopub.status.busy": "2026-02-21T14:20:30.212390Z",
     "iopub.status.idle": "2026-02-21T14:20:46.630903Z",
     "shell.execute_reply": "2026-02-21T14:20:46.630172Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformers version: 5.1.0\n",
      "Loading google/gemma-3-12b-it...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ede1c2a944f4018b7a1ef93b112265f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/1065 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: 12.2B params, 24.4 GB GPU, 14s\n",
      "BOS token id: 2\n",
      "Model dtype: torch.bfloat16\n",
      "Attn implementation: eager\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Load model + tokenizer\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import transformers\n",
    "\n",
    "print(f\"transformers version: {transformers.__version__}\")\n",
    "\n",
    "print(f\"Loading {MODEL_NAME}...\")\n",
    "t0 = time.time()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, token=HF_TOKEN)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    attn_implementation=\"eager\",\n",
    "    token=HF_TOKEN,\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "n_params = sum(p.numel() for p in model.parameters()) / 1e9\n",
    "gpu_mem = torch.cuda.memory_allocated() / 1e9\n",
    "print(f\"Loaded: {n_params:.1f}B params, {gpu_mem:.1f} GB GPU, {time.time()-t0:.0f}s\")\n",
    "print(f\"BOS token id: {tokenizer.bos_token_id}\")\n",
    "print(f\"Model dtype: {model.dtype}\")\n",
    "print(f\"Attn implementation: {model.config._attn_implementation}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a008182",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-21T14:20:46.634421Z",
     "iopub.status.busy": "2026-02-21T14:20:46.633981Z",
     "iopub.status.idle": "2026-02-21T14:20:47.529716Z",
     "shell.execute_reply": "2026-02-21T14:20:47.528958Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mask sanity check: custom causal mask vs default forward...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Max logit diff: 0.000000\n",
      "  PASS: Dict-based mask API verified.\n",
      "  Trunc vs Full mask: 40 positions differ (expect 5*8=40)\n",
      "  PASS: Truncation mask correctly blocks 5 surrogate positions from 8 cont tokens.\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Phase A/B attention masks + sanity check\n",
    "#\n",
    "# Reused from Exp 01. Two-pass design:\n",
    "#   Phase A: Process [BOS, surrogate, doc] -> cache KV states\n",
    "#   Phase B: Process [query, answer] using cached KVs -> NLL\n",
    "#\n",
    "# Phase B truncate parameter controls whether surrogate positions are masked:\n",
    "#   truncate=True  -> surrogate blocked (indirect channel only)\n",
    "#   truncate=False -> surrogate accessible (direct + indirect channel)\n",
    "\n",
    "def make_phase_a_mask(n_s, n_d, mode=\"causal\", dtype=torch.bfloat16):\n",
    "    # Phase A mask for prefix [BOS, surrogate, doc].\n",
    "    # Returns (1, 1, n_prefix, n_prefix).\n",
    "    # Always causal in this experiment (mode parameter kept for API compatibility).\n",
    "    n_prefix = 1 + n_s + n_d\n",
    "    min_val = torch.finfo(dtype).min\n",
    "    if mode == \"prefix_lm\":\n",
    "        mask = torch.zeros((n_prefix, n_prefix), dtype=dtype)\n",
    "    else:\n",
    "        mask = torch.triu(torch.full((n_prefix, n_prefix), min_val, dtype=dtype),\n",
    "                          diagonal=1)\n",
    "    return mask.unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "\n",
    "def make_phase_b_mask(n_s, n_d, n_q, n_a, truncate=True, dtype=torch.bfloat16):\n",
    "    # Phase B mask for continuation [query, answer] attending to cached prefix.\n",
    "    # Returns (1, 1, n_cont, n_prefix + n_cont).\n",
    "    # Left block: attend to cached BOS + doc; mask surrogate if truncate.\n",
    "    # Right block: causal self-attention among continuation tokens.\n",
    "    n_prefix = 1 + n_s + n_d\n",
    "    n_cont = n_q + n_a\n",
    "    min_val = torch.finfo(dtype).min\n",
    "\n",
    "    mask = torch.full((n_cont, n_prefix + n_cont), min_val, dtype=dtype)\n",
    "\n",
    "    # Attend to all cached prefix positions\n",
    "    mask[:, :n_prefix] = 0.0\n",
    "\n",
    "    # Truncation: mask surrogate positions (1..n_s) from continuation\n",
    "    if truncate and n_s > 0:\n",
    "        mask[:, 1:1 + n_s] = min_val\n",
    "\n",
    "    # Causal self-attention among continuation tokens\n",
    "    mask[:, n_prefix:] = torch.triu(\n",
    "        torch.full((n_cont, n_cont), min_val, dtype=dtype), diagonal=1\n",
    "    )\n",
    "\n",
    "    return mask.unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "\n",
    "def make_mask_dict(mask_4d):\n",
    "    # Wrap 4D mask in Gemma 3's dict format (bypasses internal mask creation).\n",
    "    # Both full and sliding attention layers get the same mask (seq < 1024).\n",
    "    return {\"full_attention\": mask_4d, \"sliding_attention\": mask_4d}\n",
    "\n",
    "\n",
    "# --- Sanity check: custom causal mask matches default forward ---\n",
    "print(\"Mask sanity check: custom causal mask vs default forward...\")\n",
    "test_text = \"The quick brown fox jumps over the lazy dog.\"\n",
    "test_ids = tokenizer(test_text, return_tensors=\"pt\",\n",
    "                     add_special_tokens=True).input_ids.to(DEVICE)\n",
    "Lt = test_ids.shape[1]\n",
    "\n",
    "with torch.no_grad():\n",
    "    out_default = model(input_ids=test_ids)\n",
    "\n",
    "# Build custom causal mask (treat entire sequence as bare prefix, no continuation)\n",
    "causal_mask = make_phase_a_mask(0, Lt - 1, mode=\"causal\")\n",
    "causal_dict = make_mask_dict(causal_mask.to(DEVICE))\n",
    "causal_pos = torch.arange(Lt, device=DEVICE).unsqueeze(0)\n",
    "\n",
    "with torch.no_grad():\n",
    "    out_custom = model(input_ids=test_ids, attention_mask=causal_dict,\n",
    "                       position_ids=causal_pos)\n",
    "\n",
    "max_diff = (out_default.logits - out_custom.logits).abs().max().item()\n",
    "print(f\"  Max logit diff: {max_diff:.6f}\")\n",
    "assert max_diff < 0.1, (\n",
    "    f\"FAIL: Custom causal mask doesn't match default (max_diff={max_diff:.4f}). \"\n",
    "    f\"Dict-based mask API may not work with this model/version.\")\n",
    "print(f\"  PASS: Dict-based mask API verified.\")\n",
    "\n",
    "# --- Sanity check: truncate=False gives different mask than truncate=True ---\n",
    "test_mask_trunc = make_phase_b_mask(5, 10, 3, 5, truncate=True)\n",
    "test_mask_full = make_phase_b_mask(5, 10, 3, 5, truncate=False)\n",
    "n_diff = (test_mask_trunc != test_mask_full).sum().item()\n",
    "print(f\"  Trunc vs Full mask: {n_diff} positions differ (expect 5*8=40)\")\n",
    "assert n_diff == 40, f\"FAIL: Expected 40 differing positions, got {n_diff}\"\n",
    "print(f\"  PASS: Truncation mask correctly blocks 5 surrogate positions from 8 cont tokens.\")\n",
    "\n",
    "del out_default, out_custom\n",
    "gc.collect(); torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8e1297e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-21T14:20:47.532823Z",
     "iopub.status.busy": "2026-02-21T14:20:47.532548Z",
     "iopub.status.idle": "2026-02-21T14:20:48.952927Z",
     "shell.execute_reply": "2026-02-21T14:20:48.952182Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MS MARCO v1.1 validation...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total candidates: 1500\n",
      "Loaded 500 samples\n",
      "Mean passage words: 74\n",
      "Mean query words: 6\n",
      "Mean answer words: 14\n",
      "Mean query-doc overlap (Jaccard): 0.072\n",
      "\n",
      "Example wrong_query: 'how thick does concrete need to be garden wall...'\n",
      "Example random prefix: 'creative exchange platform military involved pleasant standard learning'\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Load MS MARCO data + generate surrogates, wrong queries, overlap\n",
    "from lib.data import count_words\n",
    "from datasets import load_dataset\n",
    "\n",
    "STOP_WORDS = {\n",
    "    'a', 'an', 'the', 'is', 'are', 'was', 'were', 'be', 'been', 'being',\n",
    "    'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could',\n",
    "    'should', 'may', 'might', 'can', 'shall', 'to', 'of', 'in', 'for',\n",
    "    'on', 'with', 'at', 'by', 'from', 'as', 'into', 'through', 'during',\n",
    "    'before', 'after', 'above', 'below', 'between', 'and', 'but', 'or',\n",
    "    'not', 'no', 'if', 'then', 'than', 'so', 'up', 'out', 'about',\n",
    "    'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those',\n",
    "    'it', 'its', 'i', 'me', 'my', 'we', 'our', 'you', 'your', 'he',\n",
    "    'him', 'his', 'she', 'her', 'they', 'them', 'their', 'how', 'when',\n",
    "    'where', 'why', 'much', 'many', 'some', 'any', 'all', 'each',\n",
    "    'does', 'also', 'just', 'more', 'most', 'very', 'too', 'only',\n",
    "}\n",
    "\n",
    "def extract_keywords(text, top_k=10):\n",
    "    words = re.sub(r'[^\\w\\s]', '', text.lower()).split()\n",
    "    content = [w for w in words if w not in STOP_WORDS and len(w) > 2]\n",
    "    if not content:\n",
    "        return [\"information\"]\n",
    "    counts = Counter(content)\n",
    "    return [w for w, _ in counts.most_common(top_k)]\n",
    "\n",
    "WORD_POOL = [\n",
    "    \"computer\", \"mountain\", \"hospital\", \"children\", \"building\", \"national\",\n",
    "    \"business\", \"research\", \"students\", \"american\", \"possible\", \"economic\",\n",
    "    \"personal\", \"together\", \"products\", \"services\", \"actually\", \"remember\",\n",
    "    \"practice\", \"training\", \"industry\", \"complete\", \"critical\", \"function\",\n",
    "    \"language\", \"standard\", \"material\", \"original\", \"physical\", \"security\",\n",
    "    \"interest\", \"problems\", \"consider\", \"response\", \"pressure\", \"politics\",\n",
    "    \"movement\", \"evidence\", \"southern\", \"northern\", \"exchange\", \"decision\",\n",
    "    \"position\", \"increase\", \"describe\", \"military\", \"required\", \"approach\",\n",
    "    \"strategy\", \"customer\", \"resource\", \"employee\", \"audience\", \"location\",\n",
    "    \"property\", \"cultural\", \"activity\", \"strength\", \"analysis\", \"powerful\",\n",
    "    \"election\", \"argument\", \"campaign\", \"maintain\", \"question\", \"behavior\",\n",
    "    \"majority\", \"solution\", \"software\", \"consumer\", \"creative\", \"reaction\",\n",
    "    \"european\", \"delivery\", \"organize\", \"involved\", \"relative\", \"learning\",\n",
    "    \"positive\", \"numerous\", \"familiar\", \"engineer\", \"platform\", \"indicate\",\n",
    "    \"previous\", \"pleasure\", \"opposite\", \"magazine\", \"document\", \"religion\",\n",
    "    \"scenario\", \"workshop\", \"minority\", \"guidance\", \"estimate\", \"recently\",\n",
    "    \"surprise\", \"champion\", \"pleasant\", \"grateful\", \"moderate\", \"boundary\",\n",
    "]\n",
    "\n",
    "print(\"Loading MS MARCO v1.1 validation...\")\n",
    "ds = load_dataset(\"microsoft/ms_marco\", \"v1.1\", split=\"validation\")\n",
    "\n",
    "all_candidates = []\n",
    "for item in ds:\n",
    "    if len(all_candidates) >= 3 * N_SAMPLES:\n",
    "        break\n",
    "    passages = item.get('passages', {})\n",
    "    ptexts = passages.get('passage_text', [])\n",
    "    is_sel = passages.get('is_selected', [])\n",
    "    query = item.get('query', '')\n",
    "    answers = item.get('answers', [])\n",
    "    well_formed = item.get('wellFormedAnswers', [])\n",
    "    answer = None\n",
    "    if well_formed and len(well_formed) > 0 and well_formed[0] not in ('[]', ''):\n",
    "        answer = well_formed[0]\n",
    "    elif answers and len(answers) > 0 and answers[0] != 'No Answer Present.':\n",
    "        answer = answers[0]\n",
    "    if not answer:\n",
    "        continue\n",
    "    for pt, sel in zip(ptexts, is_sel):\n",
    "        wc = count_words(pt)\n",
    "        if sel == 1 and 30 <= wc <= 300:\n",
    "            all_candidates.append({\n",
    "                'passage': pt, 'query': query, 'answer': answer,\n",
    "                'word_count': wc,\n",
    "            })\n",
    "            break\n",
    "\n",
    "print(f\"Total candidates: {len(all_candidates)}\")\n",
    "np.random.seed(SEED)\n",
    "indices = np.random.permutation(len(all_candidates))\n",
    "samples = [all_candidates[i] for i in indices[:N_SAMPLES]]\n",
    "del ds, all_candidates\n",
    "gc.collect()\n",
    "\n",
    "# Generate surrogates, wrong queries, and overlap stats\n",
    "for i, s in enumerate(samples):\n",
    "    # Wrong query: deterministic rotation -- matched style/length, wrong content\n",
    "    s['wrong_query'] = samples[(i + 1) % len(samples)]['query']\n",
    "\n",
    "    # Random prefix (same as Exp 01)\n",
    "    rng = np.random.RandomState(SEED + i + 20000)\n",
    "    words = rng.choice(WORD_POOL, size=8, replace=False)\n",
    "    s['random_prefix'] = \" \".join(words)\n",
    "\n",
    "    # Query-document token overlap (Jaccard on content words)\n",
    "    q_words = set(re.sub(r'[^\\w\\s]', '', s['query'].lower()).split()) - STOP_WORDS\n",
    "    d_words = set(re.sub(r'[^\\w\\s]', '', s['passage'].lower()).split()) - STOP_WORDS\n",
    "    union = q_words | d_words\n",
    "    s['query_doc_overlap'] = len(q_words & d_words) / len(union) if len(union) > 0 else 0.0\n",
    "\n",
    "print(f\"Loaded {len(samples)} samples\")\n",
    "print(f\"Mean passage words: {np.mean([s['word_count'] for s in samples]):.0f}\")\n",
    "print(f\"Mean query words: {np.mean([count_words(s['query']) for s in samples]):.0f}\")\n",
    "print(f\"Mean answer words: {np.mean([count_words(s['answer']) for s in samples]):.0f}\")\n",
    "print(f\"Mean query-doc overlap (Jaccard): {np.mean([s['query_doc_overlap'] for s in samples]):.3f}\")\n",
    "print(f\"\\nExample wrong_query: '{samples[0]['wrong_query'][:80]}...'\")\n",
    "print(f\"Example random prefix: '{samples[0]['random_prefix']}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "205b2aef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-21T14:20:48.956738Z",
     "iopub.status.busy": "2026-02-21T14:20:48.956050Z",
     "iopub.status.idle": "2026-02-21T14:20:48.970092Z",
     "shell.execute_reply": "2026-02-21T14:20:48.969414Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scoring function defined (two-pass, 8 conditions per sample).\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: score_sample() -- two-pass scoring\n",
    "#\n",
    "# Phase A (offline): Forward [BOS, surr, doc] with causal mask, use_cache=True\n",
    "# Phase B (online):  Forward [query, answer] using cached KVs\n",
    "#\n",
    "# For _trunc conditions: Phase B masks surrogate positions (indirect channel only)\n",
    "# For _full conditions:  Phase B attends to ALL cached positions (direct + indirect)\n",
    "\n",
    "def score_sample(model, tokenizer, sample, device, conditions):\n",
    "    # Score one MS MARCO sample under all conditions.\n",
    "    # Returns dict mapping nll_{cname} -> mean NLL, plus prefix lengths.\n",
    "    passage = sample['passage']\n",
    "    query = sample['query']\n",
    "    answer = sample['answer']\n",
    "    wrong_query_text = sample['wrong_query']\n",
    "    random_prefix = sample['random_prefix']\n",
    "\n",
    "    bos_id = tokenizer.bos_token_id\n",
    "\n",
    "    # Tokenize segments (no special tokens -- we add BOS manually)\n",
    "    doc_ids = tokenizer(passage, add_special_tokens=False, truncation=True,\n",
    "                        max_length=1024).input_ids\n",
    "    query_ids = tokenizer(query, add_special_tokens=False, truncation=True,\n",
    "                          max_length=512).input_ids\n",
    "    answer_ids = tokenizer(answer, add_special_tokens=False, truncation=True,\n",
    "                           max_length=256).input_ids\n",
    "\n",
    "    if len(answer_ids) == 0:\n",
    "        return None\n",
    "\n",
    "    # Surrogate token IDs for each prefix type\n",
    "    oracle_ids = tokenizer(query, add_special_tokens=False, truncation=True,\n",
    "                           max_length=256).input_ids\n",
    "    wrong_query_ids = tokenizer(wrong_query_text, add_special_tokens=False,\n",
    "                                truncation=True, max_length=256).input_ids\n",
    "    random_ids = tokenizer(random_prefix, add_special_tokens=False, truncation=True,\n",
    "                           max_length=256).input_ids\n",
    "    answer_leak_ids = answer_ids[:5]\n",
    "\n",
    "    prefix_map = {\n",
    "        \"bare\": [],\n",
    "        \"oracle\": oracle_ids,\n",
    "        \"wrong_query\": wrong_query_ids,\n",
    "        \"random\": random_ids,\n",
    "        \"answer_leak\": answer_leak_ids,\n",
    "    }\n",
    "\n",
    "    n_q = len(query_ids)\n",
    "    n_a = len(answer_ids)\n",
    "    n_d = len(doc_ids)\n",
    "\n",
    "    targets = torch.tensor(answer_ids, dtype=torch.long, device=device)\n",
    "    result = {}\n",
    "\n",
    "    # Store prefix lengths for post-hoc length regression\n",
    "    result['n_oracle'] = len(oracle_ids)\n",
    "    result['n_wrong_query'] = len(wrong_query_ids)\n",
    "\n",
    "    for prefix_type, truncate in conditions:\n",
    "        cname = condition_name(prefix_type, truncate)\n",
    "\n",
    "        surr_ids = prefix_map[prefix_type]\n",
    "        n_s = len(surr_ids)\n",
    "        n_prefix = 1 + n_s + n_d\n",
    "\n",
    "        # === Phase A: Cache [BOS, surrogate, doc] with causal attention ===\n",
    "        prefix_tokens = [bos_id] + surr_ids + doc_ids\n",
    "        prefix_input = torch.tensor([prefix_tokens], dtype=torch.long, device=device)\n",
    "\n",
    "        phase_a_mask = make_phase_a_mask(n_s, n_d, mode=\"causal\")\n",
    "        phase_a_dict = make_mask_dict(phase_a_mask.to(device))\n",
    "        phase_a_pos = torch.arange(n_prefix, device=device).unsqueeze(0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            out_a = model(input_ids=prefix_input, attention_mask=phase_a_dict,\n",
    "                          position_ids=phase_a_pos, use_cache=True)\n",
    "        past_kv = out_a.past_key_values\n",
    "\n",
    "        # === Phase B: Evaluate [query, answer] with cached KVs ===\n",
    "        cont_tokens = query_ids + answer_ids\n",
    "        n_cont = len(cont_tokens)\n",
    "        cont_input = torch.tensor([cont_tokens], dtype=torch.long, device=device)\n",
    "\n",
    "        phase_b_mask = make_phase_b_mask(n_s, n_d, n_q, n_a, truncate=truncate)\n",
    "        phase_b_dict = make_mask_dict(phase_b_mask.to(device))\n",
    "        phase_b_pos = torch.arange(n_prefix, n_prefix + n_cont,\n",
    "                                    device=device).unsqueeze(0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            out_b = model(input_ids=cont_input, attention_mask=phase_b_dict,\n",
    "                          position_ids=phase_b_pos, past_key_values=past_kv)\n",
    "\n",
    "        # === Compute NLL on answer tokens ===\n",
    "        # Position n_q-1 in Phase B predicts first answer token\n",
    "        answer_logits = out_b.logits[0, n_q - 1 : n_q + n_a - 1, :]\n",
    "        log_probs = F.log_softmax(answer_logits, dim=-1)\n",
    "        token_nlls = -log_probs.gather(1, targets.unsqueeze(1)).squeeze(1)\n",
    "        mean_nll = token_nlls.mean().item()\n",
    "\n",
    "        result[f'nll_{cname}'] = mean_nll\n",
    "\n",
    "        del out_a, out_b, past_kv, prefix_input, cont_input\n",
    "        del phase_a_mask, phase_b_mask, phase_a_dict, phase_b_dict\n",
    "        del answer_logits, log_probs, token_nlls\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "print(\"Scoring function defined (two-pass, 8 conditions per sample).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00c98c22",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-21T14:20:48.973134Z",
     "iopub.status.busy": "2026-02-21T14:20:48.972570Z",
     "iopub.status.idle": "2026-02-21T14:36:30.260477Z",
     "shell.execute_reply": "2026-02-21T14:36:30.259353Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "MAIN SCORING LOOP\n",
      "======================================================================\n",
      "Starting fresh: 500 samples x 8 conditions\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f58fe64c78d24860a749c8fa0920d894",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Scoring:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done: 500 samples in 15.7 min\n",
      "\n",
      "Quick summary:\n",
      "  bare                      NLL=2.9572\n",
      "  oracle_trunc              NLL=1.9678\n",
      "  wrong_query_trunc         NLL=2.2230\n",
      "  random_trunc              NLL=2.2979\n",
      "  oracle_full               NLL=2.0478\n",
      "  wrong_query_full          NLL=2.3380\n",
      "  random_full               NLL=2.4132\n",
      "  answer_leak_trunc         NLL=2.3971\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Main scoring loop\n",
    "from lib.data import count_words as _cw\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"MAIN SCORING LOOP\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "CKPT_PATH = RESULTS_DIR / \"checkpoint.json\"\n",
    "\n",
    "# Resume from checkpoint\n",
    "all_results = []\n",
    "start_idx = 0\n",
    "if CKPT_PATH.exists():\n",
    "    ckpt = json.loads(CKPT_PATH.read_text())\n",
    "    if len(ckpt.get('results', [])) > 0:\n",
    "        saved_queries = [r['query'][:50] for r in ckpt['results']]\n",
    "        current_queries = [s['query'][:50] for s in samples[:len(saved_queries)]]\n",
    "        if saved_queries == current_queries:\n",
    "            all_results = ckpt['results']\n",
    "            start_idx = len(all_results)\n",
    "            print(f\"Resuming from checkpoint: {start_idx}/{N_SAMPLES}\")\n",
    "\n",
    "if start_idx == 0:\n",
    "    print(f\"Starting fresh: {N_SAMPLES} samples x {len(CONDITIONS)} conditions\")\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "for i in tqdm(range(start_idx, N_SAMPLES), initial=start_idx, total=N_SAMPLES,\n",
    "              desc=\"Scoring\"):\n",
    "    s = samples[i]\n",
    "    try:\n",
    "        result = score_sample(model, tokenizer, s, DEVICE, CONDITIONS)\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR at sample {i}: {e}\")\n",
    "        result = None\n",
    "\n",
    "    if result is None:\n",
    "        continue\n",
    "\n",
    "    # Store metadata for post-hoc analysis\n",
    "    result['query'] = s['query'][:50]\n",
    "    result['query_doc_overlap'] = s['query_doc_overlap']\n",
    "    result['answer_wc'] = _cw(s['answer'])\n",
    "    result['doc_wc'] = s['word_count']\n",
    "    all_results.append(result)\n",
    "\n",
    "    if (i + 1) % 25 == 0 or i == N_SAMPLES - 1:\n",
    "        ckpt = {\n",
    "            'model': MODEL_NAME,\n",
    "            'n_total': N_SAMPLES,\n",
    "            'n_conditions': len(CONDITIONS),\n",
    "            'condition_names': COND_NAMES,\n",
    "            'results': all_results,\n",
    "            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        }\n",
    "        CKPT_PATH.write_text(json.dumps(ckpt))\n",
    "\n",
    "    if (i + 1) % 100 == 0:\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "elapsed = time.time() - t0\n",
    "print(f\"\\nDone: {len(all_results)} samples in {elapsed/60:.1f} min\")\n",
    "print(f\"\\nQuick summary:\")\n",
    "for cn in COND_NAMES:\n",
    "    vals = [r[f'nll_{cn}'] for r in all_results]\n",
    "    print(f\"  {cn:<25} NLL={np.mean(vals):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f4c4cbe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-21T14:36:30.264573Z",
     "iopub.status.busy": "2026-02-21T14:36:30.264285Z",
     "iopub.status.idle": "2026-02-21T14:36:30.289963Z",
     "shell.execute_reply": "2026-02-21T14:36:30.289241Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "RESULTS: EFFECT SIZES AND SIGNIFICANCE\n",
      "======================================================================\n",
      "\n",
      "--- Mean NLL (500 samples) ---\n",
      "\n",
      "  Condition                   Mean NLL      Std\n",
      "  ---------------------------------------------\n",
      "  bare                          2.9572   3.8767\n",
      "  oracle_trunc                  1.9678   2.1826\n",
      "  wrong_query_trunc             2.2230   2.7716\n",
      "  random_trunc                  2.2979   2.8989\n",
      "  oracle_full                   2.0478   2.3372\n",
      "  wrong_query_full              2.3380   2.9512\n",
      "  random_full                   2.4132   3.0595\n",
      "  answer_leak_trunc             2.3971   3.0773\n",
      "\n",
      "--- 2x3 Factorial: Mean NLL by (content x truncation) ---\n",
      "\n",
      "  Content              Trunc       Full    Diff(T-F)        d\n",
      "  ----------------------------------------------------------\n",
      "  oracle              1.9678     2.0478      -0.0801   -0.121\n",
      "  wrong_query         2.2230     2.3380      -0.1150   -0.281\n",
      "  random              2.2979     2.4132      -0.1153   -0.292\n",
      "\n",
      "  bare:               2.9572\n",
      "  answer_leak_t:      2.3971\n",
      "\n",
      "--- Key Comparisons (positive d = first condition is better) ---\n",
      "\n",
      "  Comparison                                                     d    win%            p   sig\n",
      "  ------------------------------------------------------------------------------------------\n",
      "  A. d_structural: bare vs random_trunc                     +0.475   86.0%     7.13e-24   ***\n",
      "  B. d_semantic_trunc: oracle_trunc vs wrong_query_trunc    +0.255   55.6%     2.00e-08   ***\n",
      "  C. d_semantic_full: oracle_full vs wrong_query_full       +0.235   64.2%     2.29e-07   ***\n",
      "  D. interaction: semantic_full - semantic_trunc            +0.046   60.4%     3.04e-01    ns\n",
      "  E1. d_trunc_oracle: full vs trunc (oracle)                -0.121   51.6%     7.01e-03    **\n",
      "  E2. d_trunc_wrong_query: full vs trunc (wrong_query)      -0.281   36.2%     7.67e-10   ***\n",
      "  E3. d_trunc_random: full vs trunc (random)                -0.292   31.6%     1.54e-10   ***\n",
      "  F. d_answer_leak: answer_leak vs random_trunc             -0.112   41.2%     1.25e-02     *\n",
      "     d_oracle_vs_random_trunc: oracle vs random (trunc)     +0.266   58.4%     4.94e-09   ***\n",
      "     d_oracle_vs_random_full: oracle vs random (full)       +0.263   64.8%     7.94e-09   ***\n",
      "\n",
      "  Structural fraction (d_random / d_oracle):\n",
      "    Truncation: 105.1%  (d_oracle=+0.452, d_random=+0.475)\n",
      "    Full:       108.5%  (d_oracle=+0.422, d_random=+0.458)\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Effect sizes and significance tests (Analyses A-F)\n",
    "print(\"=\" * 70)\n",
    "print(\"RESULTS: EFFECT SIZES AND SIGNIFICANCE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Extract NLL arrays\n",
    "nll = {}\n",
    "for cn in COND_NAMES:\n",
    "    nll[cn] = np.array([r[f'nll_{cn}'] for r in all_results])\n",
    "\n",
    "N = len(all_results)\n",
    "\n",
    "# --- Mean NLL table ---\n",
    "print(f\"\\n--- Mean NLL ({N} samples) ---\\n\")\n",
    "print(f\"  {'Condition':<25} {'Mean NLL':>10} {'Std':>8}\")\n",
    "print(f\"  {'-'*45}\")\n",
    "for cn in COND_NAMES:\n",
    "    print(f\"  {cn:<25} {nll[cn].mean():>10.4f} {nll[cn].std():>8.4f}\")\n",
    "\n",
    "# --- 2x3 factorial table: content x truncation ---\n",
    "print(f\"\\n--- 2x3 Factorial: Mean NLL by (content x truncation) ---\\n\")\n",
    "print(f\"  {'Content':<15} {'Trunc':>10} {'Full':>10} {'Diff(T-F)':>12} {'d':>8}\")\n",
    "print(f\"  {'-'*58}\")\n",
    "for content in ['oracle', 'wrong_query', 'random']:\n",
    "    t_name = f\"{content}_trunc\"\n",
    "    f_name = f\"{content}_full\"\n",
    "    diff = nll[t_name] - nll[f_name]\n",
    "    d = cohens_d(diff)\n",
    "    print(f\"  {content:<15} {nll[t_name].mean():>10.4f} {nll[f_name].mean():>10.4f} \"\n",
    "          f\"{diff.mean():>+12.4f} {d:>+8.3f}\")\n",
    "\n",
    "print(f\"\\n  bare:           {nll['bare'].mean():>10.4f}\")\n",
    "print(f\"  answer_leak_t:  {nll['answer_leak_trunc'].mean():>10.4f}\")\n",
    "\n",
    "# --- Key comparisons ---\n",
    "print(f\"\\n--- Key Comparisons (positive d = first condition is better) ---\\n\")\n",
    "print(f\"  {'Comparison':<55} {'d':>8} {'win%':>7} {'p':>12} {'sig':>5}\")\n",
    "print(f\"  {'-'*90}\")\n",
    "\n",
    "comparisons = [\n",
    "    # A. Structural replication (expect d ~ +0.475)\n",
    "    (\"A. d_structural: bare vs random_trunc\",\n",
    "     nll['bare'] - nll['random_trunc']),\n",
    "\n",
    "    # B. Semantic under truncation (indirect channel only)\n",
    "    (\"B. d_semantic_trunc: oracle_trunc vs wrong_query_trunc\",\n",
    "     nll['wrong_query_trunc'] - nll['oracle_trunc']),\n",
    "\n",
    "    # C. Semantic under full access (direct + indirect channel)\n",
    "    (\"C. d_semantic_full: oracle_full vs wrong_query_full\",\n",
    "     nll['wrong_query_full'] - nll['oracle_full']),\n",
    "\n",
    "    # D. Truncation x content interaction (THE critical test)\n",
    "    (\"D. interaction: semantic_full - semantic_trunc\",\n",
    "     (nll['wrong_query_full'] - nll['oracle_full']) -\n",
    "     (nll['wrong_query_trunc'] - nll['oracle_trunc'])),\n",
    "\n",
    "    # E. Truncation main effect per content type\n",
    "    (\"E1. d_trunc_oracle: full vs trunc (oracle)\",\n",
    "     nll['oracle_trunc'] - nll['oracle_full']),\n",
    "\n",
    "    (\"E2. d_trunc_wrong_query: full vs trunc (wrong_query)\",\n",
    "     nll['wrong_query_trunc'] - nll['wrong_query_full']),\n",
    "\n",
    "    (\"E3. d_trunc_random: full vs trunc (random)\",\n",
    "     nll['random_trunc'] - nll['random_full']),\n",
    "\n",
    "    # F. Positive control\n",
    "    (\"F. d_answer_leak: answer_leak vs random_trunc\",\n",
    "     nll['random_trunc'] - nll['answer_leak_trunc']),\n",
    "\n",
    "    # Extra: oracle vs random under each truncation mode\n",
    "    (\"   d_oracle_vs_random_trunc: oracle vs random (trunc)\",\n",
    "     nll['random_trunc'] - nll['oracle_trunc']),\n",
    "\n",
    "    (\"   d_oracle_vs_random_full: oracle vs random (full)\",\n",
    "     nll['random_full'] - nll['oracle_full']),\n",
    "]\n",
    "\n",
    "for label, diff in comparisons:\n",
    "    d = cohens_d(diff)\n",
    "    _, p = stats.ttest_1samp(diff, 0)\n",
    "    sig = '***' if p < 0.001 else '**' if p < 0.01 else '*' if p < 0.05 else 'ns'\n",
    "    win = (diff > 0).mean() * 100\n",
    "    print(f\"  {label:<55} {d:>+8.3f} {win:>6.1f}% {p:>12.2e} {sig:>5}\")\n",
    "\n",
    "# --- Structural fraction under truncation ---\n",
    "d_oracle_trunc = cohens_d(nll['bare'] - nll['oracle_trunc'])\n",
    "d_random_trunc = cohens_d(nll['bare'] - nll['random_trunc'])\n",
    "struct_frac_trunc = d_random_trunc / d_oracle_trunc if d_oracle_trunc != 0 else float('nan')\n",
    "\n",
    "d_oracle_full = cohens_d(nll['bare'] - nll['oracle_full'])\n",
    "d_random_full = cohens_d(nll['bare'] - nll['random_full'])\n",
    "struct_frac_full = d_random_full / d_oracle_full if d_oracle_full != 0 else float('nan')\n",
    "\n",
    "print(f\"\\n  Structural fraction (d_random / d_oracle):\")\n",
    "print(f\"    Truncation: {struct_frac_trunc:.1%}  (d_oracle={d_oracle_trunc:+.3f}, d_random={d_random_trunc:+.3f})\")\n",
    "print(f\"    Full:       {struct_frac_full:.1%}  (d_oracle={d_oracle_full:+.3f}, d_random={d_random_full:+.3f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b76deef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-21T14:36:30.293545Z",
     "iopub.status.busy": "2026-02-21T14:36:30.293296Z",
     "iopub.status.idle": "2026-02-21T14:36:30.321866Z",
     "shell.execute_reply": "2026-02-21T14:36:30.321153Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "POST-HOC: HETEROGENEITY AND LENGTH CONTROL\n",
      "======================================================================\n",
      "\n",
      "--- G. Per-Sample Heterogeneity (N=500) ---\n",
      "\n",
      "  Effect                    x                         r            p   sig\n",
      "  ------------------------------------------------------------------------\n",
      "  semantic_trunc            query_doc_overlap    +0.052     2.43e-01    ns\n",
      "  semantic_trunc            answer_wc            -0.219     7.26e-07   ***\n",
      "  semantic_trunc            doc_wc               +0.061     1.73e-01    ns\n",
      "  semantic_full             query_doc_overlap    -0.060     1.77e-01    ns\n",
      "  semantic_full             answer_wc            -0.162     2.80e-04   ***\n",
      "  semantic_full             doc_wc               +0.047     2.90e-01    ns\n",
      "  interaction               query_doc_overlap    -0.167     1.69e-04   ***\n",
      "  interaction               answer_wc            +0.026     5.68e-01    ns\n",
      "  interaction               doc_wc               -0.003     9.41e-01    ns\n",
      "\n",
      "--- G2. Subpopulation Analysis ---\n",
      "\n",
      "  Query-doc overlap split (median=0.065):\n",
      "  Group                    N  d_sem_trunc   d_sem_full   d_interact\n",
      "  -----------------------------------------------------------------\n",
      "  High overlap           255       +0.287       +0.203       -0.065\n",
      "  Low overlap            245       +0.231       +0.264       +0.159\n",
      "\n",
      "  Answer length split (<=5w vs >5w):\n",
      "  Group                    N  d_sem_trunc   d_sem_full   d_interact\n",
      "  -----------------------------------------------------------------\n",
      "  Short (<=5w)           210       +0.428       +0.331       -0.008\n",
      "  Long (>5w)             290       -0.051       +0.293       +0.374\n",
      "\n",
      "--- H. Length-Controlled Regression ---\n",
      "\n",
      "  Oracle and wrong_query may differ in token length -> confound.\n",
      "  Regress (NLL_wq - NLL_oracle) on (n_wq - n_oracle) per sample.\n",
      "\n",
      "  Prefix length stats:\n",
      "    n_oracle:      mean=6.5, std=2.3\n",
      "    n_wrong_query: mean=6.5, std=2.3\n",
      "    delta (wq-orc): mean=0.0, std=3.2\n",
      "\n",
      "  Mode        intercept      slope      R^2        p_int      p_slope\n",
      "  -----------------------------------------------------------------\n",
      "  trunc         +0.2552   -0.00110   0.0000     2.06e-08     9.37e-01\n",
      "  full          +0.2902   +0.00527   0.0002     2.34e-07     7.60e-01\n",
      "\n",
      "  Intercept = length-controlled semantic effect.\n",
      "  If intercept is significant: genuine semantic signal beyond length.\n",
      "  If slope is significant: length is a confound.\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Post-hoc analysis (Analyses G-H)\n",
    "print(\"=\" * 70)\n",
    "print(\"POST-HOC: HETEROGENEITY AND LENGTH CONTROL\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# --- G. Per-sample heterogeneity ---\n",
    "# Correlate per-sample semantic effects with sample characteristics\n",
    "\n",
    "semantic_trunc = nll['wrong_query_trunc'] - nll['oracle_trunc']\n",
    "semantic_full = nll['wrong_query_full'] - nll['oracle_full']\n",
    "interaction = semantic_full - semantic_trunc\n",
    "\n",
    "overlap = np.array([r['query_doc_overlap'] for r in all_results])\n",
    "answer_wc = np.array([r['answer_wc'] for r in all_results])\n",
    "doc_wc = np.array([r['doc_wc'] for r in all_results])\n",
    "\n",
    "print(f\"\\n--- G. Per-Sample Heterogeneity (N={N}) ---\\n\")\n",
    "print(f\"  {'Effect':<25} {'x':<18} {'r':>8} {'p':>12} {'sig':>5}\")\n",
    "print(f\"  {'-'*72}\")\n",
    "\n",
    "effects = [\n",
    "    (\"semantic_trunc\", semantic_trunc),\n",
    "    (\"semantic_full\", semantic_full),\n",
    "    (\"interaction\", interaction),\n",
    "]\n",
    "covariates = [\n",
    "    (\"query_doc_overlap\", overlap),\n",
    "    (\"answer_wc\", answer_wc),\n",
    "    (\"doc_wc\", doc_wc),\n",
    "]\n",
    "\n",
    "for eff_name, eff_vals in effects:\n",
    "    for cov_name, cov_vals in covariates:\n",
    "        r, p = stats.pearsonr(eff_vals, cov_vals)\n",
    "        sig = '***' if p < 0.001 else '**' if p < 0.01 else '*' if p < 0.05 else 'ns'\n",
    "        print(f\"  {eff_name:<25} {cov_name:<18} {r:>+8.3f} {p:>12.2e} {sig:>5}\")\n",
    "\n",
    "# --- Subpopulation analysis ---\n",
    "print(f\"\\n--- G2. Subpopulation Analysis ---\\n\")\n",
    "\n",
    "# Split by query-doc overlap (median)\n",
    "med_overlap = np.median(overlap)\n",
    "hi_overlap = overlap >= med_overlap\n",
    "lo_overlap = ~hi_overlap\n",
    "\n",
    "print(f\"  Query-doc overlap split (median={med_overlap:.3f}):\")\n",
    "print(f\"  {'Group':<20} {'N':>5} {'d_sem_trunc':>12} {'d_sem_full':>12} {'d_interact':>12}\")\n",
    "print(f\"  {'-'*65}\")\n",
    "for label, mask in [(\"High overlap\", hi_overlap), (\"Low overlap\", lo_overlap)]:\n",
    "    d_st = cohens_d(semantic_trunc[mask])\n",
    "    d_sf = cohens_d(semantic_full[mask])\n",
    "    d_int = cohens_d(interaction[mask])\n",
    "    print(f\"  {label:<20} {mask.sum():>5} {d_st:>+12.3f} {d_sf:>+12.3f} {d_int:>+12.3f}\")\n",
    "\n",
    "# Split by answer length (<=5 vs >5 words, matching Exp 06)\n",
    "short_ans = answer_wc <= 5\n",
    "long_ans = ~short_ans\n",
    "\n",
    "print(f\"\\n  Answer length split (<=5w vs >5w):\")\n",
    "print(f\"  {'Group':<20} {'N':>5} {'d_sem_trunc':>12} {'d_sem_full':>12} {'d_interact':>12}\")\n",
    "print(f\"  {'-'*65}\")\n",
    "for label, mask in [(\"Short (<=5w)\", short_ans), (\"Long (>5w)\", long_ans)]:\n",
    "    d_st = cohens_d(semantic_trunc[mask])\n",
    "    d_sf = cohens_d(semantic_full[mask])\n",
    "    d_int = cohens_d(interaction[mask])\n",
    "    print(f\"  {label:<20} {mask.sum():>5} {d_st:>+12.3f} {d_sf:>+12.3f} {d_int:>+12.3f}\")\n",
    "\n",
    "# --- H. Length-controlled regression ---\n",
    "print(f\"\\n--- H. Length-Controlled Regression ---\\n\")\n",
    "print(f\"  Oracle and wrong_query may differ in token length -> confound.\")\n",
    "print(f\"  Regress (NLL_wq - NLL_oracle) on (n_wq - n_oracle) per sample.\\n\")\n",
    "\n",
    "n_oracle_arr = np.array([r['n_oracle'] for r in all_results])\n",
    "n_wq_arr = np.array([r['n_wrong_query'] for r in all_results])\n",
    "delta_len = n_wq_arr - n_oracle_arr\n",
    "\n",
    "print(f\"  Prefix length stats:\")\n",
    "print(f\"    n_oracle:      mean={n_oracle_arr.mean():.1f}, std={n_oracle_arr.std():.1f}\")\n",
    "print(f\"    n_wrong_query: mean={n_wq_arr.mean():.1f}, std={n_wq_arr.std():.1f}\")\n",
    "print(f\"    delta (wq-orc): mean={delta_len.mean():.1f}, std={delta_len.std():.1f}\")\n",
    "\n",
    "print(f\"\\n  {'Mode':<10} {'intercept':>10} {'slope':>10} {'R^2':>8} {'p_int':>12} {'p_slope':>12}\")\n",
    "print(f\"  {'-'*65}\")\n",
    "\n",
    "for mode, delta_nll in [(\"trunc\", semantic_trunc), (\"full\", semantic_full)]:\n",
    "    slope, intercept, r_val, p_val, se = stats.linregress(delta_len, delta_nll)\n",
    "    # p-value for intercept (length-controlled semantic effect)\n",
    "    n = len(delta_nll)\n",
    "    x_bar = delta_len.mean()\n",
    "    ss_x = np.sum((delta_len - x_bar)**2)\n",
    "    residuals = delta_nll - (intercept + slope * delta_len)\n",
    "    mse = np.sum(residuals**2) / (n - 2)\n",
    "    se_intercept = np.sqrt(mse * (1/n + x_bar**2 / ss_x))\n",
    "    t_intercept = intercept / se_intercept if se_intercept > 0 else 0\n",
    "    p_intercept = 2 * stats.t.sf(abs(t_intercept), df=n-2)\n",
    "\n",
    "    print(f\"  {mode:<10} {intercept:>+10.4f} {slope:>+10.5f} {r_val**2:>8.4f} \"\n",
    "          f\"{p_intercept:>12.2e} {p_val:>12.2e}\")\n",
    "\n",
    "print(f\"\\n  Intercept = length-controlled semantic effect.\")\n",
    "print(f\"  If intercept is significant: genuine semantic signal beyond length.\")\n",
    "print(f\"  If slope is significant: length is a confound.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77241bea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-21T14:36:30.324979Z",
     "iopub.status.busy": "2026-02-21T14:36:30.324582Z",
     "iopub.status.idle": "2026-02-21T14:36:30.349606Z",
     "shell.execute_reply": "2026-02-21T14:36:30.348973Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "SUMMARY -- Prefix LM Exp 02\n",
      "======================================================================\n",
      "\n",
      "  d_structural (bare vs random_trunc):        +0.475\n",
      "  d_semantic_trunc (oracle vs wq, trunc):     +0.255 (p=2.00e-08)\n",
      "  d_semantic_full (oracle vs wq, full):        +0.235 (p=2.29e-07)\n",
      "  d_interaction (full amplifies semantic?):     +0.046 (p=3.04e-01)\n",
      "  d_answer_leak (positive control):            -0.112 (p=1.25e-02)\n",
      "\n",
      "  VERDICT:\n",
      "  WARNING: Positive control FAILED (d_answer_leak=-0.112, p=1.25e-02).\n",
      "  The indirect channel may not transmit content at all for this model.\n",
      "  Semantic signal under full access (d=+0.235), but interaction ns.\n",
      "  Some semantic signal exists, but truncation doesn't modulate it.\n",
      "  Structural fraction (trunc): 105% -- structural dominates.\n",
      "  Structural fraction (full): 108% -- even with direct access.\n",
      "\n",
      "Results saved to ../../../results/prefix_lm_exp02/results.json\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Save final results and verdict\n",
    "print(\"=\" * 70)\n",
    "print(\"SUMMARY -- Prefix LM Exp 02\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "summary = {\n",
    "    'n_samples': N,\n",
    "    'model': MODEL_NAME,\n",
    "}\n",
    "\n",
    "# NLL means\n",
    "for cn in COND_NAMES:\n",
    "    summary[f'nll_{cn}'] = float(nll[cn].mean())\n",
    "\n",
    "# Key effect sizes\n",
    "key_effects = {\n",
    "    'd_structural': nll['bare'] - nll['random_trunc'],\n",
    "    'd_semantic_trunc': nll['wrong_query_trunc'] - nll['oracle_trunc'],\n",
    "    'd_semantic_full': nll['wrong_query_full'] - nll['oracle_full'],\n",
    "    'd_interaction': (nll['wrong_query_full'] - nll['oracle_full']) -\n",
    "                     (nll['wrong_query_trunc'] - nll['oracle_trunc']),\n",
    "    'd_trunc_oracle': nll['oracle_trunc'] - nll['oracle_full'],\n",
    "    'd_trunc_wrong_query': nll['wrong_query_trunc'] - nll['wrong_query_full'],\n",
    "    'd_trunc_random': nll['random_trunc'] - nll['random_full'],\n",
    "    'd_answer_leak': nll['random_trunc'] - nll['answer_leak_trunc'],\n",
    "}\n",
    "\n",
    "for name, diff in key_effects.items():\n",
    "    d = cohens_d(diff)\n",
    "    _, p = stats.ttest_1samp(diff, 0)\n",
    "    summary[name] = float(d)\n",
    "    summary[f'{name}_p'] = float(p)\n",
    "\n",
    "summary['structural_fraction_trunc'] = float(struct_frac_trunc)\n",
    "summary['structural_fraction_full'] = float(struct_frac_full)\n",
    "\n",
    "# --- Verdict ---\n",
    "d_sem_t = cohens_d(nll['wrong_query_trunc'] - nll['oracle_trunc'])\n",
    "_, p_sem_t = stats.ttest_1samp(nll['wrong_query_trunc'] - nll['oracle_trunc'], 0)\n",
    "d_sem_f = cohens_d(nll['wrong_query_full'] - nll['oracle_full'])\n",
    "_, p_sem_f = stats.ttest_1samp(nll['wrong_query_full'] - nll['oracle_full'], 0)\n",
    "d_inter = cohens_d((nll['wrong_query_full'] - nll['oracle_full']) -\n",
    "                   (nll['wrong_query_trunc'] - nll['oracle_trunc']))\n",
    "_, p_inter = stats.ttest_1samp(\n",
    "    (nll['wrong_query_full'] - nll['oracle_full']) -\n",
    "    (nll['wrong_query_trunc'] - nll['oracle_trunc']), 0)\n",
    "d_leak = cohens_d(nll['random_trunc'] - nll['answer_leak_trunc'])\n",
    "_, p_leak = stats.ttest_1samp(nll['random_trunc'] - nll['answer_leak_trunc'], 0)\n",
    "d_struct = cohens_d(nll['bare'] - nll['random_trunc'])\n",
    "\n",
    "print(f\"\\n  d_structural (bare vs random_trunc):        {d_struct:+.3f}\")\n",
    "print(f\"  d_semantic_trunc (oracle vs wq, trunc):     {d_sem_t:+.3f} (p={p_sem_t:.2e})\")\n",
    "print(f\"  d_semantic_full (oracle vs wq, full):        {d_sem_f:+.3f} (p={p_sem_f:.2e})\")\n",
    "print(f\"  d_interaction (full amplifies semantic?):     {d_inter:+.3f} (p={p_inter:.2e})\")\n",
    "print(f\"  d_answer_leak (positive control):            {d_leak:+.3f} (p={p_leak:.2e})\")\n",
    "\n",
    "print(f\"\\n  VERDICT:\")\n",
    "if p_leak >= 0.05 or d_leak <= 0:\n",
    "    print(f\"  WARNING: Positive control FAILED (d_answer_leak={d_leak:+.3f}, p={p_leak:.2e}).\")\n",
    "    print(f\"  The indirect channel may not transmit content at all for this model.\")\n",
    "else:\n",
    "    print(f\"  Positive control PASSED (d_answer_leak={d_leak:+.3f}, ***).\")\n",
    "\n",
    "if p_inter < 0.05 and d_inter > 0:\n",
    "    print(f\"  INTERACTION SIGNIFICANT: Full access amplifies semantic benefit (d={d_inter:+.3f}).\")\n",
    "    print(f\"  Direct channel carries semantic signal -- content-specific caching may help.\")\n",
    "elif p_sem_f < 0.05 and d_sem_f > 0:\n",
    "    print(f\"  Semantic signal under full access (d={d_sem_f:+.3f}), but interaction ns.\")\n",
    "    print(f\"  Some semantic signal exists, but truncation doesn't modulate it.\")\n",
    "elif p_sem_t < 0.05 and d_sem_t > 0:\n",
    "    print(f\"  Semantic signal even under truncation (d={d_sem_t:+.3f}).\")\n",
    "    print(f\"  Content matters through indirect channel alone.\")\n",
    "else:\n",
    "    print(f\"  NO semantic signal detected under either truncation mode.\")\n",
    "    print(f\"  Effect is purely structural -- content doesn't matter for caching.\")\n",
    "\n",
    "# Structural dominance summary\n",
    "if struct_frac_trunc > 0.8:\n",
    "    print(f\"  Structural fraction (trunc): {struct_frac_trunc:.0%} -- structural dominates.\")\n",
    "if struct_frac_full > 0.8:\n",
    "    print(f\"  Structural fraction (full): {struct_frac_full:.0%} -- even with direct access.\")\n",
    "\n",
    "# Save\n",
    "final_results = {\n",
    "    'experiment': 'prefix_lm_exp02',\n",
    "    'dataset': 'ms_marco_v1.1',\n",
    "    'model': MODEL_NAME,\n",
    "    'n_samples': N,\n",
    "    'seed': SEED,\n",
    "    'conditions': COND_NAMES,\n",
    "    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'summary': summary,\n",
    "    'exp01_references': {\n",
    "        'd_causal_oracle': 0.452,\n",
    "        'd_causal_random': 0.475,\n",
    "        'structural_fraction': 1.40,\n",
    "    },\n",
    "}\n",
    "\n",
    "with open(RESULTS_DIR / 'results.json', 'w') as f:\n",
    "    json.dump(final_results, f, indent=2)\n",
    "print(f\"\\nResults saved to {RESULTS_DIR / 'results.json'}\")\n",
    "print(\"Done!\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "008b9b7379a94aab9229e67cdfa4635c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "11cf8bbc949643c3ad4e7884a30936ec": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "128e188c8f7b493184580ca0baecb721": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "13345aad80b54db49441b5377d12eb37": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3d95d790200a499a8fd05028a4b0eb16": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "468bc18f2c9c48649e1e505a60a12a13": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_d3c522ad732640e889df5b79cd2d88aa",
       "placeholder": "",
       "style": "IPY_MODEL_d8121a5922e845219faf7907c22d042c",
       "tabbable": null,
       "tooltip": null,
       "value": "500/500[15:41&lt;00:00,1.94s/it]"
      }
     },
     "4aed1d2137a64b338d089b8b8a9406ea": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_11cf8bbc949643c3ad4e7884a30936ec",
       "max": 1065.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_807105094de14044b43738db3df3b7ab",
       "tabbable": null,
       "tooltip": null,
       "value": 1065.0
      }
     },
     "4ede1c2a944f4018b7a1ef93b112265f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_9024c5b951de440e8507a173470e7c67",
        "IPY_MODEL_4aed1d2137a64b338d089b8b8a9406ea",
        "IPY_MODEL_bb9c85710ebf49168157e96fd049f092"
       ],
       "layout": "IPY_MODEL_a633387b9c0344d1bf537566c03db94f",
       "tabbable": null,
       "tooltip": null
      }
     },
     "58803c5cd1ef4c84ac1a360ea7a68925": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "7679f8c2e9cc4621b15505c748efb002": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "7acac60c52ed42baa3ca6229aeb14efa": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_7bc8820c35a549de9003d4d52a195776",
       "max": 500.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_3d95d790200a499a8fd05028a4b0eb16",
       "tabbable": null,
       "tooltip": null,
       "value": 500.0
      }
     },
     "7bc8820c35a549de9003d4d52a195776": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "807105094de14044b43738db3df3b7ab": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "8ef3f396a9104fd3b658150e0a05f08e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_13345aad80b54db49441b5377d12eb37",
       "placeholder": "",
       "style": "IPY_MODEL_ecb84269692241dd8abdcb4a8e79ece5",
       "tabbable": null,
       "tooltip": null,
       "value": "Scoring:100%"
      }
     },
     "9024c5b951de440e8507a173470e7c67": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_008b9b7379a94aab9229e67cdfa4635c",
       "placeholder": "",
       "style": "IPY_MODEL_a649d0876fad48b1858e7442c058cd23",
       "tabbable": null,
       "tooltip": null,
       "value": "Loadingweights:100%"
      }
     },
     "a633387b9c0344d1bf537566c03db94f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a649d0876fad48b1858e7442c058cd23": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "bb9c85710ebf49168157e96fd049f092": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_58803c5cd1ef4c84ac1a360ea7a68925",
       "placeholder": "",
       "style": "IPY_MODEL_7679f8c2e9cc4621b15505c748efb002",
       "tabbable": null,
       "tooltip": null,
       "value": "1065/1065[00:07&lt;00:00,664.54it/s,Materializingparam=model.vision_tower.vision_model.post_layernorm.weight]"
      }
     },
     "d3c522ad732640e889df5b79cd2d88aa": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d8121a5922e845219faf7907c22d042c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "ecb84269692241dd8abdcb4a8e79ece5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "f58fe64c78d24860a749c8fa0920d894": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_8ef3f396a9104fd3b658150e0a05f08e",
        "IPY_MODEL_7acac60c52ed42baa3ca6229aeb14efa",
        "IPY_MODEL_468bc18f2c9c48649e1e505a60a12a13"
       ],
       "layout": "IPY_MODEL_128e188c8f7b493184580ca0baecb721",
       "tabbable": null,
       "tooltip": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
