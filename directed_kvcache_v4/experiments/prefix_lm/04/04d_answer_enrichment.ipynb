{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0af3cc0",
   "metadata": {},
   "source": [
    "# Prefix LM Exp 04d: Answer Enrichment (No Copy Shortcut)\n",
    "\n",
    "## Motivation\n",
    "\n",
    "Exp 04c showed answer priming beats random (d=+0.261, ***) in single-pass layout\n",
    "`[BOS, prime, doc, query, answer]`. But the answer tokens can **directly attend** to\n",
    "the identical answer tokens in the prime -- a copy shortcut, not genuine enrichment.\n",
    "\n",
    "Model-generated answers and wrong answers performed WORSE than random (d=-0.265 and\n",
    "d=-0.222 respectively). But was this direct semantic interference, or did they actually\n",
    "poison document representations?\n",
    "\n",
    "## Design: Two-Pass Truncation\n",
    "\n",
    "Removes the copy shortcut. Forces ALL benefit to flow through document enrichment:\n",
    "\n",
    "- **Phase A (offline)**: Process `[BOS, prime, doc]` with causal attention, `use_cache=True`\n",
    "- **Phase B (online)**: Process `[query, answer]` with cached KVs, **prime positions masked**\n",
    "\n",
    "The answer can only benefit if the prime made doc representations better during Phase A.\n",
    "No direct attention from answer to prime.\n",
    "\n",
    "## Conditions (7)\n",
    "\n",
    "| # | Condition | Prime in Phase A | What it tests |\n",
    "|---|-----------|-----------------|---------------|\n",
    "| 1 | `bare` | (none) | Baseline -- no enrichment |\n",
    "| 2 | `random` | 8 random words | Structural control |\n",
    "| 3 | `oracle` | real query | Standard oracle |\n",
    "| 4 | `answer_prime` | actual answer | Does answer enrich doc? |\n",
    "| 5 | `wrong_answer` | answer from (i+1)%N | Style-matched, wrong content |\n",
    "| 6 | `answer_5tok` | first 5 answer tokens | Partial content |\n",
    "| 7 | `model_answer` | LLM-generated answer | LLM surrogate |\n",
    "\n",
    "## Key Predictions\n",
    "\n",
    "- If **answer_prime > random**: genuine enrichment -- answer content helps doc reps\n",
    "- If **answer_prime ~ random**: Exp 04c content effect was pure copy artifact\n",
    "- If **model_answer ~ random**: Exp 04c interference was direct-attention, removed by truncation\n",
    "- If **model_answer < random**: misleading content poisons doc reps during Phase A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f527bd5e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-22T14:03:42.373861Z",
     "iopub.status.busy": "2026-02-22T14:03:42.373307Z",
     "iopub.status.idle": "2026-02-22T14:03:46.882935Z",
     "shell.execute_reply": "2026-02-22T14:03:46.881952Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prefix LM Exp 04d: Answer Enrichment (No Copy Shortcut)\n",
      "N: 500, Conditions: 7\n",
      "DEVICE: cuda\n",
      "GPU: NVIDIA A100-SXM4-40GB\n",
      "GPU memory: 42.3 GB\n",
      "\n",
      "Conditions: ['bare', 'random', 'oracle', 'answer_prime', 'wrong_answer', 'answer_5tok', 'model_answer']\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Setup\n",
    "import os\n",
    "os.umask(0o000)\n",
    "\n",
    "import sys, json, time, gc, re\n",
    "import random as pyrandom\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from scipy import stats\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "sys.path.insert(0, \"../../..\")\n",
    "from lib.analysis import cohens_d\n",
    "\n",
    "SEED = 42\n",
    "N_SAMPLES = 500\n",
    "\n",
    "MODEL_NAME = \"google/gemma-3-12b-it\"\n",
    "\n",
    "RESULTS_DIR = Path(\"../../../results/prefix_lm_exp04d\")\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "pyrandom.seed(SEED)\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv())\n",
    "HF_TOKEN = os.environ.get(\"HF_TOKEN\")\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "CONDITIONS = [\n",
    "    \"bare\",            # no prime -- baseline\n",
    "    \"random\",          # 8 random words -- structural control\n",
    "    \"oracle\",          # real query -- standard oracle\n",
    "    \"answer_prime\",    # actual answer text\n",
    "    \"wrong_answer\",    # answer from sample (i+1)%N\n",
    "    \"answer_5tok\",     # first 5 answer tokens\n",
    "    \"model_answer\",    # LLM-generated answer (from Exp 04c cache)\n",
    "]\n",
    "\n",
    "print(f\"Prefix LM Exp 04d: Answer Enrichment (No Copy Shortcut)\")\n",
    "print(f\"N: {N_SAMPLES}, Conditions: {len(CONDITIONS)}\")\n",
    "print(f\"DEVICE: {DEVICE}\")\n",
    "print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "print(f\"\\nConditions: {CONDITIONS}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3552e71a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-22T14:03:46.886912Z",
     "iopub.status.busy": "2026-02-22T14:03:46.886098Z",
     "iopub.status.idle": "2026-02-22T14:04:02.312904Z",
     "shell.execute_reply": "2026-02-22T14:04:02.312008Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformers version: 5.1.0\n",
      "Loading google/gemma-3-12b-it...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c142c6103b61448187f0fb93784ec21b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/1065 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: 12.2B params, 24.4 GB GPU, 13s\n",
      "BOS token id: 2\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Load model + tokenizer\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import transformers\n",
    "\n",
    "print(f\"transformers version: {transformers.__version__}\")\n",
    "\n",
    "print(f\"Loading {MODEL_NAME}...\")\n",
    "t0 = time.time()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, token=HF_TOKEN)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    attn_implementation=\"eager\",\n",
    "    token=HF_TOKEN,\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "n_params = sum(p.numel() for p in model.parameters()) / 1e9\n",
    "gpu_mem = torch.cuda.memory_allocated() / 1e9\n",
    "print(f\"Loaded: {n_params:.1f}B params, {gpu_mem:.1f} GB GPU, {time.time()-t0:.0f}s\")\n",
    "print(f\"BOS token id: {tokenizer.bos_token_id}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "407752ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-22T14:04:02.316643Z",
     "iopub.status.busy": "2026-02-22T14:04:02.316207Z",
     "iopub.status.idle": "2026-02-22T14:04:03.220318Z",
     "shell.execute_reply": "2026-02-22T14:04:03.219512Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mask sanity check: custom causal mask vs default forward...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Max logit diff: 0.000000\n",
      "  PASS: Dict-based mask API verified.\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Phase A/B attention masks + sanity check\n",
    "#\n",
    "# Two-pass design (same as Exp 01/02):\n",
    "#   Phase A: Process [BOS, surrogate, doc] -> cache KV states (causal attention)\n",
    "#   Phase B: Process [query, answer] using cached KVs -> NLL\n",
    "#            Surrogate positions MASKED from continuation (truncate=True always)\n",
    "\n",
    "def make_phase_a_mask(n_s, n_d, dtype=torch.bfloat16):\n",
    "    # Phase A mask for [BOS, surrogate, doc] under causal attention.\n",
    "    # Returns (1, 1, n_prefix, n_prefix).\n",
    "    n_prefix = 1 + n_s + n_d\n",
    "    min_val = torch.finfo(dtype).min\n",
    "    mask = torch.triu(torch.full((n_prefix, n_prefix), min_val, dtype=dtype),\n",
    "                      diagonal=1)\n",
    "    return mask.unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "\n",
    "def make_phase_b_mask(n_s, n_d, n_q, n_a, dtype=torch.bfloat16):\n",
    "    # Phase B mask for [query, answer] attending to cached prefix.\n",
    "    # Surrogate positions (1..n_s) are ALWAYS masked (truncation).\n",
    "    # Returns (1, 1, n_cont, n_prefix + n_cont).\n",
    "    n_prefix = 1 + n_s + n_d\n",
    "    n_cont = n_q + n_a\n",
    "    min_val = torch.finfo(dtype).min\n",
    "\n",
    "    mask = torch.full((n_cont, n_prefix + n_cont), min_val, dtype=dtype)\n",
    "\n",
    "    # Attend to all cached prefix positions\n",
    "    mask[:, :n_prefix] = 0.0\n",
    "\n",
    "    # Truncation: mask surrogate positions (1..n_s) from continuation\n",
    "    if n_s > 0:\n",
    "        mask[:, 1:1 + n_s] = min_val\n",
    "\n",
    "    # Causal self-attention among continuation tokens\n",
    "    mask[:, n_prefix:] = torch.triu(\n",
    "        torch.full((n_cont, n_cont), min_val, dtype=dtype), diagonal=1\n",
    "    )\n",
    "\n",
    "    return mask.unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "\n",
    "def make_mask_dict(mask_4d):\n",
    "    # Wrap 4D mask in Gemma 3's dict format.\n",
    "    return {\"full_attention\": mask_4d, \"sliding_attention\": mask_4d}\n",
    "\n",
    "\n",
    "# --- Sanity check: custom causal mask matches default forward ---\n",
    "print(\"Mask sanity check: custom causal mask vs default forward...\")\n",
    "test_text = \"The quick brown fox jumps over the lazy dog.\"\n",
    "test_ids = tokenizer(test_text, return_tensors=\"pt\",\n",
    "                     add_special_tokens=True).input_ids.to(DEVICE)\n",
    "Lt = test_ids.shape[1]\n",
    "\n",
    "with torch.no_grad():\n",
    "    out_default = model(input_ids=test_ids)\n",
    "\n",
    "# Build custom causal mask (treat entire sequence as bare prefix)\n",
    "causal_mask = make_phase_a_mask(0, Lt - 1)\n",
    "causal_dict = make_mask_dict(causal_mask.to(DEVICE))\n",
    "causal_pos = torch.arange(Lt, device=DEVICE).unsqueeze(0)\n",
    "\n",
    "with torch.no_grad():\n",
    "    out_custom = model(input_ids=test_ids, attention_mask=causal_dict,\n",
    "                       position_ids=causal_pos)\n",
    "\n",
    "max_diff = (out_default.logits - out_custom.logits).abs().max().item()\n",
    "print(f\"  Max logit diff: {max_diff:.6f}\")\n",
    "assert max_diff < 0.1, (\n",
    "    f\"FAIL: Custom causal mask doesn't match default (max_diff={max_diff:.4f}).\")\n",
    "print(f\"  PASS: Dict-based mask API verified.\")\n",
    "\n",
    "del out_default, out_custom\n",
    "gc.collect(); torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc590b01",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-22T14:04:03.223898Z",
     "iopub.status.busy": "2026-02-22T14:04:03.223360Z",
     "iopub.status.idle": "2026-02-22T14:04:04.682198Z",
     "shell.execute_reply": "2026-02-22T14:04:04.681414Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MS MARCO v1.1 validation...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total candidates: 1500\n",
      "Loaded 500 cached model answers from Exp 04c.\n",
      "\n",
      "Loaded 500 samples\n",
      "Mean passage words: 74\n",
      "Mean query words: 6\n",
      "Mean answer words: 14\n",
      "Mean model answer overlap: 0.092\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Load MS MARCO data + cached model answers from Exp 04c\n",
    "from lib.data import count_words\n",
    "from datasets import load_dataset\n",
    "\n",
    "STOP_WORDS = {\n",
    "    'a', 'an', 'the', 'is', 'are', 'was', 'were', 'be', 'been', 'being',\n",
    "    'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could',\n",
    "    'should', 'may', 'might', 'can', 'shall', 'to', 'of', 'in', 'for',\n",
    "    'on', 'with', 'at', 'by', 'from', 'as', 'into', 'through', 'during',\n",
    "    'before', 'after', 'above', 'below', 'between', 'and', 'but', 'or',\n",
    "    'not', 'no', 'if', 'then', 'than', 'so', 'up', 'out', 'about',\n",
    "    'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those',\n",
    "    'it', 'its', 'i', 'me', 'my', 'we', 'our', 'you', 'your', 'he',\n",
    "    'him', 'his', 'she', 'her', 'they', 'them', 'their', 'how', 'when',\n",
    "    'where', 'why', 'much', 'many', 'some', 'any', 'all', 'each',\n",
    "    'does', 'also', 'just', 'more', 'most', 'very', 'too', 'only',\n",
    "}\n",
    "\n",
    "WORD_POOL = [\n",
    "    \"computer\", \"mountain\", \"hospital\", \"children\", \"building\", \"national\",\n",
    "    \"business\", \"research\", \"students\", \"american\", \"possible\", \"economic\",\n",
    "    \"personal\", \"together\", \"products\", \"services\", \"actually\", \"remember\",\n",
    "    \"practice\", \"training\", \"industry\", \"complete\", \"critical\", \"function\",\n",
    "    \"language\", \"standard\", \"material\", \"original\", \"physical\", \"security\",\n",
    "    \"interest\", \"problems\", \"consider\", \"response\", \"pressure\", \"politics\",\n",
    "    \"movement\", \"evidence\", \"southern\", \"northern\", \"exchange\", \"decision\",\n",
    "    \"position\", \"increase\", \"describe\", \"military\", \"required\", \"approach\",\n",
    "    \"strategy\", \"customer\", \"resource\", \"employee\", \"audience\", \"location\",\n",
    "    \"property\", \"cultural\", \"activity\", \"strength\", \"analysis\", \"powerful\",\n",
    "    \"election\", \"argument\", \"campaign\", \"maintain\", \"question\", \"behavior\",\n",
    "    \"majority\", \"solution\", \"software\", \"consumer\", \"creative\", \"reaction\",\n",
    "    \"european\", \"delivery\", \"organize\", \"involved\", \"relative\", \"learning\",\n",
    "    \"positive\", \"numerous\", \"familiar\", \"engineer\", \"platform\", \"indicate\",\n",
    "    \"previous\", \"pleasure\", \"opposite\", \"magazine\", \"document\", \"religion\",\n",
    "    \"scenario\", \"workshop\", \"minority\", \"guidance\", \"estimate\", \"recently\",\n",
    "    \"surprise\", \"champion\", \"pleasant\", \"grateful\", \"moderate\", \"boundary\",\n",
    "]\n",
    "\n",
    "print(\"Loading MS MARCO v1.1 validation...\")\n",
    "ds = load_dataset(\"microsoft/ms_marco\", \"v1.1\", split=\"validation\")\n",
    "\n",
    "all_candidates = []\n",
    "for item in ds:\n",
    "    if len(all_candidates) >= 3 * N_SAMPLES:\n",
    "        break\n",
    "    passages = item.get('passages', {})\n",
    "    ptexts = passages.get('passage_text', [])\n",
    "    is_sel = passages.get('is_selected', [])\n",
    "    query = item.get('query', '')\n",
    "    answers = item.get('answers', [])\n",
    "    well_formed = item.get('wellFormedAnswers', [])\n",
    "    answer = None\n",
    "    if well_formed and len(well_formed) > 0 and well_formed[0] not in ('[]', ''):\n",
    "        answer = well_formed[0]\n",
    "    elif answers and len(answers) > 0 and answers[0] != 'No Answer Present.':\n",
    "        answer = answers[0]\n",
    "    if not answer:\n",
    "        continue\n",
    "    for pt, sel in zip(ptexts, is_sel):\n",
    "        wc = count_words(pt)\n",
    "        if sel == 1 and 30 <= wc <= 300:\n",
    "            all_candidates.append({\n",
    "                'passage': pt, 'query': query, 'answer': answer,\n",
    "                'word_count': wc,\n",
    "            })\n",
    "            break\n",
    "\n",
    "print(f\"Total candidates: {len(all_candidates)}\")\n",
    "np.random.seed(SEED)\n",
    "indices = np.random.permutation(len(all_candidates))\n",
    "samples = [all_candidates[i] for i in indices[:N_SAMPLES]]\n",
    "del ds, all_candidates\n",
    "gc.collect()\n",
    "\n",
    "# Generate random prefixes, wrong answers, and overlap\n",
    "for i, s in enumerate(samples):\n",
    "    rng = np.random.RandomState(SEED + i + 20000)\n",
    "    words = rng.choice(WORD_POOL, size=8, replace=False)\n",
    "    s['random_prefix'] = \" \".join(words)\n",
    "    s['wrong_answer'] = samples[(i + 1) % len(samples)]['answer']\n",
    "\n",
    "    q_words = set(re.sub(r'[^\\w\\s]', '', s['query'].lower()).split()) - STOP_WORDS\n",
    "    d_words = set(re.sub(r'[^\\w\\s]', '', s['passage'].lower()).split()) - STOP_WORDS\n",
    "    union = q_words | d_words\n",
    "    s['query_doc_overlap'] = len(q_words & d_words) / len(union) if len(union) > 0 else 0.0\n",
    "\n",
    "# Load cached model answers from Exp 04c\n",
    "GEN_CACHE = Path(\"../../../results/prefix_lm_exp04c/generated_answers.json\")\n",
    "if GEN_CACHE.exists():\n",
    "    gen_data = json.loads(GEN_CACHE.read_text())\n",
    "    # Verify alignment: check first few queries match\n",
    "    for i in range(min(5, len(gen_data))):\n",
    "        cached_q = gen_data[i]['query'][:50]\n",
    "        sample_q = samples[i]['query'][:50]\n",
    "        assert cached_q == sample_q, (\n",
    "            f\"Sample mismatch at {i}: cached='{cached_q}' vs current='{sample_q}'. \"\n",
    "            f\"Data pipelines differ -- cannot reuse cached model answers.\")\n",
    "    for i, s in enumerate(samples):\n",
    "        s['model_answer'] = gen_data[i]['model_answer']\n",
    "    print(f\"Loaded {len(gen_data)} cached model answers from Exp 04c.\")\n",
    "else:\n",
    "    # Fallback: generate model answers if cache not available\n",
    "    print(\"WARNING: No cached model answers found. Generating from scratch...\")\n",
    "    print(\"This will add ~28 min. Run Exp 04c first to cache answers.\\n\")\n",
    "    t0 = time.time()\n",
    "    for i in tqdm(range(N_SAMPLES), desc=\"Generating\"):\n",
    "        query = samples[i]['query']\n",
    "        prompt = f\"Question: {query}\\nAnswer:\"\n",
    "        input_ids = tokenizer(prompt, return_tensors=\"pt\",\n",
    "                              add_special_tokens=True).input_ids.to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            output = model.generate(\n",
    "                input_ids, max_new_tokens=30, do_sample=False, temperature=1.0)\n",
    "        gen_ids = output[0][input_ids.shape[1]:]\n",
    "        gen_text = tokenizer.decode(gen_ids, skip_special_tokens=True).strip()\n",
    "        for stop_char in ['\\n', '. ', '? ', '! ']:\n",
    "            idx = gen_text.find(stop_char)\n",
    "            if idx >= 0:\n",
    "                gen_text = gen_text[:idx + len(stop_char)].strip()\n",
    "                break\n",
    "        samples[i]['model_answer'] = gen_text\n",
    "        if (i + 1) % 100 == 0:\n",
    "            gc.collect(); torch.cuda.empty_cache()\n",
    "    elapsed = time.time() - t0\n",
    "    print(f\"Generated {N_SAMPLES} answers in {elapsed/60:.1f} min\")\n",
    "\n",
    "# Compute model answer overlap\n",
    "model_overlaps = []\n",
    "for s in samples:\n",
    "    m_words = set(re.sub(r'[^\\w\\s]', '', s['model_answer'].lower()).split()) - STOP_WORDS\n",
    "    r_words = set(re.sub(r'[^\\w\\s]', '', s['answer'].lower()).split()) - STOP_WORDS\n",
    "    union = m_words | r_words\n",
    "    overlap = len(m_words & r_words) / len(union) if len(union) > 0 else 0.0\n",
    "    model_overlaps.append(overlap)\n",
    "    s['model_answer_overlap'] = overlap\n",
    "\n",
    "print(f\"\\nLoaded {len(samples)} samples\")\n",
    "print(f\"Mean passage words: {np.mean([s['word_count'] for s in samples]):.0f}\")\n",
    "print(f\"Mean query words: {np.mean([count_words(s['query']) for s in samples]):.0f}\")\n",
    "print(f\"Mean answer words: {np.mean([count_words(s['answer']) for s in samples]):.0f}\")\n",
    "print(f\"Mean model answer overlap: {np.mean(model_overlaps):.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f59fc2d4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-22T14:04:04.685694Z",
     "iopub.status.busy": "2026-02-22T14:04:04.685194Z",
     "iopub.status.idle": "2026-02-22T14:04:04.699421Z",
     "shell.execute_reply": "2026-02-22T14:04:04.698778Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scoring function defined (two-pass, 7 conditions per sample).\n",
      "All conditions use truncation -- prime masked from Phase B.\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: score_sample() -- two-pass, all truncate=True\n",
    "#\n",
    "# Phase A (offline): Forward [BOS, surr, doc] with causal mask, use_cache=True\n",
    "# Phase B (online):  Forward [query, answer] using cached KVs\n",
    "#                    Surrogate positions MASKED from continuation (always)\n",
    "#\n",
    "# This removes the copy shortcut: answer tokens cannot attend to prime tokens.\n",
    "\n",
    "def score_sample(model, tokenizer, sample, device):\n",
    "    passage = sample['passage']\n",
    "    query = sample['query']\n",
    "    answer = sample['answer']\n",
    "    wrong_answer_text = sample['wrong_answer']\n",
    "    random_prefix = sample['random_prefix']\n",
    "    model_answer_text = sample['model_answer']\n",
    "\n",
    "    bos_id = tokenizer.bos_token_id\n",
    "\n",
    "    doc_ids = tokenizer(passage, add_special_tokens=False, truncation=True,\n",
    "                        max_length=1024).input_ids\n",
    "    query_ids = tokenizer(query, add_special_tokens=False, truncation=True,\n",
    "                          max_length=512).input_ids\n",
    "    answer_ids = tokenizer(answer, add_special_tokens=False, truncation=True,\n",
    "                           max_length=256).input_ids\n",
    "\n",
    "    if len(answer_ids) == 0:\n",
    "        return None\n",
    "\n",
    "    # Prime token IDs for each condition\n",
    "    oracle_ids = tokenizer(query, add_special_tokens=False, truncation=True,\n",
    "                           max_length=256).input_ids\n",
    "    random_ids = tokenizer(random_prefix, add_special_tokens=False).input_ids\n",
    "    full_answer_ids = tokenizer(answer, add_special_tokens=False, truncation=True,\n",
    "                                max_length=64).input_ids\n",
    "    wrong_answer_ids = tokenizer(wrong_answer_text, add_special_tokens=False,\n",
    "                                 truncation=True, max_length=64).input_ids\n",
    "    answer_5tok_ids = full_answer_ids[:5]\n",
    "    model_answer_ids = tokenizer(model_answer_text, add_special_tokens=False,\n",
    "                                 truncation=True, max_length=64).input_ids\n",
    "\n",
    "    prefix_map = {\n",
    "        \"bare\": [],\n",
    "        \"random\": random_ids,\n",
    "        \"oracle\": oracle_ids,\n",
    "        \"answer_prime\": full_answer_ids,\n",
    "        \"wrong_answer\": wrong_answer_ids,\n",
    "        \"answer_5tok\": answer_5tok_ids,\n",
    "        \"model_answer\": model_answer_ids,\n",
    "    }\n",
    "\n",
    "    n_q = len(query_ids)\n",
    "    n_a = len(answer_ids)\n",
    "    n_d = len(doc_ids)\n",
    "\n",
    "    targets = torch.tensor(answer_ids, dtype=torch.long, device=device)\n",
    "    result = {\n",
    "        'n_doc': n_d,\n",
    "        'n_query': n_q,\n",
    "        'n_oracle': len(oracle_ids),\n",
    "        'n_answer_prime': len(full_answer_ids),\n",
    "        'n_wrong_answer': len(wrong_answer_ids),\n",
    "        'n_model_answer': len(model_answer_ids),\n",
    "    }\n",
    "\n",
    "    for cond_name in CONDITIONS:\n",
    "        surr_ids = prefix_map[cond_name]\n",
    "        n_s = len(surr_ids)\n",
    "        n_prefix = 1 + n_s + n_d\n",
    "\n",
    "        # === Phase A: Cache [BOS, surrogate, doc] with causal attention ===\n",
    "        prefix_tokens = [bos_id] + surr_ids + doc_ids\n",
    "        prefix_input = torch.tensor([prefix_tokens], dtype=torch.long, device=device)\n",
    "\n",
    "        phase_a_mask = make_phase_a_mask(n_s, n_d)\n",
    "        phase_a_dict = make_mask_dict(phase_a_mask.to(device))\n",
    "        phase_a_pos = torch.arange(n_prefix, device=device).unsqueeze(0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            out_a = model(input_ids=prefix_input, attention_mask=phase_a_dict,\n",
    "                          position_ids=phase_a_pos, use_cache=True)\n",
    "        past_kv = out_a.past_key_values\n",
    "\n",
    "        # === Phase B: Evaluate [query, answer] with cached KVs ===\n",
    "        cont_tokens = query_ids + answer_ids\n",
    "        n_cont = len(cont_tokens)\n",
    "        cont_input = torch.tensor([cont_tokens], dtype=torch.long, device=device)\n",
    "\n",
    "        phase_b_mask = make_phase_b_mask(n_s, n_d, n_q, n_a)\n",
    "        phase_b_dict = make_mask_dict(phase_b_mask.to(device))\n",
    "        phase_b_pos = torch.arange(n_prefix, n_prefix + n_cont,\n",
    "                                    device=device).unsqueeze(0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            out_b = model(input_ids=cont_input, attention_mask=phase_b_dict,\n",
    "                          position_ids=phase_b_pos, past_key_values=past_kv)\n",
    "\n",
    "        # === Compute NLL on answer tokens ===\n",
    "        answer_logits = out_b.logits[0, n_q - 1 : n_q + n_a - 1, :]\n",
    "        log_probs = F.log_softmax(answer_logits, dim=-1)\n",
    "        token_nlls = -log_probs.gather(1, targets.unsqueeze(1)).squeeze(1)\n",
    "        result[f'nll_{cond_name}'] = token_nlls.mean().item()\n",
    "\n",
    "        del out_a, out_b, past_kv, prefix_input, cont_input\n",
    "        del phase_a_mask, phase_b_mask, phase_a_dict, phase_b_dict\n",
    "        del answer_logits, log_probs, token_nlls\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "print(f\"Scoring function defined (two-pass, {len(CONDITIONS)} conditions per sample).\")\n",
    "print(f\"All conditions use truncation -- prime masked from Phase B.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac4a338e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-22T14:04:04.702358Z",
     "iopub.status.busy": "2026-02-22T14:04:04.701874Z",
     "iopub.status.idle": "2026-02-22T14:18:01.375794Z",
     "shell.execute_reply": "2026-02-22T14:18:01.374821Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "MAIN SCORING LOOP\n",
      "======================================================================\n",
      "Starting fresh: 500 samples x 7 conditions\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "132323c4d88144bf97e4d0ca34f2cdbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Scoring:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done: 500 samples in 13.9 min\n",
      "\n",
      "Quick summary:\n",
      "  bare             NLL=2.9572\n",
      "  random           NLL=2.2979\n",
      "  oracle           NLL=1.9678\n",
      "  answer_prime     NLL=2.3894\n",
      "  wrong_answer     NLL=2.4955\n",
      "  answer_5tok      NLL=2.3971\n",
      "  model_answer     NLL=2.5598\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Main scoring loop\n",
    "from lib.data import count_words as _cw\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"MAIN SCORING LOOP\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "CKPT_PATH = RESULTS_DIR / \"checkpoint.json\"\n",
    "\n",
    "all_results = []\n",
    "start_idx = 0\n",
    "if CKPT_PATH.exists():\n",
    "    ckpt = json.loads(CKPT_PATH.read_text())\n",
    "    if len(ckpt.get('results', [])) > 0:\n",
    "        saved_queries = [r['query'][:50] for r in ckpt['results']]\n",
    "        current_queries = [s['query'][:50] for s in samples[:len(saved_queries)]]\n",
    "        if saved_queries == current_queries:\n",
    "            all_results = ckpt['results']\n",
    "            start_idx = len(all_results)\n",
    "            print(f\"Resuming from checkpoint: {start_idx}/{N_SAMPLES}\")\n",
    "\n",
    "if start_idx == 0:\n",
    "    print(f\"Starting fresh: {N_SAMPLES} samples x {len(CONDITIONS)} conditions\")\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "for i in tqdm(range(start_idx, N_SAMPLES), initial=start_idx, total=N_SAMPLES,\n",
    "              desc=\"Scoring\"):\n",
    "    s = samples[i]\n",
    "    try:\n",
    "        result = score_sample(model, tokenizer, s, DEVICE)\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR at sample {i}: {e}\")\n",
    "        result = None\n",
    "\n",
    "    if result is None:\n",
    "        continue\n",
    "\n",
    "    result['query'] = s['query'][:50]\n",
    "    result['query_doc_overlap'] = s['query_doc_overlap']\n",
    "    result['model_answer_overlap'] = s['model_answer_overlap']\n",
    "    result['answer_wc'] = _cw(s['answer'])\n",
    "    result['doc_wc'] = s['word_count']\n",
    "    all_results.append(result)\n",
    "\n",
    "    if (i + 1) % 25 == 0 or i == N_SAMPLES - 1:\n",
    "        ckpt = {\n",
    "            'model': MODEL_NAME,\n",
    "            'n_total': N_SAMPLES,\n",
    "            'n_conditions': len(CONDITIONS),\n",
    "            'condition_names': CONDITIONS,\n",
    "            'results': all_results,\n",
    "            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        }\n",
    "        CKPT_PATH.write_text(json.dumps(ckpt))\n",
    "\n",
    "    if (i + 1) % 100 == 0:\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "elapsed = time.time() - t0\n",
    "print(f\"\\nDone: {len(all_results)} samples in {elapsed/60:.1f} min\")\n",
    "print(f\"\\nQuick summary:\")\n",
    "for cn in CONDITIONS:\n",
    "    vals = [r[f'nll_{cn}'] for r in all_results]\n",
    "    print(f\"  {cn:<16} NLL={np.mean(vals):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f26f701",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-22T14:18:01.379762Z",
     "iopub.status.busy": "2026-02-22T14:18:01.379378Z",
     "iopub.status.idle": "2026-02-22T14:18:01.422325Z",
     "shell.execute_reply": "2026-02-22T14:18:01.421541Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "RESULTS: ANSWER ENRICHMENT (NO COPY SHORTCUT)\n",
      "======================================================================\n",
      "\n",
      "--- A. Full Ranking (500 samples) ---\n",
      "\n",
      "  Condition          Mean NLL  d vs bare  d vs random    p vs bare   sig\n",
      "  ------------------------------------------------------------------------\n",
      "  oracle               1.9678     +0.452       +0.266     6.03e-22   ***\n",
      "  random               2.2979     +0.475       +0.000     7.13e-24   ***\n",
      "  answer_prime         2.3894     +0.469       -0.103     2.15e-23   ***\n",
      "  answer_5tok          2.3971     +0.466       -0.112     3.81e-23   ***\n",
      "  wrong_answer         2.4955     +0.451       -0.225     7.43e-22   ***\n",
      "  model_answer         2.5598     +0.369       -0.276     1.30e-15   ***\n",
      "  bare                 2.9572     +0.000       -0.475     1.00e+00    ns\n",
      "\n",
      "--- B. Key Comparisons (positive d = first is better) ---\n",
      "\n",
      "  Comparison                                                     d    win%            p   sig\n",
      "  ------------------------------------------------------------------------------------------\n",
      "  B1. random vs bare (structural replication)               +0.475   86.0%     7.13e-24   ***\n",
      "  B2. oracle vs bare                                        +0.452   83.2%     6.03e-22   ***\n",
      "  B3. answer_prime vs random (CONTENT ENRICHMENT?)          -0.103   44.4%     2.23e-02     *\n",
      "  B4. answer_prime vs oracle                                -0.276   36.4%     1.46e-09   ***\n",
      "  B5. model_answer vs random (LLM ENRICHMENT?)              -0.276   30.0%     1.41e-09   ***\n",
      "  B6. wrong_answer vs random                                -0.225   33.8%     6.91e-07   ***\n",
      "  B7. answer_5tok vs random                                 -0.112   41.2%     1.25e-02     *\n",
      "  B8. model_answer vs wrong_answer                          -0.085   41.0%     5.65e-02    ns\n",
      "\n",
      "--- C. Comparison with Exp 04c (single-pass, copy shortcut present) ---\n",
      "\n",
      "  Effect                            Exp 04c    Exp 04d      Delta Interpretation\n",
      "  -------------------------------------------------------------------------------------\n",
      "  d_random vs bare                   +0.456     +0.475     +0.019  structural\n",
      "  d_answer vs bare                   +0.851     +0.469     -0.382  answer enrichment + copy\n",
      "  d_answer vs random                 +0.261     -0.103     -0.364  COPY SHORTCUT EFFECT\n",
      "  d_model vs bare                    +0.250     +0.369     +0.119  LLM surrogate total\n",
      "  d_model vs random                  -0.265     -0.276     -0.011  LLM interference\n",
      "  d_wrong vs bare                    +0.424     +0.451     +0.027  wrong content total\n",
      "  d_wrong vs random                  -0.222     -0.225     -0.003  wrong interference\n",
      "\n",
      "--- D. Model Answer Quality vs Enrichment Benefit ---\n",
      "\n",
      "  Correlation: model_answer_overlap x enrichment_benefit\n",
      "    r=+0.027, p=5.50e-01 ns\n",
      "\n",
      "  Split by model answer quality (median overlap=0.055):\n",
      "  Group                         N    d_model   d_random  d_model_vs_rand\n",
      "  ----------------------------------------------------------------------\n",
      "  High overlap (good gen)     250     +0.367     +0.455           -0.246\n",
      "  Low overlap (bad gen)       250     +0.372     +0.493           -0.304\n",
      "\n",
      "--- E. Answer Length Subpopulation ---\n",
      "\n",
      "  Group               N   d_answer    d_model    d_wrong   d_random   d_oracle\n",
      "  ---------------------------------------------------------------------------\n",
      "  Short (<=5w)      210     +0.712     +0.536     +0.659     +0.699     +0.698\n",
      "  Long (>5w)        290     +0.564     +0.482     +0.580     +0.816     +0.690\n",
      "\n",
      "--- F. Structural Fraction ---\n",
      "\n",
      "  d_oracle=+0.452, d_random=+0.475\n",
      "  Structural fraction: 105%\n",
      "  (Exp 01/02 reference: ~105-140%)\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Analysis\n",
    "print(\"=\" * 70)\n",
    "print(\"RESULTS: ANSWER ENRICHMENT (NO COPY SHORTCUT)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "nll = {}\n",
    "for cn in CONDITIONS:\n",
    "    nll[cn] = np.array([r[f'nll_{cn}'] for r in all_results])\n",
    "\n",
    "N = len(all_results)\n",
    "\n",
    "# --- A. Full ranking ---\n",
    "print(f\"\\n--- A. Full Ranking ({N} samples) ---\\n\")\n",
    "print(f\"  {'Condition':<16} {'Mean NLL':>10} {'d vs bare':>10} {'d vs random':>12} {'p vs bare':>12} {'sig':>5}\")\n",
    "print(f\"  {'-'*72}\")\n",
    "\n",
    "ranked = sorted(CONDITIONS, key=lambda cn: nll[cn].mean())\n",
    "for cn in ranked:\n",
    "    if cn == \"bare\":\n",
    "        d_base = 0.0\n",
    "        d_rand = cohens_d(nll['random'] - nll[cn])\n",
    "        p_base = 1.0\n",
    "    else:\n",
    "        diff_base = nll['bare'] - nll[cn]\n",
    "        d_base = cohens_d(diff_base)\n",
    "        _, p_base = stats.ttest_1samp(diff_base, 0)\n",
    "        diff_rand = nll['random'] - nll[cn]\n",
    "        d_rand = cohens_d(diff_rand)\n",
    "    sig = '***' if p_base < 0.001 else '**' if p_base < 0.01 else '*' if p_base < 0.05 else 'ns'\n",
    "    print(f\"  {cn:<16} {nll[cn].mean():>10.4f} {d_base:>+10.3f} {d_rand:>+12.3f} {p_base:>12.2e} {sig:>5}\")\n",
    "\n",
    "# --- B. Key comparisons ---\n",
    "print(f\"\\n--- B. Key Comparisons (positive d = first is better) ---\\n\")\n",
    "print(f\"  {'Comparison':<55} {'d':>8} {'win%':>7} {'p':>12} {'sig':>5}\")\n",
    "print(f\"  {'-'*90}\")\n",
    "\n",
    "comparisons = [\n",
    "    # Structural replication (should match Exp 01/02: d~+0.45-0.48)\n",
    "    (\"B1. random vs bare (structural replication)\",\n",
    "     nll['bare'] - nll['random']),\n",
    "\n",
    "    # Oracle vs bare (standard enrichment)\n",
    "    (\"B2. oracle vs bare\",\n",
    "     nll['bare'] - nll['oracle']),\n",
    "\n",
    "    # THE KEY TEST: answer_prime vs random (content beyond structural)\n",
    "    (\"B3. answer_prime vs random (CONTENT ENRICHMENT?)\",\n",
    "     nll['random'] - nll['answer_prime']),\n",
    "\n",
    "    # answer_prime vs oracle (does answer content beat query content?)\n",
    "    (\"B4. answer_prime vs oracle\",\n",
    "     nll['oracle'] - nll['answer_prime']),\n",
    "\n",
    "    # model_answer vs random (LLM surrogate enrichment vs structural)\n",
    "    (\"B5. model_answer vs random (LLM ENRICHMENT?)\",\n",
    "     nll['random'] - nll['model_answer']),\n",
    "\n",
    "    # wrong_answer vs random (wrong content enrichment)\n",
    "    (\"B6. wrong_answer vs random\",\n",
    "     nll['random'] - nll['wrong_answer']),\n",
    "\n",
    "    # answer_5tok vs random (partial content)\n",
    "    (\"B7. answer_5tok vs random\",\n",
    "     nll['random'] - nll['answer_5tok']),\n",
    "\n",
    "    # model_answer vs wrong_answer\n",
    "    (\"B8. model_answer vs wrong_answer\",\n",
    "     nll['wrong_answer'] - nll['model_answer']),\n",
    "]\n",
    "\n",
    "for label, diff in comparisons:\n",
    "    d = cohens_d(diff)\n",
    "    _, p = stats.ttest_1samp(diff, 0)\n",
    "    sig = '***' if p < 0.001 else '**' if p < 0.01 else '*' if p < 0.05 else 'ns'\n",
    "    win = (diff > 0).mean() * 100\n",
    "    print(f\"  {label:<55} {d:>+8.3f} {win:>6.1f}% {p:>12.2e} {sig:>5}\")\n",
    "\n",
    "# --- C. Comparison with Exp 04c (copy shortcut present) ---\n",
    "print(f\"\\n--- C. Comparison with Exp 04c (single-pass, copy shortcut present) ---\\n\")\n",
    "\n",
    "# Exp 04c reference values (from results)\n",
    "exp04c_ref = {\n",
    "    'd_answer_vs_bare': 0.851,\n",
    "    'd_answer_vs_random': 0.261,\n",
    "    'd_model_vs_bare': 0.250,\n",
    "    'd_model_vs_random': -0.265,\n",
    "    'd_wrong_vs_bare': 0.424,\n",
    "    'd_wrong_vs_random': -0.222,\n",
    "    'd_random_vs_bare': 0.456,\n",
    "}\n",
    "\n",
    "d_answer_vs_bare_04d = cohens_d(nll['bare'] - nll['answer_prime'])\n",
    "d_answer_vs_random_04d = cohens_d(nll['random'] - nll['answer_prime'])\n",
    "d_model_vs_bare_04d = cohens_d(nll['bare'] - nll['model_answer'])\n",
    "d_model_vs_random_04d = cohens_d(nll['random'] - nll['model_answer'])\n",
    "d_wrong_vs_bare_04d = cohens_d(nll['bare'] - nll['wrong_answer'])\n",
    "d_wrong_vs_random_04d = cohens_d(nll['random'] - nll['wrong_answer'])\n",
    "d_random_vs_bare_04d = cohens_d(nll['bare'] - nll['random'])\n",
    "\n",
    "print(f\"  {'Effect':<30} {'Exp 04c':>10} {'Exp 04d':>10} {'Delta':>10} {'Interpretation'}\")\n",
    "print(f\"  {'-'*85}\")\n",
    "rows = [\n",
    "    (\"d_random vs bare\",\n",
    "     exp04c_ref['d_random_vs_bare'], d_random_vs_bare_04d, \"structural\"),\n",
    "    (\"d_answer vs bare\",\n",
    "     exp04c_ref['d_answer_vs_bare'], d_answer_vs_bare_04d, \"answer enrichment + copy\"),\n",
    "    (\"d_answer vs random\",\n",
    "     exp04c_ref['d_answer_vs_random'], d_answer_vs_random_04d, \"COPY SHORTCUT EFFECT\"),\n",
    "    (\"d_model vs bare\",\n",
    "     exp04c_ref['d_model_vs_bare'], d_model_vs_bare_04d, \"LLM surrogate total\"),\n",
    "    (\"d_model vs random\",\n",
    "     exp04c_ref['d_model_vs_random'], d_model_vs_random_04d, \"LLM interference\"),\n",
    "    (\"d_wrong vs bare\",\n",
    "     exp04c_ref['d_wrong_vs_bare'], d_wrong_vs_bare_04d, \"wrong content total\"),\n",
    "    (\"d_wrong vs random\",\n",
    "     exp04c_ref['d_wrong_vs_random'], d_wrong_vs_random_04d, \"wrong interference\"),\n",
    "]\n",
    "for label, v_04c, v_04d, interp in rows:\n",
    "    delta = v_04d - v_04c\n",
    "    print(f\"  {label:<30} {v_04c:>+10.3f} {v_04d:>+10.3f} {delta:>+10.3f}  {interp}\")\n",
    "\n",
    "# --- D. Model answer quality analysis ---\n",
    "print(f\"\\n--- D. Model Answer Quality vs Enrichment Benefit ---\\n\")\n",
    "\n",
    "model_overlap = np.array([r['model_answer_overlap'] for r in all_results])\n",
    "model_benefit = nll['bare'] - nll['model_answer']\n",
    "\n",
    "r_val, p_val = stats.pearsonr(model_overlap, model_benefit)\n",
    "sig = '***' if p_val < 0.001 else '**' if p_val < 0.01 else '*' if p_val < 0.05 else 'ns'\n",
    "print(f\"  Correlation: model_answer_overlap x enrichment_benefit\")\n",
    "print(f\"    r={r_val:+.3f}, p={p_val:.2e} {sig}\")\n",
    "\n",
    "hi_overlap = model_overlap > np.median(model_overlap)\n",
    "lo_overlap = ~hi_overlap\n",
    "print(f\"\\n  Split by model answer quality (median overlap={np.median(model_overlap):.3f}):\")\n",
    "print(f\"  {'Group':<25} {'N':>5} {'d_model':>10} {'d_random':>10} {'d_model_vs_rand':>16}\")\n",
    "print(f\"  {'-'*70}\")\n",
    "for label, mask in [(\"High overlap (good gen)\", hi_overlap),\n",
    "                     (\"Low overlap (bad gen)\", lo_overlap)]:\n",
    "    d_ma = cohens_d((nll['bare'] - nll['model_answer'])[mask])\n",
    "    d_rn = cohens_d((nll['bare'] - nll['random'])[mask])\n",
    "    d_mr = cohens_d((nll['random'] - nll['model_answer'])[mask])\n",
    "    print(f\"  {label:<25} {mask.sum():>5} {d_ma:>+10.3f} {d_rn:>+10.3f} {d_mr:>+16.3f}\")\n",
    "\n",
    "# --- E. Answer length subpopulation ---\n",
    "print(f\"\\n--- E. Answer Length Subpopulation ---\\n\")\n",
    "answer_wc = np.array([r['answer_wc'] for r in all_results])\n",
    "short = answer_wc <= 5\n",
    "long = ~short\n",
    "\n",
    "print(f\"  {'Group':<15} {'N':>5} {'d_answer':>10} {'d_model':>10} {'d_wrong':>10} {'d_random':>10} {'d_oracle':>10}\")\n",
    "print(f\"  {'-'*75}\")\n",
    "for label, mask in [(\"Short (<=5w)\", short), (\"Long (>5w)\", long)]:\n",
    "    d_ans = cohens_d((nll['bare'] - nll['answer_prime'])[mask])\n",
    "    d_mod = cohens_d((nll['bare'] - nll['model_answer'])[mask])\n",
    "    d_wrg = cohens_d((nll['bare'] - nll['wrong_answer'])[mask])\n",
    "    d_rnd = cohens_d((nll['bare'] - nll['random'])[mask])\n",
    "    d_orc = cohens_d((nll['bare'] - nll['oracle'])[mask])\n",
    "    print(f\"  {label:<15} {mask.sum():>5} {d_ans:>+10.3f} {d_mod:>+10.3f} {d_wrg:>+10.3f} {d_rnd:>+10.3f} {d_orc:>+10.3f}\")\n",
    "\n",
    "# --- F. Structural fraction ---\n",
    "print(f\"\\n--- F. Structural Fraction ---\\n\")\n",
    "d_oracle = cohens_d(nll['bare'] - nll['oracle'])\n",
    "d_random = cohens_d(nll['bare'] - nll['random'])\n",
    "struct_frac = d_random / d_oracle if d_oracle != 0 else float('nan')\n",
    "print(f\"  d_oracle={d_oracle:+.3f}, d_random={d_random:+.3f}\")\n",
    "print(f\"  Structural fraction: {struct_frac:.0%}\")\n",
    "print(f\"  (Exp 01/02 reference: ~105-140%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7307cb1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-22T14:18:01.426032Z",
     "iopub.status.busy": "2026-02-22T14:18:01.425758Z",
     "iopub.status.idle": "2026-02-22T14:18:01.443683Z",
     "shell.execute_reply": "2026-02-22T14:18:01.442865Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "SUMMARY -- Prefix LM Exp 04d: Answer Enrichment (No Copy Shortcut)\n",
      "======================================================================\n",
      "\n",
      "  Structural: d_random vs bare = +0.475\n",
      "  Oracle:     d_oracle vs bare = +0.452\n",
      "\n",
      "  answer_prime vs random: d=-0.103 (p=2.23e-02)\n",
      "  model_answer vs random: d=-0.276 (p=1.41e-09)\n",
      "  wrong_answer vs random: d=-0.225 (p=6.91e-07)\n",
      "  answer_5tok  vs random: d=-0.112 (p=1.25e-02)\n",
      "\n",
      "  VERDICT:\n",
      "  Answer prime slightly worse than random (d=-0.103).\n",
      "  Answer content may create mild interference even through enrichment.\n",
      "  Model answer STILL hurts vs random (d=-0.276).\n",
      "  Misleading content poisons doc representations during Phase A.\n",
      "\n",
      "Results saved to ../../../results/prefix_lm_exp04d/results.json\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Save results and verdict\n",
    "print(\"=\" * 70)\n",
    "print(\"SUMMARY -- Prefix LM Exp 04d: Answer Enrichment (No Copy Shortcut)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "d_struct = cohens_d(nll['bare'] - nll['random'])\n",
    "d_oracle_base = cohens_d(nll['bare'] - nll['oracle'])\n",
    "\n",
    "d_ans_vs_rand = cohens_d(nll['random'] - nll['answer_prime'])\n",
    "_, p_ans_vs_rand = stats.ttest_1samp(nll['random'] - nll['answer_prime'], 0)\n",
    "\n",
    "d_model_vs_rand = cohens_d(nll['random'] - nll['model_answer'])\n",
    "_, p_model_vs_rand = stats.ttest_1samp(nll['random'] - nll['model_answer'], 0)\n",
    "\n",
    "d_wrong_vs_rand = cohens_d(nll['random'] - nll['wrong_answer'])\n",
    "_, p_wrong_vs_rand = stats.ttest_1samp(nll['random'] - nll['wrong_answer'], 0)\n",
    "\n",
    "d_5tok_vs_rand = cohens_d(nll['random'] - nll['answer_5tok'])\n",
    "_, p_5tok_vs_rand = stats.ttest_1samp(nll['random'] - nll['answer_5tok'], 0)\n",
    "\n",
    "print(f\"\\n  Structural: d_random vs bare = {d_struct:+.3f}\")\n",
    "print(f\"  Oracle:     d_oracle vs bare = {d_oracle_base:+.3f}\")\n",
    "print(f\"\")\n",
    "print(f\"  answer_prime vs random: d={d_ans_vs_rand:+.3f} (p={p_ans_vs_rand:.2e})\")\n",
    "print(f\"  model_answer vs random: d={d_model_vs_rand:+.3f} (p={p_model_vs_rand:.2e})\")\n",
    "print(f\"  wrong_answer vs random: d={d_wrong_vs_rand:+.3f} (p={p_wrong_vs_rand:.2e})\")\n",
    "print(f\"  answer_5tok  vs random: d={d_5tok_vs_rand:+.3f} (p={p_5tok_vs_rand:.2e})\")\n",
    "\n",
    "print(f\"\\n  VERDICT:\")\n",
    "\n",
    "# Compare with Exp 04c\n",
    "d_ans_04c = 0.261  # answer_prime vs random in Exp 04c\n",
    "d_model_04c = -0.265  # model_answer vs random in Exp 04c\n",
    "\n",
    "if p_ans_vs_rand < 0.05 and d_ans_vs_rand > 0.1:\n",
    "    copy_frac = 1.0 - d_ans_vs_rand / d_ans_04c if d_ans_04c != 0 else float('nan')\n",
    "    print(f\"  Answer prime STILL beats random (d={d_ans_vs_rand:+.3f}).\")\n",
    "    print(f\"  Genuine enrichment -- not just copy artifact.\")\n",
    "    print(f\"  Copy shortcut accounted for ~{copy_frac:.0%} of Exp 04c's d=+0.261.\")\n",
    "elif p_ans_vs_rand >= 0.05:\n",
    "    print(f\"  Answer prime ~ random (d={d_ans_vs_rand:+.3f}, ns).\")\n",
    "    print(f\"  Exp 04c's d=+0.261 was ENTIRELY a copy shortcut.\")\n",
    "    print(f\"  Answer content does NOT enrich document representations.\")\n",
    "else:\n",
    "    print(f\"  Answer prime slightly worse than random (d={d_ans_vs_rand:+.3f}).\")\n",
    "    print(f\"  Answer content may create mild interference even through enrichment.\")\n",
    "\n",
    "if p_model_vs_rand >= 0.05:\n",
    "    print(f\"  Model answer ~ random (d={d_model_vs_rand:+.3f}, ns).\")\n",
    "    print(f\"  Exp 04c interference (d={d_model_04c:+.3f}) was direct-attention artifact.\")\n",
    "elif d_model_vs_rand < -0.05 and p_model_vs_rand < 0.05:\n",
    "    print(f\"  Model answer STILL hurts vs random (d={d_model_vs_rand:+.3f}).\")\n",
    "    print(f\"  Misleading content poisons doc representations during Phase A.\")\n",
    "else:\n",
    "    print(f\"  Model answer marginally different from random (d={d_model_vs_rand:+.3f}).\")\n",
    "\n",
    "# Save\n",
    "summary = {'n_samples': N, 'model': MODEL_NAME}\n",
    "for cn in CONDITIONS:\n",
    "    summary[f'nll_{cn}'] = float(nll[cn].mean())\n",
    "summary['d_structural'] = float(d_struct)\n",
    "summary['d_oracle'] = float(d_oracle_base)\n",
    "summary['d_answer_vs_random'] = float(d_ans_vs_rand)\n",
    "summary['d_model_vs_random'] = float(d_model_vs_rand)\n",
    "summary['d_wrong_vs_random'] = float(d_wrong_vs_rand)\n",
    "summary['d_answer_vs_random_04c'] = float(d_ans_04c)\n",
    "summary['d_model_vs_random_04c'] = float(d_model_04c)\n",
    "\n",
    "final_results = {\n",
    "    'experiment': 'prefix_lm_exp04d',\n",
    "    'dataset': 'ms_marco_v1.1',\n",
    "    'model': MODEL_NAME,\n",
    "    'n_samples': N,\n",
    "    'seed': SEED,\n",
    "    'conditions': CONDITIONS,\n",
    "    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'summary': summary,\n",
    "    'exp04c_reference': {\n",
    "        'd_answer_vs_random': 0.261,\n",
    "        'd_model_vs_random': -0.265,\n",
    "        'd_wrong_vs_random': -0.222,\n",
    "        'd_random_vs_bare': 0.456,\n",
    "    },\n",
    "}\n",
    "\n",
    "with open(RESULTS_DIR / 'results.json', 'w') as f:\n",
    "    json.dump(final_results, f, indent=2)\n",
    "print(f\"\\nResults saved to {RESULTS_DIR / 'results.json'}\")\n",
    "print(\"Done!\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "132323c4d88144bf97e4d0ca34f2cdbb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_7222aa36f32744a492a4141122cc0132",
        "IPY_MODEL_9f928fd9c0204127a3b7189931606deb",
        "IPY_MODEL_4bab0b5014b748699bce3f35d0a37083"
       ],
       "layout": "IPY_MODEL_3c508e7a65fe4aa2a9a263e0575b9836",
       "tabbable": null,
       "tooltip": null
      }
     },
     "205257a15876467ab9b510ded3c0db54": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "2a65101a02784a608336bb6b2a303c70": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_7bc33dbddd1141949b8e4cb91b728ddc",
       "placeholder": "​",
       "style": "IPY_MODEL_5fc5fc0c6ab0446da8ed8dc57435527f",
       "tabbable": null,
       "tooltip": null,
       "value": " 1065/1065 [00:06&lt;00:00, 661.15it/s, Materializing param=model.vision_tower.vision_model.post_layernorm.weight]"
      }
     },
     "2ca578cde12549d4823242eb3b4c4f56": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3c508e7a65fe4aa2a9a263e0575b9836": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3efeb1eedf544d569eec275f5bb82154": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "4226e03a2bed461aa8bbac17534bd0be": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_3efeb1eedf544d569eec275f5bb82154",
       "placeholder": "​",
       "style": "IPY_MODEL_205257a15876467ab9b510ded3c0db54",
       "tabbable": null,
       "tooltip": null,
       "value": "Loading weights: 100%"
      }
     },
     "4280f56fc5f5422ca52529be27ab529b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "4bab0b5014b748699bce3f35d0a37083": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_7f299bd360df4a43b8523c23b467b487",
       "placeholder": "​",
       "style": "IPY_MODEL_a5cca5996ce64a02b29477786ade1e29",
       "tabbable": null,
       "tooltip": null,
       "value": " 500/500 [13:56&lt;00:00,  1.73s/it]"
      }
     },
     "4d0d5a8954e64d768cb4615192bb2772": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "4dadc9c7865f4af68d6ff193fb8eb10b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_2ca578cde12549d4823242eb3b4c4f56",
       "max": 1065.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_4d0d5a8954e64d768cb4615192bb2772",
       "tabbable": null,
       "tooltip": null,
       "value": 1065.0
      }
     },
     "5fc5fc0c6ab0446da8ed8dc57435527f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "717affe971794faab4519f607e37de84": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "7222aa36f32744a492a4141122cc0132": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_717affe971794faab4519f607e37de84",
       "placeholder": "​",
       "style": "IPY_MODEL_8ecc251b22bb412ebad8399105631ef0",
       "tabbable": null,
       "tooltip": null,
       "value": "Scoring: 100%"
      }
     },
     "7bc33dbddd1141949b8e4cb91b728ddc": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "7f299bd360df4a43b8523c23b467b487": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "8ecc251b22bb412ebad8399105631ef0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "931a64ac469349b5a63c35393bfa11e3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "9f928fd9c0204127a3b7189931606deb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_b3705a48a7534be1a0dc48520d515ac3",
       "max": 500.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_4280f56fc5f5422ca52529be27ab529b",
       "tabbable": null,
       "tooltip": null,
       "value": 500.0
      }
     },
     "a5cca5996ce64a02b29477786ade1e29": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "b3705a48a7534be1a0dc48520d515ac3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c142c6103b61448187f0fb93784ec21b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_4226e03a2bed461aa8bbac17534bd0be",
        "IPY_MODEL_4dadc9c7865f4af68d6ff193fb8eb10b",
        "IPY_MODEL_2a65101a02784a608336bb6b2a303c70"
       ],
       "layout": "IPY_MODEL_931a64ac469349b5a63c35393bfa11e3",
       "tabbable": null,
       "tooltip": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
