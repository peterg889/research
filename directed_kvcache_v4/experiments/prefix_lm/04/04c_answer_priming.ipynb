{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c447031",
   "metadata": {},
   "source": [
    "# Prefix LM Exp 04c: Answer Priming Test\n",
    "\n",
    "## Motivation\n",
    "\n",
    "Exp 04b showed primes before the doc help (structural d~+0.45) and semantic valence\n",
    "matters (positive > negative, d=+0.248). But no positive instruction beat random tokens.\n",
    "\n",
    "**Question**: What if you prime with the ANSWER itself? Or an LLM-generated surrogate?\n",
    "\n",
    "## Conditions (6)\n",
    "\n",
    "All use single forward pass, native causal attention.\n",
    "\n",
    "| # | Condition | Prime content | What it tests |\n",
    "|---|-----------|---------------|---------------|\n",
    "| 1 | `doc_query` | (none) | Baseline |\n",
    "| 2 | `random_prime` | 8 random words | Structural control |\n",
    "| 3 | `answer_prime` | actual answer text | Ceiling: answer leak |\n",
    "| 4 | `wrong_answer` | answer from sample (i+1)%N | Style-matched, wrong content |\n",
    "| 5 | `answer_5tok` | first 5 tokens of actual answer | Partial answer leak |\n",
    "| 6 | `model_answer` | model-generated answer (query only, no doc) | LLM surrogate |\n",
    "\n",
    "Layout: `[BOS, prime, doc, query, answer]` for primed conditions.\n",
    "\n",
    "## Key Questions\n",
    "\n",
    "- **A**: Does answer priming beat random? (content-specific benefit beyond structural)\n",
    "- **B**: How big is the ceiling? (answer_prime vs baseline)\n",
    "- **C**: Does the model-generated answer help? (LLM surrogate viability)\n",
    "- **D**: Does wrong_answer match random? (answer style vs content)\n",
    "- **E**: Does partial leak (5 tokens) help proportionally?\n",
    "- **F**: How does model answer quality correlate with priming benefit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e26e4cb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-22T12:56:13.863957Z",
     "iopub.status.busy": "2026-02-22T12:56:13.863438Z",
     "iopub.status.idle": "2026-02-22T12:56:18.510883Z",
     "shell.execute_reply": "2026-02-22T12:56:18.509978Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prefix LM Exp 04c: Answer Priming Test\n",
      "N: 500, Conditions: 6\n",
      "DEVICE: cuda\n",
      "GPU: NVIDIA A100-SXM4-40GB\n",
      "GPU memory: 42.3 GB\n",
      "\n",
      "Conditions: ['doc_query', 'random_prime', 'answer_prime', 'wrong_answer', 'answer_5tok', 'model_answer']\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Setup\n",
    "import os\n",
    "os.umask(0o000)\n",
    "\n",
    "import sys, json, time, gc, re\n",
    "import random as pyrandom\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from scipy import stats\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "sys.path.insert(0, \"../../..\")\n",
    "from lib.analysis import cohens_d\n",
    "\n",
    "SEED = 42\n",
    "N_SAMPLES = 500\n",
    "\n",
    "MODEL_NAME = \"google/gemma-3-12b-it\"\n",
    "\n",
    "RESULTS_DIR = Path(\"../../../results/prefix_lm_exp04c\")\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "pyrandom.seed(SEED)\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv())\n",
    "HF_TOKEN = os.environ.get(\"HF_TOKEN\")\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "CONDITIONS = [\n",
    "    \"doc_query\",       # baseline: [BOS, doc, query, answer]\n",
    "    \"random_prime\",    # [BOS, random_8_words, doc, query, answer]\n",
    "    \"answer_prime\",    # [BOS, actual_answer, doc, query, answer]\n",
    "    \"wrong_answer\",    # [BOS, answer_from_other_sample, doc, query, answer]\n",
    "    \"answer_5tok\",     # [BOS, first_5_answer_tokens, doc, query, answer]\n",
    "    \"model_answer\",    # [BOS, model_generated_answer, doc, query, answer]\n",
    "]\n",
    "\n",
    "print(f\"Prefix LM Exp 04c: Answer Priming Test\")\n",
    "print(f\"N: {N_SAMPLES}, Conditions: {len(CONDITIONS)}\")\n",
    "print(f\"DEVICE: {DEVICE}\")\n",
    "print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "print(f\"\\nConditions: {CONDITIONS}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c21b47c2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-22T12:56:18.514298Z",
     "iopub.status.busy": "2026-02-22T12:56:18.513859Z",
     "iopub.status.idle": "2026-02-22T12:56:34.069928Z",
     "shell.execute_reply": "2026-02-22T12:56:34.069156Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformers version: 5.1.0\n",
      "Loading google/gemma-3-12b-it...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24773c23907d47a49126f39df5612efb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/1065 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: 12.2B params, 24.4 GB GPU, 13s\n",
      "BOS token id: 2\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Load model + tokenizer\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import transformers\n",
    "\n",
    "print(f\"transformers version: {transformers.__version__}\")\n",
    "\n",
    "print(f\"Loading {MODEL_NAME}...\")\n",
    "t0 = time.time()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, token=HF_TOKEN)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    attn_implementation=\"eager\",\n",
    "    token=HF_TOKEN,\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "n_params = sum(p.numel() for p in model.parameters()) / 1e9\n",
    "gpu_mem = torch.cuda.memory_allocated() / 1e9\n",
    "print(f\"Loaded: {n_params:.1f}B params, {gpu_mem:.1f} GB GPU, {time.time()-t0:.0f}s\")\n",
    "print(f\"BOS token id: {tokenizer.bos_token_id}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fadd19c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-22T12:56:34.073311Z",
     "iopub.status.busy": "2026-02-22T12:56:34.072877Z",
     "iopub.status.idle": "2026-02-22T12:56:35.728554Z",
     "shell.execute_reply": "2026-02-22T12:56:35.727768Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MS MARCO v1.1 validation...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total candidates: 1500\n",
      "Loaded 500 samples\n",
      "Mean passage words: 74\n",
      "Mean query words: 6\n",
      "Mean answer words: 14\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Load MS MARCO data (same pipeline as Exp 01-04)\n",
    "from lib.data import count_words\n",
    "from datasets import load_dataset\n",
    "\n",
    "STOP_WORDS = {\n",
    "    'a', 'an', 'the', 'is', 'are', 'was', 'were', 'be', 'been', 'being',\n",
    "    'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could',\n",
    "    'should', 'may', 'might', 'can', 'shall', 'to', 'of', 'in', 'for',\n",
    "    'on', 'with', 'at', 'by', 'from', 'as', 'into', 'through', 'during',\n",
    "    'before', 'after', 'above', 'below', 'between', 'and', 'but', 'or',\n",
    "    'not', 'no', 'if', 'then', 'than', 'so', 'up', 'out', 'about',\n",
    "    'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those',\n",
    "    'it', 'its', 'i', 'me', 'my', 'we', 'our', 'you', 'your', 'he',\n",
    "    'him', 'his', 'she', 'her', 'they', 'them', 'their', 'how', 'when',\n",
    "    'where', 'why', 'much', 'many', 'some', 'any', 'all', 'each',\n",
    "    'does', 'also', 'just', 'more', 'most', 'very', 'too', 'only',\n",
    "}\n",
    "\n",
    "WORD_POOL = [\n",
    "    \"computer\", \"mountain\", \"hospital\", \"children\", \"building\", \"national\",\n",
    "    \"business\", \"research\", \"students\", \"american\", \"possible\", \"economic\",\n",
    "    \"personal\", \"together\", \"products\", \"services\", \"actually\", \"remember\",\n",
    "    \"practice\", \"training\", \"industry\", \"complete\", \"critical\", \"function\",\n",
    "    \"language\", \"standard\", \"material\", \"original\", \"physical\", \"security\",\n",
    "    \"interest\", \"problems\", \"consider\", \"response\", \"pressure\", \"politics\",\n",
    "    \"movement\", \"evidence\", \"southern\", \"northern\", \"exchange\", \"decision\",\n",
    "    \"position\", \"increase\", \"describe\", \"military\", \"required\", \"approach\",\n",
    "    \"strategy\", \"customer\", \"resource\", \"employee\", \"audience\", \"location\",\n",
    "    \"property\", \"cultural\", \"activity\", \"strength\", \"analysis\", \"powerful\",\n",
    "    \"election\", \"argument\", \"campaign\", \"maintain\", \"question\", \"behavior\",\n",
    "    \"majority\", \"solution\", \"software\", \"consumer\", \"creative\", \"reaction\",\n",
    "    \"european\", \"delivery\", \"organize\", \"involved\", \"relative\", \"learning\",\n",
    "    \"positive\", \"numerous\", \"familiar\", \"engineer\", \"platform\", \"indicate\",\n",
    "    \"previous\", \"pleasure\", \"opposite\", \"magazine\", \"document\", \"religion\",\n",
    "    \"scenario\", \"workshop\", \"minority\", \"guidance\", \"estimate\", \"recently\",\n",
    "    \"surprise\", \"champion\", \"pleasant\", \"grateful\", \"moderate\", \"boundary\",\n",
    "]\n",
    "\n",
    "print(\"Loading MS MARCO v1.1 validation...\")\n",
    "ds = load_dataset(\"microsoft/ms_marco\", \"v1.1\", split=\"validation\")\n",
    "\n",
    "all_candidates = []\n",
    "for item in ds:\n",
    "    if len(all_candidates) >= 3 * N_SAMPLES:\n",
    "        break\n",
    "    passages = item.get('passages', {})\n",
    "    ptexts = passages.get('passage_text', [])\n",
    "    is_sel = passages.get('is_selected', [])\n",
    "    query = item.get('query', '')\n",
    "    answers = item.get('answers', [])\n",
    "    well_formed = item.get('wellFormedAnswers', [])\n",
    "    answer = None\n",
    "    if well_formed and len(well_formed) > 0 and well_formed[0] not in ('[]', ''):\n",
    "        answer = well_formed[0]\n",
    "    elif answers and len(answers) > 0 and answers[0] != 'No Answer Present.':\n",
    "        answer = answers[0]\n",
    "    if not answer:\n",
    "        continue\n",
    "    for pt, sel in zip(ptexts, is_sel):\n",
    "        wc = count_words(pt)\n",
    "        if sel == 1 and 30 <= wc <= 300:\n",
    "            all_candidates.append({\n",
    "                'passage': pt, 'query': query, 'answer': answer,\n",
    "                'word_count': wc,\n",
    "            })\n",
    "            break\n",
    "\n",
    "print(f\"Total candidates: {len(all_candidates)}\")\n",
    "np.random.seed(SEED)\n",
    "indices = np.random.permutation(len(all_candidates))\n",
    "samples = [all_candidates[i] for i in indices[:N_SAMPLES]]\n",
    "del ds, all_candidates\n",
    "gc.collect()\n",
    "\n",
    "# Generate random prefixes, wrong answers, and overlap\n",
    "for i, s in enumerate(samples):\n",
    "    rng = np.random.RandomState(SEED + i + 20000)\n",
    "    words = rng.choice(WORD_POOL, size=8, replace=False)\n",
    "    s['random_prefix'] = \" \".join(words)\n",
    "    s['wrong_answer'] = samples[(i + 1) % len(samples)]['answer']\n",
    "\n",
    "    q_words = set(re.sub(r'[^\\w\\s]', '', s['query'].lower()).split()) - STOP_WORDS\n",
    "    d_words = set(re.sub(r'[^\\w\\s]', '', s['passage'].lower()).split()) - STOP_WORDS\n",
    "    union = q_words | d_words\n",
    "    s['query_doc_overlap'] = len(q_words & d_words) / len(union) if len(union) > 0 else 0.0\n",
    "\n",
    "print(f\"Loaded {len(samples)} samples\")\n",
    "print(f\"Mean passage words: {np.mean([s['word_count'] for s in samples]):.0f}\")\n",
    "print(f\"Mean query words: {np.mean([count_words(s['query']) for s in samples]):.0f}\")\n",
    "print(f\"Mean answer words: {np.mean([count_words(s['answer']) for s in samples]):.0f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e1fba4a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-22T12:56:35.732138Z",
     "iopub.status.busy": "2026-02-22T12:56:35.731598Z",
     "iopub.status.idle": "2026-02-22T13:24:50.043671Z",
     "shell.execute_reply": "2026-02-22T13:24:50.042960Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating model answers from query alone (no document)...\n",
      "This runs generation for each sample -- ~5-8 minutes.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6743543114824f82acb1c9250f299466",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated 500 answers in 28.2 min\n",
      "Cached to ../../../results/prefix_lm_exp04c/generated_answers.json\n",
      "\n",
      "--- Generated Answer Examples ---\n",
      "\n",
      "  Q: what is the link between alveoli and capillaries\n",
      "  Model: The link between alveoli and capillaries is incredibly close and vital\n",
      "  Real:  Diffusion\n",
      "\n",
      "  Q: how thick does concrete need to be garden wall\n",
      "  Model: The ideal thickness for a garden wall made of concrete depends on a fe\n",
      "  Real:  For walls up to 3ft, 5.5 inches thick.\n",
      "\n",
      "  Q: average nurse salary singapore\n",
      "  Model: The average salary for a Registered Nurse in Singapore is **SGD 4,800 \n",
      "  Real:  S$34,924 per year\n",
      "\n",
      "  Q: pharmacist salary in oregon\n",
      "  Model: The salary for pharmacists in Oregon varies depending on experience, l\n",
      "  Real:  Average $33,000 per year.\n",
      "\n",
      "  Q: what is the average temperature in pei in july\n",
      "  Model: The average high temperature in Prince Edward Island (PEI) in July is \n",
      "  Real:  34 degrees C\n",
      "\n",
      "Model answer stats:\n",
      "  Mean tokens: 21.7\n",
      "  Mean word overlap with real answer (Jaccard): 0.092\n",
      "  Overlap > 0: 288 / 500\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Generate model answers from query alone (no document)\n",
    "#\n",
    "# For each sample, prompt the model with just the query and generate a short answer.\n",
    "# This simulates an LLM surrogate: \"What would the model guess without seeing the doc?\"\n",
    "# Generated answers are used as primes in the scoring phase.\n",
    "\n",
    "GEN_CKPT_PATH = RESULTS_DIR / \"generated_answers.json\"\n",
    "\n",
    "if GEN_CKPT_PATH.exists():\n",
    "    print(\"Loading cached generated answers...\")\n",
    "    gen_data = json.loads(GEN_CKPT_PATH.read_text())\n",
    "    for i, s in enumerate(samples):\n",
    "        s['model_answer'] = gen_data[i]['model_answer']\n",
    "    print(f\"Loaded {len(gen_data)} cached generated answers.\")\n",
    "else:\n",
    "    print(\"Generating model answers from query alone (no document)...\")\n",
    "    print(\"This runs generation for each sample -- ~5-8 minutes.\\n\")\n",
    "\n",
    "    t0 = time.time()\n",
    "    for i in tqdm(range(N_SAMPLES), desc=\"Generating\"):\n",
    "        query = samples[i]['query']\n",
    "\n",
    "        # Simple prompt: just the question with a prompt for answer\n",
    "        prompt = f\"Question: {query}\\nAnswer:\"\n",
    "        input_ids = tokenizer(prompt, return_tensors=\"pt\",\n",
    "                              add_special_tokens=True).input_ids.to(DEVICE)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model.generate(\n",
    "                input_ids,\n",
    "                max_new_tokens=30,\n",
    "                do_sample=False,       # greedy for reproducibility\n",
    "                temperature=1.0,\n",
    "            )\n",
    "\n",
    "        # Extract only the generated tokens (after the prompt)\n",
    "        gen_ids = output[0][input_ids.shape[1]:]\n",
    "        gen_text = tokenizer.decode(gen_ids, skip_special_tokens=True).strip()\n",
    "\n",
    "        # Truncate to first sentence or 30 tokens (whichever is shorter)\n",
    "        # Stop at first period, newline, or end\n",
    "        for stop_char in ['\\n', '. ', '? ', '! ']:\n",
    "            idx = gen_text.find(stop_char)\n",
    "            if idx >= 0:\n",
    "                gen_text = gen_text[:idx + len(stop_char)].strip()\n",
    "                break\n",
    "\n",
    "        samples[i]['model_answer'] = gen_text\n",
    "\n",
    "        if (i + 1) % 100 == 0:\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    elapsed = time.time() - t0\n",
    "    print(f\"\\nGenerated {N_SAMPLES} answers in {elapsed/60:.1f} min\")\n",
    "\n",
    "    # Cache generated answers\n",
    "    gen_data = [{'query': s['query'][:80], 'model_answer': s['model_answer'],\n",
    "                 'real_answer': s['answer'][:80]} for s in samples]\n",
    "    GEN_CKPT_PATH.write_text(json.dumps(gen_data, indent=2))\n",
    "    print(f\"Cached to {GEN_CKPT_PATH}\")\n",
    "\n",
    "# Show examples and stats\n",
    "print(f\"\\n--- Generated Answer Examples ---\\n\")\n",
    "for i in range(5):\n",
    "    print(f\"  Q: {samples[i]['query'][:70]}\")\n",
    "    print(f\"  Model: {samples[i]['model_answer'][:70]}\")\n",
    "    print(f\"  Real:  {samples[i]['answer'][:70]}\")\n",
    "    print()\n",
    "\n",
    "# Token overlap between model answer and real answer\n",
    "model_overlaps = []\n",
    "for s in samples:\n",
    "    m_words = set(re.sub(r'[^\\w\\s]', '', s['model_answer'].lower()).split()) - STOP_WORDS\n",
    "    r_words = set(re.sub(r'[^\\w\\s]', '', s['answer'].lower()).split()) - STOP_WORDS\n",
    "    union = m_words | r_words\n",
    "    overlap = len(m_words & r_words) / len(union) if len(union) > 0 else 0.0\n",
    "    model_overlaps.append(overlap)\n",
    "    s['model_answer_overlap'] = overlap\n",
    "\n",
    "print(f\"Model answer stats:\")\n",
    "print(f\"  Mean tokens: {np.mean([len(tokenizer(s['model_answer'], add_special_tokens=False).input_ids) for s in samples]):.1f}\")\n",
    "print(f\"  Mean word overlap with real answer (Jaccard): {np.mean(model_overlaps):.3f}\")\n",
    "print(f\"  Overlap > 0: {sum(1 for o in model_overlaps if o > 0)} / {N_SAMPLES}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2a3d899",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-22T13:24:50.047144Z",
     "iopub.status.busy": "2026-02-22T13:24:50.046589Z",
     "iopub.status.idle": "2026-02-22T13:24:50.057320Z",
     "shell.execute_reply": "2026-02-22T13:24:50.056692Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scoring function defined (6 conditions per sample).\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: score_sample() -- single forward pass, native causal attention\n",
    "\n",
    "def score_sample(model, tokenizer, sample, device):\n",
    "    passage = sample['passage']\n",
    "    query = sample['query']\n",
    "    answer = sample['answer']\n",
    "    wrong_answer_text = sample['wrong_answer']\n",
    "    random_prefix = sample['random_prefix']\n",
    "    model_answer_text = sample['model_answer']\n",
    "\n",
    "    bos_id = tokenizer.bos_token_id\n",
    "\n",
    "    doc_ids = tokenizer(passage, add_special_tokens=False, truncation=True,\n",
    "                        max_length=1024).input_ids\n",
    "    query_ids = tokenizer(query, add_special_tokens=False, truncation=True,\n",
    "                          max_length=512).input_ids\n",
    "    answer_ids = tokenizer(answer, add_special_tokens=False, truncation=True,\n",
    "                           max_length=256).input_ids\n",
    "\n",
    "    if len(answer_ids) == 0:\n",
    "        return None\n",
    "\n",
    "    # Prime token IDs\n",
    "    random_ids = tokenizer(random_prefix, add_special_tokens=False).input_ids\n",
    "    full_answer_ids = tokenizer(answer, add_special_tokens=False, truncation=True,\n",
    "                                max_length=64).input_ids\n",
    "    wrong_answer_ids = tokenizer(wrong_answer_text, add_special_tokens=False,\n",
    "                                 truncation=True, max_length=64).input_ids\n",
    "    answer_5tok_ids = full_answer_ids[:5]\n",
    "    model_answer_ids = tokenizer(model_answer_text, add_special_tokens=False,\n",
    "                                 truncation=True, max_length=64).input_ids\n",
    "\n",
    "    n_a = len(answer_ids)\n",
    "    targets = torch.tensor(answer_ids, dtype=torch.long, device=device)\n",
    "\n",
    "    # Build sequences: [BOS, (prime), doc, query, answer]\n",
    "    base = doc_ids + query_ids + answer_ids\n",
    "    sequences = {\n",
    "        \"doc_query\":     [bos_id] + base,\n",
    "        \"random_prime\":  [bos_id] + random_ids + base,\n",
    "        \"answer_prime\":  [bos_id] + full_answer_ids + base,\n",
    "        \"wrong_answer\":  [bos_id] + wrong_answer_ids + base,\n",
    "        \"answer_5tok\":   [bos_id] + answer_5tok_ids + base,\n",
    "        \"model_answer\":  [bos_id] + model_answer_ids + base,\n",
    "    }\n",
    "\n",
    "    result = {\n",
    "        'n_doc': len(doc_ids),\n",
    "        'n_query': len(query_ids),\n",
    "        'n_answer_prime': len(full_answer_ids),\n",
    "        'n_wrong_answer': len(wrong_answer_ids),\n",
    "        'n_model_answer': len(model_answer_ids),\n",
    "    }\n",
    "\n",
    "    for name, seq in sequences.items():\n",
    "        input_tensor = torch.tensor([seq], dtype=torch.long, device=device)\n",
    "        n_before = len(seq) - n_a\n",
    "\n",
    "        with torch.no_grad():\n",
    "            out = model(input_ids=input_tensor)\n",
    "\n",
    "        answer_logits = out.logits[0, n_before - 1 : n_before + n_a - 1, :]\n",
    "        log_probs = F.log_softmax(answer_logits, dim=-1)\n",
    "        token_nlls = -log_probs.gather(1, targets.unsqueeze(1)).squeeze(1)\n",
    "        result[f'nll_{name}'] = token_nlls.mean().item()\n",
    "\n",
    "        del out, input_tensor, answer_logits, log_probs, token_nlls\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "print(f\"Scoring function defined ({len(CONDITIONS)} conditions per sample).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d979f774",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-22T13:24:50.060332Z",
     "iopub.status.busy": "2026-02-22T13:24:50.059814Z",
     "iopub.status.idle": "2026-02-22T13:30:55.122045Z",
     "shell.execute_reply": "2026-02-22T13:30:55.121317Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "MAIN SCORING LOOP\n",
      "======================================================================\n",
      "Starting fresh: 500 samples x 6 conditions\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b3f6f7c84f143dca41a525451fe8068",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Scoring:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done: 500 samples in 6.1 min\n",
      "\n",
      "Quick summary:\n",
      "  doc_query        NLL=2.9538\n",
      "  random_prime     NLL=2.4158\n",
      "  answer_prime     NLL=2.1604\n",
      "  wrong_answer     NLL=2.6007\n",
      "  answer_5tok      NLL=2.3534\n",
      "  model_answer     NLL=2.6769\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Main scoring loop\n",
    "from lib.data import count_words as _cw\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"MAIN SCORING LOOP\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "CKPT_PATH = RESULTS_DIR / \"checkpoint.json\"\n",
    "\n",
    "all_results = []\n",
    "start_idx = 0\n",
    "if CKPT_PATH.exists():\n",
    "    ckpt = json.loads(CKPT_PATH.read_text())\n",
    "    if len(ckpt.get('results', [])) > 0:\n",
    "        saved_queries = [r['query'][:50] for r in ckpt['results']]\n",
    "        current_queries = [s['query'][:50] for s in samples[:len(saved_queries)]]\n",
    "        if saved_queries == current_queries:\n",
    "            all_results = ckpt['results']\n",
    "            start_idx = len(all_results)\n",
    "            print(f\"Resuming from checkpoint: {start_idx}/{N_SAMPLES}\")\n",
    "\n",
    "if start_idx == 0:\n",
    "    print(f\"Starting fresh: {N_SAMPLES} samples x {len(CONDITIONS)} conditions\")\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "for i in tqdm(range(start_idx, N_SAMPLES), initial=start_idx, total=N_SAMPLES,\n",
    "              desc=\"Scoring\"):\n",
    "    s = samples[i]\n",
    "    try:\n",
    "        result = score_sample(model, tokenizer, s, DEVICE)\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR at sample {i}: {e}\")\n",
    "        result = None\n",
    "\n",
    "    if result is None:\n",
    "        continue\n",
    "\n",
    "    result['query'] = s['query'][:50]\n",
    "    result['query_doc_overlap'] = s['query_doc_overlap']\n",
    "    result['model_answer_overlap'] = s['model_answer_overlap']\n",
    "    result['answer_wc'] = _cw(s['answer'])\n",
    "    result['doc_wc'] = s['word_count']\n",
    "    all_results.append(result)\n",
    "\n",
    "    if (i + 1) % 25 == 0 or i == N_SAMPLES - 1:\n",
    "        ckpt = {\n",
    "            'model': MODEL_NAME,\n",
    "            'n_total': N_SAMPLES,\n",
    "            'n_conditions': len(CONDITIONS),\n",
    "            'condition_names': CONDITIONS,\n",
    "            'results': all_results,\n",
    "            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        }\n",
    "        CKPT_PATH.write_text(json.dumps(ckpt))\n",
    "\n",
    "    if (i + 1) % 100 == 0:\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "elapsed = time.time() - t0\n",
    "print(f\"\\nDone: {len(all_results)} samples in {elapsed/60:.1f} min\")\n",
    "print(f\"\\nQuick summary:\")\n",
    "for cn in CONDITIONS:\n",
    "    vals = [r[f'nll_{cn}'] for r in all_results]\n",
    "    print(f\"  {cn:<16} NLL={np.mean(vals):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "463f525b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-22T13:30:55.125640Z",
     "iopub.status.busy": "2026-02-22T13:30:55.125328Z",
     "iopub.status.idle": "2026-02-22T13:30:55.159760Z",
     "shell.execute_reply": "2026-02-22T13:30:55.159125Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "RESULTS: ANSWER PRIMING TEST\n",
      "======================================================================\n",
      "\n",
      "--- A. Full Ranking (500 samples) ---\n",
      "\n",
      "  Condition          Mean NLL  d vs baseline  d vs random            p   sig\n",
      "  ------------------------------------------------------------------------\n",
      "  answer_prime         2.1604         +0.851       +0.261     4.07e-61   ***\n",
      "  answer_5tok          2.3534         +0.655       +0.071     1.11e-40   ***\n",
      "  random_prime         2.4158         +0.456       +0.000     2.85e-22   ***\n",
      "  wrong_answer         2.6007         +0.424       -0.222     1.02e-19   ***\n",
      "  model_answer         2.6769         +0.250       -0.265     3.76e-08   ***\n",
      "  doc_query            2.9538         +0.000       -0.456     1.00e+00    ns\n",
      "\n",
      "--- B. Key Comparisons (positive d = first is better) ---\n",
      "\n",
      "  Comparison                                                d    win%            p   sig\n",
      "  ----------------------------------------------------------------------------------\n",
      "  B1. answer_prime vs baseline (CEILING)               +0.851   97.6%     4.07e-61   ***\n",
      "  B2. answer_prime vs random                           +0.261   75.8%     9.33e-09   ***\n",
      "  B3. model_answer vs baseline                         +0.250   62.8%     3.76e-08   ***\n",
      "  B4. model_answer vs random                           -0.265   25.2%     5.99e-09   ***\n",
      "  B5. wrong_answer vs random                           -0.222   34.6%     9.87e-07   ***\n",
      "  B6. answer_5tok vs random                            +0.071   61.8%     1.15e-01    ns\n",
      "  B7. model_answer vs wrong_answer                     -0.086   36.0%     5.55e-02    ns\n",
      "  B8. answer_prime vs model_answer (gap)               +0.475   86.8%     6.71e-24   ***\n",
      "\n",
      "--- C. Prime Length Stats ---\n",
      "\n",
      "  answer_prime     mean=18.5, std=16.9, min=1, max=64\n",
      "  wrong_answer     mean=18.5, std=16.9, min=1, max=64\n",
      "  model_answer     mean=21.7, std=6.8, min=2, max=32\n",
      "\n",
      "--- D. Model Answer Quality vs Priming Benefit ---\n",
      "\n",
      "  Correlation: model_answer_overlap x priming_benefit\n",
      "    r=+0.049, p=2.76e-01 ns\n",
      "\n",
      "  Split by model answer quality (median overlap=0.055):\n",
      "  Group                         N  d_model_ans   d_random\n",
      "  -------------------------------------------------------\n",
      "  High overlap (good gen)     250       +0.255     +0.448\n",
      "  Low overlap (bad gen)       250       +0.248     +0.463\n",
      "\n",
      "--- E. Answer Length Subpopulation ---\n",
      "\n",
      "  Group               N   d_answer    d_model    d_wrong   d_random\n",
      "  -----------------------------------------------------------------\n",
      "  Short (<=5w)      210     +1.035     +0.373     +0.625     +0.677\n",
      "  Long (>5w)        290     +0.928     +0.188     +0.503     +0.777\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Analysis\n",
    "print(\"=\" * 70)\n",
    "print(\"RESULTS: ANSWER PRIMING TEST\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "nll = {}\n",
    "for cn in CONDITIONS:\n",
    "    nll[cn] = np.array([r[f'nll_{cn}'] for r in all_results])\n",
    "\n",
    "N = len(all_results)\n",
    "\n",
    "# --- A. Full ranking ---\n",
    "print(f\"\\n--- A. Full Ranking ({N} samples) ---\\n\")\n",
    "print(f\"  {'Condition':<16} {'Mean NLL':>10} {'d vs baseline':>14} {'d vs random':>12} {'p':>12} {'sig':>5}\")\n",
    "print(f\"  {'-'*72}\")\n",
    "\n",
    "ranked = sorted(CONDITIONS, key=lambda cn: nll[cn].mean())\n",
    "for cn in ranked:\n",
    "    if cn == \"doc_query\":\n",
    "        d_base, d_rand = 0.0, cohens_d(nll['random_prime'] - nll[cn])\n",
    "        p_base = 1.0\n",
    "    else:\n",
    "        diff_base = nll['doc_query'] - nll[cn]\n",
    "        d_base = cohens_d(diff_base)\n",
    "        _, p_base = stats.ttest_1samp(diff_base, 0)\n",
    "        diff_rand = nll['random_prime'] - nll[cn]\n",
    "        d_rand = cohens_d(diff_rand)\n",
    "    sig = '***' if p_base < 0.001 else '**' if p_base < 0.01 else '*' if p_base < 0.05 else 'ns'\n",
    "    print(f\"  {cn:<16} {nll[cn].mean():>10.4f} {d_base:>+14.3f} {d_rand:>+12.3f} {p_base:>12.2e} {sig:>5}\")\n",
    "\n",
    "# --- B. Key comparisons ---\n",
    "print(f\"\\n--- B. Key Comparisons (positive d = first is better) ---\\n\")\n",
    "print(f\"  {'Comparison':<50} {'d':>8} {'win%':>7} {'p':>12} {'sig':>5}\")\n",
    "print(f\"  {'-'*82}\")\n",
    "\n",
    "comparisons = [\n",
    "    # Ceiling: answer leak vs baseline\n",
    "    (\"B1. answer_prime vs baseline (CEILING)\",\n",
    "     nll['doc_query'] - nll['answer_prime']),\n",
    "\n",
    "    # Answer leak vs random (content-specific beyond structural)\n",
    "    (\"B2. answer_prime vs random\",\n",
    "     nll['random_prime'] - nll['answer_prime']),\n",
    "\n",
    "    # Model answer vs baseline\n",
    "    (\"B3. model_answer vs baseline\",\n",
    "     nll['doc_query'] - nll['model_answer']),\n",
    "\n",
    "    # Model answer vs random (does LLM surrogate beat random?)\n",
    "    (\"B4. model_answer vs random\",\n",
    "     nll['random_prime'] - nll['model_answer']),\n",
    "\n",
    "    # Wrong answer vs random (answer style vs content)\n",
    "    (\"B5. wrong_answer vs random\",\n",
    "     nll['random_prime'] - nll['wrong_answer']),\n",
    "\n",
    "    # Partial leak vs random\n",
    "    (\"B6. answer_5tok vs random\",\n",
    "     nll['random_prime'] - nll['answer_5tok']),\n",
    "\n",
    "    # Model answer vs wrong answer (quality of LLM surrogate)\n",
    "    (\"B7. model_answer vs wrong_answer\",\n",
    "     nll['wrong_answer'] - nll['model_answer']),\n",
    "\n",
    "    # Answer prime vs model answer (ceiling - LLM surrogate)\n",
    "    (\"B8. answer_prime vs model_answer (gap)\",\n",
    "     nll['model_answer'] - nll['answer_prime']),\n",
    "]\n",
    "\n",
    "for label, diff in comparisons:\n",
    "    d = cohens_d(diff)\n",
    "    _, p = stats.ttest_1samp(diff, 0)\n",
    "    sig = '***' if p < 0.001 else '**' if p < 0.01 else '*' if p < 0.05 else 'ns'\n",
    "    win = (diff > 0).mean() * 100\n",
    "    print(f\"  {label:<50} {d:>+8.3f} {win:>6.1f}% {p:>12.2e} {sig:>5}\")\n",
    "\n",
    "# --- C. Prime length stats ---\n",
    "print(f\"\\n--- C. Prime Length Stats ---\\n\")\n",
    "for field, label in [('n_answer_prime', 'answer_prime'),\n",
    "                      ('n_wrong_answer', 'wrong_answer'),\n",
    "                      ('n_model_answer', 'model_answer')]:\n",
    "    vals = [r[field] for r in all_results]\n",
    "    print(f\"  {label:<16} mean={np.mean(vals):.1f}, std={np.std(vals):.1f}, \"\n",
    "          f\"min={np.min(vals)}, max={np.max(vals)}\")\n",
    "\n",
    "# --- D. Model answer quality analysis ---\n",
    "print(f\"\\n--- D. Model Answer Quality vs Priming Benefit ---\\n\")\n",
    "\n",
    "model_overlap = np.array([r['model_answer_overlap'] for r in all_results])\n",
    "model_benefit = nll['doc_query'] - nll['model_answer']  # positive = model_answer helps\n",
    "\n",
    "r_val, p_val = stats.pearsonr(model_overlap, model_benefit)\n",
    "sig = '***' if p_val < 0.001 else '**' if p_val < 0.01 else '*' if p_val < 0.05 else 'ns'\n",
    "print(f\"  Correlation: model_answer_overlap x priming_benefit\")\n",
    "print(f\"    r={r_val:+.3f}, p={p_val:.2e} {sig}\")\n",
    "\n",
    "# Split by model answer quality\n",
    "hi_overlap = model_overlap > np.median(model_overlap)\n",
    "lo_overlap = ~hi_overlap\n",
    "print(f\"\\n  Split by model answer quality (median overlap={np.median(model_overlap):.3f}):\")\n",
    "print(f\"  {'Group':<25} {'N':>5} {'d_model_ans':>12} {'d_random':>10}\")\n",
    "print(f\"  {'-'*55}\")\n",
    "for label, mask in [(\"High overlap (good gen)\", hi_overlap),\n",
    "                     (\"Low overlap (bad gen)\", lo_overlap)]:\n",
    "    d_ma = cohens_d(model_benefit[mask])\n",
    "    d_rn = cohens_d((nll['doc_query'] - nll['random_prime'])[mask])\n",
    "    print(f\"  {label:<25} {mask.sum():>5} {d_ma:>+12.3f} {d_rn:>+10.3f}\")\n",
    "\n",
    "# --- E. Answer length subpopulation ---\n",
    "print(f\"\\n--- E. Answer Length Subpopulation ---\\n\")\n",
    "answer_wc = np.array([r['answer_wc'] for r in all_results])\n",
    "short = answer_wc <= 5\n",
    "long = ~short\n",
    "\n",
    "print(f\"  {'Group':<15} {'N':>5} {'d_answer':>10} {'d_model':>10} {'d_wrong':>10} {'d_random':>10}\")\n",
    "print(f\"  {'-'*65}\")\n",
    "for label, mask in [(\"Short (<=5w)\", short), (\"Long (>5w)\", long)]:\n",
    "    d_ans = cohens_d((nll['doc_query'] - nll['answer_prime'])[mask])\n",
    "    d_mod = cohens_d((nll['doc_query'] - nll['model_answer'])[mask])\n",
    "    d_wrg = cohens_d((nll['doc_query'] - nll['wrong_answer'])[mask])\n",
    "    d_rnd = cohens_d((nll['doc_query'] - nll['random_prime'])[mask])\n",
    "    print(f\"  {label:<15} {mask.sum():>5} {d_ans:>+10.3f} {d_mod:>+10.3f} {d_wrg:>+10.3f} {d_rnd:>+10.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9c6514d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-22T13:30:55.162784Z",
     "iopub.status.busy": "2026-02-22T13:30:55.162322Z",
     "iopub.status.idle": "2026-02-22T13:30:55.177515Z",
     "shell.execute_reply": "2026-02-22T13:30:55.176896Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "SUMMARY -- Prefix LM Exp 04c: Answer Priming Test\n",
      "======================================================================\n",
      "\n",
      "  d_ceiling (answer_prime vs baseline): +0.851\n",
      "  d_answer_vs_random:                   +0.261 (p=9.33e-09)\n",
      "  d_model_answer (vs baseline):         +0.250\n",
      "  d_model_vs_random:                    -0.265 (p=5.99e-09)\n",
      "  d_wrong_answer_vs_random:             -0.222 (p=9.87e-07)\n",
      "\n",
      "  VERDICT:\n",
      "  Answer prime beats random (d=+0.261, ***): content matters for answers!\n",
      "  Total ceiling: d=+0.851 vs baseline.\n",
      "\n",
      "Results saved to ../../../results/prefix_lm_exp04c/results.json\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Save results and verdict\n",
    "print(\"=\" * 70)\n",
    "print(\"SUMMARY -- Prefix LM Exp 04c: Answer Priming Test\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "d_ceiling = cohens_d(nll['doc_query'] - nll['answer_prime'])\n",
    "d_ans_vs_rand = cohens_d(nll['random_prime'] - nll['answer_prime'])\n",
    "_, p_ans_vs_rand = stats.ttest_1samp(nll['random_prime'] - nll['answer_prime'], 0)\n",
    "d_model = cohens_d(nll['doc_query'] - nll['model_answer'])\n",
    "d_model_vs_rand = cohens_d(nll['random_prime'] - nll['model_answer'])\n",
    "_, p_model_vs_rand = stats.ttest_1samp(nll['random_prime'] - nll['model_answer'], 0)\n",
    "d_wrong = cohens_d(nll['doc_query'] - nll['wrong_answer'])\n",
    "d_wrong_vs_rand = cohens_d(nll['random_prime'] - nll['wrong_answer'])\n",
    "_, p_wrong_vs_rand = stats.ttest_1samp(nll['random_prime'] - nll['wrong_answer'], 0)\n",
    "\n",
    "print(f\"\\n  d_ceiling (answer_prime vs baseline): {d_ceiling:+.3f}\")\n",
    "print(f\"  d_answer_vs_random:                   {d_ans_vs_rand:+.3f} (p={p_ans_vs_rand:.2e})\")\n",
    "print(f\"  d_model_answer (vs baseline):         {d_model:+.3f}\")\n",
    "print(f\"  d_model_vs_random:                    {d_model_vs_rand:+.3f} (p={p_model_vs_rand:.2e})\")\n",
    "print(f\"  d_wrong_answer_vs_random:             {d_wrong_vs_rand:+.3f} (p={p_wrong_vs_rand:.2e})\")\n",
    "\n",
    "print(f\"\\n  VERDICT:\")\n",
    "\n",
    "# Answer ceiling\n",
    "if d_ans_vs_rand > 0.1 and p_ans_vs_rand < 0.05:\n",
    "    print(f\"  Answer prime beats random (d={d_ans_vs_rand:+.3f}, ***): content matters for answers!\")\n",
    "    print(f\"  Total ceiling: d={d_ceiling:+.3f} vs baseline.\")\n",
    "elif p_ans_vs_rand >= 0.05:\n",
    "    print(f\"  Answer prime ~ random (d={d_ans_vs_rand:+.3f}, ns): even the actual answer\")\n",
    "    print(f\"  doesn't help beyond structural. Content is irrelevant.\")\n",
    "\n",
    "# Model answer\n",
    "if p_model_vs_rand < 0.05 and d_model_vs_rand > 0:\n",
    "    print(f\"  LLM surrogate beats random (d={d_model_vs_rand:+.3f}): generated answer adds value!\")\n",
    "    frac = d_model_vs_rand / d_ans_vs_rand if d_ans_vs_rand != 0 else float('nan')\n",
    "    print(f\"  LLM captures {frac:.0%} of the answer-prime ceiling.\")\n",
    "elif p_model_vs_rand >= 0.05:\n",
    "    print(f\"  LLM surrogate ~ random (d={d_model_vs_rand:+.3f}, ns): generation doesn't help.\")\n",
    "\n",
    "# Wrong answer\n",
    "if p_wrong_vs_rand < 0.05 and d_wrong_vs_rand > 0:\n",
    "    print(f\"  Wrong answer > random (d={d_wrong_vs_rand:+.3f}): answer STYLE helps.\")\n",
    "elif p_wrong_vs_rand >= 0.05:\n",
    "    print(f\"  Wrong answer ~ random (d={d_wrong_vs_rand:+.3f}, ns): answer style doesn't matter.\")\n",
    "\n",
    "# Save\n",
    "summary = {'n_samples': N, 'model': MODEL_NAME}\n",
    "for cn in CONDITIONS:\n",
    "    summary[f'nll_{cn}'] = float(nll[cn].mean())\n",
    "summary['d_ceiling'] = float(d_ceiling)\n",
    "summary['d_answer_vs_random'] = float(d_ans_vs_rand)\n",
    "summary['d_model_vs_baseline'] = float(d_model)\n",
    "summary['d_model_vs_random'] = float(d_model_vs_rand)\n",
    "\n",
    "final_results = {\n",
    "    'experiment': 'prefix_lm_exp04c',\n",
    "    'dataset': 'ms_marco_v1.1',\n",
    "    'model': MODEL_NAME,\n",
    "    'n_samples': N,\n",
    "    'seed': SEED,\n",
    "    'conditions': CONDITIONS,\n",
    "    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'summary': summary,\n",
    "    'exp04b_reference': {\n",
    "        'd_random_vs_baseline': 0.456,\n",
    "        'd_positive_vs_baseline': 0.431,\n",
    "    },\n",
    "}\n",
    "\n",
    "with open(RESULTS_DIR / 'results.json', 'w') as f:\n",
    "    json.dump(final_results, f, indent=2)\n",
    "print(f\"\\nResults saved to {RESULTS_DIR / 'results.json'}\")\n",
    "print(\"Done!\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "066a4a5dd63848f585c2ec0b28cd7927": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "0969aec3fa5744048d594e837d632cdc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "0efcdfd56880485da372b533874508a8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_1022f25c494942bebc67b0a579964afb",
       "placeholder": "​",
       "style": "IPY_MODEL_14b822fff14c4e6c8c5a4ef8687fb77d",
       "tabbable": null,
       "tooltip": null,
       "value": " 1065/1065 [00:07&lt;00:00, 664.66it/s, Materializing param=model.vision_tower.vision_model.post_layernorm.weight]"
      }
     },
     "1022f25c494942bebc67b0a579964afb": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "14b822fff14c4e6c8c5a4ef8687fb77d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "1ab657f42dff44b08120516de589a597": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_8bc901b6a9764249af105f0bd5cebf8a",
       "placeholder": "​",
       "style": "IPY_MODEL_2e7d208ac6054b488279803ff8c4ac9f",
       "tabbable": null,
       "tooltip": null,
       "value": " 500/500 [28:14&lt;00:00,  3.50s/it]"
      }
     },
     "1bbf953d560442d0ad61eb5759fafd7d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_34f07a947999483b9e6bf3954fd5d44c",
       "max": 500.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_48e00819748b4f6090a79803f780da8b",
       "tabbable": null,
       "tooltip": null,
       "value": 500.0
      }
     },
     "24773c23907d47a49126f39df5612efb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_986c427037354ba180f42148091d4e9b",
        "IPY_MODEL_4843a9073fa34e53aa54477cac1dfb75",
        "IPY_MODEL_0efcdfd56880485da372b533874508a8"
       ],
       "layout": "IPY_MODEL_3519efbfd6b44c47b185251484a2dc38",
       "tabbable": null,
       "tooltip": null
      }
     },
     "2e7d208ac6054b488279803ff8c4ac9f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "31a00d94e9f64fedabfcb5e9b580b8f2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_066a4a5dd63848f585c2ec0b28cd7927",
       "max": 500.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_4fba2e293e354b7e8211190bc9777b3a",
       "tabbable": null,
       "tooltip": null,
       "value": 500.0
      }
     },
     "34f07a947999483b9e6bf3954fd5d44c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3519efbfd6b44c47b185251484a2dc38": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3ca72253377f47598a7bb529a459df25": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "45c92d86518a4086aff5fe02ba05a0ed": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "4843a9073fa34e53aa54477cac1dfb75": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_a9223f03cac04cf0b00e3f2f0a3dda05",
       "max": 1065.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_3ca72253377f47598a7bb529a459df25",
       "tabbable": null,
       "tooltip": null,
       "value": 1065.0
      }
     },
     "48e00819748b4f6090a79803f780da8b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "495d2a05155740fba940ac3ae166bef3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "4fba2e293e354b7e8211190bc9777b3a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "56db7cff32cd497992d3ef87548191d6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6743543114824f82acb1c9250f299466": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_cf2d3f36bc8b435aa59a235d864a26f6",
        "IPY_MODEL_1bbf953d560442d0ad61eb5759fafd7d",
        "IPY_MODEL_1ab657f42dff44b08120516de589a597"
       ],
       "layout": "IPY_MODEL_e6a9dddf748a4bd8a5614b4ea3ca734a",
       "tabbable": null,
       "tooltip": null
      }
     },
     "6b3f6f7c84f143dca41a525451fe8068": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_79ea17c7374c47f09e6a2a27c6c922cf",
        "IPY_MODEL_31a00d94e9f64fedabfcb5e9b580b8f2",
        "IPY_MODEL_af5b3183cc7746b4945d98b5b37f6ce2"
       ],
       "layout": "IPY_MODEL_495d2a05155740fba940ac3ae166bef3",
       "tabbable": null,
       "tooltip": null
      }
     },
     "79ea17c7374c47f09e6a2a27c6c922cf": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_debd5dce8dfd4636b5b279a6151c55ab",
       "placeholder": "​",
       "style": "IPY_MODEL_d64eabe50f6b46149e99c3efec9ec3ee",
       "tabbable": null,
       "tooltip": null,
       "value": "Scoring: 100%"
      }
     },
     "8bc901b6a9764249af105f0bd5cebf8a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "9570548265dd4ac8be7e7fa40e317e91": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "986c427037354ba180f42148091d4e9b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_9570548265dd4ac8be7e7fa40e317e91",
       "placeholder": "​",
       "style": "IPY_MODEL_a6b6bd1df36b48dc832d1a83d1d446f8",
       "tabbable": null,
       "tooltip": null,
       "value": "Loading weights: 100%"
      }
     },
     "a6b6bd1df36b48dc832d1a83d1d446f8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "a9223f03cac04cf0b00e3f2f0a3dda05": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "af5b3183cc7746b4945d98b5b37f6ce2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_ebab78f33c7b45fd992b5b6b859fdf09",
       "placeholder": "​",
       "style": "IPY_MODEL_45c92d86518a4086aff5fe02ba05a0ed",
       "tabbable": null,
       "tooltip": null,
       "value": " 500/500 [06:05&lt;00:00,  1.27it/s]"
      }
     },
     "cf2d3f36bc8b435aa59a235d864a26f6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_56db7cff32cd497992d3ef87548191d6",
       "placeholder": "​",
       "style": "IPY_MODEL_0969aec3fa5744048d594e837d632cdc",
       "tabbable": null,
       "tooltip": null,
       "value": "Generating: 100%"
      }
     },
     "d64eabe50f6b46149e99c3efec9ec3ee": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "debd5dce8dfd4636b5b279a6151c55ab": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e6a9dddf748a4bd8a5614b4ea3ca734a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ebab78f33c7b45fd992b5b6b859fdf09": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
