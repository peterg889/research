{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "889e6728",
   "metadata": {},
   "source": [
    "# Prefix LM Exp 04f: Attention Probing\n",
    "\n",
    "## Motivation\n",
    "\n",
    "Random tokens work as well as oracle under truncation (structural fraction ~105%).\n",
    "But WHY? Does the oracle query actually change doc-token attention patterns, and\n",
    "the effect just doesn't survive truncation? Or do oracle and random shift attention\n",
    "identically?\n",
    "\n",
    "## Design\n",
    "\n",
    "During Phase A (`[BOS, prime, doc]` with causal attention), extract attention weights\n",
    "and compute:\n",
    "\n",
    "1. **Doc-to-prime attention**: What fraction of each doc token's attention goes to\n",
    "   prime positions vs BOS vs other doc tokens?\n",
    "2. **Attention entropy**: Is attention more focused (lower entropy) for oracle vs random?\n",
    "3. **Layer-by-layer**: Which layers show the biggest prime-type differences?\n",
    "4. **Correlation with NLL**: Do samples where oracle gets more attention also get more NLL benefit?\n",
    "\n",
    "## Conditions (3)\n",
    "\n",
    "| # | Condition | Prime | What it tests |\n",
    "|---|-----------|-------|---------------|\n",
    "| 1 | `bare` | (none) | No prime baseline |\n",
    "| 2 | `random` | 8 random words | Structural attention pattern |\n",
    "| 3 | `oracle` | real query | Semantic attention pattern |\n",
    "\n",
    "N=500 samples. Attention extracted from every 4th layer (11 layers total)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3edeca98",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-22T16:15:07.594458Z",
     "iopub.status.busy": "2026-02-22T16:15:07.593854Z",
     "iopub.status.idle": "2026-02-22T16:15:12.230781Z",
     "shell.execute_reply": "2026-02-22T16:15:12.229744Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prefix LM Exp 04f: Attention Probing\n",
      "N: 500, Conditions: 3\n",
      "Probe layers: [0, 4, 8, 12, 16, 20, 24, 28, 32, 36, 40, 44, 47] (13 layers)\n",
      "DEVICE: cuda\n",
      "GPU: NVIDIA A100-SXM4-40GB\n",
      "GPU memory: 42.3 GB\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Setup\n",
    "import os\n",
    "os.umask(0o000)\n",
    "\n",
    "import sys, json, time, gc, re\n",
    "import random as pyrandom\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from scipy import stats\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "sys.path.insert(0, \"../../..\")\n",
    "from lib.analysis import cohens_d\n",
    "\n",
    "SEED = 42\n",
    "N_SAMPLES = 500\n",
    "\n",
    "MODEL_NAME = \"google/gemma-3-12b-it\"\n",
    "\n",
    "RESULTS_DIR = Path(\"../../../results/prefix_lm_exp04f\")\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "pyrandom.seed(SEED)\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv())\n",
    "HF_TOKEN = os.environ.get(\"HF_TOKEN\")\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "CONDITIONS = [\"bare\", \"random\", \"oracle\"]\n",
    "\n",
    "# Probe every 4th layer + last layer\n",
    "# Gemma 3 12B has 48 layers\n",
    "PROBE_LAYERS = list(range(0, 48, 4)) + [47]\n",
    "PROBE_LAYERS = sorted(set(PROBE_LAYERS))\n",
    "\n",
    "print(f\"Prefix LM Exp 04f: Attention Probing\")\n",
    "print(f\"N: {N_SAMPLES}, Conditions: {len(CONDITIONS)}\")\n",
    "print(f\"Probe layers: {PROBE_LAYERS} ({len(PROBE_LAYERS)} layers)\")\n",
    "print(f\"DEVICE: {DEVICE}\")\n",
    "print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2648c8ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-22T16:15:12.235372Z",
     "iopub.status.busy": "2026-02-22T16:15:12.234525Z",
     "iopub.status.idle": "2026-02-22T16:15:28.062708Z",
     "shell.execute_reply": "2026-02-22T16:15:28.061936Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformers version: 5.1.0\n",
      "Loading google/gemma-3-12b-it...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3687a77a7644612ab2ace6adb8ebb3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/1065 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: 12.2B params, 24.4 GB GPU, 13s\n",
      "Model has 48 layers\n",
      "Adjusted probe layers: [0, 4, 8, 12, 16, 20, 24, 28, 32, 36, 40, 44, 47] (13 layers)\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Load model + tokenizer\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import transformers\n",
    "\n",
    "print(f\"transformers version: {transformers.__version__}\")\n",
    "\n",
    "print(f\"Loading {MODEL_NAME}...\")\n",
    "t0 = time.time()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, token=HF_TOKEN)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    attn_implementation=\"eager\",\n",
    "    token=HF_TOKEN,\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "n_params = sum(p.numel() for p in model.parameters()) / 1e9\n",
    "gpu_mem = torch.cuda.memory_allocated() / 1e9\n",
    "print(f\"Loaded: {n_params:.1f}B params, {gpu_mem:.1f} GB GPU, {time.time()-t0:.0f}s\")\n",
    "\n",
    "# Verify number of layers (Gemma 3 config uses text_config)\n",
    "n_layers = model.config.text_config.num_hidden_layers\n",
    "print(f\"Model has {n_layers} layers\")\n",
    "# Update PROBE_LAYERS if needed\n",
    "PROBE_LAYERS = [l for l in PROBE_LAYERS if l < n_layers]\n",
    "if (n_layers - 1) not in PROBE_LAYERS:\n",
    "    PROBE_LAYERS.append(n_layers - 1)\n",
    "PROBE_LAYERS = sorted(set(PROBE_LAYERS))\n",
    "print(f\"Adjusted probe layers: {PROBE_LAYERS} ({len(PROBE_LAYERS)} layers)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8fd33b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-22T16:15:28.066426Z",
     "iopub.status.busy": "2026-02-22T16:15:28.065603Z",
     "iopub.status.idle": "2026-02-22T16:15:28.072904Z",
     "shell.execute_reply": "2026-02-22T16:15:28.072284Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mask functions defined.\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Mask functions (same as Exp 04d/04e)\n",
    "\n",
    "def make_phase_a_mask(n_s, n_d, dtype=torch.bfloat16):\n",
    "    n_prefix = 1 + n_s + n_d\n",
    "    min_val = torch.finfo(dtype).min\n",
    "    mask = torch.triu(torch.full((n_prefix, n_prefix), min_val, dtype=dtype),\n",
    "                      diagonal=1)\n",
    "    return mask.unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "\n",
    "def make_phase_b_mask(n_s, n_d, n_q, n_a, dtype=torch.bfloat16):\n",
    "    n_prefix = 1 + n_s + n_d\n",
    "    n_cont = n_q + n_a\n",
    "    min_val = torch.finfo(dtype).min\n",
    "    mask = torch.full((n_cont, n_prefix + n_cont), min_val, dtype=dtype)\n",
    "    mask[:, :n_prefix] = 0.0\n",
    "    if n_s > 0:\n",
    "        mask[:, 1:1 + n_s] = min_val\n",
    "    mask[:, n_prefix:] = torch.triu(\n",
    "        torch.full((n_cont, n_cont), min_val, dtype=dtype), diagonal=1\n",
    "    )\n",
    "    return mask.unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "\n",
    "def make_mask_dict(mask_4d):\n",
    "    return {\"full_attention\": mask_4d, \"sliding_attention\": mask_4d}\n",
    "\n",
    "\n",
    "print(\"Mask functions defined.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55c05097",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-22T16:15:28.076259Z",
     "iopub.status.busy": "2026-02-22T16:15:28.075996Z",
     "iopub.status.idle": "2026-02-22T16:15:29.466506Z",
     "shell.execute_reply": "2026-02-22T16:15:29.465751Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MS MARCO v1.1 validation...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total candidates: 1500\n",
      "Loaded 500 samples\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Load MS MARCO data (same pipeline)\n",
    "from lib.data import count_words\n",
    "from datasets import load_dataset\n",
    "\n",
    "STOP_WORDS = {\n",
    "    'a', 'an', 'the', 'is', 'are', 'was', 'were', 'be', 'been', 'being',\n",
    "    'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could',\n",
    "    'should', 'may', 'might', 'can', 'shall', 'to', 'of', 'in', 'for',\n",
    "    'on', 'with', 'at', 'by', 'from', 'as', 'into', 'through', 'during',\n",
    "    'before', 'after', 'above', 'below', 'between', 'and', 'but', 'or',\n",
    "    'not', 'no', 'if', 'then', 'than', 'so', 'up', 'out', 'about',\n",
    "    'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those',\n",
    "    'it', 'its', 'i', 'me', 'my', 'we', 'our', 'you', 'your', 'he',\n",
    "    'him', 'his', 'she', 'her', 'they', 'them', 'their', 'how', 'when',\n",
    "    'where', 'why', 'much', 'many', 'some', 'any', 'all', 'each',\n",
    "    'does', 'also', 'just', 'more', 'most', 'very', 'too', 'only',\n",
    "}\n",
    "\n",
    "WORD_POOL = [\n",
    "    \"computer\", \"mountain\", \"hospital\", \"children\", \"building\", \"national\",\n",
    "    \"business\", \"research\", \"students\", \"american\", \"possible\", \"economic\",\n",
    "    \"personal\", \"together\", \"products\", \"services\", \"actually\", \"remember\",\n",
    "    \"practice\", \"training\", \"industry\", \"complete\", \"critical\", \"function\",\n",
    "    \"language\", \"standard\", \"material\", \"original\", \"physical\", \"security\",\n",
    "    \"interest\", \"problems\", \"consider\", \"response\", \"pressure\", \"politics\",\n",
    "    \"movement\", \"evidence\", \"southern\", \"northern\", \"exchange\", \"decision\",\n",
    "    \"position\", \"increase\", \"describe\", \"military\", \"required\", \"approach\",\n",
    "    \"strategy\", \"customer\", \"resource\", \"employee\", \"audience\", \"location\",\n",
    "    \"property\", \"cultural\", \"activity\", \"strength\", \"analysis\", \"powerful\",\n",
    "    \"election\", \"argument\", \"campaign\", \"maintain\", \"question\", \"behavior\",\n",
    "    \"majority\", \"solution\", \"software\", \"consumer\", \"creative\", \"reaction\",\n",
    "    \"european\", \"delivery\", \"organize\", \"involved\", \"relative\", \"learning\",\n",
    "    \"positive\", \"numerous\", \"familiar\", \"engineer\", \"platform\", \"indicate\",\n",
    "    \"previous\", \"pleasure\", \"opposite\", \"magazine\", \"document\", \"religion\",\n",
    "    \"scenario\", \"workshop\", \"minority\", \"guidance\", \"estimate\", \"recently\",\n",
    "    \"surprise\", \"champion\", \"pleasant\", \"grateful\", \"moderate\", \"boundary\",\n",
    "]\n",
    "\n",
    "print(\"Loading MS MARCO v1.1 validation...\")\n",
    "ds = load_dataset(\"microsoft/ms_marco\", \"v1.1\", split=\"validation\")\n",
    "\n",
    "all_candidates = []\n",
    "for item in ds:\n",
    "    if len(all_candidates) >= 3 * N_SAMPLES:\n",
    "        break\n",
    "    passages = item.get('passages', {})\n",
    "    ptexts = passages.get('passage_text', [])\n",
    "    is_sel = passages.get('is_selected', [])\n",
    "    query = item.get('query', '')\n",
    "    answers = item.get('answers', [])\n",
    "    well_formed = item.get('wellFormedAnswers', [])\n",
    "    answer = None\n",
    "    if well_formed and len(well_formed) > 0 and well_formed[0] not in ('[]', ''):\n",
    "        answer = well_formed[0]\n",
    "    elif answers and len(answers) > 0 and answers[0] != 'No Answer Present.':\n",
    "        answer = answers[0]\n",
    "    if not answer:\n",
    "        continue\n",
    "    for pt, sel in zip(ptexts, is_sel):\n",
    "        wc = count_words(pt)\n",
    "        if sel == 1 and 30 <= wc <= 300:\n",
    "            all_candidates.append({\n",
    "                'passage': pt, 'query': query, 'answer': answer,\n",
    "                'word_count': wc,\n",
    "            })\n",
    "            break\n",
    "\n",
    "print(f\"Total candidates: {len(all_candidates)}\")\n",
    "np.random.seed(SEED)\n",
    "indices = np.random.permutation(len(all_candidates))\n",
    "samples = [all_candidates[i] for i in indices[:N_SAMPLES]]\n",
    "del ds, all_candidates\n",
    "gc.collect()\n",
    "\n",
    "for i, s in enumerate(samples):\n",
    "    rng = np.random.RandomState(SEED + i + 20000)\n",
    "    words = rng.choice(WORD_POOL, size=8, replace=False)\n",
    "    s['random_prefix'] = \" \".join(words)\n",
    "\n",
    "    q_words = set(re.sub(r'[^\\w\\s]', '', s['query'].lower()).split()) - STOP_WORDS\n",
    "    d_words = set(re.sub(r'[^\\w\\s]', '', s['passage'].lower()).split()) - STOP_WORDS\n",
    "    union = q_words | d_words\n",
    "    s['query_doc_overlap'] = len(q_words & d_words) / len(union) if len(union) > 0 else 0.0\n",
    "\n",
    "print(f\"Loaded {len(samples)} samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c4381bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-22T16:15:29.470191Z",
     "iopub.status.busy": "2026-02-22T16:15:29.469706Z",
     "iopub.status.idle": "2026-02-22T16:15:29.485345Z",
     "shell.execute_reply": "2026-02-22T16:15:29.484677Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scoring function defined (with attention probing).\n",
      "Probing 13 layers per condition.\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: score_sample_with_attention()\n",
    "#\n",
    "# Phase A: Forward with output_attentions=True, extract attention stats\n",
    "# Phase B: Forward with cached KVs (truncated), compute NLL\n",
    "#\n",
    "# For each probed layer, compute (averaged over heads):\n",
    "#   - frac_bos: fraction of doc-token attention going to BOS\n",
    "#   - frac_prime: fraction going to prime positions\n",
    "#   - frac_doc: fraction going to other doc positions\n",
    "#   - entropy: attention entropy for doc tokens\n",
    "\n",
    "def score_sample_with_attention(model, tokenizer, sample, device, probe_layers):\n",
    "    passage = sample['passage']\n",
    "    query = sample['query']\n",
    "    answer = sample['answer']\n",
    "    random_prefix = sample['random_prefix']\n",
    "\n",
    "    bos_id = tokenizer.bos_token_id\n",
    "\n",
    "    doc_ids = tokenizer(passage, add_special_tokens=False, truncation=True,\n",
    "                        max_length=1024).input_ids\n",
    "    query_ids = tokenizer(query, add_special_tokens=False, truncation=True,\n",
    "                          max_length=512).input_ids\n",
    "    answer_ids = tokenizer(answer, add_special_tokens=False, truncation=True,\n",
    "                           max_length=256).input_ids\n",
    "\n",
    "    if len(answer_ids) == 0:\n",
    "        return None\n",
    "\n",
    "    oracle_ids = tokenizer(query, add_special_tokens=False, truncation=True,\n",
    "                           max_length=256).input_ids\n",
    "    random_ids = tokenizer(random_prefix, add_special_tokens=False).input_ids\n",
    "\n",
    "    prefix_map = {\n",
    "        \"bare\": [],\n",
    "        \"random\": random_ids,\n",
    "        \"oracle\": oracle_ids,\n",
    "    }\n",
    "\n",
    "    n_q = len(query_ids)\n",
    "    n_a = len(answer_ids)\n",
    "    n_d = len(doc_ids)\n",
    "\n",
    "    targets = torch.tensor(answer_ids, dtype=torch.long, device=device)\n",
    "    result = {'n_doc': n_d, 'n_query': n_q}\n",
    "\n",
    "    for cond_name in CONDITIONS:\n",
    "        surr_ids = prefix_map[cond_name]\n",
    "        n_s = len(surr_ids)\n",
    "        n_prefix = 1 + n_s + n_d\n",
    "        doc_start = 1 + n_s  # first doc token position\n",
    "\n",
    "        # === Phase A: with attention extraction ===\n",
    "        prefix_tokens = [bos_id] + surr_ids + doc_ids\n",
    "        prefix_input = torch.tensor([prefix_tokens], dtype=torch.long, device=device)\n",
    "\n",
    "        phase_a_mask = make_phase_a_mask(n_s, n_d)\n",
    "        phase_a_dict = make_mask_dict(phase_a_mask.to(device))\n",
    "        phase_a_pos = torch.arange(n_prefix, device=device).unsqueeze(0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            out_a = model(input_ids=prefix_input, attention_mask=phase_a_dict,\n",
    "                          position_ids=phase_a_pos, use_cache=True,\n",
    "                          output_attentions=True)\n",
    "        past_kv = out_a.past_key_values\n",
    "        attentions = out_a.attentions  # tuple of (1, n_heads, n_prefix, n_prefix)\n",
    "\n",
    "        # Extract attention statistics for doc tokens\n",
    "        for layer_idx in probe_layers:\n",
    "            if layer_idx >= len(attentions):\n",
    "                continue\n",
    "            attn = attentions[layer_idx][0]  # (n_heads, n_prefix, n_prefix)\n",
    "\n",
    "            # Doc tokens: positions doc_start .. n_prefix-1\n",
    "            if n_d == 0:\n",
    "                continue\n",
    "            doc_attn = attn[:, doc_start:, :]  # (n_heads, n_d, n_prefix)\n",
    "\n",
    "            # Softmax already applied by model, so rows sum to 1\n",
    "            # Fraction of attention to each region\n",
    "            frac_bos = doc_attn[:, :, 0].mean().item()\n",
    "\n",
    "            if n_s > 0:\n",
    "                frac_prime = doc_attn[:, :, 1:doc_start].sum(dim=-1).mean().item()\n",
    "            else:\n",
    "                frac_prime = 0.0\n",
    "\n",
    "            frac_doc = doc_attn[:, :, doc_start:].sum(dim=-1).mean().item()\n",
    "\n",
    "            # Attention entropy (over full context, averaged over heads and doc tokens)\n",
    "            # Add small epsilon to avoid log(0)\n",
    "            eps = 1e-10\n",
    "            ent = -(doc_attn * (doc_attn + eps).log()).sum(dim=-1).mean().item()\n",
    "\n",
    "            result[f'{cond_name}_L{layer_idx}_frac_bos'] = frac_bos\n",
    "            result[f'{cond_name}_L{layer_idx}_frac_prime'] = frac_prime\n",
    "            result[f'{cond_name}_L{layer_idx}_frac_doc'] = frac_doc\n",
    "            result[f'{cond_name}_L{layer_idx}_entropy'] = ent\n",
    "\n",
    "        del attentions\n",
    "\n",
    "        # === Phase B: NLL (truncated) ===\n",
    "        cont_tokens = query_ids + answer_ids\n",
    "        n_cont = len(cont_tokens)\n",
    "        cont_input = torch.tensor([cont_tokens], dtype=torch.long, device=device)\n",
    "\n",
    "        phase_b_mask = make_phase_b_mask(n_s, n_d, n_q, n_a)\n",
    "        phase_b_dict = make_mask_dict(phase_b_mask.to(device))\n",
    "        phase_b_pos = torch.arange(n_prefix, n_prefix + n_cont,\n",
    "                                    device=device).unsqueeze(0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            out_b = model(input_ids=cont_input, attention_mask=phase_b_dict,\n",
    "                          position_ids=phase_b_pos, past_key_values=past_kv)\n",
    "\n",
    "        answer_logits = out_b.logits[0, n_q - 1 : n_q + n_a - 1, :]\n",
    "        log_probs = F.log_softmax(answer_logits, dim=-1)\n",
    "        token_nlls = -log_probs.gather(1, targets.unsqueeze(1)).squeeze(1)\n",
    "        result[f'nll_{cond_name}'] = token_nlls.mean().item()\n",
    "\n",
    "        del out_a, out_b, past_kv, prefix_input, cont_input\n",
    "        del phase_a_mask, phase_b_mask, phase_a_dict, phase_b_dict\n",
    "        del answer_logits, log_probs, token_nlls\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "print(f\"Scoring function defined (with attention probing).\")\n",
    "print(f\"Probing {len(PROBE_LAYERS)} layers per condition.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "517f9128",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-22T16:15:29.488443Z",
     "iopub.status.busy": "2026-02-22T16:15:29.488181Z",
     "iopub.status.idle": "2026-02-22T16:21:44.791795Z",
     "shell.execute_reply": "2026-02-22T16:21:44.790861Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "MAIN SCORING LOOP (with attention probing)\n",
      "======================================================================\n",
      "Starting fresh: 500 samples x 3 conditions\n",
      "Attention probing: 13 layers\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50b9ee8c8fba42339350ca59e84620a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Scoring+Attn:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done: 500 samples in 6.3 min\n",
      "\n",
      "NLL summary:\n",
      "  bare       NLL=2.9572\n",
      "  random     NLL=2.2979\n",
      "  oracle     NLL=1.9678\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Main scoring loop\n",
    "from lib.data import count_words as _cw\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"MAIN SCORING LOOP (with attention probing)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "CKPT_PATH = RESULTS_DIR / \"checkpoint.json\"\n",
    "\n",
    "all_results = []\n",
    "start_idx = 0\n",
    "if CKPT_PATH.exists():\n",
    "    ckpt = json.loads(CKPT_PATH.read_text())\n",
    "    if len(ckpt.get('results', [])) > 0:\n",
    "        saved_queries = [r['query'][:50] for r in ckpt['results']]\n",
    "        current_queries = [s['query'][:50] for s in samples[:len(saved_queries)]]\n",
    "        if saved_queries == current_queries:\n",
    "            all_results = ckpt['results']\n",
    "            start_idx = len(all_results)\n",
    "            print(f\"Resuming from checkpoint: {start_idx}/{N_SAMPLES}\")\n",
    "\n",
    "if start_idx == 0:\n",
    "    print(f\"Starting fresh: {N_SAMPLES} samples x {len(CONDITIONS)} conditions\")\n",
    "    print(f\"Attention probing: {len(PROBE_LAYERS)} layers\")\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "for i in tqdm(range(start_idx, N_SAMPLES), initial=start_idx, total=N_SAMPLES,\n",
    "              desc=\"Scoring+Attn\"):\n",
    "    s = samples[i]\n",
    "    try:\n",
    "        result = score_sample_with_attention(model, tokenizer, s, DEVICE, PROBE_LAYERS)\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR at sample {i}: {e}\")\n",
    "        import traceback; traceback.print_exc()\n",
    "        result = None\n",
    "\n",
    "    if result is None:\n",
    "        continue\n",
    "\n",
    "    result['query'] = s['query'][:50]\n",
    "    result['query_doc_overlap'] = s['query_doc_overlap']\n",
    "    result['answer_wc'] = _cw(s['answer'])\n",
    "    result['doc_wc'] = s['word_count']\n",
    "    all_results.append(result)\n",
    "\n",
    "    if (i + 1) % 25 == 0 or i == N_SAMPLES - 1:\n",
    "        ckpt = {\n",
    "            'model': MODEL_NAME,\n",
    "            'n_total': N_SAMPLES,\n",
    "            'n_conditions': len(CONDITIONS),\n",
    "            'probe_layers': PROBE_LAYERS,\n",
    "            'results': all_results,\n",
    "            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        }\n",
    "        CKPT_PATH.write_text(json.dumps(ckpt))\n",
    "\n",
    "    if (i + 1) % 50 == 0:\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "elapsed = time.time() - t0\n",
    "print(f\"\\nDone: {len(all_results)} samples in {elapsed/60:.1f} min\")\n",
    "\n",
    "print(f\"\\nNLL summary:\")\n",
    "for cn in CONDITIONS:\n",
    "    vals = [r[f'nll_{cn}'] for r in all_results]\n",
    "    print(f\"  {cn:<10} NLL={np.mean(vals):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91eaf9d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-22T16:21:44.795506Z",
     "iopub.status.busy": "2026-02-22T16:21:44.795198Z",
     "iopub.status.idle": "2026-02-22T16:21:44.885691Z",
     "shell.execute_reply": "2026-02-22T16:21:44.884727Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "RESULTS: ATTENTION PROBING\n",
      "======================================================================\n",
      "\n",
      "--- A. NLL Replication (500 samples) ---\n",
      "\n",
      "  d_oracle vs bare: +0.452\n",
      "  d_random vs bare: +0.475\n",
      "  d_oracle vs random (semantic): +0.266 (p=4.94e-09)\n",
      "\n",
      "--- B. Doc-Token Attention Fractions by Layer ---\n",
      "\n",
      "  (fraction of doc-token attention going to BOS / prime / other-doc)\n",
      "  bare has no prime positions, so frac_prime=0 by definition.\n",
      "\n",
      "   Layer |        --- bare ---        |       --- random ---       |       --- oracle ---      \n",
      "         |      BOS      doc      ent |      BOS    prime      ent |      BOS    prime      ent\n",
      "  ------------------------------------------------------------------------------------------\n",
      "  L   0 |   0.2537   0.7462    1.989 |   0.2249   0.0968    2.178 |   0.2298   0.0886    2.140\n",
      "  L   4 |   0.5922   0.4078    1.053 |   0.5581   0.0349    1.159 |   0.5665   0.0260    1.123\n",
      "  L   8 |   0.5709   0.4290    1.082 |   0.5530   0.0207    1.154 |   0.5558   0.0231    1.138\n",
      "  L  12 |   0.4929   0.5072    1.583 |   0.4522   0.0625    1.758 |   0.4674   0.0549    1.680\n",
      "  L  16 |   0.3520   0.6480    1.880 |   0.3071   0.0372    2.025 |   0.3200   0.0425    1.993\n",
      "  L  20 |   0.3909   0.6092    2.009 |   0.3588   0.0326    2.120 |   0.3550   0.0609    2.154\n",
      "  L  24 |   0.3778   0.6223    1.956 |   0.3536   0.0240    2.066 |   0.3507   0.0346    2.078\n",
      "  L  28 |   0.5455   0.4546    1.466 |   0.5268   0.0208    1.550 |   0.5266   0.0374    1.562\n",
      "  L  32 |   0.7895   0.2104    0.765 |   0.7807   0.0133    0.822 |   0.7805   0.0168    0.814\n",
      "  L  36 |   0.7975   0.2024    0.833 |   0.7850   0.0184    0.906 |   0.7870   0.0281    0.894\n",
      "  L  40 |   0.8555   0.1446    0.580 |   0.8471   0.0095    0.629 |   0.8485   0.0078    0.615\n",
      "  L  44 |   0.6978   0.3021    1.053 |   0.6876   0.0174    1.130 |   0.6844   0.0194    1.122\n",
      "  L  47 |   0.6666   0.3334    1.083 |   0.6278   0.0478    1.303 |   0.6353   0.0494    1.241\n",
      "\n",
      "--- C. Oracle vs Random: Does Prime Type Change Attention? ---\n",
      "\n",
      "  For each layer: paired t-test on frac_prime (oracle vs random)\n",
      "  Positive d means oracle prime gets MORE doc-token attention than random prime.\n",
      "\n",
      "   Layer  d(frac_prime)            p   sig   d(entropy)            p   sig\n",
      "  ---------------------------------------------------------------------------\n",
      "  L   0         -0.353     1.92e-14   ***       -0.769     2.37e-52   ***\n",
      "  L   4         -1.190     9.44e-98   ***       -1.616    2.22e-141   ***\n",
      "  L   8         +0.416     4.31e-19   ***       -0.705     9.28e-46   ***\n",
      "  L  12         -0.514     2.97e-27   ***       -1.762    2.89e-155   ***\n",
      "  L  16         +0.402     5.53e-18   ***       -0.793     5.89e-55   ***\n",
      "  L  20         +1.342    8.29e-114   ***       +0.554     6.67e-31   ***\n",
      "  L  24         +1.095     1.65e-87   ***       +0.289     2.37e-10   ***\n",
      "  L  28         +1.343    5.92e-114   ***       +0.275     1.62e-09   ***\n",
      "  L  32         +0.672     2.48e-42   ***       -0.305     2.49e-11   ***\n",
      "  L  36         +1.197     1.81e-98   ***       -0.347     4.88e-14   ***\n",
      "  L  40         -0.618     4.99e-37   ***       -0.493     1.92e-25   ***\n",
      "  L  44         +0.371     1.09e-15   ***       -0.330     6.27e-13   ***\n",
      "  L  47         +0.120     7.40e-03    **       -0.779     2.15e-53   ***\n",
      "\n",
      "--- D. BOS Attention Sink: Bare vs Primed ---\n",
      "\n",
      "  Does adding a prime reduce BOS absorption? (bare has no prime -> more BOS)\n",
      "  Positive d = bare has MORE BOS attention.\n",
      "\n",
      "   Layer   bare_bos   rand_bos   orac_bos   d(bare-rand)            p   sig\n",
      "  ---------------------------------------------------------------------------\n",
      "  L   0     0.2537     0.2249     0.2298         +3.028    1.18e-253   ***\n",
      "  L   4     0.5922     0.5581     0.5665         +3.261    3.09e-268   ***\n",
      "  L   8     0.5709     0.5530     0.5558         +2.505    2.54e-217   ***\n",
      "  L  12     0.4929     0.4522     0.4674         +2.816    1.34e-239   ***\n",
      "  L  16     0.3520     0.3071     0.3200         +3.345    2.51e-273   ***\n",
      "  L  20     0.3909     0.3588     0.3550         +2.095    7.50e-185   ***\n",
      "  L  24     0.3778     0.3536     0.3507         +2.206    5.32e-194   ***\n",
      "  L  28     0.5455     0.5268     0.5266         +2.010    1.40e-177   ***\n",
      "  L  32     0.7895     0.7807     0.7805         +1.513    3.09e-131   ***\n",
      "  L  36     0.7975     0.7850     0.7870         +1.861    2.06e-164   ***\n",
      "  L  40     0.8555     0.8471     0.8485         +1.197     1.59e-98   ***\n",
      "  L  44     0.6978     0.6876     0.6844         +1.776    1.25e-156   ***\n",
      "  L  47     0.6666     0.6278     0.6353         +2.356    5.88e-206   ***\n",
      "\n",
      "--- E. Attention-to-Prime x NLL Benefit Correlation ---\n",
      "\n",
      "  Does more doc-to-prime attention predict better NLL?\n",
      "  Use last probed layer for this analysis.\n",
      "\n",
      "  random: frac_prime x nll_benefit -> r=+0.167, p=1.69e-04 ***\n",
      "  oracle: frac_prime x nll_benefit -> r=+0.134, p=2.65e-03 **\n",
      "\n",
      "  Attention diff (oracle-random) x NLL diff (oracle-random):\n",
      "    r=+0.031, p=4.91e-01 ns\n",
      "    (positive r = samples where oracle gets more attention also show more NLL benefit)\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Attention analysis\n",
    "print(\"=\" * 70)\n",
    "print(\"RESULTS: ATTENTION PROBING\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "N = len(all_results)\n",
    "\n",
    "# --- A. NLL replication ---\n",
    "nll = {}\n",
    "for cn in CONDITIONS:\n",
    "    nll[cn] = np.array([r[f'nll_{cn}'] for r in all_results])\n",
    "\n",
    "print(f\"\\n--- A. NLL Replication ({N} samples) ---\\n\")\n",
    "d_oracle = cohens_d(nll['bare'] - nll['oracle'])\n",
    "d_random = cohens_d(nll['bare'] - nll['random'])\n",
    "d_sem = cohens_d(nll['random'] - nll['oracle'])\n",
    "_, p_sem = stats.ttest_1samp(nll['random'] - nll['oracle'], 0)\n",
    "print(f\"  d_oracle vs bare: {d_oracle:+.3f}\")\n",
    "print(f\"  d_random vs bare: {d_random:+.3f}\")\n",
    "print(f\"  d_oracle vs random (semantic): {d_sem:+.3f} (p={p_sem:.2e})\")\n",
    "\n",
    "# --- B. Layer-by-layer attention fractions ---\n",
    "print(f\"\\n--- B. Doc-Token Attention Fractions by Layer ---\\n\")\n",
    "print(f\"  (fraction of doc-token attention going to BOS / prime / other-doc)\")\n",
    "print(f\"  bare has no prime positions, so frac_prime=0 by definition.\\n\")\n",
    "\n",
    "print(f\"  {'Layer':>6} | {'--- bare ---':^26} | {'--- random ---':^26} | {'--- oracle ---':^26}\")\n",
    "print(f\"  {'':>6} | {'BOS':>8} {'doc':>8} {'ent':>8} | {'BOS':>8} {'prime':>8} {'ent':>8} | {'BOS':>8} {'prime':>8} {'ent':>8}\")\n",
    "print(f\"  {'-'*90}\")\n",
    "\n",
    "layer_data = {}\n",
    "for layer_idx in PROBE_LAYERS:\n",
    "    layer_data[layer_idx] = {}\n",
    "    for cn in CONDITIONS:\n",
    "        bos_key = f'{cn}_L{layer_idx}_frac_bos'\n",
    "        prime_key = f'{cn}_L{layer_idx}_frac_prime'\n",
    "        doc_key = f'{cn}_L{layer_idx}_frac_doc'\n",
    "        ent_key = f'{cn}_L{layer_idx}_entropy'\n",
    "\n",
    "        # Check if keys exist (some layers might be missing)\n",
    "        if bos_key not in all_results[0]:\n",
    "            continue\n",
    "\n",
    "        bos_vals = np.array([r[bos_key] for r in all_results])\n",
    "        prime_vals = np.array([r[prime_key] for r in all_results])\n",
    "        doc_vals = np.array([r[doc_key] for r in all_results])\n",
    "        ent_vals = np.array([r[ent_key] for r in all_results])\n",
    "\n",
    "        layer_data[layer_idx][cn] = {\n",
    "            'bos': bos_vals,\n",
    "            'prime': prime_vals,\n",
    "            'doc': doc_vals,\n",
    "            'entropy': ent_vals,\n",
    "        }\n",
    "\n",
    "    if not layer_data[layer_idx]:\n",
    "        continue\n",
    "\n",
    "    bare = layer_data[layer_idx].get('bare', {})\n",
    "    rand = layer_data[layer_idx].get('random', {})\n",
    "    orac = layer_data[layer_idx].get('oracle', {})\n",
    "\n",
    "    if bare and rand and orac:\n",
    "        print(f\"  L{layer_idx:>4} | \"\n",
    "              f\"{bare['bos'].mean():>8.4f} {bare['doc'].mean():>8.4f} {bare['entropy'].mean():>8.3f} | \"\n",
    "              f\"{rand['bos'].mean():>8.4f} {rand['prime'].mean():>8.4f} {rand['entropy'].mean():>8.3f} | \"\n",
    "              f\"{orac['bos'].mean():>8.4f} {orac['prime'].mean():>8.4f} {orac['entropy'].mean():>8.3f}\")\n",
    "\n",
    "# --- C. Oracle vs Random attention comparison ---\n",
    "print(f\"\\n--- C. Oracle vs Random: Does Prime Type Change Attention? ---\\n\")\n",
    "print(f\"  For each layer: paired t-test on frac_prime (oracle vs random)\")\n",
    "print(f\"  Positive d means oracle prime gets MORE doc-token attention than random prime.\\n\")\n",
    "\n",
    "print(f\"  {'Layer':>6} {'d(frac_prime)':>14} {'p':>12} {'sig':>5} {'d(entropy)':>12} {'p':>12} {'sig':>5}\")\n",
    "print(f\"  {'-'*75}\")\n",
    "\n",
    "for layer_idx in PROBE_LAYERS:\n",
    "    if layer_idx not in layer_data:\n",
    "        continue\n",
    "    rand = layer_data[layer_idx].get('random', {})\n",
    "    orac = layer_data[layer_idx].get('oracle', {})\n",
    "    if not rand or not orac:\n",
    "        continue\n",
    "\n",
    "    # Compare frac_prime: oracle vs random\n",
    "    diff_prime = orac['prime'] - rand['prime']\n",
    "    d_prime = cohens_d(diff_prime)\n",
    "    _, p_prime = stats.ttest_1samp(diff_prime, 0)\n",
    "    sig_prime = '***' if p_prime < 0.001 else '**' if p_prime < 0.01 else '*' if p_prime < 0.05 else 'ns'\n",
    "\n",
    "    # Compare entropy\n",
    "    diff_ent = orac['entropy'] - rand['entropy']\n",
    "    d_ent = cohens_d(diff_ent)\n",
    "    _, p_ent = stats.ttest_1samp(diff_ent, 0)\n",
    "    sig_ent = '***' if p_ent < 0.001 else '**' if p_ent < 0.01 else '*' if p_ent < 0.05 else 'ns'\n",
    "\n",
    "    print(f\"  L{layer_idx:>4} {d_prime:>+14.3f} {p_prime:>12.2e} {sig_prime:>5} \"\n",
    "          f\"{d_ent:>+12.3f} {p_ent:>12.2e} {sig_ent:>5}\")\n",
    "\n",
    "# --- D. BOS attention: bare vs primed ---\n",
    "print(f\"\\n--- D. BOS Attention Sink: Bare vs Primed ---\\n\")\n",
    "print(f\"  Does adding a prime reduce BOS absorption? (bare has no prime -> more BOS)\")\n",
    "print(f\"  Positive d = bare has MORE BOS attention.\\n\")\n",
    "\n",
    "print(f\"  {'Layer':>6} {'bare_bos':>10} {'rand_bos':>10} {'orac_bos':>10} {'d(bare-rand)':>14} {'p':>12} {'sig':>5}\")\n",
    "print(f\"  {'-'*75}\")\n",
    "\n",
    "for layer_idx in PROBE_LAYERS:\n",
    "    if layer_idx not in layer_data:\n",
    "        continue\n",
    "    bare = layer_data[layer_idx].get('bare', {})\n",
    "    rand = layer_data[layer_idx].get('random', {})\n",
    "    orac = layer_data[layer_idx].get('oracle', {})\n",
    "    if not bare or not rand:\n",
    "        continue\n",
    "\n",
    "    diff_bos = bare['bos'] - rand['bos']\n",
    "    d_bos = cohens_d(diff_bos)\n",
    "    _, p_bos = stats.ttest_1samp(diff_bos, 0)\n",
    "    sig_bos = '***' if p_bos < 0.001 else '**' if p_bos < 0.01 else '*' if p_bos < 0.05 else 'ns'\n",
    "\n",
    "    print(f\"  L{layer_idx:>4} {bare['bos'].mean():>10.4f} {rand['bos'].mean():>10.4f} \"\n",
    "          f\"{orac['bos'].mean():>10.4f} {d_bos:>+14.3f} {p_bos:>12.2e} {sig_bos:>5}\")\n",
    "\n",
    "# --- E. Correlation: attention to prime x NLL benefit ---\n",
    "print(f\"\\n--- E. Attention-to-Prime x NLL Benefit Correlation ---\\n\")\n",
    "print(f\"  Does more doc-to-prime attention predict better NLL?\")\n",
    "print(f\"  Use last probed layer for this analysis.\\n\")\n",
    "\n",
    "last_layer = PROBE_LAYERS[-1]\n",
    "for cn in ['random', 'oracle']:\n",
    "    if cn not in layer_data.get(last_layer, {}):\n",
    "        continue\n",
    "    frac_prime = layer_data[last_layer][cn]['prime']\n",
    "    nll_benefit = nll['bare'] - nll[cn]  # positive = condition helps\n",
    "\n",
    "    r, p = stats.pearsonr(frac_prime, nll_benefit)\n",
    "    sig = '***' if p < 0.001 else '**' if p < 0.01 else '*' if p < 0.05 else 'ns'\n",
    "    print(f\"  {cn}: frac_prime x nll_benefit -> r={r:+.3f}, p={p:.2e} {sig}\")\n",
    "\n",
    "# Also correlate oracle-random attention difference with oracle-random NLL difference\n",
    "if 'random' in layer_data.get(last_layer, {}) and 'oracle' in layer_data.get(last_layer, {}):\n",
    "    attn_diff = layer_data[last_layer]['oracle']['prime'] - layer_data[last_layer]['random']['prime']\n",
    "    nll_diff = nll['random'] - nll['oracle']  # positive = oracle better\n",
    "    r, p = stats.pearsonr(attn_diff, nll_diff)\n",
    "    sig = '***' if p < 0.001 else '**' if p < 0.01 else '*' if p < 0.05 else 'ns'\n",
    "    print(f\"\\n  Attention diff (oracle-random) x NLL diff (oracle-random):\")\n",
    "    print(f\"    r={r:+.3f}, p={p:.2e} {sig}\")\n",
    "    print(f\"    (positive r = samples where oracle gets more attention also show more NLL benefit)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4977626",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-22T16:21:44.889536Z",
     "iopub.status.busy": "2026-02-22T16:21:44.888834Z",
     "iopub.status.idle": "2026-02-22T16:21:44.913894Z",
     "shell.execute_reply": "2026-02-22T16:21:44.912938Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "SUMMARY -- Prefix LM Exp 04f: Attention Probing\n",
      "======================================================================\n",
      "\n",
      "  Late layers used for summary: [36, 40, 44, 47]\n",
      "  random: mean_bos=0.7369, mean_prime=0.0233\n",
      "  oracle: mean_bos=0.7388, mean_prime=0.0262\n",
      "\n",
      "  VERDICT:\n",
      "  Oracle vs random frac_prime differs in 13/13 probed layers.\n",
      "  Oracle DOES attract different attention than random.\n",
      "  The semantic signal exists in attention but doesn't survive truncation.\n",
      "\n",
      "Results saved to ../../../results/prefix_lm_exp04f/results.json\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Save results + verdict\n",
    "print(\"=\" * 70)\n",
    "print(\"SUMMARY -- Prefix LM Exp 04f: Attention Probing\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "d_oracle_v_bare = cohens_d(nll['bare'] - nll['oracle'])\n",
    "d_random_v_bare = cohens_d(nll['bare'] - nll['random'])\n",
    "\n",
    "# Summary: average attention fractions across late layers\n",
    "late_layers = [l for l in PROBE_LAYERS if l >= PROBE_LAYERS[-1] - 12]\n",
    "print(f\"\\n  Late layers used for summary: {late_layers}\")\n",
    "\n",
    "for cn in ['random', 'oracle']:\n",
    "    bos_vals = []\n",
    "    prime_vals = []\n",
    "    for l in late_layers:\n",
    "        if cn in layer_data.get(l, {}):\n",
    "            bos_vals.append(layer_data[l][cn]['bos'].mean())\n",
    "            prime_vals.append(layer_data[l][cn]['prime'].mean())\n",
    "    if bos_vals:\n",
    "        print(f\"  {cn}: mean_bos={np.mean(bos_vals):.4f}, mean_prime={np.mean(prime_vals):.4f}\")\n",
    "\n",
    "print(f\"\\n  VERDICT:\")\n",
    "# Check if oracle attention pattern differs from random\n",
    "n_sig_layers = 0\n",
    "for layer_idx in PROBE_LAYERS:\n",
    "    rand = layer_data.get(layer_idx, {}).get('random', {})\n",
    "    orac = layer_data.get(layer_idx, {}).get('oracle', {})\n",
    "    if not rand or not orac:\n",
    "        continue\n",
    "    diff = orac['prime'] - rand['prime']\n",
    "    _, p = stats.ttest_1samp(diff, 0)\n",
    "    if p < 0.05:\n",
    "        n_sig_layers += 1\n",
    "\n",
    "total_layers = len([l for l in PROBE_LAYERS if 'random' in layer_data.get(l, {})])\n",
    "print(f\"  Oracle vs random frac_prime differs in {n_sig_layers}/{total_layers} probed layers.\")\n",
    "\n",
    "if n_sig_layers > total_layers * 0.5:\n",
    "    print(f\"  Oracle DOES attract different attention than random.\")\n",
    "    print(f\"  The semantic signal exists in attention but doesn't survive truncation.\")\n",
    "else:\n",
    "    print(f\"  Oracle attention pattern ~ random. No meaningful attention difference.\")\n",
    "    print(f\"  Content literally doesn't affect how doc tokens attend to the prime.\")\n",
    "\n",
    "# Save\n",
    "summary = {\n",
    "    'n_samples': N,\n",
    "    'model': MODEL_NAME,\n",
    "    'd_oracle': float(d_oracle_v_bare),\n",
    "    'd_random': float(d_random_v_bare),\n",
    "    'n_sig_layers': n_sig_layers,\n",
    "    'total_probed_layers': total_layers,\n",
    "    'probe_layers': PROBE_LAYERS,\n",
    "}\n",
    "\n",
    "final_results = {\n",
    "    'experiment': 'prefix_lm_exp04f',\n",
    "    'dataset': 'ms_marco_v1.1',\n",
    "    'model': MODEL_NAME,\n",
    "    'n_samples': N,\n",
    "    'seed': SEED,\n",
    "    'conditions': CONDITIONS,\n",
    "    'probe_layers': PROBE_LAYERS,\n",
    "    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'summary': summary,\n",
    "}\n",
    "\n",
    "with open(RESULTS_DIR / 'results.json', 'w') as f:\n",
    "    json.dump(final_results, f, indent=2)\n",
    "print(f\"\\nResults saved to {RESULTS_DIR / 'results.json'}\")\n",
    "print(\"Done!\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "06e020b1883e42569d549663998ecdb9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_a397da7cb77949328656fca8dc5ee05a",
       "placeholder": "​",
       "style": "IPY_MODEL_a15c3df304ed418e9f189b340f19cb63",
       "tabbable": null,
       "tooltip": null,
       "value": "Scoring+Attn: 100%"
      }
     },
     "1713aedcb7984ce599a827d09b464a51": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "39b0b6312d924605a0f72a667530d602": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "4aa2a50bc5274da69a70cea1a3c767fd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "4bfb1f7b0c1a44409d8a71ddef67c7f4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "50b9ee8c8fba42339350ca59e84620a0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_06e020b1883e42569d549663998ecdb9",
        "IPY_MODEL_bff19728798d4c88b45d4b125d346375",
        "IPY_MODEL_bec169a1beed480c9de2068c5a340486"
       ],
       "layout": "IPY_MODEL_1713aedcb7984ce599a827d09b464a51",
       "tabbable": null,
       "tooltip": null
      }
     },
     "5e6daa331cb94a32bbaf025e9da0afd9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "5ff7c236ad464e9c9287095ffbb6bdef": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_8390748f6f2142a9bc191d59f2a3e88b",
       "placeholder": "​",
       "style": "IPY_MODEL_4bfb1f7b0c1a44409d8a71ddef67c7f4",
       "tabbable": null,
       "tooltip": null,
       "value": " 1065/1065 [00:07&lt;00:00, 650.91it/s, Materializing param=model.vision_tower.vision_model.post_layernorm.weight]"
      }
     },
     "7d3b2dc2eb904437910e2bbe42acf13c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "805899d3f3df4aab8f4b19310730e6ee": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "8390748f6f2142a9bc191d59f2a3e88b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "85018a1d572741b4800e3f83feca14a9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "8c10d3f9c7af4a409d655c93a4bec07b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "8e729008e2e744a1af9bff00d063d15a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "9b8a2a54a7984cdf9e506771e840efbd": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a0d0058985f34fb280712fe4404464d5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_85018a1d572741b4800e3f83feca14a9",
       "placeholder": "​",
       "style": "IPY_MODEL_7d3b2dc2eb904437910e2bbe42acf13c",
       "tabbable": null,
       "tooltip": null,
       "value": "Loading weights: 100%"
      }
     },
     "a15c3df304ed418e9f189b340f19cb63": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "a397da7cb77949328656fca8dc5ee05a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b9595225153245a0abc78559232e3fb8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_39b0b6312d924605a0f72a667530d602",
       "max": 1065.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_8e729008e2e744a1af9bff00d063d15a",
       "tabbable": null,
       "tooltip": null,
       "value": 1065.0
      }
     },
     "bec169a1beed480c9de2068c5a340486": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_805899d3f3df4aab8f4b19310730e6ee",
       "placeholder": "​",
       "style": "IPY_MODEL_5e6daa331cb94a32bbaf025e9da0afd9",
       "tabbable": null,
       "tooltip": null,
       "value": " 500/500 [06:15&lt;00:00,  1.22it/s]"
      }
     },
     "bff19728798d4c88b45d4b125d346375": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_9b8a2a54a7984cdf9e506771e840efbd",
       "max": 500.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_4aa2a50bc5274da69a70cea1a3c767fd",
       "tabbable": null,
       "tooltip": null,
       "value": 500.0
      }
     },
     "d3687a77a7644612ab2ace6adb8ebb3f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_a0d0058985f34fb280712fe4404464d5",
        "IPY_MODEL_b9595225153245a0abc78559232e3fb8",
        "IPY_MODEL_5ff7c236ad464e9c9287095ffbb6bdef"
       ],
       "layout": "IPY_MODEL_8c10d3f9c7af4a409d655c93a4bec07b",
       "tabbable": null,
       "tooltip": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
