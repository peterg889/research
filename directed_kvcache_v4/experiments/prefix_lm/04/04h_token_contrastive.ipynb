{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2af3c0cf",
   "metadata": {},
   "source": [
    "# Prefix LM Exp 04h: Token Stratification & Contrastive Evaluation\n",
    "\n",
    "## Motivation\n",
    "\n",
    "Exp 04g showed all 5 mechanistic approaches to force prime information into document\n",
    "value vectors HURT performance. Under average NLL, random ~ oracle -- the structural\n",
    "interpretation holds.\n",
    "\n",
    "But average NLL weights all answer tokens equally. In \"The capital of France is\n",
    "**Paris**\", the word \"Paris\" requires document understanding while \"The\", \"of\", \"is\"\n",
    "are syntactically predictable. If the semantic signal (oracle vs random) is concentrated\n",
    "on hard/informative tokens, NLL averaging hides it.\n",
    "\n",
    "Additionally, NLL measures absolute prediction quality, not **discrimination** -- can the\n",
    "model distinguish the correct answer from a wrong one? Enrichment might improve\n",
    "discrimination without reducing average NLL.\n",
    "\n",
    "## Two Reframings\n",
    "\n",
    "**1. Token stratification**: Split answer tokens by difficulty (how surprising they are\n",
    "under the bare condition). Check if oracle beats random specifically on hard tokens.\n",
    "\n",
    "**2. Contrastive evaluation**: For each sample, evaluate BOTH a correct and a wrong\n",
    "answer. Measure the NLL gap (wrong - correct). Does enrichment increase the model's\n",
    "ability to prefer the correct answer?\n",
    "\n",
    "## Conditions (5)\n",
    "\n",
    "| # | Condition | Description |\n",
    "|---|-----------|-------------|\n",
    "| 1 | `no_doc` | No document -- `[BOS, query, answer]` single-pass causal. Model's prior. |\n",
    "| 2 | `bare` | `[BOS, doc]` cached, Phase B `[query, answer]` |\n",
    "| 3 | `random` | `[BOS, random_8w, doc]` cached, truncated Phase B |\n",
    "| 4 | `oracle` | `[BOS, query, doc]` cached, truncated Phase B |\n",
    "| 5 | `oracle_plus_vocab` | `[BOS, query+answer_vocab, doc]` cached, truncated Phase B |\n",
    "\n",
    "## Wrong Answer Assignment\n",
    "\n",
    "For each sample i, the wrong answer is the gold answer from sample `(i + 250) % 500`.\n",
    "Deterministic, well-separated pairings.\n",
    "\n",
    "## Key Analyses\n",
    "\n",
    "- **A**: Token stratification by difficulty (bare NLL quartiles)\n",
    "- **B**: Token stratification by document dependence (no_doc - bare quartiles)\n",
    "- **C**: Content word vs function word effects\n",
    "- **D**: Contrastive evaluation (NLL gap, AUC, prior-controlled discrimination)\n",
    "- **E**: Contrastive x difficulty interaction\n",
    "- **F**: Summary + replication of prior results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80ca1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Setup\n",
    "import os\n",
    "os.umask(0o000)\n",
    "\n",
    "import sys, json, time, gc, re\n",
    "import random as pyrandom\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "from scipy import stats\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "sys.path.insert(0, \"../../..\")\n",
    "from lib.analysis import cohens_d\n",
    "\n",
    "SEED = 42\n",
    "N_SAMPLES = 500\n",
    "\n",
    "MODEL_NAME = \"google/gemma-3-12b-it\"\n",
    "\n",
    "RESULTS_DIR = Path(\"../../../results/prefix_lm_exp04h\")\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "pyrandom.seed(SEED)\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv())\n",
    "HF_TOKEN = os.environ.get(\"HF_TOKEN\")\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "CONDITIONS = [\"no_doc\", \"bare\", \"random\", \"oracle\", \"oracle_plus_vocab\"]\n",
    "TWO_PASS_CONDITIONS = [\"bare\", \"random\", \"oracle\", \"oracle_plus_vocab\"]\n",
    "\n",
    "print(f\"Prefix LM Exp 04h: Token Stratification & Contrastive Evaluation\")\n",
    "print(f\"N: {N_SAMPLES}, Conditions: {len(CONDITIONS)}\")\n",
    "print(f\"DEVICE: {DEVICE}\")\n",
    "print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "print(f\"\\nConditions: {CONDITIONS}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20623e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Load model + tokenizer\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import transformers\n",
    "\n",
    "print(f\"transformers version: {transformers.__version__}\")\n",
    "\n",
    "print(f\"Loading {MODEL_NAME}...\")\n",
    "t0 = time.time()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, token=HF_TOKEN)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    attn_implementation=\"eager\",\n",
    "    token=HF_TOKEN,\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "n_params = sum(p.numel() for p in model.parameters()) / 1e9\n",
    "gpu_mem = torch.cuda.memory_allocated() / 1e9\n",
    "print(f\"Loaded: {n_params:.1f}B params, {gpu_mem:.1f} GB GPU, {time.time()-t0:.0f}s\")\n",
    "print(f\"BOS token id: {tokenizer.bos_token_id}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4fcadc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Mask functions + sanity check\n",
    "\n",
    "def make_causal_mask(n, dtype=torch.bfloat16):\n",
    "    # Standard causal mask: lower triangle = 0, upper triangle = min_val\n",
    "    min_val = torch.finfo(dtype).min\n",
    "    mask = torch.triu(torch.full((n, n), min_val, dtype=dtype), diagonal=1)\n",
    "    return mask.unsqueeze(0).unsqueeze(0)  # (1, 1, n, n)\n",
    "\n",
    "\n",
    "def make_phase_b_mask(n_s, n_d, n_q, n_a, dtype=torch.bfloat16):\n",
    "    # Phase B: continuation [query, answer] sees [BOS, doc] but NOT prime (truncation)\n",
    "    n_prefix = 1 + n_s + n_d\n",
    "    n_cont = n_q + n_a\n",
    "    min_val = torch.finfo(dtype).min\n",
    "    mask = torch.full((n_cont, n_prefix + n_cont), min_val, dtype=dtype)\n",
    "    # Allow attending to all prefix positions\n",
    "    mask[:, :n_prefix] = 0.0\n",
    "    # Mask out prime positions (truncation)\n",
    "    if n_s > 0:\n",
    "        mask[:, 1:1 + n_s] = min_val\n",
    "    # Causal mask for continuation tokens among themselves\n",
    "    mask[:, n_prefix:] = torch.triu(\n",
    "        torch.full((n_cont, n_cont), min_val, dtype=dtype), diagonal=1\n",
    "    )\n",
    "    return mask.unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "\n",
    "def make_mask_dict(mask_4d):\n",
    "    return {\"full_attention\": mask_4d, \"sliding_attention\": mask_4d}\n",
    "\n",
    "\n",
    "# --- Sanity check ---\n",
    "print(\"Mask sanity check: custom causal mask vs default forward...\")\n",
    "test_text = \"The quick brown fox jumps over the lazy dog.\"\n",
    "test_ids = tokenizer(test_text, return_tensors=\"pt\",\n",
    "                     add_special_tokens=True).input_ids.to(DEVICE)\n",
    "Lt = test_ids.shape[1]\n",
    "\n",
    "with torch.no_grad():\n",
    "    out_default = model(input_ids=test_ids)\n",
    "\n",
    "causal_mask = make_causal_mask(Lt)\n",
    "causal_dict = make_mask_dict(causal_mask.to(DEVICE))\n",
    "causal_pos = torch.arange(Lt, device=DEVICE).unsqueeze(0)\n",
    "\n",
    "with torch.no_grad():\n",
    "    out_custom = model(input_ids=test_ids, attention_mask=causal_dict,\n",
    "                       position_ids=causal_pos)\n",
    "\n",
    "max_diff = (out_default.logits - out_custom.logits).abs().max().item()\n",
    "print(f\"  Max logit diff: {max_diff:.6f}\")\n",
    "assert max_diff < 0.1, f\"FAIL: max_diff={max_diff:.4f}\"\n",
    "print(f\"  PASS: Dict-based mask API verified.\")\n",
    "\n",
    "del out_default, out_custom\n",
    "gc.collect(); torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5fc455d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Load MS MARCO + prepare per-sample fields + wrong-answer pairing\n",
    "from lib.data import count_words\n",
    "from datasets import load_dataset\n",
    "\n",
    "STOP_WORDS = {\n",
    "    'a', 'an', 'the', 'is', 'are', 'was', 'were', 'be', 'been', 'being',\n",
    "    'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could',\n",
    "    'should', 'may', 'might', 'can', 'shall', 'to', 'of', 'in', 'for',\n",
    "    'on', 'with', 'at', 'by', 'from', 'as', 'into', 'through', 'during',\n",
    "    'before', 'after', 'above', 'below', 'between', 'and', 'but', 'or',\n",
    "    'not', 'no', 'if', 'then', 'than', 'so', 'up', 'out', 'about',\n",
    "    'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those',\n",
    "    'it', 'its', 'i', 'me', 'my', 'we', 'our', 'you', 'your', 'he',\n",
    "    'him', 'his', 'she', 'her', 'they', 'them', 'their', 'how', 'when',\n",
    "    'where', 'why', 'much', 'many', 'some', 'any', 'all', 'each',\n",
    "    'does', 'also', 'just', 'more', 'most', 'very', 'too', 'only',\n",
    "}\n",
    "\n",
    "WORD_POOL = [\n",
    "    \"computer\", \"mountain\", \"hospital\", \"children\", \"building\", \"national\",\n",
    "    \"business\", \"research\", \"students\", \"american\", \"possible\", \"economic\",\n",
    "    \"personal\", \"together\", \"products\", \"services\", \"actually\", \"remember\",\n",
    "    \"practice\", \"training\", \"industry\", \"complete\", \"critical\", \"function\",\n",
    "    \"language\", \"standard\", \"material\", \"original\", \"physical\", \"security\",\n",
    "    \"interest\", \"problems\", \"consider\", \"response\", \"pressure\", \"politics\",\n",
    "    \"movement\", \"evidence\", \"southern\", \"northern\", \"exchange\", \"decision\",\n",
    "    \"position\", \"increase\", \"describe\", \"military\", \"required\", \"approach\",\n",
    "    \"strategy\", \"customer\", \"resource\", \"employee\", \"audience\", \"location\",\n",
    "    \"property\", \"cultural\", \"activity\", \"strength\", \"analysis\", \"powerful\",\n",
    "    \"election\", \"argument\", \"campaign\", \"maintain\", \"question\", \"behavior\",\n",
    "    \"majority\", \"solution\", \"software\", \"consumer\", \"creative\", \"reaction\",\n",
    "    \"european\", \"delivery\", \"organize\", \"involved\", \"relative\", \"learning\",\n",
    "    \"positive\", \"numerous\", \"familiar\", \"engineer\", \"platform\", \"indicate\",\n",
    "    \"previous\", \"pleasure\", \"opposite\", \"magazine\", \"document\", \"religion\",\n",
    "    \"scenario\", \"workshop\", \"minority\", \"guidance\", \"estimate\", \"recently\",\n",
    "    \"surprise\", \"champion\", \"pleasant\", \"grateful\", \"moderate\", \"boundary\",\n",
    "]\n",
    "\n",
    "def content_words(text):\n",
    "    words = re.sub(r'[^\\w\\s]', '', text.lower()).split()\n",
    "    return [w for w in words if w not in STOP_WORDS and len(w) > 2]\n",
    "\n",
    "def jaccard(set_a, set_b):\n",
    "    union = set_a | set_b\n",
    "    return len(set_a & set_b) / len(union) if union else 0.0\n",
    "\n",
    "print(\"Loading MS MARCO v1.1 validation...\")\n",
    "ds = load_dataset(\"microsoft/ms_marco\", \"v1.1\", split=\"validation\")\n",
    "\n",
    "all_candidates = []\n",
    "for item in ds:\n",
    "    if len(all_candidates) >= 3 * N_SAMPLES:\n",
    "        break\n",
    "    passages = item.get('passages', {})\n",
    "    ptexts = passages.get('passage_text', [])\n",
    "    is_sel = passages.get('is_selected', [])\n",
    "    query = item.get('query', '')\n",
    "    answers = item.get('answers', [])\n",
    "    well_formed = item.get('wellFormedAnswers', [])\n",
    "    answer = None\n",
    "    if well_formed and len(well_formed) > 0 and well_formed[0] not in ('[]', ''):\n",
    "        answer = well_formed[0]\n",
    "    elif answers and len(answers) > 0 and answers[0] != 'No Answer Present.':\n",
    "        answer = answers[0]\n",
    "    if not answer:\n",
    "        continue\n",
    "    for pt, sel in zip(ptexts, is_sel):\n",
    "        wc = count_words(pt)\n",
    "        if sel == 1 and 30 <= wc <= 300:\n",
    "            all_candidates.append({\n",
    "                'passage': pt, 'query': query, 'answer': answer,\n",
    "                'word_count': wc,\n",
    "            })\n",
    "            break\n",
    "\n",
    "print(f\"Total candidates: {len(all_candidates)}\")\n",
    "np.random.seed(SEED)\n",
    "indices = np.random.permutation(len(all_candidates))\n",
    "samples = [all_candidates[i] for i in indices[:N_SAMPLES]]\n",
    "del ds, all_candidates\n",
    "gc.collect()\n",
    "\n",
    "# --- Prepare per-sample fields ---\n",
    "for i, s in enumerate(samples):\n",
    "    # Random prefix (8 words, same seed as all prior experiments)\n",
    "    rng = np.random.RandomState(SEED + i + 20000)\n",
    "    words = rng.choice(WORD_POOL, size=8, replace=False)\n",
    "    s['random_prefix'] = \" \".join(words)\n",
    "\n",
    "    # Query-doc overlap\n",
    "    q_words = set(content_words(s['query']))\n",
    "    d_words = set(content_words(s['passage']))\n",
    "    a_words = set(content_words(s['answer']))\n",
    "    s['query_doc_overlap'] = jaccard(q_words, d_words)\n",
    "\n",
    "    # Answer-doc overlap words (for oracle_plus_vocab)\n",
    "    overlap_words = sorted(a_words & d_words)\n",
    "    if not overlap_words:\n",
    "        overlap_words = content_words(s['answer'])[:5]\n",
    "    s['answer_vocab'] = \" \".join(overlap_words[:10])\n",
    "    s['oracle_plus_vocab'] = s['query'] + \" \" + s['answer_vocab']\n",
    "\n",
    "    s['answer_wc'] = count_words(s['answer'])\n",
    "\n",
    "# --- Wrong-answer pairing ---\n",
    "for i in range(N_SAMPLES):\n",
    "    j = (i + 250) % N_SAMPLES\n",
    "    samples[i]['wrong_answer'] = samples[j]['answer']\n",
    "\n",
    "print(f\"\\nLoaded {len(samples)} samples\")\n",
    "print(f\"Mean passage words: {np.mean([s['word_count'] for s in samples]):.0f}\")\n",
    "print(f\"Mean query words: {np.mean([count_words(s['query']) for s in samples]):.0f}\")\n",
    "print(f\"Mean answer words: {np.mean([s['answer_wc'] for s in samples]):.0f}\")\n",
    "\n",
    "# Verify wrong-answer pairing\n",
    "print(f\"\\n--- Wrong-answer examples ---\")\n",
    "for j in [0, 1, 250]:\n",
    "    print(f\"  Sample {j}: correct='{samples[j]['answer'][:50]}...'\")\n",
    "    print(f\"             wrong='{samples[j]['wrong_answer'][:50]}...'\")\n",
    "    assert samples[j]['wrong_answer'] == samples[(j + 250) % N_SAMPLES]['answer']\n",
    "print(f\"  Pairing verified.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eaa7019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: score_sample() -- 5 conditions x 2 answers, per-token NLLs\n",
    "\n",
    "def score_sample(model, tokenizer, sample, device):\n",
    "    passage = sample['passage']\n",
    "    query = sample['query']\n",
    "    correct_answer = sample['answer']\n",
    "    wrong_answer = sample['wrong_answer']\n",
    "\n",
    "    bos_id = tokenizer.bos_token_id\n",
    "\n",
    "    doc_ids = tokenizer(passage, add_special_tokens=False, truncation=True,\n",
    "                        max_length=1024).input_ids\n",
    "    query_ids = tokenizer(query, add_special_tokens=False, truncation=True,\n",
    "                          max_length=512).input_ids\n",
    "    correct_answer_ids = tokenizer(correct_answer, add_special_tokens=False,\n",
    "                                   truncation=True, max_length=256).input_ids\n",
    "    wrong_answer_ids = tokenizer(wrong_answer, add_special_tokens=False,\n",
    "                                 truncation=True, max_length=256).input_ids\n",
    "\n",
    "    if len(correct_answer_ids) == 0 or len(wrong_answer_ids) == 0:\n",
    "        return None\n",
    "\n",
    "    # Prime variants\n",
    "    oracle_ids = tokenizer(query, add_special_tokens=False, truncation=True,\n",
    "                           max_length=256).input_ids\n",
    "    random_ids = tokenizer(sample['random_prefix'],\n",
    "                           add_special_tokens=False).input_ids\n",
    "    oracle_plus_vocab_ids = tokenizer(sample['oracle_plus_vocab'],\n",
    "                                      add_special_tokens=False,\n",
    "                                      truncation=True, max_length=256).input_ids\n",
    "\n",
    "    prime_map = {\n",
    "        \"bare\": [],\n",
    "        \"random\": random_ids,\n",
    "        \"oracle\": oracle_ids,\n",
    "        \"oracle_plus_vocab\": oracle_plus_vocab_ids,\n",
    "    }\n",
    "\n",
    "    n_q = len(query_ids)\n",
    "    n_d = len(doc_ids)\n",
    "\n",
    "    result = {\n",
    "        'n_doc': n_d,\n",
    "        'n_query': n_q,\n",
    "        'correct_answer_ids': correct_answer_ids,\n",
    "        'wrong_answer_ids': wrong_answer_ids,\n",
    "    }\n",
    "\n",
    "    answer_map = {\n",
    "        'correct': correct_answer_ids,\n",
    "        'wrong': wrong_answer_ids,\n",
    "    }\n",
    "\n",
    "    for answer_type, answer_ids in answer_map.items():\n",
    "        n_a = len(answer_ids)\n",
    "        targets = torch.tensor(answer_ids, dtype=torch.long, device=device)\n",
    "\n",
    "        # --- no_doc: single-pass [BOS, query, answer] ---\n",
    "        no_doc_tokens = [bos_id] + query_ids + answer_ids\n",
    "        no_doc_input = torch.tensor([no_doc_tokens], dtype=torch.long,\n",
    "                                    device=device)\n",
    "        n_total = len(no_doc_tokens)\n",
    "\n",
    "        no_doc_mask = make_causal_mask(n_total)\n",
    "        no_doc_dict = make_mask_dict(no_doc_mask.to(device))\n",
    "        no_doc_pos = torch.arange(n_total, device=device).unsqueeze(0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            out = model(input_ids=no_doc_input, attention_mask=no_doc_dict,\n",
    "                        position_ids=no_doc_pos)\n",
    "\n",
    "        # Logit at position n_q (last query token) predicts first answer token\n",
    "        # Input: [BOS, q0..q_{nq-1}, a0..a_{na-1}]\n",
    "        # Positions:  0    1..n_q      n_q+1..n_q+n_a\n",
    "        answer_logits = out.logits[0, n_q : n_q + n_a, :]\n",
    "        log_probs = F.log_softmax(answer_logits, dim=-1)\n",
    "        token_nlls = -log_probs.gather(1, targets.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "        result[f'token_nlls_no_doc_{answer_type}'] = token_nlls.cpu().tolist()\n",
    "        result[f'nll_no_doc_{answer_type}'] = token_nlls.mean().item()\n",
    "\n",
    "        del out, no_doc_input, no_doc_mask, no_doc_dict\n",
    "        del answer_logits, log_probs, token_nlls\n",
    "\n",
    "        # --- Two-pass conditions ---\n",
    "        for cond_name in TWO_PASS_CONDITIONS:\n",
    "            surr_ids = prime_map[cond_name]\n",
    "            n_s = len(surr_ids)\n",
    "            n_prefix = 1 + n_s + n_d\n",
    "\n",
    "            # Phase A: cache [BOS, prime, doc]\n",
    "            prefix_tokens = [bos_id] + surr_ids + doc_ids\n",
    "            prefix_input = torch.tensor([prefix_tokens], dtype=torch.long,\n",
    "                                        device=device)\n",
    "\n",
    "            phase_a_mask = make_causal_mask(n_prefix)\n",
    "            phase_a_dict = make_mask_dict(phase_a_mask.to(device))\n",
    "            phase_a_pos = torch.arange(n_prefix, device=device).unsqueeze(0)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                out_a = model(input_ids=prefix_input,\n",
    "                              attention_mask=phase_a_dict,\n",
    "                              position_ids=phase_a_pos, use_cache=True)\n",
    "            past_kv = out_a.past_key_values\n",
    "\n",
    "            # Phase B: evaluate [query, answer] with truncation\n",
    "            cont_tokens = query_ids + answer_ids\n",
    "            n_cont = len(cont_tokens)\n",
    "            cont_input = torch.tensor([cont_tokens], dtype=torch.long,\n",
    "                                      device=device)\n",
    "\n",
    "            phase_b_mask = make_phase_b_mask(n_s, n_d, n_q, n_a)\n",
    "            phase_b_dict = make_mask_dict(phase_b_mask.to(device))\n",
    "            phase_b_pos = torch.arange(n_prefix, n_prefix + n_cont,\n",
    "                                       device=device).unsqueeze(0)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                out_b = model(input_ids=cont_input,\n",
    "                              attention_mask=phase_b_dict,\n",
    "                              position_ids=phase_b_pos,\n",
    "                              past_key_values=past_kv)\n",
    "\n",
    "            # Logit at Phase B position n_q-1 predicts first answer token\n",
    "            answer_logits = out_b.logits[0, n_q - 1 : n_q + n_a - 1, :]\n",
    "            log_probs = F.log_softmax(answer_logits, dim=-1)\n",
    "            token_nlls = -log_probs.gather(\n",
    "                1, targets.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "            result[f'token_nlls_{cond_name}_{answer_type}'] = \\\n",
    "                token_nlls.cpu().tolist()\n",
    "            result[f'nll_{cond_name}_{answer_type}'] = \\\n",
    "                token_nlls.mean().item()\n",
    "\n",
    "            del out_a, out_b, past_kv, prefix_input, cont_input\n",
    "            del phase_a_mask, phase_a_dict, phase_b_mask, phase_b_dict\n",
    "            del answer_logits, log_probs, token_nlls\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "# Forward count: no_doc x 2 = 2, two-pass x 4 x 2 = 16, total = 18\n",
    "print(f\"Scoring function defined.\")\n",
    "print(f\"  {len(CONDITIONS)} conditions x 2 answers = {len(CONDITIONS)*2} NLL arrays/sample\")\n",
    "print(f\"  18 forwards/sample\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6f52ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Main scoring loop\n",
    "from lib.data import count_words as _cw\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"MAIN SCORING LOOP\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "CKPT_PATH = RESULTS_DIR / \"checkpoint.json\"\n",
    "\n",
    "all_results = []\n",
    "start_idx = 0\n",
    "if CKPT_PATH.exists():\n",
    "    ckpt = json.loads(CKPT_PATH.read_text())\n",
    "    if len(ckpt.get('results', [])) > 0:\n",
    "        saved_queries = [r['query'][:50] for r in ckpt['results']]\n",
    "        current_queries = [s['query'][:50] for s in samples[:len(saved_queries)]]\n",
    "        if saved_queries == current_queries:\n",
    "            all_results = ckpt['results']\n",
    "            start_idx = len(all_results)\n",
    "            print(f\"Resuming from checkpoint: {start_idx}/{N_SAMPLES}\")\n",
    "\n",
    "if start_idx == 0:\n",
    "    print(f\"Starting fresh: {N_SAMPLES} samples x {len(CONDITIONS)} conds x 2 answers\")\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "for i in tqdm(range(start_idx, N_SAMPLES), initial=start_idx, total=N_SAMPLES,\n",
    "              desc=\"Scoring\"):\n",
    "    s = samples[i]\n",
    "    try:\n",
    "        result = score_sample(model, tokenizer, s, DEVICE)\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR at sample {i}: {e}\")\n",
    "        import traceback; traceback.print_exc()\n",
    "        result = None\n",
    "\n",
    "    if result is None:\n",
    "        continue\n",
    "\n",
    "    result['query'] = s['query'][:50]\n",
    "    result['query_doc_overlap'] = s['query_doc_overlap']\n",
    "    result['answer_wc'] = s['answer_wc']\n",
    "    result['query_wc'] = _cw(s['query'])\n",
    "    result['doc_wc'] = s['word_count']\n",
    "    all_results.append(result)\n",
    "\n",
    "    if (i + 1) % 25 == 0 or i == N_SAMPLES - 1:\n",
    "        ckpt = {\n",
    "            'model': MODEL_NAME,\n",
    "            'n_total': N_SAMPLES,\n",
    "            'n_conditions': len(CONDITIONS),\n",
    "            'condition_names': CONDITIONS,\n",
    "            'results': all_results,\n",
    "            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        }\n",
    "        CKPT_PATH.write_text(json.dumps(ckpt))\n",
    "\n",
    "    if (i + 1) % 50 == 0:\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "elapsed = time.time() - t0\n",
    "print(f\"\\nDone: {len(all_results)} samples in {elapsed/60:.1f} min\")\n",
    "\n",
    "print(f\"\\nQuick summary (correct answer, mean NLL):\")\n",
    "for cn in CONDITIONS:\n",
    "    vals = [r[f'nll_{cn}_correct'] for r in all_results]\n",
    "    print(f\"  {cn:<20} NLL={np.mean(vals):.4f}\")\n",
    "\n",
    "print(f\"\\nQuick summary (wrong answer, mean NLL):\")\n",
    "for cn in CONDITIONS:\n",
    "    vals = [r[f'nll_{cn}_wrong'] for r in all_results]\n",
    "    print(f\"  {cn:<20} NLL={np.mean(vals):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b2c4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Analysis A-C -- Token Stratification\n",
    "print(\"=\" * 70)\n",
    "print(\"ANALYSIS A-C: TOKEN STRATIFICATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "N = len(all_results)\n",
    "\n",
    "# --- Gather per-token NLLs for correct answers into flat arrays ---\n",
    "tok_data = {cn: [] for cn in CONDITIONS}\n",
    "tok_ids_flat = []\n",
    "tok_sample_idx = []\n",
    "\n",
    "for i, r in enumerate(all_results):\n",
    "    n_a = len(r['correct_answer_ids'])\n",
    "    for cn in CONDITIONS:\n",
    "        tok_data[cn].extend(r[f'token_nlls_{cn}_correct'])\n",
    "    tok_ids_flat.extend(r['correct_answer_ids'])\n",
    "    tok_sample_idx.extend([i] * n_a)\n",
    "\n",
    "for cn in CONDITIONS:\n",
    "    tok_data[cn] = np.array(tok_data[cn])\n",
    "tok_ids_flat = np.array(tok_ids_flat)\n",
    "tok_sample_idx = np.array(tok_sample_idx)\n",
    "\n",
    "n_tokens_total = len(tok_ids_flat)\n",
    "print(f\"Total correct-answer tokens pooled: {n_tokens_total}\")\n",
    "print(f\"Mean tokens per sample: {n_tokens_total / N:.1f}\")\n",
    "\n",
    "# ============================================================\n",
    "# A. Token Stratification by Difficulty\n",
    "# ============================================================\n",
    "print(f\"\\n--- A. Token Stratification by Difficulty (bare NLL quartiles) ---\\n\")\n",
    "\n",
    "# Difficulty = bare_correct per-token NLL\n",
    "difficulty = tok_data['bare']\n",
    "q25, q50, q75 = np.percentile(difficulty, [25, 50, 75])\n",
    "quartile = np.digitize(difficulty, bins=[q25, q50, q75])  # 0=easy, 3=hard\n",
    "\n",
    "print(f\"  Quartile thresholds: Q1={q25:.3f}, Q2={q50:.3f}, Q3={q75:.3f}\")\n",
    "print(f\"  Quartile sizes: {[(quartile == q).sum() for q in range(4)]}\")\n",
    "\n",
    "q_labels = [\"Q1 (easy)\", \"Q2\", \"Q3\", \"Q4 (hard)\"]\n",
    "print(f\"\\n  {'Quartile':<12} {'N':>6} {'bare':>8} {'no_doc':>8} {'random':>8}\"\n",
    "      f\" {'oracle':>8} {'opv':>8} {'d(o-r)':>8} {'d(o-b)':>8} {'d(opv-r)':>9}\")\n",
    "print(f\"  {'-'*96}\")\n",
    "\n",
    "for q in range(4):\n",
    "    mask = quartile == q\n",
    "    n_tok = mask.sum()\n",
    "    bare_m = tok_data['bare'][mask].mean()\n",
    "    nodoc_m = tok_data['no_doc'][mask].mean()\n",
    "    rand_m = tok_data['random'][mask].mean()\n",
    "    orac_m = tok_data['oracle'][mask].mean()\n",
    "    opv_m = tok_data['oracle_plus_vocab'][mask].mean()\n",
    "    d_or = cohens_d(tok_data['random'][mask] - tok_data['oracle'][mask])\n",
    "    d_ob = cohens_d(tok_data['bare'][mask] - tok_data['oracle'][mask])\n",
    "    d_opvr = cohens_d(tok_data['random'][mask] -\n",
    "                      tok_data['oracle_plus_vocab'][mask])\n",
    "    print(f\"  {q_labels[q]:<12} {n_tok:>6} {bare_m:>8.3f} {nodoc_m:>8.3f}\"\n",
    "          f\" {rand_m:>8.3f} {orac_m:>8.3f} {opv_m:>8.3f}\"\n",
    "          f\" {d_or:>+8.3f} {d_ob:>+8.3f} {d_opvr:>+9.3f}\")\n",
    "\n",
    "d_or_q1 = cohens_d(tok_data['random'][quartile == 0] -\n",
    "                    tok_data['oracle'][quartile == 0])\n",
    "d_or_q4 = cohens_d(tok_data['random'][quartile == 3] -\n",
    "                    tok_data['oracle'][quartile == 3])\n",
    "print(f\"\\n  d(oracle-random): Q1(easy)={d_or_q1:+.3f}, Q4(hard)={d_or_q4:+.3f}\")\n",
    "print(f\"  Gradient (Q4-Q1): {d_or_q4 - d_or_q1:+.3f}\")\n",
    "if d_or_q4 > d_or_q1 + 0.05:\n",
    "    print(f\"  => Semantic signal IS concentrated on hard tokens!\")\n",
    "else:\n",
    "    print(f\"  => Semantic signal NOT concentrated on hard tokens.\")\n",
    "\n",
    "# ============================================================\n",
    "# B. Token Stratification by Document Dependence\n",
    "# ============================================================\n",
    "print(f\"\\n--- B. Token Stratification by Document Dependence ---\\n\")\n",
    "\n",
    "# Doc dependence = no_doc - bare per token (positive = doc helps)\n",
    "doc_dep = tok_data['no_doc'] - tok_data['bare']\n",
    "dq25, dq50, dq75 = np.percentile(doc_dep, [25, 50, 75])\n",
    "dep_quartile = np.digitize(doc_dep, bins=[dq25, dq50, dq75])\n",
    "\n",
    "print(f\"  Doc-dependence thresholds: Q1={dq25:.3f}, Q2={dq50:.3f}, Q3={dq75:.3f}\")\n",
    "print(f\"  (positive = document helps reduce NLL)\")\n",
    "\n",
    "dep_labels = [\"Q1 (doc-indep)\", \"Q2\", \"Q3\", \"Q4 (doc-dep)\"]\n",
    "print(f\"\\n  {'Quartile':<16} {'N':>6} {'dep':>8} {'d(o-r)':>8}\"\n",
    "      f\" {'d(o-b)':>8} {'d(opv-r)':>9}\")\n",
    "print(f\"  {'-'*62}\")\n",
    "\n",
    "for q in range(4):\n",
    "    mask = dep_quartile == q\n",
    "    n_tok = mask.sum()\n",
    "    dep_m = doc_dep[mask].mean()\n",
    "    d_or = cohens_d(tok_data['random'][mask] - tok_data['oracle'][mask])\n",
    "    d_ob = cohens_d(tok_data['bare'][mask] - tok_data['oracle'][mask])\n",
    "    d_opvr = cohens_d(tok_data['random'][mask] -\n",
    "                      tok_data['oracle_plus_vocab'][mask])\n",
    "    print(f\"  {dep_labels[q]:<16} {n_tok:>6} {dep_m:>+8.3f} {d_or:>+8.3f}\"\n",
    "          f\" {d_ob:>+8.3f} {d_opvr:>+9.3f}\")\n",
    "\n",
    "d_or_dep1 = cohens_d(tok_data['random'][dep_quartile == 0] -\n",
    "                      tok_data['oracle'][dep_quartile == 0])\n",
    "d_or_dep4 = cohens_d(tok_data['random'][dep_quartile == 3] -\n",
    "                      tok_data['oracle'][dep_quartile == 3])\n",
    "print(f\"\\n  d(oracle-random): doc-indep={d_or_dep1:+.3f}, doc-dep={d_or_dep4:+.3f}\")\n",
    "if d_or_dep4 > d_or_dep1 + 0.05:\n",
    "    print(f\"  => Semantic signal appears on document-dependent tokens!\")\n",
    "else:\n",
    "    print(f\"  => Semantic signal does NOT concentrate on doc-dependent tokens.\")\n",
    "\n",
    "# ============================================================\n",
    "# C. Content Word vs Function Word\n",
    "# ============================================================\n",
    "print(f\"\\n--- C. Content Word vs Function Word ---\\n\")\n",
    "\n",
    "is_content = np.zeros(n_tokens_total, dtype=bool)\n",
    "for idx in range(n_tokens_total):\n",
    "    word = tokenizer.decode([int(tok_ids_flat[idx])]).strip().lower()\n",
    "    word_clean = re.sub(r'[^\\w]', '', word)\n",
    "    if word_clean and word_clean not in STOP_WORDS and len(word_clean) > 2:\n",
    "        is_content[idx] = True\n",
    "\n",
    "n_content = is_content.sum()\n",
    "n_function = (~is_content).sum()\n",
    "print(f\"  Content tokens: {n_content} ({100*n_content/n_tokens_total:.1f}%)\")\n",
    "print(f\"  Function tokens: {n_function} ({100*n_function/n_tokens_total:.1f}%)\")\n",
    "\n",
    "print(f\"\\n  {'Type':<12} {'N':>6} {'bare':>8} {'random':>8} {'oracle':>8}\"\n",
    "      f\" {'d(o-r)':>8} {'d(o-b)':>8}\")\n",
    "print(f\"  {'-'*58}\")\n",
    "\n",
    "for label, mask in [(\"Content\", is_content), (\"Function\", ~is_content)]:\n",
    "    bare_m = tok_data['bare'][mask].mean()\n",
    "    rand_m = tok_data['random'][mask].mean()\n",
    "    orac_m = tok_data['oracle'][mask].mean()\n",
    "    d_or = cohens_d(tok_data['random'][mask] - tok_data['oracle'][mask])\n",
    "    d_ob = cohens_d(tok_data['bare'][mask] - tok_data['oracle'][mask])\n",
    "    print(f\"  {label:<12} {mask.sum():>6} {bare_m:>8.3f} {rand_m:>8.3f}\"\n",
    "          f\" {orac_m:>8.3f} {d_or:>+8.3f} {d_ob:>+8.3f}\")\n",
    "\n",
    "# Cross-tabulate: content x difficulty\n",
    "print(f\"\\n  Content x Difficulty (d oracle-random):\")\n",
    "print(f\"  {'':>16} {'Content':>10} {'Function':>10}\")\n",
    "print(f\"  {'-'*38}\")\n",
    "for q in range(4):\n",
    "    q_mask = quartile == q\n",
    "    d_content = cohens_d(tok_data['random'][q_mask & is_content] -\n",
    "                         tok_data['oracle'][q_mask & is_content])\n",
    "    d_function = cohens_d(tok_data['random'][q_mask & ~is_content] -\n",
    "                          tok_data['oracle'][q_mask & ~is_content])\n",
    "    print(f\"  {q_labels[q]:<16} {d_content:>+10.3f} {d_function:>+10.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fcf617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Analysis D-E -- Contrastive Evaluation\n",
    "print(\"=\" * 70)\n",
    "print(\"ANALYSIS D-E: CONTRASTIVE EVALUATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# --- Gather per-sample mean NLLs ---\n",
    "nll_correct = {}\n",
    "nll_wrong = {}\n",
    "for cn in CONDITIONS:\n",
    "    nll_correct[cn] = np.array([r[f'nll_{cn}_correct'] for r in all_results])\n",
    "    nll_wrong[cn] = np.array([r[f'nll_{cn}_wrong'] for r in all_results])\n",
    "\n",
    "# ============================================================\n",
    "# D. Contrastive Evaluation\n",
    "# ============================================================\n",
    "print(f\"\\n--- D. Contrastive Evaluation ({N} samples) ---\\n\")\n",
    "\n",
    "# Gap = wrong_NLL - correct_NLL (positive = model prefers correct)\n",
    "gap = {}\n",
    "for cn in CONDITIONS:\n",
    "    gap[cn] = nll_wrong[cn] - nll_correct[cn]\n",
    "\n",
    "print(f\"  {'Condition':<20} {'mean_gap':>10} {'std_gap':>10} {'AUC':>8}\"\n",
    "      f\" {'d(gap)':>8} {'p(gap>0)':>12} {'sig':>5}\")\n",
    "print(f\"  {'-'*78}\")\n",
    "\n",
    "for cn in CONDITIONS:\n",
    "    g = gap[cn]\n",
    "    auc = (g > 0).mean()\n",
    "    d_gap = cohens_d(g)\n",
    "    _, p_gap = stats.ttest_1samp(g, 0)\n",
    "    sig = ('***' if p_gap < 0.001 else '**' if p_gap < 0.01\n",
    "           else '*' if p_gap < 0.05 else 'ns')\n",
    "    print(f\"  {cn:<20} {g.mean():>+10.3f} {g.std():>10.3f} {auc:>8.1%}\"\n",
    "          f\" {d_gap:>+8.3f} {p_gap:>12.2e} {sig:>5}\")\n",
    "\n",
    "# Prior-controlled discrimination\n",
    "print(f\"\\n  Prior-controlled discrimination (gap - gap_no_doc):\\n\")\n",
    "print(f\"  {'Condition':<20} {'mean':>10} {'d':>8} {'p':>12} {'sig':>5}\")\n",
    "print(f\"  {'-'*58}\")\n",
    "\n",
    "for cn in CONDITIONS:\n",
    "    if cn == \"no_doc\":\n",
    "        continue\n",
    "    doc_discrim = gap[cn] - gap['no_doc']\n",
    "    d_dd = cohens_d(doc_discrim)\n",
    "    _, p_dd = stats.ttest_1samp(doc_discrim, 0)\n",
    "    sig = ('***' if p_dd < 0.001 else '**' if p_dd < 0.01\n",
    "           else '*' if p_dd < 0.05 else 'ns')\n",
    "    print(f\"  {cn:<20} {doc_discrim.mean():>+10.3f} {d_dd:>+8.3f}\"\n",
    "          f\" {p_dd:>12.2e} {sig:>5}\")\n",
    "\n",
    "# Oracle vs random discrimination\n",
    "diff_discrim = gap['oracle'] - gap['random']\n",
    "d_discrim = cohens_d(diff_discrim)\n",
    "_, p_discrim = stats.ttest_1samp(diff_discrim, 0)\n",
    "sig_d = ('***' if p_discrim < 0.001 else '**' if p_discrim < 0.01\n",
    "         else '*' if p_discrim < 0.05 else 'ns')\n",
    "print(f\"\\n  Oracle vs random discrimination:\")\n",
    "print(f\"    gap(oracle) - gap(random): mean={diff_discrim.mean():+.3f},\"\n",
    "      f\" d={d_discrim:+.3f}, p={p_discrim:.2e} {sig_d}\")\n",
    "if p_discrim < 0.05 and d_discrim > 0:\n",
    "    print(f\"    => Oracle enrichment DOES increase discrimination over random!\")\n",
    "else:\n",
    "    print(f\"    => Oracle enrichment does NOT increase discrimination over random.\")\n",
    "\n",
    "# ============================================================\n",
    "# E. Contrastive x Difficulty Interaction\n",
    "# ============================================================\n",
    "print(f\"\\n--- E. Contrastive x Difficulty Interaction ---\\n\")\n",
    "\n",
    "# Split samples by difficulty (mean bare_correct NLL)\n",
    "sample_difficulty = nll_correct['bare']\n",
    "sq25, sq50, sq75 = np.percentile(sample_difficulty, [25, 50, 75])\n",
    "sample_quartile = np.digitize(sample_difficulty, bins=[sq25, sq50, sq75])\n",
    "\n",
    "print(f\"  Sample difficulty quartiles (bare correct NLL):\")\n",
    "print(f\"    Q1: <{sq25:.3f}, Q2: {sq25:.3f}-{sq50:.3f},\"\n",
    "      f\" Q3: {sq50:.3f}-{sq75:.3f}, Q4: >{sq75:.3f}\")\n",
    "\n",
    "s_labels = [\"Q1 (easy)\", \"Q2\", \"Q3\", \"Q4 (hard)\"]\n",
    "print(f\"\\n  {'Quartile':<12} {'N':>5} {'AUC_bare':>10} {'AUC_rand':>10}\"\n",
    "      f\" {'AUC_orac':>10} {'gap_orac':>10} {'gap_rand':>10} {'d(o-r)':>8}\")\n",
    "print(f\"  {'-'*80}\")\n",
    "\n",
    "for q in range(4):\n",
    "    mask = sample_quartile == q\n",
    "    n_sq = mask.sum()\n",
    "    auc_bare = (gap['bare'][mask] > 0).mean()\n",
    "    auc_rand = (gap['random'][mask] > 0).mean()\n",
    "    auc_orac = (gap['oracle'][mask] > 0).mean()\n",
    "    gap_orac = gap['oracle'][mask].mean()\n",
    "    gap_rand = gap['random'][mask].mean()\n",
    "    diff = gap['oracle'][mask] - gap['random'][mask]\n",
    "    d_or = cohens_d(diff) if len(diff) > 1 else 0.0\n",
    "    print(f\"  {s_labels[q]:<12} {n_sq:>5} {auc_bare:>10.1%} {auc_rand:>10.1%}\"\n",
    "          f\" {auc_orac:>10.1%} {gap_orac:>+10.3f} {gap_rand:>+10.3f}\"\n",
    "          f\" {d_or:>+8.3f}\")\n",
    "\n",
    "# Per-quartile significance\n",
    "print(f\"\\n  Per-quartile: oracle vs random discrimination (gap difference):\")\n",
    "for q in range(4):\n",
    "    mask = sample_quartile == q\n",
    "    diff = gap['oracle'][mask] - gap['random'][mask]\n",
    "    d_or = cohens_d(diff) if len(diff) > 1 else 0.0\n",
    "    _, p = stats.ttest_1samp(diff, 0) if len(diff) > 1 else (None, 1.0)\n",
    "    sig = ('***' if p < 0.001 else '**' if p < 0.01\n",
    "           else '*' if p < 0.05 else 'ns')\n",
    "    print(f\"    {s_labels[q]}: d={d_or:+.3f}, p={p:.2e} {sig}\")\n",
    "\n",
    "# Also check: correct-only NLL by difficulty quartile\n",
    "print(f\"\\n  Correct-answer NLL effect by difficulty quartile (d vs bare):\")\n",
    "print(f\"  {'Quartile':<12} {'d_oracle':>10} {'d_random':>10} {'d(o-r)':>10}\")\n",
    "print(f\"  {'-'*46}\")\n",
    "for q in range(4):\n",
    "    mask = sample_quartile == q\n",
    "    d_orc = cohens_d((nll_correct['bare'] - nll_correct['oracle'])[mask])\n",
    "    d_rnd = cohens_d((nll_correct['bare'] - nll_correct['random'])[mask])\n",
    "    d_or = cohens_d((nll_correct['random'] - nll_correct['oracle'])[mask])\n",
    "    print(f\"  {s_labels[q]:<12} {d_orc:>+10.3f} {d_rnd:>+10.3f} {d_or:>+10.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b693a89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Analysis F -- Summary, Replication, Save\n",
    "print(\"=\" * 70)\n",
    "print(\"ANALYSIS F: SUMMARY & REPLICATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "nll_c = nll_correct  # shorthand\n",
    "\n",
    "# --- Replication of prior results using correct-answer NLLs ---\n",
    "d_oracle = cohens_d(nll_c['bare'] - nll_c['oracle'])\n",
    "d_random = cohens_d(nll_c['bare'] - nll_c['random'])\n",
    "struct_frac = d_random / d_oracle if d_oracle != 0 else float('nan')\n",
    "\n",
    "print(f\"\\n  Replication (correct answer NLLs, d vs bare):\")\n",
    "print(f\"    d_oracle = {d_oracle:+.3f} (expected ~+0.452)\")\n",
    "print(f\"    d_random = {d_random:+.3f} (expected ~+0.475)\")\n",
    "print(f\"    structural fraction = {struct_frac:.0%} (expected ~105%)\")\n",
    "\n",
    "# Oracle vs random\n",
    "diff_or = nll_c['random'] - nll_c['oracle']\n",
    "d_or = cohens_d(diff_or)\n",
    "_, p_or = stats.ttest_1samp(diff_or, 0)\n",
    "sig_or = ('***' if p_or < 0.001 else '**' if p_or < 0.01\n",
    "          else '*' if p_or < 0.05 else 'ns')\n",
    "print(f\"    d(oracle vs random) = {d_or:+.3f} (p={p_or:.2e}) {sig_or}\")\n",
    "\n",
    "# oracle_plus_vocab vs random\n",
    "diff_opv = nll_c['random'] - nll_c['oracle_plus_vocab']\n",
    "d_opv = cohens_d(diff_opv)\n",
    "_, p_opv = stats.ttest_1samp(diff_opv, 0)\n",
    "sig_opv = ('***' if p_opv < 0.001 else '**' if p_opv < 0.01\n",
    "           else '*' if p_opv < 0.05 else 'ns')\n",
    "print(f\"    d(oracle_plus_vocab vs random) = {d_opv:+.3f}\"\n",
    "      f\" (expected ~+0.311, p={p_opv:.2e}) {sig_opv}\")\n",
    "\n",
    "# oracle_plus_vocab vs oracle\n",
    "diff_opvo = nll_c['oracle'] - nll_c['oracle_plus_vocab']\n",
    "d_opvo = cohens_d(diff_opvo)\n",
    "_, p_opvo = stats.ttest_1samp(diff_opvo, 0)\n",
    "sig_opvo = ('***' if p_opvo < 0.001 else '**' if p_opvo < 0.01\n",
    "            else '*' if p_opvo < 0.05 else 'ns')\n",
    "print(f\"    d(oracle_plus_vocab vs oracle) = {d_opvo:+.3f}\"\n",
    "      f\" (p={p_opvo:.2e}) {sig_opvo}\")\n",
    "\n",
    "# --- Sanity checks ---\n",
    "print(f\"\\n  Sanity checks:\")\n",
    "print(f\"    Contrastive: AUC(bare) = {(gap['bare'] > 0).mean():.1%}\"\n",
    "      f\" (should be > 50%)\")\n",
    "print(f\"    Prior: AUC(no_doc) = {(gap['no_doc'] > 0).mean():.1%}\")\n",
    "\n",
    "# Token count check\n",
    "tok_ok = True\n",
    "for cn in CONDITIONS:\n",
    "    for at in ['correct', 'wrong']:\n",
    "        lengths = [len(r[f'token_nlls_{cn}_{at}']) for r in all_results]\n",
    "        answer_key = f'{at}_answer_ids'\n",
    "        expected = [len(r[answer_key]) for r in all_results]\n",
    "        if not all(l == e for l, e in zip(lengths, expected)):\n",
    "            print(f\"    WARNING: token count mismatch for {cn}_{at}!\")\n",
    "            tok_ok = False\n",
    "if tok_ok:\n",
    "    print(f\"    Token counts: all verified OK\")\n",
    "\n",
    "# --- Overall verdict ---\n",
    "print(f\"\\n  --- VERDICT ---\")\n",
    "\n",
    "# Token stratification\n",
    "d_or_easy = cohens_d(tok_data['random'][quartile == 0] -\n",
    "                     tok_data['oracle'][quartile == 0])\n",
    "d_or_hard = cohens_d(tok_data['random'][quartile == 3] -\n",
    "                     tok_data['oracle'][quartile == 3])\n",
    "if d_or_hard > d_or_easy + 0.05:\n",
    "    print(f\"  TOKEN STRATIFICATION: Semantic signal concentrated on hard tokens\")\n",
    "    print(f\"    d(oracle-random): easy={d_or_easy:+.3f}, hard={d_or_hard:+.3f}\")\n",
    "else:\n",
    "    print(f\"  TOKEN STRATIFICATION: No concentration on hard tokens\")\n",
    "    print(f\"    d(oracle-random): easy={d_or_easy:+.3f}, hard={d_or_hard:+.3f}\")\n",
    "\n",
    "# Contrastive\n",
    "if p_discrim < 0.05 and d_discrim > 0:\n",
    "    print(f\"  CONTRASTIVE: Oracle enrichment increases discrimination\"\n",
    "          f\" (d={d_discrim:+.3f})\")\n",
    "else:\n",
    "    print(f\"  CONTRASTIVE: Oracle enrichment does NOT increase discrimination\"\n",
    "          f\" (d={d_discrim:+.3f})\")\n",
    "\n",
    "# --- Save results ---\n",
    "summary = {\n",
    "    'n_samples': N,\n",
    "    'model': MODEL_NAME,\n",
    "    'd_oracle_vs_bare': float(d_oracle),\n",
    "    'd_random_vs_bare': float(d_random),\n",
    "    'structural_fraction': float(struct_frac),\n",
    "    'd_opv_vs_random': float(d_opv),\n",
    "    'd_oracle_vs_random_correct': float(d_or),\n",
    "    'd_discrimination_oracle_vs_random': float(d_discrim),\n",
    "}\n",
    "for cn in CONDITIONS:\n",
    "    summary[f'nll_{cn}_correct'] = float(nll_c[cn].mean())\n",
    "    summary[f'nll_{cn}_wrong'] = float(nll_wrong[cn].mean())\n",
    "    summary[f'auc_{cn}'] = float((gap[cn] > 0).mean())\n",
    "\n",
    "final_results = {\n",
    "    'experiment': 'prefix_lm_exp04h',\n",
    "    'dataset': 'ms_marco_v1.1',\n",
    "    'model': MODEL_NAME,\n",
    "    'n_samples': N,\n",
    "    'seed': SEED,\n",
    "    'conditions': CONDITIONS,\n",
    "    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'summary': summary,\n",
    "}\n",
    "\n",
    "with open(RESULTS_DIR / 'results.json', 'w') as f:\n",
    "    json.dump(final_results, f, indent=2)\n",
    "print(f\"\\nResults saved to {RESULTS_DIR / 'results.json'}\")\n",
    "print(\"Done!\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
