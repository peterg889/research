{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64726130",
   "metadata": {},
   "source": [
    "# Prefix LM Exp 04: Causal Ordering Test\n",
    "\n",
    "## Motivation\n",
    "\n",
    "Exp 01-03 showed: surrogate enrichment works (d~+0.45) but structural fraction is 105% --\n",
    "random tokens help as much as oracle. The semantic signal is small (d=+0.255) and\n",
    "instruction content barely matters (Exp 03).\n",
    "\n",
    "**Fundamental question**: Does token ORDER have any effect at all in causal LMs?\n",
    "\n",
    "## Design\n",
    "\n",
    "The simplest possible test. Single forward pass, native causal attention, no custom masks.\n",
    "\n",
    "| # | Condition | Input sequence | What doc sees | What query sees |\n",
    "|---|-----------|---------------|---------------|-----------------|\n",
    "| 1 | `doc_only` | `[BOS, doc, answer]` | preceding doc | n/a |\n",
    "| 2 | `doc_query` | `[BOS, doc, query, answer]` | preceding doc | doc |\n",
    "| 3 | `query_doc` | `[BOS, query, doc, answer]` | **query + preceding doc** | nothing |\n",
    "\n",
    "In all cases, answer tokens see EVERYTHING before them (full causal access to both\n",
    "doc and query). The ONLY difference is the internal representations:\n",
    "- `doc_query`: doc representations are \"pure\" (encoded without query context)\n",
    "- `query_doc`: doc representations are \"enriched\" (each doc token attended to query)\n",
    "\n",
    "**Key comparison**: `doc_query` vs `query_doc`\n",
    "- If `query_doc` is better: enrichment is real -- doc reps conditioned on query are more useful\n",
    "- If equal: order doesn't matter -- answer tokens extract info regardless\n",
    "- If `doc_query` is better: standard order is better (query seeing doc > doc seeing query)\n",
    "\n",
    "## Connection to Two-Phase Experiments\n",
    "\n",
    "The two-phase surrogate design (Exp 01-03) is a more constrained version:\n",
    "- Phase A: `[BOS, surrogate, doc]` cached → equivalent to prefix of `query_doc`\n",
    "- Phase B: `[query, answer]` with surrogate truncated → answer sees doc + query but NOT prefix\n",
    "\n",
    "This single-pass test removes all that complexity. If order doesn't matter here,\n",
    "the two-phase enrichment effect must come from something else (position shifts, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92ec6ba2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-22T00:16:34.966666Z",
     "iopub.status.busy": "2026-02-22T00:16:34.966204Z",
     "iopub.status.idle": "2026-02-22T00:16:39.511815Z",
     "shell.execute_reply": "2026-02-22T00:16:39.510917Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prefix LM Exp 04: Causal Ordering Test\n",
      "N: 500, Conditions: 3\n",
      "DEVICE: cuda\n",
      "GPU: NVIDIA A100-SXM4-40GB\n",
      "GPU memory: 42.3 GB\n",
      "\n",
      "Conditions:\n",
      "  doc_only\n",
      "  doc_query\n",
      "  query_doc\n",
      "\n",
      "Note: Single forward pass, native causal attention, NO custom masks.\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Setup\n",
    "import os\n",
    "os.umask(0o000)\n",
    "\n",
    "import sys, json, time, gc, re\n",
    "import random as pyrandom\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from scipy import stats\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "sys.path.insert(0, \"../../..\")\n",
    "from lib.analysis import cohens_d\n",
    "\n",
    "SEED = 42\n",
    "N_SAMPLES = 500\n",
    "\n",
    "MODEL_NAME = \"google/gemma-3-12b-it\"\n",
    "\n",
    "RESULTS_DIR = Path(\"../../../results/prefix_lm_exp04\")\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "pyrandom.seed(SEED)\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv())\n",
    "HF_TOKEN = os.environ.get(\"HF_TOKEN\")\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "CONDITIONS = [\"doc_only\", \"doc_query\", \"query_doc\"]\n",
    "\n",
    "print(f\"Prefix LM Exp 04: Causal Ordering Test\")\n",
    "print(f\"N: {N_SAMPLES}, Conditions: {len(CONDITIONS)}\")\n",
    "print(f\"DEVICE: {DEVICE}\")\n",
    "print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "print(f\"\\nConditions:\")\n",
    "for cn in CONDITIONS:\n",
    "    print(f\"  {cn}\")\n",
    "print(f\"\\nNote: Single forward pass, native causal attention, NO custom masks.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd9e0a95",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-22T00:16:39.515766Z",
     "iopub.status.busy": "2026-02-22T00:16:39.514959Z",
     "iopub.status.idle": "2026-02-22T00:16:55.206803Z",
     "shell.execute_reply": "2026-02-22T00:16:55.206064Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformers version: 5.1.0\n",
      "Loading google/gemma-3-12b-it...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11479c481d2b4a4fa60fbf11b7767016",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/1065 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: 12.2B params, 24.4 GB GPU, 13s\n",
      "BOS token id: 2\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Load model + tokenizer\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import transformers\n",
    "\n",
    "print(f\"transformers version: {transformers.__version__}\")\n",
    "\n",
    "print(f\"Loading {MODEL_NAME}...\")\n",
    "t0 = time.time()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, token=HF_TOKEN)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    attn_implementation=\"eager\",\n",
    "    token=HF_TOKEN,\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "n_params = sum(p.numel() for p in model.parameters()) / 1e9\n",
    "gpu_mem = torch.cuda.memory_allocated() / 1e9\n",
    "print(f\"Loaded: {n_params:.1f}B params, {gpu_mem:.1f} GB GPU, {time.time()-t0:.0f}s\")\n",
    "print(f\"BOS token id: {tokenizer.bos_token_id}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55af6f3f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-22T00:16:55.210415Z",
     "iopub.status.busy": "2026-02-22T00:16:55.209708Z",
     "iopub.status.idle": "2026-02-22T00:16:56.757296Z",
     "shell.execute_reply": "2026-02-22T00:16:56.756561Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MS MARCO v1.1 validation...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total candidates: 1500\n",
      "Loaded 500 samples\n",
      "Mean passage words: 74\n",
      "Mean query words: 6\n",
      "Mean answer words: 14\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Load MS MARCO data (same pipeline as Exp 01-03)\n",
    "from lib.data import count_words\n",
    "from datasets import load_dataset\n",
    "\n",
    "STOP_WORDS = {\n",
    "    'a', 'an', 'the', 'is', 'are', 'was', 'were', 'be', 'been', 'being',\n",
    "    'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could',\n",
    "    'should', 'may', 'might', 'can', 'shall', 'to', 'of', 'in', 'for',\n",
    "    'on', 'with', 'at', 'by', 'from', 'as', 'into', 'through', 'during',\n",
    "    'before', 'after', 'above', 'below', 'between', 'and', 'but', 'or',\n",
    "    'not', 'no', 'if', 'then', 'than', 'so', 'up', 'out', 'about',\n",
    "    'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those',\n",
    "    'it', 'its', 'i', 'me', 'my', 'we', 'our', 'you', 'your', 'he',\n",
    "    'him', 'his', 'she', 'her', 'they', 'them', 'their', 'how', 'when',\n",
    "    'where', 'why', 'much', 'many', 'some', 'any', 'all', 'each',\n",
    "    'does', 'also', 'just', 'more', 'most', 'very', 'too', 'only',\n",
    "}\n",
    "\n",
    "print(\"Loading MS MARCO v1.1 validation...\")\n",
    "ds = load_dataset(\"microsoft/ms_marco\", \"v1.1\", split=\"validation\")\n",
    "\n",
    "all_candidates = []\n",
    "for item in ds:\n",
    "    if len(all_candidates) >= 3 * N_SAMPLES:\n",
    "        break\n",
    "    passages = item.get('passages', {})\n",
    "    ptexts = passages.get('passage_text', [])\n",
    "    is_sel = passages.get('is_selected', [])\n",
    "    query = item.get('query', '')\n",
    "    answers = item.get('answers', [])\n",
    "    well_formed = item.get('wellFormedAnswers', [])\n",
    "    answer = None\n",
    "    if well_formed and len(well_formed) > 0 and well_formed[0] not in ('[]', ''):\n",
    "        answer = well_formed[0]\n",
    "    elif answers and len(answers) > 0 and answers[0] != 'No Answer Present.':\n",
    "        answer = answers[0]\n",
    "    if not answer:\n",
    "        continue\n",
    "    for pt, sel in zip(ptexts, is_sel):\n",
    "        wc = count_words(pt)\n",
    "        if sel == 1 and 30 <= wc <= 300:\n",
    "            all_candidates.append({\n",
    "                'passage': pt, 'query': query, 'answer': answer,\n",
    "                'word_count': wc,\n",
    "            })\n",
    "            break\n",
    "\n",
    "print(f\"Total candidates: {len(all_candidates)}\")\n",
    "np.random.seed(SEED)\n",
    "indices = np.random.permutation(len(all_candidates))\n",
    "samples = [all_candidates[i] for i in indices[:N_SAMPLES]]\n",
    "del ds, all_candidates\n",
    "gc.collect()\n",
    "\n",
    "# Query-document overlap for post-hoc analysis\n",
    "for i, s in enumerate(samples):\n",
    "    q_words = set(re.sub(r'[^\\w\\s]', '', s['query'].lower()).split()) - STOP_WORDS\n",
    "    d_words = set(re.sub(r'[^\\w\\s]', '', s['passage'].lower()).split()) - STOP_WORDS\n",
    "    union = q_words | d_words\n",
    "    s['query_doc_overlap'] = len(q_words & d_words) / len(union) if len(union) > 0 else 0.0\n",
    "\n",
    "print(f\"Loaded {len(samples)} samples\")\n",
    "print(f\"Mean passage words: {np.mean([s['word_count'] for s in samples]):.0f}\")\n",
    "print(f\"Mean query words: {np.mean([count_words(s['query']) for s in samples]):.0f}\")\n",
    "print(f\"Mean answer words: {np.mean([count_words(s['answer']) for s in samples]):.0f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d9f0728",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-22T00:16:56.760727Z",
     "iopub.status.busy": "2026-02-22T00:16:56.760016Z",
     "iopub.status.idle": "2026-02-22T00:16:56.769344Z",
     "shell.execute_reply": "2026-02-22T00:16:56.768649Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scoring function defined (single-pass, 3 conditions per sample).\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: score_sample() -- single forward pass, native causal attention\n",
    "#\n",
    "# No custom masks, no two-phase, no truncation.\n",
    "# Just build the input sequence in different orders and compute NLL on answer tokens.\n",
    "\n",
    "def score_sample(model, tokenizer, sample, device):\n",
    "    passage = sample['passage']\n",
    "    query = sample['query']\n",
    "    answer = sample['answer']\n",
    "\n",
    "    bos_id = tokenizer.bos_token_id\n",
    "\n",
    "    doc_ids = tokenizer(passage, add_special_tokens=False, truncation=True,\n",
    "                        max_length=1024).input_ids\n",
    "    query_ids = tokenizer(query, add_special_tokens=False, truncation=True,\n",
    "                          max_length=512).input_ids\n",
    "    answer_ids = tokenizer(answer, add_special_tokens=False, truncation=True,\n",
    "                           max_length=256).input_ids\n",
    "\n",
    "    if len(answer_ids) == 0:\n",
    "        return None\n",
    "\n",
    "    n_a = len(answer_ids)\n",
    "    targets = torch.tensor(answer_ids, dtype=torch.long, device=device)\n",
    "\n",
    "    # Three input orderings -- answer always last\n",
    "    sequences = {\n",
    "        \"doc_only\":  [bos_id] + doc_ids + answer_ids,\n",
    "        \"doc_query\": [bos_id] + doc_ids + query_ids + answer_ids,\n",
    "        \"query_doc\": [bos_id] + query_ids + doc_ids + answer_ids,\n",
    "    }\n",
    "\n",
    "    result = {\n",
    "        'n_doc': len(doc_ids),\n",
    "        'n_query': len(query_ids),\n",
    "    }\n",
    "\n",
    "    for name, seq in sequences.items():\n",
    "        input_tensor = torch.tensor([seq], dtype=torch.long, device=device)\n",
    "        n_before = len(seq) - n_a\n",
    "\n",
    "        with torch.no_grad():\n",
    "            out = model(input_ids=input_tensor)\n",
    "\n",
    "        # Logit at position n_before-1 predicts first answer token\n",
    "        answer_logits = out.logits[0, n_before - 1 : n_before + n_a - 1, :]\n",
    "        log_probs = F.log_softmax(answer_logits, dim=-1)\n",
    "        token_nlls = -log_probs.gather(1, targets.unsqueeze(1)).squeeze(1)\n",
    "        result[f'nll_{name}'] = token_nlls.mean().item()\n",
    "\n",
    "        del out, input_tensor, answer_logits, log_probs, token_nlls\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "print(\"Scoring function defined (single-pass, 3 conditions per sample).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0697c5c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-22T00:16:56.772261Z",
     "iopub.status.busy": "2026-02-22T00:16:56.771758Z",
     "iopub.status.idle": "2026-02-22T00:19:57.570162Z",
     "shell.execute_reply": "2026-02-22T00:19:57.569322Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "MAIN SCORING LOOP\n",
      "======================================================================\n",
      "Starting fresh: 500 samples x 3 conditions\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b516b43e02784da29e4d015b46ed2865",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Scoring:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done: 500 samples in 3.0 min\n",
      "\n",
      "Quick summary:\n",
      "  doc_only     NLL=3.9482\n",
      "  doc_query    NLL=2.9538\n",
      "  query_doc    NLL=3.1655\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Main scoring loop\n",
    "from lib.data import count_words as _cw\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"MAIN SCORING LOOP\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "CKPT_PATH = RESULTS_DIR / \"checkpoint.json\"\n",
    "\n",
    "# Resume from checkpoint\n",
    "all_results = []\n",
    "start_idx = 0\n",
    "if CKPT_PATH.exists():\n",
    "    ckpt = json.loads(CKPT_PATH.read_text())\n",
    "    if len(ckpt.get('results', [])) > 0:\n",
    "        saved_queries = [r['query'][:50] for r in ckpt['results']]\n",
    "        current_queries = [s['query'][:50] for s in samples[:len(saved_queries)]]\n",
    "        if saved_queries == current_queries:\n",
    "            all_results = ckpt['results']\n",
    "            start_idx = len(all_results)\n",
    "            print(f\"Resuming from checkpoint: {start_idx}/{N_SAMPLES}\")\n",
    "\n",
    "if start_idx == 0:\n",
    "    print(f\"Starting fresh: {N_SAMPLES} samples x {len(CONDITIONS)} conditions\")\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "for i in tqdm(range(start_idx, N_SAMPLES), initial=start_idx, total=N_SAMPLES,\n",
    "              desc=\"Scoring\"):\n",
    "    s = samples[i]\n",
    "    try:\n",
    "        result = score_sample(model, tokenizer, s, DEVICE)\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR at sample {i}: {e}\")\n",
    "        result = None\n",
    "\n",
    "    if result is None:\n",
    "        continue\n",
    "\n",
    "    result['query'] = s['query'][:50]\n",
    "    result['query_doc_overlap'] = s['query_doc_overlap']\n",
    "    result['answer_wc'] = _cw(s['answer'])\n",
    "    result['doc_wc'] = s['word_count']\n",
    "    all_results.append(result)\n",
    "\n",
    "    if (i + 1) % 25 == 0 or i == N_SAMPLES - 1:\n",
    "        ckpt = {\n",
    "            'model': MODEL_NAME,\n",
    "            'n_total': N_SAMPLES,\n",
    "            'n_conditions': len(CONDITIONS),\n",
    "            'condition_names': CONDITIONS,\n",
    "            'results': all_results,\n",
    "            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        }\n",
    "        CKPT_PATH.write_text(json.dumps(ckpt))\n",
    "\n",
    "    if (i + 1) % 100 == 0:\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "elapsed = time.time() - t0\n",
    "print(f\"\\nDone: {len(all_results)} samples in {elapsed/60:.1f} min\")\n",
    "print(f\"\\nQuick summary:\")\n",
    "for cn in CONDITIONS:\n",
    "    vals = [r[f'nll_{cn}'] for r in all_results]\n",
    "    print(f\"  {cn:<12} NLL={np.mean(vals):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81b644fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-22T00:19:57.573603Z",
     "iopub.status.busy": "2026-02-22T00:19:57.573334Z",
     "iopub.status.idle": "2026-02-22T00:19:57.600095Z",
     "shell.execute_reply": "2026-02-22T00:19:57.599420Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "RESULTS: CAUSAL ORDERING TEST\n",
      "======================================================================\n",
      "\n",
      "--- Mean NLL (500 samples) ---\n",
      "\n",
      "  Condition      Mean NLL      Std\n",
      "  --------------------------------\n",
      "  doc_only         3.9482   5.2458\n",
      "  doc_query        2.9538   3.8715\n",
      "  query_doc        3.1655   4.1263\n",
      "\n",
      "--- Key Comparisons ---\n",
      "\n",
      "  Comparison                                                d    win%            p   sig\n",
      "  -------------------------------------------------------------------------------------\n",
      "  ORDERING: query_doc vs doc_query                     -0.126   35.4%     5.16e-03    **\n",
      "  QUERY BENEFIT (standard): doc_only vs doc_query      +0.471   88.8%     1.61e-23   ***\n",
      "  QUERY BENEFIT (reversed): doc_only vs query_doc      +0.513   86.6%     3.20e-27   ***\n",
      "\n",
      "--- Ordering Effect Distribution ---\n",
      "\n",
      "  Per-sample NLL(doc_query) - NLL(query_doc):\n",
      "    Mean:   -0.2117\n",
      "    Std:    1.6833\n",
      "    Median: -0.0977\n",
      "    % where query_doc wins: 35.4%\n",
      "    5th/25th/75th/95th: -2.307 / -0.438 / +0.069 / +1.753\n",
      "\n",
      "--- Per-Sample Heterogeneity ---\n",
      "\n",
      "  Correlations with ordering effect (positive = query_doc better):\n",
      "  Covariate                   r            p   sig\n",
      "  ------------------------------------------------\n",
      "  query_doc_overlap      -0.021     6.39e-01    ns\n",
      "  answer_wc              +0.060     1.81e-01    ns\n",
      "  doc_wc                 +0.015     7.32e-01    ns\n",
      "  n_query_tokens         +0.080     7.39e-02    ns\n",
      "\n",
      "  Answer length split:\n",
      "  Group               N   d_ordering  d_q_benefit_std  d_q_benefit_rev\n",
      "  --------------------------------------------------------------------\n",
      "  Short (<=5w)      210       -0.118           +0.650           +0.799\n",
      "  Long (>5w)        290       -0.430           +0.807           +0.665\n",
      "\n",
      "  Query length split:\n",
      "  Group                    N   d_ordering\n",
      "  ----------------------------------------\n",
      "  Short q (<=6 tok)      291       -0.185\n",
      "  Long q (>6 tok)        209       -0.047\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Analysis\n",
    "print(\"=\" * 70)\n",
    "print(\"RESULTS: CAUSAL ORDERING TEST\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Extract NLL arrays\n",
    "nll = {}\n",
    "for cn in CONDITIONS:\n",
    "    nll[cn] = np.array([r[f'nll_{cn}'] for r in all_results])\n",
    "\n",
    "N = len(all_results)\n",
    "\n",
    "# --- Mean NLL table ---\n",
    "print(f\"\\n--- Mean NLL ({N} samples) ---\\n\")\n",
    "print(f\"  {'Condition':<12} {'Mean NLL':>10} {'Std':>8}\")\n",
    "print(f\"  {'-'*32}\")\n",
    "for cn in CONDITIONS:\n",
    "    print(f\"  {cn:<12} {nll[cn].mean():>10.4f} {nll[cn].std():>8.4f}\")\n",
    "\n",
    "# --- Key comparisons ---\n",
    "print(f\"\\n--- Key Comparisons ---\\n\")\n",
    "print(f\"  {'Comparison':<50} {'d':>8} {'win%':>7} {'p':>12} {'sig':>5}\")\n",
    "print(f\"  {'-'*85}\")\n",
    "\n",
    "comparisons = [\n",
    "    # THE core test: does order matter?\n",
    "    (\"ORDERING: query_doc vs doc_query\",\n",
    "     nll['doc_query'] - nll['query_doc']),\n",
    "\n",
    "    # Query benefit in standard order\n",
    "    (\"QUERY BENEFIT (standard): doc_only vs doc_query\",\n",
    "     nll['doc_only'] - nll['doc_query']),\n",
    "\n",
    "    # Query benefit in reversed order\n",
    "    (\"QUERY BENEFIT (reversed): doc_only vs query_doc\",\n",
    "     nll['doc_only'] - nll['query_doc']),\n",
    "]\n",
    "\n",
    "for label, diff in comparisons:\n",
    "    d = cohens_d(diff)\n",
    "    _, p = stats.ttest_1samp(diff, 0)\n",
    "    sig = '***' if p < 0.001 else '**' if p < 0.01 else '*' if p < 0.05 else 'ns'\n",
    "    win = (diff > 0).mean() * 100\n",
    "    print(f\"  {label:<50} {d:>+8.3f} {win:>6.1f}% {p:>12.2e} {sig:>5}\")\n",
    "\n",
    "# --- Per-sample ordering effect distribution ---\n",
    "ordering_effect = nll['doc_query'] - nll['query_doc']\n",
    "print(f\"\\n--- Ordering Effect Distribution ---\\n\")\n",
    "print(f\"  Per-sample NLL(doc_query) - NLL(query_doc):\")\n",
    "print(f\"    Mean:   {ordering_effect.mean():+.4f}\")\n",
    "print(f\"    Std:    {ordering_effect.std():.4f}\")\n",
    "print(f\"    Median: {np.median(ordering_effect):+.4f}\")\n",
    "print(f\"    % where query_doc wins: {(ordering_effect > 0).mean()*100:.1f}%\")\n",
    "pcts = np.percentile(ordering_effect, [5, 25, 75, 95])\n",
    "print(f\"    5th/25th/75th/95th: {pcts[0]:+.3f} / {pcts[1]:+.3f} / {pcts[2]:+.3f} / {pcts[3]:+.3f}\")\n",
    "\n",
    "# --- Per-sample heterogeneity ---\n",
    "print(f\"\\n--- Per-Sample Heterogeneity ---\\n\")\n",
    "\n",
    "overlap = np.array([r['query_doc_overlap'] for r in all_results])\n",
    "answer_wc = np.array([r['answer_wc'] for r in all_results])\n",
    "doc_wc = np.array([r['doc_wc'] for r in all_results])\n",
    "n_query = np.array([r['n_query'] for r in all_results])\n",
    "\n",
    "print(f\"  Correlations with ordering effect (positive = query_doc better):\")\n",
    "print(f\"  {'Covariate':<20} {'r':>8} {'p':>12} {'sig':>5}\")\n",
    "print(f\"  {'-'*48}\")\n",
    "\n",
    "for cov_name, cov_vals in [(\"query_doc_overlap\", overlap), (\"answer_wc\", answer_wc),\n",
    "                            (\"doc_wc\", doc_wc), (\"n_query_tokens\", n_query)]:\n",
    "    r, p = stats.pearsonr(ordering_effect, cov_vals)\n",
    "    sig = '***' if p < 0.001 else '**' if p < 0.01 else '*' if p < 0.05 else 'ns'\n",
    "    print(f\"  {cov_name:<20} {r:>+8.3f} {p:>12.2e} {sig:>5}\")\n",
    "\n",
    "# --- Answer length subpopulation ---\n",
    "print(f\"\\n  Answer length split:\")\n",
    "short = answer_wc <= 5\n",
    "long = ~short\n",
    "print(f\"  {'Group':<15} {'N':>5} {'d_ordering':>12} {'d_q_benefit_std':>16} {'d_q_benefit_rev':>16}\")\n",
    "print(f\"  {'-'*68}\")\n",
    "for label, mask in [(\"Short (<=5w)\", short), (\"Long (>5w)\", long)]:\n",
    "    d_ord = cohens_d(ordering_effect[mask])\n",
    "    d_std = cohens_d((nll['doc_only'] - nll['doc_query'])[mask])\n",
    "    d_rev = cohens_d((nll['doc_only'] - nll['query_doc'])[mask])\n",
    "    print(f\"  {label:<15} {mask.sum():>5} {d_ord:>+12.3f} {d_std:>+16.3f} {d_rev:>+16.3f}\")\n",
    "\n",
    "# --- Query length subpopulation ---\n",
    "print(f\"\\n  Query length split:\")\n",
    "med_nq = np.median(n_query)\n",
    "short_q = n_query <= med_nq\n",
    "long_q = ~short_q\n",
    "print(f\"  {'Group':<20} {'N':>5} {'d_ordering':>12}\")\n",
    "print(f\"  {'-'*40}\")\n",
    "for label, mask in [(f\"Short q (<={med_nq:.0f} tok)\", short_q),\n",
    "                     (f\"Long q (>{med_nq:.0f} tok)\", long_q)]:\n",
    "    d_ord = cohens_d(ordering_effect[mask])\n",
    "    print(f\"  {label:<20} {mask.sum():>5} {d_ord:>+12.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a46e00f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-22T00:19:57.603382Z",
     "iopub.status.busy": "2026-02-22T00:19:57.602778Z",
     "iopub.status.idle": "2026-02-22T00:19:57.615277Z",
     "shell.execute_reply": "2026-02-22T00:19:57.614633Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "SUMMARY -- Prefix LM Exp 04: Causal Ordering Test\n",
      "======================================================================\n",
      "\n",
      "  d_ordering (query_doc vs doc_query):  -0.126 (p=5.16e-03)\n",
      "  d_query_benefit (standard order):     +0.471\n",
      "  d_query_benefit (reversed order):     +0.513\n",
      "\n",
      "  VERDICT:\n",
      "  ORDER MATTERS but REVERSED: doc_query > query_doc (d=-0.126, ***).\n",
      "  Standard reading order is better -- query seeing doc matters more\n",
      "  than doc seeing query.\n",
      "  -> The two-phase enrichment effect is NOT about doc representation quality.\n",
      "\n",
      "  Connection to Exp 01-03 (two-phase enrichment d~+0.45):\n",
      "  In two-phase: Phase A caches [BOS,surr,doc], Phase B uses [query,answer].\n",
      "  Surrogate positions are MASKED from Phase B (truncation).\n",
      "  The enrichment thus operates ONLY through modified doc representations\n",
      "  (indirect channel) plus position ID shifts.\n",
      "\n",
      "Results saved to ../../../results/prefix_lm_exp04/results.json\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Save results and verdict\n",
    "print(\"=\" * 70)\n",
    "print(\"SUMMARY -- Prefix LM Exp 04: Causal Ordering Test\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "d_ordering = cohens_d(nll['doc_query'] - nll['query_doc'])\n",
    "_, p_ordering = stats.ttest_1samp(nll['doc_query'] - nll['query_doc'], 0)\n",
    "d_q_std = cohens_d(nll['doc_only'] - nll['doc_query'])\n",
    "d_q_rev = cohens_d(nll['doc_only'] - nll['query_doc'])\n",
    "\n",
    "print(f\"\\n  d_ordering (query_doc vs doc_query):  {d_ordering:+.3f} (p={p_ordering:.2e})\")\n",
    "print(f\"  d_query_benefit (standard order):     {d_q_std:+.3f}\")\n",
    "print(f\"  d_query_benefit (reversed order):     {d_q_rev:+.3f}\")\n",
    "\n",
    "print(f\"\\n  VERDICT:\")\n",
    "if p_ordering < 0.05 and d_ordering > 0:\n",
    "    print(f\"  ORDER MATTERS: query_doc > doc_query (d={d_ordering:+.3f}, ***).\")\n",
    "    print(f\"  Doc representations enriched by query ARE more useful.\")\n",
    "    print(f\"  -> Enrichment is real, even in single-pass causal LM.\")\n",
    "    gap = d_q_rev - d_q_std\n",
    "    print(f\"  -> Ordering bonus: {gap:+.3f} additional d from query-first order.\")\n",
    "elif p_ordering < 0.05 and d_ordering < 0:\n",
    "    print(f\"  ORDER MATTERS but REVERSED: doc_query > query_doc (d={d_ordering:+.3f}, ***).\")\n",
    "    print(f\"  Standard reading order is better -- query seeing doc matters more\")\n",
    "    print(f\"  than doc seeing query.\")\n",
    "    print(f\"  -> The two-phase enrichment effect is NOT about doc representation quality.\")\n",
    "else:\n",
    "    print(f\"  ORDER DOES NOT MATTER (d={d_ordering:+.3f}, ns).\")\n",
    "    print(f\"  Answer tokens extract the same information regardless of internal rep quality.\")\n",
    "    print(f\"  -> The two-phase enrichment (d~+0.45) comes from something else entirely:\")\n",
    "    print(f\"     position shifts, attention pattern changes, or RoPE effects.\")\n",
    "\n",
    "# Connection to Exp 01-03\n",
    "print(f\"\\n  Connection to Exp 01-03 (two-phase enrichment d~+0.45):\")\n",
    "print(f\"  In two-phase: Phase A caches [BOS,surr,doc], Phase B uses [query,answer].\")\n",
    "print(f\"  Surrogate positions are MASKED from Phase B (truncation).\")\n",
    "print(f\"  The enrichment thus operates ONLY through modified doc representations\")\n",
    "print(f\"  (indirect channel) plus position ID shifts.\")\n",
    "\n",
    "summary = {\n",
    "    'n_samples': N,\n",
    "    'model': MODEL_NAME,\n",
    "    'nll_doc_only': float(nll['doc_only'].mean()),\n",
    "    'nll_doc_query': float(nll['doc_query'].mean()),\n",
    "    'nll_query_doc': float(nll['query_doc'].mean()),\n",
    "    'd_ordering': float(d_ordering),\n",
    "    'd_ordering_p': float(p_ordering),\n",
    "    'd_query_benefit_standard': float(d_q_std),\n",
    "    'd_query_benefit_reversed': float(d_q_rev),\n",
    "}\n",
    "\n",
    "final_results = {\n",
    "    'experiment': 'prefix_lm_exp04',\n",
    "    'dataset': 'ms_marco_v1.1',\n",
    "    'model': MODEL_NAME,\n",
    "    'n_samples': N,\n",
    "    'seed': SEED,\n",
    "    'conditions': CONDITIONS,\n",
    "    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'summary': summary,\n",
    "    'exp01_03_references': {\n",
    "        'd_causal_oracle_trunc': 0.452,\n",
    "        'd_causal_random_trunc': 0.475,\n",
    "        'structural_fraction': 1.051,\n",
    "    },\n",
    "}\n",
    "\n",
    "with open(RESULTS_DIR / 'results.json', 'w') as f:\n",
    "    json.dump(final_results, f, indent=2)\n",
    "print(f\"\\nResults saved to {RESULTS_DIR / 'results.json'}\")\n",
    "print(\"Done!\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "11479c481d2b4a4fa60fbf11b7767016": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_fe624f43fb3c474a9b68cc39a508f8c1",
        "IPY_MODEL_a527183997ef40caba6718da57f3254b",
        "IPY_MODEL_d1600c5a7e3343b48151d0b073c024ae"
       ],
       "layout": "IPY_MODEL_d7872b97b7f74a8b933fd77526c7ab95",
       "tabbable": null,
       "tooltip": null
      }
     },
     "1b9bebd78f4c4202855f635f138d726f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_650211fbb826406e915dd489550ee333",
       "placeholder": "​",
       "style": "IPY_MODEL_f3c9151fb5524920a10e0e11fa591dc5",
       "tabbable": null,
       "tooltip": null,
       "value": "Scoring: 100%"
      }
     },
     "335fb5e201ff4b99982d7d33611d2282": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_3536e6cca95943c78054d760433b647a",
       "max": 500.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_6e9bb406cd684cf58d6cf2bc1e600e02",
       "tabbable": null,
       "tooltip": null,
       "value": 500.0
      }
     },
     "3536e6cca95943c78054d760433b647a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "4d0b038e9b724278ada3ae2db3667255": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "50ea2ea217144f0192d7799d6883f72e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "56c20e7aeaa24b3d962b3d35d8289edb": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "650211fbb826406e915dd489550ee333": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "65bfa70b347d4eb7be39d79512306074": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "6d76559c60d6433bb67de0152b09051b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6e9bb406cd684cf58d6cf2bc1e600e02": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "8a8b30d632c14c77aa7eb9ed500887ab": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_56c20e7aeaa24b3d962b3d35d8289edb",
       "placeholder": "​",
       "style": "IPY_MODEL_65bfa70b347d4eb7be39d79512306074",
       "tabbable": null,
       "tooltip": null,
       "value": " 500/500 [03:00&lt;00:00,  2.40it/s]"
      }
     },
     "96e6a9b5d0ec4b8196349cacebfb5a29": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "a527183997ef40caba6718da57f3254b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_50ea2ea217144f0192d7799d6883f72e",
       "max": 1065.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_e231aeedb17440f5b15f5c10c3b512b8",
       "tabbable": null,
       "tooltip": null,
       "value": 1065.0
      }
     },
     "b516b43e02784da29e4d015b46ed2865": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_1b9bebd78f4c4202855f635f138d726f",
        "IPY_MODEL_335fb5e201ff4b99982d7d33611d2282",
        "IPY_MODEL_8a8b30d632c14c77aa7eb9ed500887ab"
       ],
       "layout": "IPY_MODEL_6d76559c60d6433bb67de0152b09051b",
       "tabbable": null,
       "tooltip": null
      }
     },
     "d1600c5a7e3343b48151d0b073c024ae": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_4d0b038e9b724278ada3ae2db3667255",
       "placeholder": "​",
       "style": "IPY_MODEL_ed50e04be6b64187bfc0205b8e74d263",
       "tabbable": null,
       "tooltip": null,
       "value": " 1065/1065 [00:07&lt;00:00, 634.72it/s, Materializing param=model.vision_tower.vision_model.post_layernorm.weight]"
      }
     },
     "d7872b97b7f74a8b933fd77526c7ab95": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e231aeedb17440f5b15f5c10c3b512b8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "e31d1a959e03431d820ddfe81718a1e8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ed50e04be6b64187bfc0205b8e74d263": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "f3c9151fb5524920a10e0e11fa591dc5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "fe624f43fb3c474a9b68cc39a508f8c1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_e31d1a959e03431d820ddfe81718a1e8",
       "placeholder": "​",
       "style": "IPY_MODEL_96e6a9b5d0ec4b8196349cacebfb5a29",
       "tabbable": null,
       "tooltip": null,
       "value": "Loading weights: 100%"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
