{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b855a13",
   "metadata": {},
   "source": [
    "# Experiment 3E: Attention Mechanism Probing\n",
    "\n",
    "## Motivation\n",
    "\n",
    "Experiments 2B and 3D established that **~85% of the oracle headroom is \"structural\"** --\n",
    "prepending ANY text (even \"the the the...\") to the encoder improves document\n",
    "representations. But we don't understand WHY.\n",
    "\n",
    "**Three hypotheses**:\n",
    "1. **Attention redistribution**: prefix tokens absorb attention mass, changing how\n",
    "   document tokens attend to each other\n",
    "2. **RoPE position shift**: document tokens move to later positions, changing their\n",
    "   frequency signatures\n",
    "3. **Representation regularization**: prefix acts as noise injection that produces\n",
    "   more distributed/robust representations\n",
    "\n",
    "This experiment extracts encoder attention weights and hidden states to directly\n",
    "measure what changes.\n",
    "\n",
    "## Architecture\n",
    "\n",
    "- 34 encoder layers, 5 full-attention (layers 5, 11, 17, 23, 29), 29 sliding window\n",
    "- 8 attention heads (GQA: 4 KV heads expanded to 8)\n",
    "- Hidden size: 2560, head dim: 256\n",
    "- Sliding window: bidirectional, 513 tokens each direction\n",
    "- For a 20-token prefix + 600-word doc: only the 5 full-attention layers can directly\n",
    "  connect prefix to distant document tokens\n",
    "\n",
    "## Design\n",
    "\n",
    "**N=500** from neural-bridge/rag-12000 (same samples as Exp 3D).\n",
    "\n",
    "**4 conditions** (all with truncation mask):\n",
    "1. `bare` -- document only\n",
    "2. `oracle_trunc` -- real query + document\n",
    "3. `random_matched_trunc` -- random words + document\n",
    "4. `repeat_the_trunc` -- \"the\"xN + document\n",
    "\n",
    "## Probes\n",
    "\n",
    "| Probe | What it measures |\n",
    "|-------|-----------------|\n",
    "| **A: Attention mass on prefix** | Fraction of document tokens' attention going to prefix |\n",
    "| **B: Attention entropy** | Whether prefix increases or decreases entropy of doc token attention |\n",
    "| **C: Doc-doc redistribution** | How the remaining doc-doc attention changes with prefix |\n",
    "| **D: Shift magnitude** | L2 distance of doc token representations (bare vs prefixed) |\n",
    "| **E: Shift direction** | Cosine similarity of shift vectors across conditions (structural vs semantic) |\n",
    "| **F: Attention sinks** | Whether prefix tokens take over the \"sink\" role |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ecf01658",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T14:12:51.146818Z",
     "iopub.status.busy": "2026-02-18T14:12:51.146459Z",
     "iopub.status.idle": "2026-02-18T14:12:55.489519Z",
     "shell.execute_reply": "2026-02-18T14:12:55.488804Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidates (q>=15w, a>=5w): 3384\n",
      "\n",
      "Sample statistics (N=500):\n",
      "  Query length:  mean=17.8, range=[15, 37]\n",
      "  Doc length:    mean=604.2, range=[115, 1192]\n",
      "Dataset loaded.\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Setup + load dataset\n",
    "import os\n",
    "os.umask(0o000)\n",
    "\n",
    "import sys, json, time, re, gc, random as pyrandom\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from scipy import stats\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "sys.path.insert(0, \".\")\n",
    "from lib.analysis import cohens_d\n",
    "\n",
    "SEED = 42\n",
    "N_SAMPLES = 500\n",
    "MODEL_NAME = \"google/t5gemma-2-4b-4b\"\n",
    "\n",
    "RESULTS_DIR = Path(\"results/exp03e\")\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CHECKPOINT_PATH = RESULTS_DIR / \"checkpoint.json\"\n",
    "\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "pyrandom.seed(SEED)\n",
    "\n",
    "# Load dataset (same as Exp 3D)\n",
    "from datasets import load_dataset\n",
    "\n",
    "print(\"Loading dataset...\")\n",
    "ds = load_dataset(\"neural-bridge/rag-dataset-12000\", split=\"train\")\n",
    "\n",
    "all_candidates = []\n",
    "for row in ds:\n",
    "    q = row.get(\"question\", \"\")\n",
    "    doc = row.get(\"context\", \"\")\n",
    "    answer = row.get(\"answer\", \"\")\n",
    "    if not q or not doc or not answer:\n",
    "        continue\n",
    "    q_words = len(q.split())\n",
    "    a_words = len(answer.split())\n",
    "    if q_words >= 15 and a_words >= 5:\n",
    "        all_candidates.append({\n",
    "            \"query\": q,\n",
    "            \"document\": doc,\n",
    "            \"answer\": answer,\n",
    "            \"query_words\": q_words,\n",
    "            \"doc_words\": len(doc.split()),\n",
    "            \"answer_words\": a_words,\n",
    "        })\n",
    "\n",
    "print(f\"Candidates (q>=15w, a>=5w): {len(all_candidates)}\")\n",
    "\n",
    "# Same shuffle and selection as Exp 3D\n",
    "np.random.seed(SEED)\n",
    "indices = np.random.permutation(len(all_candidates))\n",
    "samples = [all_candidates[i] for i in indices[:N_SAMPLES]]\n",
    "\n",
    "q_lens = np.array([s[\"query_words\"] for s in samples])\n",
    "d_lens = np.array([s[\"doc_words\"] for s in samples])\n",
    "\n",
    "print(f\"\\nSample statistics (N={N_SAMPLES}):\")\n",
    "print(f\"  Query length:  mean={q_lens.mean():.1f}, range=[{q_lens.min()}, {q_lens.max()}]\")\n",
    "print(f\"  Doc length:    mean={d_lens.mean():.1f}, range=[{d_lens.min()}, {d_lens.max()}]\")\n",
    "\n",
    "del ds\n",
    "gc.collect()\n",
    "print(\"Dataset loaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "594bf15c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T14:12:55.493175Z",
     "iopub.status.busy": "2026-02-18T14:12:55.492766Z",
     "iopub.status.idle": "2026-02-18T14:13:14.585727Z",
     "shell.execute_reply": "2026-02-18T14:13:14.584741Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading google/t5gemma-2-4b-4b with attn_implementation='eager'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "311861944a8d432288bc394e7992925f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/1327 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded. dtype=torch.bfloat16\n",
      "GPU memory: 15.02 GB\n",
      "Encoder layers: 34\n",
      "Encoder attn_implementation: eager\n",
      "Full-attention layers: [5, 11, 17, 23, 29]\n",
      "Sliding-window layers: [0, 1, 2, 3, 4, 6, 7, 8, 9, 10, 12, 13, 14, 15, 16, 18, 19, 20, 21, 22, 24, 25, 26, 27, 28, 30, 31, 32, 33]\n",
      "\n",
      "Attention probe layers: [0, 5, 11, 17, 23, 29]\n",
      "Hidden state probe layers: [0, 5, 11, 17, 23, 29, 33]\n",
      "Hooks and helpers defined.\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Load model with eager attention + define hooks\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "HF_TOKEN = os.environ.get(\"HF_TOKEN\")\n",
    "\n",
    "from transformers import AutoProcessor, AutoModelForSeq2SeqLM\n",
    "\n",
    "print(f\"Loading {MODEL_NAME} with attn_implementation='eager'...\")\n",
    "processor = AutoProcessor.from_pretrained(MODEL_NAME, token=HF_TOKEN)\n",
    "tokenizer = processor.tokenizer\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    token=HF_TOKEN,\n",
    "    attn_implementation=\"eager\",\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "DEVICE = next(model.parameters()).device\n",
    "print(f\"Model loaded. dtype={next(model.parameters()).dtype}\")\n",
    "print(f\"GPU memory: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "\n",
    "# Get encoder text model reference (T5Gemma2 has multimodal wrapper)\n",
    "# model.model.encoder = T5Gemma2Encoder (multimodal)\n",
    "# model.model.encoder.text_model = T5Gemma2TextEncoder (has .layers)\n",
    "encoder_text = model.model.encoder.text_model\n",
    "n_layers = len(encoder_text.layers)\n",
    "print(f\"Encoder layers: {n_layers}\")\n",
    "\n",
    "# Verify eager attention is active (SDPA returns None for attention weights)\n",
    "enc_attn_impl = encoder_text.layers[0].self_attn.config._attn_implementation\n",
    "print(f\"Encoder attn_implementation: {enc_attn_impl}\")\n",
    "assert enc_attn_impl == \"eager\", (\n",
    "    f\"Expected 'eager' but got '{enc_attn_impl}'. \"\n",
    "    f\"SDPA will return None attention weights!\"\n",
    ")\n",
    "\n",
    "# Identify full-attention vs sliding-window layers\n",
    "layer_types = []\n",
    "full_attn_layers = []\n",
    "for i in range(n_layers):\n",
    "    lt = encoder_text.layers[i].attention_type\n",
    "    layer_types.append(lt)\n",
    "    if lt == \"full_attention\":\n",
    "        full_attn_layers.append(i)\n",
    "\n",
    "print(f\"Full-attention layers: {full_attn_layers}\")\n",
    "print(f\"Sliding-window layers: {[i for i in range(n_layers) if layer_types[i] != 'full_attention']}\")\n",
    "\n",
    "# Layers to probe: first layer, all full-attention layers, final layer\n",
    "ATTN_LAYERS = sorted(set([0] + full_attn_layers))\n",
    "HIDDEN_LAYERS = sorted(set([0] + full_attn_layers + [n_layers - 1]))\n",
    "print(f\"\\nAttention probe layers: {ATTN_LAYERS}\")\n",
    "print(f\"Hidden state probe layers: {HIDDEN_LAYERS}\")\n",
    "\n",
    "# Hook infrastructure: stores captured tensors per forward pass\n",
    "captured_attn = {}\n",
    "captured_hidden = {}\n",
    "hook_handles = []\n",
    "\n",
    "\n",
    "def make_attn_hook(layer_idx):\n",
    "    # Hook on self_attn: captures (attn_output, attn_weights)\n",
    "    def hook_fn(module, input, output):\n",
    "        attn_output, attn_weights = output\n",
    "        if attn_weights is not None:\n",
    "            # attn_weights: (batch, n_heads, seq_len, seq_len)\n",
    "            captured_attn[layer_idx] = attn_weights.detach().float()\n",
    "        else:\n",
    "            print(f\"WARNING: Layer {layer_idx} returned None attention weights!\")\n",
    "    return hook_fn\n",
    "\n",
    "\n",
    "def make_hidden_hook(layer_idx):\n",
    "    # Hook on encoder layer: captures hidden_states output\n",
    "    def hook_fn(module, input, output):\n",
    "        # T5GemmaEncoderLayer.forward returns just hidden_states (a single tensor)\n",
    "        if isinstance(output, tuple):\n",
    "            h = output[0]\n",
    "        else:\n",
    "            h = output\n",
    "        captured_hidden[layer_idx] = h.detach().float()\n",
    "    return hook_fn\n",
    "\n",
    "\n",
    "def register_hooks():\n",
    "    global hook_handles\n",
    "    remove_hooks()\n",
    "    for layer_idx in ATTN_LAYERS:\n",
    "        h = encoder_text.layers[layer_idx].self_attn.register_forward_hook(\n",
    "            make_attn_hook(layer_idx)\n",
    "        )\n",
    "        hook_handles.append(h)\n",
    "    for layer_idx in HIDDEN_LAYERS:\n",
    "        h = encoder_text.layers[layer_idx].register_forward_hook(\n",
    "            make_hidden_hook(layer_idx)\n",
    "        )\n",
    "        hook_handles.append(h)\n",
    "\n",
    "\n",
    "def remove_hooks():\n",
    "    global hook_handles\n",
    "    for h in hook_handles:\n",
    "        h.remove()\n",
    "    hook_handles = []\n",
    "\n",
    "\n",
    "def clear_captures():\n",
    "    captured_attn.clear()\n",
    "    captured_hidden.clear()\n",
    "\n",
    "\n",
    "# Token counting helper (same as Exp 3D)\n",
    "def count_prefix_tokens(prefix_text, document_text):\n",
    "    full_text = prefix_text + \"\\n\" + document_text\n",
    "    full_ids = tokenizer(full_text, add_special_tokens=True, truncation=True,\n",
    "                         max_length=2048).input_ids\n",
    "    doc_ids = tokenizer(document_text, add_special_tokens=True, truncation=True,\n",
    "                        max_length=2048).input_ids\n",
    "    return len(full_ids) - len(doc_ids)\n",
    "\n",
    "\n",
    "print(\"Hooks and helpers defined.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ddff9de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T14:13:14.589862Z",
     "iopub.status.busy": "2026-02-18T14:13:14.589031Z",
     "iopub.status.idle": "2026-02-18T14:13:15.203239Z",
     "shell.execute_reply": "2026-02-18T14:13:15.202510Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conditions: ['bare', 'oracle_trunc', 'random_matched_trunc', 'repeat_the_trunc']\n",
      "\n",
      "Prefix token counts (first 50 samples):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  oracle_trunc                 mean=22.1, range=[17, 38]\n",
      "  random_matched_trunc         mean=24.2, range=[17, 41]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  repeat_the_trunc             mean=18.4, range=[16, 23]\n",
      "\n",
      "Example (sample 0):\n",
      "  Query (21w): What are some of the conditions that require a reduction in lumber or connector plate design values ...\n",
      "  Document (530w): Designing for Damp Conditions\n",
      "Designing for Damp Conditions\n",
      "a reduction in lumbe...\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Generate conditions for each sample (same as Exp 3D)\n",
    "\n",
    "# Build word pool from unrelated documents\n",
    "other_words_pool = []\n",
    "for i, s in enumerate(samples):\n",
    "    other_idx = (i + N_SAMPLES // 2) % N_SAMPLES\n",
    "    other_doc = samples[other_idx]['document']\n",
    "    other_words_pool.append(other_doc.split())\n",
    "\n",
    "for i, s in enumerate(samples):\n",
    "    query_words = s['query'].split()\n",
    "    n_query_words = len(query_words)\n",
    "    other_words = other_words_pool[i]\n",
    "\n",
    "    # random_matched: N random words from unrelated doc\n",
    "    if len(other_words) >= n_query_words:\n",
    "        s['random_matched'] = \" \".join(other_words[:n_query_words])\n",
    "    else:\n",
    "        padded = other_words * ((n_query_words // len(other_words)) + 1)\n",
    "        s['random_matched'] = \" \".join(padded[:n_query_words])\n",
    "\n",
    "    # repeat_the: \"the\" repeated N times\n",
    "    s['repeat_the'] = \" \".join([\"the\"] * n_query_words)\n",
    "\n",
    "COND_NAMES = ['bare', 'oracle_trunc', 'random_matched_trunc', 'repeat_the_trunc']\n",
    "\n",
    "# Show prefix token stats\n",
    "print(f\"Conditions: {COND_NAMES}\")\n",
    "print(f\"\\nPrefix token counts (first 50 samples):\")\n",
    "for c in COND_NAMES:\n",
    "    if c == 'bare':\n",
    "        continue\n",
    "    if c == 'oracle_trunc':\n",
    "        toks = [count_prefix_tokens(s['query'], s['document']) for s in samples[:50]]\n",
    "    else:\n",
    "        key = c.replace('_trunc', '')\n",
    "        toks = [count_prefix_tokens(s[key], s['document']) for s in samples[:50]]\n",
    "    print(f\"  {c:<28} mean={np.mean(toks):.1f}, range=[{min(toks)}, {max(toks)}]\")\n",
    "\n",
    "# Example\n",
    "ex = samples[0]\n",
    "print(f\"\\nExample (sample 0):\")\n",
    "print(f\"  Query ({ex['query_words']}w): {ex['query'][:100]}...\")\n",
    "print(f\"  Document ({ex['doc_words']}w): {ex['document'][:80]}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76c59461",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T14:13:15.206754Z",
     "iopub.status.busy": "2026-02-18T14:13:15.206484Z",
     "iopub.status.idle": "2026-02-18T14:22:33.148874Z",
     "shell.execute_reply": "2026-02-18T14:22:33.148067Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "EXTRACTION LOOP\n",
      "======================================================================\n",
      "Starting fresh: 500 samples x 4 conditions = 2000 forward passes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63dabb861ef44624a5753e6b37bf4c95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 20/500 | 0.4m | ETA 9.1m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 40/500 | 0.7m | ETA 8.4m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 60/500 | 1.1m | ETA 8.0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 80/500 | 1.5m | ETA 7.7m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 100/500 | 1.8m | ETA 7.3m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 120/500 | 2.2m | ETA 6.9m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 140/500 | 2.6m | ETA 6.6m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 160/500 | 2.9m | ETA 6.2m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 180/500 | 3.3m | ETA 5.9m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 200/500 | 3.7m | ETA 5.5m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 220/500 | 4.0m | ETA 5.1m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 240/500 | 4.4m | ETA 4.8m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 260/500 | 4.8m | ETA 4.4m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 280/500 | 5.2m | ETA 4.1m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 300/500 | 5.5m | ETA 3.7m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 320/500 | 5.9m | ETA 3.3m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 340/500 | 6.3m | ETA 3.0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 360/500 | 6.6m | ETA 2.6m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 380/500 | 7.0m | ETA 2.2m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 400/500 | 7.4m | ETA 1.8m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 420/500 | 7.8m | ETA 1.5m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 440/500 | 8.2m | ETA 1.1m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 460/500 | 8.5m | ETA 0.7m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 480/500 | 8.9m | ETA 0.4m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 500/500 | 9.3m | ETA 0.0m\n",
      "\n",
      "Extraction complete: 500 samples, 4 conditions in 9.3 min\n",
      "NLL cross-check: bare=1.3132, oracle=1.2228\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Run extraction loop\n",
    "# For each sample x condition: run encoder, capture attention + hidden states,\n",
    "# compute all probe metrics on-the-fly, store only aggregated results.\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"EXTRACTION LOOP\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Initialize accumulators for all probes\n",
    "# Indexed by [condition][layer] where appropriate.\n",
    "\n",
    "# Probe A: mean attention mass on prefix per doc token, per layer/head\n",
    "# Shape per entry: (n_heads,) -- mean across doc tokens and samples\n",
    "probe_a_mass = {c: {l: [] for l in ATTN_LAYERS} for c in COND_NAMES if c != 'bare'}\n",
    "# Also store per-head mass distribution across doc token positions (mean across samples)\n",
    "probe_a_by_position = {c: {l: [] for l in ATTN_LAYERS} for c in COND_NAMES if c != 'bare'}\n",
    "\n",
    "# Probe B: attention entropy per doc token, per layer\n",
    "# Store: mean entropy per layer, for each condition\n",
    "probe_b_entropy = {c: {l: [] for l in ATTN_LAYERS} for c in COND_NAMES}\n",
    "\n",
    "# Probe C: doc-doc attention pattern divergence\n",
    "# KL divergence between bare doc-doc attention and prefixed doc-doc attention\n",
    "probe_c_kl = {c: {l: [] for l in ATTN_LAYERS} for c in COND_NAMES if c != 'bare'}\n",
    "# Also: entropy of doc-doc sub-pattern (separate from full entropy)\n",
    "probe_c_docdoc_entropy = {c: {l: [] for l in ATTN_LAYERS} for c in COND_NAMES}\n",
    "\n",
    "# Probe D: representation shift magnitude per layer\n",
    "# Mean L2 distance of doc token reps (bare vs prefixed)\n",
    "probe_d_shift = {c: {l: [] for l in HIDDEN_LAYERS} for c in COND_NAMES if c != 'bare'}\n",
    "# Per-position shift (binned into 10 position bins)\n",
    "N_POS_BINS = 10\n",
    "probe_d_by_position = {c: {l: [] for l in HIDDEN_LAYERS} for c in COND_NAMES if c != 'bare'}\n",
    "\n",
    "# Probe E: shift direction similarity\n",
    "# Cosine similarity between (h_oracle - h_bare) and (h_X - h_bare) per doc token per layer\n",
    "probe_e_cosine = {c: {l: [] for l in HIDDEN_LAYERS}\n",
    "                  for c in COND_NAMES if c not in ('bare', 'oracle_trunc')}\n",
    "\n",
    "# Probe F: attention sinks -- total attention received by each position\n",
    "# For bare: which doc positions absorb most attention\n",
    "# For prefixed: do prefix tokens absorb it instead\n",
    "probe_f_received = {c: {l: [] for l in ATTN_LAYERS} for c in COND_NAMES}\n",
    "# Separate: total attention received by prefix positions\n",
    "probe_f_prefix_received = {c: {l: [] for l in ATTN_LAYERS}\n",
    "                           for c in COND_NAMES if c != 'bare'}\n",
    "\n",
    "# NLL scores for cross-reference with Exp 3D\n",
    "nll_scores = {c: [] for c in COND_NAMES}\n",
    "\n",
    "# Per-sample metadata\n",
    "sample_meta = []\n",
    "\n",
    "# ---- Helper: compute attention entropy ----\n",
    "def attn_entropy(weights, mask=None):\n",
    "    # weights: (n_heads, seq, seq) -- attention probabilities\n",
    "    # mask: (seq,) boolean -- True for positions to include\n",
    "    # Returns: (n_heads, seq) mean entropy per head per query position\n",
    "    eps = 1e-10\n",
    "    # Only compute for positions indicated by mask\n",
    "    log_w = torch.log(weights + eps)\n",
    "    ent = -(weights * log_w).sum(dim=-1)  # (n_heads, seq)\n",
    "    return ent\n",
    "\n",
    "# ---- Helper: encode and extract ----\n",
    "def encode_and_extract(text):\n",
    "    # Returns encoder outputs (via hooks on text_model layers) and input_ids\n",
    "    enc_ids = tokenizer(text, return_tensors=\"pt\",\n",
    "                        add_special_tokens=True, truncation=True,\n",
    "                        max_length=2048).input_ids.to(DEVICE)\n",
    "    enc_mask = torch.ones(1, enc_ids.shape[1], device=DEVICE, dtype=torch.long)\n",
    "    clear_captures()\n",
    "    with torch.no_grad():\n",
    "        encoder_outputs = model.get_encoder()(\n",
    "            input_ids=enc_ids, attention_mask=enc_mask\n",
    "        )\n",
    "    return enc_ids, encoder_outputs\n",
    "\n",
    "# ---- Helper: score NLL ----\n",
    "def score_nll(encoder_outputs, total_enc_len, answer_text, prefix_token_count=0,\n",
    "              truncate=False):\n",
    "    if truncate and prefix_token_count > 0:\n",
    "        cross_attn_mask = torch.ones(1, total_enc_len, device=DEVICE, dtype=torch.long)\n",
    "        cross_attn_mask[:, :prefix_token_count] = 0\n",
    "    else:\n",
    "        cross_attn_mask = torch.ones(1, total_enc_len, device=DEVICE, dtype=torch.long)\n",
    "\n",
    "    ans_ids = tokenizer(answer_text, return_tensors=\"pt\",\n",
    "                        add_special_tokens=False, truncation=True,\n",
    "                        max_length=256).input_ids.to(DEVICE)\n",
    "    if ans_ids.shape[1] == 0:\n",
    "        return 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            encoder_outputs=encoder_outputs,\n",
    "            attention_mask=cross_attn_mask,\n",
    "            labels=ans_ids,\n",
    "        )\n",
    "\n",
    "    logits = outputs.logits\n",
    "    log_probs = F.log_softmax(logits, dim=-1)\n",
    "    token_log_probs = log_probs[0].gather(1, ans_ids[0].unsqueeze(1)).squeeze(1)\n",
    "    mean_nll = -token_log_probs.mean().item()\n",
    "\n",
    "    del outputs, logits, log_probs\n",
    "    return mean_nll\n",
    "\n",
    "# ---- Main loop ----\n",
    "register_hooks()\n",
    "\n",
    "start_idx = 0\n",
    "if CHECKPOINT_PATH.exists():\n",
    "    ckpt = json.loads(CHECKPOINT_PATH.read_text())\n",
    "    if (ckpt.get('n_total') == N_SAMPLES\n",
    "            and len(ckpt.get('sample_meta', [])) > 0):\n",
    "        saved_qs = [m['query'][:50] for m in ckpt['sample_meta']]\n",
    "        current_qs = [s['query'][:50] for s in samples[:len(saved_qs)]]\n",
    "        if saved_qs == current_qs:\n",
    "            start_idx = len(saved_qs)\n",
    "            # Restore accumulators\n",
    "            for key in ['probe_a_mass', 'probe_b_entropy', 'probe_c_kl',\n",
    "                        'probe_c_docdoc_entropy', 'probe_d_shift',\n",
    "                        'probe_d_by_position', 'probe_e_cosine',\n",
    "                        'probe_f_received', 'probe_f_prefix_received',\n",
    "                        'nll_scores', 'sample_meta']:\n",
    "                saved = ckpt.get(key)\n",
    "                if saved is not None:\n",
    "                    local_var = locals()[key]\n",
    "                    if isinstance(local_var, dict) and isinstance(saved, dict):\n",
    "                        # Nested dict: convert string keys back to int\n",
    "                        for ck, cv in saved.items():\n",
    "                            if isinstance(cv, dict):\n",
    "                                local_var[ck] = {int(lk): lv for lk, lv in cv.items()}\n",
    "                            else:\n",
    "                                local_var[ck] = cv\n",
    "                    elif isinstance(local_var, list):\n",
    "                        local_var.clear()\n",
    "                        local_var.extend(saved)\n",
    "            print(f\"Resuming from checkpoint: {start_idx}/{N_SAMPLES}\")\n",
    "\n",
    "if start_idx == 0:\n",
    "    print(f\"Starting fresh: {N_SAMPLES} samples x {len(COND_NAMES)} conditions \"\n",
    "          f\"= {N_SAMPLES * len(COND_NAMES)} forward passes\")\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "for sample_idx in tqdm(range(start_idx, N_SAMPLES), initial=start_idx,\n",
    "                       total=N_SAMPLES, desc=\"Extracting\"):\n",
    "    s = samples[sample_idx]\n",
    "\n",
    "    # ---- Build encoder texts for each condition ----\n",
    "    cond_texts = {}\n",
    "    cond_ptoks = {}\n",
    "    for c in COND_NAMES:\n",
    "        if c == 'bare':\n",
    "            cond_texts[c] = s['document']\n",
    "            cond_ptoks[c] = 0\n",
    "        elif c == 'oracle_trunc':\n",
    "            cond_texts[c] = s['query'] + \"\\n\" + s['document']\n",
    "            cond_ptoks[c] = count_prefix_tokens(s['query'], s['document'])\n",
    "        else:\n",
    "            key = c.replace('_trunc', '')\n",
    "            cond_texts[c] = s[key] + \"\\n\" + s['document']\n",
    "            cond_ptoks[c] = count_prefix_tokens(s[key], s['document'])\n",
    "\n",
    "    # ---- Run encoder for each condition ----\n",
    "    cond_attn = {}    # condition -> layer -> attn_weights\n",
    "    cond_hidden = {}  # condition -> layer -> hidden_states\n",
    "\n",
    "    for c in COND_NAMES:\n",
    "        enc_ids, enc_out = encode_and_extract(cond_texts[c])\n",
    "        seq_len = enc_ids.shape[1]\n",
    "\n",
    "        # Copy captured tensors (they'll be overwritten next forward pass)\n",
    "        cond_attn[c] = {l: captured_attn[l].clone() for l in ATTN_LAYERS\n",
    "                        if l in captured_attn}\n",
    "        cond_hidden[c] = {l: captured_hidden[l].clone() for l in HIDDEN_LAYERS\n",
    "                          if l in captured_hidden}\n",
    "\n",
    "        # Score NLL for cross-reference\n",
    "        nll = score_nll(enc_out, seq_len, s['answer'],\n",
    "                        cond_ptoks[c], truncate=(c != 'bare'))\n",
    "        nll_scores[c].append(nll)\n",
    "\n",
    "        del enc_ids, enc_out\n",
    "        clear_captures()\n",
    "\n",
    "    # ---- Record metadata ----\n",
    "    bare_seq_len = list(cond_hidden['bare'].values())[0].shape[1]\n",
    "    sample_meta.append({\n",
    "        'query': s['query'],\n",
    "        'query_words': s['query_words'],\n",
    "        'doc_words': s['doc_words'],\n",
    "        'bare_seq_len': bare_seq_len,\n",
    "        'prefix_tokens': {c: cond_ptoks[c] for c in COND_NAMES},\n",
    "    })\n",
    "\n",
    "    # ---- Compute probe metrics ----\n",
    "    n_doc_bare = bare_seq_len  # all tokens are \"document\" in bare\n",
    "\n",
    "    for c in COND_NAMES:\n",
    "        ptoks = cond_ptoks[c]\n",
    "        n_doc = (list(cond_hidden[c].values())[0].shape[1]) - ptoks\n",
    "\n",
    "        for l in ATTN_LAYERS:\n",
    "            if l not in cond_attn[c]:\n",
    "                continue\n",
    "            # attn: (1, n_heads, seq, seq) -> (n_heads, seq, seq)\n",
    "            attn = cond_attn[c][l][0]\n",
    "            n_heads = attn.shape[0]\n",
    "            seq = attn.shape[1]\n",
    "\n",
    "            if c != 'bare':\n",
    "                # Probe A: attention mass on prefix from each doc token\n",
    "                # doc tokens are positions [ptoks:], prefix at [:ptoks]\n",
    "                doc_to_prefix = attn[:, ptoks:, :ptoks]  # (heads, n_doc, ptoks)\n",
    "                mass_per_head = doc_to_prefix.sum(dim=-1).mean(dim=-1)  # (heads,)\n",
    "                probe_a_mass[c][l].append(mass_per_head.cpu().numpy().tolist())\n",
    "\n",
    "                # Position-dependent: mass on prefix by doc token position (10 bins)\n",
    "                doc_mass = doc_to_prefix.sum(dim=-1).mean(dim=0)  # (n_doc,)\n",
    "                if n_doc >= N_POS_BINS:\n",
    "                    bins = np.array_split(doc_mass.cpu().numpy(), N_POS_BINS)\n",
    "                    binned = [float(np.mean(b)) for b in bins]\n",
    "                else:\n",
    "                    binned = doc_mass.cpu().numpy().tolist()\n",
    "                probe_a_by_position[c][l].append(binned)\n",
    "\n",
    "            # Probe B: attention entropy for doc tokens\n",
    "            if c == 'bare':\n",
    "                doc_attn_rows = attn[:, :, :]  # all tokens are doc\n",
    "            else:\n",
    "                doc_attn_rows = attn[:, ptoks:, :]  # doc token rows\n",
    "            ent = attn_entropy(doc_attn_rows)  # (heads, n_doc)\n",
    "            mean_ent = ent.mean(dim=-1).mean(dim=0).item()  # scalar\n",
    "            probe_b_entropy[c][l].append(mean_ent)\n",
    "\n",
    "            # Probe C: doc-doc sub-attention\n",
    "            if c == 'bare':\n",
    "                docdoc = attn[:, :, :]  # all is doc-doc\n",
    "            else:\n",
    "                docdoc = attn[:, ptoks:, ptoks:]  # doc-to-doc submatrix\n",
    "            # Renormalize doc-doc to sum to 1 per row\n",
    "            docdoc_sum = docdoc.sum(dim=-1, keepdim=True).clamp(min=1e-10)\n",
    "            docdoc_norm = docdoc / docdoc_sum\n",
    "            ent_dd = attn_entropy(docdoc_norm)\n",
    "            mean_ent_dd = ent_dd.mean(dim=-1).mean(dim=0).item()\n",
    "            probe_c_docdoc_entropy[c][l].append(mean_ent_dd)\n",
    "\n",
    "            if c != 'bare':\n",
    "                # KL(bare_docdoc || prefixed_docdoc) for matching doc positions\n",
    "                # bare doc-doc: all positions are doc\n",
    "                bare_attn = cond_attn['bare'][l][0]\n",
    "                bare_n = bare_attn.shape[1]\n",
    "                # Use the last min(bare_n, n_doc) positions for alignment\n",
    "                align_n = min(bare_n, n_doc)\n",
    "                if align_n > 0:\n",
    "                    bare_dd = bare_attn[:, -align_n:, -align_n:]\n",
    "                    pref_dd = docdoc_norm[:, -align_n:, -align_n:]\n",
    "                    # Renormalize bare to same window\n",
    "                    bare_dd_sum = bare_dd.sum(dim=-1, keepdim=True).clamp(min=1e-10)\n",
    "                    bare_dd_norm = bare_dd / bare_dd_sum\n",
    "                    # KL divergence per position per head, then mean\n",
    "                    eps = 1e-10\n",
    "                    kl = (bare_dd_norm * (torch.log(bare_dd_norm + eps)\n",
    "                                          - torch.log(pref_dd + eps)))\n",
    "                    kl = kl.sum(dim=-1).mean(dim=-1).mean(dim=0).item()\n",
    "                    probe_c_kl[c][l].append(kl)\n",
    "                else:\n",
    "                    probe_c_kl[c][l].append(0.0)\n",
    "\n",
    "            # Probe F: attention received per position (attention sink)\n",
    "            # Sum attention each position receives from all other positions\n",
    "            received = attn.sum(dim=1).mean(dim=0)  # (seq,) mean across heads\n",
    "            # Normalize by seq_len so it's comparable across conditions\n",
    "            received = received / seq\n",
    "            if c == 'bare':\n",
    "                # Store stats about top-k sinks\n",
    "                top_vals, top_idxs = received.topk(min(5, seq))\n",
    "                probe_f_received[c][l].append({\n",
    "                    'top5_vals': top_vals.cpu().numpy().tolist(),\n",
    "                    'top5_idxs': top_idxs.cpu().numpy().tolist(),\n",
    "                    'first_pos_val': received[0].item(),\n",
    "                    'mean_val': received.mean().item(),\n",
    "                })\n",
    "            else:\n",
    "                prefix_recv = received[:ptoks].mean().item() if ptoks > 0 else 0\n",
    "                doc_recv = received[ptoks:].mean().item()\n",
    "                top_vals, top_idxs = received.topk(min(5, seq))\n",
    "                probe_f_received[c][l].append({\n",
    "                    'prefix_mean_recv': prefix_recv,\n",
    "                    'doc_mean_recv': doc_recv,\n",
    "                    'top5_vals': top_vals.cpu().numpy().tolist(),\n",
    "                    'top5_idxs': top_idxs.cpu().numpy().tolist(),\n",
    "                })\n",
    "                probe_f_prefix_received[c][l].append(prefix_recv)\n",
    "\n",
    "        # Probe D: representation shift\n",
    "        if c != 'bare':\n",
    "            for l in HIDDEN_LAYERS:\n",
    "                if l not in cond_hidden[c] or l not in cond_hidden['bare']:\n",
    "                    continue\n",
    "                h_bare = cond_hidden['bare'][l][0]   # (bare_seq, hidden)\n",
    "                h_pref = cond_hidden[c][l][0]        # (pref_seq, hidden)\n",
    "\n",
    "                # Align on last n_doc positions (document tokens)\n",
    "                align_n = min(h_bare.shape[0], n_doc)\n",
    "                if align_n > 0:\n",
    "                    h_b = h_bare[-align_n:]\n",
    "                    h_p = h_pref[-align_n:]\n",
    "                    shifts = (h_p - h_b).norm(dim=-1)  # (align_n,)\n",
    "                    probe_d_shift[c][l].append(shifts.mean().item())\n",
    "\n",
    "                    # Position-binned shifts\n",
    "                    if align_n >= N_POS_BINS:\n",
    "                        bins = np.array_split(shifts.cpu().numpy(), N_POS_BINS)\n",
    "                        binned = [float(np.mean(b)) for b in bins]\n",
    "                    else:\n",
    "                        binned = shifts.cpu().numpy().tolist()\n",
    "                    probe_d_by_position[c][l].append(binned)\n",
    "\n",
    "        # Probe E: shift direction cosine similarity\n",
    "        if c not in ('bare', 'oracle_trunc'):\n",
    "            for l in HIDDEN_LAYERS:\n",
    "                if (l not in cond_hidden[c] or l not in cond_hidden['bare']\n",
    "                        or l not in cond_hidden.get('oracle_trunc', {})):\n",
    "                    continue\n",
    "                h_bare = cond_hidden['bare'][l][0]\n",
    "                h_oracle = cond_hidden['oracle_trunc'][l][0]\n",
    "                h_other = cond_hidden[c][l][0]\n",
    "\n",
    "                n_doc_oracle = h_oracle.shape[0] - cond_ptoks['oracle_trunc']\n",
    "                n_doc_other = h_other.shape[0] - cond_ptoks[c]\n",
    "                align_n = min(h_bare.shape[0], n_doc_oracle, n_doc_other)\n",
    "\n",
    "                if align_n > 0:\n",
    "                    shift_oracle = h_oracle[-align_n:] - h_bare[-align_n:]\n",
    "                    shift_other = h_other[-align_n:] - h_bare[-align_n:]\n",
    "                    # Cosine similarity per token\n",
    "                    cos = F.cosine_similarity(shift_oracle, shift_other, dim=-1)\n",
    "                    probe_e_cosine[c][l].append(cos.mean().item())\n",
    "\n",
    "    # Free condition tensors\n",
    "    del cond_attn, cond_hidden\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # Checkpoint every 20 samples\n",
    "    if (sample_idx + 1) % 20 == 0 or sample_idx == N_SAMPLES - 1:\n",
    "        # Convert all accumulators to serializable form\n",
    "        def to_serializable(obj):\n",
    "            if isinstance(obj, dict):\n",
    "                return {str(k): to_serializable(v) for k, v in obj.items()}\n",
    "            elif isinstance(obj, list):\n",
    "                return obj\n",
    "            elif isinstance(obj, (np.floating, np.integer)):\n",
    "                return float(obj)\n",
    "            return obj\n",
    "\n",
    "        ckpt = {\n",
    "            'n_total': N_SAMPLES,\n",
    "            'probe_a_mass': to_serializable(probe_a_mass),\n",
    "            'probe_b_entropy': to_serializable(probe_b_entropy),\n",
    "            'probe_c_kl': to_serializable(probe_c_kl),\n",
    "            'probe_c_docdoc_entropy': to_serializable(probe_c_docdoc_entropy),\n",
    "            'probe_d_shift': to_serializable(probe_d_shift),\n",
    "            'probe_d_by_position': to_serializable(probe_d_by_position),\n",
    "            'probe_e_cosine': to_serializable(probe_e_cosine),\n",
    "            'probe_f_received': to_serializable(probe_f_received),\n",
    "            'probe_f_prefix_received': to_serializable(probe_f_prefix_received),\n",
    "            'nll_scores': nll_scores,\n",
    "            'sample_meta': sample_meta,\n",
    "            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        }\n",
    "        CHECKPOINT_PATH.write_text(json.dumps(ckpt))\n",
    "        elapsed = time.time() - t0\n",
    "        done = sample_idx - start_idx + 1\n",
    "        eta = (N_SAMPLES - sample_idx - 1) * elapsed / done if done > 0 else 0\n",
    "        tqdm.write(f\"  Checkpoint {sample_idx+1}/{N_SAMPLES} | \"\n",
    "                   f\"{elapsed/60:.1f}m | ETA {eta/60:.1f}m\")\n",
    "\n",
    "remove_hooks()\n",
    "\n",
    "elapsed = time.time() - t0\n",
    "print(f\"\\nExtraction complete: {N_SAMPLES} samples, \"\n",
    "      f\"{len(COND_NAMES)} conditions in {elapsed/60:.1f} min\")\n",
    "print(f\"NLL cross-check: bare={np.mean(nll_scores['bare']):.4f}, \"\n",
    "      f\"oracle={np.mean(nll_scores['oracle_trunc']):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99e342de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T14:22:33.152749Z",
     "iopub.status.busy": "2026-02-18T14:22:33.152455Z",
     "iopub.status.idle": "2026-02-18T14:22:33.194462Z",
     "shell.execute_reply": "2026-02-18T14:22:33.193727Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PROBE A: ATTENTION MASS ON PREFIX\n",
      "======================================================================\n",
      "For prefixed conditions, what fraction of each document token's attention\n",
      "goes to prefix tokens? By layer and head.\n",
      "\n",
      "\n",
      "--- oracle_trunc ---\n",
      " Layer       Type   Mean mass      Std   Min head   Max head\n",
      "------------------------------------------------------------\n",
      "     0        slid      0.0430   0.0240     0.0005     0.1002\n",
      "     5        full      0.1324   0.0643     0.0765     0.2213\n",
      "    11        full      0.1416   0.0734     0.0979     0.2374\n",
      "    17        full      0.1516   0.0858     0.0941     0.2326\n",
      "    23        full      0.2489   0.1470     0.1628     0.3324\n",
      "    29        full      0.2628   0.1717     0.1192     0.3386\n",
      "\n",
      "--- random_matched_trunc ---\n",
      " Layer       Type   Mean mass      Std   Min head   Max head\n",
      "------------------------------------------------------------\n",
      "     0        slid      0.0423   0.0236     0.0005     0.0944\n",
      "     5        full      0.1339   0.0686     0.0724     0.2203\n",
      "    11        full      0.1197   0.0661     0.0473     0.2152\n",
      "    17        full      0.1324   0.0774     0.0606     0.2194\n",
      "    23        full      0.2350   0.1394     0.1463     0.3268\n",
      "    29        full      0.2512   0.1629     0.1146     0.3267\n",
      "\n",
      "--- repeat_the_trunc ---\n",
      " Layer       Type   Mean mass      Std   Min head   Max head\n",
      "------------------------------------------------------------\n",
      "     0        slid      0.0406   0.0219     0.0003     0.1351\n",
      "     5        full      0.0979   0.0499     0.0407     0.1743\n",
      "    11        full      0.0992   0.0616     0.0330     0.1907\n",
      "    17        full      0.1194   0.0792     0.0501     0.2052\n",
      "    23        full      0.2247   0.1483     0.1411     0.3090\n",
      "    29        full      0.2402   0.1710     0.1105     0.3070\n",
      "\n",
      "\n",
      "--- Cross-condition comparison (full-attention layers only) ---\n",
      " Layer     oracle     random   repeat_the   orc-rand\n",
      "------------------------------------------------------------\n",
      "     5     0.1324     0.1339       0.0979    -0.0015\n",
      "    11     0.1416     0.1197       0.0992    +0.0219\n",
      "    17     0.1516     0.1324       0.1194    +0.0192\n",
      "    23     0.2489     0.2350       0.2247    +0.0139\n",
      "    29     0.2628     0.2512       0.2402    +0.0115\n",
      "\n",
      "\n",
      "--- Position-dependent prefix attention mass (oracle, mean across heads) ---\n",
      "Position bin: 0=start of doc, 9=end of doc\n",
      "  Layer 5: 0.161 0.140 0.134 0.130 0.130 0.126 0.124 0.123 0.126 0.129\n",
      "  Layer 11: 0.179 0.168 0.155 0.144 0.138 0.132 0.126 0.123 0.122 0.126\n",
      "  Layer 17: 0.191 0.180 0.168 0.157 0.150 0.143 0.138 0.133 0.130 0.125\n",
      "  Layer 23: 0.287 0.283 0.273 0.262 0.253 0.244 0.234 0.225 0.218 0.210\n",
      "  Layer 29: 0.316 0.303 0.289 0.274 0.262 0.254 0.246 0.240 0.229 0.214\n",
      "\n",
      "Key question: is mass UNIFORM across doc positions or CONCENTRATED?\n",
      "  Layer 5: CV (coeff of variation) = 0.079 (uniform)\n",
      "  Layer 11: CV (coeff of variation) = 0.133 (uniform)\n",
      "  Layer 17: CV (coeff of variation) = 0.140 (uniform)\n",
      "  Layer 23: CV (coeff of variation) = 0.104 (uniform)\n",
      "  Layer 29: CV (coeff of variation) = 0.118 (uniform)\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Probe A â€” Attention mass on prefix\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PROBE A: ATTENTION MASS ON PREFIX\")\n",
    "print(\"=\" * 70)\n",
    "print(\"For prefixed conditions, what fraction of each document token's attention\")\n",
    "print(\"goes to prefix tokens? By layer and head.\\n\")\n",
    "\n",
    "for c in ['oracle_trunc', 'random_matched_trunc', 'repeat_the_trunc']:\n",
    "    print(f\"\\n--- {c} ---\")\n",
    "    print(f\"{'Layer':>6} {'Type':>10} {'Mean mass':>11} {'Std':>8} {'Min head':>10} {'Max head':>10}\")\n",
    "    print(\"-\" * 60)\n",
    "    for l in ATTN_LAYERS:\n",
    "        data = probe_a_mass[c][l]\n",
    "        if not data:\n",
    "            continue\n",
    "        # data is list of (n_heads,) per sample -> (N, n_heads)\n",
    "        arr = np.array(data)  # (N, n_heads)\n",
    "        mean_per_head = arr.mean(axis=0)  # (n_heads,)\n",
    "        overall_mean = mean_per_head.mean()\n",
    "        overall_std = arr.mean(axis=1).std()\n",
    "        lt = layer_types[l][:4]\n",
    "        print(f\"  {l:>4}  {lt:>10} {overall_mean:>11.4f} {overall_std:>8.4f} \"\n",
    "              f\"{mean_per_head.min():>10.4f} {mean_per_head.max():>10.4f}\")\n",
    "\n",
    "# Compare across conditions at full-attention layers\n",
    "print(f\"\\n\\n--- Cross-condition comparison (full-attention layers only) ---\")\n",
    "print(f\"{'Layer':>6} {'oracle':>10} {'random':>10} {'repeat_the':>12} {'orc-rand':>10}\")\n",
    "print(\"-\" * 60)\n",
    "for l in full_attn_layers:\n",
    "    o_mass = np.array(probe_a_mass['oracle_trunc'][l]).mean()\n",
    "    r_mass = np.array(probe_a_mass['random_matched_trunc'][l]).mean()\n",
    "    t_mass = np.array(probe_a_mass['repeat_the_trunc'][l]).mean()\n",
    "    print(f\"  {l:>4} {o_mass:>10.4f} {r_mass:>10.4f} {t_mass:>12.4f} \"\n",
    "          f\"{o_mass - r_mass:>+10.4f}\")\n",
    "\n",
    "# Position-dependent attention mass (does prefix attract equally from all doc positions?)\n",
    "print(f\"\\n\\n--- Position-dependent prefix attention mass (oracle, mean across heads) ---\")\n",
    "print(f\"Position bin: 0=start of doc, 9=end of doc\")\n",
    "for l in full_attn_layers:\n",
    "    data = probe_a_by_position['oracle_trunc'][l]\n",
    "    if not data or not all(len(d) == N_POS_BINS for d in data):\n",
    "        # Skip if variable length\n",
    "        continue\n",
    "    arr = np.array(data)  # (N, N_POS_BINS)\n",
    "    means = arr.mean(axis=0)\n",
    "    print(f\"  Layer {l}: \" + \" \".join(f\"{m:.3f}\" for m in means))\n",
    "\n",
    "print(f\"\\nKey question: is mass UNIFORM across doc positions or CONCENTRATED?\")\n",
    "for l in full_attn_layers:\n",
    "    data = probe_a_by_position['oracle_trunc'][l]\n",
    "    if not data or not all(len(d) == N_POS_BINS for d in data):\n",
    "        continue\n",
    "    arr = np.array(data).mean(axis=0)\n",
    "    cv = np.std(arr) / np.mean(arr) if np.mean(arr) > 0 else 0\n",
    "    print(f\"  Layer {l}: CV (coeff of variation) = {cv:.3f} \"\n",
    "          f\"({'uniform' if cv < 0.3 else 'concentrated'})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6b879cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T14:22:33.197837Z",
     "iopub.status.busy": "2026-02-18T14:22:33.197317Z",
     "iopub.status.idle": "2026-02-18T14:22:33.230379Z",
     "shell.execute_reply": "2026-02-18T14:22:33.229679Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PROBE B: ATTENTION ENTROPY\n",
      "======================================================================\n",
      "Does the prefix INCREASE entropy (dilution/regularization)\n",
      "or DECREASE it (focusing)?\n",
      "\n",
      " Layer       Type       bare     oracle     random     repeat   orc-bare  rand-bare\n",
      "-------------------------------------------------------------------------------------\n",
      "     0        slid      4.066      4.094      4.098      4.094     +0.028     +0.033\n",
      "     5        full      3.517      3.541      3.542      3.588     +0.024     +0.024\n",
      "    11        full      3.844      3.865      3.882      3.870     +0.021     +0.037\n",
      "    17        full      3.809      3.837      3.833      3.843     +0.028     +0.025\n",
      "    23        full      2.687      2.725      2.771      2.726     +0.038     +0.083\n",
      "    29        full      1.805      1.839      1.871      1.838     +0.034     +0.066\n",
      "\n",
      "--- Statistical tests ---\n",
      "  oracle_trunc layer 5 (full): d=+0.909, p=2.30e-67 ***\n",
      "  oracle_trunc layer 11 (full): d=+0.502, p=3.33e-26 ***\n",
      "  oracle_trunc layer 17 (full): d=+0.495, p=1.26e-25 ***\n",
      "  oracle_trunc layer 23 (full): d=+0.396, p=1.38e-17 ***\n",
      "  oracle_trunc layer 29 (full): d=+0.273, p=2.02e-09 ***\n",
      "  random_matched_trunc layer 5 (full): d=+0.800, p=1.18e-55 ***\n",
      "  random_matched_trunc layer 11 (full): d=+0.723, p=1.48e-47 ***\n",
      "  random_matched_trunc layer 17 (full): d=+0.378, p=3.08e-16 ***\n",
      "  random_matched_trunc layer 23 (full): d=+0.772, p=1.08e-52 ***\n",
      "  random_matched_trunc layer 29 (full): d=+0.485, p=9.39e-25 ***\n",
      "  repeat_the_trunc layer 5 (full): d=+1.620, p=9.33e-142 ***\n",
      "  repeat_the_trunc layer 11 (full): d=+0.758, p=2.90e-51 ***\n",
      "  repeat_the_trunc layer 17 (full): d=+0.606, p=8.54e-36 ***\n",
      "  repeat_the_trunc layer 23 (full): d=+0.381, p=1.88e-16 ***\n",
      "  repeat_the_trunc layer 29 (full): d=+0.241, p=1.11e-07 ***\n",
      "\n",
      "--- Summary ---\n",
      "  oracle_trunc: entropy INCREASES in 6/6 layers\n",
      "  random_matched_trunc: entropy INCREASES in 6/6 layers\n",
      "  repeat_the_trunc: entropy INCREASES in 6/6 layers\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Probe B â€” Attention entropy\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PROBE B: ATTENTION ENTROPY\")\n",
    "print(\"=\" * 70)\n",
    "print(\"Does the prefix INCREASE entropy (dilution/regularization)\")\n",
    "print(\"or DECREASE it (focusing)?\\n\")\n",
    "\n",
    "print(f\"{'Layer':>6} {'Type':>10} {'bare':>10} {'oracle':>10} {'random':>10} \"\n",
    "      f\"{'repeat':>10} {'orc-bare':>10} {'rand-bare':>10}\")\n",
    "print(\"-\" * 85)\n",
    "\n",
    "entropy_changes = {c: [] for c in COND_NAMES if c != 'bare'}\n",
    "\n",
    "for l in ATTN_LAYERS:\n",
    "    bare_ent = np.mean(probe_b_entropy['bare'][l])\n",
    "    lt = layer_types[l][:4]\n",
    "    vals = [f\"  {l:>4}  {lt:>10} {bare_ent:>10.3f}\"]\n",
    "    for c in ['oracle_trunc', 'random_matched_trunc', 'repeat_the_trunc']:\n",
    "        c_ent = np.mean(probe_b_entropy[c][l])\n",
    "        vals.append(f\"{c_ent:>10.3f}\")\n",
    "    # Deltas\n",
    "    for c in ['oracle_trunc', 'random_matched_trunc']:\n",
    "        c_ent = np.mean(probe_b_entropy[c][l])\n",
    "        delta = c_ent - bare_ent\n",
    "        vals.append(f\"{delta:>+10.3f}\")\n",
    "        entropy_changes[c].append(delta)\n",
    "    print(\" \".join(vals))\n",
    "\n",
    "# Statistical test: is entropy change significant?\n",
    "print(f\"\\n--- Statistical tests ---\")\n",
    "for c in ['oracle_trunc', 'random_matched_trunc', 'repeat_the_trunc']:\n",
    "    for l in ATTN_LAYERS:\n",
    "        bare_arr = np.array(probe_b_entropy['bare'][l])\n",
    "        cond_arr = np.array(probe_b_entropy[c][l])\n",
    "        if len(bare_arr) == len(cond_arr) and len(bare_arr) > 1:\n",
    "            diff = cond_arr - bare_arr\n",
    "            d = cohens_d(diff)\n",
    "            _, p = stats.ttest_1samp(diff, 0)\n",
    "            sig = '***' if p < 0.001 else '**' if p < 0.01 else '*' if p < 0.05 else 'ns'\n",
    "            if l in full_attn_layers:\n",
    "                print(f\"  {c} layer {l} (full): d={d:+.3f}, p={p:.2e} {sig}\")\n",
    "\n",
    "# Summary\n",
    "print(f\"\\n--- Summary ---\")\n",
    "for c in ['oracle_trunc', 'random_matched_trunc', 'repeat_the_trunc']:\n",
    "    increases = 0\n",
    "    decreases = 0\n",
    "    for l in ATTN_LAYERS:\n",
    "        bare_ent = np.mean(probe_b_entropy['bare'][l])\n",
    "        cond_ent = np.mean(probe_b_entropy[c][l])\n",
    "        if cond_ent > bare_ent:\n",
    "            increases += 1\n",
    "        else:\n",
    "            decreases += 1\n",
    "    direction = \"INCREASES\" if increases > decreases else \"DECREASES\"\n",
    "    print(f\"  {c}: entropy {direction} in {increases}/{len(ATTN_LAYERS)} layers\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9daf5c53",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T14:22:33.233695Z",
     "iopub.status.busy": "2026-02-18T14:22:33.233113Z",
     "iopub.status.idle": "2026-02-18T14:22:33.253211Z",
     "shell.execute_reply": "2026-02-18T14:22:33.252543Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PROBE C: DOC-DOC ATTENTION REDISTRIBUTION\n",
      "======================================================================\n",
      "After removing attention to prefix, how does the remaining doc-doc\n",
      "attention pattern compare to bare?\n",
      "\n",
      " Layer    bare dd    oracle dd    random dd    repeat dd   orc-bare\n",
      "---------------------------------------------------------------------------\n",
      "     0      4.066        4.112        4.112        4.112     +0.046\n",
      "     5      3.517        3.556        3.555        3.601     +0.038\n",
      "    11      3.844        3.938        3.943        3.951     +0.093\n",
      "    17      3.809        3.916        3.905        3.926     +0.107\n",
      "    23      2.687        2.799        2.835        2.795     +0.112\n",
      "    29      1.805        1.750        1.765        1.738     -0.055\n",
      "\n",
      "--- KL divergence: bare doc-doc || prefixed doc-doc ---\n",
      " Layer       Type    oracle KL    random KL    repeat KL\n",
      "------------------------------------------------------------\n",
      "     0        slid       0.0944       0.0945       0.0944\n",
      "     5        full       0.4800       0.5033       0.5203\n",
      "    11        full       0.5515       0.5565       0.5349\n",
      "    17        full       0.9462       0.8845       0.8860\n",
      "    23        full       2.0111       1.9873       1.9236\n",
      "    29        full       2.9658       2.7937       2.8111\n",
      "\n",
      "--- Does oracle redistribute more than random? ---\n",
      "  Layer 5: d=-0.142, p=1.61e-03 ** [random redistributes more]\n",
      "  Layer 11: d=-0.043, p=3.37e-01 ns [random redistributes more]\n",
      "  Layer 17: d=+0.256, p=1.88e-08 *** [oracle redistributes more]\n",
      "  Layer 23: d=+0.042, p=3.53e-01 ns [oracle redistributes more]\n",
      "  Layer 29: d=+0.174, p=1.10e-04 *** [oracle redistributes more]\n",
      "\n",
      "Interpretation:\n",
      "  High KL = prefix causes large change to doc-doc attention pattern\n",
      "  Similar KL across conditions = redistribution is structural\n",
      "  Oracle KL > Random KL = oracle causes content-specific redistribution\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Probe C â€” Document-document attention redistribution\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PROBE C: DOC-DOC ATTENTION REDISTRIBUTION\")\n",
    "print(\"=\" * 70)\n",
    "print(\"After removing attention to prefix, how does the remaining doc-doc\")\n",
    "print(\"attention pattern compare to bare?\\n\")\n",
    "\n",
    "# Doc-doc entropy\n",
    "print(f\"{'Layer':>6} {'bare dd':>10} {'oracle dd':>12} {'random dd':>12} \"\n",
    "      f\"{'repeat dd':>12} {'orc-bare':>10}\")\n",
    "print(\"-\" * 75)\n",
    "\n",
    "for l in ATTN_LAYERS:\n",
    "    bare_dd = np.mean(probe_c_docdoc_entropy['bare'][l])\n",
    "    vals = [f\"  {l:>4} {bare_dd:>10.3f}\"]\n",
    "    for c in ['oracle_trunc', 'random_matched_trunc', 'repeat_the_trunc']:\n",
    "        c_dd = np.mean(probe_c_docdoc_entropy[c][l])\n",
    "        vals.append(f\"{c_dd:>12.3f}\")\n",
    "    delta = np.mean(probe_c_docdoc_entropy['oracle_trunc'][l]) - bare_dd\n",
    "    vals.append(f\"{delta:>+10.3f}\")\n",
    "    print(\" \".join(vals))\n",
    "\n",
    "# KL divergence\n",
    "print(f\"\\n--- KL divergence: bare doc-doc || prefixed doc-doc ---\")\n",
    "print(f\"{'Layer':>6} {'Type':>10} {'oracle KL':>12} {'random KL':>12} {'repeat KL':>12}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for l in ATTN_LAYERS:\n",
    "    lt = layer_types[l][:4]\n",
    "    vals = [f\"  {l:>4}  {lt:>10}\"]\n",
    "    for c in ['oracle_trunc', 'random_matched_trunc', 'repeat_the_trunc']:\n",
    "        kl = np.mean(probe_c_kl[c][l]) if probe_c_kl[c][l] else 0\n",
    "        vals.append(f\"{kl:>12.4f}\")\n",
    "    print(\" \".join(vals))\n",
    "\n",
    "# Key question: does oracle redistribute MORE than random?\n",
    "print(f\"\\n--- Does oracle redistribute more than random? ---\")\n",
    "for l in full_attn_layers:\n",
    "    o_kl = np.array(probe_c_kl['oracle_trunc'][l])\n",
    "    r_kl = np.array(probe_c_kl['random_matched_trunc'][l])\n",
    "    if len(o_kl) > 1 and len(r_kl) > 1:\n",
    "        diff = o_kl - r_kl\n",
    "        d = cohens_d(diff)\n",
    "        _, p = stats.ttest_1samp(diff, 0)\n",
    "        sig = '***' if p < 0.001 else '**' if p < 0.01 else '*' if p < 0.05 else 'ns'\n",
    "        winner = \"oracle\" if d > 0 else \"random\"\n",
    "        print(f\"  Layer {l}: d={d:+.3f}, p={p:.2e} {sig} [{winner} redistributes more]\")\n",
    "\n",
    "print(f\"\\nInterpretation:\")\n",
    "print(f\"  High KL = prefix causes large change to doc-doc attention pattern\")\n",
    "print(f\"  Similar KL across conditions = redistribution is structural\")\n",
    "print(f\"  Oracle KL > Random KL = oracle causes content-specific redistribution\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97879302",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T14:22:33.256225Z",
     "iopub.status.busy": "2026-02-18T14:22:33.255873Z",
     "iopub.status.idle": "2026-02-18T14:22:33.280315Z",
     "shell.execute_reply": "2026-02-18T14:22:33.279628Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PROBE D: REPRESENTATION SHIFT MAGNITUDE\n",
      "======================================================================\n",
      "L2 distance between bare and prefixed doc token representations.\n",
      "\n",
      " Layer    oracle L2    random L2    repeat L2   orc/rand\n",
      "------------------------------------------------------------\n",
      "     0      14.9720      15.6198      16.4955       0.96x\n",
      "     5      72.6610      77.1320      88.4581       0.94x\n",
      "    11     210.4406     206.9711     186.9697       1.02x\n",
      "    17     666.2358     684.7180     528.4344       0.97x\n",
      "    23    1466.2288    1562.9356    1188.2344       0.94x\n",
      "    29    2362.4877    2522.1909    1978.7692       0.94x\n",
      "    33    4331.4442    4850.9914    3635.6309       0.89x\n",
      "\n",
      "--- Shift growth across layers ---\n",
      "  oracle_trunc: first layer=14.9720, last layer=4331.4442, ratio=289.3x\n",
      "  random_matched_trunc: first layer=15.6198, last layer=4850.9914, ratio=310.6x\n",
      "  repeat_the_trunc: first layer=16.4955, last layer=3635.6309, ratio=220.4x\n",
      "\n",
      "--- Position-dependent shift (last hidden layer) ---\n",
      "Position bin: 0=start of doc, 9=end of doc\n",
      "  oracle_trunc: 9416.162 5035.506 4441.514 4223.903 3920.301 3791.659 3549.649 3364.228 3066.494 2422.984\n",
      "  random_matched_trunc: 9225.955 5449.468 4935.862 4839.350 4497.511 4394.355 4201.650 4044.319 3730.882 3124.357\n",
      "  repeat_the_trunc: 7387.137 3893.830 3678.396 3656.314 3382.239 3290.838 3117.368 2943.788 2719.172 2233.822\n",
      "\n",
      "--- Oracle vs Random shift magnitude ---\n",
      "  Layer 0: d=-0.153, p=6.64e-04 ***\n",
      "  Layer 5: d=-0.196, p=1.46e-05 ***\n",
      "  Layer 11: d=+0.082, p=6.74e-02 ns\n",
      "  Layer 17: d=-0.100, p=2.52e-02 *\n",
      "  Layer 23: d=-0.233, p=2.76e-07 ***\n",
      "  Layer 29: d=-0.253, p=2.57e-08 ***\n",
      "  Layer 33: d=-0.402, p=5.39e-18 ***\n",
      "\n",
      "Key question: if oracle and random shift by SIMILAR amounts,\n",
      "  the shift is structural. If oracle shifts MORE, there's a semantic component.\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Probe D â€” Representation shift magnitude\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PROBE D: REPRESENTATION SHIFT MAGNITUDE\")\n",
    "print(\"=\" * 70)\n",
    "print(\"L2 distance between bare and prefixed doc token representations.\\n\")\n",
    "\n",
    "print(f\"{'Layer':>6} {'oracle L2':>12} {'random L2':>12} {'repeat L2':>12} \"\n",
    "      f\"{'orc/rand':>10}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for l in HIDDEN_LAYERS:\n",
    "    vals = [f\"  {l:>4}\"]\n",
    "    o_shift = np.mean(probe_d_shift['oracle_trunc'][l])\n",
    "    r_shift = np.mean(probe_d_shift['random_matched_trunc'][l])\n",
    "    t_shift = np.mean(probe_d_shift['repeat_the_trunc'][l])\n",
    "    ratio = o_shift / r_shift if r_shift > 0 else 0\n",
    "    vals.append(f\"{o_shift:>12.4f}\")\n",
    "    vals.append(f\"{r_shift:>12.4f}\")\n",
    "    vals.append(f\"{t_shift:>12.4f}\")\n",
    "    vals.append(f\"{ratio:>10.2f}x\")\n",
    "    print(\" \".join(vals))\n",
    "\n",
    "# Does shift grow with layer depth?\n",
    "print(f\"\\n--- Shift growth across layers ---\")\n",
    "for c in ['oracle_trunc', 'random_matched_trunc', 'repeat_the_trunc']:\n",
    "    shifts = [np.mean(probe_d_shift[c][l]) for l in HIDDEN_LAYERS\n",
    "              if probe_d_shift[c][l]]\n",
    "    if len(shifts) >= 2:\n",
    "        ratio = shifts[-1] / shifts[0] if shifts[0] > 0 else 0\n",
    "        print(f\"  {c}: first layer={shifts[0]:.4f}, last layer={shifts[-1]:.4f}, \"\n",
    "              f\"ratio={ratio:.1f}x\")\n",
    "\n",
    "# Position-dependent shift\n",
    "print(f\"\\n--- Position-dependent shift (last hidden layer) ---\")\n",
    "last_l = HIDDEN_LAYERS[-1]\n",
    "print(f\"Position bin: 0=start of doc, 9=end of doc\")\n",
    "for c in ['oracle_trunc', 'random_matched_trunc', 'repeat_the_trunc']:\n",
    "    data = probe_d_by_position[c][last_l]\n",
    "    if data and all(len(d) == N_POS_BINS for d in data):\n",
    "        arr = np.array(data).mean(axis=0)\n",
    "        print(f\"  {c}: \" + \" \".join(f\"{v:.3f}\" for v in arr))\n",
    "\n",
    "# Statistical test: does oracle shift MORE than random?\n",
    "print(f\"\\n--- Oracle vs Random shift magnitude ---\")\n",
    "for l in HIDDEN_LAYERS:\n",
    "    o_arr = np.array(probe_d_shift['oracle_trunc'][l])\n",
    "    r_arr = np.array(probe_d_shift['random_matched_trunc'][l])\n",
    "    if len(o_arr) > 1 and len(r_arr) > 1:\n",
    "        diff = o_arr - r_arr\n",
    "        d = cohens_d(diff)\n",
    "        _, p = stats.ttest_1samp(diff, 0)\n",
    "        sig = '***' if p < 0.001 else '**' if p < 0.01 else '*' if p < 0.05 else 'ns'\n",
    "        print(f\"  Layer {l}: d={d:+.3f}, p={p:.2e} {sig}\")\n",
    "\n",
    "print(f\"\\nKey question: if oracle and random shift by SIMILAR amounts,\")\n",
    "print(f\"  the shift is structural. If oracle shifts MORE, there's a semantic component.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d76f9b14",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T14:22:33.283576Z",
     "iopub.status.busy": "2026-02-18T14:22:33.283303Z",
     "iopub.status.idle": "2026-02-18T14:22:33.298699Z",
     "shell.execute_reply": "2026-02-18T14:22:33.298010Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PROBE E: REPRESENTATION SHIFT DIRECTION\n",
      "======================================================================\n",
      "Cosine similarity between shift vectors: (h_oracle - h_bare) vs (h_X - h_bare)\n",
      "High cosine = all prefixes push in same direction (structural)\n",
      "Low cosine = different prefixes push differently (semantic)\n",
      "\n",
      " Layer    rand vs orc    repeat vs orc       interpretation\n",
      "------------------------------------------------------------\n",
      "     0         0.4894         0.5012                MIXED\n",
      "     5         0.3949         0.2233                MIXED\n",
      "    11         0.2708         0.2305             SEMANTIC\n",
      "    17         0.2793         0.2641             SEMANTIC\n",
      "    23         0.2950         0.2903             SEMANTIC\n",
      "    29         0.3055         0.3002                MIXED\n",
      "    33         0.2989         0.3018                MIXED\n",
      "\n",
      "--- Distribution across samples (layer 29) ---\n",
      "  random_matched_trunc:\n",
      "    mean=0.3055, std=0.1110\n",
      "    10th=0.172, 25th=0.231, median=0.303, 75th=0.380, 90th=0.442\n",
      "    % > 0.5: 4.6%\n",
      "    % > 0.7: 0.0%\n",
      "  repeat_the_trunc:\n",
      "    mean=0.3002, std=0.0926\n",
      "    10th=0.190, 25th=0.235, median=0.294, 75th=0.361, 90th=0.420\n",
      "    % > 0.5: 2.4%\n",
      "    % > 0.7: 0.0%\n",
      "\n",
      "--- random vs repeat_the shift direction ---\n",
      "  Layer 0: random-vs-oracle=0.4894, repeat-vs-oracle=0.5012\n",
      "  Layer 5: random-vs-oracle=0.3949, repeat-vs-oracle=0.2233\n",
      "  Layer 11: random-vs-oracle=0.2708, repeat-vs-oracle=0.2305\n",
      "  Layer 17: random-vs-oracle=0.2793, repeat-vs-oracle=0.2641\n",
      "  Layer 23: random-vs-oracle=0.2950, repeat-vs-oracle=0.2903\n",
      "  Layer 29: random-vs-oracle=0.3055, repeat-vs-oracle=0.3002\n",
      "  Layer 33: random-vs-oracle=0.2989, repeat-vs-oracle=0.3018\n",
      "\n",
      "Summary:\n",
      "  Overall mean cosine: 0.3175\n",
      "  --> MIXED: partially structural, partially content-dependent\n"
     ]
    }
   ],
   "source": [
    "# Cell 10: Probe E â€” Representation shift direction\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PROBE E: REPRESENTATION SHIFT DIRECTION\")\n",
    "print(\"=\" * 70)\n",
    "print(\"Cosine similarity between shift vectors: (h_oracle - h_bare) vs (h_X - h_bare)\")\n",
    "print(\"High cosine = all prefixes push in same direction (structural)\")\n",
    "print(\"Low cosine = different prefixes push differently (semantic)\\n\")\n",
    "\n",
    "print(f\"{'Layer':>6} {'rand vs orc':>14} {'repeat vs orc':>16} {'interpretation':>20}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for l in HIDDEN_LAYERS:\n",
    "    vals = [f\"  {l:>4}\"]\n",
    "    cosines = {}\n",
    "    for c in ['random_matched_trunc', 'repeat_the_trunc']:\n",
    "        if probe_e_cosine[c][l]:\n",
    "            cos_mean = np.mean(probe_e_cosine[c][l])\n",
    "            cosines[c] = cos_mean\n",
    "            vals.append(f\"{cos_mean:>14.4f}\")\n",
    "        else:\n",
    "            vals.append(f\"{'N/A':>14}\")\n",
    "\n",
    "    # Interpretation\n",
    "    if cosines:\n",
    "        avg_cos = np.mean(list(cosines.values()))\n",
    "        if avg_cos > 0.7:\n",
    "            interp = \"STRUCTURAL\"\n",
    "        elif avg_cos > 0.3:\n",
    "            interp = \"MIXED\"\n",
    "        else:\n",
    "            interp = \"SEMANTIC\"\n",
    "        vals.append(f\"{interp:>20}\")\n",
    "    print(\" \".join(vals))\n",
    "\n",
    "# Distribution across samples (for the last full-attention layer)\n",
    "print(f\"\\n--- Distribution across samples (layer {full_attn_layers[-1]}) ---\")\n",
    "target_l = full_attn_layers[-1]\n",
    "for c in ['random_matched_trunc', 'repeat_the_trunc']:\n",
    "    if probe_e_cosine[c][target_l]:\n",
    "        arr = np.array(probe_e_cosine[c][target_l])\n",
    "        print(f\"  {c}:\")\n",
    "        print(f\"    mean={arr.mean():.4f}, std={arr.std():.4f}\")\n",
    "        pcts = np.percentile(arr, [10, 25, 50, 75, 90])\n",
    "        print(f\"    10th={pcts[0]:.3f}, 25th={pcts[1]:.3f}, median={pcts[2]:.3f}, \"\n",
    "              f\"75th={pcts[3]:.3f}, 90th={pcts[4]:.3f}\")\n",
    "        print(f\"    % > 0.5: {100*np.mean(arr > 0.5):.1f}%\")\n",
    "        print(f\"    % > 0.7: {100*np.mean(arr > 0.7):.1f}%\")\n",
    "\n",
    "# Cosine between random and repeat_the shifts (both non-oracle)\n",
    "print(f\"\\n--- random vs repeat_the shift direction ---\")\n",
    "for l in HIDDEN_LAYERS:\n",
    "    r_cos = probe_e_cosine.get('random_matched_trunc', {}).get(l, [])\n",
    "    t_cos = probe_e_cosine.get('repeat_the_trunc', {}).get(l, [])\n",
    "    if r_cos and t_cos:\n",
    "        # Both are vs oracle. If both have high cosine with oracle,\n",
    "        # they also have high cosine with each other.\n",
    "        r_mean = np.mean(r_cos)\n",
    "        t_mean = np.mean(t_cos)\n",
    "        print(f\"  Layer {l}: random-vs-oracle={r_mean:.4f}, \"\n",
    "              f\"repeat-vs-oracle={t_mean:.4f}\")\n",
    "\n",
    "print(f\"\\nSummary:\")\n",
    "overall_cos = []\n",
    "for c in ['random_matched_trunc', 'repeat_the_trunc']:\n",
    "    for l in HIDDEN_LAYERS:\n",
    "        if probe_e_cosine[c][l]:\n",
    "            overall_cos.append(np.mean(probe_e_cosine[c][l]))\n",
    "if overall_cos:\n",
    "    avg = np.mean(overall_cos)\n",
    "    print(f\"  Overall mean cosine: {avg:.4f}\")\n",
    "    if avg > 0.7:\n",
    "        print(f\"  --> Shift is overwhelmingly STRUCTURAL (same direction regardless of prefix)\")\n",
    "    elif avg > 0.3:\n",
    "        print(f\"  --> MIXED: partially structural, partially content-dependent\")\n",
    "    else:\n",
    "        print(f\"  --> Shift is content-DEPENDENT (different prefixes push differently)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "83eaa04d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T14:22:33.301980Z",
     "iopub.status.busy": "2026-02-18T14:22:33.301304Z",
     "iopub.status.idle": "2026-02-18T14:22:33.327477Z",
     "shell.execute_reply": "2026-02-18T14:22:33.326805Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PROBE F: ATTENTION SINK ANALYSIS\n",
      "======================================================================\n",
      "Which positions absorb the most attention from other tokens?\n",
      "Do prefix tokens take over the 'sink' role?\n",
      "\n",
      "--- Bare condition: top sink positions ---\n",
      "  Layer 5: first_pos receives 0.0916 (avg pos receives 0.0016), ratio=56.4x\n",
      "  Layer 11: first_pos receives 0.0987 (avg pos receives 0.0016), ratio=60.7x\n",
      "  Layer 17: first_pos receives 0.1165 (avg pos receives 0.0016), ratio=71.7x\n",
      "  Layer 23: first_pos receives 0.2129 (avg pos receives 0.0016), ratio=131.1x\n",
      "  Layer 29: first_pos receives 0.2322 (avg pos receives 0.0016), ratio=143.0x\n",
      "\n",
      "--- Prefixed conditions: prefix vs doc attention received ---\n",
      " Layer                  Condition    prefix recv     doc recv   prefix/doc\n",
      "---------------------------------------------------------------------------\n",
      "     5                oracle_trunc         0.0064       0.0013          4.8x\n",
      "     5        random_matched_trunc         0.0060       0.0013          4.5x\n",
      "     5            repeat_the_trunc         0.0059       0.0014          4.2x\n",
      "    11                oracle_trunc         0.0068       0.0013          5.2x\n",
      "    11        random_matched_trunc         0.0057       0.0013          4.3x\n",
      "    11            repeat_the_trunc         0.0059       0.0014          4.3x\n",
      "    17                oracle_trunc         0.0073       0.0013          5.7x\n",
      "    17        random_matched_trunc         0.0062       0.0013          4.8x\n",
      "    17            repeat_the_trunc         0.0071       0.0013          5.3x\n",
      "    23                oracle_trunc         0.0117       0.0011         10.9x\n",
      "    23        random_matched_trunc         0.0102       0.0011          9.3x\n",
      "    23            repeat_the_trunc         0.0125       0.0011         11.2x\n",
      "    29                oracle_trunc         0.0124       0.0010         12.0x\n",
      "    29        random_matched_trunc         0.0108       0.0011         10.2x\n",
      "    29            repeat_the_trunc         0.0133       0.0011         12.4x\n",
      "\n",
      "--- Oracle vs Random prefix attention received ---\n",
      "  Layer 5: d=+0.259, p=1.22e-08 *** [oracle receives more]\n",
      "  Layer 11: d=+0.766, p=4.45e-52 *** [oracle receives more]\n",
      "  Layer 17: d=+0.597, p=5.81e-35 *** [oracle receives more]\n",
      "  Layer 23: d=+0.441, p=4.53e-21 *** [oracle receives more]\n",
      "  Layer 29: d=+0.410, p=1.34e-18 *** [oracle receives more]\n",
      "\n",
      "--- Sink transfer hypothesis ---\n",
      "  Layer 5: bare pos0=0.0916, prefixed prefix_mean=0.0064, prefixed doc_mean=0.0013\n",
      "  Layer 11: bare pos0=0.0987, prefixed prefix_mean=0.0068, prefixed doc_mean=0.0013\n",
      "  Layer 17: bare pos0=0.1165, prefixed prefix_mean=0.0073, prefixed doc_mean=0.0013\n",
      "  Layer 23: bare pos0=0.2129, prefixed prefix_mean=0.0117, prefixed doc_mean=0.0011\n",
      "  Layer 29: bare pos0=0.2322, prefixed prefix_mean=0.0124, prefixed doc_mean=0.0010\n"
     ]
    }
   ],
   "source": [
    "# Cell 11: Probe F â€” Attention sink analysis\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PROBE F: ATTENTION SINK ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "print(\"Which positions absorb the most attention from other tokens?\")\n",
    "print(\"Do prefix tokens take over the 'sink' role?\\n\")\n",
    "\n",
    "# Bare: which positions are sinks?\n",
    "print(\"--- Bare condition: top sink positions ---\")\n",
    "for l in full_attn_layers:\n",
    "    data = probe_f_received['bare'][l]\n",
    "    if not data:\n",
    "        continue\n",
    "    # Average first-position attention received\n",
    "    first_vals = [d['first_pos_val'] for d in data]\n",
    "    mean_vals = [d['mean_val'] for d in data]\n",
    "    first_mean = np.mean(first_vals)\n",
    "    avg_mean = np.mean(mean_vals)\n",
    "    ratio = first_mean / avg_mean if avg_mean > 0 else 0\n",
    "    print(f\"  Layer {l}: first_pos receives {first_mean:.4f} \"\n",
    "          f\"(avg pos receives {avg_mean:.4f}), ratio={ratio:.1f}x\")\n",
    "\n",
    "# Prefixed: do prefix tokens absorb attention?\n",
    "print(f\"\\n--- Prefixed conditions: prefix vs doc attention received ---\")\n",
    "print(f\"{'Layer':>6} {'Condition':>26} {'prefix recv':>14} {'doc recv':>12} \"\n",
    "      f\"{'prefix/doc':>12}\")\n",
    "print(\"-\" * 75)\n",
    "\n",
    "for l in full_attn_layers:\n",
    "    for c in ['oracle_trunc', 'random_matched_trunc', 'repeat_the_trunc']:\n",
    "        data = probe_f_received[c][l]\n",
    "        if not data:\n",
    "            continue\n",
    "        p_recv = np.mean([d['prefix_mean_recv'] for d in data])\n",
    "        d_recv = np.mean([d['doc_mean_recv'] for d in data])\n",
    "        ratio = p_recv / d_recv if d_recv > 0 else 0\n",
    "        print(f\"  {l:>4}  {c:>26} {p_recv:>14.4f} {d_recv:>12.4f} {ratio:>12.1f}x\")\n",
    "\n",
    "# Does prefix absorb MORE attention for oracle vs random?\n",
    "print(f\"\\n--- Oracle vs Random prefix attention received ---\")\n",
    "for l in full_attn_layers:\n",
    "    o_data = probe_f_prefix_received['oracle_trunc'][l]\n",
    "    r_data = probe_f_prefix_received['random_matched_trunc'][l]\n",
    "    if len(o_data) > 1 and len(r_data) > 1:\n",
    "        o_arr = np.array(o_data)\n",
    "        r_arr = np.array(r_data)\n",
    "        diff = o_arr - r_arr\n",
    "        d = cohens_d(diff)\n",
    "        _, p = stats.ttest_1samp(diff, 0)\n",
    "        sig = '***' if p < 0.001 else '**' if p < 0.01 else '*' if p < 0.05 else 'ns'\n",
    "        winner = \"oracle\" if d > 0 else \"random\"\n",
    "        print(f\"  Layer {l}: d={d:+.3f}, p={p:.2e} {sig} [{winner} receives more]\")\n",
    "\n",
    "# Sink transfer: in bare, position 0 is often the sink.\n",
    "# With prefix, does position 0 of the prefix take over?\n",
    "print(f\"\\n--- Sink transfer hypothesis ---\")\n",
    "for l in full_attn_layers:\n",
    "    bare_data = probe_f_received['bare'][l]\n",
    "    pref_data = probe_f_received['oracle_trunc'][l]\n",
    "    if bare_data and pref_data:\n",
    "        bare_first = np.mean([d['first_pos_val'] for d in bare_data])\n",
    "        pref_prefix = np.mean([d['prefix_mean_recv'] for d in pref_data])\n",
    "        pref_doc_first = np.mean([d['doc_mean_recv'] for d in pref_data])\n",
    "        print(f\"  Layer {l}: bare pos0={bare_first:.4f}, \"\n",
    "              f\"prefixed prefix_mean={pref_prefix:.4f}, \"\n",
    "              f\"prefixed doc_mean={pref_doc_first:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "38c1fe1e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T14:22:33.330863Z",
     "iopub.status.busy": "2026-02-18T14:22:33.330574Z",
     "iopub.status.idle": "2026-02-18T14:22:34.006283Z",
     "shell.execute_reply": "2026-02-18T14:22:34.005320Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "SYNTHESIS: ATTENTION MECHANISM PROBING RESULTS\n",
      "======================================================================\n",
      "\n",
      "1. NLL CROSS-REFERENCE:\n",
      "   bare NLL:   1.3132\n",
      "   oracle NLL: 1.2228\n",
      "   headroom:   +0.0904 (d=+0.604)\n",
      "   (Should match Exp 3D results)\n",
      "\n",
      "2. PROBE A â€” ATTENTION MASS ON PREFIX:\n",
      "   Layer 5 oracle: 13.2% of doc attention -> prefix\n",
      "   Layer 5 random_matched: 13.4% of doc attention -> prefix\n",
      "   Layer 5 repeat_the: 9.8% of doc attention -> prefix\n",
      "   Layer 11 oracle: 14.2% of doc attention -> prefix\n",
      "   Layer 11 random_matched: 12.0% of doc attention -> prefix\n",
      "   Layer 11 repeat_the: 9.9% of doc attention -> prefix\n",
      "   Layer 17 oracle: 15.2% of doc attention -> prefix\n",
      "   Layer 17 random_matched: 13.2% of doc attention -> prefix\n",
      "   Layer 17 repeat_the: 11.9% of doc attention -> prefix\n",
      "   Layer 23 oracle: 24.9% of doc attention -> prefix\n",
      "   Layer 23 random_matched: 23.5% of doc attention -> prefix\n",
      "   Layer 23 repeat_the: 22.5% of doc attention -> prefix\n",
      "   Layer 29 oracle: 26.3% of doc attention -> prefix\n",
      "   Layer 29 random_matched: 25.1% of doc attention -> prefix\n",
      "   Layer 29 repeat_the: 24.0% of doc attention -> prefix\n",
      "\n",
      "3. PROBE B â€” ENTROPY CHANGE:\n",
      "   oracle_trunc: entropy INCREASES in 6/6 layers\n",
      "   random_matched_trunc: entropy INCREASES in 6/6 layers\n",
      "   repeat_the_trunc: entropy INCREASES in 6/6 layers\n",
      "\n",
      "4. PROBE C â€” DOC-DOC REDISTRIBUTION:\n",
      "   Layer 5: oracle KL=0.4800, random KL=0.5033, ratio=0.95x\n",
      "   Layer 11: oracle KL=0.5515, random KL=0.5565, ratio=0.99x\n",
      "   Layer 17: oracle KL=0.9462, random KL=0.8845, ratio=1.07x\n",
      "   Layer 23: oracle KL=2.0111, random KL=1.9873, ratio=1.01x\n",
      "   Layer 29: oracle KL=2.9658, random KL=2.7937, ratio=1.06x\n",
      "\n",
      "5. PROBE D â€” SHIFT MAGNITUDE:\n",
      "   Last layer (33) oracle_trunc: mean L2=4331.4442\n",
      "   Last layer (33) random_matched_trunc: mean L2=4850.9914\n",
      "   Last layer (33) repeat_the_trunc: mean L2=3635.6309\n",
      "   Oracle/Random ratio: 0.89x\n",
      "\n",
      "6. PROBE E â€” SHIFT DIRECTION (structural vs semantic):\n",
      "   Layer 0: mean cosine=0.4953 [MIXED]\n",
      "   Layer 5: mean cosine=0.3091 [MIXED]\n",
      "   Layer 11: mean cosine=0.2506 [SEMANTIC]\n",
      "   Layer 17: mean cosine=0.2717 [SEMANTIC]\n",
      "   Layer 23: mean cosine=0.2927 [SEMANTIC]\n",
      "   Layer 29: mean cosine=0.3028 [MIXED]\n",
      "   Layer 33: mean cosine=0.3004 [MIXED]\n",
      "\n",
      "7. PROBE F â€” ATTENTION SINKS:\n",
      "   Layer 5: bare sink=0.0916, prefix absorbs=0.0064\n",
      "   Layer 11: bare sink=0.0987, prefix absorbs=0.0068\n",
      "   Layer 17: bare sink=0.1165, prefix absorbs=0.0073\n",
      "   Layer 23: bare sink=0.2129, prefix absorbs=0.0117\n",
      "   Layer 29: bare sink=0.2322, prefix absorbs=0.0124\n",
      "\n",
      "======================================================================\n",
      "INTERPRETATION:\n",
      "\n",
      "  Hypothesis 1 (Attention redistribution):\n",
      "    Prefix absorbs 17.3% of doc token attention (full-attn layers)\n",
      "    Entropy increases -> dilution/regularization\n",
      "\n",
      "  Hypothesis 2 (RoPE position shift):\n",
      "    If dominant, shift magnitude would scale with prefix length\n",
      "    and direction would differ by prefix type.\n",
      "    Shift direction cosine: 0.3175 (divergent = possibly RoPE-driven)\n",
      "\n",
      "  Hypothesis 3 (Representation regularization):\n",
      "    Shift magnitudes: oracle=4331.4442, random=4850.9914, repeat=3635.6309\n",
      "    Oracle/Random: 0.89x (1.0 = purely structural)\n",
      "\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results saved to results/exp03e/results.json\n",
      "\n",
      "Cleaning up GPU memory...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory: 15.30 GB -> 0.29 GB\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Cell 12: Synthesis + save results\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"SYNTHESIS: ATTENTION MECHANISM PROBING RESULTS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# ---- NLL cross-reference with Exp 3D ----\n",
    "bare_nlls = np.array(nll_scores['bare'])\n",
    "oracle_nlls = np.array(nll_scores['oracle_trunc'])\n",
    "oracle_benefit = bare_nlls - oracle_nlls\n",
    "\n",
    "print(f\"\\n1. NLL CROSS-REFERENCE:\")\n",
    "print(f\"   bare NLL:   {bare_nlls.mean():.4f}\")\n",
    "print(f\"   oracle NLL: {oracle_nlls.mean():.4f}\")\n",
    "print(f\"   headroom:   {oracle_benefit.mean():+.4f} (d={cohens_d(oracle_benefit):+.3f})\")\n",
    "print(f\"   (Should match Exp 3D results)\")\n",
    "\n",
    "# ---- Probe A summary ----\n",
    "print(f\"\\n2. PROBE A â€” ATTENTION MASS ON PREFIX:\")\n",
    "for l in full_attn_layers:\n",
    "    for c in ['oracle_trunc', 'random_matched_trunc', 'repeat_the_trunc']:\n",
    "        if probe_a_mass[c][l]:\n",
    "            mass = np.array(probe_a_mass[c][l]).mean()\n",
    "            cname = c.replace('_trunc', '')\n",
    "            print(f\"   Layer {l} {cname}: {mass:.1%} of doc attention -> prefix\")\n",
    "\n",
    "# ---- Probe B summary ----\n",
    "print(f\"\\n3. PROBE B â€” ENTROPY CHANGE:\")\n",
    "for c in ['oracle_trunc', 'random_matched_trunc', 'repeat_the_trunc']:\n",
    "    increases = sum(1 for l in ATTN_LAYERS\n",
    "                    if np.mean(probe_b_entropy[c][l]) > np.mean(probe_b_entropy['bare'][l]))\n",
    "    direction = \"INCREASES\" if increases > len(ATTN_LAYERS)/2 else \"DECREASES\"\n",
    "    print(f\"   {c}: entropy {direction} in {increases}/{len(ATTN_LAYERS)} layers\")\n",
    "\n",
    "# ---- Probe C summary ----\n",
    "print(f\"\\n4. PROBE C â€” DOC-DOC REDISTRIBUTION:\")\n",
    "for l in full_attn_layers:\n",
    "    o_kl = np.mean(probe_c_kl['oracle_trunc'][l]) if probe_c_kl['oracle_trunc'][l] else 0\n",
    "    r_kl = np.mean(probe_c_kl['random_matched_trunc'][l]) if probe_c_kl['random_matched_trunc'][l] else 0\n",
    "    print(f\"   Layer {l}: oracle KL={o_kl:.4f}, random KL={r_kl:.4f}, \"\n",
    "          f\"ratio={o_kl/r_kl:.2f}x\" if r_kl > 0 else f\"   Layer {l}: oracle KL={o_kl:.4f}\")\n",
    "\n",
    "# ---- Probe D summary ----\n",
    "print(f\"\\n5. PROBE D â€” SHIFT MAGNITUDE:\")\n",
    "last_l = HIDDEN_LAYERS[-1]\n",
    "for c in ['oracle_trunc', 'random_matched_trunc', 'repeat_the_trunc']:\n",
    "    if probe_d_shift[c][last_l]:\n",
    "        shift = np.mean(probe_d_shift[c][last_l])\n",
    "        print(f\"   Last layer ({last_l}) {c}: mean L2={shift:.4f}\")\n",
    "o_shift = np.mean(probe_d_shift['oracle_trunc'][last_l])\n",
    "r_shift = np.mean(probe_d_shift['random_matched_trunc'][last_l])\n",
    "if r_shift > 0:\n",
    "    print(f\"   Oracle/Random ratio: {o_shift/r_shift:.2f}x\")\n",
    "\n",
    "# ---- Probe E summary ----\n",
    "print(f\"\\n6. PROBE E â€” SHIFT DIRECTION (structural vs semantic):\")\n",
    "for l in HIDDEN_LAYERS:\n",
    "    cosines = []\n",
    "    for c in ['random_matched_trunc', 'repeat_the_trunc']:\n",
    "        if probe_e_cosine[c][l]:\n",
    "            cosines.append(np.mean(probe_e_cosine[c][l]))\n",
    "    if cosines:\n",
    "        avg = np.mean(cosines)\n",
    "        label = \"STRUCTURAL\" if avg > 0.7 else \"MIXED\" if avg > 0.3 else \"SEMANTIC\"\n",
    "        print(f\"   Layer {l}: mean cosine={avg:.4f} [{label}]\")\n",
    "\n",
    "# ---- Probe F summary ----\n",
    "print(f\"\\n7. PROBE F â€” ATTENTION SINKS:\")\n",
    "for l in full_attn_layers:\n",
    "    if (probe_f_received['bare'][l] and\n",
    "            probe_f_received['oracle_trunc'][l]):\n",
    "        bare_first = np.mean([d['first_pos_val']\n",
    "                              for d in probe_f_received['bare'][l]])\n",
    "        pref_prefix = np.mean([d['prefix_mean_recv']\n",
    "                               for d in probe_f_received['oracle_trunc'][l]])\n",
    "        print(f\"   Layer {l}: bare sink={bare_first:.4f}, \"\n",
    "              f\"prefix absorbs={pref_prefix:.4f}\")\n",
    "\n",
    "# ---- Overall interpretation ----\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"INTERPRETATION:\")\n",
    "\n",
    "# Determine dominant mechanism\n",
    "# Check if shifts are structural (probe E cosine > 0.7)\n",
    "mean_cosines = []\n",
    "for c in ['random_matched_trunc', 'repeat_the_trunc']:\n",
    "    for l in HIDDEN_LAYERS:\n",
    "        if probe_e_cosine[c][l]:\n",
    "            mean_cosines.append(np.mean(probe_e_cosine[c][l]))\n",
    "overall_cos = np.mean(mean_cosines) if mean_cosines else 0\n",
    "\n",
    "# Check if entropy increases or decreases\n",
    "ent_increases = 0\n",
    "ent_total = 0\n",
    "for l in full_attn_layers:\n",
    "    for c in ['oracle_trunc', 'random_matched_trunc', 'repeat_the_trunc']:\n",
    "        bare_ent = np.mean(probe_b_entropy['bare'][l])\n",
    "        cond_ent = np.mean(probe_b_entropy[c][l])\n",
    "        ent_total += 1\n",
    "        if cond_ent > bare_ent:\n",
    "            ent_increases += 1\n",
    "ent_direction = \"increases\" if ent_increases > ent_total / 2 else \"decreases\"\n",
    "\n",
    "# Check prefix attention mass\n",
    "mean_mass = []\n",
    "for l in full_attn_layers:\n",
    "    for c in ['oracle_trunc', 'random_matched_trunc', 'repeat_the_trunc']:\n",
    "        if probe_a_mass[c][l]:\n",
    "            mean_mass.append(np.array(probe_a_mass[c][l]).mean())\n",
    "overall_mass = np.mean(mean_mass) if mean_mass else 0\n",
    "\n",
    "print(f\"\\n  Hypothesis 1 (Attention redistribution):\")\n",
    "print(f\"    Prefix absorbs {overall_mass:.1%} of doc token attention (full-attn layers)\")\n",
    "print(f\"    Entropy {ent_direction} -> {'dilution/regularization' if ent_direction == 'increases' else 'focusing'}\")\n",
    "\n",
    "print(f\"\\n  Hypothesis 2 (RoPE position shift):\")\n",
    "print(f\"    If dominant, shift magnitude would scale with prefix length\")\n",
    "print(f\"    and direction would differ by prefix type.\")\n",
    "print(f\"    Shift direction cosine: {overall_cos:.4f} \"\n",
    "      f\"({'consistent = not RoPE-driven' if overall_cos > 0.5 else 'divergent = possibly RoPE-driven'})\")\n",
    "\n",
    "print(f\"\\n  Hypothesis 3 (Representation regularization):\")\n",
    "o_last = np.mean(probe_d_shift['oracle_trunc'][last_l])\n",
    "r_last = np.mean(probe_d_shift['random_matched_trunc'][last_l])\n",
    "t_last = np.mean(probe_d_shift['repeat_the_trunc'][last_l])\n",
    "print(f\"    Shift magnitudes: oracle={o_last:.4f}, random={r_last:.4f}, \"\n",
    "      f\"repeat={t_last:.4f}\")\n",
    "if r_last > 0:\n",
    "    print(f\"    Oracle/Random: {o_last/r_last:.2f}x (1.0 = purely structural)\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "\n",
    "# Save results\n",
    "final_results = {\n",
    "    'experiment': 'exp03e_attention_probing',\n",
    "    'model': MODEL_NAME,\n",
    "    'dataset': 'neural-bridge/rag-dataset-12000',\n",
    "    'n_samples': N_SAMPLES,\n",
    "    'attn_implementation': 'eager',\n",
    "    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'architecture': {\n",
    "        'n_layers': n_layers,\n",
    "        'full_attn_layers': full_attn_layers,\n",
    "        'attn_probe_layers': ATTN_LAYERS,\n",
    "        'hidden_probe_layers': HIDDEN_LAYERS,\n",
    "    },\n",
    "    'nll_crossref': {\n",
    "        'bare_nll': float(bare_nlls.mean()),\n",
    "        'oracle_nll': float(oracle_nlls.mean()),\n",
    "        'oracle_d': float(cohens_d(oracle_benefit)),\n",
    "    },\n",
    "    'probe_a_prefix_mass': {\n",
    "        c: {str(l): float(np.array(probe_a_mass[c][l]).mean())\n",
    "            for l in full_attn_layers if probe_a_mass[c][l]}\n",
    "        for c in ['oracle_trunc', 'random_matched_trunc', 'repeat_the_trunc']\n",
    "    },\n",
    "    'probe_b_entropy': {\n",
    "        c: {str(l): float(np.mean(probe_b_entropy[c][l]))\n",
    "            for l in ATTN_LAYERS if probe_b_entropy[c][l]}\n",
    "        for c in COND_NAMES\n",
    "    },\n",
    "    'probe_c_kl': {\n",
    "        c: {str(l): float(np.mean(probe_c_kl[c][l]))\n",
    "            for l in ATTN_LAYERS if probe_c_kl[c][l]}\n",
    "        for c in ['oracle_trunc', 'random_matched_trunc', 'repeat_the_trunc']\n",
    "    },\n",
    "    'probe_d_shift': {\n",
    "        c: {str(l): float(np.mean(probe_d_shift[c][l]))\n",
    "            for l in HIDDEN_LAYERS if probe_d_shift[c][l]}\n",
    "        for c in ['oracle_trunc', 'random_matched_trunc', 'repeat_the_trunc']\n",
    "    },\n",
    "    'probe_e_cosine': {\n",
    "        c: {str(l): float(np.mean(probe_e_cosine[c][l]))\n",
    "            for l in HIDDEN_LAYERS if probe_e_cosine[c][l]}\n",
    "        for c in ['random_matched_trunc', 'repeat_the_trunc']\n",
    "    },\n",
    "    'probe_e_overall_cosine': float(overall_cos),\n",
    "    'interpretation': {\n",
    "        'prefix_mass_pct': float(overall_mass),\n",
    "        'entropy_direction': ent_direction,\n",
    "        'shift_direction_cosine': float(overall_cos),\n",
    "        'shift_structural': bool(overall_cos > 0.7),\n",
    "    },\n",
    "}\n",
    "\n",
    "with open(RESULTS_DIR / 'results.json', 'w') as f:\n",
    "    json.dump(final_results, f, indent=2)\n",
    "print(f\"\\nResults saved to {RESULTS_DIR / 'results.json'}\")\n",
    "\n",
    "# Cleanup\n",
    "print(f\"\\nCleaning up GPU memory...\")\n",
    "mem_before = torch.cuda.memory_allocated() / 1e9\n",
    "del model, processor, tokenizer, encoder_text\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "mem_after = torch.cuda.memory_allocated() / 1e9\n",
    "print(f\"GPU memory: {mem_before:.2f} GB -> {mem_after:.2f} GB\")\n",
    "print(\"Done!\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "034b3ede1b304ee7b9370a959df70bb1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_c481ae71c85f4f0080fafe2d942a341a",
       "max": 1327.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_9cec3c85978a42c49cbf84eb0ec5e907",
       "tabbable": null,
       "tooltip": null,
       "value": 1327.0
      }
     },
     "076dc1f5869c4f51ade6f8d23cbf56a8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "150daf59a48a49289efa474b750bb2bb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_3dd30c0335c846719dbcab46ffcac274",
       "max": 500.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_8fd2b724ba2942e8935de009f0f58adb",
       "tabbable": null,
       "tooltip": null,
       "value": 500.0
      }
     },
     "177eeed2d53d4e90925014005b612699": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_5fe08fe661fe4a4980a85c1c5752008d",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_7713f1ffb6714a159d144682afc2e74e",
       "tabbable": null,
       "tooltip": null,
       "value": "â€‡1327/1327â€‡[00:04&lt;00:00,â€‡655.40it/s,â€‡Materializingâ€‡param=model.encoder.vision_tower.vision_model.post_layernorm.weight]"
      }
     },
     "24632d90abd84de6af68c643259f63e4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "311861944a8d432288bc394e7992925f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_a0b4465d3f234f7b9efab2a9a716085d",
        "IPY_MODEL_034b3ede1b304ee7b9370a959df70bb1",
        "IPY_MODEL_177eeed2d53d4e90925014005b612699"
       ],
       "layout": "IPY_MODEL_24632d90abd84de6af68c643259f63e4",
       "tabbable": null,
       "tooltip": null
      }
     },
     "36a3de62b6e540e198fe1788e77efc1f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3dd30c0335c846719dbcab46ffcac274": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "42be2c12359546aba83d0a8c02ac661d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "51d89ab5fc724f5da6542c70fa576f83": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_db25e3fd360346e6a7537c70a9b0cbf1",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_b5b5a3e7b60d47c587bbf3b5bb379daa",
       "tabbable": null,
       "tooltip": null,
       "value": "â€‡500/500â€‡[09:17&lt;00:00,â€‡â€‡1.27s/it]"
      }
     },
     "5d2631b9bb3c4ea78206054f660896df": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "5fe08fe661fe4a4980a85c1c5752008d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "63dabb861ef44624a5753e6b37bf4c95": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_f9cb45d4b30b430da6876e1ba6a4659e",
        "IPY_MODEL_150daf59a48a49289efa474b750bb2bb",
        "IPY_MODEL_51d89ab5fc724f5da6542c70fa576f83"
       ],
       "layout": "IPY_MODEL_36a3de62b6e540e198fe1788e77efc1f",
       "tabbable": null,
       "tooltip": null
      }
     },
     "7713f1ffb6714a159d144682afc2e74e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "8fd2b724ba2942e8935de009f0f58adb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "9cec3c85978a42c49cbf84eb0ec5e907": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "a0b4465d3f234f7b9efab2a9a716085d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_076dc1f5869c4f51ade6f8d23cbf56a8",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_e1d1e602eeda4e43bf8a77ea8168de55",
       "tabbable": null,
       "tooltip": null,
       "value": "Loadingâ€‡weights:â€‡100%"
      }
     },
     "b5b5a3e7b60d47c587bbf3b5bb379daa": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "c481ae71c85f4f0080fafe2d942a341a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "db25e3fd360346e6a7537c70a9b0cbf1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e1d1e602eeda4e43bf8a77ea8168de55": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "f9cb45d4b30b430da6876e1ba6a4659e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_5d2631b9bb3c4ea78206054f660896df",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_42be2c12359546aba83d0a8c02ac661d",
       "tabbable": null,
       "tooltip": null,
       "value": "Extracting:â€‡100%"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
