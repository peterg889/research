{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62fd4324",
   "metadata": {},
   "source": [
    "# Experiment 01: Truncation Test -- Disentangling the Benefit## Does the benefit come from improved document representations or the decoder reading query tokens?### BackgroundExp 33b showed that encoding [query + document] in T5Gemma's bidirectional encoderdramatically helps answer prediction (oracle d=+0.345, surr_doc captures 96%).But the decoder cross-attended to ALL encoder tokens, including query/surrogate tokens.### The key questionIs the decoder just reading the query from the encoder output (trivial), or are thedocument representations genuinely improved by bidirectional co-encoding with the query?### Method: MaskingEncode [prefix + document] with full bidirectional attention (encoder sees everything).Then MASK the prefix tokens from decoder cross-attention. The decoder can only cross-attendto document positions.This is safe because T5Gemma2's cross-attention keys have NO RoPE applied -- there areno positional embeddings to invalidate when masking.### Conditions (6 total)| Condition | Encoder input | Decoder cross-attends to | Tests ||-----------|---------------|--------------------------|-------|| bare | [document] | all (=document) | Baseline || oracle_full | [query + doc] | all (query + doc) | Upper bound (=Exp 33b) || oracle_trunc | [query + doc] | document only | Value contamination || surr_para_full | [para + doc] | all (para + doc) | Surrogate upper bound || surr_para_trunc | [para + doc] | document only | Surrogate value contamination || surr_doc_full | [kw + doc] | all (kw + doc) | Doc-keyword upper bound || surr_doc_trunc | [kw + doc] | document only | Doc-keyword value contamination |### Success criteria- oracle_full >> bare (replicates 33b, expected d~+0.35)- oracle_trunc > bare (document reps improved, the real prize)- oracle_trunc / oracle_full > 30% (significant fraction from doc reps, not just query reading)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0686ec5c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-17T20:29:30.947886Z",
     "iopub.status.busy": "2026-02-17T20:29:30.947114Z",
     "iopub.status.idle": "2026-02-17T20:29:35.458604Z",
     "shell.execute_reply": "2026-02-17T20:29:35.457710Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exp 01: Truncation Test -- Disentangling the Benefit\n",
      "Model: google/t5gemma-2-4b-4b\n",
      "N: 200\n",
      "CUDA: NVIDIA A100-SXM4-40GB\n",
      "GPU memory: 42.3 GB\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Setup\n",
    "import os\n",
    "os.umask(0o000)\n",
    "\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "import gc\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from scipy import stats\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "sys.path.insert(0, \".\")\n",
    "\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "RESULTS_DIR = Path(\"results/exp01\")\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CHECKPOINT_PATH = RESULTS_DIR / \"checkpoint.json\"\n",
    "\n",
    "N_SAMPLES = 200\n",
    "MODEL_NAME = \"google/t5gemma-2-4b-4b\"\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "HF_TOKEN = os.environ.get(\"HF_TOKEN\")\n",
    "\n",
    "print(f\"Exp 01: Truncation Test -- Disentangling the Benefit\")\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"N: {N_SAMPLES}\")\n",
    "print(f\"CUDA: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ff47204",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-17T20:29:35.462489Z",
     "iopub.status.busy": "2026-02-17T20:29:35.461678Z",
     "iopub.status.idle": "2026-02-17T20:29:51.955205Z",
     "shell.execute_reply": "2026-02-17T20:29:51.954274Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading google/t5gemma-2-4b-4b...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a17f48147644df29d7adfbd6d9ab8ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/1327 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded. dtype=torch.bfloat16\n",
      "GPU memory used: 15.02 GB\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Load model\n",
    "from transformers import AutoProcessor, AutoModelForSeq2SeqLM\n",
    "\n",
    "print(f\"Loading {MODEL_NAME}...\")\n",
    "processor = AutoProcessor.from_pretrained(MODEL_NAME, token=HF_TOKEN)\n",
    "tokenizer = processor.tokenizer\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    token=HF_TOKEN,\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "DEVICE = next(model.parameters()).device\n",
    "print(f\"Model loaded. dtype={next(model.parameters()).dtype}\")\n",
    "print(f\"GPU memory used: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0597ea2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-17T20:29:51.959025Z",
     "iopub.status.busy": "2026-02-17T20:29:51.958571Z",
     "iopub.status.idle": "2026-02-17T20:29:51.974558Z",
     "shell.execute_reply": "2026-02-17T20:29:51.973743Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helpers defined.\n",
      "  score_nll(encoder_text, answer, prefix_token_count, truncate)\n",
      "  Key: truncate=True masks prefix from decoder cross-attention\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Scoring helpers with truncation support\n",
    "\n",
    "def score_nll(encoder_text, answer_text, prefix_token_count=0, truncate=False):\n",
    "    '''Score NLL of answer tokens with optional truncation.\n",
    "\n",
    "    Args:\n",
    "        encoder_text: Full text for encoder (e.g., \"[query]\\n[document]\")\n",
    "        answer_text: Answer text for decoder (NO query in decoder)\n",
    "        prefix_token_count: Number of prefix tokens (query/surrogate) to potentially mask\n",
    "        truncate: If True, mask prefix tokens from decoder cross-attention\n",
    "\n",
    "    When truncate=True:\n",
    "        - Encoder processes full [prefix + document] with bidirectional attention\n",
    "        - But decoder can only cross-attend to document positions\n",
    "        - Tests whether document representations are improved by co-encoding\n",
    "\n",
    "    When truncate=False:\n",
    "        - Decoder cross-attends to all encoder tokens (prefix + document)\n",
    "        - This is the Exp 33b setup\n",
    "    '''\n",
    "    # Tokenize encoder input\n",
    "    enc_ids = tokenizer(encoder_text, return_tensors=\"pt\",\n",
    "                        add_special_tokens=True, truncation=True,\n",
    "                        max_length=2048).input_ids.to(DEVICE)\n",
    "    total_enc_len = enc_ids.shape[1]\n",
    "\n",
    "    # Full mask for encoder (bidirectional, sees everything)\n",
    "    enc_mask = torch.ones(1, total_enc_len, device=DEVICE, dtype=torch.long)\n",
    "\n",
    "    # Run encoder with full bidirectional attention\n",
    "    with torch.no_grad():\n",
    "        encoder_outputs = model.get_encoder()(\n",
    "            input_ids=enc_ids, attention_mask=enc_mask\n",
    "        )\n",
    "\n",
    "    # Build cross-attention mask for decoder\n",
    "    if truncate and prefix_token_count > 0:\n",
    "        # Mask prefix tokens: decoder can only attend to document positions\n",
    "        cross_attn_mask = torch.ones(1, total_enc_len, device=DEVICE, dtype=torch.long)\n",
    "        cross_attn_mask[:, :prefix_token_count] = 0\n",
    "    else:\n",
    "        # Full cross-attention (decoder sees all encoder tokens)\n",
    "        cross_attn_mask = torch.ones(1, total_enc_len, device=DEVICE, dtype=torch.long)\n",
    "\n",
    "    # Tokenize answer for decoder\n",
    "    ans_ids = tokenizer(answer_text, return_tensors=\"pt\",\n",
    "                        add_special_tokens=False, truncation=True,\n",
    "                        max_length=256).input_ids.to(DEVICE)\n",
    "\n",
    "    if ans_ids.shape[1] == 0:\n",
    "        return 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            encoder_outputs=encoder_outputs,\n",
    "            attention_mask=cross_attn_mask,  # This controls decoder cross-attention\n",
    "            labels=ans_ids,\n",
    "        )\n",
    "\n",
    "    # Per-token NLL\n",
    "    logits = outputs.logits\n",
    "    log_probs = F.log_softmax(logits, dim=-1)\n",
    "    token_log_probs = log_probs[0].gather(1, ans_ids[0].unsqueeze(1)).squeeze(1)\n",
    "    mean_nll = -token_log_probs.mean().item()\n",
    "\n",
    "    del encoder_outputs, outputs, logits, log_probs\n",
    "    return mean_nll\n",
    "\n",
    "\n",
    "def count_prefix_tokens(prefix_text, document_text):\n",
    "    '''Count how many tokens the prefix occupies in the concatenated encoding.\n",
    "\n",
    "    Tokenizes \"[prefix]\\n[document]\" and \"[document]\" separately,\n",
    "    returns the difference in token count.\n",
    "    '''\n",
    "    full_text = prefix_text + \"\\n\" + document_text\n",
    "    full_ids = tokenizer(full_text, add_special_tokens=True).input_ids\n",
    "    doc_ids = tokenizer(document_text, add_special_tokens=True).input_ids\n",
    "    return len(full_ids) - len(doc_ids)\n",
    "\n",
    "\n",
    "# === Surrogate generation ===\n",
    "STOP_WORDS = {\n",
    "    'a', 'an', 'the', 'is', 'are', 'was', 'were', 'be', 'been', 'being',\n",
    "    'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could',\n",
    "    'should', 'may', 'might', 'can', 'shall', 'to', 'of', 'in', 'for',\n",
    "    'on', 'with', 'at', 'by', 'from', 'as', 'into', 'through', 'during',\n",
    "    'before', 'after', 'above', 'below', 'between', 'and', 'but', 'or',\n",
    "    'not', 'no', 'if', 'then', 'than', 'so', 'up', 'out', 'about',\n",
    "    'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those',\n",
    "    'it', 'its', 'i', 'me', 'my', 'we', 'our', 'you', 'your', 'he',\n",
    "    'him', 'his', 'she', 'her', 'they', 'them', 'their', 'how', 'when',\n",
    "    'where', 'why', 'much', 'many', 'some', 'any', 'all', 'each',\n",
    "    'does', 'also', 'just', 'more', 'most', 'very', 'too', 'only',\n",
    "}\n",
    "\n",
    "def extract_keywords(text):\n",
    "    words = re.sub(r'[^\\w\\s]', '', text.lower()).split()\n",
    "    return [w for w in words if w not in STOP_WORDS and len(w) > 2]\n",
    "\n",
    "def make_surrogate_paraphrase(query):\n",
    "    keywords = extract_keywords(query)\n",
    "    return \" \".join(keywords[::-1]) if keywords else query\n",
    "\n",
    "def make_surrogate_from_doc(passage):\n",
    "    content_words = extract_keywords(passage)\n",
    "    if not content_words:\n",
    "        return \"information\"\n",
    "    counts = Counter(content_words)\n",
    "    return \" \".join(w for w, _ in counts.most_common(5))\n",
    "\n",
    "print(\"Helpers defined.\")\n",
    "print(\"  score_nll(encoder_text, answer, prefix_token_count, truncate)\")\n",
    "print(\"  Key: truncate=True masks prefix from decoder cross-attention\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cadd340c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-17T20:29:51.978392Z",
     "iopub.status.busy": "2026-02-17T20:29:51.978115Z",
     "iopub.status.idle": "2026-02-17T20:29:53.439398Z",
     "shell.execute_reply": "2026-02-17T20:29:53.438701Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MS MARCO v1.1 validation...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 200 samples, mean words=74\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Load data\n",
    "from lib.data import count_words\n",
    "from datasets import load_dataset\n",
    "\n",
    "print(\"Loading MS MARCO v1.1 validation...\")\n",
    "ds = load_dataset(\"microsoft/ms_marco\", \"v1.1\", split=\"validation\")\n",
    "\n",
    "samples = []\n",
    "for item in ds:\n",
    "    if len(samples) >= N_SAMPLES * 3:\n",
    "        break\n",
    "    passages = item.get('passages', {})\n",
    "    ptexts = passages.get('passage_text', [])\n",
    "    is_sel = passages.get('is_selected', [])\n",
    "    query = item.get('query', '')\n",
    "    answers = item.get('answers', [])\n",
    "    well_formed = item.get('wellFormedAnswers', [])\n",
    "\n",
    "    answer = None\n",
    "    if well_formed and len(well_formed) > 0 and well_formed[0] not in ('[]', ''):\n",
    "        answer = well_formed[0]\n",
    "    elif answers and len(answers) > 0 and answers[0] != 'No Answer Present.':\n",
    "        answer = answers[0]\n",
    "    if not answer:\n",
    "        continue\n",
    "\n",
    "    for pt, sel in zip(ptexts, is_sel):\n",
    "        wc = count_words(pt)\n",
    "        if sel == 1 and 30 <= wc <= 300:\n",
    "            samples.append({\n",
    "                'passage': pt, 'query': query, 'answer': answer,\n",
    "                'word_count': wc\n",
    "            })\n",
    "            break\n",
    "\n",
    "np.random.seed(SEED)\n",
    "np.random.shuffle(samples)\n",
    "samples = samples[:N_SAMPLES]\n",
    "del ds\n",
    "gc.collect()\n",
    "\n",
    "for s in samples:\n",
    "    s['surrogate_para'] = make_surrogate_paraphrase(s['query'])\n",
    "    s['surrogate_doc_kw'] = make_surrogate_from_doc(s['passage'])\n",
    "\n",
    "print(f\"Selected {len(samples)} samples, mean words={np.mean([s['word_count'] for s in samples]):.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c87fbe1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-17T20:29:53.442949Z",
     "iopub.status.busy": "2026-02-17T20:29:53.442152Z",
     "iopub.status.idle": "2026-02-17T20:29:53.456227Z",
     "shell.execute_reply": "2026-02-17T20:29:53.455579Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "EXPERIMENTAL CONDITIONS\n",
      "======================================================================\n",
      "\n",
      "Example query:  when did the triceratops appear in the fossil record\n",
      "Example answer: They first appeared during the late Maastrichtian stage of the late Cr\n",
      "\n",
      "CONDITION         ENCODER INPUT              DECODER CROSS-ATTENDS TO    PREFIX TOKENS\n",
      "-------------------------------------------------------------------------------------\n",
      "bare              [document]                 all (= document)            0\n",
      "oracle_full       [query + doc]              all (query + doc)           ~13\n",
      "oracle_trunc      [query + doc]              document ONLY               ~13 (masked)\n",
      "surr_para_full    [paraphrase + doc]         all (paraphrase + doc)      ~8\n",
      "surr_para_trunc   [paraphrase + doc]         document ONLY               ~8 (masked)\n",
      "surr_doc_full     [doc_keywords + doc]       all (keywords + doc)        ~12\n",
      "surr_doc_trunc    [doc_keywords + doc]       document ONLY               ~12 (masked)\n",
      "\n",
      "KEY INSIGHT:\n",
      "  _full conditions:  decoder reads prefix + gets improved doc reps\n",
      "  _trunc conditions: decoder gets improved doc reps ONLY (prefix hidden)\n",
      "  If _trunc ≈ _full:  benefit is from improved document representations\n",
      "  If _trunc ≈ bare:   benefit was just the decoder reading the prefix\n",
      "\n",
      "--- Token count verification ---\n",
      "  Sample 0: query='when did the triceratops appear in the f...' prefix_tokens=13, doc_tokens=155\n",
      "  Sample 1: query='kilaya name meaning...' prefix_tokens=5, doc_tokens=149\n",
      "  Sample 2: query='what biome has plants spaced far apart...' prefix_tokens=8, doc_tokens=99\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Explain conditions\n",
    "print(\"=\" * 70)\n",
    "print(\"EXPERIMENTAL CONDITIONS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "ex = samples[0]\n",
    "print(f\"\\nExample query:  {ex['query'][:70]}\")\n",
    "print(f\"Example answer: {ex['answer'][:70]}\")\n",
    "\n",
    "# Count prefix tokens for the example\n",
    "oracle_prefix_tokens = count_prefix_tokens(ex['query'], ex['passage'])\n",
    "para_prefix_tokens = count_prefix_tokens(ex['surrogate_para'], ex['passage'])\n",
    "doc_prefix_tokens = count_prefix_tokens(ex['surrogate_doc_kw'], ex['passage'])\n",
    "\n",
    "conditions_explained = f'''\n",
    "CONDITION         ENCODER INPUT              DECODER CROSS-ATTENDS TO    PREFIX TOKENS\n",
    "-------------------------------------------------------------------------------------\n",
    "bare              [document]                 all (= document)            0\n",
    "oracle_full       [query + doc]              all (query + doc)           ~{oracle_prefix_tokens}\n",
    "oracle_trunc      [query + doc]              document ONLY               ~{oracle_prefix_tokens} (masked)\n",
    "surr_para_full    [paraphrase + doc]         all (paraphrase + doc)      ~{para_prefix_tokens}\n",
    "surr_para_trunc   [paraphrase + doc]         document ONLY               ~{para_prefix_tokens} (masked)\n",
    "surr_doc_full     [doc_keywords + doc]       all (keywords + doc)        ~{doc_prefix_tokens}\n",
    "surr_doc_trunc    [doc_keywords + doc]       document ONLY               ~{doc_prefix_tokens} (masked)\n",
    "\n",
    "KEY INSIGHT:\n",
    "  _full conditions:  decoder reads prefix + gets improved doc reps\n",
    "  _trunc conditions: decoder gets improved doc reps ONLY (prefix hidden)\n",
    "  If _trunc ≈ _full:  benefit is from improved document representations\n",
    "  If _trunc ≈ bare:   benefit was just the decoder reading the prefix\n",
    "'''\n",
    "print(conditions_explained)\n",
    "\n",
    "# Verify masking by checking token counts\n",
    "print(\"--- Token count verification ---\")\n",
    "for i in range(3):\n",
    "    s = samples[i]\n",
    "    full_text = s['query'] + \"\\n\" + s['passage']\n",
    "    doc_text = s['passage']\n",
    "    full_ids = tokenizer(full_text, add_special_tokens=True).input_ids\n",
    "    doc_ids = tokenizer(doc_text, add_special_tokens=True).input_ids\n",
    "    prefix_toks = len(full_ids) - len(doc_ids)\n",
    "    print(f\"  Sample {i}: query='{s['query'][:40]}...' prefix_tokens={prefix_toks}, doc_tokens={len(doc_ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d6fe118",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-17T20:29:53.459569Z",
     "iopub.status.busy": "2026-02-17T20:29:53.459074Z",
     "iopub.status.idle": "2026-02-17T20:34:58.612716Z",
     "shell.execute_reply": "2026-02-17T20:34:58.612049Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "RUNNING EXPERIMENT\n",
      "======================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c32245368324dafb0263063aafe8357",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Scoring:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 20/200 | 0.5m | ETA 4.6m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 40/200 | 1.0m | ETA 4.1m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 60/200 | 1.5m | ETA 3.6m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 80/200 | 2.0m | ETA 3.1m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 100/200 | 2.5m | ETA 2.5m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 120/200 | 3.0m | ETA 2.0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 140/200 | 3.6m | ETA 1.5m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 160/200 | 4.1m | ETA 1.0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 180/200 | 4.6m | ETA 0.5m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 200/200 | 5.1m | ETA 0.0m\n",
      "\n",
      "Scoring complete: 200 samples in 5.1 min\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Run scoring\n",
    "print(\"=\" * 70)\n",
    "print(\"RUNNING EXPERIMENT\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Define all conditions\n",
    "def make_conditions(sample):\n",
    "    '''Return dict of {name: (encoder_text, prefix_token_count, truncate)}'''\n",
    "    query = sample['query']\n",
    "    passage = sample['passage']\n",
    "    para = sample['surrogate_para']\n",
    "    doc_kw = sample['surrogate_doc_kw']\n",
    "\n",
    "    # Count prefix tokens for each condition\n",
    "    oracle_prefix = count_prefix_tokens(query, passage)\n",
    "    para_prefix = count_prefix_tokens(para, passage)\n",
    "    doc_prefix = count_prefix_tokens(doc_kw, passage)\n",
    "\n",
    "    return {\n",
    "        'bare':           (passage,                          0,              False),\n",
    "        'oracle_full':    (query + \"\\n\" + passage,           0,              False),\n",
    "        'oracle_trunc':   (query + \"\\n\" + passage,           oracle_prefix,  True),\n",
    "        'surr_para_full': (para + \"\\n\" + passage,            0,              False),\n",
    "        'surr_para_trunc':(para + \"\\n\" + passage,            para_prefix,    True),\n",
    "        'surr_doc_full':  (doc_kw + \"\\n\" + passage,          0,              False),\n",
    "        'surr_doc_trunc': (doc_kw + \"\\n\" + passage,          doc_prefix,     True),\n",
    "    }\n",
    "\n",
    "cond_names = ['bare', 'oracle_full', 'oracle_trunc',\n",
    "              'surr_para_full', 'surr_para_trunc',\n",
    "              'surr_doc_full', 'surr_doc_trunc']\n",
    "\n",
    "# Resume from checkpoint\n",
    "all_results = []\n",
    "start_idx = 0\n",
    "\n",
    "if CHECKPOINT_PATH.exists():\n",
    "    ckpt = json.loads(CHECKPOINT_PATH.read_text())\n",
    "    if ckpt.get('n_total') == N_SAMPLES and len(ckpt.get('results', [])) > 0:\n",
    "        saved_queries = [r['query'][:50] for r in ckpt['results']]\n",
    "        current_queries = [s['query'][:50] for s in samples[:len(saved_queries)]]\n",
    "        if saved_queries == current_queries:\n",
    "            all_results = ckpt['results']\n",
    "            start_idx = len(all_results)\n",
    "            print(f\"Resuming: {start_idx}/{N_SAMPLES}\")\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "for i in tqdm(range(start_idx, N_SAMPLES), initial=start_idx, total=N_SAMPLES, desc=\"Scoring\"):\n",
    "    s = samples[i]\n",
    "    conditions = make_conditions(s)\n",
    "\n",
    "    result = {\n",
    "        'query': s['query'], 'answer': s['answer'],\n",
    "        'passage_words': s['word_count'],\n",
    "    }\n",
    "\n",
    "    for cond_name in cond_names:\n",
    "        enc_text, prefix_count, trunc = conditions[cond_name]\n",
    "        nll = score_nll(enc_text, s['answer'], prefix_count, trunc)\n",
    "        result[f'nll_{cond_name}'] = nll\n",
    "\n",
    "    all_results.append(result)\n",
    "\n",
    "    if (i + 1) % 20 == 0 or i == N_SAMPLES - 1:\n",
    "        ckpt = {\n",
    "            'n_total': N_SAMPLES, 'results': all_results,\n",
    "            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        }\n",
    "        CHECKPOINT_PATH.write_text(json.dumps(ckpt))\n",
    "        elapsed = time.time() - t0\n",
    "        done = i - start_idx + 1\n",
    "        eta = (N_SAMPLES - i - 1) * elapsed / done if done > 0 else 0\n",
    "        tqdm.write(f\"  Checkpoint {i+1}/{N_SAMPLES} | {elapsed/60:.1f}m | ETA {eta/60:.1f}m\")\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "elapsed = time.time() - t0\n",
    "print(f\"\\nScoring complete: {len(all_results)} samples in {elapsed/60:.1f} min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cbb08597",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-17T20:34:58.615952Z",
     "iopub.status.busy": "2026-02-17T20:34:58.615688Z",
     "iopub.status.idle": "2026-02-17T20:34:58.638002Z",
     "shell.execute_reply": "2026-02-17T20:34:58.637059Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "RESULTS (N=200)\n",
      "======================================================================\n",
      "\n",
      "Condition              Mean NLL    vs Bare        d     Win%            p   sig\n",
      "------------------------------------------------------------------------------\n",
      "bare                     3.7188         --       --       --           --    --\n",
      "oracle_full              3.4306    +0.2882   +0.345    81.5%     2.14e-06   ***\n",
      "oracle_trunc             3.0833    +0.6355   +0.408    94.0%     2.99e-08   ***\n",
      "surr_para_full           3.5175    +0.2013   +0.293    72.0%     5.06e-05   ***\n",
      "surr_para_trunc          3.2078    +0.5110   +0.357    87.5%     1.03e-06   ***\n",
      "surr_doc_full            3.4435    +0.2753   +0.312    79.5%     1.64e-05   ***\n",
      "surr_doc_trunc           3.1645    +0.5543   +0.363    85.0%     6.70e-07   ***\n",
      "\n",
      "======================================================================\n",
      "FULL vs TRUNCATED -- The Key Comparison\n",
      "======================================================================\n",
      "\n",
      "  oracle:\n",
      "    full  vs bare: NLL gap = +0.2882\n",
      "    trunc vs bare: NLL gap = +0.6355\n",
      "    Retention: 220% of full benefit survives truncation\n",
      "    full vs trunc: d=+0.340, p=3.00e-06\n",
      "    --> DOCUMENT REPRESENTATIONS carry majority of the benefit\n",
      "\n",
      "  surr_para:\n",
      "    full  vs bare: NLL gap = +0.2013\n",
      "    trunc vs bare: NLL gap = +0.5110\n",
      "    Retention: 254% of full benefit survives truncation\n",
      "    full vs trunc: d=+0.321, p=9.99e-06\n",
      "    --> DOCUMENT REPRESENTATIONS carry majority of the benefit\n",
      "\n",
      "  surr_doc:\n",
      "    full  vs bare: NLL gap = +0.2753\n",
      "    trunc vs bare: NLL gap = +0.5543\n",
      "    Retention: 201% of full benefit survives truncation\n",
      "    full vs trunc: d=+0.314, p=1.53e-05\n",
      "    --> DOCUMENT REPRESENTATIONS carry majority of the benefit\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Results\n",
    "from lib.analysis import cohens_d\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"RESULTS (N={len(all_results)})\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "bare_nlls = np.array([r['nll_bare'] for r in all_results])\n",
    "\n",
    "print(f\"\\n{'Condition':<20} {'Mean NLL':>10} {'vs Bare':>10} {'d':>8} {'Win%':>8} {'p':>12} {'sig':>5}\")\n",
    "print(\"-\" * 78)\n",
    "\n",
    "analysis = {}\n",
    "for cond in cond_names:\n",
    "    nlls = np.array([r[f'nll_{cond}'] for r in all_results])\n",
    "    mean_nll = nlls.mean()\n",
    "    diff = bare_nlls - nlls\n",
    "    d = cohens_d(diff)\n",
    "    win_pct = 100 * np.mean(diff > 0)\n",
    "\n",
    "    if cond == 'bare':\n",
    "        print(f\"{cond:<20} {mean_nll:>10.4f} {'--':>10} {'--':>8} {'--':>8} {'--':>12} {'--':>5}\")\n",
    "        analysis[cond] = {'mean_nll': float(mean_nll)}\n",
    "    else:\n",
    "        t_stat, p_val = stats.ttest_1samp(diff, 0)\n",
    "        sig = '***' if p_val < 0.001 else '**' if p_val < 0.01 else '*' if p_val < 0.05 else 'ns'\n",
    "        print(f\"{cond:<20} {mean_nll:>10.4f} {diff.mean():>+10.4f} {d:>+8.3f} {win_pct:>7.1f}% {p_val:>12.2e} {sig:>5}\")\n",
    "        analysis[cond] = {\n",
    "            'mean_nll': float(mean_nll), 'delta': float(diff.mean()),\n",
    "            'd': float(d), 'win_pct': float(win_pct), 'p': float(p_val),\n",
    "        }\n",
    "\n",
    "# ---- Full vs Truncated comparison ----\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"FULL vs TRUNCATED -- The Key Comparison\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for prefix_type in ['oracle', 'surr_para', 'surr_doc']:\n",
    "    full_nlls = np.array([r[f'nll_{prefix_type}_full'] for r in all_results])\n",
    "    trunc_nlls = np.array([r[f'nll_{prefix_type}_trunc'] for r in all_results])\n",
    "\n",
    "    full_gap = bare_nlls.mean() - full_nlls.mean()     # full vs bare\n",
    "    trunc_gap = bare_nlls.mean() - trunc_nlls.mean()   # trunc vs bare\n",
    "\n",
    "    if full_gap > 0:\n",
    "        retention = trunc_gap / full_gap * 100\n",
    "    else:\n",
    "        retention = float('nan')\n",
    "\n",
    "    # Direct full vs trunc comparison\n",
    "    diff_ft = full_nlls - trunc_nlls  # positive = trunc better\n",
    "    d_ft = cohens_d(diff_ft)\n",
    "    t_ft, p_ft = stats.ttest_1samp(diff_ft, 0) if np.std(diff_ft) > 0 else (0, 1)\n",
    "\n",
    "    print(f\"\\n  {prefix_type}:\")\n",
    "    print(f\"    full  vs bare: NLL gap = {full_gap:+.4f}\")\n",
    "    print(f\"    trunc vs bare: NLL gap = {trunc_gap:+.4f}\")\n",
    "    print(f\"    Retention: {retention:.0f}% of full benefit survives truncation\")\n",
    "    print(f\"    full vs trunc: d={d_ft:+.3f}, p={p_ft:.2e}\")\n",
    "    if retention > 50:\n",
    "        print(f\"    --> DOCUMENT REPRESENTATIONS carry majority of the benefit\")\n",
    "    elif retention > 20:\n",
    "        print(f\"    --> MIXED: both doc reps and direct query reading contribute\")\n",
    "    else:\n",
    "        print(f\"    --> DECODER QUERY READING is the primary mechanism\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05ec0dd3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-17T20:34:58.641143Z",
     "iopub.status.busy": "2026-02-17T20:34:58.640601Z",
     "iopub.status.idle": "2026-02-17T20:34:58.651808Z",
     "shell.execute_reply": "2026-02-17T20:34:58.651162Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "HARDNESS GRADIENT\n",
      "======================================================================\n",
      "\n",
      "--- Oracle: full vs trunc by hardness ---\n",
      "Quintile        N       bare   orc_full  orc_trunc   full_gap  trunc_gap    retain%\n",
      "------------------------------------------------------------------------------\n",
      "Q1 easy        40     0.5338     0.4804     0.4481    +0.0534    +0.0857       161%\n",
      "Q2             40     0.9906     0.9307     0.8419    +0.0600    +0.1487       248%\n",
      "Q3             40     1.7232     1.6213     1.4878    +0.1020    +0.2354       231%\n",
      "Q4             40     3.0816     2.9090     2.6498    +0.1727    +0.4318       250%\n",
      "Q5 hard        40    12.2648    11.2117     9.9891    +1.0531    +2.2758       216%\n",
      "\n",
      "Correlation(oracle_full benefit, oracle_trunc benefit): r=0.800 (p=1.01e-45)\n",
      "  (high r = same samples helped by both, suggesting shared mechanism)\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Hardness gradient\n",
    "print(\"=\" * 70)\n",
    "print(\"HARDNESS GRADIENT\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "quintile_bounds = np.percentile(bare_nlls, [20, 40, 60, 80])\n",
    "quintiles = np.digitize(bare_nlls, quintile_bounds)\n",
    "\n",
    "# Show full vs trunc retention by hardness\n",
    "print(f\"\\n--- Oracle: full vs trunc by hardness ---\")\n",
    "print(f\"{'Quintile':<12} {'N':>4} {'bare':>10} {'orc_full':>10} {'orc_trunc':>10} {'full_gap':>10} {'trunc_gap':>10} {'retain%':>10}\")\n",
    "print(\"-\" * 78)\n",
    "\n",
    "for q in range(5):\n",
    "    mask = quintiles == q\n",
    "    n_q = mask.sum()\n",
    "    if n_q < 3:\n",
    "        continue\n",
    "    qlabel = ['Q1 easy', 'Q2', 'Q3', 'Q4', 'Q5 hard'][q]\n",
    "    b = bare_nlls[mask].mean()\n",
    "    of = np.array([all_results[j]['nll_oracle_full'] for j in range(len(all_results)) if mask[j]]).mean()\n",
    "    ot = np.array([all_results[j]['nll_oracle_trunc'] for j in range(len(all_results)) if mask[j]]).mean()\n",
    "    fg = b - of\n",
    "    tg = b - ot\n",
    "    ret = tg / fg * 100 if fg > 0 else float('nan')\n",
    "    print(f\"{qlabel:<12} {n_q:>4} {b:>10.4f} {of:>10.4f} {ot:>10.4f} {fg:>+10.4f} {tg:>+10.4f} {ret:>9.0f}%\")\n",
    "\n",
    "# Correlations\n",
    "oracle_full_delta = bare_nlls - np.array([r['nll_oracle_full'] for r in all_results])\n",
    "oracle_trunc_delta = bare_nlls - np.array([r['nll_oracle_trunc'] for r in all_results])\n",
    "r_ft, p_ft = stats.pearsonr(oracle_full_delta, oracle_trunc_delta)\n",
    "print(f\"\\nCorrelation(oracle_full benefit, oracle_trunc benefit): r={r_ft:.3f} (p={p_ft:.2e})\")\n",
    "print(\"  (high r = same samples helped by both, suggesting shared mechanism)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5207de85",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-17T20:34:58.654671Z",
     "iopub.status.busy": "2026-02-17T20:34:58.654422Z",
     "iopub.status.idle": "2026-02-17T20:34:58.664938Z",
     "shell.execute_reply": "2026-02-17T20:34:58.664341Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "VERDICT -- Exp 01: Truncation Test\n",
      "======================================================================\n",
      "\n",
      "Model: google/t5gemma-2-4b-4b\n",
      "N: 200 samples\n",
      "\n",
      "--- Oracle (the most important comparison) ---\n",
      "  full  d = +0.345 (decoder sees query + improved doc reps)\n",
      "  trunc d = +0.408 (decoder sees improved doc reps ONLY)\n",
      "  Retention: 220%\n",
      "\n",
      "  STRONG: Document representations carry substantial query-specific benefit.\n",
      "  Bidirectional co-encoding with the query genuinely improves document reps.\n",
      "  This is the REAL mechanism, not just the decoder reading the query.\n",
      "\n",
      "--- Surrogate transfer (truncated) ---\n",
      "  surr_para: full d=+0.293, trunc d=+0.357\n",
      "  surr_doc: full d=+0.312, trunc d=+0.363\n",
      "\n",
      "--- Cross-architecture comparison ---\n",
      "  Decoder-only (Gemma 3 4B):\n",
      "    Oracle full-context:    d ~ +0.023 (ns)\n",
      "    Oracle values-only:     d ~ +0.211 ***\n",
      "  Encoder-decoder (T5Gemma 2 4B-4B):\n",
      "    Oracle full:            d = +0.345\n",
      "    Oracle trunc (this exp): d = +0.408\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Results saved to results/exp01/results.json\n"
     ]
    }
   ],
   "source": [
    "# Cell 10: Verdict and save\n",
    "print(\"=\" * 70)\n",
    "print(\"VERDICT -- Exp 01: Truncation Test\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\nModel: {MODEL_NAME}\")\n",
    "print(f\"N: {len(all_results)} samples\")\n",
    "\n",
    "# Key numbers\n",
    "oracle_full_d = analysis.get('oracle_full', {}).get('d', 0)\n",
    "oracle_trunc_d = analysis.get('oracle_trunc', {}).get('d', 0)\n",
    "\n",
    "oracle_full_gap = bare_nlls.mean() - np.array([r['nll_oracle_full'] for r in all_results]).mean()\n",
    "oracle_trunc_gap = bare_nlls.mean() - np.array([r['nll_oracle_trunc'] for r in all_results]).mean()\n",
    "oracle_retention = oracle_trunc_gap / oracle_full_gap * 100 if oracle_full_gap > 0 else 0\n",
    "\n",
    "print(f\"\\n--- Oracle (the most important comparison) ---\")\n",
    "print(f\"  full  d = {oracle_full_d:+.3f} (decoder sees query + improved doc reps)\")\n",
    "print(f\"  trunc d = {oracle_trunc_d:+.3f} (decoder sees improved doc reps ONLY)\")\n",
    "print(f\"  Retention: {oracle_retention:.0f}%\")\n",
    "\n",
    "if oracle_trunc_d > 0.2:\n",
    "    print(f\"\\n  STRONG: Document representations carry substantial query-specific benefit.\")\n",
    "    print(f\"  Bidirectional co-encoding with the query genuinely improves document reps.\")\n",
    "    print(f\"  This is the REAL mechanism, not just the decoder reading the query.\")\n",
    "elif oracle_trunc_d > 0.05:\n",
    "    print(f\"\\n  MODERATE: Some benefit from improved doc reps, but decoder query reading\")\n",
    "    print(f\"  also contributes significantly.\")\n",
    "elif oracle_trunc_d > 0:\n",
    "    print(f\"\\n  WEAK: Most benefit comes from the decoder reading query tokens directly.\")\n",
    "    print(f\"  Document representations are only marginally improved by co-encoding.\")\n",
    "else:\n",
    "    print(f\"\\n  NONE: Truncation eliminates all benefit. The decoder was just reading\")\n",
    "    print(f\"  the query from encoder output. Document representations are NOT improved\")\n",
    "    print(f\"  by bidirectional co-encoding.\")\n",
    "\n",
    "# Surrogate transfer with truncation\n",
    "print(f\"\\n--- Surrogate transfer (truncated) ---\")\n",
    "for name in ['surr_para', 'surr_doc']:\n",
    "    full_d = analysis.get(f'{name}_full', {}).get('d', 0)\n",
    "    trunc_d = analysis.get(f'{name}_trunc', {}).get('d', 0)\n",
    "    print(f\"  {name}: full d={full_d:+.3f}, trunc d={trunc_d:+.3f}\")\n",
    "\n",
    "# Comparison to decoder-only\n",
    "print(f\"\\n--- Cross-architecture comparison ---\")\n",
    "print(f\"  Decoder-only (Gemma 3 4B):\")\n",
    "print(f\"    Oracle full-context:    d ~ +0.023 (ns)\")\n",
    "print(f\"    Oracle values-only:     d ~ +0.211 ***\")\n",
    "print(f\"  Encoder-decoder (T5Gemma 2 4B-4B):\")\n",
    "print(f\"    Oracle full:            d = {oracle_full_d:+.3f}\")\n",
    "print(f\"    Oracle trunc (this exp): d = {oracle_trunc_d:+.3f}\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "\n",
    "# Save\n",
    "final_results = {\n",
    "    'experiment': 'exp01_truncation_test',\n",
    "    'model': MODEL_NAME,\n",
    "    'n_samples': len(all_results),\n",
    "    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'analysis': analysis,\n",
    "    'oracle_retention_pct': float(oracle_retention),\n",
    "}\n",
    "\n",
    "with open(RESULTS_DIR / 'results.json', 'w') as f:\n",
    "    json.dump(final_results, f, indent=2)\n",
    "print(f\"\\nResults saved to {RESULTS_DIR / 'results.json'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd9ab4d4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-17T20:34:58.667782Z",
     "iopub.status.busy": "2026-02-17T20:34:58.667237Z",
     "iopub.status.idle": "2026-02-17T20:34:59.186311Z",
     "shell.execute_reply": "2026-02-17T20:34:59.185143Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning up GPU memory...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory: 15.03 GB -> 0.01 GB\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Cell 11: Cleanup\n",
    "print(\"Cleaning up GPU memory...\")\n",
    "mem_before = torch.cuda.memory_allocated() / 1e9\n",
    "del model, processor, tokenizer\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "mem_after = torch.cuda.memory_allocated() / 1e9\n",
    "print(f\"GPU memory: {mem_before:.2f} GB -> {mem_after:.2f} GB\")\n",
    "print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "076e3946eb3b409f8722f61365afb338": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "1d09a879f5cf4f16b045ebf4bdd43536": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "1ff81d7a23a0486bb79203108d4cb8b5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "211f601b9b2347778d57d6640412b132": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_1d09a879f5cf4f16b045ebf4bdd43536",
       "placeholder": "​",
       "style": "IPY_MODEL_8257342e83c3405281db3d5ede4bdf8d",
       "tabbable": null,
       "tooltip": null,
       "value": " 1327/1327 [00:04&lt;00:00, 688.67it/s, Materializing param=model.encoder.vision_tower.vision_model.post_layernorm.weight]"
      }
     },
     "23e370dfe46248a5a500ca2b02673842": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_076e3946eb3b409f8722f61365afb338",
       "placeholder": "​",
       "style": "IPY_MODEL_be0f19c36de546e9af83a9e544bd85c0",
       "tabbable": null,
       "tooltip": null,
       "value": "Scoring: 100%"
      }
     },
     "3a3ce22f4eb246b393fda95e37cfd28c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3c2efe3708c34ae6aa455723a438996b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_3a3ce22f4eb246b393fda95e37cfd28c",
       "max": 1327.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_aec6adcffc1843f697586b8d50e79348",
       "tabbable": null,
       "tooltip": null,
       "value": 1327.0
      }
     },
     "49870e04f68848af99713bfc092e615c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "59fba6b301a44144950d6df6eeb7839f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_83785b17ce454db6b6155bd51179e60b",
       "placeholder": "​",
       "style": "IPY_MODEL_1ff81d7a23a0486bb79203108d4cb8b5",
       "tabbable": null,
       "tooltip": null,
       "value": "Loading weights: 100%"
      }
     },
     "5c32245368324dafb0263063aafe8357": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_23e370dfe46248a5a500ca2b02673842",
        "IPY_MODEL_c9cedd0118954fbaacafdb134ca66394",
        "IPY_MODEL_b501afb1623f4de990200f85ac66b987"
       ],
       "layout": "IPY_MODEL_a2b0facf80964631b1fcf6bbb950f689",
       "tabbable": null,
       "tooltip": null
      }
     },
     "8257342e83c3405281db3d5ede4bdf8d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "83785b17ce454db6b6155bd51179e60b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "9a17f48147644df29d7adfbd6d9ab8ec": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_59fba6b301a44144950d6df6eeb7839f",
        "IPY_MODEL_3c2efe3708c34ae6aa455723a438996b",
        "IPY_MODEL_211f601b9b2347778d57d6640412b132"
       ],
       "layout": "IPY_MODEL_b0a2000db20a483eb88ab17bfcc60588",
       "tabbable": null,
       "tooltip": null
      }
     },
     "a2b0facf80964631b1fcf6bbb950f689": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "aec6adcffc1843f697586b8d50e79348": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "b02009fc18de4c4689843e30f06832da": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b0a2000db20a483eb88ab17bfcc60588": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b14f22a82eda4e5683aa6f2d332bfc23": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b501afb1623f4de990200f85ac66b987": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_b02009fc18de4c4689843e30f06832da",
       "placeholder": "​",
       "style": "IPY_MODEL_dc6a8be39d774c76aa0dcd6f0b3f057c",
       "tabbable": null,
       "tooltip": null,
       "value": " 200/200 [05:05&lt;00:00,  1.53s/it]"
      }
     },
     "be0f19c36de546e9af83a9e544bd85c0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "c9cedd0118954fbaacafdb134ca66394": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_b14f22a82eda4e5683aa6f2d332bfc23",
       "max": 200.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_49870e04f68848af99713bfc092e615c",
       "tabbable": null,
       "tooltip": null,
       "value": 200.0
      }
     },
     "dc6a8be39d774c76aa0dcd6f0b3f057c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
