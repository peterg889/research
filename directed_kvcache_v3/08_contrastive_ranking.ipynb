{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25a02ccf",
   "metadata": {},
   "source": [
    "# Experiment 08: Contrastive & Differential Ranking Analysis## Can we isolate the semantic residual for ranking?### MotivationExp 04A showed zero oracle differential (d=-0.007): priming helps relevant andirrelevant passages equally. The ~85% structural component is document-independent.But ~15% IS semantic. Can we isolate it?### Key idea: oracle_cross_truncEach query's passage pool is scored with a **different** query's oracle text.Since the structural benefit depends on prefix LENGTH (not content), oracle andoracle_cross should provide ~identical structural lift. The **difference** betweenNLL(oracle) and NLL(oracle_cross) isolates the **semantic interaction** between thecorrect query and each passage.### Conditions (7)| # | Condition | Prefix | Notes ||---|-----------|--------|-------|| 1 | bare | (none) | lower bound || 2 | oracle\\_trunc | real query | upper bound || 3 | oracle\\_cross\\_trunc | DIFFERENT query | **NEW** -- structural control || 4 | surr\\_template\\_trunc | \"What is [kw]?\" per passage | doc-derived || 5 | surr\\_doc\\_trunc | TF keywords per passage | doc-derived || 6 | random\\_trunc | unrelated text | structural control || 7 | static\\_fact\\_trunc | \"What are the key facts?\" | content-agnostic |### Six Analysis Strategies (all from the same NLL data)**A. Standard ranking**: rank by raw NLL (replicates 04A)**B. Delta-NLL ranking**: rank by NLL(bare) - NLL(condition)**C. Relative delta**: subtract pool-mean delta, rank by deviation**D. Per-doc template contrastive**: variation across per-passage templates**E. Oracle cross-contrastive**: rank by NLL(oracle) - NLL(oracle_cross) -- KEY**F. Vocabulary overlap correlation**: correlate delta with Jaccard(query, passage)### N=400 queries (~8.2 passages/query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8e446c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T13:26:04.628029Z",
     "iopub.status.busy": "2026-02-19T13:26:04.627410Z",
     "iopub.status.idle": "2026-02-19T13:26:09.211552Z",
     "shell.execute_reply": "2026-02-19T13:26:09.210657Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exp 08: Contrastive & Differential Ranking Analysis\n",
      "Model: google/t5gemma-2-4b-4b\n",
      "N queries: 400\n",
      "Bonferroni comparisons: 6\n",
      "CUDA: NVIDIA A100-SXM4-40GB\n",
      "GPU memory: 42.3 GB\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Setup\n",
    "import os\n",
    "os.umask(0o000)\n",
    "\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "import gc\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "from collections import Counter, defaultdict\n",
    "from scipy import stats\n",
    "from scipy.stats import wilcoxon\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "sys.path.insert(0, \".\")\n",
    "\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "RESULTS_DIR = Path(\"results/exp08\")\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CHECKPOINT_PATH = RESULTS_DIR / \"checkpoint.json\"\n",
    "\n",
    "N_SAMPLES = 400   # queries\n",
    "MODEL_NAME = \"google/t5gemma-2-4b-4b\"\n",
    "N_BONFERRONI = 6  # 6 non-bare conditions\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "HF_TOKEN = os.environ.get(\"HF_TOKEN\")\n",
    "\n",
    "print(f\"Exp 08: Contrastive & Differential Ranking Analysis\")\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"N queries: {N_SAMPLES}\")\n",
    "print(f\"Bonferroni comparisons: {N_BONFERRONI}\")\n",
    "print(f\"CUDA: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "daf4ca85",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T13:26:09.215441Z",
     "iopub.status.busy": "2026-02-19T13:26:09.214646Z",
     "iopub.status.idle": "2026-02-19T13:26:25.635689Z",
     "shell.execute_reply": "2026-02-19T13:26:25.634945Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading google/t5gemma-2-4b-4b...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab1d97128dbe4cff9fa69fcd9232e761",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/1327 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded. dtype=torch.bfloat16\n",
      "GPU memory used: 15.02 GB\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Load model\n",
    "from transformers import AutoProcessor, AutoModelForSeq2SeqLM\n",
    "\n",
    "print(f\"Loading {MODEL_NAME}...\")\n",
    "processor = AutoProcessor.from_pretrained(MODEL_NAME, token=HF_TOKEN)\n",
    "tokenizer = processor.tokenizer\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    token=HF_TOKEN,\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "DEVICE = next(model.parameters()).device\n",
    "print(f\"Model loaded. dtype={next(model.parameters()).dtype}\")\n",
    "print(f\"GPU memory used: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8318eb7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T13:26:25.639224Z",
     "iopub.status.busy": "2026-02-19T13:26:25.638764Z",
     "iopub.status.idle": "2026-02-19T13:26:25.658671Z",
     "shell.execute_reply": "2026-02-19T13:26:25.658046Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helpers defined.\n",
      "  Scoring: score_nll (answer-likelihood)\n",
      "  Surrogates: doc_kw, template, static_fact\n",
      "  Ranking: AUC, AUC_higher_better, MRR@k, Hit@k\n",
      "  Analysis: compute_vocab_overlap (Jaccard)\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Scoring and ranking helpers\n",
    "\n",
    "def score_nll(encoder_text, answer_text, prefix_token_count=0, truncate=False):\n",
    "    # Score NLL of answer tokens with optional truncation.\n",
    "    enc_ids = tokenizer(encoder_text, return_tensors=\"pt\",\n",
    "                        add_special_tokens=True, truncation=True,\n",
    "                        max_length=8192).input_ids.to(DEVICE)\n",
    "    total_enc_len = enc_ids.shape[1]\n",
    "    enc_mask = torch.ones(1, total_enc_len, device=DEVICE, dtype=torch.long)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        encoder_outputs = model.get_encoder()(\n",
    "            input_ids=enc_ids, attention_mask=enc_mask\n",
    "        )\n",
    "\n",
    "    if truncate and prefix_token_count > 0:\n",
    "        cross_attn_mask = torch.ones(1, total_enc_len, device=DEVICE, dtype=torch.long)\n",
    "        cross_attn_mask[:, :prefix_token_count] = 0\n",
    "    else:\n",
    "        cross_attn_mask = torch.ones(1, total_enc_len, device=DEVICE, dtype=torch.long)\n",
    "\n",
    "    ans_ids = tokenizer(answer_text, return_tensors=\"pt\",\n",
    "                        add_special_tokens=False, truncation=True,\n",
    "                        max_length=256).input_ids.to(DEVICE)\n",
    "\n",
    "    if ans_ids.shape[1] == 0:\n",
    "        return 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            encoder_outputs=encoder_outputs,\n",
    "            attention_mask=cross_attn_mask,\n",
    "            labels=ans_ids,\n",
    "        )\n",
    "\n",
    "    logits = outputs.logits\n",
    "    log_probs = F.log_softmax(logits, dim=-1)\n",
    "    token_log_probs = log_probs[0].gather(1, ans_ids[0].unsqueeze(1)).squeeze(1)\n",
    "    mean_nll = -token_log_probs.mean().item()\n",
    "\n",
    "    del encoder_outputs, outputs, logits, log_probs\n",
    "    return mean_nll\n",
    "\n",
    "\n",
    "def count_prefix_tokens(prefix_text, document_text):\n",
    "    # Count prefix tokens in [prefix + newline + document].\n",
    "    full_text = prefix_text + \"\\n\" + document_text\n",
    "    full_ids = tokenizer(full_text, add_special_tokens=True).input_ids\n",
    "    doc_ids = tokenizer(document_text, add_special_tokens=True).input_ids\n",
    "    return len(full_ids) - len(doc_ids)\n",
    "\n",
    "\n",
    "# === Surrogate generation ===\n",
    "STOP_WORDS = {\n",
    "    'a', 'an', 'the', 'is', 'are', 'was', 'were', 'be', 'been', 'being',\n",
    "    'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could',\n",
    "    'should', 'may', 'might', 'can', 'shall', 'to', 'of', 'in', 'for',\n",
    "    'on', 'with', 'at', 'by', 'from', 'as', 'into', 'through', 'during',\n",
    "    'before', 'after', 'above', 'below', 'between', 'and', 'but', 'or',\n",
    "    'not', 'no', 'if', 'then', 'than', 'so', 'up', 'out', 'about',\n",
    "    'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those',\n",
    "    'it', 'its', 'i', 'me', 'my', 'we', 'our', 'you', 'your', 'he',\n",
    "    'him', 'his', 'she', 'her', 'they', 'them', 'their', 'how', 'when',\n",
    "    'where', 'why', 'much', 'many', 'some', 'any', 'all', 'each',\n",
    "    'does', 'also', 'just', 'more', 'most', 'very', 'too', 'only',\n",
    "}\n",
    "\n",
    "def extract_keywords(text):\n",
    "    words = re.sub(r'[^\\w\\s]', '', text.lower()).split()\n",
    "    return [w for w in words if w not in STOP_WORDS and len(w) > 2]\n",
    "\n",
    "def make_surrogate_doc_kw(passage):\n",
    "    content_words = extract_keywords(passage)\n",
    "    if not content_words:\n",
    "        return \"information\"\n",
    "    counts = Counter(content_words)\n",
    "    return \" \".join(w for w, _ in counts.most_common(5))\n",
    "\n",
    "def make_surrogate_template(passage):\n",
    "    content_words = extract_keywords(passage)\n",
    "    if not content_words:\n",
    "        return \"What is this about?\"\n",
    "    counts = Counter(content_words)\n",
    "    top_word = counts.most_common(1)[0][0]\n",
    "    return f\"What is {top_word}?\"\n",
    "\n",
    "STATIC_FACT = \"What are the key facts I need to know?\"\n",
    "\n",
    "\n",
    "# === Ranking metrics ===\n",
    "def compute_auc(nlls, relevant_idx):\n",
    "    # AUC: lower NLL = more relevant. Exactly one relevant passage.\n",
    "    rel_nll = nlls[relevant_idx]\n",
    "    irrel_nlls = [nlls[i] for i in range(len(nlls)) if i != relevant_idx]\n",
    "    n_irrel = len(irrel_nlls)\n",
    "    if n_irrel == 0:\n",
    "        return 0.5\n",
    "    wins = sum(1 for nll in irrel_nlls if nll > rel_nll)\n",
    "    ties = sum(1 for nll in irrel_nlls if nll == rel_nll)\n",
    "    return (wins + 0.5 * ties) / n_irrel\n",
    "\n",
    "def compute_auc_higher_better(scores, relevant_idx):\n",
    "    # AUC when higher score = more relevant.\n",
    "    rel_score = scores[relevant_idx]\n",
    "    irrel_scores = [scores[i] for i in range(len(scores)) if i != relevant_idx]\n",
    "    n_irrel = len(irrel_scores)\n",
    "    if n_irrel == 0:\n",
    "        return 0.5\n",
    "    wins = sum(1 for s in irrel_scores if s < rel_score)\n",
    "    ties = sum(1 for s in irrel_scores if s == rel_score)\n",
    "    return (wins + 0.5 * ties) / n_irrel\n",
    "\n",
    "def compute_mrr_at_k(nlls, relevant_idx, k=3):\n",
    "    # MRR@k: rank by ascending NLL (lower = more relevant).\n",
    "    ranked_indices = list(np.argsort(nlls))\n",
    "    for rank, idx in enumerate(ranked_indices[:k], 1):\n",
    "        if idx == relevant_idx:\n",
    "            return 1.0 / rank\n",
    "    return 0.0\n",
    "\n",
    "def compute_hit_at_k(nlls, relevant_idx, k=1):\n",
    "    # Hit@k: 1 if relevant in top-k by ascending NLL.\n",
    "    ranked_indices = set(np.argsort(nlls)[:k].tolist())\n",
    "    return 1.0 if relevant_idx in ranked_indices else 0.0\n",
    "\n",
    "def compute_vocab_overlap(query, passage):\n",
    "    # Jaccard overlap between query and passage content words.\n",
    "    q_words = set(extract_keywords(query))\n",
    "    p_words = set(extract_keywords(passage))\n",
    "    if not q_words or not p_words:\n",
    "        return 0.0\n",
    "    return len(q_words & p_words) / len(q_words | p_words)\n",
    "\n",
    "\n",
    "print(\"Helpers defined.\")\n",
    "print(\"  Scoring: score_nll (answer-likelihood)\")\n",
    "print(\"  Surrogates: doc_kw, template, static_fact\")\n",
    "print(\"  Ranking: AUC, AUC_higher_better, MRR@k, Hit@k\")\n",
    "print(\"  Analysis: compute_vocab_overlap (Jaccard)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3ecea4d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T13:26:25.661590Z",
     "iopub.status.busy": "2026-02-19T13:26:25.661178Z",
     "iopub.status.idle": "2026-02-19T13:26:28.019442Z",
     "shell.execute_reply": "2026-02-19T13:26:28.018730Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MS MARCO v1.1 validation...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 400 queries for ranking\n",
      "Passages per query: mean=8.2, median=9, min=4, max=10\n",
      "Total passage scorings: 23079\n",
      "Estimated runtime: ~2.6 hours\n",
      "\n",
      "Query lengths: oracle mean=6.2, cross mean=6.2\n",
      "Length correlation: r=-0.058\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Load MS MARCO ranking data + assign cross-queries\n",
    "from lib.data import count_words\n",
    "from datasets import load_dataset\n",
    "\n",
    "print(\"Loading MS MARCO v1.1 validation...\")\n",
    "ds = load_dataset(\"microsoft/ms_marco\", \"v1.1\", split=\"validation\")\n",
    "\n",
    "queries = []\n",
    "\n",
    "for item in ds:\n",
    "    passages_data = item.get('passages', {})\n",
    "    ptexts = passages_data.get('passage_text', [])\n",
    "    is_sel = passages_data.get('is_selected', [])\n",
    "    query = item.get('query', '')\n",
    "    answers = item.get('answers', [])\n",
    "    well_formed = item.get('wellFormedAnswers', [])\n",
    "\n",
    "    answer = None\n",
    "    if well_formed and len(well_formed) > 0 and well_formed[0] not in ('[]', ''):\n",
    "        answer = well_formed[0]\n",
    "    elif answers and len(answers) > 0 and answers[0] != 'No Answer Present.':\n",
    "        answer = answers[0]\n",
    "\n",
    "    if not answer:\n",
    "        continue\n",
    "\n",
    "    word_counts = [count_words(pt) for pt in ptexts]\n",
    "    if not all(30 <= wc <= 300 for wc in word_counts):\n",
    "        continue\n",
    "\n",
    "    n_selected = sum(is_sel)\n",
    "    n_not_selected = len(is_sel) - n_selected\n",
    "    if n_selected != 1 or n_not_selected < 2:\n",
    "        continue\n",
    "\n",
    "    relevant_idx = is_sel.index(1)\n",
    "\n",
    "    passages = []\n",
    "    for p_idx, (pt, sel) in enumerate(zip(ptexts, is_sel)):\n",
    "        passages.append({\n",
    "            'text': pt,\n",
    "            'is_selected': sel,\n",
    "            'word_count': word_counts[p_idx],\n",
    "            'surr_doc_kw': make_surrogate_doc_kw(pt),\n",
    "            'surr_template': make_surrogate_template(pt),\n",
    "        })\n",
    "\n",
    "    queries.append({\n",
    "        'query': query,\n",
    "        'answer': answer,\n",
    "        'passages': passages,\n",
    "        'relevant_idx': relevant_idx,\n",
    "        'n_passages': len(passages),\n",
    "    })\n",
    "\n",
    "    if len(queries) >= N_SAMPLES * 3:\n",
    "        break\n",
    "\n",
    "del ds\n",
    "gc.collect()\n",
    "\n",
    "# Shuffle and select\n",
    "np.random.seed(SEED)\n",
    "np.random.shuffle(queries)\n",
    "queries = queries[:N_SAMPLES]\n",
    "\n",
    "# Assign cross-queries: each query i gets a different query's text\n",
    "# Use circular offset to ensure no self-assignment\n",
    "for i, q in enumerate(queries):\n",
    "    cross_idx = (i + 1) % len(queries)\n",
    "    q['cross_query'] = queries[cross_idx]['query']\n",
    "\n",
    "# Generate per-query random surrogates\n",
    "for i, q in enumerate(queries):\n",
    "    other_idx = (i + N_SAMPLES // 2) % len(queries)\n",
    "    other_passage = queries[other_idx]['passages'][queries[other_idx]['relevant_idx']]['text']\n",
    "    q['surr_random'] = \" \".join(other_passage.split()[:20])\n",
    "\n",
    "# Stats\n",
    "n_passages_list = [q['n_passages'] for q in queries]\n",
    "print(f\"Selected {len(queries)} queries for ranking\")\n",
    "print(f\"Passages per query: mean={np.mean(n_passages_list):.1f}, \"\n",
    "      f\"median={np.median(n_passages_list):.0f}, \"\n",
    "      f\"min={np.min(n_passages_list)}, max={np.max(n_passages_list)}\")\n",
    "total_scorings = sum(n_passages_list) * 7  # 7 conditions\n",
    "print(f\"Total passage scorings: {total_scorings}\")\n",
    "print(f\"Estimated runtime: ~{total_scorings * 0.4 / 3600:.1f} hours\")\n",
    "\n",
    "# Verify cross-query lengths are similar to oracle\n",
    "oracle_lens = [len(q['query'].split()) for q in queries]\n",
    "cross_lens = [len(q['cross_query'].split()) for q in queries]\n",
    "print(f\"\\nQuery lengths: oracle mean={np.mean(oracle_lens):.1f}, \"\n",
    "      f\"cross mean={np.mean(cross_lens):.1f}\")\n",
    "print(f\"Length correlation: r={np.corrcoef(oracle_lens, cross_lens)[0,1]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a347a46c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T13:26:28.023042Z",
     "iopub.status.busy": "2026-02-19T13:26:28.022270Z",
     "iopub.status.idle": "2026-02-19T13:26:28.031532Z",
     "shell.execute_reply": "2026-02-19T13:26:28.030913Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "CONDITION EXAMPLES\n",
      "======================================================================\n",
      "\n",
      "Query: how many bugatti veyrons have been made\n",
      "Cross-query: how do property managers charge\n",
      "Answer: 400.\n",
      "Passages: 10 (9 irrelevant, 1 relevant at idx 4)\n",
      "\n",
      "Relevant passage (idx 4):\n",
      "  Text: Of the 400 purchased so far, 300 were the Veyron 16.4 or 16.4 Super Sport, both coupes. The Super Sport is the fastest p...\n",
      "  surr_template: What is veyron?\n",
      "  surr_doc_kw: veyron 164 sport super car\n",
      "\n",
      "What the encoder sees for the relevant passage:\n",
      "  bare                    : Of the 400 purchased so far, 300 were the Veyron 16.4 or 16.4 Super Sp...\n",
      "  oracle_trunc            : how many bugatti veyrons have ... | Of the 400 purchased so far, 3...\n",
      "  oracle_cross_trunc      : how do property managers charg... | Of the 400 purchased so far, 3...\n",
      "  surr_template_trunc     : What is veyron? | Of the 400 purchased so far, 3...\n",
      "  surr_doc_trunc          : veyron 164 sport super car | Of the 400 purchased so far, 3...\n",
      "  random_trunc            : You should keep utility and cr... | Of the 400 purchased so far, 3...\n",
      "  static_fact_trunc       : What are the key facts I need ... | Of the 400 purchased so far, 3...\n",
      "\n",
      "Key insight: oracle_cross_trunc uses query 'how do property managers charge...'\n",
      "  This provides the SAME structural benefit as oracle_trunc\n",
      "  (similar prefix length) but WRONG semantic content.\n",
      "  Difference isolates the semantic signal.\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Condition examples (including oracle_cross)\n",
    "print(\"=\" * 70)\n",
    "print(\"CONDITION EXAMPLES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "COND_NAMES = ['bare', 'oracle_trunc', 'oracle_cross_trunc',\n",
    "              'surr_template_trunc', 'surr_doc_trunc',\n",
    "              'random_trunc', 'static_fact_trunc']\n",
    "\n",
    "ex = queries[0]\n",
    "print(f\"\\nQuery: {ex['query']}\")\n",
    "print(f\"Cross-query: {ex['cross_query']}\")\n",
    "print(f\"Answer: {ex['answer']}\")\n",
    "print(f\"Passages: {ex['n_passages']} ({ex['n_passages']-1} irrelevant, \"\n",
    "      f\"1 relevant at idx {ex['relevant_idx']})\")\n",
    "\n",
    "rel_p = ex['passages'][ex['relevant_idx']]\n",
    "print(f\"\\nRelevant passage (idx {ex['relevant_idx']}):\")\n",
    "print(f\"  Text: {rel_p['text'][:120]}...\")\n",
    "print(f\"  surr_template: {rel_p['surr_template']}\")\n",
    "print(f\"  surr_doc_kw: {rel_p['surr_doc_kw']}\")\n",
    "\n",
    "print(f\"\\nWhat the encoder sees for the relevant passage:\")\n",
    "for cond in COND_NAMES:\n",
    "    if cond == 'bare':\n",
    "        enc = rel_p['text'][:70] + \"...\"\n",
    "    elif cond == 'oracle_trunc':\n",
    "        enc = ex['query'][:30] + \"... | \" + rel_p['text'][:30] + \"...\"\n",
    "    elif cond == 'oracle_cross_trunc':\n",
    "        enc = ex['cross_query'][:30] + \"... | \" + rel_p['text'][:30] + \"...\"\n",
    "    elif cond == 'surr_template_trunc':\n",
    "        enc = rel_p['surr_template'] + \" | \" + rel_p['text'][:30] + \"...\"\n",
    "    elif cond == 'surr_doc_trunc':\n",
    "        enc = rel_p['surr_doc_kw'] + \" | \" + rel_p['text'][:30] + \"...\"\n",
    "    elif cond == 'random_trunc':\n",
    "        enc = ex['surr_random'][:30] + \"... | \" + rel_p['text'][:30] + \"...\"\n",
    "    elif cond == 'static_fact_trunc':\n",
    "        enc = STATIC_FACT[:30] + \"... | \" + rel_p['text'][:30] + \"...\"\n",
    "    print(f\"  {cond:<24s}: {enc}\")\n",
    "\n",
    "print(f\"\\nKey insight: oracle_cross_trunc uses query '{ex['cross_query'][:50]}...'\")\n",
    "print(f\"  This provides the SAME structural benefit as oracle_trunc\")\n",
    "print(f\"  (similar prefix length) but WRONG semantic content.\")\n",
    "print(f\"  Difference isolates the semantic signal.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cfabbdf8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T13:26:28.034488Z",
     "iopub.status.busy": "2026-02-19T13:26:28.034217Z",
     "iopub.status.idle": "2026-02-19T14:41:07.084688Z",
     "shell.execute_reply": "2026-02-19T14:41:07.084004Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "RUNNING CONTRASTIVE RANKING EXPERIMENT\n",
      "======================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0907da87024e4db29e11207464053890",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Queries:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 20/400 | 3.9m | ETA 73.7m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 40/400 | 7.5m | ETA 67.1m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 60/400 | 11.1m | ETA 62.7m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 80/400 | 14.9m | ETA 59.6m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 100/400 | 18.6m | ETA 55.9m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 120/400 | 22.5m | ETA 52.6m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 140/400 | 26.5m | ETA 49.3m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 160/400 | 30.3m | ETA 45.4m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 180/400 | 33.9m | ETA 41.4m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 200/400 | 37.6m | ETA 37.6m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 220/400 | 41.0m | ETA 33.5m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 240/400 | 44.9m | ETA 29.9m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 260/400 | 48.5m | ETA 26.1m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 280/400 | 52.2m | ETA 22.4m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 300/400 | 56.1m | ETA 18.7m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 320/400 | 60.0m | ETA 15.0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 340/400 | 63.4m | ETA 11.2m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 360/400 | 67.4m | ETA 7.5m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 380/400 | 70.8m | ETA 3.7m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 400/400 | 74.6m | ETA 0.0m\n",
      "\n",
      "Scoring complete: 400 queries in 74.7 min\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Scoring loop (7 conditions x all passages)\n",
    "print(\"=\" * 70)\n",
    "print(\"RUNNING CONTRASTIVE RANKING EXPERIMENT\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "def build_condition_input(cond_name, passage_data, query_data):\n",
    "    # Return (encoder_text, prefix_token_count, truncate) for a condition.\n",
    "    passage_text = passage_data['text']\n",
    "\n",
    "    if cond_name == 'bare':\n",
    "        return passage_text, 0, False\n",
    "    elif cond_name == 'oracle_trunc':\n",
    "        surr = query_data['query']\n",
    "    elif cond_name == 'oracle_cross_trunc':\n",
    "        surr = query_data['cross_query']\n",
    "    elif cond_name == 'surr_template_trunc':\n",
    "        surr = passage_data['surr_template']\n",
    "    elif cond_name == 'surr_doc_trunc':\n",
    "        surr = passage_data['surr_doc_kw']\n",
    "    elif cond_name == 'random_trunc':\n",
    "        surr = query_data['surr_random']\n",
    "    elif cond_name == 'static_fact_trunc':\n",
    "        surr = STATIC_FACT\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown condition: {cond_name}\")\n",
    "\n",
    "    enc_text = surr + \"\\n\" + passage_text\n",
    "    prefix_count = count_prefix_tokens(surr, passage_text)\n",
    "    return enc_text, prefix_count, True\n",
    "\n",
    "\n",
    "# Resume from checkpoint\n",
    "results = []\n",
    "start_idx = 0\n",
    "if CHECKPOINT_PATH.exists():\n",
    "    saved = json.loads(CHECKPOINT_PATH.read_text())\n",
    "    if saved.get('n_total') == N_SAMPLES:\n",
    "        saved_results = saved.get('results', [])\n",
    "        saved_queries = [r['query'][:50] for r in saved_results]\n",
    "        current_queries = [q['query'][:50] for q in queries[:len(saved_results)]]\n",
    "        if saved_queries == current_queries:\n",
    "            results = saved_results\n",
    "            start_idx = len(results)\n",
    "            print(f\"Resumed from checkpoint: {start_idx}/{N_SAMPLES} queries\")\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "for q_idx in tqdm(range(start_idx, N_SAMPLES), initial=start_idx, total=N_SAMPLES,\n",
    "                  desc=\"Queries\"):\n",
    "    q = queries[q_idx]\n",
    "    answer = q['answer']\n",
    "\n",
    "    query_result = {\n",
    "        'query_idx': q_idx,\n",
    "        'query': q['query'],\n",
    "        'cross_query': q['cross_query'],\n",
    "        'answer': answer,\n",
    "        'n_passages': q['n_passages'],\n",
    "        'relevant_idx': q['relevant_idx'],\n",
    "        'is_selected': [p['is_selected'] for p in q['passages']],\n",
    "        'scores': {},\n",
    "    }\n",
    "\n",
    "    for cond_name in COND_NAMES:\n",
    "        cond_nlls = []\n",
    "        for p_idx, passage_data in enumerate(q['passages']):\n",
    "            enc_text, prefix_count, truncate = build_condition_input(\n",
    "                cond_name, passage_data, q)\n",
    "            nll = score_nll(enc_text, answer, prefix_count, truncate)\n",
    "            cond_nlls.append(nll)\n",
    "        query_result['scores'][cond_name] = cond_nlls\n",
    "\n",
    "    results.append(query_result)\n",
    "\n",
    "    if (q_idx + 1) % 20 == 0 or q_idx == N_SAMPLES - 1:\n",
    "        ckpt = {\n",
    "            'n_total': N_SAMPLES,\n",
    "            'results': results,\n",
    "            'completed': len(results),\n",
    "            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        }\n",
    "        CHECKPOINT_PATH.write_text(json.dumps(ckpt))\n",
    "        elapsed = time.time() - t0\n",
    "        done = q_idx - start_idx + 1\n",
    "        eta = (N_SAMPLES - q_idx - 1) * elapsed / done if done > 0 else 0\n",
    "        tqdm.write(f\"  Checkpoint {q_idx+1}/{N_SAMPLES} | {elapsed/60:.1f}m | ETA {eta/60:.1f}m\")\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "elapsed_total = time.time() - t0\n",
    "print(f\"\\nScoring complete: {len(results)} queries in {elapsed_total/60:.1f} min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d7fc562",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T14:41:07.087779Z",
     "iopub.status.busy": "2026-02-19T14:41:07.087500Z",
     "iopub.status.idle": "2026-02-19T14:41:07.153241Z",
     "shell.execute_reply": "2026-02-19T14:41:07.152595Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "STRATEGY A: STANDARD RANKING (raw NLL)\n",
      "======================================================================\n",
      "Rank by raw NLL. Should replicate Exp 04A for shared conditions.\n",
      "\n",
      "Condition                     AUC    MRR@3    Hit@1    Hit@3    AUC d        AUC p   sig\n",
      "------------------------------------------------------------------------------------------\n",
      "bare                        0.849    0.735    0.662    0.828       --           --    --\n",
      "oracle_trunc                0.855    0.748    0.680    0.838   +0.047     3.13e-01    ns\n",
      "oracle_cross_trunc          0.858    0.759    0.698    0.838   +0.064     1.61e-01    ns\n",
      "surr_template_trunc         0.861    0.762    0.698    0.848   +0.094     3.62e-02    ns\n",
      "surr_doc_trunc              0.865    0.762    0.698    0.853   +0.124     7.26e-03     *\n",
      "random_trunc                0.865    0.758    0.690    0.848   +0.123     1.87e-02    ns\n",
      "static_fact_trunc           0.857    0.757    0.688    0.848   +0.063     1.99e-01    ns\n",
      "\n",
      "Exp 04A reference (6 shared conditions):\n",
      "  bare AUC ~ 0.845, oracle AUC ~ 0.853 (ns)\n",
      "  oracle differential d = -0.007 (ns)\n",
      "\n",
      "NEW: oracle_cross_trunc AUC = 0.858\n",
      "     oracle_trunc AUC = 0.855\n",
      "     Difference = -0.003\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Strategy A -- Standard ranking (replicates Exp 04A)\n",
    "from lib.analysis import cohens_d\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"STRATEGY A: STANDARD RANKING (raw NLL)\")\n",
    "print(\"=\" * 70)\n",
    "print(\"Rank by raw NLL. Should replicate Exp 04A for shared conditions.\\n\")\n",
    "\n",
    "metrics_a = {cond: {'auc': [], 'mrr3': [], 'hit1': [], 'hit3': []}\n",
    "             for cond in COND_NAMES}\n",
    "\n",
    "for r in results:\n",
    "    rel_idx = r['relevant_idx']\n",
    "    for cond in COND_NAMES:\n",
    "        nlls = np.array(r['scores'][cond])\n",
    "        metrics_a[cond]['auc'].append(compute_auc(nlls, rel_idx))\n",
    "        metrics_a[cond]['mrr3'].append(compute_mrr_at_k(nlls, rel_idx, k=3))\n",
    "        metrics_a[cond]['hit1'].append(compute_hit_at_k(nlls, rel_idx, k=1))\n",
    "        metrics_a[cond]['hit3'].append(compute_hit_at_k(nlls, rel_idx, k=3))\n",
    "\n",
    "for cond in COND_NAMES:\n",
    "    for m in metrics_a[cond]:\n",
    "        metrics_a[cond][m] = np.array(metrics_a[cond][m])\n",
    "\n",
    "# Results table\n",
    "print(f\"{'Condition':<24} {'AUC':>8} {'MRR@3':>8} {'Hit@1':>8} {'Hit@3':>8} \"\n",
    "      f\"{'AUC d':>8} {'AUC p':>12} {'sig':>5}\")\n",
    "print(\"-\" * 90)\n",
    "\n",
    "bare_aucs = metrics_a['bare']['auc']\n",
    "strategy_a = {}\n",
    "\n",
    "for cond in COND_NAMES:\n",
    "    auc_mean = metrics_a[cond]['auc'].mean()\n",
    "    mrr_mean = metrics_a[cond]['mrr3'].mean()\n",
    "    hit1_mean = metrics_a[cond]['hit1'].mean()\n",
    "    hit3_mean = metrics_a[cond]['hit3'].mean()\n",
    "\n",
    "    if cond == 'bare':\n",
    "        print(f\"{cond:<24} {auc_mean:>8.3f} {mrr_mean:>8.3f} {hit1_mean:>8.3f} \"\n",
    "              f\"{hit3_mean:>8.3f} {'--':>8} {'--':>12} {'--':>5}\")\n",
    "        strategy_a[cond] = {'auc': float(auc_mean)}\n",
    "    else:\n",
    "        diff = metrics_a[cond]['auc'] - bare_aucs\n",
    "        d = cohens_d(diff)\n",
    "        nonzero = diff[diff != 0]\n",
    "        if len(nonzero) >= 10:\n",
    "            try:\n",
    "                _, p_val = wilcoxon(nonzero)\n",
    "            except ValueError:\n",
    "                p_val = 1.0\n",
    "        else:\n",
    "            p_val = 1.0\n",
    "        sig = ('***' if p_val < 0.001/N_BONFERRONI else\n",
    "               '**' if p_val < 0.01/N_BONFERRONI else\n",
    "               '*' if p_val < 0.05/N_BONFERRONI else 'ns')\n",
    "        print(f\"{cond:<24} {auc_mean:>8.3f} {mrr_mean:>8.3f} {hit1_mean:>8.3f} \"\n",
    "              f\"{hit3_mean:>8.3f} {d:>+8.3f} {p_val:>12.2e} {sig:>5}\")\n",
    "        strategy_a[cond] = {\n",
    "            'auc': float(auc_mean), 'd': float(d), 'p': float(p_val),\n",
    "        }\n",
    "\n",
    "# Cross-reference with Exp 04A\n",
    "print(f\"\\nExp 04A reference (6 shared conditions):\")\n",
    "print(f\"  bare AUC ~ 0.845, oracle AUC ~ 0.853 (ns)\")\n",
    "print(f\"  oracle differential d = -0.007 (ns)\")\n",
    "\n",
    "# New condition: oracle_cross_trunc\n",
    "cross_auc = strategy_a.get('oracle_cross_trunc', {}).get('auc', 0)\n",
    "oracle_auc = strategy_a.get('oracle_trunc', {}).get('auc', 0)\n",
    "print(f\"\\nNEW: oracle_cross_trunc AUC = {cross_auc:.3f}\")\n",
    "print(f\"     oracle_trunc AUC = {oracle_auc:.3f}\")\n",
    "print(f\"     Difference = {oracle_auc - cross_auc:+.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4075c50",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T14:41:07.156283Z",
     "iopub.status.busy": "2026-02-19T14:41:07.155781Z",
     "iopub.status.idle": "2026-02-19T14:41:07.222672Z",
     "shell.execute_reply": "2026-02-19T14:41:07.222010Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "STRATEGY B: DELTA-NLL RANKING\n",
      "======================================================================\n",
      "Rank by delta = NLL(bare) - NLL(condition).\n",
      "Higher delta = more improved by priming = predicted relevant.\n",
      "\n",
      "Condition                  B: delta AUC   d vs 0.5   sig   C: rel delta   d vs 0.5   sig\n",
      "------------------------------------------------------------------------------------------\n",
      "  oracle_trunc                    0.555     +0.172   ***          0.555     +0.172   ***\n",
      "  oracle_cross_trunc              0.606     +0.345   ***          0.606     +0.345   ***\n",
      "  surr_template_trunc             0.665     +0.524   ***          0.665     +0.524   ***\n",
      "  surr_doc_trunc                  0.587     +0.287   ***          0.587     +0.287   ***\n",
      "  random_trunc                    0.660     +0.549   ***          0.660     +0.549   ***\n",
      "  static_fact_trunc               0.686     +0.612   ***          0.686     +0.612   ***\n",
      "\n",
      "Interpretation:\n",
      "  Strategy B: higher delta = more improved -> predicted relevant\n",
      "  Strategy C: remove pool-level shift, rank by deviation from mean\n",
      "  AUC > 0.5 with p<0.05 = ranking signal detected\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Strategies B+C -- Delta-NLL and Relative Delta ranking\n",
    "print(\"=\" * 70)\n",
    "print(\"STRATEGY B: DELTA-NLL RANKING\")\n",
    "print(\"=\" * 70)\n",
    "print(\"Rank by delta = NLL(bare) - NLL(condition).\")\n",
    "print(\"Higher delta = more improved by priming = predicted relevant.\\n\")\n",
    "\n",
    "strategy_b = {}\n",
    "strategy_c = {}\n",
    "\n",
    "for cond in COND_NAMES[1:]:  # skip bare\n",
    "    aucs_b = []\n",
    "    aucs_c = []\n",
    "\n",
    "    for r in results:\n",
    "        rel_idx = r['relevant_idx']\n",
    "        bare_nlls_q = np.array(r['scores']['bare'])\n",
    "        cond_nlls_q = np.array(r['scores'][cond])\n",
    "\n",
    "        # Strategy B: rank by delta (higher = better)\n",
    "        deltas = bare_nlls_q - cond_nlls_q\n",
    "        aucs_b.append(compute_auc_higher_better(deltas, rel_idx))\n",
    "\n",
    "        # Strategy C: rank by relative delta (subtract pool mean)\n",
    "        pool_mean_delta = deltas.mean()\n",
    "        relative_deltas = deltas - pool_mean_delta\n",
    "        aucs_c.append(compute_auc_higher_better(relative_deltas, rel_idx))\n",
    "\n",
    "    aucs_b = np.array(aucs_b)\n",
    "    aucs_c = np.array(aucs_c)\n",
    "\n",
    "    # Compare to Strategy A and chance\n",
    "    d_b = cohens_d(aucs_b - 0.5)  # vs chance\n",
    "    d_c = cohens_d(aucs_c - 0.5)\n",
    "\n",
    "    _, p_b = stats.ttest_1samp(aucs_b, 0.5)\n",
    "    _, p_c = stats.ttest_1samp(aucs_c, 0.5)\n",
    "\n",
    "    sig_b = ('***' if p_b < 0.001 else '**' if p_b < 0.01 else\n",
    "             '*' if p_b < 0.05 else 'ns')\n",
    "    sig_c = ('***' if p_c < 0.001 else '**' if p_c < 0.01 else\n",
    "             '*' if p_c < 0.05 else 'ns')\n",
    "\n",
    "    strategy_b[cond] = {'auc': float(aucs_b.mean()), 'd_vs_chance': float(d_b),\n",
    "                         'p': float(p_b)}\n",
    "    strategy_c[cond] = {'auc': float(aucs_c.mean()), 'd_vs_chance': float(d_c),\n",
    "                         'p': float(p_c)}\n",
    "\n",
    "print(f\"{'Condition':<24} {'B: delta AUC':>14} {'d vs 0.5':>10} {'sig':>5} \"\n",
    "      f\"{'C: rel delta':>14} {'d vs 0.5':>10} {'sig':>5}\")\n",
    "print(\"-\" * 90)\n",
    "\n",
    "for cond in COND_NAMES[1:]:\n",
    "    b = strategy_b[cond]\n",
    "    c = strategy_c[cond]\n",
    "    sig_b = ('***' if b['p'] < 0.001 else '**' if b['p'] < 0.01 else\n",
    "             '*' if b['p'] < 0.05 else 'ns')\n",
    "    sig_c = ('***' if c['p'] < 0.001 else '**' if c['p'] < 0.01 else\n",
    "             '*' if c['p'] < 0.05 else 'ns')\n",
    "    print(f\"  {cond:<22} {b['auc']:>14.3f} {b['d_vs_chance']:>+10.3f} {sig_b:>5} \"\n",
    "          f\"{c['auc']:>14.3f} {c['d_vs_chance']:>+10.3f} {sig_c:>5}\")\n",
    "\n",
    "print(f\"\\nInterpretation:\")\n",
    "print(f\"  Strategy B: higher delta = more improved -> predicted relevant\")\n",
    "print(f\"  Strategy C: remove pool-level shift, rank by deviation from mean\")\n",
    "print(f\"  AUC > 0.5 with p<0.05 = ranking signal detected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "21a1d419",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T14:41:07.225757Z",
     "iopub.status.busy": "2026-02-19T14:41:07.225185Z",
     "iopub.status.idle": "2026-02-19T14:41:07.246031Z",
     "shell.execute_reply": "2026-02-19T14:41:07.245381Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "STRATEGY D: PER-DOC TEMPLATE CONTRASTIVE\n",
      "======================================================================\n",
      "surr_template is per-passage (different keyword per doc).\n",
      "Structural component is ~constant -> variation is semantic match.\n",
      "\n",
      "Template contrastive AUC: 0.506 (d vs 0.5: +0.019, p=7.09e-01 ns)\n",
      "  Relevant passage template uplift:   +0.0258\n",
      "  Irrelevant passage template uplift: +0.0064\n",
      "  Differential (rel - irrel):          +0.0194\n",
      "  Differential d=+0.046, p=3.57e-01 ns\n",
      "\n",
      "  >>> No ranking signal from per-doc template variation.\n"
     ]
    }
   ],
   "source": [
    "# Cell 10: Strategy D -- Per-doc template contrastive\n",
    "print(\"=\" * 70)\n",
    "print(\"STRATEGY D: PER-DOC TEMPLATE CONTRASTIVE\")\n",
    "print(\"=\" * 70)\n",
    "print(\"surr_template is per-passage (different keyword per doc).\")\n",
    "print(\"Structural component is ~constant -> variation is semantic match.\\n\")\n",
    "\n",
    "# For each query, compute the template delta for each passage\n",
    "# Then check if relevant passage has higher template delta\n",
    "strategy_d = {'auc': [], 'delta_rel': [], 'delta_irrel': []}\n",
    "\n",
    "for r in results:\n",
    "    rel_idx = r['relevant_idx']\n",
    "    bare_nlls_q = np.array(r['scores']['bare'])\n",
    "    template_nlls_q = np.array(r['scores']['surr_template_trunc'])\n",
    "    random_nlls_q = np.array(r['scores']['random_trunc'])\n",
    "\n",
    "    # Template delta minus random delta: isolates per-passage keyword match\n",
    "    template_delta = bare_nlls_q - template_nlls_q\n",
    "    random_delta = bare_nlls_q - random_nlls_q\n",
    "    contrastive_delta = template_delta - random_delta\n",
    "\n",
    "    # Rank by contrastive_delta (higher = template helped more than random)\n",
    "    strategy_d['auc'].append(compute_auc_higher_better(contrastive_delta, rel_idx))\n",
    "    strategy_d['delta_rel'].append(contrastive_delta[rel_idx])\n",
    "    irrel_deltas = [contrastive_delta[i] for i in range(len(contrastive_delta))\n",
    "                    if i != rel_idx]\n",
    "    strategy_d['delta_irrel'].append(np.mean(irrel_deltas))\n",
    "\n",
    "for k in strategy_d:\n",
    "    strategy_d[k] = np.array(strategy_d[k])\n",
    "\n",
    "auc_mean = strategy_d['auc'].mean()\n",
    "d_vs_chance = cohens_d(strategy_d['auc'] - 0.5)\n",
    "_, p_val = stats.ttest_1samp(strategy_d['auc'], 0.5)\n",
    "sig = ('***' if p_val < 0.001 else '**' if p_val < 0.01 else\n",
    "       '*' if p_val < 0.05 else 'ns')\n",
    "\n",
    "print(f\"Template contrastive AUC: {auc_mean:.3f} (d vs 0.5: {d_vs_chance:+.3f}, \"\n",
    "      f\"p={p_val:.2e} {sig})\")\n",
    "print(f\"  Relevant passage template uplift:   {strategy_d['delta_rel'].mean():+.4f}\")\n",
    "print(f\"  Irrelevant passage template uplift: {strategy_d['delta_irrel'].mean():+.4f}\")\n",
    "print(f\"  Differential (rel - irrel):          \"\n",
    "      f\"{(strategy_d['delta_rel'] - strategy_d['delta_irrel']).mean():+.4f}\")\n",
    "\n",
    "diff_rel_irrel = strategy_d['delta_rel'] - strategy_d['delta_irrel']\n",
    "d_diff = cohens_d(diff_rel_irrel)\n",
    "_, p_diff = stats.ttest_1samp(diff_rel_irrel, 0)\n",
    "sig_diff = ('***' if p_diff < 0.001 else '**' if p_diff < 0.01 else\n",
    "            '*' if p_diff < 0.05 else 'ns')\n",
    "print(f\"  Differential d={d_diff:+.3f}, p={p_diff:.2e} {sig_diff}\")\n",
    "\n",
    "if auc_mean > 0.52 and p_val < 0.05:\n",
    "    print(f\"\\n  >>> TEMPLATE CONTRASTIVE detects ranking signal!\")\n",
    "    print(f\"      Per-passage keywords DO create differential benefit.\")\n",
    "else:\n",
    "    print(f\"\\n  >>> No ranking signal from per-doc template variation.\")\n",
    "\n",
    "strategy_d_summary = {\n",
    "    'auc': float(auc_mean), 'd_vs_chance': float(d_vs_chance),\n",
    "    'p': float(p_val), 'differential_d': float(d_diff),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3527176a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T14:41:07.248988Z",
     "iopub.status.busy": "2026-02-19T14:41:07.248428Z",
     "iopub.status.idle": "2026-02-19T14:41:07.285937Z",
     "shell.execute_reply": "2026-02-19T14:41:07.285271Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "STRATEGY E: ORACLE CROSS-CONTRASTIVE RANKING\n",
      "======================================================================\n",
      "Rank by NLL(oracle) - NLL(oracle_cross).\n",
      "Lower = oracle helps more than wrong query = more relevant.\n",
      "Structural benefit cancels. Residual = semantic query-doc match.\n",
      "\n",
      "Oracle cross-contrastive ranking:\n",
      "  AUC:   0.437 (d vs 0.5: -0.194, p=1.20e-04 ***)\n",
      "  MRR@3: 0.179\n",
      "  Hit@1: 0.092\n",
      "\n",
      "--- Contrastive values ---\n",
      "  Relevant mean:   -0.0838\n",
      "  Irrelevant mean: -0.1357\n",
      "  Differential: +0.0519 (d=+0.095, p=5.94e-02 ns)\n",
      "  % queries where relevant more negative: 40.0%\n",
      "\n",
      "--- Structural benefit check ---\n",
      "  Oracle mean NLL drop: +0.5770\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Cross mean NLL drop:  +0.4477\n",
      "  (Should be similar if structural benefit is length-dependent)\n",
      "\n",
      "  >>> No cross-contrastive signal: semantic residual too small for ranking\n"
     ]
    }
   ],
   "source": [
    "# Cell 11: Strategy E -- Oracle cross-contrastive ranking (KEY NOVEL ANALYSIS)\n",
    "print(\"=\" * 70)\n",
    "print(\"STRATEGY E: ORACLE CROSS-CONTRASTIVE RANKING\")\n",
    "print(\"=\" * 70)\n",
    "print(\"Rank by NLL(oracle) - NLL(oracle_cross).\")\n",
    "print(\"Lower = oracle helps more than wrong query = more relevant.\")\n",
    "print(\"Structural benefit cancels. Residual = semantic query-doc match.\\n\")\n",
    "\n",
    "strategy_e = {'auc': [], 'contrastive_rel': [], 'contrastive_irrel': [],\n",
    "              'mrr3': [], 'hit1': []}\n",
    "\n",
    "for r in results:\n",
    "    rel_idx = r['relevant_idx']\n",
    "    oracle_nlls_q = np.array(r['scores']['oracle_trunc'])\n",
    "    cross_nlls_q = np.array(r['scores']['oracle_cross_trunc'])\n",
    "\n",
    "    # Contrastive score = NLL(oracle) - NLL(oracle_cross)\n",
    "    # More negative = oracle helps more than cross = more relevant\n",
    "    contrastive = oracle_nlls_q - cross_nlls_q\n",
    "\n",
    "    # Rank by contrastive (lower = more relevant)\n",
    "    strategy_e['auc'].append(compute_auc(contrastive, rel_idx))\n",
    "\n",
    "    # Also compute MRR@3 and Hit@1\n",
    "    ranked = np.argsort(contrastive)\n",
    "    for rank, idx in enumerate(ranked[:3], 1):\n",
    "        if idx == rel_idx:\n",
    "            strategy_e['mrr3'].append(1.0 / rank)\n",
    "            break\n",
    "    else:\n",
    "        strategy_e['mrr3'].append(0.0)\n",
    "    strategy_e['hit1'].append(1.0 if ranked[0] == rel_idx else 0.0)\n",
    "\n",
    "    # Track contrastive values\n",
    "    strategy_e['contrastive_rel'].append(contrastive[rel_idx])\n",
    "    irrel_vals = [contrastive[i] for i in range(len(contrastive)) if i != rel_idx]\n",
    "    strategy_e['contrastive_irrel'].append(np.mean(irrel_vals))\n",
    "\n",
    "for k in strategy_e:\n",
    "    strategy_e[k] = np.array(strategy_e[k])\n",
    "\n",
    "auc_mean = strategy_e['auc'].mean()\n",
    "mrr_mean = strategy_e['mrr3'].mean()\n",
    "hit1_mean = strategy_e['hit1'].mean()\n",
    "d_vs_chance = cohens_d(strategy_e['auc'] - 0.5)\n",
    "_, p_val = stats.ttest_1samp(strategy_e['auc'], 0.5)\n",
    "sig = ('***' if p_val < 0.001 else '**' if p_val < 0.01 else\n",
    "       '*' if p_val < 0.05 else 'ns')\n",
    "\n",
    "print(f\"Oracle cross-contrastive ranking:\")\n",
    "print(f\"  AUC:   {auc_mean:.3f} (d vs 0.5: {d_vs_chance:+.3f}, p={p_val:.2e} {sig})\")\n",
    "print(f\"  MRR@3: {mrr_mean:.3f}\")\n",
    "print(f\"  Hit@1: {hit1_mean:.3f}\")\n",
    "\n",
    "# Contrastive value analysis\n",
    "print(f\"\\n--- Contrastive values ---\")\n",
    "print(f\"  Relevant mean:   {strategy_e['contrastive_rel'].mean():+.4f}\")\n",
    "print(f\"  Irrelevant mean: {strategy_e['contrastive_irrel'].mean():+.4f}\")\n",
    "\n",
    "diff_re = strategy_e['contrastive_rel'] - strategy_e['contrastive_irrel']\n",
    "d_diff = cohens_d(diff_re)\n",
    "_, p_diff = stats.ttest_1samp(diff_re, 0)\n",
    "sig_diff = ('***' if p_diff < 0.001 else '**' if p_diff < 0.01 else\n",
    "            '*' if p_diff < 0.05 else 'ns')\n",
    "print(f\"  Differential: {diff_re.mean():+.4f} (d={d_diff:+.3f}, p={p_diff:.2e} {sig_diff})\")\n",
    "print(f\"  % queries where relevant more negative: \"\n",
    "      f\"{100*np.mean(diff_re < 0):.1f}%\")\n",
    "\n",
    "# Structural benefit comparison: oracle vs oracle_cross\n",
    "print(f\"\\n--- Structural benefit check ---\")\n",
    "print(f\"  Oracle mean NLL drop: \"\n",
    "      f\"{np.mean([np.mean(np.array(r['scores']['bare']) - np.array(r['scores']['oracle_trunc'])) for r in results]):+.4f}\")\n",
    "print(f\"  Cross mean NLL drop:  \"\n",
    "      f\"{np.mean([np.mean(np.array(r['scores']['bare']) - np.array(r['scores']['oracle_cross_trunc'])) for r in results]):+.4f}\")\n",
    "print(f\"  (Should be similar if structural benefit is length-dependent)\")\n",
    "\n",
    "if auc_mean > 0.52 and p_val < 0.05:\n",
    "    print(f\"\\n  >>> CROSS-CONTRASTIVE RANKING WORKS!\")\n",
    "    print(f\"      The semantic residual CAN be isolated for ranking.\")\n",
    "    print(f\"      This is the signal that v2 and Exp 04A could never detect.\")\n",
    "elif auc_mean > 0.51:\n",
    "    print(f\"\\n  >>> MARGINAL signal: {auc_mean:.3f} (needs larger N to confirm)\")\n",
    "else:\n",
    "    print(f\"\\n  >>> No cross-contrastive signal: semantic residual too small for ranking\")\n",
    "\n",
    "strategy_e_summary = {\n",
    "    'auc': float(auc_mean), 'mrr3': float(mrr_mean), 'hit1': float(hit1_mean),\n",
    "    'd_vs_chance': float(d_vs_chance), 'p': float(p_val),\n",
    "    'contrastive_rel_mean': float(strategy_e['contrastive_rel'].mean()),\n",
    "    'contrastive_irrel_mean': float(strategy_e['contrastive_irrel'].mean()),\n",
    "    'differential_d': float(d_diff),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "29bda2c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T14:41:07.289004Z",
     "iopub.status.busy": "2026-02-19T14:41:07.288410Z",
     "iopub.status.idle": "2026-02-19T14:41:07.536165Z",
     "shell.execute_reply": "2026-02-19T14:41:07.535427Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "STRATEGY F: VOCABULARY OVERLAP CORRELATION\n",
      "======================================================================\n",
      "Correlate delta(bare, oracle) with Jaccard overlap between query and passage.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall correlation (all passages):\n",
      "  Pearson r(overlap, delta) = +0.002 (p=8.90e-01)\n",
      "  Relevant only:   r = +0.106 (p=3.33e-02)\n",
      "  Irrelevant only: r = -0.011 (p=5.51e-01)\n",
      "\n",
      "Vocabulary overlap stats:\n",
      "  Relevant passages:   mean=0.078\n",
      "  Irrelevant passages: mean=0.065\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Overlap-based ranking AUC: 0.604 (d vs 0.5: +0.321)\n",
      "\n",
      "  >>> No correlation: oracle benefit is independent of vocab overlap.\n",
      "      Consistent with ~85% structural mechanism.\n"
     ]
    }
   ],
   "source": [
    "# Cell 12: Strategy F -- Vocabulary overlap correlation\n",
    "print(\"=\" * 70)\n",
    "print(\"STRATEGY F: VOCABULARY OVERLAP CORRELATION\")\n",
    "print(\"=\" * 70)\n",
    "print(\"Correlate delta(bare, oracle) with Jaccard overlap between query and passage.\\n\")\n",
    "\n",
    "# For each passage, compute vocab overlap and oracle delta\n",
    "per_passage_data = []\n",
    "for r in results:\n",
    "    query_text = r['query']\n",
    "    bare_nlls_q = np.array(r['scores']['bare'])\n",
    "    oracle_nlls_q = np.array(r['scores']['oracle_trunc'])\n",
    "    rel_idx = r['relevant_idx']\n",
    "\n",
    "    q_idx = r['query_idx']\n",
    "    for p_idx in range(r['n_passages']):\n",
    "        p_text = queries[q_idx]['passages'][p_idx]['text']\n",
    "        overlap = compute_vocab_overlap(query_text, p_text)\n",
    "        delta = bare_nlls_q[p_idx] - oracle_nlls_q[p_idx]\n",
    "        per_passage_data.append({\n",
    "            'overlap': overlap,\n",
    "            'delta': delta,\n",
    "            'is_relevant': 1 if p_idx == rel_idx else 0,\n",
    "        })\n",
    "\n",
    "overlaps = np.array([d['overlap'] for d in per_passage_data])\n",
    "deltas = np.array([d['delta'] for d in per_passage_data])\n",
    "is_rel = np.array([d['is_relevant'] for d in per_passage_data])\n",
    "\n",
    "# Overall correlation\n",
    "r_all, p_all = stats.pearsonr(overlaps, deltas)\n",
    "print(f\"Overall correlation (all passages):\")\n",
    "print(f\"  Pearson r(overlap, delta) = {r_all:+.3f} (p={p_all:.2e})\")\n",
    "\n",
    "# Relevant vs irrelevant\n",
    "r_rel, p_rel = stats.pearsonr(overlaps[is_rel == 1], deltas[is_rel == 1])\n",
    "r_irrel, p_irrel = stats.pearsonr(overlaps[is_rel == 0], deltas[is_rel == 0])\n",
    "print(f\"  Relevant only:   r = {r_rel:+.3f} (p={p_rel:.2e})\")\n",
    "print(f\"  Irrelevant only: r = {r_irrel:+.3f} (p={p_irrel:.2e})\")\n",
    "\n",
    "# Overlap difference: relevant vs irrelevant\n",
    "rel_overlaps = overlaps[is_rel == 1]\n",
    "irrel_overlaps = overlaps[is_rel == 0]\n",
    "print(f\"\\nVocabulary overlap stats:\")\n",
    "print(f\"  Relevant passages:   mean={rel_overlaps.mean():.3f}\")\n",
    "print(f\"  Irrelevant passages: mean={irrel_overlaps.mean():.3f}\")\n",
    "\n",
    "# Can overlap alone predict relevance?\n",
    "overlap_auc_per_query = []\n",
    "for r in results:\n",
    "    q_text = r['query']\n",
    "    q_idx = r['query_idx']\n",
    "    rel_idx = r['relevant_idx']\n",
    "    passage_overlaps = []\n",
    "    for p_idx in range(r['n_passages']):\n",
    "        p_text = queries[q_idx]['passages'][p_idx]['text']\n",
    "        passage_overlaps.append(-compute_vocab_overlap(q_text, p_text))\n",
    "    passage_overlaps = np.array(passage_overlaps)\n",
    "    overlap_auc_per_query.append(compute_auc(passage_overlaps, rel_idx))\n",
    "\n",
    "overlap_auc = np.mean(overlap_auc_per_query)\n",
    "d_overlap = cohens_d(np.array(overlap_auc_per_query) - 0.5)\n",
    "print(f\"\\nOverlap-based ranking AUC: {overlap_auc:.3f} (d vs 0.5: {d_overlap:+.3f})\")\n",
    "\n",
    "if abs(r_all) > 0.1 and p_all < 0.05:\n",
    "    print(f\"\\n  >>> Vocabulary overlap DOES predict oracle benefit.\")\n",
    "    print(f\"      Higher overlap passages get more help from the oracle query.\")\n",
    "else:\n",
    "    print(f\"\\n  >>> No correlation: oracle benefit is independent of vocab overlap.\")\n",
    "    print(f\"      Consistent with ~85% structural mechanism.\")\n",
    "\n",
    "strategy_f_summary = {\n",
    "    'r_overall': float(r_all), 'p_overall': float(p_all),\n",
    "    'r_relevant': float(r_rel), 'r_irrelevant': float(r_irrel),\n",
    "    'overlap_ranking_auc': float(overlap_auc),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "99928be8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T14:41:07.539352Z",
     "iopub.status.busy": "2026-02-19T14:41:07.539080Z",
     "iopub.status.idle": "2026-02-19T14:41:08.223124Z",
     "shell.execute_reply": "2026-02-19T14:41:08.222400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "SUMMARY: AUC ACROSS ALL 6 STRATEGIES\n",
      "======================================================================\n",
      "\n",
      "Strategy                           Best AUC            Best condition   vs 0.5\n",
      "--------------------------------------------------------------------------------\n",
      "  A: Standard (raw NLL)               0.865              random_trunc   +0.365\n",
      "  B: Delta-NLL                        0.686         static_fact_trunc   +0.186\n",
      "  C: Relative delta                   0.686         static_fact_trunc   +0.186\n",
      "  D: Template contrastive             0.506           template-random   +0.006\n",
      "  E: Oracle cross-contrastive         0.437              oracle-cross   -0.063\n",
      "  F: Vocab overlap                    0.604                   overlap   +0.104\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plot saved to results/exp08/strategy_comparison.png\n",
      "\n",
      "Best strategy: A: Standard (raw NLL) (AUC=0.865)\n",
      "  >>> Ranking signal detected! Best approach: A: Standard (raw NLL)\n"
     ]
    }
   ],
   "source": [
    "# Cell 13: Summary table comparing all 6 strategies\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"SUMMARY: AUC ACROSS ALL 6 STRATEGIES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Collect best AUC per strategy\n",
    "summary = {\n",
    "    'A: Standard (raw NLL)': {\n",
    "        'best_cond': max([(c, strategy_a[c]['auc']) for c in COND_NAMES[1:]],\n",
    "                         key=lambda x: x[1]),\n",
    "        'oracle_auc': strategy_a.get('oracle_trunc', {}).get('auc', 0.5),\n",
    "    },\n",
    "    'B: Delta-NLL': {\n",
    "        'best_cond': max([(c, strategy_b[c]['auc']) for c in strategy_b],\n",
    "                         key=lambda x: x[1]),\n",
    "        'oracle_auc': strategy_b.get('oracle_trunc', {}).get('auc', 0.5),\n",
    "    },\n",
    "    'C: Relative delta': {\n",
    "        'best_cond': max([(c, strategy_c[c]['auc']) for c in strategy_c],\n",
    "                         key=lambda x: x[1]),\n",
    "        'oracle_auc': strategy_c.get('oracle_trunc', {}).get('auc', 0.5),\n",
    "    },\n",
    "    'D: Template contrastive': {\n",
    "        'best_cond': ('template-random', strategy_d_summary['auc']),\n",
    "        'oracle_auc': strategy_d_summary['auc'],\n",
    "    },\n",
    "    'E: Oracle cross-contrastive': {\n",
    "        'best_cond': ('oracle-cross', strategy_e_summary['auc']),\n",
    "        'oracle_auc': strategy_e_summary['auc'],\n",
    "    },\n",
    "    'F: Vocab overlap': {\n",
    "        'best_cond': ('overlap', strategy_f_summary['overlap_ranking_auc']),\n",
    "        'oracle_auc': strategy_f_summary['overlap_ranking_auc'],\n",
    "    },\n",
    "}\n",
    "\n",
    "print(f\"\\n{'Strategy':<32} {'Best AUC':>10} {'Best condition':>25} {'vs 0.5':>8}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "strategy_aucs = []\n",
    "strategy_names = []\n",
    "for name, data in summary.items():\n",
    "    best_cond, best_auc = data['best_cond']\n",
    "    delta = best_auc - 0.5\n",
    "    print(f\"  {name:<30} {best_auc:>10.3f} {best_cond:>25} {delta:>+8.3f}\")\n",
    "    strategy_aucs.append(best_auc)\n",
    "    strategy_names.append(name.split(':')[0])\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "x = np.arange(len(strategy_aucs))\n",
    "colors = ['steelblue' if a > 0.52 else 'gray' if a > 0.5 else 'lightcoral'\n",
    "          for a in strategy_aucs]\n",
    "ax.bar(x, strategy_aucs, color=colors, edgecolor='black', linewidth=0.5)\n",
    "ax.axhline(y=0.5, color='red', linestyle='--', alpha=0.7, label='Chance')\n",
    "ax.set_ylabel('AUC')\n",
    "ax.set_title('Ranking AUC Across All 6 Strategies')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(strategy_names, rotation=30, ha='right')\n",
    "ax.legend()\n",
    "ax.set_ylim(0.45, max(strategy_aucs) + 0.05)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plot_path = RESULTS_DIR / 'strategy_comparison.png'\n",
    "plt.savefig(plot_path, dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"Plot saved to {plot_path}\")\n",
    "\n",
    "# Best strategy?\n",
    "best_idx = np.argmax(strategy_aucs)\n",
    "best_name = list(summary.keys())[best_idx]\n",
    "best_auc = strategy_aucs[best_idx]\n",
    "print(f\"\\nBest strategy: {best_name} (AUC={best_auc:.3f})\")\n",
    "if best_auc > 0.52:\n",
    "    print(f\"  >>> Ranking signal detected! Best approach: {best_name}\")\n",
    "else:\n",
    "    print(f\"  >>> No strategy achieves meaningful ranking. Confirmed: ranking is dead.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c64d9100",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T14:41:08.226631Z",
     "iopub.status.busy": "2026-02-19T14:41:08.226047Z",
     "iopub.status.idle": "2026-02-19T14:41:08.741642Z",
     "shell.execute_reply": "2026-02-19T14:41:08.740940Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "VERDICT -- Exp 08: Contrastive & Differential Ranking\n",
      "======================================================================\n",
      "\n",
      "Model: google/t5gemma-2-4b-4b\n",
      "N queries: 400\n",
      "Mean passages per query: 8.2\n",
      "\n",
      "--- Strategy results ---\n",
      "  A (standard):          oracle AUC = 0.855\n",
      "  B (delta-NLL):         oracle AUC = 0.555\n",
      "  C (relative delta):    oracle AUC = 0.555\n",
      "  D (template contrast): AUC = 0.506\n",
      "  E (cross-contrastive): AUC = 0.437 *** KEY ***\n",
      "  F (vocab overlap):     AUC = 0.604\n",
      "\n",
      "--- Strategy E: Cross-contrastive deep dive ---\n",
      "  Contrastive = NLL(oracle) - NLL(oracle_cross)\n",
      "  Relevant:   -0.0838\n",
      "  Irrelevant: -0.1357\n",
      "  Differential d: +0.095\n",
      "\n",
      "--- OVERALL VERDICT ---\n",
      "  MARGINAL signal via A: Standard (raw NLL) (AUC=0.865)\n",
      "  Cross-contrastive did NOT achieve clear signal.\n",
      "\n",
      "--- Exp 04A comparison ---\n",
      "  04A oracle AUC: ~0.853 (Strategy A only)\n",
      "  08 oracle AUC:  0.855 (Strategy A)\n",
      "  Replication: YES\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Results saved to results/exp08/results.json\n",
      "Cleaning up GPU memory...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory: 15.03 GB -> 0.01 GB\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Cell 14: Verdict + save + cleanup\n",
    "print(\"=\" * 70)\n",
    "print(\"VERDICT -- Exp 08: Contrastive & Differential Ranking\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\nModel: {MODEL_NAME}\")\n",
    "print(f\"N queries: {N_SAMPLES}\")\n",
    "print(f\"Mean passages per query: {np.mean([r['n_passages'] for r in results]):.1f}\")\n",
    "\n",
    "# Key results by strategy\n",
    "print(f\"\\n--- Strategy results ---\")\n",
    "print(f\"  A (standard):          oracle AUC = {strategy_a.get('oracle_trunc', {}).get('auc', 0):.3f}\")\n",
    "print(f\"  B (delta-NLL):         oracle AUC = {strategy_b.get('oracle_trunc', {}).get('auc', 0):.3f}\")\n",
    "print(f\"  C (relative delta):    oracle AUC = {strategy_c.get('oracle_trunc', {}).get('auc', 0):.3f}\")\n",
    "print(f\"  D (template contrast): AUC = {strategy_d_summary['auc']:.3f}\")\n",
    "print(f\"  E (cross-contrastive): AUC = {strategy_e_summary['auc']:.3f} *** KEY ***\")\n",
    "print(f\"  F (vocab overlap):     AUC = {strategy_f_summary['overlap_ranking_auc']:.3f}\")\n",
    "\n",
    "# Strategy E deep dive\n",
    "print(f\"\\n--- Strategy E: Cross-contrastive deep dive ---\")\n",
    "print(f\"  Contrastive = NLL(oracle) - NLL(oracle_cross)\")\n",
    "print(f\"  Relevant:   {strategy_e_summary['contrastive_rel_mean']:+.4f}\")\n",
    "print(f\"  Irrelevant: {strategy_e_summary['contrastive_irrel_mean']:+.4f}\")\n",
    "print(f\"  Differential d: {strategy_e_summary['differential_d']:+.3f}\")\n",
    "\n",
    "# Overall verdict\n",
    "print(f\"\\n--- OVERALL VERDICT ---\")\n",
    "any_signal = any(a > 0.52 for a in strategy_aucs)\n",
    "cross_works = strategy_e_summary['auc'] > 0.52 and strategy_e_summary['p'] < 0.05\n",
    "\n",
    "if cross_works:\n",
    "    print(f\"  CROSS-CONTRASTIVE RANKING WORKS (AUC={strategy_e_summary['auc']:.3f})\")\n",
    "    print(f\"  The semantic residual (~15%) CAN be isolated by subtracting structural\")\n",
    "    print(f\"  benefit using a wrong query. This is the first positive ranking result\")\n",
    "    print(f\"  across 8 experiments (v2) and 3 experiments (v3).\")\n",
    "elif any_signal:\n",
    "    best_idx = np.argmax(strategy_aucs)\n",
    "    best_name = list(summary.keys())[best_idx]\n",
    "    print(f\"  MARGINAL signal via {best_name} (AUC={strategy_aucs[best_idx]:.3f})\")\n",
    "    print(f\"  Cross-contrastive did NOT achieve clear signal.\")\n",
    "else:\n",
    "    print(f\"  NO strategy produces ranking signal.\")\n",
    "    print(f\"  Confirmed: the structural mechanism is document-independent,\")\n",
    "    print(f\"  and the semantic residual is too small or noisy for ranking.\")\n",
    "    print(f\"  This closes the ranking investigation definitively.\")\n",
    "\n",
    "# Exp 04A comparison\n",
    "print(f\"\\n--- Exp 04A comparison ---\")\n",
    "print(f\"  04A oracle AUC: ~0.853 (Strategy A only)\")\n",
    "print(f\"  08 oracle AUC:  {strategy_a.get('oracle_trunc', {}).get('auc', 0):.3f} (Strategy A)\")\n",
    "print(f\"  Replication: {'YES' if abs(strategy_a.get('oracle_trunc', {}).get('auc', 0) - 0.853) < 0.02 else 'CLOSE' if abs(strategy_a.get('oracle_trunc', {}).get('auc', 0) - 0.853) < 0.05 else 'DIFFERENT'}\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "\n",
    "# Save results\n",
    "final_results = {\n",
    "    'experiment': 'exp08_contrastive_ranking',\n",
    "    'model': MODEL_NAME,\n",
    "    'n_queries': N_SAMPLES,\n",
    "    'n_bonferroni': N_BONFERRONI,\n",
    "    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'strategy_a': strategy_a,\n",
    "    'strategy_b': strategy_b,\n",
    "    'strategy_c': strategy_c,\n",
    "    'strategy_d': strategy_d_summary,\n",
    "    'strategy_e': strategy_e_summary,\n",
    "    'strategy_f': strategy_f_summary,\n",
    "    'pool_stats': {\n",
    "        'mean_passages_per_query': float(np.mean([r['n_passages'] for r in results])),\n",
    "        'total_passages': int(sum(r['n_passages'] for r in results)),\n",
    "    },\n",
    "}\n",
    "\n",
    "with open(RESULTS_DIR / 'results.json', 'w') as f:\n",
    "    json.dump(final_results, f, indent=2)\n",
    "print(f\"\\nResults saved to {RESULTS_DIR / 'results.json'}\")\n",
    "\n",
    "# Cleanup\n",
    "print(\"Cleaning up GPU memory...\")\n",
    "mem_before = torch.cuda.memory_allocated() / 1e9\n",
    "del model, processor, tokenizer\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "mem_after = torch.cuda.memory_allocated() / 1e9\n",
    "print(f\"GPU memory: {mem_before:.2f} GB -> {mem_after:.2f} GB\")\n",
    "print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "031de8fc17c14083a8f5f5916414fe88": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "0907da87024e4db29e11207464053890": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_b8e257516cd440e6993b5f7979c619cb",
        "IPY_MODEL_d98557ebff0b47179224cea7c2afba41",
        "IPY_MODEL_c69b3c8f5e244fa88d9e3d687bc42d24"
       ],
       "layout": "IPY_MODEL_a157937599fa47a6952dac433b4abe4a",
       "tabbable": null,
       "tooltip": null
      }
     },
     "32a6516603f64f238f056ac89a6f035e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "339d6d7aad464de4805b5380ec40c9fa": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_9162df6a8cf44637acb853636b16937f",
       "max": 1327.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_af0cbde76a0042bc9769680a6b3fff70",
       "tabbable": null,
       "tooltip": null,
       "value": 1327.0
      }
     },
     "42038dbf583345d19a4e65c8f8d0ca36": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_32a6516603f64f238f056ac89a6f035e",
       "placeholder": "​",
       "style": "IPY_MODEL_440bd861c4b947faa7046d24b6b991b2",
       "tabbable": null,
       "tooltip": null,
       "value": " 1327/1327 [00:04&lt;00:00, 720.49it/s, Materializing param=model.encoder.vision_tower.vision_model.post_layernorm.weight]"
      }
     },
     "4216fc9470fa47329f1eda4a74bd2c66": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "440bd861c4b947faa7046d24b6b991b2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "73ffa0e3536b439cba21468b4e31acc3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "82dba9c5e303431c813bd705c170c12a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "837e510ce8b746bbbd415b2a7cb82232": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_8c09870753504d76aec9b88e17bc9593",
       "placeholder": "​",
       "style": "IPY_MODEL_4216fc9470fa47329f1eda4a74bd2c66",
       "tabbable": null,
       "tooltip": null,
       "value": "Loading weights: 100%"
      }
     },
     "88ab4803612344039e72c613436f6e9e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "8b1b000bc491444f830abe67ee78dd73": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "8c09870753504d76aec9b88e17bc9593": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "9162df6a8cf44637acb853636b16937f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a157937599fa47a6952dac433b4abe4a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ab1d97128dbe4cff9fa69fcd9232e761": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_837e510ce8b746bbbd415b2a7cb82232",
        "IPY_MODEL_339d6d7aad464de4805b5380ec40c9fa",
        "IPY_MODEL_42038dbf583345d19a4e65c8f8d0ca36"
       ],
       "layout": "IPY_MODEL_031de8fc17c14083a8f5f5916414fe88",
       "tabbable": null,
       "tooltip": null
      }
     },
     "af0cbde76a0042bc9769680a6b3fff70": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "b587bfabfb6048f69fad00e2b01de2bb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "b8e257516cd440e6993b5f7979c619cb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_82dba9c5e303431c813bd705c170c12a",
       "placeholder": "​",
       "style": "IPY_MODEL_b587bfabfb6048f69fad00e2b01de2bb",
       "tabbable": null,
       "tooltip": null,
       "value": "Queries: 100%"
      }
     },
     "c69b3c8f5e244fa88d9e3d687bc42d24": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_8b1b000bc491444f830abe67ee78dd73",
       "placeholder": "​",
       "style": "IPY_MODEL_88ab4803612344039e72c613436f6e9e",
       "tabbable": null,
       "tooltip": null,
       "value": " 400/400 [1:14:39&lt;00:00, 12.48s/it]"
      }
     },
     "d98557ebff0b47179224cea7c2afba41": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_e92162561c0540bea863c07a67d35aa1",
       "max": 400.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_73ffa0e3536b439cba21468b4e31acc3",
       "tabbable": null,
       "tooltip": null,
       "value": 400.0
      }
     },
     "e92162561c0540bea863c07a67d35aa1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
