{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 01: Truncation Test -- Disentangling the Benefit",
    "## Does the benefit come from improved document representations or the decoder reading query tokens?",
    "",
    "### Background",
    "Exp 33b showed that encoding [query + document] in T5Gemma's bidirectional encoder",
    "dramatically helps answer prediction (oracle d=+0.345, surr_doc captures 96%).",
    "But the decoder cross-attended to ALL encoder tokens, including query/surrogate tokens.",
    "",
    "### The key question",
    "Is the decoder just reading the query from the encoder output (trivial), or are the",
    "document representations genuinely improved by bidirectional co-encoding with the query?",
    "",
    "### Method: Masking",
    "Encode [prefix + document] with full bidirectional attention (encoder sees everything).",
    "Then MASK the prefix tokens from decoder cross-attention. The decoder can only cross-attend",
    "to document positions.",
    "",
    "This is safe because T5Gemma2's cross-attention keys have NO RoPE applied -- there are",
    "no positional embeddings to invalidate when masking.",
    "",
    "### Conditions (6 total)",
    "| Condition | Encoder input | Decoder cross-attends to | Tests |",
    "|-----------|---------------|--------------------------|-------|",
    "| bare | [document] | all (=document) | Baseline |",
    "| oracle_full | [query + doc] | all (query + doc) | Upper bound (=Exp 33b) |",
    "| oracle_trunc | [query + doc] | document only | Value contamination |",
    "| surr_para_full | [para + doc] | all (para + doc) | Surrogate upper bound |",
    "| surr_para_trunc | [para + doc] | document only | Surrogate value contamination |",
    "| surr_doc_full | [kw + doc] | all (kw + doc) | Doc-keyword upper bound |",
    "| surr_doc_trunc | [kw + doc] | document only | Doc-keyword value contamination |",
    "",
    "### Success criteria",
    "- oracle_full >> bare (replicates 33b, expected d~+0.35)",
    "- oracle_trunc > bare (document reps improved, the real prize)",
    "- oracle_trunc / oracle_full > 30% (significant fraction from doc reps, not just query reading)",
    ""
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 2: Setup\n",
    "import os\n",
    "os.umask(0o000)\n",
    "\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "import gc\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from scipy import stats\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "sys.path.insert(0, \".\")\n",
    "\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "RESULTS_DIR = Path(\"results/exp01\")\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CHECKPOINT_PATH = RESULTS_DIR / \"checkpoint.json\"\n",
    "\n",
    "N_SAMPLES = 200\n",
    "MODEL_NAME = \"google/t5gemma-2-4b-4b\"\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "HF_TOKEN = os.environ.get(\"HF_TOKEN\")\n",
    "\n",
    "print(f\"Exp 01: Truncation Test -- Disentangling the Benefit\")\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"N: {N_SAMPLES}\")\n",
    "print(f\"CUDA: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 3: Load model\n",
    "from transformers import AutoProcessor, AutoModelForSeq2SeqLM\n",
    "\n",
    "print(f\"Loading {MODEL_NAME}...\")\n",
    "processor = AutoProcessor.from_pretrained(MODEL_NAME, token=HF_TOKEN)\n",
    "tokenizer = processor.tokenizer\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    token=HF_TOKEN,\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "DEVICE = next(model.parameters()).device\n",
    "print(f\"Model loaded. dtype={next(model.parameters()).dtype}\")\n",
    "print(f\"GPU memory used: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 4: Scoring helpers with truncation support\n",
    "\n",
    "def score_nll(encoder_text, answer_text, prefix_token_count=0, truncate=False):\n",
    "    '''Score NLL of answer tokens with optional truncation.\n",
    "\n",
    "    Args:\n",
    "        encoder_text: Full text for encoder (e.g., \"[query]\\n[document]\")\n",
    "        answer_text: Answer text for decoder (NO query in decoder)\n",
    "        prefix_token_count: Number of prefix tokens (query/surrogate) to potentially mask\n",
    "        truncate: If True, mask prefix tokens from decoder cross-attention\n",
    "\n",
    "    When truncate=True:\n",
    "        - Encoder processes full [prefix + document] with bidirectional attention\n",
    "        - But decoder can only cross-attend to document positions\n",
    "        - Tests whether document representations are improved by co-encoding\n",
    "\n",
    "    When truncate=False:\n",
    "        - Decoder cross-attends to all encoder tokens (prefix + document)\n",
    "        - This is the Exp 33b setup\n",
    "    '''\n",
    "    # Tokenize encoder input\n",
    "    enc_ids = tokenizer(encoder_text, return_tensors=\"pt\",\n",
    "                        add_special_tokens=True, truncation=True,\n",
    "                        max_length=2048).input_ids.to(DEVICE)\n",
    "    total_enc_len = enc_ids.shape[1]\n",
    "\n",
    "    # Full mask for encoder (bidirectional, sees everything)\n",
    "    enc_mask = torch.ones(1, total_enc_len, device=DEVICE, dtype=torch.long)\n",
    "\n",
    "    # Run encoder with full bidirectional attention\n",
    "    with torch.no_grad():\n",
    "        encoder_outputs = model.get_encoder()(\n",
    "            input_ids=enc_ids, attention_mask=enc_mask\n",
    "        )\n",
    "\n",
    "    # Build cross-attention mask for decoder\n",
    "    if truncate and prefix_token_count > 0:\n",
    "        # Mask prefix tokens: decoder can only attend to document positions\n",
    "        cross_attn_mask = torch.ones(1, total_enc_len, device=DEVICE, dtype=torch.long)\n",
    "        cross_attn_mask[:, :prefix_token_count] = 0\n",
    "    else:\n",
    "        # Full cross-attention (decoder sees all encoder tokens)\n",
    "        cross_attn_mask = torch.ones(1, total_enc_len, device=DEVICE, dtype=torch.long)\n",
    "\n",
    "    # Tokenize answer for decoder\n",
    "    ans_ids = tokenizer(answer_text, return_tensors=\"pt\",\n",
    "                        add_special_tokens=False, truncation=True,\n",
    "                        max_length=256).input_ids.to(DEVICE)\n",
    "\n",
    "    if ans_ids.shape[1] == 0:\n",
    "        return 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            encoder_outputs=encoder_outputs,\n",
    "            attention_mask=cross_attn_mask,  # This controls decoder cross-attention\n",
    "            labels=ans_ids,\n",
    "        )\n",
    "\n",
    "    # Per-token NLL\n",
    "    logits = outputs.logits\n",
    "    log_probs = F.log_softmax(logits, dim=-1)\n",
    "    token_log_probs = log_probs[0].gather(1, ans_ids[0].unsqueeze(1)).squeeze(1)\n",
    "    mean_nll = -token_log_probs.mean().item()\n",
    "\n",
    "    del encoder_outputs, outputs, logits, log_probs\n",
    "    return mean_nll\n",
    "\n",
    "\n",
    "def count_prefix_tokens(prefix_text, document_text):\n",
    "    '''Count how many tokens the prefix occupies in the concatenated encoding.\n",
    "\n",
    "    Tokenizes \"[prefix]\\n[document]\" and \"[document]\" separately,\n",
    "    returns the difference in token count.\n",
    "    '''\n",
    "    full_text = prefix_text + \"\\n\" + document_text\n",
    "    full_ids = tokenizer(full_text, add_special_tokens=True).input_ids\n",
    "    doc_ids = tokenizer(document_text, add_special_tokens=True).input_ids\n",
    "    return len(full_ids) - len(doc_ids)\n",
    "\n",
    "\n",
    "# === Surrogate generation ===\n",
    "STOP_WORDS = {\n",
    "    'a', 'an', 'the', 'is', 'are', 'was', 'were', 'be', 'been', 'being',\n",
    "    'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could',\n",
    "    'should', 'may', 'might', 'can', 'shall', 'to', 'of', 'in', 'for',\n",
    "    'on', 'with', 'at', 'by', 'from', 'as', 'into', 'through', 'during',\n",
    "    'before', 'after', 'above', 'below', 'between', 'and', 'but', 'or',\n",
    "    'not', 'no', 'if', 'then', 'than', 'so', 'up', 'out', 'about',\n",
    "    'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those',\n",
    "    'it', 'its', 'i', 'me', 'my', 'we', 'our', 'you', 'your', 'he',\n",
    "    'him', 'his', 'she', 'her', 'they', 'them', 'their', 'how', 'when',\n",
    "    'where', 'why', 'much', 'many', 'some', 'any', 'all', 'each',\n",
    "    'does', 'also', 'just', 'more', 'most', 'very', 'too', 'only',\n",
    "}\n",
    "\n",
    "def extract_keywords(text):\n",
    "    words = re.sub(r'[^\\w\\s]', '', text.lower()).split()\n",
    "    return [w for w in words if w not in STOP_WORDS and len(w) > 2]\n",
    "\n",
    "def make_surrogate_paraphrase(query):\n",
    "    keywords = extract_keywords(query)\n",
    "    return \" \".join(keywords[::-1]) if keywords else query\n",
    "\n",
    "def make_surrogate_from_doc(passage):\n",
    "    content_words = extract_keywords(passage)\n",
    "    if not content_words:\n",
    "        return \"information\"\n",
    "    counts = Counter(content_words)\n",
    "    return \" \".join(w for w, _ in counts.most_common(5))\n",
    "\n",
    "print(\"Helpers defined.\")\n",
    "print(\"  score_nll(encoder_text, answer, prefix_token_count, truncate)\")\n",
    "print(\"  Key: truncate=True masks prefix from decoder cross-attention\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 5: Load data\n",
    "from lib.data import count_words\n",
    "from datasets import load_dataset\n",
    "\n",
    "print(\"Loading MS MARCO v1.1 validation...\")\n",
    "ds = load_dataset(\"microsoft/ms_marco\", \"v1.1\", split=\"validation\")\n",
    "\n",
    "samples = []\n",
    "for item in ds:\n",
    "    if len(samples) >= N_SAMPLES * 3:\n",
    "        break\n",
    "    passages = item.get('passages', {})\n",
    "    ptexts = passages.get('passage_text', [])\n",
    "    is_sel = passages.get('is_selected', [])\n",
    "    query = item.get('query', '')\n",
    "    answers = item.get('answers', [])\n",
    "    well_formed = item.get('wellFormedAnswers', [])\n",
    "\n",
    "    answer = None\n",
    "    if well_formed and len(well_formed) > 0 and well_formed[0] not in ('[]', ''):\n",
    "        answer = well_formed[0]\n",
    "    elif answers and len(answers) > 0 and answers[0] != 'No Answer Present.':\n",
    "        answer = answers[0]\n",
    "    if not answer:\n",
    "        continue\n",
    "\n",
    "    for pt, sel in zip(ptexts, is_sel):\n",
    "        wc = count_words(pt)\n",
    "        if sel == 1 and 30 <= wc <= 300:\n",
    "            samples.append({\n",
    "                'passage': pt, 'query': query, 'answer': answer,\n",
    "                'word_count': wc\n",
    "            })\n",
    "            break\n",
    "\n",
    "np.random.seed(SEED)\n",
    "np.random.shuffle(samples)\n",
    "samples = samples[:N_SAMPLES]\n",
    "del ds\n",
    "gc.collect()\n",
    "\n",
    "for s in samples:\n",
    "    s['surrogate_para'] = make_surrogate_paraphrase(s['query'])\n",
    "    s['surrogate_doc_kw'] = make_surrogate_from_doc(s['passage'])\n",
    "\n",
    "print(f\"Selected {len(samples)} samples, mean words={np.mean([s['word_count'] for s in samples]):.0f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 6: Explain conditions\n",
    "print(\"=\" * 70)\n",
    "print(\"EXPERIMENTAL CONDITIONS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "ex = samples[0]\n",
    "print(f\"\\nExample query:  {ex['query'][:70]}\")\n",
    "print(f\"Example answer: {ex['answer'][:70]}\")\n",
    "\n",
    "# Count prefix tokens for the example\n",
    "oracle_prefix_tokens = count_prefix_tokens(ex['query'], ex['passage'])\n",
    "para_prefix_tokens = count_prefix_tokens(ex['surrogate_para'], ex['passage'])\n",
    "doc_prefix_tokens = count_prefix_tokens(ex['surrogate_doc_kw'], ex['passage'])\n",
    "\n",
    "# Show actual prefix text for each condition\n",
    "surrogates = {\n",
    "    'oracle (real query)': ex['query'],\n",
    "    'surr_para (reversed kw)': ex['surrogate_para'],\n",
    "    'surr_doc (top-5 doc kw)': ex['surrogate_doc_kw'],\n",
    "}\n",
    "print(f\"\\nExample passage: {ex['passage'][:80]}...\")\n",
    "print(f\"\\nActual prefix text for each condition:\")\n",
    "for name, text in surrogates.items():\n",
    "    ptoks = count_prefix_tokens(text, ex['passage'])\n",
    "    print(f\"  {name:<25} ({ptoks:>3} prefix toks): {text[:70]}\")\n",
    "\n",
    "conditions_explained = f'''\n",
    "CONDITION         ENCODER INPUT              DECODER CROSS-ATTENDS TO    PREFIX TOKENS\n",
    "-------------------------------------------------------------------------------------\n",
    "bare              [document]                 all (= document)            0\n",
    "oracle_full       [query + doc]              all (query + doc)           ~{oracle_prefix_tokens}\n",
    "oracle_trunc      [query + doc]              document ONLY               ~{oracle_prefix_tokens} (masked)\n",
    "surr_para_full    [paraphrase + doc]         all (paraphrase + doc)      ~{para_prefix_tokens}\n",
    "surr_para_trunc   [paraphrase + doc]         document ONLY               ~{para_prefix_tokens} (masked)\n",
    "surr_doc_full     [doc_keywords + doc]       all (keywords + doc)        ~{doc_prefix_tokens}\n",
    "surr_doc_trunc    [doc_keywords + doc]       document ONLY               ~{doc_prefix_tokens} (masked)\n",
    "\n",
    "KEY INSIGHT:\n",
    "  _full conditions:  decoder reads prefix + gets improved doc reps\n",
    "  _trunc conditions: decoder gets improved doc reps ONLY (prefix hidden)\n",
    "  If _trunc \u2248 _full:  benefit is from improved document representations\n",
    "  If _trunc \u2248 bare:   benefit was just the decoder reading the prefix\n",
    "'''\n",
    "print(conditions_explained)\n",
    "\n",
    "# Verify masking by checking token counts\n",
    "print(\"--- Token count verification ---\")\n",
    "for i in range(3):\n",
    "    s = samples[i]\n",
    "    full_text = s['query'] + \"\\n\" + s['passage']\n",
    "    doc_text = s['passage']\n",
    "    full_ids = tokenizer(full_text, add_special_tokens=True).input_ids\n",
    "    doc_ids = tokenizer(doc_text, add_special_tokens=True).input_ids\n",
    "    prefix_toks = len(full_ids) - len(doc_ids)\n",
    "    print(f\"  Sample {i}: query='{s['query'][:40]}...' prefix_tokens={prefix_toks}, doc_tokens={len(doc_ids)}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 7: Run scoring\n",
    "print(\"=\" * 70)\n",
    "print(\"RUNNING EXPERIMENT\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Define all conditions\n",
    "def make_conditions(sample):\n",
    "    '''Return dict of {name: (encoder_text, prefix_token_count, truncate)}'''\n",
    "    query = sample['query']\n",
    "    passage = sample['passage']\n",
    "    para = sample['surrogate_para']\n",
    "    doc_kw = sample['surrogate_doc_kw']\n",
    "\n",
    "    # Count prefix tokens for each condition\n",
    "    oracle_prefix = count_prefix_tokens(query, passage)\n",
    "    para_prefix = count_prefix_tokens(para, passage)\n",
    "    doc_prefix = count_prefix_tokens(doc_kw, passage)\n",
    "\n",
    "    return {\n",
    "        'bare':           (passage,                          0,              False),\n",
    "        'oracle_full':    (query + \"\\n\" + passage,           0,              False),\n",
    "        'oracle_trunc':   (query + \"\\n\" + passage,           oracle_prefix,  True),\n",
    "        'surr_para_full': (para + \"\\n\" + passage,            0,              False),\n",
    "        'surr_para_trunc':(para + \"\\n\" + passage,            para_prefix,    True),\n",
    "        'surr_doc_full':  (doc_kw + \"\\n\" + passage,          0,              False),\n",
    "        'surr_doc_trunc': (doc_kw + \"\\n\" + passage,          doc_prefix,     True),\n",
    "    }\n",
    "\n",
    "cond_names = ['bare', 'oracle_full', 'oracle_trunc',\n",
    "              'surr_para_full', 'surr_para_trunc',\n",
    "              'surr_doc_full', 'surr_doc_trunc']\n",
    "\n",
    "# Resume from checkpoint\n",
    "all_results = []\n",
    "start_idx = 0\n",
    "\n",
    "if CHECKPOINT_PATH.exists():\n",
    "    ckpt = json.loads(CHECKPOINT_PATH.read_text())\n",
    "    if ckpt.get('n_total') == N_SAMPLES and len(ckpt.get('results', [])) > 0:\n",
    "        saved_queries = [r['query'][:50] for r in ckpt['results']]\n",
    "        current_queries = [s['query'][:50] for s in samples[:len(saved_queries)]]\n",
    "        if saved_queries == current_queries:\n",
    "            all_results = ckpt['results']\n",
    "            start_idx = len(all_results)\n",
    "            print(f\"Resuming: {start_idx}/{N_SAMPLES}\")\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "for i in tqdm(range(start_idx, N_SAMPLES), initial=start_idx, total=N_SAMPLES, desc=\"Scoring\"):\n",
    "    s = samples[i]\n",
    "    conditions = make_conditions(s)\n",
    "\n",
    "    result = {\n",
    "        'query': s['query'], 'answer': s['answer'],\n",
    "        'passage_words': s['word_count'],\n",
    "    }\n",
    "\n",
    "    for cond_name in cond_names:\n",
    "        enc_text, prefix_count, trunc = conditions[cond_name]\n",
    "        nll = score_nll(enc_text, s['answer'], prefix_count, trunc)\n",
    "        result[f'nll_{cond_name}'] = nll\n",
    "\n",
    "    all_results.append(result)\n",
    "\n",
    "    if (i + 1) % 20 == 0 or i == N_SAMPLES - 1:\n",
    "        ckpt = {\n",
    "            'n_total': N_SAMPLES, 'results': all_results,\n",
    "            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        }\n",
    "        CHECKPOINT_PATH.write_text(json.dumps(ckpt))\n",
    "        elapsed = time.time() - t0\n",
    "        done = i - start_idx + 1\n",
    "        eta = (N_SAMPLES - i - 1) * elapsed / done if done > 0 else 0\n",
    "        tqdm.write(f\"  Checkpoint {i+1}/{N_SAMPLES} | {elapsed/60:.1f}m | ETA {eta/60:.1f}m\")\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "elapsed = time.time() - t0\n",
    "print(f\"\\nScoring complete: {len(all_results)} samples in {elapsed/60:.1f} min\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 8: Results\n",
    "from lib.analysis import cohens_d\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"RESULTS (N={len(all_results)})\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "bare_nlls = np.array([r['nll_bare'] for r in all_results])\n",
    "\n",
    "print(f\"\\n{'Condition':<20} {'Mean NLL':>10} {'vs Bare':>10} {'d':>8} {'Win%':>8} {'p':>12} {'sig':>5}\")\n",
    "print(\"-\" * 78)\n",
    "\n",
    "analysis = {}\n",
    "for cond in cond_names:\n",
    "    nlls = np.array([r[f'nll_{cond}'] for r in all_results])\n",
    "    mean_nll = nlls.mean()\n",
    "    diff = bare_nlls - nlls\n",
    "    d = cohens_d(diff)\n",
    "    win_pct = 100 * np.mean(diff > 0)\n",
    "\n",
    "    if cond == 'bare':\n",
    "        print(f\"{cond:<20} {mean_nll:>10.4f} {'--':>10} {'--':>8} {'--':>8} {'--':>12} {'--':>5}\")\n",
    "        analysis[cond] = {'mean_nll': float(mean_nll)}\n",
    "    else:\n",
    "        t_stat, p_val = stats.ttest_1samp(diff, 0)\n",
    "        sig = '***' if p_val < 0.001 else '**' if p_val < 0.01 else '*' if p_val < 0.05 else 'ns'\n",
    "        print(f\"{cond:<20} {mean_nll:>10.4f} {diff.mean():>+10.4f} {d:>+8.3f} {win_pct:>7.1f}% {p_val:>12.2e} {sig:>5}\")\n",
    "        analysis[cond] = {\n",
    "            'mean_nll': float(mean_nll), 'delta': float(diff.mean()),\n",
    "            'd': float(d), 'win_pct': float(win_pct), 'p': float(p_val),\n",
    "        }\n",
    "\n",
    "# ---- Full vs Truncated comparison ----\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"FULL vs TRUNCATED -- The Key Comparison\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for prefix_type in ['oracle', 'surr_para', 'surr_doc']:\n",
    "    full_nlls = np.array([r[f'nll_{prefix_type}_full'] for r in all_results])\n",
    "    trunc_nlls = np.array([r[f'nll_{prefix_type}_trunc'] for r in all_results])\n",
    "\n",
    "    full_gap = bare_nlls.mean() - full_nlls.mean()     # full vs bare\n",
    "    trunc_gap = bare_nlls.mean() - trunc_nlls.mean()   # trunc vs bare\n",
    "\n",
    "    if full_gap > 0:\n",
    "        retention = trunc_gap / full_gap * 100\n",
    "    else:\n",
    "        retention = float('nan')\n",
    "\n",
    "    # Direct full vs trunc comparison\n",
    "    diff_ft = full_nlls - trunc_nlls  # positive = trunc better\n",
    "    d_ft = cohens_d(diff_ft)\n",
    "    t_ft, p_ft = stats.ttest_1samp(diff_ft, 0) if np.std(diff_ft) > 0 else (0, 1)\n",
    "\n",
    "    print(f\"\\n  {prefix_type}:\")\n",
    "    print(f\"    full  vs bare: NLL gap = {full_gap:+.4f}\")\n",
    "    print(f\"    trunc vs bare: NLL gap = {trunc_gap:+.4f}\")\n",
    "    print(f\"    Retention: {retention:.0f}% of full benefit survives truncation\")\n",
    "    print(f\"    full vs trunc: d={d_ft:+.3f}, p={p_ft:.2e}\")\n",
    "    if retention > 50:\n",
    "        print(f\"    --> DOCUMENT REPRESENTATIONS carry majority of the benefit\")\n",
    "    elif retention > 20:\n",
    "        print(f\"    --> MIXED: both doc reps and direct query reading contribute\")\n",
    "    else:\n",
    "        print(f\"    --> DECODER QUERY READING is the primary mechanism\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 9: Hardness gradient\n",
    "print(\"=\" * 70)\n",
    "print(\"HARDNESS GRADIENT\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "quintile_bounds = np.percentile(bare_nlls, [20, 40, 60, 80])\n",
    "quintiles = np.digitize(bare_nlls, quintile_bounds)\n",
    "\n",
    "# Show full vs trunc retention by hardness\n",
    "print(f\"\\n--- Oracle: full vs trunc by hardness ---\")\n",
    "print(f\"{'Quintile':<12} {'N':>4} {'bare':>10} {'orc_full':>10} {'orc_trunc':>10} {'full_gap':>10} {'trunc_gap':>10} {'retain%':>10}\")\n",
    "print(\"-\" * 78)\n",
    "\n",
    "for q in range(5):\n",
    "    mask = quintiles == q\n",
    "    n_q = mask.sum()\n",
    "    if n_q < 3:\n",
    "        continue\n",
    "    qlabel = ['Q1 easy', 'Q2', 'Q3', 'Q4', 'Q5 hard'][q]\n",
    "    b = bare_nlls[mask].mean()\n",
    "    of = np.array([all_results[j]['nll_oracle_full'] for j in range(len(all_results)) if mask[j]]).mean()\n",
    "    ot = np.array([all_results[j]['nll_oracle_trunc'] for j in range(len(all_results)) if mask[j]]).mean()\n",
    "    fg = b - of\n",
    "    tg = b - ot\n",
    "    ret = tg / fg * 100 if fg > 0 else float('nan')\n",
    "    print(f\"{qlabel:<12} {n_q:>4} {b:>10.4f} {of:>10.4f} {ot:>10.4f} {fg:>+10.4f} {tg:>+10.4f} {ret:>9.0f}%\")\n",
    "\n",
    "# Correlations\n",
    "oracle_full_delta = bare_nlls - np.array([r['nll_oracle_full'] for r in all_results])\n",
    "oracle_trunc_delta = bare_nlls - np.array([r['nll_oracle_trunc'] for r in all_results])\n",
    "r_ft, p_ft = stats.pearsonr(oracle_full_delta, oracle_trunc_delta)\n",
    "print(f\"\\nCorrelation(oracle_full benefit, oracle_trunc benefit): r={r_ft:.3f} (p={p_ft:.2e})\")\n",
    "print(\"  (high r = same samples helped by both, suggesting shared mechanism)\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 10: Verdict and save\n",
    "print(\"=\" * 70)\n",
    "print(\"VERDICT -- Exp 01: Truncation Test\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\nModel: {MODEL_NAME}\")\n",
    "print(f\"N: {len(all_results)} samples\")\n",
    "\n",
    "# Key numbers\n",
    "oracle_full_d = analysis.get('oracle_full', {}).get('d', 0)\n",
    "oracle_trunc_d = analysis.get('oracle_trunc', {}).get('d', 0)\n",
    "\n",
    "oracle_full_gap = bare_nlls.mean() - np.array([r['nll_oracle_full'] for r in all_results]).mean()\n",
    "oracle_trunc_gap = bare_nlls.mean() - np.array([r['nll_oracle_trunc'] for r in all_results]).mean()\n",
    "oracle_retention = oracle_trunc_gap / oracle_full_gap * 100 if oracle_full_gap > 0 else 0\n",
    "\n",
    "print(f\"\\n--- Oracle (the most important comparison) ---\")\n",
    "print(f\"  full  d = {oracle_full_d:+.3f} (decoder sees query + improved doc reps)\")\n",
    "print(f\"  trunc d = {oracle_trunc_d:+.3f} (decoder sees improved doc reps ONLY)\")\n",
    "print(f\"  Retention: {oracle_retention:.0f}%\")\n",
    "\n",
    "if oracle_trunc_d > 0.2:\n",
    "    print(f\"\\n  STRONG: Document representations carry substantial query-specific benefit.\")\n",
    "    print(f\"  Bidirectional co-encoding with the query genuinely improves document reps.\")\n",
    "    print(f\"  This is the REAL mechanism, not just the decoder reading the query.\")\n",
    "elif oracle_trunc_d > 0.05:\n",
    "    print(f\"\\n  MODERATE: Some benefit from improved doc reps, but decoder query reading\")\n",
    "    print(f\"  also contributes significantly.\")\n",
    "elif oracle_trunc_d > 0:\n",
    "    print(f\"\\n  WEAK: Most benefit comes from the decoder reading query tokens directly.\")\n",
    "    print(f\"  Document representations are only marginally improved by co-encoding.\")\n",
    "else:\n",
    "    print(f\"\\n  NONE: Truncation eliminates all benefit. The decoder was just reading\")\n",
    "    print(f\"  the query from encoder output. Document representations are NOT improved\")\n",
    "    print(f\"  by bidirectional co-encoding.\")\n",
    "\n",
    "# Surrogate transfer with truncation\n",
    "print(f\"\\n--- Surrogate transfer (truncated) ---\")\n",
    "for name in ['surr_para', 'surr_doc']:\n",
    "    full_d = analysis.get(f'{name}_full', {}).get('d', 0)\n",
    "    trunc_d = analysis.get(f'{name}_trunc', {}).get('d', 0)\n",
    "    print(f\"  {name}: full d={full_d:+.3f}, trunc d={trunc_d:+.3f}\")\n",
    "\n",
    "# Comparison to decoder-only\n",
    "print(f\"\\n--- Cross-architecture comparison ---\")\n",
    "print(f\"  Decoder-only (Gemma 3 4B):\")\n",
    "print(f\"    Oracle full-context:    d ~ +0.023 (ns)\")\n",
    "print(f\"    Oracle values-only:     d ~ +0.211 ***\")\n",
    "print(f\"  Encoder-decoder (T5Gemma 2 4B-4B):\")\n",
    "print(f\"    Oracle full:            d = {oracle_full_d:+.3f}\")\n",
    "print(f\"    Oracle trunc (this exp): d = {oracle_trunc_d:+.3f}\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "\n",
    "# Save\n",
    "final_results = {\n",
    "    'experiment': 'exp01_truncation_test',\n",
    "    'model': MODEL_NAME,\n",
    "    'n_samples': len(all_results),\n",
    "    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'analysis': analysis,\n",
    "    'oracle_retention_pct': float(oracle_retention),\n",
    "}\n",
    "\n",
    "with open(RESULTS_DIR / 'results.json', 'w') as f:\n",
    "    json.dump(final_results, f, indent=2)\n",
    "print(f\"\\nResults saved to {RESULTS_DIR / 'results.json'}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 11: Cleanup\n",
    "print(\"Cleaning up GPU memory...\")\n",
    "mem_before = torch.cuda.memory_allocated() / 1e9\n",
    "del model, processor, tokenizer\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "mem_after = torch.cuda.memory_allocated() / 1e9\n",
    "print(f\"GPU memory: {mem_before:.2f} GB -> {mem_after:.2f} GB\")\n",
    "print(\"Done!\")"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}