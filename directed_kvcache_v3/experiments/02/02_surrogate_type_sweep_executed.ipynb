{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06777cd0",
   "metadata": {},
   "source": [
    "# Experiment 02: Surrogate Type Sweep## Which surrogate works best for bidirectional co-encoding?### BackgroundExp 01 proved that document representations are genuinely improved by co-encodingwith a query/surrogate (truncation made the benefit STRONGER: d=+0.408 trunc vs+0.345 full). But we only tested 2 surrogate types.v2 showed surprising results on decoder-only models:- Static \"What are the key facts?\" beat LLM-generated surrogates 2x (Exp 07)- No semantic content gradient detected (Exp 10, Spearman r=+0.036)- Content barely mattered -- the mechanism was structural (value contamination)T5Gemma's bidirectional encoder should be fundamentally different. If the encodercreates genuine query-document interactions, then content-specific surrogatesshould outperform content-agnostic ones.### Conditions (9 total, all truncated)| # | Condition | Source | Semantic relevance ||---|-----------|--------|--------------------|| 1 | bare | -- | Lower bound || 2 | oracle_trunc | Real query | Upper bound (100%) || 3 | surr_doc_trunc | Top-5 TF keywords from document | Doc-specific || 4 | surr_para_trunc | Query keywords reversed | Query-specific || 5 | static_fact_trunc | \"What are the key facts?\" | Content-agnostic || 6 | static_howto_trunc | \"How do I do this?\" | Content-agnostic || 7 | random_trunc | Passage from unrelated sample | Structural control || 8 | surr_lead_trunc | First sentence of document | Doc-specific (rich) || 9 | surr_template_trunc | \"What is [top_keyword]?\" | Doc-specific (minimal) |### Success criteria- oracle_trunc replicates Exp 01 (d ~ +0.41)- Content gradient: oracle > para > doc_kw > lead > template > static > random- If gradient exists: confirms bidirectional mechanism is semantic- If no gradient: mechanism is structural (like v2), just stronger in encoder-decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6c0af3a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-17T21:30:24.520181Z",
     "iopub.status.busy": "2026-02-17T21:30:24.519718Z",
     "iopub.status.idle": "2026-02-17T21:30:29.101219Z",
     "shell.execute_reply": "2026-02-17T21:30:29.100300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exp 02: Surrogate Type Sweep\n",
      "Model: google/t5gemma-2-4b-4b\n",
      "N: 500\n",
      "CUDA: NVIDIA A100-SXM4-40GB\n",
      "GPU memory: 42.3 GB\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Setup\n",
    "import os\n",
    "os.umask(0o000)\n",
    "\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "import gc\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from scipy import stats\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "sys.path.insert(0, \"../..\")\n",
    "\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "RESULTS_DIR = Path(\"../../results/exp02\")\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CHECKPOINT_PATH = RESULTS_DIR / \"checkpoint.json\"\n",
    "\n",
    "N_SAMPLES = 500\n",
    "MODEL_NAME = \"google/t5gemma-2-4b-4b\"\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv())\n",
    "HF_TOKEN = os.environ.get(\"HF_TOKEN\")\n",
    "\n",
    "print(f\"Exp 02: Surrogate Type Sweep\")\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"N: {N_SAMPLES}\")\n",
    "print(f\"CUDA: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28172523",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-17T21:30:29.104764Z",
     "iopub.status.busy": "2026-02-17T21:30:29.104301Z",
     "iopub.status.idle": "2026-02-17T21:30:46.279313Z",
     "shell.execute_reply": "2026-02-17T21:30:46.278588Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading google/t5gemma-2-4b-4b...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cd885291ee8470f989e012b11e9156c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/1327 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded. dtype=torch.bfloat16\n",
      "GPU memory used: 15.02 GB\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Load model\n",
    "from transformers import AutoProcessor, AutoModelForSeq2SeqLM\n",
    "\n",
    "print(f\"Loading {MODEL_NAME}...\")\n",
    "processor = AutoProcessor.from_pretrained(MODEL_NAME, token=HF_TOKEN)\n",
    "tokenizer = processor.tokenizer\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    token=HF_TOKEN,\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "DEVICE = next(model.parameters()).device\n",
    "print(f\"Model loaded. dtype={next(model.parameters()).dtype}\")\n",
    "print(f\"GPU memory used: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "801271ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-17T21:30:46.282715Z",
     "iopub.status.busy": "2026-02-17T21:30:46.282196Z",
     "iopub.status.idle": "2026-02-17T21:30:46.299666Z",
     "shell.execute_reply": "2026-02-17T21:30:46.299040Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helpers defined. Surrogate types:\n",
      "  1. oracle      - real query (upper bound)\n",
      "  2. surr_doc    - top-5 TF keywords from document\n",
      "  3. surr_para   - reversed query keywords\n",
      "  4. static_fact - 'What are the key facts I need to know?'\n",
      "  5. static_howto- 'How do I do this?'\n",
      "  6. random      - passage from unrelated sample\n",
      "  7. surr_lead   - first sentence of document\n",
      "  8. surr_template - 'What is [top_keyword]?'\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Scoring and surrogate helpers\n",
    "\n",
    "def score_nll(encoder_text, answer_text, prefix_token_count=0, truncate=False):\n",
    "    # Score NLL of answer tokens with optional truncation.\n",
    "    # When truncate=True: encoder processes full input bidirectionally,\n",
    "    # but decoder can only cross-attend to document positions (prefix masked).\n",
    "    enc_ids = tokenizer(encoder_text, return_tensors=\"pt\",\n",
    "                        add_special_tokens=True, truncation=True,\n",
    "                        max_length=2048).input_ids.to(DEVICE)\n",
    "    total_enc_len = enc_ids.shape[1]\n",
    "\n",
    "    enc_mask = torch.ones(1, total_enc_len, device=DEVICE, dtype=torch.long)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        encoder_outputs = model.get_encoder()(\n",
    "            input_ids=enc_ids, attention_mask=enc_mask\n",
    "        )\n",
    "\n",
    "    if truncate and prefix_token_count > 0:\n",
    "        cross_attn_mask = torch.ones(1, total_enc_len, device=DEVICE, dtype=torch.long)\n",
    "        cross_attn_mask[:, :prefix_token_count] = 0\n",
    "    else:\n",
    "        cross_attn_mask = torch.ones(1, total_enc_len, device=DEVICE, dtype=torch.long)\n",
    "\n",
    "    ans_ids = tokenizer(answer_text, return_tensors=\"pt\",\n",
    "                        add_special_tokens=False, truncation=True,\n",
    "                        max_length=256).input_ids.to(DEVICE)\n",
    "\n",
    "    if ans_ids.shape[1] == 0:\n",
    "        return 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            encoder_outputs=encoder_outputs,\n",
    "            attention_mask=cross_attn_mask,\n",
    "            labels=ans_ids,\n",
    "        )\n",
    "\n",
    "    logits = outputs.logits\n",
    "    log_probs = F.log_softmax(logits, dim=-1)\n",
    "    token_log_probs = log_probs[0].gather(1, ans_ids[0].unsqueeze(1)).squeeze(1)\n",
    "    mean_nll = -token_log_probs.mean().item()\n",
    "\n",
    "    del encoder_outputs, outputs, logits, log_probs\n",
    "    return mean_nll\n",
    "\n",
    "\n",
    "def count_prefix_tokens(prefix_text, document_text):\n",
    "    # Count how many tokens the prefix occupies in the concatenated encoding.\n",
    "    full_text = prefix_text + \"\\n\" + document_text\n",
    "    full_ids = tokenizer(full_text, add_special_tokens=True).input_ids\n",
    "    doc_ids = tokenizer(document_text, add_special_tokens=True).input_ids\n",
    "    return len(full_ids) - len(doc_ids)\n",
    "\n",
    "\n",
    "# === Surrogate generators ===\n",
    "STOP_WORDS = {\n",
    "    'a', 'an', 'the', 'is', 'are', 'was', 'were', 'be', 'been', 'being',\n",
    "    'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could',\n",
    "    'should', 'may', 'might', 'can', 'shall', 'to', 'of', 'in', 'for',\n",
    "    'on', 'with', 'at', 'by', 'from', 'as', 'into', 'through', 'during',\n",
    "    'before', 'after', 'above', 'below', 'between', 'and', 'but', 'or',\n",
    "    'not', 'no', 'if', 'then', 'than', 'so', 'up', 'out', 'about',\n",
    "    'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those',\n",
    "    'it', 'its', 'i', 'me', 'my', 'we', 'our', 'you', 'your', 'he',\n",
    "    'him', 'his', 'she', 'her', 'they', 'them', 'their', 'how', 'when',\n",
    "    'where', 'why', 'much', 'many', 'some', 'any', 'all', 'each',\n",
    "    'does', 'also', 'just', 'more', 'most', 'very', 'too', 'only',\n",
    "}\n",
    "\n",
    "def extract_keywords(text):\n",
    "    words = re.sub(r'[^\\w\\s]', '', text.lower()).split()\n",
    "    return [w for w in words if w not in STOP_WORDS and len(w) > 2]\n",
    "\n",
    "\n",
    "def make_surrogate_paraphrase(query):\n",
    "    # Reversed query keywords (paraphrase proxy)\n",
    "    keywords = extract_keywords(query)\n",
    "    return \" \".join(keywords[::-1]) if keywords else query\n",
    "\n",
    "\n",
    "def make_surrogate_doc_kw(passage):\n",
    "    # Top-5 TF keywords from the document\n",
    "    content_words = extract_keywords(passage)\n",
    "    if not content_words:\n",
    "        return \"information\"\n",
    "    counts = Counter(content_words)\n",
    "    return \" \".join(w for w, _ in counts.most_common(5))\n",
    "\n",
    "\n",
    "def make_surrogate_lead(passage):\n",
    "    # First sentence of the document\n",
    "    # Split on sentence boundaries\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', passage.strip())\n",
    "    first = sentences[0] if sentences else passage[:100]\n",
    "    # Cap length to avoid very long surrogates\n",
    "    words = first.split()\n",
    "    if len(words) > 25:\n",
    "        first = \" \".join(words[:25])\n",
    "    return first\n",
    "\n",
    "\n",
    "def make_surrogate_template(passage):\n",
    "    # Auto-generated question template: 'What is [top_keyword]?'\n",
    "    content_words = extract_keywords(passage)\n",
    "    if not content_words:\n",
    "        return \"What is this about?\"\n",
    "    counts = Counter(content_words)\n",
    "    top_word = counts.most_common(1)[0][0]\n",
    "    return f\"What is {top_word}?\"\n",
    "\n",
    "\n",
    "STATIC_FACT = \"What are the key facts I need to know?\"\n",
    "STATIC_HOWTO = \"How do I do this?\"\n",
    "\n",
    "print(\"Helpers defined. Surrogate types:\")\n",
    "print(\"  1. oracle      - real query (upper bound)\")\n",
    "print(\"  2. surr_doc    - top-5 TF keywords from document\")\n",
    "print(\"  3. surr_para   - reversed query keywords\")\n",
    "print(\"  4. static_fact - 'What are the key facts I need to know?'\")\n",
    "print(\"  5. static_howto- 'How do I do this?'\")\n",
    "print(\"  6. random      - passage from unrelated sample\")\n",
    "print(\"  7. surr_lead   - first sentence of document\")\n",
    "print(\"  8. surr_template - 'What is [top_keyword]?'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ec587f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-17T21:30:46.302795Z",
     "iopub.status.busy": "2026-02-17T21:30:46.302299Z",
     "iopub.status.idle": "2026-02-17T21:30:47.915428Z",
     "shell.execute_reply": "2026-02-17T21:30:47.914663Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MS MARCO v1.1 validation...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 500 samples, mean words=74\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Load data\n",
    "from lib.data import count_words\n",
    "from datasets import load_dataset\n",
    "\n",
    "print(\"Loading MS MARCO v1.1 validation...\")\n",
    "ds = load_dataset(\"microsoft/ms_marco\", \"v1.1\", split=\"validation\")\n",
    "\n",
    "samples = []\n",
    "for item in ds:\n",
    "    if len(samples) >= N_SAMPLES * 3:\n",
    "        break\n",
    "    passages = item.get('passages', {})\n",
    "    ptexts = passages.get('passage_text', [])\n",
    "    is_sel = passages.get('is_selected', [])\n",
    "    query = item.get('query', '')\n",
    "    answers = item.get('answers', [])\n",
    "    well_formed = item.get('wellFormedAnswers', [])\n",
    "\n",
    "    answer = None\n",
    "    if well_formed and len(well_formed) > 0 and well_formed[0] not in ('[]', ''):\n",
    "        answer = well_formed[0]\n",
    "    elif answers and len(answers) > 0 and answers[0] != 'No Answer Present.':\n",
    "        answer = answers[0]\n",
    "    if not answer:\n",
    "        continue\n",
    "\n",
    "    for pt, sel in zip(ptexts, is_sel):\n",
    "        wc = count_words(pt)\n",
    "        if sel == 1 and 30 <= wc <= 300:\n",
    "            samples.append({\n",
    "                'passage': pt, 'query': query, 'answer': answer,\n",
    "                'word_count': wc\n",
    "            })\n",
    "            break\n",
    "\n",
    "np.random.seed(SEED)\n",
    "np.random.shuffle(samples)\n",
    "samples = samples[:N_SAMPLES]\n",
    "del ds\n",
    "gc.collect()\n",
    "\n",
    "# Pre-compute surrogates\n",
    "for i, s in enumerate(samples):\n",
    "    s['surr_para'] = make_surrogate_paraphrase(s['query'])\n",
    "    s['surr_doc_kw'] = make_surrogate_doc_kw(s['passage'])\n",
    "    s['surr_lead'] = make_surrogate_lead(s['passage'])\n",
    "    s['surr_template'] = make_surrogate_template(s['passage'])\n",
    "    # Random: use passage from a different sample (circular offset)\n",
    "    other_idx = (i + N_SAMPLES // 2) % len(samples)\n",
    "    other_passage = samples[other_idx]['passage']\n",
    "    # Use first ~20 words of the other passage as the random surrogate\n",
    "    s['surr_random'] = \" \".join(other_passage.split()[:20])\n",
    "\n",
    "print(f\"Selected {len(samples)} samples, mean words={np.mean([s['word_count'] for s in samples]):.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6285b57",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-17T21:30:47.919111Z",
     "iopub.status.busy": "2026-02-17T21:30:47.918303Z",
     "iopub.status.idle": "2026-02-17T21:30:47.934302Z",
     "shell.execute_reply": "2026-02-17T21:30:47.933660Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "EXPERIMENTAL CONDITIONS (all with truncation)\n",
      "======================================================================\n",
      "\n",
      "Example query:     what is the link between alveoli and capillaries\n",
      "Example answer:    Diffusion\n",
      "Example passage:   Gas exchange in the lungs takes place between the blood in the capillary network...\n",
      "\n",
      "Condition               Prefix tokens Surrogate text (first 60 chars)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "  oracle                           10 what is the link between alveoli and capillaries\n",
      "  surr_doc                          7 alveoli gas partial pressure exchange\n",
      "  surr_para                         7 capillaries alveoli link\n",
      "  static_fact                      11 What are the key facts I need to know?\n",
      "  static_howto                      7 How do I do this?\n",
      "  random                           33 You are here Donair History. Donairs-in the past-are traditi...\n",
      "  surr_lead                        29 Gas exchange in the lungs takes place between the blood in t...\n",
      "  surr_template                     6 What is alveoli?\n",
      "\n",
      "  bare                            0 (no surrogate -- lower bound)\n",
      "\n",
      "--- Semantic relevance ranking (for gradient test) ---\n",
      "  1=most relevant (oracle) ... 8=least relevant (random)\n",
      "  1. oracle_trunc\n",
      "  2. surr_para_trunc\n",
      "  3. surr_doc_trunc\n",
      "  4. surr_lead_trunc\n",
      "  5. surr_template_trunc\n",
      "  6. static_fact_trunc\n",
      "  7. static_howto_trunc\n",
      "  8. random_trunc\n",
      "\n",
      "--- Key question ---\n",
      "  v2 Mistral: NO content gradient (Spearman r=+0.036)\n",
      "  v3 T5Gemma: Does bidirectional attention create a genuine gradient?\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Explain conditions with concrete examples\n",
    "print(\"=\" * 70)\n",
    "print(\"EXPERIMENTAL CONDITIONS (all with truncation)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "ex = samples[0]\n",
    "print(f\"\\nExample query:     {ex['query'][:80]}\")\n",
    "print(f\"Example answer:    {ex['answer'][:80]}\")\n",
    "print(f\"Example passage:   {ex['passage'][:80]}...\")\n",
    "print()\n",
    "\n",
    "surrogates = {\n",
    "    'oracle':        ex['query'],\n",
    "    'surr_doc':      ex['surr_doc_kw'],\n",
    "    'surr_para':     ex['surr_para'],\n",
    "    'static_fact':   STATIC_FACT,\n",
    "    'static_howto':  STATIC_HOWTO,\n",
    "    'random':        ex['surr_random'],\n",
    "    'surr_lead':     ex['surr_lead'],\n",
    "    'surr_template': ex['surr_template'],\n",
    "}\n",
    "\n",
    "# Expected semantic relevance ranking (for gradient test)\n",
    "SEMANTIC_RANK = {\n",
    "    'oracle_trunc': 1,        # exact match\n",
    "    'surr_para_trunc': 2,     # query-derived\n",
    "    'surr_doc_trunc': 3,      # doc-specific keywords\n",
    "    'surr_lead_trunc': 4,     # doc-specific sentence\n",
    "    'surr_template_trunc': 5, # doc-specific minimal\n",
    "    'static_fact_trunc': 6,   # content-agnostic (best v2)\n",
    "    'static_howto_trunc': 7,  # content-agnostic\n",
    "    'random_trunc': 8,        # structural control\n",
    "}\n",
    "\n",
    "print(f\"{'Condition':<22} {'Prefix tokens':>14} {'Surrogate text (first 60 chars)'}\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "for name, surr_text in surrogates.items():\n",
    "    ptoks = count_prefix_tokens(surr_text, ex['passage'])\n",
    "    display = surr_text[:60] + ('...' if len(surr_text) > 60 else '')\n",
    "    print(f\"  {name:<20} {ptoks:>14} {display}\")\n",
    "\n",
    "print(f\"\\n  bare                            0 (no surrogate -- lower bound)\")\n",
    "\n",
    "print(f\"\\n--- Semantic relevance ranking (for gradient test) ---\")\n",
    "print(\"  1=most relevant (oracle) ... 8=least relevant (random)\")\n",
    "for name, rank in sorted(SEMANTIC_RANK.items(), key=lambda x: x[1]):\n",
    "    print(f\"  {rank}. {name}\")\n",
    "\n",
    "print(\"\\n--- Key question ---\")\n",
    "print(\"  v2 Mistral: NO content gradient (Spearman r=+0.036)\")\n",
    "print(\"  v3 T5Gemma: Does bidirectional attention create a genuine gradient?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a088465f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-17T21:30:47.937222Z",
     "iopub.status.busy": "2026-02-17T21:30:47.936945Z",
     "iopub.status.idle": "2026-02-17T21:46:57.183490Z",
     "shell.execute_reply": "2026-02-17T21:46:57.182528Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "RUNNING EXPERIMENT\n",
      "======================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22918dd24857411199be9d97230a32c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Scoring:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 20/500 | 0.6m | ETA 15.5m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 40/500 | 1.3m | ETA 14.9m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 60/500 | 1.9m | ETA 14.2m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 80/500 | 2.6m | ETA 13.6m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 100/500 | 3.2m | ETA 13.0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 120/500 | 3.9m | ETA 12.3m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 140/500 | 4.5m | ETA 11.6m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 160/500 | 5.2m | ETA 11.0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 180/500 | 5.8m | ETA 10.3m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 200/500 | 6.4m | ETA 9.6m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 220/500 | 7.1m | ETA 9.0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 240/500 | 7.7m | ETA 8.4m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 260/500 | 8.4m | ETA 7.7m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 280/500 | 9.0m | ETA 7.1m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 300/500 | 9.7m | ETA 6.5m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 320/500 | 10.3m | ETA 5.8m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 340/500 | 11.0m | ETA 5.2m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 360/500 | 11.6m | ETA 4.5m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 380/500 | 12.3m | ETA 3.9m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 400/500 | 12.9m | ETA 3.2m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 420/500 | 13.6m | ETA 2.6m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 440/500 | 14.2m | ETA 1.9m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 460/500 | 14.9m | ETA 1.3m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 480/500 | 15.5m | ETA 0.6m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 500/500 | 16.2m | ETA 0.0m\n",
      "\n",
      "Scoring complete: 500 samples in 16.2 min\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Run scoring\n",
    "print(\"=\" * 70)\n",
    "print(\"RUNNING EXPERIMENT\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "COND_NAMES = [\n",
    "    'bare',\n",
    "    'oracle_trunc',\n",
    "    'surr_doc_trunc',\n",
    "    'surr_para_trunc',\n",
    "    'static_fact_trunc',\n",
    "    'static_howto_trunc',\n",
    "    'random_trunc',\n",
    "    'surr_lead_trunc',\n",
    "    'surr_template_trunc',\n",
    "]\n",
    "\n",
    "\n",
    "def make_conditions(sample):\n",
    "    # Return dict of {name: (encoder_text, prefix_token_count, truncate)}\n",
    "    query = sample['query']\n",
    "    passage = sample['passage']\n",
    "\n",
    "    surr_map = {\n",
    "        'oracle':        query,\n",
    "        'surr_doc':      sample['surr_doc_kw'],\n",
    "        'surr_para':     sample['surr_para'],\n",
    "        'static_fact':   STATIC_FACT,\n",
    "        'static_howto':  STATIC_HOWTO,\n",
    "        'random':        sample['surr_random'],\n",
    "        'surr_lead':     sample['surr_lead'],\n",
    "        'surr_template': sample['surr_template'],\n",
    "    }\n",
    "\n",
    "    conditions = {\n",
    "        'bare': (passage, 0, False),\n",
    "    }\n",
    "\n",
    "    for surr_name, surr_text in surr_map.items():\n",
    "        cond_name = f'{surr_name}_trunc'\n",
    "        enc_text = surr_text + \"\\n\" + passage\n",
    "        prefix_count = count_prefix_tokens(surr_text, passage)\n",
    "        conditions[cond_name] = (enc_text, prefix_count, True)\n",
    "\n",
    "    return conditions\n",
    "\n",
    "\n",
    "# Resume from checkpoint\n",
    "all_results = []\n",
    "start_idx = 0\n",
    "\n",
    "if CHECKPOINT_PATH.exists():\n",
    "    ckpt = json.loads(CHECKPOINT_PATH.read_text())\n",
    "    if ckpt.get('n_total') == N_SAMPLES and len(ckpt.get('results', [])) > 0:\n",
    "        saved_queries = [r['query'][:50] for r in ckpt['results']]\n",
    "        current_queries = [s['query'][:50] for s in samples[:len(saved_queries)]]\n",
    "        if saved_queries == current_queries:\n",
    "            all_results = ckpt['results']\n",
    "            start_idx = len(all_results)\n",
    "            print(f\"Resuming: {start_idx}/{N_SAMPLES}\")\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "for i in tqdm(range(start_idx, N_SAMPLES), initial=start_idx, total=N_SAMPLES, desc=\"Scoring\"):\n",
    "    s = samples[i]\n",
    "    conditions = make_conditions(s)\n",
    "\n",
    "    result = {\n",
    "        'query': s['query'], 'answer': s['answer'],\n",
    "        'passage_words': s['word_count'],\n",
    "    }\n",
    "\n",
    "    for cond_name in COND_NAMES:\n",
    "        enc_text, prefix_count, trunc = conditions[cond_name]\n",
    "        nll = score_nll(enc_text, s['answer'], prefix_count, trunc)\n",
    "        result[f'nll_{cond_name}'] = nll\n",
    "\n",
    "    all_results.append(result)\n",
    "\n",
    "    if (i + 1) % 20 == 0 or i == N_SAMPLES - 1:\n",
    "        ckpt = {\n",
    "            'n_total': N_SAMPLES, 'results': all_results,\n",
    "            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        }\n",
    "        CHECKPOINT_PATH.write_text(json.dumps(ckpt))\n",
    "        elapsed = time.time() - t0\n",
    "        done = i - start_idx + 1\n",
    "        eta = (N_SAMPLES - i - 1) * elapsed / done if done > 0 else 0\n",
    "        tqdm.write(f\"  Checkpoint {i+1}/{N_SAMPLES} | {elapsed/60:.1f}m | ETA {eta/60:.1f}m\")\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "elapsed = time.time() - t0\n",
    "print(f\"\\nScoring complete: {len(all_results)} samples in {elapsed/60:.1f} min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97f8b4ba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-17T21:46:57.186827Z",
     "iopub.status.busy": "2026-02-17T21:46:57.186526Z",
     "iopub.status.idle": "2026-02-17T21:46:57.207505Z",
     "shell.execute_reply": "2026-02-17T21:46:57.206829Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "RESULTS (N=500)\n",
      "======================================================================\n",
      "\n",
      "Condition                   Mean NLL    vs Bare        d     Win%            p   sig   % oracle\n",
      "-----------------------------------------------------------------------------------------------\n",
      "bare                          3.6765         --       --       --           --    --         --\n",
      "oracle_trunc                  2.9929    +0.6836   +0.376    92.6%     4.79e-16   ***  100% (UB)\n",
      "surr_doc_trunc                3.0560    +0.6205   +0.322    87.4%     2.10e-12   ***        86%\n",
      "surr_para_trunc               3.0870    +0.5894   +0.305    89.2%     2.83e-11   ***        81%\n",
      "static_fact_trunc             3.2584    +0.4180   +0.372    83.8%     8.70e-16   ***        99%\n",
      "static_howto_trunc            3.2280    +0.4484   +0.346    85.6%     5.96e-14   ***        92%\n",
      "random_trunc                  3.1432    +0.5333   +0.303    87.6%     3.61e-11   ***        81%\n",
      "surr_lead_trunc               3.4672    +0.2093   +0.151    64.2%     7.94e-04   ***        40%\n",
      "surr_template_trunc           3.1173    +0.5592   +0.336    90.8%     2.63e-13   ***        90%\n",
      "\n",
      "Bonferroni threshold: p < 0.0063 (alpha=0.05, 8 tests)\n",
      "  oracle_trunc: p=4.79e-16 -- SIGNIFICANT after Bonferroni\n",
      "  surr_doc_trunc: p=2.10e-12 -- SIGNIFICANT after Bonferroni\n",
      "  surr_para_trunc: p=2.83e-11 -- SIGNIFICANT after Bonferroni\n",
      "  static_fact_trunc: p=8.70e-16 -- SIGNIFICANT after Bonferroni\n",
      "  static_howto_trunc: p=5.96e-14 -- SIGNIFICANT after Bonferroni\n",
      "  random_trunc: p=3.61e-11 -- SIGNIFICANT after Bonferroni\n",
      "  surr_lead_trunc: p=7.94e-04 -- SIGNIFICANT after Bonferroni\n",
      "  surr_template_trunc: p=2.63e-13 -- SIGNIFICANT after Bonferroni\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Results\n",
    "from lib.analysis import cohens_d\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"RESULTS (N={len(all_results)})\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "bare_nlls = np.array([r['nll_bare'] for r in all_results])\n",
    "\n",
    "print(f\"\\n{'Condition':<25} {'Mean NLL':>10} {'vs Bare':>10} {'d':>8} {'Win%':>8} {'p':>12} {'sig':>5} {'% oracle':>10}\")\n",
    "print(\"-\" * 95)\n",
    "\n",
    "analysis = {}\n",
    "oracle_d = None\n",
    "\n",
    "for cond in COND_NAMES:\n",
    "    nlls = np.array([r[f'nll_{cond}'] for r in all_results])\n",
    "    mean_nll = nlls.mean()\n",
    "    diff = bare_nlls - nlls\n",
    "    d = cohens_d(diff)\n",
    "    win_pct = 100 * np.mean(diff > 0)\n",
    "\n",
    "    if cond == 'bare':\n",
    "        print(f\"{cond:<25} {mean_nll:>10.4f} {'--':>10} {'--':>8} {'--':>8} {'--':>12} {'--':>5} {'--':>10}\")\n",
    "        analysis[cond] = {'mean_nll': float(mean_nll)}\n",
    "    else:\n",
    "        t_stat, p_val = stats.ttest_1samp(diff, 0)\n",
    "        sig = '***' if p_val < 0.001 else '**' if p_val < 0.01 else '*' if p_val < 0.05 else 'ns'\n",
    "\n",
    "        if cond == 'oracle_trunc':\n",
    "            oracle_d = d\n",
    "            pct_oracle = '100% (UB)'\n",
    "        elif oracle_d and oracle_d > 0:\n",
    "            pct_oracle = f\"{d / oracle_d * 100:.0f}%\"\n",
    "        else:\n",
    "            pct_oracle = '--'\n",
    "\n",
    "        print(f\"{cond:<25} {mean_nll:>10.4f} {diff.mean():>+10.4f} {d:>+8.3f} {win_pct:>7.1f}% {p_val:>12.2e} {sig:>5} {pct_oracle:>10}\")\n",
    "        analysis[cond] = {\n",
    "            'mean_nll': float(mean_nll), 'delta': float(diff.mean()),\n",
    "            'd': float(d), 'win_pct': float(win_pct), 'p': float(p_val),\n",
    "        }\n",
    "\n",
    "# Bonferroni correction\n",
    "n_tests = len(COND_NAMES) - 1  # exclude bare\n",
    "bonferroni_threshold = 0.05 / n_tests\n",
    "print(f\"\\nBonferroni threshold: p < {bonferroni_threshold:.4f} (alpha=0.05, {n_tests} tests)\")\n",
    "for cond, a in analysis.items():\n",
    "    if cond != 'bare' and 'p' in a:\n",
    "        bf_sig = a['p'] < bonferroni_threshold\n",
    "        if bf_sig:\n",
    "            print(f\"  {cond}: p={a['p']:.2e} -- SIGNIFICANT after Bonferroni\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "341b37cf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-17T21:46:57.210712Z",
     "iopub.status.busy": "2026-02-17T21:46:57.210451Z",
     "iopub.status.idle": "2026-02-17T21:46:57.239252Z",
     "shell.execute_reply": "2026-02-17T21:46:57.238570Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "CONTENT GRADIENT ANALYSIS\n",
      "======================================================================\n",
      "Does more relevant content produce better document representations?\n",
      "\n",
      "Semantic relevance rank vs Cohen's d:\n",
      "  Rank   Condition                        d\n",
      "  ------------------------------------------\n",
      "  1      oracle_trunc                +0.376\n",
      "  2      surr_para_trunc             +0.305\n",
      "  3      surr_doc_trunc              +0.322\n",
      "  4      surr_lead_trunc             +0.151\n",
      "  5      surr_template_trunc         +0.336\n",
      "  6      static_fact_trunc           +0.372\n",
      "  7      static_howto_trunc          +0.346\n",
      "  8      random_trunc                +0.303\n",
      "\n",
      "  Spearman rho = -0.167 (p=0.693)\n",
      "  Pearson r    = +0.013 (p=0.976)\n",
      "\n",
      "  NO CONTENT GRADIENT (like v2): Surrogate content does not\n",
      "  predict effect size. The mechanism may be structural rather\n",
      "  than semantic, even with bidirectional attention.\n",
      "\n",
      "  v2 Mistral (Exp 10): Spearman r=+0.036 (no gradient)\n",
      "  v3 T5Gemma (this):   Spearman r=-0.167\n",
      "\n",
      "======================================================================\n",
      "GROUP COMPARISON: Content-specific vs Content-agnostic\n",
      "======================================================================\n",
      "\n",
      "  Content-specific (oracle, para, doc_kw, lead, template):\n",
      "    Mean NLL = 3.1441\n",
      "  Content-agnostic (static_fact, static_howto, random):\n",
      "    Mean NLL = 3.2099\n",
      "\n",
      "  Difference: d=+0.121, win%=53.6%, p=7.25e-03\n",
      "  --> Content-SPECIFIC surrogates are significantly better.\n",
      "      Bidirectional co-encoding is genuinely semantic!\n",
      "\n",
      "======================================================================\n",
      "DOCUMENT-DERIVED vs QUERY-DERIVED (deployment question)\n",
      "======================================================================\n",
      "Can we get good results without any query information?\n",
      "\n",
      "  Document-derived surrogates (no query needed at build time):\n",
      "    surr_doc_trunc: d=+0.322\n",
      "    surr_lead_trunc: d=+0.151\n",
      "    surr_template_trunc: d=+0.336\n",
      "    Mean d: +0.270\n",
      "\n",
      "  Query-derived surrogates (needs query proxy at build time):\n",
      "    surr_para_trunc: d=+0.305\n",
      "\n",
      "  Best document-derived: surr_template_trunc (d=+0.336, 90% of oracle)\n",
      "  --> PRACTICAL:  doc-derived surrogates capture enough of the oracle gap for deployment\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Content gradient analysis\n",
    "print(\"=\" * 70)\n",
    "print(\"CONTENT GRADIENT ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "print(\"Does more relevant content produce better document representations?\")\n",
    "\n",
    "# Spearman rank correlation: semantic relevance rank vs effect size (d)\n",
    "ranks = []\n",
    "ds = []\n",
    "cond_labels = []\n",
    "\n",
    "for cond, rank in sorted(SEMANTIC_RANK.items(), key=lambda x: x[1]):\n",
    "    if cond in analysis and 'd' in analysis[cond]:\n",
    "        ranks.append(rank)\n",
    "        ds.append(analysis[cond]['d'])\n",
    "        cond_labels.append(cond)\n",
    "\n",
    "r_spearman, p_spearman = stats.spearmanr(ranks, ds)\n",
    "r_pearson, p_pearson = stats.pearsonr(ranks, ds)\n",
    "\n",
    "print(f\"\\nSemantic relevance rank vs Cohen's d:\")\n",
    "print(f\"  {'Rank':<6} {'Condition':<25} {'d':>8}\")\n",
    "print(f\"  {'-'*42}\")\n",
    "for rank, cond, d_val in zip(ranks, cond_labels, ds):\n",
    "    print(f\"  {rank:<6} {cond:<25} {d_val:>+8.3f}\")\n",
    "\n",
    "print(f\"\\n  Spearman rho = {r_spearman:+.3f} (p={p_spearman:.3f})\")\n",
    "print(f\"  Pearson r    = {r_pearson:+.3f} (p={p_pearson:.3f})\")\n",
    "\n",
    "if r_spearman < -0.5 and p_spearman < 0.05:\n",
    "    print(f\"\\n  STRONG CONTENT GRADIENT: More relevant surrogates produce\")\n",
    "    print(f\"  significantly better representations. The bidirectional mechanism\")\n",
    "    print(f\"  is genuinely semantic -- content matters.\")\n",
    "elif r_spearman < -0.3:\n",
    "    print(f\"\\n  MODERATE GRADIENT: Some evidence that content helps, but\")\n",
    "    print(f\"  not a clean monotonic relationship.\")\n",
    "else:\n",
    "    print(f\"\\n  NO CONTENT GRADIENT (like v2): Surrogate content does not\")\n",
    "    print(f\"  predict effect size. The mechanism may be structural rather\")\n",
    "    print(f\"  than semantic, even with bidirectional attention.\")\n",
    "\n",
    "# v2 comparison\n",
    "print(f\"\\n  v2 Mistral (Exp 10): Spearman r=+0.036 (no gradient)\")\n",
    "print(f\"  v3 T5Gemma (this):   Spearman r={r_spearman:+.3f}\")\n",
    "\n",
    "# --- Group comparison: content-specific vs content-agnostic ---\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"GROUP COMPARISON: Content-specific vs Content-agnostic\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "content_specific = ['oracle_trunc', 'surr_para_trunc', 'surr_doc_trunc',\n",
    "                    'surr_lead_trunc', 'surr_template_trunc']\n",
    "content_agnostic = ['static_fact_trunc', 'static_howto_trunc', 'random_trunc']\n",
    "\n",
    "# Per-sample comparison: average NLL across content-specific vs content-agnostic\n",
    "specific_nlls = []\n",
    "agnostic_nlls = []\n",
    "for r in all_results:\n",
    "    spec = np.mean([r[f'nll_{c}'] for c in content_specific])\n",
    "    agn = np.mean([r[f'nll_{c}'] for c in content_agnostic])\n",
    "    specific_nlls.append(spec)\n",
    "    agnostic_nlls.append(agn)\n",
    "\n",
    "specific_nlls = np.array(specific_nlls)\n",
    "agnostic_nlls = np.array(agnostic_nlls)\n",
    "diff_groups = agnostic_nlls - specific_nlls  # positive = specific better\n",
    "\n",
    "d_groups = cohens_d(diff_groups)\n",
    "win_groups = 100 * np.mean(diff_groups > 0)\n",
    "t_groups, p_groups = stats.ttest_1samp(diff_groups, 0)\n",
    "\n",
    "print(f\"\\n  Content-specific (oracle, para, doc_kw, lead, template):\")\n",
    "print(f\"    Mean NLL = {specific_nlls.mean():.4f}\")\n",
    "print(f\"  Content-agnostic (static_fact, static_howto, random):\")\n",
    "print(f\"    Mean NLL = {agnostic_nlls.mean():.4f}\")\n",
    "print(f\"\\n  Difference: d={d_groups:+.3f}, win%={win_groups:.1f}%, p={p_groups:.2e}\")\n",
    "if d_groups > 0.1 and p_groups < 0.05:\n",
    "    print(f\"  --> Content-SPECIFIC surrogates are significantly better.\")\n",
    "    print(f\"      Bidirectional co-encoding is genuinely semantic!\")\n",
    "elif d_groups > 0:\n",
    "    print(f\"  --> Content-specific slightly better but not significant.\")\n",
    "else:\n",
    "    print(f\"  --> Content-agnostic is as good or better (structural mechanism).\")\n",
    "\n",
    "# --- Document-derived vs query-derived ---\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"DOCUMENT-DERIVED vs QUERY-DERIVED (deployment question)\")\n",
    "print(\"=\" * 70)\n",
    "print(\"Can we get good results without any query information?\")\n",
    "\n",
    "doc_derived = ['surr_doc_trunc', 'surr_lead_trunc', 'surr_template_trunc']\n",
    "query_derived = ['surr_para_trunc']  # only para uses query info (oracle excluded -- it IS the query)\n",
    "\n",
    "doc_ds = [analysis[c]['d'] for c in doc_derived if c in analysis]\n",
    "query_ds = [analysis[c]['d'] for c in query_derived if c in analysis]\n",
    "\n",
    "print(f\"\\n  Document-derived surrogates (no query needed at build time):\")\n",
    "for c in doc_derived:\n",
    "    if c in analysis:\n",
    "        print(f\"    {c}: d={analysis[c]['d']:+.3f}\")\n",
    "print(f\"    Mean d: {np.mean(doc_ds):+.3f}\")\n",
    "\n",
    "print(f\"\\n  Query-derived surrogates (needs query proxy at build time):\")\n",
    "for c in query_derived:\n",
    "    if c in analysis:\n",
    "        print(f\"    {c}: d={analysis[c]['d']:+.3f}\")\n",
    "\n",
    "best_doc = max(doc_derived, key=lambda c: analysis.get(c, {}).get('d', -999))\n",
    "best_doc_d = analysis.get(best_doc, {}).get('d', 0)\n",
    "oracle_d_val = analysis.get('oracle_trunc', {}).get('d', 0)\n",
    "pct_of_oracle = best_doc_d / oracle_d_val * 100 if oracle_d_val > 0 else 0\n",
    "\n",
    "print(f\"\\n  Best document-derived: {best_doc} (d={best_doc_d:+.3f}, {pct_of_oracle:.0f}% of oracle)\")\n",
    "print(f\"  --> {'PRACTICAL: ' if pct_of_oracle > 70 else 'NEEDS QUERY INFO: '}\",\n",
    "      f\"{'doc-derived surrogates capture enough of the oracle gap for deployment' if pct_of_oracle > 70 else 'may need query-like surrogates for practical benefit'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8da4d94",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-17T21:46:57.242119Z",
     "iopub.status.busy": "2026-02-17T21:46:57.241856Z",
     "iopub.status.idle": "2026-02-17T21:46:57.264090Z",
     "shell.execute_reply": "2026-02-17T21:46:57.263428Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "HARDNESS STRATIFICATION\n",
      "======================================================================\n",
      "\n",
      "Quintile        N       oracle     surr_doc  static_fact       random\n",
      "------------------------------------------------------------\n",
      "Q1 easy       100       +1.027       +0.815       +0.939       +1.147\n",
      "Q2             99       +1.213       +0.955       +1.023       +1.058\n",
      "Q3             98       +1.518       +1.169       +1.128       +1.144\n",
      "Q4            103       +1.219       +1.037       +0.753       +0.892\n",
      "Q5 hard       100       +0.674       +0.596       +0.685       +0.561\n",
      "\n",
      "--- Hardness-benefit correlation (r with bare NLL) ---\n",
      "  oracle_trunc              r=+0.850 (p=9.35e-141)\n",
      "  surr_doc_trunc            r=+0.859 (p=2.98e-147)\n",
      "  surr_para_trunc           r=+0.806 (p=1.15e-115)\n",
      "  static_fact_trunc         r=+0.818 (p=9.70e-122)\n",
      "  static_howto_trunc        r=+0.812 (p=9.77e-119)\n",
      "  random_trunc              r=+0.849 (p=5.21e-140)\n",
      "  surr_lead_trunc           r=+0.813 (p=2.85e-119)\n",
      "  surr_template_trunc       r=+0.849 (p=3.45e-140)\n"
     ]
    }
   ],
   "source": [
    "# Cell 10: Hardness stratification\n",
    "print(\"=\" * 70)\n",
    "print(\"HARDNESS STRATIFICATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "quintile_bounds = np.percentile(bare_nlls, [20, 40, 60, 80])\n",
    "quintiles = np.digitize(bare_nlls, quintile_bounds)\n",
    "\n",
    "print(f\"\\n{'Quintile':<12} {'N':>4}\", end=\"\")\n",
    "for cond in ['oracle_trunc', 'surr_doc_trunc', 'static_fact_trunc', 'random_trunc']:\n",
    "    print(f\" {cond.replace('_trunc',''):>12}\", end=\"\")\n",
    "print()\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for q in range(5):\n",
    "    mask = quintiles == q\n",
    "    n_q = mask.sum()\n",
    "    if n_q < 3:\n",
    "        continue\n",
    "    qlabel = ['Q1 easy', 'Q2', 'Q3', 'Q4', 'Q5 hard'][q]\n",
    "    print(f\"{qlabel:<12} {n_q:>4}\", end=\"\")\n",
    "\n",
    "    b_q = bare_nlls[mask]\n",
    "    for cond in ['oracle_trunc', 'surr_doc_trunc', 'static_fact_trunc', 'random_trunc']:\n",
    "        c_nlls = np.array([all_results[j][f'nll_{cond}'] for j in range(len(all_results)) if mask[j]])\n",
    "        diff_q = b_q - c_nlls\n",
    "        d_q = cohens_d(diff_q)\n",
    "        print(f\" {d_q:>+12.3f}\", end=\"\")\n",
    "    print()\n",
    "\n",
    "# Hardness-benefit correlation for each condition\n",
    "print(f\"\\n--- Hardness-benefit correlation (r with bare NLL) ---\")\n",
    "for cond in COND_NAMES[1:]:  # skip bare\n",
    "    cond_nlls = np.array([r[f'nll_{cond}'] for r in all_results])\n",
    "    benefit = bare_nlls - cond_nlls\n",
    "    r_hb, p_hb = stats.pearsonr(bare_nlls, benefit)\n",
    "    print(f\"  {cond:<25} r={r_hb:+.3f} (p={p_hb:.2e})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "15171920",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-17T21:46:57.267027Z",
     "iopub.status.busy": "2026-02-17T21:46:57.266509Z",
     "iopub.status.idle": "2026-02-17T21:46:57.282328Z",
     "shell.execute_reply": "2026-02-17T21:46:57.281671Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PAIRWISE HEAD-TO-HEAD COMPARISONS\n",
      "======================================================================\n",
      "Direct comparisons between key surrogate pairs.\n",
      "\n",
      "Comparison                                                d     Win%            p   sig\n",
      "-------------------------------------------------------------------------------------\n",
      "  Oracle vs doc keywords (how close?)                +0.080    63.6%     7.38e-02    ns  [oracle]\n",
      "  Oracle vs paraphrase                               +0.181    64.2%     6.10e-05   ***  [oracle]\n",
      "  Doc keywords vs static (content matters?)          +0.169    59.2%     1.71e-04   ***  [surr_doc]\n",
      "  Doc keywords vs lead sentence                      +0.365    68.6%     2.94e-15   ***  [surr_doc]\n",
      "  Doc keywords vs random (semantic signal?)          +0.130    54.6%     3.87e-03    **  [surr_doc]\n",
      "  Static fact vs random (structural baseline)        -0.129    43.6%     4.04e-03    **  [random]\n",
      "  Lead sentence vs question template                 -0.427    32.6%     5.50e-20   ***  [surr_template]\n"
     ]
    }
   ],
   "source": [
    "# Cell 11: Pairwise comparisons (direct head-to-head)\n",
    "print(\"=\" * 70)\n",
    "print(\"PAIRWISE HEAD-TO-HEAD COMPARISONS\")\n",
    "print(\"=\" * 70)\n",
    "print(\"Direct comparisons between key surrogate pairs.\\n\")\n",
    "\n",
    "pairs = [\n",
    "    ('oracle_trunc', 'surr_doc_trunc', \"Oracle vs doc keywords (how close?)\"),\n",
    "    ('oracle_trunc', 'surr_para_trunc', \"Oracle vs paraphrase\"),\n",
    "    ('surr_doc_trunc', 'static_fact_trunc', \"Doc keywords vs static (content matters?)\"),\n",
    "    ('surr_doc_trunc', 'surr_lead_trunc', \"Doc keywords vs lead sentence\"),\n",
    "    ('surr_doc_trunc', 'random_trunc', \"Doc keywords vs random (semantic signal?)\"),\n",
    "    ('static_fact_trunc', 'random_trunc', \"Static fact vs random (structural baseline)\"),\n",
    "    ('surr_lead_trunc', 'surr_template_trunc', \"Lead sentence vs question template\"),\n",
    "]\n",
    "\n",
    "print(f\"{'Comparison':<50} {'d':>8} {'Win%':>8} {'p':>12} {'sig':>5}\")\n",
    "print(\"-\" * 85)\n",
    "\n",
    "for cond_a, cond_b, desc in pairs:\n",
    "    nlls_a = np.array([r[f'nll_{cond_a}'] for r in all_results])\n",
    "    nlls_b = np.array([r[f'nll_{cond_b}'] for r in all_results])\n",
    "    diff = nlls_b - nlls_a  # positive = A is better (lower NLL)\n",
    "    d = cohens_d(diff)\n",
    "    win = 100 * np.mean(diff > 0)\n",
    "    t, p = stats.ttest_1samp(diff, 0)\n",
    "    sig = '***' if p < 0.001 else '**' if p < 0.01 else '*' if p < 0.05 else 'ns'\n",
    "    winner = cond_a.replace('_trunc', '') if d > 0 else cond_b.replace('_trunc', '')\n",
    "    print(f\"  {desc:<48} {d:>+8.3f} {win:>7.1f}% {p:>12.2e} {sig:>5}  [{winner}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4701a4c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-17T21:46:57.285235Z",
     "iopub.status.busy": "2026-02-17T21:46:57.284998Z",
     "iopub.status.idle": "2026-02-17T21:46:57.295982Z",
     "shell.execute_reply": "2026-02-17T21:46:57.295240Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "VERDICT -- Exp 02: Surrogate Type Sweep\n",
      "======================================================================\n",
      "\n",
      "Model: google/t5gemma-2-4b-4b\n",
      "N: 500 samples\n",
      "\n",
      "--- Rankings (by Cohen's d) ---\n",
      "  1. oracle_trunc              d=+0.376 (100% of oracle)\n",
      "  2. static_fact_trunc         d=+0.372 (99% of oracle)\n",
      "  3. static_howto_trunc        d=+0.346 (92% of oracle)\n",
      "  4. surr_template_trunc       d=+0.336 (90% of oracle)\n",
      "  5. surr_doc_trunc            d=+0.322 (86% of oracle)\n",
      "  6. surr_para_trunc           d=+0.305 (81% of oracle)\n",
      "  7. random_trunc              d=+0.303 (81% of oracle)\n",
      "  8. surr_lead_trunc           d=+0.151 (40% of oracle)\n",
      "\n",
      "--- Recommendation for Exps 03-04 ---\n",
      "  Best document-derived surrogate: surr_template_trunc\n",
      "  d=+0.336 (90% of oracle)\n",
      "\n",
      "--- Content gradient ---\n",
      "  Spearman rho = -0.167 (p=0.693)\n",
      "  NO CONTENT GRADIENT (same as v2): Content does not predict effect size.\n",
      "  The benefit may be structural rather than semantic.\n",
      "\n",
      "--- Three-point spectrum ---\n",
      "  Upper bound (oracle_trunc):      d=+0.376\n",
      "  Middle ground (best doc-derived): d=+0.336 (90%)\n",
      "  Lower bound (bare):              d=0.000\n",
      "  Structural floor (random):       d=+0.303\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Results saved to results/exp02/results.json\n"
     ]
    }
   ],
   "source": [
    "# Cell 12: Verdict and save\n",
    "print(\"=\" * 70)\n",
    "print(\"VERDICT -- Exp 02: Surrogate Type Sweep\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\nModel: {MODEL_NAME}\")\n",
    "print(f\"N: {len(all_results)} samples\")\n",
    "\n",
    "# Rank conditions by d\n",
    "ranked = [(c, analysis[c]['d']) for c in COND_NAMES[1:] if 'd' in analysis.get(c, {})]\n",
    "ranked.sort(key=lambda x: -x[1])\n",
    "\n",
    "print(f\"\\n--- Rankings (by Cohen's d) ---\")\n",
    "oracle_d_val = analysis.get('oracle_trunc', {}).get('d', 0)\n",
    "for i, (cond, d_val) in enumerate(ranked, 1):\n",
    "    pct = d_val / oracle_d_val * 100 if oracle_d_val > 0 else 0\n",
    "    print(f\"  {i}. {cond:<25} d={d_val:+.3f} ({pct:.0f}% of oracle)\")\n",
    "\n",
    "# Best document-derived (for Exp 03/04)\n",
    "doc_surrogates = ['surr_doc_trunc', 'surr_lead_trunc', 'surr_template_trunc']\n",
    "best_doc = max(doc_surrogates, key=lambda c: analysis.get(c, {}).get('d', -999))\n",
    "best_doc_d = analysis.get(best_doc, {}).get('d', 0)\n",
    "best_doc_pct = best_doc_d / oracle_d_val * 100 if oracle_d_val > 0 else 0\n",
    "\n",
    "print(f\"\\n--- Recommendation for Exps 03-04 ---\")\n",
    "print(f\"  Best document-derived surrogate: {best_doc}\")\n",
    "print(f\"  d={best_doc_d:+.3f} ({best_doc_pct:.0f}% of oracle)\")\n",
    "\n",
    "# Content gradient conclusion\n",
    "print(f\"\\n--- Content gradient ---\")\n",
    "print(f\"  Spearman rho = {r_spearman:+.3f} (p={p_spearman:.3f})\")\n",
    "if r_spearman < -0.5 and p_spearman < 0.05:\n",
    "    print(f\"  CONTENT GRADIENT CONFIRMED: T5Gemma's bidirectional encoding is semantic.\")\n",
    "    print(f\"  More relevant surrogates genuinely produce better document representations.\")\n",
    "else:\n",
    "    print(f\"  NO CONTENT GRADIENT (same as v2): Content does not predict effect size.\")\n",
    "    print(f\"  The benefit may be structural rather than semantic.\")\n",
    "\n",
    "# Upper/lower/middle bound framing\n",
    "print(f\"\\n--- Three-point spectrum ---\")\n",
    "print(f\"  Upper bound (oracle_trunc):      d={oracle_d_val:+.3f}\")\n",
    "print(f\"  Middle ground (best doc-derived): d={best_doc_d:+.3f} ({best_doc_pct:.0f}%)\")\n",
    "print(f\"  Lower bound (bare):              d=0.000\")\n",
    "print(f\"  Structural floor (random):       d={analysis.get('random_trunc', {}).get('d', 0):+.3f}\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "\n",
    "# Save\n",
    "final_results = {\n",
    "    'experiment': 'exp02_surrogate_type_sweep',\n",
    "    'model': MODEL_NAME,\n",
    "    'n_samples': len(all_results),\n",
    "    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'analysis': analysis,\n",
    "    'content_gradient': {\n",
    "        'spearman_rho': float(r_spearman),\n",
    "        'spearman_p': float(p_spearman),\n",
    "        'pearson_r': float(r_pearson),\n",
    "        'pearson_p': float(p_pearson),\n",
    "    },\n",
    "    'best_doc_derived': best_doc,\n",
    "    'best_doc_derived_d': float(best_doc_d),\n",
    "    'best_doc_derived_pct_oracle': float(best_doc_pct),\n",
    "}\n",
    "\n",
    "with open(RESULTS_DIR / 'results.json', 'w') as f:\n",
    "    json.dump(final_results, f, indent=2)\n",
    "print(f\"\\nResults saved to {RESULTS_DIR / 'results.json'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab3118c7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-17T21:46:57.298636Z",
     "iopub.status.busy": "2026-02-17T21:46:57.298376Z",
     "iopub.status.idle": "2026-02-17T21:46:57.772884Z",
     "shell.execute_reply": "2026-02-17T21:46:57.771948Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning up GPU memory...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory: 15.03 GB -> 0.01 GB\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Cell 13: Cleanup\n",
    "print(\"Cleaning up GPU memory...\")\n",
    "mem_before = torch.cuda.memory_allocated() / 1e9\n",
    "del model, processor, tokenizer\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "mem_after = torch.cuda.memory_allocated() / 1e9\n",
    "print(f\"GPU memory: {mem_before:.2f} GB -> {mem_after:.2f} GB\")\n",
    "print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "03ee0a6ffed9496596a8efa68824ae81": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "0cd885291ee8470f989e012b11e9156c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_a7caf94adb6d40459f5c290cd2d82e74",
        "IPY_MODEL_773be7a9340d41b594b52357c369c908",
        "IPY_MODEL_7993dad69e844429bf703d7ec65fea7a"
       ],
       "layout": "IPY_MODEL_b1ed3850254b4fc3a379a5cee59cb20c",
       "tabbable": null,
       "tooltip": null
      }
     },
     "0cfd77681c69448caadb9282b6c63922": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_b562b14607844500893877f841b58bfa",
       "max": 500.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_daf94dcb63c94a95b521381d121be6fa",
       "tabbable": null,
       "tooltip": null,
       "value": 500.0
      }
     },
     "1397266b97704ceeb66850d5de4d643b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_6fc160ce9ed04541a56adb5805e173f9",
       "placeholder": "",
       "style": "IPY_MODEL_a3b43d84f5774c7e8ddf0dfd70ea8d03",
       "tabbable": null,
       "tooltip": null,
       "value": "Scoring:100%"
      }
     },
     "22918dd24857411199be9d97230a32c3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_1397266b97704ceeb66850d5de4d643b",
        "IPY_MODEL_0cfd77681c69448caadb9282b6c63922",
        "IPY_MODEL_803f357aba3041cd991d6710eefc00dc"
       ],
       "layout": "IPY_MODEL_7e6f59c8ca00436f980c8c7c457625a5",
       "tabbable": null,
       "tooltip": null
      }
     },
     "2add7a20b8434da58ae6fbd14fc85ddc": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "5217f04ccbdb4a09b5534335bd086283": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "6fc160ce9ed04541a56adb5805e173f9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "773be7a9340d41b594b52357c369c908": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_03ee0a6ffed9496596a8efa68824ae81",
       "max": 1327.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_7aa52f28816743a8b66affc65727ba6b",
       "tabbable": null,
       "tooltip": null,
       "value": 1327.0
      }
     },
     "7993dad69e844429bf703d7ec65fea7a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_7b91eb25d65f44d0bc26d0692c3d80c3",
       "placeholder": "",
       "style": "IPY_MODEL_c3103069322b45b08d9fc119cc88387b",
       "tabbable": null,
       "tooltip": null,
       "value": "1327/1327[00:04&lt;00:00,671.51it/s,Materializingparam=model.encoder.vision_tower.vision_model.post_layernorm.weight]"
      }
     },
     "7a19e9f4e64f40d3ba0a793f5ee9b1a6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "7aa52f28816743a8b66affc65727ba6b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "7b91eb25d65f44d0bc26d0692c3d80c3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "7e6f59c8ca00436f980c8c7c457625a5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "803f357aba3041cd991d6710eefc00dc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_b802e81c4a6e4266869f26a343177149",
       "placeholder": "",
       "style": "IPY_MODEL_7a19e9f4e64f40d3ba0a793f5ee9b1a6",
       "tabbable": null,
       "tooltip": null,
       "value": "500/500[16:09&lt;00:00,1.93s/it]"
      }
     },
     "a3b43d84f5774c7e8ddf0dfd70ea8d03": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "a7caf94adb6d40459f5c290cd2d82e74": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_2add7a20b8434da58ae6fbd14fc85ddc",
       "placeholder": "",
       "style": "IPY_MODEL_5217f04ccbdb4a09b5534335bd086283",
       "tabbable": null,
       "tooltip": null,
       "value": "Loadingweights:100%"
      }
     },
     "b1ed3850254b4fc3a379a5cee59cb20c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b562b14607844500893877f841b58bfa": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b802e81c4a6e4266869f26a343177149": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c3103069322b45b08d9fc119cc88387b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "daf94dcb63c94a95b521381d121be6fa": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
