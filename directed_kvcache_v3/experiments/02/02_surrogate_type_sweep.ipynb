{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 02: Surrogate Type Sweep",
    "## Which surrogate works best for bidirectional co-encoding?",
    "",
    "### Background",
    "Exp 01 proved that document representations are genuinely improved by co-encoding",
    "with a query/surrogate (truncation made the benefit STRONGER: d=+0.408 trunc vs",
    "+0.345 full). But we only tested 2 surrogate types.",
    "",
    "v2 showed surprising results on decoder-only models:",
    "- Static \"What are the key facts?\" beat LLM-generated surrogates 2x (Exp 07)",
    "- No semantic content gradient detected (Exp 10, Spearman r=+0.036)",
    "- Content barely mattered -- the mechanism was structural (value contamination)",
    "",
    "T5Gemma's bidirectional encoder should be fundamentally different. If the encoder",
    "creates genuine query-document interactions, then content-specific surrogates",
    "should outperform content-agnostic ones.",
    "",
    "### Conditions (9 total, all truncated)",
    "| # | Condition | Source | Semantic relevance |",
    "|---|-----------|--------|--------------------|",
    "| 1 | bare | -- | Lower bound |",
    "| 2 | oracle_trunc | Real query | Upper bound (100%) |",
    "| 3 | surr_doc_trunc | Top-5 TF keywords from document | Doc-specific |",
    "| 4 | surr_para_trunc | Query keywords reversed | Query-specific |",
    "| 5 | static_fact_trunc | \"What are the key facts?\" | Content-agnostic |",
    "| 6 | static_howto_trunc | \"How do I do this?\" | Content-agnostic |",
    "| 7 | random_trunc | Passage from unrelated sample | Structural control |",
    "| 8 | surr_lead_trunc | First sentence of document | Doc-specific (rich) |",
    "| 9 | surr_template_trunc | \"What is [top_keyword]?\" | Doc-specific (minimal) |",
    "",
    "### Success criteria",
    "- oracle_trunc replicates Exp 01 (d ~ +0.41)",
    "- Content gradient: oracle > para > doc_kw > lead > template > static > random",
    "- If gradient exists: confirms bidirectional mechanism is semantic",
    "- If no gradient: mechanism is structural (like v2), just stronger in encoder-decoder",
    ""
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 2: Setup\n",
    "import os\n",
    "os.umask(0o000)\n",
    "\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "import gc\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from scipy import stats\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "sys.path.insert(0, \"../..\")\n",
    "\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "RESULTS_DIR = Path(\"../../results/exp02\")\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CHECKPOINT_PATH = RESULTS_DIR / \"checkpoint.json\"\n",
    "\n",
    "N_SAMPLES = 500\n",
    "MODEL_NAME = \"google/t5gemma-2-4b-4b\"\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv())\n",
    "HF_TOKEN = os.environ.get(\"HF_TOKEN\")\n",
    "\n",
    "print(f\"Exp 02: Surrogate Type Sweep\")\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"N: {N_SAMPLES}\")\n",
    "print(f\"CUDA: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 3: Load model\n",
    "from transformers import AutoProcessor, AutoModelForSeq2SeqLM\n",
    "\n",
    "print(f\"Loading {MODEL_NAME}...\")\n",
    "processor = AutoProcessor.from_pretrained(MODEL_NAME, token=HF_TOKEN)\n",
    "tokenizer = processor.tokenizer\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    token=HF_TOKEN,\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "DEVICE = next(model.parameters()).device\n",
    "print(f\"Model loaded. dtype={next(model.parameters()).dtype}\")\n",
    "print(f\"GPU memory used: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 4: Scoring and surrogate helpers\n",
    "\n",
    "def score_nll(encoder_text, answer_text, prefix_token_count=0, truncate=False):\n",
    "    # Score NLL of answer tokens with optional truncation.\n",
    "    # When truncate=True: encoder processes full input bidirectionally,\n",
    "    # but decoder can only cross-attend to document positions (prefix masked).\n",
    "    enc_ids = tokenizer(encoder_text, return_tensors=\"pt\",\n",
    "                        add_special_tokens=True, truncation=True,\n",
    "                        max_length=2048).input_ids.to(DEVICE)\n",
    "    total_enc_len = enc_ids.shape[1]\n",
    "\n",
    "    enc_mask = torch.ones(1, total_enc_len, device=DEVICE, dtype=torch.long)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        encoder_outputs = model.get_encoder()(\n",
    "            input_ids=enc_ids, attention_mask=enc_mask\n",
    "        )\n",
    "\n",
    "    if truncate and prefix_token_count > 0:\n",
    "        cross_attn_mask = torch.ones(1, total_enc_len, device=DEVICE, dtype=torch.long)\n",
    "        cross_attn_mask[:, :prefix_token_count] = 0\n",
    "    else:\n",
    "        cross_attn_mask = torch.ones(1, total_enc_len, device=DEVICE, dtype=torch.long)\n",
    "\n",
    "    ans_ids = tokenizer(answer_text, return_tensors=\"pt\",\n",
    "                        add_special_tokens=False, truncation=True,\n",
    "                        max_length=256).input_ids.to(DEVICE)\n",
    "\n",
    "    if ans_ids.shape[1] == 0:\n",
    "        return 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            encoder_outputs=encoder_outputs,\n",
    "            attention_mask=cross_attn_mask,\n",
    "            labels=ans_ids,\n",
    "        )\n",
    "\n",
    "    logits = outputs.logits\n",
    "    log_probs = F.log_softmax(logits, dim=-1)\n",
    "    token_log_probs = log_probs[0].gather(1, ans_ids[0].unsqueeze(1)).squeeze(1)\n",
    "    mean_nll = -token_log_probs.mean().item()\n",
    "\n",
    "    del encoder_outputs, outputs, logits, log_probs\n",
    "    return mean_nll\n",
    "\n",
    "\n",
    "def count_prefix_tokens(prefix_text, document_text):\n",
    "    # Count how many tokens the prefix occupies in the concatenated encoding.\n",
    "    full_text = prefix_text + \"\\n\" + document_text\n",
    "    full_ids = tokenizer(full_text, add_special_tokens=True).input_ids\n",
    "    doc_ids = tokenizer(document_text, add_special_tokens=True).input_ids\n",
    "    return len(full_ids) - len(doc_ids)\n",
    "\n",
    "\n",
    "# === Surrogate generators ===\n",
    "STOP_WORDS = {\n",
    "    'a', 'an', 'the', 'is', 'are', 'was', 'were', 'be', 'been', 'being',\n",
    "    'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could',\n",
    "    'should', 'may', 'might', 'can', 'shall', 'to', 'of', 'in', 'for',\n",
    "    'on', 'with', 'at', 'by', 'from', 'as', 'into', 'through', 'during',\n",
    "    'before', 'after', 'above', 'below', 'between', 'and', 'but', 'or',\n",
    "    'not', 'no', 'if', 'then', 'than', 'so', 'up', 'out', 'about',\n",
    "    'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those',\n",
    "    'it', 'its', 'i', 'me', 'my', 'we', 'our', 'you', 'your', 'he',\n",
    "    'him', 'his', 'she', 'her', 'they', 'them', 'their', 'how', 'when',\n",
    "    'where', 'why', 'much', 'many', 'some', 'any', 'all', 'each',\n",
    "    'does', 'also', 'just', 'more', 'most', 'very', 'too', 'only',\n",
    "}\n",
    "\n",
    "def extract_keywords(text):\n",
    "    words = re.sub(r'[^\\w\\s]', '', text.lower()).split()\n",
    "    return [w for w in words if w not in STOP_WORDS and len(w) > 2]\n",
    "\n",
    "\n",
    "def make_surrogate_paraphrase(query):\n",
    "    # Reversed query keywords (paraphrase proxy)\n",
    "    keywords = extract_keywords(query)\n",
    "    return \" \".join(keywords[::-1]) if keywords else query\n",
    "\n",
    "\n",
    "def make_surrogate_doc_kw(passage):\n",
    "    # Top-5 TF keywords from the document\n",
    "    content_words = extract_keywords(passage)\n",
    "    if not content_words:\n",
    "        return \"information\"\n",
    "    counts = Counter(content_words)\n",
    "    return \" \".join(w for w, _ in counts.most_common(5))\n",
    "\n",
    "\n",
    "def make_surrogate_lead(passage):\n",
    "    # First sentence of the document\n",
    "    # Split on sentence boundaries\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', passage.strip())\n",
    "    first = sentences[0] if sentences else passage[:100]\n",
    "    # Cap length to avoid very long surrogates\n",
    "    words = first.split()\n",
    "    if len(words) > 25:\n",
    "        first = \" \".join(words[:25])\n",
    "    return first\n",
    "\n",
    "\n",
    "def make_surrogate_template(passage):\n",
    "    # Auto-generated question template: 'What is [top_keyword]?'\n",
    "    content_words = extract_keywords(passage)\n",
    "    if not content_words:\n",
    "        return \"What is this about?\"\n",
    "    counts = Counter(content_words)\n",
    "    top_word = counts.most_common(1)[0][0]\n",
    "    return f\"What is {top_word}?\"\n",
    "\n",
    "\n",
    "STATIC_FACT = \"What are the key facts I need to know?\"\n",
    "STATIC_HOWTO = \"How do I do this?\"\n",
    "\n",
    "print(\"Helpers defined. Surrogate types:\")\n",
    "print(\"  1. oracle      - real query (upper bound)\")\n",
    "print(\"  2. surr_doc    - top-5 TF keywords from document\")\n",
    "print(\"  3. surr_para   - reversed query keywords\")\n",
    "print(\"  4. static_fact - 'What are the key facts I need to know?'\")\n",
    "print(\"  5. static_howto- 'How do I do this?'\")\n",
    "print(\"  6. random      - passage from unrelated sample\")\n",
    "print(\"  7. surr_lead   - first sentence of document\")\n",
    "print(\"  8. surr_template - 'What is [top_keyword]?'\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 5: Load data\n",
    "from lib.data import count_words\n",
    "from datasets import load_dataset\n",
    "\n",
    "print(\"Loading MS MARCO v1.1 validation...\")\n",
    "ds = load_dataset(\"microsoft/ms_marco\", \"v1.1\", split=\"validation\")\n",
    "\n",
    "samples = []\n",
    "for item in ds:\n",
    "    if len(samples) >= N_SAMPLES * 3:\n",
    "        break\n",
    "    passages = item.get('passages', {})\n",
    "    ptexts = passages.get('passage_text', [])\n",
    "    is_sel = passages.get('is_selected', [])\n",
    "    query = item.get('query', '')\n",
    "    answers = item.get('answers', [])\n",
    "    well_formed = item.get('wellFormedAnswers', [])\n",
    "\n",
    "    answer = None\n",
    "    if well_formed and len(well_formed) > 0 and well_formed[0] not in ('[]', ''):\n",
    "        answer = well_formed[0]\n",
    "    elif answers and len(answers) > 0 and answers[0] != 'No Answer Present.':\n",
    "        answer = answers[0]\n",
    "    if not answer:\n",
    "        continue\n",
    "\n",
    "    for pt, sel in zip(ptexts, is_sel):\n",
    "        wc = count_words(pt)\n",
    "        if sel == 1 and 30 <= wc <= 300:\n",
    "            samples.append({\n",
    "                'passage': pt, 'query': query, 'answer': answer,\n",
    "                'word_count': wc\n",
    "            })\n",
    "            break\n",
    "\n",
    "np.random.seed(SEED)\n",
    "np.random.shuffle(samples)\n",
    "samples = samples[:N_SAMPLES]\n",
    "del ds\n",
    "gc.collect()\n",
    "\n",
    "# Pre-compute surrogates\n",
    "for i, s in enumerate(samples):\n",
    "    s['surr_para'] = make_surrogate_paraphrase(s['query'])\n",
    "    s['surr_doc_kw'] = make_surrogate_doc_kw(s['passage'])\n",
    "    s['surr_lead'] = make_surrogate_lead(s['passage'])\n",
    "    s['surr_template'] = make_surrogate_template(s['passage'])\n",
    "    # Random: use passage from a different sample (circular offset)\n",
    "    other_idx = (i + N_SAMPLES // 2) % len(samples)\n",
    "    other_passage = samples[other_idx]['passage']\n",
    "    # Use first ~20 words of the other passage as the random surrogate\n",
    "    s['surr_random'] = \" \".join(other_passage.split()[:20])\n",
    "\n",
    "print(f\"Selected {len(samples)} samples, mean words={np.mean([s['word_count'] for s in samples]):.0f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 6: Explain conditions with concrete examples\n",
    "print(\"=\" * 70)\n",
    "print(\"EXPERIMENTAL CONDITIONS (all with truncation)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "ex = samples[0]\n",
    "print(f\"\\nExample query:     {ex['query'][:80]}\")\n",
    "print(f\"Example answer:    {ex['answer'][:80]}\")\n",
    "print(f\"Example passage:   {ex['passage'][:80]}...\")\n",
    "print()\n",
    "\n",
    "surrogates = {\n",
    "    'oracle':        ex['query'],\n",
    "    'surr_doc':      ex['surr_doc_kw'],\n",
    "    'surr_para':     ex['surr_para'],\n",
    "    'static_fact':   STATIC_FACT,\n",
    "    'static_howto':  STATIC_HOWTO,\n",
    "    'random':        ex['surr_random'],\n",
    "    'surr_lead':     ex['surr_lead'],\n",
    "    'surr_template': ex['surr_template'],\n",
    "}\n",
    "\n",
    "# Expected semantic relevance ranking (for gradient test)\n",
    "SEMANTIC_RANK = {\n",
    "    'oracle_trunc': 1,        # exact match\n",
    "    'surr_para_trunc': 2,     # query-derived\n",
    "    'surr_doc_trunc': 3,      # doc-specific keywords\n",
    "    'surr_lead_trunc': 4,     # doc-specific sentence\n",
    "    'surr_template_trunc': 5, # doc-specific minimal\n",
    "    'static_fact_trunc': 6,   # content-agnostic (best v2)\n",
    "    'static_howto_trunc': 7,  # content-agnostic\n",
    "    'random_trunc': 8,        # structural control\n",
    "}\n",
    "\n",
    "print(f\"{'Condition':<22} {'Prefix tokens':>14} {'Surrogate text (first 60 chars)'}\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "for name, surr_text in surrogates.items():\n",
    "    ptoks = count_prefix_tokens(surr_text, ex['passage'])\n",
    "    display = surr_text[:60] + ('...' if len(surr_text) > 60 else '')\n",
    "    print(f\"  {name:<20} {ptoks:>14} {display}\")\n",
    "\n",
    "print(f\"\\n  bare                            0 (no surrogate -- lower bound)\")\n",
    "\n",
    "print(f\"\\n--- Semantic relevance ranking (for gradient test) ---\")\n",
    "print(\"  1=most relevant (oracle) ... 8=least relevant (random)\")\n",
    "for name, rank in sorted(SEMANTIC_RANK.items(), key=lambda x: x[1]):\n",
    "    print(f\"  {rank}. {name}\")\n",
    "\n",
    "print(\"\\n--- Key question ---\")\n",
    "print(\"  v2 Mistral: NO content gradient (Spearman r=+0.036)\")\n",
    "print(\"  v3 T5Gemma: Does bidirectional attention create a genuine gradient?\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 7: Run scoring\n",
    "print(\"=\" * 70)\n",
    "print(\"RUNNING EXPERIMENT\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "COND_NAMES = [\n",
    "    'bare',\n",
    "    'oracle_trunc',\n",
    "    'surr_doc_trunc',\n",
    "    'surr_para_trunc',\n",
    "    'static_fact_trunc',\n",
    "    'static_howto_trunc',\n",
    "    'random_trunc',\n",
    "    'surr_lead_trunc',\n",
    "    'surr_template_trunc',\n",
    "]\n",
    "\n",
    "\n",
    "def make_conditions(sample):\n",
    "    # Return dict of {name: (encoder_text, prefix_token_count, truncate)}\n",
    "    query = sample['query']\n",
    "    passage = sample['passage']\n",
    "\n",
    "    surr_map = {\n",
    "        'oracle':        query,\n",
    "        'surr_doc':      sample['surr_doc_kw'],\n",
    "        'surr_para':     sample['surr_para'],\n",
    "        'static_fact':   STATIC_FACT,\n",
    "        'static_howto':  STATIC_HOWTO,\n",
    "        'random':        sample['surr_random'],\n",
    "        'surr_lead':     sample['surr_lead'],\n",
    "        'surr_template': sample['surr_template'],\n",
    "    }\n",
    "\n",
    "    conditions = {\n",
    "        'bare': (passage, 0, False),\n",
    "    }\n",
    "\n",
    "    for surr_name, surr_text in surr_map.items():\n",
    "        cond_name = f'{surr_name}_trunc'\n",
    "        enc_text = surr_text + \"\\n\" + passage\n",
    "        prefix_count = count_prefix_tokens(surr_text, passage)\n",
    "        conditions[cond_name] = (enc_text, prefix_count, True)\n",
    "\n",
    "    return conditions\n",
    "\n",
    "\n",
    "# Resume from checkpoint\n",
    "all_results = []\n",
    "start_idx = 0\n",
    "\n",
    "if CHECKPOINT_PATH.exists():\n",
    "    ckpt = json.loads(CHECKPOINT_PATH.read_text())\n",
    "    if ckpt.get('n_total') == N_SAMPLES and len(ckpt.get('results', [])) > 0:\n",
    "        saved_queries = [r['query'][:50] for r in ckpt['results']]\n",
    "        current_queries = [s['query'][:50] for s in samples[:len(saved_queries)]]\n",
    "        if saved_queries == current_queries:\n",
    "            all_results = ckpt['results']\n",
    "            start_idx = len(all_results)\n",
    "            print(f\"Resuming: {start_idx}/{N_SAMPLES}\")\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "for i in tqdm(range(start_idx, N_SAMPLES), initial=start_idx, total=N_SAMPLES, desc=\"Scoring\"):\n",
    "    s = samples[i]\n",
    "    conditions = make_conditions(s)\n",
    "\n",
    "    result = {\n",
    "        'query': s['query'], 'answer': s['answer'],\n",
    "        'passage_words': s['word_count'],\n",
    "    }\n",
    "\n",
    "    for cond_name in COND_NAMES:\n",
    "        enc_text, prefix_count, trunc = conditions[cond_name]\n",
    "        nll = score_nll(enc_text, s['answer'], prefix_count, trunc)\n",
    "        result[f'nll_{cond_name}'] = nll\n",
    "\n",
    "    all_results.append(result)\n",
    "\n",
    "    if (i + 1) % 20 == 0 or i == N_SAMPLES - 1:\n",
    "        ckpt = {\n",
    "            'n_total': N_SAMPLES, 'results': all_results,\n",
    "            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        }\n",
    "        CHECKPOINT_PATH.write_text(json.dumps(ckpt))\n",
    "        elapsed = time.time() - t0\n",
    "        done = i - start_idx + 1\n",
    "        eta = (N_SAMPLES - i - 1) * elapsed / done if done > 0 else 0\n",
    "        tqdm.write(f\"  Checkpoint {i+1}/{N_SAMPLES} | {elapsed/60:.1f}m | ETA {eta/60:.1f}m\")\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "elapsed = time.time() - t0\n",
    "print(f\"\\nScoring complete: {len(all_results)} samples in {elapsed/60:.1f} min\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 8: Results\n",
    "from lib.analysis import cohens_d\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"RESULTS (N={len(all_results)})\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "bare_nlls = np.array([r['nll_bare'] for r in all_results])\n",
    "\n",
    "print(f\"\\n{'Condition':<25} {'Mean NLL':>10} {'vs Bare':>10} {'d':>8} {'Win%':>8} {'p':>12} {'sig':>5} {'% oracle':>10}\")\n",
    "print(\"-\" * 95)\n",
    "\n",
    "analysis = {}\n",
    "oracle_d = None\n",
    "\n",
    "for cond in COND_NAMES:\n",
    "    nlls = np.array([r[f'nll_{cond}'] for r in all_results])\n",
    "    mean_nll = nlls.mean()\n",
    "    diff = bare_nlls - nlls\n",
    "    d = cohens_d(diff)\n",
    "    win_pct = 100 * np.mean(diff > 0)\n",
    "\n",
    "    if cond == 'bare':\n",
    "        print(f\"{cond:<25} {mean_nll:>10.4f} {'--':>10} {'--':>8} {'--':>8} {'--':>12} {'--':>5} {'--':>10}\")\n",
    "        analysis[cond] = {'mean_nll': float(mean_nll)}\n",
    "    else:\n",
    "        t_stat, p_val = stats.ttest_1samp(diff, 0)\n",
    "        sig = '***' if p_val < 0.001 else '**' if p_val < 0.01 else '*' if p_val < 0.05 else 'ns'\n",
    "\n",
    "        if cond == 'oracle_trunc':\n",
    "            oracle_d = d\n",
    "            pct_oracle = '100% (UB)'\n",
    "        elif oracle_d and oracle_d > 0:\n",
    "            pct_oracle = f\"{d / oracle_d * 100:.0f}%\"\n",
    "        else:\n",
    "            pct_oracle = '--'\n",
    "\n",
    "        print(f\"{cond:<25} {mean_nll:>10.4f} {diff.mean():>+10.4f} {d:>+8.3f} {win_pct:>7.1f}% {p_val:>12.2e} {sig:>5} {pct_oracle:>10}\")\n",
    "        analysis[cond] = {\n",
    "            'mean_nll': float(mean_nll), 'delta': float(diff.mean()),\n",
    "            'd': float(d), 'win_pct': float(win_pct), 'p': float(p_val),\n",
    "        }\n",
    "\n",
    "# Bonferroni correction\n",
    "n_tests = len(COND_NAMES) - 1  # exclude bare\n",
    "bonferroni_threshold = 0.05 / n_tests\n",
    "print(f\"\\nBonferroni threshold: p < {bonferroni_threshold:.4f} (alpha=0.05, {n_tests} tests)\")\n",
    "for cond, a in analysis.items():\n",
    "    if cond != 'bare' and 'p' in a:\n",
    "        bf_sig = a['p'] < bonferroni_threshold\n",
    "        if bf_sig:\n",
    "            print(f\"  {cond}: p={a['p']:.2e} -- SIGNIFICANT after Bonferroni\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 9: Content gradient analysis\n",
    "print(\"=\" * 70)\n",
    "print(\"CONTENT GRADIENT ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "print(\"Does more relevant content produce better document representations?\")\n",
    "\n",
    "# Spearman rank correlation: semantic relevance rank vs effect size (d)\n",
    "ranks = []\n",
    "ds = []\n",
    "cond_labels = []\n",
    "\n",
    "for cond, rank in sorted(SEMANTIC_RANK.items(), key=lambda x: x[1]):\n",
    "    if cond in analysis and 'd' in analysis[cond]:\n",
    "        ranks.append(rank)\n",
    "        ds.append(analysis[cond]['d'])\n",
    "        cond_labels.append(cond)\n",
    "\n",
    "r_spearman, p_spearman = stats.spearmanr(ranks, ds)\n",
    "r_pearson, p_pearson = stats.pearsonr(ranks, ds)\n",
    "\n",
    "print(f\"\\nSemantic relevance rank vs Cohen's d:\")\n",
    "print(f\"  {'Rank':<6} {'Condition':<25} {'d':>8}\")\n",
    "print(f\"  {'-'*42}\")\n",
    "for rank, cond, d_val in zip(ranks, cond_labels, ds):\n",
    "    print(f\"  {rank:<6} {cond:<25} {d_val:>+8.3f}\")\n",
    "\n",
    "print(f\"\\n  Spearman rho = {r_spearman:+.3f} (p={p_spearman:.3f})\")\n",
    "print(f\"  Pearson r    = {r_pearson:+.3f} (p={p_pearson:.3f})\")\n",
    "\n",
    "if r_spearman < -0.5 and p_spearman < 0.05:\n",
    "    print(f\"\\n  STRONG CONTENT GRADIENT: More relevant surrogates produce\")\n",
    "    print(f\"  significantly better representations. The bidirectional mechanism\")\n",
    "    print(f\"  is genuinely semantic -- content matters.\")\n",
    "elif r_spearman < -0.3:\n",
    "    print(f\"\\n  MODERATE GRADIENT: Some evidence that content helps, but\")\n",
    "    print(f\"  not a clean monotonic relationship.\")\n",
    "else:\n",
    "    print(f\"\\n  NO CONTENT GRADIENT (like v2): Surrogate content does not\")\n",
    "    print(f\"  predict effect size. The mechanism may be structural rather\")\n",
    "    print(f\"  than semantic, even with bidirectional attention.\")\n",
    "\n",
    "# v2 comparison\n",
    "print(f\"\\n  v2 Mistral (Exp 10): Spearman r=+0.036 (no gradient)\")\n",
    "print(f\"  v3 T5Gemma (this):   Spearman r={r_spearman:+.3f}\")\n",
    "\n",
    "# --- Group comparison: content-specific vs content-agnostic ---\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"GROUP COMPARISON: Content-specific vs Content-agnostic\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "content_specific = ['oracle_trunc', 'surr_para_trunc', 'surr_doc_trunc',\n",
    "                    'surr_lead_trunc', 'surr_template_trunc']\n",
    "content_agnostic = ['static_fact_trunc', 'static_howto_trunc', 'random_trunc']\n",
    "\n",
    "# Per-sample comparison: average NLL across content-specific vs content-agnostic\n",
    "specific_nlls = []\n",
    "agnostic_nlls = []\n",
    "for r in all_results:\n",
    "    spec = np.mean([r[f'nll_{c}'] for c in content_specific])\n",
    "    agn = np.mean([r[f'nll_{c}'] for c in content_agnostic])\n",
    "    specific_nlls.append(spec)\n",
    "    agnostic_nlls.append(agn)\n",
    "\n",
    "specific_nlls = np.array(specific_nlls)\n",
    "agnostic_nlls = np.array(agnostic_nlls)\n",
    "diff_groups = agnostic_nlls - specific_nlls  # positive = specific better\n",
    "\n",
    "d_groups = cohens_d(diff_groups)\n",
    "win_groups = 100 * np.mean(diff_groups > 0)\n",
    "t_groups, p_groups = stats.ttest_1samp(diff_groups, 0)\n",
    "\n",
    "print(f\"\\n  Content-specific (oracle, para, doc_kw, lead, template):\")\n",
    "print(f\"    Mean NLL = {specific_nlls.mean():.4f}\")\n",
    "print(f\"  Content-agnostic (static_fact, static_howto, random):\")\n",
    "print(f\"    Mean NLL = {agnostic_nlls.mean():.4f}\")\n",
    "print(f\"\\n  Difference: d={d_groups:+.3f}, win%={win_groups:.1f}%, p={p_groups:.2e}\")\n",
    "if d_groups > 0.1 and p_groups < 0.05:\n",
    "    print(f\"  --> Content-SPECIFIC surrogates are significantly better.\")\n",
    "    print(f\"      Bidirectional co-encoding is genuinely semantic!\")\n",
    "elif d_groups > 0:\n",
    "    print(f\"  --> Content-specific slightly better but not significant.\")\n",
    "else:\n",
    "    print(f\"  --> Content-agnostic is as good or better (structural mechanism).\")\n",
    "\n",
    "# --- Document-derived vs query-derived ---\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"DOCUMENT-DERIVED vs QUERY-DERIVED (deployment question)\")\n",
    "print(\"=\" * 70)\n",
    "print(\"Can we get good results without any query information?\")\n",
    "\n",
    "doc_derived = ['surr_doc_trunc', 'surr_lead_trunc', 'surr_template_trunc']\n",
    "query_derived = ['surr_para_trunc']  # only para uses query info (oracle excluded -- it IS the query)\n",
    "\n",
    "doc_ds = [analysis[c]['d'] for c in doc_derived if c in analysis]\n",
    "query_ds = [analysis[c]['d'] for c in query_derived if c in analysis]\n",
    "\n",
    "print(f\"\\n  Document-derived surrogates (no query needed at build time):\")\n",
    "for c in doc_derived:\n",
    "    if c in analysis:\n",
    "        print(f\"    {c}: d={analysis[c]['d']:+.3f}\")\n",
    "print(f\"    Mean d: {np.mean(doc_ds):+.3f}\")\n",
    "\n",
    "print(f\"\\n  Query-derived surrogates (needs query proxy at build time):\")\n",
    "for c in query_derived:\n",
    "    if c in analysis:\n",
    "        print(f\"    {c}: d={analysis[c]['d']:+.3f}\")\n",
    "\n",
    "best_doc = max(doc_derived, key=lambda c: analysis.get(c, {}).get('d', -999))\n",
    "best_doc_d = analysis.get(best_doc, {}).get('d', 0)\n",
    "oracle_d_val = analysis.get('oracle_trunc', {}).get('d', 0)\n",
    "pct_of_oracle = best_doc_d / oracle_d_val * 100 if oracle_d_val > 0 else 0\n",
    "\n",
    "print(f\"\\n  Best document-derived: {best_doc} (d={best_doc_d:+.3f}, {pct_of_oracle:.0f}% of oracle)\")\n",
    "print(f\"  --> {'PRACTICAL: ' if pct_of_oracle > 70 else 'NEEDS QUERY INFO: '}\",\n",
    "      f\"{'doc-derived surrogates capture enough of the oracle gap for deployment' if pct_of_oracle > 70 else 'may need query-like surrogates for practical benefit'}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 10: Hardness stratification\n",
    "print(\"=\" * 70)\n",
    "print(\"HARDNESS STRATIFICATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "quintile_bounds = np.percentile(bare_nlls, [20, 40, 60, 80])\n",
    "quintiles = np.digitize(bare_nlls, quintile_bounds)\n",
    "\n",
    "print(f\"\\n{'Quintile':<12} {'N':>4}\", end=\"\")\n",
    "for cond in ['oracle_trunc', 'surr_doc_trunc', 'static_fact_trunc', 'random_trunc']:\n",
    "    print(f\" {cond.replace('_trunc',''):>12}\", end=\"\")\n",
    "print()\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for q in range(5):\n",
    "    mask = quintiles == q\n",
    "    n_q = mask.sum()\n",
    "    if n_q < 3:\n",
    "        continue\n",
    "    qlabel = ['Q1 easy', 'Q2', 'Q3', 'Q4', 'Q5 hard'][q]\n",
    "    print(f\"{qlabel:<12} {n_q:>4}\", end=\"\")\n",
    "\n",
    "    b_q = bare_nlls[mask]\n",
    "    for cond in ['oracle_trunc', 'surr_doc_trunc', 'static_fact_trunc', 'random_trunc']:\n",
    "        c_nlls = np.array([all_results[j][f'nll_{cond}'] for j in range(len(all_results)) if mask[j]])\n",
    "        diff_q = b_q - c_nlls\n",
    "        d_q = cohens_d(diff_q)\n",
    "        print(f\" {d_q:>+12.3f}\", end=\"\")\n",
    "    print()\n",
    "\n",
    "# Hardness-benefit correlation for each condition\n",
    "print(f\"\\n--- Hardness-benefit correlation (r with bare NLL) ---\")\n",
    "for cond in COND_NAMES[1:]:  # skip bare\n",
    "    cond_nlls = np.array([r[f'nll_{cond}'] for r in all_results])\n",
    "    benefit = bare_nlls - cond_nlls\n",
    "    r_hb, p_hb = stats.pearsonr(bare_nlls, benefit)\n",
    "    print(f\"  {cond:<25} r={r_hb:+.3f} (p={p_hb:.2e})\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 11: Pairwise comparisons (direct head-to-head)\n",
    "print(\"=\" * 70)\n",
    "print(\"PAIRWISE HEAD-TO-HEAD COMPARISONS\")\n",
    "print(\"=\" * 70)\n",
    "print(\"Direct comparisons between key surrogate pairs.\\n\")\n",
    "\n",
    "pairs = [\n",
    "    ('oracle_trunc', 'surr_doc_trunc', \"Oracle vs doc keywords (how close?)\"),\n",
    "    ('oracle_trunc', 'surr_para_trunc', \"Oracle vs paraphrase\"),\n",
    "    ('surr_doc_trunc', 'static_fact_trunc', \"Doc keywords vs static (content matters?)\"),\n",
    "    ('surr_doc_trunc', 'surr_lead_trunc', \"Doc keywords vs lead sentence\"),\n",
    "    ('surr_doc_trunc', 'random_trunc', \"Doc keywords vs random (semantic signal?)\"),\n",
    "    ('static_fact_trunc', 'random_trunc', \"Static fact vs random (structural baseline)\"),\n",
    "    ('surr_lead_trunc', 'surr_template_trunc', \"Lead sentence vs question template\"),\n",
    "]\n",
    "\n",
    "print(f\"{'Comparison':<50} {'d':>8} {'Win%':>8} {'p':>12} {'sig':>5}\")\n",
    "print(\"-\" * 85)\n",
    "\n",
    "for cond_a, cond_b, desc in pairs:\n",
    "    nlls_a = np.array([r[f'nll_{cond_a}'] for r in all_results])\n",
    "    nlls_b = np.array([r[f'nll_{cond_b}'] for r in all_results])\n",
    "    diff = nlls_b - nlls_a  # positive = A is better (lower NLL)\n",
    "    d = cohens_d(diff)\n",
    "    win = 100 * np.mean(diff > 0)\n",
    "    t, p = stats.ttest_1samp(diff, 0)\n",
    "    sig = '***' if p < 0.001 else '**' if p < 0.01 else '*' if p < 0.05 else 'ns'\n",
    "    winner = cond_a.replace('_trunc', '') if d > 0 else cond_b.replace('_trunc', '')\n",
    "    print(f\"  {desc:<48} {d:>+8.3f} {win:>7.1f}% {p:>12.2e} {sig:>5}  [{winner}]\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 12: Verdict and save\n",
    "print(\"=\" * 70)\n",
    "print(\"VERDICT -- Exp 02: Surrogate Type Sweep\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\nModel: {MODEL_NAME}\")\n",
    "print(f\"N: {len(all_results)} samples\")\n",
    "\n",
    "# Rank conditions by d\n",
    "ranked = [(c, analysis[c]['d']) for c in COND_NAMES[1:] if 'd' in analysis.get(c, {})]\n",
    "ranked.sort(key=lambda x: -x[1])\n",
    "\n",
    "print(f\"\\n--- Rankings (by Cohen's d) ---\")\n",
    "oracle_d_val = analysis.get('oracle_trunc', {}).get('d', 0)\n",
    "for i, (cond, d_val) in enumerate(ranked, 1):\n",
    "    pct = d_val / oracle_d_val * 100 if oracle_d_val > 0 else 0\n",
    "    print(f\"  {i}. {cond:<25} d={d_val:+.3f} ({pct:.0f}% of oracle)\")\n",
    "\n",
    "# Best document-derived (for Exp 03/04)\n",
    "doc_surrogates = ['surr_doc_trunc', 'surr_lead_trunc', 'surr_template_trunc']\n",
    "best_doc = max(doc_surrogates, key=lambda c: analysis.get(c, {}).get('d', -999))\n",
    "best_doc_d = analysis.get(best_doc, {}).get('d', 0)\n",
    "best_doc_pct = best_doc_d / oracle_d_val * 100 if oracle_d_val > 0 else 0\n",
    "\n",
    "print(f\"\\n--- Recommendation for Exps 03-04 ---\")\n",
    "print(f\"  Best document-derived surrogate: {best_doc}\")\n",
    "print(f\"  d={best_doc_d:+.3f} ({best_doc_pct:.0f}% of oracle)\")\n",
    "\n",
    "# Content gradient conclusion\n",
    "print(f\"\\n--- Content gradient ---\")\n",
    "print(f\"  Spearman rho = {r_spearman:+.3f} (p={p_spearman:.3f})\")\n",
    "if r_spearman < -0.5 and p_spearman < 0.05:\n",
    "    print(f\"  CONTENT GRADIENT CONFIRMED: T5Gemma's bidirectional encoding is semantic.\")\n",
    "    print(f\"  More relevant surrogates genuinely produce better document representations.\")\n",
    "else:\n",
    "    print(f\"  NO CONTENT GRADIENT (same as v2): Content does not predict effect size.\")\n",
    "    print(f\"  The benefit may be structural rather than semantic.\")\n",
    "\n",
    "# Upper/lower/middle bound framing\n",
    "print(f\"\\n--- Three-point spectrum ---\")\n",
    "print(f\"  Upper bound (oracle_trunc):      d={oracle_d_val:+.3f}\")\n",
    "print(f\"  Middle ground (best doc-derived): d={best_doc_d:+.3f} ({best_doc_pct:.0f}%)\")\n",
    "print(f\"  Lower bound (bare):              d=0.000\")\n",
    "print(f\"  Structural floor (random):       d={analysis.get('random_trunc', {}).get('d', 0):+.3f}\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "\n",
    "# Save\n",
    "final_results = {\n",
    "    'experiment': 'exp02_surrogate_type_sweep',\n",
    "    'model': MODEL_NAME,\n",
    "    'n_samples': len(all_results),\n",
    "    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'analysis': analysis,\n",
    "    'content_gradient': {\n",
    "        'spearman_rho': float(r_spearman),\n",
    "        'spearman_p': float(p_spearman),\n",
    "        'pearson_r': float(r_pearson),\n",
    "        'pearson_p': float(p_pearson),\n",
    "    },\n",
    "    'best_doc_derived': best_doc,\n",
    "    'best_doc_derived_d': float(best_doc_d),\n",
    "    'best_doc_derived_pct_oracle': float(best_doc_pct),\n",
    "}\n",
    "\n",
    "with open(RESULTS_DIR / 'results.json', 'w') as f:\n",
    "    json.dump(final_results, f, indent=2)\n",
    "print(f\"\\nResults saved to {RESULTS_DIR / 'results.json'}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 13: Cleanup\n",
    "print(\"Cleaning up GPU memory...\")\n",
    "mem_before = torch.cuda.memory_allocated() / 1e9\n",
    "del model, processor, tokenizer\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "mem_after = torch.cuda.memory_allocated() / 1e9\n",
    "print(f\"GPU memory: {mem_before:.2f} GB -> {mem_after:.2f} GB\")\n",
    "print(\"Done!\")"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
