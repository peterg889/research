{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33a09335",
   "metadata": {},
   "source": [
    "# Experiment 2B: Structural vs Semantic Mechanism Decomposition\n",
    "\n",
    "## Motivation\n",
    "\n",
    "Exp 02 found that the **content of the surrogate barely matters** — random text\n",
    "captures 81% of the oracle benefit, and there is no content gradient\n",
    "(Spearman rho = -0.167, p = 0.693).\n",
    "\n",
    "Yet pairwise comparisons show doc-specific surrogates DO beat random on a\n",
    "per-sample basis (surr_doc > random: d=+0.130, p=0.004). And static surrogates\n",
    "appear to match oracle by Cohen's d despite smaller absolute NLL improvements.\n",
    "\n",
    "Something subtle is going on. This experiment decomposes the mechanism.\n",
    "\n",
    "## v2 vs v3: Different Mechanisms, Same Symptom\n",
    "\n",
    "| Property | v2 (decoder-only) | v3 (encoder-decoder) |\n",
    "|----------|-------------------|---------------------|\n",
    "| Attention | Causal (forward only) | Bidirectional |\n",
    "| Mechanism | Value contamination | Co-encoding |\n",
    "| Truncation effect | Removed benefit | **Improved** benefit |\n",
    "| Document-specific? | No (same contamination for all docs) | Yes (bidirectional) |\n",
    "| Content gradient | None (Exp 10) | None (Exp 02) |\n",
    "\n",
    "Both show no content gradient, but the mechanisms are fundamentally different.\n",
    "\n",
    "## Hypotheses for the Structural Benefit\n",
    "\n",
    "1. **Attention redistribution**: adding tokens changes softmax normalization\n",
    "2. **Position shift**: document tokens at different RoPE positions with prefix\n",
    "3. **Implicit regularization**: prefix = noise injection that improves representations\n",
    "4. **Information injection**: content genuinely flows into document representations\n",
    "\n",
    "## Design\n",
    "\n",
    "**Part 1**: Re-analyze Exp 02 data (no GPU needed)\n",
    "- Document length stratification (does semantic gap change with length?)\n",
    "- Hardness stratification (does semantic advantage emerge for hard samples?)\n",
    "- Variance decomposition (why static has high d but low delta)\n",
    "\n",
    "**Part 2**: Prefix length titration (random words: 1, 3, 5, 10, 20, 50)\n",
    "- Saturation curve: does 1 token suffice (switch) or do we need many (gradual)?\n",
    "\n",
    "**Part 3**: Content ablation (all length-matched to oracle)\n",
    "- `bare → random_matched`: structure (any prefix)\n",
    "- `random_matched → scrambled_oracle`: vocabulary (right words, wrong order)\n",
    "- `scrambled_oracle → oracle`: semantics (right word order, full meaning)\n",
    "\n",
    "**Part 4**: Token diversity (all ~10 words)\n",
    "- `\"the\" x10` vs `doc_keyword x10` vs `diverse random words`\n",
    "- Does diversity matter, or is any 10-token prefix equivalent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3172c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Setup\n",
    "import os\n",
    "os.umask(0o000)\n",
    "\n",
    "import sys, json, time, re, gc\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from scipy import stats\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "sys.path.insert(0, \"../..\")\n",
    "\n",
    "SEED = 42\n",
    "N_SAMPLES = 500\n",
    "MODEL_NAME = \"google/t5gemma-2-4b-4b\"\n",
    "\n",
    "RESULTS_DIR = Path(\"../../results/exp02b\")\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CHECKPOINT_PATH = RESULTS_DIR / \"checkpoint.json\"\n",
    "EXP02_CHECKPOINT = Path(\"../../results/exp02/checkpoint.json\")\n",
    "\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "print(\"Exp 2B: Structural vs Semantic Mechanism Decomposition\")\n",
    "print(f\"N: {N_SAMPLES}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2ff522",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Load Exp 02 data and reconstruct samples\n",
    "from lib.data import count_words\n",
    "from lib.analysis import cohens_d\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load Exp 02 checkpoint (has all NLLs for 9 conditions x 500 samples)\n",
    "print(\"Loading Exp 02 checkpoint...\")\n",
    "exp02_ckpt = json.loads(EXP02_CHECKPOINT.read_text())\n",
    "exp02_results = exp02_ckpt['results']\n",
    "assert len(exp02_results) == N_SAMPLES, f\"Expected {N_SAMPLES}, got {len(exp02_results)}\"\n",
    "\n",
    "# Extract key condition NLLs from Exp 02\n",
    "bare_nlls = np.array([r['nll_bare'] for r in exp02_results])\n",
    "oracle_nlls = np.array([r['nll_oracle_trunc'] for r in exp02_results])\n",
    "random_nlls = np.array([r['nll_random_trunc'] for r in exp02_results])\n",
    "surr_doc_nlls = np.array([r['nll_surr_doc_trunc'] for r in exp02_results])\n",
    "surr_template_nlls = np.array([r['nll_surr_template_trunc'] for r in exp02_results])\n",
    "static_fact_nlls = np.array([r['nll_static_fact_trunc'] for r in exp02_results])\n",
    "surr_lead_nlls = np.array([r['nll_surr_lead_trunc'] for r in exp02_results])\n",
    "surr_para_nlls = np.array([r['nll_surr_para_trunc'] for r in exp02_results])\n",
    "static_howto_nlls = np.array([r['nll_static_howto_trunc'] for r in exp02_results])\n",
    "passage_words = np.array([r['passage_words'] for r in exp02_results])\n",
    "\n",
    "# Pre-compute benefits (positive = condition is better than bare)\n",
    "oracle_benefit = bare_nlls - oracle_nlls\n",
    "random_benefit = bare_nlls - random_nlls\n",
    "surr_doc_benefit = bare_nlls - surr_doc_nlls\n",
    "surr_template_benefit = bare_nlls - surr_template_nlls\n",
    "static_fact_benefit = bare_nlls - static_fact_nlls\n",
    "surr_lead_benefit = bare_nlls - surr_lead_nlls\n",
    "semantic_gap = oracle_benefit - random_benefit  # positive = oracle beats random\n",
    "\n",
    "# Reload dataset to get passage text (needed for new conditions in Parts 2-4)\n",
    "print(\"Loading MS MARCO to reconstruct samples...\")\n",
    "ds = load_dataset(\"microsoft/ms_marco\", \"v1.1\", split=\"validation\")\n",
    "\n",
    "samples = []\n",
    "for item in ds:\n",
    "    if len(samples) >= N_SAMPLES * 3:\n",
    "        break\n",
    "    passages = item.get('passages', {})\n",
    "    ptexts = passages.get('passage_text', [])\n",
    "    is_sel = passages.get('is_selected', [])\n",
    "    query = item.get('query', '')\n",
    "    answers = item.get('answers', [])\n",
    "    well_formed = item.get('wellFormedAnswers', [])\n",
    "    answer = None\n",
    "    if well_formed and len(well_formed) > 0 and well_formed[0] not in ('[]', ''):\n",
    "        answer = well_formed[0]\n",
    "    elif answers and len(answers) > 0 and answers[0] != 'No Answer Present.':\n",
    "        answer = answers[0]\n",
    "    if not answer:\n",
    "        continue\n",
    "    for pt, sel in zip(ptexts, is_sel):\n",
    "        wc = count_words(pt)\n",
    "        if sel == 1 and 30 <= wc <= 300:\n",
    "            samples.append({\n",
    "                'passage': pt, 'query': query, 'answer': answer,\n",
    "                'word_count': wc,\n",
    "            })\n",
    "            break\n",
    "\n",
    "np.random.seed(SEED)\n",
    "np.random.shuffle(samples)\n",
    "samples = samples[:N_SAMPLES]\n",
    "del ds\n",
    "gc.collect()\n",
    "\n",
    "# Verify samples match Exp 02\n",
    "for i in range(min(20, N_SAMPLES)):\n",
    "    assert samples[i]['query'] == exp02_results[i]['query'], \\\n",
    "        f\"Sample {i} query mismatch: {samples[i]['query'][:40]} != {exp02_results[i]['query'][:40]}\"\n",
    "print(f\"Verified: {N_SAMPLES} samples match Exp 02\")\n",
    "print(f\"Document lengths: {passage_words.min()}-{passage_words.max()} words, \"\n",
    "      f\"mean={passage_words.mean():.0f}, median={np.median(passage_words):.0f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab5a938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Part 1a - Document Length Stratification\n",
    "# Does the structural/semantic ratio change with document length?\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PART 1A: DOCUMENT LENGTH STRATIFICATION\")\n",
    "print(\"=\" * 70)\n",
    "print(\"Does the semantic advantage (oracle - random) change with doc length?\\n\")\n",
    "\n",
    "# Use quartiles for even bin sizes\n",
    "quartile_bounds = np.percentile(passage_words, [25, 50, 75])\n",
    "length_bins = np.digitize(passage_words, quartile_bounds)\n",
    "bin_labels = [\n",
    "    f\"Q1 short ({passage_words[length_bins==0].min()}-{passage_words[length_bins==0].max()}w)\",\n",
    "    f\"Q2 ({passage_words[length_bins==1].min()}-{passage_words[length_bins==1].max()}w)\",\n",
    "    f\"Q3 ({passage_words[length_bins==2].min()}-{passage_words[length_bins==2].max()}w)\",\n",
    "    f\"Q4 long ({passage_words[length_bins==3].min()}-{passage_words[length_bins==3].max()}w)\",\n",
    "]\n",
    "\n",
    "print(f\"{'Length bin':<25} {'N':>4} {'Oracle d':>10} {'Random d':>10} \"\n",
    "      f\"{'Doc_kw d':>10} {'Sem gap':>10} {'p(gap>0)':>10}\")\n",
    "print(\"-\" * 85)\n",
    "\n",
    "for q in range(4):\n",
    "    mask = length_bins == q\n",
    "    n = mask.sum()\n",
    "    o_d = cohens_d(oracle_benefit[mask])\n",
    "    r_d = cohens_d(random_benefit[mask])\n",
    "    dk_d = cohens_d(surr_doc_benefit[mask])\n",
    "    gap = semantic_gap[mask]\n",
    "    g_mean = gap.mean()\n",
    "    _, p_gap = stats.ttest_1samp(gap, 0)\n",
    "    sig = '***' if p_gap < 0.001 else '**' if p_gap < 0.01 else '*' if p_gap < 0.05 else 'ns'\n",
    "    print(f\"{bin_labels[q]:<25} {n:>4} {o_d:>+10.3f} {r_d:>+10.3f} \"\n",
    "          f\"{dk_d:>+10.3f} {g_mean:>+10.4f} {p_gap:>9.2e} {sig}\")\n",
    "\n",
    "# Correlation: doc length vs semantic gap\n",
    "r_len, p_len = stats.pearsonr(passage_words, semantic_gap)\n",
    "print(f\"\\nCorrelation: doc_length vs semantic_gap: r={r_len:+.3f} (p={p_len:.3f})\")\n",
    "\n",
    "# Also: does structural (random) benefit scale with length?\n",
    "r_struct, p_struct = stats.pearsonr(passage_words, random_benefit)\n",
    "r_oracle, p_oracle = stats.pearsonr(passage_words, oracle_benefit)\n",
    "print(f\"  doc_length vs random_benefit:  r={r_struct:+.3f} (p={p_struct:.3f})\")\n",
    "print(f\"  doc_length vs oracle_benefit:  r={r_oracle:+.3f} (p={p_oracle:.3f})\")\n",
    "\n",
    "# Interpretation\n",
    "if abs(r_len) < 0.1 and p_len > 0.05:\n",
    "    print(\"\\n  --> Semantic gap is STABLE across document lengths.\")\n",
    "    print(\"      Structural and semantic benefits scale similarly.\")\n",
    "elif r_len > 0.1:\n",
    "    print(\"\\n  --> Semantic gap GROWS with document length.\")\n",
    "    print(\"      Content matters MORE for longer documents.\")\n",
    "else:\n",
    "    print(\"\\n  --> Semantic gap SHRINKS with document length.\")\n",
    "    print(\"      Structural mechanism dominates for longer documents.\")\n",
    "\n",
    "# Check surr_lead anomaly: is it a length artifact?\n",
    "print(f\"\\n--- surr_lead anomaly check ---\")\n",
    "print(f\"surr_lead was weakest at 40% oracle. Is this a surrogate LENGTH artifact?\")\n",
    "# surr_lead uses the first sentence, which is often long (many tokens)\n",
    "# Compare surr_lead to other surrogates by length bin\n",
    "for q in range(4):\n",
    "    mask = length_bins == q\n",
    "    lead_d = cohens_d(surr_lead_benefit[mask])\n",
    "    tmpl_d = cohens_d(surr_template_benefit[mask])\n",
    "    print(f\"  {bin_labels[q]:<25} surr_lead d={lead_d:+.3f}, surr_template d={tmpl_d:+.3f}, \"\n",
    "          f\"gap={tmpl_d - lead_d:+.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4423ed6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Part 1b - Hardness Stratification\n",
    "# Does the semantic advantage (oracle > random) emerge for harder documents?\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PART 1B: HARDNESS STRATIFICATION\")\n",
    "print(\"=\" * 70)\n",
    "print(\"Does the semantic advantage grow or shrink for harder documents?\\n\")\n",
    "\n",
    "quintile_bounds = np.percentile(bare_nlls, [20, 40, 60, 80])\n",
    "quintiles = np.digitize(bare_nlls, quintile_bounds)\n",
    "\n",
    "print(f\"{'Quintile':<12} {'N':>4} {'Bare NLL':>10} {'Oracle d':>10} {'Random d':>10} \"\n",
    "      f\"{'SurrDoc d':>10} {'Sem gap':>10} {'p(gap)':>10}\")\n",
    "print(\"-\" * 85)\n",
    "\n",
    "gap_by_q = []\n",
    "hardness_by_q = []\n",
    "for q in range(5):\n",
    "    mask = quintiles == q\n",
    "    n_q = mask.sum()\n",
    "    qlabel = ['Q1 easy', 'Q2', 'Q3', 'Q4', 'Q5 hard'][q]\n",
    "    bare_q = bare_nlls[mask].mean()\n",
    "\n",
    "    o_d = cohens_d(oracle_benefit[mask])\n",
    "    r_d = cohens_d(random_benefit[mask])\n",
    "    sd_d = cohens_d(surr_doc_benefit[mask])\n",
    "    gap = semantic_gap[mask]\n",
    "    g_mean = gap.mean()\n",
    "    _, p_gap = stats.ttest_1samp(gap, 0)\n",
    "    sig = '***' if p_gap < 0.001 else '**' if p_gap < 0.01 else '*' if p_gap < 0.05 else 'ns'\n",
    "\n",
    "    print(f\"{qlabel:<12} {n_q:>4} {bare_q:>10.3f} {o_d:>+10.3f} {r_d:>+10.3f} \"\n",
    "          f\"{sd_d:>+10.3f} {g_mean:>+10.4f} {p_gap:>9.2e} {sig}\")\n",
    "\n",
    "    gap_by_q.append(g_mean)\n",
    "    hardness_by_q.append(bare_q)\n",
    "\n",
    "# Correlation: hardness vs semantic gap\n",
    "r_hard, p_hard = stats.pearsonr(bare_nlls, semantic_gap)\n",
    "print(f\"\\nCorrelation: hardness vs semantic_gap: r={r_hard:+.3f} (p={p_hard:.3f})\")\n",
    "\n",
    "# Per-sample: when does oracle beat random?\n",
    "oracle_wins = oracle_nlls < random_nlls\n",
    "print(f\"\\nOracle beats random on {oracle_wins.sum()}/{N_SAMPLES} samples \"\n",
    "      f\"({oracle_wins.mean()*100:.1f}%)\")\n",
    "\n",
    "for q in range(5):\n",
    "    mask = quintiles == q\n",
    "    qlabel = ['Q1 easy', 'Q2', 'Q3', 'Q4', 'Q5 hard'][q]\n",
    "    win_rate = oracle_wins[mask].mean() * 100\n",
    "    print(f\"  {qlabel}: oracle beats random {win_rate:.1f}% of the time\")\n",
    "\n",
    "# Key question: for which samples does content help MOST?\n",
    "print(f\"\\n--- Where does content matter? ---\")\n",
    "print(\"Samples where oracle >> random (semantic advantage > median):\")\n",
    "med_gap = np.median(semantic_gap)\n",
    "high_semantic = semantic_gap > med_gap\n",
    "print(f\"  High-semantic samples: mean bare NLL = {bare_nlls[high_semantic].mean():.3f}\")\n",
    "print(f\"  Low-semantic samples:  mean bare NLL = {bare_nlls[~high_semantic].mean():.3f}\")\n",
    "print(f\"  High-semantic samples: mean doc length = {passage_words[high_semantic].mean():.0f} words\")\n",
    "print(f\"  Low-semantic samples:  mean doc length = {passage_words[~high_semantic].mean():.0f} words\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20d3fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Part 1c - Variance Decomposition\n",
    "# Why does static_fact have higher Cohen's d (0.372) despite lower delta (0.418)?\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PART 1C: VARIANCE DECOMPOSITION\")\n",
    "print(\"=\" * 70)\n",
    "print(\"Cohen's d = mean(benefit) / std(benefit).\")\n",
    "print(\"A condition can have HIGH d with LOW mean if its variance is very low.\\n\")\n",
    "\n",
    "all_conds = {\n",
    "    'oracle_trunc': oracle_benefit,\n",
    "    'surr_doc_trunc': surr_doc_benefit,\n",
    "    'surr_template_trunc': surr_template_benefit,\n",
    "    'surr_para_trunc': bare_nlls - surr_para_nlls,\n",
    "    'static_fact_trunc': static_fact_benefit,\n",
    "    'static_howto_trunc': bare_nlls - static_howto_nlls,\n",
    "    'random_trunc': random_benefit,\n",
    "    'surr_lead_trunc': surr_lead_benefit,\n",
    "}\n",
    "\n",
    "print(f\"{'Condition':<25} {'Mean':>10} {'Std':>10} {'d':>8} {'CV':>8} {'Skew':>8} {'IQR':>10}\")\n",
    "print(\"-\" * 85)\n",
    "\n",
    "for name, benefit in all_conds.items():\n",
    "    mu = benefit.mean()\n",
    "    sd = benefit.std(ddof=1)\n",
    "    d = mu / sd if sd > 0 else 0\n",
    "    cv = sd / abs(mu) if abs(mu) > 0 else float('inf')\n",
    "    skew = stats.skew(benefit)\n",
    "    iqr = np.percentile(benefit, 75) - np.percentile(benefit, 25)\n",
    "    print(f\"{name:<25} {mu:>+10.4f} {sd:>10.4f} {d:>+8.3f} {cv:>8.2f} {skew:>+8.2f} {iqr:>10.4f}\")\n",
    "\n",
    "print(f\"\\nKey insight:\")\n",
    "print(f\"  static_fact: mean=+0.418, std={all_conds['static_fact_trunc'].std():.3f} --> d=+0.372\")\n",
    "print(f\"  surr_doc:    mean=+0.620, std={all_conds['surr_doc_trunc'].std():.3f} --> d=+0.322\")\n",
    "print(f\"  static_fact has 48% LESS improvement but even LESS variance.\")\n",
    "print(f\"  The doc-specific surrogates are noisier: they help a lot on some samples,\")\n",
    "print(f\"  very little on others. Static surrogates provide a uniform 'lift'.\")\n",
    "\n",
    "# Cross-condition benefit correlations\n",
    "print(f\"\\n--- Cross-condition correlations ---\")\n",
    "print(f\"How much does knowing one condition's benefit predict another?\")\n",
    "\n",
    "pairs = [\n",
    "    ('oracle', oracle_benefit, 'random', random_benefit),\n",
    "    ('oracle', oracle_benefit, 'surr_doc', surr_doc_benefit),\n",
    "    ('surr_doc', surr_doc_benefit, 'random', random_benefit),\n",
    "    ('static_fact', static_fact_benefit, 'random', random_benefit),\n",
    "    ('oracle', oracle_benefit, 'static_fact', static_fact_benefit),\n",
    "]\n",
    "\n",
    "for name_a, ben_a, name_b, ben_b in pairs:\n",
    "    r, p = stats.pearsonr(ben_a, ben_b)\n",
    "    print(f\"  {name_a:<12} vs {name_b:<12}: r={r:.3f} (p={p:.2e})\")\n",
    "\n",
    "print(f\"\\nIf all benefits are highly correlated, the mechanism is shared (structural).\")\n",
    "print(f\"If doc-specific benefits diverge from random, there is a semantic component.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0479c751",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Load model and define scoring helpers\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv())\n",
    "HF_TOKEN = os.environ.get(\"HF_TOKEN\")\n",
    "\n",
    "from transformers import AutoProcessor, AutoModelForSeq2SeqLM\n",
    "\n",
    "print(f\"Loading {MODEL_NAME}...\")\n",
    "processor = AutoProcessor.from_pretrained(MODEL_NAME, token=HF_TOKEN)\n",
    "tokenizer = processor.tokenizer\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    MODEL_NAME, device_map=\"auto\", torch_dtype=torch.bfloat16, token=HF_TOKEN,\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "DEVICE = next(model.parameters()).device\n",
    "print(f\"Model loaded. dtype={next(model.parameters()).dtype}\")\n",
    "print(f\"GPU memory: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "\n",
    "\n",
    "def score_nll(encoder_text, answer_text, prefix_token_count=0, truncate=False):\n",
    "    # Score NLL of answer given encoder text, with optional prefix truncation.\n",
    "    # Encoder processes full text bidirectionally; if truncate=True, decoder\n",
    "    # cross-attention is masked for the first prefix_token_count positions.\n",
    "    enc_ids = tokenizer(encoder_text, return_tensors=\"pt\",\n",
    "                        add_special_tokens=True, truncation=True,\n",
    "                        max_length=2048).input_ids.to(DEVICE)\n",
    "    total_enc_len = enc_ids.shape[1]\n",
    "    enc_mask = torch.ones(1, total_enc_len, device=DEVICE, dtype=torch.long)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        encoder_outputs = model.get_encoder()(\n",
    "            input_ids=enc_ids, attention_mask=enc_mask\n",
    "        )\n",
    "\n",
    "    if truncate and prefix_token_count > 0:\n",
    "        cross_attn_mask = torch.ones(1, total_enc_len, device=DEVICE, dtype=torch.long)\n",
    "        cross_attn_mask[:, :prefix_token_count] = 0\n",
    "    else:\n",
    "        cross_attn_mask = torch.ones(1, total_enc_len, device=DEVICE, dtype=torch.long)\n",
    "\n",
    "    ans_ids = tokenizer(answer_text, return_tensors=\"pt\",\n",
    "                        add_special_tokens=False, truncation=True,\n",
    "                        max_length=256).input_ids.to(DEVICE)\n",
    "    if ans_ids.shape[1] == 0:\n",
    "        return 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            encoder_outputs=encoder_outputs,\n",
    "            attention_mask=cross_attn_mask,\n",
    "            labels=ans_ids,\n",
    "        )\n",
    "\n",
    "    logits = outputs.logits\n",
    "    log_probs = F.log_softmax(logits, dim=-1)\n",
    "    token_log_probs = log_probs[0].gather(1, ans_ids[0].unsqueeze(1)).squeeze(1)\n",
    "    mean_nll = -token_log_probs.mean().item()\n",
    "\n",
    "    del encoder_outputs, outputs, logits, log_probs\n",
    "    return mean_nll\n",
    "\n",
    "\n",
    "def count_prefix_tokens(prefix_text, document_text):\n",
    "    # Count how many tokens the prefix occupies in [prefix + newline + document].\n",
    "    # Uses BPE-aware subtraction: len(full) - len(doc_only).\n",
    "    full_text = prefix_text + \"\\n\" + document_text\n",
    "    full_ids = tokenizer(full_text, add_special_tokens=True).input_ids\n",
    "    doc_ids = tokenizer(document_text, add_special_tokens=True).input_ids\n",
    "    return len(full_ids) - len(doc_ids)\n",
    "\n",
    "\n",
    "STOP_WORDS = {\n",
    "    'a', 'an', 'the', 'is', 'are', 'was', 'were', 'be', 'been', 'being',\n",
    "    'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could',\n",
    "    'should', 'may', 'might', 'can', 'shall', 'to', 'of', 'in', 'for',\n",
    "    'on', 'with', 'at', 'by', 'from', 'as', 'into', 'through', 'during',\n",
    "    'before', 'after', 'above', 'below', 'between', 'and', 'but', 'or',\n",
    "    'not', 'no', 'if', 'then', 'than', 'so', 'up', 'out', 'about',\n",
    "    'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those',\n",
    "    'it', 'its', 'i', 'me', 'my', 'we', 'our', 'you', 'your', 'he',\n",
    "    'him', 'his', 'she', 'her', 'they', 'them', 'their', 'how', 'when',\n",
    "    'where', 'why', 'much', 'many', 'some', 'any', 'all', 'each',\n",
    "    'does', 'also', 'just', 'more', 'most', 'very', 'too', 'only',\n",
    "}\n",
    "\n",
    "print(\"Helpers defined.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6c4032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Generate surrogate conditions for each sample\n",
    "\n",
    "# ---- Group A: Prefix length titration (random words) ----\n",
    "# For each sample, use words from an unrelated passage at varying word counts.\n",
    "# This gives real text (not garbage tokens) while controlling length.\n",
    "TITRATION_LENGTHS = [1, 3, 5, 10, 20, 50]\n",
    "\n",
    "# ---- Group B: Content ablation (length-matched to oracle) ----\n",
    "# oracle: real query (reuse Exp 02 NLLs)\n",
    "# scrambled: same words as oracle, shuffled randomly\n",
    "# rand_matched: random words, same word count as oracle\n",
    "\n",
    "# ---- Group C: Token diversity (all ~10 words) ----\n",
    "# repeat_the: \"the the the...\" (zero semantic content)\n",
    "# repeat_kw: top document keyword repeated (single concept, doc-specific)\n",
    "# rand_10w: diverse random words (from Group A, reused)\n",
    "\n",
    "for i, s in enumerate(samples):\n",
    "    other_idx = (i + N_SAMPLES // 2) % len(samples)\n",
    "    other_passage = samples[other_idx]['passage']\n",
    "    other_words = other_passage.split()\n",
    "\n",
    "    # Group A: random words at different lengths\n",
    "    for nw in TITRATION_LENGTHS:\n",
    "        key = f'rand_{nw}w'\n",
    "        s[key] = \" \".join(other_words[:nw]) if len(other_words) >= nw else \" \".join(other_words)\n",
    "\n",
    "    # Group B: scrambled oracle (same words, random order)\n",
    "    query_words = s['query'].split()\n",
    "    rng = np.random.RandomState(SEED + i)  # per-sample deterministic shuffle\n",
    "    shuffled = list(query_words)\n",
    "    rng.shuffle(shuffled)\n",
    "    s['scrambled_oracle'] = \" \".join(shuffled)\n",
    "\n",
    "    # Group B: random text, matched to oracle word count\n",
    "    n_query_words = len(query_words)\n",
    "    s['rand_matched'] = \" \".join(other_words[:n_query_words])\n",
    "\n",
    "    # Group C: \"the\" repeated ~10 times\n",
    "    s['repeat_the'] = \" \".join([\"the\"] * 10)\n",
    "\n",
    "    # Group C: top document keyword repeated ~10 times\n",
    "    doc_words = re.sub(r'[^\\w\\s]', '', s['passage'].lower()).split()\n",
    "    content = [w for w in doc_words if w not in STOP_WORDS and len(w) > 2]\n",
    "    if content:\n",
    "        counts = Counter(content)\n",
    "        top_word = counts.most_common(1)[0][0]\n",
    "    else:\n",
    "        top_word = \"information\"\n",
    "    s['repeat_kw'] = \" \".join([top_word] * 10)\n",
    "\n",
    "# Define all new conditions to score\n",
    "NEW_COND_NAMES = []\n",
    "\n",
    "# Group A: prefix length titration\n",
    "for nw in TITRATION_LENGTHS:\n",
    "    NEW_COND_NAMES.append(f'rand_{nw}w_trunc')\n",
    "\n",
    "# Group B: content ablation (oracle reused from Exp 02)\n",
    "NEW_COND_NAMES.append('scrambled_oracle_trunc')\n",
    "NEW_COND_NAMES.append('rand_matched_trunc')\n",
    "\n",
    "# Group C: diversity (rand_10w_trunc from Group A = diverse control)\n",
    "NEW_COND_NAMES.append('repeat_the_trunc')\n",
    "NEW_COND_NAMES.append('repeat_kw_trunc')\n",
    "\n",
    "print(f\"New conditions to score: {len(NEW_COND_NAMES)}\")\n",
    "for c in NEW_COND_NAMES:\n",
    "    print(f\"  {c}\")\n",
    "\n",
    "# Show example\n",
    "ex = samples[0]\n",
    "print(f\"\\nExample (sample 0):\")\n",
    "print(f\"  Query:   {ex['query'][:80]}\")\n",
    "print(f\"  Answer:  {ex['answer'][:80]}\")\n",
    "print(f\"  Passage: {ex['passage'][:80]}...\")\n",
    "print()\n",
    "\n",
    "for c in NEW_COND_NAMES:\n",
    "    key = c.replace('_trunc', '')\n",
    "    text = ex.get(key, '???')\n",
    "    ptoks = count_prefix_tokens(text, ex['passage'])\n",
    "    print(f\"  {c:<28} ({ptoks:>3} prefix toks): {str(text)[:60]}\")\n",
    "\n",
    "# Report token count statistics across a subsample\n",
    "print(f\"\\nPrefix token counts (first 50 samples):\")\n",
    "for c in NEW_COND_NAMES:\n",
    "    key = c.replace('_trunc', '')\n",
    "    toks = [count_prefix_tokens(s[key], s['passage']) for s in samples[:50]]\n",
    "    print(f\"  {c:<28} mean={np.mean(toks):.1f}, range=[{min(toks)}, {max(toks)}]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be80e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Run scoring (with checkpointing)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"RUNNING NEW CONDITIONS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Resume from checkpoint if available\n",
    "new_results = []\n",
    "start_idx = 0\n",
    "\n",
    "if CHECKPOINT_PATH.exists():\n",
    "    ckpt = json.loads(CHECKPOINT_PATH.read_text())\n",
    "    if ckpt.get('n_total') == N_SAMPLES and len(ckpt.get('results', [])) > 0:\n",
    "        saved_queries = [r['query'][:50] for r in ckpt['results']]\n",
    "        current_queries = [s['query'][:50] for s in samples[:len(saved_queries)]]\n",
    "        if saved_queries == current_queries:\n",
    "            new_results = ckpt['results']\n",
    "            start_idx = len(new_results)\n",
    "            print(f\"Resuming from checkpoint: {start_idx}/{N_SAMPLES}\")\n",
    "\n",
    "if start_idx == 0:\n",
    "    print(f\"Starting fresh: {len(NEW_COND_NAMES)} conditions x {N_SAMPLES} samples \"\n",
    "          f\"= {len(NEW_COND_NAMES) * N_SAMPLES} scorings\")\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "for i in tqdm(range(start_idx, N_SAMPLES), initial=start_idx, total=N_SAMPLES,\n",
    "              desc=\"Scoring\"):\n",
    "    s = samples[i]\n",
    "    result = {\n",
    "        'query': s['query'],\n",
    "        'answer': s['answer'],\n",
    "        'passage_words': s['word_count'],\n",
    "    }\n",
    "\n",
    "    for cond_name in NEW_COND_NAMES:\n",
    "        key = cond_name.replace('_trunc', '')\n",
    "        surr_text = s[key]\n",
    "        enc_text = surr_text + \"\\n\" + s['passage']\n",
    "        prefix_count = count_prefix_tokens(surr_text, s['passage'])\n",
    "        nll = score_nll(enc_text, s['answer'], prefix_count, truncate=True)\n",
    "        result[f'nll_{cond_name}'] = nll\n",
    "        result[f'ptoks_{cond_name}'] = prefix_count\n",
    "\n",
    "    new_results.append(result)\n",
    "\n",
    "    if (i + 1) % 20 == 0 or i == N_SAMPLES - 1:\n",
    "        ckpt = {\n",
    "            'n_total': N_SAMPLES,\n",
    "            'results': new_results,\n",
    "            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        }\n",
    "        CHECKPOINT_PATH.write_text(json.dumps(ckpt))\n",
    "        elapsed = time.time() - t0\n",
    "        done = i - start_idx + 1\n",
    "        eta = (N_SAMPLES - i - 1) * elapsed / done if done > 0 else 0\n",
    "        tqdm.write(f\"  Checkpoint {i+1}/{N_SAMPLES} | {elapsed/60:.1f}m | ETA {eta/60:.1f}m\")\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "elapsed = time.time() - t0\n",
    "print(f\"\\nScoring complete: {len(new_results)} samples, \"\n",
    "      f\"{len(NEW_COND_NAMES)} conditions in {elapsed/60:.1f} min\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70967d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Part 2 - Prefix Length Titration\n",
    "# How many random prefix tokens does the encoder need?\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PART 2: PREFIX LENGTH TITRATION\")\n",
    "print(\"=\" * 70)\n",
    "print(\"Random words as prefix, varying from 1 to 50 words.\")\n",
    "print(\"Key question: saturation curve shape.\\n\")\n",
    "\n",
    "titration_conds = [f'rand_{nw}w_trunc' for nw in TITRATION_LENGTHS]\n",
    "oracle_d_val = cohens_d(oracle_benefit)\n",
    "\n",
    "print(f\"{'Condition':<20} {'~Prefix toks':>12} {'Mean NLL':>10} {'Delta':>8} \"\n",
    "      f\"{'d':>8} {'Win%':>7} {'% Oracle':>10}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "titration_ds = []\n",
    "titration_toks = []\n",
    "\n",
    "for cond in titration_conds:\n",
    "    nlls = np.array([r[f'nll_{cond}'] for r in new_results])\n",
    "    ptoks = np.array([r[f'ptoks_{cond}'] for r in new_results])\n",
    "    benefit = bare_nlls - nlls\n",
    "    d = cohens_d(benefit)\n",
    "    delta = benefit.mean()\n",
    "    win = 100 * np.mean(benefit > 0)\n",
    "    pct = d / oracle_d_val * 100 if oracle_d_val > 0 else 0\n",
    "    mean_ptoks = ptoks.mean()\n",
    "\n",
    "    titration_ds.append(d)\n",
    "    titration_toks.append(mean_ptoks)\n",
    "\n",
    "    print(f\"{cond:<20} {mean_ptoks:>12.1f} {nlls.mean():>10.4f} {delta:>+8.4f} \"\n",
    "          f\"{d:>+8.3f} {win:>6.1f}% {pct:>9.1f}%\")\n",
    "\n",
    "# Reference points\n",
    "print(f\"\\n--- Reference points from Exp 02 ---\")\n",
    "print(f\"  bare (0 tokens):            d=  0.000\")\n",
    "rand_exp02_d = cohens_d(random_benefit)\n",
    "print(f\"  random_trunc (~33 toks):    d={rand_exp02_d:>+.3f}\")\n",
    "print(f\"  oracle_trunc (~10 toks):    d={oracle_d_val:>+.3f}\")\n",
    "\n",
    "# Saturation analysis\n",
    "print(f\"\\n--- Saturation analysis ---\")\n",
    "max_d = titration_ds[-1]\n",
    "for i, (nw, d_val) in enumerate(zip(TITRATION_LENGTHS, titration_ds)):\n",
    "    pct_max = d_val / max_d * 100 if max_d > 0 else 0\n",
    "    pct_oracle = d_val / oracle_d_val * 100 if oracle_d_val > 0 else 0\n",
    "    print(f\"  {nw:>2} words ({titration_toks[i]:>5.1f} toks): \"\n",
    "          f\"d={d_val:+.3f} ({pct_max:.0f}% of 50w, {pct_oracle:.0f}% of oracle)\")\n",
    "\n",
    "# Fit: logarithmic vs linear\n",
    "log_toks = np.log(titration_toks)\n",
    "r_log, p_log = stats.pearsonr(log_toks, titration_ds)\n",
    "r_lin, p_lin = stats.pearsonr(titration_toks, titration_ds)\n",
    "print(f\"\\n  Log fit (d ~ log(tokens)):   r={r_log:.3f} (p={p_log:.3f})\")\n",
    "print(f\"  Linear fit (d ~ tokens):     r={r_lin:.3f} (p={p_lin:.3f})\")\n",
    "\n",
    "if titration_ds[0] / max_d > 0.7:\n",
    "    print(f\"\\n  --> SWITCH MECHANISM: 1 word captures >{titration_ds[0]/max_d*100:.0f}% of the benefit.\")\n",
    "    print(f\"      The encoder just needs *something* in the prefix. Like a gate.\")\n",
    "elif r_log > r_lin:\n",
    "    print(f\"\\n  --> LOGARITHMIC scaling: diminishing returns with more tokens.\")\n",
    "    print(f\"      The encoder benefits from prefix but saturates quickly.\")\n",
    "else:\n",
    "    print(f\"\\n  --> LINEAR scaling: benefit grows proportionally with prefix length.\")\n",
    "    print(f\"      More tokens = more attention redistribution = better representations.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba9906a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Part 3 - Content Ablation\n",
    "# Decompose the total benefit into structure + vocabulary + semantics\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PART 3: CONTENT ABLATION\")\n",
    "print(\"=\" * 70)\n",
    "print(\"Decompose: bare -> random_matched -> scrambled_oracle -> oracle\")\n",
    "print(\"  Structure:  bare -> random_matched (any prefix helps)\")\n",
    "print(\"  Vocabulary: random_matched -> scrambled (right words, wrong order)\")\n",
    "print(\"  Semantics:  scrambled -> oracle (right word order, full meaning)\\n\")\n",
    "\n",
    "scrambled_nlls = np.array([r['nll_scrambled_oracle_trunc'] for r in new_results])\n",
    "randmatch_nlls = np.array([r['nll_rand_matched_trunc'] for r in new_results])\n",
    "\n",
    "# Component benefits (all positive = improvement)\n",
    "struct_comp = bare_nlls - randmatch_nlls        # structure\n",
    "vocab_comp = randmatch_nlls - scrambled_nlls     # vocabulary (scrambled has query words)\n",
    "sem_comp = scrambled_nlls - oracle_nlls          # semantics (oracle has correct order)\n",
    "total_comp = bare_nlls - oracle_nlls             # total\n",
    "\n",
    "print(f\"{'Component':<30} {'Mean NLL':>10} {'Delta':>8} {'% total':>9} \"\n",
    "      f\"{'d':>8} {'p':>12} {'sig':>5}\")\n",
    "print(\"-\" * 85)\n",
    "\n",
    "# Per-step NLLs\n",
    "steps = [\n",
    "    ('bare (baseline)', bare_nlls, None),\n",
    "    ('rand_matched_trunc', randmatch_nlls, struct_comp),\n",
    "    ('scrambled_oracle_trunc', scrambled_nlls, vocab_comp),\n",
    "    ('oracle_trunc', oracle_nlls, sem_comp),\n",
    "]\n",
    "\n",
    "total_mean = total_comp.mean()\n",
    "\n",
    "for name, nlls, component in steps:\n",
    "    if component is None:\n",
    "        print(f\"  {name:<28} {nlls.mean():>10.4f}\")\n",
    "        continue\n",
    "    mu = component.mean()\n",
    "    pct = mu / total_mean * 100 if total_mean != 0 else 0\n",
    "    d = cohens_d(component)\n",
    "    _, p = stats.ttest_1samp(component, 0)\n",
    "    sig = '***' if p < 0.001 else '**' if p < 0.01 else '*' if p < 0.05 else 'ns'\n",
    "    label = {'rand_matched_trunc': '+ Structure',\n",
    "             'scrambled_oracle_trunc': '+ Vocabulary',\n",
    "             'oracle_trunc': '+ Semantics'}[name]\n",
    "    print(f\"  {label:<28} {nlls.mean():>10.4f} {mu:>+8.4f} {pct:>8.1f}% \"\n",
    "          f\"{d:>+8.3f} {p:>12.2e} {sig}\")\n",
    "\n",
    "print(f\"  {'TOTAL':<28} {'':>10} {total_mean:>+8.4f} {'100.0%':>9}\")\n",
    "\n",
    "# Verify decomposition sums correctly\n",
    "residual = total_mean - (struct_comp.mean() + vocab_comp.mean() + sem_comp.mean())\n",
    "print(f\"\\n  Decomposition residual: {residual:.6f} (should be ~0)\")\n",
    "\n",
    "# Token count verification (are lengths actually matched?)\n",
    "print(f\"\\n--- Length matching verification ---\")\n",
    "oracle_toks_all = [count_prefix_tokens(s['query'], s['passage']) for s in samples]\n",
    "scrambled_toks_all = [r['ptoks_scrambled_oracle_trunc'] for r in new_results]\n",
    "randmatch_toks_all = [r['ptoks_rand_matched_trunc'] for r in new_results]\n",
    "print(f\"  Oracle:     mean={np.mean(oracle_toks_all):.1f} toks (range {min(oracle_toks_all)}-{max(oracle_toks_all)})\")\n",
    "print(f\"  Scrambled:  mean={np.mean(scrambled_toks_all):.1f} toks (range {min(scrambled_toks_all)}-{max(scrambled_toks_all)})\")\n",
    "print(f\"  RandMatch:  mean={np.mean(randmatch_toks_all):.1f} toks (range {min(randmatch_toks_all)}-{max(randmatch_toks_all)})\")\n",
    "\n",
    "tok_diff = np.abs(np.array(oracle_toks_all) - np.array(scrambled_toks_all))\n",
    "print(f\"  Oracle-Scrambled token diff: mean={tok_diff.mean():.1f}, max={tok_diff.max()}\")\n",
    "\n",
    "# Does vocabulary help MORE for hard samples?\n",
    "print(f\"\\n--- Component x hardness interaction ---\")\n",
    "for name, comp in [('Structure', struct_comp), ('Vocabulary', vocab_comp),\n",
    "                    ('Semantics', sem_comp)]:\n",
    "    r, p = stats.pearsonr(bare_nlls, comp)\n",
    "    print(f\"  {name:<15} vs hardness: r={r:+.3f} (p={p:.2e})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ff593e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Part 4 - Token Diversity\n",
    "# Does token diversity matter, or is any 10-token prefix equivalent?\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PART 4: TOKEN DIVERSITY\")\n",
    "print(\"=\" * 70)\n",
    "print(\"All conditions use ~10 words. Test whether content/diversity matters.\\n\")\n",
    "\n",
    "repeat_the_nlls = np.array([r['nll_repeat_the_trunc'] for r in new_results])\n",
    "repeat_kw_nlls = np.array([r['nll_repeat_kw_trunc'] for r in new_results])\n",
    "rand_10w_nlls = np.array([r['nll_rand_10w_trunc'] for r in new_results])\n",
    "\n",
    "diversity_conds = [\n",
    "    ('repeat_the_trunc', repeat_the_nlls, '\"the\" x10 (zero info, same for all)'),\n",
    "    ('repeat_kw_trunc', repeat_kw_nlls, 'doc keyword x10 (one concept, doc-specific)'),\n",
    "    ('rand_10w_trunc', rand_10w_nlls, '10 diverse random words (from unrelated passage)'),\n",
    "]\n",
    "\n",
    "oracle_d_val = cohens_d(oracle_benefit)\n",
    "\n",
    "print(f\"{'Description':<50} {'NLL':>8} {'Delta':>8} {'d':>8} {'Win%':>7} {'%Orc':>6}\")\n",
    "print(\"-\" * 90)\n",
    "\n",
    "for name, nlls, desc in diversity_conds:\n",
    "    benefit = bare_nlls - nlls\n",
    "    d = cohens_d(benefit)\n",
    "    delta = benefit.mean()\n",
    "    win = 100 * np.mean(benefit > 0)\n",
    "    pct = d / oracle_d_val * 100 if oracle_d_val > 0 else 0\n",
    "    print(f\"  {desc:<48} {nlls.mean():>8.4f} {delta:>+8.4f} {d:>+8.3f} {win:>6.1f}% {pct:>5.0f}%\")\n",
    "\n",
    "# Pairwise comparisons\n",
    "print(f\"\\n--- Pairwise head-to-head ---\")\n",
    "pairs = [\n",
    "    ('\"the\" x10', repeat_the_nlls, '10 diverse random', rand_10w_nlls,\n",
    "     \"Does diversity help?\"),\n",
    "    ('keyword x10', repeat_kw_nlls, '10 diverse random', rand_10w_nlls,\n",
    "     \"Does a relevant keyword beat diverse noise?\"),\n",
    "    ('\"the\" x10', repeat_the_nlls, 'keyword x10', repeat_kw_nlls,\n",
    "     \"Does keyword content help vs pure filler?\"),\n",
    "]\n",
    "\n",
    "for name_a, nlls_a, name_b, nlls_b, question in pairs:\n",
    "    diff = nlls_b - nlls_a  # positive = A is better (lower NLL)\n",
    "    d = cohens_d(diff)\n",
    "    win = 100 * np.mean(diff > 0)\n",
    "    _, p = stats.ttest_1samp(diff, 0)\n",
    "    sig = '***' if p < 0.001 else '**' if p < 0.01 else '*' if p < 0.05 else 'ns'\n",
    "    winner = name_a if d > 0 else name_b\n",
    "    print(f\"  {question}\")\n",
    "    print(f\"    {name_a} vs {name_b}: d={d:+.3f}, win={win:.1f}%, p={p:.2e} {sig} [{winner}]\")\n",
    "\n",
    "# Token counts\n",
    "print(f\"\\n--- Token counts ---\")\n",
    "for name, nlls, desc in diversity_conds:\n",
    "    ptoks = [r.get(f'ptoks_{name}', 0) for r in new_results[:50]]\n",
    "    print(f\"  {name:<25} mean={np.mean(ptoks):.1f} tokens\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd072601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13: Synthesis + Save\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"SYNTHESIS: STRUCTURAL vs SEMANTIC MECHANISM IN T5GEMMA\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# --- 1. Length titration summary ---\n",
    "print(f\"\\n1. PREFIX LENGTH TITRATION:\")\n",
    "titration_ds_final = []\n",
    "for nw in TITRATION_LENGTHS:\n",
    "    cond = f'rand_{nw}w_trunc'\n",
    "    nlls = np.array([r[f'nll_{cond}'] for r in new_results])\n",
    "    titration_ds_final.append(cohens_d(bare_nlls - nlls))\n",
    "\n",
    "one_word_d = titration_ds_final[0]\n",
    "fifty_word_d = titration_ds_final[-1]\n",
    "oracle_d_val = cohens_d(oracle_benefit)\n",
    "one_pct_fifty = one_word_d / fifty_word_d * 100 if fifty_word_d > 0 else 0\n",
    "one_pct_oracle = one_word_d / oracle_d_val * 100 if oracle_d_val > 0 else 0\n",
    "\n",
    "print(f\"   1 random word:  d={one_word_d:+.3f} ({one_pct_fifty:.0f}% of 50-word, \"\n",
    "      f\"{one_pct_oracle:.0f}% of oracle)\")\n",
    "print(f\"   50 random words: d={fifty_word_d:+.3f} ({fifty_word_d/oracle_d_val*100:.0f}% of oracle)\")\n",
    "\n",
    "if one_pct_fifty > 70:\n",
    "    titration_finding = \"SWITCH\"\n",
    "    print(f\"   --> SWITCH: 1 word captures {one_pct_fifty:.0f}% — mechanism is binary (on/off).\")\n",
    "elif one_pct_fifty > 40:\n",
    "    titration_finding = \"GRADUAL\"\n",
    "    print(f\"   --> GRADUAL: benefit scales with prefix length, diminishing returns.\")\n",
    "else:\n",
    "    titration_finding = \"LENGTH_DEPENDENT\"\n",
    "    print(f\"   --> LENGTH_DEPENDENT: needs substantial prefix for full benefit.\")\n",
    "\n",
    "# --- 2. Content ablation summary ---\n",
    "print(f\"\\n2. CONTENT ABLATION:\")\n",
    "struct_pct = struct_comp.mean() / total_comp.mean() * 100\n",
    "vocab_pct = vocab_comp.mean() / total_comp.mean() * 100\n",
    "sem_pct = sem_comp.mean() / total_comp.mean() * 100\n",
    "print(f\"   Structure:  {struct_pct:>5.1f}% of total benefit\")\n",
    "print(f\"   Vocabulary: {vocab_pct:>5.1f}% of total benefit\")\n",
    "print(f\"   Semantics:  {sem_pct:>5.1f}% of total benefit\")\n",
    "\n",
    "if struct_pct > 70:\n",
    "    ablation_finding = \"STRUCTURAL\"\n",
    "    print(f\"   --> Primarily STRUCTURAL — any prefix captures most of the benefit.\")\n",
    "elif struct_pct > 50:\n",
    "    ablation_finding = \"MIXED\"\n",
    "    print(f\"   --> MIXED — structure dominates but content provides meaningful uplift.\")\n",
    "else:\n",
    "    ablation_finding = \"SEMANTIC\"\n",
    "    print(f\"   --> Primarily SEMANTIC — content matters more than structure.\")\n",
    "\n",
    "# --- 3. Diversity summary ---\n",
    "print(f\"\\n3. TOKEN DIVERSITY:\")\n",
    "d_the = cohens_d(bare_nlls - repeat_the_nlls)\n",
    "d_kw = cohens_d(bare_nlls - repeat_kw_nlls)\n",
    "d_diverse = cohens_d(bare_nlls - rand_10w_nlls)\n",
    "print(f\"   'the' x10:      d={d_the:+.3f}\")\n",
    "print(f\"   keyword x10:    d={d_kw:+.3f}\")\n",
    "print(f\"   diverse random: d={d_diverse:+.3f}\")\n",
    "\n",
    "diversity_gap = d_diverse - d_the\n",
    "if abs(diversity_gap) < 0.03:\n",
    "    diversity_finding = \"NO_DIVERSITY_EFFECT\"\n",
    "    print(f\"   --> Diversity does NOT matter (gap={diversity_gap:+.3f})\")\n",
    "else:\n",
    "    diversity_finding = \"DIVERSITY_HELPS\" if diversity_gap > 0 else \"DIVERSITY_HURTS\"\n",
    "    print(f\"   --> Diversity {'helps' if diversity_gap > 0 else 'hurts'} (gap={diversity_gap:+.3f})\")\n",
    "\n",
    "# --- 4. v2 comparison ---\n",
    "print(f\"\\n4. COMPARISON WITH v2 IMPLICIT REGULARIZATION:\")\n",
    "print(f\"   v2 mechanism: value contamination (causal, document-independent)\")\n",
    "print(f\"     - Content didn't matter (same)\")\n",
    "print(f\"     - Truncation REMOVED benefit (DIFFERENT — v3 truncation IMPROVES it)\")\n",
    "print(f\"     - Benefit diluted at ~200 tokens (TBD for v3 — Exp 03)\")\n",
    "print(f\"   v3 mechanism: bidirectional co-encoding (document-specific)\")\n",
    "print(f\"     - Prefix changes document reps via bidirectional self-attention\")\n",
    "print(f\"     - Even with prefix tokens masked from decoder, benefit persists\")\n",
    "print(f\"     - This is NOT value contamination — it is representation enrichment\")\n",
    "\n",
    "# --- Overall conclusion ---\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"CONCLUSION:\")\n",
    "\n",
    "if ablation_finding == \"STRUCTURAL\" and titration_finding == \"SWITCH\":\n",
    "    print(f\"  The mechanism is primarily STRUCTURAL and acts like a SWITCH.\")\n",
    "    print(f\"  Adding any prefix — even 1 random word — triggers a mode shift\")\n",
    "    print(f\"  in the encoder that improves document representations.\")\n",
    "    print(f\"  This is analogous to v2's implicit regularization but operates\")\n",
    "    print(f\"  through bidirectional attention rather than value contamination.\")\n",
    "    same_as_v2 = True\n",
    "elif ablation_finding == \"STRUCTURAL\":\n",
    "    print(f\"  The mechanism is primarily STRUCTURAL but scales with prefix length.\")\n",
    "    print(f\"  The encoder benefits from having more attention targets, suggesting\")\n",
    "    print(f\"  attention redistribution rather than a simple on/off switch.\")\n",
    "    same_as_v2 = False\n",
    "elif ablation_finding == \"MIXED\":\n",
    "    print(f\"  The mechanism is MIXED: a large structural base with a meaningful\")\n",
    "    print(f\"  semantic component. The encoder benefits from any prefix (structure)\")\n",
    "    print(f\"  but extracts additional value from relevant content (semantics).\")\n",
    "    print(f\"  This is DIFFERENT from v2, where content provided zero uplift.\")\n",
    "    same_as_v2 = False\n",
    "else:\n",
    "    print(f\"  The mechanism is primarily SEMANTIC — content matters.\")\n",
    "    print(f\"  This is fundamentally different from v2.\")\n",
    "    same_as_v2 = False\n",
    "\n",
    "print(f\"\\n  Mechanism type: {'Same as v2 (structural)' if same_as_v2 else 'Different from v2'}\")\n",
    "print(f\"  Titration: {titration_finding}\")\n",
    "print(f\"  Ablation: struct={struct_pct:.0f}%, vocab={vocab_pct:.0f}%, sem={sem_pct:.0f}%\")\n",
    "print(f\"  Diversity: {diversity_finding}\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "# --- Save results ---\n",
    "final_results = {\n",
    "    'experiment': 'exp02b_mechanism_decomposition',\n",
    "    'model': MODEL_NAME,\n",
    "    'n_samples': N_SAMPLES,\n",
    "    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'titration': {\n",
    "        str(nw): {\n",
    "            'd': float(titration_ds_final[i]),\n",
    "            'mean_ptoks': float(np.mean([r[f'ptoks_rand_{nw}w_trunc'] for r in new_results])),\n",
    "        }\n",
    "        for i, nw in enumerate(TITRATION_LENGTHS)\n",
    "    },\n",
    "    'ablation': {\n",
    "        'structure_pct': float(struct_pct),\n",
    "        'vocabulary_pct': float(vocab_pct),\n",
    "        'semantics_pct': float(sem_pct),\n",
    "        'structure_d': float(cohens_d(struct_comp)),\n",
    "        'vocabulary_d': float(cohens_d(vocab_comp)),\n",
    "        'semantics_d': float(cohens_d(sem_comp)),\n",
    "    },\n",
    "    'diversity': {\n",
    "        'repeat_the_d': float(d_the),\n",
    "        'repeat_kw_d': float(d_kw),\n",
    "        'diverse_random_d': float(d_diverse),\n",
    "    },\n",
    "    'conclusion': {\n",
    "        'titration_finding': titration_finding,\n",
    "        'ablation_finding': ablation_finding,\n",
    "        'diversity_finding': diversity_finding,\n",
    "        'same_as_v2': same_as_v2,\n",
    "    },\n",
    "}\n",
    "\n",
    "with open(RESULTS_DIR / 'results.json', 'w') as f:\n",
    "    json.dump(final_results, f, indent=2)\n",
    "print(f\"\\nResults saved to {RESULTS_DIR / 'results.json'}\")\n",
    "\n",
    "# Cleanup\n",
    "print(f\"\\nCleaning up GPU memory...\")\n",
    "mem_before = torch.cuda.memory_allocated() / 1e9\n",
    "del model, processor, tokenizer\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "mem_after = torch.cuda.memory_allocated() / 1e9\n",
    "print(f\"GPU memory: {mem_before:.2f} GB -> {mem_after:.2f} GB\")\n",
    "print(\"Done!\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
