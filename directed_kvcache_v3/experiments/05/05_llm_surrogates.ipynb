{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6756bbc",
   "metadata": {},
   "source": [
    "# Experiment 05: LLM-Generated Surrogate Queries\n",
    "\n",
    "## Motivation\n",
    "\n",
    "Exps 2B through 3F established the mechanism:\n",
    "- **~85% structural** (attention sink redistribution, Exp 3E)\n",
    "- **~15% vocabulary** at higher repetitions (Exp 3F)\n",
    "- **~10% word-order semantics**, flat regardless of repetition (Exp 3F)\n",
    "- **Stop words help** — stripping them halves the semantic ratio (Exp 3F)\n",
    "- **Complementary > overlapping** — surr_lead (echoes doc) was worst at 40% (Exp 02)\n",
    "- **Real queries cause interference** on neural-bridge (all surrogates > oracle, Exp 3D)\n",
    "\n",
    "Can an LLM generate surrogates that beat simple heuristics like \"What is [keyword]?\"?\n",
    "\n",
    "## Prompt Design (Informed by Prior Findings)\n",
    "\n",
    "### Prompt A — \"need\" (hypothesis for best)\n",
    "> Write a short web search (5-8 words) someone would type to find this\n",
    "> document. Focus on what the searcher NEEDS, not what the document says.\n",
    "> Use simple everyday language.\n",
    "\n",
    "Rationale: complementary vocabulary (avoids surr_lead overlap), natural stop-word\n",
    "scaffolding (Exp 3F showed stop words help), broad not specific (avoids semantic\n",
    "interference from Exp 3D).\n",
    "\n",
    "### Prompt B — \"question\" (traditional approach)\n",
    "> Write a short question (5-8 words) that this document answers.\n",
    "\n",
    "Rationale: traditional query generation. May cause semantic interference.\n",
    "\n",
    "### Prompt C — \"keywords\" (stop-word-free control)\n",
    "> List 3-5 search keywords for this document, separated by spaces.\n",
    "> Only content words, no filler words.\n",
    "\n",
    "Rationale: tests stop-word scaffolding hypothesis. If keywords underperform\n",
    "need-focused, confirms stop words matter.\n",
    "\n",
    "## Conditions (14)\n",
    "\n",
    "| # | Condition | Prefix | Group |\n",
    "|---|-----------|--------|-------|\n",
    "| 1 | `bare` | (none) | control |\n",
    "| 2 | `oracle_x1_trunc` | query x 1 | upper bound |\n",
    "| 3 | `oracle_x4_trunc` | query x 4 | upper bound + rep |\n",
    "| 4 | `llm_need_x1_trunc` | Prompt A output x 1 | LLM (main) |\n",
    "| 5 | `llm_need_x4_trunc` | Prompt A output x 4 | LLM + rep |\n",
    "| 6 | `llm_question_x1_trunc` | Prompt B output x 1 | LLM (question) |\n",
    "| 7 | `llm_question_x4_trunc` | Prompt B output x 4 | LLM + rep |\n",
    "| 8 | `llm_keywords_x1_trunc` | Prompt C output x 1 | LLM (keywords) |\n",
    "| 9 | `llm_keywords_x4_trunc` | Prompt C output x 4 | LLM + rep |\n",
    "| 10 | `surr_template_x1_trunc` | \"What is [kw]?\" x 1 | heuristic |\n",
    "| 11 | `surr_template_x4_trunc` | \"What is [kw]?\" x 4 | heuristic + rep |\n",
    "| 12 | `random_x1_trunc` | random_matched x 1 | structural ctrl |\n",
    "| 13 | `random_x4_trunc` | random_matched x 4 | structural ctrl + rep |\n",
    "| 14 | `scrambled_llm_need_x4_trunc` | shuffled Prompt A x 4 | decomposition |\n",
    "\n",
    "## Analysis\n",
    "\n",
    "- **Part 1**: LLM vs heuristic (key question)\n",
    "- **Part 2**: Prompt variant comparison (need vs question vs keywords)\n",
    "- **Part 3**: Repetition + vocabulary/semantics decomposition\n",
    "- **Part 4**: Hardness interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a7bb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Setup\n",
    "import os\n",
    "os.umask(0o000)\n",
    "\n",
    "import sys, json, time, re, gc, random as pyrandom\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from scipy import stats\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "sys.path.insert(0, \"../..\")\n",
    "from lib.analysis import cohens_d\n",
    "\n",
    "SEED = 42\n",
    "N_SAMPLES = 500\n",
    "T5GEMMA_NAME = \"google/t5gemma-2-4b-4b\"\n",
    "GEMMA_IT_NAME = \"google/gemma-2-9b-it\"\n",
    "\n",
    "RESULTS_DIR = Path(\"../../results/exp05\")\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "SURROGATES_PATH = RESULTS_DIR / \"surrogates.json\"\n",
    "CHECKPOINT_PATH = RESULTS_DIR / \"checkpoint.json\"\n",
    "EXP02_CHECKPOINT = Path(\"../../results/exp02/checkpoint.json\")\n",
    "\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "pyrandom.seed(SEED)\n",
    "\n",
    "# Prompt templates for Gemma 2 9B-IT\n",
    "PROMPT_NEED = (\n",
    "    \"Write a short web search (5-8 words) someone would type to find this \"\n",
    "    \"document. Focus on what the searcher NEEDS, not what the document says. \"\n",
    "    \"Use simple everyday language.\"\n",
    ")\n",
    "PROMPT_QUESTION = (\n",
    "    \"Write a short question (5-8 words) that this document answers.\"\n",
    ")\n",
    "PROMPT_KEYWORDS = (\n",
    "    \"List 3-5 search keywords for this document, separated by spaces. \"\n",
    "    \"Only content words, no filler words.\"\n",
    ")\n",
    "\n",
    "PROMPT_VARIANTS = {\n",
    "    'need': PROMPT_NEED,\n",
    "    'question': PROMPT_QUESTION,\n",
    "    'keywords': PROMPT_KEYWORDS,\n",
    "}\n",
    "\n",
    "print(\"Exp 05: LLM-Generated Surrogate Queries\")\n",
    "print(f\"N: {N_SAMPLES}\")\n",
    "print(f\"Generation model: {GEMMA_IT_NAME}\")\n",
    "print(f\"Scoring model: {T5GEMMA_NAME}\")\n",
    "print(f\"Prompt variants: {list(PROMPT_VARIANTS.keys())}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b2e587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Load MS MARCO and reconstruct same 500 samples as Exp 02\n",
    "from lib.data import count_words\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load Exp 02 checkpoint for sample alignment verification\n",
    "print(\"Loading Exp 02 checkpoint...\")\n",
    "exp02_ckpt = json.loads(EXP02_CHECKPOINT.read_text())\n",
    "exp02_results = exp02_ckpt['results']\n",
    "assert len(exp02_results) == N_SAMPLES, f\"Expected {N_SAMPLES}, got {len(exp02_results)}\"\n",
    "\n",
    "# Reconstruct dataset samples\n",
    "print(\"Loading MS MARCO to reconstruct samples...\")\n",
    "ds = load_dataset(\"microsoft/ms_marco\", \"v1.1\", split=\"validation\")\n",
    "\n",
    "samples = []\n",
    "for item in ds:\n",
    "    if len(samples) >= N_SAMPLES * 3:\n",
    "        break\n",
    "    passages = item.get('passages', {})\n",
    "    ptexts = passages.get('passage_text', [])\n",
    "    is_sel = passages.get('is_selected', [])\n",
    "    query = item.get('query', '')\n",
    "    answers = item.get('answers', [])\n",
    "    well_formed = item.get('wellFormedAnswers', [])\n",
    "    answer = None\n",
    "    if well_formed and len(well_formed) > 0 and well_formed[0] not in ('[]', ''):\n",
    "        answer = well_formed[0]\n",
    "    elif answers and len(answers) > 0 and answers[0] != 'No Answer Present.':\n",
    "        answer = answers[0]\n",
    "    if not answer:\n",
    "        continue\n",
    "    for pt, sel in zip(ptexts, is_sel):\n",
    "        wc = count_words(pt)\n",
    "        if sel == 1 and 30 <= wc <= 300:\n",
    "            samples.append({\n",
    "                'passage': pt, 'query': query, 'answer': answer,\n",
    "                'word_count': wc,\n",
    "            })\n",
    "            break\n",
    "\n",
    "np.random.seed(SEED)\n",
    "np.random.shuffle(samples)\n",
    "samples = samples[:N_SAMPLES]\n",
    "del ds\n",
    "gc.collect()\n",
    "\n",
    "# Verify samples match Exp 02\n",
    "for i in range(min(20, N_SAMPLES)):\n",
    "    assert samples[i]['query'] == exp02_results[i]['query'], \\\n",
    "        f\"Sample {i} query mismatch: {samples[i]['query'][:40]} != {exp02_results[i]['query'][:40]}\"\n",
    "passage_words = np.array([s['word_count'] for s in samples])\n",
    "print(f\"Verified: {N_SAMPLES} samples match Exp 02\")\n",
    "print(f\"Document lengths: {passage_words.min()}-{passage_words.max()} words, \"\n",
    "      f\"mean={passage_words.mean():.0f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25ab9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Phase 1 — Generate surrogates with Gemma 2 9B-IT\n",
    "# Skip if surrogates already cached\n",
    "\n",
    "if SURROGATES_PATH.exists():\n",
    "    print(\"Loading cached surrogates...\")\n",
    "    surrogates = json.loads(SURROGATES_PATH.read_text())\n",
    "    # Verify alignment\n",
    "    assert len(surrogates) == N_SAMPLES, f\"Expected {N_SAMPLES}, got {len(surrogates)}\"\n",
    "    for i in range(min(10, N_SAMPLES)):\n",
    "        assert surrogates[i]['query'][:50] == samples[i]['query'][:50], \\\n",
    "            f\"Sample {i} query mismatch\"\n",
    "    print(f\"Loaded {len(surrogates)} cached surrogates\")\n",
    "    print(f\"Keys per sample: {list(surrogates[0].keys())}\")\n",
    "else:\n",
    "    print(f\"Loading {GEMMA_IT_NAME} for surrogate generation...\")\n",
    "    from dotenv import load_dotenv, find_dotenv\n",
    "    load_dotenv(find_dotenv())\n",
    "    HF_TOKEN = os.environ.get(\"HF_TOKEN\")\n",
    "\n",
    "    from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "    gen_tokenizer = AutoTokenizer.from_pretrained(GEMMA_IT_NAME, token=HF_TOKEN)\n",
    "    gen_model = AutoModelForCausalLM.from_pretrained(\n",
    "        GEMMA_IT_NAME, device_map=\"auto\", torch_dtype=torch.bfloat16, token=HF_TOKEN,\n",
    "    )\n",
    "    gen_model.eval()\n",
    "    GEN_DEVICE = next(gen_model.parameters()).device\n",
    "    print(f\"Model loaded. GPU memory: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "\n",
    "    def generate_surrogate(passage_text, prompt_text):\n",
    "        # Truncate passage to first 150 words\n",
    "        words = passage_text.split()[:150]\n",
    "        truncated = \" \".join(words)\n",
    "\n",
    "        # Build chat message for Gemma IT\n",
    "        messages = [\n",
    "            {\"role\": \"user\",\n",
    "             \"content\": f\"{prompt_text}\\n\\nDocument:\\n{truncated}\"}\n",
    "        ]\n",
    "        chat_text = gen_tokenizer.apply_chat_template(\n",
    "            messages, tokenize=False, add_generation_prompt=True\n",
    "        )\n",
    "        inputs = gen_tokenizer(chat_text, return_tensors=\"pt\",\n",
    "                               truncation=True, max_length=1024).to(GEN_DEVICE)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output_ids = gen_model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=30,\n",
    "                temperature=0.7,\n",
    "                top_p=0.9,\n",
    "                do_sample=True,\n",
    "            )\n",
    "\n",
    "        # Decode only the new tokens\n",
    "        new_tokens = output_ids[0, inputs['input_ids'].shape[1]:]\n",
    "        raw_text = gen_tokenizer.decode(new_tokens, skip_special_tokens=True)\n",
    "\n",
    "        # Post-process: strip, take first line, remove quotes, truncate to 15 words\n",
    "        cleaned = raw_text.strip().split(\"\\n\")[0].strip()\n",
    "        cleaned = cleaned.strip('\"').strip(\"'\").strip()\n",
    "        cleaned = \" \".join(cleaned.split()[:15])\n",
    "        return cleaned\n",
    "\n",
    "    # Generate with checkpointing\n",
    "    surrogates = []\n",
    "    gen_ckpt_path = RESULTS_DIR / \"gen_checkpoint.json\"\n",
    "\n",
    "    if gen_ckpt_path.exists():\n",
    "        gen_ckpt = json.loads(gen_ckpt_path.read_text())\n",
    "        if gen_ckpt.get('n_total') == N_SAMPLES:\n",
    "            surrogates = gen_ckpt['surrogates']\n",
    "            print(f\"Resuming generation from {len(surrogates)}/{N_SAMPLES}\")\n",
    "\n",
    "    start_gen = len(surrogates)\n",
    "    t0 = time.time()\n",
    "\n",
    "    for i in tqdm(range(start_gen, N_SAMPLES), initial=start_gen, total=N_SAMPLES,\n",
    "                  desc=\"Generating\"):\n",
    "        s = samples[i]\n",
    "        entry = {'query': s['query']}\n",
    "\n",
    "        for variant_name, prompt_text in PROMPT_VARIANTS.items():\n",
    "            torch.manual_seed(SEED + i * 10 + hash(variant_name) % 100)\n",
    "            text = generate_surrogate(s['passage'], prompt_text)\n",
    "            entry[f'llm_{variant_name}'] = text\n",
    "\n",
    "        surrogates.append(entry)\n",
    "\n",
    "        if (i + 1) % 50 == 0 or i == N_SAMPLES - 1:\n",
    "            gen_ckpt = {'n_total': N_SAMPLES, 'surrogates': surrogates,\n",
    "                        'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')}\n",
    "            gen_ckpt_path.write_text(json.dumps(gen_ckpt))\n",
    "            elapsed = time.time() - t0\n",
    "            done = i - start_gen + 1\n",
    "            eta = (N_SAMPLES - i - 1) * elapsed / done if done > 0 else 0\n",
    "            tqdm.write(f\"  Gen checkpoint {i+1}/{N_SAMPLES} | {elapsed/60:.1f}m | ETA {eta/60:.1f}m\")\n",
    "\n",
    "    elapsed = time.time() - t0\n",
    "    print(f\"\\nGeneration complete: {len(surrogates)} samples in {elapsed/60:.1f} min\")\n",
    "\n",
    "    # Save final surrogates\n",
    "    SURROGATES_PATH.write_text(json.dumps(surrogates, indent=2))\n",
    "    print(f\"Saved surrogates to {SURROGATES_PATH}\")\n",
    "\n",
    "    # Free VRAM\n",
    "    print(\"Freeing generation model VRAM...\")\n",
    "    mem_before = torch.cuda.memory_allocated() / 1e9\n",
    "    del gen_model, gen_tokenizer\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    mem_after = torch.cuda.memory_allocated() / 1e9\n",
    "    print(f\"GPU memory: {mem_before:.2f} GB -> {mem_after:.2f} GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a83aaee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Inspect surrogates — examples, word counts, vocabulary overlap\n",
    "\n",
    "STOP_WORDS = {\n",
    "    'a', 'an', 'the', 'is', 'are', 'was', 'were', 'be', 'been', 'being',\n",
    "    'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could',\n",
    "    'should', 'may', 'might', 'can', 'shall', 'to', 'of', 'in', 'for',\n",
    "    'on', 'with', 'at', 'by', 'from', 'as', 'into', 'through', 'during',\n",
    "    'before', 'after', 'above', 'below', 'between', 'and', 'but', 'or',\n",
    "    'not', 'no', 'if', 'then', 'than', 'so', 'up', 'out', 'about',\n",
    "    'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those',\n",
    "    'it', 'its', 'i', 'me', 'my', 'we', 'our', 'you', 'your', 'he',\n",
    "    'him', 'his', 'she', 'her', 'they', 'them', 'their', 'how', 'when',\n",
    "    'where', 'why', 'much', 'many', 'some', 'any', 'all', 'each',\n",
    "    'does', 'also', 'just', 'more', 'most', 'very', 'too', 'only',\n",
    "}\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"SURROGATE INSPECTION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Show 5 examples\n",
    "for idx in range(5):\n",
    "    s = samples[idx]\n",
    "    surr = surrogates[idx]\n",
    "    print(f\"\\n--- Sample {idx} ---\")\n",
    "    print(f\"  Passage: {s['passage'][:100]}...\")\n",
    "    print(f\"  Query:   {s['query']}\")\n",
    "    for variant in PROMPT_VARIANTS:\n",
    "        print(f\"  llm_{variant}: {surr[f'llm_{variant}']}\")\n",
    "\n",
    "# Word count distributions\n",
    "print(f\"\\n--- Word count distributions ---\")\n",
    "for variant in PROMPT_VARIANTS:\n",
    "    wc = [len(surr[f'llm_{variant}'].split()) for surr in surrogates]\n",
    "    wc = np.array(wc)\n",
    "    print(f\"  llm_{variant}: mean={wc.mean():.1f}, median={np.median(wc):.0f}, \"\n",
    "          f\"range=[{wc.min()}, {wc.max()}], std={wc.std():.1f}\")\n",
    "\n",
    "# Oracle query word counts for comparison\n",
    "oracle_wc = np.array([len(s['query'].split()) for s in samples])\n",
    "print(f\"  oracle query: mean={oracle_wc.mean():.1f}, median={np.median(oracle_wc):.0f}, \"\n",
    "      f\"range=[{oracle_wc.min()}, {oracle_wc.max()}]\")\n",
    "\n",
    "# Vocabulary overlap with document (content words only)\n",
    "print(f\"\\n--- Vocabulary overlap with document (content words) ---\")\n",
    "for variant in PROMPT_VARIANTS:\n",
    "    overlaps = []\n",
    "    for i in range(N_SAMPLES):\n",
    "        doc_words = set(re.sub(r'[^\\w\\s]', '', samples[i]['passage'].lower()).split())\n",
    "        doc_content = doc_words - STOP_WORDS\n",
    "        surr_words = set(re.sub(r'[^\\w\\s]', '', surrogates[i][f'llm_{variant}'].lower()).split())\n",
    "        surr_content = surr_words - STOP_WORDS\n",
    "        if len(surr_content) > 0:\n",
    "            overlap = len(surr_content & doc_content) / len(surr_content)\n",
    "        else:\n",
    "            overlap = 0.0\n",
    "        overlaps.append(overlap)\n",
    "    overlaps = np.array(overlaps)\n",
    "    print(f\"  llm_{variant}: mean={overlaps.mean():.3f}, median={np.median(overlaps):.3f}\")\n",
    "\n",
    "# Oracle overlap for comparison\n",
    "oracle_overlaps = []\n",
    "for i in range(N_SAMPLES):\n",
    "    doc_words = set(re.sub(r'[^\\w\\s]', '', samples[i]['passage'].lower()).split())\n",
    "    doc_content = doc_words - STOP_WORDS\n",
    "    q_words = set(re.sub(r'[^\\w\\s]', '', samples[i]['query'].lower()).split())\n",
    "    q_content = q_words - STOP_WORDS\n",
    "    if len(q_content) > 0:\n",
    "        overlap = len(q_content & doc_content) / len(q_content)\n",
    "    else:\n",
    "        overlap = 0.0\n",
    "    oracle_overlaps.append(overlap)\n",
    "print(f\"  oracle query: mean={np.mean(oracle_overlaps):.3f}, \"\n",
    "      f\"median={np.median(oracle_overlaps):.3f}\")\n",
    "\n",
    "# Stop word fraction in each variant\n",
    "print(f\"\\n--- Stop word fraction ---\")\n",
    "for variant in PROMPT_VARIANTS:\n",
    "    fracs = []\n",
    "    for surr in surrogates:\n",
    "        words = surr[f'llm_{variant}'].lower().split()\n",
    "        if len(words) > 0:\n",
    "            frac = sum(1 for w in words if w in STOP_WORDS) / len(words)\n",
    "        else:\n",
    "            frac = 0.0\n",
    "        fracs.append(frac)\n",
    "    fracs = np.array(fracs)\n",
    "    print(f\"  llm_{variant}: mean={fracs.mean():.3f}\")\n",
    "\n",
    "oracle_fracs = []\n",
    "for s in samples:\n",
    "    words = s['query'].lower().split()\n",
    "    if len(words) > 0:\n",
    "        frac = sum(1 for w in words if w in STOP_WORDS) / len(words)\n",
    "    else:\n",
    "        frac = 0.0\n",
    "    oracle_fracs.append(frac)\n",
    "print(f\"  oracle query: mean={np.mean(oracle_fracs):.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61fc790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Phase 2 — Load T5Gemma and define scoring helpers\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv())\n",
    "HF_TOKEN = os.environ.get(\"HF_TOKEN\")\n",
    "\n",
    "from transformers import AutoProcessor, AutoModelForSeq2SeqLM\n",
    "\n",
    "print(f\"Loading {T5GEMMA_NAME}...\")\n",
    "processor = AutoProcessor.from_pretrained(T5GEMMA_NAME, token=HF_TOKEN)\n",
    "tokenizer = processor.tokenizer\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    T5GEMMA_NAME, device_map=\"auto\", torch_dtype=torch.bfloat16, token=HF_TOKEN,\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "DEVICE = next(model.parameters()).device\n",
    "print(f\"Model loaded. dtype={next(model.parameters()).dtype}\")\n",
    "print(f\"GPU memory: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "\n",
    "\n",
    "def score_nll(encoder_text, answer_text, prefix_token_count=0, truncate=False):\n",
    "    # Score NLL of answer given encoder text, with optional prefix truncation.\n",
    "    enc_ids = tokenizer(encoder_text, return_tensors=\"pt\",\n",
    "                        add_special_tokens=True, truncation=True,\n",
    "                        max_length=2048).input_ids.to(DEVICE)\n",
    "    total_enc_len = enc_ids.shape[1]\n",
    "    enc_mask = torch.ones(1, total_enc_len, device=DEVICE, dtype=torch.long)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        encoder_outputs = model.get_encoder()(\n",
    "            input_ids=enc_ids, attention_mask=enc_mask\n",
    "        )\n",
    "\n",
    "    if truncate and prefix_token_count > 0:\n",
    "        cross_attn_mask = torch.ones(1, total_enc_len, device=DEVICE, dtype=torch.long)\n",
    "        cross_attn_mask[:, :prefix_token_count] = 0\n",
    "    else:\n",
    "        cross_attn_mask = torch.ones(1, total_enc_len, device=DEVICE, dtype=torch.long)\n",
    "\n",
    "    ans_ids = tokenizer(answer_text, return_tensors=\"pt\",\n",
    "                        add_special_tokens=False, truncation=True,\n",
    "                        max_length=256).input_ids.to(DEVICE)\n",
    "    if ans_ids.shape[1] == 0:\n",
    "        return 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            encoder_outputs=encoder_outputs,\n",
    "            attention_mask=cross_attn_mask,\n",
    "            labels=ans_ids,\n",
    "        )\n",
    "\n",
    "    logits = outputs.logits\n",
    "    log_probs = F.log_softmax(logits, dim=-1)\n",
    "    token_log_probs = log_probs[0].gather(1, ans_ids[0].unsqueeze(1)).squeeze(1)\n",
    "    mean_nll = -token_log_probs.mean().item()\n",
    "\n",
    "    del encoder_outputs, outputs, logits, log_probs\n",
    "    return mean_nll\n",
    "\n",
    "\n",
    "def count_prefix_tokens(prefix_text, document_text):\n",
    "    # Count how many tokens the prefix occupies in [prefix + newline + document].\n",
    "    full_text = prefix_text + \"\\n\" + document_text\n",
    "    full_ids = tokenizer(full_text, add_special_tokens=True, truncation=True,\n",
    "                         max_length=2048).input_ids\n",
    "    doc_ids = tokenizer(document_text, add_special_tokens=True, truncation=True,\n",
    "                        max_length=2048).input_ids\n",
    "    return len(full_ids) - len(doc_ids)\n",
    "\n",
    "\n",
    "print(\"Helpers defined.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76009a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Generate all 14 scoring conditions per sample\n",
    "\n",
    "# Pre-compute per-sample data\n",
    "for i, s in enumerate(samples):\n",
    "    surr = surrogates[i]\n",
    "    query = s['query']\n",
    "    passage = s['passage']\n",
    "\n",
    "    # Random text from unrelated passage (same as Exp 02/2B/3F)\n",
    "    other_idx = (i + N_SAMPLES // 2) % len(samples)\n",
    "    other_words = samples[other_idx]['passage'].split()\n",
    "    query_words = query.split()\n",
    "    random_matched = \" \".join(other_words[:len(query_words)])\n",
    "\n",
    "    # Template surrogate: \"What is [keyword]?\"\n",
    "    doc_words_clean = re.sub(r'[^\\w\\s]', '', passage.lower()).split()\n",
    "    content = [w for w in doc_words_clean if w not in STOP_WORDS and len(w) > 2]\n",
    "    if content:\n",
    "        kw = Counter(content).most_common(1)[0][0]\n",
    "    else:\n",
    "        kw = \"information\"\n",
    "\n",
    "    # Scrambled LLM need output (for decomposition)\n",
    "    need_words = surr['llm_need'].split()\n",
    "    rng = np.random.RandomState(SEED + i)\n",
    "    shuffled_need = list(need_words)\n",
    "    rng.shuffle(shuffled_need)\n",
    "\n",
    "    # Store all prefix texts\n",
    "    s['oracle_x1'] = query\n",
    "    s['oracle_x4'] = \" \".join([query] * 4)\n",
    "    s['llm_need_x1'] = surr['llm_need']\n",
    "    s['llm_need_x4'] = \" \".join([surr['llm_need']] * 4)\n",
    "    s['llm_question_x1'] = surr['llm_question']\n",
    "    s['llm_question_x4'] = \" \".join([surr['llm_question']] * 4)\n",
    "    s['llm_keywords_x1'] = surr['llm_keywords']\n",
    "    s['llm_keywords_x4'] = \" \".join([surr['llm_keywords']] * 4)\n",
    "    s['surr_template_x1'] = f\"What is {kw}?\"\n",
    "    s['surr_template_x4'] = \" \".join([f\"What is {kw}?\"] * 4)\n",
    "    s['random_x1'] = random_matched\n",
    "    s['random_x4'] = \" \".join([random_matched] * 4)\n",
    "    s['scrambled_llm_need_x4'] = \" \".join([\" \".join(shuffled_need)] * 4)\n",
    "\n",
    "# Define all scoring conditions\n",
    "COND_NAMES = [\n",
    "    'bare',\n",
    "    'oracle_x1_trunc',\n",
    "    'oracle_x4_trunc',\n",
    "    'llm_need_x1_trunc',\n",
    "    'llm_need_x4_trunc',\n",
    "    'llm_question_x1_trunc',\n",
    "    'llm_question_x4_trunc',\n",
    "    'llm_keywords_x1_trunc',\n",
    "    'llm_keywords_x4_trunc',\n",
    "    'surr_template_x1_trunc',\n",
    "    'surr_template_x4_trunc',\n",
    "    'random_x1_trunc',\n",
    "    'random_x4_trunc',\n",
    "    'scrambled_llm_need_x4_trunc',\n",
    "]\n",
    "\n",
    "print(f\"Conditions ({len(COND_NAMES)}):\")\n",
    "for c in COND_NAMES:\n",
    "    print(f\"  {c}\")\n",
    "\n",
    "# Show example\n",
    "ex = samples[0]\n",
    "print(f\"\\nExample (sample 0):\")\n",
    "print(f\"  Query:   {ex['query'][:80]}\")\n",
    "print(f\"  Answer:  {ex['answer'][:80]}\")\n",
    "print(f\"  Passage: {ex['passage'][:80]}...\")\n",
    "print()\n",
    "for c in COND_NAMES:\n",
    "    if c == 'bare':\n",
    "        print(f\"  {c:<35}: [document only]\")\n",
    "    else:\n",
    "        key = c.replace('_trunc', '')\n",
    "        text = ex[key]\n",
    "        ptoks = count_prefix_tokens(text, ex['passage'])\n",
    "        print(f\"  {c:<35} ({ptoks:>3} toks): {str(text)[:55]}\")\n",
    "\n",
    "# Token count stats across first 50 samples\n",
    "print(f\"\\nPrefix token counts (first 50 samples):\")\n",
    "for c in COND_NAMES:\n",
    "    if c == 'bare':\n",
    "        continue\n",
    "    key = c.replace('_trunc', '')\n",
    "    toks = [count_prefix_tokens(s[key], s['passage']) for s in samples[:50]]\n",
    "    print(f\"  {c:<35} mean={np.mean(toks):.1f}, range=[{min(toks)}, {max(toks)}]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85f2ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Scoring loop with checkpointing\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"SCORING ALL CONDITIONS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "results = []\n",
    "start_idx = 0\n",
    "\n",
    "if CHECKPOINT_PATH.exists():\n",
    "    ckpt = json.loads(CHECKPOINT_PATH.read_text())\n",
    "    if ckpt.get('n_total') == N_SAMPLES and len(ckpt.get('results', [])) > 0:\n",
    "        saved_queries = [r['query'][:50] for r in ckpt['results']]\n",
    "        current_queries = [s['query'][:50] for s in samples[:len(saved_queries)]]\n",
    "        if saved_queries == current_queries:\n",
    "            results = ckpt['results']\n",
    "            start_idx = len(results)\n",
    "            print(f\"Resuming from checkpoint: {start_idx}/{N_SAMPLES}\")\n",
    "\n",
    "if start_idx == 0:\n",
    "    print(f\"Starting fresh: {len(COND_NAMES)} conditions x {N_SAMPLES} samples \"\n",
    "          f\"= {len(COND_NAMES) * N_SAMPLES} scorings\")\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "for i in tqdm(range(start_idx, N_SAMPLES), initial=start_idx, total=N_SAMPLES,\n",
    "              desc=\"Scoring\"):\n",
    "    s = samples[i]\n",
    "    result = {\n",
    "        'query': s['query'],\n",
    "        'answer': s['answer'],\n",
    "        'passage_words': s['word_count'],\n",
    "    }\n",
    "\n",
    "    for cond in COND_NAMES:\n",
    "        if cond == 'bare':\n",
    "            nll = score_nll(s['passage'], s['answer'])\n",
    "            result['nll_bare'] = nll\n",
    "        else:\n",
    "            key = cond.replace('_trunc', '')\n",
    "            prefix = s[key]\n",
    "            enc_text = prefix + \"\\n\" + s['passage']\n",
    "            ptoks = count_prefix_tokens(prefix, s['passage'])\n",
    "            nll = score_nll(enc_text, s['answer'], ptoks, truncate=True)\n",
    "            result[f'nll_{cond}'] = nll\n",
    "            result[f'ptoks_{cond}'] = ptoks\n",
    "\n",
    "    results.append(result)\n",
    "\n",
    "    if (i + 1) % 20 == 0 or i == N_SAMPLES - 1:\n",
    "        ckpt = {\n",
    "            'n_total': N_SAMPLES,\n",
    "            'results': results,\n",
    "            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        }\n",
    "        CHECKPOINT_PATH.write_text(json.dumps(ckpt))\n",
    "        elapsed = time.time() - t0\n",
    "        done = i - start_idx + 1\n",
    "        eta = (N_SAMPLES - i - 1) * elapsed / done if done > 0 else 0\n",
    "        tqdm.write(f\"  Checkpoint {i+1}/{N_SAMPLES} | {elapsed/60:.1f}m | ETA {eta/60:.1f}m\")\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "elapsed = time.time() - t0\n",
    "print(f\"\\nScoring complete: {len(results)} samples, \"\n",
    "      f\"{len(COND_NAMES)} conditions in {elapsed/60:.1f} min\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8958f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Part 1 — LLM vs Heuristic (the key question)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PART 1: LLM vs HEURISTIC\")\n",
    "print(\"=\" * 70)\n",
    "print(\"Key question: does LLM-generated surrogate beat 'What is [keyword]?'\\n\")\n",
    "\n",
    "# Extract NLLs\n",
    "bare_nlls = np.array([r['nll_bare'] for r in results])\n",
    "oracle_x1_nlls = np.array([r['nll_oracle_x1_trunc'] for r in results])\n",
    "oracle_x4_nlls = np.array([r['nll_oracle_x4_trunc'] for r in results])\n",
    "\n",
    "# Verify against Exp 02\n",
    "exp02_bare = np.array([r['nll_bare'] for r in exp02_results])\n",
    "exp02_oracle = np.array([r['nll_oracle_trunc'] for r in exp02_results])\n",
    "bare_diff = np.abs(bare_nlls - exp02_bare)\n",
    "oracle_diff = np.abs(oracle_x1_nlls - exp02_oracle)\n",
    "print(f\"--- Verification against Exp 02 ---\")\n",
    "print(f\"  bare NLL max diff: {bare_diff.max():.6f} (should be ~0)\")\n",
    "print(f\"  oracle_x1 vs Exp02 oracle max diff: {oracle_diff.max():.6f} (should be ~0)\")\n",
    "\n",
    "# All conditions table\n",
    "oracle_x1_d = cohens_d(bare_nlls - oracle_x1_nlls)\n",
    "\n",
    "all_conds = [\n",
    "    ('oracle_x1_trunc', 'Real query x1 (upper bound)'),\n",
    "    ('oracle_x4_trunc', 'Real query x4'),\n",
    "    ('llm_need_x1_trunc', 'LLM need x1'),\n",
    "    ('llm_need_x4_trunc', 'LLM need x4'),\n",
    "    ('llm_question_x1_trunc', 'LLM question x1'),\n",
    "    ('llm_question_x4_trunc', 'LLM question x4'),\n",
    "    ('llm_keywords_x1_trunc', 'LLM keywords x1'),\n",
    "    ('llm_keywords_x4_trunc', 'LLM keywords x4'),\n",
    "    ('surr_template_x1_trunc', '\"What is [kw]?\" x1'),\n",
    "    ('surr_template_x4_trunc', '\"What is [kw]?\" x4'),\n",
    "    ('random_x1_trunc', 'Random matched x1'),\n",
    "    ('random_x4_trunc', 'Random matched x4'),\n",
    "    ('scrambled_llm_need_x4_trunc', 'Scrambled LLM need x4'),\n",
    "]\n",
    "\n",
    "# Bonferroni threshold\n",
    "alpha_bonf = 0.05 / len(all_conds)\n",
    "\n",
    "print(f\"\\n{'Condition':<40} {'NLL':>8} {'Delta':>8} {'d':>8} \"\n",
    "      f\"{'Win%':>7} {'%Orc':>6} {'p':>12} {'sig':>5}\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "for cond, desc in all_conds:\n",
    "    nlls = np.array([r[f'nll_{cond}'] for r in results])\n",
    "    benefit = bare_nlls - nlls\n",
    "    d = cohens_d(benefit)\n",
    "    delta = benefit.mean()\n",
    "    win = 100 * np.mean(benefit > 0)\n",
    "    pct = d / oracle_x1_d * 100 if oracle_x1_d > 0 else 0\n",
    "    _, p = stats.ttest_1samp(benefit, 0)\n",
    "    sig = '***' if p < alpha_bonf / 10 else '**' if p < alpha_bonf else '*' if p < 0.05 else 'ns'\n",
    "    print(f\"  {desc:<38} {nlls.mean():>8.4f} {delta:>+8.4f} {d:>+8.3f} \"\n",
    "          f\"{win:>6.1f}% {pct:>5.0f}% {p:>12.2e} {sig}\")\n",
    "\n",
    "print(f\"\\n  bare (lower bound): {bare_nlls.mean():.4f}\")\n",
    "print(f\"  Bonferroni threshold: alpha={alpha_bonf:.4f}\")\n",
    "\n",
    "# Head-to-head: LLM need vs surr_template\n",
    "print(f\"\\n--- Head-to-head: LLM need vs surr_template ---\")\n",
    "for rep in ['x1', 'x4']:\n",
    "    need_nlls = np.array([r[f'nll_llm_need_{rep}_trunc'] for r in results])\n",
    "    tmpl_nlls = np.array([r[f'nll_surr_template_{rep}_trunc'] for r in results])\n",
    "    diff = tmpl_nlls - need_nlls  # positive = need is better\n",
    "    d = cohens_d(diff)\n",
    "    win = 100 * np.mean(diff > 0)\n",
    "    _, p = stats.ttest_1samp(diff, 0)\n",
    "    sig = '***' if p < 0.001 else '**' if p < 0.01 else '*' if p < 0.05 else 'ns'\n",
    "    winner = 'llm_need' if d > 0 else 'surr_template'\n",
    "    print(f\"  {rep}: llm_need vs surr_template: d={d:+.3f}, win={win:.1f}%, \"\n",
    "          f\"p={p:.2e} {sig} [{winner}]\")\n",
    "\n",
    "# Head-to-head: LLM need vs random (is there semantic uplift?)\n",
    "print(f\"\\n--- Head-to-head: LLM need vs random ---\")\n",
    "for rep in ['x1', 'x4']:\n",
    "    need_nlls = np.array([r[f'nll_llm_need_{rep}_trunc'] for r in results])\n",
    "    rand_nlls = np.array([r[f'nll_random_{rep}_trunc'] for r in results])\n",
    "    diff = rand_nlls - need_nlls  # positive = need is better\n",
    "    d = cohens_d(diff)\n",
    "    win = 100 * np.mean(diff > 0)\n",
    "    _, p = stats.ttest_1samp(diff, 0)\n",
    "    sig = '***' if p < 0.001 else '**' if p < 0.01 else '*' if p < 0.05 else 'ns'\n",
    "    winner = 'llm_need' if d > 0 else 'random'\n",
    "    print(f\"  {rep}: llm_need vs random: d={d:+.3f}, win={win:.1f}%, \"\n",
    "          f\"p={p:.2e} {sig} [{winner}]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5905cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Part 2 — Prompt Variant Comparison\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PART 2: PROMPT VARIANT COMPARISON\")\n",
    "print(\"=\" * 70)\n",
    "print(\"Which prompt style generates the best surrogates?\\n\")\n",
    "\n",
    "for rep in ['x1', 'x4']:\n",
    "    print(f\"--- {rep} ---\")\n",
    "    variants = ['llm_need', 'llm_question', 'llm_keywords']\n",
    "\n",
    "    # Pairwise comparisons\n",
    "    for j in range(len(variants)):\n",
    "        for k in range(j + 1, len(variants)):\n",
    "            a_name = variants[j]\n",
    "            b_name = variants[k]\n",
    "            a_nlls = np.array([r[f'nll_{a_name}_{rep}_trunc'] for r in results])\n",
    "            b_nlls = np.array([r[f'nll_{b_name}_{rep}_trunc'] for r in results])\n",
    "            diff = b_nlls - a_nlls  # positive = A is better\n",
    "            d = cohens_d(diff)\n",
    "            win = 100 * np.mean(diff > 0)\n",
    "            _, p = stats.ttest_1samp(diff, 0)\n",
    "            sig = '***' if p < 0.001 else '**' if p < 0.01 else '*' if p < 0.05 else 'ns'\n",
    "            winner = a_name if d > 0 else b_name\n",
    "            print(f\"  {a_name} vs {b_name}: d={d:+.3f}, win={win:.1f}%, \"\n",
    "                  f\"p={p:.2e} {sig} [{winner}]\")\n",
    "    print()\n",
    "\n",
    "# Do keywords underperform need? (tests stop-word hypothesis)\n",
    "print(\"--- Stop-word hypothesis test ---\")\n",
    "for rep in ['x1', 'x4']:\n",
    "    need_nlls = np.array([r[f'nll_llm_need_{rep}_trunc'] for r in results])\n",
    "    kw_nlls = np.array([r[f'nll_llm_keywords_{rep}_trunc'] for r in results])\n",
    "    diff = kw_nlls - need_nlls  # positive = need is better\n",
    "    d = cohens_d(diff)\n",
    "    _, p = stats.ttest_1samp(diff, 0)\n",
    "    sig = '***' if p < 0.001 else '**' if p < 0.01 else '*' if p < 0.05 else 'ns'\n",
    "    if d > 0 and p < 0.05:\n",
    "        verdict = \"CONFIRMED: need > keywords (stop words help)\"\n",
    "    elif d < 0 and p < 0.05:\n",
    "        verdict = \"REFUTED: keywords > need (stop words hurt)\"\n",
    "    else:\n",
    "        verdict = \"INCONCLUSIVE: no significant difference\"\n",
    "    print(f\"  {rep}: need vs keywords: d={d:+.3f}, p={p:.2e} {sig}\")\n",
    "    print(f\"    {verdict}\")\n",
    "\n",
    "# Does question framing cause semantic interference?\n",
    "print(f\"\\n--- Semantic interference test ---\")\n",
    "for rep in ['x1', 'x4']:\n",
    "    need_nlls = np.array([r[f'nll_llm_need_{rep}_trunc'] for r in results])\n",
    "    q_nlls = np.array([r[f'nll_llm_question_{rep}_trunc'] for r in results])\n",
    "    diff = q_nlls - need_nlls  # positive = need is better\n",
    "    d = cohens_d(diff)\n",
    "    _, p = stats.ttest_1samp(diff, 0)\n",
    "    sig = '***' if p < 0.001 else '**' if p < 0.01 else '*' if p < 0.05 else 'ns'\n",
    "    if d > 0 and p < 0.05:\n",
    "        verdict = \"CONFIRMED: need > question (question causes interference)\"\n",
    "    elif d < 0 and p < 0.05:\n",
    "        verdict = \"REFUTED: question > need\"\n",
    "    else:\n",
    "        verdict = \"INCONCLUSIVE: no significant difference\"\n",
    "    print(f\"  {rep}: need vs question: d={d:+.3f}, p={p:.2e} {sig}\")\n",
    "    print(f\"    {verdict}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78987340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Part 3 — Repetition + Vocabulary/Semantics Decomposition\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PART 3: REPETITION + DECOMPOSITION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# x1 vs x4 for each condition\n",
    "print(\"--- x1 vs x4 ---\")\n",
    "print(f\"  {'Condition':<25} {'x1 d':>8} {'x4 d':>8} {'Uplift':>8} {'p':>12} {'sig':>5}\")\n",
    "print(f\"  {'-'*65}\")\n",
    "\n",
    "paired_conds = [\n",
    "    ('oracle', 'oracle_x1_trunc', 'oracle_x4_trunc'),\n",
    "    ('llm_need', 'llm_need_x1_trunc', 'llm_need_x4_trunc'),\n",
    "    ('llm_question', 'llm_question_x1_trunc', 'llm_question_x4_trunc'),\n",
    "    ('llm_keywords', 'llm_keywords_x1_trunc', 'llm_keywords_x4_trunc'),\n",
    "    ('surr_template', 'surr_template_x1_trunc', 'surr_template_x4_trunc'),\n",
    "    ('random', 'random_x1_trunc', 'random_x4_trunc'),\n",
    "]\n",
    "\n",
    "for name, x1_cond, x4_cond in paired_conds:\n",
    "    x1_nlls = np.array([r[f'nll_{x1_cond}'] for r in results])\n",
    "    x4_nlls = np.array([r[f'nll_{x4_cond}'] for r in results])\n",
    "    d_x1 = cohens_d(bare_nlls - x1_nlls)\n",
    "    d_x4 = cohens_d(bare_nlls - x4_nlls)\n",
    "    uplift = d_x4 - d_x1\n",
    "    diff = x1_nlls - x4_nlls  # positive = x4 is better\n",
    "    _, p = stats.ttest_1samp(diff, 0)\n",
    "    sig = '***' if p < 0.001 else '**' if p < 0.01 else '*' if p < 0.05 else 'ns'\n",
    "    print(f\"  {name:<25} {d_x1:>+8.3f} {d_x4:>+8.3f} {uplift:>+8.3f} {p:>12.2e} {sig}\")\n",
    "\n",
    "# 3-Way decomposition for llm_need_x4\n",
    "print(f\"\\n--- 3-Way Decomposition: llm_need_x4 ---\")\n",
    "print(f\"  bare -> random_x4 -> scrambled_llm_need_x4 -> llm_need_x4\")\n",
    "print(f\"    Structure:  bare -> random_x4\")\n",
    "print(f\"    Vocabulary: random_x4 -> scrambled_llm_need_x4\")\n",
    "print(f\"    Semantics:  scrambled_llm_need_x4 -> llm_need_x4\\n\")\n",
    "\n",
    "random_x4_nlls = np.array([r['nll_random_x4_trunc'] for r in results])\n",
    "scrambled_x4_nlls = np.array([r['nll_scrambled_llm_need_x4_trunc'] for r in results])\n",
    "llm_need_x4_nlls = np.array([r['nll_llm_need_x4_trunc'] for r in results])\n",
    "\n",
    "struct_comp = bare_nlls - random_x4_nlls\n",
    "vocab_comp = random_x4_nlls - scrambled_x4_nlls\n",
    "sem_comp = scrambled_x4_nlls - llm_need_x4_nlls\n",
    "total_comp = bare_nlls - llm_need_x4_nlls\n",
    "\n",
    "total_mean = total_comp.mean()\n",
    "\n",
    "print(f\"  {'Component':<20} {'Delta':>10} {'%total':>8} {'d':>8} {'p':>12} {'sig':>5}\")\n",
    "print(f\"  {'-'*65}\")\n",
    "\n",
    "for label, comp in [('Structure', struct_comp), ('Vocabulary', vocab_comp),\n",
    "                    ('Semantics', sem_comp)]:\n",
    "    mu = comp.mean()\n",
    "    pct = mu / total_mean * 100 if total_mean != 0 else 0\n",
    "    d = cohens_d(comp)\n",
    "    _, p = stats.ttest_1samp(comp, 0)\n",
    "    sig = '***' if p < 0.001 else '**' if p < 0.01 else '*' if p < 0.05 else 'ns'\n",
    "    print(f\"  {label:<20} {mu:>+10.4f} {pct:>7.1f}% {d:>+8.3f} {p:>12.2e} {sig}\")\n",
    "\n",
    "print(f\"  {'TOTAL':<20} {total_mean:>+10.4f} {'100.0%':>8}\")\n",
    "residual = total_mean - (struct_comp.mean() + vocab_comp.mean() + sem_comp.mean())\n",
    "print(f\"\\n  Decomposition residual: {residual:.6f} (should be ~0)\")\n",
    "\n",
    "# Compare with oracle decomposition from Exp 2B\n",
    "print(f\"\\n--- Comparison with Exp 2B oracle decomposition (N=1) ---\")\n",
    "print(f\"  Exp 2B (oracle, N=1): Structure=84.7%, Vocabulary=5.5%, Semantics=9.7%\")\n",
    "struct_pct = struct_comp.mean() / total_mean * 100 if total_mean != 0 else 0\n",
    "vocab_pct = vocab_comp.mean() / total_mean * 100 if total_mean != 0 else 0\n",
    "sem_pct = sem_comp.mean() / total_mean * 100 if total_mean != 0 else 0\n",
    "print(f\"  LLM need (x4):       Structure={struct_pct:.1f}%, Vocabulary={vocab_pct:.1f}%, \"\n",
    "      f\"Semantics={sem_pct:.1f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e05312",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Part 4 — Hardness Interaction\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PART 4: HARDNESS INTERACTION\")\n",
    "print(\"=\" * 70)\n",
    "print(\"Does LLM surrogate help more for harder samples?\\n\")\n",
    "\n",
    "quintile_bounds = np.percentile(bare_nlls, [20, 40, 60, 80])\n",
    "quintiles = np.digitize(bare_nlls, quintile_bounds)\n",
    "q_labels = ['Q1 easy', 'Q2', 'Q3', 'Q4', 'Q5 hard']\n",
    "\n",
    "# Table: key conditions by quintile\n",
    "key_conds = [\n",
    "    ('oracle_x1_trunc', 'Oracle x1'),\n",
    "    ('llm_need_x1_trunc', 'LLM need x1'),\n",
    "    ('surr_template_x1_trunc', 'Template x1'),\n",
    "    ('random_x1_trunc', 'Random x1'),\n",
    "    ('llm_need_x4_trunc', 'LLM need x4'),\n",
    "    ('surr_template_x4_trunc', 'Template x4'),\n",
    "    ('random_x4_trunc', 'Random x4'),\n",
    "]\n",
    "\n",
    "print(f\"  {'Quintile':<12} {'Bare NLL':>10}\", end=\"\")\n",
    "for _, desc in key_conds:\n",
    "    print(f\"  {desc:>12}\", end=\"\")\n",
    "print()\n",
    "print(f\"  {'-'*(14 + 14 * len(key_conds))}\")\n",
    "\n",
    "for q in range(5):\n",
    "    mask = quintiles == q\n",
    "    n = mask.sum()\n",
    "    row = f\"  {q_labels[q]:<12} {bare_nlls[mask].mean():>10.3f}\"\n",
    "    for cond, _ in key_conds:\n",
    "        nlls_c = np.array([r[f'nll_{cond}'] for r in results])[mask]\n",
    "        d = cohens_d(bare_nlls[mask] - nlls_c)\n",
    "        row += f\"  {d:>+12.3f}\"\n",
    "    print(row)\n",
    "\n",
    "# Semantic gap by quintile: LLM need vs random\n",
    "print(f\"\\n--- Semantic gap (LLM need - random) by quintile ---\")\n",
    "print(f\"  {'Quintile':<12} {'x1 gap':>10} {'x4 gap':>10} {'x4 sem frac':>12}\")\n",
    "print(f\"  {'-'*50}\")\n",
    "\n",
    "for q in range(5):\n",
    "    mask = quintiles == q\n",
    "    for rep in ['x1', 'x4']:\n",
    "        need_nlls_q = np.array([r[f'nll_llm_need_{rep}_trunc'] for r in results])[mask]\n",
    "        rand_nlls_q = np.array([r[f'nll_random_{rep}_trunc'] for r in results])[mask]\n",
    "        if rep == 'x1':\n",
    "            gap_x1 = (rand_nlls_q - need_nlls_q).mean()\n",
    "        else:\n",
    "            gap_x4 = (rand_nlls_q - need_nlls_q).mean()\n",
    "            total_q = (bare_nlls[mask] - need_nlls_q).mean()\n",
    "            sem_frac = gap_x4 / total_q * 100 if total_q > 0 else 0\n",
    "    print(f\"  {q_labels[q]:<12} {gap_x1:>+10.4f} {gap_x4:>+10.4f} {sem_frac:>11.1f}%\")\n",
    "\n",
    "# Head-to-head: LLM need vs surr_template by quintile\n",
    "print(f\"\\n--- LLM need vs surr_template by quintile (x1) ---\")\n",
    "for q in range(5):\n",
    "    mask = quintiles == q\n",
    "    need_nlls_q = np.array([r['nll_llm_need_x1_trunc'] for r in results])[mask]\n",
    "    tmpl_nlls_q = np.array([r['nll_surr_template_x1_trunc'] for r in results])[mask]\n",
    "    diff = tmpl_nlls_q - need_nlls_q  # positive = need is better\n",
    "    d = cohens_d(diff)\n",
    "    win = 100 * np.mean(diff > 0)\n",
    "    _, p = stats.ttest_1samp(diff, 0)\n",
    "    sig = '***' if p < 0.001 else '**' if p < 0.01 else '*' if p < 0.05 else 'ns'\n",
    "    winner = 'need' if d > 0 else 'template'\n",
    "    print(f\"  {q_labels[q]}: d={d:+.3f}, win={win:.1f}%, p={p:.2e} {sig} [{winner}]\")\n",
    "\n",
    "# Correlation: hardness vs LLM need semantic gap\n",
    "print(f\"\\n--- Correlations ---\")\n",
    "for rep in ['x1', 'x4']:\n",
    "    need_nlls_all = np.array([r[f'nll_llm_need_{rep}_trunc'] for r in results])\n",
    "    rand_nlls_all = np.array([r[f'nll_random_{rep}_trunc'] for r in results])\n",
    "    sem_gap = rand_nlls_all - need_nlls_all\n",
    "    r_val, p_val = stats.pearsonr(bare_nlls, sem_gap)\n",
    "    sig = '***' if p_val < 0.001 else '**' if p_val < 0.01 else '*' if p_val < 0.05 else 'ns'\n",
    "    print(f\"  {rep}: hardness vs LLM_need semantic gap: r={r_val:+.3f} (p={p_val:.2e}) {sig}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a451144a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13: Synthesis + Save\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"SYNTHESIS: LLM-GENERATED SURROGATE RESULTS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "oracle_x1_d = cohens_d(bare_nlls - oracle_x1_nlls)\n",
    "\n",
    "# 1. All conditions summary\n",
    "print(f\"\\n1. ALL CONDITIONS (d, % oracle):\")\n",
    "all_summary = []\n",
    "for cond, desc in all_conds:\n",
    "    nlls = np.array([r[f'nll_{cond}'] for r in results])\n",
    "    d = cohens_d(bare_nlls - nlls)\n",
    "    pct = d / oracle_x1_d * 100 if oracle_x1_d > 0 else 0\n",
    "    all_summary.append((desc, d, pct))\n",
    "    print(f\"   {desc:<38} d={d:>+.3f} ({pct:>5.1f}% oracle)\")\n",
    "\n",
    "# 2. Key comparisons\n",
    "print(f\"\\n2. KEY COMPARISONS:\")\n",
    "\n",
    "need_x1_d = cohens_d(bare_nlls - np.array([r['nll_llm_need_x1_trunc'] for r in results]))\n",
    "tmpl_x1_d = cohens_d(bare_nlls - np.array([r['nll_surr_template_x1_trunc'] for r in results]))\n",
    "rand_x1_d = cohens_d(bare_nlls - np.array([r['nll_random_x1_trunc'] for r in results]))\n",
    "\n",
    "need_x4_d = cohens_d(bare_nlls - np.array([r['nll_llm_need_x4_trunc'] for r in results]))\n",
    "tmpl_x4_d = cohens_d(bare_nlls - np.array([r['nll_surr_template_x4_trunc'] for r in results]))\n",
    "rand_x4_d = cohens_d(bare_nlls - np.array([r['nll_random_x4_trunc'] for r in results]))\n",
    "\n",
    "print(f\"   LLM need x1 vs template x1:  {need_x1_d:+.3f} vs {tmpl_x1_d:+.3f} \"\n",
    "      f\"(uplift: {need_x1_d - tmpl_x1_d:+.3f})\")\n",
    "print(f\"   LLM need x4 vs template x4:  {need_x4_d:+.3f} vs {tmpl_x4_d:+.3f} \"\n",
    "      f\"(uplift: {need_x4_d - tmpl_x4_d:+.3f})\")\n",
    "print(f\"   LLM need x1 vs random x1:    {need_x1_d:+.3f} vs {rand_x1_d:+.3f} \"\n",
    "      f\"(semantic uplift: {need_x1_d - rand_x1_d:+.3f})\")\n",
    "\n",
    "# 3. Decomposition summary\n",
    "print(f\"\\n3. DECOMPOSITION (llm_need x4):\")\n",
    "print(f\"   Structure:  {struct_pct:.1f}%\")\n",
    "print(f\"   Vocabulary: {vocab_pct:.1f}%\")\n",
    "print(f\"   Semantics:  {sem_pct:.1f}%\")\n",
    "\n",
    "# 4. Practical conclusion\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"CONCLUSIONS:\")\n",
    "\n",
    "# Is LLM surrogate worth it?\n",
    "need_tmpl_x1_diff = need_x1_d - tmpl_x1_d\n",
    "need_tmpl_x4_diff = need_x4_d - tmpl_x4_d\n",
    "\n",
    "if need_tmpl_x1_diff > 0.05:\n",
    "    print(f\"  1. LLM surrogates provide MEANINGFUL uplift over template heuristic\")\n",
    "    print(f\"     (d uplift: {need_tmpl_x1_diff:+.3f} at x1, {need_tmpl_x4_diff:+.3f} at x4)\")\n",
    "    practical = \"POSITIVE_ROI\"\n",
    "elif need_tmpl_x1_diff > 0.02:\n",
    "    print(f\"  1. LLM surrogates provide SMALL uplift over template heuristic\")\n",
    "    print(f\"     (d uplift: {need_tmpl_x1_diff:+.3f} at x1, {need_tmpl_x4_diff:+.3f} at x4)\")\n",
    "    practical = \"MARGINAL_ROI\"\n",
    "else:\n",
    "    print(f\"  1. LLM surrogates provide NO meaningful uplift over template heuristic\")\n",
    "    print(f\"     (d uplift: {need_tmpl_x1_diff:+.3f} at x1, {need_tmpl_x4_diff:+.3f} at x4)\")\n",
    "    print(f\"     This confirms that surrogate generation has negative ROI.\")\n",
    "    practical = \"NEGATIVE_ROI\"\n",
    "\n",
    "# Which prompt is best?\n",
    "q_x1_d = cohens_d(bare_nlls - np.array([r['nll_llm_question_x1_trunc'] for r in results]))\n",
    "kw_x1_d = cohens_d(bare_nlls - np.array([r['nll_llm_keywords_x1_trunc'] for r in results]))\n",
    "best_variant = 'need' if need_x1_d >= max(q_x1_d, kw_x1_d) else \\\n",
    "               'question' if q_x1_d >= kw_x1_d else 'keywords'\n",
    "print(f\"  2. Best prompt variant: {best_variant}\")\n",
    "print(f\"     need={need_x1_d:+.3f}, question={q_x1_d:+.3f}, keywords={kw_x1_d:+.3f}\")\n",
    "\n",
    "# Stop-word hypothesis\n",
    "if need_x1_d > kw_x1_d + 0.02:\n",
    "    print(f\"  3. Stop-word hypothesis CONFIRMED: need > keywords by {need_x1_d - kw_x1_d:+.3f}\")\n",
    "elif kw_x1_d > need_x1_d + 0.02:\n",
    "    print(f\"  3. Stop-word hypothesis REFUTED: keywords > need by {kw_x1_d - need_x1_d:+.3f}\")\n",
    "else:\n",
    "    print(f\"  3. Stop-word hypothesis INCONCLUSIVE: need ~ keywords\")\n",
    "\n",
    "# Overall\n",
    "print(f\"\\n  Practical recommendation: \", end=\"\")\n",
    "if practical == \"POSITIVE_ROI\":\n",
    "    print(f\"Use LLM-generated surrogates ('{best_variant}' prompt).\")\n",
    "elif practical == \"MARGINAL_ROI\":\n",
    "    print(f\"LLM surrogates offer marginal improvement. Template heuristic is simpler.\")\n",
    "else:\n",
    "    print(f\"Use 'What is [keyword]?' heuristic. LLM generation is not worth the cost.\")\n",
    "    print(f\"  The mechanism is ~{struct_pct:.0f}% structural — any short prefix works.\")\n",
    "\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "# Save results\n",
    "final_results = {\n",
    "    'experiment': 'exp05_llm_surrogates',\n",
    "    'generation_model': GEMMA_IT_NAME,\n",
    "    'scoring_model': T5GEMMA_NAME,\n",
    "    'dataset': 'ms_marco_v1.1',\n",
    "    'n_samples': N_SAMPLES,\n",
    "    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'conditions': {},\n",
    "    'decomposition': {\n",
    "        'structure_pct': float(struct_pct),\n",
    "        'vocabulary_pct': float(vocab_pct),\n",
    "        'semantics_pct': float(sem_pct),\n",
    "        'structure_d': float(cohens_d(struct_comp)),\n",
    "        'vocabulary_d': float(cohens_d(vocab_comp)),\n",
    "        'semantics_d': float(cohens_d(sem_comp)),\n",
    "    },\n",
    "    'key_comparisons': {\n",
    "        'need_x1_vs_template_x1_uplift': float(need_tmpl_x1_diff),\n",
    "        'need_x4_vs_template_x4_uplift': float(need_tmpl_x4_diff),\n",
    "        'need_x1_vs_random_x1_uplift': float(need_x1_d - rand_x1_d),\n",
    "    },\n",
    "    'conclusion': {\n",
    "        'practical': practical,\n",
    "        'best_variant': best_variant,\n",
    "    },\n",
    "}\n",
    "\n",
    "# Add per-condition results\n",
    "for cond, desc in all_conds:\n",
    "    nlls = np.array([r[f'nll_{cond}'] for r in results])\n",
    "    benefit = bare_nlls - nlls\n",
    "    d = cohens_d(benefit)\n",
    "    _, p = stats.ttest_1samp(benefit, 0)\n",
    "    final_results['conditions'][cond] = {\n",
    "        'description': desc,\n",
    "        'd': float(d),\n",
    "        'mean_nll': float(nlls.mean()),\n",
    "        'mean_delta': float(benefit.mean()),\n",
    "        'pct_oracle': float(d / oracle_x1_d * 100) if oracle_x1_d > 0 else 0,\n",
    "        'p': float(p),\n",
    "    }\n",
    "\n",
    "with open(RESULTS_DIR / 'results.json', 'w') as f:\n",
    "    json.dump(final_results, f, indent=2)\n",
    "print(f\"\\nResults saved to {RESULTS_DIR / 'results.json'}\")\n",
    "\n",
    "# Cleanup\n",
    "print(f\"\\nCleaning up GPU memory...\")\n",
    "mem_before = torch.cuda.memory_allocated() / 1e9\n",
    "del model, processor, tokenizer\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "mem_after = torch.cuda.memory_allocated() / 1e9\n",
    "print(f\"GPU memory: {mem_before:.2f} GB -> {mem_after:.2f} GB\")\n",
    "print(\"Done!\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
