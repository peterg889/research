{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75026afb",
   "metadata": {},
   "source": [
    "# Experiment 06: Factoid Subsample Validation\n",
    "\n",
    "## Motivation\n",
    "\n",
    "The \"85% structural\" finding from Exp 2B is an aggregate that hides two distinct\n",
    "populations (discovered in post-hoc analysis of Exp 02/05 data):\n",
    "\n",
    "| Population | N (in Exp 02) | Structural % | Semantic % | Characteristic |\n",
    "|---|---|---|---|---|\n",
    "| Short factoid answers (<=9 words) | ~248 | ~37% | ~63% | Disambiguation required |\n",
    "| Long descriptive answers (>9 words) | ~252 | ~113% | ~-13% | Broad generation |\n",
    "| **Weighted average** | 500 | **85%** | **15%** | |\n",
    "\n",
    "For short precise answers, the model must select ONE specific fact from the passage.\n",
    "The query tells it which fact. A random prefix fixes the attention sink but cannot\n",
    "help with disambiguation. For long answers, broad representation suffices.\n",
    "\n",
    "This experiment validates the finding on a **fresh sample** of 500 MS MARCO examples\n",
    "filtered to short factoid answers (<=5 words), eliminating look-ahead bias from\n",
    "the post-hoc analysis.\n",
    "\n",
    "## Prediction\n",
    "\n",
    "If the two-population hypothesis is correct:\n",
    "- Structural% should drop from 85% to **~40-50%**\n",
    "- Vocabulary and semantics should contribute **~50-60%** combined\n",
    "- surr_template should significantly beat random (it couldn't on the mixed sample)\n",
    "- The \"directed\" in directed KV cache should finally matter\n",
    "\n",
    "## Conditions (8)\n",
    "\n",
    "| # | Condition | Prefix | Role |\n",
    "|---|-----------|--------|------|\n",
    "| 1 | `bare` | (none) | lower bound |\n",
    "| 2 | `oracle_x1_trunc` | query x 1 | upper bound |\n",
    "| 3 | `oracle_x4_trunc` | query x 4 | upper bound + rep |\n",
    "| 4 | `random_x1_trunc` | random_matched x 1 | structural control |\n",
    "| 5 | `random_x4_trunc` | random_matched x 4 | structural + rep |\n",
    "| 6 | `scrambled_oracle_trunc` | shuffled query x 1 | vocabulary control |\n",
    "| 7 | `surr_template_x1_trunc` | \"What is [kw]?\" x 1 | heuristic |\n",
    "| 8 | `surr_template_x4_trunc` | \"What is [kw]?\" x 4 | heuristic + rep |\n",
    "\n",
    "## Analysis\n",
    "\n",
    "1. Baseline characterization (verify short-answer distribution)\n",
    "2. 3-way decomposition (structure / vocabulary / semantics) — compare to Exp 2B\n",
    "3. Surrogate comparison: does template beat random for factoid QA?\n",
    "4. Hardness interaction: does semantic% vary by difficulty within factoid?\n",
    "5. Direct comparison with Exp 2B full-sample results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea576abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Setup\n",
    "import os\n",
    "os.umask(0o000)\n",
    "\n",
    "import sys, json, time, re, gc, random as pyrandom\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from scipy import stats\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "sys.path.insert(0, \"../..\")\n",
    "from lib.analysis import cohens_d\n",
    "\n",
    "SEED = 43  # Different seed from Exp 02 (42) for a fresh sample\n",
    "N_SAMPLES = 500\n",
    "MAX_ANSWER_WORDS = 5\n",
    "MODEL_NAME = \"google/t5gemma-2-4b-4b\"\n",
    "\n",
    "RESULTS_DIR = Path(\"../../results/exp06\")\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CHECKPOINT_PATH = RESULTS_DIR / \"checkpoint.json\"\n",
    "\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "pyrandom.seed(SEED)\n",
    "\n",
    "print(\"Exp 06: Factoid Subsample Validation\")\n",
    "print(f\"N: {N_SAMPLES}, max answer words: {MAX_ANSWER_WORDS}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3a606e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Load MS MARCO and filter to short factoid answers\n",
    "from lib.data import count_words\n",
    "from datasets import load_dataset\n",
    "\n",
    "print(\"Loading MS MARCO...\")\n",
    "ds = load_dataset(\"microsoft/ms_marco\", \"v1.1\", split=\"validation\")\n",
    "\n",
    "# Collect ALL eligible samples (not just 3*N), then filter to short answers\n",
    "all_candidates = []\n",
    "for item in ds:\n",
    "    if len(all_candidates) >= 20000:\n",
    "        break\n",
    "    passages = item.get('passages', {})\n",
    "    ptexts = passages.get('passage_text', [])\n",
    "    is_sel = passages.get('is_selected', [])\n",
    "    query = item.get('query', '')\n",
    "    answers = item.get('answers', [])\n",
    "    well_formed = item.get('wellFormedAnswers', [])\n",
    "    answer = None\n",
    "    if well_formed and len(well_formed) > 0 and well_formed[0] not in ('[]', ''):\n",
    "        answer = well_formed[0]\n",
    "    elif answers and len(answers) > 0 and answers[0] != 'No Answer Present.':\n",
    "        answer = answers[0]\n",
    "    if not answer:\n",
    "        continue\n",
    "    # Filter: answer must be short (factoid)\n",
    "    answer_words = count_words(answer)\n",
    "    if answer_words > MAX_ANSWER_WORDS:\n",
    "        continue\n",
    "    for pt, sel in zip(ptexts, is_sel):\n",
    "        wc = count_words(pt)\n",
    "        if sel == 1 and 30 <= wc <= 300:\n",
    "            all_candidates.append({\n",
    "                'passage': pt, 'query': query, 'answer': answer,\n",
    "                'word_count': wc, 'answer_words': answer_words,\n",
    "            })\n",
    "            break\n",
    "\n",
    "print(f\"Total short-answer candidates: {len(all_candidates)}\")\n",
    "\n",
    "# Shuffle with our seed and take N_SAMPLES\n",
    "np.random.seed(SEED)\n",
    "indices = np.random.permutation(len(all_candidates))\n",
    "samples = [all_candidates[i] for i in indices[:N_SAMPLES]]\n",
    "del ds, all_candidates\n",
    "gc.collect()\n",
    "\n",
    "# Dataset statistics\n",
    "passage_words = np.array([s['word_count'] for s in samples])\n",
    "answer_words = np.array([s['answer_words'] for s in samples])\n",
    "query_words = np.array([len(s['query'].split()) for s in samples])\n",
    "\n",
    "print(f\"\\nSample statistics (N={N_SAMPLES}):\")\n",
    "print(f\"  Answer length:  mean={answer_words.mean():.1f}, median={np.median(answer_words):.0f}, \"\n",
    "      f\"range=[{answer_words.min()}, {answer_words.max()}]\")\n",
    "print(f\"  Answer distribution: \" + \", \".join(\n",
    "    f\"{w}w={np.sum(answer_words==w)}\" for w in range(1, MAX_ANSWER_WORDS + 1)))\n",
    "print(f\"  Passage length: mean={passage_words.mean():.1f}, median={np.median(passage_words):.0f}\")\n",
    "print(f\"  Query length:   mean={query_words.mean():.1f}, median={np.median(query_words):.0f}\")\n",
    "\n",
    "# Show 5 examples\n",
    "for i in range(5):\n",
    "    s = samples[i]\n",
    "    print(f\"\\nExample {i}:\")\n",
    "    print(f\"  Q: {s['query']}\")\n",
    "    print(f\"  A ({s['answer_words']}w): {s['answer']}\")\n",
    "    print(f\"  P ({s['word_count']}w): {s['passage'][:100]}...\")\n",
    "\n",
    "# Compare with Exp 02 distribution\n",
    "print(f\"\\n--- Comparison with Exp 02 (mixed answers) ---\")\n",
    "print(f\"  Exp 02: mean answer ~14w (range 1-96), mean passage ~74w\")\n",
    "print(f\"  This:   mean answer {answer_words.mean():.1f}w (range {answer_words.min()}-{answer_words.max()}), \"\n",
    "      f\"mean passage {passage_words.mean():.0f}w\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a72bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Load model and define scoring helpers\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv())\n",
    "HF_TOKEN = os.environ.get(\"HF_TOKEN\")\n",
    "\n",
    "from transformers import AutoProcessor, AutoModelForSeq2SeqLM\n",
    "\n",
    "print(f\"Loading {MODEL_NAME}...\")\n",
    "processor = AutoProcessor.from_pretrained(MODEL_NAME, token=HF_TOKEN)\n",
    "tokenizer = processor.tokenizer\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    MODEL_NAME, device_map=\"auto\", torch_dtype=torch.bfloat16, token=HF_TOKEN,\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "DEVICE = next(model.parameters()).device\n",
    "print(f\"Model loaded. dtype={next(model.parameters()).dtype}\")\n",
    "print(f\"GPU memory: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "\n",
    "\n",
    "def score_nll(encoder_text, answer_text, prefix_token_count=0, truncate=False):\n",
    "    # Score NLL of answer given encoder text, with optional prefix truncation.\n",
    "    enc_ids = tokenizer(encoder_text, return_tensors=\"pt\",\n",
    "                        add_special_tokens=True, truncation=True,\n",
    "                        max_length=2048).input_ids.to(DEVICE)\n",
    "    total_enc_len = enc_ids.shape[1]\n",
    "    enc_mask = torch.ones(1, total_enc_len, device=DEVICE, dtype=torch.long)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        encoder_outputs = model.get_encoder()(\n",
    "            input_ids=enc_ids, attention_mask=enc_mask\n",
    "        )\n",
    "\n",
    "    if truncate and prefix_token_count > 0:\n",
    "        cross_attn_mask = torch.ones(1, total_enc_len, device=DEVICE, dtype=torch.long)\n",
    "        cross_attn_mask[:, :prefix_token_count] = 0\n",
    "    else:\n",
    "        cross_attn_mask = torch.ones(1, total_enc_len, device=DEVICE, dtype=torch.long)\n",
    "\n",
    "    ans_ids = tokenizer(answer_text, return_tensors=\"pt\",\n",
    "                        add_special_tokens=False, truncation=True,\n",
    "                        max_length=256).input_ids.to(DEVICE)\n",
    "    if ans_ids.shape[1] == 0:\n",
    "        return 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            encoder_outputs=encoder_outputs,\n",
    "            attention_mask=cross_attn_mask,\n",
    "            labels=ans_ids,\n",
    "        )\n",
    "\n",
    "    logits = outputs.logits\n",
    "    log_probs = F.log_softmax(logits, dim=-1)\n",
    "    token_log_probs = log_probs[0].gather(1, ans_ids[0].unsqueeze(1)).squeeze(1)\n",
    "    mean_nll = -token_log_probs.mean().item()\n",
    "\n",
    "    del encoder_outputs, outputs, logits, log_probs\n",
    "    return mean_nll\n",
    "\n",
    "\n",
    "def count_prefix_tokens(prefix_text, document_text):\n",
    "    # Count how many tokens the prefix occupies in [prefix + newline + document].\n",
    "    full_text = prefix_text + \"\\n\" + document_text\n",
    "    full_ids = tokenizer(full_text, add_special_tokens=True, truncation=True,\n",
    "                         max_length=2048).input_ids\n",
    "    doc_ids = tokenizer(document_text, add_special_tokens=True, truncation=True,\n",
    "                        max_length=2048).input_ids\n",
    "    return len(full_ids) - len(doc_ids)\n",
    "\n",
    "\n",
    "STOP_WORDS = {\n",
    "    'a', 'an', 'the', 'is', 'are', 'was', 'were', 'be', 'been', 'being',\n",
    "    'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could',\n",
    "    'should', 'may', 'might', 'can', 'shall', 'to', 'of', 'in', 'for',\n",
    "    'on', 'with', 'at', 'by', 'from', 'as', 'into', 'through', 'during',\n",
    "    'before', 'after', 'above', 'below', 'between', 'and', 'but', 'or',\n",
    "    'not', 'no', 'if', 'then', 'than', 'so', 'up', 'out', 'about',\n",
    "    'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those',\n",
    "    'it', 'its', 'i', 'me', 'my', 'we', 'our', 'you', 'your', 'he',\n",
    "    'him', 'his', 'she', 'her', 'they', 'them', 'their', 'how', 'when',\n",
    "    'where', 'why', 'much', 'many', 'some', 'any', 'all', 'each',\n",
    "    'does', 'also', 'just', 'more', 'most', 'very', 'too', 'only',\n",
    "}\n",
    "\n",
    "print(\"Helpers defined.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbe3b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Generate all 8 scoring conditions per sample\n",
    "\n",
    "for i, s in enumerate(samples):\n",
    "    query = s['query']\n",
    "    passage = s['passage']\n",
    "    query_words_list = query.split()\n",
    "\n",
    "    # Random text from unrelated passage\n",
    "    other_idx = (i + N_SAMPLES // 2) % len(samples)\n",
    "    other_words = samples[other_idx]['passage'].split()\n",
    "    random_matched = \" \".join(other_words[:len(query_words_list)])\n",
    "\n",
    "    # Scrambled oracle (same words, random order)\n",
    "    rng = np.random.RandomState(SEED + i)\n",
    "    shuffled = list(query_words_list)\n",
    "    rng.shuffle(shuffled)\n",
    "    scrambled = \" \".join(shuffled)\n",
    "\n",
    "    # Template surrogate: \"What is [keyword]?\"\n",
    "    doc_words_clean = re.sub(r'[^\\w\\s]', '', passage.lower()).split()\n",
    "    content = [w for w in doc_words_clean if w not in STOP_WORDS and len(w) > 2]\n",
    "    if content:\n",
    "        kw = Counter(content).most_common(1)[0][0]\n",
    "    else:\n",
    "        kw = \"information\"\n",
    "\n",
    "    s['oracle_x1'] = query\n",
    "    s['oracle_x4'] = \" \".join([query] * 4)\n",
    "    s['random_x1'] = random_matched\n",
    "    s['random_x4'] = \" \".join([random_matched] * 4)\n",
    "    s['scrambled_oracle'] = scrambled\n",
    "    s['surr_template_x1'] = f\"What is {kw}?\"\n",
    "    s['surr_template_x4'] = \" \".join([f\"What is {kw}?\"] * 4)\n",
    "\n",
    "COND_NAMES = [\n",
    "    'bare',\n",
    "    'oracle_x1_trunc',\n",
    "    'oracle_x4_trunc',\n",
    "    'random_x1_trunc',\n",
    "    'random_x4_trunc',\n",
    "    'scrambled_oracle_trunc',\n",
    "    'surr_template_x1_trunc',\n",
    "    'surr_template_x4_trunc',\n",
    "]\n",
    "\n",
    "print(f\"Conditions ({len(COND_NAMES)}):\")\n",
    "for c in COND_NAMES:\n",
    "    print(f\"  {c}\")\n",
    "\n",
    "# Show example\n",
    "ex = samples[0]\n",
    "print(f\"\\nExample (sample 0):\")\n",
    "print(f\"  Query: {ex['query']}\")\n",
    "print(f\"  Answer ({ex['answer_words']}w): {ex['answer']}\")\n",
    "for c in COND_NAMES:\n",
    "    if c == 'bare':\n",
    "        print(f\"  {c:<30}: [document only]\")\n",
    "    else:\n",
    "        key = c.replace('_trunc', '')\n",
    "        text = ex[key]\n",
    "        ptoks = count_prefix_tokens(text, ex['passage'])\n",
    "        print(f\"  {c:<30} ({ptoks:>3} toks): {str(text)[:55]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403e2565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Scoring loop with checkpointing\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"SCORING ALL CONDITIONS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "results = []\n",
    "start_idx = 0\n",
    "\n",
    "if CHECKPOINT_PATH.exists():\n",
    "    ckpt = json.loads(CHECKPOINT_PATH.read_text())\n",
    "    if ckpt.get('n_total') == N_SAMPLES and len(ckpt.get('results', [])) > 0:\n",
    "        saved_queries = [r['query'][:50] for r in ckpt['results']]\n",
    "        current_queries = [s['query'][:50] for s in samples[:len(saved_queries)]]\n",
    "        if saved_queries == current_queries:\n",
    "            results = ckpt['results']\n",
    "            start_idx = len(results)\n",
    "            print(f\"Resuming from checkpoint: {start_idx}/{N_SAMPLES}\")\n",
    "\n",
    "if start_idx == 0:\n",
    "    print(f\"Starting fresh: {len(COND_NAMES)} conditions x {N_SAMPLES} samples \"\n",
    "          f\"= {len(COND_NAMES) * N_SAMPLES} scorings\")\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "for i in tqdm(range(start_idx, N_SAMPLES), initial=start_idx, total=N_SAMPLES,\n",
    "              desc=\"Scoring\"):\n",
    "    s = samples[i]\n",
    "    result = {\n",
    "        'query': s['query'],\n",
    "        'answer': s['answer'],\n",
    "        'passage_words': s['word_count'],\n",
    "        'answer_words': s['answer_words'],\n",
    "    }\n",
    "\n",
    "    for cond in COND_NAMES:\n",
    "        if cond == 'bare':\n",
    "            nll = score_nll(s['passage'], s['answer'])\n",
    "            result['nll_bare'] = nll\n",
    "        else:\n",
    "            key = cond.replace('_trunc', '')\n",
    "            prefix = s[key]\n",
    "            enc_text = prefix + \"\\n\" + s['passage']\n",
    "            ptoks = count_prefix_tokens(prefix, s['passage'])\n",
    "            nll = score_nll(enc_text, s['answer'], ptoks, truncate=True)\n",
    "            result[f'nll_{cond}'] = nll\n",
    "            result[f'ptoks_{cond}'] = ptoks\n",
    "\n",
    "    results.append(result)\n",
    "\n",
    "    if (i + 1) % 20 == 0 or i == N_SAMPLES - 1:\n",
    "        ckpt = {\n",
    "            'n_total': N_SAMPLES,\n",
    "            'results': results,\n",
    "            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        }\n",
    "        CHECKPOINT_PATH.write_text(json.dumps(ckpt))\n",
    "        elapsed = time.time() - t0\n",
    "        done = i - start_idx + 1\n",
    "        eta = (N_SAMPLES - i - 1) * elapsed / done if done > 0 else 0\n",
    "        tqdm.write(f\"  Checkpoint {i+1}/{N_SAMPLES} | {elapsed/60:.1f}m | ETA {eta/60:.1f}m\")\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "elapsed = time.time() - t0\n",
    "print(f\"\\nScoring complete: {len(results)} samples, \"\n",
    "      f\"{len(COND_NAMES)} conditions in {elapsed/60:.1f} min\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb047d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Part 1 — Baseline Characterization\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PART 1: BASELINE CHARACTERIZATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "bare_nlls = np.array([r['nll_bare'] for r in results])\n",
    "oracle_x1_nlls = np.array([r['nll_oracle_x1_trunc'] for r in results])\n",
    "oracle_benefit = bare_nlls - oracle_x1_nlls\n",
    "oracle_d = cohens_d(oracle_benefit)\n",
    "\n",
    "print(f\"\\nBaseline (bare):  mean NLL = {bare_nlls.mean():.4f}, std = {bare_nlls.std():.4f}\")\n",
    "print(f\"Oracle (x1):      mean NLL = {oracle_x1_nlls.mean():.4f}\")\n",
    "print(f\"Oracle headroom:  delta = {oracle_benefit.mean():+.4f}, d = {oracle_d:+.3f}\")\n",
    "\n",
    "_, p_oracle = stats.ttest_1samp(oracle_benefit, 0)\n",
    "win_rate = np.mean(oracle_benefit > 0) * 100\n",
    "print(f\"Oracle win rate:  {win_rate:.1f}% (p={p_oracle:.2e})\")\n",
    "\n",
    "# Answer length distribution\n",
    "aw = np.array([r['answer_words'] for r in results])\n",
    "print(f\"\\nAnswer distribution: \" + \", \".join(\n",
    "    f\"{w}w: {np.sum(aw==w)} ({np.sum(aw==w)/len(aw)*100:.0f}%)\" for w in range(1, MAX_ANSWER_WORDS + 1)))\n",
    "\n",
    "# Comparison with Exp 02\n",
    "print(f\"\\n--- Comparison with Exp 02 (mixed answers) ---\")\n",
    "print(f\"  Exp 02: bare NLL=3.68, oracle d=+0.376, headroom=+0.684\")\n",
    "print(f\"  This:   bare NLL={bare_nlls.mean():.2f}, oracle d={oracle_d:+.3f}, \"\n",
    "      f\"headroom={oracle_benefit.mean():+.3f}\")\n",
    "\n",
    "# All conditions overview\n",
    "print(f\"\\n{'Condition':<30} {'NLL':>8} {'Delta':>8} {'d':>8} {'Win%':>7} {'%Orc':>6}\")\n",
    "print(\"-\" * 75)\n",
    "\n",
    "all_cond_pairs = [\n",
    "    ('oracle_x1_trunc', 'Oracle x1'),\n",
    "    ('oracle_x4_trunc', 'Oracle x4'),\n",
    "    ('scrambled_oracle_trunc', 'Scrambled oracle'),\n",
    "    ('surr_template_x1_trunc', 'Template x1'),\n",
    "    ('surr_template_x4_trunc', 'Template x4'),\n",
    "    ('random_x1_trunc', 'Random x1'),\n",
    "    ('random_x4_trunc', 'Random x4'),\n",
    "]\n",
    "\n",
    "for cond, desc in all_cond_pairs:\n",
    "    nlls = np.array([r[f'nll_{cond}'] for r in results])\n",
    "    benefit = bare_nlls - nlls\n",
    "    d = cohens_d(benefit)\n",
    "    delta = benefit.mean()\n",
    "    win = 100 * np.mean(benefit > 0)\n",
    "    pct = d / oracle_d * 100 if oracle_d > 0 else 0\n",
    "    print(f\"  {desc:<28} {nlls.mean():>8.4f} {delta:>+8.4f} {d:>+8.3f} {win:>6.1f}% {pct:>5.0f}%\")\n",
    "\n",
    "print(f\"  {'bare (lower bound)':<28} {bare_nlls.mean():>8.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0075958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Part 2 — 3-Way Decomposition\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PART 2: 3-WAY DECOMPOSITION (the key test)\")\n",
    "print(\"=\" * 70)\n",
    "print(\"Decompose: bare -> random_x1 -> scrambled_oracle -> oracle_x1\")\n",
    "print(\"  Structure:  bare -> random_x1 (any prefix helps)\")\n",
    "print(\"  Vocabulary: random_x1 -> scrambled (right words, wrong order)\")\n",
    "print(\"  Semantics:  scrambled -> oracle (right word order)\\n\")\n",
    "\n",
    "random_x1_nlls = np.array([r['nll_random_x1_trunc'] for r in results])\n",
    "scrambled_nlls = np.array([r['nll_scrambled_oracle_trunc'] for r in results])\n",
    "\n",
    "struct_comp = bare_nlls - random_x1_nlls\n",
    "vocab_comp = random_x1_nlls - scrambled_nlls\n",
    "sem_comp = scrambled_nlls - oracle_x1_nlls\n",
    "total_comp = bare_nlls - oracle_x1_nlls\n",
    "\n",
    "total_mean = total_comp.mean()\n",
    "\n",
    "print(f\"  {'Component':<20} {'Delta':>10} {'%total':>8} {'d':>8} {'p':>12} {'sig':>5}\")\n",
    "print(f\"  {'-'*65}\")\n",
    "\n",
    "for label, comp in [('Structure', struct_comp), ('Vocabulary', vocab_comp),\n",
    "                    ('Semantics', sem_comp)]:\n",
    "    mu = comp.mean()\n",
    "    pct = mu / total_mean * 100 if total_mean != 0 else 0\n",
    "    d = cohens_d(comp)\n",
    "    _, p = stats.ttest_1samp(comp, 0)\n",
    "    sig = '***' if p < 0.001 else '**' if p < 0.01 else '*' if p < 0.05 else 'ns'\n",
    "    print(f\"  {label:<20} {mu:>+10.4f} {pct:>7.1f}% {d:>+8.3f} {p:>12.2e} {sig}\")\n",
    "\n",
    "print(f\"  {'TOTAL':<20} {total_mean:>+10.4f} {'100.0%':>8}\")\n",
    "residual = total_mean - (struct_comp.mean() + vocab_comp.mean() + sem_comp.mean())\n",
    "print(f\"\\n  Decomposition residual: {residual:.6f}\")\n",
    "\n",
    "struct_pct = struct_comp.mean() / total_mean * 100 if total_mean != 0 else 0\n",
    "vocab_pct = vocab_comp.mean() / total_mean * 100 if total_mean != 0 else 0\n",
    "sem_pct = sem_comp.mean() / total_mean * 100 if total_mean != 0 else 0\n",
    "\n",
    "# THE KEY COMPARISON\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"  KEY COMPARISON: Exp 2B (mixed) vs Exp 06 (factoid only)\")\n",
    "print(f\"  {'Component':<15} {'Exp 2B (mixed)':>18} {'Exp 06 (factoid)':>20} {'Change':>10}\")\n",
    "print(f\"  {'-'*65}\")\n",
    "print(f\"  {'Structure':<15} {'84.7%':>18} {struct_pct:>19.1f}% {struct_pct-84.7:>+9.1f}pp\")\n",
    "print(f\"  {'Vocabulary':<15} {'5.5% (ns)':>18} {vocab_pct:>19.1f}% {vocab_pct-5.5:>+9.1f}pp\")\n",
    "print(f\"  {'Semantics':<15} {'9.7% (***)':>18} {sem_pct:>19.1f}% {sem_pct-9.7:>+9.1f}pp\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "if struct_pct < 60:\n",
    "    print(f\"\\n  --> PREDICTION CONFIRMED: structural% dropped to {struct_pct:.0f}%.\")\n",
    "    print(f\"      For short factoid answers, query content matters.\")\n",
    "    print(f\"      The 'directed' in directed KV cache IS valuable for this task type.\")\n",
    "elif struct_pct < 75:\n",
    "    print(f\"\\n  --> PARTIAL: structural% dropped to {struct_pct:.0f}% (from 85%).\")\n",
    "    print(f\"      Meaningful shift but structure still dominates.\")\n",
    "else:\n",
    "    print(f\"\\n  --> PREDICTION FAILED: structural% is still {struct_pct:.0f}%.\")\n",
    "    print(f\"      Even on factoid QA, the mechanism is primarily structural.\")\n",
    "\n",
    "# Per-sample structural% distribution\n",
    "per_sample_struct_pct = []\n",
    "for i in range(N_SAMPLES):\n",
    "    total_i = total_comp[i]\n",
    "    if total_i > 0.01:  # avoid division by near-zero\n",
    "        per_sample_struct_pct.append(struct_comp[i] / total_i * 100)\n",
    "per_sample_struct_pct = np.array(per_sample_struct_pct)\n",
    "print(f\"\\n  Per-sample structural% (N={len(per_sample_struct_pct)} with total>0.01):\")\n",
    "print(f\"    Mean: {per_sample_struct_pct.mean():.1f}%, Median: {np.median(per_sample_struct_pct):.1f}%\")\n",
    "print(f\"    % samples with structural < 50%: {np.mean(per_sample_struct_pct < 50)*100:.1f}%\")\n",
    "print(f\"    % samples with structural < 0% (random hurts): {np.mean(per_sample_struct_pct < 0)*100:.1f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de2db7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Part 3 — Surrogate Comparison\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PART 3: SURROGATE COMPARISON\")\n",
    "print(\"=\" * 70)\n",
    "print(\"Does 'What is [keyword]?' beat random for factoid QA?\\n\")\n",
    "\n",
    "surr_x1_nlls = np.array([r['nll_surr_template_x1_trunc'] for r in results])\n",
    "surr_x4_nlls = np.array([r['nll_surr_template_x4_trunc'] for r in results])\n",
    "random_x4_nlls = np.array([r['nll_random_x4_trunc'] for r in results])\n",
    "oracle_x4_nlls = np.array([r['nll_oracle_x4_trunc'] for r in results])\n",
    "\n",
    "# Head-to-head comparisons\n",
    "pairs = [\n",
    "    ('oracle_x1', oracle_x1_nlls, 'random_x1', random_x1_nlls,\n",
    "     \"Does oracle beat random? (semantic signal exists?)\"),\n",
    "    ('surr_template_x1', surr_x1_nlls, 'random_x1', random_x1_nlls,\n",
    "     \"Does template beat random? (heuristic captures semantics?)\"),\n",
    "    ('oracle_x1', oracle_x1_nlls, 'surr_template_x1', surr_x1_nlls,\n",
    "     \"Does oracle beat template? (room for improvement?)\"),\n",
    "    ('surr_template_x4', surr_x4_nlls, 'random_x4', random_x4_nlls,\n",
    "     \"Does template x4 beat random x4? (amplified by repetition?)\"),\n",
    "    ('oracle_x4', oracle_x4_nlls, 'surr_template_x4', surr_x4_nlls,\n",
    "     \"Does oracle x4 beat template x4?\"),\n",
    "]\n",
    "\n",
    "for name_a, nlls_a, name_b, nlls_b, question in pairs:\n",
    "    diff = nlls_b - nlls_a  # positive = A is better\n",
    "    d = cohens_d(diff)\n",
    "    win = 100 * np.mean(diff > 0)\n",
    "    _, p = stats.ttest_1samp(diff, 0)\n",
    "    sig = '***' if p < 0.001 else '**' if p < 0.01 else '*' if p < 0.05 else 'ns'\n",
    "    winner = name_a if d > 0 else name_b\n",
    "    print(f\"  {question}\")\n",
    "    print(f\"    {name_a} vs {name_b}: d={d:+.3f}, win={win:.1f}%, p={p:.2e} {sig} [{winner}]\")\n",
    "    print()\n",
    "\n",
    "# Compare with Exp 02 (mixed sample)\n",
    "print(f\"--- Comparison with Exp 02 ---\")\n",
    "print(f\"  Exp 02 oracle vs random: d=+0.080 (ns on mixed sample)\")\n",
    "surr_rand_d = cohens_d(random_x1_nlls - surr_x1_nlls)\n",
    "oracle_rand_d = cohens_d(random_x1_nlls - oracle_x1_nlls)\n",
    "print(f\"  Exp 06 oracle vs random: d={oracle_rand_d:+.3f} (factoid)\")\n",
    "print(f\"  Exp 06 template vs random: d={surr_rand_d:+.3f} (factoid)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4b9713",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Part 4 — Hardness Interaction\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PART 4: HARDNESS INTERACTION (within factoid)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "quintile_bounds = np.percentile(bare_nlls, [20, 40, 60, 80])\n",
    "quintiles = np.digitize(bare_nlls, quintile_bounds)\n",
    "q_labels = ['Q1 easy', 'Q2', 'Q3', 'Q4', 'Q5 hard']\n",
    "\n",
    "print(f\"\\n  {'Quintile':<12} {'N':>4} {'Bare NLL':>10} {'Struct%':>9} {'Vocab%':>8} \"\n",
    "      f\"{'Sem%':>7} {'Oracle d':>10} {'Orc vs Rand':>12}\")\n",
    "print(f\"  {'-'*80}\")\n",
    "\n",
    "for q in range(5):\n",
    "    mask = quintiles == q\n",
    "    n = mask.sum()\n",
    "    bare_q = bare_nlls[mask].mean()\n",
    "\n",
    "    s_comp = (bare_nlls[mask] - random_x1_nlls[mask]).mean()\n",
    "    v_comp = (random_x1_nlls[mask] - scrambled_nlls[mask]).mean()\n",
    "    sm_comp = (scrambled_nlls[mask] - oracle_x1_nlls[mask]).mean()\n",
    "    t_comp = (bare_nlls[mask] - oracle_x1_nlls[mask]).mean()\n",
    "\n",
    "    if t_comp > 0:\n",
    "        s_pct = s_comp / t_comp * 100\n",
    "        v_pct = v_comp / t_comp * 100\n",
    "        sm_pct = sm_comp / t_comp * 100\n",
    "    else:\n",
    "        s_pct = v_pct = sm_pct = 0\n",
    "\n",
    "    o_d = cohens_d(bare_nlls[mask] - oracle_x1_nlls[mask])\n",
    "\n",
    "    # Oracle vs random head-to-head\n",
    "    diff_q = random_x1_nlls[mask] - oracle_x1_nlls[mask]\n",
    "    or_d = cohens_d(diff_q)\n",
    "    _, or_p = stats.ttest_1samp(diff_q, 0)\n",
    "    or_sig = '***' if or_p < 0.001 else '**' if or_p < 0.01 else '*' if or_p < 0.05 else 'ns'\n",
    "\n",
    "    print(f\"  {q_labels[q]:<12} {n:>4} {bare_q:>10.3f} {s_pct:>8.1f}% {v_pct:>7.1f}% \"\n",
    "          f\"{sm_pct:>6.1f}% {o_d:>+10.3f} {or_d:>+8.3f} {or_sig}\")\n",
    "\n",
    "# Correlations\n",
    "print(f\"\\n--- Correlations ---\")\n",
    "r_s, p_s = stats.pearsonr(bare_nlls, struct_comp)\n",
    "r_v, p_v = stats.pearsonr(bare_nlls, vocab_comp)\n",
    "r_sm, p_sm = stats.pearsonr(bare_nlls, sem_comp)\n",
    "print(f\"  hardness vs structure:  r={r_s:+.3f} (p={p_s:.2e})\")\n",
    "print(f\"  hardness vs vocabulary: r={r_v:+.3f} (p={p_v:.2e})\")\n",
    "print(f\"  hardness vs semantics:  r={r_sm:+.3f} (p={p_sm:.2e})\")\n",
    "\n",
    "# By answer length (1w vs 2-3w vs 4-5w)\n",
    "print(f\"\\n--- By answer length ---\")\n",
    "for aw_label, aw_min, aw_max in [('1 word', 1, 1), ('2-3 words', 2, 3), ('4-5 words', 4, 5)]:\n",
    "    aw = np.array([r['answer_words'] for r in results])\n",
    "    mask = (aw >= aw_min) & (aw <= aw_max)\n",
    "    n = mask.sum()\n",
    "    if n < 10:\n",
    "        continue\n",
    "    t = (bare_nlls[mask] - oracle_x1_nlls[mask]).mean()\n",
    "    s = (bare_nlls[mask] - random_x1_nlls[mask]).mean()\n",
    "    if t > 0:\n",
    "        s_pct = s / t * 100\n",
    "        sm_pct = ((scrambled_nlls[mask] - oracle_x1_nlls[mask]).mean()) / t * 100\n",
    "    else:\n",
    "        s_pct = sm_pct = 0\n",
    "    o_d = cohens_d(bare_nlls[mask] - oracle_x1_nlls[mask])\n",
    "    print(f\"  {aw_label} (N={n}): struct={s_pct:.1f}%, sem={sm_pct:.1f}%, oracle d={o_d:+.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25647c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Synthesis + Save\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"SYNTHESIS: FACTOID SUBSAMPLE RESULTS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Summary numbers\n",
    "print(f\"\\n1. DATASET: MS MARCO v1.1, filtered to answer <= {MAX_ANSWER_WORDS} words\")\n",
    "print(f\"   N: {N_SAMPLES}, mean answer: {answer_words.mean():.1f} words\")\n",
    "print(f\"   Oracle headroom: d={oracle_d:+.3f}, delta={oracle_benefit.mean():+.3f}\")\n",
    "\n",
    "print(f\"\\n2. 3-WAY DECOMPOSITION:\")\n",
    "print(f\"   {'Component':<15} {'Exp 2B (mixed)':>18} {'Exp 06 (factoid)':>20}\")\n",
    "print(f\"   {'-'*55}\")\n",
    "print(f\"   {'Structure':<15} {'84.7%':>18} {struct_pct:>19.1f}%\")\n",
    "print(f\"   {'Vocabulary':<15} {'5.5%':>18} {vocab_pct:>19.1f}%\")\n",
    "print(f\"   {'Semantics':<15} {'9.7%':>18} {sem_pct:>19.1f}%\")\n",
    "\n",
    "print(f\"\\n3. SURROGATE COMPARISON:\")\n",
    "oracle_rand_d_val = cohens_d(random_x1_nlls - oracle_x1_nlls)\n",
    "surr_rand_d_val = cohens_d(random_x1_nlls - surr_x1_nlls)\n",
    "_, p_or = stats.ttest_1samp(random_x1_nlls - oracle_x1_nlls, 0)\n",
    "_, p_sr = stats.ttest_1samp(random_x1_nlls - surr_x1_nlls, 0)\n",
    "or_sig = '***' if p_or < 0.001 else '**' if p_or < 0.01 else '*' if p_or < 0.05 else 'ns'\n",
    "sr_sig = '***' if p_sr < 0.001 else '**' if p_sr < 0.01 else '*' if p_sr < 0.05 else 'ns'\n",
    "print(f\"   Oracle vs random:   d={oracle_rand_d_val:+.3f} ({or_sig})\")\n",
    "print(f\"   Template vs random: d={surr_rand_d_val:+.3f} ({sr_sig})\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"CONCLUSIONS:\")\n",
    "\n",
    "if struct_pct < 60:\n",
    "    mechanism = \"SEMANTIC_DOMINANT\"\n",
    "    print(f\"  1. PREDICTION CONFIRMED: factoid QA is {struct_pct:.0f}% structural / \"\n",
    "          f\"{100-struct_pct:.0f}% content.\")\n",
    "    print(f\"     The '85% structural' finding was an artifact of averaging over two populations.\")\n",
    "    print(f\"     For short factoid answers, the directed KV cache IS worth directing.\")\n",
    "elif struct_pct < 75:\n",
    "    mechanism = \"MIXED\"\n",
    "    print(f\"  1. PARTIAL CONFIRMATION: structural% dropped to {struct_pct:.0f}% (from 85%).\")\n",
    "    print(f\"     Content matters more for factoid QA but structure still dominates.\")\n",
    "else:\n",
    "    mechanism = \"STILL_STRUCTURAL\"\n",
    "    print(f\"  1. PREDICTION FAILED: structural% is still {struct_pct:.0f}% even for factoid QA.\")\n",
    "\n",
    "if surr_rand_d_val > 0.05 and p_sr < 0.05:\n",
    "    surrogate_value = \"POSITIVE\"\n",
    "    print(f\"  2. Template surrogate significantly beats random (d={surr_rand_d_val:+.3f}).\")\n",
    "    print(f\"     Heuristic surrogates have positive ROI for factoid QA.\")\n",
    "elif oracle_rand_d_val > 0.05 and p_or < 0.05:\n",
    "    surrogate_value = \"ORACLE_ONLY\"\n",
    "    print(f\"  2. Oracle beats random (d={oracle_rand_d_val:+.3f}) but template does not.\")\n",
    "    print(f\"     The semantic signal exists but the heuristic doesn't capture it.\")\n",
    "else:\n",
    "    surrogate_value = \"NONE\"\n",
    "    print(f\"  2. Even oracle barely beats random on factoid QA.\")\n",
    "\n",
    "print(f\"\\n  Practical implication: \", end=\"\")\n",
    "if mechanism == \"SEMANTIC_DOMINANT\" and surrogate_value == \"POSITIVE\":\n",
    "    print(\"for factoid extraction tasks, invest in content-aware surrogates.\")\n",
    "elif mechanism == \"SEMANTIC_DOMINANT\":\n",
    "    print(\"semantic content matters but better surrogates are needed to capture it.\")\n",
    "else:\n",
    "    print(\"the structural mechanism dominates even for factoid QA.\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "# Save results\n",
    "final_results = {\n",
    "    'experiment': 'exp06_factoid_subsample',\n",
    "    'model': MODEL_NAME,\n",
    "    'dataset': 'ms_marco_v1.1_short_answers',\n",
    "    'n_samples': N_SAMPLES,\n",
    "    'max_answer_words': MAX_ANSWER_WORDS,\n",
    "    'mean_answer_words': float(answer_words.mean()),\n",
    "    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'baseline': {\n",
    "        'bare_nll': float(bare_nlls.mean()),\n",
    "        'oracle_d': float(oracle_d),\n",
    "        'oracle_headroom': float(oracle_benefit.mean()),\n",
    "    },\n",
    "    'decomposition': {\n",
    "        'structure_pct': float(struct_pct),\n",
    "        'vocabulary_pct': float(vocab_pct),\n",
    "        'semantics_pct': float(sem_pct),\n",
    "        'structure_d': float(cohens_d(struct_comp)),\n",
    "        'vocabulary_d': float(cohens_d(vocab_comp)),\n",
    "        'semantics_d': float(cohens_d(sem_comp)),\n",
    "    },\n",
    "    'surrogate_comparison': {\n",
    "        'oracle_vs_random_d': float(oracle_rand_d_val),\n",
    "        'oracle_vs_random_p': float(p_or),\n",
    "        'template_vs_random_d': float(surr_rand_d_val),\n",
    "        'template_vs_random_p': float(p_sr),\n",
    "    },\n",
    "    'conditions': {},\n",
    "    'conclusion': {\n",
    "        'mechanism': mechanism,\n",
    "        'surrogate_value': surrogate_value,\n",
    "    },\n",
    "}\n",
    "\n",
    "for cond, desc in all_cond_pairs:\n",
    "    nlls = np.array([r[f'nll_{cond}'] for r in results])\n",
    "    benefit = bare_nlls - nlls\n",
    "    d = cohens_d(benefit)\n",
    "    _, p = stats.ttest_1samp(benefit, 0)\n",
    "    final_results['conditions'][cond] = {\n",
    "        'description': desc,\n",
    "        'd': float(d),\n",
    "        'mean_nll': float(nlls.mean()),\n",
    "        'mean_delta': float(benefit.mean()),\n",
    "        'pct_oracle': float(d / oracle_d * 100) if oracle_d > 0 else 0,\n",
    "        'p': float(p),\n",
    "    }\n",
    "\n",
    "with open(RESULTS_DIR / 'results.json', 'w') as f:\n",
    "    json.dump(final_results, f, indent=2)\n",
    "print(f\"\\nResults saved to {RESULTS_DIR / 'results.json'}\")\n",
    "\n",
    "# Cleanup\n",
    "print(f\"\\nCleaning up GPU memory...\")\n",
    "mem_before = torch.cuda.memory_allocated() / 1e9\n",
    "del model, processor, tokenizer\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "mem_after = torch.cuda.memory_allocated() / 1e9\n",
    "print(f\"GPU memory: {mem_before:.2f} GB -> {mem_after:.2f} GB\")\n",
    "print(\"Done!\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
