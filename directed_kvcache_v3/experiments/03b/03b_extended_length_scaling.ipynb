{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 03B: Extended Length Scaling with All Surrogate Types",
    "## Does the structural switch still fire at very long documents? How far can we push?",
    "",
    "### Background",
    "Exp 03 showed NO DECAY up to 2048 tokens for oracle/surr\\_doc/surr\\_para. But:",
    "1. Only content-specific surrogates were tested",
    "2. Exp 2B proved 85% of the benefit is structural (1 random word = 85% of oracle)",
    "3. 2048 tokens is not actually that long for real-world documents",
    "",
    "### Architecture constraint",
    "T5Gemma encoder uses sliding\\_window=1024 with full attention every 6th layer (5 out of 34).",
    "At document lengths >> 1024, most encoder layers can only connect nearby tokens to the",
    "surrogate. The global layers must propagate the signal across the full document.",
    "",
    "### The questions",
    "1. Does `random\\_trunc` hold up at 4096+ tokens? (structural mechanism at scale)",
    "2. Does the semantic gap (oracle - random) change with length?",
    "3. Where does the encoder architecture actually break down?",
    "",
    "### Conditions (6, all with truncation)",
    "| Condition | Source | Type | Exp 02 d |",
    "|-----------|--------|------|----------|",
    "| bare | — | Lower bound | — |",
    "| oracle\\_trunc | Real query | Upper bound | +0.376 |",
    "| scrambled\\_oracle\\_trunc | Query words, random order | Vocabulary control | Exp 2B |",
    "| random\\_matched\\_trunc | Random words, query length | Length-matched structural | Exp 2B |",
    "| random\\_trunc | Unrelated passage | Structural control | +0.303 |",
    "| static\\_fact\\_trunc | \"What are the key facts?\" | Content-agnostic | +0.372 |",
    "| surr\\_template\\_trunc | \"What is [keyword]?\" | Doc-derived minimal | +0.336 |",
    "| surr\\_doc\\_trunc | Top-5 TF keywords | Doc-derived | +0.322 |",
    "",
    "### Three-way decomposition (at each length)",
    "- **Structure** = bare → random\\_matched (any prefix helps)",
    "- **Vocabulary** = random\\_matched → scrambled\\_oracle (right words, wrong order)",
    "- **Semantics** = scrambled\\_oracle → oracle (right word order)",
    "",
    "### Length bins (pushing 3x beyond Exp 03)",
    "| Bin | Target tokens | Exp 03 oracle d | Sliding window coverage |",
    "|-----|--------------|-----------------|------------------------|",
    "| original | ~130 tok | +0.384*** | 100% within window |",
    "| 512 | padded | +0.442*** | 100% within window |",
    "| 1024 | padded | +0.452*** | ~100% (boundary) |",
    "| 2048 | padded | +0.392*** | ~50% in window layers |",
    "| 3072 | padded (new) | — | ~33% in window layers |",
    "| 4096 | padded (new) | — | ~25% in window layers |",
    "| 6144 | padded (new) | — | ~17% in window layers |",
    "",
    "### N=400, Bonferroni for 49 comparisons (7 non-bare conditions x 7 lengths)",
    ""
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 2: Setup\n",
    "import os\n",
    "os.umask(0o000)\n",
    "\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "import gc\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from scipy import stats\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "sys.path.insert(0, \"../..\")\n",
    "\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "RESULTS_DIR = Path(\"../../results/exp03b\")\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CHECKPOINT_PATH = RESULTS_DIR / \"checkpoint.json\"\n",
    "\n",
    "N_SAMPLES = 400\n",
    "MODEL_NAME = \"google/t5gemma-2-4b-4b\"\n",
    "LENGTH_BINS = [\"original\", \"512\", \"1024\", \"2048\", \"3072\", \"4096\", \"6144\"]\n",
    "N_BONFERRONI = 49  # 7 non-bare conditions x 7 lengths\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv())\n",
    "HF_TOKEN = os.environ.get(\"HF_TOKEN\")\n",
    "\n",
    "print(f\"Exp 03B: Extended Length Scaling with All Surrogate Types\")\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"N: {N_SAMPLES}\")\n",
    "print(f\"Length bins: {LENGTH_BINS}\")\n",
    "print(f\"Bonferroni comparisons: {N_BONFERRONI}\")\n",
    "print(f\"CUDA: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 3: Load model\n",
    "from transformers import AutoProcessor, AutoModelForSeq2SeqLM\n",
    "\n",
    "print(f\"Loading {MODEL_NAME}...\")\n",
    "processor = AutoProcessor.from_pretrained(MODEL_NAME, token=HF_TOKEN)\n",
    "tokenizer = processor.tokenizer\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    token=HF_TOKEN,\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "DEVICE = next(model.parameters()).device\n",
    "print(f\"Model loaded. dtype={next(model.parameters()).dtype}\")\n",
    "print(f\"GPU memory used: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 4: Scoring helpers\n",
    "\n",
    "def score_nll(encoder_text, answer_text, prefix_token_count=0, truncate=False):\n",
    "    '''Score NLL of answer tokens with optional truncation.'''\n",
    "    # Tokenize encoder input — no truncation limit (let model handle long sequences)\n",
    "    enc_ids = tokenizer(encoder_text, return_tensors=\"pt\",\n",
    "                        add_special_tokens=True, truncation=True,\n",
    "                        max_length=8192).input_ids.to(DEVICE)\n",
    "    total_enc_len = enc_ids.shape[1]\n",
    "\n",
    "    # Full mask for encoder (bidirectional, sees everything)\n",
    "    enc_mask = torch.ones(1, total_enc_len, device=DEVICE, dtype=torch.long)\n",
    "\n",
    "    # Run encoder with full bidirectional attention\n",
    "    with torch.no_grad():\n",
    "        encoder_outputs = model.get_encoder()(\n",
    "            input_ids=enc_ids, attention_mask=enc_mask\n",
    "        )\n",
    "\n",
    "    # Build cross-attention mask for decoder\n",
    "    if truncate and prefix_token_count > 0:\n",
    "        cross_attn_mask = torch.ones(1, total_enc_len, device=DEVICE, dtype=torch.long)\n",
    "        cross_attn_mask[:, :prefix_token_count] = 0\n",
    "    else:\n",
    "        cross_attn_mask = torch.ones(1, total_enc_len, device=DEVICE, dtype=torch.long)\n",
    "\n",
    "    # Tokenize answer for decoder\n",
    "    ans_ids = tokenizer(answer_text, return_tensors=\"pt\",\n",
    "                        add_special_tokens=False, truncation=True,\n",
    "                        max_length=256).input_ids.to(DEVICE)\n",
    "\n",
    "    if ans_ids.shape[1] == 0:\n",
    "        return 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            encoder_outputs=encoder_outputs,\n",
    "            attention_mask=cross_attn_mask,\n",
    "            labels=ans_ids,\n",
    "        )\n",
    "\n",
    "    logits = outputs.logits\n",
    "    log_probs = F.log_softmax(logits, dim=-1)\n",
    "    token_log_probs = log_probs[0].gather(1, ans_ids[0].unsqueeze(1)).squeeze(1)\n",
    "    mean_nll = -token_log_probs.mean().item()\n",
    "\n",
    "    del encoder_outputs, outputs, logits, log_probs\n",
    "    return mean_nll\n",
    "\n",
    "\n",
    "def count_prefix_tokens(prefix_text, document_text):\n",
    "    '''Count how many tokens the prefix occupies in the concatenated encoding.'''\n",
    "    full_text = prefix_text + \"\\n\" + document_text\n",
    "    full_ids = tokenizer(full_text, add_special_tokens=True).input_ids\n",
    "    doc_ids = tokenizer(document_text, add_special_tokens=True).input_ids\n",
    "    return len(full_ids) - len(doc_ids)\n",
    "\n",
    "\n",
    "# === Surrogate generation ===\n",
    "STOP_WORDS = {\n",
    "    'a', 'an', 'the', 'is', 'are', 'was', 'were', 'be', 'been', 'being',\n",
    "    'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could',\n",
    "    'should', 'may', 'might', 'can', 'shall', 'to', 'of', 'in', 'for',\n",
    "    'on', 'with', 'at', 'by', 'from', 'as', 'into', 'through', 'during',\n",
    "    'before', 'after', 'above', 'below', 'between', 'and', 'but', 'or',\n",
    "    'not', 'no', 'if', 'then', 'than', 'so', 'up', 'out', 'about',\n",
    "    'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those',\n",
    "    'it', 'its', 'i', 'me', 'my', 'we', 'our', 'you', 'your', 'he',\n",
    "    'him', 'his', 'she', 'her', 'they', 'them', 'their', 'how', 'when',\n",
    "    'where', 'why', 'much', 'many', 'some', 'any', 'all', 'each',\n",
    "    'does', 'also', 'just', 'more', 'most', 'very', 'too', 'only',\n",
    "}\n",
    "\n",
    "def extract_keywords(text):\n",
    "    words = re.sub(r'[^\\w\\s]', '', text.lower()).split()\n",
    "    return [w for w in words if w not in STOP_WORDS and len(w) > 2]\n",
    "\n",
    "def make_surrogate_doc_kw(passage):\n",
    "    content_words = extract_keywords(passage)\n",
    "    if not content_words:\n",
    "        return \"information\"\n",
    "    counts = Counter(content_words)\n",
    "    return \" \".join(w for w, _ in counts.most_common(5))\n",
    "\n",
    "def make_surrogate_template(passage):\n",
    "    content_words = extract_keywords(passage)\n",
    "    if not content_words:\n",
    "        return \"What is this about?\"\n",
    "    counts = Counter(content_words)\n",
    "    top_word = counts.most_common(1)[0][0]\n",
    "    return f\"What is {top_word}?\"\n",
    "\n",
    "STATIC_FACT = \"What are the key facts I need to know?\"\n",
    "\n",
    "# Build a vocabulary pool for random_matched surrogates\n",
    "# Use content words from all passages — populated after data loading\n",
    "VOCAB_POOL = []\n",
    "\n",
    "def make_surrogate_random_matched(query, vocab_pool, rng):\n",
    "    '''Random words, same count as query — length-matched structural control.'''\n",
    "    n_words = len(query.split())\n",
    "    if len(vocab_pool) == 0:\n",
    "        return query  # fallback\n",
    "    words = rng.choice(vocab_pool, size=min(n_words, len(vocab_pool)), replace=True).tolist()\n",
    "    return \" \".join(words)\n",
    "\n",
    "def make_surrogate_scrambled_oracle(query, rng):\n",
    "    '''Query words in random order — vocabulary control.'''\n",
    "    words = query.split()\n",
    "    if len(words) <= 1:\n",
    "        return query\n",
    "    rng.shuffle(words)\n",
    "    return \" \".join(words)\n",
    "\n",
    "print(\"Helpers defined.\")\n",
    "print(\"  Conditions: bare, oracle_trunc, scrambled_oracle_trunc, random_matched_trunc,\")\n",
    "print(\"              random_trunc, static_fact_trunc, surr_template_trunc, surr_doc_trunc\")\n",
    "print(\"  Three-way decomposition: Structure | Vocabulary | Semantics\")\n",
    "print(f\"  NOTE: max_length=8192 for long padded documents\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 5: Load data and build padding pool\n",
    "from lib.data import count_words\n",
    "from datasets import load_dataset\n",
    "\n",
    "print(\"Loading MS MARCO v1.1 validation...\")\n",
    "ds = load_dataset(\"microsoft/ms_marco\", \"v1.1\", split=\"validation\")\n",
    "\n",
    "# Collect target samples AND a large padding pool\n",
    "samples = []\n",
    "padding_pool = []\n",
    "\n",
    "for item in ds:\n",
    "    passages = item.get('passages', {})\n",
    "    ptexts = passages.get('passage_text', [])\n",
    "    is_sel = passages.get('is_selected', [])\n",
    "    query = item.get('query', '')\n",
    "    answers = item.get('answers', [])\n",
    "    well_formed = item.get('wellFormedAnswers', [])\n",
    "\n",
    "    answer = None\n",
    "    if well_formed and len(well_formed) > 0 and well_formed[0] not in ('[]', ''):\n",
    "        answer = well_formed[0]\n",
    "    elif answers and len(answers) > 0 and answers[0] != 'No Answer Present.':\n",
    "        answer = answers[0]\n",
    "\n",
    "    for pt, sel in zip(ptexts, is_sel):\n",
    "        wc = count_words(pt)\n",
    "        if sel == 1 and 30 <= wc <= 300 and answer:\n",
    "            if len(samples) < N_SAMPLES * 3:\n",
    "                samples.append({\n",
    "                    'passage': pt, 'query': query, 'answer': answer,\n",
    "                    'word_count': wc\n",
    "                })\n",
    "        elif sel == 0 and 20 <= wc <= 200:\n",
    "            padding_pool.append(pt)\n",
    "\n",
    "    # Need a very large pool for 6144-token documents\n",
    "    if len(samples) >= N_SAMPLES * 3 and len(padding_pool) >= 20000:\n",
    "        break\n",
    "\n",
    "np.random.seed(SEED)\n",
    "np.random.shuffle(samples)\n",
    "samples = samples[:N_SAMPLES]\n",
    "np.random.shuffle(padding_pool)\n",
    "\n",
    "del ds\n",
    "gc.collect()\n",
    "\n",
    "# Build vocabulary pool from all passages for random_matched\n",
    "all_passage_words = []\n",
    "for s in samples:\n",
    "    all_passage_words.extend(extract_keywords(s['passage']))\n",
    "VOCAB_POOL.extend(list(set(all_passage_words)))\n",
    "print(f\"Vocabulary pool: {len(VOCAB_POOL)} unique content words\")\n",
    "\n",
    "# Generate surrogates\n",
    "rng = np.random.RandomState(SEED + 1)  # Separate RNG for surrogates\n",
    "for i, s in enumerate(samples):\n",
    "    s['surr_doc_kw'] = make_surrogate_doc_kw(s['passage'])\n",
    "    s['surr_template'] = make_surrogate_template(s['passage'])\n",
    "    # Random: use passage from a different sample (circular offset)\n",
    "    other_idx = (i + N_SAMPLES // 2) % len(samples)\n",
    "    s['surr_random'] = \" \".join(samples[other_idx]['passage'].split()[:20])\n",
    "    # Decomposition surrogates\n",
    "    s['surr_scrambled_oracle'] = make_surrogate_scrambled_oracle(s['query'], rng)\n",
    "    s['surr_random_matched'] = make_surrogate_random_matched(s['query'], np.array(VOCAB_POOL), rng)\n",
    "\n",
    "print(f\"Selected {len(samples)} target samples, mean words={np.mean([s['word_count'] for s in samples]):.0f}\")\n",
    "print(f\"Padding pool: {len(padding_pool)} unrelated passages\")\n",
    "\n",
    "# Show token counts for target passages\n",
    "target_tok_counts = []\n",
    "for s in samples:\n",
    "    toks = tokenizer(s['passage'], add_special_tokens=True).input_ids\n",
    "    target_tok_counts.append(len(toks))\n",
    "print(f\"Target passage tokens: mean={np.mean(target_tok_counts):.0f}, \"\n",
    "      f\"median={np.median(target_tok_counts):.0f}, \"\n",
    "      f\"min={np.min(target_tok_counts)}, max={np.max(target_tok_counts)}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 6: Build padded documents at each length bin\n",
    "print(\"=\" * 70)\n",
    "print(\"BUILDING PADDED DOCUMENTS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "TARGET_LENGTHS = {\n",
    "    \"original\": None,\n",
    "    \"512\": 512,\n",
    "    \"1024\": 1024,\n",
    "    \"2048\": 2048,\n",
    "    \"3072\": 3072,\n",
    "    \"4096\": 4096,\n",
    "    \"6144\": 6144,\n",
    "}\n",
    "\n",
    "def pad_passage_to_length(passage, target_tokens, padding_pool, pool_offset):\n",
    "    '''Pad a passage to target_tokens by appending unrelated passages.'''\n",
    "    if target_tokens is None:\n",
    "        toks = tokenizer(passage, add_special_tokens=True).input_ids\n",
    "        return passage, len(toks), 0\n",
    "\n",
    "    current_ids = tokenizer(passage, add_special_tokens=True).input_ids\n",
    "    if len(current_ids) >= target_tokens:\n",
    "        return passage, len(current_ids), 0\n",
    "\n",
    "    padded = passage\n",
    "    n_used = 0\n",
    "    idx = pool_offset\n",
    "\n",
    "    while True:\n",
    "        if idx >= len(padding_pool):\n",
    "            idx = 0\n",
    "        candidate = padded + \"\\n\\n\" + padding_pool[idx]\n",
    "        candidate_ids = tokenizer(candidate, add_special_tokens=True).input_ids\n",
    "        if len(candidate_ids) >= target_tokens:\n",
    "            # Fine-grained: add words until we hit target\n",
    "            pad_words = padding_pool[idx].split()\n",
    "            for w_end in range(1, len(pad_words) + 1):\n",
    "                partial = padded + \"\\n\\n\" + \" \".join(pad_words[:w_end])\n",
    "                partial_ids = tokenizer(partial, add_special_tokens=True).input_ids\n",
    "                if len(partial_ids) >= target_tokens:\n",
    "                    padded = partial\n",
    "                    break\n",
    "            else:\n",
    "                padded = candidate\n",
    "            n_used += 1\n",
    "            break\n",
    "        padded = candidate\n",
    "        n_used += 1\n",
    "        idx += 1\n",
    "\n",
    "    final_ids = tokenizer(padded, add_special_tokens=True).input_ids\n",
    "    return padded, len(final_ids), n_used\n",
    "\n",
    "\n",
    "# Build padded versions for each sample at each length\n",
    "padded_docs = {}\n",
    "padded_stats = {}\n",
    "\n",
    "for length_bin, target_tokens in TARGET_LENGTHS.items():\n",
    "    padded_docs[length_bin] = []\n",
    "    tok_counts = []\n",
    "\n",
    "    for i, s in enumerate(samples):\n",
    "        pool_offset = i * 100  # Wider spread for longer docs\n",
    "        padded_text, actual_tokens, n_pad = pad_passage_to_length(\n",
    "            s['passage'], target_tokens, padding_pool, pool_offset\n",
    "        )\n",
    "        padded_docs[length_bin].append(padded_text)\n",
    "        tok_counts.append(actual_tokens)\n",
    "\n",
    "    padded_stats[length_bin] = {\n",
    "        'mean': np.mean(tok_counts),\n",
    "        'min': int(np.min(tok_counts)),\n",
    "        'max': int(np.max(tok_counts)),\n",
    "        'median': np.median(tok_counts),\n",
    "    }\n",
    "\n",
    "    print(f\"  {length_bin:>8s}: mean={padded_stats[length_bin]['mean']:.0f} tokens \"\n",
    "          f\"(min={padded_stats[length_bin]['min']}, max={padded_stats[length_bin]['max']}, \"\n",
    "          f\"median={padded_stats[length_bin]['median']:.0f})\")\n",
    "\n",
    "# Preview\n",
    "print(f\"\\n--- Sample 0 preview ---\")\n",
    "print(f\"  Query:  {samples[0]['query'][:80]}\")\n",
    "print(f\"  Answer: {samples[0]['answer'][:80]}\")\n",
    "for lb in LENGTH_BINS:\n",
    "    preview = padded_docs[lb][0]\n",
    "    tok_count = len(tokenizer(preview, add_special_tokens=True).input_ids)\n",
    "    print(f\"  {lb:>8s}: {tok_count} tokens\")\n",
    "\n",
    "# Show actual prefix text for each non-bare condition\n",
    "print(f\"\\n--- Actual prefix text for each condition (sample 0) ---\")\n",
    "ex = samples[0]\n",
    "ex_doc = padded_docs[\"original\"][0]\n",
    "surr_items = {\n",
    "    'oracle': ex['query'],\n",
    "    'scrambled_oracle': ex['surr_scrambled_oracle'],\n",
    "    'random_matched': ex['surr_random_matched'],\n",
    "    'random (~20w)': ex['surr_random'],\n",
    "    'static_fact': STATIC_FACT,\n",
    "    'surr_template': ex['surr_template'],\n",
    "    'surr_doc (kw)': ex['surr_doc_kw'],\n",
    "}\n",
    "for name, text in surr_items.items():\n",
    "    ptoks = count_prefix_tokens(text, ex_doc)\n",
    "    print(f\"  {name:<22} ({ptoks:>3} prefix toks): {str(text)[:60]}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 7: Explain conditions and estimate runtime\n",
    "print(\"=\" * 70)\n",
    "print(\"EXPERIMENTAL CONDITIONS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "ex = samples[0]\n",
    "ex_doc = padded_docs[\"original\"][0]\n",
    "\n",
    "oracle_prefix = count_prefix_tokens(ex['query'], ex_doc)\n",
    "scrambled_prefix = count_prefix_tokens(ex['surr_scrambled_oracle'], ex_doc)\n",
    "matched_prefix = count_prefix_tokens(ex['surr_random_matched'], ex_doc)\n",
    "random_prefix = count_prefix_tokens(ex['surr_random'], ex_doc)\n",
    "static_prefix = count_prefix_tokens(STATIC_FACT, ex_doc)\n",
    "template_prefix = count_prefix_tokens(ex['surr_template'], ex_doc)\n",
    "doc_prefix = count_prefix_tokens(ex['surr_doc_kw'], ex_doc)\n",
    "\n",
    "print(\"CONDITIONS (all with truncation):\")\n",
    "print()\n",
    "print(f\"  {'CONDITION':<26} {'SURROGATE TYPE':<28} {'PREFIX TOKENS':<16}\")\n",
    "print(f\"  {'-'*70}\")\n",
    "print(f\"  {'bare':<26} {'(none)':<28} {'0':<16}\")\n",
    "print(f\"  {'oracle_trunc':<26} {'Real query':<28} {'~' + str(oracle_prefix):<16}\")\n",
    "print(f\"  {'scrambled_oracle_trunc':<26} {'Query words, random order':<28} {'~' + str(scrambled_prefix):<16}\")\n",
    "print(f\"  {'random_matched_trunc':<26} {'Random words, query length':<28} {'~' + str(matched_prefix):<16}\")\n",
    "print(f\"  {'random_trunc':<26} {'Unrelated passage (~20w)':<28} {'~' + str(random_prefix):<16}\")\n",
    "print(f\"  {'static_fact_trunc':<26} {'Fixed question':<28} {'~' + str(static_prefix):<16}\")\n",
    "print(f\"  {'surr_template_trunc':<26} {'What is [keyword]?':<28} {'~' + str(template_prefix):<16}\")\n",
    "print(f\"  {'surr_doc_trunc':<26} {'Top-5 TF keywords':<28} {'~' + str(doc_prefix):<16}\")\n",
    "print()\n",
    "print(\"THREE-WAY DECOMPOSITION:\")\n",
    "print(\"  Structure  = bare -> random_matched       (any prefix helps)\")\n",
    "print(\"  Vocabulary = random_matched -> scrambled   (right words, wrong order)\")\n",
    "print(\"  Semantics  = scrambled -> oracle           (right word order)\")\n",
    "print()\n",
    "print(f\"LENGTH BINS: {LENGTH_BINS}\")\n",
    "print(f\"ENCODER SLIDING WINDOW: 1024 tokens (full attention every 6th layer)\")\n",
    "N_CONDITIONS = 8  # bare + 7 non-bare\n",
    "n_calls = N_SAMPLES * len(LENGTH_BINS) * N_CONDITIONS\n",
    "print(f\"Total scoring calls: {N_SAMPLES} x {len(LENGTH_BINS)} x {N_CONDITIONS} = {n_calls}\")\n",
    "print(f\"Bonferroni: {N_BONFERRONI} comparisons (7 conditions x 7 lengths)\")\n",
    "\n",
    "# Rough runtime estimate: longer docs take longer\n",
    "avg_time_per_call = {\n",
    "    \"original\": 0.4, \"512\": 0.6, \"1024\": 0.9, \"2048\": 1.5,\n",
    "    \"3072\": 2.2, \"4096\": 3.0, \"6144\": 5.0,\n",
    "}\n",
    "total_est = sum(N_SAMPLES * N_CONDITIONS * avg_time_per_call.get(lb, 3.0) for lb in LENGTH_BINS)\n",
    "print(f\"Estimated runtime: ~{total_est/3600:.1f} hours\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 8: Run scoring — outer loop over length bins\n",
    "print(\"=\" * 70)\n",
    "print(\"RUNNING EXPERIMENT\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "COND_NAMES = ['bare', 'oracle_trunc', 'scrambled_oracle_trunc', 'random_matched_trunc',\n",
    "              'random_trunc', 'static_fact_trunc', 'surr_template_trunc', 'surr_doc_trunc']\n",
    "\n",
    "def make_conditions(sample, padded_passage):\n",
    "    '''Return dict of {name: (encoder_text, prefix_token_count, truncate)}'''\n",
    "    query = sample['query']\n",
    "    surr_map = {\n",
    "        'oracle':           query,\n",
    "        'scrambled_oracle':  sample['surr_scrambled_oracle'],\n",
    "        'random_matched':    sample['surr_random_matched'],\n",
    "        'random':            sample['surr_random'],\n",
    "        'static_fact':       STATIC_FACT,\n",
    "        'surr_template':     sample['surr_template'],\n",
    "        'surr_doc':          sample['surr_doc_kw'],\n",
    "    }\n",
    "\n",
    "    conditions = {'bare': (padded_passage, 0, False)}\n",
    "\n",
    "    for surr_name, surr_text in surr_map.items():\n",
    "        cond_name = f'{surr_name}_trunc'\n",
    "        enc_text = surr_text + \"\\n\" + padded_passage\n",
    "        prefix_count = count_prefix_tokens(surr_text, padded_passage)\n",
    "        conditions[cond_name] = (enc_text, prefix_count, True)\n",
    "\n",
    "    return conditions\n",
    "\n",
    "# Resume from checkpoint\n",
    "all_checkpoint = {}\n",
    "if CHECKPOINT_PATH.exists():\n",
    "    saved = json.loads(CHECKPOINT_PATH.read_text())\n",
    "    if saved.get('n_total') == N_SAMPLES:\n",
    "        all_checkpoint = saved.get('bins', {})\n",
    "        summary = ', '.join(f'{k}={len(v.get(\"results\",[]))}' for k,v in all_checkpoint.items())\n",
    "        print(f\"Loaded checkpoint: {summary}\")\n",
    "\n",
    "t0_total = time.time()\n",
    "\n",
    "for length_bin in LENGTH_BINS:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"LENGTH BIN: {length_bin} (target={TARGET_LENGTHS[length_bin] or 'no padding'})\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "    # Check for existing results for this bin\n",
    "    bin_results = []\n",
    "    start_idx = 0\n",
    "    if length_bin in all_checkpoint:\n",
    "        bin_data = all_checkpoint[length_bin]\n",
    "        saved_results = bin_data.get('results', [])\n",
    "        saved_queries = [r['query'][:50] for r in saved_results]\n",
    "        current_queries = [s['query'][:50] for s in samples[:len(saved_queries)]]\n",
    "        if saved_queries == current_queries:\n",
    "            bin_results = saved_results\n",
    "            start_idx = len(bin_results)\n",
    "            print(f\"  Resuming from sample {start_idx}/{N_SAMPLES}\")\n",
    "\n",
    "    if start_idx >= N_SAMPLES:\n",
    "        print(f\"  Already complete ({len(bin_results)} results)\")\n",
    "        all_checkpoint[length_bin] = {\"results\": bin_results, \"completed\": N_SAMPLES}\n",
    "        continue\n",
    "\n",
    "    t0_bin = time.time()\n",
    "\n",
    "    for i in tqdm(range(start_idx, N_SAMPLES), initial=start_idx, total=N_SAMPLES,\n",
    "                  desc=f\"  {length_bin}\"):\n",
    "        s = samples[i]\n",
    "        padded_passage = padded_docs[length_bin][i]\n",
    "        conditions = make_conditions(s, padded_passage)\n",
    "\n",
    "        result = {\n",
    "            'query': s['query'],\n",
    "            'answer': s['answer'],\n",
    "            'passage_words': s['word_count'],\n",
    "            'padded_tokens': len(tokenizer(padded_passage, add_special_tokens=True).input_ids),\n",
    "        }\n",
    "\n",
    "        for cond_name in COND_NAMES:\n",
    "            enc_text, prefix_count, trunc = conditions[cond_name]\n",
    "            nll = score_nll(enc_text, s['answer'], prefix_count, trunc)\n",
    "            result[f'nll_{cond_name}'] = nll\n",
    "\n",
    "        bin_results.append(result)\n",
    "\n",
    "        if (i + 1) % 20 == 0 or i == N_SAMPLES - 1:\n",
    "            all_checkpoint[length_bin] = {\"results\": bin_results, \"completed\": len(bin_results)}\n",
    "            ckpt = {\n",
    "                'n_total': N_SAMPLES,\n",
    "                'bins': all_checkpoint,\n",
    "                'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            }\n",
    "            CHECKPOINT_PATH.write_text(json.dumps(ckpt))\n",
    "            elapsed_bin = time.time() - t0_bin\n",
    "            done = i - start_idx + 1\n",
    "            eta = (N_SAMPLES - i - 1) * elapsed_bin / done if done > 0 else 0\n",
    "            tqdm.write(f\"    Checkpoint {i+1}/{N_SAMPLES} | {elapsed_bin/60:.1f}m | ETA {eta/60:.1f}m\")\n",
    "\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    elapsed_bin = time.time() - t0_bin\n",
    "    print(f\"  {length_bin} complete: {len(bin_results)} samples in {elapsed_bin/60:.1f} min\")\n",
    "\n",
    "    # Quick peek\n",
    "    bare_nlls = np.array([r['nll_bare'] for r in bin_results])\n",
    "    oracle_nlls = np.array([r['nll_oracle_trunc'] for r in bin_results])\n",
    "    random_nlls = np.array([r['nll_random_trunc'] for r in bin_results])\n",
    "    from lib.analysis import cohens_d\n",
    "    d_oracle = cohens_d(bare_nlls - oracle_nlls)\n",
    "    d_random = cohens_d(bare_nlls - random_nlls)\n",
    "    print(f\"  Quick peek: oracle d={d_oracle:+.3f}, random d={d_random:+.3f}\")\n",
    "\n",
    "elapsed_total = time.time() - t0_total\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"ALL BINS COMPLETE: {elapsed_total/60:.1f} min total\")\n",
    "print(f\"{'='*70}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 9: Results — per-length table\n",
    "from lib.analysis import cohens_d\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"RESULTS: Per-Length Condition Comparison\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "results_by_bin = {}\n",
    "for length_bin in LENGTH_BINS:\n",
    "    results_by_bin[length_bin] = all_checkpoint[length_bin]['results']\n",
    "\n",
    "analysis = {}\n",
    "for length_bin in LENGTH_BINS:\n",
    "    bin_results = results_by_bin[length_bin]\n",
    "    n = len(bin_results)\n",
    "    bare_nlls = np.array([r['nll_bare'] for r in bin_results])\n",
    "    mean_tokens = np.mean([r['padded_tokens'] for r in bin_results])\n",
    "\n",
    "    print(f\"\\n--- {length_bin} (mean {mean_tokens:.0f} tokens, N={n}) ---\")\n",
    "    print(f\"  {'Condition':<22} {'Mean NLL':>10} {'vs Bare':>10} {'d':>8} {'Win%':>8} {'p':>12} {'sig':>5}\")\n",
    "    print(f\"  {'-'*82}\")\n",
    "\n",
    "    analysis[length_bin] = {}\n",
    "    for cond in COND_NAMES:\n",
    "        nlls = np.array([r[f'nll_{cond}'] for r in bin_results])\n",
    "        mean_nll = nlls.mean()\n",
    "        diff = bare_nlls - nlls\n",
    "\n",
    "        if cond == 'bare':\n",
    "            print(f\"  {cond:<22} {mean_nll:>10.4f} {'--':>10} {'--':>8} {'--':>8} {'--':>12} {'--':>5}\")\n",
    "            analysis[length_bin][cond] = {'mean_nll': float(mean_nll)}\n",
    "        else:\n",
    "            d = cohens_d(diff)\n",
    "            win_pct = 100 * np.mean(diff > 0)\n",
    "            t_stat, p_val = stats.ttest_1samp(diff, 0)\n",
    "            sig = '***' if p_val < 0.001/N_BONFERRONI else '**' if p_val < 0.01/N_BONFERRONI else '*' if p_val < 0.05/N_BONFERRONI else 'ns'\n",
    "            print(f\"  {cond:<22} {mean_nll:>10.4f} {diff.mean():>+10.4f} {d:>+8.3f} {win_pct:>7.1f}% {p_val:>12.2e} {sig:>5}\")\n",
    "            analysis[length_bin][cond] = {\n",
    "                'mean_nll': float(mean_nll), 'delta': float(diff.mean()),\n",
    "                'd': float(d), 'win_pct': float(win_pct), 'p': float(p_val),\n",
    "            }"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 10: Structural vs Semantic analysis across lengths\n",
    "print(\"=\" * 70)\n",
    "print(\"STRUCTURAL vs SEMANTIC ACROSS LENGTHS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Key comparison: random (structural) vs oracle (structural + semantic)\n",
    "print(f\"\\n--- Structural floor (random) vs Oracle across lengths ---\")\n",
    "print(f\"  {'Length':<10} {'oracle d':>10} {'random d':>10} {'gap':>8} {'random/oracle':>14} {'semantic p':>12}\")\n",
    "print(f\"  {'-'*68}\")\n",
    "\n",
    "for length_bin in LENGTH_BINS:\n",
    "    bin_results = results_by_bin[length_bin]\n",
    "    bare_nlls = np.array([r['nll_bare'] for r in bin_results])\n",
    "    oracle_nlls = np.array([r['nll_oracle_trunc'] for r in bin_results])\n",
    "    random_nlls = np.array([r['nll_random_trunc'] for r in bin_results])\n",
    "\n",
    "    d_oracle = cohens_d(bare_nlls - oracle_nlls)\n",
    "    d_random = cohens_d(bare_nlls - random_nlls)\n",
    "    gap = d_oracle - d_random\n",
    "    ratio = d_random / d_oracle * 100 if d_oracle > 0 else 0\n",
    "\n",
    "    # Direct oracle vs random test\n",
    "    diff_or = oracle_nlls - random_nlls  # positive = random has higher NLL (oracle better)\n",
    "    t_or, p_or = stats.ttest_1samp(diff_or, 0) if np.std(diff_or) > 0 else (0, 1)\n",
    "\n",
    "    print(f\"  {length_bin:<10} {d_oracle:>+10.3f} {d_random:>+10.3f} {gap:>+8.3f} {ratio:>13.0f}% {p_or:>12.2e}\")\n",
    "\n",
    "# All surrogate types across lengths\n",
    "print(f\"\\n--- All conditions: Cohen's d vs Length ---\")\n",
    "header = f\"  {'Length':<10}\"\n",
    "for cond in COND_NAMES[1:]:  # skip bare\n",
    "    short = cond.replace('_trunc', '').replace('surr_', '').replace('static_', 's_')\n",
    "    header += f\" {short:>10}\"\n",
    "print(header)\n",
    "print(f\"  {'-'*(10 + 11 * len(COND_NAMES[1:]))}\")\n",
    "\n",
    "for length_bin in LENGTH_BINS:\n",
    "    row = f\"  {length_bin:<10}\"\n",
    "    for cond in COND_NAMES[1:]:\n",
    "        a = analysis[length_bin].get(cond, {})\n",
    "        d = a.get('d', 0)\n",
    "        p = a.get('p', 1)\n",
    "        sig = '***' if p < 0.001/N_BONFERRONI else ' **' if p < 0.01/N_BONFERRONI else '  *' if p < 0.05/N_BONFERRONI else ' ns'\n",
    "        row += f\" {d:>+6.3f}{sig}\"\n",
    "\n",
    "    print(row)\n",
    "\n",
    "# Content-specific vs content-agnostic by length\n",
    "print(f\"\\n--- Content-specific vs Content-agnostic by length ---\")\n",
    "print(f\"  {'Length':<10} {'content-spec':>14} {'content-agn':>14} {'gap':>8}\")\n",
    "print(f\"  {'-'*50}\")\n",
    "\n",
    "for length_bin in LENGTH_BINS:\n",
    "    bin_results = results_by_bin[length_bin]\n",
    "    bare_nlls = np.array([r['nll_bare'] for r in bin_results])\n",
    "\n",
    "    # Content-specific: surr_doc, surr_template\n",
    "    cs_nlls = np.mean([\n",
    "        np.array([r['nll_surr_doc_trunc'] for r in bin_results]),\n",
    "        np.array([r['nll_surr_template_trunc'] for r in bin_results]),\n",
    "    ], axis=0)\n",
    "    d_cs = cohens_d(bare_nlls - cs_nlls)\n",
    "\n",
    "    # Content-agnostic: random, static_fact\n",
    "    ca_nlls = np.mean([\n",
    "        np.array([r['nll_random_trunc'] for r in bin_results]),\n",
    "        np.array([r['nll_static_fact_trunc'] for r in bin_results]),\n",
    "    ], axis=0)\n",
    "    d_ca = cohens_d(bare_nlls - ca_nlls)\n",
    "\n",
    "    gap = d_cs - d_ca\n",
    "    print(f\"  {length_bin:<10} {d_cs:>+14.3f} {d_ca:>+14.3f} {gap:>+8.3f}\")\n",
    "\n",
    "# Three-way decomposition at each length\n",
    "print(f\"\\n--- THREE-WAY DECOMPOSITION (Structure / Vocabulary / Semantics) ---\")\n",
    "print(f\"  {'Length':<10} {'Structure':>12} {'Vocabulary':>12} {'Semantics':>12} {'Total':>10} {'Struct%':>10} {'Vocab%':>10} {'Sem%':>10}\")\n",
    "print(f\"  {'-'*90}\")\n",
    "\n",
    "for length_bin in LENGTH_BINS:\n",
    "    bin_results = results_by_bin[length_bin]\n",
    "    bare_nlls = np.array([r['nll_bare'] for r in bin_results])\n",
    "    oracle_nlls = np.array([r['nll_oracle_trunc'] for r in bin_results])\n",
    "    scrambled_nlls = np.array([r['nll_scrambled_oracle_trunc'] for r in bin_results])\n",
    "    matched_nlls = np.array([r['nll_random_matched_trunc'] for r in bin_results])\n",
    "\n",
    "    # Three-way decomposition (per sample, then average)\n",
    "    structure = bare_nlls - matched_nlls          # bare -> random_matched\n",
    "    vocabulary = matched_nlls - scrambled_nlls     # random_matched -> scrambled_oracle\n",
    "    semantics = scrambled_nlls - oracle_nlls       # scrambled_oracle -> oracle\n",
    "    total = bare_nlls - oracle_nlls                # bare -> oracle\n",
    "\n",
    "    s_mean = structure.mean()\n",
    "    v_mean = vocabulary.mean()\n",
    "    sem_mean = semantics.mean()\n",
    "    t_mean = total.mean()\n",
    "\n",
    "    s_pct = s_mean / t_mean * 100 if t_mean > 0 else 0\n",
    "    v_pct = v_mean / t_mean * 100 if t_mean > 0 else 0\n",
    "    sem_pct = sem_mean / t_mean * 100 if t_mean > 0 else 0\n",
    "\n",
    "    print(f\"  {length_bin:<10} {s_mean:>+12.4f} {v_mean:>+12.4f} {sem_mean:>+12.4f} {t_mean:>+10.4f} {s_pct:>9.0f}% {v_pct:>9.0f}% {sem_pct:>9.0f}%\")\n",
    "\n",
    "# Statistical tests for each component\n",
    "print(f\"\\n--- Component significance at each length ---\")\n",
    "print(f\"  {'Length':<10} {'Structure':>20} {'Vocabulary':>20} {'Semantics':>20}\")\n",
    "print(f\"  {'-'*72}\")\n",
    "\n",
    "for length_bin in LENGTH_BINS:\n",
    "    bin_results = results_by_bin[length_bin]\n",
    "    bare_nlls = np.array([r['nll_bare'] for r in bin_results])\n",
    "    oracle_nlls = np.array([r['nll_oracle_trunc'] for r in bin_results])\n",
    "    scrambled_nlls = np.array([r['nll_scrambled_oracle_trunc'] for r in bin_results])\n",
    "    matched_nlls = np.array([r['nll_random_matched_trunc'] for r in bin_results])\n",
    "\n",
    "    structure = bare_nlls - matched_nlls\n",
    "    vocabulary = matched_nlls - scrambled_nlls\n",
    "    semantics = scrambled_nlls - oracle_nlls\n",
    "\n",
    "    parts = []\n",
    "    for comp, label in [(structure, 'struct'), (vocabulary, 'vocab'), (semantics, 'sem')]:\n",
    "        d = cohens_d(comp)\n",
    "        t, p = stats.ttest_1samp(comp, 0)\n",
    "        sig = '***' if p < 0.001/N_BONFERRONI else '**' if p < 0.01/N_BONFERRONI else '*' if p < 0.05/N_BONFERRONI else 'ns'\n",
    "        parts.append(f\"d={d:+.3f} {sig:>3s}\")\n",
    "\n",
    "    print(f\"  {length_bin:<10} {parts[0]:>20} {parts[1]:>20} {parts[2]:>20}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 11: Decay curve plots\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "x_tokens = [padded_stats[lb]['mean'] for lb in LENGTH_BINS]\n",
    "\n",
    "# --- Panel 1: All conditions decay curves ---\n",
    "ax = axes[0, 0]\n",
    "colors = {\n",
    "    'oracle_trunc': 'tab:red', 'scrambled_oracle_trunc': 'tab:pink',\n",
    "    'random_matched_trunc': 'tab:olive', 'random_trunc': 'tab:gray',\n",
    "    'static_fact_trunc': 'tab:orange', 'surr_template_trunc': 'tab:purple',\n",
    "    'surr_doc_trunc': 'tab:blue',\n",
    "}\n",
    "markers = {\n",
    "    'oracle_trunc': 'o', 'scrambled_oracle_trunc': 'v',\n",
    "    'random_matched_trunc': 'P', 'random_trunc': 'x',\n",
    "    'static_fact_trunc': 's', 'surr_template_trunc': '^',\n",
    "    'surr_doc_trunc': 'D',\n",
    "}\n",
    "\n",
    "for cond in COND_NAMES[1:]:\n",
    "    d_vals = [analysis[lb].get(cond, {}).get('d', 0) for lb in LENGTH_BINS]\n",
    "    ax.plot(x_tokens, d_vals, f'-{markers[cond]}', color=colors[cond],\n",
    "            label=cond.replace('_trunc', ''), markersize=7)\n",
    "\n",
    "ax.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "ax.axvline(x=1024, color='gray', linestyle=':', alpha=0.3, label='sliding window')\n",
    "ax.set_xlabel('Document Length (tokens)')\n",
    "ax.set_ylabel(\"Cohen's d (vs bare)\")\n",
    "ax.set_title('All Conditions vs Length')\n",
    "ax.legend(fontsize=8)\n",
    "ax.set_xscale('log', base=2)\n",
    "ax.set_xticks(x_tokens)\n",
    "ax.set_xticklabels(LENGTH_BINS, rotation=45)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# --- Panel 2: Structural vs Semantic gap ---\n",
    "ax = axes[0, 1]\n",
    "oracle_d = [analysis[lb].get('oracle_trunc', {}).get('d', 0) for lb in LENGTH_BINS]\n",
    "random_d = [analysis[lb].get('random_trunc', {}).get('d', 0) for lb in LENGTH_BINS]\n",
    "semantic_gap = [o - r for o, r in zip(oracle_d, random_d)]\n",
    "\n",
    "ax.plot(x_tokens, oracle_d, '-o', color='tab:red', label='oracle (structural+semantic)', markersize=7)\n",
    "ax.plot(x_tokens, random_d, '-x', color='tab:gray', label='random (structural only)', markersize=7)\n",
    "ax.fill_between(x_tokens, random_d, oracle_d, alpha=0.2, color='tab:green', label='semantic gap')\n",
    "ax.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "ax.axvline(x=1024, color='gray', linestyle=':', alpha=0.3)\n",
    "ax.set_xlabel('Document Length (tokens)')\n",
    "ax.set_ylabel(\"Cohen's d (vs bare)\")\n",
    "ax.set_title('Structural vs Semantic Decomposition')\n",
    "ax.legend(fontsize=8)\n",
    "ax.set_xscale('log', base=2)\n",
    "ax.set_xticks(x_tokens)\n",
    "ax.set_xticklabels(LENGTH_BINS, rotation=45)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# --- Panel 3: Cross-architecture comparison (oracle only) ---\n",
    "ax = axes[1, 0]\n",
    "ax.plot(x_tokens, oracle_d, '-o', color='tab:red', label='v3 T5Gemma oracle_trunc', markersize=7)\n",
    "ax.plot(x_tokens, random_d, '-x', color='tab:gray', label='v3 T5Gemma random_trunc', markersize=7)\n",
    "\n",
    "# v2 Exp 20 data (only where lengths overlap)\n",
    "v2_data = {\"original\": 0.303, \"512\": 0.034, \"1024\": -0.043}\n",
    "v2_x = [padded_stats[lb]['mean'] for lb in v2_data.keys() if lb in padded_stats]\n",
    "v2_y = [v2_data[lb] for lb in v2_data.keys() if lb in padded_stats]\n",
    "ax.plot(v2_x, v2_y, '-s', color='tab:purple', label='v2 Gemma 3 4B oracle', markersize=7)\n",
    "\n",
    "ax.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "ax.axvline(x=1024, color='gray', linestyle=':', alpha=0.3)\n",
    "ax.set_xlabel('Document Length (tokens)')\n",
    "ax.set_ylabel(\"Cohen's d (vs bare)\")\n",
    "ax.set_title('Cross-Architecture: v3 vs v2')\n",
    "ax.legend(fontsize=8)\n",
    "ax.set_xscale('log', base=2)\n",
    "ax.set_xticks(x_tokens)\n",
    "ax.set_xticklabels(LENGTH_BINS, rotation=45)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# --- Panel 4: Three-way decomposition stacked ---\n",
    "ax = axes[1, 1]\n",
    "struct_pcts = []\n",
    "vocab_pcts = []\n",
    "sem_pcts = []\n",
    "for lb in LENGTH_BINS:\n",
    "    br = results_by_bin[lb]\n",
    "    bare = np.array([r['nll_bare'] for r in br])\n",
    "    orc = np.array([r['nll_oracle_trunc'] for r in br])\n",
    "    scr = np.array([r['nll_scrambled_oracle_trunc'] for r in br])\n",
    "    mat = np.array([r['nll_random_matched_trunc'] for r in br])\n",
    "    total = (bare - orc).mean()\n",
    "    if total > 0:\n",
    "        struct_pcts.append((bare - mat).mean() / total * 100)\n",
    "        vocab_pcts.append((mat - scr).mean() / total * 100)\n",
    "        sem_pcts.append((scr - orc).mean() / total * 100)\n",
    "    else:\n",
    "        struct_pcts.append(0)\n",
    "        vocab_pcts.append(0)\n",
    "        sem_pcts.append(0)\n",
    "\n",
    "x_pos = np.arange(len(LENGTH_BINS))\n",
    "ax.bar(x_pos, struct_pcts, label='Structure', color='tab:gray', alpha=0.8)\n",
    "ax.bar(x_pos, vocab_pcts, bottom=struct_pcts, label='Vocabulary', color='tab:orange', alpha=0.8)\n",
    "bottoms = [s + v for s, v in zip(struct_pcts, vocab_pcts)]\n",
    "ax.bar(x_pos, sem_pcts, bottom=bottoms, label='Semantics', color='tab:red', alpha=0.8)\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels(LENGTH_BINS, rotation=45)\n",
    "ax.set_ylabel('% of Oracle Benefit')\n",
    "ax.set_title('Three-Way Decomposition vs Length')\n",
    "ax.legend(fontsize=8)\n",
    "ax.set_ylim(0, 120)\n",
    "ax.axhline(y=100, color='gray', linestyle='--', alpha=0.3)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plot_path = RESULTS_DIR / 'decay_curves_extended.png'\n",
    "plt.savefig(plot_path, dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"Plot saved to {plot_path}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 12: Verdict and save\n",
    "print(\"=\" * 70)\n",
    "print(\"VERDICT -- Exp 03B: Extended Length Scaling\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\nModel: {MODEL_NAME}\")\n",
    "print(f\"N: {N_SAMPLES} samples per length bin\")\n",
    "print(f\"Length bins: {LENGTH_BINS}\")\n",
    "print(f\"Encoder: sliding_window=1024, full attention every 6th layer (5/34 layers)\")\n",
    "\n",
    "# Key question 1: Does random hold up?\n",
    "print(f\"\\n--- Q1: Does the structural switch hold at long documents? ---\")\n",
    "for lb in LENGTH_BINS:\n",
    "    d = analysis[lb].get('random_trunc', {}).get('d', 0)\n",
    "    p = analysis[lb].get('random_trunc', {}).get('p', 1)\n",
    "    sig = \"SIGNIFICANT\" if p < 0.05/N_BONFERRONI else \"ns\"\n",
    "    print(f\"  {lb:>8s}: random d={d:+.3f} [{sig}]\")\n",
    "\n",
    "# Key question 2: Does semantic gap change with length?\n",
    "print(f\"\\n--- Q2: Does semantic gap (oracle - random) change with length? ---\")\n",
    "semantic_gaps = []\n",
    "for lb in LENGTH_BINS:\n",
    "    d_oracle = analysis[lb].get('oracle_trunc', {}).get('d', 0)\n",
    "    d_random = analysis[lb].get('random_trunc', {}).get('d', 0)\n",
    "    gap = d_oracle - d_random\n",
    "    semantic_gaps.append(gap)\n",
    "    print(f\"  {lb:>8s}: oracle d={d_oracle:+.3f}, random d={d_random:+.3f}, semantic gap={gap:+.3f}\")\n",
    "\n",
    "# Trend in semantic gap\n",
    "from scipy.stats import spearmanr\n",
    "mean_tokens_list = [padded_stats[lb]['mean'] for lb in LENGTH_BINS]\n",
    "rho, p_rho = spearmanr(mean_tokens_list, semantic_gaps)\n",
    "print(f\"\\n  Spearman correlation (length vs semantic gap): rho={rho:+.3f} (p={p_rho:.3f})\")\n",
    "if p_rho < 0.05:\n",
    "    if rho > 0:\n",
    "        print(f\"  Semantic gap GROWS with length — content matters MORE for long documents\")\n",
    "    else:\n",
    "        print(f\"  Semantic gap SHRINKS with length — structural mechanism dominates at scale\")\n",
    "else:\n",
    "    print(f\"  No significant trend — semantic gap is stable across lengths\")\n",
    "\n",
    "# Key question 3: Where does it break down?\n",
    "print(f\"\\n--- Q3: At what length does the encoder break down? ---\")\n",
    "last_sig = {}\n",
    "for cond in COND_NAMES[1:]:\n",
    "    for lb in LENGTH_BINS:\n",
    "        p = analysis[lb].get(cond, {}).get('p', 1)\n",
    "        if p < 0.05/N_BONFERRONI:\n",
    "            last_sig[cond] = lb\n",
    "    sig_lb = last_sig.get(cond, 'none')\n",
    "    print(f\"  {cond:<22s}: significant up to {sig_lb}\")\n",
    "\n",
    "# Overall verdict\n",
    "print(f\"\\n--- Overall Verdict ---\")\n",
    "all_sig = all(\n",
    "    analysis[lb].get('random_trunc', {}).get('p', 1) < 0.05/N_BONFERRONI\n",
    "    for lb in LENGTH_BINS\n",
    ")\n",
    "if all_sig:\n",
    "    print(f\"  STRUCTURAL SWITCH holds at ALL lengths up to {LENGTH_BINS[-1]} tokens\")\n",
    "    print(f\"  Random prefix is sufficient — no need for content-specific surrogates\")\n",
    "    print(f\"  The encoder's global attention layers (every 6th) propagate the signal\")\n",
    "else:\n",
    "    first_ns = next(\n",
    "        (lb for lb in LENGTH_BINS\n",
    "         if analysis[lb].get('random_trunc', {}).get('p', 1) >= 0.05/N_BONFERRONI),\n",
    "        None\n",
    "    )\n",
    "    print(f\"  Structural switch breaks down at {first_ns} tokens\")\n",
    "    print(f\"  Beyond this, content-specific surrogates may be needed\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "\n",
    "# Save\n",
    "final_results = {\n",
    "    'experiment': 'exp03b_extended_length_scaling',\n",
    "    'model': MODEL_NAME,\n",
    "    'n_samples': N_SAMPLES,\n",
    "    'length_bins': LENGTH_BINS,\n",
    "    'n_bonferroni': N_BONFERRONI,\n",
    "    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'analysis': analysis,\n",
    "    'padded_stats': padded_stats,\n",
    "    'encoder_config': {\n",
    "        'sliding_window': 1024,\n",
    "        'full_attention_every': 6,\n",
    "        'num_layers': 34,\n",
    "        'num_global_layers': 5,\n",
    "    },\n",
    "}\n",
    "\n",
    "with open(RESULTS_DIR / 'results.json', 'w') as f:\n",
    "    json.dump(final_results, f, indent=2)\n",
    "print(f\"\\nResults saved to {RESULTS_DIR / 'results.json'}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 13: Cleanup\n",
    "print(\"Cleaning up GPU memory...\")\n",
    "mem_before = torch.cuda.memory_allocated() / 1e9\n",
    "del model, processor, tokenizer\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "mem_after = torch.cuda.memory_allocated() / 1e9\n",
    "print(f\"GPU memory: {mem_before:.2f} GB -> {mem_after:.2f} GB\")\n",
    "print(\"Done!\")"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
