{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 04B: Amazon ESCI Query-Likelihood Ranking\n",
    "## The real ad-serving use case: which product best explains the user's query?\n",
    "\n",
    "### Context\n",
    "Exp 04A tested answer-likelihood ranking on MS MARCO (same scoring as Exps 01-03).\n",
    "This experiment tests **query-likelihood** ranking on a commercial product search dataset.\n",
    "\n",
    "### Key difference: Query-Likelihood Scoring\n",
    "No gold answer text in ESCI. Instead, the decoder scores the query:\n",
    "\n",
    "```\n",
    "NLL(query | encode([condition + product_text]))\n",
    "```\n",
    "\n",
    "\"Given this product representation, how likely is the user's query?\"\n",
    "This IS the ad-serving question.\n",
    "\n",
    "### Dataset: Amazon ESCI\n",
    "- Real product search with graded relevance: Exact (3), Substitute (2), Complement (1), Irrelevant (0)\n",
    "- 130K+ queries, up to 40 candidates per query\n",
    "- Product text = title + bullet\\_points + description\n",
    "- Truly irrelevant products (unlike MS MARCO where all candidates are topically related)\n",
    "\n",
    "### Pre-Screen\n",
    "v2 Exp 31 got QL AUC=0.578 on MS MARCO (barely above chance) because all candidates\n",
    "are topically similar. ESCI has truly Irrelevant products -> expect higher AUC.\n",
    "**Abort if bare QL AUC < 0.55 on first 20 queries.**\n",
    "\n",
    "### Conditions (6)\n",
    "| # | Condition | Encoder input | Note |\n",
    "|---|-----------|--------------|------|\n",
    "| 1 | bare | product\\_text only | Lower bound |\n",
    "| 2 | oracle\\_trunc | real query + product\\_text | Upper bound |\n",
    "| 3 | surr\\_title\\_trunc | product\\_title + product\\_text | Natural for ads! |\n",
    "| 4 | surr\\_doc\\_trunc | TF keywords from product\\_text | Doc-derived |\n",
    "| 5 | surr\\_template\\_trunc | \"What is [keyword]?\" + product\\_text | Templated |\n",
    "| 6 | random\\_trunc | unrelated product text | Structural control |\n",
    "\n",
    "### Metrics\n",
    "- **NDCG@1**, **NDCG@3**, **NDCG@5** (graded: E=3, S=2, C=1, I=0)\n",
    "- **AUC** (binary: E+S vs C+I)\n",
    "- **MRR@3** (binary: E+S relevant)\n",
    "- **Hit@1** (binary: E+S relevant)\n",
    "\n",
    "### N=400 queries, Bonferroni=5\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 2: Setup\n",
    "import os\n",
    "os.umask(0o000)\n",
    "\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "import gc\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "from collections import Counter, defaultdict\n",
    "from scipy import stats\n",
    "from scipy.stats import wilcoxon\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "sys.path.insert(0, \"../..\")\n",
    "\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "RESULTS_DIR = Path(\"../../results/exp04b\")\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CHECKPOINT_PATH = RESULTS_DIR / \"checkpoint.json\"\n",
    "\n",
    "N_SAMPLES = 400   # queries\n",
    "N_PRESCREEN = 20  # queries for pre-screen\n",
    "MODEL_NAME = \"google/t5gemma-2-4b-4b\"\n",
    "N_BONFERRONI = 5  # 5 non-bare conditions\n",
    "\n",
    "# Graded relevance mapping\n",
    "ESCI_RELEVANCE = {'Exact': 3, 'Substitute': 2, 'Complement': 1, 'Irrelevant': 0}\n",
    "ESCI_BINARY = {'Exact': 1, 'Substitute': 1, 'Complement': 0, 'Irrelevant': 0}\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv())\n",
    "HF_TOKEN = os.environ.get(\"HF_TOKEN\")\n",
    "\n",
    "print(f\"Exp 04B: Amazon ESCI Query-Likelihood Ranking\")\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"N queries: {N_SAMPLES} (pre-screen: {N_PRESCREEN})\")\n",
    "print(f\"Relevance: {ESCI_RELEVANCE}\")\n",
    "print(f\"Bonferroni comparisons: {N_BONFERRONI}\")\n",
    "print(f\"CUDA: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 3: Load model\n",
    "from transformers import AutoProcessor, AutoModelForSeq2SeqLM\n",
    "\n",
    "print(f\"Loading {MODEL_NAME}...\")\n",
    "processor = AutoProcessor.from_pretrained(MODEL_NAME, token=HF_TOKEN)\n",
    "tokenizer = processor.tokenizer\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    token=HF_TOKEN,\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "DEVICE = next(model.parameters()).device\n",
    "print(f\"Model loaded. dtype={next(model.parameters()).dtype}\")\n",
    "print(f\"GPU memory used: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 4: Scoring and ranking helpers\n",
    "\n",
    "def score_nll(encoder_text, target_text, prefix_token_count=0, truncate=False):\n",
    "    '''Score NLL of target tokens with optional truncation.\n",
    "    For ESCI: target_text is the QUERY (query-likelihood scoring).'''\n",
    "    enc_ids = tokenizer(encoder_text, return_tensors=\"pt\",\n",
    "                        add_special_tokens=True, truncation=True,\n",
    "                        max_length=8192).input_ids.to(DEVICE)\n",
    "    total_enc_len = enc_ids.shape[1]\n",
    "\n",
    "    enc_mask = torch.ones(1, total_enc_len, device=DEVICE, dtype=torch.long)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        encoder_outputs = model.get_encoder()(\n",
    "            input_ids=enc_ids, attention_mask=enc_mask\n",
    "        )\n",
    "\n",
    "    if truncate and prefix_token_count > 0:\n",
    "        cross_attn_mask = torch.ones(1, total_enc_len, device=DEVICE, dtype=torch.long)\n",
    "        cross_attn_mask[:, :prefix_token_count] = 0\n",
    "    else:\n",
    "        cross_attn_mask = torch.ones(1, total_enc_len, device=DEVICE, dtype=torch.long)\n",
    "\n",
    "    target_ids = tokenizer(target_text, return_tensors=\"pt\",\n",
    "                           add_special_tokens=False, truncation=True,\n",
    "                           max_length=256).input_ids.to(DEVICE)\n",
    "\n",
    "    if target_ids.shape[1] == 0:\n",
    "        return 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            encoder_outputs=encoder_outputs,\n",
    "            attention_mask=cross_attn_mask,\n",
    "            labels=target_ids,\n",
    "        )\n",
    "\n",
    "    logits = outputs.logits\n",
    "    log_probs = F.log_softmax(logits, dim=-1)\n",
    "    token_log_probs = log_probs[0].gather(1, target_ids[0].unsqueeze(1)).squeeze(1)\n",
    "    mean_nll = -token_log_probs.mean().item()\n",
    "\n",
    "    del encoder_outputs, outputs, logits, log_probs\n",
    "    return mean_nll\n",
    "\n",
    "\n",
    "def count_prefix_tokens(prefix_text, document_text):\n",
    "    '''Count how many tokens the prefix occupies in the concatenated encoding.'''\n",
    "    full_text = prefix_text + \"\\n\" + document_text\n",
    "    full_ids = tokenizer(full_text, add_special_tokens=True).input_ids\n",
    "    doc_ids = tokenizer(document_text, add_special_tokens=True).input_ids\n",
    "    return len(full_ids) - len(doc_ids)\n",
    "\n",
    "\n",
    "# === Surrogate generation ===\n",
    "STOP_WORDS = {\n",
    "    'a', 'an', 'the', 'is', 'are', 'was', 'were', 'be', 'been', 'being',\n",
    "    'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could',\n",
    "    'should', 'may', 'might', 'can', 'shall', 'to', 'of', 'in', 'for',\n",
    "    'on', 'with', 'at', 'by', 'from', 'as', 'into', 'through', 'during',\n",
    "    'before', 'after', 'above', 'below', 'between', 'and', 'but', 'or',\n",
    "    'not', 'no', 'if', 'then', 'than', 'so', 'up', 'out', 'about',\n",
    "    'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those',\n",
    "    'it', 'its', 'i', 'me', 'my', 'we', 'our', 'you', 'your', 'he',\n",
    "    'him', 'his', 'she', 'her', 'they', 'them', 'their', 'how', 'when',\n",
    "    'where', 'why', 'much', 'many', 'some', 'any', 'all', 'each',\n",
    "    'does', 'also', 'just', 'more', 'most', 'very', 'too', 'only',\n",
    "}\n",
    "\n",
    "def extract_keywords(text):\n",
    "    words = re.sub(r'[^\\w\\s]', '', text.lower()).split()\n",
    "    return [w for w in words if w not in STOP_WORDS and len(w) > 2]\n",
    "\n",
    "def make_surrogate_doc_kw(text):\n",
    "    content_words = extract_keywords(text)\n",
    "    if not content_words:\n",
    "        return \"information\"\n",
    "    counts = Counter(content_words)\n",
    "    return \" \".join(w for w, _ in counts.most_common(5))\n",
    "\n",
    "def make_surrogate_template(text):\n",
    "    content_words = extract_keywords(text)\n",
    "    if not content_words:\n",
    "        return \"What is this about?\"\n",
    "    counts = Counter(content_words)\n",
    "    top_word = counts.most_common(1)[0][0]\n",
    "    return f\"What is {top_word}?\"\n",
    "\n",
    "\n",
    "# === Ranking metrics ===\n",
    "def ndcg_at_k(relevance_scores_ranked, k):\n",
    "    '''NDCG@k. relevance_scores_ranked: ALL relevance scores in predicted rank order.'''\n",
    "    rel = np.asarray(relevance_scores_ranked[:k], dtype=float)\n",
    "    dcg = np.sum(rel / np.log2(np.arange(2, len(rel) + 2)))\n",
    "    all_rel = np.asarray(relevance_scores_ranked, dtype=float)\n",
    "    ideal = np.sort(all_rel)[::-1][:k]\n",
    "    idcg = np.sum(ideal / np.log2(np.arange(2, len(ideal) + 2)))\n",
    "    return dcg / idcg if idcg > 0 else 0.0\n",
    "\n",
    "def compute_auc_binary(nlls, binary_labels):\n",
    "    '''AUC for binary relevance (E+S=1 vs C+I=0). Lower NLL = more relevant.'''\n",
    "    pos_nlls = [nlls[i] for i in range(len(nlls)) if binary_labels[i] == 1]\n",
    "    neg_nlls = [nlls[i] for i in range(len(nlls)) if binary_labels[i] == 0]\n",
    "    if len(pos_nlls) == 0 or len(neg_nlls) == 0:\n",
    "        return 0.5\n",
    "    wins = sum(1 for p in pos_nlls for n in neg_nlls if n > p)\n",
    "    ties = sum(1 for p in pos_nlls for n in neg_nlls if n == p)\n",
    "    return (wins + 0.5 * ties) / (len(pos_nlls) * len(neg_nlls))\n",
    "\n",
    "def compute_mrr_at_k(nlls, binary_labels, k=3):\n",
    "    '''MRR@k: reciprocal rank of first relevant item in top-k by ascending NLL.'''\n",
    "    ranked_indices = list(np.argsort(nlls))\n",
    "    for rank, idx in enumerate(ranked_indices[:k], 1):\n",
    "        if binary_labels[idx] == 1:\n",
    "            return 1.0 / rank\n",
    "    return 0.0\n",
    "\n",
    "def compute_hit_at_k(nlls, binary_labels, k=1):\n",
    "    '''Hit@k: 1 if any relevant item in top-k by ascending NLL.'''\n",
    "    ranked_indices = np.argsort(nlls)[:k]\n",
    "    return 1.0 if any(binary_labels[idx] == 1 for idx in ranked_indices) else 0.0\n",
    "\n",
    "print(\"Helpers defined.\")\n",
    "print(\"  Scoring: score_nll (query-likelihood)\")\n",
    "print(\"  Surrogates: title, doc_kw, template, random\")\n",
    "print(\"  Ranking metrics: NDCG@k, AUC, MRR@k, Hit@k\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 5: Load Amazon ESCI data and build ranking pools\n",
    "from datasets import load_dataset\n",
    "\n",
    "print(\"Loading Amazon ESCI dataset...\")\n",
    "print(\"  Source: tasksource/esci (pre-joined, ~2M rows)\")\n",
    "ds = load_dataset(\"tasksource/esci\", split=\"train\")\n",
    "print(f\"  Loaded {len(ds)} rows\")\n",
    "print(f\"  Columns: {ds.column_names}\")\n",
    "\n",
    "# Filter to US locale and build product text\n",
    "print(\"\\nFiltering to US locale and building query pools...\")\n",
    "\n",
    "def safe_str(val):\n",
    "    '''Handle None/NaN values in product fields.'''\n",
    "    if val is None:\n",
    "        return \"\"\n",
    "    s = str(val).strip()\n",
    "    if s.lower() in ('nan', 'none', ''):\n",
    "        return \"\"\n",
    "    return s\n",
    "\n",
    "def build_product_text(row):\n",
    "    '''Use pre-built product_text if available, else concatenate fields.'''\n",
    "    pt = safe_str(row.get('product_text', ''))\n",
    "    if pt:\n",
    "        return pt\n",
    "    parts = []\n",
    "    for field in ['product_title', 'product_bullet_point', 'product_description']:\n",
    "        val = safe_str(row.get(field, ''))\n",
    "        if val:\n",
    "            parts.append(val)\n",
    "    return \" \".join(parts)\n",
    "\n",
    "# Group by query\n",
    "query_pools = defaultdict(list)\n",
    "\n",
    "for row in tqdm(ds, desc=\"Filtering\"):\n",
    "    if safe_str(row.get('product_locale', '')) != 'us':\n",
    "        continue\n",
    "\n",
    "    label = safe_str(row.get('esci_label', ''))\n",
    "    if label not in ESCI_RELEVANCE:\n",
    "        continue\n",
    "\n",
    "    product_text = build_product_text(row)\n",
    "    if len(product_text.split()) < 5:\n",
    "        continue\n",
    "\n",
    "    query = safe_str(row.get('query', ''))\n",
    "    if not query:\n",
    "        continue\n",
    "\n",
    "    product_title = safe_str(row.get('product_title', ''))\n",
    "\n",
    "    query_pools[query].append({\n",
    "        'product_text': product_text,\n",
    "        'product_title': product_title if product_title else product_text.split()[0],\n",
    "        'label': label,\n",
    "        'relevance': ESCI_RELEVANCE[label],\n",
    "        'binary_label': ESCI_BINARY[label],\n",
    "    })\n",
    "\n",
    "del ds\n",
    "gc.collect()\n",
    "\n",
    "print(f\"US queries with products: {len(query_pools)}\")\n",
    "\n",
    "# Filter: 4+ candidates, at least 1 relevant and 1 irrelevant\n",
    "qualifying_queries = []\n",
    "for query_text, products in query_pools.items():\n",
    "    if len(products) < 4:\n",
    "        continue\n",
    "    has_relevant = any(p['binary_label'] == 1 for p in products)\n",
    "    has_irrelevant = any(p['binary_label'] == 0 for p in products)\n",
    "    if not (has_relevant and has_irrelevant):\n",
    "        continue\n",
    "    qualifying_queries.append({\n",
    "        'query': query_text,\n",
    "        'products': products,\n",
    "        'n_products': len(products),\n",
    "    })\n",
    "\n",
    "del query_pools\n",
    "gc.collect()\n",
    "\n",
    "print(f\"Qualifying queries (4+ candidates, rel + irrel): {len(qualifying_queries)}\")\n",
    "\n",
    "if len(qualifying_queries) < N_SAMPLES + 50:\n",
    "    print(f\"WARNING: Only {len(qualifying_queries)} qualifying queries (need {N_SAMPLES}+50)\")\n",
    "    print(f\"Reducing N_SAMPLES to {max(0, len(qualifying_queries) - 50)}\")\n",
    "    N_SAMPLES = max(0, len(qualifying_queries) - 50)\n",
    "\n",
    "# Shuffle and select\n",
    "np.random.seed(SEED)\n",
    "np.random.shuffle(qualifying_queries)\n",
    "\n",
    "# We need N_SAMPLES + some buffer; keep extra for pre-screen\n",
    "queries = qualifying_queries[:N_SAMPLES + 50]\n",
    "del qualifying_queries\n",
    "gc.collect()\n",
    "\n",
    "# Generate surrogates for each product in each query\n",
    "for i, q in enumerate(queries):\n",
    "    for p in q['products']:\n",
    "        p['surr_doc_kw'] = make_surrogate_doc_kw(p['product_text'])\n",
    "        p['surr_template'] = make_surrogate_template(p['product_text'])\n",
    "\n",
    "    # Random: use product text from another query (circular offset)\n",
    "    other_idx = (i + len(queries) // 2) % len(queries)\n",
    "    other_product = queries[other_idx]['products'][0]\n",
    "    q['surr_random'] = \" \".join(other_product['product_text'].split()[:20])\n",
    "\n",
    "# Stats\n",
    "n_products = [q['n_products'] for q in queries[:N_SAMPLES]]\n",
    "print(f\"\\nSelected {min(len(queries), N_SAMPLES)} queries\")\n",
    "print(f\"Products per query: mean={np.mean(n_products):.1f}, \"\n",
    "      f\"median={np.median(n_products):.0f}, \"\n",
    "      f\"min={np.min(n_products)}, max={np.max(n_products)}\")\n",
    "\n",
    "# Label distribution\n",
    "all_labels = [p['label'] for q in queries[:N_SAMPLES] for p in q['products']]\n",
    "for label in ['Exact', 'Substitute', 'Complement', 'Irrelevant']:\n",
    "    count = sum(1 for l in all_labels if l == label)\n",
    "    print(f\"  {label}: {count} ({100*count/len(all_labels):.1f}%)\")\n",
    "\n",
    "total_calls = sum(n_products) * 6  # 6 conditions\n",
    "print(f\"Total scoring calls: {total_calls}\")\n",
    "print(f\"Estimated runtime: ~{total_calls * 0.5 / 3600:.1f} hours\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 6: Pre-screen -- bare QL AUC on first 20 queries\n",
    "print(\"=\" * 70)\n",
    "print(\"PRE-SCREEN: Bare Query-Likelihood AUC\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Scoring {N_PRESCREEN} queries with bare condition only...\")\n",
    "print(f\"v2 Exp 31 got QL AUC=0.578 on MS MARCO (near chance)\")\n",
    "print(f\"ESCI has truly Irrelevant products -> expect higher AUC\")\n",
    "print(f\"Abort threshold: AUC < 0.55\")\n",
    "\n",
    "prescreen_aucs = []\n",
    "t0 = time.time()\n",
    "\n",
    "for q_idx in tqdm(range(N_PRESCREEN), desc=\"Pre-screen\"):\n",
    "    q = queries[q_idx]\n",
    "    query_text = q['query']\n",
    "    nlls = []\n",
    "\n",
    "    for p in q['products']:\n",
    "        nll = score_nll(p['product_text'], query_text, 0, False)\n",
    "        nlls.append(nll)\n",
    "\n",
    "    binary_labels = [p['binary_label'] for p in q['products']]\n",
    "    auc = compute_auc_binary(nlls, binary_labels)\n",
    "    prescreen_aucs.append(auc)\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "elapsed = time.time() - t0\n",
    "mean_auc = np.mean(prescreen_aucs)\n",
    "print(f\"\\nPre-screen complete: {elapsed/60:.1f} min\")\n",
    "print(f\"Bare QL AUC: mean={mean_auc:.3f}, median={np.median(prescreen_aucs):.3f}\")\n",
    "print(f\"Range: [{np.min(prescreen_aucs):.3f}, {np.max(prescreen_aucs):.3f}]\")\n",
    "\n",
    "PRESCREEN_PASSED = mean_auc >= 0.55\n",
    "if PRESCREEN_PASSED:\n",
    "    print(f\"\\n>>> PRE-SCREEN PASSED (AUC={mean_auc:.3f} >= 0.55)\")\n",
    "    print(f\">>> Proceeding with full experiment\")\n",
    "else:\n",
    "    print(f\"\\n>>> PRE-SCREEN FAILED (AUC={mean_auc:.3f} < 0.55)\")\n",
    "    print(f\">>> QL scoring cannot distinguish relevant from irrelevant on ESCI\")\n",
    "    print(f\">>> ABORTING -- no point running 400 queries\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 7: Explain conditions with concrete example\n",
    "if not PRESCREEN_PASSED:\n",
    "    print(\"SKIPPED (pre-screen failed)\")\n",
    "else:\n",
    "    print(\"=\" * 70)\n",
    "    print(\"CONDITION EXAMPLES\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    COND_NAMES = ['bare', 'oracle_trunc', 'surr_title_trunc',\n",
    "                  'surr_doc_trunc', 'surr_template_trunc', 'random_trunc']\n",
    "\n",
    "    ex = queries[0]\n",
    "    print(f\"\\nQuery: {ex['query']}\")\n",
    "    print(f\"Products: {ex['n_products']}\")\n",
    "\n",
    "    # Show one relevant and one irrelevant product\n",
    "    rel_p = next(p for p in ex['products'] if p['binary_label'] == 1)\n",
    "    irr_p = next(p for p in ex['products'] if p['binary_label'] == 0)\n",
    "\n",
    "    print(f\"\\n--- Relevant product (label={rel_p['label']}) ---\")\n",
    "    print(f\"  Title: {rel_p['product_title'][:100]}\")\n",
    "    print(f\"  Text: {rel_p['product_text'][:150]}...\")\n",
    "\n",
    "    print(f\"\\n--- Irrelevant product (label={irr_p['label']}) ---\")\n",
    "    print(f\"  Title: {irr_p['product_title'][:100]}\")\n",
    "    print(f\"  Text: {irr_p['product_text'][:150]}...\")\n",
    "\n",
    "    print(f\"\\n--- Encoder input for relevant product ---\")\n",
    "    for cond in COND_NAMES:\n",
    "        if cond == 'bare':\n",
    "            enc = rel_p['product_text'][:80] + \"...\"\n",
    "        elif cond == 'oracle_trunc':\n",
    "            enc = ex['query'] + \" | \" + rel_p['product_text'][:60] + \"...\"\n",
    "        elif cond == 'surr_title_trunc':\n",
    "            enc = rel_p['product_title'][:40] + \" | \" + rel_p['product_text'][:40] + \"...\"\n",
    "        elif cond == 'surr_doc_trunc':\n",
    "            enc = rel_p['surr_doc_kw'] + \" | \" + rel_p['product_text'][:50] + \"...\"\n",
    "        elif cond == 'surr_template_trunc':\n",
    "            enc = rel_p['surr_template'] + \" | \" + rel_p['product_text'][:50] + \"...\"\n",
    "        elif cond == 'random_trunc':\n",
    "            enc = ex['surr_random'][:40] + \"... | \" + rel_p['product_text'][:40] + \"...\"\n",
    "        print(f\"  {cond:<22s}: {enc}\")\n",
    "\n",
    "    print(f\"\\n  Decoder target: '{ex['query']}' (query-likelihood)\")\n",
    "    print(f\"\\n--- Note: surr_title_trunc uses the product's OWN title as prefix ---\")\n",
    "    print(f\"  This is the most natural ad-serving surrogate (title always available)\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 8: Run scoring -- outer loop over queries\n",
    "if not PRESCREEN_PASSED:\n",
    "    print(\"SKIPPED (pre-screen failed)\")\n",
    "    results = []\n",
    "else:\n",
    "    print(\"=\" * 70)\n",
    "    print(\"RUNNING RANKING EXPERIMENT\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    def build_condition_input(cond_name, product_data, query_data):\n",
    "        '''Return (encoder_text, prefix_token_count, truncate) for a condition.'''\n",
    "        product_text = product_data['product_text']\n",
    "\n",
    "        if cond_name == 'bare':\n",
    "            return product_text, 0, False\n",
    "        elif cond_name == 'oracle_trunc':\n",
    "            surr = query_data['query']\n",
    "        elif cond_name == 'surr_title_trunc':\n",
    "            surr = product_data['product_title']\n",
    "        elif cond_name == 'surr_doc_trunc':\n",
    "            surr = product_data['surr_doc_kw']\n",
    "        elif cond_name == 'surr_template_trunc':\n",
    "            surr = product_data['surr_template']\n",
    "        elif cond_name == 'random_trunc':\n",
    "            surr = query_data['surr_random']\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown condition: {cond_name}\")\n",
    "\n",
    "        enc_text = surr + \"\\n\" + product_text\n",
    "        prefix_count = count_prefix_tokens(surr, product_text)\n",
    "        return enc_text, prefix_count, True\n",
    "\n",
    "    # Resume from checkpoint\n",
    "    results = []\n",
    "    start_idx = 0\n",
    "    if CHECKPOINT_PATH.exists():\n",
    "        saved = json.loads(CHECKPOINT_PATH.read_text())\n",
    "        if saved.get('n_total') == N_SAMPLES:\n",
    "            saved_results = saved.get('results', [])\n",
    "            saved_queries = [r['query'][:50] for r in saved_results]\n",
    "            current_queries = [q['query'][:50] for q in queries[:len(saved_results)]]\n",
    "            if saved_queries == current_queries:\n",
    "                results = saved_results\n",
    "                start_idx = len(results)\n",
    "                print(f\"Resumed from checkpoint: {start_idx}/{N_SAMPLES} queries\")\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    for q_idx in tqdm(range(start_idx, N_SAMPLES), initial=start_idx, total=N_SAMPLES,\n",
    "                      desc=\"Queries\"):\n",
    "        q = queries[q_idx]\n",
    "        query_text = q['query']\n",
    "\n",
    "        query_result = {\n",
    "            'query_idx': q_idx,\n",
    "            'query': query_text,\n",
    "            'n_products': q['n_products'],\n",
    "            'labels': [p['label'] for p in q['products']],\n",
    "            'relevances': [p['relevance'] for p in q['products']],\n",
    "            'binary_labels': [p['binary_label'] for p in q['products']],\n",
    "            'scores': {},\n",
    "        }\n",
    "\n",
    "        for cond_name in COND_NAMES:\n",
    "            cond_nlls = []\n",
    "            for p_idx, product_data in enumerate(q['products']):\n",
    "                enc_text, prefix_count, truncate = build_condition_input(\n",
    "                    cond_name, product_data, q)\n",
    "                nll = score_nll(enc_text, query_text, prefix_count, truncate)\n",
    "                cond_nlls.append(nll)\n",
    "            query_result['scores'][cond_name] = cond_nlls\n",
    "\n",
    "        results.append(query_result)\n",
    "\n",
    "        if (q_idx + 1) % 20 == 0 or q_idx == N_SAMPLES - 1:\n",
    "            ckpt = {\n",
    "                'n_total': N_SAMPLES,\n",
    "                'results': results,\n",
    "                'completed': len(results),\n",
    "                'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            }\n",
    "            CHECKPOINT_PATH.write_text(json.dumps(ckpt))\n",
    "            elapsed = time.time() - t0\n",
    "            done = q_idx - start_idx + 1\n",
    "            eta = (N_SAMPLES - q_idx - 1) * elapsed / done if done > 0 else 0\n",
    "            tqdm.write(f\"  Checkpoint {q_idx+1}/{N_SAMPLES} | {elapsed/60:.1f}m | ETA {eta/60:.1f}m\")\n",
    "\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    elapsed_total = time.time() - t0\n",
    "    print(f\"\\nScoring complete: {len(results)} queries in {elapsed_total/60:.1f} min\")\n",
    "\n",
    "    # Quick peek\n",
    "    bare_aucs = []\n",
    "    for r in results:\n",
    "        nlls = np.array(r['scores']['bare'])\n",
    "        bare_aucs.append(compute_auc_binary(nlls, r['binary_labels']))\n",
    "    print(f\"Bare QL AUC: mean={np.mean(bare_aucs):.3f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 9: Compute ranking metrics\n",
    "if not PRESCREEN_PASSED or len(results) == 0:\n",
    "    print(\"SKIPPED (pre-screen failed or no results)\")\n",
    "    metrics = {}\n",
    "else:\n",
    "    print(\"=\" * 70)\n",
    "    print(\"COMPUTING RANKING METRICS\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    # For each query x condition: NDCG@1/3/5, AUC, MRR@3, Hit@1\n",
    "    metrics = {cond: {'ndcg1': [], 'ndcg3': [], 'ndcg5': [],\n",
    "                      'auc': [], 'mrr3': [], 'hit1': []}\n",
    "               for cond in COND_NAMES}\n",
    "\n",
    "    for r in results:\n",
    "        binary_labels = r['binary_labels']\n",
    "        relevances = r['relevances']\n",
    "\n",
    "        for cond in COND_NAMES:\n",
    "            nlls = np.array(r['scores'][cond])\n",
    "            ranked_indices = np.argsort(nlls)  # ascending NLL = best first\n",
    "            ranked_relevances = [relevances[i] for i in ranked_indices]\n",
    "\n",
    "            metrics[cond]['ndcg1'].append(ndcg_at_k(ranked_relevances, k=1))\n",
    "            metrics[cond]['ndcg3'].append(ndcg_at_k(ranked_relevances, k=3))\n",
    "            metrics[cond]['ndcg5'].append(ndcg_at_k(ranked_relevances, k=5))\n",
    "            metrics[cond]['auc'].append(compute_auc_binary(nlls, binary_labels))\n",
    "            metrics[cond]['mrr3'].append(compute_mrr_at_k(nlls, binary_labels, k=3))\n",
    "            metrics[cond]['hit1'].append(compute_hit_at_k(nlls, binary_labels, k=1))\n",
    "\n",
    "    # Convert to arrays\n",
    "    for cond in COND_NAMES:\n",
    "        for m in metrics[cond]:\n",
    "            metrics[cond][m] = np.array(metrics[cond][m])\n",
    "\n",
    "    # Quick summary\n",
    "    for cond in COND_NAMES:\n",
    "        print(f\"  {cond:<22s}: AUC={metrics[cond]['auc'].mean():.3f}  \"\n",
    "              f\"NDCG@3={metrics[cond]['ndcg3'].mean():.3f}  \"\n",
    "              f\"MRR@3={metrics[cond]['mrr3'].mean():.3f}  \"\n",
    "              f\"Hit@1={metrics[cond]['hit1'].mean():.3f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 10: Results table with statistical tests\n",
    "if not PRESCREEN_PASSED or len(metrics) == 0:\n",
    "    print(\"SKIPPED (pre-screen failed)\")\n",
    "    analysis = {}\n",
    "else:\n",
    "    from lib.analysis import cohens_d\n",
    "\n",
    "    print(\"=\" * 70)\n",
    "    print(\"RESULTS: ESCI Query-Likelihood Ranking (N=%d queries)\" % len(results))\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    METRIC_NAMES = ['ndcg1', 'ndcg3', 'ndcg5', 'auc', 'mrr3', 'hit1']\n",
    "    METRIC_LABELS = {'ndcg1': 'NDCG@1', 'ndcg3': 'NDCG@3', 'ndcg5': 'NDCG@5',\n",
    "                     'auc': 'AUC', 'mrr3': 'MRR@3', 'hit1': 'Hit@1'}\n",
    "\n",
    "    analysis = {}\n",
    "\n",
    "    for metric_name in METRIC_NAMES:\n",
    "        print(f\"\\n--- {METRIC_LABELS[metric_name]} ---\")\n",
    "        print(f\"  {'Condition':<22} {'Mean':>8} {'vs Bare':>10} {'d':>8} {'p':>12} {'sig':>5}\")\n",
    "        print(f\"  {'-'*70}\")\n",
    "\n",
    "        bare_vals = metrics['bare'][metric_name]\n",
    "        analysis[metric_name] = {}\n",
    "\n",
    "        for cond in COND_NAMES:\n",
    "            vals = metrics[cond][metric_name]\n",
    "            mean_val = vals.mean()\n",
    "\n",
    "            if cond == 'bare':\n",
    "                print(f\"  {cond:<22} {mean_val:>8.3f} {'--':>10} {'--':>8} {'--':>12} {'--':>5}\")\n",
    "                analysis[metric_name][cond] = {'mean': float(mean_val)}\n",
    "            else:\n",
    "                diff = vals - bare_vals\n",
    "                d = cohens_d(diff)\n",
    "\n",
    "                nonzero = diff[diff != 0]\n",
    "                if len(nonzero) >= 10:\n",
    "                    try:\n",
    "                        stat, p_val = wilcoxon(nonzero)\n",
    "                    except ValueError:\n",
    "                        p_val = 1.0\n",
    "                else:\n",
    "                    p_val = 1.0\n",
    "\n",
    "                sig = ('***' if p_val < 0.001/N_BONFERRONI else\n",
    "                       '**' if p_val < 0.01/N_BONFERRONI else\n",
    "                       '*' if p_val < 0.05/N_BONFERRONI else 'ns')\n",
    "\n",
    "                print(f\"  {cond:<22} {mean_val:>8.3f} {diff.mean():>+10.4f} \"\n",
    "                      f\"{d:>+8.3f} {p_val:>12.2e} {sig:>5}\")\n",
    "                analysis[metric_name][cond] = {\n",
    "                    'mean': float(mean_val), 'delta': float(diff.mean()),\n",
    "                    'd': float(d), 'p': float(p_val),\n",
    "                }\n",
    "\n",
    "    # Headline\n",
    "    oracle_auc = analysis['auc']['oracle_trunc']['mean']\n",
    "    bare_auc = analysis['auc']['bare']['mean']\n",
    "    title_auc = analysis['auc'].get('surr_title_trunc', {}).get('mean', 0)\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"HEADLINE:\")\n",
    "    print(f\"  bare AUC = {bare_auc:.3f}\")\n",
    "    print(f\"  oracle AUC = {oracle_auc:.3f} (gain = {oracle_auc - bare_auc:+.3f})\")\n",
    "    print(f\"  surr_title AUC = {title_auc:.3f} (gain = {title_auc - bare_auc:+.3f})\")\n",
    "    print(f\"{'='*70}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 11: Differential signal analysis\n",
    "if not PRESCREEN_PASSED or len(results) == 0:\n",
    "    print(\"SKIPPED (pre-screen failed)\")\n",
    "    diff_analysis = {}\n",
    "else:\n",
    "    import matplotlib\n",
    "    matplotlib.use('Agg')\n",
    "    import matplotlib.pyplot as plt\n",
    "    from lib.analysis import cohens_d\n",
    "\n",
    "    print(\"=\" * 70)\n",
    "    print(\"DIFFERENTIAL SIGNAL ANALYSIS\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"Does priming help RELEVANT products MORE than IRRELEVANT ones?\")\n",
    "\n",
    "    diff_analysis = {}\n",
    "    for cond in COND_NAMES[1:]:\n",
    "        delta_rels = []\n",
    "        delta_irrels = []\n",
    "\n",
    "        for r in results:\n",
    "            bare_nlls = np.array(r['scores']['bare'])\n",
    "            cond_nlls = np.array(r['scores'][cond])\n",
    "            binary = r['binary_labels']\n",
    "\n",
    "            rel_mask = [i for i, b in enumerate(binary) if b == 1]\n",
    "            irrel_mask = [i for i, b in enumerate(binary) if b == 0]\n",
    "\n",
    "            if len(rel_mask) == 0 or len(irrel_mask) == 0:\n",
    "                continue\n",
    "\n",
    "            delta_rel = np.mean([bare_nlls[i] - cond_nlls[i] for i in rel_mask])\n",
    "            delta_irrel = np.mean([bare_nlls[i] - cond_nlls[i] for i in irrel_mask])\n",
    "\n",
    "            delta_rels.append(delta_rel)\n",
    "            delta_irrels.append(delta_irrel)\n",
    "\n",
    "        delta_rels = np.array(delta_rels)\n",
    "        delta_irrels = np.array(delta_irrels)\n",
    "        differential = delta_rels - delta_irrels\n",
    "\n",
    "        d = cohens_d(differential)\n",
    "        nonzero = differential[differential != 0]\n",
    "        if len(nonzero) >= 10:\n",
    "            try:\n",
    "                stat, p_val = wilcoxon(nonzero)\n",
    "                p_val_onesided = p_val / 2 if np.mean(differential) > 0 else 1 - p_val / 2\n",
    "            except ValueError:\n",
    "                p_val_onesided = 1.0\n",
    "        else:\n",
    "            p_val_onesided = 1.0\n",
    "\n",
    "        diff_analysis[cond] = {\n",
    "            'delta_rel_mean': float(delta_rels.mean()),\n",
    "            'delta_irrel_mean': float(delta_irrels.mean()),\n",
    "            'differential_mean': float(differential.mean()),\n",
    "            'd': float(d),\n",
    "            'p_onesided': float(p_val_onesided),\n",
    "            'pct_positive': float(100 * np.mean(differential > 0)),\n",
    "        }\n",
    "\n",
    "        sig = ('***' if p_val_onesided < 0.001/N_BONFERRONI else\n",
    "               '**' if p_val_onesided < 0.01/N_BONFERRONI else\n",
    "               '*' if p_val_onesided < 0.05/N_BONFERRONI else 'ns')\n",
    "\n",
    "        print(f\"\\n  {cond}:\")\n",
    "        print(f\"    delta_rel  (mean NLL drop for relevant):   {delta_rels.mean():+.4f}\")\n",
    "        print(f\"    delta_irrel (mean NLL drop for irrelevant): {delta_irrels.mean():+.4f}\")\n",
    "        print(f\"    differential (rel - irrel):                 {differential.mean():+.4f}  \"\n",
    "              f\"d={d:+.3f}  p={p_val_onesided:.2e}  {sig}\")\n",
    "        print(f\"    % queries where relevant helped MORE:       {100*np.mean(differential > 0):.1f}%\")\n",
    "\n",
    "    # Plot\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    for ax_idx, cond in enumerate(['oracle_trunc', 'surr_title_trunc', 'random_trunc']):\n",
    "        ax = axes[ax_idx]\n",
    "        delta_rels_plot = []\n",
    "        delta_irrels_plot = []\n",
    "        for r in results:\n",
    "            bare_nlls = np.array(r['scores']['bare'])\n",
    "            cond_nlls = np.array(r['scores'][cond])\n",
    "            binary = r['binary_labels']\n",
    "            rel_mask = [i for i, b in enumerate(binary) if b == 1]\n",
    "            irrel_mask = [i for i, b in enumerate(binary) if b == 0]\n",
    "            if len(rel_mask) == 0 or len(irrel_mask) == 0:\n",
    "                continue\n",
    "            delta_rels_plot.append(np.mean([bare_nlls[i] - cond_nlls[i] for i in rel_mask]))\n",
    "            delta_irrels_plot.append(np.mean([bare_nlls[i] - cond_nlls[i] for i in irrel_mask]))\n",
    "\n",
    "        ax.scatter(delta_irrels_plot, delta_rels_plot, alpha=0.3, s=10)\n",
    "        lims = [min(min(delta_irrels_plot), min(delta_rels_plot)) - 0.5,\n",
    "                max(max(delta_irrels_plot), max(delta_rels_plot)) + 0.5]\n",
    "        ax.plot(lims, lims, 'r--', alpha=0.5, label='equal help')\n",
    "        ax.set_xlabel('delta_irrelevant')\n",
    "        ax.set_ylabel('delta_relevant')\n",
    "        ax.set_title(f'{cond.replace(\"_trunc\", \"\")}')\n",
    "        ax.legend(fontsize=8)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.suptitle('ESCI Differential Signal: Points ABOVE red line = ranking improves', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plot_path = RESULTS_DIR / 'differential_signal.png'\n",
    "    plt.savefig(plot_path, dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f\"\\nPlot saved to {plot_path}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 12: Hardness stratification\n",
    "if not PRESCREEN_PASSED or len(metrics) == 0:\n",
    "    print(\"SKIPPED (pre-screen failed)\")\n",
    "    hardness_analysis = {}\n",
    "else:\n",
    "    import matplotlib\n",
    "    matplotlib.use('Agg')\n",
    "    import matplotlib.pyplot as plt\n",
    "    from lib.analysis import cohens_d\n",
    "\n",
    "    print(\"=\" * 70)\n",
    "    print(\"HARDNESS STRATIFICATION (by bare AUC quintiles)\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    bare_aucs = metrics['bare']['auc']\n",
    "    quintile_boundaries = np.percentile(bare_aucs, [20, 40, 60, 80])\n",
    "    quintile_labels = ['Q1 (hardest)', 'Q2', 'Q3', 'Q4', 'Q5 (easiest)']\n",
    "\n",
    "    def get_quintile(auc):\n",
    "        for q, bound in enumerate(quintile_boundaries):\n",
    "            if auc <= bound:\n",
    "                return q\n",
    "        return 4\n",
    "\n",
    "    quintile_assignments = np.array([get_quintile(a) for a in bare_aucs])\n",
    "\n",
    "    print(f\"\\nQuintile boundaries (bare AUC): {quintile_boundaries}\")\n",
    "    for q in range(5):\n",
    "        mask = quintile_assignments == q\n",
    "        print(f\"  {quintile_labels[q]}: N={mask.sum()}, mean bare AUC={bare_aucs[mask].mean():.3f}\")\n",
    "\n",
    "    # Per-quintile NDCG@3 and AUC gains\n",
    "    hardness_analysis = {}\n",
    "    print(f\"\\n--- AUC gain by hardness quintile ---\")\n",
    "    header = f\"  {'Quintile':<16}\"\n",
    "    for cond in COND_NAMES[1:]:\n",
    "        short = cond.replace('_trunc', '')\n",
    "        header += f\" {short:>14}\"\n",
    "    print(header)\n",
    "    print(f\"  {'-'*(16 + 15 * len(COND_NAMES[1:]))}\")\n",
    "\n",
    "    for q in range(5):\n",
    "        mask = quintile_assignments == q\n",
    "        row = f\"  {quintile_labels[q]:<16}\"\n",
    "        hardness_analysis[quintile_labels[q]] = {}\n",
    "\n",
    "        for cond in COND_NAMES[1:]:\n",
    "            cond_aucs = metrics[cond]['auc'][mask]\n",
    "            bare_q_aucs = bare_aucs[mask]\n",
    "            gain = (cond_aucs - bare_q_aucs).mean()\n",
    "            d = cohens_d(cond_aucs - bare_q_aucs) if mask.sum() > 1 else 0\n",
    "            row += f\" {gain:>+7.3f} d={d:>+5.2f}\"\n",
    "            hardness_analysis[quintile_labels[q]][cond] = {\n",
    "                'auc_gain': float(gain), 'd': float(d)}\n",
    "\n",
    "        print(row)\n",
    "\n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    for cond in ['oracle_trunc', 'surr_title_trunc', 'random_trunc']:\n",
    "        gains = [hardness_analysis[quintile_labels[q]].get(cond, {}).get('auc_gain', 0)\n",
    "                 for q in range(5)]\n",
    "        ax.plot(range(5), gains, '-o', label=cond.replace('_trunc', ''), markersize=8)\n",
    "\n",
    "    ax.set_xticks(range(5))\n",
    "    ax.set_xticklabels(quintile_labels, rotation=15)\n",
    "    ax.set_ylabel('AUC gain vs bare')\n",
    "    ax.set_title('ESCI Ranking Gain by Query Hardness')\n",
    "    ax.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plot_path = RESULTS_DIR / 'hardness_stratification.png'\n",
    "    plt.savefig(plot_path, dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f\"Plot saved to {plot_path}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 13: Verdict and save results\n",
    "print(\"=\" * 70)\n",
    "print(\"VERDICT -- Exp 04B: Amazon ESCI Query-Likelihood Ranking\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\nModel: {MODEL_NAME}\")\n",
    "print(f\"N queries: {len(results) if results else 0}\")\n",
    "print(f\"Pre-screen: {'PASSED' if PRESCREEN_PASSED else 'FAILED'}\")\n",
    "print(f\"Pre-screen bare AUC: {np.mean(prescreen_aucs):.3f}\")\n",
    "\n",
    "if not PRESCREEN_PASSED:\n",
    "    print(f\"\\n--- EXPERIMENT ABORTED ---\")\n",
    "    print(f\"Query-likelihood scoring cannot discriminate relevant from irrelevant\")\n",
    "    print(f\"products on ESCI with bare encoding.\")\n",
    "    print(f\"This matches v2 Exp 31 finding (QL AUC near chance on MS MARCO).\")\n",
    "\n",
    "    final_results = {\n",
    "        'experiment': 'exp04b_esci_ranking',\n",
    "        'model': MODEL_NAME,\n",
    "        'status': 'ABORTED_PRESCREEN',\n",
    "        'prescreen_bare_auc': float(np.mean(prescreen_aucs)),\n",
    "        'prescreen_n': N_PRESCREEN,\n",
    "        'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    }\n",
    "else:\n",
    "    # Full results\n",
    "    print(f\"\\n--- Ranking metrics summary ---\")\n",
    "    for metric_name in METRIC_NAMES:\n",
    "        print(f\"\\n  {METRIC_LABELS[metric_name]}:\")\n",
    "        bare_val = analysis[metric_name]['bare']['mean']\n",
    "        print(f\"    bare:           {bare_val:.3f}\")\n",
    "        for cond in COND_NAMES[1:]:\n",
    "            a = analysis[metric_name].get(cond, {})\n",
    "            mean = a.get('mean', 0)\n",
    "            delta = a.get('delta', 0)\n",
    "            d = a.get('d', 0)\n",
    "            p = a.get('p', 1)\n",
    "            sig = ('***' if p < 0.001/N_BONFERRONI else '**' if p < 0.01/N_BONFERRONI\n",
    "                   else '*' if p < 0.05/N_BONFERRONI else 'ns')\n",
    "            print(f\"    {cond:<22s}: {mean:.3f} ({delta:+.4f}, d={d:+.3f}) {sig}\")\n",
    "\n",
    "    # Differential verdict\n",
    "    print(f\"\\n--- Differential signal ---\")\n",
    "    for cond in COND_NAMES[1:]:\n",
    "        da = diff_analysis.get(cond, {})\n",
    "        diff_mean = da.get('differential_mean', 0)\n",
    "        d = da.get('d', 0)\n",
    "        p = da.get('p_onesided', 1)\n",
    "        pct = da.get('pct_positive', 0)\n",
    "        sig = ('***' if p < 0.001/N_BONFERRONI else '**' if p < 0.01/N_BONFERRONI\n",
    "               else '*' if p < 0.05/N_BONFERRONI else 'ns')\n",
    "        print(f\"  {cond:<22s}: diff={diff_mean:+.4f} d={d:+.3f} {pct:.0f}% positive {sig}\")\n",
    "\n",
    "    # surr_title spotlight\n",
    "    print(f\"\\n--- surr_title_trunc (the ad-serving surrogate) ---\")\n",
    "    title_auc = analysis['auc'].get('surr_title_trunc', {}).get('mean', 0)\n",
    "    bare_auc = analysis['auc']['bare']['mean']\n",
    "    oracle_auc = analysis['auc']['oracle_trunc']['mean']\n",
    "    if oracle_auc > bare_auc:\n",
    "        title_ratio = (title_auc - bare_auc) / (oracle_auc - bare_auc) * 100\n",
    "    else:\n",
    "        title_ratio = 0\n",
    "    print(f\"  AUC: {title_auc:.3f} ({title_ratio:.0f}% of oracle gain)\")\n",
    "    title_ndcg3 = analysis['ndcg3'].get('surr_title_trunc', {}).get('mean', 0)\n",
    "    bare_ndcg3 = analysis['ndcg3']['bare']['mean']\n",
    "    oracle_ndcg3 = analysis['ndcg3']['oracle_trunc']['mean']\n",
    "    print(f\"  NDCG@3: {title_ndcg3:.3f} (bare={bare_ndcg3:.3f}, oracle={oracle_ndcg3:.3f})\")\n",
    "\n",
    "    # Overall verdict\n",
    "    print(f\"\\n--- OVERALL VERDICT ---\")\n",
    "    oracle_auc_p = analysis['auc'].get('oracle_trunc', {}).get('p', 1)\n",
    "    oracle_auc_d = analysis['auc'].get('oracle_trunc', {}).get('d', 0)\n",
    "    if oracle_auc_p < 0.05/N_BONFERRONI and oracle_auc_d > 0:\n",
    "        print(f\"  RANKING SIGNAL DETECTED on ESCI\")\n",
    "        title_p = analysis['auc'].get('surr_title_trunc', {}).get('p', 1)\n",
    "        if title_p < 0.05/N_BONFERRONI:\n",
    "            print(f\"  surr_title_trunc also significant -- product title is a viable surrogate!\")\n",
    "            print(f\"  >>> This validates the ad-serving use case\")\n",
    "        else:\n",
    "            print(f\"  surr_title_trunc is NS -- need oracle query for ranking benefit\")\n",
    "    else:\n",
    "        print(f\"  NO ranking signal on ESCI (same outcome as v2)\")\n",
    "        print(f\"  Query-likelihood may not be the right scoring approach\")\n",
    "\n",
    "    final_results = {\n",
    "        'experiment': 'exp04b_esci_ranking',\n",
    "        'model': MODEL_NAME,\n",
    "        'status': 'COMPLETED',\n",
    "        'n_queries': len(results),\n",
    "        'n_bonferroni': N_BONFERRONI,\n",
    "        'prescreen_bare_auc': float(np.mean(prescreen_aucs)),\n",
    "        'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'analysis': analysis,\n",
    "        'diff_analysis': diff_analysis,\n",
    "        'hardness_analysis': hardness_analysis,\n",
    "        'pool_stats': {\n",
    "            'mean_products_per_query': float(np.mean([r['n_products'] for r in results])),\n",
    "            'total_products': int(sum(r['n_products'] for r in results)),\n",
    "        },\n",
    "    }\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "\n",
    "with open(RESULTS_DIR / 'results.json', 'w') as f:\n",
    "    json.dump(final_results, f, indent=2)\n",
    "print(f\"Results saved to {RESULTS_DIR / 'results.json'}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 14: Cleanup\n",
    "print(\"Cleaning up GPU memory...\")\n",
    "mem_before = torch.cuda.memory_allocated() / 1e9\n",
    "del model, processor, tokenizer\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "mem_after = torch.cuda.memory_allocated() / 1e9\n",
    "print(f\"GPU memory: {mem_before:.2f} GB -> {mem_after:.2f} GB\")\n",
    "print(\"Done!\")"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
