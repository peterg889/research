{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12573c35",
   "metadata": {},
   "source": [
    "# Experiment 04B: Amazon ESCI Query-Likelihood Ranking## The real ad-serving use case: which product best explains the user's query?### ContextExp 04A tested answer-likelihood ranking on MS MARCO (same scoring as Exps 01-03).This experiment tests **query-likelihood** ranking on a commercial product search dataset.### Key difference: Query-Likelihood ScoringNo gold answer text in ESCI. Instead, the decoder scores the query:```NLL(query | encode([condition + product_text]))```\"Given this product representation, how likely is the user's query?\"This IS the ad-serving question.### Dataset: Amazon ESCI- Real product search with graded relevance: Exact (3), Substitute (2), Complement (1), Irrelevant (0)- 130K+ queries, up to 40 candidates per query- Product text = title + bullet\\_points + description- Truly irrelevant products (unlike MS MARCO where all candidates are topically related)### Pre-Screenv2 Exp 31 got QL AUC=0.578 on MS MARCO (barely above chance) because all candidatesare topically similar. ESCI has truly Irrelevant products -> expect higher AUC.**Abort if bare QL AUC < 0.55 on first 20 queries.**### Conditions (6)| # | Condition | Encoder input | Note ||---|-----------|--------------|------|| 1 | bare | product\\_text only | Lower bound || 2 | oracle\\_trunc | real query + product\\_text | Upper bound || 3 | surr\\_title\\_trunc | product\\_title + product\\_text | Natural for ads! || 4 | surr\\_doc\\_trunc | TF keywords from product\\_text | Doc-derived || 5 | surr\\_template\\_trunc | \"What is [keyword]?\" + product\\_text | Templated || 6 | random\\_trunc | unrelated product text | Structural control |### Metrics- **NDCG@1**, **NDCG@3**, **NDCG@5** (graded: E=3, S=2, C=1, I=0)- **AUC** (binary: E+S vs C+I)- **MRR@3** (binary: E+S relevant)- **Hit@1** (binary: E+S relevant)### N=400 queries, Bonferroni=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "779fe7c7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T16:12:30.934242Z",
     "iopub.status.busy": "2026-02-18T16:12:30.933957Z",
     "iopub.status.idle": "2026-02-18T16:12:34.877334Z",
     "shell.execute_reply": "2026-02-18T16:12:34.876224Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exp 04B: Amazon ESCI Query-Likelihood Ranking\n",
      "Model: google/t5gemma-2-4b-4b\n",
      "N queries: 400 (pre-screen: 20)\n",
      "Relevance: {'Exact': 3, 'Substitute': 2, 'Complement': 1, 'Irrelevant': 0}\n",
      "Bonferroni comparisons: 5\n",
      "CUDA: NVIDIA L4\n",
      "GPU memory: 23.6 GB\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Setup\n",
    "import os\n",
    "os.umask(0o000)\n",
    "\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "import gc\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "from collections import Counter, defaultdict\n",
    "from scipy import stats\n",
    "from scipy.stats import wilcoxon\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "sys.path.insert(0, \"../..\")\n",
    "\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "RESULTS_DIR = Path(\"../../results/exp04b\")\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CHECKPOINT_PATH = RESULTS_DIR / \"checkpoint.json\"\n",
    "\n",
    "N_SAMPLES = 400   # queries\n",
    "N_PRESCREEN = 20  # queries for pre-screen\n",
    "MODEL_NAME = \"google/t5gemma-2-4b-4b\"\n",
    "N_BONFERRONI = 5  # 5 non-bare conditions\n",
    "\n",
    "# Graded relevance mapping\n",
    "ESCI_RELEVANCE = {'Exact': 3, 'Substitute': 2, 'Complement': 1, 'Irrelevant': 0}\n",
    "ESCI_BINARY = {'Exact': 1, 'Substitute': 1, 'Complement': 0, 'Irrelevant': 0}\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv())\n",
    "HF_TOKEN = os.environ.get(\"HF_TOKEN\")\n",
    "\n",
    "print(f\"Exp 04B: Amazon ESCI Query-Likelihood Ranking\")\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"N queries: {N_SAMPLES} (pre-screen: {N_PRESCREEN})\")\n",
    "print(f\"Relevance: {ESCI_RELEVANCE}\")\n",
    "print(f\"Bonferroni comparisons: {N_BONFERRONI}\")\n",
    "print(f\"CUDA: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4646c92a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T16:12:34.881433Z",
     "iopub.status.busy": "2026-02-18T16:12:34.880592Z",
     "iopub.status.idle": "2026-02-18T16:13:48.250879Z",
     "shell.execute_reply": "2026-02-18T16:13:48.249958Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading google/t5gemma-2-4b-4b...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a30f7056bb94bdfaf6234749925461a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/1327 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded. dtype=torch.bfloat16\n",
      "GPU memory used: 15.02 GB\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Load model\n",
    "from transformers import AutoProcessor, AutoModelForSeq2SeqLM\n",
    "\n",
    "print(f\"Loading {MODEL_NAME}...\")\n",
    "processor = AutoProcessor.from_pretrained(MODEL_NAME, token=HF_TOKEN)\n",
    "tokenizer = processor.tokenizer\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    token=HF_TOKEN,\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "DEVICE = next(model.parameters()).device\n",
    "print(f\"Model loaded. dtype={next(model.parameters()).dtype}\")\n",
    "print(f\"GPU memory used: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b2d4685",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T16:13:48.255271Z",
     "iopub.status.busy": "2026-02-18T16:13:48.254676Z",
     "iopub.status.idle": "2026-02-18T16:13:48.275646Z",
     "shell.execute_reply": "2026-02-18T16:13:48.274812Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helpers defined.\n",
      "  Scoring: score_nll (query-likelihood)\n",
      "  Surrogates: title, doc_kw, template, random\n",
      "  Ranking metrics: NDCG@k, AUC, MRR@k, Hit@k\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Scoring and ranking helpers\n",
    "\n",
    "def score_nll(encoder_text, target_text, prefix_token_count=0, truncate=False):\n",
    "    '''Score NLL of target tokens with optional truncation.\n",
    "    For ESCI: target_text is the QUERY (query-likelihood scoring).'''\n",
    "    enc_ids = tokenizer(encoder_text, return_tensors=\"pt\",\n",
    "                        add_special_tokens=True, truncation=True,\n",
    "                        max_length=8192).input_ids.to(DEVICE)\n",
    "    total_enc_len = enc_ids.shape[1]\n",
    "\n",
    "    enc_mask = torch.ones(1, total_enc_len, device=DEVICE, dtype=torch.long)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        encoder_outputs = model.get_encoder()(\n",
    "            input_ids=enc_ids, attention_mask=enc_mask\n",
    "        )\n",
    "\n",
    "    if truncate and prefix_token_count > 0:\n",
    "        cross_attn_mask = torch.ones(1, total_enc_len, device=DEVICE, dtype=torch.long)\n",
    "        cross_attn_mask[:, :prefix_token_count] = 0\n",
    "    else:\n",
    "        cross_attn_mask = torch.ones(1, total_enc_len, device=DEVICE, dtype=torch.long)\n",
    "\n",
    "    target_ids = tokenizer(target_text, return_tensors=\"pt\",\n",
    "                           add_special_tokens=False, truncation=True,\n",
    "                           max_length=256).input_ids.to(DEVICE)\n",
    "\n",
    "    if target_ids.shape[1] == 0:\n",
    "        return 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            encoder_outputs=encoder_outputs,\n",
    "            attention_mask=cross_attn_mask,\n",
    "            labels=target_ids,\n",
    "        )\n",
    "\n",
    "    logits = outputs.logits\n",
    "    log_probs = F.log_softmax(logits, dim=-1)\n",
    "    token_log_probs = log_probs[0].gather(1, target_ids[0].unsqueeze(1)).squeeze(1)\n",
    "    mean_nll = -token_log_probs.mean().item()\n",
    "\n",
    "    del encoder_outputs, outputs, logits, log_probs\n",
    "    return mean_nll\n",
    "\n",
    "\n",
    "def count_prefix_tokens(prefix_text, document_text):\n",
    "    '''Count how many tokens the prefix occupies in the concatenated encoding.'''\n",
    "    full_text = prefix_text + \"\\n\" + document_text\n",
    "    full_ids = tokenizer(full_text, add_special_tokens=True).input_ids\n",
    "    doc_ids = tokenizer(document_text, add_special_tokens=True).input_ids\n",
    "    return len(full_ids) - len(doc_ids)\n",
    "\n",
    "\n",
    "# === Surrogate generation ===\n",
    "STOP_WORDS = {\n",
    "    'a', 'an', 'the', 'is', 'are', 'was', 'were', 'be', 'been', 'being',\n",
    "    'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could',\n",
    "    'should', 'may', 'might', 'can', 'shall', 'to', 'of', 'in', 'for',\n",
    "    'on', 'with', 'at', 'by', 'from', 'as', 'into', 'through', 'during',\n",
    "    'before', 'after', 'above', 'below', 'between', 'and', 'but', 'or',\n",
    "    'not', 'no', 'if', 'then', 'than', 'so', 'up', 'out', 'about',\n",
    "    'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those',\n",
    "    'it', 'its', 'i', 'me', 'my', 'we', 'our', 'you', 'your', 'he',\n",
    "    'him', 'his', 'she', 'her', 'they', 'them', 'their', 'how', 'when',\n",
    "    'where', 'why', 'much', 'many', 'some', 'any', 'all', 'each',\n",
    "    'does', 'also', 'just', 'more', 'most', 'very', 'too', 'only',\n",
    "}\n",
    "\n",
    "def extract_keywords(text):\n",
    "    words = re.sub(r'[^\\w\\s]', '', text.lower()).split()\n",
    "    return [w for w in words if w not in STOP_WORDS and len(w) > 2]\n",
    "\n",
    "def make_surrogate_doc_kw(text):\n",
    "    content_words = extract_keywords(text)\n",
    "    if not content_words:\n",
    "        return \"information\"\n",
    "    counts = Counter(content_words)\n",
    "    return \" \".join(w for w, _ in counts.most_common(5))\n",
    "\n",
    "def make_surrogate_template(text):\n",
    "    content_words = extract_keywords(text)\n",
    "    if not content_words:\n",
    "        return \"What is this about?\"\n",
    "    counts = Counter(content_words)\n",
    "    top_word = counts.most_common(1)[0][0]\n",
    "    return f\"What is {top_word}?\"\n",
    "\n",
    "\n",
    "# === Ranking metrics ===\n",
    "def ndcg_at_k(relevance_scores_ranked, k):\n",
    "    '''NDCG@k. relevance_scores_ranked: ALL relevance scores in predicted rank order.'''\n",
    "    rel = np.asarray(relevance_scores_ranked[:k], dtype=float)\n",
    "    dcg = np.sum(rel / np.log2(np.arange(2, len(rel) + 2)))\n",
    "    all_rel = np.asarray(relevance_scores_ranked, dtype=float)\n",
    "    ideal = np.sort(all_rel)[::-1][:k]\n",
    "    idcg = np.sum(ideal / np.log2(np.arange(2, len(ideal) + 2)))\n",
    "    return dcg / idcg if idcg > 0 else 0.0\n",
    "\n",
    "def compute_auc_binary(nlls, binary_labels):\n",
    "    '''AUC for binary relevance (E+S=1 vs C+I=0). Lower NLL = more relevant.'''\n",
    "    pos_nlls = [nlls[i] for i in range(len(nlls)) if binary_labels[i] == 1]\n",
    "    neg_nlls = [nlls[i] for i in range(len(nlls)) if binary_labels[i] == 0]\n",
    "    if len(pos_nlls) == 0 or len(neg_nlls) == 0:\n",
    "        return 0.5\n",
    "    wins = sum(1 for p in pos_nlls for n in neg_nlls if n > p)\n",
    "    ties = sum(1 for p in pos_nlls for n in neg_nlls if n == p)\n",
    "    return (wins + 0.5 * ties) / (len(pos_nlls) * len(neg_nlls))\n",
    "\n",
    "def compute_mrr_at_k(nlls, binary_labels, k=3):\n",
    "    '''MRR@k: reciprocal rank of first relevant item in top-k by ascending NLL.'''\n",
    "    ranked_indices = list(np.argsort(nlls))\n",
    "    for rank, idx in enumerate(ranked_indices[:k], 1):\n",
    "        if binary_labels[idx] == 1:\n",
    "            return 1.0 / rank\n",
    "    return 0.0\n",
    "\n",
    "def compute_hit_at_k(nlls, binary_labels, k=1):\n",
    "    '''Hit@k: 1 if any relevant item in top-k by ascending NLL.'''\n",
    "    ranked_indices = np.argsort(nlls)[:k]\n",
    "    return 1.0 if any(binary_labels[idx] == 1 for idx in ranked_indices) else 0.0\n",
    "\n",
    "print(\"Helpers defined.\")\n",
    "print(\"  Scoring: score_nll (query-likelihood)\")\n",
    "print(\"  Surrogates: title, doc_kw, template, random\")\n",
    "print(\"  Ranking metrics: NDCG@k, AUC, MRR@k, Hit@k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e89eb8a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T16:13:48.279227Z",
     "iopub.status.busy": "2026-02-18T16:13:48.278592Z",
     "iopub.status.idle": "2026-02-18T16:19:09.552741Z",
     "shell.execute_reply": "2026-02-18T16:19:09.551541Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Amazon ESCI dataset...\n",
      "  Source: tasksource/esci (pre-joined, ~2M rows)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Loaded 2027874 rows\n",
      "  Columns: ['example_id', 'query', 'query_id', 'product_id', 'product_locale', 'esci_label', 'small_version', 'large_version', 'product_title', 'product_description', 'product_bullet_point', 'product_brand', 'product_color', 'product_text']\n",
      "\n",
      "Filtering to US locale and building query pools...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6c90bd2d715409890067a85b57251dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filtering:   0%|          | 0/2027874 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "US queries with products: 74888\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qualifying queries (4+ candidates, rel + irrel): 30099\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Selected 400 queries\n",
      "Products per query: mean=21.6, median=16, min=9, max=138\n",
      "  Exact: 4857 (56.2%)\n",
      "  Substitute: 1684 (19.5%)\n",
      "  Complement: 401 (4.6%)\n",
      "  Irrelevant: 1701 (19.7%)\n",
      "Total scoring calls: 51858\n",
      "Estimated runtime: ~7.2 hours\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Load Amazon ESCI data and build ranking pools\n",
    "from datasets import load_dataset\n",
    "\n",
    "print(\"Loading Amazon ESCI dataset...\")\n",
    "print(\"  Source: tasksource/esci (pre-joined, ~2M rows)\")\n",
    "ds = load_dataset(\"tasksource/esci\", split=\"train\")\n",
    "print(f\"  Loaded {len(ds)} rows\")\n",
    "print(f\"  Columns: {ds.column_names}\")\n",
    "\n",
    "# Filter to US locale and build product text\n",
    "print(\"\\nFiltering to US locale and building query pools...\")\n",
    "\n",
    "def safe_str(val):\n",
    "    '''Handle None/NaN values in product fields.'''\n",
    "    if val is None:\n",
    "        return \"\"\n",
    "    s = str(val).strip()\n",
    "    if s.lower() in ('nan', 'none', ''):\n",
    "        return \"\"\n",
    "    return s\n",
    "\n",
    "def build_product_text(row):\n",
    "    '''Use pre-built product_text if available, else concatenate fields.'''\n",
    "    pt = safe_str(row.get('product_text', ''))\n",
    "    if pt:\n",
    "        return pt\n",
    "    parts = []\n",
    "    for field in ['product_title', 'product_bullet_point', 'product_description']:\n",
    "        val = safe_str(row.get(field, ''))\n",
    "        if val:\n",
    "            parts.append(val)\n",
    "    return \" \".join(parts)\n",
    "\n",
    "# Group by query\n",
    "query_pools = defaultdict(list)\n",
    "\n",
    "for row in tqdm(ds, desc=\"Filtering\"):\n",
    "    if safe_str(row.get('product_locale', '')) != 'us':\n",
    "        continue\n",
    "\n",
    "    label = safe_str(row.get('esci_label', ''))\n",
    "    if label not in ESCI_RELEVANCE:\n",
    "        continue\n",
    "\n",
    "    product_text = build_product_text(row)\n",
    "    if len(product_text.split()) < 5:\n",
    "        continue\n",
    "\n",
    "    query = safe_str(row.get('query', ''))\n",
    "    if not query:\n",
    "        continue\n",
    "\n",
    "    product_title = safe_str(row.get('product_title', ''))\n",
    "\n",
    "    query_pools[query].append({\n",
    "        'product_text': product_text,\n",
    "        'product_title': product_title if product_title else product_text.split()[0],\n",
    "        'label': label,\n",
    "        'relevance': ESCI_RELEVANCE[label],\n",
    "        'binary_label': ESCI_BINARY[label],\n",
    "    })\n",
    "\n",
    "del ds\n",
    "gc.collect()\n",
    "\n",
    "print(f\"US queries with products: {len(query_pools)}\")\n",
    "\n",
    "# Filter: 4+ candidates, at least 1 relevant and 1 irrelevant\n",
    "qualifying_queries = []\n",
    "for query_text, products in query_pools.items():\n",
    "    if len(products) < 4:\n",
    "        continue\n",
    "    has_relevant = any(p['binary_label'] == 1 for p in products)\n",
    "    has_irrelevant = any(p['binary_label'] == 0 for p in products)\n",
    "    if not (has_relevant and has_irrelevant):\n",
    "        continue\n",
    "    qualifying_queries.append({\n",
    "        'query': query_text,\n",
    "        'products': products,\n",
    "        'n_products': len(products),\n",
    "    })\n",
    "\n",
    "del query_pools\n",
    "gc.collect()\n",
    "\n",
    "print(f\"Qualifying queries (4+ candidates, rel + irrel): {len(qualifying_queries)}\")\n",
    "\n",
    "if len(qualifying_queries) < N_SAMPLES + 50:\n",
    "    print(f\"WARNING: Only {len(qualifying_queries)} qualifying queries (need {N_SAMPLES}+50)\")\n",
    "    print(f\"Reducing N_SAMPLES to {max(0, len(qualifying_queries) - 50)}\")\n",
    "    N_SAMPLES = max(0, len(qualifying_queries) - 50)\n",
    "\n",
    "# Shuffle and select\n",
    "np.random.seed(SEED)\n",
    "np.random.shuffle(qualifying_queries)\n",
    "\n",
    "# We need N_SAMPLES + some buffer; keep extra for pre-screen\n",
    "queries = qualifying_queries[:N_SAMPLES + 50]\n",
    "del qualifying_queries\n",
    "gc.collect()\n",
    "\n",
    "# Generate surrogates for each product in each query\n",
    "for i, q in enumerate(queries):\n",
    "    for p in q['products']:\n",
    "        p['surr_doc_kw'] = make_surrogate_doc_kw(p['product_text'])\n",
    "        p['surr_template'] = make_surrogate_template(p['product_text'])\n",
    "\n",
    "    # Random: use product text from another query (circular offset)\n",
    "    other_idx = (i + len(queries) // 2) % len(queries)\n",
    "    other_product = queries[other_idx]['products'][0]\n",
    "    q['surr_random'] = \" \".join(other_product['product_text'].split()[:20])\n",
    "\n",
    "# Stats\n",
    "n_products = [q['n_products'] for q in queries[:N_SAMPLES]]\n",
    "print(f\"\\nSelected {min(len(queries), N_SAMPLES)} queries\")\n",
    "print(f\"Products per query: mean={np.mean(n_products):.1f}, \"\n",
    "      f\"median={np.median(n_products):.0f}, \"\n",
    "      f\"min={np.min(n_products)}, max={np.max(n_products)}\")\n",
    "\n",
    "# Label distribution\n",
    "all_labels = [p['label'] for q in queries[:N_SAMPLES] for p in q['products']]\n",
    "for label in ['Exact', 'Substitute', 'Complement', 'Irrelevant']:\n",
    "    count = sum(1 for l in all_labels if l == label)\n",
    "    print(f\"  {label}: {count} ({100*count/len(all_labels):.1f}%)\")\n",
    "\n",
    "total_calls = sum(n_products) * 6  # 6 conditions\n",
    "print(f\"Total scoring calls: {total_calls}\")\n",
    "print(f\"Estimated runtime: ~{total_calls * 0.5 / 3600:.1f} hours\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dbac1db4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T16:19:09.558041Z",
     "iopub.status.busy": "2026-02-18T16:19:09.556767Z",
     "iopub.status.idle": "2026-02-18T16:20:51.224252Z",
     "shell.execute_reply": "2026-02-18T16:20:51.223378Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PRE-SCREEN: Bare Query-Likelihood AUC\n",
      "======================================================================\n",
      "Scoring 20 queries with bare condition only...\n",
      "v2 Exp 31 got QL AUC=0.578 on MS MARCO (near chance)\n",
      "ESCI has truly Irrelevant products -> expect higher AUC\n",
      "Abort threshold: AUC < 0.55\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "704e236855364d07b5e871658ee12f76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pre-screen:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pre-screen complete: 1.7 min\n",
      "Bare QL AUC: mean=0.713, median=0.775\n",
      "Range: [0.179, 1.000]\n",
      "\n",
      ">>> PRE-SCREEN PASSED (AUC=0.713 >= 0.55)\n",
      ">>> Proceeding with full experiment\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Pre-screen -- bare QL AUC on first 20 queries\n",
    "print(\"=\" * 70)\n",
    "print(\"PRE-SCREEN: Bare Query-Likelihood AUC\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Scoring {N_PRESCREEN} queries with bare condition only...\")\n",
    "print(f\"v2 Exp 31 got QL AUC=0.578 on MS MARCO (near chance)\")\n",
    "print(f\"ESCI has truly Irrelevant products -> expect higher AUC\")\n",
    "print(f\"Abort threshold: AUC < 0.55\")\n",
    "\n",
    "prescreen_aucs = []\n",
    "t0 = time.time()\n",
    "\n",
    "for q_idx in tqdm(range(N_PRESCREEN), desc=\"Pre-screen\"):\n",
    "    q = queries[q_idx]\n",
    "    query_text = q['query']\n",
    "    nlls = []\n",
    "\n",
    "    for p in q['products']:\n",
    "        nll = score_nll(p['product_text'], query_text, 0, False)\n",
    "        nlls.append(nll)\n",
    "\n",
    "    binary_labels = [p['binary_label'] for p in q['products']]\n",
    "    auc = compute_auc_binary(nlls, binary_labels)\n",
    "    prescreen_aucs.append(auc)\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "elapsed = time.time() - t0\n",
    "mean_auc = np.mean(prescreen_aucs)\n",
    "print(f\"\\nPre-screen complete: {elapsed/60:.1f} min\")\n",
    "print(f\"Bare QL AUC: mean={mean_auc:.3f}, median={np.median(prescreen_aucs):.3f}\")\n",
    "print(f\"Range: [{np.min(prescreen_aucs):.3f}, {np.max(prescreen_aucs):.3f}]\")\n",
    "\n",
    "PRESCREEN_PASSED = mean_auc >= 0.55\n",
    "if PRESCREEN_PASSED:\n",
    "    print(f\"\\n>>> PRE-SCREEN PASSED (AUC={mean_auc:.3f} >= 0.55)\")\n",
    "    print(f\">>> Proceeding with full experiment\")\n",
    "else:\n",
    "    print(f\"\\n>>> PRE-SCREEN FAILED (AUC={mean_auc:.3f} < 0.55)\")\n",
    "    print(f\">>> QL scoring cannot distinguish relevant from irrelevant on ESCI\")\n",
    "    print(f\">>> ABORTING -- no point running 400 queries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2e36112",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T16:20:51.228125Z",
     "iopub.status.busy": "2026-02-18T16:20:51.227838Z",
     "iopub.status.idle": "2026-02-18T16:20:51.237744Z",
     "shell.execute_reply": "2026-02-18T16:20:51.236882Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "CONDITION EXAMPLES\n",
      "======================================================================\n",
      "\n",
      "Query: day hike backpack\n",
      "Products: 12\n",
      "\n",
      "--- Relevant product (label=Exact) ---\n",
      "  Title: Osprey Packs Talon 22 Men's Hiking Backpack, Medium/Large, Black\n",
      "  Text: Osprey Packs Talon 22 Men's Hiking Backpack, Medium/Large, Black\n",
      "Osprey\n",
      "Black\n",
      "None\n",
      "Dual-zippered access to main compartment\n",
      "External hydration sleeve ...\n",
      "\n",
      "--- Irrelevant product (label=Irrelevant) ---\n",
      "  Title: Columbia womens Newton Ridge Plus Waterproof Amped Hiking Boot, Light Lichen/Canvas Tan, 8 US\n",
      "  Text: Columbia womens Newton Ridge Plus Waterproof Amped Hiking Boot, Light Lichen/Canvas Tan, 8 US\n",
      "Columbia\n",
      "Light Lichen/Canvas Tan\n",
      "None\n",
      "Advanced technolog...\n",
      "\n",
      "--- Encoder input for relevant product ---\n",
      "  bare                  : Osprey Packs Talon 22 Men's Hiking Backpack, Medium/Large, Black\n",
      "Osprey\n",
      "Black\n",
      "No...\n",
      "  oracle_trunc          : day hike backpack | Osprey Packs Talon 22 Men's Hiking Backpack, Medium/Large, B...\n",
      "  surr_title_trunc      : Osprey Packs Talon 22 Men's Hiking Backp | Osprey Packs Talon 22 Men's Hiking Backp...\n",
      "  surr_doc_trunc        : osprey black pockets packs talon | Osprey Packs Talon 22 Men's Hiking Backpack, Mediu...\n",
      "  surr_template_trunc   : What is osprey? | Osprey Packs Talon 22 Men's Hiking Backpack, Mediu...\n",
      "  random_trunc          : Klimoto condenser | fits Nissan 350Z 200... | Osprey Packs Talon 22 Men's Hiking Backp...\n",
      "\n",
      "  Decoder target: 'day hike backpack' (query-likelihood)\n",
      "\n",
      "--- Note: surr_title_trunc uses the product's OWN title as prefix ---\n",
      "  This is the most natural ad-serving surrogate (title always available)\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Explain conditions with concrete example\n",
    "if not PRESCREEN_PASSED:\n",
    "    print(\"SKIPPED (pre-screen failed)\")\n",
    "else:\n",
    "    print(\"=\" * 70)\n",
    "    print(\"CONDITION EXAMPLES\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    COND_NAMES = ['bare', 'oracle_trunc', 'surr_title_trunc',\n",
    "                  'surr_doc_trunc', 'surr_template_trunc', 'random_trunc']\n",
    "\n",
    "    ex = queries[0]\n",
    "    print(f\"\\nQuery: {ex['query']}\")\n",
    "    print(f\"Products: {ex['n_products']}\")\n",
    "\n",
    "    # Show one relevant and one irrelevant product\n",
    "    rel_p = next(p for p in ex['products'] if p['binary_label'] == 1)\n",
    "    irr_p = next(p for p in ex['products'] if p['binary_label'] == 0)\n",
    "\n",
    "    print(f\"\\n--- Relevant product (label={rel_p['label']}) ---\")\n",
    "    print(f\"  Title: {rel_p['product_title'][:100]}\")\n",
    "    print(f\"  Text: {rel_p['product_text'][:150]}...\")\n",
    "\n",
    "    print(f\"\\n--- Irrelevant product (label={irr_p['label']}) ---\")\n",
    "    print(f\"  Title: {irr_p['product_title'][:100]}\")\n",
    "    print(f\"  Text: {irr_p['product_text'][:150]}...\")\n",
    "\n",
    "    print(f\"\\n--- Encoder input for relevant product ---\")\n",
    "    for cond in COND_NAMES:\n",
    "        if cond == 'bare':\n",
    "            enc = rel_p['product_text'][:80] + \"...\"\n",
    "        elif cond == 'oracle_trunc':\n",
    "            enc = ex['query'] + \" | \" + rel_p['product_text'][:60] + \"...\"\n",
    "        elif cond == 'surr_title_trunc':\n",
    "            enc = rel_p['product_title'][:40] + \" | \" + rel_p['product_text'][:40] + \"...\"\n",
    "        elif cond == 'surr_doc_trunc':\n",
    "            enc = rel_p['surr_doc_kw'] + \" | \" + rel_p['product_text'][:50] + \"...\"\n",
    "        elif cond == 'surr_template_trunc':\n",
    "            enc = rel_p['surr_template'] + \" | \" + rel_p['product_text'][:50] + \"...\"\n",
    "        elif cond == 'random_trunc':\n",
    "            enc = ex['surr_random'][:40] + \"... | \" + rel_p['product_text'][:40] + \"...\"\n",
    "        print(f\"  {cond:<22s}: {enc}\")\n",
    "\n",
    "    print(f\"\\n  Decoder target: '{ex['query']}' (query-likelihood)\")\n",
    "    print(f\"\\n--- Note: surr_title_trunc uses the product's OWN title as prefix ---\")\n",
    "    print(f\"  This is the most natural ad-serving surrogate (title always available)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86be2e56",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T16:20:51.240984Z",
     "iopub.status.busy": "2026-02-18T16:20:51.240572Z",
     "iopub.status.idle": "2026-02-18T19:35:20.648566Z",
     "shell.execute_reply": "2026-02-18T19:35:20.647688Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "RUNNING RANKING EXPERIMENT\n",
      "======================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cadae4947404068b1f947a9eda989e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Queries:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 20/400 | 9.8m | ETA 186.1m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 40/400 | 18.6m | ETA 167.7m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 60/400 | 26.2m | ETA 148.4m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 80/400 | 37.6m | ETA 150.3m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 100/400 | 46.8m | ETA 140.5m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 120/400 | 57.1m | ETA 133.2m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 140/400 | 67.4m | ETA 125.1m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 160/400 | 77.3m | ETA 115.9m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 180/400 | 86.2m | ETA 105.4m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 200/400 | 96.6m | ETA 96.6m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 220/400 | 107.8m | ETA 88.2m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 240/400 | 117.5m | ETA 78.4m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 260/400 | 125.4m | ETA 67.5m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 280/400 | 135.3m | ETA 58.0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 300/400 | 144.7m | ETA 48.2m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 320/400 | 154.2m | ETA 38.5m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 340/400 | 164.5m | ETA 29.0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 360/400 | 175.1m | ETA 19.5m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 380/400 | 184.3m | ETA 9.7m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 400/400 | 194.5m | ETA 0.0m\n",
      "\n",
      "Scoring complete: 400 queries in 194.5 min\n",
      "Bare QL AUC: mean=0.709\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Run scoring -- outer loop over queries\n",
    "if not PRESCREEN_PASSED:\n",
    "    print(\"SKIPPED (pre-screen failed)\")\n",
    "    results = []\n",
    "else:\n",
    "    print(\"=\" * 70)\n",
    "    print(\"RUNNING RANKING EXPERIMENT\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    def build_condition_input(cond_name, product_data, query_data):\n",
    "        '''Return (encoder_text, prefix_token_count, truncate) for a condition.'''\n",
    "        product_text = product_data['product_text']\n",
    "\n",
    "        if cond_name == 'bare':\n",
    "            return product_text, 0, False\n",
    "        elif cond_name == 'oracle_trunc':\n",
    "            surr = query_data['query']\n",
    "        elif cond_name == 'surr_title_trunc':\n",
    "            surr = product_data['product_title']\n",
    "        elif cond_name == 'surr_doc_trunc':\n",
    "            surr = product_data['surr_doc_kw']\n",
    "        elif cond_name == 'surr_template_trunc':\n",
    "            surr = product_data['surr_template']\n",
    "        elif cond_name == 'random_trunc':\n",
    "            surr = query_data['surr_random']\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown condition: {cond_name}\")\n",
    "\n",
    "        enc_text = surr + \"\\n\" + product_text\n",
    "        prefix_count = count_prefix_tokens(surr, product_text)\n",
    "        return enc_text, prefix_count, True\n",
    "\n",
    "    # Resume from checkpoint\n",
    "    results = []\n",
    "    start_idx = 0\n",
    "    if CHECKPOINT_PATH.exists():\n",
    "        saved = json.loads(CHECKPOINT_PATH.read_text())\n",
    "        if saved.get('n_total') == N_SAMPLES:\n",
    "            saved_results = saved.get('results', [])\n",
    "            saved_queries = [r['query'][:50] for r in saved_results]\n",
    "            current_queries = [q['query'][:50] for q in queries[:len(saved_results)]]\n",
    "            if saved_queries == current_queries:\n",
    "                results = saved_results\n",
    "                start_idx = len(results)\n",
    "                print(f\"Resumed from checkpoint: {start_idx}/{N_SAMPLES} queries\")\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    for q_idx in tqdm(range(start_idx, N_SAMPLES), initial=start_idx, total=N_SAMPLES,\n",
    "                      desc=\"Queries\"):\n",
    "        q = queries[q_idx]\n",
    "        query_text = q['query']\n",
    "\n",
    "        query_result = {\n",
    "            'query_idx': q_idx,\n",
    "            'query': query_text,\n",
    "            'n_products': q['n_products'],\n",
    "            'labels': [p['label'] for p in q['products']],\n",
    "            'relevances': [p['relevance'] for p in q['products']],\n",
    "            'binary_labels': [p['binary_label'] for p in q['products']],\n",
    "            'scores': {},\n",
    "        }\n",
    "\n",
    "        for cond_name in COND_NAMES:\n",
    "            cond_nlls = []\n",
    "            for p_idx, product_data in enumerate(q['products']):\n",
    "                enc_text, prefix_count, truncate = build_condition_input(\n",
    "                    cond_name, product_data, q)\n",
    "                nll = score_nll(enc_text, query_text, prefix_count, truncate)\n",
    "                cond_nlls.append(nll)\n",
    "            query_result['scores'][cond_name] = cond_nlls\n",
    "\n",
    "        results.append(query_result)\n",
    "\n",
    "        if (q_idx + 1) % 20 == 0 or q_idx == N_SAMPLES - 1:\n",
    "            ckpt = {\n",
    "                'n_total': N_SAMPLES,\n",
    "                'results': results,\n",
    "                'completed': len(results),\n",
    "                'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            }\n",
    "            CHECKPOINT_PATH.write_text(json.dumps(ckpt))\n",
    "            elapsed = time.time() - t0\n",
    "            done = q_idx - start_idx + 1\n",
    "            eta = (N_SAMPLES - q_idx - 1) * elapsed / done if done > 0 else 0\n",
    "            tqdm.write(f\"  Checkpoint {q_idx+1}/{N_SAMPLES} | {elapsed/60:.1f}m | ETA {eta/60:.1f}m\")\n",
    "\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    elapsed_total = time.time() - t0\n",
    "    print(f\"\\nScoring complete: {len(results)} queries in {elapsed_total/60:.1f} min\")\n",
    "\n",
    "    # Quick peek\n",
    "    bare_aucs = []\n",
    "    for r in results:\n",
    "        nlls = np.array(r['scores']['bare'])\n",
    "        bare_aucs.append(compute_auc_binary(nlls, r['binary_labels']))\n",
    "    print(f\"Bare QL AUC: mean={np.mean(bare_aucs):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9439ea8c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T19:35:20.652205Z",
     "iopub.status.busy": "2026-02-18T19:35:20.651617Z",
     "iopub.status.idle": "2026-02-18T19:35:20.981650Z",
     "shell.execute_reply": "2026-02-18T19:35:20.980782Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "COMPUTING RANKING METRICS\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  bare                  : AUC=0.709  NDCG@3=0.876  MRR@3=0.943  Hit@1=0.915\n",
      "  oracle_trunc          : AUC=0.699  NDCG@3=0.865  MRR@3=0.938  Hit@1=0.900\n",
      "  surr_title_trunc      : AUC=0.696  NDCG@3=0.863  MRR@3=0.935  Hit@1=0.897\n",
      "  surr_doc_trunc        : AUC=0.718  NDCG@3=0.880  MRR@3=0.938  Hit@1=0.902\n",
      "  surr_template_trunc   : AUC=0.724  NDCG@3=0.881  MRR@3=0.941  Hit@1=0.907\n",
      "  random_trunc          : AUC=0.717  NDCG@3=0.888  MRR@3=0.946  Hit@1=0.920\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Compute ranking metrics\n",
    "if not PRESCREEN_PASSED or len(results) == 0:\n",
    "    print(\"SKIPPED (pre-screen failed or no results)\")\n",
    "    metrics = {}\n",
    "else:\n",
    "    print(\"=\" * 70)\n",
    "    print(\"COMPUTING RANKING METRICS\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    # For each query x condition: NDCG@1/3/5, AUC, MRR@3, Hit@1\n",
    "    metrics = {cond: {'ndcg1': [], 'ndcg3': [], 'ndcg5': [],\n",
    "                      'auc': [], 'mrr3': [], 'hit1': []}\n",
    "               for cond in COND_NAMES}\n",
    "\n",
    "    for r in results:\n",
    "        binary_labels = r['binary_labels']\n",
    "        relevances = r['relevances']\n",
    "\n",
    "        for cond in COND_NAMES:\n",
    "            nlls = np.array(r['scores'][cond])\n",
    "            ranked_indices = np.argsort(nlls)  # ascending NLL = best first\n",
    "            ranked_relevances = [relevances[i] for i in ranked_indices]\n",
    "\n",
    "            metrics[cond]['ndcg1'].append(ndcg_at_k(ranked_relevances, k=1))\n",
    "            metrics[cond]['ndcg3'].append(ndcg_at_k(ranked_relevances, k=3))\n",
    "            metrics[cond]['ndcg5'].append(ndcg_at_k(ranked_relevances, k=5))\n",
    "            metrics[cond]['auc'].append(compute_auc_binary(nlls, binary_labels))\n",
    "            metrics[cond]['mrr3'].append(compute_mrr_at_k(nlls, binary_labels, k=3))\n",
    "            metrics[cond]['hit1'].append(compute_hit_at_k(nlls, binary_labels, k=1))\n",
    "\n",
    "    # Convert to arrays\n",
    "    for cond in COND_NAMES:\n",
    "        for m in metrics[cond]:\n",
    "            metrics[cond][m] = np.array(metrics[cond][m])\n",
    "\n",
    "    # Quick summary\n",
    "    for cond in COND_NAMES:\n",
    "        print(f\"  {cond:<22s}: AUC={metrics[cond]['auc'].mean():.3f}  \"\n",
    "              f\"NDCG@3={metrics[cond]['ndcg3'].mean():.3f}  \"\n",
    "              f\"MRR@3={metrics[cond]['mrr3'].mean():.3f}  \"\n",
    "              f\"Hit@1={metrics[cond]['hit1'].mean():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "365e23f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T19:35:20.985504Z",
     "iopub.status.busy": "2026-02-18T19:35:20.984872Z",
     "iopub.status.idle": "2026-02-18T19:35:21.035229Z",
     "shell.execute_reply": "2026-02-18T19:35:21.034372Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "RESULTS: ESCI Query-Likelihood Ranking (N=400 queries)\n",
      "======================================================================\n",
      "\n",
      "--- NDCG@1 ---\n",
      "  Condition                  Mean    vs Bare        d            p   sig\n",
      "  ----------------------------------------------------------------------\n",
      "  bare                      0.896         --       --           --    --\n",
      "  oracle_trunc              0.887    -0.0092   -0.037     4.16e-01    ns\n",
      "  surr_title_trunc          0.873    -0.0233   -0.084     9.51e-02    ns\n",
      "  surr_doc_trunc            0.892    -0.0042   -0.020     5.88e-01    ns\n",
      "  surr_template_trunc       0.896    +0.0000   +0.000     7.98e-01    ns\n",
      "  random_trunc              0.908    +0.0125   +0.067     2.72e-01    ns\n",
      "\n",
      "--- NDCG@3 ---\n",
      "  Condition                  Mean    vs Bare        d            p   sig\n",
      "  ----------------------------------------------------------------------\n",
      "  bare                      0.876         --       --           --    --\n",
      "  oracle_trunc              0.865    -0.0106   -0.086     1.71e-01    ns\n",
      "  surr_title_trunc          0.863    -0.0126   -0.089     7.43e-02    ns\n",
      "  surr_doc_trunc            0.880    +0.0044   +0.042     1.94e-01    ns\n",
      "  surr_template_trunc       0.881    +0.0052   +0.048     2.59e-01    ns\n",
      "  random_trunc              0.888    +0.0120   +0.112     6.01e-03     *\n",
      "\n",
      "--- NDCG@5 ---\n",
      "  Condition                  Mean    vs Bare        d            p   sig\n",
      "  ----------------------------------------------------------------------\n",
      "  bare                      0.873         --       --           --    --\n",
      "  oracle_trunc              0.862    -0.0110   -0.121     3.50e-02    ns\n",
      "  surr_title_trunc          0.858    -0.0157   -0.152     6.63e-03     *\n",
      "  surr_doc_trunc            0.872    -0.0013   -0.018     9.29e-01    ns\n",
      "  surr_template_trunc       0.876    +0.0024   +0.030     4.87e-01    ns\n",
      "  random_trunc              0.875    +0.0019   +0.023     6.34e-01    ns\n",
      "\n",
      "--- AUC ---\n",
      "  Condition                  Mean    vs Bare        d            p   sig\n",
      "  ----------------------------------------------------------------------\n",
      "  bare                      0.709         --       --           --    --\n",
      "  oracle_trunc              0.699    -0.0098   -0.082     7.15e-02    ns\n",
      "  surr_title_trunc          0.696    -0.0131   -0.106     1.64e-01    ns\n",
      "  surr_doc_trunc            0.718    +0.0096   +0.093     6.58e-04    **\n",
      "  surr_template_trunc       0.724    +0.0151   +0.152     1.12e-03    **\n",
      "  random_trunc              0.717    +0.0086   +0.080     6.17e-02    ns\n",
      "\n",
      "--- MRR@3 ---\n",
      "  Condition                  Mean    vs Bare        d            p   sig\n",
      "  ----------------------------------------------------------------------\n",
      "  bare                      0.943         --       --           --    --\n",
      "  oracle_trunc              0.938    -0.0058   -0.035     4.32e-01    ns\n",
      "  surr_title_trunc          0.935    -0.0088   -0.049     4.56e-01    ns\n",
      "  surr_doc_trunc            0.938    -0.0054   -0.037     4.12e-01    ns\n",
      "  surr_template_trunc       0.941    -0.0025   -0.020     6.45e-01    ns\n",
      "  random_trunc              0.946    +0.0029   +0.025     7.05e-01    ns\n",
      "\n",
      "--- Hit@1 ---\n",
      "  Condition                  Mean    vs Bare        d            p   sig\n",
      "  ----------------------------------------------------------------------\n",
      "  bare                      0.915         --       --           --    --\n",
      "  oracle_trunc              0.900    -0.0150   -0.055     3.49e-01    ns\n",
      "  surr_title_trunc          0.897    -0.0175   -0.058     3.23e-01    ns\n",
      "  surr_doc_trunc            0.902    -0.0125   -0.050     3.96e-01    ns\n",
      "  surr_template_trunc       0.907    -0.0075   -0.033     5.85e-01    ns\n",
      "  random_trunc              0.920    +0.0050   +0.024     7.02e-01    ns\n",
      "\n",
      "======================================================================\n",
      "HEADLINE:\n",
      "  bare AUC = 0.709\n",
      "  oracle AUC = 0.699 (gain = -0.010)\n",
      "  surr_title AUC = 0.696 (gain = -0.013)\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Cell 10: Results table with statistical tests\n",
    "if not PRESCREEN_PASSED or len(metrics) == 0:\n",
    "    print(\"SKIPPED (pre-screen failed)\")\n",
    "    analysis = {}\n",
    "else:\n",
    "    from lib.analysis import cohens_d\n",
    "\n",
    "    print(\"=\" * 70)\n",
    "    print(\"RESULTS: ESCI Query-Likelihood Ranking (N=%d queries)\" % len(results))\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    METRIC_NAMES = ['ndcg1', 'ndcg3', 'ndcg5', 'auc', 'mrr3', 'hit1']\n",
    "    METRIC_LABELS = {'ndcg1': 'NDCG@1', 'ndcg3': 'NDCG@3', 'ndcg5': 'NDCG@5',\n",
    "                     'auc': 'AUC', 'mrr3': 'MRR@3', 'hit1': 'Hit@1'}\n",
    "\n",
    "    analysis = {}\n",
    "\n",
    "    for metric_name in METRIC_NAMES:\n",
    "        print(f\"\\n--- {METRIC_LABELS[metric_name]} ---\")\n",
    "        print(f\"  {'Condition':<22} {'Mean':>8} {'vs Bare':>10} {'d':>8} {'p':>12} {'sig':>5}\")\n",
    "        print(f\"  {'-'*70}\")\n",
    "\n",
    "        bare_vals = metrics['bare'][metric_name]\n",
    "        analysis[metric_name] = {}\n",
    "\n",
    "        for cond in COND_NAMES:\n",
    "            vals = metrics[cond][metric_name]\n",
    "            mean_val = vals.mean()\n",
    "\n",
    "            if cond == 'bare':\n",
    "                print(f\"  {cond:<22} {mean_val:>8.3f} {'--':>10} {'--':>8} {'--':>12} {'--':>5}\")\n",
    "                analysis[metric_name][cond] = {'mean': float(mean_val)}\n",
    "            else:\n",
    "                diff = vals - bare_vals\n",
    "                d = cohens_d(diff)\n",
    "\n",
    "                nonzero = diff[diff != 0]\n",
    "                if len(nonzero) >= 10:\n",
    "                    try:\n",
    "                        stat, p_val = wilcoxon(nonzero)\n",
    "                    except ValueError:\n",
    "                        p_val = 1.0\n",
    "                else:\n",
    "                    p_val = 1.0\n",
    "\n",
    "                sig = ('***' if p_val < 0.001/N_BONFERRONI else\n",
    "                       '**' if p_val < 0.01/N_BONFERRONI else\n",
    "                       '*' if p_val < 0.05/N_BONFERRONI else 'ns')\n",
    "\n",
    "                print(f\"  {cond:<22} {mean_val:>8.3f} {diff.mean():>+10.4f} \"\n",
    "                      f\"{d:>+8.3f} {p_val:>12.2e} {sig:>5}\")\n",
    "                analysis[metric_name][cond] = {\n",
    "                    'mean': float(mean_val), 'delta': float(diff.mean()),\n",
    "                    'd': float(d), 'p': float(p_val),\n",
    "                }\n",
    "\n",
    "    # Headline\n",
    "    oracle_auc = analysis['auc']['oracle_trunc']['mean']\n",
    "    bare_auc = analysis['auc']['bare']['mean']\n",
    "    title_auc = analysis['auc'].get('surr_title_trunc', {}).get('mean', 0)\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"HEADLINE:\")\n",
    "    print(f\"  bare AUC = {bare_auc:.3f}\")\n",
    "    print(f\"  oracle AUC = {oracle_auc:.3f} (gain = {oracle_auc - bare_auc:+.3f})\")\n",
    "    print(f\"  surr_title AUC = {title_auc:.3f} (gain = {title_auc - bare_auc:+.3f})\")\n",
    "    print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1de79dfb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T19:35:21.038835Z",
     "iopub.status.busy": "2026-02-18T19:35:21.038442Z",
     "iopub.status.idle": "2026-02-18T19:35:22.781153Z",
     "shell.execute_reply": "2026-02-18T19:35:22.780295Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "DIFFERENTIAL SIGNAL ANALYSIS\n",
      "======================================================================\n",
      "Does priming help RELEVANT products MORE than IRRELEVANT ones?\n",
      "\n",
      "  oracle_trunc:\n",
      "    delta_rel  (mean NLL drop for relevant):   +0.7597\n",
      "    delta_irrel (mean NLL drop for irrelevant): +1.0247\n",
      "    differential (rel - irrel):                 -0.2649  d=-0.269  p=1.00e+00  ns\n",
      "    % queries where relevant helped MORE:       37.0%\n",
      "\n",
      "  surr_title_trunc:\n",
      "    delta_rel  (mean NLL drop for relevant):   +0.3720\n",
      "    delta_irrel (mean NLL drop for irrelevant): +0.6171\n",
      "    differential (rel - irrel):                 -0.2451  d=-0.276  p=1.00e+00  ns\n",
      "    % queries where relevant helped MORE:       40.0%\n",
      "\n",
      "  surr_doc_trunc:\n",
      "    delta_rel  (mean NLL drop for relevant):   +0.6588\n",
      "    delta_irrel (mean NLL drop for irrelevant): +0.7140\n",
      "    differential (rel - irrel):                 -0.0552  d=-0.056  p=8.67e-01  ns\n",
      "    % queries where relevant helped MORE:       54.8%\n",
      "\n",
      "  surr_template_trunc:\n",
      "    delta_rel  (mean NLL drop for relevant):   +0.8336\n",
      "    delta_irrel (mean NLL drop for irrelevant): +0.8419\n",
      "    differential (rel - irrel):                 -0.0082  d=-0.010  p=9.78e-01  ns\n",
      "    % queries where relevant helped MORE:       55.2%\n",
      "\n",
      "  random_trunc:\n",
      "    delta_rel  (mean NLL drop for relevant):   +0.7853\n",
      "    delta_irrel (mean NLL drop for irrelevant): +0.8835\n",
      "    differential (rel - irrel):                 -0.0983  d=-0.120  p=8.63e-01  ns\n",
      "    % queries where relevant helped MORE:       47.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Plot saved to results/exp04b/differential_signal.png\n"
     ]
    }
   ],
   "source": [
    "# Cell 11: Differential signal analysis\n",
    "if not PRESCREEN_PASSED or len(results) == 0:\n",
    "    print(\"SKIPPED (pre-screen failed)\")\n",
    "    diff_analysis = {}\n",
    "else:\n",
    "    import matplotlib\n",
    "    matplotlib.use('Agg')\n",
    "    import matplotlib.pyplot as plt\n",
    "    from lib.analysis import cohens_d\n",
    "\n",
    "    print(\"=\" * 70)\n",
    "    print(\"DIFFERENTIAL SIGNAL ANALYSIS\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"Does priming help RELEVANT products MORE than IRRELEVANT ones?\")\n",
    "\n",
    "    diff_analysis = {}\n",
    "    for cond in COND_NAMES[1:]:\n",
    "        delta_rels = []\n",
    "        delta_irrels = []\n",
    "\n",
    "        for r in results:\n",
    "            bare_nlls = np.array(r['scores']['bare'])\n",
    "            cond_nlls = np.array(r['scores'][cond])\n",
    "            binary = r['binary_labels']\n",
    "\n",
    "            rel_mask = [i for i, b in enumerate(binary) if b == 1]\n",
    "            irrel_mask = [i for i, b in enumerate(binary) if b == 0]\n",
    "\n",
    "            if len(rel_mask) == 0 or len(irrel_mask) == 0:\n",
    "                continue\n",
    "\n",
    "            delta_rel = np.mean([bare_nlls[i] - cond_nlls[i] for i in rel_mask])\n",
    "            delta_irrel = np.mean([bare_nlls[i] - cond_nlls[i] for i in irrel_mask])\n",
    "\n",
    "            delta_rels.append(delta_rel)\n",
    "            delta_irrels.append(delta_irrel)\n",
    "\n",
    "        delta_rels = np.array(delta_rels)\n",
    "        delta_irrels = np.array(delta_irrels)\n",
    "        differential = delta_rels - delta_irrels\n",
    "\n",
    "        d = cohens_d(differential)\n",
    "        nonzero = differential[differential != 0]\n",
    "        if len(nonzero) >= 10:\n",
    "            try:\n",
    "                stat, p_val = wilcoxon(nonzero)\n",
    "                p_val_onesided = p_val / 2 if np.mean(differential) > 0 else 1 - p_val / 2\n",
    "            except ValueError:\n",
    "                p_val_onesided = 1.0\n",
    "        else:\n",
    "            p_val_onesided = 1.0\n",
    "\n",
    "        diff_analysis[cond] = {\n",
    "            'delta_rel_mean': float(delta_rels.mean()),\n",
    "            'delta_irrel_mean': float(delta_irrels.mean()),\n",
    "            'differential_mean': float(differential.mean()),\n",
    "            'd': float(d),\n",
    "            'p_onesided': float(p_val_onesided),\n",
    "            'pct_positive': float(100 * np.mean(differential > 0)),\n",
    "        }\n",
    "\n",
    "        sig = ('***' if p_val_onesided < 0.001/N_BONFERRONI else\n",
    "               '**' if p_val_onesided < 0.01/N_BONFERRONI else\n",
    "               '*' if p_val_onesided < 0.05/N_BONFERRONI else 'ns')\n",
    "\n",
    "        print(f\"\\n  {cond}:\")\n",
    "        print(f\"    delta_rel  (mean NLL drop for relevant):   {delta_rels.mean():+.4f}\")\n",
    "        print(f\"    delta_irrel (mean NLL drop for irrelevant): {delta_irrels.mean():+.4f}\")\n",
    "        print(f\"    differential (rel - irrel):                 {differential.mean():+.4f}  \"\n",
    "              f\"d={d:+.3f}  p={p_val_onesided:.2e}  {sig}\")\n",
    "        print(f\"    % queries where relevant helped MORE:       {100*np.mean(differential > 0):.1f}%\")\n",
    "\n",
    "    # Plot\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    for ax_idx, cond in enumerate(['oracle_trunc', 'surr_title_trunc', 'random_trunc']):\n",
    "        ax = axes[ax_idx]\n",
    "        delta_rels_plot = []\n",
    "        delta_irrels_plot = []\n",
    "        for r in results:\n",
    "            bare_nlls = np.array(r['scores']['bare'])\n",
    "            cond_nlls = np.array(r['scores'][cond])\n",
    "            binary = r['binary_labels']\n",
    "            rel_mask = [i for i, b in enumerate(binary) if b == 1]\n",
    "            irrel_mask = [i for i, b in enumerate(binary) if b == 0]\n",
    "            if len(rel_mask) == 0 or len(irrel_mask) == 0:\n",
    "                continue\n",
    "            delta_rels_plot.append(np.mean([bare_nlls[i] - cond_nlls[i] for i in rel_mask]))\n",
    "            delta_irrels_plot.append(np.mean([bare_nlls[i] - cond_nlls[i] for i in irrel_mask]))\n",
    "\n",
    "        ax.scatter(delta_irrels_plot, delta_rels_plot, alpha=0.3, s=10)\n",
    "        lims = [min(min(delta_irrels_plot), min(delta_rels_plot)) - 0.5,\n",
    "                max(max(delta_irrels_plot), max(delta_rels_plot)) + 0.5]\n",
    "        ax.plot(lims, lims, 'r--', alpha=0.5, label='equal help')\n",
    "        ax.set_xlabel('delta_irrelevant')\n",
    "        ax.set_ylabel('delta_relevant')\n",
    "        ax.set_title(f'{cond.replace(\"_trunc\", \"\")}')\n",
    "        ax.legend(fontsize=8)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.suptitle('ESCI Differential Signal: Points ABOVE red line = ranking improves', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plot_path = RESULTS_DIR / 'differential_signal.png'\n",
    "    plt.savefig(plot_path, dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f\"\\nPlot saved to {plot_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "13f478f9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T19:35:22.784804Z",
     "iopub.status.busy": "2026-02-18T19:35:22.784308Z",
     "iopub.status.idle": "2026-02-18T19:35:23.280564Z",
     "shell.execute_reply": "2026-02-18T19:35:23.279496Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "HARDNESS STRATIFICATION (by bare AUC quintiles)\n",
      "======================================================================\n",
      "\n",
      "Quintile boundaries (bare AUC): [0.51684982 0.68705197 0.8073132  0.93333333]\n",
      "  Q1 (hardest): N=80, mean bare AUC=0.347\n",
      "  Q2: N=80, mean bare AUC=0.601\n",
      "  Q3: N=80, mean bare AUC=0.746\n",
      "  Q4: N=90, mean bare AUC=0.877\n",
      "  Q5 (easiest): N=70, mean bare AUC=0.987\n",
      "\n",
      "--- AUC gain by hardness quintile ---\n",
      "  Quintile                 oracle     surr_title       surr_doc  surr_template         random\n",
      "  -------------------------------------------------------------------------------------------\n",
      "  Q1 (hardest)      +0.038 d=+0.27  +0.028 d=+0.21  +0.034 d=+0.28  +0.035 d=+0.26  +0.029 d=+0.22\n",
      "  Q2                +0.018 d=+0.19  +0.005 d=+0.04  +0.034 d=+0.44  +0.031 d=+0.29  +0.030 d=+0.24\n",
      "  Q3                -0.013 d=-0.11  -0.016 d=-0.12  +0.014 d=+0.11  +0.019 d=+0.20  +0.013 d=+0.13\n",
      "  Q4                -0.046 d=-0.38  -0.041 d=-0.36  -0.015 d=-0.16  +0.001 d=+0.01  -0.014 d=-0.15\n",
      "  Q5 (easiest)      -0.045 d=-0.49  -0.042 d=-0.45  -0.021 d=-0.29  -0.012 d=-0.20  -0.015 d=-0.23\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plot saved to results/exp04b/hardness_stratification.png\n"
     ]
    }
   ],
   "source": [
    "# Cell 12: Hardness stratification\n",
    "if not PRESCREEN_PASSED or len(metrics) == 0:\n",
    "    print(\"SKIPPED (pre-screen failed)\")\n",
    "    hardness_analysis = {}\n",
    "else:\n",
    "    import matplotlib\n",
    "    matplotlib.use('Agg')\n",
    "    import matplotlib.pyplot as plt\n",
    "    from lib.analysis import cohens_d\n",
    "\n",
    "    print(\"=\" * 70)\n",
    "    print(\"HARDNESS STRATIFICATION (by bare AUC quintiles)\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    bare_aucs = metrics['bare']['auc']\n",
    "    quintile_boundaries = np.percentile(bare_aucs, [20, 40, 60, 80])\n",
    "    quintile_labels = ['Q1 (hardest)', 'Q2', 'Q3', 'Q4', 'Q5 (easiest)']\n",
    "\n",
    "    def get_quintile(auc):\n",
    "        for q, bound in enumerate(quintile_boundaries):\n",
    "            if auc <= bound:\n",
    "                return q\n",
    "        return 4\n",
    "\n",
    "    quintile_assignments = np.array([get_quintile(a) for a in bare_aucs])\n",
    "\n",
    "    print(f\"\\nQuintile boundaries (bare AUC): {quintile_boundaries}\")\n",
    "    for q in range(5):\n",
    "        mask = quintile_assignments == q\n",
    "        print(f\"  {quintile_labels[q]}: N={mask.sum()}, mean bare AUC={bare_aucs[mask].mean():.3f}\")\n",
    "\n",
    "    # Per-quintile NDCG@3 and AUC gains\n",
    "    hardness_analysis = {}\n",
    "    print(f\"\\n--- AUC gain by hardness quintile ---\")\n",
    "    header = f\"  {'Quintile':<16}\"\n",
    "    for cond in COND_NAMES[1:]:\n",
    "        short = cond.replace('_trunc', '')\n",
    "        header += f\" {short:>14}\"\n",
    "    print(header)\n",
    "    print(f\"  {'-'*(16 + 15 * len(COND_NAMES[1:]))}\")\n",
    "\n",
    "    for q in range(5):\n",
    "        mask = quintile_assignments == q\n",
    "        row = f\"  {quintile_labels[q]:<16}\"\n",
    "        hardness_analysis[quintile_labels[q]] = {}\n",
    "\n",
    "        for cond in COND_NAMES[1:]:\n",
    "            cond_aucs = metrics[cond]['auc'][mask]\n",
    "            bare_q_aucs = bare_aucs[mask]\n",
    "            gain = (cond_aucs - bare_q_aucs).mean()\n",
    "            d = cohens_d(cond_aucs - bare_q_aucs) if mask.sum() > 1 else 0\n",
    "            row += f\" {gain:>+7.3f} d={d:>+5.2f}\"\n",
    "            hardness_analysis[quintile_labels[q]][cond] = {\n",
    "                'auc_gain': float(gain), 'd': float(d)}\n",
    "\n",
    "        print(row)\n",
    "\n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    for cond in ['oracle_trunc', 'surr_title_trunc', 'random_trunc']:\n",
    "        gains = [hardness_analysis[quintile_labels[q]].get(cond, {}).get('auc_gain', 0)\n",
    "                 for q in range(5)]\n",
    "        ax.plot(range(5), gains, '-o', label=cond.replace('_trunc', ''), markersize=8)\n",
    "\n",
    "    ax.set_xticks(range(5))\n",
    "    ax.set_xticklabels(quintile_labels, rotation=15)\n",
    "    ax.set_ylabel('AUC gain vs bare')\n",
    "    ax.set_title('ESCI Ranking Gain by Query Hardness')\n",
    "    ax.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plot_path = RESULTS_DIR / 'hardness_stratification.png'\n",
    "    plt.savefig(plot_path, dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f\"Plot saved to {plot_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c6cf703",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T19:35:23.284937Z",
     "iopub.status.busy": "2026-02-18T19:35:23.284372Z",
     "iopub.status.idle": "2026-02-18T19:35:23.304855Z",
     "shell.execute_reply": "2026-02-18T19:35:23.304140Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "VERDICT -- Exp 04B: Amazon ESCI Query-Likelihood Ranking\n",
      "======================================================================\n",
      "\n",
      "Model: google/t5gemma-2-4b-4b\n",
      "N queries: 400\n",
      "Pre-screen: PASSED\n",
      "Pre-screen bare AUC: 0.713\n",
      "\n",
      "--- Ranking metrics summary ---\n",
      "\n",
      "  NDCG@1:\n",
      "    bare:           0.896\n",
      "    oracle_trunc          : 0.887 (-0.0092, d=-0.037) ns\n",
      "    surr_title_trunc      : 0.873 (-0.0233, d=-0.084) ns\n",
      "    surr_doc_trunc        : 0.892 (-0.0042, d=-0.020) ns\n",
      "    surr_template_trunc   : 0.896 (+0.0000, d=+0.000) ns\n",
      "    random_trunc          : 0.908 (+0.0125, d=+0.067) ns\n",
      "\n",
      "  NDCG@3:\n",
      "    bare:           0.876\n",
      "    oracle_trunc          : 0.865 (-0.0106, d=-0.086) ns\n",
      "    surr_title_trunc      : 0.863 (-0.0126, d=-0.089) ns\n",
      "    surr_doc_trunc        : 0.880 (+0.0044, d=+0.042) ns\n",
      "    surr_template_trunc   : 0.881 (+0.0052, d=+0.048) ns\n",
      "    random_trunc          : 0.888 (+0.0120, d=+0.112) *\n",
      "\n",
      "  NDCG@5:\n",
      "    bare:           0.873\n",
      "    oracle_trunc          : 0.862 (-0.0110, d=-0.121) ns\n",
      "    surr_title_trunc      : 0.858 (-0.0157, d=-0.152) *\n",
      "    surr_doc_trunc        : 0.872 (-0.0013, d=-0.018) ns\n",
      "    surr_template_trunc   : 0.876 (+0.0024, d=+0.030) ns\n",
      "    random_trunc          : 0.875 (+0.0019, d=+0.023) ns\n",
      "\n",
      "  AUC:\n",
      "    bare:           0.709\n",
      "    oracle_trunc          : 0.699 (-0.0098, d=-0.082) ns\n",
      "    surr_title_trunc      : 0.696 (-0.0131, d=-0.106) ns\n",
      "    surr_doc_trunc        : 0.718 (+0.0096, d=+0.093) **\n",
      "    surr_template_trunc   : 0.724 (+0.0151, d=+0.152) **\n",
      "    random_trunc          : 0.717 (+0.0086, d=+0.080) ns\n",
      "\n",
      "  MRR@3:\n",
      "    bare:           0.943\n",
      "    oracle_trunc          : 0.938 (-0.0058, d=-0.035) ns\n",
      "    surr_title_trunc      : 0.935 (-0.0088, d=-0.049) ns\n",
      "    surr_doc_trunc        : 0.938 (-0.0054, d=-0.037) ns\n",
      "    surr_template_trunc   : 0.941 (-0.0025, d=-0.020) ns\n",
      "    random_trunc          : 0.946 (+0.0029, d=+0.025) ns\n",
      "\n",
      "  Hit@1:\n",
      "    bare:           0.915\n",
      "    oracle_trunc          : 0.900 (-0.0150, d=-0.055) ns\n",
      "    surr_title_trunc      : 0.897 (-0.0175, d=-0.058) ns\n",
      "    surr_doc_trunc        : 0.902 (-0.0125, d=-0.050) ns\n",
      "    surr_template_trunc   : 0.907 (-0.0075, d=-0.033) ns\n",
      "    random_trunc          : 0.920 (+0.0050, d=+0.024) ns\n",
      "\n",
      "--- Differential signal ---\n",
      "  oracle_trunc          : diff=-0.2649 d=-0.269 37% positive ns\n",
      "  surr_title_trunc      : diff=-0.2451 d=-0.276 40% positive ns\n",
      "  surr_doc_trunc        : diff=-0.0552 d=-0.056 55% positive ns\n",
      "  surr_template_trunc   : diff=-0.0082 d=-0.010 55% positive ns\n",
      "  random_trunc          : diff=-0.0983 d=-0.120 47% positive ns\n",
      "\n",
      "--- surr_title_trunc (the ad-serving surrogate) ---\n",
      "  AUC: 0.696 (0% of oracle gain)\n",
      "  NDCG@3: 0.863 (bare=0.876, oracle=0.865)\n",
      "\n",
      "--- OVERALL VERDICT ---\n",
      "  NO ranking signal on ESCI (same outcome as v2)\n",
      "  Query-likelihood may not be the right scoring approach\n",
      "\n",
      "======================================================================\n",
      "Results saved to results/exp04b/results.json\n"
     ]
    }
   ],
   "source": [
    "# Cell 13: Verdict and save results\n",
    "print(\"=\" * 70)\n",
    "print(\"VERDICT -- Exp 04B: Amazon ESCI Query-Likelihood Ranking\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\nModel: {MODEL_NAME}\")\n",
    "print(f\"N queries: {len(results) if results else 0}\")\n",
    "print(f\"Pre-screen: {'PASSED' if PRESCREEN_PASSED else 'FAILED'}\")\n",
    "print(f\"Pre-screen bare AUC: {np.mean(prescreen_aucs):.3f}\")\n",
    "\n",
    "if not PRESCREEN_PASSED:\n",
    "    print(f\"\\n--- EXPERIMENT ABORTED ---\")\n",
    "    print(f\"Query-likelihood scoring cannot discriminate relevant from irrelevant\")\n",
    "    print(f\"products on ESCI with bare encoding.\")\n",
    "    print(f\"This matches v2 Exp 31 finding (QL AUC near chance on MS MARCO).\")\n",
    "\n",
    "    final_results = {\n",
    "        'experiment': 'exp04b_esci_ranking',\n",
    "        'model': MODEL_NAME,\n",
    "        'status': 'ABORTED_PRESCREEN',\n",
    "        'prescreen_bare_auc': float(np.mean(prescreen_aucs)),\n",
    "        'prescreen_n': N_PRESCREEN,\n",
    "        'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    }\n",
    "else:\n",
    "    # Full results\n",
    "    print(f\"\\n--- Ranking metrics summary ---\")\n",
    "    for metric_name in METRIC_NAMES:\n",
    "        print(f\"\\n  {METRIC_LABELS[metric_name]}:\")\n",
    "        bare_val = analysis[metric_name]['bare']['mean']\n",
    "        print(f\"    bare:           {bare_val:.3f}\")\n",
    "        for cond in COND_NAMES[1:]:\n",
    "            a = analysis[metric_name].get(cond, {})\n",
    "            mean = a.get('mean', 0)\n",
    "            delta = a.get('delta', 0)\n",
    "            d = a.get('d', 0)\n",
    "            p = a.get('p', 1)\n",
    "            sig = ('***' if p < 0.001/N_BONFERRONI else '**' if p < 0.01/N_BONFERRONI\n",
    "                   else '*' if p < 0.05/N_BONFERRONI else 'ns')\n",
    "            print(f\"    {cond:<22s}: {mean:.3f} ({delta:+.4f}, d={d:+.3f}) {sig}\")\n",
    "\n",
    "    # Differential verdict\n",
    "    print(f\"\\n--- Differential signal ---\")\n",
    "    for cond in COND_NAMES[1:]:\n",
    "        da = diff_analysis.get(cond, {})\n",
    "        diff_mean = da.get('differential_mean', 0)\n",
    "        d = da.get('d', 0)\n",
    "        p = da.get('p_onesided', 1)\n",
    "        pct = da.get('pct_positive', 0)\n",
    "        sig = ('***' if p < 0.001/N_BONFERRONI else '**' if p < 0.01/N_BONFERRONI\n",
    "               else '*' if p < 0.05/N_BONFERRONI else 'ns')\n",
    "        print(f\"  {cond:<22s}: diff={diff_mean:+.4f} d={d:+.3f} {pct:.0f}% positive {sig}\")\n",
    "\n",
    "    # surr_title spotlight\n",
    "    print(f\"\\n--- surr_title_trunc (the ad-serving surrogate) ---\")\n",
    "    title_auc = analysis['auc'].get('surr_title_trunc', {}).get('mean', 0)\n",
    "    bare_auc = analysis['auc']['bare']['mean']\n",
    "    oracle_auc = analysis['auc']['oracle_trunc']['mean']\n",
    "    if oracle_auc > bare_auc:\n",
    "        title_ratio = (title_auc - bare_auc) / (oracle_auc - bare_auc) * 100\n",
    "    else:\n",
    "        title_ratio = 0\n",
    "    print(f\"  AUC: {title_auc:.3f} ({title_ratio:.0f}% of oracle gain)\")\n",
    "    title_ndcg3 = analysis['ndcg3'].get('surr_title_trunc', {}).get('mean', 0)\n",
    "    bare_ndcg3 = analysis['ndcg3']['bare']['mean']\n",
    "    oracle_ndcg3 = analysis['ndcg3']['oracle_trunc']['mean']\n",
    "    print(f\"  NDCG@3: {title_ndcg3:.3f} (bare={bare_ndcg3:.3f}, oracle={oracle_ndcg3:.3f})\")\n",
    "\n",
    "    # Overall verdict\n",
    "    print(f\"\\n--- OVERALL VERDICT ---\")\n",
    "    oracle_auc_p = analysis['auc'].get('oracle_trunc', {}).get('p', 1)\n",
    "    oracle_auc_d = analysis['auc'].get('oracle_trunc', {}).get('d', 0)\n",
    "    if oracle_auc_p < 0.05/N_BONFERRONI and oracle_auc_d > 0:\n",
    "        print(f\"  RANKING SIGNAL DETECTED on ESCI\")\n",
    "        title_p = analysis['auc'].get('surr_title_trunc', {}).get('p', 1)\n",
    "        if title_p < 0.05/N_BONFERRONI:\n",
    "            print(f\"  surr_title_trunc also significant -- product title is a viable surrogate!\")\n",
    "            print(f\"  >>> This validates the ad-serving use case\")\n",
    "        else:\n",
    "            print(f\"  surr_title_trunc is NS -- need oracle query for ranking benefit\")\n",
    "    else:\n",
    "        print(f\"  NO ranking signal on ESCI (same outcome as v2)\")\n",
    "        print(f\"  Query-likelihood may not be the right scoring approach\")\n",
    "\n",
    "    final_results = {\n",
    "        'experiment': 'exp04b_esci_ranking',\n",
    "        'model': MODEL_NAME,\n",
    "        'status': 'COMPLETED',\n",
    "        'n_queries': len(results),\n",
    "        'n_bonferroni': N_BONFERRONI,\n",
    "        'prescreen_bare_auc': float(np.mean(prescreen_aucs)),\n",
    "        'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'analysis': analysis,\n",
    "        'diff_analysis': diff_analysis,\n",
    "        'hardness_analysis': hardness_analysis,\n",
    "        'pool_stats': {\n",
    "            'mean_products_per_query': float(np.mean([r['n_products'] for r in results])),\n",
    "            'total_products': int(sum(r['n_products'] for r in results)),\n",
    "        },\n",
    "    }\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "\n",
    "with open(RESULTS_DIR / 'results.json', 'w') as f:\n",
    "    json.dump(final_results, f, indent=2)\n",
    "print(f\"Results saved to {RESULTS_DIR / 'results.json'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9efc4618",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T19:35:23.308071Z",
     "iopub.status.busy": "2026-02-18T19:35:23.307599Z",
     "iopub.status.idle": "2026-02-18T19:35:23.857800Z",
     "shell.execute_reply": "2026-02-18T19:35:23.856790Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning up GPU memory...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory: 15.03 GB -> 0.01 GB\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Cell 14: Cleanup\n",
    "print(\"Cleaning up GPU memory...\")\n",
    "mem_before = torch.cuda.memory_allocated() / 1e9\n",
    "del model, processor, tokenizer\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "mem_after = torch.cuda.memory_allocated() / 1e9\n",
    "print(f\"GPU memory: {mem_before:.2f} GB -> {mem_after:.2f} GB\")\n",
    "print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0606f97d5ae04075ad7a986ecc373113": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "0cb67c5d96fc40a8b8be950f1c7c72a4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "0e4ce851da234a6498e1cd2f5647310f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_27b856823505496bb005c613e727835c",
       "max": 400.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_e3925f5761f6482ebae42685fc841485",
       "tabbable": null,
       "tooltip": null,
       "value": 400.0
      }
     },
     "10aab241f53e42aebb7d5019eba1bb68": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "15544581f3014275a992c780d486aee9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "227748a898b94649ba1f359b488eb355": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_99ca69e7052c4bb4a61cfdd2121c6777",
       "placeholder": "​",
       "style": "IPY_MODEL_7da32a12704f446f927c072c0107165d",
       "tabbable": null,
       "tooltip": null,
       "value": "Filtering: 100%"
      }
     },
     "25ca93c9c8c2475c8e52b4ddb688a9d9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_fd14e16b8f3e40819ac4c591cd33e968",
       "placeholder": "​",
       "style": "IPY_MODEL_10aab241f53e42aebb7d5019eba1bb68",
       "tabbable": null,
       "tooltip": null,
       "value": "Loading weights: 100%"
      }
     },
     "27b856823505496bb005c613e727835c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2db126b62a2248c48d69439576aa002b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "2ec6f894a601462181fb8d6743079d6d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "31e61d335fe0419aa12e784accf1591e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "33de32b800924b23bfbf90bce5608a13": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3c1833d37f7145f9adfea7674f571b62": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_9cb92525bbbd41d897590851dd2a35f6",
       "placeholder": "​",
       "style": "IPY_MODEL_cdee6daacfd94a21b01ee3581439a0f2",
       "tabbable": null,
       "tooltip": null,
       "value": " 2027874/2027874 [05:12&lt;00:00, 7261.94it/s]"
      }
     },
     "3ce7849811f84fa997f35e7eb9769d4c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_f5ae01745b6b42e994bc48662b44acd5",
       "max": 20.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_0606f97d5ae04075ad7a986ecc373113",
       "tabbable": null,
       "tooltip": null,
       "value": 20.0
      }
     },
     "401b7e01c8d3422fb7031c10ace76b11": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_e75720d28ad649baa83fdf6840b596b7",
       "max": 1327.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_2db126b62a2248c48d69439576aa002b",
       "tabbable": null,
       "tooltip": null,
       "value": 1327.0
      }
     },
     "4fe531a6034044368e518cc5b4a43077": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "5a66a5ab32004e32a427aaa43318b0be": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "5cadae4947404068b1f947a9eda989e5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_d181b3afcba24b0d8067d508324449db",
        "IPY_MODEL_0e4ce851da234a6498e1cd2f5647310f",
        "IPY_MODEL_7bc680cbb2644847abbe80250c64df12"
       ],
       "layout": "IPY_MODEL_0cb67c5d96fc40a8b8be950f1c7c72a4",
       "tabbable": null,
       "tooltip": null
      }
     },
     "60fe55a43dbb40fa9ff1969d0a3bc007": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "704e236855364d07b5e871658ee12f76": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_a5212499966d4d519650c6d35454fb3b",
        "IPY_MODEL_3ce7849811f84fa997f35e7eb9769d4c",
        "IPY_MODEL_a7076b830acb464faf0c672a3db49b70"
       ],
       "layout": "IPY_MODEL_db3e3f357c5d4deb8d9cb647db8dcabc",
       "tabbable": null,
       "tooltip": null
      }
     },
     "7bc680cbb2644847abbe80250c64df12": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_2ec6f894a601462181fb8d6743079d6d",
       "placeholder": "​",
       "style": "IPY_MODEL_ab591a42d2f3406aac66b904b1355a5b",
       "tabbable": null,
       "tooltip": null,
       "value": " 400/400 [3:14:29&lt;00:00, 33.99s/it]"
      }
     },
     "7da32a12704f446f927c072c0107165d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "8a30f7056bb94bdfaf6234749925461a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_25ca93c9c8c2475c8e52b4ddb688a9d9",
        "IPY_MODEL_401b7e01c8d3422fb7031c10ace76b11",
        "IPY_MODEL_eb6f151671c845488c5035b1996f904c"
       ],
       "layout": "IPY_MODEL_31e61d335fe0419aa12e784accf1591e",
       "tabbable": null,
       "tooltip": null
      }
     },
     "910927343eb843b4ab03dd200e7caa2e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "99ca69e7052c4bb4a61cfdd2121c6777": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "9cb92525bbbd41d897590851dd2a35f6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a2991c63a6fd44b8a1a6eaf127acd4ba": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_33de32b800924b23bfbf90bce5608a13",
       "max": 2027874.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_15544581f3014275a992c780d486aee9",
       "tabbable": null,
       "tooltip": null,
       "value": 2027874.0
      }
     },
     "a5212499966d4d519650c6d35454fb3b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_baf5c786967a4b4c8a12d194dff62e41",
       "placeholder": "​",
       "style": "IPY_MODEL_c1d7e9a159b04d949c34541b6f5b40e0",
       "tabbable": null,
       "tooltip": null,
       "value": "Pre-screen: 100%"
      }
     },
     "a57faa46525f4f9c9d953649f788c243": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "a7076b830acb464faf0c672a3db49b70": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_f599259f771c4b2c8b6867e3a02def97",
       "placeholder": "​",
       "style": "IPY_MODEL_a57faa46525f4f9c9d953649f788c243",
       "tabbable": null,
       "tooltip": null,
       "value": " 20/20 [01:41&lt;00:00,  3.86s/it]"
      }
     },
     "ab591a42d2f3406aac66b904b1355a5b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "baf5c786967a4b4c8a12d194dff62e41": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c1d7e9a159b04d949c34541b6f5b40e0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "c597566bee62439ab61c6a432295d2cd": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c6c90bd2d715409890067a85b57251dc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_227748a898b94649ba1f359b488eb355",
        "IPY_MODEL_a2991c63a6fd44b8a1a6eaf127acd4ba",
        "IPY_MODEL_3c1833d37f7145f9adfea7674f571b62"
       ],
       "layout": "IPY_MODEL_910927343eb843b4ab03dd200e7caa2e",
       "tabbable": null,
       "tooltip": null
      }
     },
     "cdee6daacfd94a21b01ee3581439a0f2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "d181b3afcba24b0d8067d508324449db": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_c597566bee62439ab61c6a432295d2cd",
       "placeholder": "​",
       "style": "IPY_MODEL_4fe531a6034044368e518cc5b4a43077",
       "tabbable": null,
       "tooltip": null,
       "value": "Queries: 100%"
      }
     },
     "db3e3f357c5d4deb8d9cb647db8dcabc": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e3925f5761f6482ebae42685fc841485": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "e75720d28ad649baa83fdf6840b596b7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "eb6f151671c845488c5035b1996f904c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_60fe55a43dbb40fa9ff1969d0a3bc007",
       "placeholder": "​",
       "style": "IPY_MODEL_5a66a5ab32004e32a427aaa43318b0be",
       "tabbable": null,
       "tooltip": null,
       "value": " 1327/1327 [00:58&lt;00:00, 134.98it/s, Materializing param=model.encoder.vision_tower.vision_model.post_layernorm.weight]"
      }
     },
     "f599259f771c4b2c8b6867e3a02def97": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f5ae01745b6b42e994bc48662b44acd5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "fd14e16b8f3e40819ac4c591cd33e968": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
