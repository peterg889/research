{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 03: Length Scaling -- Does the Benefit Survive Longer Documents?",
    "## The #1 limitation from v2: value contamination diluted at ~200 tokens. Does T5Gemma's bidirectional encoder change this?",
    "",
    "### Background",
    "Exp 01 showed that bidirectional co-encoding genuinely improves document representations:",
    "- oracle_trunc d=+0.408 (94% win, p=3e-08)",
    "- surr_doc_trunc d=+0.363 (89% of oracle benefit)",
    "- surr_para_trunc d=+0.357 (87% of oracle benefit)",
    "",
    "But those passages were short (~130 tokens). In v2 Exp 20 (decoder-only Gemma 3 4B),",
    "the benefit collapsed as documents got longer:",
    "",
    "| Length | v2 Exp 20 oracle d |",
    "|--------|-------------------|",
    "| ~130 tok | +0.303*** |",
    "| 256 tok | +0.114 (ns) |",
    "| 512 tok | +0.034 (ns) |",
    "| 1024 tok | -0.043 (ns) |",
    "",
    "### Why T5Gemma should be different",
    "In decoder-only models, the surrogate's influence propagates only forward through causal",
    "attention — by ~200 tokens downstream, it's diluted to noise. T5Gemma's bidirectional",
    "encoder lets every document token attend to the surrogate in BOTH directions via global",
    "self-attention. The surrogate influence should be distributed more uniformly.",
    "",
    "### Method",
    "Take the same short MS MARCO passages and pad them to controlled lengths with unrelated",
    "MS MARCO text (separated by `\\n\\n`). Same questions, same answers, same passages —",
    "only the document length changes. This is a within-subject design for maximum statistical power.",
    "",
    "### Conditions (all with truncation, per Exp 01)",
    "| Condition | Encoder input | Decoder cross-attends to |",
    "|-----------|---------------|--------------------------|",
    "| bare | [document] | all (= document) |",
    "| oracle_trunc | [query + doc] | document only |",
    "| surr_doc_trunc | [doc_kw + doc] | document only |",
    "| surr_para_trunc | [para + doc] | document only |",
    "",
    "### Length bins",
    "| Bin | Target tokens | v2 Exp 20 result |",
    "|-----|--------------|------------------|",
    "| original | ~130 tok (no padding) | d=+0.303*** |",
    "| 256 | padded | d=+0.114 (ns) |",
    "| 384 | padded (new, near v2 cliff) | not tested in v2 |",
    "| 512 | padded | d=+0.034 (ns) |",
    "| 1024 | padded | d=-0.043 (ns) |",
    "| 2048 | padded (new) | not tested in v2 |",
    "",
    "### Design: N=400 for power at longer lengths",
    "N=200 with Bonferroni can only detect d>=0.25. The decay tail (d~0.15 at 512+) is where",
    "the interesting science is. N=400 gives power to detect d>=0.15 with Bonferroni correction.",
    "",
    "### Success criteria",
    "- If d stays significant at 512+ tokens: bidirectional encoder is qualitatively better than causal",
    "- If decay is gradual (not a cliff): surrogate influence distributed more uniformly",
    "- Critical threshold: 512 tokens (where ESCI product descriptions live)",
    ""
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 2: Setup\n",
    "import os\n",
    "os.umask(0o000)\n",
    "\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "import gc\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from scipy import stats\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "sys.path.insert(0, \"../..\")\n",
    "\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "RESULTS_DIR = Path(\"../../results/exp03\")\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CHECKPOINT_PATH = RESULTS_DIR / \"checkpoint.json\"\n",
    "\n",
    "N_SAMPLES = 400\n",
    "MODEL_NAME = \"google/t5gemma-2-4b-4b\"\n",
    "LENGTH_BINS = [\"original\", \"256\", \"384\", \"512\", \"1024\", \"2048\"]\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv())\n",
    "HF_TOKEN = os.environ.get(\"HF_TOKEN\")\n",
    "\n",
    "print(f\"Exp 03: Length Scaling -- Does the Benefit Survive Longer Documents?\")\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"N: {N_SAMPLES}\")\n",
    "print(f\"Length bins: {LENGTH_BINS}\")\n",
    "print(f\"CUDA: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 3: Load model\n",
    "from transformers import AutoProcessor, AutoModelForSeq2SeqLM\n",
    "\n",
    "print(f\"Loading {MODEL_NAME}...\")\n",
    "processor = AutoProcessor.from_pretrained(MODEL_NAME, token=HF_TOKEN)\n",
    "tokenizer = processor.tokenizer\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    token=HF_TOKEN,\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "DEVICE = next(model.parameters()).device\n",
    "print(f\"Model loaded. dtype={next(model.parameters()).dtype}\")\n",
    "print(f\"GPU memory used: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 4: Scoring helpers (reused from Exp 01)\n",
    "\n",
    "def score_nll(encoder_text, answer_text, prefix_token_count=0, truncate=False):\n",
    "    '''Score NLL of answer tokens with optional truncation.\n",
    "\n",
    "    Args:\n",
    "        encoder_text: Full text for encoder (e.g., \"[query]\\n[document]\")\n",
    "        answer_text: Answer text for decoder (NO query in decoder)\n",
    "        prefix_token_count: Number of prefix tokens (query/surrogate) to potentially mask\n",
    "        truncate: If True, mask prefix tokens from decoder cross-attention\n",
    "    '''\n",
    "    # Tokenize encoder input\n",
    "    enc_ids = tokenizer(encoder_text, return_tensors=\"pt\",\n",
    "                        add_special_tokens=True, truncation=True,\n",
    "                        max_length=4096).input_ids.to(DEVICE)\n",
    "    total_enc_len = enc_ids.shape[1]\n",
    "\n",
    "    # Full mask for encoder (bidirectional, sees everything)\n",
    "    enc_mask = torch.ones(1, total_enc_len, device=DEVICE, dtype=torch.long)\n",
    "\n",
    "    # Run encoder with full bidirectional attention\n",
    "    with torch.no_grad():\n",
    "        encoder_outputs = model.get_encoder()(\n",
    "            input_ids=enc_ids, attention_mask=enc_mask\n",
    "        )\n",
    "\n",
    "    # Build cross-attention mask for decoder\n",
    "    if truncate and prefix_token_count > 0:\n",
    "        # Mask prefix tokens: decoder can only attend to document positions\n",
    "        cross_attn_mask = torch.ones(1, total_enc_len, device=DEVICE, dtype=torch.long)\n",
    "        cross_attn_mask[:, :prefix_token_count] = 0\n",
    "    else:\n",
    "        # Full cross-attention (decoder sees all encoder tokens)\n",
    "        cross_attn_mask = torch.ones(1, total_enc_len, device=DEVICE, dtype=torch.long)\n",
    "\n",
    "    # Tokenize answer for decoder\n",
    "    ans_ids = tokenizer(answer_text, return_tensors=\"pt\",\n",
    "                        add_special_tokens=False, truncation=True,\n",
    "                        max_length=256).input_ids.to(DEVICE)\n",
    "\n",
    "    if ans_ids.shape[1] == 0:\n",
    "        return 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            encoder_outputs=encoder_outputs,\n",
    "            attention_mask=cross_attn_mask,  # This controls decoder cross-attention\n",
    "            labels=ans_ids,\n",
    "        )\n",
    "\n",
    "    # Per-token NLL\n",
    "    logits = outputs.logits\n",
    "    log_probs = F.log_softmax(logits, dim=-1)\n",
    "    token_log_probs = log_probs[0].gather(1, ans_ids[0].unsqueeze(1)).squeeze(1)\n",
    "    mean_nll = -token_log_probs.mean().item()\n",
    "\n",
    "    del encoder_outputs, outputs, logits, log_probs\n",
    "    return mean_nll\n",
    "\n",
    "\n",
    "def count_prefix_tokens(prefix_text, document_text):\n",
    "    '''Count how many tokens the prefix occupies in the concatenated encoding.'''\n",
    "    full_text = prefix_text + \"\\n\" + document_text\n",
    "    full_ids = tokenizer(full_text, add_special_tokens=True).input_ids\n",
    "    doc_ids = tokenizer(document_text, add_special_tokens=True).input_ids\n",
    "    return len(full_ids) - len(doc_ids)\n",
    "\n",
    "\n",
    "# === Surrogate generation (from Exp 01) ===\n",
    "STOP_WORDS = {\n",
    "    'a', 'an', 'the', 'is', 'are', 'was', 'were', 'be', 'been', 'being',\n",
    "    'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could',\n",
    "    'should', 'may', 'might', 'can', 'shall', 'to', 'of', 'in', 'for',\n",
    "    'on', 'with', 'at', 'by', 'from', 'as', 'into', 'through', 'during',\n",
    "    'before', 'after', 'above', 'below', 'between', 'and', 'but', 'or',\n",
    "    'not', 'no', 'if', 'then', 'than', 'so', 'up', 'out', 'about',\n",
    "    'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those',\n",
    "    'it', 'its', 'i', 'me', 'my', 'we', 'our', 'you', 'your', 'he',\n",
    "    'him', 'his', 'she', 'her', 'they', 'them', 'their', 'how', 'when',\n",
    "    'where', 'why', 'much', 'many', 'some', 'any', 'all', 'each',\n",
    "    'does', 'also', 'just', 'more', 'most', 'very', 'too', 'only',\n",
    "}\n",
    "\n",
    "def extract_keywords(text):\n",
    "    words = re.sub(r'[^\\w\\s]', '', text.lower()).split()\n",
    "    return [w for w in words if w not in STOP_WORDS and len(w) > 2]\n",
    "\n",
    "def make_surrogate_paraphrase(query):\n",
    "    keywords = extract_keywords(query)\n",
    "    return \" \".join(keywords[::-1]) if keywords else query\n",
    "\n",
    "def make_surrogate_from_doc(passage):\n",
    "    content_words = extract_keywords(passage)\n",
    "    if not content_words:\n",
    "        return \"information\"\n",
    "    counts = Counter(content_words)\n",
    "    return \" \".join(w for w, _ in counts.most_common(5))\n",
    "\n",
    "print(\"Helpers defined.\")\n",
    "print(\"  score_nll(encoder_text, answer, prefix_token_count, truncate)\")\n",
    "print(\"  NOTE: max_length=4096 (up from 2048 in Exp 01) for longer padded documents\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 5: Load data and build padding pool\n",
    "from lib.data import count_words\n",
    "from datasets import load_dataset\n",
    "\n",
    "print(\"Loading MS MARCO v1.1 validation...\")\n",
    "ds = load_dataset(\"microsoft/ms_marco\", \"v1.1\", split=\"validation\")\n",
    "\n",
    "# Collect target samples (short passages with answers)\n",
    "samples = []\n",
    "# Collect unrelated passages for padding pool\n",
    "padding_pool = []\n",
    "\n",
    "for item in ds:\n",
    "    passages = item.get('passages', {})\n",
    "    ptexts = passages.get('passage_text', [])\n",
    "    is_sel = passages.get('is_selected', [])\n",
    "    query = item.get('query', '')\n",
    "    answers = item.get('answers', [])\n",
    "    well_formed = item.get('wellFormedAnswers', [])\n",
    "\n",
    "    answer = None\n",
    "    if well_formed and len(well_formed) > 0 and well_formed[0] not in ('[]', ''):\n",
    "        answer = well_formed[0]\n",
    "    elif answers and len(answers) > 0 and answers[0] != 'No Answer Present.':\n",
    "        answer = answers[0]\n",
    "\n",
    "    for pt, sel in zip(ptexts, is_sel):\n",
    "        wc = count_words(pt)\n",
    "        if sel == 1 and 30 <= wc <= 300 and answer:\n",
    "            if len(samples) < N_SAMPLES * 3:\n",
    "                samples.append({\n",
    "                    'passage': pt, 'query': query, 'answer': answer,\n",
    "                    'word_count': wc\n",
    "                })\n",
    "        elif sel == 0 and 20 <= wc <= 200:\n",
    "            # Non-selected passages make good padding material\n",
    "            padding_pool.append(pt)\n",
    "\n",
    "    if len(samples) >= N_SAMPLES * 3 and len(padding_pool) >= 5000:\n",
    "        break\n",
    "\n",
    "np.random.seed(SEED)\n",
    "np.random.shuffle(samples)\n",
    "samples = samples[:N_SAMPLES]\n",
    "\n",
    "# Shuffle padding pool so concatenation order is random\n",
    "np.random.shuffle(padding_pool)\n",
    "\n",
    "del ds\n",
    "gc.collect()\n",
    "\n",
    "# Generate surrogates\n",
    "for s in samples:\n",
    "    s['surrogate_para'] = make_surrogate_paraphrase(s['query'])\n",
    "    s['surrogate_doc_kw'] = make_surrogate_from_doc(s['passage'])\n",
    "\n",
    "print(f\"Selected {len(samples)} target samples, mean words={np.mean([s['word_count'] for s in samples]):.0f}\")\n",
    "print(f\"Padding pool: {len(padding_pool)} unrelated passages\")\n",
    "\n",
    "# Show token counts for target passages\n",
    "target_tok_counts = []\n",
    "for s in samples:\n",
    "    toks = tokenizer(s['passage'], add_special_tokens=True).input_ids\n",
    "    target_tok_counts.append(len(toks))\n",
    "print(f\"Target passage tokens: mean={np.mean(target_tok_counts):.0f}, \"\n",
    "      f\"median={np.median(target_tok_counts):.0f}, \"\n",
    "      f\"min={np.min(target_tok_counts)}, max={np.max(target_tok_counts)}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 6: Build padded documents at each length bin\n",
    "print(\"=\" * 70)\n",
    "print(\"BUILDING PADDED DOCUMENTS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "TARGET_LENGTHS = {\n",
    "    \"original\": None,  # No padding\n",
    "    \"256\": 256,\n",
    "    \"384\": 384,\n",
    "    \"512\": 512,\n",
    "    \"1024\": 1024,\n",
    "    \"2048\": 2048,\n",
    "}\n",
    "\n",
    "def pad_passage_to_length(passage, target_tokens, padding_pool, pool_offset):\n",
    "    '''Pad a passage to target_tokens by appending unrelated passages.\n",
    "\n",
    "    Args:\n",
    "        passage: Original passage text\n",
    "        target_tokens: Target token count (None = no padding)\n",
    "        padding_pool: List of unrelated passages\n",
    "        pool_offset: Starting index into padding pool (for reproducibility)\n",
    "\n",
    "    Returns:\n",
    "        (padded_text, actual_token_count, n_padding_passages_used)\n",
    "    '''\n",
    "    if target_tokens is None:\n",
    "        toks = tokenizer(passage, add_special_tokens=True).input_ids\n",
    "        return passage, len(toks), 0\n",
    "\n",
    "    # Check if passage already exceeds target\n",
    "    current_ids = tokenizer(passage, add_special_tokens=True).input_ids\n",
    "    if len(current_ids) >= target_tokens:\n",
    "        return passage, len(current_ids), 0\n",
    "\n",
    "    # Append padding passages until we reach target length\n",
    "    padded = passage\n",
    "    n_used = 0\n",
    "    idx = pool_offset\n",
    "\n",
    "    while True:\n",
    "        if idx >= len(padding_pool):\n",
    "            idx = 0  # Wrap around if needed\n",
    "        candidate = padded + \"\\n\\n\" + padding_pool[idx]\n",
    "        candidate_ids = tokenizer(candidate, add_special_tokens=True).input_ids\n",
    "        if len(candidate_ids) >= target_tokens:\n",
    "            # This passage would push us over — truncate it to fit\n",
    "            # Add words from this padding passage one at a time\n",
    "            pad_words = padding_pool[idx].split()\n",
    "            for w_end in range(1, len(pad_words) + 1):\n",
    "                partial = padded + \"\\n\\n\" + \" \".join(pad_words[:w_end])\n",
    "                partial_ids = tokenizer(partial, add_special_tokens=True).input_ids\n",
    "                if len(partial_ids) >= target_tokens:\n",
    "                    padded = partial\n",
    "                    break\n",
    "            else:\n",
    "                padded = candidate\n",
    "            n_used += 1\n",
    "            break\n",
    "        padded = candidate\n",
    "        n_used += 1\n",
    "        idx += 1\n",
    "\n",
    "    final_ids = tokenizer(padded, add_special_tokens=True).input_ids\n",
    "    return padded, len(final_ids), n_used\n",
    "\n",
    "\n",
    "# Build padded versions for each sample at each length\n",
    "padded_docs = {}  # {length_bin: [padded_passage_text, ...]}\n",
    "padded_stats = {}  # {length_bin: {mean_tokens, min_tokens, max_tokens}}\n",
    "\n",
    "for length_bin, target_tokens in TARGET_LENGTHS.items():\n",
    "    padded_docs[length_bin] = []\n",
    "    tok_counts = []\n",
    "\n",
    "    for i, s in enumerate(samples):\n",
    "        # Use different pool offset per sample for diversity\n",
    "        pool_offset = i * 50  # Spread across pool\n",
    "        padded_text, actual_tokens, n_pad = pad_passage_to_length(\n",
    "            s['passage'], target_tokens, padding_pool, pool_offset\n",
    "        )\n",
    "        padded_docs[length_bin].append(padded_text)\n",
    "        tok_counts.append(actual_tokens)\n",
    "\n",
    "    padded_stats[length_bin] = {\n",
    "        'mean': np.mean(tok_counts),\n",
    "        'min': int(np.min(tok_counts)),\n",
    "        'max': int(np.max(tok_counts)),\n",
    "        'median': np.median(tok_counts),\n",
    "    }\n",
    "\n",
    "    print(f\"\\n  {length_bin:>8s}: mean={padded_stats[length_bin]['mean']:.0f} tokens \"\n",
    "          f\"(min={padded_stats[length_bin]['min']}, max={padded_stats[length_bin]['max']}, \"\n",
    "          f\"median={padded_stats[length_bin]['median']:.0f})\")\n",
    "\n",
    "# Verify: show first sample at each length\n",
    "print(f\"\\n--- Sample 0 preview ---\")\n",
    "print(f\"  Query:  {samples[0]['query'][:80]}\")\n",
    "print(f\"  Answer: {samples[0]['answer'][:80]}\")\n",
    "for lb in LENGTH_BINS:\n",
    "    preview = padded_docs[lb][0]\n",
    "    tok_count = len(tokenizer(preview, add_special_tokens=True).input_ids)\n",
    "    print(f\"  {lb:>8s}: {tok_count} tokens, starts='{preview[:60]}...', ends='...{preview[-40:]}'\")\n",
    "\n",
    "# Show actual surrogate prefix text for each condition\n",
    "print(f\"\\n--- Actual prefix text for each condition (sample 0) ---\")\n",
    "surrogates = {\n",
    "    'oracle (real query)': samples[0]['query'],\n",
    "    'surr_para (reversed kw)': samples[0]['surrogate_para'],\n",
    "    'surr_doc (top-5 doc kw)': samples[0]['surrogate_doc_kw'],\n",
    "}\n",
    "for name, text in surrogates.items():\n",
    "    ptoks = count_prefix_tokens(text, padded_docs[\"original\"][0])\n",
    "    print(f\"  {name:<25} ({ptoks:>3} prefix toks): {text[:70]}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 7: Explain conditions\n",
    "print(\"=\" * 70)\n",
    "print(\"EXPERIMENTAL CONDITIONS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "ex = samples[0]\n",
    "ex_doc = padded_docs[\"original\"][0]\n",
    "\n",
    "# Count prefix tokens for the example\n",
    "oracle_prefix = count_prefix_tokens(ex['query'], ex_doc)\n",
    "para_prefix = count_prefix_tokens(ex['surrogate_para'], ex_doc)\n",
    "doc_prefix = count_prefix_tokens(ex['surrogate_doc_kw'], ex_doc)\n",
    "\n",
    "conditions_explained = f'''\n",
    "CONDITIONS (all with truncation, per Exp 01 findings):\n",
    "\n",
    "  CONDITION          ENCODER INPUT              DECODER CROSS-ATTENDS TO    PREFIX TOKENS\n",
    "  ---------------------------------------------------------------------------------\n",
    "  bare               [document]                 all (= document)            0\n",
    "  oracle_trunc       [query + doc]              document ONLY               ~{oracle_prefix} (masked)\n",
    "  surr_doc_trunc     [doc_kw + doc]             document ONLY               ~{doc_prefix} (masked)\n",
    "  surr_para_trunc    [para + doc]               document ONLY               ~{para_prefix} (masked)\n",
    "\n",
    "LENGTH BINS: {LENGTH_BINS}\n",
    "\n",
    "DESIGN:\n",
    "  - Same {N_SAMPLES} samples at every length (within-subject, matched design)\n",
    "  - Outer loop = length bin (all samples at one length before moving to next)\n",
    "  - Padding: unrelated MS MARCO passages appended after target passage\n",
    "  - Question and answer unchanged — only document length varies\n",
    "  - Total scoring calls: {N_SAMPLES} samples x {len(LENGTH_BINS)} lengths x 4 conditions = {N_SAMPLES * len(LENGTH_BINS) * 4}\n",
    "'''\n",
    "print(conditions_explained)\n",
    "\n",
    "# Estimate runtime\n",
    "print(f\"Estimated runtime: ~{N_SAMPLES * len(LENGTH_BINS) * 4 * 0.5 / 60:.0f} min \"\n",
    "      f\"(assuming ~0.5s per scoring call)\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 8: Run scoring — outer loop over length bins\n",
    "print(\"=\" * 70)\n",
    "print(\"RUNNING EXPERIMENT\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "COND_NAMES = ['bare', 'oracle_trunc', 'surr_doc_trunc', 'surr_para_trunc']\n",
    "\n",
    "def make_conditions(sample, padded_passage):\n",
    "    '''Return dict of {name: (encoder_text, prefix_token_count, truncate)}'''\n",
    "    query = sample['query']\n",
    "    para = sample['surrogate_para']\n",
    "    doc_kw = sample['surrogate_doc_kw']\n",
    "\n",
    "    # Count prefix tokens for each condition (using padded passage)\n",
    "    oracle_prefix = count_prefix_tokens(query, padded_passage)\n",
    "    para_prefix = count_prefix_tokens(para, padded_passage)\n",
    "    doc_prefix = count_prefix_tokens(doc_kw, padded_passage)\n",
    "\n",
    "    return {\n",
    "        'bare':            (padded_passage,                        0,              False),\n",
    "        'oracle_trunc':    (query + \"\\n\" + padded_passage,         oracle_prefix,  True),\n",
    "        'surr_doc_trunc':  (doc_kw + \"\\n\" + padded_passage,        doc_prefix,     True),\n",
    "        'surr_para_trunc': (para + \"\\n\" + padded_passage,          para_prefix,    True),\n",
    "    }\n",
    "\n",
    "# Resume from checkpoint\n",
    "# Checkpoint format: {length_bin: {\"results\": [...], \"completed\": N}, ...}\n",
    "all_checkpoint = {}\n",
    "if CHECKPOINT_PATH.exists():\n",
    "    saved = json.loads(CHECKPOINT_PATH.read_text())\n",
    "    if saved.get('n_total') == N_SAMPLES:\n",
    "        all_checkpoint = saved.get('bins', {})\n",
    "        summary = ', '.join(f'{k}={len(v.get(\"results\",[]))}' for k,v in all_checkpoint.items())\n",
    "        print(f\"Loaded checkpoint: {summary}\")\n",
    "\n",
    "t0_total = time.time()\n",
    "\n",
    "for length_bin in LENGTH_BINS:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"LENGTH BIN: {length_bin} (target={TARGET_LENGTHS[length_bin] or 'no padding'})\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "    # Check for existing results for this bin\n",
    "    bin_results = []\n",
    "    start_idx = 0\n",
    "    if length_bin in all_checkpoint:\n",
    "        bin_data = all_checkpoint[length_bin]\n",
    "        saved_results = bin_data.get('results', [])\n",
    "        # Verify alignment\n",
    "        saved_queries = [r['query'][:50] for r in saved_results]\n",
    "        current_queries = [s['query'][:50] for s in samples[:len(saved_queries)]]\n",
    "        if saved_queries == current_queries:\n",
    "            bin_results = saved_results\n",
    "            start_idx = len(bin_results)\n",
    "            print(f\"  Resuming from sample {start_idx}/{N_SAMPLES}\")\n",
    "\n",
    "    if start_idx >= N_SAMPLES:\n",
    "        print(f\"  Already complete ({len(bin_results)} results)\")\n",
    "        all_checkpoint[length_bin] = {\"results\": bin_results, \"completed\": N_SAMPLES}\n",
    "        continue\n",
    "\n",
    "    t0_bin = time.time()\n",
    "\n",
    "    for i in tqdm(range(start_idx, N_SAMPLES), initial=start_idx, total=N_SAMPLES,\n",
    "                  desc=f\"  {length_bin}\"):\n",
    "        s = samples[i]\n",
    "        padded_passage = padded_docs[length_bin][i]\n",
    "        conditions = make_conditions(s, padded_passage)\n",
    "\n",
    "        result = {\n",
    "            'query': s['query'],\n",
    "            'answer': s['answer'],\n",
    "            'passage_words': s['word_count'],\n",
    "            'padded_tokens': len(tokenizer(padded_passage, add_special_tokens=True).input_ids),\n",
    "        }\n",
    "\n",
    "        for cond_name in COND_NAMES:\n",
    "            enc_text, prefix_count, trunc = conditions[cond_name]\n",
    "            nll = score_nll(enc_text, s['answer'], prefix_count, trunc)\n",
    "            result[f'nll_{cond_name}'] = nll\n",
    "\n",
    "        bin_results.append(result)\n",
    "\n",
    "        if (i + 1) % 20 == 0 or i == N_SAMPLES - 1:\n",
    "            all_checkpoint[length_bin] = {\"results\": bin_results, \"completed\": len(bin_results)}\n",
    "            ckpt = {\n",
    "                'n_total': N_SAMPLES,\n",
    "                'bins': all_checkpoint,\n",
    "                'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            }\n",
    "            CHECKPOINT_PATH.write_text(json.dumps(ckpt))\n",
    "            elapsed_bin = time.time() - t0_bin\n",
    "            done = i - start_idx + 1\n",
    "            eta = (N_SAMPLES - i - 1) * elapsed_bin / done if done > 0 else 0\n",
    "            tqdm.write(f\"    Checkpoint {i+1}/{N_SAMPLES} | {elapsed_bin/60:.1f}m | ETA {eta/60:.1f}m\")\n",
    "\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    elapsed_bin = time.time() - t0_bin\n",
    "    print(f\"  {length_bin} complete: {len(bin_results)} samples in {elapsed_bin/60:.1f} min\")\n",
    "\n",
    "    # Quick peek at this bin's results\n",
    "    bare_nlls = np.array([r['nll_bare'] for r in bin_results])\n",
    "    oracle_nlls = np.array([r['nll_oracle_trunc'] for r in bin_results])\n",
    "    diff = bare_nlls - oracle_nlls\n",
    "    from lib.analysis import cohens_d\n",
    "    d = cohens_d(diff)\n",
    "    win = 100 * np.mean(diff > 0)\n",
    "    print(f\"  Quick peek: oracle_trunc d={d:+.3f}, win={win:.0f}%\")\n",
    "\n",
    "elapsed_total = time.time() - t0_total\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"ALL BINS COMPLETE: {elapsed_total/60:.1f} min total\")\n",
    "print(f\"{'='*70}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 9: Results — per-length table\n",
    "from lib.analysis import cohens_d\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"RESULTS: Per-Length Condition Comparison\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Collect results from checkpoint\n",
    "results_by_bin = {}\n",
    "for length_bin in LENGTH_BINS:\n",
    "    results_by_bin[length_bin] = all_checkpoint[length_bin]['results']\n",
    "\n",
    "# Full results table\n",
    "analysis = {}\n",
    "for length_bin in LENGTH_BINS:\n",
    "    bin_results = results_by_bin[length_bin]\n",
    "    n = len(bin_results)\n",
    "    bare_nlls = np.array([r['nll_bare'] for r in bin_results])\n",
    "    mean_tokens = np.mean([r['padded_tokens'] for r in bin_results])\n",
    "\n",
    "    print(f\"\\n--- {length_bin} (mean {mean_tokens:.0f} tokens, N={n}) ---\")\n",
    "    print(f\"  {'Condition':<20} {'Mean NLL':>10} {'vs Bare':>10} {'d':>8} {'Win%':>8} {'p':>12} {'sig':>5}\")\n",
    "    print(f\"  {'-'*78}\")\n",
    "\n",
    "    analysis[length_bin] = {}\n",
    "    for cond in COND_NAMES:\n",
    "        nlls = np.array([r[f'nll_{cond}'] for r in bin_results])\n",
    "        mean_nll = nlls.mean()\n",
    "        diff = bare_nlls - nlls\n",
    "\n",
    "        if cond == 'bare':\n",
    "            print(f\"  {cond:<20} {mean_nll:>10.4f} {'--':>10} {'--':>8} {'--':>8} {'--':>12} {'--':>5}\")\n",
    "            analysis[length_bin][cond] = {'mean_nll': float(mean_nll)}\n",
    "        else:\n",
    "            d = cohens_d(diff)\n",
    "            win_pct = 100 * np.mean(diff > 0)\n",
    "            t_stat, p_val = stats.ttest_1samp(diff, 0)\n",
    "            # Bonferroni: 3 conditions x 6 lengths = 18 comparisons\n",
    "            sig = '***' if p_val < 0.001/18 else '**' if p_val < 0.01/18 else '*' if p_val < 0.05/18 else 'ns'\n",
    "            print(f\"  {cond:<20} {mean_nll:>10.4f} {diff.mean():>+10.4f} {d:>+8.3f} {win_pct:>7.1f}% {p_val:>12.2e} {sig:>5}\")\n",
    "            analysis[length_bin][cond] = {\n",
    "                'mean_nll': float(mean_nll), 'delta': float(diff.mean()),\n",
    "                'd': float(d), 'win_pct': float(win_pct), 'p': float(p_val),\n",
    "            }"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 10: Decay curve analysis\n",
    "print(\"=\" * 70)\n",
    "print(\"DECAY CURVE ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Collect d values across lengths for each condition\n",
    "print(f\"\\n--- Cohen's d vs Length ---\")\n",
    "print(f\"  {'Length':<10} {'oracle_trunc':>14} {'surr_doc':>14} {'surr_para':>14} {'mean_tokens':>12}\")\n",
    "print(f\"  {'-'*68}\")\n",
    "\n",
    "# v2 Exp 20 results for comparison\n",
    "v2_oracle_d = {\n",
    "    \"original\": 0.303,\n",
    "    \"256\": 0.114,\n",
    "    \"384\": None,   # Not tested in v2\n",
    "    \"512\": 0.034,\n",
    "    \"1024\": -0.043,\n",
    "    \"2048\": None,  # Not tested in v2\n",
    "}\n",
    "\n",
    "decay_data = {'length_bin': [], 'mean_tokens': []}\n",
    "for cond in ['oracle_trunc', 'surr_doc_trunc', 'surr_para_trunc']:\n",
    "    decay_data[f'd_{cond}'] = []\n",
    "    decay_data[f'p_{cond}'] = []\n",
    "\n",
    "for length_bin in LENGTH_BINS:\n",
    "    bin_results = results_by_bin[length_bin]\n",
    "    mean_tokens = np.mean([r['padded_tokens'] for r in bin_results])\n",
    "    decay_data['length_bin'].append(length_bin)\n",
    "    decay_data['mean_tokens'].append(mean_tokens)\n",
    "\n",
    "    d_vals = []\n",
    "    for cond in ['oracle_trunc', 'surr_doc_trunc', 'surr_para_trunc']:\n",
    "        a = analysis[length_bin].get(cond, {})\n",
    "        d = a.get('d', 0)\n",
    "        p = a.get('p', 1)\n",
    "        decay_data[f'd_{cond}'].append(d)\n",
    "        decay_data[f'p_{cond}'].append(p)\n",
    "        d_vals.append(f\"{d:+.3f}\")\n",
    "\n",
    "    v2_d = v2_oracle_d.get(length_bin)\n",
    "    v2_str = f\"{v2_d:+.3f}\" if v2_d is not None else \"  N/A\"\n",
    "    print(f\"  {length_bin:<10} {d_vals[0]:>14} {d_vals[1]:>14} {d_vals[2]:>14} {mean_tokens:>11.0f}\")\n",
    "\n",
    "# Compare to v2 decay\n",
    "print(f\"\\n--- v3 (T5Gemma) vs v2 (Gemma 3 4B) Oracle Decay ---\")\n",
    "print(f\"  {'Length':<10} {'v3 oracle_trunc':>16} {'v2 oracle (Exp20)':>18} {'v3/v2 ratio':>12}\")\n",
    "print(f\"  {'-'*60}\")\n",
    "\n",
    "for length_bin in LENGTH_BINS:\n",
    "    v3_d = analysis[length_bin].get('oracle_trunc', {}).get('d', 0)\n",
    "    v2_d = v2_oracle_d.get(length_bin)\n",
    "    if v2_d is not None and v2_d != 0:\n",
    "        ratio = v3_d / v2_d\n",
    "        print(f\"  {length_bin:<10} {v3_d:>+16.3f} {v2_d:>+18.3f} {ratio:>11.1f}x\")\n",
    "    elif v2_d is not None:\n",
    "        print(f\"  {length_bin:<10} {v3_d:>+16.3f} {v2_d:>+18.3f} {'--':>12}\")\n",
    "    else:\n",
    "        print(f\"  {length_bin:<10} {v3_d:>+16.3f} {'N/A':>18} {'--':>12}\")\n",
    "\n",
    "# Decay rate analysis\n",
    "print(f\"\\n--- Decay Rate ---\")\n",
    "orig_d_oracle = analysis[\"original\"].get('oracle_trunc', {}).get('d', 0)\n",
    "for cond in ['oracle_trunc', 'surr_doc_trunc', 'surr_para_trunc']:\n",
    "    orig_d = analysis[\"original\"].get(cond, {}).get('d', 0)\n",
    "    if orig_d == 0:\n",
    "        continue\n",
    "    print(f\"\\n  {cond}:\")\n",
    "    for length_bin in LENGTH_BINS:\n",
    "        d = analysis[length_bin].get(cond, {}).get('d', 0)\n",
    "        retention = d / orig_d * 100 if orig_d > 0 else 0\n",
    "        sig = analysis[length_bin].get(cond, {}).get('p', 1)\n",
    "        sig_str = '***' if sig < 0.001/18 else '**' if sig < 0.01/18 else '*' if sig < 0.05/18 else 'ns'\n",
    "        print(f\"    {length_bin:>8s}: d={d:+.3f} ({retention:5.1f}% of original) {sig_str}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 11: Decay curve plot\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Map length bins to numeric x values for plotting\n",
    "x_tokens = decay_data['mean_tokens']\n",
    "\n",
    "# --- Left panel: v3 decay curves for all conditions ---\n",
    "ax = axes[0]\n",
    "for cond, color, marker in [('oracle_trunc', 'tab:red', 'o'),\n",
    "                              ('surr_doc_trunc', 'tab:blue', 's'),\n",
    "                              ('surr_para_trunc', 'tab:green', '^')]:\n",
    "    d_vals = decay_data[f'd_{cond}']\n",
    "    p_vals = decay_data[f'p_{cond}']\n",
    "    ax.plot(x_tokens, d_vals, f'-{marker}', color=color, label=cond, markersize=8)\n",
    "    # Mark significant points\n",
    "    for x, d, p in zip(x_tokens, d_vals, p_vals):\n",
    "        if p < 0.05 / 15:  # Bonferroni\n",
    "            ax.annotate('*', (x, d), textcoords=\"offset points\",\n",
    "                       xytext=(0, 8), ha='center', fontsize=14, color=color)\n",
    "\n",
    "ax.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "ax.set_xlabel('Document Length (tokens)')\n",
    "ax.set_ylabel(\"Cohen's d (vs bare)\")\n",
    "ax.set_title('v3 T5Gemma: Effect Size vs Document Length')\n",
    "ax.legend()\n",
    "ax.set_xscale('log', base=2)\n",
    "ax.set_xticks(x_tokens)\n",
    "ax.set_xticklabels(LENGTH_BINS, rotation=45)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# --- Right panel: v3 oracle vs v2 oracle ---\n",
    "ax = axes[1]\n",
    "v3_oracle_d = decay_data['d_oracle_trunc']\n",
    "ax.plot(x_tokens, v3_oracle_d, '-o', color='tab:red', label='v3 T5Gemma (oracle_trunc)', markersize=8)\n",
    "\n",
    "# v2 data points (only where available)\n",
    "v2_lengths = []\n",
    "v2_d_vals = []\n",
    "for lb, tok in zip(LENGTH_BINS, x_tokens):\n",
    "    v2_d = v2_oracle_d.get(lb)\n",
    "    if v2_d is not None:\n",
    "        v2_lengths.append(tok)\n",
    "        v2_d_vals.append(v2_d)\n",
    "\n",
    "ax.plot(v2_lengths, v2_d_vals, '-s', color='tab:purple', label='v2 Gemma 3 4B (oracle)', markersize=8)\n",
    "\n",
    "ax.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "ax.set_xlabel('Document Length (tokens)')\n",
    "ax.set_ylabel(\"Cohen's d (vs bare)\")\n",
    "ax.set_title('Cross-Architecture Comparison: Decay Curves')\n",
    "ax.legend()\n",
    "ax.set_xscale('log', base=2)\n",
    "ax.set_xticks(x_tokens)\n",
    "ax.set_xticklabels(LENGTH_BINS, rotation=45)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plot_path = RESULTS_DIR / 'decay_curves.png'\n",
    "plt.savefig(plot_path, dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"Plot saved to {plot_path}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 12: Verdict and save\n",
    "print(\"=\" * 70)\n",
    "print(\"VERDICT -- Exp 03: Length Scaling\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\nModel: {MODEL_NAME}\")\n",
    "print(f\"N: {N_SAMPLES} samples per length bin\")\n",
    "print(f\"Length bins: {LENGTH_BINS}\")\n",
    "\n",
    "# Key question: at what length does the benefit become non-significant?\n",
    "print(f\"\\n--- Oracle Decay Summary ---\")\n",
    "orig_d = analysis[\"original\"].get('oracle_trunc', {}).get('d', 0)\n",
    "last_sig_bin = \"original\"\n",
    "for length_bin in LENGTH_BINS:\n",
    "    d = analysis[length_bin].get('oracle_trunc', {}).get('d', 0)\n",
    "    p = analysis[length_bin].get('oracle_trunc', {}).get('p', 1)\n",
    "    sig = p < 0.05 / 15  # Bonferroni\n",
    "    if sig:\n",
    "        last_sig_bin = length_bin\n",
    "    retention = d / orig_d * 100 if orig_d > 0 else 0\n",
    "    sig_str = \"SIGNIFICANT\" if sig else \"ns\"\n",
    "    print(f\"  {length_bin:>8s}: d={d:+.3f} ({retention:5.1f}% retained) [{sig_str}]\")\n",
    "\n",
    "print(f\"\\n  Last significant length bin: {last_sig_bin}\")\n",
    "\n",
    "# Compare architectures\n",
    "print(f\"\\n--- Cross-Architecture Verdict ---\")\n",
    "# v2 cliff was at ~200 tokens (256 bin was already ns)\n",
    "v2_cliff = \"~200 tokens (256 bin ns)\"\n",
    "\n",
    "# Determine v3 pattern\n",
    "v3_pattern = []\n",
    "for lb in LENGTH_BINS:\n",
    "    d = analysis[lb].get('oracle_trunc', {}).get('d', 0)\n",
    "    p = analysis[lb].get('oracle_trunc', {}).get('p', 1)\n",
    "    v3_pattern.append((lb, d, p < 0.05/18))\n",
    "\n",
    "sig_bins = [lb for lb, d, sig in v3_pattern if sig]\n",
    "if len(sig_bins) >= 4:\n",
    "    print(f\"  T5Gemma shows ROBUST length scaling — benefit persists to {sig_bins[-1]} tokens\")\n",
    "    print(f\"  This is qualitatively different from v2's cliff at {v2_cliff}\")\n",
    "    print(f\"  Bidirectional attention distributes surrogate influence across document length\")\n",
    "elif len(sig_bins) >= 3:\n",
    "    print(f\"  T5Gemma shows MODERATE length scaling — benefit persists to {sig_bins[-1]} tokens\")\n",
    "    print(f\"  Better than v2's cliff at {v2_cliff}, but still decays\")\n",
    "elif len(sig_bins) >= 2:\n",
    "    print(f\"  T5Gemma shows LIMITED improvement — benefit extends to {sig_bins[-1]} tokens\")\n",
    "    print(f\"  Somewhat better than v2's cliff at {v2_cliff}\")\n",
    "else:\n",
    "    print(f\"  T5Gemma shows NO improvement over v2 — benefit only at original length\")\n",
    "    print(f\"  Bidirectional attention does NOT help with length scaling\")\n",
    "\n",
    "# Surrogate comparison across lengths\n",
    "print(f\"\\n--- Surrogate Performance Across Lengths ---\")\n",
    "print(f\"  {'Length':<10} {'oracle':>10} {'doc_kw':>10} {'para':>10} {'doc_kw/oracle':>14} {'para/oracle':>12}\")\n",
    "print(f\"  {'-'*60}\")\n",
    "for lb in LENGTH_BINS:\n",
    "    od = analysis[lb].get('oracle_trunc', {}).get('d', 0)\n",
    "    dd = analysis[lb].get('surr_doc_trunc', {}).get('d', 0)\n",
    "    pd = analysis[lb].get('surr_para_trunc', {}).get('d', 0)\n",
    "    dr = dd / od * 100 if od > 0 else 0\n",
    "    pr = pd / od * 100 if od > 0 else 0\n",
    "    print(f\"  {lb:<10} {od:>+10.3f} {dd:>+10.3f} {pd:>+10.3f} {dr:>13.0f}% {pr:>11.0f}%\")\n",
    "\n",
    "# Implications for Exp 04\n",
    "print(f\"\\n--- Implications for Exp 04 (Ranking) ---\")\n",
    "d_512 = analysis.get(\"512\", {}).get('oracle_trunc', {}).get('d', 0)\n",
    "p_512 = analysis.get(\"512\", {}).get('oracle_trunc', {}).get('p', 1)\n",
    "if p_512 < 0.05:\n",
    "    print(f\"  512-token d={d_512:+.3f} (p={p_512:.2e}) — ESCI product descriptions ({chr(126)}100-500 words) are in range\")\n",
    "    print(f\"  Proceed with Exp 04\")\n",
    "else:\n",
    "    print(f\"  512-token d={d_512:+.3f} (ns) — ESCI product descriptions may be too long\")\n",
    "    print(f\"  Consider filtering to shorter product texts in Exp 04\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "\n",
    "# Save\n",
    "final_results = {\n",
    "    'experiment': 'exp03_length_scaling',\n",
    "    'model': MODEL_NAME,\n",
    "    'n_samples': N_SAMPLES,\n",
    "    'length_bins': LENGTH_BINS,\n",
    "    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'analysis': analysis,\n",
    "    'decay_data': decay_data,\n",
    "    'padded_stats': padded_stats,\n",
    "    'v2_oracle_d': v2_oracle_d,\n",
    "}\n",
    "\n",
    "with open(RESULTS_DIR / 'results.json', 'w') as f:\n",
    "    json.dump(final_results, f, indent=2)\n",
    "print(f\"\\nResults saved to {RESULTS_DIR / 'results.json'}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 13: Cleanup\n",
    "print(\"Cleaning up GPU memory...\")\n",
    "mem_before = torch.cuda.memory_allocated() / 1e9\n",
    "del model, processor, tokenizer\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "mem_after = torch.cuda.memory_allocated() / 1e9\n",
    "print(f\"GPU memory: {mem_before:.2f} GB -> {mem_after:.2f} GB\")\n",
    "print(\"Done!\")"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
