{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a03ee256",
   "metadata": {},
   "source": [
    "# Experiment 3D: Cross-Dataset Content Ablation (Long Queries)\n",
    "\n",
    "## Motivation\n",
    "\n",
    "Exp 2B on MS MARCO found that **85% of the oracle headroom is pure structure** —\n",
    "any prefix triggers a mode shift in the encoder. But MS MARCO queries are only\n",
    "~6 words long. For such short queries:\n",
    "- **Scrambled oracle** preserves bag-of-words, which IS most of the semantics\n",
    "- **Vocabulary test** (random → scrambled) is weak because 6 random words cover\n",
    "  a lot of vocabulary space relative to a 6-word query\n",
    "- **Word order** barely matters in 6 words\n",
    "\n",
    "This experiment tests the same decomposition on a dataset with **3x longer queries**\n",
    "(~18 words) and **5x longer documents** (~600 words). If the split changes, the\n",
    "MS MARCO finding was an artifact of short queries. If it holds, the structural\n",
    "mechanism is genuine.\n",
    "\n",
    "## Three-Point Framework\n",
    "\n",
    "| Label | Encoder input | Cost | Role |\n",
    "|-------|--------------|------|------|\n",
    "| **Upper bound** | [real_query + document], mask query from decoder | O(Q×D) | Ideal but too expensive |\n",
    "| **Lower bound** | [document] only | O(1) | Current worst case |\n",
    "| **Middle ground** | [surrogate + document], mask surrogate from decoder | O(1) | Our hypothesis |\n",
    "\n",
    "## Dataset: neural-bridge/rag-dataset-12000\n",
    "\n",
    "- Synthetic QA pairs with retrieved context passages\n",
    "- Filtered to: query ≥ 15 words, answer ≥ 5 words → ~3,384 samples\n",
    "- Mean query: 17.7 words (3x MS MARCO), document: 603 words, answer: 42.7 words\n",
    "- Ceiling pre-screen: PASS (mean bare NLL = 0.98, strong oracle headroom +0.255)\n",
    "\n",
    "## Design\n",
    "\n",
    "**Conditions** (all with truncation):\n",
    "1. `bare` — document only (lower bound)\n",
    "2. `oracle_trunc` — real query + document (upper bound)\n",
    "3. `random_matched_trunc` — N random words (N = query word count)\n",
    "4. `scrambled_oracle_trunc` — query words in random order\n",
    "5. `surr_template_trunc` — \"What is [top_keyword]?\"\n",
    "6. `repeat_the_trunc` — \"the\" repeated N times (N = query word count)\n",
    "7. `repeat_kw_trunc` — top doc keyword repeated N times\n",
    "\n",
    "**Content ablation decomposition**:\n",
    "- Structure = bare → random_matched\n",
    "- Vocabulary = random_matched → scrambled_oracle\n",
    "- Semantics = scrambled_oracle → oracle\n",
    "\n",
    "**Key prediction**: With 18-word queries, scrambling should lose more information\n",
    "than with 6-word queries. We expect the semantic component to grow from 10% to\n",
    "potentially 20-30%+."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4e6c1d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T01:08:31.526995Z",
     "iopub.status.busy": "2026-02-18T01:08:31.526458Z",
     "iopub.status.idle": "2026-02-18T01:08:33.648495Z",
     "shell.execute_reply": "2026-02-18T01:08:33.647751Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exp 3D: Cross-Dataset Content Ablation (Long Queries)\n",
      "Dataset: neural-bridge/rag-dataset-12000\n",
      "N: 500\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Setup\n",
    "import os\n",
    "os.umask(0o000)\n",
    "\n",
    "import sys, json, time, re, gc, random as pyrandom\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from scipy import stats\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "sys.path.insert(0, \"../..\")\n",
    "from lib.analysis import cohens_d\n",
    "\n",
    "SEED = 42\n",
    "N_SAMPLES = 500\n",
    "MODEL_NAME = \"google/t5gemma-2-4b-4b\"\n",
    "\n",
    "RESULTS_DIR = Path(\"../../results/exp03d\")\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CHECKPOINT_PATH = RESULTS_DIR / \"checkpoint.json\"\n",
    "\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "pyrandom.seed(SEED)\n",
    "\n",
    "print(\"Exp 3D: Cross-Dataset Content Ablation (Long Queries)\")\n",
    "print(f\"Dataset: neural-bridge/rag-dataset-12000\")\n",
    "print(f\"N: {N_SAMPLES}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "239b8234",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T01:08:33.651963Z",
     "iopub.status.busy": "2026-02-18T01:08:33.651425Z",
     "iopub.status.idle": "2026-02-18T01:08:35.742318Z",
     "shell.execute_reply": "2026-02-18T01:08:35.741638Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 9600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidates (q>=15w, a>=5w): 3384\n",
      "\n",
      "Sample statistics (N=500):\n",
      "  Query length:  mean=17.8, median=17, range=[15, 37]\n",
      "  Doc length:    mean=604.2, median=591, range=[115, 1192]\n",
      "  Answer length: mean=42.9, median=32, range=[6, 209]\n",
      "\n",
      "Example 1:\n",
      "  Q (21w): What are some of the conditions that require a reduction in lumber or connector plate design values according to ANSI/TP...\n",
      "  D (530w): Designing for Damp Conditions\n",
      "Designing for Damp Conditions\n",
      "a reduction in lumber or plate design pr...\n",
      "  A (42w): Conditions that require a reduction in lumber or connector plate design values according to ANSI/TPI...\n",
      "\n",
      "Example 2:\n",
      "  Q (17w): What does Severide discover about his old home and how does he help the family living there?...\n",
      "  D (528w): ‘Chicago Fire’: Sylvie Brett and Matt Casey Continue Their Drama and Severide Gets Sentimental\n",
      "Chica...\n",
      "  A (107w): Severide discovers that his old home has been hit by a fire, injuring a mother and her son, Dylan. T...\n",
      "\n",
      "Example 3:\n",
      "  Q (15w): What are some of the activities and offerings at Finley's Irish Pub in Largo, Florida?...\n",
      "  D (475w): Due to a hockey tournament nearby, our team and families were in the area. We were looking for a pla...\n",
      "  A (65w): Finley's Irish Pub in Largo, Florida offers a variety of activities and food options. They serve dis...\n",
      "\n",
      "--- Comparison with MS MARCO ---\n",
      "  MS MARCO: mean query = 6.0w, mean doc = ~60w, mean answer = ~20w\n",
      "  This dataset: mean query = 17.8w, mean doc = 604w, mean answer = 43w\n",
      "  Query ratio: 3.0x longer\n",
      "  Doc ratio: 10.1x longer\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 3: Load neural-bridge/rag-dataset-12000 and prepare samples\n",
    "from datasets import load_dataset\n",
    "\n",
    "print(\"Loading dataset...\")\n",
    "ds = load_dataset(\"neural-bridge/rag-dataset-12000\", split=\"train\")\n",
    "print(f\"Total samples: {len(ds)}\")\n",
    "\n",
    "# Filter to long queries with real answers\n",
    "all_candidates = []\n",
    "for row in ds:\n",
    "    q = row.get(\"question\", \"\")\n",
    "    doc = row.get(\"context\", \"\")\n",
    "    answer = row.get(\"answer\", \"\")\n",
    "    if not q or not doc or not answer:\n",
    "        continue\n",
    "    q_words = len(q.split())\n",
    "    a_words = len(answer.split())\n",
    "    if q_words >= 15 and a_words >= 5:\n",
    "        all_candidates.append({\n",
    "            \"query\": q,\n",
    "            \"document\": doc,\n",
    "            \"answer\": answer,\n",
    "            \"query_words\": q_words,\n",
    "            \"doc_words\": len(doc.split()),\n",
    "            \"answer_words\": a_words,\n",
    "        })\n",
    "\n",
    "print(f\"Candidates (q>=15w, a>=5w): {len(all_candidates)}\")\n",
    "\n",
    "# Shuffle and take N_SAMPLES\n",
    "np.random.seed(SEED)\n",
    "indices = np.random.permutation(len(all_candidates))\n",
    "samples = [all_candidates[i] for i in indices[:N_SAMPLES]]\n",
    "\n",
    "# Dataset statistics\n",
    "q_lens = np.array([s[\"query_words\"] for s in samples])\n",
    "d_lens = np.array([s[\"doc_words\"] for s in samples])\n",
    "a_lens = np.array([s[\"answer_words\"] for s in samples])\n",
    "\n",
    "print(f\"\\nSample statistics (N={N_SAMPLES}):\")\n",
    "print(f\"  Query length:  mean={q_lens.mean():.1f}, median={np.median(q_lens):.0f}, \"\n",
    "      f\"range=[{q_lens.min()}, {q_lens.max()}]\")\n",
    "print(f\"  Doc length:    mean={d_lens.mean():.1f}, median={np.median(d_lens):.0f}, \"\n",
    "      f\"range=[{d_lens.min()}, {d_lens.max()}]\")\n",
    "print(f\"  Answer length: mean={a_lens.mean():.1f}, median={np.median(a_lens):.0f}, \"\n",
    "      f\"range=[{a_lens.min()}, {a_lens.max()}]\")\n",
    "\n",
    "# Show 3 examples\n",
    "for i in range(3):\n",
    "    s = samples[i]\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"  Q ({s['query_words']}w): {s['query'][:120]}...\")\n",
    "    print(f\"  D ({s['doc_words']}w): {s['document'][:100]}...\")\n",
    "    print(f\"  A ({s['answer_words']}w): {s['answer'][:100]}...\")\n",
    "\n",
    "# Compare with MS MARCO distribution\n",
    "print(f\"\\n--- Comparison with MS MARCO ---\")\n",
    "print(f\"  MS MARCO: mean query = 6.0w, mean doc = ~60w, mean answer = ~20w\")\n",
    "print(f\"  This dataset: mean query = {q_lens.mean():.1f}w, \"\n",
    "      f\"mean doc = {d_lens.mean():.0f}w, mean answer = {a_lens.mean():.0f}w\")\n",
    "print(f\"  Query ratio: {q_lens.mean()/6.0:.1f}x longer\")\n",
    "print(f\"  Doc ratio: {d_lens.mean()/60:.1f}x longer\")\n",
    "\n",
    "del ds\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dfeab356",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T01:08:35.746521Z",
     "iopub.status.busy": "2026-02-18T01:08:35.746075Z",
     "iopub.status.idle": "2026-02-18T01:08:54.199162Z",
     "shell.execute_reply": "2026-02-18T01:08:54.198093Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading google/t5gemma-2-4b-4b...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c1f00b31fb949998388d8f65871912c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/1327 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded. dtype=torch.bfloat16\n",
      "GPU memory: 15.02 GB\n",
      "Helpers defined.\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Load model and define scoring helpers\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv())\n",
    "HF_TOKEN = os.environ.get(\"HF_TOKEN\")\n",
    "\n",
    "from transformers import AutoProcessor, AutoModelForSeq2SeqLM\n",
    "\n",
    "print(f\"Loading {MODEL_NAME}...\")\n",
    "processor = AutoProcessor.from_pretrained(MODEL_NAME, token=HF_TOKEN)\n",
    "tokenizer = processor.tokenizer\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    MODEL_NAME, device_map=\"auto\", torch_dtype=torch.bfloat16, token=HF_TOKEN,\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "DEVICE = next(model.parameters()).device\n",
    "print(f\"Model loaded. dtype={next(model.parameters()).dtype}\")\n",
    "print(f\"GPU memory: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "\n",
    "\n",
    "def score_nll(encoder_text, answer_text, prefix_token_count=0, truncate=False):\n",
    "    # Score NLL of answer given encoder text, with optional prefix truncation.\n",
    "    enc_ids = tokenizer(encoder_text, return_tensors=\"pt\",\n",
    "                        add_special_tokens=True, truncation=True,\n",
    "                        max_length=2048).input_ids.to(DEVICE)\n",
    "    total_enc_len = enc_ids.shape[1]\n",
    "    enc_mask = torch.ones(1, total_enc_len, device=DEVICE, dtype=torch.long)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        encoder_outputs = model.get_encoder()(\n",
    "            input_ids=enc_ids, attention_mask=enc_mask\n",
    "        )\n",
    "\n",
    "    if truncate and prefix_token_count > 0:\n",
    "        cross_attn_mask = torch.ones(1, total_enc_len, device=DEVICE, dtype=torch.long)\n",
    "        cross_attn_mask[:, :prefix_token_count] = 0\n",
    "    else:\n",
    "        cross_attn_mask = torch.ones(1, total_enc_len, device=DEVICE, dtype=torch.long)\n",
    "\n",
    "    ans_ids = tokenizer(answer_text, return_tensors=\"pt\",\n",
    "                        add_special_tokens=False, truncation=True,\n",
    "                        max_length=256).input_ids.to(DEVICE)\n",
    "    if ans_ids.shape[1] == 0:\n",
    "        return 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            encoder_outputs=encoder_outputs,\n",
    "            attention_mask=cross_attn_mask,\n",
    "            labels=ans_ids,\n",
    "        )\n",
    "\n",
    "    logits = outputs.logits\n",
    "    log_probs = F.log_softmax(logits, dim=-1)\n",
    "    token_log_probs = log_probs[0].gather(1, ans_ids[0].unsqueeze(1)).squeeze(1)\n",
    "    mean_nll = -token_log_probs.mean().item()\n",
    "\n",
    "    del encoder_outputs, outputs, logits, log_probs\n",
    "    return mean_nll\n",
    "\n",
    "\n",
    "def count_prefix_tokens(prefix_text, document_text):\n",
    "    # Count how many tokens the prefix occupies in [prefix + newline + document].\n",
    "    full_text = prefix_text + \"\\n\" + document_text\n",
    "    full_ids = tokenizer(full_text, add_special_tokens=True, truncation=True,\n",
    "                         max_length=2048).input_ids\n",
    "    doc_ids = tokenizer(document_text, add_special_tokens=True, truncation=True,\n",
    "                        max_length=2048).input_ids\n",
    "    return len(full_ids) - len(doc_ids)\n",
    "\n",
    "\n",
    "STOP_WORDS = {\n",
    "    'a', 'an', 'the', 'is', 'are', 'was', 'were', 'be', 'been', 'being',\n",
    "    'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could',\n",
    "    'should', 'may', 'might', 'can', 'shall', 'to', 'of', 'in', 'for',\n",
    "    'on', 'with', 'at', 'by', 'from', 'as', 'into', 'through', 'during',\n",
    "    'before', 'after', 'above', 'below', 'between', 'and', 'but', 'or',\n",
    "    'not', 'no', 'if', 'then', 'than', 'so', 'up', 'out', 'about',\n",
    "    'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those',\n",
    "    'it', 'its', 'i', 'me', 'my', 'we', 'our', 'you', 'your', 'he',\n",
    "    'him', 'his', 'she', 'her', 'they', 'them', 'their', 'how', 'when',\n",
    "    'where', 'why', 'much', 'many', 'some', 'any', 'all', 'each',\n",
    "    'does', 'also', 'just', 'more', 'most', 'very', 'too', 'only',\n",
    "}\n",
    "\n",
    "print(\"Helpers defined.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e075a6e2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T01:08:54.203069Z",
     "iopub.status.busy": "2026-02-18T01:08:54.202174Z",
     "iopub.status.idle": "2026-02-18T01:08:55.605046Z",
     "shell.execute_reply": "2026-02-18T01:08:55.604345Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conditions to score: 7\n",
      "  bare\n",
      "  oracle_trunc\n",
      "  random_matched_trunc\n",
      "  scrambled_oracle_trunc\n",
      "  surr_template_trunc\n",
      "  repeat_the_trunc\n",
      "  repeat_kw_trunc\n",
      "\n",
      "Example (sample 0):\n",
      "  Query (21w): What are some of the conditions that require a reduction in lumber or connector plate design values according to ANSI/TP\n",
      "  Document (530w): Designing for Damp Conditions\n",
      "Designing for Damp Conditions\n",
      "a reduction in lumbe...\n",
      "  Answer (42w): Conditions that require a reduction in lumber or connector plate design values a...\n",
      "\n",
      "  bare                        : [document only]\n",
      "  oracle_trunc                 ( 32 prefix toks): What are some of the conditions that require a reduction in ...\n",
      "  random_matched_trunc         ( 27 prefix toks): Free whores in Huntsville Look Sexual Partners General Prost\n",
      "  scrambled_oracle_trunc       ( 32 prefix toks): What according design are a conditions lumber of to values c\n",
      "  surr_template_trunc          (  5 prefix toks): What is wood?\n",
      "  repeat_the_trunc             ( 22 prefix toks): the the the the the the the the the the the the the the the \n",
      "  repeat_kw_trunc              ( 22 prefix toks): wood wood wood wood wood wood wood wood wood wood wood wood \n",
      "\n",
      "Prefix token counts (first 50 samples):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  oracle_trunc                 mean=22.1, range=[17, 38]\n",
      "  random_matched_trunc         mean=24.2, range=[17, 41]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  scrambled_oracle_trunc       mean=22.2, range=[17, 38]\n",
      "  surr_template_trunc          mean=5.3, range=[5, 9]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  repeat_the_trunc             mean=18.4, range=[16, 23]\n",
      "  repeat_kw_trunc              mean=23.9, range=[16, 95]\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Generate surrogate conditions for each sample\n",
    "\n",
    "# Build a pool of \"other\" words from unrelated documents for random conditions\n",
    "other_words_pool = []\n",
    "for i, s in enumerate(samples):\n",
    "    other_idx = (i + N_SAMPLES // 2) % N_SAMPLES\n",
    "    other_doc = samples[other_idx]['document']\n",
    "    other_words_pool.append(other_doc.split())\n",
    "\n",
    "for i, s in enumerate(samples):\n",
    "    query_words = s['query'].split()\n",
    "    n_query_words = len(query_words)\n",
    "    other_words = other_words_pool[i]\n",
    "\n",
    "    # --- Content ablation (length-matched to oracle) ---\n",
    "\n",
    "    # scrambled_oracle: same words as query, random order\n",
    "    rng = np.random.RandomState(SEED + i)\n",
    "    shuffled = list(query_words)\n",
    "    rng.shuffle(shuffled)\n",
    "    s['scrambled_oracle'] = \" \".join(shuffled)\n",
    "\n",
    "    # random_matched: N random words from unrelated doc (same count as query)\n",
    "    if len(other_words) >= n_query_words:\n",
    "        s['random_matched'] = \" \".join(other_words[:n_query_words])\n",
    "    else:\n",
    "        # Pad with repeated words if other doc is too short\n",
    "        padded = other_words * ((n_query_words // len(other_words)) + 1)\n",
    "        s['random_matched'] = \" \".join(padded[:n_query_words])\n",
    "\n",
    "    # --- Token diversity (length-matched to oracle word count) ---\n",
    "\n",
    "    # repeat_the: \"the\" repeated N times\n",
    "    s['repeat_the'] = \" \".join([\"the\"] * n_query_words)\n",
    "\n",
    "    # repeat_kw: top document keyword repeated N times\n",
    "    doc_words_clean = re.sub(r'[^\\w\\s]', '', s['document'].lower()).split()\n",
    "    content = [w for w in doc_words_clean if w not in STOP_WORDS and len(w) > 2]\n",
    "    if content:\n",
    "        counts = Counter(content)\n",
    "        top_word = counts.most_common(1)[0][0]\n",
    "    else:\n",
    "        top_word = \"information\"\n",
    "    s['repeat_kw'] = \" \".join([top_word] * n_query_words)\n",
    "\n",
    "    # --- Practical surrogate ---\n",
    "\n",
    "    # surr_template: \"What is [top_keyword]?\"\n",
    "    if content:\n",
    "        kw = counts.most_common(1)[0][0]\n",
    "    else:\n",
    "        kw = \"information\"\n",
    "    s['surr_template'] = f\"What is {kw}?\"\n",
    "\n",
    "# Define all conditions to score\n",
    "COND_NAMES = [\n",
    "    'bare',\n",
    "    'oracle_trunc',\n",
    "    'random_matched_trunc',\n",
    "    'scrambled_oracle_trunc',\n",
    "    'surr_template_trunc',\n",
    "    'repeat_the_trunc',\n",
    "    'repeat_kw_trunc',\n",
    "]\n",
    "\n",
    "print(f\"Conditions to score: {len(COND_NAMES)}\")\n",
    "for c in COND_NAMES:\n",
    "    print(f\"  {c}\")\n",
    "\n",
    "# Show example\n",
    "ex = samples[0]\n",
    "print(f\"\\nExample (sample 0):\")\n",
    "print(f\"  Query ({ex['query_words']}w): {ex['query'][:120]}\")\n",
    "print(f\"  Document ({ex['doc_words']}w): {ex['document'][:80]}...\")\n",
    "print(f\"  Answer ({ex['answer_words']}w): {ex['answer'][:80]}...\")\n",
    "print()\n",
    "\n",
    "for c in COND_NAMES:\n",
    "    if c == 'bare':\n",
    "        print(f\"  {c:<28}: [document only]\")\n",
    "    elif c == 'oracle_trunc':\n",
    "        ptoks = count_prefix_tokens(ex['query'], ex['document'])\n",
    "        print(f\"  {c:<28} ({ptoks:>3} prefix toks): {ex['query'][:60]}...\")\n",
    "    else:\n",
    "        key = c.replace('_trunc', '')\n",
    "        text = ex[key]\n",
    "        ptoks = count_prefix_tokens(text, ex['document'])\n",
    "        print(f\"  {c:<28} ({ptoks:>3} prefix toks): {str(text)[:60]}\")\n",
    "\n",
    "# Report prefix token stats across first 50 samples\n",
    "print(f\"\\nPrefix token counts (first 50 samples):\")\n",
    "for c in COND_NAMES:\n",
    "    if c == 'bare':\n",
    "        continue\n",
    "    if c == 'oracle_trunc':\n",
    "        toks = [count_prefix_tokens(s['query'], s['document']) for s in samples[:50]]\n",
    "    else:\n",
    "        key = c.replace('_trunc', '')\n",
    "        toks = [count_prefix_tokens(s[key], s['document']) for s in samples[:50]]\n",
    "    print(f\"  {c:<28} mean={np.mean(toks):.1f}, range=[{min(toks)}, {max(toks)}]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ff0950e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T01:08:55.608951Z",
     "iopub.status.busy": "2026-02-18T01:08:55.608673Z",
     "iopub.status.idle": "2026-02-18T01:22:18.741605Z",
     "shell.execute_reply": "2026-02-18T01:22:18.740902Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "SCORING ALL CONDITIONS\n",
      "======================================================================\n",
      "Starting fresh: 7 conditions x 500 samples = 3500 scorings\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a5e5f3c1088464e861474af323aaf10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Scoring:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 20/500 | 0.6m | ETA 13.3m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 40/500 | 1.1m | ETA 12.4m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 60/500 | 1.6m | ETA 11.8m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 80/500 | 2.1m | ETA 11.3m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 100/500 | 2.7m | ETA 10.7m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 120/500 | 3.2m | ETA 10.2m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 140/500 | 3.7m | ETA 9.6m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 160/500 | 4.3m | ETA 9.1m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 180/500 | 4.8m | ETA 8.6m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 200/500 | 5.4m | ETA 8.0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 220/500 | 5.9m | ETA 7.5m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 240/500 | 6.4m | ETA 7.0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 260/500 | 7.0m | ETA 6.4m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 280/500 | 7.5m | ETA 5.9m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 300/500 | 8.0m | ETA 5.4m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 320/500 | 8.6m | ETA 4.8m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 340/500 | 9.1m | ETA 4.3m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 360/500 | 9.6m | ETA 3.7m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 380/500 | 10.2m | ETA 3.2m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 400/500 | 10.7m | ETA 2.7m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 420/500 | 11.2m | ETA 2.1m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 440/500 | 11.8m | ETA 1.6m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 460/500 | 12.3m | ETA 1.1m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 480/500 | 12.8m | ETA 0.5m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 500/500 | 13.4m | ETA 0.0m\n",
      "\n",
      "Scoring complete: 500 samples, 7 conditions in 13.4 min\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Run scoring with checkpointing\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"SCORING ALL CONDITIONS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "results = []\n",
    "start_idx = 0\n",
    "\n",
    "if CHECKPOINT_PATH.exists():\n",
    "    ckpt = json.loads(CHECKPOINT_PATH.read_text())\n",
    "    if ckpt.get('n_total') == N_SAMPLES and len(ckpt.get('results', [])) > 0:\n",
    "        saved_queries = [r['query'][:50] for r in ckpt['results']]\n",
    "        current_queries = [s['query'][:50] for s in samples[:len(saved_queries)]]\n",
    "        if saved_queries == current_queries:\n",
    "            results = ckpt['results']\n",
    "            start_idx = len(results)\n",
    "            print(f\"Resuming from checkpoint: {start_idx}/{N_SAMPLES}\")\n",
    "\n",
    "if start_idx == 0:\n",
    "    print(f\"Starting fresh: {len(COND_NAMES)} conditions x {N_SAMPLES} samples \"\n",
    "          f\"= {len(COND_NAMES) * N_SAMPLES} scorings\")\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "for i in tqdm(range(start_idx, N_SAMPLES), initial=start_idx, total=N_SAMPLES,\n",
    "              desc=\"Scoring\"):\n",
    "    s = samples[i]\n",
    "    result = {\n",
    "        'query': s['query'],\n",
    "        'answer': s['answer'],\n",
    "        'query_words': s['query_words'],\n",
    "        'doc_words': s['doc_words'],\n",
    "        'answer_words': s['answer_words'],\n",
    "    }\n",
    "\n",
    "    for cond in COND_NAMES:\n",
    "        if cond == 'bare':\n",
    "            nll = score_nll(s['document'], s['answer'])\n",
    "            result['nll_bare'] = nll\n",
    "        elif cond == 'oracle_trunc':\n",
    "            enc_text = s['query'] + \"\\n\" + s['document']\n",
    "            ptoks = count_prefix_tokens(s['query'], s['document'])\n",
    "            nll = score_nll(enc_text, s['answer'], ptoks, truncate=True)\n",
    "            result['nll_oracle_trunc'] = nll\n",
    "            result['ptoks_oracle'] = ptoks\n",
    "        else:\n",
    "            key = cond.replace('_trunc', '')\n",
    "            surr_text = s[key]\n",
    "            enc_text = surr_text + \"\\n\" + s['document']\n",
    "            ptoks = count_prefix_tokens(surr_text, s['document'])\n",
    "            nll = score_nll(enc_text, s['answer'], ptoks, truncate=True)\n",
    "            result[f'nll_{cond}'] = nll\n",
    "            result[f'ptoks_{cond}'] = ptoks\n",
    "\n",
    "    results.append(result)\n",
    "\n",
    "    if (i + 1) % 20 == 0 or i == N_SAMPLES - 1:\n",
    "        ckpt = {\n",
    "            'n_total': N_SAMPLES,\n",
    "            'results': results,\n",
    "            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        }\n",
    "        CHECKPOINT_PATH.write_text(json.dumps(ckpt))\n",
    "        elapsed = time.time() - t0\n",
    "        done = i - start_idx + 1\n",
    "        eta = (N_SAMPLES - i - 1) * elapsed / done if done > 0 else 0\n",
    "        tqdm.write(f\"  Checkpoint {i+1}/{N_SAMPLES} | {elapsed/60:.1f}m | ETA {eta/60:.1f}m\")\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "elapsed = time.time() - t0\n",
    "print(f\"\\nScoring complete: {len(results)} samples, \"\n",
    "      f\"{len(COND_NAMES)} conditions in {elapsed/60:.1f} min\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd74a1d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T01:22:18.746653Z",
     "iopub.status.busy": "2026-02-18T01:22:18.746374Z",
     "iopub.status.idle": "2026-02-18T01:22:18.759800Z",
     "shell.execute_reply": "2026-02-18T01:22:18.759140Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PART 1: BASELINE CHARACTERIZATION\n",
      "======================================================================\n",
      "\n",
      "Baseline NLL (document only):  mean=1.3135, std=0.6500\n",
      "Oracle NLL (query + document): mean=1.2239, std=0.6030\n",
      "Oracle headroom: delta=+0.0896, d=+0.592\n",
      "Oracle win rate: 85.0% (p=1.88e-34)\n",
      "\n",
      "Good: Oracle headroom is significant (d=+0.592, p=1.88e-34)\n",
      "\n",
      "Oracle benefit distribution:\n",
      "  10th pctile: -0.0156\n",
      "  25th pctile: +0.0273\n",
      "  Median:      +0.0703\n",
      "  75th pctile: +0.1338\n",
      "  90th pctile: +0.2344\n",
      "  % negative:  13.6%\n",
      "\n",
      "--- Comparison with MS MARCO Exp 02 ---\n",
      "  MS MARCO: bare NLL=3.68, oracle NLL=2.99, headroom=+0.68, d=+0.376\n",
      "  This:     bare NLL=1.31, oracle NLL=1.22, headroom=+0.090, d=+0.592\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Part 1 — Baseline Characterization\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PART 1: BASELINE CHARACTERIZATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "bare_nlls = np.array([r['nll_bare'] for r in results])\n",
    "oracle_nlls = np.array([r['nll_oracle_trunc'] for r in results])\n",
    "oracle_benefit = bare_nlls - oracle_nlls\n",
    "\n",
    "print(f\"\\nBaseline NLL (document only):  mean={bare_nlls.mean():.4f}, std={bare_nlls.std():.4f}\")\n",
    "print(f\"Oracle NLL (query + document): mean={oracle_nlls.mean():.4f}, std={oracle_nlls.std():.4f}\")\n",
    "print(f\"Oracle headroom: delta={oracle_benefit.mean():+.4f}, d={cohens_d(oracle_benefit):+.3f}\")\n",
    "\n",
    "# Win rate and significance\n",
    "win_rate = np.mean(oracle_benefit > 0) * 100\n",
    "_, p_oracle = stats.ttest_1samp(oracle_benefit, 0)\n",
    "print(f\"Oracle win rate: {win_rate:.1f}% (p={p_oracle:.2e})\")\n",
    "\n",
    "if oracle_benefit.mean() < 0.05:\n",
    "    print(\"\\nWARNING: Oracle headroom is very small. Results may not be meaningful.\")\n",
    "elif p_oracle > 0.05:\n",
    "    print(\"\\nWARNING: Oracle headroom is not statistically significant.\")\n",
    "else:\n",
    "    print(f\"\\nGood: Oracle headroom is significant (d={cohens_d(oracle_benefit):+.3f}, \"\n",
    "          f\"p={p_oracle:.2e})\")\n",
    "\n",
    "# Headroom distribution\n",
    "pcts = np.percentile(oracle_benefit, [10, 25, 50, 75, 90])\n",
    "print(f\"\\nOracle benefit distribution:\")\n",
    "print(f\"  10th pctile: {pcts[0]:+.4f}\")\n",
    "print(f\"  25th pctile: {pcts[1]:+.4f}\")\n",
    "print(f\"  Median:      {pcts[2]:+.4f}\")\n",
    "print(f\"  75th pctile: {pcts[3]:+.4f}\")\n",
    "print(f\"  90th pctile: {pcts[4]:+.4f}\")\n",
    "print(f\"  % negative:  {np.mean(oracle_benefit < 0)*100:.1f}%\")\n",
    "\n",
    "# Comparison with MS MARCO\n",
    "print(f\"\\n--- Comparison with MS MARCO Exp 02 ---\")\n",
    "print(f\"  MS MARCO: bare NLL=3.68, oracle NLL=2.99, headroom=+0.68, d=+0.376\")\n",
    "print(f\"  This:     bare NLL={bare_nlls.mean():.2f}, oracle NLL={oracle_nlls.mean():.2f}, \"\n",
    "      f\"headroom={oracle_benefit.mean():+.3f}, d={cohens_d(oracle_benefit):+.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e343e2bb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T01:22:18.763034Z",
     "iopub.status.busy": "2026-02-18T01:22:18.762642Z",
     "iopub.status.idle": "2026-02-18T01:22:18.780508Z",
     "shell.execute_reply": "2026-02-18T01:22:18.779680Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PART 2: CONTENT ABLATION\n",
      "======================================================================\n",
      "Decompose: bare -> random_matched -> scrambled_oracle -> oracle\n",
      "  Structure:  bare -> random_matched (any prefix helps)\n",
      "  Vocabulary: random_matched -> scrambled (right words, wrong order)\n",
      "  Semantics:  scrambled -> oracle (right word order, full meaning)\n",
      "\n",
      "Step                               Mean NLL      Delta   % total        d            p   sig\n",
      "------------------------------------------------------------------------------------------\n",
      "  bare (baseline)                    1.3135\n",
      "  + Structure                        1.2380    +0.0755     84.3%   +0.858     6.63e-62 ***\n",
      "  + Vocabulary                       1.2202    +0.0178     19.9%   +0.209     3.92e-06 ***\n",
      "  + Semantics                        1.2239    -0.0037     -4.2%   -0.031     4.83e-01 ns\n",
      "  TOTAL                                        +0.0896    100.0%\n",
      "\n",
      "  Decomposition residual: 0.000000 (should be ~0)\n",
      "\n",
      "  SUMMARY: Structure=84.3%, Vocabulary=19.9%, Semantics=-4.2%\n",
      "\n",
      "--- Comparison with Exp 2B (MS MARCO, 6-word queries) ---\n",
      "  MS MARCO:  Structure=84.7%, Vocabulary=5.5% (ns), Semantics=9.7% (***)\n",
      "  This data: Structure=84.3%, Vocabulary=19.9%, Semantics=-4.2%\n",
      "  Structure shift: -0.4 percentage points\n",
      "  Semantics shift: -13.9 percentage points\n",
      "\n",
      "  --> Semantic component is similar to MS MARCO.\n",
      "      The 85% structural finding appears to be a genuine property of the mechanism.\n",
      "\n",
      "--- Length matching verification ---\n",
      "  Oracle:     mean=22.4 toks\n",
      "  Scrambled:  mean=22.5 toks\n",
      "  RandMatch:  mean=25.6 toks\n",
      "  Oracle-Scrambled token diff: mean=0.1, max=2\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Part 2 — Content Ablation Decomposition\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PART 2: CONTENT ABLATION\")\n",
    "print(\"=\" * 70)\n",
    "print(\"Decompose: bare -> random_matched -> scrambled_oracle -> oracle\")\n",
    "print(\"  Structure:  bare -> random_matched (any prefix helps)\")\n",
    "print(\"  Vocabulary: random_matched -> scrambled (right words, wrong order)\")\n",
    "print(\"  Semantics:  scrambled -> oracle (right word order, full meaning)\\n\")\n",
    "\n",
    "randmatch_nlls = np.array([r['nll_random_matched_trunc'] for r in results])\n",
    "scrambled_nlls = np.array([r['nll_scrambled_oracle_trunc'] for r in results])\n",
    "\n",
    "# Component benefits\n",
    "struct_comp = bare_nlls - randmatch_nlls\n",
    "vocab_comp = randmatch_nlls - scrambled_nlls\n",
    "sem_comp = scrambled_nlls - oracle_nlls\n",
    "total_comp = bare_nlls - oracle_nlls\n",
    "\n",
    "total_mean = total_comp.mean()\n",
    "\n",
    "# Table: condition NLLs and incremental gains\n",
    "print(f\"{'Step':<32} {'Mean NLL':>10} {'Delta':>10} {'% total':>9} \"\n",
    "      f\"{'d':>8} {'p':>12} {'sig':>5}\")\n",
    "print(\"-\" * 90)\n",
    "\n",
    "print(f\"  {'bare (baseline)':<30} {bare_nlls.mean():>10.4f}\")\n",
    "\n",
    "for name, nlls, component, label in [\n",
    "    ('random_matched_trunc', randmatch_nlls, struct_comp, '+ Structure'),\n",
    "    ('scrambled_oracle_trunc', scrambled_nlls, vocab_comp, '+ Vocabulary'),\n",
    "    ('oracle_trunc', oracle_nlls, sem_comp, '+ Semantics'),\n",
    "]:\n",
    "    mu = component.mean()\n",
    "    pct = mu / total_mean * 100 if total_mean != 0 else 0\n",
    "    d = cohens_d(component)\n",
    "    _, p = stats.ttest_1samp(component, 0)\n",
    "    sig = '***' if p < 0.001 else '**' if p < 0.01 else '*' if p < 0.05 else 'ns'\n",
    "    print(f\"  {label:<30} {nlls.mean():>10.4f} {mu:>+10.4f} {pct:>8.1f}% \"\n",
    "          f\"{d:>+8.3f} {p:>12.2e} {sig}\")\n",
    "\n",
    "print(f\"  {'TOTAL':<30} {'':>10} {total_mean:>+10.4f} {'100.0%':>9}\")\n",
    "\n",
    "# Decomposition residual\n",
    "residual = total_mean - (struct_comp.mean() + vocab_comp.mean() + sem_comp.mean())\n",
    "print(f\"\\n  Decomposition residual: {residual:.6f} (should be ~0)\")\n",
    "\n",
    "# Percentages\n",
    "struct_pct = struct_comp.mean() / total_mean * 100 if total_mean != 0 else 0\n",
    "vocab_pct = vocab_comp.mean() / total_mean * 100 if total_mean != 0 else 0\n",
    "sem_pct = sem_comp.mean() / total_mean * 100 if total_mean != 0 else 0\n",
    "\n",
    "print(f\"\\n  SUMMARY: Structure={struct_pct:.1f}%, Vocabulary={vocab_pct:.1f}%, \"\n",
    "      f\"Semantics={sem_pct:.1f}%\")\n",
    "\n",
    "# Compare with MS MARCO Exp 2B\n",
    "print(f\"\\n--- Comparison with Exp 2B (MS MARCO, 6-word queries) ---\")\n",
    "print(f\"  MS MARCO:  Structure=84.7%, Vocabulary=5.5% (ns), Semantics=9.7% (***)\")\n",
    "print(f\"  This data: Structure={struct_pct:.1f}%, Vocabulary={vocab_pct:.1f}%, \"\n",
    "      f\"Semantics={sem_pct:.1f}%\")\n",
    "\n",
    "delta_struct = struct_pct - 84.7\n",
    "delta_sem = sem_pct - 9.7\n",
    "print(f\"  Structure shift: {delta_struct:+.1f} percentage points\")\n",
    "print(f\"  Semantics shift: {delta_sem:+.1f} percentage points\")\n",
    "\n",
    "if sem_pct > 20:\n",
    "    print(f\"\\n  --> SEMANTIC COMPONENT GREW with longer queries!\")\n",
    "    print(f\"      This suggests the MS MARCO finding was partly an artifact of short queries.\")\n",
    "elif sem_pct > 15:\n",
    "    print(f\"\\n  --> MODERATE semantic growth. Content matters somewhat more with longer queries.\")\n",
    "else:\n",
    "    print(f\"\\n  --> Semantic component is similar to MS MARCO.\")\n",
    "    print(f\"      The 85% structural finding appears to be a genuine property of the mechanism.\")\n",
    "\n",
    "# Token count verification\n",
    "print(f\"\\n--- Length matching verification ---\")\n",
    "oracle_toks = np.array([r['ptoks_oracle'] for r in results])\n",
    "scrambled_toks = np.array([r['ptoks_scrambled_oracle_trunc'] for r in results])\n",
    "randmatch_toks = np.array([r['ptoks_random_matched_trunc'] for r in results])\n",
    "print(f\"  Oracle:     mean={oracle_toks.mean():.1f} toks\")\n",
    "print(f\"  Scrambled:  mean={scrambled_toks.mean():.1f} toks\")\n",
    "print(f\"  RandMatch:  mean={randmatch_toks.mean():.1f} toks\")\n",
    "tok_diff = np.abs(oracle_toks - scrambled_toks)\n",
    "print(f\"  Oracle-Scrambled token diff: mean={tok_diff.mean():.1f}, max={tok_diff.max()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bcf8f485",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T01:22:18.783651Z",
     "iopub.status.busy": "2026-02-18T01:22:18.783134Z",
     "iopub.status.idle": "2026-02-18T01:22:18.800319Z",
     "shell.execute_reply": "2026-02-18T01:22:18.799725Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PART 3: TOKEN DIVERSITY\n",
      "======================================================================\n",
      "All conditions length-matched to query word count. Does content matter?\n",
      "\n",
      "Description                                     NLL      Delta        d    Win%   %Orc\n",
      "-------------------------------------------------------------------------------------\n",
      "  Real query (upper bound)                   1.2239    +0.0896   +0.592   85.0%   100%\n",
      "  Query words, shuffled                      1.2202    +0.0933   +0.889   91.2%   150%\n",
      "  Random words, length-matched               1.2380    +0.0755   +0.858   86.4%   145%\n",
      "  \"What is [keyword]?\"                       1.2406    +0.0729   +0.933   88.4%   158%\n",
      "  Doc keyword repeated N times               1.2461    +0.0674   +0.887   86.4%   150%\n",
      "  \"the\" repeated N times                     1.2484    +0.0651   +0.888   84.6%   150%\n",
      "  bare (lower bound)                         1.3135\n",
      "\n",
      "--- Pairwise head-to-head ---\n",
      "  Does word ORDER matter?\n",
      "    oracle vs scrambled: d=-0.031, win=51.0%, p=4.83e-01 ns [scrambled]\n",
      "  Do the right WORDS matter?\n",
      "    scrambled vs random_matched: d=+0.209, win=59.4%, p=3.92e-06 *** [scrambled]\n",
      "  Does ANY content matter (oracle vs random)?\n",
      "    oracle vs random_matched: d=+0.105, win=56.4%, p=1.93e-02 * [oracle]\n",
      "  Does diversity help (uniform vs diverse)?\n",
      "    repeat_the vs random_matched: d=-0.197, win=38.0%, p=1.36e-05 *** [random_matched]\n",
      "  Does keyword content help vs pure filler?\n",
      "    repeat_kw vs repeat_the: d=+0.069, win=46.8%, p=1.21e-01 ns [repeat_kw]\n",
      "  Does template surrogate beat random?\n",
      "    surr_template vs random_matched: d=-0.047, win=44.8%, p=2.90e-01 ns [random_matched]\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Part 3 — Token Diversity\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PART 3: TOKEN DIVERSITY\")\n",
    "print(\"=\" * 70)\n",
    "print(\"All conditions length-matched to query word count. Does content matter?\\n\")\n",
    "\n",
    "repeat_the_nlls = np.array([r['nll_repeat_the_trunc'] for r in results])\n",
    "repeat_kw_nlls = np.array([r['nll_repeat_kw_trunc'] for r in results])\n",
    "surr_tmpl_nlls = np.array([r['nll_surr_template_trunc'] for r in results])\n",
    "\n",
    "oracle_d = cohens_d(oracle_benefit)\n",
    "\n",
    "all_conds = [\n",
    "    ('oracle_trunc', oracle_nlls, 'Real query (upper bound)'),\n",
    "    ('scrambled_oracle_trunc', scrambled_nlls, 'Query words, shuffled'),\n",
    "    ('random_matched_trunc', randmatch_nlls, 'Random words, length-matched'),\n",
    "    ('surr_template_trunc', surr_tmpl_nlls, '\"What is [keyword]?\"'),\n",
    "    ('repeat_kw_trunc', repeat_kw_nlls, 'Doc keyword repeated N times'),\n",
    "    ('repeat_the_trunc', repeat_the_nlls, '\"the\" repeated N times'),\n",
    "]\n",
    "\n",
    "print(f\"{'Description':<42} {'NLL':>8} {'Delta':>10} {'d':>8} {'Win%':>7} {'%Orc':>6}\")\n",
    "print(\"-\" * 85)\n",
    "\n",
    "for name, nlls, desc in all_conds:\n",
    "    benefit = bare_nlls - nlls\n",
    "    d = cohens_d(benefit)\n",
    "    delta = benefit.mean()\n",
    "    win = 100 * np.mean(benefit > 0)\n",
    "    pct = d / oracle_d * 100 if oracle_d > 0 else 0\n",
    "    print(f\"  {desc:<40} {nlls.mean():>8.4f} {delta:>+10.4f} {d:>+8.3f} {win:>6.1f}% {pct:>5.0f}%\")\n",
    "\n",
    "print(f\"  {'bare (lower bound)':<40} {bare_nlls.mean():>8.4f}\")\n",
    "\n",
    "# Pairwise comparisons\n",
    "print(f\"\\n--- Pairwise head-to-head ---\")\n",
    "pairs = [\n",
    "    ('oracle', oracle_nlls, 'scrambled', scrambled_nlls,\n",
    "     \"Does word ORDER matter?\"),\n",
    "    ('scrambled', scrambled_nlls, 'random_matched', randmatch_nlls,\n",
    "     \"Do the right WORDS matter?\"),\n",
    "    ('oracle', oracle_nlls, 'random_matched', randmatch_nlls,\n",
    "     \"Does ANY content matter (oracle vs random)?\"),\n",
    "    ('repeat_the', repeat_the_nlls, 'random_matched', randmatch_nlls,\n",
    "     \"Does diversity help (uniform vs diverse)?\"),\n",
    "    ('repeat_kw', repeat_kw_nlls, 'repeat_the', repeat_the_nlls,\n",
    "     \"Does keyword content help vs pure filler?\"),\n",
    "    ('surr_template', surr_tmpl_nlls, 'random_matched', randmatch_nlls,\n",
    "     \"Does template surrogate beat random?\"),\n",
    "]\n",
    "\n",
    "for name_a, nlls_a, name_b, nlls_b, question in pairs:\n",
    "    diff = nlls_b - nlls_a  # positive = A is better\n",
    "    d = cohens_d(diff)\n",
    "    win = 100 * np.mean(diff > 0)\n",
    "    _, p = stats.ttest_1samp(diff, 0)\n",
    "    sig = '***' if p < 0.001 else '**' if p < 0.01 else '*' if p < 0.05 else 'ns'\n",
    "    winner = name_a if d > 0 else name_b\n",
    "    print(f\"  {question}\")\n",
    "    print(f\"    {name_a} vs {name_b}: d={d:+.3f}, win={win:.1f}%, p={p:.2e} {sig} [{winner}]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "abfe44d2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T01:22:18.803440Z",
     "iopub.status.busy": "2026-02-18T01:22:18.802972Z",
     "iopub.status.idle": "2026-02-18T01:22:18.822980Z",
     "shell.execute_reply": "2026-02-18T01:22:18.822367Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PART 4: QUERY LENGTH STRATIFICATION\n",
      "======================================================================\n",
      "Does the structural/semantic balance shift with query length?\n",
      "\n",
      "Query bin                              N   Struct%   Vocab%    Sem%   Oracle d    Rand d\n",
      "------------------------------------------------------------------------------------------\n",
      "  Short (15-15w, mean=15)            118     75.5%    13.9%   10.6%     +0.652    +0.898\n",
      "  Medium (16-17w, mean=17)           173     84.5%    17.8%   -2.3%     +0.575    +0.814\n",
      "  Long (18-37w, mean=20)             209     89.7%    25.8%  -15.6%     +0.577    +0.894\n",
      "\n",
      "--- Correlations ---\n",
      "  query_len vs structure:  r=-0.019 (p=0.680)\n",
      "  query_len vs vocabulary: r=+0.020 (p=0.657)\n",
      "  query_len vs semantics:  r=-0.075 (p=0.093)\n",
      "  query_len vs total:      r=-0.059 (p=0.189)\n",
      "  query_len vs semantic_gap (oracle-random): r=-0.054 (p=0.226)\n",
      "\n",
      "  --> Semantic gap is STABLE across query lengths (within this dataset).\n"
     ]
    }
   ],
   "source": [
    "# Cell 10: Part 4 — Query Length Stratification (within-dataset)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PART 4: QUERY LENGTH STRATIFICATION\")\n",
    "print(\"=\" * 70)\n",
    "print(\"Does the structural/semantic balance shift with query length?\\n\")\n",
    "\n",
    "q_lens = np.array([r['query_words'] for r in results])\n",
    "\n",
    "# Use terciles for reasonable bin sizes\n",
    "tercile_bounds = np.percentile(q_lens, [33, 67])\n",
    "q_bins = np.digitize(q_lens, tercile_bounds)\n",
    "bin_labels = []\n",
    "for b in range(3):\n",
    "    mask = q_bins == b\n",
    "    bmin, bmax = q_lens[mask].min(), q_lens[mask].max()\n",
    "    bmean = q_lens[mask].mean()\n",
    "    label = f\"{'Short' if b==0 else 'Medium' if b==1 else 'Long'} ({bmin}-{bmax}w, mean={bmean:.0f})\"\n",
    "    bin_labels.append(label)\n",
    "\n",
    "print(f\"{'Query bin':<35} {'N':>4} {'Struct%':>9} {'Vocab%':>8} {'Sem%':>7} \"\n",
    "      f\"{'Oracle d':>10} {'Rand d':>9}\")\n",
    "print(\"-\" * 90)\n",
    "\n",
    "bin_struct_pcts = []\n",
    "bin_sem_pcts = []\n",
    "bin_q_means = []\n",
    "\n",
    "for b in range(3):\n",
    "    mask = q_bins == b\n",
    "    n = mask.sum()\n",
    "\n",
    "    # Compute components for this bin\n",
    "    b_struct = (bare_nlls[mask] - randmatch_nlls[mask]).mean()\n",
    "    b_vocab = (randmatch_nlls[mask] - scrambled_nlls[mask]).mean()\n",
    "    b_sem = (scrambled_nlls[mask] - oracle_nlls[mask]).mean()\n",
    "    b_total = (bare_nlls[mask] - oracle_nlls[mask]).mean()\n",
    "\n",
    "    if b_total > 0:\n",
    "        s_pct = b_struct / b_total * 100\n",
    "        v_pct = b_vocab / b_total * 100\n",
    "        sm_pct = b_sem / b_total * 100\n",
    "    else:\n",
    "        s_pct = v_pct = sm_pct = 0\n",
    "\n",
    "    o_d = cohens_d(bare_nlls[mask] - oracle_nlls[mask])\n",
    "    r_d = cohens_d(bare_nlls[mask] - randmatch_nlls[mask])\n",
    "\n",
    "    bin_struct_pcts.append(s_pct)\n",
    "    bin_sem_pcts.append(sm_pct)\n",
    "    bin_q_means.append(q_lens[mask].mean())\n",
    "\n",
    "    print(f\"  {bin_labels[b]:<33} {n:>4} {s_pct:>8.1f}% {v_pct:>7.1f}% {sm_pct:>6.1f}% \"\n",
    "          f\"{o_d:>+10.3f} {r_d:>+9.3f}\")\n",
    "\n",
    "# Correlation: query length vs each component\n",
    "print(f\"\\n--- Correlations ---\")\n",
    "r_struct_q, p_struct_q = stats.pearsonr(q_lens, struct_comp)\n",
    "r_vocab_q, p_vocab_q = stats.pearsonr(q_lens, vocab_comp)\n",
    "r_sem_q, p_sem_q = stats.pearsonr(q_lens, sem_comp)\n",
    "r_total_q, p_total_q = stats.pearsonr(q_lens, total_comp)\n",
    "print(f\"  query_len vs structure:  r={r_struct_q:+.3f} (p={p_struct_q:.3f})\")\n",
    "print(f\"  query_len vs vocabulary: r={r_vocab_q:+.3f} (p={p_vocab_q:.3f})\")\n",
    "print(f\"  query_len vs semantics:  r={r_sem_q:+.3f} (p={p_sem_q:.3f})\")\n",
    "print(f\"  query_len vs total:      r={r_total_q:+.3f} (p={p_total_q:.3f})\")\n",
    "\n",
    "# Semantic gap: does oracle beat random more for longer queries?\n",
    "semantic_gap = oracle_benefit - (bare_nlls - randmatch_nlls)\n",
    "r_gap_q, p_gap_q = stats.pearsonr(q_lens, semantic_gap)\n",
    "print(f\"  query_len vs semantic_gap (oracle-random): r={r_gap_q:+.3f} (p={p_gap_q:.3f})\")\n",
    "\n",
    "if r_gap_q > 0.1 and p_gap_q < 0.05:\n",
    "    print(f\"\\n  --> Semantic gap GROWS with query length!\")\n",
    "    print(f\"      Content matters MORE for longer queries, as hypothesized.\")\n",
    "elif r_gap_q < -0.1 and p_gap_q < 0.05:\n",
    "    print(f\"\\n  --> Semantic gap SHRINKS with query length.\")\n",
    "    print(f\"      Structure becomes MORE dominant for longer queries.\")\n",
    "else:\n",
    "    print(f\"\\n  --> Semantic gap is STABLE across query lengths (within this dataset).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e87b760",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T01:22:18.826031Z",
     "iopub.status.busy": "2026-02-18T01:22:18.825452Z",
     "iopub.status.idle": "2026-02-18T01:22:18.840078Z",
     "shell.execute_reply": "2026-02-18T01:22:18.839506Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PART 5: HARDNESS STRATIFICATION\n",
      "======================================================================\n",
      "Does the decomposition change for harder documents?\n",
      "\n",
      "Quintile        N   Bare NLL   Struct%   Vocab%    Sem%   Oracle d    Sem gap\n",
      "--------------------------------------------------------------------------------\n",
      "  Q1 easy      98      0.586     90.9%    25.3%  -16.2%     +0.744    +0.0037\n",
      "  Q2          102      0.939    137.0%    37.0%  -74.1%     +0.374    -0.0134\n",
      "  Q3           98      1.213     82.6%     9.6%    7.8%     +0.904    +0.0149\n",
      "  Q4          101      1.500    102.0%    24.1%  -26.1%     +0.453    -0.0016\n",
      "  Q5 hard     101      2.308     67.5%    18.3%   14.1%     +0.977    +0.0667\n",
      "\n",
      "--- Correlations ---\n",
      "  hardness vs structure:  r=+0.427 (p=1.25e-23)\n",
      "  hardness vs vocabulary: r=+0.064 (p=1.52e-01)\n",
      "  hardness vs semantics:  r=+0.166 (p=1.89e-04)\n",
      "\n",
      "--- Semantic gap by hardness ---\n",
      "  MS MARCO Exp 2B: Q1 gap=+0.013, Q5 gap=+0.397\n",
      "  This data Q1 easy: gap=+0.0037\n",
      "  This data Q2: gap=-0.0134\n",
      "  This data Q3: gap=+0.0149\n",
      "  This data Q4: gap=-0.0016\n",
      "  This data Q5 hard: gap=+0.0667\n"
     ]
    }
   ],
   "source": [
    "# Cell 11: Part 5 — Hardness Stratification\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PART 5: HARDNESS STRATIFICATION\")\n",
    "print(\"=\" * 70)\n",
    "print(\"Does the decomposition change for harder documents?\\n\")\n",
    "\n",
    "quintile_bounds = np.percentile(bare_nlls, [20, 40, 60, 80])\n",
    "quintiles = np.digitize(bare_nlls, quintile_bounds)\n",
    "\n",
    "print(f\"{'Quintile':<12} {'N':>4} {'Bare NLL':>10} {'Struct%':>9} {'Vocab%':>8} \"\n",
    "      f\"{'Sem%':>7} {'Oracle d':>10} {'Sem gap':>10}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for q in range(5):\n",
    "    mask = quintiles == q\n",
    "    n = mask.sum()\n",
    "    qlabel = ['Q1 easy', 'Q2', 'Q3', 'Q4', 'Q5 hard'][q]\n",
    "    bare_q = bare_nlls[mask].mean()\n",
    "\n",
    "    b_struct = (bare_nlls[mask] - randmatch_nlls[mask]).mean()\n",
    "    b_vocab = (randmatch_nlls[mask] - scrambled_nlls[mask]).mean()\n",
    "    b_sem = (scrambled_nlls[mask] - oracle_nlls[mask]).mean()\n",
    "    b_total = (bare_nlls[mask] - oracle_nlls[mask]).mean()\n",
    "\n",
    "    if b_total > 0:\n",
    "        s_pct = b_struct / b_total * 100\n",
    "        v_pct = b_vocab / b_total * 100\n",
    "        sm_pct = b_sem / b_total * 100\n",
    "    else:\n",
    "        s_pct = v_pct = sm_pct = 0\n",
    "\n",
    "    o_d = cohens_d(bare_nlls[mask] - oracle_nlls[mask])\n",
    "    gap = (oracle_benefit[mask] - (bare_nlls[mask] - randmatch_nlls[mask])).mean()\n",
    "\n",
    "    print(f\"  {qlabel:<10} {n:>4} {bare_q:>10.3f} {s_pct:>8.1f}% {v_pct:>7.1f}% \"\n",
    "          f\"{sm_pct:>6.1f}% {o_d:>+10.3f} {gap:>+10.4f}\")\n",
    "\n",
    "# Correlation: hardness vs each component\n",
    "print(f\"\\n--- Correlations ---\")\n",
    "r_s, p_s = stats.pearsonr(bare_nlls, struct_comp)\n",
    "r_v, p_v = stats.pearsonr(bare_nlls, vocab_comp)\n",
    "r_sm, p_sm = stats.pearsonr(bare_nlls, sem_comp)\n",
    "print(f\"  hardness vs structure:  r={r_s:+.3f} (p={p_s:.2e})\")\n",
    "print(f\"  hardness vs vocabulary: r={r_v:+.3f} (p={p_v:.2e})\")\n",
    "print(f\"  hardness vs semantics:  r={r_sm:+.3f} (p={p_sm:.2e})\")\n",
    "\n",
    "# Semantic component x hardness: does content matter more for hard samples?\n",
    "# (This was true on MS MARCO: Q1 gap=+0.013 -> Q5 gap=+0.397)\n",
    "print(f\"\\n--- Semantic gap by hardness ---\")\n",
    "print(f\"  MS MARCO Exp 2B: Q1 gap=+0.013, Q5 gap=+0.397\")\n",
    "for q in range(5):\n",
    "    mask = quintiles == q\n",
    "    qlabel = ['Q1 easy', 'Q2', 'Q3', 'Q4', 'Q5 hard'][q]\n",
    "    gap = (oracle_benefit[mask] - (bare_nlls[mask] - randmatch_nlls[mask])).mean()\n",
    "    print(f\"  This data {qlabel}: gap={gap:+.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cca28a11",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T01:22:18.842919Z",
     "iopub.status.busy": "2026-02-18T01:22:18.842647Z",
     "iopub.status.idle": "2026-02-18T01:22:19.375285Z",
     "shell.execute_reply": "2026-02-18T01:22:19.374582Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "SYNTHESIS: CROSS-DATASET CONTENT ABLATION RESULTS\n",
      "======================================================================\n",
      "\n",
      "1. DATASET: neural-bridge/rag-dataset-12000\n",
      "   Query length: 17.8w (vs 6.0w MS MARCO)\n",
      "   Doc length: 604w\n",
      "   N: 500\n",
      "\n",
      "2. ORACLE HEADROOM:\n",
      "   Cohen's d: +0.592\n",
      "   NLL improvement: +0.0896\n",
      "   Win rate: 85.0% (p=1.88e-34)\n",
      "\n",
      "3. CONTENT ABLATION:\n",
      "   Component          This dataset        MS MARCO     Change\n",
      "   ------------------------------------------------------------\n",
      "   Structure                 84.3%           84.7%      -0.4pp\n",
      "   Vocabulary                19.9%            5.5%     +14.4pp\n",
      "   Semantics                 -4.2%            9.7%     -13.9pp\n",
      "\n",
      "4. TOKEN DIVERSITY:\n",
      "   repeat_the: d=+0.888 (150% oracle)\n",
      "   repeat_kw:  d=+0.887 (150% oracle)\n",
      "   rand_match: d=+0.858 (145% oracle)\n",
      "   surr_tmpl:  d=+0.933 (158% oracle)\n",
      "\n",
      "5. QUERY LENGTH EFFECT:\n",
      "   Short (15-15w, mean=15): semantic=10.6%\n",
      "   Medium (16-17w, mean=17): semantic=-2.3%\n",
      "   Long (18-37w, mean=20): semantic=-15.6%\n",
      "\n",
      "======================================================================\n",
      "CONCLUSION:\n",
      "  Mechanism: STRUCTURAL\n",
      "  Cross-dataset consistency: CONSISTENT\n",
      "  The 85% structural finding holds with 3x longer queries on a different dataset.\n",
      "  This is a genuine property of the T5Gemma bidirectional encoder mechanism,\n",
      "  not an artifact of MS MARCO's short queries.\n",
      "======================================================================\n",
      "\n",
      "Results saved to results/exp03d/results.json\n",
      "\n",
      "Cleaning up GPU memory...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory: 15.03 GB -> 0.01 GB\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Cell 12: Synthesis + Save\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"SYNTHESIS: CROSS-DATASET CONTENT ABLATION RESULTS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Key numbers\n",
    "oracle_d = cohens_d(oracle_benefit)\n",
    "struct_d = cohens_d(struct_comp)\n",
    "vocab_d = cohens_d(vocab_comp)\n",
    "sem_d = cohens_d(sem_comp)\n",
    "\n",
    "struct_pct = struct_comp.mean() / total_comp.mean() * 100 if total_comp.mean() != 0 else 0\n",
    "vocab_pct = vocab_comp.mean() / total_comp.mean() * 100 if total_comp.mean() != 0 else 0\n",
    "sem_pct = sem_comp.mean() / total_comp.mean() * 100 if total_comp.mean() != 0 else 0\n",
    "\n",
    "repeat_the_d = cohens_d(bare_nlls - repeat_the_nlls)\n",
    "repeat_kw_d = cohens_d(bare_nlls - repeat_kw_nlls)\n",
    "randmatch_d = cohens_d(bare_nlls - randmatch_nlls)\n",
    "surr_tmpl_d = cohens_d(bare_nlls - surr_tmpl_nlls)\n",
    "\n",
    "# All condition NLLs for the summary\n",
    "print(f\"\\n1. DATASET: neural-bridge/rag-dataset-12000\")\n",
    "print(f\"   Query length: {q_lens.mean():.1f}w (vs 6.0w MS MARCO)\")\n",
    "print(f\"   Doc length: {np.array([r['doc_words'] for r in results]).mean():.0f}w\")\n",
    "print(f\"   N: {N_SAMPLES}\")\n",
    "\n",
    "print(f\"\\n2. ORACLE HEADROOM:\")\n",
    "print(f\"   Cohen's d: {oracle_d:+.3f}\")\n",
    "print(f\"   NLL improvement: {oracle_benefit.mean():+.4f}\")\n",
    "_, p_oracle = stats.ttest_1samp(oracle_benefit, 0)\n",
    "print(f\"   Win rate: {np.mean(oracle_benefit > 0)*100:.1f}% (p={p_oracle:.2e})\")\n",
    "\n",
    "print(f\"\\n3. CONTENT ABLATION:\")\n",
    "print(f\"   {'Component':<15} {'This dataset':>15} {'MS MARCO':>15} {'Change':>10}\")\n",
    "print(f\"   {'-'*60}\")\n",
    "print(f\"   {'Structure':<15} {struct_pct:>14.1f}% {'84.7%':>15} {struct_pct-84.7:>+9.1f}pp\")\n",
    "print(f\"   {'Vocabulary':<15} {vocab_pct:>14.1f}% {'5.5%':>15} {vocab_pct-5.5:>+9.1f}pp\")\n",
    "print(f\"   {'Semantics':<15} {sem_pct:>14.1f}% {'9.7%':>15} {sem_pct-9.7:>+9.1f}pp\")\n",
    "\n",
    "print(f\"\\n4. TOKEN DIVERSITY:\")\n",
    "print(f\"   repeat_the: d={repeat_the_d:+.3f} ({repeat_the_d/oracle_d*100:.0f}% oracle)\")\n",
    "print(f\"   repeat_kw:  d={repeat_kw_d:+.3f} ({repeat_kw_d/oracle_d*100:.0f}% oracle)\")\n",
    "print(f\"   rand_match: d={randmatch_d:+.3f} ({randmatch_d/oracle_d*100:.0f}% oracle)\")\n",
    "print(f\"   surr_tmpl:  d={surr_tmpl_d:+.3f} ({surr_tmpl_d/oracle_d*100:.0f}% oracle)\")\n",
    "\n",
    "print(f\"\\n5. QUERY LENGTH EFFECT:\")\n",
    "for b in range(3):\n",
    "    mask = q_bins == b\n",
    "    b_total = (bare_nlls[mask] - oracle_nlls[mask]).mean()\n",
    "    if b_total > 0:\n",
    "        b_sem_pct = (scrambled_nlls[mask] - oracle_nlls[mask]).mean() / b_total * 100\n",
    "    else:\n",
    "        b_sem_pct = 0\n",
    "    print(f\"   {bin_labels[b]}: semantic={b_sem_pct:.1f}%\")\n",
    "\n",
    "# Determine overall conclusion\n",
    "if struct_pct > 70:\n",
    "    mechanism = \"STRUCTURAL\"\n",
    "elif struct_pct > 50:\n",
    "    mechanism = \"MIXED\"\n",
    "else:\n",
    "    mechanism = \"SEMANTIC\"\n",
    "\n",
    "if abs(struct_pct - 84.7) < 10:\n",
    "    cross_dataset = \"CONSISTENT\"\n",
    "elif struct_pct < 74.7:\n",
    "    cross_dataset = \"SEMANTIC_GREW\"\n",
    "else:\n",
    "    cross_dataset = \"STRUCTURAL_GREW\"\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"CONCLUSION:\")\n",
    "print(f\"  Mechanism: {mechanism}\")\n",
    "print(f\"  Cross-dataset consistency: {cross_dataset}\")\n",
    "if mechanism == \"STRUCTURAL\":\n",
    "    print(f\"  The 85% structural finding holds with 3x longer queries on a different dataset.\")\n",
    "    print(f\"  This is a genuine property of the T5Gemma bidirectional encoder mechanism,\")\n",
    "    print(f\"  not an artifact of MS MARCO's short queries.\")\n",
    "elif mechanism == \"MIXED\":\n",
    "    print(f\"  With longer queries, the semantic component grew meaningfully.\")\n",
    "    print(f\"  The mechanism is still primarily structural, but content matters more\")\n",
    "    print(f\"  when queries carry more semantic structure to preserve.\")\n",
    "else:\n",
    "    print(f\"  With longer queries, the semantic component dominates.\")\n",
    "    print(f\"  The MS MARCO finding WAS an artifact of short queries.\")\n",
    "    print(f\"  Content-aware surrogates are worthwhile for long-query use cases.\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "# Save results\n",
    "final_results = {\n",
    "    'experiment': 'exp03d_cross_dataset_content_ablation',\n",
    "    'model': MODEL_NAME,\n",
    "    'dataset': 'neural-bridge/rag-dataset-12000',\n",
    "    'n_samples': N_SAMPLES,\n",
    "    'mean_query_words': float(q_lens.mean()),\n",
    "    'mean_doc_words': float(np.array([r['doc_words'] for r in results]).mean()),\n",
    "    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'baseline': {\n",
    "        'bare_nll': float(bare_nlls.mean()),\n",
    "        'oracle_nll': float(oracle_nlls.mean()),\n",
    "        'oracle_d': float(oracle_d),\n",
    "        'oracle_headroom': float(oracle_benefit.mean()),\n",
    "        'oracle_p': float(p_oracle),\n",
    "    },\n",
    "    'ablation': {\n",
    "        'structure_pct': float(struct_pct),\n",
    "        'vocabulary_pct': float(vocab_pct),\n",
    "        'semantics_pct': float(sem_pct),\n",
    "        'structure_d': float(struct_d),\n",
    "        'vocabulary_d': float(vocab_d),\n",
    "        'semantics_d': float(sem_d),\n",
    "    },\n",
    "    'conditions': {\n",
    "        'oracle_trunc_d': float(oracle_d),\n",
    "        'scrambled_oracle_trunc_d': float(cohens_d(bare_nlls - scrambled_nlls)),\n",
    "        'random_matched_trunc_d': float(randmatch_d),\n",
    "        'surr_template_trunc_d': float(surr_tmpl_d),\n",
    "        'repeat_the_trunc_d': float(repeat_the_d),\n",
    "        'repeat_kw_trunc_d': float(repeat_kw_d),\n",
    "    },\n",
    "    'comparison_with_msmarco': {\n",
    "        'struct_pct_delta': float(struct_pct - 84.7),\n",
    "        'vocab_pct_delta': float(vocab_pct - 5.5),\n",
    "        'sem_pct_delta': float(sem_pct - 9.7),\n",
    "    },\n",
    "    'conclusion': {\n",
    "        'mechanism': mechanism,\n",
    "        'cross_dataset': cross_dataset,\n",
    "    },\n",
    "}\n",
    "\n",
    "with open(RESULTS_DIR / 'results.json', 'w') as f:\n",
    "    json.dump(final_results, f, indent=2)\n",
    "print(f\"\\nResults saved to {RESULTS_DIR / 'results.json'}\")\n",
    "\n",
    "# Cleanup\n",
    "print(f\"\\nCleaning up GPU memory...\")\n",
    "mem_before = torch.cuda.memory_allocated() / 1e9\n",
    "del model, processor, tokenizer\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "mem_after = torch.cuda.memory_allocated() / 1e9\n",
    "print(f\"GPU memory: {mem_before:.2f} GB -> {mem_after:.2f} GB\")\n",
    "print(\"Done!\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "241129bdb2ac4456b2293e1e7ea83b83": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_e901a87caa134e258bb601e51bdb9638",
       "placeholder": "​",
       "style": "IPY_MODEL_72163c64015c4d34ae9633dc39285f15",
       "tabbable": null,
       "tooltip": null,
       "value": "Loading weights: 100%"
      }
     },
     "2719aa51d4d6405283315e181d34dfd6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "3a5e5f3c1088464e861474af323aaf10": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_ede5357e7a364fadba5bd4cfcf007922",
        "IPY_MODEL_68fc54c6f0a740ab9c3767184172c30d",
        "IPY_MODEL_fb2a19e73d70466c99ebd48cf295d4f3"
       ],
       "layout": "IPY_MODEL_b3ad9494f64c4c9faab4499c6f948b1c",
       "tabbable": null,
       "tooltip": null
      }
     },
     "3eae7140ad194dc28aab61ddad064729": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "4971b7e24a224f9a909aeffab3b5217d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "4ae76dfeb7574db3abcf2da8e7197310": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_ca2ef426f5644a0a9510b6b2e4be0470",
       "max": 1327.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_7c36520b3d274990b2e477e86880d634",
       "tabbable": null,
       "tooltip": null,
       "value": 1327.0
      }
     },
     "68fc54c6f0a740ab9c3767184172c30d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_e680291f34db4668b21a34a510fb5388",
       "max": 500.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_4971b7e24a224f9a909aeffab3b5217d",
       "tabbable": null,
       "tooltip": null,
       "value": 500.0
      }
     },
     "72163c64015c4d34ae9633dc39285f15": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "7c36520b3d274990b2e477e86880d634": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "8035a18adf6c469482894ffa4d4b3d1c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "8e6629a4d0b742358ca09cb71c377c4f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "9c1f00b31fb949998388d8f65871912c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_241129bdb2ac4456b2293e1e7ea83b83",
        "IPY_MODEL_4ae76dfeb7574db3abcf2da8e7197310",
        "IPY_MODEL_f3512843bf224682be7eb73eebd18206"
       ],
       "layout": "IPY_MODEL_8e6629a4d0b742358ca09cb71c377c4f",
       "tabbable": null,
       "tooltip": null
      }
     },
     "b3ad9494f64c4c9faab4499c6f948b1c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c87814323cb0472e8f2fb505b4e7d4d0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "ca2ef426f5644a0a9510b6b2e4be0470": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "df9df3f1e3f242e4aa02c312ae3e8243": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "e680291f34db4668b21a34a510fb5388": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e901a87caa134e258bb601e51bdb9638": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ea7e89c8c9dd401984b30f7021e3c085": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ede5357e7a364fadba5bd4cfcf007922": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_3eae7140ad194dc28aab61ddad064729",
       "placeholder": "​",
       "style": "IPY_MODEL_df9df3f1e3f242e4aa02c312ae3e8243",
       "tabbable": null,
       "tooltip": null,
       "value": "Scoring: 100%"
      }
     },
     "f3512843bf224682be7eb73eebd18206": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_ea7e89c8c9dd401984b30f7021e3c085",
       "placeholder": "​",
       "style": "IPY_MODEL_2719aa51d4d6405283315e181d34dfd6",
       "tabbable": null,
       "tooltip": null,
       "value": " 1327/1327 [00:04&lt;00:00, 692.16it/s, Materializing param=model.encoder.vision_tower.vision_model.post_layernorm.weight]"
      }
     },
     "fb2a19e73d70466c99ebd48cf295d4f3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_8035a18adf6c469482894ffa4d4b3d1c",
       "placeholder": "​",
       "style": "IPY_MODEL_c87814323cb0472e8f2fb505b4e7d4d0",
       "tabbable": null,
       "tooltip": null,
       "value": " 500/500 [13:23&lt;00:00,  1.67s/it]"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
