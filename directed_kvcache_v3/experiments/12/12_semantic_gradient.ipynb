{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52323f13",
   "metadata": {},
   "source": [
    "# Experiment 12: Graded Semantic Relevance Sweep\n",
    "\n",
    "## Motivation\n",
    "\n",
    "Exp 2B/3E established the mechanism decomposition: ~85% structural, ~10% semantic,\n",
    "~6% vocabulary. The structural effect is essentially binary (any 5+ token prefix\n",
    "triggers it). But the semantic component was measured only as a binary contrast\n",
    "(oracle vs scrambled). To properly isolate and characterize the semantic gradient,\n",
    "we need experiments that **hold structure constant** and vary only semantic content.\n",
    "\n",
    "This experiment traces the full semantic gradient across 6 levels of relevance,\n",
    "all length-matched to control for structural effects.\n",
    "\n",
    "## Conditions (7)\n",
    "\n",
    "| # | Condition | Prefix | Semantic relevance |\n",
    "|---|-----------|--------|--------------------|\n",
    "| 1 | `bare` | (none) | N/A (lower bound) |\n",
    "| 2 | `oracle_trunc` | real query | maximal |\n",
    "| 3 | `paraphrase_trunc` | LLM paraphrase of query | high (same meaning, different words) |\n",
    "| 4 | `same_topic_trunc` | LLM question about same topic | medium (right topic, wrong question) |\n",
    "| 5 | `unrelated_query_trunc` | real query from different sample | low (real query syntax, wrong topic) |\n",
    "| 6 | `scrambled_oracle_trunc` | oracle words shuffled | vocabulary only (right words, no structure) |\n",
    "| 7 | `random_matched_trunc` | words from random passage | none (structural baseline) |\n",
    "\n",
    "All non-bare conditions are length-matched to oracle query word count per sample.\n",
    "\n",
    "## Analysis\n",
    "\n",
    "- **Part 1**: Standard condition table (d, win%, p, % oracle)\n",
    "- **Part 2**: Semantic gradient — delta by relevance rank, monotonicity test\n",
    "- **Part 3**: Fine-grained decomposition chain\n",
    "- **Part 4**: Hardness interaction — does the semantic gradient steepen for harder samples?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e349dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Setup\n",
    "import os\n",
    "os.umask(0o000)\n",
    "\n",
    "import sys, json, time, re, gc, random as pyrandom\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from scipy import stats\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "sys.path.insert(0, \"../..\")\n",
    "from lib.analysis import cohens_d\n",
    "\n",
    "SEED = 42\n",
    "N_SAMPLES = 500\n",
    "T5GEMMA_NAME = \"google/t5gemma-2-4b-4b\"\n",
    "GEMMA_IT_NAME = \"google/gemma-3-12b-it\"\n",
    "\n",
    "RESULTS_DIR = Path(\"../../results/exp12\")\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "SURROGATES_PATH = RESULTS_DIR / \"surrogates.json\"\n",
    "CHECKPOINT_PATH = RESULTS_DIR / \"checkpoint.json\"\n",
    "\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "pyrandom.seed(SEED)\n",
    "\n",
    "# Prompt templates for Gemma 2 9B-IT\n",
    "PROMPT_PARAPHRASE = (\n",
    "    \"Rephrase this search query using completely different words but keeping \"\n",
    "    \"the same meaning. Keep it to 5-8 words. Output only the rephrased query.\"\n",
    ")\n",
    "PROMPT_SAME_TOPIC = (\n",
    "    \"Write a question about the same topic as this document but asking for \"\n",
    "    \"DIFFERENT information. Keep it to 5-8 words. Output only the question.\"\n",
    ")\n",
    "\n",
    "print(\"Exp 12: Graded Semantic Relevance Sweep\")\n",
    "print(f\"N: {N_SAMPLES}\")\n",
    "print(f\"Generation model: {GEMMA_IT_NAME}\")\n",
    "print(f\"Scoring model: {T5GEMMA_NAME}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3bb78d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Load MS MARCO and select samples\n",
    "from lib.data import count_words\n",
    "from datasets import load_dataset\n",
    "\n",
    "print(\"Loading MS MARCO...\")\n",
    "ds = load_dataset(\"microsoft/ms_marco\", \"v1.1\", split=\"validation\")\n",
    "\n",
    "samples = []\n",
    "for item in ds:\n",
    "    if len(samples) >= N_SAMPLES * 3:\n",
    "        break\n",
    "    passages = item.get('passages', {})\n",
    "    ptexts = passages.get('passage_text', [])\n",
    "    is_sel = passages.get('is_selected', [])\n",
    "    query = item.get('query', '')\n",
    "    answers = item.get('answers', [])\n",
    "    well_formed = item.get('wellFormedAnswers', [])\n",
    "    answer = None\n",
    "    if well_formed and len(well_formed) > 0 and well_formed[0] not in ('[]', ''):\n",
    "        answer = well_formed[0]\n",
    "    elif answers and len(answers) > 0 and answers[0] != 'No Answer Present.':\n",
    "        answer = answers[0]\n",
    "    if not answer:\n",
    "        continue\n",
    "    for pt, sel in zip(ptexts, is_sel):\n",
    "        wc = count_words(pt)\n",
    "        if sel == 1 and 30 <= wc <= 300:\n",
    "            samples.append({\n",
    "                'passage': pt, 'query': query, 'answer': answer,\n",
    "                'word_count': wc,\n",
    "            })\n",
    "            break\n",
    "\n",
    "np.random.seed(SEED)\n",
    "np.random.shuffle(samples)\n",
    "samples = samples[:N_SAMPLES]\n",
    "del ds\n",
    "gc.collect()\n",
    "\n",
    "passage_words = np.array([s['word_count'] for s in samples])\n",
    "query_words = np.array([len(s['query'].split()) for s in samples])\n",
    "print(f\"Selected {N_SAMPLES} samples\")\n",
    "print(f\"Document lengths: {passage_words.min()}-{passage_words.max()} words, \"\n",
    "      f\"mean={passage_words.mean():.0f}\")\n",
    "print(f\"Query lengths: {query_words.min()}-{query_words.max()} words, \"\n",
    "      f\"mean={query_words.mean():.1f}\")\n",
    "\n",
    "# Show 5 examples\n",
    "for i in range(5):\n",
    "    s = samples[i]\n",
    "    print(f\"\\nExample {i}:\")\n",
    "    print(f\"  Q: {s['query']}\")\n",
    "    print(f\"  A: {s['answer'][:80]}\")\n",
    "    print(f\"  P ({s['word_count']}w): {s['passage'][:100]}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56cead8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Phase 1 — Generate surrogates with Gemma 2 9B-IT\n",
    "# Skip if surrogates already cached\n",
    "\n",
    "if SURROGATES_PATH.exists():\n",
    "    print(\"Loading cached surrogates...\")\n",
    "    surrogates = json.loads(SURROGATES_PATH.read_text())\n",
    "    assert len(surrogates) == N_SAMPLES, f\"Expected {N_SAMPLES}, got {len(surrogates)}\"\n",
    "    for i in range(min(10, N_SAMPLES)):\n",
    "        assert surrogates[i]['query'][:50] == samples[i]['query'][:50], \\\n",
    "            f\"Sample {i} query mismatch\"\n",
    "    print(f\"Loaded {len(surrogates)} cached surrogates\")\n",
    "    print(f\"Keys per sample: {list(surrogates[0].keys())}\")\n",
    "else:\n",
    "    print(f\"Loading {GEMMA_IT_NAME} for surrogate generation...\")\n",
    "    from dotenv import load_dotenv, find_dotenv\n",
    "    load_dotenv(find_dotenv())\n",
    "    HF_TOKEN = os.environ.get(\"HF_TOKEN\")\n",
    "\n",
    "    from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "    gen_tokenizer = AutoTokenizer.from_pretrained(GEMMA_IT_NAME, token=HF_TOKEN)\n",
    "    gen_model = AutoModelForCausalLM.from_pretrained(\n",
    "        GEMMA_IT_NAME, device_map=\"auto\", torch_dtype=torch.bfloat16, token=HF_TOKEN,\n",
    "    )\n",
    "    gen_model.eval()\n",
    "    GEN_DEVICE = next(gen_model.parameters()).device\n",
    "    print(f\"Model loaded. GPU memory: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "\n",
    "    def generate_text(input_text, prompt_text):\n",
    "        # Generate text from a prompt + input using Gemma IT.\n",
    "        messages = [\n",
    "            {\"role\": \"user\",\n",
    "             \"content\": f\"{prompt_text}\\n\\n{input_text}\"}\n",
    "        ]\n",
    "        chat_text = gen_tokenizer.apply_chat_template(\n",
    "            messages, tokenize=False, add_generation_prompt=True\n",
    "        )\n",
    "        inputs = gen_tokenizer(chat_text, return_tensors=\"pt\",\n",
    "                               truncation=True, max_length=1024).to(GEN_DEVICE)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output_ids = gen_model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=30,\n",
    "                temperature=0.7,\n",
    "                top_p=0.9,\n",
    "                do_sample=True,\n",
    "            )\n",
    "\n",
    "        new_tokens = output_ids[0, inputs['input_ids'].shape[1]:]\n",
    "        raw_text = gen_tokenizer.decode(new_tokens, skip_special_tokens=True)\n",
    "\n",
    "        # Post-process: strip, take first line, remove quotes, truncate to 15 words\n",
    "        cleaned = raw_text.strip().split(\"\\n\")[0].strip()\n",
    "        cleaned = cleaned.strip('\"').strip(\"'\").strip()\n",
    "        cleaned = \" \".join(cleaned.split()[:15])\n",
    "        return cleaned\n",
    "\n",
    "    # Generate with checkpointing\n",
    "    surrogates = []\n",
    "    gen_ckpt_path = RESULTS_DIR / \"gen_checkpoint.json\"\n",
    "\n",
    "    if gen_ckpt_path.exists():\n",
    "        gen_ckpt = json.loads(gen_ckpt_path.read_text())\n",
    "        if gen_ckpt.get('n_total') == N_SAMPLES:\n",
    "            surrogates = gen_ckpt['surrogates']\n",
    "            print(f\"Resuming generation from {len(surrogates)}/{N_SAMPLES}\")\n",
    "\n",
    "    start_gen = len(surrogates)\n",
    "    t0 = time.time()\n",
    "\n",
    "    for i in tqdm(range(start_gen, N_SAMPLES), initial=start_gen, total=N_SAMPLES,\n",
    "                  desc=\"Generating\"):\n",
    "        s = samples[i]\n",
    "        entry = {'query': s['query']}\n",
    "\n",
    "        # Paraphrase: rephrase the query\n",
    "        torch.manual_seed(SEED + i * 10)\n",
    "        entry['paraphrase'] = generate_text(\n",
    "            f\"Query: {s['query']}\", PROMPT_PARAPHRASE\n",
    "        )\n",
    "\n",
    "        # Same-topic: question about same topic but different info\n",
    "        torch.manual_seed(SEED + i * 10 + 1)\n",
    "        words = s['passage'].split()[:150]\n",
    "        entry['same_topic'] = generate_text(\n",
    "            f\"Document:\\n{' '.join(words)}\", PROMPT_SAME_TOPIC\n",
    "        )\n",
    "\n",
    "        surrogates.append(entry)\n",
    "\n",
    "        if (i + 1) % 50 == 0 or i == N_SAMPLES - 1:\n",
    "            gen_ckpt = {'n_total': N_SAMPLES, 'surrogates': surrogates,\n",
    "                        'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')}\n",
    "            gen_ckpt_path.write_text(json.dumps(gen_ckpt))\n",
    "            elapsed = time.time() - t0\n",
    "            done = i - start_gen + 1\n",
    "            eta = (N_SAMPLES - i - 1) * elapsed / done if done > 0 else 0\n",
    "            tqdm.write(f\"  Gen checkpoint {i+1}/{N_SAMPLES} | {elapsed/60:.1f}m | ETA {eta/60:.1f}m\")\n",
    "\n",
    "    elapsed = time.time() - t0\n",
    "    print(f\"\\nGeneration complete: {len(surrogates)} samples in {elapsed/60:.1f} min\")\n",
    "\n",
    "    # Save final surrogates\n",
    "    SURROGATES_PATH.write_text(json.dumps(surrogates, indent=2))\n",
    "    print(f\"Saved surrogates to {SURROGATES_PATH}\")\n",
    "\n",
    "    # Free VRAM\n",
    "    print(\"Freeing generation model VRAM...\")\n",
    "    mem_before = torch.cuda.memory_allocated() / 1e9\n",
    "    del gen_model, gen_tokenizer\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    mem_after = torch.cuda.memory_allocated() / 1e9\n",
    "    print(f\"GPU memory: {mem_before:.2f} GB -> {mem_after:.2f} GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da67e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Inspect surrogates — examples, word counts, vocabulary overlap\n",
    "\n",
    "STOP_WORDS = {\n",
    "    'a', 'an', 'the', 'is', 'are', 'was', 'were', 'be', 'been', 'being',\n",
    "    'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could',\n",
    "    'should', 'may', 'might', 'can', 'shall', 'to', 'of', 'in', 'for',\n",
    "    'on', 'with', 'at', 'by', 'from', 'as', 'into', 'through', 'during',\n",
    "    'before', 'after', 'above', 'below', 'between', 'and', 'but', 'or',\n",
    "    'not', 'no', 'if', 'then', 'than', 'so', 'up', 'out', 'about',\n",
    "    'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those',\n",
    "    'it', 'its', 'i', 'me', 'my', 'we', 'our', 'you', 'your', 'he',\n",
    "    'him', 'his', 'she', 'her', 'they', 'them', 'their', 'how', 'when',\n",
    "    'where', 'why', 'much', 'many', 'some', 'any', 'all', 'each',\n",
    "    'does', 'also', 'just', 'more', 'most', 'very', 'too', 'only',\n",
    "}\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"SURROGATE INSPECTION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Show 5 examples\n",
    "for idx in range(5):\n",
    "    s = samples[idx]\n",
    "    surr = surrogates[idx]\n",
    "    print(f\"\\n--- Sample {idx} ---\")\n",
    "    print(f\"  Passage:     {s['passage'][:100]}...\")\n",
    "    print(f\"  Query:       {s['query']}\")\n",
    "    print(f\"  Paraphrase:  {surr['paraphrase']}\")\n",
    "    print(f\"  Same-topic:  {surr['same_topic']}\")\n",
    "\n",
    "# Word count distributions\n",
    "print(f\"\\n--- Word count distributions ---\")\n",
    "for label, key in [('oracle query', None), ('paraphrase', 'paraphrase'),\n",
    "                    ('same_topic', 'same_topic')]:\n",
    "    if key is None:\n",
    "        wc = np.array([len(s['query'].split()) for s in samples])\n",
    "    else:\n",
    "        wc = np.array([len(surr[key].split()) for surr in surrogates])\n",
    "    print(f\"  {label}: mean={wc.mean():.1f}, median={np.median(wc):.0f}, \"\n",
    "          f\"range=[{wc.min()}, {wc.max()}], std={wc.std():.1f}\")\n",
    "\n",
    "# Vocabulary overlap with document (content words only)\n",
    "print(f\"\\n--- Vocabulary overlap with document (content words) ---\")\n",
    "for label, get_text in [\n",
    "    ('oracle query', lambda i: samples[i]['query']),\n",
    "    ('paraphrase', lambda i: surrogates[i]['paraphrase']),\n",
    "    ('same_topic', lambda i: surrogates[i]['same_topic']),\n",
    "]:\n",
    "    overlaps = []\n",
    "    for i in range(N_SAMPLES):\n",
    "        doc_words = set(re.sub(r'[^\\w\\s]', '', samples[i]['passage'].lower()).split())\n",
    "        doc_content = doc_words - STOP_WORDS\n",
    "        text_words = set(re.sub(r'[^\\w\\s]', '', get_text(i).lower()).split())\n",
    "        text_content = text_words - STOP_WORDS\n",
    "        if len(text_content) > 0:\n",
    "            overlap = len(text_content & doc_content) / len(text_content)\n",
    "        else:\n",
    "            overlap = 0.0\n",
    "        overlaps.append(overlap)\n",
    "    overlaps = np.array(overlaps)\n",
    "    print(f\"  {label}: mean={overlaps.mean():.3f}, median={np.median(overlaps):.3f}\")\n",
    "\n",
    "# Vocabulary overlap with oracle query (content words only)\n",
    "print(f\"\\n--- Vocabulary overlap with oracle query (content words) ---\")\n",
    "for label, get_text in [\n",
    "    ('paraphrase', lambda i: surrogates[i]['paraphrase']),\n",
    "    ('same_topic', lambda i: surrogates[i]['same_topic']),\n",
    "]:\n",
    "    overlaps = []\n",
    "    for i in range(N_SAMPLES):\n",
    "        q_words = set(re.sub(r'[^\\w\\s]', '', samples[i]['query'].lower()).split())\n",
    "        q_content = q_words - STOP_WORDS\n",
    "        text_words = set(re.sub(r'[^\\w\\s]', '', get_text(i).lower()).split())\n",
    "        text_content = text_words - STOP_WORDS\n",
    "        if len(text_content) > 0 and len(q_content) > 0:\n",
    "            overlap = len(text_content & q_content) / len(text_content)\n",
    "        else:\n",
    "            overlap = 0.0\n",
    "        overlaps.append(overlap)\n",
    "    overlaps = np.array(overlaps)\n",
    "    print(f\"  {label}: mean={overlaps.mean():.3f}, median={np.median(overlaps):.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbf16a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Phase 2 — Load T5Gemma and define scoring helpers\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv())\n",
    "HF_TOKEN = os.environ.get(\"HF_TOKEN\")\n",
    "\n",
    "from transformers import AutoProcessor, AutoModelForSeq2SeqLM\n",
    "\n",
    "print(f\"Loading {T5GEMMA_NAME}...\")\n",
    "processor = AutoProcessor.from_pretrained(T5GEMMA_NAME, token=HF_TOKEN)\n",
    "tokenizer = processor.tokenizer\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    T5GEMMA_NAME, device_map=\"auto\", torch_dtype=torch.bfloat16, token=HF_TOKEN,\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "DEVICE = next(model.parameters()).device\n",
    "print(f\"Model loaded. dtype={next(model.parameters()).dtype}\")\n",
    "print(f\"GPU memory: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "\n",
    "\n",
    "def score_nll(encoder_text, answer_text, prefix_token_count=0, truncate=False):\n",
    "    # Score NLL of answer given encoder text, with optional prefix truncation.\n",
    "    enc_ids = tokenizer(encoder_text, return_tensors=\"pt\",\n",
    "                        add_special_tokens=True, truncation=True,\n",
    "                        max_length=2048).input_ids.to(DEVICE)\n",
    "    total_enc_len = enc_ids.shape[1]\n",
    "    enc_mask = torch.ones(1, total_enc_len, device=DEVICE, dtype=torch.long)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        encoder_outputs = model.get_encoder()(\n",
    "            input_ids=enc_ids, attention_mask=enc_mask\n",
    "        )\n",
    "\n",
    "    if truncate and prefix_token_count > 0:\n",
    "        cross_attn_mask = torch.ones(1, total_enc_len, device=DEVICE, dtype=torch.long)\n",
    "        cross_attn_mask[:, :prefix_token_count] = 0\n",
    "    else:\n",
    "        cross_attn_mask = torch.ones(1, total_enc_len, device=DEVICE, dtype=torch.long)\n",
    "\n",
    "    ans_ids = tokenizer(answer_text, return_tensors=\"pt\",\n",
    "                        add_special_tokens=False, truncation=True,\n",
    "                        max_length=256).input_ids.to(DEVICE)\n",
    "    if ans_ids.shape[1] == 0:\n",
    "        return 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            encoder_outputs=encoder_outputs,\n",
    "            attention_mask=cross_attn_mask,\n",
    "            labels=ans_ids,\n",
    "        )\n",
    "\n",
    "    logits = outputs.logits\n",
    "    log_probs = F.log_softmax(logits, dim=-1)\n",
    "    token_log_probs = log_probs[0].gather(1, ans_ids[0].unsqueeze(1)).squeeze(1)\n",
    "    mean_nll = -token_log_probs.mean().item()\n",
    "\n",
    "    del encoder_outputs, outputs, logits, log_probs\n",
    "    return mean_nll\n",
    "\n",
    "\n",
    "def count_prefix_tokens(prefix_text, document_text):\n",
    "    # Count how many tokens the prefix occupies in [prefix + newline + document].\n",
    "    full_text = prefix_text + \"\\n\" + document_text\n",
    "    full_ids = tokenizer(full_text, add_special_tokens=True, truncation=True,\n",
    "                         max_length=2048).input_ids\n",
    "    doc_ids = tokenizer(document_text, add_special_tokens=True, truncation=True,\n",
    "                        max_length=2048).input_ids\n",
    "    return len(full_ids) - len(doc_ids)\n",
    "\n",
    "\n",
    "print(\"Helpers defined.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a01569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Generate all 7 scoring conditions per sample\n",
    "\n",
    "for i, s in enumerate(samples):\n",
    "    surr = surrogates[i]\n",
    "    query = s['query']\n",
    "    passage = s['passage']\n",
    "    query_words_list = query.split()\n",
    "    n_query_words = len(query_words_list)\n",
    "\n",
    "    # Unrelated query: query from a distant sample\n",
    "    unrelated_idx = (i + N_SAMPLES // 2) % len(samples)\n",
    "    unrelated_query = samples[unrelated_idx]['query']\n",
    "    # Length-match to oracle: truncate or pad to same word count\n",
    "    unrelated_words = unrelated_query.split()[:n_query_words]\n",
    "    if len(unrelated_words) < n_query_words:\n",
    "        # Pad with words from another sample if too short\n",
    "        pad_idx = (i + N_SAMPLES // 3) % len(samples)\n",
    "        pad_words = samples[pad_idx]['query'].split()\n",
    "        unrelated_words = (unrelated_words + pad_words)[:n_query_words]\n",
    "    s['unrelated_query'] = \" \".join(unrelated_words)\n",
    "\n",
    "    # Scrambled oracle: same words, random order\n",
    "    rng = np.random.RandomState(SEED + i)\n",
    "    shuffled = list(query_words_list)\n",
    "    rng.shuffle(shuffled)\n",
    "    s['scrambled_oracle'] = \" \".join(shuffled)\n",
    "\n",
    "    # Random matched: words from unrelated passage, same word count as oracle\n",
    "    other_idx = (i + N_SAMPLES // 2) % len(samples)\n",
    "    other_words = samples[other_idx]['passage'].split()\n",
    "    s['random_matched'] = \" \".join(other_words[:n_query_words])\n",
    "\n",
    "    # Paraphrase and same-topic from LLM generation\n",
    "    s['paraphrase'] = surr['paraphrase']\n",
    "    s['same_topic'] = surr['same_topic']\n",
    "\n",
    "    # Oracle (just the query)\n",
    "    s['oracle'] = query\n",
    "\n",
    "# Define all scoring conditions\n",
    "COND_NAMES = [\n",
    "    'bare',\n",
    "    'oracle_trunc',\n",
    "    'paraphrase_trunc',\n",
    "    'same_topic_trunc',\n",
    "    'unrelated_query_trunc',\n",
    "    'scrambled_oracle_trunc',\n",
    "    'random_matched_trunc',\n",
    "]\n",
    "\n",
    "# Semantic relevance ordering (for gradient analysis)\n",
    "RELEVANCE_ORDER = [\n",
    "    ('random_matched_trunc', 'Random matched', 0, 'none (structural baseline)'),\n",
    "    ('scrambled_oracle_trunc', 'Scrambled oracle', 1, 'vocabulary only'),\n",
    "    ('unrelated_query_trunc', 'Unrelated query', 2, 'low (wrong topic)'),\n",
    "    ('same_topic_trunc', 'Same topic', 3, 'medium (right topic)'),\n",
    "    ('paraphrase_trunc', 'Paraphrase', 4, 'high (same meaning)'),\n",
    "    ('oracle_trunc', 'Oracle', 5, 'maximal (exact query)'),\n",
    "]\n",
    "\n",
    "print(f\"Conditions ({len(COND_NAMES)}):\")\n",
    "for c in COND_NAMES:\n",
    "    print(f\"  {c}\")\n",
    "\n",
    "# Show example\n",
    "ex = samples[0]\n",
    "print(f\"\\nExample (sample 0):\")\n",
    "print(f\"  Query:   {ex['query'][:80]}\")\n",
    "print(f\"  Answer:  {ex['answer'][:80]}\")\n",
    "print(f\"  Passage: {ex['passage'][:80]}...\")\n",
    "print()\n",
    "for c in COND_NAMES:\n",
    "    if c == 'bare':\n",
    "        print(f\"  {c:<30}: [document only]\")\n",
    "    else:\n",
    "        key = c.replace('_trunc', '')\n",
    "        text = ex[key]\n",
    "        ptoks = count_prefix_tokens(text, ex['passage'])\n",
    "        print(f\"  {c:<30} ({ptoks:>3} toks): {str(text)[:55]}\")\n",
    "\n",
    "# Token count stats across first 50 samples\n",
    "print(f\"\\nPrefix token counts (first 50 samples):\")\n",
    "for c in COND_NAMES:\n",
    "    if c == 'bare':\n",
    "        continue\n",
    "    key = c.replace('_trunc', '')\n",
    "    toks = [count_prefix_tokens(s[key], s['passage']) for s in samples[:50]]\n",
    "    print(f\"  {c:<30} mean={np.mean(toks):.1f}, range=[{min(toks)}, {max(toks)}]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40cf21d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Scoring loop with checkpointing\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"SCORING ALL CONDITIONS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "results = []\n",
    "start_idx = 0\n",
    "\n",
    "if CHECKPOINT_PATH.exists():\n",
    "    ckpt = json.loads(CHECKPOINT_PATH.read_text())\n",
    "    if ckpt.get('n_total') == N_SAMPLES and len(ckpt.get('results', [])) > 0:\n",
    "        saved_queries = [r['query'][:50] for r in ckpt['results']]\n",
    "        current_queries = [s['query'][:50] for s in samples[:len(saved_queries)]]\n",
    "        if saved_queries == current_queries:\n",
    "            results = ckpt['results']\n",
    "            start_idx = len(results)\n",
    "            print(f\"Resuming from checkpoint: {start_idx}/{N_SAMPLES}\")\n",
    "\n",
    "if start_idx == 0:\n",
    "    print(f\"Starting fresh: {len(COND_NAMES)} conditions x {N_SAMPLES} samples \"\n",
    "          f\"= {len(COND_NAMES) * N_SAMPLES} scorings\")\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "for i in tqdm(range(start_idx, N_SAMPLES), initial=start_idx, total=N_SAMPLES,\n",
    "              desc=\"Scoring\"):\n",
    "    s = samples[i]\n",
    "    result = {\n",
    "        'query': s['query'],\n",
    "        'answer': s['answer'],\n",
    "        'passage_words': s['word_count'],\n",
    "    }\n",
    "\n",
    "    for cond in COND_NAMES:\n",
    "        if cond == 'bare':\n",
    "            nll = score_nll(s['passage'], s['answer'])\n",
    "            result['nll_bare'] = nll\n",
    "        else:\n",
    "            key = cond.replace('_trunc', '')\n",
    "            prefix = s[key]\n",
    "            enc_text = prefix + \"\\n\" + s['passage']\n",
    "            ptoks = count_prefix_tokens(prefix, s['passage'])\n",
    "            nll = score_nll(enc_text, s['answer'], ptoks, truncate=True)\n",
    "            result[f'nll_{cond}'] = nll\n",
    "            result[f'ptoks_{cond}'] = ptoks\n",
    "\n",
    "    results.append(result)\n",
    "\n",
    "    if (i + 1) % 20 == 0 or i == N_SAMPLES - 1:\n",
    "        ckpt = {\n",
    "            'n_total': N_SAMPLES,\n",
    "            'results': results,\n",
    "            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        }\n",
    "        CHECKPOINT_PATH.write_text(json.dumps(ckpt))\n",
    "        elapsed = time.time() - t0\n",
    "        done = i - start_idx + 1\n",
    "        eta = (N_SAMPLES - i - 1) * elapsed / done if done > 0 else 0\n",
    "        tqdm.write(f\"  Checkpoint {i+1}/{N_SAMPLES} | {elapsed/60:.1f}m | ETA {eta/60:.1f}m\")\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "elapsed = time.time() - t0\n",
    "print(f\"\\nScoring complete: {len(results)} samples, \"\n",
    "      f\"{len(COND_NAMES)} conditions in {elapsed/60:.1f} min\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c551d4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Part 1 — Standard Condition Table\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PART 1: STANDARD CONDITION TABLE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "bare_nlls = np.array([r['nll_bare'] for r in results])\n",
    "oracle_nlls = np.array([r['nll_oracle_trunc'] for r in results])\n",
    "oracle_benefit = bare_nlls - oracle_nlls\n",
    "oracle_d = cohens_d(oracle_benefit)\n",
    "\n",
    "all_conds = [\n",
    "    ('oracle_trunc', 'Oracle (real query)'),\n",
    "    ('paraphrase_trunc', 'Paraphrase (same meaning)'),\n",
    "    ('same_topic_trunc', 'Same topic (diff question)'),\n",
    "    ('unrelated_query_trunc', 'Unrelated query (wrong topic)'),\n",
    "    ('scrambled_oracle_trunc', 'Scrambled oracle (vocab only)'),\n",
    "    ('random_matched_trunc', 'Random matched (structural)'),\n",
    "]\n",
    "\n",
    "# Bonferroni threshold\n",
    "alpha_bonf = 0.05 / len(all_conds)\n",
    "\n",
    "print(f\"\\n{'Condition':<38} {'NLL':>8} {'Delta':>8} {'d':>8} \"\n",
    "      f\"{'Win%':>7} {'%Orc':>6} {'p':>12} {'sig':>5}\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "for cond, desc in all_conds:\n",
    "    nlls = np.array([r[f'nll_{cond}'] for r in results])\n",
    "    benefit = bare_nlls - nlls\n",
    "    d = cohens_d(benefit)\n",
    "    delta = benefit.mean()\n",
    "    win = 100 * np.mean(benefit > 0)\n",
    "    pct = d / oracle_d * 100 if oracle_d > 0 else 0\n",
    "    _, p = stats.ttest_1samp(benefit, 0)\n",
    "    sig = '***' if p < alpha_bonf / 10 else '**' if p < alpha_bonf else '*' if p < 0.05 else 'ns'\n",
    "    print(f\"  {desc:<36} {nlls.mean():>8.4f} {delta:>+8.4f} {d:>+8.3f} \"\n",
    "          f\"{win:>6.1f}% {pct:>5.0f}% {p:>12.2e} {sig}\")\n",
    "\n",
    "print(f\"\\n  bare (lower bound): {bare_nlls.mean():.4f}\")\n",
    "print(f\"  Bonferroni threshold: alpha={alpha_bonf:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a36fcec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Part 2 — Semantic Gradient\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PART 2: SEMANTIC GRADIENT\")\n",
    "print(\"=\" * 70)\n",
    "print(\"Does NLL improvement increase monotonically with semantic relevance?\\n\")\n",
    "\n",
    "# Compute delta = bare_nll - cond_nll for each condition\n",
    "# Use random_matched as structural baseline: semantic_delta = delta - random_delta\n",
    "random_nlls = np.array([r['nll_random_matched_trunc'] for r in results])\n",
    "random_benefit = bare_nlls - random_nlls  # structural benefit\n",
    "\n",
    "print(\"--- Raw delta (benefit over bare) ---\")\n",
    "print(f\"  {'Condition':<30} {'Relevance':>10} {'Mean delta':>12} {'d':>8} {'%Oracle':>8}\")\n",
    "print(f\"  {'-'*75}\")\n",
    "\n",
    "gradient_ds = []\n",
    "gradient_labels = []\n",
    "\n",
    "for cond, desc, rank, rel_desc in RELEVANCE_ORDER:\n",
    "    nlls = np.array([r[f'nll_{cond}'] for r in results])\n",
    "    benefit = bare_nlls - nlls\n",
    "    d = cohens_d(benefit)\n",
    "    pct = d / oracle_d * 100 if oracle_d > 0 else 0\n",
    "    print(f\"  {desc:<30} {rank:>10} {benefit.mean():>+12.4f} {d:>+8.3f} {pct:>7.0f}%\")\n",
    "    gradient_ds.append(d)\n",
    "    gradient_labels.append(desc)\n",
    "\n",
    "# Semantic delta (above structural baseline)\n",
    "print(f\"\\n--- Semantic delta (above random_matched baseline) ---\")\n",
    "print(f\"  {'Condition':<30} {'Semantic d':>12} {'p vs random':>14} {'sig':>5}\")\n",
    "print(f\"  {'-'*65}\")\n",
    "\n",
    "semantic_ds = []\n",
    "\n",
    "for cond, desc, rank, rel_desc in RELEVANCE_ORDER:\n",
    "    nlls = np.array([r[f'nll_{cond}'] for r in results])\n",
    "    diff = random_nlls - nlls  # positive = condition is better than random\n",
    "    d = cohens_d(diff)\n",
    "    _, p = stats.ttest_1samp(diff, 0)\n",
    "    sig = '***' if p < 0.001 else '**' if p < 0.01 else '*' if p < 0.05 else 'ns'\n",
    "    print(f\"  {desc:<30} {d:>+12.3f} {p:>14.2e} {sig}\")\n",
    "    semantic_ds.append(d)\n",
    "\n",
    "# Monotonicity test (Spearman)\n",
    "ranks = [rank for _, _, rank, _ in RELEVANCE_ORDER]\n",
    "rho, p_mono = stats.spearmanr(ranks, gradient_ds)\n",
    "sig_mono = '***' if p_mono < 0.001 else '**' if p_mono < 0.01 else '*' if p_mono < 0.05 else 'ns'\n",
    "print(f\"\\n--- Monotonicity test ---\")\n",
    "print(f\"  Spearman rho (relevance rank vs Cohen's d): rho={rho:+.3f}, p={p_mono:.4f} {sig_mono}\")\n",
    "\n",
    "rho_sem, p_sem = stats.spearmanr(ranks, semantic_ds)\n",
    "sig_sem = '***' if p_sem < 0.001 else '**' if p_sem < 0.01 else '*' if p_sem < 0.05 else 'ns'\n",
    "print(f\"  Spearman rho (relevance rank vs semantic d): rho={rho_sem:+.3f}, p={p_sem:.4f} {sig_sem}\")\n",
    "\n",
    "if rho > 0.8 and p_mono < 0.05:\n",
    "    print(f\"  --> MONOTONIC: clear semantic gradient (rho={rho:+.3f})\")\n",
    "elif rho > 0.5:\n",
    "    print(f\"  --> PARTIAL: imperfect gradient (rho={rho:+.3f})\")\n",
    "else:\n",
    "    print(f\"  --> FLAT: no clear gradient (rho={rho:+.3f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fac49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Part 3 — Fine-Grained Decomposition Chain\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PART 3: FINE-GRAINED DECOMPOSITION CHAIN\")\n",
    "print(\"=\" * 70)\n",
    "print(\"bare -> random_matched -> scrambled_oracle -> unrelated_query -> \"\n",
    "      \"same_topic -> paraphrase -> oracle\\n\")\n",
    "print(\"Each step adds one aspect of semantic relevance:\\n\")\n",
    "print(\"  bare -> random_matched:     STRUCTURE (any prefix helps)\")\n",
    "print(\"  random_matched -> scrambled: VOCABULARY (right words, wrong order)\")\n",
    "print(\"  scrambled -> unrelated:      QUERY SYNTAX (real query structure, wrong topic)\")\n",
    "print(\"  unrelated -> same_topic:     TOPIC RELEVANCE (right topic, different question)\")\n",
    "print(\"  same_topic -> paraphrase:    SEMANTIC PRECISION (same meaning, diff words)\")\n",
    "print(\"  paraphrase -> oracle:        EXACT MATCH (same words + meaning)\")\n",
    "print()\n",
    "\n",
    "scrambled_nlls = np.array([r['nll_scrambled_oracle_trunc'] for r in results])\n",
    "unrelated_nlls = np.array([r['nll_unrelated_query_trunc'] for r in results])\n",
    "same_topic_nlls = np.array([r['nll_same_topic_trunc'] for r in results])\n",
    "paraphrase_nlls = np.array([r['nll_paraphrase_trunc'] for r in results])\n",
    "\n",
    "chain = [\n",
    "    ('Structure', bare_nlls - random_nlls),\n",
    "    ('Vocabulary', random_nlls - scrambled_nlls),\n",
    "    ('Query syntax', scrambled_nlls - unrelated_nlls),\n",
    "    ('Topic relevance', unrelated_nlls - same_topic_nlls),\n",
    "    ('Semantic precision', same_topic_nlls - paraphrase_nlls),\n",
    "    ('Exact match', paraphrase_nlls - oracle_nlls),\n",
    "]\n",
    "\n",
    "total = bare_nlls - oracle_nlls\n",
    "total_mean = total.mean()\n",
    "\n",
    "print(f\"  {'Component':<22} {'Delta':>10} {'%total':>8} {'d':>8} {'p':>12} {'sig':>5}\")\n",
    "print(f\"  {'-'*70}\")\n",
    "\n",
    "chain_pcts = {}\n",
    "for label, comp in chain:\n",
    "    mu = comp.mean()\n",
    "    pct = mu / total_mean * 100 if total_mean != 0 else 0\n",
    "    d = cohens_d(comp)\n",
    "    _, p = stats.ttest_1samp(comp, 0)\n",
    "    sig = '***' if p < 0.001 else '**' if p < 0.01 else '*' if p < 0.05 else 'ns'\n",
    "    print(f\"  {label:<22} {mu:>+10.4f} {pct:>7.1f}% {d:>+8.3f} {p:>12.2e} {sig}\")\n",
    "    chain_pcts[label] = pct\n",
    "\n",
    "print(f\"  {'TOTAL':<22} {total_mean:>+10.4f} {'100.0%':>8}\")\n",
    "residual = total_mean - sum(comp.mean() for _, comp in chain)\n",
    "print(f\"\\n  Decomposition residual: {residual:.6f} (should be ~0)\")\n",
    "\n",
    "# Grouped summary: structure vs all semantic components\n",
    "struct_pct = chain_pcts['Structure']\n",
    "semantic_pct = 100 - struct_pct\n",
    "print(f\"\\n--- Grouped Summary ---\")\n",
    "print(f\"  Structure:                {struct_pct:>6.1f}%\")\n",
    "print(f\"  All semantic components:  {semantic_pct:>6.1f}%\")\n",
    "print(f\"    Vocabulary:             {chain_pcts['Vocabulary']:>6.1f}%\")\n",
    "print(f\"    Query syntax:           {chain_pcts['Query syntax']:>6.1f}%\")\n",
    "print(f\"    Topic relevance:        {chain_pcts['Topic relevance']:>6.1f}%\")\n",
    "print(f\"    Semantic precision:     {chain_pcts['Semantic precision']:>6.1f}%\")\n",
    "print(f\"    Exact match:            {chain_pcts['Exact match']:>6.1f}%\")\n",
    "\n",
    "# Compare with Exp 2B\n",
    "print(f\"\\n--- Comparison with Exp 2B (3-way decomposition) ---\")\n",
    "print(f\"  Exp 2B: Structure=84.7%, Vocabulary=5.5%, Semantics=9.7%\")\n",
    "vocab_total = chain_pcts['Vocabulary']\n",
    "sem_total = (chain_pcts['Query syntax'] + chain_pcts['Topic relevance'] +\n",
    "             chain_pcts['Semantic precision'] + chain_pcts['Exact match'])\n",
    "print(f\"  Exp 12: Structure={struct_pct:.1f}%, Vocabulary={vocab_total:.1f}%, \"\n",
    "      f\"Semantics (broad)={sem_total:.1f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6fdebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Part 4 — Hardness Interaction\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PART 4: HARDNESS INTERACTION\")\n",
    "print(\"=\" * 70)\n",
    "print(\"Does the semantic gradient steepen for harder samples?\\n\")\n",
    "\n",
    "quintile_bounds = np.percentile(bare_nlls, [20, 40, 60, 80])\n",
    "quintiles = np.digitize(bare_nlls, quintile_bounds)\n",
    "q_labels = ['Q1 easy', 'Q2', 'Q3', 'Q4', 'Q5 hard']\n",
    "\n",
    "# Semantic delta (cond_nll - random_nll) by quintile for each condition\n",
    "print(\"--- Semantic delta by quintile (above random baseline) ---\")\n",
    "print(f\"  {'Quintile':<12} {'Bare NLL':>10}\", end=\"\")\n",
    "for _, desc, _, _ in RELEVANCE_ORDER[1:]:  # skip random itself\n",
    "    print(f\"  {desc:>12}\", end=\"\")\n",
    "print()\n",
    "print(f\"  {'-'*(14 + 14 * (len(RELEVANCE_ORDER) - 1))}\")\n",
    "\n",
    "for q in range(5):\n",
    "    mask = quintiles == q\n",
    "    row = f\"  {q_labels[q]:<12} {bare_nlls[mask].mean():>10.3f}\"\n",
    "    for cond, desc, _, _ in RELEVANCE_ORDER[1:]:\n",
    "        nlls_c = np.array([r[f'nll_{cond}'] for r in results])[mask]\n",
    "        rand_c = random_nlls[mask]\n",
    "        sem_delta = cohens_d(rand_c - nlls_c)\n",
    "        row += f\"  {sem_delta:>+12.3f}\"\n",
    "    print(row)\n",
    "\n",
    "# Decomposition chain by quintile\n",
    "print(f\"\\n--- Structure vs semantic % by quintile ---\")\n",
    "print(f\"  {'Quintile':<12} {'Struct%':>9} {'Vocab%':>8} {'Syntax%':>9} \"\n",
    "      f\"{'Topic%':>8} {'Precis%':>9} {'Exact%':>8}\")\n",
    "print(f\"  {'-'*65}\")\n",
    "\n",
    "for q in range(5):\n",
    "    mask = quintiles == q\n",
    "    total_q = (bare_nlls[mask] - oracle_nlls[mask]).mean()\n",
    "    if total_q > 0:\n",
    "        s_pct = (bare_nlls[mask] - random_nlls[mask]).mean() / total_q * 100\n",
    "        v_pct = (random_nlls[mask] - scrambled_nlls[mask]).mean() / total_q * 100\n",
    "        syn_pct = (scrambled_nlls[mask] - unrelated_nlls[mask]).mean() / total_q * 100\n",
    "        top_pct = (unrelated_nlls[mask] - same_topic_nlls[mask]).mean() / total_q * 100\n",
    "        pre_pct = (same_topic_nlls[mask] - paraphrase_nlls[mask]).mean() / total_q * 100\n",
    "        ex_pct = (paraphrase_nlls[mask] - oracle_nlls[mask]).mean() / total_q * 100\n",
    "    else:\n",
    "        s_pct = v_pct = syn_pct = top_pct = pre_pct = ex_pct = 0\n",
    "    print(f\"  {q_labels[q]:<12} {s_pct:>8.1f}% {v_pct:>7.1f}% {syn_pct:>8.1f}% \"\n",
    "          f\"{top_pct:>7.1f}% {pre_pct:>8.1f}% {ex_pct:>7.1f}%\")\n",
    "\n",
    "# Correlation: hardness vs semantic delta for each condition\n",
    "print(f\"\\n--- Correlations: hardness vs semantic delta ---\")\n",
    "for cond, desc, rank, _ in RELEVANCE_ORDER[1:]:\n",
    "    nlls_c = np.array([r[f'nll_{cond}'] for r in results])\n",
    "    sem_delta = random_nlls - nlls_c\n",
    "    r_val, p_val = stats.pearsonr(bare_nlls, sem_delta)\n",
    "    sig = '***' if p_val < 0.001 else '**' if p_val < 0.01 else '*' if p_val < 0.05 else 'ns'\n",
    "    print(f\"  {desc:<25} r={r_val:+.3f} (p={p_val:.2e}) {sig}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c99f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13: Synthesis + Save\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"SYNTHESIS: GRADED SEMANTIC RELEVANCE RESULTS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# 1. Gradient summary\n",
    "print(f\"\\n1. SEMANTIC GRADIENT (d, % oracle):\")\n",
    "for cond, desc, rank, rel_desc in RELEVANCE_ORDER:\n",
    "    nlls = np.array([r[f'nll_{cond}'] for r in results])\n",
    "    d = cohens_d(bare_nlls - nlls)\n",
    "    pct = d / oracle_d * 100 if oracle_d > 0 else 0\n",
    "    print(f\"   [{rank}] {desc:<25} d={d:>+.3f} ({pct:>5.1f}% oracle) — {rel_desc}\")\n",
    "\n",
    "# 2. Monotonicity\n",
    "print(f\"\\n2. MONOTONICITY:\")\n",
    "print(f\"   Spearman rho (raw d): {rho:+.3f} (p={p_mono:.4f})\")\n",
    "print(f\"   Spearman rho (semantic d): {rho_sem:+.3f} (p={p_sem:.4f})\")\n",
    "\n",
    "# 3. Decomposition chain\n",
    "print(f\"\\n3. DECOMPOSITION CHAIN:\")\n",
    "for label, pct in chain_pcts.items():\n",
    "    print(f\"   {label:<22} {pct:>6.1f}%\")\n",
    "\n",
    "# 4. Conclusions\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"CONCLUSIONS:\")\n",
    "\n",
    "if rho > 0.8 and p_mono < 0.05:\n",
    "    print(f\"  1. CLEAR SEMANTIC GRADIENT: increasing relevance monotonically improves NLL\")\n",
    "    print(f\"     (Spearman rho={rho:+.3f}, p={p_mono:.4f})\")\n",
    "elif rho > 0.5:\n",
    "    print(f\"  1. PARTIAL GRADIENT: general trend but imperfect monotonicity\")\n",
    "    print(f\"     (Spearman rho={rho:+.3f}, p={p_mono:.4f})\")\n",
    "else:\n",
    "    print(f\"  1. NO CLEAR GRADIENT: semantic relevance does not reliably predict benefit\")\n",
    "    print(f\"     (Spearman rho={rho:+.3f}, p={p_mono:.4f})\")\n",
    "\n",
    "if struct_pct > 75:\n",
    "    print(f\"  2. Structure still dominates ({struct_pct:.0f}%), consistent with Exp 2B\")\n",
    "elif struct_pct > 50:\n",
    "    print(f\"  2. Structure is largest component ({struct_pct:.0f}%) but semantic components \"\n",
    "          f\"are substantial ({100-struct_pct:.0f}%)\")\n",
    "else:\n",
    "    print(f\"  2. Semantic components dominate ({100-struct_pct:.0f}%) — \"\n",
    "          f\"the fine-grained gradient reveals structure is not dominant\")\n",
    "\n",
    "# Which semantic component is largest?\n",
    "sem_components = [(k, v) for k, v in chain_pcts.items() if k != 'Structure']\n",
    "largest_sem = max(sem_components, key=lambda x: abs(x[1]))\n",
    "print(f\"  3. Largest semantic component: {largest_sem[0]} ({largest_sem[1]:+.1f}%)\")\n",
    "\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "# Save results\n",
    "final_results = {\n",
    "    'experiment': 'exp12_semantic_gradient',\n",
    "    'generation_model': GEMMA_IT_NAME,\n",
    "    'scoring_model': T5GEMMA_NAME,\n",
    "    'dataset': 'ms_marco_v1.1',\n",
    "    'n_samples': N_SAMPLES,\n",
    "    'seed': SEED,\n",
    "    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'conditions': {},\n",
    "    'gradient': {\n",
    "        'spearman_rho_raw': float(rho),\n",
    "        'spearman_p_raw': float(p_mono),\n",
    "        'spearman_rho_semantic': float(rho_sem),\n",
    "        'spearman_p_semantic': float(p_sem),\n",
    "    },\n",
    "    'decomposition_chain': {k: float(v) for k, v in chain_pcts.items()},\n",
    "    'structure_pct': float(struct_pct),\n",
    "}\n",
    "\n",
    "# Add per-condition results\n",
    "for cond, desc, rank, rel_desc in RELEVANCE_ORDER:\n",
    "    nlls = np.array([r[f'nll_{cond}'] for r in results])\n",
    "    benefit = bare_nlls - nlls\n",
    "    d = cohens_d(benefit)\n",
    "    _, p = stats.ttest_1samp(benefit, 0)\n",
    "    # Semantic delta vs random\n",
    "    sem_diff = random_nlls - nlls\n",
    "    sem_d = cohens_d(sem_diff)\n",
    "    _, sem_p = stats.ttest_1samp(sem_diff, 0)\n",
    "    final_results['conditions'][cond] = {\n",
    "        'description': desc,\n",
    "        'relevance_rank': rank,\n",
    "        'relevance_label': rel_desc,\n",
    "        'd': float(d),\n",
    "        'mean_nll': float(nlls.mean()),\n",
    "        'mean_delta': float(benefit.mean()),\n",
    "        'pct_oracle': float(d / oracle_d * 100) if oracle_d > 0 else 0,\n",
    "        'p': float(p),\n",
    "        'semantic_d': float(sem_d),\n",
    "        'semantic_p': float(sem_p),\n",
    "    }\n",
    "\n",
    "with open(RESULTS_DIR / 'results.json', 'w') as f:\n",
    "    json.dump(final_results, f, indent=2)\n",
    "print(f\"\\nResults saved to {RESULTS_DIR / 'results.json'}\")\n",
    "\n",
    "# Cleanup\n",
    "print(f\"\\nCleaning up GPU memory...\")\n",
    "mem_before = torch.cuda.memory_allocated() / 1e9\n",
    "del model, processor, tokenizer\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "mem_after = torch.cuda.memory_allocated() / 1e9\n",
    "print(f\"GPU memory: {mem_before:.2f} GB -> {mem_after:.2f} GB\")\n",
    "print(\"Done!\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
