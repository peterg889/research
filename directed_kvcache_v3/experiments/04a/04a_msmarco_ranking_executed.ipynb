{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03bef349",
   "metadata": {},
   "source": [
    "# Experiment 04A: MS MARCO Ranking with Surrogate Co-Encoding## Does surrogate priming create differential ranking signal?### The critical questionExps 01-03B proved that co-encoding a surrogate with a document improves the document'srepresentation (measured by NLL on a gold answer). But NLL improvement alone doesn'tguarantee ranking improvement. The question:> Does priming help the **relevant** passage MORE than the **irrelevant** passages?If yes, NLL-based ranking improves. If no (priming helps all passages equally), theabsolute NLL drops but relative ordering doesn't change.### v2 ranking history (all failed)| Exp | Method | Result ||-----|--------|--------|| 14 | Causal priming + ranking | AUC flat || 15 | Values-only priming + ranking | AUC flat || 22 | T5Gemma answer-likelihood | AUC=0.828, priming +0.001 || 23 | Contrastive ranking | Failed || 28 | Hinge loss ranking | Failed || 31 | Query-likelihood ranking | QL AUC=0.578 (near chance) |**Why v2 failed**: Decoder-only value contamination was document-INDEPENDENT. The KV cachemodification lowered NLL equally for relevant and irrelevant passages.**Why v3 might succeed**: Bidirectional encoder creates document-SPECIFIC representations.The surrogate tokens interact with document tokens differently depending on document content.### Scoring`NLL(answer | encode([condition + passage]))` -- same as Exps 01-03, applied to ALLcandidate passages per query.### Conditions (6)| # | Condition | Encoder input | Purpose ||---|-----------|--------------|---------|| 1 | bare | passage only | Lower bound || 2 | oracle\\_trunc | real query + passage | Upper bound || 3 | surr\\_template\\_trunc | \"What is [keyword]?\" + passage | Best doc-derived (Exp 02) || 4 | surr\\_doc\\_trunc | TF keywords + passage | Doc-derived control || 5 | random\\_trunc | unrelated text + passage | Structural control || 6 | static\\_fact\\_trunc | \"What are the key facts?\" + passage | Content-agnostic |### Metrics- **AUC** (binary: selected vs not-selected) per query- **MRR@3** (reciprocal rank of relevant passage in top-3)- **Hit@1**, **Hit@3** per query- **Differential signal**: delta\\_relevant vs delta\\_irrelevant### Statistical testing- Wilcoxon signed-rank for paired metric differences (condition vs bare)- Cohen's d on per-query metric differences- Bonferroni: 5 comparisons (5 non-bare conditions)### N=400 queries (~4000 passage scorings per condition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "916a1ba4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T14:05:21.860301Z",
     "iopub.status.busy": "2026-02-18T14:05:21.859939Z",
     "iopub.status.idle": "2026-02-18T14:05:26.952821Z",
     "shell.execute_reply": "2026-02-18T14:05:26.951654Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exp 04A: MS MARCO Ranking with Surrogate Co-Encoding\n",
      "Model: google/t5gemma-2-4b-4b\n",
      "N queries: 400\n",
      "Bonferroni comparisons: 5\n",
      "CUDA: NVIDIA L4\n",
      "GPU memory: 23.6 GB\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Setup\n",
    "import os\n",
    "os.umask(0o000)\n",
    "\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "import gc\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "from collections import Counter, defaultdict\n",
    "from scipy import stats\n",
    "from scipy.stats import wilcoxon\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "sys.path.insert(0, \"../..\")\n",
    "\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "RESULTS_DIR = Path(\"../../results/exp04a\")\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CHECKPOINT_PATH = RESULTS_DIR / \"checkpoint.json\"\n",
    "\n",
    "N_SAMPLES = 400   # queries\n",
    "MODEL_NAME = \"google/t5gemma-2-4b-4b\"\n",
    "N_BONFERRONI = 5  # 5 non-bare conditions\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv())\n",
    "HF_TOKEN = os.environ.get(\"HF_TOKEN\")\n",
    "\n",
    "print(f\"Exp 04A: MS MARCO Ranking with Surrogate Co-Encoding\")\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"N queries: {N_SAMPLES}\")\n",
    "print(f\"Bonferroni comparisons: {N_BONFERRONI}\")\n",
    "print(f\"CUDA: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc682d7e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T14:05:26.957120Z",
     "iopub.status.busy": "2026-02-18T14:05:26.956233Z",
     "iopub.status.idle": "2026-02-18T14:06:38.699954Z",
     "shell.execute_reply": "2026-02-18T14:06:38.698825Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading google/t5gemma-2-4b-4b...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e790da46140f43ebb8de4b47175a4dcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/1327 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded. dtype=torch.bfloat16\n",
      "GPU memory used: 15.02 GB\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Load model\n",
    "from transformers import AutoProcessor, AutoModelForSeq2SeqLM\n",
    "\n",
    "print(f\"Loading {MODEL_NAME}...\")\n",
    "processor = AutoProcessor.from_pretrained(MODEL_NAME, token=HF_TOKEN)\n",
    "tokenizer = processor.tokenizer\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    token=HF_TOKEN,\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "DEVICE = next(model.parameters()).device\n",
    "print(f\"Model loaded. dtype={next(model.parameters()).dtype}\")\n",
    "print(f\"GPU memory used: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55946c12",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T14:06:38.703933Z",
     "iopub.status.busy": "2026-02-18T14:06:38.703454Z",
     "iopub.status.idle": "2026-02-18T14:06:38.725129Z",
     "shell.execute_reply": "2026-02-18T14:06:38.724364Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helpers defined.\n",
      "  Scoring: score_nll (answer-likelihood)\n",
      "  Surrogates: doc_kw, template, static_fact, random\n",
      "  Ranking metrics: AUC, MRR@k, Hit@k\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Scoring and ranking helpers\n",
    "\n",
    "def score_nll(encoder_text, answer_text, prefix_token_count=0, truncate=False):\n",
    "    '''Score NLL of answer tokens with optional truncation.'''\n",
    "    enc_ids = tokenizer(encoder_text, return_tensors=\"pt\",\n",
    "                        add_special_tokens=True, truncation=True,\n",
    "                        max_length=8192).input_ids.to(DEVICE)\n",
    "    total_enc_len = enc_ids.shape[1]\n",
    "\n",
    "    enc_mask = torch.ones(1, total_enc_len, device=DEVICE, dtype=torch.long)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        encoder_outputs = model.get_encoder()(\n",
    "            input_ids=enc_ids, attention_mask=enc_mask\n",
    "        )\n",
    "\n",
    "    if truncate and prefix_token_count > 0:\n",
    "        cross_attn_mask = torch.ones(1, total_enc_len, device=DEVICE, dtype=torch.long)\n",
    "        cross_attn_mask[:, :prefix_token_count] = 0\n",
    "    else:\n",
    "        cross_attn_mask = torch.ones(1, total_enc_len, device=DEVICE, dtype=torch.long)\n",
    "\n",
    "    ans_ids = tokenizer(answer_text, return_tensors=\"pt\",\n",
    "                        add_special_tokens=False, truncation=True,\n",
    "                        max_length=256).input_ids.to(DEVICE)\n",
    "\n",
    "    if ans_ids.shape[1] == 0:\n",
    "        return 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            encoder_outputs=encoder_outputs,\n",
    "            attention_mask=cross_attn_mask,\n",
    "            labels=ans_ids,\n",
    "        )\n",
    "\n",
    "    logits = outputs.logits\n",
    "    log_probs = F.log_softmax(logits, dim=-1)\n",
    "    token_log_probs = log_probs[0].gather(1, ans_ids[0].unsqueeze(1)).squeeze(1)\n",
    "    mean_nll = -token_log_probs.mean().item()\n",
    "\n",
    "    del encoder_outputs, outputs, logits, log_probs\n",
    "    return mean_nll\n",
    "\n",
    "\n",
    "def count_prefix_tokens(prefix_text, document_text):\n",
    "    '''Count how many tokens the prefix occupies in the concatenated encoding.'''\n",
    "    full_text = prefix_text + \"\\n\" + document_text\n",
    "    full_ids = tokenizer(full_text, add_special_tokens=True).input_ids\n",
    "    doc_ids = tokenizer(document_text, add_special_tokens=True).input_ids\n",
    "    return len(full_ids) - len(doc_ids)\n",
    "\n",
    "\n",
    "# === Surrogate generation ===\n",
    "STOP_WORDS = {\n",
    "    'a', 'an', 'the', 'is', 'are', 'was', 'were', 'be', 'been', 'being',\n",
    "    'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could',\n",
    "    'should', 'may', 'might', 'can', 'shall', 'to', 'of', 'in', 'for',\n",
    "    'on', 'with', 'at', 'by', 'from', 'as', 'into', 'through', 'during',\n",
    "    'before', 'after', 'above', 'below', 'between', 'and', 'but', 'or',\n",
    "    'not', 'no', 'if', 'then', 'than', 'so', 'up', 'out', 'about',\n",
    "    'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those',\n",
    "    'it', 'its', 'i', 'me', 'my', 'we', 'our', 'you', 'your', 'he',\n",
    "    'him', 'his', 'she', 'her', 'they', 'them', 'their', 'how', 'when',\n",
    "    'where', 'why', 'much', 'many', 'some', 'any', 'all', 'each',\n",
    "    'does', 'also', 'just', 'more', 'most', 'very', 'too', 'only',\n",
    "}\n",
    "\n",
    "def extract_keywords(text):\n",
    "    words = re.sub(r'[^\\w\\s]', '', text.lower()).split()\n",
    "    return [w for w in words if w not in STOP_WORDS and len(w) > 2]\n",
    "\n",
    "def make_surrogate_doc_kw(passage):\n",
    "    content_words = extract_keywords(passage)\n",
    "    if not content_words:\n",
    "        return \"information\"\n",
    "    counts = Counter(content_words)\n",
    "    return \" \".join(w for w, _ in counts.most_common(5))\n",
    "\n",
    "def make_surrogate_template(passage):\n",
    "    content_words = extract_keywords(passage)\n",
    "    if not content_words:\n",
    "        return \"What is this about?\"\n",
    "    counts = Counter(content_words)\n",
    "    top_word = counts.most_common(1)[0][0]\n",
    "    return f\"What is {top_word}?\"\n",
    "\n",
    "STATIC_FACT = \"What are the key facts I need to know?\"\n",
    "\n",
    "\n",
    "# === Ranking metrics ===\n",
    "def compute_auc(nlls, relevant_idx):\n",
    "    '''AUC when exactly one passage is relevant. Lower NLL = more relevant.'''\n",
    "    rel_nll = nlls[relevant_idx]\n",
    "    irrel_nlls = [nlls[i] for i in range(len(nlls)) if i != relevant_idx]\n",
    "    n_irrel = len(irrel_nlls)\n",
    "    if n_irrel == 0:\n",
    "        return 0.5\n",
    "    wins = sum(1 for nll in irrel_nlls if nll > rel_nll)\n",
    "    ties = sum(1 for nll in irrel_nlls if nll == rel_nll)\n",
    "    return (wins + 0.5 * ties) / n_irrel\n",
    "\n",
    "def compute_mrr_at_k(nlls, relevant_idx, k=3):\n",
    "    '''MRR@k: reciprocal rank of relevant passage in top-k by ascending NLL.'''\n",
    "    ranked_indices = list(np.argsort(nlls))\n",
    "    for rank, idx in enumerate(ranked_indices[:k], 1):\n",
    "        if idx == relevant_idx:\n",
    "            return 1.0 / rank\n",
    "    return 0.0\n",
    "\n",
    "def compute_hit_at_k(nlls, relevant_idx, k=1):\n",
    "    '''Hit@k: 1 if relevant passage is in top-k by ascending NLL.'''\n",
    "    ranked_indices = set(np.argsort(nlls)[:k].tolist())\n",
    "    return 1.0 if relevant_idx in ranked_indices else 0.0\n",
    "\n",
    "print(\"Helpers defined.\")\n",
    "print(\"  Scoring: score_nll (answer-likelihood)\")\n",
    "print(\"  Surrogates: doc_kw, template, static_fact, random\")\n",
    "print(\"  Ranking metrics: AUC, MRR@k, Hit@k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e6ce59d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T14:06:38.728465Z",
     "iopub.status.busy": "2026-02-18T14:06:38.727984Z",
     "iopub.status.idle": "2026-02-18T14:06:41.376051Z",
     "shell.execute_reply": "2026-02-18T14:06:41.375110Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MS MARCO v1.1 validation...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 400 queries for ranking\n",
      "Passages per query: mean=8.2, median=9, min=4, max=10\n",
      "Total passage scorings per condition: 3297\n",
      "Total scoring calls: 19782\n",
      "Estimated runtime: ~2.2 hours\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Load MS MARCO ranking data\n",
    "from lib.data import count_words\n",
    "from datasets import load_dataset\n",
    "\n",
    "print(\"Loading MS MARCO v1.1 validation...\")\n",
    "ds = load_dataset(\"microsoft/ms_marco\", \"v1.1\", split=\"validation\")\n",
    "\n",
    "# Collect queries with full passage pools for ranking\n",
    "queries = []\n",
    "\n",
    "for item in ds:\n",
    "    passages_data = item.get('passages', {})\n",
    "    ptexts = passages_data.get('passage_text', [])\n",
    "    is_sel = passages_data.get('is_selected', [])\n",
    "    query = item.get('query', '')\n",
    "    answers = item.get('answers', [])\n",
    "    well_formed = item.get('wellFormedAnswers', [])\n",
    "\n",
    "    # Get best answer\n",
    "    answer = None\n",
    "    if well_formed and len(well_formed) > 0 and well_formed[0] not in ('[]', ''):\n",
    "        answer = well_formed[0]\n",
    "    elif answers and len(answers) > 0 and answers[0] != 'No Answer Present.':\n",
    "        answer = answers[0]\n",
    "\n",
    "    if not answer:\n",
    "        continue\n",
    "\n",
    "    # Check passage pool: all passages 30-300 words\n",
    "    word_counts = [count_words(pt) for pt in ptexts]\n",
    "    if not all(30 <= wc <= 300 for wc in word_counts):\n",
    "        continue\n",
    "\n",
    "    # Exactly 1 selected, 2+ non-selected\n",
    "    n_selected = sum(is_sel)\n",
    "    n_not_selected = len(is_sel) - n_selected\n",
    "    if n_selected != 1 or n_not_selected < 2:\n",
    "        continue\n",
    "\n",
    "    # Find relevant passage index\n",
    "    relevant_idx = is_sel.index(1)\n",
    "\n",
    "    passages = []\n",
    "    for p_idx, (pt, sel) in enumerate(zip(ptexts, is_sel)):\n",
    "        passages.append({\n",
    "            'text': pt,\n",
    "            'is_selected': sel,\n",
    "            'word_count': word_counts[p_idx],\n",
    "            'surr_doc_kw': make_surrogate_doc_kw(pt),\n",
    "            'surr_template': make_surrogate_template(pt),\n",
    "        })\n",
    "\n",
    "    queries.append({\n",
    "        'query': query,\n",
    "        'answer': answer,\n",
    "        'passages': passages,\n",
    "        'relevant_idx': relevant_idx,\n",
    "        'n_passages': len(passages),\n",
    "    })\n",
    "\n",
    "    if len(queries) >= N_SAMPLES * 3:\n",
    "        break\n",
    "\n",
    "del ds\n",
    "gc.collect()\n",
    "\n",
    "# Shuffle and select N_SAMPLES\n",
    "np.random.seed(SEED)\n",
    "np.random.shuffle(queries)\n",
    "queries = queries[:N_SAMPLES]\n",
    "\n",
    "# Generate per-query random surrogates (from another query's relevant passage)\n",
    "for i, q in enumerate(queries):\n",
    "    other_idx = (i + N_SAMPLES // 2) % len(queries)\n",
    "    other_passage = queries[other_idx]['passages'][queries[other_idx]['relevant_idx']]['text']\n",
    "    q['surr_random'] = \" \".join(other_passage.split()[:20])\n",
    "\n",
    "# Stats\n",
    "n_passages_list = [q['n_passages'] for q in queries]\n",
    "print(f\"Selected {len(queries)} queries for ranking\")\n",
    "print(f\"Passages per query: mean={np.mean(n_passages_list):.1f}, \"\n",
    "      f\"median={np.median(n_passages_list):.0f}, \"\n",
    "      f\"min={np.min(n_passages_list)}, max={np.max(n_passages_list)}\")\n",
    "print(f\"Total passage scorings per condition: {sum(n_passages_list)}\")\n",
    "total_calls = sum(n_passages_list) * 6  # 6 conditions\n",
    "print(f\"Total scoring calls: {total_calls}\")\n",
    "print(f\"Estimated runtime: ~{total_calls * 0.4 / 3600:.1f} hours\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8dfced77",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T14:06:41.380543Z",
     "iopub.status.busy": "2026-02-18T14:06:41.380066Z",
     "iopub.status.idle": "2026-02-18T14:06:41.390692Z",
     "shell.execute_reply": "2026-02-18T14:06:41.389883Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "CONDITION EXAMPLES\n",
      "======================================================================\n",
      "\n",
      "Query: how many bugatti veyrons have been made\n",
      "Answer: 400.\n",
      "Passages: 10 (9 irrelevant, 1 relevant at idx 4)\n",
      "\n",
      "--- Relevant passage (idx 4) ---\n",
      "  Text: Of the 400 purchased so far, 300 were the Veyron 16.4 or 16.4 Super Sport, both coupes. The Super Sport is the fastest p...\n",
      "  surr_doc_kw: veyron 164 sport super car\n",
      "  surr_template: What is veyron?\n",
      "\n",
      "--- Irrelevant passage (idx 0) ---\n",
      "  Text: Bugatti The $2.91 million Vitesse Jean-Pierre Wimille special edition was the 400th Vugatti Veyron sold. Since 2005, Bug...\n",
      "  surr_doc_kw: veyron bugatti special 400th sold\n",
      "  surr_template: What is veyron?\n",
      "\n",
      "--- What the encoder sees for the relevant passage ---\n",
      "  bare                  : Of the 400 purchased so far, 300 were the Veyron 16.4 or 16.4 Super Sport, both ...\n",
      "  oracle_trunc          : how many bugatti veyrons have been made | Of the 400 purchased so far, 300 were the Veyron 16.4 or 16....\n",
      "  surr_template_trunc   : What is veyron? | Of the 400 purchased so far, 300 were the Veyron 16.4 or 16....\n",
      "  surr_doc_trunc        : veyron 164 sport super car | Of the 400 purchased so far, 300 were the Veyron 16.4 or 16....\n",
      "  random_trunc          : You should keep utility and credit card ... | Of the 400 purchased so far, 300 were th...\n",
      "  static_fact_trunc     : What are the key facts I need to know? | Of the 400 purchased so far, 300 were the Veyron 16.4 or 16....\n",
      "\n",
      "--- Key insight ---\n",
      "  Oracle surrogate = real query (same for all passages of this query)\n",
      "  Doc-derived surrogates (template, doc_kw) are PER-PASSAGE (different content)\n",
      "  Random and static_fact are query-level (same for all passages)\n",
      "  If oracle helps relevant MORE than irrelevant -> ranking improves\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Explain conditions with concrete example\n",
    "print(\"=\" * 70)\n",
    "print(\"CONDITION EXAMPLES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "COND_NAMES = ['bare', 'oracle_trunc', 'surr_template_trunc',\n",
    "              'surr_doc_trunc', 'random_trunc', 'static_fact_trunc']\n",
    "\n",
    "ex = queries[0]\n",
    "print(f\"\\nQuery: {ex['query']}\")\n",
    "print(f\"Answer: {ex['answer']}\")\n",
    "print(f\"Passages: {ex['n_passages']} ({ex['n_passages']-1} irrelevant, 1 relevant at idx {ex['relevant_idx']})\")\n",
    "\n",
    "# Show relevant and one irrelevant passage\n",
    "rel_p = ex['passages'][ex['relevant_idx']]\n",
    "irr_p = ex['passages'][0 if ex['relevant_idx'] != 0 else 1]\n",
    "\n",
    "print(f\"\\n--- Relevant passage (idx {ex['relevant_idx']}) ---\")\n",
    "print(f\"  Text: {rel_p['text'][:120]}...\")\n",
    "print(f\"  surr_doc_kw: {rel_p['surr_doc_kw']}\")\n",
    "print(f\"  surr_template: {rel_p['surr_template']}\")\n",
    "\n",
    "irr_idx = 0 if ex['relevant_idx'] != 0 else 1\n",
    "print(f\"\\n--- Irrelevant passage (idx {irr_idx}) ---\")\n",
    "print(f\"  Text: {irr_p['text'][:120]}...\")\n",
    "print(f\"  surr_doc_kw: {irr_p['surr_doc_kw']}\")\n",
    "print(f\"  surr_template: {irr_p['surr_template']}\")\n",
    "\n",
    "print(f\"\\n--- What the encoder sees for the relevant passage ---\")\n",
    "for cond in COND_NAMES:\n",
    "    if cond == 'bare':\n",
    "        enc = rel_p['text'][:80] + \"...\"\n",
    "    elif cond == 'oracle_trunc':\n",
    "        enc = ex['query'] + \" | \" + rel_p['text'][:60] + \"...\"\n",
    "    elif cond == 'surr_template_trunc':\n",
    "        enc = rel_p['surr_template'] + \" | \" + rel_p['text'][:60] + \"...\"\n",
    "    elif cond == 'surr_doc_trunc':\n",
    "        enc = rel_p['surr_doc_kw'] + \" | \" + rel_p['text'][:60] + \"...\"\n",
    "    elif cond == 'random_trunc':\n",
    "        enc = ex['surr_random'][:40] + \"... | \" + rel_p['text'][:40] + \"...\"\n",
    "    elif cond == 'static_fact_trunc':\n",
    "        enc = STATIC_FACT + \" | \" + rel_p['text'][:60] + \"...\"\n",
    "    print(f\"  {cond:<22s}: {enc}\")\n",
    "\n",
    "print(f\"\\n--- Key insight ---\")\n",
    "print(f\"  Oracle surrogate = real query (same for all passages of this query)\")\n",
    "print(f\"  Doc-derived surrogates (template, doc_kw) are PER-PASSAGE (different content)\")\n",
    "print(f\"  Random and static_fact are query-level (same for all passages)\")\n",
    "print(f\"  If oracle helps relevant MORE than irrelevant -> ranking improves\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0558a94",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T14:06:41.394173Z",
     "iopub.status.busy": "2026-02-18T14:06:41.393853Z",
     "iopub.status.idle": "2026-02-18T15:15:34.053175Z",
     "shell.execute_reply": "2026-02-18T15:15:34.051987Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "RUNNING RANKING EXPERIMENT\n",
      "======================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9b50f51b5054bb7b51ff74e7b74e6bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Queries:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 20/400 | 3.6m | ETA 69.2m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 40/400 | 7.0m | ETA 62.9m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 60/400 | 10.3m | ETA 58.2m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 80/400 | 13.8m | ETA 55.2m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 100/400 | 17.2m | ETA 51.7m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 120/400 | 20.8m | ETA 48.6m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 140/400 | 24.5m | ETA 45.5m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 160/400 | 27.9m | ETA 41.9m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 180/400 | 31.3m | ETA 38.2m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 200/400 | 34.6m | ETA 34.6m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 220/400 | 37.8m | ETA 30.9m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 240/400 | 41.4m | ETA 27.6m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 260/400 | 44.7m | ETA 24.1m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 280/400 | 48.1m | ETA 20.6m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 300/400 | 51.7m | ETA 17.2m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 320/400 | 55.2m | ETA 13.8m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 340/400 | 58.5m | ETA 10.3m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 360/400 | 62.1m | ETA 6.9m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 380/400 | 65.3m | ETA 3.4m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checkpoint 400/400 | 68.9m | ETA 0.0m\n",
      "\n",
      "Scoring complete: 400 queries in 68.9 min\n",
      "Bare AUC: mean=0.845, median=1.000\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Run scoring -- outer loop over queries\n",
    "print(\"=\" * 70)\n",
    "print(\"RUNNING RANKING EXPERIMENT\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "def build_condition_input(cond_name, passage_data, query_data):\n",
    "    '''Return (encoder_text, prefix_token_count, truncate) for a condition.'''\n",
    "    passage_text = passage_data['text']\n",
    "\n",
    "    if cond_name == 'bare':\n",
    "        return passage_text, 0, False\n",
    "    elif cond_name == 'oracle_trunc':\n",
    "        surr = query_data['query']\n",
    "    elif cond_name == 'surr_template_trunc':\n",
    "        surr = passage_data['surr_template']\n",
    "    elif cond_name == 'surr_doc_trunc':\n",
    "        surr = passage_data['surr_doc_kw']\n",
    "    elif cond_name == 'random_trunc':\n",
    "        surr = query_data['surr_random']\n",
    "    elif cond_name == 'static_fact_trunc':\n",
    "        surr = STATIC_FACT\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown condition: {cond_name}\")\n",
    "\n",
    "    enc_text = surr + \"\\n\" + passage_text\n",
    "    prefix_count = count_prefix_tokens(surr, passage_text)\n",
    "    return enc_text, prefix_count, True\n",
    "\n",
    "\n",
    "# Resume from checkpoint\n",
    "results = []\n",
    "start_idx = 0\n",
    "if CHECKPOINT_PATH.exists():\n",
    "    saved = json.loads(CHECKPOINT_PATH.read_text())\n",
    "    if saved.get('n_total') == N_SAMPLES:\n",
    "        saved_results = saved.get('results', [])\n",
    "        # Validate alignment\n",
    "        saved_queries = [r['query'][:50] for r in saved_results]\n",
    "        current_queries = [q['query'][:50] for q in queries[:len(saved_results)]]\n",
    "        if saved_queries == current_queries:\n",
    "            results = saved_results\n",
    "            start_idx = len(results)\n",
    "            print(f\"Resumed from checkpoint: {start_idx}/{N_SAMPLES} queries\")\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "for q_idx in tqdm(range(start_idx, N_SAMPLES), initial=start_idx, total=N_SAMPLES,\n",
    "                  desc=\"Queries\"):\n",
    "    q = queries[q_idx]\n",
    "    answer = q['answer']\n",
    "\n",
    "    query_result = {\n",
    "        'query_idx': q_idx,\n",
    "        'query': q['query'],\n",
    "        'answer': answer,\n",
    "        'n_passages': q['n_passages'],\n",
    "        'relevant_idx': q['relevant_idx'],\n",
    "        'is_selected': [p['is_selected'] for p in q['passages']],\n",
    "        'scores': {},\n",
    "    }\n",
    "\n",
    "    for cond_name in COND_NAMES:\n",
    "        cond_nlls = []\n",
    "        for p_idx, passage_data in enumerate(q['passages']):\n",
    "            enc_text, prefix_count, truncate = build_condition_input(\n",
    "                cond_name, passage_data, q)\n",
    "            nll = score_nll(enc_text, answer, prefix_count, truncate)\n",
    "            cond_nlls.append(nll)\n",
    "        query_result['scores'][cond_name] = cond_nlls\n",
    "\n",
    "    results.append(query_result)\n",
    "\n",
    "    if (q_idx + 1) % 20 == 0 or q_idx == N_SAMPLES - 1:\n",
    "        ckpt = {\n",
    "            'n_total': N_SAMPLES,\n",
    "            'results': results,\n",
    "            'completed': len(results),\n",
    "            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        }\n",
    "        CHECKPOINT_PATH.write_text(json.dumps(ckpt))\n",
    "        elapsed = time.time() - t0\n",
    "        done = q_idx - start_idx + 1\n",
    "        eta = (N_SAMPLES - q_idx - 1) * elapsed / done if done > 0 else 0\n",
    "        tqdm.write(f\"  Checkpoint {q_idx+1}/{N_SAMPLES} | {elapsed/60:.1f}m | ETA {eta/60:.1f}m\")\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "elapsed_total = time.time() - t0\n",
    "print(f\"\\nScoring complete: {len(results)} queries in {elapsed_total/60:.1f} min\")\n",
    "\n",
    "# Quick peek at bare AUC\n",
    "bare_aucs = []\n",
    "for r in results:\n",
    "    nlls = np.array(r['scores']['bare'])\n",
    "    bare_aucs.append(compute_auc(nlls, r['relevant_idx']))\n",
    "print(f\"Bare AUC: mean={np.mean(bare_aucs):.3f}, median={np.median(bare_aucs):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d50f428d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T15:15:34.057275Z",
     "iopub.status.busy": "2026-02-18T15:15:34.056710Z",
     "iopub.status.idle": "2026-02-18T15:15:34.114352Z",
     "shell.execute_reply": "2026-02-18T15:15:34.113464Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "COMPUTING RANKING METRICS\n",
      "======================================================================\n",
      "  bare                  : AUC=0.845  MRR@3=0.737  Hit@1=0.670  Hit@3=0.820\n",
      "  oracle_trunc          : AUC=0.853  MRR@3=0.752  Hit@1=0.693  Hit@3=0.835\n",
      "  surr_template_trunc   : AUC=0.861  MRR@3=0.756  Hit@1=0.685  Hit@3=0.848\n",
      "  surr_doc_trunc        : AUC=0.867  MRR@3=0.769  Hit@1=0.710  Hit@3=0.848\n",
      "  random_trunc          : AUC=0.866  MRR@3=0.764  Hit@1=0.700  Hit@3=0.845\n",
      "  static_fact_trunc     : AUC=0.860  MRR@3=0.762  Hit@1=0.698  Hit@3=0.848\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Compute ranking metrics for all conditions\n",
    "print(\"=\" * 70)\n",
    "print(\"COMPUTING RANKING METRICS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# For each query x condition, compute AUC, MRR@3, Hit@1, Hit@3\n",
    "metrics = {cond: {'auc': [], 'mrr3': [], 'hit1': [], 'hit3': []}\n",
    "           for cond in COND_NAMES}\n",
    "\n",
    "for r in results:\n",
    "    rel_idx = r['relevant_idx']\n",
    "    for cond in COND_NAMES:\n",
    "        nlls = np.array(r['scores'][cond])\n",
    "        metrics[cond]['auc'].append(compute_auc(nlls, rel_idx))\n",
    "        metrics[cond]['mrr3'].append(compute_mrr_at_k(nlls, rel_idx, k=3))\n",
    "        metrics[cond]['hit1'].append(compute_hit_at_k(nlls, rel_idx, k=1))\n",
    "        metrics[cond]['hit3'].append(compute_hit_at_k(nlls, rel_idx, k=3))\n",
    "\n",
    "# Convert to arrays\n",
    "for cond in COND_NAMES:\n",
    "    for m in metrics[cond]:\n",
    "        metrics[cond][m] = np.array(metrics[cond][m])\n",
    "\n",
    "# Quick summary\n",
    "for cond in COND_NAMES:\n",
    "    print(f\"  {cond:<22s}: AUC={metrics[cond]['auc'].mean():.3f}  \"\n",
    "          f\"MRR@3={metrics[cond]['mrr3'].mean():.3f}  \"\n",
    "          f\"Hit@1={metrics[cond]['hit1'].mean():.3f}  \"\n",
    "          f\"Hit@3={metrics[cond]['hit3'].mean():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17744f4e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T15:15:34.117726Z",
     "iopub.status.busy": "2026-02-18T15:15:34.117451Z",
     "iopub.status.idle": "2026-02-18T15:15:34.155203Z",
     "shell.execute_reply": "2026-02-18T15:15:34.154265Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "RESULTS: Ranking Metrics per Condition (N=400 queries)\n",
      "======================================================================\n",
      "\n",
      "--- AUC ---\n",
      "  Condition                  Mean    vs Bare        d            p   sig\n",
      "  ----------------------------------------------------------------------\n",
      "  bare                      0.845         --       --           --    --\n",
      "  oracle_trunc              0.853    +0.0082   +0.067     1.12e-01    ns\n",
      "  surr_template_trunc       0.861    +0.0153   +0.121     6.25e-03     *\n",
      "  surr_doc_trunc            0.867    +0.0218   +0.157     1.86e-03    **\n",
      "  random_trunc              0.866    +0.0210   +0.150     3.03e-03     *\n",
      "  static_fact_trunc         0.860    +0.0145   +0.117     7.14e-03     *\n",
      "\n",
      "--- MRR@3 ---\n",
      "  Condition                  Mean    vs Bare        d            p   sig\n",
      "  ----------------------------------------------------------------------\n",
      "  bare                      0.737         --       --           --    --\n",
      "  oracle_trunc              0.752    +0.0154   +0.081     9.79e-02    ns\n",
      "  surr_template_trunc       0.756    +0.0187   +0.103     4.67e-02    ns\n",
      "  surr_doc_trunc            0.769    +0.0317   +0.148     3.70e-03     *\n",
      "  random_trunc              0.764    +0.0267   +0.128     1.89e-02    ns\n",
      "  static_fact_trunc         0.762    +0.0250   +0.135     6.58e-03     *\n",
      "\n",
      "--- Hit@1 ---\n",
      "  Condition                  Mean    vs Bare        d            p   sig\n",
      "  ----------------------------------------------------------------------\n",
      "  bare                      0.670         --       --           --    --\n",
      "  oracle_trunc              0.693    +0.0225   +0.081     1.63e-01    ns\n",
      "  surr_template_trunc       0.685    +0.0150   +0.061     3.03e-01    ns\n",
      "  surr_doc_trunc            0.710    +0.0400   +0.131     2.34e-02    ns\n",
      "  random_trunc              0.700    +0.0300   +0.103     7.52e-02    ns\n",
      "  static_fact_trunc         0.698    +0.0275   +0.106     6.55e-02    ns\n",
      "\n",
      "--- Hit@3 ---\n",
      "  Condition                  Mean    vs Bare        d            p   sig\n",
      "  ----------------------------------------------------------------------\n",
      "  bare                      0.820         --       --           --    --\n",
      "  oracle_trunc              0.835    +0.0150   +0.061     3.03e-01    ns\n",
      "  surr_template_trunc       0.848    +0.0275   +0.121     3.51e-02    ns\n",
      "  surr_doc_trunc            0.848    +0.0275   +0.121     3.51e-02    ns\n",
      "  random_trunc              0.845    +0.0250   +0.107     6.34e-02    ns\n",
      "  static_fact_trunc         0.848    +0.0275   +0.127     2.58e-02    ns\n",
      "\n",
      "======================================================================\n",
      "HEADLINE: oracle_trunc AUC = 0.853 vs bare AUC = 0.845 (gain = +0.008)\n",
      "  >>> Marginal ranking signal (v2 Exp 22 got +0.001)\n",
      "v2 Exp 22 reference: bare AUC = 0.828, primed AUC = 0.829\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Results table with statistical tests\n",
    "from lib.analysis import cohens_d\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"RESULTS: Ranking Metrics per Condition (N=%d queries)\" % N_SAMPLES)\n",
    "print(\"=\" * 70)\n",
    "\n",
    "METRIC_NAMES = ['auc', 'mrr3', 'hit1', 'hit3']\n",
    "METRIC_LABELS = {'auc': 'AUC', 'mrr3': 'MRR@3', 'hit1': 'Hit@1', 'hit3': 'Hit@3'}\n",
    "\n",
    "analysis = {}\n",
    "\n",
    "for metric_name in METRIC_NAMES:\n",
    "    print(f\"\\n--- {METRIC_LABELS[metric_name]} ---\")\n",
    "    print(f\"  {'Condition':<22} {'Mean':>8} {'vs Bare':>10} {'d':>8} {'p':>12} {'sig':>5}\")\n",
    "    print(f\"  {'-'*70}\")\n",
    "\n",
    "    bare_vals = metrics['bare'][metric_name]\n",
    "    analysis[metric_name] = {}\n",
    "\n",
    "    for cond in COND_NAMES:\n",
    "        vals = metrics[cond][metric_name]\n",
    "        mean_val = vals.mean()\n",
    "\n",
    "        if cond == 'bare':\n",
    "            print(f\"  {cond:<22} {mean_val:>8.3f} {'--':>10} {'--':>8} {'--':>12} {'--':>5}\")\n",
    "            analysis[metric_name][cond] = {'mean': float(mean_val)}\n",
    "        else:\n",
    "            diff = vals - bare_vals\n",
    "            d = cohens_d(diff)\n",
    "\n",
    "            # Wilcoxon signed-rank test (non-parametric, appropriate for ranking metrics)\n",
    "            nonzero = diff[diff != 0]\n",
    "            if len(nonzero) >= 10:\n",
    "                try:\n",
    "                    stat, p_val = wilcoxon(nonzero)\n",
    "                except ValueError:\n",
    "                    p_val = 1.0\n",
    "            else:\n",
    "                p_val = 1.0\n",
    "\n",
    "            sig = ('***' if p_val < 0.001/N_BONFERRONI else\n",
    "                   '**' if p_val < 0.01/N_BONFERRONI else\n",
    "                   '*' if p_val < 0.05/N_BONFERRONI else 'ns')\n",
    "\n",
    "            print(f\"  {cond:<22} {mean_val:>8.3f} {diff.mean():>+10.4f} {d:>+8.3f} {p_val:>12.2e} {sig:>5}\")\n",
    "            analysis[metric_name][cond] = {\n",
    "                'mean': float(mean_val), 'delta': float(diff.mean()),\n",
    "                'd': float(d), 'p': float(p_val),\n",
    "            }\n",
    "\n",
    "# Headline result\n",
    "oracle_auc = analysis['auc']['oracle_trunc']['mean']\n",
    "bare_auc = analysis['auc']['bare']['mean']\n",
    "auc_gain = oracle_auc - bare_auc\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"HEADLINE: oracle_trunc AUC = {oracle_auc:.3f} vs bare AUC = {bare_auc:.3f} (gain = {auc_gain:+.3f})\")\n",
    "if auc_gain > 0.01:\n",
    "    print(f\"  >>> BREAKTHROUGH: Ranking signal detected! v2 never achieved this.\")\n",
    "elif auc_gain > 0.005:\n",
    "    print(f\"  >>> Marginal ranking signal (v2 Exp 22 got +0.001)\")\n",
    "else:\n",
    "    print(f\"  >>> No ranking signal -- same as v2\")\n",
    "print(f\"v2 Exp 22 reference: bare AUC = 0.828, primed AUC = 0.829\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "707cd691",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T15:15:34.158876Z",
     "iopub.status.busy": "2026-02-18T15:15:34.158607Z",
     "iopub.status.idle": "2026-02-18T15:15:35.756436Z",
     "shell.execute_reply": "2026-02-18T15:15:35.755534Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "DIFFERENTIAL SIGNAL ANALYSIS\n",
      "======================================================================\n",
      "Core test: does priming help the RELEVANT passage MORE than IRRELEVANT ones?\n",
      "\n",
      "  oracle_trunc:\n",
      "    delta_rel  (mean NLL drop for relevant):   +0.5719\n",
      "    delta_irrel (mean NLL drop for irrelevant): +0.5792\n",
      "    differential (rel - irrel):                 -0.0073  d=-0.007  p=7.22e-01  ns\n",
      "    % queries where relevant helped MORE:       53.8%\n",
      "\n",
      "  surr_template_trunc:\n",
      "    delta_rel  (mean NLL drop for relevant):   +0.4893\n",
      "    delta_irrel (mean NLL drop for irrelevant): +0.3717\n",
      "    differential (rel - irrel):                 +0.1176  d=+0.130  p=1.77e-11  ***\n",
      "    % queries where relevant helped MORE:       67.8%\n",
      "\n",
      "  surr_doc_trunc:\n",
      "    delta_rel  (mean NLL drop for relevant):   +0.4965\n",
      "    delta_irrel (mean NLL drop for irrelevant): +0.4434\n",
      "    differential (rel - irrel):                 +0.0531  d=+0.053  p=2.78e-03  *\n",
      "    % queries where relevant helped MORE:       58.5%\n",
      "\n",
      "  random_trunc:\n",
      "    delta_rel  (mean NLL drop for relevant):   +0.4692\n",
      "    delta_irrel (mean NLL drop for irrelevant): +0.3635\n",
      "    differential (rel - irrel):                 +0.1057  d=+0.117  p=1.55e-09  ***\n",
      "    % queries where relevant helped MORE:       66.0%\n",
      "\n",
      "  static_fact_trunc:\n",
      "    delta_rel  (mean NLL drop for relevant):   +0.3823\n",
      "    delta_irrel (mean NLL drop for irrelevant): +0.2605\n",
      "    differential (rel - irrel):                 +0.1218  d=+0.153  p=3.89e-15  ***\n",
      "    % queries where relevant helped MORE:       73.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Plot saved to results/exp04a/differential_signal.png\n"
     ]
    }
   ],
   "source": [
    "# Cell 10: Differential signal analysis\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"DIFFERENTIAL SIGNAL ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "print(\"Core test: does priming help the RELEVANT passage MORE than IRRELEVANT ones?\")\n",
    "\n",
    "# Compute per-query differential: delta_rel vs mean(delta_irrel)\n",
    "diff_analysis = {}\n",
    "for cond in COND_NAMES[1:]:  # skip bare\n",
    "    delta_rels = []\n",
    "    delta_irrels = []\n",
    "\n",
    "    for r in results:\n",
    "        rel_idx = r['relevant_idx']\n",
    "        bare_nlls = r['scores']['bare']\n",
    "        cond_nlls = r['scores'][cond]\n",
    "\n",
    "        # Delta for relevant passage (positive = priming helped)\n",
    "        delta_rel = bare_nlls[rel_idx] - cond_nlls[rel_idx]\n",
    "\n",
    "        # Mean delta for irrelevant passages\n",
    "        irrel_deltas = [bare_nlls[i] - cond_nlls[i]\n",
    "                        for i in range(len(bare_nlls)) if i != rel_idx]\n",
    "        delta_irrel = np.mean(irrel_deltas)\n",
    "\n",
    "        delta_rels.append(delta_rel)\n",
    "        delta_irrels.append(delta_irrel)\n",
    "\n",
    "    delta_rels = np.array(delta_rels)\n",
    "    delta_irrels = np.array(delta_irrels)\n",
    "    differential = delta_rels - delta_irrels  # positive = helps ranking\n",
    "\n",
    "    # Test: is differential > 0?\n",
    "    d = cohens_d(differential)\n",
    "    nonzero = differential[differential != 0]\n",
    "    if len(nonzero) >= 10:\n",
    "        try:\n",
    "            stat, p_val = wilcoxon(nonzero)\n",
    "            # One-sided: we care about positive differential\n",
    "            p_val_onesided = p_val / 2 if np.mean(differential) > 0 else 1 - p_val / 2\n",
    "        except ValueError:\n",
    "            p_val_onesided = 1.0\n",
    "    else:\n",
    "        p_val_onesided = 1.0\n",
    "\n",
    "    diff_analysis[cond] = {\n",
    "        'delta_rel_mean': float(delta_rels.mean()),\n",
    "        'delta_irrel_mean': float(delta_irrels.mean()),\n",
    "        'differential_mean': float(differential.mean()),\n",
    "        'd': float(d),\n",
    "        'p_onesided': float(p_val_onesided),\n",
    "        'pct_positive': float(100 * np.mean(differential > 0)),\n",
    "    }\n",
    "\n",
    "    sig = ('***' if p_val_onesided < 0.001/N_BONFERRONI else\n",
    "           '**' if p_val_onesided < 0.01/N_BONFERRONI else\n",
    "           '*' if p_val_onesided < 0.05/N_BONFERRONI else 'ns')\n",
    "\n",
    "    print(f\"\\n  {cond}:\")\n",
    "    print(f\"    delta_rel  (mean NLL drop for relevant):   {delta_rels.mean():+.4f}\")\n",
    "    print(f\"    delta_irrel (mean NLL drop for irrelevant): {delta_irrels.mean():+.4f}\")\n",
    "    print(f\"    differential (rel - irrel):                 {differential.mean():+.4f}  \"\n",
    "          f\"d={d:+.3f}  p={p_val_onesided:.2e}  {sig}\")\n",
    "    print(f\"    % queries where relevant helped MORE:       {100*np.mean(differential > 0):.1f}%\")\n",
    "\n",
    "# Plot: delta_rel vs delta_irrel scatter for oracle\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "for ax_idx, cond in enumerate(['oracle_trunc', 'surr_template_trunc', 'random_trunc']):\n",
    "    ax = axes[ax_idx]\n",
    "    delta_rels = []\n",
    "    delta_irrels = []\n",
    "    for r in results:\n",
    "        rel_idx = r['relevant_idx']\n",
    "        bare_nlls = r['scores']['bare']\n",
    "        cond_nlls = r['scores'][cond]\n",
    "        delta_rels.append(bare_nlls[rel_idx] - cond_nlls[rel_idx])\n",
    "        irrel_deltas = [bare_nlls[i] - cond_nlls[i]\n",
    "                        for i in range(len(bare_nlls)) if i != rel_idx]\n",
    "        delta_irrels.append(np.mean(irrel_deltas))\n",
    "\n",
    "    ax.scatter(delta_irrels, delta_rels, alpha=0.3, s=10)\n",
    "    lims = [min(min(delta_irrels), min(delta_rels)) - 0.5,\n",
    "            max(max(delta_irrels), max(delta_rels)) + 0.5]\n",
    "    ax.plot(lims, lims, 'r--', alpha=0.5, label='equal help')\n",
    "    ax.set_xlabel('delta_irrelevant (mean NLL drop)')\n",
    "    ax.set_ylabel('delta_relevant (NLL drop)')\n",
    "    ax.set_title(f'{cond.replace(\"_trunc\", \"\")}')\n",
    "    ax.legend(fontsize=8)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Differential Signal: Points ABOVE red line = ranking improves', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plot_path = RESULTS_DIR / 'differential_signal.png'\n",
    "plt.savefig(plot_path, dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"\\nPlot saved to {plot_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24e70550",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T15:15:35.760207Z",
     "iopub.status.busy": "2026-02-18T15:15:35.759424Z",
     "iopub.status.idle": "2026-02-18T15:15:36.112893Z",
     "shell.execute_reply": "2026-02-18T15:15:36.111625Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "HARDNESS STRATIFICATION\n",
      "======================================================================\n",
      "Split queries by bare AUC quintiles, check if priming helps more for hard queries\n",
      "\n",
      "Quintile boundaries (bare AUC): [0.66666667 1.         1.         1.        ]\n",
      "  Q1 (hardest): N=82, mean bare AUC=0.343\n",
      "  Q2: N=318, mean bare AUC=0.975\n",
      "  Q3: N=0, mean bare AUC=nan\n",
      "  Q4: N=0, mean bare AUC=nan\n",
      "  Q5 (easiest): N=0, mean bare AUC=nan\n",
      "\n",
      "--- AUC gain by hardness quintile ---\n",
      "  Quintile                 oracle  surr_template       surr_doc         random    static_fact\n",
      "  -------------------------------------------------------------------------------------------\n",
      "  Q1 (hardest)      +0.081 d=+0.45  +0.105 d=+0.51  +0.130 d=+0.57  +0.122 d=+0.51  +0.092 d=+0.44\n",
      "  Q2                -0.011 d=-0.11  -0.008 d=-0.09  -0.006 d=-0.07  -0.005 d=-0.06  -0.006 d=-0.07\n",
      "  Q3                  +nan d=+0.00    +nan d=+0.00    +nan d=+0.00    +nan d=+0.00    +nan d=+0.00\n",
      "  Q4                  +nan d=+0.00    +nan d=+0.00    +nan d=+0.00    +nan d=+0.00    +nan d=+0.00\n",
      "  Q5 (easiest)        +nan d=+0.00    +nan d=+0.00    +nan d=+0.00    +nan d=+0.00    +nan d=+0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_2475236/3855188158.py:25: RuntimeWarning: Mean of empty slice.\n",
      "  mean_bare_auc = bare_aucs[mask].mean()\n",
      "/opt/conda/lib/python3.10/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/var/tmp/ipykernel_2475236/3855188158.py:46: RuntimeWarning: Mean of empty slice.\n",
      "  gain = (cond_aucs - bare_q_aucs).mean()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plot saved to results/exp04a/hardness_stratification.png\n"
     ]
    }
   ],
   "source": [
    "# Cell 11: Hardness stratification by bare AUC\n",
    "print(\"=\" * 70)\n",
    "print(\"HARDNESS STRATIFICATION\")\n",
    "print(\"=\" * 70)\n",
    "print(\"Split queries by bare AUC quintiles, check if priming helps more for hard queries\")\n",
    "\n",
    "bare_aucs = metrics['bare']['auc']\n",
    "\n",
    "# Quintile boundaries\n",
    "quintile_boundaries = np.percentile(bare_aucs, [20, 40, 60, 80])\n",
    "quintile_labels = ['Q1 (hardest)', 'Q2', 'Q3', 'Q4', 'Q5 (easiest)']\n",
    "\n",
    "def get_quintile(auc):\n",
    "    for q, bound in enumerate(quintile_boundaries):\n",
    "        if auc <= bound:\n",
    "            return q\n",
    "    return 4\n",
    "\n",
    "quintile_assignments = np.array([get_quintile(a) for a in bare_aucs])\n",
    "\n",
    "print(f\"\\nQuintile boundaries (bare AUC): {quintile_boundaries}\")\n",
    "for q in range(5):\n",
    "    mask = quintile_assignments == q\n",
    "    n = mask.sum()\n",
    "    mean_bare_auc = bare_aucs[mask].mean()\n",
    "    print(f\"  {quintile_labels[q]}: N={n}, mean bare AUC={mean_bare_auc:.3f}\")\n",
    "\n",
    "# Per-quintile metric gains\n",
    "print(f\"\\n--- AUC gain by hardness quintile ---\")\n",
    "header = f\"  {'Quintile':<16}\"\n",
    "for cond in COND_NAMES[1:]:\n",
    "    short = cond.replace('_trunc', '')\n",
    "    header += f\" {short:>14}\"\n",
    "print(header)\n",
    "print(f\"  {'-'*(16 + 15 * len(COND_NAMES[1:]))}\")\n",
    "\n",
    "hardness_analysis = {}\n",
    "for q in range(5):\n",
    "    mask = quintile_assignments == q\n",
    "    row = f\"  {quintile_labels[q]:<16}\"\n",
    "    hardness_analysis[quintile_labels[q]] = {}\n",
    "\n",
    "    for cond in COND_NAMES[1:]:\n",
    "        cond_aucs = metrics[cond]['auc'][mask]\n",
    "        bare_q_aucs = bare_aucs[mask]\n",
    "        gain = (cond_aucs - bare_q_aucs).mean()\n",
    "        d = cohens_d(cond_aucs - bare_q_aucs) if mask.sum() > 1 else 0\n",
    "        row += f\" {gain:>+7.3f} d={d:>+5.2f}\"\n",
    "        hardness_analysis[quintile_labels[q]][cond] = {\n",
    "            'auc_gain': float(gain), 'd': float(d)}\n",
    "\n",
    "    print(row)\n",
    "\n",
    "# Plot: oracle gain vs hardness quintile\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "for cond in ['oracle_trunc', 'surr_template_trunc', 'random_trunc']:\n",
    "    gains = [hardness_analysis[quintile_labels[q]].get(cond, {}).get('auc_gain', 0)\n",
    "             for q in range(5)]\n",
    "    ax.plot(range(5), gains, '-o', label=cond.replace('_trunc', ''), markersize=8)\n",
    "\n",
    "ax.set_xticks(range(5))\n",
    "ax.set_xticklabels(quintile_labels, rotation=15)\n",
    "ax.set_ylabel('AUC gain vs bare')\n",
    "ax.set_title('Ranking Gain by Query Hardness (bare AUC quintiles)')\n",
    "ax.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plot_path = RESULTS_DIR / 'hardness_stratification.png'\n",
    "plt.savefig(plot_path, dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"Plot saved to {plot_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d7b7f6f0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T15:15:36.116884Z",
     "iopub.status.busy": "2026-02-18T15:15:36.116582Z",
     "iopub.status.idle": "2026-02-18T15:15:36.141682Z",
     "shell.execute_reply": "2026-02-18T15:15:36.139333Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "VERDICT -- Exp 04A: MS MARCO Ranking\n",
      "======================================================================\n",
      "\n",
      "Model: google/t5gemma-2-4b-4b\n",
      "N queries: 400\n",
      "Mean passages per query: 8.2\n",
      "\n",
      "--- Ranking metrics summary ---\n",
      "\n",
      "  AUC:\n",
      "    bare:           0.845\n",
      "    oracle_trunc          : 0.853 (+0.0082, d=+0.067) ns\n",
      "    surr_template_trunc   : 0.861 (+0.0153, d=+0.121) *\n",
      "    surr_doc_trunc        : 0.867 (+0.0218, d=+0.157) **\n",
      "    random_trunc          : 0.866 (+0.0210, d=+0.150) *\n",
      "    static_fact_trunc     : 0.860 (+0.0145, d=+0.117) *\n",
      "\n",
      "  MRR@3:\n",
      "    bare:           0.737\n",
      "    oracle_trunc          : 0.752 (+0.0154, d=+0.081) ns\n",
      "    surr_template_trunc   : 0.756 (+0.0187, d=+0.103) ns\n",
      "    surr_doc_trunc        : 0.769 (+0.0317, d=+0.148) *\n",
      "    random_trunc          : 0.764 (+0.0267, d=+0.128) ns\n",
      "    static_fact_trunc     : 0.762 (+0.0250, d=+0.135) *\n",
      "\n",
      "  Hit@1:\n",
      "    bare:           0.670\n",
      "    oracle_trunc          : 0.693 (+0.0225, d=+0.081) ns\n",
      "    surr_template_trunc   : 0.685 (+0.0150, d=+0.061) ns\n",
      "    surr_doc_trunc        : 0.710 (+0.0400, d=+0.131) ns\n",
      "    random_trunc          : 0.700 (+0.0300, d=+0.103) ns\n",
      "    static_fact_trunc     : 0.698 (+0.0275, d=+0.106) ns\n",
      "\n",
      "  Hit@3:\n",
      "    bare:           0.820\n",
      "    oracle_trunc          : 0.835 (+0.0150, d=+0.061) ns\n",
      "    surr_template_trunc   : 0.848 (+0.0275, d=+0.121) ns\n",
      "    surr_doc_trunc        : 0.848 (+0.0275, d=+0.121) ns\n",
      "    random_trunc          : 0.845 (+0.0250, d=+0.107) ns\n",
      "    static_fact_trunc     : 0.848 (+0.0275, d=+0.127) ns\n",
      "\n",
      "--- v2 comparison ---\n",
      "  v2 Exp 22 (T5Gemma, same model, no truncation):\n",
      "    bare AUC = 0.828, primed AUC = 0.829 (gain = +0.001)\n",
      "  v3 Exp 04A (with truncation + surrogate co-encoding):\n",
      "    bare AUC = 0.845, oracle AUC = 0.853 (gain = +0.008)\n",
      "\n",
      "--- Differential signal verdict ---\n",
      "  oracle_trunc          : differential=-0.0073 d=-0.007 54% positive ns\n",
      "  surr_template_trunc   : differential=+0.1176 d=+0.130 68% positive ***\n",
      "  surr_doc_trunc        : differential=+0.0531 d=+0.053 58% positive *\n",
      "  random_trunc          : differential=+0.1057 d=+0.117 66% positive ***\n",
      "  static_fact_trunc     : differential=+0.1218 d=+0.153 73% positive ***\n",
      "\n",
      "--- OVERALL VERDICT ---\n",
      "  NO ranking signal (oracle AUC d=+0.067, p=1.12e-01)\n",
      "  Consistent with v2: benefit is document-independent, no differential\n",
      "\n",
      "--- Surrogate ranking performance ---\n",
      "  surr_template_trunc: AUC=0.861 (187% of oracle ranking gain)\n",
      "  surr_doc_trunc: AUC=0.867 (266% of oracle ranking gain)\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Results saved to results/exp04a/results.json\n"
     ]
    }
   ],
   "source": [
    "# Cell 12: Verdict and save results\n",
    "print(\"=\" * 70)\n",
    "print(\"VERDICT -- Exp 04A: MS MARCO Ranking\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\nModel: {MODEL_NAME}\")\n",
    "print(f\"N queries: {N_SAMPLES}\")\n",
    "print(f\"Mean passages per query: {np.mean([r['n_passages'] for r in results]):.1f}\")\n",
    "\n",
    "# Key results\n",
    "print(f\"\\n--- Ranking metrics summary ---\")\n",
    "for metric_name in METRIC_NAMES:\n",
    "    print(f\"\\n  {METRIC_LABELS[metric_name]}:\")\n",
    "    bare_val = analysis[metric_name]['bare']['mean']\n",
    "    print(f\"    bare:           {bare_val:.3f}\")\n",
    "    for cond in COND_NAMES[1:]:\n",
    "        a = analysis[metric_name].get(cond, {})\n",
    "        mean = a.get('mean', 0)\n",
    "        delta = a.get('delta', 0)\n",
    "        d = a.get('d', 0)\n",
    "        p = a.get('p', 1)\n",
    "        sig = ('***' if p < 0.001/N_BONFERRONI else '**' if p < 0.01/N_BONFERRONI\n",
    "               else '*' if p < 0.05/N_BONFERRONI else 'ns')\n",
    "        print(f\"    {cond:<22s}: {mean:.3f} ({delta:+.4f}, d={d:+.3f}) {sig}\")\n",
    "\n",
    "# v2 comparison\n",
    "print(f\"\\n--- v2 comparison ---\")\n",
    "print(f\"  v2 Exp 22 (T5Gemma, same model, no truncation):\")\n",
    "print(f\"    bare AUC = 0.828, primed AUC = 0.829 (gain = +0.001)\")\n",
    "print(f\"  v3 Exp 04A (with truncation + surrogate co-encoding):\")\n",
    "oracle_auc = analysis['auc']['oracle_trunc']['mean']\n",
    "bare_auc = analysis['auc']['bare']['mean']\n",
    "print(f\"    bare AUC = {bare_auc:.3f}, oracle AUC = {oracle_auc:.3f} (gain = {oracle_auc - bare_auc:+.3f})\")\n",
    "\n",
    "# Differential verdict\n",
    "print(f\"\\n--- Differential signal verdict ---\")\n",
    "for cond in COND_NAMES[1:]:\n",
    "    da = diff_analysis.get(cond, {})\n",
    "    diff_mean = da.get('differential_mean', 0)\n",
    "    d = da.get('d', 0)\n",
    "    p = da.get('p_onesided', 1)\n",
    "    pct = da.get('pct_positive', 0)\n",
    "    sig = ('***' if p < 0.001/N_BONFERRONI else '**' if p < 0.01/N_BONFERRONI\n",
    "           else '*' if p < 0.05/N_BONFERRONI else 'ns')\n",
    "    print(f\"  {cond:<22s}: differential={diff_mean:+.4f} d={d:+.3f} {pct:.0f}% positive {sig}\")\n",
    "\n",
    "# Overall verdict\n",
    "print(f\"\\n--- OVERALL VERDICT ---\")\n",
    "oracle_auc_d = analysis['auc'].get('oracle_trunc', {}).get('d', 0)\n",
    "oracle_auc_p = analysis['auc'].get('oracle_trunc', {}).get('p', 1)\n",
    "oracle_diff_d = diff_analysis.get('oracle_trunc', {}).get('d', 0)\n",
    "oracle_diff_p = diff_analysis.get('oracle_trunc', {}).get('p_onesided', 1)\n",
    "\n",
    "if oracle_auc_p < 0.05/N_BONFERRONI and oracle_auc_d > 0:\n",
    "    print(f\"  RANKING SIGNAL DETECTED (oracle AUC d={oracle_auc_d:+.3f}, p={oracle_auc_p:.2e})\")\n",
    "    if oracle_diff_p < 0.05/N_BONFERRONI:\n",
    "        print(f\"  DIFFERENTIAL CONFIRMED: priming helps relevant passages MORE (d={oracle_diff_d:+.3f})\")\n",
    "        print(f\"  >>> This is the v3 breakthrough that v2 could never achieve\")\n",
    "    else:\n",
    "        print(f\"  BUT differential is NS -- benefit may be uniform across passages\")\n",
    "else:\n",
    "    print(f\"  NO ranking signal (oracle AUC d={oracle_auc_d:+.3f}, p={oracle_auc_p:.2e})\")\n",
    "    if oracle_diff_p < 0.05/N_BONFERRONI and oracle_diff_d > 0:\n",
    "        print(f\"  BUT differential IS significant -- effect exists but too small for AUC\")\n",
    "    else:\n",
    "        print(f\"  Consistent with v2: benefit is document-independent, no differential\")\n",
    "\n",
    "# Check surrogates\n",
    "print(f\"\\n--- Surrogate ranking performance ---\")\n",
    "for cond in ['surr_template_trunc', 'surr_doc_trunc']:\n",
    "    cond_auc = analysis['auc'].get(cond, {}).get('mean', 0)\n",
    "    cond_p = analysis['auc'].get(cond, {}).get('p', 1)\n",
    "    if cond_p < 0.05/N_BONFERRONI and cond_auc > bare_auc:\n",
    "        ratio = (cond_auc - bare_auc) / (oracle_auc - bare_auc) * 100 if oracle_auc > bare_auc else 0\n",
    "        print(f\"  {cond}: AUC={cond_auc:.3f} ({ratio:.0f}% of oracle ranking gain)\")\n",
    "    else:\n",
    "        print(f\"  {cond}: AUC={cond_auc:.3f} (ns)\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "\n",
    "# Save results\n",
    "final_results = {\n",
    "    'experiment': 'exp04a_msmarco_ranking',\n",
    "    'model': MODEL_NAME,\n",
    "    'n_queries': N_SAMPLES,\n",
    "    'n_bonferroni': N_BONFERRONI,\n",
    "    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'analysis': analysis,\n",
    "    'diff_analysis': diff_analysis,\n",
    "    'hardness_analysis': hardness_analysis,\n",
    "    'v2_comparison': {\n",
    "        'exp22_bare_auc': 0.828,\n",
    "        'exp22_primed_auc': 0.829,\n",
    "    },\n",
    "    'pool_stats': {\n",
    "        'mean_passages_per_query': float(np.mean([r['n_passages'] for r in results])),\n",
    "        'total_passages': int(sum(r['n_passages'] for r in results)),\n",
    "    },\n",
    "}\n",
    "\n",
    "with open(RESULTS_DIR / 'results.json', 'w') as f:\n",
    "    json.dump(final_results, f, indent=2)\n",
    "print(f\"\\nResults saved to {RESULTS_DIR / 'results.json'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0845ecc7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T15:15:36.145034Z",
     "iopub.status.busy": "2026-02-18T15:15:36.144762Z",
     "iopub.status.idle": "2026-02-18T15:15:36.711893Z",
     "shell.execute_reply": "2026-02-18T15:15:36.710904Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning up GPU memory...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory: 15.03 GB -> 0.01 GB\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Cell 13: Cleanup\n",
    "print(\"Cleaning up GPU memory...\")\n",
    "mem_before = torch.cuda.memory_allocated() / 1e9\n",
    "del model, processor, tokenizer\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "mem_after = torch.cuda.memory_allocated() / 1e9\n",
    "print(f\"GPU memory: {mem_before:.2f} GB -> {mem_after:.2f} GB\")\n",
    "print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0d9aff0e57f84254994cadb344a0e084": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_e01e4a6e809045e0bc3b2506f1f0393f",
       "placeholder": "​",
       "style": "IPY_MODEL_90bde052585246eba6f96c28fdf4d5b8",
       "tabbable": null,
       "tooltip": null,
       "value": "Queries: 100%"
      }
     },
     "43fb011ff2534f9f96178465ddfe40b8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "486edd923e9342d6b04eb7e74c2f1264": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "4a38629fee144bb0a91d7928286a4cb1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_8e8950ab298c4073b5be7631858b9453",
       "max": 400.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_486edd923e9342d6b04eb7e74c2f1264",
       "tabbable": null,
       "tooltip": null,
       "value": 400.0
      }
     },
     "696c5e12c01b4833b0ae9137ef8897a3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "709576ddfb18463599b91b883ae7c778": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_9402f534585745a985a2b15b6db400ce",
       "placeholder": "​",
       "style": "IPY_MODEL_a372fe628f1242b896135512fe8d4070",
       "tabbable": null,
       "tooltip": null,
       "value": " 1327/1327 [00:56&lt;00:00, 133.95it/s, Materializing param=model.encoder.vision_tower.vision_model.post_layernorm.weight]"
      }
     },
     "813b1e15077d41aba03bc4a944f998b8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "88fb528add8b4ed18c33ae001a9cb77e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "8e8950ab298c4073b5be7631858b9453": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "8fb2d63791064a70b884826dad4dc5e9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "90bde052585246eba6f96c28fdf4d5b8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "9402f534585745a985a2b15b6db400ce": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "9a9f078d38834c609c8a7af9fd6fb91d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "a372fe628f1242b896135512fe8d4070": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "a9b50f51b5054bb7b51ff74e7b74e6bb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_0d9aff0e57f84254994cadb344a0e084",
        "IPY_MODEL_4a38629fee144bb0a91d7928286a4cb1",
        "IPY_MODEL_fe01a2251f5e4397a91293a17492bf27"
       ],
       "layout": "IPY_MODEL_fc07d957670c468295c4eb12d7819e69",
       "tabbable": null,
       "tooltip": null
      }
     },
     "c6b8a76dd71c4db395b486d70313de62": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_df475237b8e3498fbe6756c24bd9921d",
       "placeholder": "​",
       "style": "IPY_MODEL_88fb528add8b4ed18c33ae001a9cb77e",
       "tabbable": null,
       "tooltip": null,
       "value": "Loading weights: 100%"
      }
     },
     "d83683222a034e38b77afe280e749fe5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_813b1e15077d41aba03bc4a944f998b8",
       "max": 1327.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_9a9f078d38834c609c8a7af9fd6fb91d",
       "tabbable": null,
       "tooltip": null,
       "value": 1327.0
      }
     },
     "df475237b8e3498fbe6756c24bd9921d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e01e4a6e809045e0bc3b2506f1f0393f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e790da46140f43ebb8de4b47175a4dcb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_c6b8a76dd71c4db395b486d70313de62",
        "IPY_MODEL_d83683222a034e38b77afe280e749fe5",
        "IPY_MODEL_709576ddfb18463599b91b883ae7c778"
       ],
       "layout": "IPY_MODEL_696c5e12c01b4833b0ae9137ef8897a3",
       "tabbable": null,
       "tooltip": null
      }
     },
     "fc07d957670c468295c4eb12d7819e69": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "fe01a2251f5e4397a91293a17492bf27": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_43fb011ff2534f9f96178465ddfe40b8",
       "placeholder": "​",
       "style": "IPY_MODEL_8fb2d63791064a70b884826dad4dc5e9",
       "tabbable": null,
       "tooltip": null,
       "value": " 400/400 [1:08:52&lt;00:00, 11.47s/it]"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
