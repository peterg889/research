{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 04A: MS MARCO Ranking with Surrogate Co-Encoding\n",
    "## Does surrogate priming create differential ranking signal?\n",
    "\n",
    "### The critical question\n",
    "Exps 01-03B proved that co-encoding a surrogate with a document improves the document's\n",
    "representation (measured by NLL on a gold answer). But NLL improvement alone doesn't\n",
    "guarantee ranking improvement. The question:\n",
    "\n",
    "> Does priming help the **relevant** passage MORE than the **irrelevant** passages?\n",
    "\n",
    "If yes, NLL-based ranking improves. If no (priming helps all passages equally), the\n",
    "absolute NLL drops but relative ordering doesn't change.\n",
    "\n",
    "### v2 ranking history (all failed)\n",
    "| Exp | Method | Result |\n",
    "|-----|--------|--------|\n",
    "| 14 | Causal priming + ranking | AUC flat |\n",
    "| 15 | Values-only priming + ranking | AUC flat |\n",
    "| 22 | T5Gemma answer-likelihood | AUC=0.828, priming +0.001 |\n",
    "| 23 | Contrastive ranking | Failed |\n",
    "| 28 | Hinge loss ranking | Failed |\n",
    "| 31 | Query-likelihood ranking | QL AUC=0.578 (near chance) |\n",
    "\n",
    "**Why v2 failed**: Decoder-only value contamination was document-INDEPENDENT. The KV cache\n",
    "modification lowered NLL equally for relevant and irrelevant passages.\n",
    "\n",
    "**Why v3 might succeed**: Bidirectional encoder creates document-SPECIFIC representations.\n",
    "The surrogate tokens interact with document tokens differently depending on document content.\n",
    "\n",
    "### Scoring\n",
    "`NLL(answer | encode([condition + passage]))` -- same as Exps 01-03, applied to ALL\n",
    "candidate passages per query.\n",
    "\n",
    "### Conditions (6)\n",
    "| # | Condition | Encoder input | Purpose |\n",
    "|---|-----------|--------------|---------|\n",
    "| 1 | bare | passage only | Lower bound |\n",
    "| 2 | oracle\\_trunc | real query + passage | Upper bound |\n",
    "| 3 | surr\\_template\\_trunc | \"What is [keyword]?\" + passage | Best doc-derived (Exp 02) |\n",
    "| 4 | surr\\_doc\\_trunc | TF keywords + passage | Doc-derived control |\n",
    "| 5 | random\\_trunc | unrelated text + passage | Structural control |\n",
    "| 6 | static\\_fact\\_trunc | \"What are the key facts?\" + passage | Content-agnostic |\n",
    "\n",
    "### Metrics\n",
    "- **AUC** (binary: selected vs not-selected) per query\n",
    "- **MRR@3** (reciprocal rank of relevant passage in top-3)\n",
    "- **Hit@1**, **Hit@3** per query\n",
    "- **Differential signal**: delta\\_relevant vs delta\\_irrelevant\n",
    "\n",
    "### Statistical testing\n",
    "- Wilcoxon signed-rank for paired metric differences (condition vs bare)\n",
    "- Cohen's d on per-query metric differences\n",
    "- Bonferroni: 5 comparisons (5 non-bare conditions)\n",
    "\n",
    "### N=400 queries (~4000 passage scorings per condition)\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 2: Setup\n",
    "import os\n",
    "os.umask(0o000)\n",
    "\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "import gc\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "from collections import Counter, defaultdict\n",
    "from scipy import stats\n",
    "from scipy.stats import wilcoxon\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "sys.path.insert(0, \"../..\")\n",
    "\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "RESULTS_DIR = Path(\"../../results/exp04a\")\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CHECKPOINT_PATH = RESULTS_DIR / \"checkpoint.json\"\n",
    "\n",
    "N_SAMPLES = 400   # queries\n",
    "MODEL_NAME = \"google/t5gemma-2-4b-4b\"\n",
    "N_BONFERRONI = 5  # 5 non-bare conditions\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv())\n",
    "HF_TOKEN = os.environ.get(\"HF_TOKEN\")\n",
    "\n",
    "print(f\"Exp 04A: MS MARCO Ranking with Surrogate Co-Encoding\")\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"N queries: {N_SAMPLES}\")\n",
    "print(f\"Bonferroni comparisons: {N_BONFERRONI}\")\n",
    "print(f\"CUDA: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 3: Load model\n",
    "from transformers import AutoProcessor, AutoModelForSeq2SeqLM\n",
    "\n",
    "print(f\"Loading {MODEL_NAME}...\")\n",
    "processor = AutoProcessor.from_pretrained(MODEL_NAME, token=HF_TOKEN)\n",
    "tokenizer = processor.tokenizer\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    token=HF_TOKEN,\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "DEVICE = next(model.parameters()).device\n",
    "print(f\"Model loaded. dtype={next(model.parameters()).dtype}\")\n",
    "print(f\"GPU memory used: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 4: Scoring and ranking helpers\n",
    "\n",
    "def score_nll(encoder_text, answer_text, prefix_token_count=0, truncate=False):\n",
    "    '''Score NLL of answer tokens with optional truncation.'''\n",
    "    enc_ids = tokenizer(encoder_text, return_tensors=\"pt\",\n",
    "                        add_special_tokens=True, truncation=True,\n",
    "                        max_length=8192).input_ids.to(DEVICE)\n",
    "    total_enc_len = enc_ids.shape[1]\n",
    "\n",
    "    enc_mask = torch.ones(1, total_enc_len, device=DEVICE, dtype=torch.long)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        encoder_outputs = model.get_encoder()(\n",
    "            input_ids=enc_ids, attention_mask=enc_mask\n",
    "        )\n",
    "\n",
    "    if truncate and prefix_token_count > 0:\n",
    "        cross_attn_mask = torch.ones(1, total_enc_len, device=DEVICE, dtype=torch.long)\n",
    "        cross_attn_mask[:, :prefix_token_count] = 0\n",
    "    else:\n",
    "        cross_attn_mask = torch.ones(1, total_enc_len, device=DEVICE, dtype=torch.long)\n",
    "\n",
    "    ans_ids = tokenizer(answer_text, return_tensors=\"pt\",\n",
    "                        add_special_tokens=False, truncation=True,\n",
    "                        max_length=256).input_ids.to(DEVICE)\n",
    "\n",
    "    if ans_ids.shape[1] == 0:\n",
    "        return 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            encoder_outputs=encoder_outputs,\n",
    "            attention_mask=cross_attn_mask,\n",
    "            labels=ans_ids,\n",
    "        )\n",
    "\n",
    "    logits = outputs.logits\n",
    "    log_probs = F.log_softmax(logits, dim=-1)\n",
    "    token_log_probs = log_probs[0].gather(1, ans_ids[0].unsqueeze(1)).squeeze(1)\n",
    "    mean_nll = -token_log_probs.mean().item()\n",
    "\n",
    "    del encoder_outputs, outputs, logits, log_probs\n",
    "    return mean_nll\n",
    "\n",
    "\n",
    "def count_prefix_tokens(prefix_text, document_text):\n",
    "    '''Count how many tokens the prefix occupies in the concatenated encoding.'''\n",
    "    full_text = prefix_text + \"\\n\" + document_text\n",
    "    full_ids = tokenizer(full_text, add_special_tokens=True).input_ids\n",
    "    doc_ids = tokenizer(document_text, add_special_tokens=True).input_ids\n",
    "    return len(full_ids) - len(doc_ids)\n",
    "\n",
    "\n",
    "# === Surrogate generation ===\n",
    "STOP_WORDS = {\n",
    "    'a', 'an', 'the', 'is', 'are', 'was', 'were', 'be', 'been', 'being',\n",
    "    'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could',\n",
    "    'should', 'may', 'might', 'can', 'shall', 'to', 'of', 'in', 'for',\n",
    "    'on', 'with', 'at', 'by', 'from', 'as', 'into', 'through', 'during',\n",
    "    'before', 'after', 'above', 'below', 'between', 'and', 'but', 'or',\n",
    "    'not', 'no', 'if', 'then', 'than', 'so', 'up', 'out', 'about',\n",
    "    'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those',\n",
    "    'it', 'its', 'i', 'me', 'my', 'we', 'our', 'you', 'your', 'he',\n",
    "    'him', 'his', 'she', 'her', 'they', 'them', 'their', 'how', 'when',\n",
    "    'where', 'why', 'much', 'many', 'some', 'any', 'all', 'each',\n",
    "    'does', 'also', 'just', 'more', 'most', 'very', 'too', 'only',\n",
    "}\n",
    "\n",
    "def extract_keywords(text):\n",
    "    words = re.sub(r'[^\\w\\s]', '', text.lower()).split()\n",
    "    return [w for w in words if w not in STOP_WORDS and len(w) > 2]\n",
    "\n",
    "def make_surrogate_doc_kw(passage):\n",
    "    content_words = extract_keywords(passage)\n",
    "    if not content_words:\n",
    "        return \"information\"\n",
    "    counts = Counter(content_words)\n",
    "    return \" \".join(w for w, _ in counts.most_common(5))\n",
    "\n",
    "def make_surrogate_template(passage):\n",
    "    content_words = extract_keywords(passage)\n",
    "    if not content_words:\n",
    "        return \"What is this about?\"\n",
    "    counts = Counter(content_words)\n",
    "    top_word = counts.most_common(1)[0][0]\n",
    "    return f\"What is {top_word}?\"\n",
    "\n",
    "STATIC_FACT = \"What are the key facts I need to know?\"\n",
    "\n",
    "\n",
    "# === Ranking metrics ===\n",
    "def compute_auc(nlls, relevant_idx):\n",
    "    '''AUC when exactly one passage is relevant. Lower NLL = more relevant.'''\n",
    "    rel_nll = nlls[relevant_idx]\n",
    "    irrel_nlls = [nlls[i] for i in range(len(nlls)) if i != relevant_idx]\n",
    "    n_irrel = len(irrel_nlls)\n",
    "    if n_irrel == 0:\n",
    "        return 0.5\n",
    "    wins = sum(1 for nll in irrel_nlls if nll > rel_nll)\n",
    "    ties = sum(1 for nll in irrel_nlls if nll == rel_nll)\n",
    "    return (wins + 0.5 * ties) / n_irrel\n",
    "\n",
    "def compute_mrr_at_k(nlls, relevant_idx, k=3):\n",
    "    '''MRR@k: reciprocal rank of relevant passage in top-k by ascending NLL.'''\n",
    "    ranked_indices = list(np.argsort(nlls))\n",
    "    for rank, idx in enumerate(ranked_indices[:k], 1):\n",
    "        if idx == relevant_idx:\n",
    "            return 1.0 / rank\n",
    "    return 0.0\n",
    "\n",
    "def compute_hit_at_k(nlls, relevant_idx, k=1):\n",
    "    '''Hit@k: 1 if relevant passage is in top-k by ascending NLL.'''\n",
    "    ranked_indices = set(np.argsort(nlls)[:k].tolist())\n",
    "    return 1.0 if relevant_idx in ranked_indices else 0.0\n",
    "\n",
    "print(\"Helpers defined.\")\n",
    "print(\"  Scoring: score_nll (answer-likelihood)\")\n",
    "print(\"  Surrogates: doc_kw, template, static_fact, random\")\n",
    "print(\"  Ranking metrics: AUC, MRR@k, Hit@k\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 5: Load MS MARCO ranking data\n",
    "from lib.data import count_words\n",
    "from datasets import load_dataset\n",
    "\n",
    "print(\"Loading MS MARCO v1.1 validation...\")\n",
    "ds = load_dataset(\"microsoft/ms_marco\", \"v1.1\", split=\"validation\")\n",
    "\n",
    "# Collect queries with full passage pools for ranking\n",
    "queries = []\n",
    "\n",
    "for item in ds:\n",
    "    passages_data = item.get('passages', {})\n",
    "    ptexts = passages_data.get('passage_text', [])\n",
    "    is_sel = passages_data.get('is_selected', [])\n",
    "    query = item.get('query', '')\n",
    "    answers = item.get('answers', [])\n",
    "    well_formed = item.get('wellFormedAnswers', [])\n",
    "\n",
    "    # Get best answer\n",
    "    answer = None\n",
    "    if well_formed and len(well_formed) > 0 and well_formed[0] not in ('[]', ''):\n",
    "        answer = well_formed[0]\n",
    "    elif answers and len(answers) > 0 and answers[0] != 'No Answer Present.':\n",
    "        answer = answers[0]\n",
    "\n",
    "    if not answer:\n",
    "        continue\n",
    "\n",
    "    # Check passage pool: all passages 30-300 words\n",
    "    word_counts = [count_words(pt) for pt in ptexts]\n",
    "    if not all(30 <= wc <= 300 for wc in word_counts):\n",
    "        continue\n",
    "\n",
    "    # Exactly 1 selected, 2+ non-selected\n",
    "    n_selected = sum(is_sel)\n",
    "    n_not_selected = len(is_sel) - n_selected\n",
    "    if n_selected != 1 or n_not_selected < 2:\n",
    "        continue\n",
    "\n",
    "    # Find relevant passage index\n",
    "    relevant_idx = is_sel.index(1)\n",
    "\n",
    "    passages = []\n",
    "    for p_idx, (pt, sel) in enumerate(zip(ptexts, is_sel)):\n",
    "        passages.append({\n",
    "            'text': pt,\n",
    "            'is_selected': sel,\n",
    "            'word_count': word_counts[p_idx],\n",
    "            'surr_doc_kw': make_surrogate_doc_kw(pt),\n",
    "            'surr_template': make_surrogate_template(pt),\n",
    "        })\n",
    "\n",
    "    queries.append({\n",
    "        'query': query,\n",
    "        'answer': answer,\n",
    "        'passages': passages,\n",
    "        'relevant_idx': relevant_idx,\n",
    "        'n_passages': len(passages),\n",
    "    })\n",
    "\n",
    "    if len(queries) >= N_SAMPLES * 3:\n",
    "        break\n",
    "\n",
    "del ds\n",
    "gc.collect()\n",
    "\n",
    "# Shuffle and select N_SAMPLES\n",
    "np.random.seed(SEED)\n",
    "np.random.shuffle(queries)\n",
    "queries = queries[:N_SAMPLES]\n",
    "\n",
    "# Generate per-query random surrogates (from another query's relevant passage)\n",
    "for i, q in enumerate(queries):\n",
    "    other_idx = (i + N_SAMPLES // 2) % len(queries)\n",
    "    other_passage = queries[other_idx]['passages'][queries[other_idx]['relevant_idx']]['text']\n",
    "    q['surr_random'] = \" \".join(other_passage.split()[:20])\n",
    "\n",
    "# Stats\n",
    "n_passages_list = [q['n_passages'] for q in queries]\n",
    "print(f\"Selected {len(queries)} queries for ranking\")\n",
    "print(f\"Passages per query: mean={np.mean(n_passages_list):.1f}, \"\n",
    "      f\"median={np.median(n_passages_list):.0f}, \"\n",
    "      f\"min={np.min(n_passages_list)}, max={np.max(n_passages_list)}\")\n",
    "print(f\"Total passage scorings per condition: {sum(n_passages_list)}\")\n",
    "total_calls = sum(n_passages_list) * 6  # 6 conditions\n",
    "print(f\"Total scoring calls: {total_calls}\")\n",
    "print(f\"Estimated runtime: ~{total_calls * 0.4 / 3600:.1f} hours\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 6: Explain conditions with concrete example\n",
    "print(\"=\" * 70)\n",
    "print(\"CONDITION EXAMPLES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "COND_NAMES = ['bare', 'oracle_trunc', 'surr_template_trunc',\n",
    "              'surr_doc_trunc', 'random_trunc', 'static_fact_trunc']\n",
    "\n",
    "ex = queries[0]\n",
    "print(f\"\\nQuery: {ex['query']}\")\n",
    "print(f\"Answer: {ex['answer']}\")\n",
    "print(f\"Passages: {ex['n_passages']} ({ex['n_passages']-1} irrelevant, 1 relevant at idx {ex['relevant_idx']})\")\n",
    "\n",
    "# Show relevant and one irrelevant passage\n",
    "rel_p = ex['passages'][ex['relevant_idx']]\n",
    "irr_p = ex['passages'][0 if ex['relevant_idx'] != 0 else 1]\n",
    "\n",
    "print(f\"\\n--- Relevant passage (idx {ex['relevant_idx']}) ---\")\n",
    "print(f\"  Text: {rel_p['text'][:120]}...\")\n",
    "print(f\"  surr_doc_kw: {rel_p['surr_doc_kw']}\")\n",
    "print(f\"  surr_template: {rel_p['surr_template']}\")\n",
    "\n",
    "irr_idx = 0 if ex['relevant_idx'] != 0 else 1\n",
    "print(f\"\\n--- Irrelevant passage (idx {irr_idx}) ---\")\n",
    "print(f\"  Text: {irr_p['text'][:120]}...\")\n",
    "print(f\"  surr_doc_kw: {irr_p['surr_doc_kw']}\")\n",
    "print(f\"  surr_template: {irr_p['surr_template']}\")\n",
    "\n",
    "print(f\"\\n--- What the encoder sees for the relevant passage ---\")\n",
    "for cond in COND_NAMES:\n",
    "    if cond == 'bare':\n",
    "        enc = rel_p['text'][:80] + \"...\"\n",
    "    elif cond == 'oracle_trunc':\n",
    "        enc = ex['query'] + \" | \" + rel_p['text'][:60] + \"...\"\n",
    "    elif cond == 'surr_template_trunc':\n",
    "        enc = rel_p['surr_template'] + \" | \" + rel_p['text'][:60] + \"...\"\n",
    "    elif cond == 'surr_doc_trunc':\n",
    "        enc = rel_p['surr_doc_kw'] + \" | \" + rel_p['text'][:60] + \"...\"\n",
    "    elif cond == 'random_trunc':\n",
    "        enc = ex['surr_random'][:40] + \"... | \" + rel_p['text'][:40] + \"...\"\n",
    "    elif cond == 'static_fact_trunc':\n",
    "        enc = STATIC_FACT + \" | \" + rel_p['text'][:60] + \"...\"\n",
    "    print(f\"  {cond:<22s}: {enc}\")\n",
    "\n",
    "print(f\"\\n--- Key insight ---\")\n",
    "print(f\"  Oracle surrogate = real query (same for all passages of this query)\")\n",
    "print(f\"  Doc-derived surrogates (template, doc_kw) are PER-PASSAGE (different content)\")\n",
    "print(f\"  Random and static_fact are query-level (same for all passages)\")\n",
    "print(f\"  If oracle helps relevant MORE than irrelevant -> ranking improves\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 7: Run scoring -- outer loop over queries\n",
    "print(\"=\" * 70)\n",
    "print(\"RUNNING RANKING EXPERIMENT\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "def build_condition_input(cond_name, passage_data, query_data):\n",
    "    '''Return (encoder_text, prefix_token_count, truncate) for a condition.'''\n",
    "    passage_text = passage_data['text']\n",
    "\n",
    "    if cond_name == 'bare':\n",
    "        return passage_text, 0, False\n",
    "    elif cond_name == 'oracle_trunc':\n",
    "        surr = query_data['query']\n",
    "    elif cond_name == 'surr_template_trunc':\n",
    "        surr = passage_data['surr_template']\n",
    "    elif cond_name == 'surr_doc_trunc':\n",
    "        surr = passage_data['surr_doc_kw']\n",
    "    elif cond_name == 'random_trunc':\n",
    "        surr = query_data['surr_random']\n",
    "    elif cond_name == 'static_fact_trunc':\n",
    "        surr = STATIC_FACT\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown condition: {cond_name}\")\n",
    "\n",
    "    enc_text = surr + \"\\n\" + passage_text\n",
    "    prefix_count = count_prefix_tokens(surr, passage_text)\n",
    "    return enc_text, prefix_count, True\n",
    "\n",
    "\n",
    "# Resume from checkpoint\n",
    "results = []\n",
    "start_idx = 0\n",
    "if CHECKPOINT_PATH.exists():\n",
    "    saved = json.loads(CHECKPOINT_PATH.read_text())\n",
    "    if saved.get('n_total') == N_SAMPLES:\n",
    "        saved_results = saved.get('results', [])\n",
    "        # Validate alignment\n",
    "        saved_queries = [r['query'][:50] for r in saved_results]\n",
    "        current_queries = [q['query'][:50] for q in queries[:len(saved_results)]]\n",
    "        if saved_queries == current_queries:\n",
    "            results = saved_results\n",
    "            start_idx = len(results)\n",
    "            print(f\"Resumed from checkpoint: {start_idx}/{N_SAMPLES} queries\")\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "for q_idx in tqdm(range(start_idx, N_SAMPLES), initial=start_idx, total=N_SAMPLES,\n",
    "                  desc=\"Queries\"):\n",
    "    q = queries[q_idx]\n",
    "    answer = q['answer']\n",
    "\n",
    "    query_result = {\n",
    "        'query_idx': q_idx,\n",
    "        'query': q['query'],\n",
    "        'answer': answer,\n",
    "        'n_passages': q['n_passages'],\n",
    "        'relevant_idx': q['relevant_idx'],\n",
    "        'is_selected': [p['is_selected'] for p in q['passages']],\n",
    "        'scores': {},\n",
    "    }\n",
    "\n",
    "    for cond_name in COND_NAMES:\n",
    "        cond_nlls = []\n",
    "        for p_idx, passage_data in enumerate(q['passages']):\n",
    "            enc_text, prefix_count, truncate = build_condition_input(\n",
    "                cond_name, passage_data, q)\n",
    "            nll = score_nll(enc_text, answer, prefix_count, truncate)\n",
    "            cond_nlls.append(nll)\n",
    "        query_result['scores'][cond_name] = cond_nlls\n",
    "\n",
    "    results.append(query_result)\n",
    "\n",
    "    if (q_idx + 1) % 20 == 0 or q_idx == N_SAMPLES - 1:\n",
    "        ckpt = {\n",
    "            'n_total': N_SAMPLES,\n",
    "            'results': results,\n",
    "            'completed': len(results),\n",
    "            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        }\n",
    "        CHECKPOINT_PATH.write_text(json.dumps(ckpt))\n",
    "        elapsed = time.time() - t0\n",
    "        done = q_idx - start_idx + 1\n",
    "        eta = (N_SAMPLES - q_idx - 1) * elapsed / done if done > 0 else 0\n",
    "        tqdm.write(f\"  Checkpoint {q_idx+1}/{N_SAMPLES} | {elapsed/60:.1f}m | ETA {eta/60:.1f}m\")\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "elapsed_total = time.time() - t0\n",
    "print(f\"\\nScoring complete: {len(results)} queries in {elapsed_total/60:.1f} min\")\n",
    "\n",
    "# Quick peek at bare AUC\n",
    "bare_aucs = []\n",
    "for r in results:\n",
    "    nlls = np.array(r['scores']['bare'])\n",
    "    bare_aucs.append(compute_auc(nlls, r['relevant_idx']))\n",
    "print(f\"Bare AUC: mean={np.mean(bare_aucs):.3f}, median={np.median(bare_aucs):.3f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 8: Compute ranking metrics for all conditions\n",
    "print(\"=\" * 70)\n",
    "print(\"COMPUTING RANKING METRICS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# For each query x condition, compute AUC, MRR@3, Hit@1, Hit@3\n",
    "metrics = {cond: {'auc': [], 'mrr3': [], 'hit1': [], 'hit3': []}\n",
    "           for cond in COND_NAMES}\n",
    "\n",
    "for r in results:\n",
    "    rel_idx = r['relevant_idx']\n",
    "    for cond in COND_NAMES:\n",
    "        nlls = np.array(r['scores'][cond])\n",
    "        metrics[cond]['auc'].append(compute_auc(nlls, rel_idx))\n",
    "        metrics[cond]['mrr3'].append(compute_mrr_at_k(nlls, rel_idx, k=3))\n",
    "        metrics[cond]['hit1'].append(compute_hit_at_k(nlls, rel_idx, k=1))\n",
    "        metrics[cond]['hit3'].append(compute_hit_at_k(nlls, rel_idx, k=3))\n",
    "\n",
    "# Convert to arrays\n",
    "for cond in COND_NAMES:\n",
    "    for m in metrics[cond]:\n",
    "        metrics[cond][m] = np.array(metrics[cond][m])\n",
    "\n",
    "# Quick summary\n",
    "for cond in COND_NAMES:\n",
    "    print(f\"  {cond:<22s}: AUC={metrics[cond]['auc'].mean():.3f}  \"\n",
    "          f\"MRR@3={metrics[cond]['mrr3'].mean():.3f}  \"\n",
    "          f\"Hit@1={metrics[cond]['hit1'].mean():.3f}  \"\n",
    "          f\"Hit@3={metrics[cond]['hit3'].mean():.3f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 9: Results table with statistical tests\n",
    "from lib.analysis import cohens_d\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"RESULTS: Ranking Metrics per Condition (N=%d queries)\" % N_SAMPLES)\n",
    "print(\"=\" * 70)\n",
    "\n",
    "METRIC_NAMES = ['auc', 'mrr3', 'hit1', 'hit3']\n",
    "METRIC_LABELS = {'auc': 'AUC', 'mrr3': 'MRR@3', 'hit1': 'Hit@1', 'hit3': 'Hit@3'}\n",
    "\n",
    "analysis = {}\n",
    "\n",
    "for metric_name in METRIC_NAMES:\n",
    "    print(f\"\\n--- {METRIC_LABELS[metric_name]} ---\")\n",
    "    print(f\"  {'Condition':<22} {'Mean':>8} {'vs Bare':>10} {'d':>8} {'p':>12} {'sig':>5}\")\n",
    "    print(f\"  {'-'*70}\")\n",
    "\n",
    "    bare_vals = metrics['bare'][metric_name]\n",
    "    analysis[metric_name] = {}\n",
    "\n",
    "    for cond in COND_NAMES:\n",
    "        vals = metrics[cond][metric_name]\n",
    "        mean_val = vals.mean()\n",
    "\n",
    "        if cond == 'bare':\n",
    "            print(f\"  {cond:<22} {mean_val:>8.3f} {'--':>10} {'--':>8} {'--':>12} {'--':>5}\")\n",
    "            analysis[metric_name][cond] = {'mean': float(mean_val)}\n",
    "        else:\n",
    "            diff = vals - bare_vals\n",
    "            d = cohens_d(diff)\n",
    "\n",
    "            # Wilcoxon signed-rank test (non-parametric, appropriate for ranking metrics)\n",
    "            nonzero = diff[diff != 0]\n",
    "            if len(nonzero) >= 10:\n",
    "                try:\n",
    "                    stat, p_val = wilcoxon(nonzero)\n",
    "                except ValueError:\n",
    "                    p_val = 1.0\n",
    "            else:\n",
    "                p_val = 1.0\n",
    "\n",
    "            sig = ('***' if p_val < 0.001/N_BONFERRONI else\n",
    "                   '**' if p_val < 0.01/N_BONFERRONI else\n",
    "                   '*' if p_val < 0.05/N_BONFERRONI else 'ns')\n",
    "\n",
    "            print(f\"  {cond:<22} {mean_val:>8.3f} {diff.mean():>+10.4f} {d:>+8.3f} {p_val:>12.2e} {sig:>5}\")\n",
    "            analysis[metric_name][cond] = {\n",
    "                'mean': float(mean_val), 'delta': float(diff.mean()),\n",
    "                'd': float(d), 'p': float(p_val),\n",
    "            }\n",
    "\n",
    "# Headline result\n",
    "oracle_auc = analysis['auc']['oracle_trunc']['mean']\n",
    "bare_auc = analysis['auc']['bare']['mean']\n",
    "auc_gain = oracle_auc - bare_auc\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"HEADLINE: oracle_trunc AUC = {oracle_auc:.3f} vs bare AUC = {bare_auc:.3f} (gain = {auc_gain:+.3f})\")\n",
    "if auc_gain > 0.01:\n",
    "    print(f\"  >>> BREAKTHROUGH: Ranking signal detected! v2 never achieved this.\")\n",
    "elif auc_gain > 0.005:\n",
    "    print(f\"  >>> Marginal ranking signal (v2 Exp 22 got +0.001)\")\n",
    "else:\n",
    "    print(f\"  >>> No ranking signal -- same as v2\")\n",
    "print(f\"v2 Exp 22 reference: bare AUC = 0.828, primed AUC = 0.829\")\n",
    "print(f\"{'='*70}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 10: Differential signal analysis\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"DIFFERENTIAL SIGNAL ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "print(\"Core test: does priming help the RELEVANT passage MORE than IRRELEVANT ones?\")\n",
    "\n",
    "# Compute per-query differential: delta_rel vs mean(delta_irrel)\n",
    "diff_analysis = {}\n",
    "for cond in COND_NAMES[1:]:  # skip bare\n",
    "    delta_rels = []\n",
    "    delta_irrels = []\n",
    "\n",
    "    for r in results:\n",
    "        rel_idx = r['relevant_idx']\n",
    "        bare_nlls = r['scores']['bare']\n",
    "        cond_nlls = r['scores'][cond]\n",
    "\n",
    "        # Delta for relevant passage (positive = priming helped)\n",
    "        delta_rel = bare_nlls[rel_idx] - cond_nlls[rel_idx]\n",
    "\n",
    "        # Mean delta for irrelevant passages\n",
    "        irrel_deltas = [bare_nlls[i] - cond_nlls[i]\n",
    "                        for i in range(len(bare_nlls)) if i != rel_idx]\n",
    "        delta_irrel = np.mean(irrel_deltas)\n",
    "\n",
    "        delta_rels.append(delta_rel)\n",
    "        delta_irrels.append(delta_irrel)\n",
    "\n",
    "    delta_rels = np.array(delta_rels)\n",
    "    delta_irrels = np.array(delta_irrels)\n",
    "    differential = delta_rels - delta_irrels  # positive = helps ranking\n",
    "\n",
    "    # Test: is differential > 0?\n",
    "    d = cohens_d(differential)\n",
    "    nonzero = differential[differential != 0]\n",
    "    if len(nonzero) >= 10:\n",
    "        try:\n",
    "            stat, p_val = wilcoxon(nonzero)\n",
    "            # One-sided: we care about positive differential\n",
    "            p_val_onesided = p_val / 2 if np.mean(differential) > 0 else 1 - p_val / 2\n",
    "        except ValueError:\n",
    "            p_val_onesided = 1.0\n",
    "    else:\n",
    "        p_val_onesided = 1.0\n",
    "\n",
    "    diff_analysis[cond] = {\n",
    "        'delta_rel_mean': float(delta_rels.mean()),\n",
    "        'delta_irrel_mean': float(delta_irrels.mean()),\n",
    "        'differential_mean': float(differential.mean()),\n",
    "        'd': float(d),\n",
    "        'p_onesided': float(p_val_onesided),\n",
    "        'pct_positive': float(100 * np.mean(differential > 0)),\n",
    "    }\n",
    "\n",
    "    sig = ('***' if p_val_onesided < 0.001/N_BONFERRONI else\n",
    "           '**' if p_val_onesided < 0.01/N_BONFERRONI else\n",
    "           '*' if p_val_onesided < 0.05/N_BONFERRONI else 'ns')\n",
    "\n",
    "    print(f\"\\n  {cond}:\")\n",
    "    print(f\"    delta_rel  (mean NLL drop for relevant):   {delta_rels.mean():+.4f}\")\n",
    "    print(f\"    delta_irrel (mean NLL drop for irrelevant): {delta_irrels.mean():+.4f}\")\n",
    "    print(f\"    differential (rel - irrel):                 {differential.mean():+.4f}  \"\n",
    "          f\"d={d:+.3f}  p={p_val_onesided:.2e}  {sig}\")\n",
    "    print(f\"    % queries where relevant helped MORE:       {100*np.mean(differential > 0):.1f}%\")\n",
    "\n",
    "# Plot: delta_rel vs delta_irrel scatter for oracle\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "for ax_idx, cond in enumerate(['oracle_trunc', 'surr_template_trunc', 'random_trunc']):\n",
    "    ax = axes[ax_idx]\n",
    "    delta_rels = []\n",
    "    delta_irrels = []\n",
    "    for r in results:\n",
    "        rel_idx = r['relevant_idx']\n",
    "        bare_nlls = r['scores']['bare']\n",
    "        cond_nlls = r['scores'][cond]\n",
    "        delta_rels.append(bare_nlls[rel_idx] - cond_nlls[rel_idx])\n",
    "        irrel_deltas = [bare_nlls[i] - cond_nlls[i]\n",
    "                        for i in range(len(bare_nlls)) if i != rel_idx]\n",
    "        delta_irrels.append(np.mean(irrel_deltas))\n",
    "\n",
    "    ax.scatter(delta_irrels, delta_rels, alpha=0.3, s=10)\n",
    "    lims = [min(min(delta_irrels), min(delta_rels)) - 0.5,\n",
    "            max(max(delta_irrels), max(delta_rels)) + 0.5]\n",
    "    ax.plot(lims, lims, 'r--', alpha=0.5, label='equal help')\n",
    "    ax.set_xlabel('delta_irrelevant (mean NLL drop)')\n",
    "    ax.set_ylabel('delta_relevant (NLL drop)')\n",
    "    ax.set_title(f'{cond.replace(\"_trunc\", \"\")}')\n",
    "    ax.legend(fontsize=8)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Differential Signal: Points ABOVE red line = ranking improves', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plot_path = RESULTS_DIR / 'differential_signal.png'\n",
    "plt.savefig(plot_path, dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"\\nPlot saved to {plot_path}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 11: Hardness stratification by bare AUC\n",
    "print(\"=\" * 70)\n",
    "print(\"HARDNESS STRATIFICATION\")\n",
    "print(\"=\" * 70)\n",
    "print(\"Split queries by bare AUC quintiles, check if priming helps more for hard queries\")\n",
    "\n",
    "bare_aucs = metrics['bare']['auc']\n",
    "\n",
    "# Quintile boundaries\n",
    "quintile_boundaries = np.percentile(bare_aucs, [20, 40, 60, 80])\n",
    "quintile_labels = ['Q1 (hardest)', 'Q2', 'Q3', 'Q4', 'Q5 (easiest)']\n",
    "\n",
    "def get_quintile(auc):\n",
    "    for q, bound in enumerate(quintile_boundaries):\n",
    "        if auc <= bound:\n",
    "            return q\n",
    "    return 4\n",
    "\n",
    "quintile_assignments = np.array([get_quintile(a) for a in bare_aucs])\n",
    "\n",
    "print(f\"\\nQuintile boundaries (bare AUC): {quintile_boundaries}\")\n",
    "for q in range(5):\n",
    "    mask = quintile_assignments == q\n",
    "    n = mask.sum()\n",
    "    mean_bare_auc = bare_aucs[mask].mean()\n",
    "    print(f\"  {quintile_labels[q]}: N={n}, mean bare AUC={mean_bare_auc:.3f}\")\n",
    "\n",
    "# Per-quintile metric gains\n",
    "print(f\"\\n--- AUC gain by hardness quintile ---\")\n",
    "header = f\"  {'Quintile':<16}\"\n",
    "for cond in COND_NAMES[1:]:\n",
    "    short = cond.replace('_trunc', '')\n",
    "    header += f\" {short:>14}\"\n",
    "print(header)\n",
    "print(f\"  {'-'*(16 + 15 * len(COND_NAMES[1:]))}\")\n",
    "\n",
    "hardness_analysis = {}\n",
    "for q in range(5):\n",
    "    mask = quintile_assignments == q\n",
    "    row = f\"  {quintile_labels[q]:<16}\"\n",
    "    hardness_analysis[quintile_labels[q]] = {}\n",
    "\n",
    "    for cond in COND_NAMES[1:]:\n",
    "        cond_aucs = metrics[cond]['auc'][mask]\n",
    "        bare_q_aucs = bare_aucs[mask]\n",
    "        gain = (cond_aucs - bare_q_aucs).mean()\n",
    "        d = cohens_d(cond_aucs - bare_q_aucs) if mask.sum() > 1 else 0\n",
    "        row += f\" {gain:>+7.3f} d={d:>+5.2f}\"\n",
    "        hardness_analysis[quintile_labels[q]][cond] = {\n",
    "            'auc_gain': float(gain), 'd': float(d)}\n",
    "\n",
    "    print(row)\n",
    "\n",
    "# Plot: oracle gain vs hardness quintile\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "for cond in ['oracle_trunc', 'surr_template_trunc', 'random_trunc']:\n",
    "    gains = [hardness_analysis[quintile_labels[q]].get(cond, {}).get('auc_gain', 0)\n",
    "             for q in range(5)]\n",
    "    ax.plot(range(5), gains, '-o', label=cond.replace('_trunc', ''), markersize=8)\n",
    "\n",
    "ax.set_xticks(range(5))\n",
    "ax.set_xticklabels(quintile_labels, rotation=15)\n",
    "ax.set_ylabel('AUC gain vs bare')\n",
    "ax.set_title('Ranking Gain by Query Hardness (bare AUC quintiles)')\n",
    "ax.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plot_path = RESULTS_DIR / 'hardness_stratification.png'\n",
    "plt.savefig(plot_path, dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"Plot saved to {plot_path}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 12: Verdict and save results\n",
    "print(\"=\" * 70)\n",
    "print(\"VERDICT -- Exp 04A: MS MARCO Ranking\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\nModel: {MODEL_NAME}\")\n",
    "print(f\"N queries: {N_SAMPLES}\")\n",
    "print(f\"Mean passages per query: {np.mean([r['n_passages'] for r in results]):.1f}\")\n",
    "\n",
    "# Key results\n",
    "print(f\"\\n--- Ranking metrics summary ---\")\n",
    "for metric_name in METRIC_NAMES:\n",
    "    print(f\"\\n  {METRIC_LABELS[metric_name]}:\")\n",
    "    bare_val = analysis[metric_name]['bare']['mean']\n",
    "    print(f\"    bare:           {bare_val:.3f}\")\n",
    "    for cond in COND_NAMES[1:]:\n",
    "        a = analysis[metric_name].get(cond, {})\n",
    "        mean = a.get('mean', 0)\n",
    "        delta = a.get('delta', 0)\n",
    "        d = a.get('d', 0)\n",
    "        p = a.get('p', 1)\n",
    "        sig = ('***' if p < 0.001/N_BONFERRONI else '**' if p < 0.01/N_BONFERRONI\n",
    "               else '*' if p < 0.05/N_BONFERRONI else 'ns')\n",
    "        print(f\"    {cond:<22s}: {mean:.3f} ({delta:+.4f}, d={d:+.3f}) {sig}\")\n",
    "\n",
    "# v2 comparison\n",
    "print(f\"\\n--- v2 comparison ---\")\n",
    "print(f\"  v2 Exp 22 (T5Gemma, same model, no truncation):\")\n",
    "print(f\"    bare AUC = 0.828, primed AUC = 0.829 (gain = +0.001)\")\n",
    "print(f\"  v3 Exp 04A (with truncation + surrogate co-encoding):\")\n",
    "oracle_auc = analysis['auc']['oracle_trunc']['mean']\n",
    "bare_auc = analysis['auc']['bare']['mean']\n",
    "print(f\"    bare AUC = {bare_auc:.3f}, oracle AUC = {oracle_auc:.3f} (gain = {oracle_auc - bare_auc:+.3f})\")\n",
    "\n",
    "# Differential verdict\n",
    "print(f\"\\n--- Differential signal verdict ---\")\n",
    "for cond in COND_NAMES[1:]:\n",
    "    da = diff_analysis.get(cond, {})\n",
    "    diff_mean = da.get('differential_mean', 0)\n",
    "    d = da.get('d', 0)\n",
    "    p = da.get('p_onesided', 1)\n",
    "    pct = da.get('pct_positive', 0)\n",
    "    sig = ('***' if p < 0.001/N_BONFERRONI else '**' if p < 0.01/N_BONFERRONI\n",
    "           else '*' if p < 0.05/N_BONFERRONI else 'ns')\n",
    "    print(f\"  {cond:<22s}: differential={diff_mean:+.4f} d={d:+.3f} {pct:.0f}% positive {sig}\")\n",
    "\n",
    "# Overall verdict\n",
    "print(f\"\\n--- OVERALL VERDICT ---\")\n",
    "oracle_auc_d = analysis['auc'].get('oracle_trunc', {}).get('d', 0)\n",
    "oracle_auc_p = analysis['auc'].get('oracle_trunc', {}).get('p', 1)\n",
    "oracle_diff_d = diff_analysis.get('oracle_trunc', {}).get('d', 0)\n",
    "oracle_diff_p = diff_analysis.get('oracle_trunc', {}).get('p_onesided', 1)\n",
    "\n",
    "if oracle_auc_p < 0.05/N_BONFERRONI and oracle_auc_d > 0:\n",
    "    print(f\"  RANKING SIGNAL DETECTED (oracle AUC d={oracle_auc_d:+.3f}, p={oracle_auc_p:.2e})\")\n",
    "    if oracle_diff_p < 0.05/N_BONFERRONI:\n",
    "        print(f\"  DIFFERENTIAL CONFIRMED: priming helps relevant passages MORE (d={oracle_diff_d:+.3f})\")\n",
    "        print(f\"  >>> This is the v3 breakthrough that v2 could never achieve\")\n",
    "    else:\n",
    "        print(f\"  BUT differential is NS -- benefit may be uniform across passages\")\n",
    "else:\n",
    "    print(f\"  NO ranking signal (oracle AUC d={oracle_auc_d:+.3f}, p={oracle_auc_p:.2e})\")\n",
    "    if oracle_diff_p < 0.05/N_BONFERRONI and oracle_diff_d > 0:\n",
    "        print(f\"  BUT differential IS significant -- effect exists but too small for AUC\")\n",
    "    else:\n",
    "        print(f\"  Consistent with v2: benefit is document-independent, no differential\")\n",
    "\n",
    "# Check surrogates\n",
    "print(f\"\\n--- Surrogate ranking performance ---\")\n",
    "for cond in ['surr_template_trunc', 'surr_doc_trunc']:\n",
    "    cond_auc = analysis['auc'].get(cond, {}).get('mean', 0)\n",
    "    cond_p = analysis['auc'].get(cond, {}).get('p', 1)\n",
    "    if cond_p < 0.05/N_BONFERRONI and cond_auc > bare_auc:\n",
    "        ratio = (cond_auc - bare_auc) / (oracle_auc - bare_auc) * 100 if oracle_auc > bare_auc else 0\n",
    "        print(f\"  {cond}: AUC={cond_auc:.3f} ({ratio:.0f}% of oracle ranking gain)\")\n",
    "    else:\n",
    "        print(f\"  {cond}: AUC={cond_auc:.3f} (ns)\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "\n",
    "# Save results\n",
    "final_results = {\n",
    "    'experiment': 'exp04a_msmarco_ranking',\n",
    "    'model': MODEL_NAME,\n",
    "    'n_queries': N_SAMPLES,\n",
    "    'n_bonferroni': N_BONFERRONI,\n",
    "    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'analysis': analysis,\n",
    "    'diff_analysis': diff_analysis,\n",
    "    'hardness_analysis': hardness_analysis,\n",
    "    'v2_comparison': {\n",
    "        'exp22_bare_auc': 0.828,\n",
    "        'exp22_primed_auc': 0.829,\n",
    "    },\n",
    "    'pool_stats': {\n",
    "        'mean_passages_per_query': float(np.mean([r['n_passages'] for r in results])),\n",
    "        'total_passages': int(sum(r['n_passages'] for r in results)),\n",
    "    },\n",
    "}\n",
    "\n",
    "with open(RESULTS_DIR / 'results.json', 'w') as f:\n",
    "    json.dump(final_results, f, indent=2)\n",
    "print(f\"\\nResults saved to {RESULTS_DIR / 'results.json'}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 13: Cleanup\n",
    "print(\"Cleaning up GPU memory...\")\n",
    "mem_before = torch.cuda.memory_allocated() / 1e9\n",
    "del model, processor, tokenizer\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "mem_after = torch.cuda.memory_allocated() / 1e9\n",
    "print(f\"GPU memory: {mem_before:.2f} GB -> {mem_after:.2f} GB\")\n",
    "print(\"Done!\")"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
