{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03137bdd",
   "metadata": {},
   "source": [
    "# Experiment 3F: Semantic Amplification\n",
    "\n",
    "## Motivation\n",
    "\n",
    "Exps 2B, 3D, and 3E established that ~85% of the prefix benefit is structural\n",
    "(attention sink redistribution). The semantic component is ~10% on MS MARCO\n",
    "but -4% on neural-bridge. This experiment asks: **can we amplify the semantic\n",
    "component by repeating the prefix?**\n",
    "\n",
    "From Exp 3E: prefix tokens absorb 13-26% of doc attention. All conditions\n",
    "produce similar KL redistribution. The structural effect saturates quickly\n",
    "(1 random word = 85% of oracle per Exp 2B).\n",
    "\n",
    "**Key insight**: if the structural effect saturates but we keep adding more\n",
    "semantic tokens, the semantic component's share should grow.\n",
    "\n",
    "## Design\n",
    "\n",
    "**Dataset**: MS MARCO v1.1 (same 500 samples as Exp 02/2B).\n",
    "\n",
    "### 20 Conditions\n",
    "\n",
    "| # | Group | Condition | Prefix |\n",
    "|---|-------|-----------|--------|\n",
    "| 1 | â€” | `bare` | (none) |\n",
    "| 2-6 | A | `oracle_x{1,3,5,10,20}_trunc` | query x N |\n",
    "| 7-11 | A | `random_x{1,3,5,10,20}_trunc` | random_matched x N |\n",
    "| 12-13 | B | `scrambled_x{5,10}_trunc` | scrambled oracle x N |\n",
    "| 14-15 | C | `content_x{5,10}_trunc` | content words only x N |\n",
    "| 16-17 | C | `random_content_x{5,10}_trunc` | random matched to content length x N |\n",
    "| 18 | D | `the_matched10_trunc` | \"the\" x M (token-matched to oracle_x10) |\n",
    "| 19 | D | `bare_short` | (none), doc truncated to 30 words |\n",
    "| 20 | D | `oracle_short_trunc` | query x 1, doc truncated to 30 words |\n",
    "\n",
    "`random_xN` repeats the SAME random words N times (not N different sets).\n",
    "\n",
    "### Analysis\n",
    "\n",
    "1. **Repetition sweep**: structural(N), semantic(N), semantic_fraction(N) vs N\n",
    "2. **3-way decomposition** at N=5 and N=10 (structure/vocabulary/semantics)\n",
    "3. **Content concentration**: content-only words vs full query at N=5,10\n",
    "4. **Short documents**: prefix/doc ratio effect\n",
    "5. **Structural saturation**: \"the\" control vs random_x10\n",
    "6. **Hardness interaction**: semantic(N) by hardness quintile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d7a22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Setup\n",
    "import os\n",
    "os.umask(0o000)\n",
    "\n",
    "import sys, json, time, re, gc\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from scipy import stats\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "sys.path.insert(0, \".\")\n",
    "from lib.analysis import cohens_d\n",
    "\n",
    "SEED = 42\n",
    "N_SAMPLES = 500\n",
    "MODEL_NAME = \"google/t5gemma-2-4b-4b\"\n",
    "REPS = [1, 3, 5, 10, 20]\n",
    "\n",
    "RESULTS_DIR = Path(\"results/exp03f\")\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CHECKPOINT_PATH = RESULTS_DIR / \"checkpoint.json\"\n",
    "EXP02_CHECKPOINT = Path(\"results/exp02/checkpoint.json\")\n",
    "\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "print(\"Exp 3F: Semantic Amplification\")\n",
    "print(f\"N: {N_SAMPLES}, Repetitions: {REPS}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db8409d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Load MS MARCO and reconstruct same 500 samples as Exp 02\n",
    "from lib.data import count_words\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load Exp 02 checkpoint for sample alignment verification\n",
    "print(\"Loading Exp 02 checkpoint...\")\n",
    "exp02_ckpt = json.loads(EXP02_CHECKPOINT.read_text())\n",
    "exp02_results = exp02_ckpt['results']\n",
    "assert len(exp02_results) == N_SAMPLES, f\"Expected {N_SAMPLES}, got {len(exp02_results)}\"\n",
    "\n",
    "# Extract Exp 02 NLLs for verification\n",
    "exp02_bare = np.array([r['nll_bare'] for r in exp02_results])\n",
    "exp02_oracle = np.array([r['nll_oracle_trunc'] for r in exp02_results])\n",
    "\n",
    "# Reconstruct dataset samples\n",
    "print(\"Loading MS MARCO to reconstruct samples...\")\n",
    "ds = load_dataset(\"microsoft/ms_marco\", \"v1.1\", split=\"validation\")\n",
    "\n",
    "samples = []\n",
    "for item in ds:\n",
    "    if len(samples) >= N_SAMPLES * 3:\n",
    "        break\n",
    "    passages = item.get('passages', {})\n",
    "    ptexts = passages.get('passage_text', [])\n",
    "    is_sel = passages.get('is_selected', [])\n",
    "    query = item.get('query', '')\n",
    "    answers = item.get('answers', [])\n",
    "    well_formed = item.get('wellFormedAnswers', [])\n",
    "    answer = None\n",
    "    if well_formed and len(well_formed) > 0 and well_formed[0] not in ('[]', ''):\n",
    "        answer = well_formed[0]\n",
    "    elif answers and len(answers) > 0 and answers[0] != 'No Answer Present.':\n",
    "        answer = answers[0]\n",
    "    if not answer:\n",
    "        continue\n",
    "    for pt, sel in zip(ptexts, is_sel):\n",
    "        wc = count_words(pt)\n",
    "        if sel == 1 and 30 <= wc <= 300:\n",
    "            samples.append({\n",
    "                'passage': pt, 'query': query, 'answer': answer,\n",
    "                'word_count': wc,\n",
    "            })\n",
    "            break\n",
    "\n",
    "np.random.seed(SEED)\n",
    "np.random.shuffle(samples)\n",
    "samples = samples[:N_SAMPLES]\n",
    "del ds\n",
    "gc.collect()\n",
    "\n",
    "# Verify samples match Exp 02\n",
    "for i in range(min(20, N_SAMPLES)):\n",
    "    assert samples[i]['query'] == exp02_results[i]['query'], \\\n",
    "        f\"Sample {i} mismatch: {samples[i]['query'][:40]} != {exp02_results[i]['query'][:40]}\"\n",
    "passage_words = np.array([s['word_count'] for s in samples])\n",
    "print(f\"Verified: {N_SAMPLES} samples match Exp 02\")\n",
    "print(f\"Document lengths: {passage_words.min()}-{passage_words.max()} words, \"\n",
    "      f\"mean={passage_words.mean():.0f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e82f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Load model and define scoring helpers\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "HF_TOKEN = os.environ.get(\"HF_TOKEN\")\n",
    "\n",
    "from transformers import AutoProcessor, AutoModelForSeq2SeqLM\n",
    "\n",
    "print(f\"Loading {MODEL_NAME}...\")\n",
    "processor = AutoProcessor.from_pretrained(MODEL_NAME, token=HF_TOKEN)\n",
    "tokenizer = processor.tokenizer\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    MODEL_NAME, device_map=\"auto\", torch_dtype=torch.bfloat16, token=HF_TOKEN,\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "DEVICE = next(model.parameters()).device\n",
    "print(f\"Model loaded. dtype={next(model.parameters()).dtype}\")\n",
    "print(f\"GPU memory: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "\n",
    "\n",
    "def score_nll(encoder_text, answer_text, prefix_token_count=0, truncate=False):\n",
    "    # Score NLL of answer given encoder text, with optional prefix truncation.\n",
    "    enc_ids = tokenizer(encoder_text, return_tensors=\"pt\",\n",
    "                        add_special_tokens=True, truncation=True,\n",
    "                        max_length=2048).input_ids.to(DEVICE)\n",
    "    total_enc_len = enc_ids.shape[1]\n",
    "    enc_mask = torch.ones(1, total_enc_len, device=DEVICE, dtype=torch.long)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        encoder_outputs = model.get_encoder()(\n",
    "            input_ids=enc_ids, attention_mask=enc_mask\n",
    "        )\n",
    "\n",
    "    if truncate and prefix_token_count > 0:\n",
    "        cross_attn_mask = torch.ones(1, total_enc_len, device=DEVICE, dtype=torch.long)\n",
    "        cross_attn_mask[:, :prefix_token_count] = 0\n",
    "    else:\n",
    "        cross_attn_mask = torch.ones(1, total_enc_len, device=DEVICE, dtype=torch.long)\n",
    "\n",
    "    ans_ids = tokenizer(answer_text, return_tensors=\"pt\",\n",
    "                        add_special_tokens=False, truncation=True,\n",
    "                        max_length=256).input_ids.to(DEVICE)\n",
    "    if ans_ids.shape[1] == 0:\n",
    "        return 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            encoder_outputs=encoder_outputs,\n",
    "            attention_mask=cross_attn_mask,\n",
    "            labels=ans_ids,\n",
    "        )\n",
    "\n",
    "    logits = outputs.logits\n",
    "    log_probs = F.log_softmax(logits, dim=-1)\n",
    "    token_log_probs = log_probs[0].gather(1, ans_ids[0].unsqueeze(1)).squeeze(1)\n",
    "    mean_nll = -token_log_probs.mean().item()\n",
    "\n",
    "    del encoder_outputs, outputs, logits, log_probs\n",
    "    return mean_nll\n",
    "\n",
    "\n",
    "def count_prefix_tokens(prefix_text, document_text):\n",
    "    # Count how many tokens the prefix occupies in [prefix + newline + document].\n",
    "    full_text = prefix_text + \"\\n\" + document_text\n",
    "    full_ids = tokenizer(full_text, add_special_tokens=True, truncation=True,\n",
    "                         max_length=2048).input_ids\n",
    "    doc_ids = tokenizer(document_text, add_special_tokens=True, truncation=True,\n",
    "                        max_length=2048).input_ids\n",
    "    return len(full_ids) - len(doc_ids)\n",
    "\n",
    "\n",
    "STOP_WORDS = {\n",
    "    'a', 'an', 'the', 'is', 'are', 'was', 'were', 'be', 'been', 'being',\n",
    "    'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could',\n",
    "    'should', 'may', 'might', 'can', 'shall', 'to', 'of', 'in', 'for',\n",
    "    'on', 'with', 'at', 'by', 'from', 'as', 'into', 'through', 'during',\n",
    "    'before', 'after', 'above', 'below', 'between', 'and', 'but', 'or',\n",
    "    'not', 'no', 'if', 'then', 'than', 'so', 'up', 'out', 'about',\n",
    "    'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those',\n",
    "    'it', 'its', 'i', 'me', 'my', 'we', 'our', 'you', 'your', 'he',\n",
    "    'him', 'his', 'she', 'her', 'they', 'them', 'their', 'how', 'when',\n",
    "    'where', 'why', 'much', 'many', 'some', 'any', 'all', 'each',\n",
    "    'does', 'also', 'just', 'more', 'most', 'very', 'too', 'only',\n",
    "}\n",
    "\n",
    "print(\"Helpers defined.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a867952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Generate all 20 conditions per sample\n",
    "\n",
    "COND_NAMES = ['bare']\n",
    "for N in REPS:\n",
    "    COND_NAMES.append(f'oracle_x{N}_trunc')\n",
    "    COND_NAMES.append(f'random_x{N}_trunc')\n",
    "COND_NAMES.extend([\n",
    "    'scrambled_x5_trunc', 'scrambled_x10_trunc',\n",
    "    'content_x5_trunc', 'content_x10_trunc',\n",
    "    'random_content_x5_trunc', 'random_content_x10_trunc',\n",
    "    'the_matched10_trunc',\n",
    "    'bare_short', 'oracle_short_trunc',\n",
    "])\n",
    "\n",
    "print(f\"Conditions ({len(COND_NAMES)}):\")\n",
    "for c in COND_NAMES:\n",
    "    print(f\"  {c}\")\n",
    "\n",
    "for i, s in enumerate(samples):\n",
    "    query = s['query']\n",
    "    passage = s['passage']\n",
    "    query_words = query.split()\n",
    "\n",
    "    # Random words from unrelated passage (same set for all reps)\n",
    "    other_idx = (i + N_SAMPLES // 2) % len(samples)\n",
    "    other_words = samples[other_idx]['passage'].split()\n",
    "    random_matched = \" \".join(other_words[:len(query_words)])\n",
    "\n",
    "    # Scrambled oracle (same words, random order)\n",
    "    rng = np.random.RandomState(SEED + i)\n",
    "    shuffled = list(query_words)\n",
    "    rng.shuffle(shuffled)\n",
    "    scrambled = \" \".join(shuffled)\n",
    "\n",
    "    # Content words (stop words removed from query)\n",
    "    content_words = [w for w in query_words if w.lower() not in STOP_WORDS]\n",
    "    if not content_words:\n",
    "        content_words = query_words[:1]  # fallback\n",
    "    content_text = \" \".join(content_words)\n",
    "    s['n_content_words'] = len(content_words)\n",
    "\n",
    "    # Random words matched to content-word count\n",
    "    random_content = \" \".join(other_words[:len(content_words)])\n",
    "\n",
    "    # Group A: oracle and random at each repetition level\n",
    "    for N in REPS:\n",
    "        s[f'oracle_x{N}'] = \" \".join([query] * N)\n",
    "        s[f'random_x{N}'] = \" \".join([random_matched] * N)\n",
    "\n",
    "    # Group B: scrambled at x5 and x10\n",
    "    s['scrambled_x5'] = \" \".join([scrambled] * 5)\n",
    "    s['scrambled_x10'] = \" \".join([scrambled] * 10)\n",
    "\n",
    "    # Group C: content concentration\n",
    "    s['content_x5'] = \" \".join([content_text] * 5)\n",
    "    s['content_x10'] = \" \".join([content_text] * 10)\n",
    "    s['random_content_x5'] = \" \".join([random_content] * 5)\n",
    "    s['random_content_x10'] = \" \".join([random_content] * 10)\n",
    "\n",
    "    # Group D: \"the\" matched to oracle_x10 token count\n",
    "    oracle_x10_toks = count_prefix_tokens(s['oracle_x10'], passage)\n",
    "    s['the_matched10'] = \" \".join([\"the\"] * oracle_x10_toks)\n",
    "    s['oracle_x10_toks'] = oracle_x10_toks\n",
    "\n",
    "    # Group D: short document (first 30 words)\n",
    "    s['short_doc'] = \" \".join(passage.split()[:30])\n",
    "\n",
    "# Show example\n",
    "ex = samples[0]\n",
    "print(f\"\\nExample (sample 0):\")\n",
    "print(f\"  Query:   {ex['query'][:80]}\")\n",
    "print(f\"  Answer:  {ex['answer'][:80]}\")\n",
    "print(f\"  Passage: {ex['passage'][:80]}...\")\n",
    "cw = [w for w in ex['query'].split() if w.lower() not in STOP_WORDS]\n",
    "print(f\"  Content words: {' '.join(cw)}\")\n",
    "print(f\"  Short doc: {ex['short_doc'][:80]}...\")\n",
    "\n",
    "# Show actual repeated prefix text for representative conditions\n",
    "print(f\"\\n--- Actual prefix text for representative conditions (sample 0) ---\")\n",
    "rep_examples = [\n",
    "    ('oracle_x1', ex['oracle_x1']),\n",
    "    ('oracle_x3', ex['oracle_x3']),\n",
    "    ('random_x1', ex['random_x1']),\n",
    "    ('random_x5', ex['random_x5']),\n",
    "    ('scrambled_x5', ex['scrambled_x5']),\n",
    "    ('content_x5', ex['content_x5']),\n",
    "    ('the_matched10', ex['the_matched10']),\n",
    "]\n",
    "for name, text in rep_examples:\n",
    "    ptoks = count_prefix_tokens(text, ex['passage'])\n",
    "    print(f\"  {name:<20} ({ptoks:>4} prefix toks): {str(text)[:70]}{'...' if len(text) > 70 else ''}\")\n",
    "\n",
    "# Token count summary for first 50 samples\n",
    "print(f\"\\nPrefix token counts (first 50 samples):\")\n",
    "for c in COND_NAMES:\n",
    "    if c in ('bare', 'bare_short'):\n",
    "        continue\n",
    "    if c == 'oracle_short_trunc':\n",
    "        toks = [count_prefix_tokens(s['query'], s['short_doc']) for s in samples[:50]]\n",
    "    else:\n",
    "        key = c.replace('_trunc', '')\n",
    "        toks = [count_prefix_tokens(s[key], s['passage']) for s in samples[:50]]\n",
    "    print(f\"  {c:<30} mean={np.mean(toks):>6.1f}, range=[{min(toks):>3}, {max(toks):>3}]\")\n",
    "\n",
    "# Content word statistics\n",
    "cw_counts = [s['n_content_words'] for s in samples]\n",
    "print(f\"\\nContent words per query: mean={np.mean(cw_counts):.1f}, \"\n",
    "      f\"range=[{min(cw_counts)}, {max(cw_counts)}]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499a8133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Scoring loop with checkpointing\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"SCORING ALL CONDITIONS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "results = []\n",
    "start_idx = 0\n",
    "\n",
    "if CHECKPOINT_PATH.exists():\n",
    "    ckpt = json.loads(CHECKPOINT_PATH.read_text())\n",
    "    if ckpt.get('n_total') == N_SAMPLES and len(ckpt.get('results', [])) > 0:\n",
    "        saved_queries = [r['query'][:50] for r in ckpt['results']]\n",
    "        current_queries = [s['query'][:50] for s in samples[:len(saved_queries)]]\n",
    "        if saved_queries == current_queries:\n",
    "            results = ckpt['results']\n",
    "            start_idx = len(results)\n",
    "            print(f\"Resuming from checkpoint: {start_idx}/{N_SAMPLES}\")\n",
    "\n",
    "if start_idx == 0:\n",
    "    print(f\"Starting fresh: {len(COND_NAMES)} conditions x {N_SAMPLES} samples \"\n",
    "          f\"= {len(COND_NAMES) * N_SAMPLES} scorings\")\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "for i in tqdm(range(start_idx, N_SAMPLES), initial=start_idx, total=N_SAMPLES,\n",
    "              desc=\"Scoring\"):\n",
    "    s = samples[i]\n",
    "    result = {\n",
    "        'query': s['query'],\n",
    "        'answer': s['answer'],\n",
    "        'passage_words': s['word_count'],\n",
    "        'n_content_words': s['n_content_words'],\n",
    "    }\n",
    "\n",
    "    for cond in COND_NAMES:\n",
    "        if cond == 'bare':\n",
    "            nll = score_nll(s['passage'], s['answer'])\n",
    "            result['nll_bare'] = nll\n",
    "        elif cond == 'bare_short':\n",
    "            nll = score_nll(s['short_doc'], s['answer'])\n",
    "            result['nll_bare_short'] = nll\n",
    "        elif cond == 'oracle_short_trunc':\n",
    "            prefix = s['query']\n",
    "            doc = s['short_doc']\n",
    "            enc_text = prefix + \"\\n\" + doc\n",
    "            ptoks = count_prefix_tokens(prefix, doc)\n",
    "            nll = score_nll(enc_text, s['answer'], ptoks, truncate=True)\n",
    "            result['nll_oracle_short_trunc'] = nll\n",
    "            result['ptoks_oracle_short_trunc'] = ptoks\n",
    "        else:\n",
    "            key = cond.replace('_trunc', '')\n",
    "            prefix = s[key]\n",
    "            enc_text = prefix + \"\\n\" + s['passage']\n",
    "            ptoks = count_prefix_tokens(prefix, s['passage'])\n",
    "            nll = score_nll(enc_text, s['answer'], ptoks, truncate=True)\n",
    "            result[f'nll_{cond}'] = nll\n",
    "            result[f'ptoks_{cond}'] = ptoks\n",
    "\n",
    "    results.append(result)\n",
    "\n",
    "    if (i + 1) % 20 == 0 or i == N_SAMPLES - 1:\n",
    "        ckpt = {\n",
    "            'n_total': N_SAMPLES,\n",
    "            'results': results,\n",
    "            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        }\n",
    "        CHECKPOINT_PATH.write_text(json.dumps(ckpt))\n",
    "        elapsed = time.time() - t0\n",
    "        done = i - start_idx + 1\n",
    "        eta = (N_SAMPLES - i - 1) * elapsed / done if done > 0 else 0\n",
    "        tqdm.write(f\"  Checkpoint {i+1}/{N_SAMPLES} | {elapsed/60:.1f}m | ETA {eta/60:.1f}m\")\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "elapsed = time.time() - t0\n",
    "print(f\"\\nScoring complete: {len(results)} samples, \"\n",
    "      f\"{len(COND_NAMES)} conditions in {elapsed/60:.1f} min\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3b60a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Part 1 -- Repetition Sweep Curves\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PART 1: REPETITION SWEEP\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Extract NLLs\n",
    "bare_nlls = np.array([r['nll_bare'] for r in results])\n",
    "\n",
    "# Verify against Exp 02\n",
    "print(\"--- Verification against Exp 02 ---\")\n",
    "bare_diff = np.abs(bare_nlls - exp02_bare)\n",
    "print(f\"  bare NLL max diff: {bare_diff.max():.6f} (should be ~0)\")\n",
    "\n",
    "oracle_x1_nlls = np.array([r['nll_oracle_x1_trunc'] for r in results])\n",
    "oracle_diff = np.abs(oracle_x1_nlls - exp02_oracle)\n",
    "print(f\"  oracle_x1 vs Exp02 oracle max diff: {oracle_diff.max():.6f} (should be ~0)\")\n",
    "\n",
    "# Sweep table\n",
    "print(f\"\\n{'N':>3} {'Struct delta':>13} {'Sem delta':>11} {'Total delta':>12} \"\n",
    "      f\"{'Sem frac':>9} {'Struct d':>9} {'Sem d':>7} {'Total d':>8}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "sweep = {}\n",
    "for N in REPS:\n",
    "    o_nlls = np.array([r[f'nll_oracle_x{N}_trunc'] for r in results])\n",
    "    r_nlls = np.array([r[f'nll_random_x{N}_trunc'] for r in results])\n",
    "\n",
    "    structural = bare_nlls - r_nlls\n",
    "    semantic = r_nlls - o_nlls\n",
    "    total = bare_nlls - o_nlls\n",
    "\n",
    "    struct_mean = structural.mean()\n",
    "    sem_mean = semantic.mean()\n",
    "    total_mean = total.mean()\n",
    "    sem_frac = sem_mean / total_mean * 100 if total_mean > 0 else 0\n",
    "\n",
    "    _, p_sem = stats.ttest_1samp(semantic, 0)\n",
    "    sig = '***' if p_sem < 0.001 else '**' if p_sem < 0.01 else '*' if p_sem < 0.05 else 'ns'\n",
    "\n",
    "    sweep[N] = {\n",
    "        'structural': structural, 'semantic': semantic, 'total': total,\n",
    "        'struct_mean': struct_mean, 'sem_mean': sem_mean, 'total_mean': total_mean,\n",
    "        'sem_frac': sem_frac, 'p_sem': p_sem,\n",
    "    }\n",
    "\n",
    "    print(f\"{N:>3} {struct_mean:>+13.4f} {sem_mean:>+11.4f} {total_mean:>+12.4f} \"\n",
    "          f\"{sem_frac:>8.1f}% {cohens_d(structural):>+9.3f} \"\n",
    "          f\"{cohens_d(semantic):>+7.3f} {cohens_d(total):>+8.3f} {sig}\")\n",
    "\n",
    "# Key test: does semantic fraction grow?\n",
    "fracs = [sweep[N]['sem_frac'] for N in REPS]\n",
    "print(f\"\\nSemantic fraction: {' -> '.join(f'{f:.1f}%' for f in fracs)}\")\n",
    "\n",
    "if fracs[-1] > fracs[0] * 1.5:\n",
    "    print(\"  --> AMPLIFICATION: semantic fraction grew with repetition!\")\n",
    "elif fracs[-1] > fracs[0] * 1.1:\n",
    "    print(\"  --> MODERATE growth in semantic fraction.\")\n",
    "else:\n",
    "    print(\"  --> NO amplification: semantic fraction is stable.\")\n",
    "\n",
    "# Structural saturation check\n",
    "struct_ds = [cohens_d(sweep[N]['structural']) for N in REPS]\n",
    "print(f\"\\nStructural d: {' -> '.join(f'{d:+.3f}' for d in struct_ds)}\")\n",
    "struct_ratio_sweep = struct_ds[-1] / struct_ds[0] if struct_ds[0] > 0 else 0\n",
    "print(f\"  x20/x1 ratio: {struct_ratio_sweep:.2f}\")\n",
    "\n",
    "if struct_ratio_sweep < 1.2:\n",
    "    print(\"  --> Structural effect is SATURATED (x1 ~ x20).\")\n",
    "else:\n",
    "    print(f\"  --> Structural effect still growing ({struct_ratio_sweep:.1f}x from x1 to x20).\")\n",
    "\n",
    "# Bonferroni significance check for all non-bare conditions vs bare\n",
    "print(f\"\\n--- Significance vs bare (Bonferroni k=19, alpha={0.05/19:.4f}) ---\")\n",
    "alpha_bonf = 0.05 / 19\n",
    "all_sig = True\n",
    "for cond in COND_NAMES:\n",
    "    if cond == 'bare':\n",
    "        continue\n",
    "    nlls_c = np.array([r[f'nll_{cond}'] for r in results])\n",
    "    if cond == 'bare_short':\n",
    "        continue  # different document, not comparable to bare\n",
    "    benefit_c = bare_nlls - nlls_c\n",
    "    _, p_c = stats.ttest_1samp(benefit_c, 0)\n",
    "    if p_c >= alpha_bonf:\n",
    "        all_sig = False\n",
    "        print(f\"  {cond:<30} p={p_c:.4e} NOT SIGNIFICANT\")\n",
    "print(f\"  All comparable conditions significant after Bonferroni: \"\n",
    "      f\"{'YES' if all_sig else 'NO'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893136ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Part 2 -- 3-Way Decomposition at N=5 and N=10\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PART 2: 3-WAY DECOMPOSITION (structure / vocabulary / semantics)\")\n",
    "print(\"=\" * 70)\n",
    "print(\"Decompose: bare -> random_xN -> scrambled_xN -> oracle_xN\\n\")\n",
    "\n",
    "for N in [5, 10]:\n",
    "    o_nlls = np.array([r[f'nll_oracle_x{N}_trunc'] for r in results])\n",
    "    r_nlls = np.array([r[f'nll_random_x{N}_trunc'] for r in results])\n",
    "    scr_nlls = np.array([r[f'nll_scrambled_x{N}_trunc'] for r in results])\n",
    "\n",
    "    structure = bare_nlls - r_nlls\n",
    "    vocabulary = r_nlls - scr_nlls\n",
    "    semantics = scr_nlls - o_nlls\n",
    "    total = bare_nlls - o_nlls\n",
    "\n",
    "    total_mean = total.mean()\n",
    "    s_pct = structure.mean() / total_mean * 100 if total_mean > 0 else 0\n",
    "    v_pct = vocabulary.mean() / total_mean * 100 if total_mean > 0 else 0\n",
    "    sm_pct = semantics.mean() / total_mean * 100 if total_mean > 0 else 0\n",
    "\n",
    "    print(f\"--- N = {N} ---\")\n",
    "    print(f\"  {'Component':<15} {'Delta':>10} {'%total':>8} {'d':>8} {'p':>12} {'sig':>4}\")\n",
    "    print(f\"  {'-'*60}\")\n",
    "\n",
    "    for label, comp in [('Structure', structure), ('Vocabulary', vocabulary),\n",
    "                        ('Semantics', semantics)]:\n",
    "        mu = comp.mean()\n",
    "        pct = mu / total_mean * 100 if total_mean > 0 else 0\n",
    "        d = cohens_d(comp)\n",
    "        _, p = stats.ttest_1samp(comp, 0)\n",
    "        sig = '***' if p < 0.001 else '**' if p < 0.01 else '*' if p < 0.05 else 'ns'\n",
    "        print(f\"  {label:<15} {mu:>+10.4f} {pct:>7.1f}% {d:>+8.3f} {p:>12.2e} {sig}\")\n",
    "\n",
    "    print(f\"  {'TOTAL':<15} {total_mean:>+10.4f} {'100.0%':>8}\")\n",
    "    residual = total_mean - (structure.mean() + vocabulary.mean() + semantics.mean())\n",
    "    print(f\"  Residual: {residual:.6f}\")\n",
    "    print()\n",
    "\n",
    "# Compare to Exp 2B baseline (N=1)\n",
    "print(\"--- Comparison with Exp 2B (N=1) ---\")\n",
    "print(f\"  Exp 2B (N=1): Structure=84.7%, Vocabulary=5.5% (ns), Semantics=9.7% (***)\")\n",
    "for N in [5, 10]:\n",
    "    o = np.array([r[f'nll_oracle_x{N}_trunc'] for r in results])\n",
    "    rn = np.array([r[f'nll_random_x{N}_trunc'] for r in results])\n",
    "    sc = np.array([r[f'nll_scrambled_x{N}_trunc'] for r in results])\n",
    "    t = (bare_nlls - o).mean()\n",
    "    s_p = (bare_nlls - rn).mean() / t * 100 if t > 0 else 0\n",
    "    v_p = (rn - sc).mean() / t * 100 if t > 0 else 0\n",
    "    sm_p = (sc - o).mean() / t * 100 if t > 0 else 0\n",
    "    print(f\"  N={N}:     Structure={s_p:.1f}%, Vocabulary={v_p:.1f}%, Semantics={sm_p:.1f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5727a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Part 3 -- Content Concentration\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PART 3: CONTENT CONCENTRATION\")\n",
    "print(\"=\" * 70)\n",
    "print(\"Does stripping stop words improve the semantic-to-structural ratio?\\n\")\n",
    "\n",
    "for N in [5, 10]:\n",
    "    # Full query\n",
    "    o_nlls = np.array([r[f'nll_oracle_x{N}_trunc'] for r in results])\n",
    "    r_nlls = np.array([r[f'nll_random_x{N}_trunc'] for r in results])\n",
    "    oracle_sem = r_nlls - o_nlls\n",
    "    oracle_struct = bare_nlls - r_nlls\n",
    "\n",
    "    # Content-only query\n",
    "    c_nlls = np.array([r[f'nll_content_x{N}_trunc'] for r in results])\n",
    "    rc_nlls = np.array([r[f'nll_random_content_x{N}_trunc'] for r in results])\n",
    "    content_sem = rc_nlls - c_nlls\n",
    "    content_struct = bare_nlls - rc_nlls\n",
    "\n",
    "    # Semantic-to-structural ratios\n",
    "    oracle_ratio = oracle_sem.mean() / oracle_struct.mean() if oracle_struct.mean() > 0 else 0\n",
    "    content_ratio = content_sem.mean() / content_struct.mean() if content_struct.mean() > 0 else 0\n",
    "\n",
    "    # Total benefits\n",
    "    oracle_total = bare_nlls - o_nlls\n",
    "    content_total = bare_nlls - c_nlls\n",
    "\n",
    "    print(f\"--- N = {N} ---\")\n",
    "    print(f\"  {'Metric':<30} {'Full query':>12} {'Content only':>14}\")\n",
    "    print(f\"  {'-'*60}\")\n",
    "    print(f\"  {'Structural delta':<30} {oracle_struct.mean():>+12.4f} {content_struct.mean():>+14.4f}\")\n",
    "    print(f\"  {'Semantic delta':<30} {oracle_sem.mean():>+12.4f} {content_sem.mean():>+14.4f}\")\n",
    "    print(f\"  {'Total delta':<30} {oracle_total.mean():>+12.4f} {content_total.mean():>+14.4f}\")\n",
    "    print(f\"  {'Semantic / Structural ratio':<30} {oracle_ratio:>12.3f} {content_ratio:>14.3f}\")\n",
    "    print(f\"  {'Total d':<30} {cohens_d(oracle_total):>+12.3f} {cohens_d(content_total):>+14.3f}\")\n",
    "\n",
    "    # Significance of content vs oracle semantic gap\n",
    "    sem_diff = content_sem - oracle_sem\n",
    "    _, p_diff = stats.ttest_1samp(sem_diff, 0)\n",
    "    sig = '***' if p_diff < 0.001 else '**' if p_diff < 0.01 else '*' if p_diff < 0.05 else 'ns'\n",
    "    print(f\"  Content-Oracle semantic gap: {sem_diff.mean():+.4f} (p={p_diff:.2e}) {sig}\")\n",
    "\n",
    "    # Token count comparison\n",
    "    o_toks = np.array([r[f'ptoks_oracle_x{N}_trunc'] for r in results])\n",
    "    c_toks = np.array([r[f'ptoks_content_x{N}_trunc'] for r in results])\n",
    "    print(f\"  Mean prefix tokens: oracle={o_toks.mean():.0f}, content={c_toks.mean():.0f} \"\n",
    "          f\"({c_toks.mean()/o_toks.mean()*100:.0f}%)\")\n",
    "    print()\n",
    "\n",
    "# Overall interpretation\n",
    "c5_rc5 = np.array([r['nll_random_content_x5_trunc'] for r in results])\n",
    "c5 = np.array([r['nll_content_x5_trunc'] for r in results])\n",
    "r5 = np.array([r['nll_random_x5_trunc'] for r in results])\n",
    "o5 = np.array([r['nll_oracle_x5_trunc'] for r in results])\n",
    "c_rat = (c5_rc5 - c5).mean() / (bare_nlls - c5_rc5).mean() if (bare_nlls - c5_rc5).mean() > 0 else 0\n",
    "o_rat = (r5 - o5).mean() / (bare_nlls - r5).mean() if (bare_nlls - r5).mean() > 0 else 0\n",
    "\n",
    "if c_rat > o_rat * 1.2:\n",
    "    print(\"  --> Concentrating meaning IMPROVES the semantic ratio.\")\n",
    "elif c_rat < o_rat * 0.8:\n",
    "    print(\"  --> Concentrating meaning HURTS -- stop words may provide useful structure.\")\n",
    "else:\n",
    "    print(\"  --> Concentrating meaning has NO significant effect on semantic ratio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2460b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Part 4 -- Short Document Analysis\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PART 4: SHORT DOCUMENTS (prefix/doc ratio effect)\")\n",
    "print(\"=\" * 70)\n",
    "print(\"Does the prefix matter more when documents are shorter?\\n\")\n",
    "\n",
    "bare_short_nlls = np.array([r['nll_bare_short'] for r in results])\n",
    "oracle_short_nlls = np.array([r['nll_oracle_short_trunc'] for r in results])\n",
    "\n",
    "# Benefits\n",
    "short_benefit = bare_short_nlls - oracle_short_nlls\n",
    "full_benefit = bare_nlls - oracle_x1_nlls\n",
    "\n",
    "print(f\"  {'Metric':<35} {'Full doc':>12} {'Short doc (30w)':>16}\")\n",
    "print(f\"  {'-'*65}\")\n",
    "print(f\"  {'Bare NLL':<35} {bare_nlls.mean():>12.4f} {bare_short_nlls.mean():>16.4f}\")\n",
    "print(f\"  {'Oracle NLL':<35} {oracle_x1_nlls.mean():>12.4f} {oracle_short_nlls.mean():>16.4f}\")\n",
    "print(f\"  {'Total benefit (delta)':<35} {full_benefit.mean():>+12.4f} {short_benefit.mean():>+16.4f}\")\n",
    "print(f\"  {'Cohen d':<35} {cohens_d(full_benefit):>+12.3f} {cohens_d(short_benefit):>+16.3f}\")\n",
    "print(f\"  {'Win rate':<35} {np.mean(full_benefit>0)*100:>11.1f}% {np.mean(short_benefit>0)*100:>15.1f}%\")\n",
    "\n",
    "# Significance\n",
    "_, p_full = stats.ttest_1samp(full_benefit, 0)\n",
    "_, p_short = stats.ttest_1samp(short_benefit, 0)\n",
    "print(f\"  {'p-value':<35} {p_full:>12.2e} {p_short:>16.2e}\")\n",
    "\n",
    "# Prefix/doc token ratio\n",
    "short_ptoks = np.array([r.get('ptoks_oracle_short_trunc', 0) for r in results])\n",
    "full_ptoks = np.array([r.get('ptoks_oracle_x1_trunc', 0) for r in results])\n",
    "short_doc_toks = np.array([len(tokenizer(s['short_doc'], add_special_tokens=True).input_ids)\n",
    "                           for s in samples])\n",
    "full_doc_toks = np.array([len(tokenizer(s['passage'], add_special_tokens=True).input_ids)\n",
    "                          for s in samples])\n",
    "\n",
    "print(f\"\\n--- Prefix/doc token ratio ---\")\n",
    "print(f\"  Full doc:  prefix={full_ptoks.mean():.0f} toks, doc={full_doc_toks.mean():.0f} toks, \"\n",
    "      f\"ratio={full_ptoks.mean()/full_doc_toks.mean():.2f}\")\n",
    "print(f\"  Short doc: prefix={short_ptoks.mean():.0f} toks, doc={short_doc_toks.mean():.0f} toks, \"\n",
    "      f\"ratio={short_ptoks.mean()/short_doc_toks.mean():.2f}\")\n",
    "\n",
    "d_full = cohens_d(full_benefit)\n",
    "d_short = cohens_d(short_benefit)\n",
    "if d_short > d_full * 1.1:\n",
    "    print(f\"\\n  --> Prefix benefit is LARGER for shorter documents (d={d_short:+.3f} vs {d_full:+.3f}).\")\n",
    "    print(f\"      The semantic signal may be diluted by document length.\")\n",
    "elif d_short < d_full * 0.9:\n",
    "    print(f\"\\n  --> Prefix benefit is SMALLER for shorter documents (d={d_short:+.3f} vs {d_full:+.3f}).\")\n",
    "    print(f\"      Longer documents benefit MORE from prefix co-encoding.\")\n",
    "else:\n",
    "    print(f\"\\n  --> Prefix benefit is similar regardless of document length \"\n",
    "          f\"(d={d_short:+.3f} vs {d_full:+.3f}).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a48ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Part 5 -- Structural Saturation + \"the\" Control\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PART 5: STRUCTURAL SATURATION\")\n",
    "print(\"=\" * 70)\n",
    "print(\"Is the structural benefit from attention MASS or token DIVERSITY?\\n\")\n",
    "\n",
    "the_nlls = np.array([r['nll_the_matched10_trunc'] for r in results])\n",
    "random_x10_nlls = np.array([r['nll_random_x10_trunc'] for r in results])\n",
    "oracle_x10_nlls = np.array([r['nll_oracle_x10_trunc'] for r in results])\n",
    "\n",
    "the_benefit = bare_nlls - the_nlls\n",
    "random_x10_benefit = bare_nlls - random_x10_nlls\n",
    "oracle_x10_benefit = bare_nlls - oracle_x10_nlls\n",
    "\n",
    "oracle_x10_d = cohens_d(oracle_x10_benefit)\n",
    "\n",
    "print(f\"  {'Condition':<30} {'NLL':>8} {'Delta':>10} {'d':>8} {'%Oracle':>9}\")\n",
    "print(f\"  {'-'*70}\")\n",
    "for name, benefit, nlls in [\n",
    "    ('the_matched10 (uniform)', the_benefit, the_nlls),\n",
    "    ('random_x10 (diverse)', random_x10_benefit, random_x10_nlls),\n",
    "    ('oracle_x10 (semantic)', oracle_x10_benefit, oracle_x10_nlls),\n",
    "]:\n",
    "    d = cohens_d(benefit)\n",
    "    pct = d / oracle_x10_d * 100 if oracle_x10_d > 0 else 0\n",
    "    print(f\"  {name:<30} {nlls.mean():>8.4f} {benefit.mean():>+10.4f} \"\n",
    "          f\"{d:>+8.3f} {pct:>8.1f}%\")\n",
    "\n",
    "# Head-to-head: \"the\" vs random_x10\n",
    "diff_the_rand = random_x10_nlls - the_nlls  # positive = \"the\" is better\n",
    "d_the_rand = cohens_d(diff_the_rand)\n",
    "_, p_the_rand = stats.ttest_1samp(diff_the_rand, 0)\n",
    "sig = '***' if p_the_rand < 0.001 else '**' if p_the_rand < 0.01 else '*' if p_the_rand < 0.05 else 'ns'\n",
    "winner = '\"the\"' if d_the_rand > 0 else 'random_x10'\n",
    "print(f\"\\n  the vs random_x10: d={d_the_rand:+.3f}, p={p_the_rand:.2e} {sig} [{winner}]\")\n",
    "\n",
    "# Token count comparison\n",
    "the_toks = np.array([r['ptoks_the_matched10_trunc'] for r in results])\n",
    "r10_toks = np.array([r['ptoks_random_x10_trunc'] for r in results])\n",
    "o10_toks = np.array([r['ptoks_oracle_x10_trunc'] for r in results])\n",
    "print(f\"\\n  Token counts: the={the_toks.mean():.0f}, random_x10={r10_toks.mean():.0f}, \"\n",
    "      f\"oracle_x10={o10_toks.mean():.0f}\")\n",
    "\n",
    "# Structural saturation curve\n",
    "print(f\"\\n--- Structural benefit saturation ---\")\n",
    "print(f\"  {'N':>3} {'Structural d':>13} {'% of x20':>9}\")\n",
    "struct_ds = []\n",
    "for N in REPS:\n",
    "    r_nlls_n = np.array([r[f'nll_random_x{N}_trunc'] for r in results])\n",
    "    d = cohens_d(bare_nlls - r_nlls_n)\n",
    "    struct_ds.append(d)\n",
    "\n",
    "max_struct_d = struct_ds[-1]\n",
    "for N, d in zip(REPS, struct_ds):\n",
    "    pct = d / max_struct_d * 100 if max_struct_d > 0 else 0\n",
    "    print(f\"  {N:>3} {d:>+13.3f} {pct:>8.1f}%\")\n",
    "\n",
    "if abs(d_the_rand) < 0.05:\n",
    "    print(f\"\\n  --> Token diversity does NOT matter. Structure comes from attention MASS.\")\n",
    "elif d_the_rand < 0:\n",
    "    print(f\"\\n  --> Diverse tokens are BETTER. Some structural benefit comes from diversity.\")\n",
    "else:\n",
    "    print(f\"\\n  --> Uniform 'the' is BETTER than diverse random. Diversity may add noise.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e5d204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Part 6 -- Hardness x Repetition Interaction\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PART 6: HARDNESS x REPETITION INTERACTION\")\n",
    "print(\"=\" * 70)\n",
    "print(\"Does repetition amplify the semantic gap for harder samples?\\n\")\n",
    "\n",
    "quintile_bounds = np.percentile(bare_nlls, [20, 40, 60, 80])\n",
    "quintiles = np.digitize(bare_nlls, quintile_bounds)\n",
    "q_labels = ['Q1 easy', 'Q2', 'Q3', 'Q4', 'Q5 hard']\n",
    "\n",
    "# Table: semantic gap by quintile and N\n",
    "header = f\"  {'Quintile':<10}\"\n",
    "for N in REPS:\n",
    "    header += f\"  {'x'+str(N)+' sem':>8}\"\n",
    "header += f\"  {'x1 frac':>8} {'x20 frac':>9}\"\n",
    "print(header)\n",
    "print(f\"  {'-'*(12 + 10 * len(REPS) + 20)}\")\n",
    "\n",
    "for q in range(5):\n",
    "    mask = quintiles == q\n",
    "    row = f\"  {q_labels[q]:<10}\"\n",
    "    fracs = []\n",
    "    for N in REPS:\n",
    "        o = np.array([r[f'nll_oracle_x{N}_trunc'] for r in results])[mask]\n",
    "        rn = np.array([r[f'nll_random_x{N}_trunc'] for r in results])[mask]\n",
    "        sem = (rn - o).mean()\n",
    "        total = (bare_nlls[mask] - o).mean()\n",
    "        frac = sem / total * 100 if total > 0 else 0\n",
    "        fracs.append(frac)\n",
    "        row += f\"  {sem:>+8.4f}\"\n",
    "    row += f\"  {fracs[0]:>7.1f}% {fracs[-1]:>8.1f}%\"\n",
    "    print(row)\n",
    "\n",
    "# Correlation: hardness vs semantic gap at each N\n",
    "print(f\"\\n--- Hardness correlations ---\")\n",
    "for N in REPS:\n",
    "    o = np.array([r[f'nll_oracle_x{N}_trunc'] for r in results])\n",
    "    rn = np.array([r[f'nll_random_x{N}_trunc'] for r in results])\n",
    "    sem_gap = rn - o\n",
    "    r_val, p_val = stats.pearsonr(bare_nlls, sem_gap)\n",
    "    sig = '***' if p_val < 0.001 else '**' if p_val < 0.01 else '*' if p_val < 0.05 else 'ns'\n",
    "    print(f\"  x{N}: r={r_val:+.3f} (p={p_val:.2e}) {sig}\")\n",
    "\n",
    "# Semantic fraction trajectory for Q1 vs Q5\n",
    "for qlabel, q_idx in [('Q1 easy', 0), ('Q5 hard', 4)]:\n",
    "    mask = quintiles == q_idx\n",
    "    fracs_q = []\n",
    "    for N in REPS:\n",
    "        o = np.array([r[f'nll_oracle_x{N}_trunc'] for r in results])[mask]\n",
    "        rn = np.array([r[f'nll_random_x{N}_trunc'] for r in results])[mask]\n",
    "        total = (bare_nlls[mask] - o).mean()\n",
    "        sem = (rn - o).mean()\n",
    "        fracs_q.append(sem / total * 100 if total > 0 else 0)\n",
    "    print(f\"\\n  {qlabel} semantic fraction: {' -> '.join(f'{f:.1f}%' for f in fracs_q)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c80bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13: Synthesis + Save\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"SYNTHESIS: SEMANTIC AMPLIFICATION RESULTS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# 1. Repetition sweep summary\n",
    "print(f\"\\n1. REPETITION SWEEP:\")\n",
    "for N in REPS:\n",
    "    f = sweep[N]['sem_frac']\n",
    "    sd = cohens_d(sweep[N]['semantic'])\n",
    "    print(f\"   x{N}: semantic fraction = {f:.1f}%, semantic d = {sd:+.3f}\")\n",
    "\n",
    "frac_x1 = sweep[1]['sem_frac']\n",
    "frac_x20 = sweep[20]['sem_frac']\n",
    "amplification = frac_x20 / frac_x1 if frac_x1 > 0 else 0\n",
    "print(f\"   Amplification: x{amplification:.1f} (x1={frac_x1:.1f}% -> x20={frac_x20:.1f}%)\")\n",
    "\n",
    "# 2. 3-Way decomposition shifts\n",
    "print(f\"\\n2. 3-WAY DECOMPOSITION SHIFT:\")\n",
    "print(f\"   Exp 2B (N=1): Structure=84.7%, Vocabulary=5.5%, Semantics=9.7%\")\n",
    "for N in [5, 10]:\n",
    "    o = np.array([r[f'nll_oracle_x{N}_trunc'] for r in results])\n",
    "    rn = np.array([r[f'nll_random_x{N}_trunc'] for r in results])\n",
    "    sc = np.array([r[f'nll_scrambled_x{N}_trunc'] for r in results])\n",
    "    t = (bare_nlls - o).mean()\n",
    "    sp = (bare_nlls - rn).mean() / t * 100 if t > 0 else 0\n",
    "    vp = (rn - sc).mean() / t * 100 if t > 0 else 0\n",
    "    smp = (sc - o).mean() / t * 100 if t > 0 else 0\n",
    "    print(f\"   N={N}: Structure={sp:.1f}%, Vocabulary={vp:.1f}%, Semantics={smp:.1f}%\")\n",
    "\n",
    "# 3. Content concentration\n",
    "print(f\"\\n3. CONTENT CONCENTRATION:\")\n",
    "for N in [5, 10]:\n",
    "    o = np.array([r[f'nll_oracle_x{N}_trunc'] for r in results])\n",
    "    rn = np.array([r[f'nll_random_x{N}_trunc'] for r in results])\n",
    "    c = np.array([r[f'nll_content_x{N}_trunc'] for r in results])\n",
    "    rc = np.array([r[f'nll_random_content_x{N}_trunc'] for r in results])\n",
    "    o_ratio = (rn - o).mean() / (bare_nlls - rn).mean() if (bare_nlls - rn).mean() > 0 else 0\n",
    "    c_ratio = (rc - c).mean() / (bare_nlls - rc).mean() if (bare_nlls - rc).mean() > 0 else 0\n",
    "    print(f\"   N={N}: oracle sem/struct={o_ratio:.3f}, content sem/struct={c_ratio:.3f}\")\n",
    "\n",
    "# 4. Short documents\n",
    "d_full = cohens_d(bare_nlls - oracle_x1_nlls)\n",
    "d_short = cohens_d(bare_short_nlls - oracle_short_nlls)\n",
    "print(f\"\\n4. SHORT DOCUMENTS:\")\n",
    "print(f\"   Full doc d = {d_full:+.3f}, Short doc d = {d_short:+.3f}\")\n",
    "\n",
    "# 5. Structural saturation\n",
    "the_d = cohens_d(bare_nlls - the_nlls)\n",
    "rand10_d = cohens_d(bare_nlls - random_x10_nlls)\n",
    "print(f\"\\n5. STRUCTURAL SATURATION:\")\n",
    "print(f\"   the_matched10 d = {the_d:+.3f}, random_x10 d = {rand10_d:+.3f}\")\n",
    "\n",
    "# 6. Key conclusions\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"CONCLUSIONS:\")\n",
    "\n",
    "if amplification > 2.0:\n",
    "    print(f\"  1. STRONG amplification: semantic fraction grew {amplification:.1f}x with repetition.\")\n",
    "elif amplification > 1.3:\n",
    "    print(f\"  1. MODERATE amplification ({amplification:.1f}x). Some semantic growth with repetition.\")\n",
    "else:\n",
    "    print(f\"  1. NO amplification ({amplification:.1f}x). Semantic fraction is stable across N.\")\n",
    "    print(f\"     Both structural and semantic components scale similarly with repetition.\")\n",
    "\n",
    "struct_sat = struct_ds[-1] / struct_ds[0] if struct_ds[0] > 0 else 0\n",
    "if struct_sat < 1.2:\n",
    "    print(f\"  2. Structural effect is SATURATED (x20/x1 = {struct_sat:.2f}).\")\n",
    "else:\n",
    "    print(f\"  2. Structural effect still GROWING (x20/x1 = {struct_sat:.2f}).\")\n",
    "\n",
    "print(f\"  3. Practical implication: \", end=\"\")\n",
    "if amplification > 1.5 and frac_x20 > 20:\n",
    "    print(\"repeating query 5-10x could meaningfully boost semantic component.\")\n",
    "else:\n",
    "    print(\"repetition does not meaningfully amplify semantic content.\")\n",
    "    print(\"     The structural mechanism dominates at all repetition levels.\")\n",
    "\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "# Save results\n",
    "final_results = {\n",
    "    'experiment': 'exp03f_semantic_amplification',\n",
    "    'model': MODEL_NAME,\n",
    "    'dataset': 'ms_marco_v1.1',\n",
    "    'n_samples': N_SAMPLES,\n",
    "    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'repetition_sweep': {\n",
    "        str(N): {\n",
    "            'structural_d': float(cohens_d(sweep[N]['structural'])),\n",
    "            'semantic_d': float(cohens_d(sweep[N]['semantic'])),\n",
    "            'total_d': float(cohens_d(sweep[N]['total'])),\n",
    "            'structural_delta': float(sweep[N]['struct_mean']),\n",
    "            'semantic_delta': float(sweep[N]['sem_mean']),\n",
    "            'total_delta': float(sweep[N]['total_mean']),\n",
    "            'semantic_fraction': float(sweep[N]['sem_frac']),\n",
    "            'semantic_p': float(sweep[N]['p_sem']),\n",
    "        }\n",
    "        for N in REPS\n",
    "    },\n",
    "    'decomposition': {},\n",
    "    'content_concentration': {},\n",
    "    'structural_saturation': {\n",
    "        'the_matched10_d': float(the_d),\n",
    "        'random_x10_d': float(rand10_d),\n",
    "    },\n",
    "    'short_documents': {\n",
    "        'full_doc_d': float(d_full),\n",
    "        'short_doc_d': float(d_short),\n",
    "    },\n",
    "    'amplification_factor': float(amplification),\n",
    "}\n",
    "\n",
    "# Add decomposition results\n",
    "for N in [5, 10]:\n",
    "    o = np.array([r[f'nll_oracle_x{N}_trunc'] for r in results])\n",
    "    rn = np.array([r[f'nll_random_x{N}_trunc'] for r in results])\n",
    "    sc = np.array([r[f'nll_scrambled_x{N}_trunc'] for r in results])\n",
    "    t = (bare_nlls - o).mean()\n",
    "    final_results['decomposition'][str(N)] = {\n",
    "        'structure_pct': float((bare_nlls - rn).mean() / t * 100) if t > 0 else 0,\n",
    "        'vocabulary_pct': float((rn - sc).mean() / t * 100) if t > 0 else 0,\n",
    "        'semantics_pct': float((sc - o).mean() / t * 100) if t > 0 else 0,\n",
    "    }\n",
    "\n",
    "# Add content concentration results\n",
    "for N in [5, 10]:\n",
    "    o = np.array([r[f'nll_oracle_x{N}_trunc'] for r in results])\n",
    "    rn = np.array([r[f'nll_random_x{N}_trunc'] for r in results])\n",
    "    c = np.array([r[f'nll_content_x{N}_trunc'] for r in results])\n",
    "    rc = np.array([r[f'nll_random_content_x{N}_trunc'] for r in results])\n",
    "    final_results['content_concentration'][str(N)] = {\n",
    "        'oracle_sem_struct_ratio': float(\n",
    "            (rn - o).mean() / (bare_nlls - rn).mean()) if (bare_nlls - rn).mean() > 0 else 0,\n",
    "        'content_sem_struct_ratio': float(\n",
    "            (rc - c).mean() / (bare_nlls - rc).mean()) if (bare_nlls - rc).mean() > 0 else 0,\n",
    "    }\n",
    "\n",
    "with open(RESULTS_DIR / 'results.json', 'w') as f:\n",
    "    json.dump(final_results, f, indent=2)\n",
    "print(f\"\\nResults saved to {RESULTS_DIR / 'results.json'}\")\n",
    "\n",
    "# Cleanup\n",
    "print(f\"\\nCleaning up GPU memory...\")\n",
    "mem_before = torch.cuda.memory_allocated() / 1e9\n",
    "del model, processor, tokenizer\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "mem_after = torch.cuda.memory_allocated() / 1e9\n",
    "print(f\"GPU memory: {mem_before:.2f} GB -> {mem_after:.2f} GB\")\n",
    "print(\"Done!\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
