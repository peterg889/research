{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Experiment 09: Prefix LM Replication with ChatGLM-6B\n",
    "\n",
    "## Motivation\n",
    "\n",
    "Experiments 01-07 used Mistral-7B (causal-only). Suffix priming failed because passage tokens never attend to suffix tokens — the suffix is just noise in the context window. The core hypothesis (surrogate-primed KV caches) requires **bidirectional attention on the prefix** so passage representations are genuinely informed by the surrogate.\n",
    "\n",
    "ChatGLM-6B (`THUDM/chatglm-6b`) is a true prefix LM: bidirectional attention on context tokens, causal on generation. This is the decisive test.\n",
    "\n",
    "## 20 Conditions\n",
    "\n",
    "- **Group A (2)**: Baselines — bare, bare_padded\n",
    "- **Group B (6)**: Suffix priming — sfx_gen_routed, sfx_perfect, sfx_irrel, sfx_shuffled, sfx_rand_tokens, sfx_summary\n",
    "- **Group C (3)**: Prefix priming (full context) — pfx_full_routed, pfx_full_perfect, pfx_full_irrel\n",
    "- **Group D (3)**: Prefix priming (truncated + 2D RoPE correction) — pfx_trunc_routed, pfx_trunc_perfect, pfx_trunc_irrel\n",
    "- **Group E (2)**: Format sensitivity — sfx_template, sfx_raw\n",
    "- **Group F (4)**: Query-free scoring — bare_noq, sfx_gen_routed_noq, sfx_irrel_noq, sfx_perfect_noq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch: 2.10.0+cu128\n",
      "CUDA available: True\n",
      "GPU: NVIDIA L4\n",
      "GPU Memory: 23.6 GB\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import copy\n",
    "import json\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "from typing import Dict, List, Any, Optional, Tuple\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sys.path.insert(0, '.')\n",
    "\n",
    "from lib import (\n",
    "    ExperimentConfig,\n",
    "    load_chatglm,\n",
    "    build_kv_cache_chatglm,\n",
    "    score_answer_with_cache_chatglm,\n",
    "    build_suffix_kv_cache_chatglm,\n",
    "    build_prefix_kv_cache_chatglm,\n",
    "    build_truncated_kv_cache_chatglm,\n",
    "    correct_2d_rope_positions,\n",
    "    generate_all_5_surrogates_chatglm,\n",
    "    generate_summary_chatglm,\n",
    "    compute_similarity,\n",
    "    load_evaluation_samples,\n",
    "    load_ms_marco,\n",
    "    TOP_5_SURROGATE_TEMPLATES,\n",
    ")\n",
    "from lib.analysis import cohens_d\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: THUDM/chatglm-6b\n",
      "Model type: chatglm\n",
      "Samples requested: 800\n",
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "config = ExperimentConfig(\n",
    "    model_name=\"THUDM/chatglm-6b\",\n",
    "    model_type=\"chatglm\",\n",
    "    num_samples=800,\n",
    "    min_passage_words=50,\n",
    "    max_passage_words=300,\n",
    "    surrogate_max_tokens=45,\n",
    "    surrogate_temperature=0.3,\n",
    "    seed=42,\n",
    "    query_template=\"\\n\\nQuery: {query}\\n\\nAnswer:\",\n",
    ")\n",
    "\n",
    "print(f\"Model: {config.model_name}\")\n",
    "print(f\"Model type: {config.model_type}\")\n",
    "print(f\"Samples requested: {config.num_samples}\")\n",
    "print(f\"Device: {config.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89bw2uetgna",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading THUDM/chatglm-6b...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbe6b7655bd34f5faaf540e944df9e7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/368 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded. Parameters: 3,889,356,800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cd48f49724040dabac3263ccfba1a4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding model loaded.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Loading {config.model_name}...\")\n",
    "model, tokenizer = load_chatglm(config)\n",
    "print(f\"Model loaded. Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# Embedding model for routing\n",
    "from sentence_transformers import SentenceTransformer\n",
    "embed_model = SentenceTransformer(\"all-MiniLM-L6-v2\", device=\"cpu\")\n",
    "print(\"Embedding model loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## Validation: Cache Shape and Prefix LM Behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache length: 4\n",
      "Key shape: torch.Size([4, 1, 32, 128])  (expected: [seq_len, batch=1, heads, head_dim])\n",
      "Num layers: 28\n",
      "PASSED: Cache has seq-first layout.\n"
     ]
    }
   ],
   "source": [
    "# Validation 1: Cache shape (seq-first)\n",
    "test_len, test_cache = build_kv_cache_chatglm(\"Hello world\", model, tokenizer, config)\n",
    "k0, v0 = test_cache[0]\n",
    "print(f\"Cache length: {test_len}\")\n",
    "print(f\"Key shape: {k0.shape}  (expected: [seq_len, batch=1, heads, head_dim])\")\n",
    "print(f\"Num layers: {len(test_cache)}\")\n",
    "assert k0.shape[0] == test_len, \"First dim should be seq_len\"\n",
    "assert k0.shape[1] == 1, \"Second dim should be batch=1\"\n",
    "print(\"PASSED: Cache has seq-first layout.\")\n",
    "\n",
    "del test_cache\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bare cache length: 15\n",
      "Suffix cache length: 25\n",
      "  Layer 0: max key diff = 0.000000\n",
      "  Layer 1: max key diff = 0.905273\n",
      "  Layer 2: max key diff = 0.879883\n",
      "  Layer 3: max key diff = 1.496094\n",
      "  Layer 4: max key diff = 1.404297\n",
      "\n",
      "PASSED: Passage KV entries DIFFER with suffix — bidirectional attention confirmed!\n",
      "This is the key difference from causal Mistral.\n"
     ]
    }
   ],
   "source": [
    "# Validation 2: Prefix LM bidirectional attention check\n",
    "# With prefix LM, passage KV entries should DIFFER when suffix is added\n",
    "# (opposite of causal Mistral where they were byte-identical)\n",
    "\n",
    "test_passage = \"The quick brown fox jumps over the lazy dog near the river.\"\n",
    "test_suffix = \"What animal is mentioned in this text?\"\n",
    "\n",
    "bare_len, bare_cache = build_kv_cache_chatglm(test_passage, model, tokenizer, config)\n",
    "sfx_len, sfx_cache = build_suffix_kv_cache_chatglm(\n",
    "    test_passage, test_suffix, model, tokenizer, config\n",
    ")\n",
    "\n",
    "print(f\"Bare cache length: {bare_len}\")\n",
    "print(f\"Suffix cache length: {sfx_len}\")\n",
    "\n",
    "# Check first few layers\n",
    "differs = False\n",
    "for layer_idx in range(min(5, len(bare_cache))):\n",
    "    bare_k = bare_cache[layer_idx][0]  # [seq, batch, heads, head_dim]\n",
    "    sfx_k = sfx_cache[layer_idx][0]\n",
    "    # Compare overlapping token positions (excluding special tokens at end)\n",
    "    min_len = min(bare_k.shape[0], sfx_k.shape[0]) - 2\n",
    "    if min_len > 0:\n",
    "        max_diff = (bare_k[:min_len] - sfx_k[:min_len]).abs().max().item()\n",
    "        print(f\"  Layer {layer_idx}: max key diff = {max_diff:.6f}\")\n",
    "        if max_diff > 1e-3:\n",
    "            differs = True\n",
    "\n",
    "if differs:\n",
    "    print(\"\\nPASSED: Passage KV entries DIFFER with suffix — bidirectional attention confirmed!\")\n",
    "    print(\"This is the key difference from causal Mistral.\")\n",
    "else:\n",
    "    print(\"\\nWARNING: Passage KV entries are similar. Check tokenization alignment.\")\n",
    "\n",
    "del bare_cache, sfx_cache\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cell-9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RoPE round-trip max error: 3.91e-03\n",
      "PASSED: 2D RoPE correction round-trip verified.\n"
     ]
    }
   ],
   "source": [
    "# Validation 3: RoPE correction round-trip\n",
    "\n",
    "# Use a small synthetic cache for the round-trip test\n",
    "head_dim = model.config.hidden_size // model.config.num_attention_heads\n",
    "n_heads = model.config.num_attention_heads\n",
    "\n",
    "# Create a small test cache matching ChatGLM dims\n",
    "test_cache_rt = tuple(\n",
    "    (\n",
    "        torch.randn(5, 1, n_heads, head_dim, device=config.device, dtype=torch.float16),\n",
    "        torch.randn(5, 1, n_heads, head_dim, device=config.device, dtype=torch.float16),\n",
    "    )\n",
    "    for _ in range(2)  # just 2 layers for speed\n",
    ")\n",
    "original = tuple((k.clone(), v.clone()) for k, v in test_cache_rt)\n",
    "\n",
    "offset = 7\n",
    "corrected = correct_2d_rope_positions(test_cache_rt, offset, model)\n",
    "restored = correct_2d_rope_positions(corrected, -offset, model)\n",
    "\n",
    "max_err = max(\n",
    "    (restored[i][0] - original[i][0]).abs().max().item()\n",
    "    for i in range(len(original))\n",
    ")\n",
    "print(f\"RoPE round-trip max error: {max_err:.2e}\")\n",
    "assert max_err < 1e-2, f\"Round-trip error too large: {max_err}\"\n",
    "print(\"PASSED: 2D RoPE correction round-trip verified.\")\n",
    "\n",
    "del test_cache_rt, original, corrected, restored\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cell-10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scoring sanity check NLL: 1.2256\n",
      "PASSED: Scoring produces finite NLL.\n"
     ]
    }
   ],
   "source": [
    "# Validation 4: Scoring sanity check\n",
    "passage = \"Paris is the capital of France.\"\n",
    "query_prompt = \"\\n\\nQuery: What is the capital of France?\\n\\nAnswer:\"\n",
    "answer = \"Paris\"\n",
    "\n",
    "ctx_len, cache = build_kv_cache_chatglm(passage, model, tokenizer, config)\n",
    "nll = score_answer_with_cache_chatglm(\n",
    "    cache, ctx_len, query_prompt, answer, model, tokenizer, config\n",
    ")\n",
    "print(f\"Scoring sanity check NLL: {nll:.4f}\")\n",
    "assert 0 < nll < 100, f\"NLL should be finite, got {nll}\"\n",
    "print(\"PASSED: Scoring produces finite NLL.\")\n",
    "\n",
    "del cache\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cell-12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`trust_remote_code` is not supported anymore.\n",
      "Please check that the Hugging Face dataset 'microsoft/ms_marco' isn't based on a loading script and remove `trust_remote_code`.\n",
      "If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading microsoft/ms_marco dataset...\n",
      "Dataset loaded: 10047 samples\n",
      "Filtering samples...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98f1cb2f66184e8ba17d7bbfc6aad66b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filtering:   0%|          | 0/10047 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 800 samples\n",
      "Raw samples after basic filtering: 800\n",
      "\n",
      "Filtering stats:\n",
      "  Excluded (answer/passage ratio > 0.5): 62\n",
      "  Excluded (answer < 2 tokens):          71\n",
      "  Remaining samples:                     667\n",
      "\n",
      "Example sample:\n",
      "  Query:   sustainability is also referred to as...\n",
      "  Passage: In a broader context, social, environmental and economic demands are considered the three pillars of...\n",
      "  Answer:  The triple bottom line\n"
     ]
    }
   ],
   "source": [
    "dataset = load_ms_marco(config)\n",
    "raw_samples = load_evaluation_samples(dataset, config, require_answer=True)\n",
    "print(f\"Raw samples after basic filtering: {len(raw_samples)}\")\n",
    "\n",
    "# Same additional filters as previous experiments\n",
    "filtered_samples = []\n",
    "excluded_ratio = 0\n",
    "excluded_short_answer = 0\n",
    "\n",
    "for s in raw_samples:\n",
    "    if len(s['answer']) / max(len(s['passage']), 1) > 0.5:\n",
    "        excluded_ratio += 1\n",
    "        continue\n",
    "    answer_ids = tokenizer.encode(s['answer'], add_special_tokens=False)\n",
    "    if isinstance(answer_ids, torch.Tensor):\n",
    "        answer_ids = answer_ids.tolist()\n",
    "    if isinstance(answer_ids[0], list):\n",
    "        answer_ids = answer_ids[0]\n",
    "    if len(answer_ids) < 2:\n",
    "        excluded_short_answer += 1\n",
    "        continue\n",
    "    filtered_samples.append(s)\n",
    "\n",
    "samples = filtered_samples\n",
    "print(f\"\\nFiltering stats:\")\n",
    "print(f\"  Excluded (answer/passage ratio > 0.5): {excluded_ratio}\")\n",
    "print(f\"  Excluded (answer < 2 tokens):          {excluded_short_answer}\")\n",
    "print(f\"  Remaining samples:                     {len(samples)}\")\n",
    "\n",
    "s = samples[0]\n",
    "print(f\"\\nExample sample:\")\n",
    "print(f\"  Query:   {s['query'][:100]}...\")\n",
    "print(f\"  Passage: {s['passage'][:100]}...\")\n",
    "print(f\"  Answer:  {s['answer'][:100]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cell-14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper functions defined.\n"
     ]
    }
   ],
   "source": [
    "def shuffle_text(text: str, rng: random.Random) -> str:\n",
    "    words = text.split()\n",
    "    rng.shuffle(words)\n",
    "    return ' '.join(words)\n",
    "\n",
    "\n",
    "def generate_random_tokens(tokenizer, n_tokens: int, rng: random.Random) -> str:\n",
    "    vocab_size = tokenizer.vocab_size\n",
    "    random_ids = [rng.randint(100, vocab_size - 1) for _ in range(n_tokens)]\n",
    "    return tokenizer.decode(random_ids)\n",
    "\n",
    "\n",
    "def get_irrelevant_query(samples: list, current_idx: int, rng: random.Random) -> str:\n",
    "    other_idx = current_idx\n",
    "    while other_idx == current_idx:\n",
    "        other_idx = rng.randint(0, len(samples) - 1)\n",
    "    return samples[other_idx]['query']\n",
    "\n",
    "\n",
    "def deep_copy_cache(cache):\n",
    "    \"\"\"Deep copy a ChatGLM tuple-of-tuples cache.\"\"\"\n",
    "    return tuple((k.clone(), v.clone()) for k, v in cache)\n",
    "\n",
    "\n",
    "print(\"Helper functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## Surrogate Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cell-16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The dtype of attention mask (torch.int64) is not bool\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'ChatGLMForConditionalGeneration' object has no attribute '_extract_past_from_model_output'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Test surrogate generation on one sample\u001b[39;00m\n\u001b[1;32m      2\u001b[0m test_sample \u001b[38;5;241m=\u001b[39m samples[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m----> 3\u001b[0m test_surrogates \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_all_5_surrogates_chatglm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_sample\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpassage\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerated surrogates:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, surr \u001b[38;5;129;01min\u001b[39;00m test_surrogates\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m~/research/directed_kvcache/./lib/surrogate.py:165\u001b[0m, in \u001b[0;36mgenerate_all_5_surrogates_chatglm\u001b[0;34m(doc_text, model, tokenizer, config)\u001b[0m\n\u001b[1;32m    163\u001b[0m surrogates \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, template \u001b[38;5;129;01min\u001b[39;00m TOP_5_SURROGATE_TEMPLATES\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m--> 165\u001b[0m     surrogates[key] \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_surrogate_with_template_chatglm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdoc_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemplate\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mprompt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m surrogates\n",
      "File \u001b[0;32m~/research/directed_kvcache/./lib/surrogate.py:143\u001b[0m, in \u001b[0;36mgenerate_surrogate_with_template_chatglm\u001b[0;34m(doc_text, template_prompt, model, tokenizer, config)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;124;03mGenerate a surrogate query using ChatGLM's chat interface.\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;124;03m    Generated surrogate query string\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    142\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtemplate_prompt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mText:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mdoc_text\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 143\u001b[0m response, _ \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msurrogate_max_tokens\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msurrogate_temperature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msurrogate_temperature\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m surrogate \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39mstrip(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    152\u001b[0m surrogate \u001b[38;5;241m=\u001b[39m surrogate\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py:124\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;66;03m# pyrefly: ignore [bad-context-manager]\u001b[39;00m\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 124\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/chatglm_hyphen_6b/modeling_chatglm.py:1286\u001b[0m, in \u001b[0;36mChatGLMForConditionalGeneration.chat\u001b[0;34m(self, tokenizer, query, history, max_length, num_beams, do_sample, top_p, temperature, logits_processor, **kwargs)\u001b[0m\n\u001b[1;32m   1284\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer([prompt], return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1285\u001b[0m inputs \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m-> 1286\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgen_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1287\u001b[0m outputs \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mtolist()[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;28mlen\u001b[39m(inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m]):]\n\u001b[1;32m   1288\u001b[0m response \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(outputs)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py:124\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;66;03m# pyrefly: ignore [bad-context-manager]\u001b[39;00m\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 124\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:2669\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, custom_generate, **kwargs)\u001b[0m\n\u001b[1;32m   2666\u001b[0m model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m generation_config\u001b[38;5;241m.\u001b[39muse_cache\n\u001b[1;32m   2668\u001b[0m \u001b[38;5;66;03m# 9. Call generation mode\u001b[39;00m\n\u001b[0;32m-> 2669\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mdecoding_method\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2670\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2671\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2672\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2673\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2674\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2675\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgeneration_mode_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2676\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2677\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2679\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:2876\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2874\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m model_forward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_inputs, return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   2875\u001b[0m prefill_consumed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m-> 2876\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_model_kwargs_for_generation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2877\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2878\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2879\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_encoder_decoder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_encoder_decoder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2880\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2882\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/chatglm_hyphen_6b/modeling_chatglm.py:1078\u001b[0m, in \u001b[0;36mChatGLMForConditionalGeneration._update_model_kwargs_for_generation\u001b[0;34m(self, outputs, model_kwargs, is_encoder_decoder, standardize_cache_format)\u001b[0m\n\u001b[1;32m   1070\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_update_model_kwargs_for_generation\u001b[39m(\n\u001b[1;32m   1071\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1072\u001b[0m     outputs: ModelOutput,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1076\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[1;32m   1077\u001b[0m     \u001b[38;5;66;03m# update past_key_values\u001b[39;00m\n\u001b[0;32m-> 1078\u001b[0m     model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpast_key_values\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_extract_past_from_model_output\u001b[49m(\n\u001b[1;32m   1079\u001b[0m         outputs, standardize_cache_format\u001b[38;5;241m=\u001b[39mstandardize_cache_format\n\u001b[1;32m   1080\u001b[0m     )\n\u001b[1;32m   1082\u001b[0m     \u001b[38;5;66;03m# update attention mask\u001b[39;00m\n\u001b[1;32m   1083\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m model_kwargs:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1965\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1963\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1964\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1965\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m   1966\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1967\u001b[0m )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ChatGLMForConditionalGeneration' object has no attribute '_extract_past_from_model_output'"
     ]
    }
   ],
   "source": [
    "# Test surrogate generation on one sample\n",
    "test_sample = samples[0]\n",
    "test_surrogates = generate_all_5_surrogates_chatglm(\n",
    "    test_sample['passage'], model, tokenizer, config\n",
    ")\n",
    "\n",
    "print(\"Generated surrogates:\")\n",
    "for key, surr in test_surrogates.items():\n",
    "    sim = compute_similarity(surr, test_sample['query'], embed_model)\n",
    "    print(f\"  {key}: {surr}  (sim={sim:.3f})\")\n",
    "\n",
    "# Test summary generation\n",
    "test_summary = generate_summary_chatglm(\n",
    "    test_sample['passage'], model, tokenizer, config\n",
    ")\n",
    "print(f\"\\nSummary: {test_summary[:120]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963x2f90wru",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Surrogate quality diagnostic: check ChatGLM surrogate quality on 20 samples\n",
    "print(\"=\" * 80)\n",
    "print(\"SURROGATE QUALITY DIAGNOSTIC (20 samples)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "diag_sims = []\n",
    "for i in range(min(20, len(samples))):\n",
    "    s = samples[i]\n",
    "    surrs = generate_all_5_surrogates_chatglm(s['passage'], model, tokenizer, config)\n",
    "    sims = {k: compute_similarity(v, s['query'], embed_model) for k, v in surrs.items()}\n",
    "    best_key = max(sims, key=sims.get)\n",
    "    diag_sims.append(sims[best_key])\n",
    "    if i < 5:\n",
    "        print(f\"  Sample {i}: routed={best_key}, sim={sims[best_key]:.3f}\")\n",
    "        print(f\"    Query: {s['query'][:80]}\")\n",
    "        print(f\"    Surrogate: {surrs[best_key][:80]}\")\n",
    "\n",
    "diag_sims = np.array(diag_sims)\n",
    "print(f\"\\nRouted surrogate similarity (N=20):\")\n",
    "print(f\"  Mean:   {diag_sims.mean():.3f}\")\n",
    "print(f\"  Median: {np.median(diag_sims):.3f}\")\n",
    "print(f\"  Min:    {diag_sims.min():.3f}\")\n",
    "print(f\"  Max:    {diag_sims.max():.3f}\")\n",
    "print(f\"  Std:    {diag_sims.std():.3f}\")\n",
    "\n",
    "# Compare with Mistral reference\n",
    "for ref_path in ['07_suffix_priming_results.json', '07_checkpoint.json']:\n",
    "    if os.path.exists(ref_path):\n",
    "        with open(ref_path) as f:\n",
    "            ref_data = json.load(f)\n",
    "        ref_results = ref_data.get('results', [])\n",
    "        if ref_results and 'sfx_gen_routed_sim' in ref_results[0]:\n",
    "            ref_sims = [r['sfx_gen_routed_sim'] for r in ref_results if 'sfx_gen_routed_sim' in r]\n",
    "            print(f\"\\nMistral reference (Exp 07, N={len(ref_sims)}):\")\n",
    "            print(f\"  Mean routed similarity: {np.mean(ref_sims):.3f}\")\n",
    "            print(f\"  ChatGLM vs Mistral:     {diag_sims.mean():.3f} vs {np.mean(ref_sims):.3f}\")\n",
    "        break\n",
    "\n",
    "if diag_sims.mean() < 0.4:\n",
    "    print(\"\\n*** WARNING: Mean surrogate similarity < 0.4 ***\")\n",
    "    print(\"ChatGLM surrogates may be too poor to test the hypothesis.\")\n",
    "    print(\"Consider using an external model for surrogate generation.\")\n",
    "elif diag_sims.mean() < 0.5:\n",
    "    print(\"\\nNOTE: Surrogate quality is moderate. Results may underestimate the effect.\")\n",
    "else:\n",
    "    print(\"\\nSurrogate quality looks adequate for the experiment.\")\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "## Pipeline Verification\n",
    "\n",
    "Test all 20 conditions on one sample to verify everything works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample = samples[0]\n",
    "test_idx = 0\n",
    "test_rng = random.Random(config.seed)\n",
    "passage = test_sample['passage']\n",
    "query = test_sample['query']\n",
    "answer = test_sample['answer']\n",
    "query_prompt = config.query_template.format(query=query)\n",
    "\n",
    "print(f\"Passage: {passage[:100]}...\")\n",
    "print(f\"Query:   {query}\")\n",
    "print(f\"Answer:  {answer[:80]}\")\n",
    "print()\n",
    "\n",
    "# Generate surrogates and route\n",
    "surrogates = generate_all_5_surrogates_chatglm(passage, model, tokenizer, config)\n",
    "sims = {k: compute_similarity(v, query, embed_model) for k, v in surrogates.items()}\n",
    "routed_key = max(sims, key=sims.get)\n",
    "routed_surr = surrogates[routed_key]\n",
    "print(f\"Routed surrogate ({routed_key}): {routed_surr}\\n\")\n",
    "\n",
    "# --- Group A: Baselines ---\n",
    "# 1. Bare\n",
    "bare_len, bare_cache = build_kv_cache_chatglm(passage, model, tokenizer, config)\n",
    "bare_nll = score_answer_with_cache_chatglm(\n",
    "    deep_copy_cache(bare_cache), bare_len, query_prompt, answer, model, tokenizer, config\n",
    ")\n",
    "print(f\" 1. bare              NLL: {bare_nll:.4f}\")\n",
    "\n",
    "# 2. Bare padded\n",
    "sfx_tok_len = len(tokenizer.encode(routed_surr, add_special_tokens=False))\n",
    "if isinstance(sfx_tok_len, torch.Tensor):\n",
    "    sfx_tok_len = sfx_tok_len.item()\n",
    "padding = \"\\n\" * sfx_tok_len\n",
    "padded_len, padded_cache = build_kv_cache_chatglm(passage + padding, model, tokenizer, config)\n",
    "padded_nll = score_answer_with_cache_chatglm(\n",
    "    deep_copy_cache(padded_cache), padded_len, query_prompt, answer, model, tokenizer, config\n",
    ")\n",
    "print(f\" 2. bare_padded       NLL: {padded_nll:.4f}\")\n",
    "\n",
    "# --- Group B: Suffix Priming ---\n",
    "# 3. sfx_gen_routed\n",
    "sl, sc = build_suffix_kv_cache_chatglm(passage, routed_surr, model, tokenizer, config)\n",
    "sfx_gen_routed_nll = score_answer_with_cache_chatglm(\n",
    "    deep_copy_cache(sc), sl, query_prompt, answer, model, tokenizer, config\n",
    ")\n",
    "print(f\" 3. sfx_gen_routed    NLL: {sfx_gen_routed_nll:.4f}\")\n",
    "\n",
    "# 4. sfx_perfect\n",
    "sl, sc = build_suffix_kv_cache_chatglm(passage, query, model, tokenizer, config)\n",
    "sfx_perfect_nll = score_answer_with_cache_chatglm(\n",
    "    deep_copy_cache(sc), sl, query_prompt, answer, model, tokenizer, config\n",
    ")\n",
    "print(f\" 4. sfx_perfect       NLL: {sfx_perfect_nll:.4f}\")\n",
    "\n",
    "# 5. sfx_irrel\n",
    "irrel_q = get_irrelevant_query(samples, test_idx, test_rng)\n",
    "sl, sc = build_suffix_kv_cache_chatglm(passage, irrel_q, model, tokenizer, config)\n",
    "sfx_irrel_nll = score_answer_with_cache_chatglm(\n",
    "    deep_copy_cache(sc), sl, query_prompt, answer, model, tokenizer, config\n",
    ")\n",
    "print(f\" 5. sfx_irrel         NLL: {sfx_irrel_nll:.4f}\")\n",
    "\n",
    "# 6. sfx_shuffled\n",
    "shuffled = shuffle_text(routed_surr, test_rng)\n",
    "sl, sc = build_suffix_kv_cache_chatglm(passage, shuffled, model, tokenizer, config)\n",
    "sfx_shuffled_nll = score_answer_with_cache_chatglm(\n",
    "    deep_copy_cache(sc), sl, query_prompt, answer, model, tokenizer, config\n",
    ")\n",
    "print(f\" 6. sfx_shuffled      NLL: {sfx_shuffled_nll:.4f}\")\n",
    "\n",
    "# 7. sfx_rand_tokens\n",
    "rt = generate_random_tokens(tokenizer, 20, test_rng)\n",
    "sl, sc = build_suffix_kv_cache_chatglm(passage, rt, model, tokenizer, config)\n",
    "sfx_rand_tokens_nll = score_answer_with_cache_chatglm(\n",
    "    deep_copy_cache(sc), sl, query_prompt, answer, model, tokenizer, config\n",
    ")\n",
    "print(f\" 7. sfx_rand_tokens   NLL: {sfx_rand_tokens_nll:.4f}\")\n",
    "\n",
    "# 8. sfx_summary\n",
    "summary = generate_summary_chatglm(passage, model, tokenizer, config)\n",
    "sl, sc = build_suffix_kv_cache_chatglm(passage, summary, model, tokenizer, config, separator=\"\\n\\nSummary: \")\n",
    "sfx_summary_nll = score_answer_with_cache_chatglm(\n",
    "    deep_copy_cache(sc), sl, query_prompt, answer, model, tokenizer, config\n",
    ")\n",
    "print(f\" 8. sfx_summary       NLL: {sfx_summary_nll:.4f}\")\n",
    "\n",
    "# --- Group C: Prefix Priming (Full Context) ---\n",
    "# 9. pfx_full_routed\n",
    "fl, fc = build_prefix_kv_cache_chatglm(routed_surr, passage, model, tokenizer, config)\n",
    "pfx_full_routed_nll = score_answer_with_cache_chatglm(\n",
    "    deep_copy_cache(fc), fl, query_prompt, answer, model, tokenizer, config\n",
    ")\n",
    "print(f\" 9. pfx_full_routed   NLL: {pfx_full_routed_nll:.4f}\")\n",
    "\n",
    "# 10. pfx_full_perfect\n",
    "fl, fc = build_prefix_kv_cache_chatglm(query, passage, model, tokenizer, config)\n",
    "pfx_full_perfect_nll = score_answer_with_cache_chatglm(\n",
    "    deep_copy_cache(fc), fl, query_prompt, answer, model, tokenizer, config\n",
    ")\n",
    "print(f\"10. pfx_full_perfect  NLL: {pfx_full_perfect_nll:.4f}\")\n",
    "\n",
    "# 11. pfx_full_irrel\n",
    "irrel_q2 = get_irrelevant_query(samples, test_idx, test_rng)\n",
    "fl, fc = build_prefix_kv_cache_chatglm(irrel_q2, passage, model, tokenizer, config)\n",
    "pfx_full_irrel_nll = score_answer_with_cache_chatglm(\n",
    "    deep_copy_cache(fc), fl, query_prompt, answer, model, tokenizer, config\n",
    ")\n",
    "print(f\"11. pfx_full_irrel    NLL: {pfx_full_irrel_nll:.4f}\")\n",
    "\n",
    "# --- Group D: Prefix Priming (Truncated + 2D RoPE) ---\n",
    "# 12. pfx_trunc_routed\n",
    "dl, dc = build_truncated_kv_cache_chatglm(routed_surr, passage, model, tokenizer, config)\n",
    "pfx_trunc_routed_nll = score_answer_with_cache_chatglm(\n",
    "    deep_copy_cache(dc), dl, query_prompt, answer, model, tokenizer, config\n",
    ")\n",
    "print(f\"12. pfx_trunc_routed  NLL: {pfx_trunc_routed_nll:.4f}\")\n",
    "\n",
    "# 13. pfx_trunc_perfect\n",
    "dl, dc = build_truncated_kv_cache_chatglm(query, passage, model, tokenizer, config)\n",
    "pfx_trunc_perfect_nll = score_answer_with_cache_chatglm(\n",
    "    deep_copy_cache(dc), dl, query_prompt, answer, model, tokenizer, config\n",
    ")\n",
    "print(f\"13. pfx_trunc_perfect NLL: {pfx_trunc_perfect_nll:.4f}\")\n",
    "\n",
    "# 14. pfx_trunc_irrel\n",
    "irrel_q3 = get_irrelevant_query(samples, test_idx, test_rng)\n",
    "dl, dc = build_truncated_kv_cache_chatglm(irrel_q3, passage, model, tokenizer, config)\n",
    "pfx_trunc_irrel_nll = score_answer_with_cache_chatglm(\n",
    "    deep_copy_cache(dc), dl, query_prompt, answer, model, tokenizer, config\n",
    ")\n",
    "print(f\"14. pfx_trunc_irrel   NLL: {pfx_trunc_irrel_nll:.4f}\")\n",
    "\n",
    "# --- Group E: Format Sensitivity ---\n",
    "# 15. sfx_template\n",
    "sl, sc = build_suffix_kv_cache_chatglm(\n",
    "    passage, routed_surr, model, tokenizer, config, separator=\"\\n\\nRelated question: \"\n",
    ")\n",
    "sfx_template_nll = score_answer_with_cache_chatglm(\n",
    "    deep_copy_cache(sc), sl, query_prompt, answer, model, tokenizer, config\n",
    ")\n",
    "print(f\"15. sfx_template      NLL: {sfx_template_nll:.4f}\")\n",
    "\n",
    "# 16. sfx_raw\n",
    "sl, sc = build_suffix_kv_cache_chatglm(\n",
    "    passage, routed_surr, model, tokenizer, config, separator=\"\"\n",
    ")\n",
    "sfx_raw_nll = score_answer_with_cache_chatglm(\n",
    "    deep_copy_cache(sc), sl, query_prompt, answer, model, tokenizer, config\n",
    ")\n",
    "print(f\"16. sfx_raw           NLL: {sfx_raw_nll:.4f}\")\n",
    "\n",
    "# --- Group F: Query-Free Scoring ---\n",
    "answer_prompt = \"\\n\\nAnswer:\"\n",
    "\n",
    "bare_noq_nll = score_answer_with_cache_chatglm(\n",
    "    deep_copy_cache(bare_cache), bare_len, answer_prompt, answer, model, tokenizer, config\n",
    ")\n",
    "print(f\"17. bare_noq          NLL: {bare_noq_nll:.4f}\")\n",
    "\n",
    "sl, sc = build_suffix_kv_cache_chatglm(passage, routed_surr, model, tokenizer, config)\n",
    "sfx_gen_noq_nll = score_answer_with_cache_chatglm(\n",
    "    deep_copy_cache(sc), sl, answer_prompt, answer, model, tokenizer, config\n",
    ")\n",
    "print(f\"18. sfx_gen_routed_noq NLL: {sfx_gen_noq_nll:.4f}\")\n",
    "\n",
    "sl, sc = build_suffix_kv_cache_chatglm(passage, irrel_q, model, tokenizer, config)\n",
    "sfx_irrel_noq_nll = score_answer_with_cache_chatglm(\n",
    "    deep_copy_cache(sc), sl, answer_prompt, answer, model, tokenizer, config\n",
    ")\n",
    "print(f\"19. sfx_irrel_noq     NLL: {sfx_irrel_noq_nll:.4f}\")\n",
    "\n",
    "sl, sc = build_suffix_kv_cache_chatglm(passage, query, model, tokenizer, config)\n",
    "sfx_perf_noq_nll = score_answer_with_cache_chatglm(\n",
    "    deep_copy_cache(sc), sl, answer_prompt, answer, model, tokenizer, config\n",
    ")\n",
    "print(f\"20. sfx_perfect_noq   NLL: {sfx_perf_noq_nll:.4f}\")\n",
    "\n",
    "print(\"\\nAll 20 conditions produce finite NLLs. Pipeline verified.\")\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "## Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_sample(\n",
    "    sample: Dict,\n",
    "    idx: int,\n",
    "    all_samples: List[Dict],\n",
    "    model,\n",
    "    tokenizer,\n",
    "    embed_model,\n",
    "    config: ExperimentConfig,\n",
    ") -> Optional[Dict]:\n",
    "    \"\"\"Evaluate a single sample across all 20 experimental conditions.\"\"\"\n",
    "    passage = sample['passage']\n",
    "    query = sample['query']\n",
    "    answer = sample['answer']\n",
    "    query_prompt = config.query_template.format(query=query)\n",
    "    rng = random.Random(config.seed + idx)\n",
    "\n",
    "    # ==================== GROUP A: BASELINES ====================\n",
    "\n",
    "    # 1. Bare passage\n",
    "    bare_len, bare_cache = build_kv_cache_chatglm(passage, model, tokenizer, config)\n",
    "    bare_nll = score_answer_with_cache_chatglm(\n",
    "        deep_copy_cache(bare_cache), bare_len, query_prompt, answer, model, tokenizer, config\n",
    "    )\n",
    "\n",
    "    # ==================== GENERATE SURROGATES ====================\n",
    "    generated_surrogates = generate_all_5_surrogates_chatglm(passage, model, tokenizer, config)\n",
    "    gen_similarities = {\n",
    "        k: compute_similarity(v, query, embed_model)\n",
    "        for k, v in generated_surrogates.items()\n",
    "    }\n",
    "    gen_routed_key = max(gen_similarities, key=gen_similarities.get)\n",
    "    routed_surr = generated_surrogates[gen_routed_key]\n",
    "\n",
    "    # 2. Bare padded\n",
    "    sfx_tok_len = len(tokenizer.encode(routed_surr, add_special_tokens=False))\n",
    "    if isinstance(sfx_tok_len, torch.Tensor):\n",
    "        sfx_tok_len = sfx_tok_len.item()\n",
    "    padding = \"\\n\" * sfx_tok_len\n",
    "    padded_len, padded_cache = build_kv_cache_chatglm(passage + padding, model, tokenizer, config)\n",
    "    padded_nll = score_answer_with_cache_chatglm(\n",
    "        deep_copy_cache(padded_cache), padded_len, query_prompt, answer, model, tokenizer, config\n",
    "    )\n",
    "\n",
    "    # ==================== GROUP B: SUFFIX PRIMING ====================\n",
    "\n",
    "    # 3. sfx_gen_routed\n",
    "    sl, sc = build_suffix_kv_cache_chatglm(passage, routed_surr, model, tokenizer, config)\n",
    "    sfx_gen_routed_nll = score_answer_with_cache_chatglm(\n",
    "        deep_copy_cache(sc), sl, query_prompt, answer, model, tokenizer, config\n",
    "    )\n",
    "\n",
    "    # 4. sfx_perfect\n",
    "    sl, sc = build_suffix_kv_cache_chatglm(passage, query, model, tokenizer, config)\n",
    "    sfx_perfect_nll = score_answer_with_cache_chatglm(\n",
    "        deep_copy_cache(sc), sl, query_prompt, answer, model, tokenizer, config\n",
    "    )\n",
    "\n",
    "    # 5. sfx_irrel\n",
    "    irrel_query = get_irrelevant_query(all_samples, idx, rng)\n",
    "    sl, sc = build_suffix_kv_cache_chatglm(passage, irrel_query, model, tokenizer, config)\n",
    "    sfx_irrel_nll = score_answer_with_cache_chatglm(\n",
    "        deep_copy_cache(sc), sl, query_prompt, answer, model, tokenizer, config\n",
    "    )\n",
    "\n",
    "    # 6. sfx_shuffled\n",
    "    shuffled_text = shuffle_text(routed_surr, rng)\n",
    "    sl, sc = build_suffix_kv_cache_chatglm(passage, shuffled_text, model, tokenizer, config)\n",
    "    sfx_shuffled_nll = score_answer_with_cache_chatglm(\n",
    "        deep_copy_cache(sc), sl, query_prompt, answer, model, tokenizer, config\n",
    "    )\n",
    "\n",
    "    # 7. sfx_rand_tokens\n",
    "    rand_tokens = generate_random_tokens(tokenizer, 20, rng)\n",
    "    sl, sc = build_suffix_kv_cache_chatglm(passage, rand_tokens, model, tokenizer, config)\n",
    "    sfx_rand_tokens_nll = score_answer_with_cache_chatglm(\n",
    "        deep_copy_cache(sc), sl, query_prompt, answer, model, tokenizer, config\n",
    "    )\n",
    "\n",
    "    # 8. sfx_summary\n",
    "    summary = generate_summary_chatglm(passage, model, tokenizer, config)\n",
    "    sl, sc = build_suffix_kv_cache_chatglm(\n",
    "        passage, summary, model, tokenizer, config, separator=\"\\n\\nSummary: \"\n",
    "    )\n",
    "    sfx_summary_nll = score_answer_with_cache_chatglm(\n",
    "        deep_copy_cache(sc), sl, query_prompt, answer, model, tokenizer, config\n",
    "    )\n",
    "\n",
    "    # ==================== GROUP C: PREFIX (FULL) ====================\n",
    "\n",
    "    # 9. pfx_full_routed\n",
    "    fl, fc = build_prefix_kv_cache_chatglm(routed_surr, passage, model, tokenizer, config)\n",
    "    pfx_full_routed_nll = score_answer_with_cache_chatglm(\n",
    "        deep_copy_cache(fc), fl, query_prompt, answer, model, tokenizer, config\n",
    "    )\n",
    "\n",
    "    # 10. pfx_full_perfect\n",
    "    fl, fc = build_prefix_kv_cache_chatglm(query, passage, model, tokenizer, config)\n",
    "    pfx_full_perfect_nll = score_answer_with_cache_chatglm(\n",
    "        deep_copy_cache(fc), fl, query_prompt, answer, model, tokenizer, config\n",
    "    )\n",
    "\n",
    "    # 11. pfx_full_irrel\n",
    "    irrel_query2 = get_irrelevant_query(all_samples, idx, rng)\n",
    "    fl, fc = build_prefix_kv_cache_chatglm(irrel_query2, passage, model, tokenizer, config)\n",
    "    pfx_full_irrel_nll = score_answer_with_cache_chatglm(\n",
    "        deep_copy_cache(fc), fl, query_prompt, answer, model, tokenizer, config\n",
    "    )\n",
    "\n",
    "    # ==================== GROUP D: PREFIX (TRUNCATED) ====================\n",
    "\n",
    "    # 12. pfx_trunc_routed\n",
    "    dl, dc = build_truncated_kv_cache_chatglm(routed_surr, passage, model, tokenizer, config)\n",
    "    pfx_trunc_routed_nll = score_answer_with_cache_chatglm(\n",
    "        deep_copy_cache(dc), dl, query_prompt, answer, model, tokenizer, config\n",
    "    )\n",
    "\n",
    "    # 13. pfx_trunc_perfect\n",
    "    dl, dc = build_truncated_kv_cache_chatglm(query, passage, model, tokenizer, config)\n",
    "    pfx_trunc_perfect_nll = score_answer_with_cache_chatglm(\n",
    "        deep_copy_cache(dc), dl, query_prompt, answer, model, tokenizer, config\n",
    "    )\n",
    "\n",
    "    # 14. pfx_trunc_irrel\n",
    "    irrel_query3 = get_irrelevant_query(all_samples, idx, rng)\n",
    "    dl, dc = build_truncated_kv_cache_chatglm(irrel_query3, passage, model, tokenizer, config)\n",
    "    pfx_trunc_irrel_nll = score_answer_with_cache_chatglm(\n",
    "        deep_copy_cache(dc), dl, query_prompt, answer, model, tokenizer, config\n",
    "    )\n",
    "\n",
    "    # ==================== GROUP E: FORMAT SENSITIVITY ====================\n",
    "\n",
    "    # 15. sfx_template\n",
    "    sl, sc = build_suffix_kv_cache_chatglm(\n",
    "        passage, routed_surr, model, tokenizer, config,\n",
    "        separator=\"\\n\\nRelated question: \"\n",
    "    )\n",
    "    sfx_template_nll = score_answer_with_cache_chatglm(\n",
    "        deep_copy_cache(sc), sl, query_prompt, answer, model, tokenizer, config\n",
    "    )\n",
    "\n",
    "    # 16. sfx_raw\n",
    "    sl, sc = build_suffix_kv_cache_chatglm(\n",
    "        passage, routed_surr, model, tokenizer, config, separator=\"\"\n",
    "    )\n",
    "    sfx_raw_nll = score_answer_with_cache_chatglm(\n",
    "        deep_copy_cache(sc), sl, query_prompt, answer, model, tokenizer, config\n",
    "    )\n",
    "\n",
    "    # ==================== GROUP F: QUERY-FREE SCORING ====================\n",
    "    # Direct comparison with Exp 08 Investigation A (Mistral causal)\n",
    "    answer_prompt = \"\\n\\nAnswer:\"\n",
    "\n",
    "    # F1. bare_noq\n",
    "    bare_noq_nll = score_answer_with_cache_chatglm(\n",
    "        deep_copy_cache(bare_cache), bare_len, answer_prompt, answer, model, tokenizer, config\n",
    "    )\n",
    "\n",
    "    # F2. sfx_gen_routed_noq\n",
    "    sl_noq, sc_noq = build_suffix_kv_cache_chatglm(passage, routed_surr, model, tokenizer, config)\n",
    "    sfx_gen_routed_noq_nll = score_answer_with_cache_chatglm(\n",
    "        deep_copy_cache(sc_noq), sl_noq, answer_prompt, answer, model, tokenizer, config\n",
    "    )\n",
    "\n",
    "    # F3. sfx_irrel_noq (reuse same irrel_query from Group B condition 5)\n",
    "    sl_noq_i, sc_noq_i = build_suffix_kv_cache_chatglm(passage, irrel_query, model, tokenizer, config)\n",
    "    sfx_irrel_noq_nll = score_answer_with_cache_chatglm(\n",
    "        deep_copy_cache(sc_noq_i), sl_noq_i, answer_prompt, answer, model, tokenizer, config\n",
    "    )\n",
    "\n",
    "    # F4. sfx_perfect_noq\n",
    "    sl_noq_p, sc_noq_p = build_suffix_kv_cache_chatglm(passage, query, model, tokenizer, config)\n",
    "    sfx_perfect_noq_nll = score_answer_with_cache_chatglm(\n",
    "        deep_copy_cache(sc_noq_p), sl_noq_p, answer_prompt, answer, model, tokenizer, config\n",
    "    )\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return {\n",
    "        'idx': idx,\n",
    "        'query': query,\n",
    "        'answer_len': len(answer),\n",
    "        'passage_len': len(passage),\n",
    "\n",
    "        # Group A\n",
    "        'bare_nll': bare_nll,\n",
    "        'padded_nll': padded_nll,\n",
    "\n",
    "        # Group B\n",
    "        'sfx_gen_routed_key': gen_routed_key,\n",
    "        'sfx_gen_routed_nll': sfx_gen_routed_nll,\n",
    "        'sfx_gen_routed_sim': gen_similarities[gen_routed_key],\n",
    "        'generated_surrogates': generated_surrogates,\n",
    "        'gen_similarities': gen_similarities,\n",
    "        'sfx_perfect_nll': sfx_perfect_nll,\n",
    "        'sfx_irrel_nll': sfx_irrel_nll,\n",
    "        'sfx_shuffled_nll': sfx_shuffled_nll,\n",
    "        'sfx_rand_tokens_nll': sfx_rand_tokens_nll,\n",
    "        'sfx_summary_nll': sfx_summary_nll,\n",
    "        'summary_text': summary,\n",
    "\n",
    "        # Group C\n",
    "        'pfx_full_routed_nll': pfx_full_routed_nll,\n",
    "        'pfx_full_perfect_nll': pfx_full_perfect_nll,\n",
    "        'pfx_full_irrel_nll': pfx_full_irrel_nll,\n",
    "\n",
    "        # Group D\n",
    "        'pfx_trunc_routed_nll': pfx_trunc_routed_nll,\n",
    "        'pfx_trunc_perfect_nll': pfx_trunc_perfect_nll,\n",
    "        'pfx_trunc_irrel_nll': pfx_trunc_irrel_nll,\n",
    "\n",
    "        # Group E\n",
    "        'sfx_template_nll': sfx_template_nll,\n",
    "        'sfx_raw_nll': sfx_raw_nll,\n",
    "\n",
    "        # Group F (query-free)\n",
    "        'bare_noq_nll': bare_noq_nll,\n",
    "        'sfx_gen_routed_noq_nll': sfx_gen_routed_noq_nll,\n",
    "        'sfx_irrel_noq_nll': sfx_irrel_noq_nll,\n",
    "        'sfx_perfect_noq_nll': sfx_perfect_noq_nll,\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"evaluate_sample() defined — 20 conditions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "## Resume from Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = '09_checkpoint.json'\n",
    "results = []\n",
    "start_from = 0\n",
    "\n",
    "if os.path.exists(checkpoint_path):\n",
    "    with open(checkpoint_path) as f:\n",
    "        ckpt = json.load(f)\n",
    "    results = ckpt['results']\n",
    "    start_from = ckpt['n_done']\n",
    "    print(f\"Resuming from checkpoint: {start_from} samples already done.\")\n",
    "else:\n",
    "    print(\"No checkpoint found. Starting from scratch.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "## Main Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SAMPLES = min(200, len(samples))\n",
    "errors = 0\n",
    "start_time = time.time()\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"RUNNING EXPERIMENT 09: PREFIX LM REPLICATION WITH CHATGLM-6B\")\n",
    "print(f\"Samples: {N_SAMPLES}, Conditions per sample: 20\")\n",
    "if start_from > 0:\n",
    "    print(f\"Resuming from sample {start_from}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for idx in tqdm(range(start_from, N_SAMPLES), desc=\"Evaluating\"):\n",
    "    sample = samples[idx]\n",
    "    try:\n",
    "        result = evaluate_sample(\n",
    "            sample, idx, samples, model, tokenizer, embed_model, config\n",
    "        )\n",
    "        if result is not None:\n",
    "            results.append(result)\n",
    "    except Exception as e:\n",
    "        errors += 1\n",
    "        if errors <= 5:\n",
    "            print(f\"\\n  Error on sample {idx}: {type(e).__name__}: {e}\")\n",
    "        continue\n",
    "\n",
    "    # Checkpoint every 10 samples\n",
    "    if len(results) > 0 and len(results) % 10 == 0:\n",
    "        elapsed = time.time() - start_time\n",
    "\n",
    "        with open(checkpoint_path, 'w') as f:\n",
    "            json.dump({\n",
    "                'n_done': len(results),\n",
    "                'n_errors': errors,\n",
    "                'elapsed': elapsed,\n",
    "                'results': results,\n",
    "            }, f, default=str)\n",
    "\n",
    "        if len(results) % 50 == 0:\n",
    "            recent = results[-min(10, len(results)):]\n",
    "            bare_mean = np.mean([r['bare_nll'] for r in recent])\n",
    "            sfx_gen_mean = np.mean([r['sfx_gen_routed_nll'] for r in recent])\n",
    "            sfx_irrel_mean = np.mean([r['sfx_irrel_nll'] for r in recent])\n",
    "            wr_gen = np.mean([r['bare_nll'] - r['sfx_gen_routed_nll'] > 0 for r in recent]) * 100\n",
    "            print(\n",
    "                f\"\\n  [{len(results):>4d} done | {elapsed/60:.0f}m elapsed]\"\n",
    "                f\"\\n  Last batch: bare={bare_mean:.3f}  sfx_gen={sfx_gen_mean:.3f} ({wr_gen:.0f}% win)\"\n",
    "                f\"  sfx_irrel={sfx_irrel_mean:.3f}\"\n",
    "            )\n",
    "\n",
    "elapsed_total = time.time() - start_time\n",
    "print(f\"\\nDone. {len(results)} evaluated, {errors} errors.\")\n",
    "print(f\"Total time: {elapsed_total/60:.1f} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-25",
   "metadata": {},
   "source": [
    "## Primary Results\n",
    "\n",
    "Summary table: all 20 conditions vs bare baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-26",
   "metadata": {},
   "outputs": [],
   "source": [
    "bare_arr = np.array([r['bare_nll'] for r in results])\n",
    "n = len(results)\n",
    "\n",
    "conditions = [\n",
    "    (' 1. bare (BASELINE)',         'bare_nll'),\n",
    "    (' 2. bare_padded',             'padded_nll'),\n",
    "    (' 3. sfx_gen_routed',          'sfx_gen_routed_nll'),\n",
    "    (' 4. sfx_perfect',             'sfx_perfect_nll'),\n",
    "    (' 5. sfx_irrel',               'sfx_irrel_nll'),\n",
    "    (' 6. sfx_shuffled',            'sfx_shuffled_nll'),\n",
    "    (' 7. sfx_rand_tokens',         'sfx_rand_tokens_nll'),\n",
    "    (' 8. sfx_summary',             'sfx_summary_nll'),\n",
    "    (' 9. pfx_full_routed',         'pfx_full_routed_nll'),\n",
    "    ('10. pfx_full_perfect',        'pfx_full_perfect_nll'),\n",
    "    ('11. pfx_full_irrel',          'pfx_full_irrel_nll'),\n",
    "    ('12. pfx_trunc_routed',        'pfx_trunc_routed_nll'),\n",
    "    ('13. pfx_trunc_perfect',       'pfx_trunc_perfect_nll'),\n",
    "    ('14. pfx_trunc_irrel',         'pfx_trunc_irrel_nll'),\n",
    "    ('15. sfx_template',            'sfx_template_nll'),\n",
    "    ('16. sfx_raw',                 'sfx_raw_nll'),\n",
    "    ('17. bare_noq',                'bare_noq_nll'),\n",
    "    ('18. sfx_gen_routed_noq',      'sfx_gen_routed_noq_nll'),\n",
    "    ('19. sfx_irrel_noq',           'sfx_irrel_noq_nll'),\n",
    "    ('20. sfx_perfect_noq',         'sfx_perfect_noq_nll'),\n",
    "]\n",
    "\n",
    "print(\"=\" * 130)\n",
    "print(f\"ALL 20 CONDITIONS vs BARE BASELINE  (N = {n})\")\n",
    "print(\"Positive delta = better than bare baseline.  Sorted by mean NLL.\")\n",
    "print(\"=\" * 130)\n",
    "header = f\"{'Condition':<28} {'Mean NLL':>10} {'Std':>8} {'Delta':>10} {'Win%':>8} {'t-stat':>8} {'p-value':>12} {'Cohen d':>10}\"\n",
    "print(header)\n",
    "print(\"-\" * 130)\n",
    "\n",
    "rows = []\n",
    "for label, key in conditions:\n",
    "    arr = np.array([r[key] for r in results])\n",
    "    delta = bare_arr - arr\n",
    "    mean_nll = np.mean(arr)\n",
    "    std_nll = np.std(arr)\n",
    "    if key == 'bare_nll':\n",
    "        rows.append((mean_nll, label, std_nll, '--', '--', '--', '--', '--'))\n",
    "    else:\n",
    "        t, p = stats.ttest_rel(bare_arr, arr)\n",
    "        d = cohens_d(delta)\n",
    "        win_rate = np.mean(delta > 0) * 100\n",
    "        rows.append((mean_nll, label, std_nll, f\"{np.mean(delta):+.4f}\", f\"{win_rate:.1f}%\",\n",
    "                      f\"{t:.3f}\", f\"{p:.6f}\", f\"{d:.4f}\"))\n",
    "\n",
    "rows.sort(key=lambda x: x[0])\n",
    "for mean_nll, label, std_nll, *rest in rows:\n",
    "    vals = [f\"{mean_nll:>10.4f}\", f\"{std_nll:>8.4f}\"] + [\n",
    "        f\"{v:>12}\" if i >= 4 else f\"{v:>10}\" for i, v in enumerate(rest)\n",
    "    ]\n",
    "    print(f\"{label:<28} {' '.join(vals)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-27",
   "metadata": {},
   "source": [
    "## Semantic Isolation Test\n",
    "\n",
    "The decisive test: with bidirectional attention, does suffix content matter?\n",
    "Compare correlations of per-sample deltas between sfx_gen_routed and controls.\n",
    "With prefix LM, we expect r << 0.5 (vs r=0.796 with causal Mistral)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-28",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.01\n",
    "n_key_tests = 4\n",
    "bonferroni_alpha = alpha / n_key_tests\n",
    "\n",
    "print(\"=\" * 110)\n",
    "print(\"SEMANTIC ISOLATION: KEY PAIRWISE COMPARISONS\")\n",
    "print(f\"Bonferroni-corrected alpha = {alpha}/{n_key_tests} = {bonferroni_alpha:.4f}\")\n",
    "print(\"=\" * 110)\n",
    "\n",
    "pairwise = [\n",
    "    (\"sfx_gen_routed (3) vs bare (1)\",          'sfx_gen_routed_nll', 'bare_nll',\n",
    "     \"Does suffix priming help with prefix LM?\"),\n",
    "    (\"sfx_gen_routed (3) vs sfx_irrel (5)\",     'sfx_gen_routed_nll', 'sfx_irrel_nll',\n",
    "     \"DECISIVE: Semantic or structural?\"),\n",
    "    (\"sfx_gen_routed (3) vs sfx_shuffled (6)\",  'sfx_gen_routed_nll', 'sfx_shuffled_nll',\n",
    "     \"Does word order matter in suffix?\"),\n",
    "    (\"sfx_gen_routed (3) vs sfx_rand_tok (7)\",  'sfx_gen_routed_nll', 'sfx_rand_tokens_nll',\n",
    "     \"Does coherence matter?\"),\n",
    "]\n",
    "\n",
    "print(f\"\\n{'Comparison':<46} {'Mean A':>8} {'Mean B':>8} {'Delta':>8} {'t':>8} {'p':>12} {'d':>8} {'Sig?':>6}\")\n",
    "print(\"-\" * 110)\n",
    "\n",
    "for label, key_a, key_b, question in pairwise:\n",
    "    a = np.array([r[key_a] for r in results])\n",
    "    b = np.array([r[key_b] for r in results])\n",
    "    diff = b - a  # positive = A is better (lower NLL)\n",
    "    t, p = stats.ttest_rel(a, b)\n",
    "    d = cohens_d(diff)\n",
    "    sig = \"YES\" if p < bonferroni_alpha else \"no\"\n",
    "    print(f\"{label:<46} {np.mean(a):>8.4f} {np.mean(b):>8.4f} {np.mean(diff):>+8.4f} {t:>8.3f} {p:>12.6f} {d:>8.4f} {sig:>6}\")\n",
    "    print(f\"  Q: {question}\")\n",
    "\n",
    "# Correlation of per-sample deltas\n",
    "print(f\"\\n{'=' * 110}\")\n",
    "print(\"CORRELATION OF PER-SAMPLE DELTAS\")\n",
    "print(\"If r << 0.5, the effect is content-dependent (unlike causal Mistral r=0.796)\")\n",
    "print(\"=\" * 110)\n",
    "\n",
    "sfx_gen_deltas = np.array([r['bare_nll'] - r['sfx_gen_routed_nll'] for r in results])\n",
    "sfx_shuf_deltas = np.array([r['bare_nll'] - r['sfx_shuffled_nll'] for r in results])\n",
    "sfx_irrel_deltas = np.array([r['bare_nll'] - r['sfx_irrel_nll'] for r in results])\n",
    "sfx_rand_deltas = np.array([r['bare_nll'] - r['sfx_rand_tokens_nll'] for r in results])\n",
    "\n",
    "correlations = [\n",
    "    (\"sfx_gen_routed vs sfx_shuffled\",     sfx_gen_deltas, sfx_shuf_deltas),\n",
    "    (\"sfx_gen_routed vs sfx_irrel\",        sfx_gen_deltas, sfx_irrel_deltas),\n",
    "    (\"sfx_gen_routed vs sfx_rand_tokens\",  sfx_gen_deltas, sfx_rand_deltas),\n",
    "]\n",
    "\n",
    "print(f\"\\n{'Delta correlation':<42} {'r':>8} {'p':>12}\")\n",
    "print(\"-\" * 65)\n",
    "for label, x, y in correlations:\n",
    "    r, p = stats.pearsonr(x, y)\n",
    "    print(f\"{label:<42} {r:>8.4f} {p:>12.6f}\")\n",
    "\n",
    "r_key = stats.pearsonr(sfx_gen_deltas, sfx_shuf_deltas)[0]\n",
    "print(f\"\\nCausal Mistral reference: r=0.796\")\n",
    "print(f\"Prefix LM ChatGLM result: r={r_key:.4f}\")\n",
    "if r_key < 0.5:\n",
    "    print(\"STRONG: Low correlation — effect IS content-dependent with prefix LM!\")\n",
    "elif r_key < 0.8:\n",
    "    print(\"MODERATE: Some reduction from causal, partial semantic signal.\")\n",
    "else:\n",
    "    print(\"WEAK: High correlation — still content-independent even with prefix LM.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2wnwsvoonzo",
   "metadata": {},
   "source": [
    "## Query-Free Scoring (Direct Exp 08 Comparison)\n",
    "\n",
    "Exp 08 Investigation A showed that removing the query did NOT reveal semantic signal with Mistral (causal): relevant vs irrelevant p=0.23. With ChatGLM (prefix LM), bidirectional attention should change passage representations, so we expect semantic differentiation even without the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "n3ocyj7n0ze",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 110)\n",
    "print(\"QUERY-FREE SCORING: DIRECT COMPARISON WITH EXP 08\")\n",
    "print(\"=\" * 110)\n",
    "\n",
    "bare_noq_arr = np.array([r['bare_noq_nll'] for r in results])\n",
    "sfx_gen_noq_arr = np.array([r['sfx_gen_routed_noq_nll'] for r in results])\n",
    "sfx_irrel_noq_arr = np.array([r['sfx_irrel_noq_nll'] for r in results])\n",
    "sfx_perf_noq_arr = np.array([r['sfx_perfect_noq_nll'] for r in results])\n",
    "\n",
    "noq_conditions = [\n",
    "    ('bare (no query) BASELINE', bare_noq_arr),\n",
    "    ('sfx_gen_routed (no query)', sfx_gen_noq_arr),\n",
    "    ('sfx_perfect (no query)', sfx_perf_noq_arr),\n",
    "    ('sfx_irrel (no query)', sfx_irrel_noq_arr),\n",
    "]\n",
    "\n",
    "print(f\"\\n{'Condition':<32} {'Mean NLL':>10} {'Std':>8} {'Delta vs bare_noq':>18} {'Win%':>8} {'t':>8} {'p':>12} {'d':>8}\")\n",
    "print(\"-\" * 110)\n",
    "\n",
    "for label, arr in noq_conditions:\n",
    "    delta = bare_noq_arr - arr\n",
    "    mn = np.mean(arr)\n",
    "    sd = np.std(arr)\n",
    "    if 'BASELINE' in label:\n",
    "        print(f\"{label:<32} {mn:>10.4f} {sd:>8.4f} {'BASELINE':>18} {'--':>8} {'--':>8} {'--':>12} {'--':>8}\")\n",
    "    else:\n",
    "        t, p = stats.ttest_rel(bare_noq_arr, arr)\n",
    "        d = cohens_d(delta)\n",
    "        wr = np.mean(delta > 0) * 100\n",
    "        print(f\"{label:<32} {mn:>10.4f} {sd:>8.4f} {np.mean(delta):>+18.4f} {wr:>7.1f}% {t:>8.3f} {p:>12.6f} {d:>8.4f}\")\n",
    "\n",
    "# Key test: relevant vs irrelevant without query\n",
    "print(f\"\\n{'=' * 110}\")\n",
    "print(\"KEY TEST: Does removing the query reveal semantic signal?\")\n",
    "print(\"=\" * 110)\n",
    "\n",
    "t_noq, p_noq = stats.ttest_rel(sfx_gen_noq_arr, sfx_irrel_noq_arr)\n",
    "d_noq = cohens_d(sfx_irrel_noq_arr - sfx_gen_noq_arr)\n",
    "print(f\"  sfx_gen_routed vs sfx_irrel (no query):\")\n",
    "print(f\"    Mean gen:   {np.mean(sfx_gen_noq_arr):.4f}\")\n",
    "print(f\"    Mean irrel: {np.mean(sfx_irrel_noq_arr):.4f}\")\n",
    "print(f\"    Delta:      {np.mean(sfx_irrel_noq_arr - sfx_gen_noq_arr):+.4f}\")\n",
    "print(f\"    t={t_noq:.3f}, p={p_noq:.6f}, d={d_noq:.4f}\")\n",
    "print(f\"\\n  Exp 08 Mistral reference: p=0.23, d~0 (NOT significant)\")\n",
    "if p_noq < 0.01 and np.mean(sfx_gen_noq_arr) < np.mean(sfx_irrel_noq_arr):\n",
    "    print(f\"  ChatGLM result: p={p_noq:.6f} — SIGNIFICANT semantic differentiation!\")\n",
    "    print(\"  Bidirectional attention enables semantic priming even without query.\")\n",
    "elif p_noq < 0.05:\n",
    "    print(f\"  ChatGLM result: p={p_noq:.6f} — Marginal semantic signal.\")\n",
    "else:\n",
    "    print(f\"  ChatGLM result: p={p_noq:.6f} — No semantic signal, same as causal Mistral.\")\n",
    "\n",
    "# Query-free delta correlation\n",
    "noq_gen_deltas = bare_noq_arr - sfx_gen_noq_arr\n",
    "noq_irrel_deltas = bare_noq_arr - sfx_irrel_noq_arr\n",
    "r_noq, p_r_noq = stats.pearsonr(noq_gen_deltas, noq_irrel_deltas)\n",
    "print(f\"\\n  Query-free delta correlation (gen vs irrel): r={r_noq:.4f} (p={p_r_noq:.6f})\")\n",
    "print(f\"  Exp 08 Mistral with-query reference: r~0.19\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-29",
   "metadata": {},
   "source": [
    "## Suffix vs Prefix Comparison\n",
    "\n",
    "Same surrogate, different placement. Both in bidirectional prefix region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-30",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 110)\n",
    "print(\"SUFFIX vs PREFIX: SAME SURROGATE, DIFFERENT PLACEMENT\")\n",
    "print(\"Both are in the bidirectional prefix region.\")\n",
    "print(\"=\" * 110)\n",
    "\n",
    "comparisons = [\n",
    "    (\"sfx_gen_routed (3) vs pfx_full_routed (9)\",\n",
    "     'sfx_gen_routed_nll', 'pfx_full_routed_nll',\n",
    "     \"Suffix vs full prefix (same routed surrogate)\"),\n",
    "    (\"sfx_perfect (4) vs pfx_full_perfect (10)\",\n",
    "     'sfx_perfect_nll', 'pfx_full_perfect_nll',\n",
    "     \"Suffix vs prefix oracle (actual query)\"),\n",
    "    (\"sfx_gen_routed (3) vs pfx_trunc_routed (12)\",\n",
    "     'sfx_gen_routed_nll', 'pfx_trunc_routed_nll',\n",
    "     \"Suffix vs truncated prefix (Mistral exp 06 comparison)\"),\n",
    "]\n",
    "\n",
    "print(f\"\\n{'Comparison':<52} {'Mean A':>8} {'Mean B':>8} {'Delta':>8} {'t':>8} {'p':>12} {'d':>8}\")\n",
    "print(\"-\" * 110)\n",
    "\n",
    "for label, key_a, key_b, question in comparisons:\n",
    "    a = np.array([r[key_a] for r in results])\n",
    "    b = np.array([r[key_b] for r in results])\n",
    "    diff = b - a  # positive = A (first) is better\n",
    "    t, p = stats.ttest_rel(a, b)\n",
    "    d = cohens_d(diff)\n",
    "    print(f\"{label:<52} {np.mean(a):>8.4f} {np.mean(b):>8.4f} {np.mean(diff):>+8.4f} {t:>8.3f} {p:>12.6f} {d:>8.4f}\")\n",
    "    print(f\"  Q: {question}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-31",
   "metadata": {},
   "source": [
    "## Truncation Analysis\n",
    "\n",
    "Does 2D RoPE correction preserve the benefit of prefix priming?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-32",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 110)\n",
    "print(\"TRUNCATION ANALYSIS: Full prefix vs Truncated prefix\")\n",
    "print(\"=\" * 110)\n",
    "\n",
    "trunc_comparisons = [\n",
    "    (\"pfx_full_routed (9) vs pfx_trunc_routed (12)\",\n",
    "     'pfx_full_routed_nll', 'pfx_trunc_routed_nll',\n",
    "     \"Does truncation + RoPE correction preserve benefit?\"),\n",
    "    (\"pfx_full_perfect (10) vs pfx_trunc_perfect (13)\",\n",
    "     'pfx_full_perfect_nll', 'pfx_trunc_perfect_nll',\n",
    "     \"Oracle version of truncation test\"),\n",
    "    (\"pfx_full_irrel (11) vs pfx_trunc_irrel (14)\",\n",
    "     'pfx_full_irrel_nll', 'pfx_trunc_irrel_nll',\n",
    "     \"Irrelevant prefix: does truncation change behavior?\"),\n",
    "]\n",
    "\n",
    "print(f\"\\n{'Comparison':<52} {'Full':>8} {'Trunc':>8} {'Delta':>8} {'t':>8} {'p':>12} {'d':>8}\")\n",
    "print(\"-\" * 110)\n",
    "\n",
    "for label, key_full, key_trunc, question in trunc_comparisons:\n",
    "    a = np.array([r[key_full] for r in results])\n",
    "    b = np.array([r[key_trunc] for r in results])\n",
    "    diff = a - b  # positive = truncated is better\n",
    "    t, p = stats.ttest_rel(a, b)\n",
    "    d = cohens_d(diff)\n",
    "    print(f\"{label:<52} {np.mean(a):>8.4f} {np.mean(b):>8.4f} {np.mean(diff):>+8.4f} {t:>8.3f} {p:>12.6f} {d:>8.4f}\")\n",
    "    print(f\"  Q: {question}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-33",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-34",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 3, figsize=(22, 18))\n",
    "fig.suptitle('Experiment 09: Prefix LM Replication with ChatGLM-6B', fontsize=16, fontweight='bold')\n",
    "\n",
    "# --- (0,0) All suffix conditions bar chart ---\n",
    "ax = axes[0, 0]\n",
    "cond_order = [\n",
    "    ('Bare', 'bare_nll'),\n",
    "    ('Padded', 'padded_nll'),\n",
    "    ('Sfx\\nRouted', 'sfx_gen_routed_nll'),\n",
    "    ('Sfx\\nPerfect', 'sfx_perfect_nll'),\n",
    "    ('Sfx\\nIrrel', 'sfx_irrel_nll'),\n",
    "    ('Sfx\\nShuf', 'sfx_shuffled_nll'),\n",
    "    ('Sfx\\nRndT', 'sfx_rand_tokens_nll'),\n",
    "    ('Sfx\\nSumm', 'sfx_summary_nll'),\n",
    "]\n",
    "means = [np.mean([r[k] for r in results]) for _, k in cond_order]\n",
    "sems = [stats.sem([r[k] for r in results]) for _, k in cond_order]\n",
    "colors = ['#888888', '#aaaaaa', '#4c72b0', '#55a868', '#c44e52', '#dd8452', '#7f7f7f', '#9467bd']\n",
    "ax.bar(range(len(cond_order)), means, yerr=sems, color=colors, capsize=3)\n",
    "ax.set_xticks(range(len(cond_order)))\n",
    "ax.set_xticklabels([l for l, _ in cond_order], fontsize=7)\n",
    "ax.set_ylabel('Mean NLL')\n",
    "ax.set_title('Group A+B: Suffix Conditions')\n",
    "\n",
    "# --- (0,1) Semantic isolation scatter ---\n",
    "ax = axes[0, 1]\n",
    "r_val, _ = stats.pearsonr(sfx_gen_deltas, sfx_shuf_deltas)\n",
    "ax.scatter(sfx_gen_deltas, sfx_shuf_deltas, alpha=0.2, s=8, c='#4c72b0')\n",
    "lims = [min(sfx_gen_deltas.min(), sfx_shuf_deltas.min()),\n",
    "        max(sfx_gen_deltas.max(), sfx_shuf_deltas.max())]\n",
    "ax.plot(lims, lims, 'r--', linewidth=1, alpha=0.5)\n",
    "ax.set_xlabel('Gen routed delta')\n",
    "ax.set_ylabel('Shuffled delta')\n",
    "ax.set_title(f'Semantic Isolation: r={r_val:.3f}\\n(Mistral causal ref: r=0.796)')\n",
    "\n",
    "# --- (0,2) Suffix vs Prefix ---\n",
    "ax = axes[0, 2]\n",
    "sp_labels = ['Sfx\\nRouted', 'Pfx Full\\nRouted', 'Pfx Trunc\\nRouted']\n",
    "sp_keys = ['sfx_gen_routed_nll', 'pfx_full_routed_nll', 'pfx_trunc_routed_nll']\n",
    "sp_means = [np.mean([r[k] for r in results]) for k in sp_keys]\n",
    "sp_sems = [stats.sem([r[k] for r in results]) for k in sp_keys]\n",
    "sp_colors = ['#4c72b0', '#55a868', '#dd8452']\n",
    "ax.bar(range(3), sp_means, yerr=sp_sems, color=sp_colors, capsize=3)\n",
    "ax.axhline(np.mean(bare_arr), color='#888888', linestyle='--', linewidth=1, label='Bare')\n",
    "ax.set_xticks(range(3))\n",
    "ax.set_xticklabels(sp_labels, fontsize=8)\n",
    "ax.set_ylabel('Mean NLL')\n",
    "ax.set_title('Suffix vs Prefix (same surrogate)')\n",
    "ax.legend(fontsize=7)\n",
    "\n",
    "# --- (1,0) Prefix full vs truncated ---\n",
    "ax = axes[1, 0]\n",
    "ft_labels = ['Full\\nRouted', 'Trunc\\nRouted', 'Full\\nPerfect', 'Trunc\\nPerfect', 'Full\\nIrrel', 'Trunc\\nIrrel']\n",
    "ft_keys = ['pfx_full_routed_nll', 'pfx_trunc_routed_nll',\n",
    "           'pfx_full_perfect_nll', 'pfx_trunc_perfect_nll',\n",
    "           'pfx_full_irrel_nll', 'pfx_trunc_irrel_nll']\n",
    "ft_means = [np.mean([r[k] for r in results]) for k in ft_keys]\n",
    "ft_sems = [stats.sem([r[k] for r in results]) for k in ft_keys]\n",
    "ft_colors = ['#55a868', '#dd8452'] * 3\n",
    "ax.bar(range(6), ft_means, yerr=ft_sems, color=ft_colors, capsize=3)\n",
    "ax.axhline(np.mean(bare_arr), color='#888888', linestyle='--', linewidth=1, label='Bare')\n",
    "ax.set_xticks(range(6))\n",
    "ax.set_xticklabels(ft_labels, fontsize=7)\n",
    "ax.set_ylabel('Mean NLL')\n",
    "ax.set_title('Full vs Truncated Prefix')\n",
    "ax.legend(fontsize=7)\n",
    "\n",
    "# --- (1,1) Format sensitivity ---\n",
    "ax = axes[1, 1]\n",
    "fmt_labels = ['Default\\n(newlines)', 'Template\\n(Related Q:)', 'Raw\\n(no sep)']\n",
    "fmt_keys = ['sfx_gen_routed_nll', 'sfx_template_nll', 'sfx_raw_nll']\n",
    "fmt_means = [np.mean([r[k] for r in results]) for k in fmt_keys]\n",
    "fmt_sems = [stats.sem([r[k] for r in results]) for k in fmt_keys]\n",
    "fmt_colors = ['#4c72b0', '#55a868', '#c44e52']\n",
    "ax.bar(range(3), fmt_means, yerr=fmt_sems, color=fmt_colors, capsize=3)\n",
    "ax.axhline(np.mean(bare_arr), color='#888888', linestyle='--', linewidth=1, label='Bare')\n",
    "ax.set_xticks(range(3))\n",
    "ax.set_xticklabels(fmt_labels, fontsize=8)\n",
    "ax.set_ylabel('Mean NLL')\n",
    "ax.set_title('Group E: Format Sensitivity')\n",
    "ax.legend(fontsize=7)\n",
    "\n",
    "# --- (1,2) Win rate by difficulty quartile ---\n",
    "ax = axes[1, 2]\n",
    "quartiles = np.percentile(bare_arr, [25, 50, 75])\n",
    "difficulty_bins = [\n",
    "    ('Q1 (easiest)', lambda x: x <= quartiles[0]),\n",
    "    ('Q2', lambda x: quartiles[0] < x <= quartiles[1]),\n",
    "    ('Q3', lambda x: quartiles[1] < x <= quartiles[2]),\n",
    "    ('Q4 (hardest)', lambda x: x > quartiles[2]),\n",
    "]\n",
    "q_labels = []\n",
    "q_sfx_gen_wr = []\n",
    "q_sfx_irrel_wr = []\n",
    "q_pfx_wr = []\n",
    "for label, cond in difficulty_bins:\n",
    "    subset = [r for r in results if cond(r['bare_nll'])]\n",
    "    if not subset:\n",
    "        continue\n",
    "    q_labels.append(label.replace(' ', '\\n'))\n",
    "    q_sfx_gen_wr.append(np.mean([r['bare_nll'] - r['sfx_gen_routed_nll'] > 0 for r in subset]) * 100)\n",
    "    q_sfx_irrel_wr.append(np.mean([r['bare_nll'] - r['sfx_irrel_nll'] > 0 for r in subset]) * 100)\n",
    "    q_pfx_wr.append(np.mean([r['bare_nll'] - r['pfx_full_routed_nll'] > 0 for r in subset]) * 100)\n",
    "\n",
    "x_pos = np.arange(len(q_labels))\n",
    "w = 0.25\n",
    "ax.bar(x_pos - w, q_sfx_gen_wr, w, label='Sfx Gen', color='#4c72b0')\n",
    "ax.bar(x_pos, q_sfx_irrel_wr, w, label='Sfx Irrel', color='#c44e52')\n",
    "ax.bar(x_pos + w, q_pfx_wr, w, label='Pfx Full', color='#55a868')\n",
    "ax.axhline(50, color='black', linestyle='--', linewidth=0.8, alpha=0.3)\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels(q_labels, fontsize=7)\n",
    "ax.set_ylabel('Win Rate vs Bare (%)')\n",
    "ax.set_title('Win Rate by Difficulty')\n",
    "ax.legend(fontsize=7)\n",
    "ax.set_ylim(0, 100)\n",
    "\n",
    "# --- (2,0) Similarity vs delta scatter ---\n",
    "ax = axes[2, 0]\n",
    "gen_sims = np.array([r['sfx_gen_routed_sim'] for r in results])\n",
    "r_sim, _ = stats.pearsonr(gen_sims, sfx_gen_deltas)\n",
    "ax.scatter(gen_sims, sfx_gen_deltas, alpha=0.2, s=8, c='#4c72b0')\n",
    "z = np.polyfit(gen_sims, sfx_gen_deltas, 1)\n",
    "p_line = np.poly1d(z)\n",
    "x_range = np.linspace(gen_sims.min(), gen_sims.max(), 100)\n",
    "ax.plot(x_range, p_line(x_range), 'r-', linewidth=1.5)\n",
    "ax.axhline(0, color='black', linestyle='--', linewidth=0.8)\n",
    "ax.set_xlabel('Surrogate-Query Cosine Similarity')\n",
    "ax.set_ylabel('Suffix Delta NLL')\n",
    "ax.set_title(f'Similarity vs Suffix Delta (r={r_sim:.3f})')\n",
    "\n",
    "# --- (2,1) Perfect suffix delta distribution ---\n",
    "ax = axes[2, 1]\n",
    "perf_delta = bare_arr - np.array([r['sfx_perfect_nll'] for r in results])\n",
    "gen_delta = bare_arr - np.array([r['sfx_gen_routed_nll'] for r in results])\n",
    "ax.hist(perf_delta, bins=30, alpha=0.6, label='Perfect suffix', color='#55a868')\n",
    "ax.hist(gen_delta, bins=30, alpha=0.6, label='Gen routed suffix', color='#4c72b0')\n",
    "ax.axvline(0, color='black', linestyle='--', linewidth=0.8)\n",
    "ax.set_xlabel('Delta NLL vs Bare')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('Delta Distributions (positive = better)')\n",
    "ax.legend(fontsize=8)\n",
    "\n",
    "# --- (2,2) Cross-condition delta correlation heatmap ---\n",
    "ax = axes[2, 2]\n",
    "delta_keys = [\n",
    "    ('sfx_gen', 'sfx_gen_routed_nll'),\n",
    "    ('sfx_perf', 'sfx_perfect_nll'),\n",
    "    ('sfx_irrel', 'sfx_irrel_nll'),\n",
    "    ('sfx_shuf', 'sfx_shuffled_nll'),\n",
    "    ('sfx_rand', 'sfx_rand_tokens_nll'),\n",
    "    ('pfx_full', 'pfx_full_routed_nll'),\n",
    "    ('pfx_trunc', 'pfx_trunc_routed_nll'),\n",
    "]\n",
    "delta_arrays = {name: bare_arr - np.array([r[key] for r in results]) for name, key in delta_keys}\n",
    "names = list(delta_arrays.keys())\n",
    "corr_matrix = np.zeros((len(names), len(names)))\n",
    "for i, n1 in enumerate(names):\n",
    "    for j, n2 in enumerate(names):\n",
    "        corr_matrix[i, j], _ = stats.pearsonr(delta_arrays[n1], delta_arrays[n2])\n",
    "im = ax.imshow(corr_matrix, cmap='RdBu_r', vmin=-1, vmax=1)\n",
    "ax.set_xticks(range(len(names)))\n",
    "ax.set_xticklabels(names, rotation=45, ha='right', fontsize=7)\n",
    "ax.set_yticks(range(len(names)))\n",
    "ax.set_yticklabels(names, fontsize=7)\n",
    "ax.set_title('Delta Correlation Matrix')\n",
    "plt.colorbar(im, ax=ax)\n",
    "for i in range(len(names)):\n",
    "    for j in range(len(names)):\n",
    "        ax.text(j, i, f\"{corr_matrix[i,j]:.2f}\", ha='center', va='center', fontsize=6)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('09_prefix_lm_results.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"Saved: 09_prefix_lm_results.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-35",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "y0jymkrkkxq",
   "metadata": {},
   "source": [
    "## Cross-Experiment Comparison: ChatGLM vs Mistral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fuzjgfao1z",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 110)\n",
    "print(\"CROSS-EXPERIMENT COMPARISON: ChatGLM (Prefix LM) vs Mistral (Causal)\")\n",
    "print(\"=\" * 110)\n",
    "\n",
    "# Key metrics from Exp 08 (hardcoded references)\n",
    "mistral_refs = {\n",
    "    'sfx_gen_vs_bare_win%': 29.5,\n",
    "    'sfx_gen_vs_irrel_p': 0.23,\n",
    "    'delta_corr_gen_vs_shuf': 0.80,\n",
    "    'query_free_rel_vs_irrel_p': 0.23,\n",
    "}\n",
    "\n",
    "# ChatGLM metrics\n",
    "chatglm_sfx_gen_wr = np.mean(bare_arr > sfx_gen_arr) * 100\n",
    "_, chatglm_gen_vs_irrel_p = stats.ttest_rel(sfx_gen_arr, sfx_irrel_arr)\n",
    "chatglm_delta_corr = stats.pearsonr(sfx_gen_deltas, sfx_shuf_deltas)[0]\n",
    "\n",
    "print(f\"\\n{'Metric':<50} {'Mistral (Causal)':>18} {'ChatGLM (Prefix)':>18}\")\n",
    "print(\"-\" * 90)\n",
    "print(f\"{'sfx_gen_routed win% vs bare':<50} {mistral_refs['sfx_gen_vs_bare_win%']:>17.1f}% {chatglm_sfx_gen_wr:>17.1f}%\")\n",
    "print(f\"{'sfx_gen vs sfx_irrel p-value':<50} {mistral_refs['sfx_gen_vs_irrel_p']:>18.4f} {chatglm_gen_vs_irrel_p:>18.6f}\")\n",
    "print(f\"{'Delta correlation (gen vs shuffled)':<50} {mistral_refs['delta_corr_gen_vs_shuf']:>18.3f} {chatglm_delta_corr:>18.4f}\")\n",
    "print(f\"{'Query-free rel vs irrel p-value':<50} {mistral_refs['query_free_rel_vs_irrel_p']:>18.4f} {p_noq:>18.6f}\")\n",
    "\n",
    "print(f\"\\n--- Interpretation ---\")\n",
    "if chatglm_delta_corr < mistral_refs['delta_corr_gen_vs_shuf'] - 0.2:\n",
    "    print(f\"Delta correlation dropped: {mistral_refs['delta_corr_gen_vs_shuf']:.3f} -> {chatglm_delta_corr:.3f}\")\n",
    "    print(\"Bidirectional attention introduces content-dependent effects.\")\n",
    "else:\n",
    "    print(f\"Delta correlation similar: {mistral_refs['delta_corr_gen_vs_shuf']:.3f} -> {chatglm_delta_corr:.3f}\")\n",
    "    print(\"Content-independence persists even with bidirectional attention.\")\n",
    "\n",
    "if chatglm_sfx_gen_wr > 55:\n",
    "    print(f\"Suffix priming win rate improved: {mistral_refs['sfx_gen_vs_bare_win%']:.1f}% -> {chatglm_sfx_gen_wr:.1f}%\")\n",
    "else:\n",
    "    print(f\"Suffix priming win rate: {mistral_refs['sfx_gen_vs_bare_win%']:.1f}% -> {chatglm_sfx_gen_wr:.1f}% (no improvement)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-36",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"AUTOMATED VERDICTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "alpha = 0.01\n",
    "n_key_tests = 6\n",
    "bonferroni_alpha = alpha / n_key_tests\n",
    "verdicts = {}\n",
    "\n",
    "# H1: Does suffix priming help with prefix LM?\n",
    "sfx_gen_arr = np.array([r['sfx_gen_routed_nll'] for r in results])\n",
    "t, p = stats.ttest_rel(bare_arr, sfx_gen_arr)\n",
    "d = cohens_d(bare_arr - sfx_gen_arr)\n",
    "verdicts['H1: Suffix priming helps (prefix LM)'] = {\n",
    "    'test': 'sfx_gen_routed (3) vs bare (1)',\n",
    "    'delta': float(np.mean(bare_arr - sfx_gen_arr)),\n",
    "    'd': float(d), 't': float(t), 'p': float(p),\n",
    "    'verdict': 'SUPPORTED' if p < bonferroni_alpha and d > 0.3 else 'NOT SUPPORTED'\n",
    "}\n",
    "\n",
    "# H2: Semantic content matters (gen vs irrel)\n",
    "sfx_irrel_arr = np.array([r['sfx_irrel_nll'] for r in results])\n",
    "t, p = stats.ttest_rel(sfx_gen_arr, sfx_irrel_arr)\n",
    "verdicts['H2: Suffix content matters (semantic)'] = {\n",
    "    'test': 'sfx_gen_routed (3) vs sfx_irrel (5)',\n",
    "    'delta': float(np.mean(sfx_irrel_arr - sfx_gen_arr)),\n",
    "    't': float(t), 'p': float(p),\n",
    "    'verdict': 'SUPPORTED' if p < bonferroni_alpha and np.mean(sfx_gen_arr) < np.mean(sfx_irrel_arr) else 'NOT SUPPORTED'\n",
    "}\n",
    "\n",
    "# H3: Low delta correlation (content-dependent)\n",
    "r_corr, p_corr = stats.pearsonr(sfx_gen_deltas, sfx_shuf_deltas)\n",
    "verdicts['H3: Content-dependent effect (r < 0.5)'] = {\n",
    "    'test': 'Delta correlation: sfx_gen_routed vs sfx_shuffled',\n",
    "    'r': float(r_corr), 'p': float(p_corr),\n",
    "    'causal_mistral_ref': 0.796,\n",
    "    'verdict': 'SUPPORTED' if r_corr < 0.5 else ('PARTIAL' if r_corr < 0.8 else 'NOT SUPPORTED')\n",
    "}\n",
    "\n",
    "# H4: Truncation with 2D RoPE correction preserves benefit\n",
    "pfx_full_arr = np.array([r['pfx_full_routed_nll'] for r in results])\n",
    "pfx_trunc_arr = np.array([r['pfx_trunc_routed_nll'] for r in results])\n",
    "t, p = stats.ttest_rel(pfx_full_arr, pfx_trunc_arr)\n",
    "verdicts['H4: Truncation preserves benefit'] = {\n",
    "    'test': 'pfx_full_routed (9) vs pfx_trunc_routed (12)',\n",
    "    'full_mean': float(np.mean(pfx_full_arr)),\n",
    "    'trunc_mean': float(np.mean(pfx_trunc_arr)),\n",
    "    't': float(t), 'p': float(p),\n",
    "    'verdict': 'SUPPORTED' if p > 0.05 or abs(np.mean(pfx_full_arr - pfx_trunc_arr)) < 0.1 else 'NOT SUPPORTED'\n",
    "}\n",
    "\n",
    "# H5: Suffix placement matches prefix placement\n",
    "t, p = stats.ttest_rel(sfx_gen_arr, pfx_full_arr)\n",
    "verdicts['H5: Suffix == Prefix (bidirectional)'] = {\n",
    "    'test': 'sfx_gen_routed (3) vs pfx_full_routed (9)',\n",
    "    'sfx_mean': float(np.mean(sfx_gen_arr)),\n",
    "    'pfx_mean': float(np.mean(pfx_full_arr)),\n",
    "    't': float(t), 'p': float(p),\n",
    "    'verdict': 'SUPPORTED' if p > 0.05 else 'NOT SUPPORTED'\n",
    "}\n",
    "\n",
    "# H6: Query-free semantic differentiation\n",
    "sfx_gen_noq_arr = np.array([r['sfx_gen_routed_noq_nll'] for r in results])\n",
    "sfx_irrel_noq_arr = np.array([r['sfx_irrel_noq_nll'] for r in results])\n",
    "t_noq_h, p_noq_h = stats.ttest_rel(sfx_gen_noq_arr, sfx_irrel_noq_arr)\n",
    "d_noq_h = cohens_d(sfx_irrel_noq_arr - sfx_gen_noq_arr)\n",
    "verdicts['H6: Query-free semantic signal'] = {\n",
    "    'test': 'sfx_gen_routed_noq vs sfx_irrel_noq',\n",
    "    'delta': float(np.mean(sfx_irrel_noq_arr - sfx_gen_noq_arr)),\n",
    "    'd': float(d_noq_h), 't': float(t_noq_h), 'p': float(p_noq_h),\n",
    "    'mistral_ref_p': 0.23,\n",
    "    'verdict': 'SUPPORTED' if p_noq_h < bonferroni_alpha and np.mean(sfx_gen_noq_arr) < np.mean(sfx_irrel_noq_arr) else 'NOT SUPPORTED'\n",
    "}\n",
    "\n",
    "for hyp, v in verdicts.items():\n",
    "    sig_marker = \"***\" if v.get('p', 1) < 0.01 else \"   \"\n",
    "    print(f\"\\n{sig_marker} {hyp}: {v['verdict']}\")\n",
    "    print(f\"    {v['test']}\")\n",
    "    if 'r' in v:\n",
    "        print(f\"    r={v['r']:.4f} (causal ref: {v['causal_mistral_ref']})\")\n",
    "    elif 'd' in v:\n",
    "        print(f\"    d={v['d']:.4f}, t={v['t']:.3f}, p={v['p']:.6f}\")\n",
    "    else:\n",
    "        print(f\"    t={v['t']:.3f}, p={v['p']:.6f}\")\n",
    "\n",
    "print(f\"\\n{'=' * 80}\")\n",
    "print(\"OVERALL ASSESSMENT\")\n",
    "print(\"=\" * 80)\n",
    "n_supported = sum(1 for v in verdicts.values() if v['verdict'] == 'SUPPORTED')\n",
    "print(f\"Hypotheses supported: {n_supported}/{len(verdicts)}\")\n",
    "\n",
    "h1_ok = verdicts['H1: Suffix priming helps (prefix LM)']['verdict'] == 'SUPPORTED'\n",
    "h2_ok = verdicts['H2: Suffix content matters (semantic)']['verdict'] == 'SUPPORTED'\n",
    "h3_ok = verdicts['H3: Content-dependent effect (r < 0.5)']['verdict'] in ('SUPPORTED', 'PARTIAL')\n",
    "h6_ok = verdicts['H6: Query-free semantic signal']['verdict'] == 'SUPPORTED'\n",
    "\n",
    "if h1_ok and h2_ok and h3_ok:\n",
    "    print(\"STRONG SUCCESS: Prefix LM enables genuine semantic priming!\")\n",
    "    print(\"The core hypothesis is validated: bidirectional attention is essential.\")\n",
    "elif h1_ok and (h2_ok or h3_ok):\n",
    "    print(\"PARTIAL SUCCESS: Prefix LM helps, some semantic signal detected.\")\n",
    "elif h6_ok:\n",
    "    print(\"QUERY-FREE SUCCESS: Semantic signal emerges only without the query.\")\n",
    "    print(\"The query masks the suffix effect even with bidirectional attention.\")\n",
    "elif h1_ok:\n",
    "    print(\"WEAK: Priming helps but still content-independent (like causal).\")\n",
    "else:\n",
    "    print(\"NEGATIVE: Even prefix LM doesn't enable effective surrogate priming.\")\n",
    "    print(\"The approach may be fundamentally limited regardless of architecture.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-37",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-38",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.makedirs('results', exist_ok=True)\n",
    "timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "output = {\n",
    "    'metadata': {\n",
    "        'experiment': '09_prefix_lm_experiment',\n",
    "        'description': (\n",
    "            'Prefix LM replication with ChatGLM-6B: 20 conditions testing '\n",
    "            'suffix priming, prefix priming (full and truncated with 2D RoPE correction), '\n",
    "            'format sensitivity, and query-free scoring. Key test: does bidirectional attention enable '\n",
    "            'genuine semantic priming (unlike causal Mistral r=0.796)?'\n",
    "        ),\n",
    "        'timestamp': datetime.datetime.now().isoformat(),\n",
    "        'model_name': config.model_name,\n",
    "        'model_type': config.model_type,\n",
    "        'num_samples_evaluated': len(results),\n",
    "        'num_errors': errors,\n",
    "        'elapsed_seconds': elapsed_total,\n",
    "        'seed': config.seed,\n",
    "    },\n",
    "    'verdicts': verdicts,\n",
    "    'results': results,\n",
    "}\n",
    "\n",
    "output_path_ts = f'results/09_prefix_lm_results_{timestamp}.json'\n",
    "with open(output_path_ts, 'w') as f:\n",
    "    json.dump(output, f, indent=2, default=str)\n",
    "print(f\"Timestamped results: {output_path_ts}\")\n",
    "\n",
    "output_path = '09_prefix_lm_results.json'\n",
    "with open(output_path, 'w') as f:\n",
    "    json.dump(output, f, indent=2, default=str)\n",
    "print(f\"Canonical results:   {output_path}\")\n",
    "print(f\"File size: {os.path.getsize(output_path) / 1e6:.1f} MB\")"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cu124.m127",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/base-cu124:m127"
  },
  "kernelspec": {
   "display_name": "Python 3 (Local)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
