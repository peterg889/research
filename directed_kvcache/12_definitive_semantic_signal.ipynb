{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "259ee2b6",
   "metadata": {},
   "source": [
    "# Experiment 12: Definitive Semantic Signal Confirmation\n",
    "\n",
    "## Motivation\n",
    "Exp 11 found a suggestive quality gradient: higher-similarity real-query surrogates produced larger NLL improvements (very_low: d=0.157, high: d=0.511). But the evidence was not ironclad:\n",
    "- Individual-level Pearson r = 0.054 (p=0.12) — CI may include 0\n",
    "- High-similarity bin had only N=65 samples\n",
    "- Single dataset (MS MARCO), single prefix template, no confound controls\n",
    "\n",
    "## Four Investigations\n",
    "- **A**: Scaled quality gradient with controls (N=2500, MS MARCO) — 11 conditions including shuffled, length-matched, and raw-query controls\n",
    "- **B**: Cross-dataset replication (N=1000 each, SQuAD v2 + TriviaQA) — 5 conditions\n",
    "- **C**: Prefix format ablation (N=400, MS MARCO) — 7 format conditions\n",
    "- **D**: Ranking with bootstrap CIs (N=1000, MS MARCO) — MRR/Hit@1/Hit@3\n",
    "\n",
    "## Pre-Registered Verdict Criteria\n",
    "- **CONFIRMED** if: (a) Pearson r bootstrap 95% CI excludes 0 on MS MARCO (N=2500), AND (b) bin-level Spearman rho > 0.7, AND (c) sim_0.60 significantly beats shuffled_0.60 (p<0.05)\n",
    "- **PARTIALLY CONFIRMED** if: (a) or (b) holds but not both, or signal on only 1 of 3 datasets\n",
    "- **REFUTED** if: Pearson r CI includes 0, no monotonic trend, shuffled matches real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df0ee3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, json, copy, time, datetime, random, re\n",
    "from typing import Dict, List, Any, Optional, Tuple\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.dpi'] = 120\n",
    "\n",
    "sys.path.insert(0, '.')\n",
    "\n",
    "from lib import (\n",
    "    ExperimentConfig,\n",
    "    build_kv_cache,\n",
    "    score_answer_with_cache,\n",
    "    extract_and_truncate_cache_with_bos,\n",
    "    correct_rope_positions_with_bos,\n",
    "    load_evaluation_samples,\n",
    "    load_ms_marco,\n",
    "    _ensure_dynamic_cache,\n",
    ")\n",
    "from lib.kv_cache import _get_cache_keys, _get_cache_values\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, DynamicCache\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from datasets import load_dataset\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1fe933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Configuration\n",
    "# ============================================================\n",
    "\n",
    "config = ExperimentConfig(\n",
    "    num_samples=8000,  # Load extra for large query pool\n",
    "    min_passage_words=50,\n",
    "    max_passage_words=300,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "N_INV_A = 2500\n",
    "N_INV_B = 1000  # per dataset\n",
    "N_INV_C = 400\n",
    "N_INV_D = 1000\n",
    "\n",
    "# Similarity bins for Investigation A (finer than Exp 11)\n",
    "SIM_BINS_A = [\n",
    "    (0.05, 0.15, 'sim_0.10'),\n",
    "    (0.25, 0.35, 'sim_0.30'),\n",
    "    (0.40, 0.50, 'sim_0.45'),\n",
    "    (0.55, 0.65, 'sim_0.60'),\n",
    "    (0.70, 0.80, 'sim_0.75'),\n",
    "    (0.80, 0.90, 'sim_0.85'),\n",
    "]\n",
    "\n",
    "# Similarity bins for Investigation B (reduced)\n",
    "SIM_BINS_B = [\n",
    "    (0.25, 0.35, 'sim_0.30'),\n",
    "    (0.55, 0.65, 'sim_0.60'),\n",
    "    (0.75, 0.85, 'sim_0.80'),\n",
    "]\n",
    "\n",
    "# Prefix formats for Investigation C\n",
    "PREFIX_FORMATS = {\n",
    "    'template': 'This document answers: {query}',\n",
    "    'raw': '{query}',\n",
    "    'question': 'Question: {query}\\n\\nPassage:',\n",
    "    'instruction': 'Find information about: {query}\\n\\n',\n",
    "    'shuffled_template': 'This document answers: {shuffled}',\n",
    "}\n",
    "\n",
    "SEEDS = {'msmarco': 42, 'squad': 43, 'triviaqa': 44}\n",
    "\n",
    "torch.manual_seed(config.seed)\n",
    "np.random.seed(config.seed)\n",
    "\n",
    "print(f\"Investigation A: {N_INV_A} samples x 11 conditions\")\n",
    "print(f\"Investigation B: {N_INV_B} samples x 2 datasets x 5 conditions\")\n",
    "print(f\"Investigation C: {N_INV_C} samples x 7 conditions\")\n",
    "print(f\"Investigation D: {N_INV_D} samples x 5 caches x 5 queries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b72647",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Load Model\n",
    "# ============================================================\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    ")\n",
    "\n",
    "print(f\"Loading {config.model_name}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "model.eval()\n",
    "print(f\"Model loaded. Layers: {model.config.num_hidden_layers}\")\n",
    "\n",
    "print(\"Loading embedding model...\")\n",
    "embed_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "print(\"Embedding model loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e13dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Load MS MARCO, Build Query Pool, Embed\n",
    "# ============================================================\n",
    "\n",
    "print(\"Loading MS MARCO dataset...\")\n",
    "dataset = load_dataset(\n",
    "    config.dataset_name,\n",
    "    config.dataset_config,\n",
    "    split=config.dataset_split,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "print(f\"Dataset loaded: {len(dataset)} samples\")\n",
    "\n",
    "all_samples = load_evaluation_samples(dataset, config, require_answer=True)\n",
    "print(f\"Loaded {len(all_samples)} evaluation samples with answers\")\n",
    "\n",
    "# Build query pool from FULL dataset\n",
    "print(\"\\nBuilding query pool (full dataset)...\")\n",
    "query_pool = []\n",
    "seen_queries = set()\n",
    "for item in tqdm(dataset, desc=\"Extracting queries\"):\n",
    "    q = item.get('query', '').strip()\n",
    "    if q and q not in seen_queries and len(q) > 10:\n",
    "        query_pool.append(q)\n",
    "        seen_queries.add(q)\n",
    "print(f\"Query pool size: {len(query_pool)}\")\n",
    "\n",
    "print(\"Embedding query pool...\")\n",
    "pool_embeddings = embed_model.encode(query_pool, show_progress_bar=True, batch_size=256)\n",
    "print(f\"Pool embeddings shape: {pool_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f30b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Load SQuAD v2 + TriviaQA, Build Per-Dataset Query Pools\n",
    "# ============================================================\n",
    "\n",
    "# --- SQuAD v2 ---\n",
    "print(\"Loading SQuAD v2...\")\n",
    "squad_dataset = load_dataset(\"rajpurkar/squad_v2\", split=\"validation\")\n",
    "print(f\"SQuAD v2 loaded: {len(squad_dataset)} samples\")\n",
    "\n",
    "squad_train = load_dataset(\"rajpurkar/squad_v2\", split=\"train\")\n",
    "print(f\"SQuAD v2 train (for query pool): {len(squad_train)} samples\")\n",
    "\n",
    "np.random.seed(SEEDS['squad'])\n",
    "squad_samples = []\n",
    "for item in squad_dataset:\n",
    "    ctx = item.get('context', '').strip()\n",
    "    q = item.get('question', '').strip()\n",
    "    answers = item.get('answers', {})\n",
    "    ans_texts = answers.get('text', [])\n",
    "    if not ctx or not q or not ans_texts or len(ans_texts) == 0:\n",
    "        continue\n",
    "    wc = len(ctx.split())\n",
    "    if 50 <= wc <= 300:\n",
    "        squad_samples.append({\n",
    "            'passage': ctx,\n",
    "            'query': q,\n",
    "            'answer': ans_texts[0],\n",
    "        })\n",
    "np.random.shuffle(squad_samples)\n",
    "squad_samples = squad_samples[:N_INV_B]\n",
    "print(f\"SQuAD evaluation samples: {len(squad_samples)}\")\n",
    "\n",
    "# SQuAD query pool from train split\n",
    "squad_query_pool = []\n",
    "seen_sq = set()\n",
    "for item in squad_train:\n",
    "    q = item.get('question', '').strip()\n",
    "    if q and q not in seen_sq and len(q) > 10:\n",
    "        squad_query_pool.append(q)\n",
    "        seen_sq.add(q)\n",
    "print(f\"SQuAD query pool: {len(squad_query_pool)}\")\n",
    "print(\"Embedding SQuAD query pool...\")\n",
    "squad_pool_embs = embed_model.encode(squad_query_pool, show_progress_bar=True, batch_size=256)\n",
    "\n",
    "# --- TriviaQA ---\n",
    "print(\"\\nLoading TriviaQA (rc.wikipedia)...\")\n",
    "tqa_dataset = load_dataset(\"trivia_qa\", \"rc.wikipedia\", split=\"validation\")\n",
    "print(f\"TriviaQA loaded: {len(tqa_dataset)} samples\")\n",
    "\n",
    "tqa_train = load_dataset(\"trivia_qa\", \"rc.wikipedia\", split=\"train\")\n",
    "print(f\"TriviaQA train (for query pool): {len(tqa_train)} samples\")\n",
    "\n",
    "np.random.seed(SEEDS['triviaqa'])\n",
    "tqa_samples = []\n",
    "for item in tqa_dataset:\n",
    "    q = item.get('question', '').strip()\n",
    "    ans = item.get('answer', {})\n",
    "    aliases = ans.get('aliases', [])\n",
    "    if not q or not aliases:\n",
    "        continue\n",
    "    # Get first Wikipedia context\n",
    "    entity_pages = item.get('entity_pages', {})\n",
    "    wiki_contexts = entity_pages.get('wiki_context', [])\n",
    "    if not wiki_contexts:\n",
    "        continue\n",
    "    ctx = wiki_contexts[0].strip()\n",
    "    # Truncate long contexts to ~300 words\n",
    "    words = ctx.split()\n",
    "    if len(words) < 50:\n",
    "        continue\n",
    "    if len(words) > 300:\n",
    "        ctx = ' '.join(words[:300])\n",
    "    tqa_samples.append({\n",
    "        'passage': ctx,\n",
    "        'query': q,\n",
    "        'answer': aliases[0],\n",
    "    })\n",
    "\n",
    "np.random.shuffle(tqa_samples)\n",
    "tqa_samples = tqa_samples[:N_INV_B]\n",
    "print(f\"TriviaQA evaluation samples: {len(tqa_samples)}\")\n",
    "\n",
    "# TriviaQA query pool from train split\n",
    "tqa_query_pool = []\n",
    "seen_tq = set()\n",
    "for item in tqa_train:\n",
    "    q = item.get('question', '').strip()\n",
    "    if q and q not in seen_tq and len(q) > 10:\n",
    "        tqa_query_pool.append(q)\n",
    "        seen_tq.add(q)\n",
    "print(f\"TriviaQA query pool: {len(tqa_query_pool)}\")\n",
    "print(\"Embedding TriviaQA query pool...\")\n",
    "tqa_pool_embs = embed_model.encode(tqa_query_pool, show_progress_bar=True, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2928255d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Helper Functions\n",
    "# ============================================================\n",
    "\n",
    "def build_matched_bare_and_truncated(\n",
    "    prefix_text: str,\n",
    "    passage: str,\n",
    "    model,\n",
    "    tokenizer,\n",
    "    config,\n",
    ") -> Tuple[int, DynamicCache, int, DynamicCache, int]:\n",
    "    \"\"\"Build BPE-matched bare and truncated caches.\n",
    "    \n",
    "    Returns: (bare_len, bare_cache, trunc_len, trunc_cache, prefix_token_len)\n",
    "    \"\"\"\n",
    "    prefix_with_sep = prefix_text.strip() + \" \"\n",
    "    \n",
    "    prefix_encoding = tokenizer(\n",
    "        prefix_with_sep, return_tensors=\"pt\", add_special_tokens=True,\n",
    "        padding=False, truncation=False\n",
    "    )\n",
    "    prefix_len = prefix_encoding['input_ids'].shape[1]\n",
    "    \n",
    "    full_context = prefix_with_sep + passage\n",
    "    full_encoding = tokenizer(\n",
    "        full_context, return_tensors=\"pt\", add_special_tokens=True,\n",
    "        padding=False, truncation=False\n",
    "    )\n",
    "    full_ids = full_encoding['input_ids'].to(config.device)\n",
    "    doc_len = full_ids.shape[1] - prefix_len\n",
    "    \n",
    "    doc_token_ids = full_ids[:, prefix_len:]\n",
    "    bos_id = full_ids[:, :1]\n",
    "    bare_ids = torch.cat([bos_id, doc_token_ids], dim=1)\n",
    "    bare_len = bare_ids.shape[1]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        bare_out = model(\n",
    "            input_ids=bare_ids,\n",
    "            attention_mask=torch.ones_like(bare_ids),\n",
    "            use_cache=True,\n",
    "            return_dict=True\n",
    "        )\n",
    "    bare_cache = _ensure_dynamic_cache(bare_out.past_key_values)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        full_out = model(\n",
    "            input_ids=full_ids,\n",
    "            attention_mask=torch.ones_like(full_ids),\n",
    "            use_cache=True,\n",
    "            return_dict=True\n",
    "        )\n",
    "    \n",
    "    truncated = extract_and_truncate_cache_with_bos(\n",
    "        full_out.past_key_values, doc_len\n",
    "    )\n",
    "    keep_len = 1 + doc_len\n",
    "    \n",
    "    assert bare_len == keep_len, f\"Length mismatch: {bare_len} vs {keep_len}\"\n",
    "    \n",
    "    surrogate_offset = prefix_len - 1\n",
    "    correct_rope_positions_with_bos(truncated, surrogate_offset, model)\n",
    "    \n",
    "    return bare_len, bare_cache, keep_len, truncated, prefix_len\n",
    "\n",
    "\n",
    "def find_surrogate_at_similarity(\n",
    "    target_query: str,\n",
    "    target_embedding: np.ndarray,\n",
    "    sim_low: float,\n",
    "    sim_high: float,\n",
    "    pool_queries: list,\n",
    "    pool_embs: np.ndarray,\n",
    "    rng: np.random.RandomState,\n",
    ") -> Optional[Tuple[str, float]]:\n",
    "    \"\"\"Find a real query from the pool within the specified similarity range.\"\"\"\n",
    "    sims = cosine_similarity([target_embedding], pool_embs)[0]\n",
    "    mask = (sims >= sim_low) & (sims < sim_high)\n",
    "    for idx in np.where(mask)[0]:\n",
    "        if pool_queries[idx].strip().lower() == target_query.strip().lower():\n",
    "            mask[idx] = False\n",
    "    \n",
    "    candidates = np.where(mask)[0]\n",
    "    if len(candidates) == 0:\n",
    "        return None\n",
    "    \n",
    "    mid = (sim_low + sim_high) / 2\n",
    "    distances_to_mid = np.abs(sims[candidates] - mid)\n",
    "    n_pick = max(1, len(candidates) // 10)\n",
    "    best_idxs = candidates[np.argsort(distances_to_mid)[:n_pick]]\n",
    "    chosen = rng.choice(best_idxs)\n",
    "    return pool_queries[chosen], float(sims[chosen])\n",
    "\n",
    "\n",
    "def shuffle_query_words(query: str, rng: np.random.RandomState) -> str:\n",
    "    \"\"\"Shuffle the words of a query, preserving word set but destroying order.\"\"\"\n",
    "    words = query.split()\n",
    "    rng.shuffle(words)\n",
    "    return ' '.join(words)\n",
    "\n",
    "\n",
    "def find_length_matched_random(\n",
    "    target_token_len: int,\n",
    "    target_query: str,\n",
    "    target_embedding: np.ndarray,\n",
    "    pool_queries: list,\n",
    "    pool_embs: np.ndarray,\n",
    "    tokenizer,\n",
    "    rng: np.random.RandomState,\n",
    "    max_sim: float = 0.15,\n",
    ") -> Optional[Tuple[str, float]]:\n",
    "    \"\"\"Find a random query with similar token count but low semantic similarity.\"\"\"\n",
    "    sims = cosine_similarity([target_embedding], pool_embs)[0]\n",
    "    low_sim_mask = sims < max_sim\n",
    "    \n",
    "    candidates = np.where(low_sim_mask)[0]\n",
    "    if len(candidates) == 0:\n",
    "        return None\n",
    "    \n",
    "    # Filter by token length (within ±2 tokens)\n",
    "    good = []\n",
    "    # Sample up to 500 candidates to check token length\n",
    "    check_idxs = rng.choice(candidates, size=min(500, len(candidates)), replace=False)\n",
    "    for ci in check_idxs:\n",
    "        q = pool_queries[ci]\n",
    "        tlen = len(tokenizer.encode(q, add_special_tokens=False))\n",
    "        if abs(tlen - target_token_len) <= 2:\n",
    "            good.append((ci, float(sims[ci])))\n",
    "    \n",
    "    if not good:\n",
    "        # Relax to ±5 tokens\n",
    "        for ci in check_idxs:\n",
    "            q = pool_queries[ci]\n",
    "            tlen = len(tokenizer.encode(q, add_special_tokens=False))\n",
    "            if abs(tlen - target_token_len) <= 5:\n",
    "                good.append((ci, float(sims[ci])))\n",
    "    \n",
    "    if not good:\n",
    "        return None\n",
    "    \n",
    "    chosen_idx, chosen_sim = good[rng.randint(0, len(good))]\n",
    "    return pool_queries[chosen_idx], chosen_sim\n",
    "\n",
    "\n",
    "def bootstrap_ci(data, stat_fn=np.mean, n_boot=10000, ci=0.95, seed=42):\n",
    "    \"\"\"Compute bootstrap confidence interval.\"\"\"\n",
    "    rng = np.random.RandomState(seed)\n",
    "    n = len(data)\n",
    "    boot_stats = np.empty(n_boot)\n",
    "    for i in range(n_boot):\n",
    "        sample = rng.choice(data, size=n, replace=True)\n",
    "        boot_stats[i] = stat_fn(sample)\n",
    "    alpha = (1 - ci) / 2\n",
    "    lo = np.percentile(boot_stats, 100 * alpha)\n",
    "    hi = np.percentile(boot_stats, 100 * (1 - alpha))\n",
    "    return float(lo), float(hi), boot_stats\n",
    "\n",
    "\n",
    "def bootstrap_corr_ci(x, y, n_boot=10000, ci=0.95, seed=42):\n",
    "    \"\"\"Bootstrap CI for Pearson correlation.\"\"\"\n",
    "    rng = np.random.RandomState(seed)\n",
    "    n = len(x)\n",
    "    boot_rs = np.empty(n_boot)\n",
    "    for i in range(n_boot):\n",
    "        idx = rng.choice(n, size=n, replace=True)\n",
    "        r, _ = stats.pearsonr(x[idx], y[idx])\n",
    "        boot_rs[i] = r\n",
    "    alpha = (1 - ci) / 2\n",
    "    lo = np.percentile(boot_rs, 100 * alpha)\n",
    "    hi = np.percentile(boot_rs, 100 * (1 - alpha))\n",
    "    return float(lo), float(hi), boot_rs\n",
    "\n",
    "\n",
    "def score_with_deepcopy(cache, cache_len, query_prompt, answer, model, tokenizer, config):\n",
    "    \"\"\"Score with a deep-copied cache to prevent mutation.\"\"\"\n",
    "    cache_copy = copy.deepcopy(cache)\n",
    "    return score_answer_with_cache(cache_copy, cache_len, query_prompt, answer, model, tokenizer, config)\n",
    "\n",
    "\n",
    "print(\"Helper functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948a77d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Investigation A: Surrogate Pre-Selection (N=2500)\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"INVESTIGATION A: SURROGATE PRE-SELECTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "samples_a = all_samples[:N_INV_A]\n",
    "print(f\"Using {len(samples_a)} samples for Investigation A\")\n",
    "\n",
    "# Embed target queries\n",
    "print(\"Embedding target queries...\")\n",
    "target_queries_a = [s['query'] for s in samples_a]\n",
    "target_embeddings_a = embed_model.encode(target_queries_a, show_progress_bar=True, batch_size=256)\n",
    "\n",
    "rng_a = np.random.RandomState(SEEDS['msmarco'])\n",
    "\n",
    "# For each sample, find surrogates at each similarity level\n",
    "# Plus: shuffled version of sim_0.60, length-matched random, raw query for sim_0.60\n",
    "sample_surrogates_a = []\n",
    "skipped_bins_a = {b[2]: 0 for b in SIM_BINS_A}\n",
    "skipped_controls = {'shuffled': 0, 'length_matched': 0}\n",
    "\n",
    "for i in tqdm(range(len(samples_a)), desc=\"Selecting surrogates\"):\n",
    "    surr = {}\n",
    "    \n",
    "    # Standard similarity bins\n",
    "    for sim_low, sim_high, bin_name in SIM_BINS_A:\n",
    "        result = find_surrogate_at_similarity(\n",
    "            target_queries_a[i], target_embeddings_a[i],\n",
    "            sim_low, sim_high,\n",
    "            query_pool, pool_embeddings, rng_a\n",
    "        )\n",
    "        if result is not None:\n",
    "            surr[bin_name] = result\n",
    "        else:\n",
    "            skipped_bins_a[bin_name] += 1\n",
    "    \n",
    "    # Controls based on sim_0.60 surrogate\n",
    "    if 'sim_0.60' in surr:\n",
    "        surr_query_060 = surr['sim_0.60'][0]\n",
    "        surr_sim_060 = surr['sim_0.60'][1]\n",
    "        \n",
    "        # Shuffled control\n",
    "        shuffled = shuffle_query_words(surr_query_060, rng_a)\n",
    "        surr['shuffled_0.60'] = (shuffled, surr_sim_060)\n",
    "        \n",
    "        # Length-matched random control\n",
    "        tlen = len(tokenizer.encode(surr_query_060, add_special_tokens=False))\n",
    "        lm_result = find_length_matched_random(\n",
    "            tlen, target_queries_a[i], target_embeddings_a[i],\n",
    "            query_pool, pool_embeddings, tokenizer, rng_a\n",
    "        )\n",
    "        if lm_result is not None:\n",
    "            surr['length_matched_random'] = lm_result\n",
    "        else:\n",
    "            skipped_controls['length_matched'] += 1\n",
    "        \n",
    "        # Raw query (no template) - store the query, we'll use it differently\n",
    "        surr['raw_query_0.60'] = (surr_query_060, surr_sim_060)\n",
    "    else:\n",
    "        skipped_controls['shuffled'] += 1\n",
    "        skipped_controls['length_matched'] += 1\n",
    "    \n",
    "    sample_surrogates_a.append(surr)\n",
    "\n",
    "print(\"\\nSurrogate coverage per bin:\")\n",
    "for sim_low, sim_high, bin_name in SIM_BINS_A:\n",
    "    n_found = N_INV_A - skipped_bins_a[bin_name]\n",
    "    print(f\"  {bin_name} ({sim_low:.2f}-{sim_high:.2f}): {n_found}/{N_INV_A} ({100*n_found/N_INV_A:.1f}%)\")\n",
    "print(f\"  shuffled_0.60: {N_INV_A - skipped_controls['shuffled']}/{N_INV_A}\")\n",
    "print(f\"  length_matched_random: {N_INV_A - skipped_controls['length_matched']}/{N_INV_A}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c57670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Investigation A: Evaluation Loop with Checkpointing\n",
    "# ============================================================\n",
    "\n",
    "results_a = []\n",
    "skipped_a = 0\n",
    "errors_a = 0\n",
    "start_a = time.time()\n",
    "\n",
    "CHECKPOINT_PATH_A = 'results/exp12/12_checkpoint_a.json'\n",
    "\n",
    "# Delete stale checkpoint\n",
    "if os.path.exists(CHECKPOINT_PATH_A):\n",
    "    # Check if it's from a previous run (stale)\n",
    "    with open(CHECKPOINT_PATH_A) as f:\n",
    "        ckpt = json.load(f)\n",
    "    if ckpt.get('experiment') != '12_inv_a':\n",
    "        os.remove(CHECKPOINT_PATH_A)\n",
    "        print(\"Deleted stale checkpoint\")\n",
    "    else:\n",
    "        results_a = ckpt['results']\n",
    "        skipped_a = ckpt['skipped']\n",
    "        errors_a = ckpt['errors']\n",
    "        print(f\"Resumed from checkpoint: {len(results_a)} results\")\n",
    "\n",
    "start_idx_a = len(results_a) + skipped_a + errors_a\n",
    "\n",
    "for idx in tqdm(range(start_idx_a, len(samples_a)), desc=\"Inv A\",\n",
    "                initial=start_idx_a, total=len(samples_a)):\n",
    "    sample = samples_a[idx]\n",
    "    passage = sample['passage']\n",
    "    query = sample['query']\n",
    "    answer = sample['answer']\n",
    "    \n",
    "    answer_ids = tokenizer(answer, return_tensors='pt', add_special_tokens=False)['input_ids']\n",
    "    if answer_ids.shape[1] < 2:\n",
    "        skipped_a += 1\n",
    "        continue\n",
    "    \n",
    "    query_prompt = config.query_template.format(query=query)\n",
    "    surrogates = sample_surrogates_a[idx]\n",
    "    \n",
    "    try:\n",
    "        result = {'idx': idx, 'query': query}\n",
    "        \n",
    "        # --- Oracle (also provides BPE-matched bare baseline) ---\n",
    "        oracle_prefix = f\"This document answers: {query}\"\n",
    "        bare_len, bare_cache, oracle_len, oracle_cache, oracle_ptl = \\\n",
    "            build_matched_bare_and_truncated(oracle_prefix, passage, model, tokenizer, config)\n",
    "        \n",
    "        nll_bare = score_answer_with_cache(\n",
    "            bare_cache, bare_len, query_prompt, answer, model, tokenizer, config\n",
    "        )\n",
    "        nll_oracle = score_answer_with_cache(\n",
    "            oracle_cache, oracle_len, query_prompt, answer, model, tokenizer, config\n",
    "        )\n",
    "        result['nll_bare'] = nll_bare\n",
    "        result['nll_oracle'] = nll_oracle\n",
    "        result['prefix_token_len_oracle'] = oracle_ptl\n",
    "        \n",
    "        # --- Similarity bins ---\n",
    "        for sim_low, sim_high, bin_name in SIM_BINS_A:\n",
    "            if bin_name in surrogates:\n",
    "                surr_query, surr_sim = surrogates[bin_name]\n",
    "                surr_prefix = f\"This document answers: {surr_query}\"\n",
    "                _, _, surr_len, surr_cache, surr_ptl = \\\n",
    "                    build_matched_bare_and_truncated(surr_prefix, passage, model, tokenizer, config)\n",
    "                nll_surr = score_answer_with_cache(\n",
    "                    surr_cache, surr_len, query_prompt, answer, model, tokenizer, config\n",
    "                )\n",
    "                result[f'nll_{bin_name}'] = nll_surr\n",
    "                result[f'sim_{bin_name}'] = surr_sim\n",
    "                result[f'ptl_{bin_name}'] = surr_ptl\n",
    "            else:\n",
    "                result[f'nll_{bin_name}'] = None\n",
    "                result[f'sim_{bin_name}'] = None\n",
    "                result[f'ptl_{bin_name}'] = None\n",
    "        \n",
    "        # --- Shuffled control (sim_0.60 words shuffled) ---\n",
    "        if 'shuffled_0.60' in surrogates:\n",
    "            shuf_query = surrogates['shuffled_0.60'][0]\n",
    "            shuf_prefix = f\"This document answers: {shuf_query}\"\n",
    "            _, _, shuf_len, shuf_cache, shuf_ptl = \\\n",
    "                build_matched_bare_and_truncated(shuf_prefix, passage, model, tokenizer, config)\n",
    "            nll_shuf = score_answer_with_cache(\n",
    "                shuf_cache, shuf_len, query_prompt, answer, model, tokenizer, config\n",
    "            )\n",
    "            result['nll_shuffled_0.60'] = nll_shuf\n",
    "            result['ptl_shuffled_0.60'] = shuf_ptl\n",
    "        else:\n",
    "            result['nll_shuffled_0.60'] = None\n",
    "        \n",
    "        # --- Length-matched random control ---\n",
    "        if 'length_matched_random' in surrogates:\n",
    "            lm_query, lm_sim = surrogates['length_matched_random']\n",
    "            lm_prefix = f\"This document answers: {lm_query}\"\n",
    "            _, _, lm_len, lm_cache, lm_ptl = \\\n",
    "                build_matched_bare_and_truncated(lm_prefix, passage, model, tokenizer, config)\n",
    "            nll_lm = score_answer_with_cache(\n",
    "                lm_cache, lm_len, query_prompt, answer, model, tokenizer, config\n",
    "            )\n",
    "            result['nll_length_matched_random'] = nll_lm\n",
    "            result['sim_length_matched_random'] = lm_sim\n",
    "            result['ptl_length_matched_random'] = lm_ptl\n",
    "        else:\n",
    "            result['nll_length_matched_random'] = None\n",
    "        \n",
    "        # --- Raw query (no template framing) ---\n",
    "        if 'raw_query_0.60' in surrogates:\n",
    "            raw_query = surrogates['raw_query_0.60'][0]\n",
    "            # Use raw query text directly as prefix, no template\n",
    "            _, _, raw_len, raw_cache, raw_ptl = \\\n",
    "                build_matched_bare_and_truncated(raw_query, passage, model, tokenizer, config)\n",
    "            nll_raw = score_answer_with_cache(\n",
    "                raw_cache, raw_len, query_prompt, answer, model, tokenizer, config\n",
    "            )\n",
    "            result['nll_raw_query_0.60'] = nll_raw\n",
    "            result['ptl_raw_query_0.60'] = raw_ptl\n",
    "        else:\n",
    "            result['nll_raw_query_0.60'] = None\n",
    "        \n",
    "        results_a.append(result)\n",
    "        \n",
    "    except Exception as e:\n",
    "        errors_a += 1\n",
    "        if errors_a <= 5:\n",
    "            print(f\"\\n  Error on sample {idx}: {e}\")\n",
    "        continue\n",
    "    finally:\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    # Checkpoint every 25 samples\n",
    "    if len(results_a) % 25 == 0:\n",
    "        with open(CHECKPOINT_PATH_A, 'w') as f:\n",
    "            json.dump({\n",
    "                'experiment': '12_inv_a',\n",
    "                'results': results_a, 'skipped': skipped_a,\n",
    "                'errors': errors_a,\n",
    "            }, f)\n",
    "        elapsed = time.time() - start_a\n",
    "        rate = len(results_a) / (elapsed / 60) if elapsed > 0 else 0\n",
    "        print(f\"\\n  [{len(results_a)} done | {elapsed/60:.0f}m | {rate:.1f} samples/min]\")\n",
    "\n",
    "elapsed_a = time.time() - start_a\n",
    "print(f\"\\nDone. {len(results_a)} evaluated, {skipped_a} skipped, {errors_a} errors.\")\n",
    "print(f\"Time: {elapsed_a/60:.1f} min ({elapsed_a/3600:.1f} hr)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f41340d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Investigation A: Analysis\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"INVESTIGATION A RESULTS: SCALED QUALITY GRADIENT WITH CONTROLS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "bare_nlls_a = np.array([r['nll_bare'] for r in results_a])\n",
    "oracle_nlls_a = np.array([r['nll_oracle'] for r in results_a])\n",
    "oracle_deltas = bare_nlls_a - oracle_nlls_a\n",
    "oracle_wr = np.mean(oracle_deltas > 0) * 100\n",
    "t_oracle, p_oracle = stats.ttest_rel(bare_nlls_a, oracle_nlls_a)\n",
    "d_oracle = np.mean(oracle_deltas) / np.std(oracle_deltas, ddof=1)\n",
    "\n",
    "print(f\"\\n{'Condition':<30} {'N':>5} {'Mean NLL':>10} {'Win%':>8} {'Delta':>10} {'Cohen d':>10} {'p-value':>12}\")\n",
    "print(\"-\" * 90)\n",
    "print(f\"{'Bare (baseline)':<30} {len(results_a):>5} {np.mean(bare_nlls_a):>10.4f} {'--':>8} {'--':>10} {'--':>10} {'--':>12}\")\n",
    "print(f\"{'Oracle':<30} {len(results_a):>5} {np.mean(oracle_nlls_a):>10.4f} {oracle_wr:>7.1f}% {np.mean(oracle_deltas):>+10.4f} {d_oracle:>10.3f} {p_oracle:>12.2e}\")\n",
    "\n",
    "# Per-bin stats\n",
    "bin_stats_a = {}\n",
    "for sim_low, sim_high, bin_name in SIM_BINS_A:\n",
    "    valid = [r for r in results_a if r.get(f'nll_{bin_name}') is not None]\n",
    "    if len(valid) < 10:\n",
    "        print(f\"{bin_name:<30} {len(valid):>5} -- insufficient data\")\n",
    "        continue\n",
    "    nlls = np.array([r[f'nll_{bin_name}'] for r in valid])\n",
    "    bares = np.array([r['nll_bare'] for r in valid])\n",
    "    sims = np.array([r[f'sim_{bin_name}'] for r in valid])\n",
    "    deltas = bares - nlls\n",
    "    wr = np.mean(deltas > 0) * 100\n",
    "    t, p = stats.ttest_rel(bares, nlls)\n",
    "    d = np.mean(deltas) / np.std(deltas, ddof=1) if np.std(deltas) > 0 else 0\n",
    "    bin_stats_a[bin_name] = {\n",
    "        'n': len(valid), 'mean_nll': float(np.mean(nlls)),\n",
    "        'mean_sim': float(np.mean(sims)), 'win_rate': float(wr),\n",
    "        'mean_delta': float(np.mean(deltas)), 'cohens_d': float(d),\n",
    "        'p_value': float(p),\n",
    "    }\n",
    "    label = f\"{bin_name} (sim~{np.mean(sims):.2f})\"\n",
    "    print(f\"{label:<30} {len(valid):>5} {np.mean(nlls):>10.4f} {wr:>7.1f}% {np.mean(deltas):>+10.4f} {d:>10.3f} {p:>12.2e}\")\n",
    "\n",
    "# --- Controls ---\n",
    "print(\"\\n--- Controls ---\")\n",
    "\n",
    "# Shuffled vs real sim_0.60\n",
    "valid_shuf = [r for r in results_a if r.get('nll_shuffled_0.60') is not None and r.get('nll_sim_0.60') is not None]\n",
    "if len(valid_shuf) >= 10:\n",
    "    real_060 = np.array([r['nll_sim_0.60'] for r in valid_shuf])\n",
    "    shuf_060 = np.array([r['nll_shuffled_0.60'] for r in valid_shuf])\n",
    "    bares_shuf = np.array([r['nll_bare'] for r in valid_shuf])\n",
    "    delta_real = bares_shuf - real_060\n",
    "    delta_shuf = bares_shuf - shuf_060\n",
    "    t_rs, p_rs = stats.ttest_rel(real_060, shuf_060)\n",
    "    wr_real_vs_shuf = np.mean(real_060 < shuf_060) * 100\n",
    "    print(f\"  sim_0.60 vs shuffled_0.60: N={len(valid_shuf)}, real wins {wr_real_vs_shuf:.1f}%, t={t_rs:.3f}, p={p_rs:.6f}\")\n",
    "    print(f\"    Real delta={np.mean(delta_real):.4f}, Shuffled delta={np.mean(delta_shuf):.4f}\")\n",
    "\n",
    "# Length-matched random vs real sim_0.60\n",
    "valid_lm = [r for r in results_a if r.get('nll_length_matched_random') is not None and r.get('nll_sim_0.60') is not None]\n",
    "if len(valid_lm) >= 10:\n",
    "    real_060_lm = np.array([r['nll_sim_0.60'] for r in valid_lm])\n",
    "    lm_nlls = np.array([r['nll_length_matched_random'] for r in valid_lm])\n",
    "    bares_lm = np.array([r['nll_bare'] for r in valid_lm])\n",
    "    t_rl, p_rl = stats.ttest_rel(real_060_lm, lm_nlls)\n",
    "    wr_real_vs_lm = np.mean(real_060_lm < lm_nlls) * 100\n",
    "    print(f\"  sim_0.60 vs length_matched_random: N={len(valid_lm)}, real wins {wr_real_vs_lm:.1f}%, t={t_rl:.3f}, p={p_rl:.6f}\")\n",
    "\n",
    "# Raw query vs template sim_0.60\n",
    "valid_raw = [r for r in results_a if r.get('nll_raw_query_0.60') is not None and r.get('nll_sim_0.60') is not None]\n",
    "if len(valid_raw) >= 10:\n",
    "    tmpl_060 = np.array([r['nll_sim_0.60'] for r in valid_raw])\n",
    "    raw_060 = np.array([r['nll_raw_query_0.60'] for r in valid_raw])\n",
    "    t_tr, p_tr = stats.ttest_rel(tmpl_060, raw_060)\n",
    "    wr_tmpl_vs_raw = np.mean(tmpl_060 < raw_060) * 100\n",
    "    print(f\"  template vs raw (sim_0.60): N={len(valid_raw)}, template wins {wr_tmpl_vs_raw:.1f}%, t={t_tr:.3f}, p={p_tr:.6f}\")\n",
    "\n",
    "# --- Critical: Pearson r with bootstrap CI ---\n",
    "print(\"\\n--- Critical Test: Similarity-Delta Correlation ---\")\n",
    "all_sims_a = []\n",
    "all_deltas_aa = []\n",
    "for r in results_a:\n",
    "    for _, _, bin_name in SIM_BINS_A:\n",
    "        if r.get(f'nll_{bin_name}') is not None and r.get(f'sim_{bin_name}') is not None:\n",
    "            all_sims_a.append(r[f'sim_{bin_name}'])\n",
    "            all_deltas_aa.append(r['nll_bare'] - r[f'nll_{bin_name}'])\n",
    "\n",
    "all_sims_a = np.array(all_sims_a)\n",
    "all_deltas_aa = np.array(all_deltas_aa)\n",
    "\n",
    "r_pearson_a, p_pearson_a = stats.pearsonr(all_sims_a, all_deltas_aa)\n",
    "r_spearman_a, p_spearman_a = stats.spearmanr(all_sims_a, all_deltas_aa)\n",
    "\n",
    "print(f\"  Total (sim, delta) pairs: {len(all_sims_a)}\")\n",
    "print(f\"  Pearson r = {r_pearson_a:.4f}, p = {p_pearson_a:.2e}\")\n",
    "print(f\"  Spearman rho = {r_spearman_a:.4f}, p = {p_spearman_a:.2e}\")\n",
    "\n",
    "# Bootstrap CI for Pearson r\n",
    "r_ci_lo, r_ci_hi, boot_rs = bootstrap_corr_ci(all_sims_a, all_deltas_aa, n_boot=10000)\n",
    "print(f\"  Bootstrap 95% CI for Pearson r: [{r_ci_lo:.4f}, {r_ci_hi:.4f}]\")\n",
    "ci_excludes_zero = r_ci_lo > 0 or r_ci_hi < 0\n",
    "print(f\"  CI excludes 0: {ci_excludes_zero}\")\n",
    "\n",
    "# Bin-level Spearman\n",
    "if len(bin_stats_a) >= 3:\n",
    "    bin_sims_ord = [bin_stats_a[b]['mean_sim'] for b in [bn for _, _, bn in SIM_BINS_A] if b in bin_stats_a]\n",
    "    bin_deltas_ord = [bin_stats_a[b]['mean_delta'] for b in [bn for _, _, bn in SIM_BINS_A] if b in bin_stats_a]\n",
    "    rho_bins_a, p_bins_a = stats.spearmanr(bin_sims_ord, bin_deltas_ord)\n",
    "    print(f\"\\n  Bin-level Spearman rho = {rho_bins_a:.3f}, p = {p_bins_a:.4f}\")\n",
    "else:\n",
    "    rho_bins_a = 0.0\n",
    "\n",
    "# --- Partial correlation: delta ~ similarity | prefix_token_length ---\n",
    "print(\"\\n--- Confound Control: Partial Correlation ---\")\n",
    "all_ptls = []\n",
    "all_sims_pc = []\n",
    "all_deltas_pc = []\n",
    "for r in results_a:\n",
    "    for _, _, bin_name in SIM_BINS_A:\n",
    "        if (r.get(f'nll_{bin_name}') is not None and \n",
    "            r.get(f'sim_{bin_name}') is not None and\n",
    "            r.get(f'ptl_{bin_name}') is not None):\n",
    "            all_sims_pc.append(r[f'sim_{bin_name}'])\n",
    "            all_deltas_pc.append(r['nll_bare'] - r[f'nll_{bin_name}'])\n",
    "            all_ptls.append(r[f'ptl_{bin_name}'])\n",
    "\n",
    "all_sims_pc = np.array(all_sims_pc)\n",
    "all_deltas_pc = np.array(all_deltas_pc)\n",
    "all_ptls = np.array(all_ptls, dtype=float)\n",
    "\n",
    "# Partial correlation: regress out prefix_token_length from both\n",
    "from numpy.linalg import lstsq\n",
    "X = np.column_stack([all_ptls, np.ones(len(all_ptls))])\n",
    "res_sim = all_sims_pc - X @ lstsq(X, all_sims_pc, rcond=None)[0]\n",
    "res_delta = all_deltas_pc - X @ lstsq(X, all_deltas_pc, rcond=None)[0]\n",
    "r_partial, p_partial = stats.pearsonr(res_sim, res_delta)\n",
    "print(f\"  Partial r (sim ~ delta | prefix_len) = {r_partial:.4f}, p = {p_partial:.2e}\")\n",
    "\n",
    "# --- High-beats-oracle check ---\n",
    "print(\"\\n--- High-Beats-Oracle Distribution Check ---\")\n",
    "has_high = [r for r in results_a if r.get('nll_sim_0.85') is not None]\n",
    "no_high = [r for r in results_a if r.get('nll_sim_0.85') is None]\n",
    "if len(has_high) >= 10 and len(no_high) >= 10:\n",
    "    bare_has = np.array([r['nll_bare'] for r in has_high])\n",
    "    bare_no = np.array([r['nll_bare'] for r in no_high])\n",
    "    t_hb, p_hb = stats.ttest_ind(bare_has, bare_no)\n",
    "    print(f\"  Bare NLL (has sim_0.85 match): mean={np.mean(bare_has):.4f}, N={len(has_high)}\")\n",
    "    print(f\"  Bare NLL (no sim_0.85 match):  mean={np.mean(bare_no):.4f}, N={len(no_high)}\")\n",
    "    print(f\"  t={t_hb:.3f}, p={p_hb:.4f}\")\n",
    "    if p_hb < 0.05:\n",
    "        print(\"  WARNING: Samples with high-sim matches have different bare NLL — selection bias!\")\n",
    "    else:\n",
    "        print(\"  OK: No significant difference — no selection bias detected.\")\n",
    "\n",
    "# --- Visualization ---\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "\n",
    "# Plot 1: Win rates by condition\n",
    "ax = axes[0, 0]\n",
    "conds = ['bare']\n",
    "wrs = [50.0]\n",
    "for _, _, bn in SIM_BINS_A:\n",
    "    if bn in bin_stats_a:\n",
    "        conds.append(f\"{bn}\\n({bin_stats_a[bn]['mean_sim']:.2f})\")\n",
    "        wrs.append(bin_stats_a[bn]['win_rate'])\n",
    "conds.append('oracle')\n",
    "wrs.append(oracle_wr)\n",
    "colors = ['#888888'] + ['#4c72b0'] * (len(conds)-2) + ['#c44e52']\n",
    "ax.bar(range(len(conds)), wrs, color=colors)\n",
    "ax.axhline(50, color='gray', linestyle='--', linewidth=0.8)\n",
    "ax.set_xticks(range(len(conds)))\n",
    "ax.set_xticklabels(conds, fontsize=6, rotation=45, ha='right')\n",
    "ax.set_ylabel('Win Rate vs Bare (%)')\n",
    "ax.set_title('Win Rate by Surrogate Quality')\n",
    "\n",
    "# Plot 2: Scatter sim vs delta\n",
    "ax = axes[0, 1]\n",
    "ax.scatter(all_sims_a, all_deltas_aa, alpha=0.05, s=3, color='#4c72b0')\n",
    "for _, _, bn in SIM_BINS_A:\n",
    "    if bn in bin_stats_a:\n",
    "        ax.scatter(bin_stats_a[bn]['mean_sim'], bin_stats_a[bn]['mean_delta'],\n",
    "                  s=100, color='#c44e52', zorder=5, edgecolor='black')\n",
    "ax.axhline(0, color='gray', linestyle='--', linewidth=0.8)\n",
    "z = np.polyfit(all_sims_a, all_deltas_aa, 1)\n",
    "x_line = np.linspace(all_sims_a.min(), all_sims_a.max(), 100)\n",
    "ax.plot(x_line, np.poly1d(z)(x_line), 'r-', linewidth=2, label=f'r={r_pearson_a:.4f}')\n",
    "ax.set_xlabel('Surrogate-Query Similarity')\n",
    "ax.set_ylabel('Delta NLL')\n",
    "ax.set_title(f'Similarity vs Improvement (r={r_pearson_a:.4f})')\n",
    "ax.legend()\n",
    "\n",
    "# Plot 3: NLL gradient\n",
    "ax = axes[0, 2]\n",
    "cl = ['bare']\n",
    "cn = [np.mean(bare_nlls_a)]\n",
    "ce = [np.std(bare_nlls_a)/np.sqrt(len(bare_nlls_a))]\n",
    "for _, _, bn in SIM_BINS_A:\n",
    "    if bn in bin_stats_a:\n",
    "        cl.append(bn)\n",
    "        cn.append(bin_stats_a[bn]['mean_nll'])\n",
    "        valid = [r for r in results_a if r.get(f'nll_{bn}') is not None]\n",
    "        ce.append(np.std([r[f'nll_{bn}'] for r in valid])/np.sqrt(len(valid)))\n",
    "cl.append('oracle')\n",
    "cn.append(np.mean(oracle_nlls_a))\n",
    "ce.append(np.std(oracle_nlls_a)/np.sqrt(len(oracle_nlls_a)))\n",
    "ax.errorbar(range(len(cl)), cn, yerr=ce, fmt='o-', color='#4c72b0', capsize=3)\n",
    "ax.set_xticks(range(len(cl)))\n",
    "ax.set_xticklabels(cl, fontsize=6, rotation=45, ha='right')\n",
    "ax.set_ylabel('Mean NLL')\n",
    "ax.set_title('NLL Gradient: Bare to Oracle')\n",
    "\n",
    "# Plot 4: Controls comparison\n",
    "ax = axes[1, 0]\n",
    "ctrl_labels = []\n",
    "ctrl_deltas_mean = []\n",
    "ctrl_deltas_err = []\n",
    "if len(valid_shuf) >= 10:\n",
    "    ctrl_labels.extend(['sim_0.60\\n(real)', 'shuffled\\n_0.60'])\n",
    "    ctrl_deltas_mean.extend([np.mean(delta_real), np.mean(delta_shuf)])\n",
    "    ctrl_deltas_err.extend([np.std(delta_real)/np.sqrt(len(delta_real)),\n",
    "                            np.std(delta_shuf)/np.sqrt(len(delta_shuf))])\n",
    "if len(valid_lm) >= 10:\n",
    "    bares_lm2 = np.array([r['nll_bare'] for r in valid_lm])\n",
    "    ctrl_labels.append('len_match\\nrandom')\n",
    "    ctrl_deltas_mean.append(float(np.mean(bares_lm2 - lm_nlls)))\n",
    "    ctrl_deltas_err.append(float(np.std(bares_lm2 - lm_nlls)/np.sqrt(len(lm_nlls))))\n",
    "if len(valid_raw) >= 10:\n",
    "    bares_raw = np.array([r['nll_bare'] for r in valid_raw])\n",
    "    ctrl_labels.append('raw_query\\n_0.60')\n",
    "    ctrl_deltas_mean.append(float(np.mean(bares_raw - raw_060)))\n",
    "    ctrl_deltas_err.append(float(np.std(bares_raw - raw_060)/np.sqrt(len(raw_060))))\n",
    "if ctrl_labels:\n",
    "    ax.bar(range(len(ctrl_labels)), ctrl_deltas_mean, yerr=ctrl_deltas_err,\n",
    "           color=['#4c72b0', '#e377c2', '#8c564b', '#bcbd22'][:len(ctrl_labels)], capsize=3)\n",
    "    ax.set_xticks(range(len(ctrl_labels)))\n",
    "    ax.set_xticklabels(ctrl_labels, fontsize=7)\n",
    "    ax.axhline(0, color='gray', linestyle='--')\n",
    "    ax.set_ylabel('Mean Delta NLL')\n",
    "    ax.set_title('Control Comparisons')\n",
    "\n",
    "# Plot 5: Bootstrap CI for Pearson r\n",
    "ax = axes[1, 1]\n",
    "ax.hist(boot_rs, bins=50, color='#4c72b0', alpha=0.7, edgecolor='black', linewidth=0.3)\n",
    "ax.axvline(r_pearson_a, color='red', linewidth=2, label=f'r={r_pearson_a:.4f}')\n",
    "ax.axvline(r_ci_lo, color='orange', linestyle='--', label=f'95% CI: [{r_ci_lo:.4f}, {r_ci_hi:.4f}]')\n",
    "ax.axvline(r_ci_hi, color='orange', linestyle='--')\n",
    "ax.axvline(0, color='gray', linestyle=':', linewidth=1)\n",
    "ax.set_xlabel('Pearson r')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('Bootstrap Distribution of Pearson r')\n",
    "ax.legend(fontsize=7)\n",
    "\n",
    "# Plot 6: Prefix length vs delta (confound check)\n",
    "ax = axes[1, 2]\n",
    "ax.scatter(all_ptls, all_deltas_pc, alpha=0.03, s=3, color='#4c72b0')\n",
    "ax.set_xlabel('Prefix Token Length')\n",
    "ax.set_ylabel('Delta NLL')\n",
    "ax.set_title(f'Prefix Length Confound Check\\n(partial r={r_partial:.4f})')\n",
    "ax.axhline(0, color='gray', linestyle='--')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/exp12/12_investigation_a.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print('Saved: 12_investigation_a.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f918de85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Investigation B: SQuAD v2 — Data Prep + Surrogate Selection\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"INVESTIGATION B: CROSS-DATASET REPLICATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# --- SQuAD v2 ---\n",
    "print(f\"\\nSQuAD v2: {len(squad_samples)} samples\")\n",
    "\n",
    "print(\"Embedding SQuAD target queries...\")\n",
    "squad_target_qs = [s['query'] for s in squad_samples]\n",
    "squad_target_embs = embed_model.encode(squad_target_qs, show_progress_bar=True, batch_size=256)\n",
    "\n",
    "rng_sq = np.random.RandomState(SEEDS['squad'])\n",
    "squad_surrogates = []\n",
    "sq_skip = {b[2]: 0 for b in SIM_BINS_B}\n",
    "\n",
    "for i in tqdm(range(len(squad_samples)), desc=\"SQuAD surrogates\"):\n",
    "    surr = {}\n",
    "    for sim_low, sim_high, bin_name in SIM_BINS_B:\n",
    "        result = find_surrogate_at_similarity(\n",
    "            squad_target_qs[i], squad_target_embs[i],\n",
    "            sim_low, sim_high,\n",
    "            squad_query_pool, squad_pool_embs, rng_sq\n",
    "        )\n",
    "        if result is not None:\n",
    "            surr[bin_name] = result\n",
    "        else:\n",
    "            sq_skip[bin_name] += 1\n",
    "    squad_surrogates.append(surr)\n",
    "\n",
    "print(\"SQuAD surrogate coverage:\")\n",
    "for _, _, bn in SIM_BINS_B:\n",
    "    n_found = len(squad_samples) - sq_skip[bn]\n",
    "    print(f\"  {bn}: {n_found}/{len(squad_samples)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62c3413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Investigation B: SQuAD v2 Evaluation Loop\n",
    "# ============================================================\n",
    "\n",
    "results_squad = []\n",
    "skipped_sq = 0\n",
    "errors_sq = 0\n",
    "start_sq = time.time()\n",
    "\n",
    "CHECKPOINT_PATH_SQ = 'results/exp12/12_checkpoint_squad.json'\n",
    "if os.path.exists(CHECKPOINT_PATH_SQ):\n",
    "    with open(CHECKPOINT_PATH_SQ) as f:\n",
    "        ckpt = json.load(f)\n",
    "    if ckpt.get('experiment') == '12_squad':\n",
    "        results_squad = ckpt['results']\n",
    "        skipped_sq = ckpt['skipped']\n",
    "        errors_sq = ckpt['errors']\n",
    "        print(f\"Resumed: {len(results_squad)} results\")\n",
    "\n",
    "start_idx_sq = len(results_squad) + skipped_sq + errors_sq\n",
    "\n",
    "for idx in tqdm(range(start_idx_sq, len(squad_samples)), desc=\"SQuAD\",\n",
    "                initial=start_idx_sq, total=len(squad_samples)):\n",
    "    sample = squad_samples[idx]\n",
    "    passage = sample['passage']\n",
    "    query = sample['query']\n",
    "    answer = sample['answer']\n",
    "    \n",
    "    answer_ids = tokenizer(answer, return_tensors='pt', add_special_tokens=False)['input_ids']\n",
    "    if answer_ids.shape[1] < 2:\n",
    "        skipped_sq += 1\n",
    "        continue\n",
    "    \n",
    "    query_prompt = config.query_template.format(query=query)\n",
    "    surrogates = squad_surrogates[idx]\n",
    "    \n",
    "    try:\n",
    "        result = {'idx': idx, 'query': query}\n",
    "        \n",
    "        oracle_prefix = f\"This document answers: {query}\"\n",
    "        bare_len, bare_cache, oracle_len, oracle_cache, _ = \\\n",
    "            build_matched_bare_and_truncated(oracle_prefix, passage, model, tokenizer, config)\n",
    "        \n",
    "        result['nll_bare'] = score_answer_with_cache(\n",
    "            bare_cache, bare_len, query_prompt, answer, model, tokenizer, config)\n",
    "        result['nll_oracle'] = score_answer_with_cache(\n",
    "            oracle_cache, oracle_len, query_prompt, answer, model, tokenizer, config)\n",
    "        \n",
    "        for sim_low, sim_high, bin_name in SIM_BINS_B:\n",
    "            if bin_name in surrogates:\n",
    "                sq, ss = surrogates[bin_name]\n",
    "                sp = f\"This document answers: {sq}\"\n",
    "                _, _, sl, sc, _ = build_matched_bare_and_truncated(sp, passage, model, tokenizer, config)\n",
    "                result[f'nll_{bin_name}'] = score_answer_with_cache(\n",
    "                    sc, sl, query_prompt, answer, model, tokenizer, config)\n",
    "                result[f'sim_{bin_name}'] = ss\n",
    "            else:\n",
    "                result[f'nll_{bin_name}'] = None\n",
    "                result[f'sim_{bin_name}'] = None\n",
    "        \n",
    "        results_squad.append(result)\n",
    "    except Exception as e:\n",
    "        errors_sq += 1\n",
    "        if errors_sq <= 3:\n",
    "            print(f\"\\n  Error: {e}\")\n",
    "        continue\n",
    "    finally:\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    if len(results_squad) % 25 == 0:\n",
    "        with open(CHECKPOINT_PATH_SQ, 'w') as f:\n",
    "            json.dump({'experiment': '12_squad', 'results': results_squad,\n",
    "                       'skipped': skipped_sq, 'errors': errors_sq}, f)\n",
    "        elapsed = time.time() - start_sq\n",
    "        print(f\"\\n  [{len(results_squad)} done | {elapsed/60:.0f}m]\")\n",
    "\n",
    "print(f\"\\nSQuAD done. {len(results_squad)} evaluated, {skipped_sq} skipped, {errors_sq} errors.\")\n",
    "print(f\"Time: {(time.time()-start_sq)/60:.1f} min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679b2af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Investigation B: TriviaQA — Data Prep + Evaluation Loop\n",
    "# ============================================================\n",
    "\n",
    "print(f\"\\nTriviaQA: {len(tqa_samples)} samples\")\n",
    "\n",
    "print(\"Embedding TriviaQA target queries...\")\n",
    "tqa_target_qs = [s['query'] for s in tqa_samples]\n",
    "tqa_target_embs = embed_model.encode(tqa_target_qs, show_progress_bar=True, batch_size=256)\n",
    "\n",
    "rng_tq = np.random.RandomState(SEEDS['triviaqa'])\n",
    "tqa_surrogates = []\n",
    "tq_skip = {b[2]: 0 for b in SIM_BINS_B}\n",
    "\n",
    "for i in tqdm(range(len(tqa_samples)), desc=\"TQA surrogates\"):\n",
    "    surr = {}\n",
    "    for sim_low, sim_high, bin_name in SIM_BINS_B:\n",
    "        result = find_surrogate_at_similarity(\n",
    "            tqa_target_qs[i], tqa_target_embs[i],\n",
    "            sim_low, sim_high,\n",
    "            tqa_query_pool, tqa_pool_embs, rng_tq\n",
    "        )\n",
    "        if result is not None:\n",
    "            surr[bin_name] = result\n",
    "        else:\n",
    "            tq_skip[bin_name] += 1\n",
    "    tqa_surrogates.append(surr)\n",
    "\n",
    "print(\"TriviaQA surrogate coverage:\")\n",
    "for _, _, bn in SIM_BINS_B:\n",
    "    n_found = len(tqa_samples) - tq_skip[bn]\n",
    "    print(f\"  {bn}: {n_found}/{len(tqa_samples)}\")\n",
    "\n",
    "# --- TriviaQA Evaluation ---\n",
    "results_tqa = []\n",
    "skipped_tq = 0\n",
    "errors_tq = 0\n",
    "start_tq = time.time()\n",
    "\n",
    "CHECKPOINT_PATH_TQ = 'results/exp12/12_checkpoint_tqa.json'\n",
    "if os.path.exists(CHECKPOINT_PATH_TQ):\n",
    "    with open(CHECKPOINT_PATH_TQ) as f:\n",
    "        ckpt = json.load(f)\n",
    "    if ckpt.get('experiment') == '12_tqa':\n",
    "        results_tqa = ckpt['results']\n",
    "        skipped_tq = ckpt['skipped']\n",
    "        errors_tq = ckpt['errors']\n",
    "        print(f\"Resumed: {len(results_tqa)} results\")\n",
    "\n",
    "start_idx_tq = len(results_tqa) + skipped_tq + errors_tq\n",
    "\n",
    "for idx in tqdm(range(start_idx_tq, len(tqa_samples)), desc=\"TriviaQA\",\n",
    "                initial=start_idx_tq, total=len(tqa_samples)):\n",
    "    sample = tqa_samples[idx]\n",
    "    passage = sample['passage']\n",
    "    query = sample['query']\n",
    "    answer = sample['answer']\n",
    "    \n",
    "    answer_ids = tokenizer(answer, return_tensors='pt', add_special_tokens=False)['input_ids']\n",
    "    if answer_ids.shape[1] < 2:\n",
    "        skipped_tq += 1\n",
    "        continue\n",
    "    \n",
    "    query_prompt = config.query_template.format(query=query)\n",
    "    surrogates = tqa_surrogates[idx]\n",
    "    \n",
    "    try:\n",
    "        result = {'idx': idx, 'query': query}\n",
    "        \n",
    "        oracle_prefix = f\"This document answers: {query}\"\n",
    "        bare_len, bare_cache, oracle_len, oracle_cache, _ = \\\n",
    "            build_matched_bare_and_truncated(oracle_prefix, passage, model, tokenizer, config)\n",
    "        \n",
    "        result['nll_bare'] = score_answer_with_cache(\n",
    "            bare_cache, bare_len, query_prompt, answer, model, tokenizer, config)\n",
    "        result['nll_oracle'] = score_answer_with_cache(\n",
    "            oracle_cache, oracle_len, query_prompt, answer, model, tokenizer, config)\n",
    "        \n",
    "        for sim_low, sim_high, bin_name in SIM_BINS_B:\n",
    "            if bin_name in surrogates:\n",
    "                sq, ss = surrogates[bin_name]\n",
    "                sp = f\"This document answers: {sq}\"\n",
    "                _, _, sl, sc, _ = build_matched_bare_and_truncated(sp, passage, model, tokenizer, config)\n",
    "                result[f'nll_{bin_name}'] = score_answer_with_cache(\n",
    "                    sc, sl, query_prompt, answer, model, tokenizer, config)\n",
    "                result[f'sim_{bin_name}'] = ss\n",
    "            else:\n",
    "                result[f'nll_{bin_name}'] = None\n",
    "                result[f'sim_{bin_name}'] = None\n",
    "        \n",
    "        results_tqa.append(result)\n",
    "    except Exception as e:\n",
    "        errors_tq += 1\n",
    "        if errors_tq <= 3:\n",
    "            print(f\"\\n  Error: {e}\")\n",
    "        continue\n",
    "    finally:\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    if len(results_tqa) % 25 == 0:\n",
    "        with open(CHECKPOINT_PATH_TQ, 'w') as f:\n",
    "            json.dump({'experiment': '12_tqa', 'results': results_tqa,\n",
    "                       'skipped': skipped_tq, 'errors': errors_tq}, f)\n",
    "        elapsed = time.time() - start_tq\n",
    "        print(f\"\\n  [{len(results_tqa)} done | {elapsed/60:.0f}m]\")\n",
    "\n",
    "print(f\"\\nTriviaQA done. {len(results_tqa)} evaluated, {skipped_tq} skipped, {errors_tq} errors.\")\n",
    "print(f\"Time: {(time.time()-start_tq)/60:.1f} min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf68befc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Investigation B: Cross-Dataset Analysis\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"INVESTIGATION B: CROSS-DATASET ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def analyze_dataset(results, dataset_name, bins):\n",
    "    \"\"\"Analyze a single dataset's results.\"\"\"\n",
    "    print(f\"\\n--- {dataset_name} (N={len(results)}) ---\")\n",
    "    bare = np.array([r['nll_bare'] for r in results])\n",
    "    oracle = np.array([r['nll_oracle'] for r in results])\n",
    "    od = bare - oracle\n",
    "    owr = np.mean(od > 0) * 100\n",
    "    print(f\"  Oracle: win%={owr:.1f}%, delta={np.mean(od):+.4f}, d={np.mean(od)/np.std(od,ddof=1):.3f}\")\n",
    "    \n",
    "    bstats = {}\n",
    "    for sim_low, sim_high, bn in bins:\n",
    "        valid = [r for r in results if r.get(f'nll_{bn}') is not None]\n",
    "        if len(valid) < 10:\n",
    "            continue\n",
    "        nlls = np.array([r[f'nll_{bn}'] for r in valid])\n",
    "        bares = np.array([r['nll_bare'] for r in valid])\n",
    "        sims = np.array([r[f'sim_{bn}'] for r in valid])\n",
    "        deltas = bares - nlls\n",
    "        wr = np.mean(deltas > 0) * 100\n",
    "        d = np.mean(deltas) / np.std(deltas, ddof=1) if np.std(deltas) > 0 else 0\n",
    "        bstats[bn] = {'n': len(valid), 'mean_sim': float(np.mean(sims)),\n",
    "                       'mean_delta': float(np.mean(deltas)), 'win_rate': float(wr), 'cohens_d': float(d)}\n",
    "        print(f\"  {bn} (sim~{np.mean(sims):.2f}): N={len(valid)}, win%={wr:.1f}%, d={d:.3f}\")\n",
    "    \n",
    "    # Monotonicity\n",
    "    if len(bstats) >= 2:\n",
    "        bs = [bstats[bn]['mean_sim'] for bn in [b[2] for b in bins] if bn in bstats]\n",
    "        bd = [bstats[bn]['mean_delta'] for bn in [b[2] for b in bins] if bn in bstats]\n",
    "        rho, p = stats.spearmanr(bs, bd) if len(bs) >= 3 else (np.nan, np.nan)\n",
    "        print(f\"  Bin-level Spearman rho = {rho:.3f}, p = {p:.4f}\" if not np.isnan(rho) else \"  Too few bins for Spearman\")\n",
    "    \n",
    "    return bstats\n",
    "\n",
    "squad_stats = analyze_dataset(results_squad, \"SQuAD v2\", SIM_BINS_B)\n",
    "tqa_stats = analyze_dataset(results_tqa, \"TriviaQA\", SIM_BINS_B)\n",
    "\n",
    "# Cross-dataset comparison visualization\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for ax_idx, (results, dname, bstats) in enumerate([\n",
    "    (results_a, \"MS MARCO\", bin_stats_a),\n",
    "    (results_squad, \"SQuAD v2\", squad_stats),\n",
    "    (results_tqa, \"TriviaQA\", tqa_stats),\n",
    "]):\n",
    "    ax = axes[ax_idx]\n",
    "    sims_plot = [0.0]\n",
    "    deltas_plot = [0.0]\n",
    "    bins_used = SIM_BINS_A if dname == \"MS MARCO\" else SIM_BINS_B\n",
    "    for _, _, bn in bins_used:\n",
    "        if bn in bstats:\n",
    "            sims_plot.append(bstats[bn]['mean_sim'])\n",
    "            deltas_plot.append(bstats[bn]['mean_delta'])\n",
    "    bare = np.array([r['nll_bare'] for r in results])\n",
    "    oracle = np.array([r['nll_oracle'] for r in results])\n",
    "    sims_plot.append(1.0)\n",
    "    deltas_plot.append(float(np.mean(bare - oracle)))\n",
    "    ax.plot(sims_plot, deltas_plot, 'o-', color='#4c72b0', linewidth=2, markersize=8)\n",
    "    ax.axhline(0, color='gray', linestyle='--', linewidth=0.8)\n",
    "    ax.set_xlabel('Surrogate Similarity')\n",
    "    ax.set_ylabel('Mean Delta NLL')\n",
    "    ax.set_title(f'{dname} (N={len(results)})')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/exp12/12_investigation_b.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print('Saved: 12_investigation_b.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd7498a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Investigation C: Prefix Format Ablation (N=400)\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"INVESTIGATION C: PREFIX FORMAT ABLATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "samples_c = all_samples[:N_INV_C]\n",
    "print(f\"Using {len(samples_c)} samples\")\n",
    "\n",
    "# Embed and find sim~0.60 surrogates\n",
    "print(\"Embedding target queries for Inv C...\")\n",
    "target_qs_c = [s['query'] for s in samples_c]\n",
    "target_embs_c = embed_model.encode(target_qs_c, batch_size=256)\n",
    "\n",
    "rng_c = np.random.RandomState(SEEDS['msmarco'] + 100)\n",
    "surrogates_c = []\n",
    "for i in tqdm(range(len(samples_c)), desc=\"C surrogates\"):\n",
    "    result = find_surrogate_at_similarity(\n",
    "        target_qs_c[i], target_embs_c[i],\n",
    "        0.55, 0.65, query_pool, pool_embeddings, rng_c\n",
    "    )\n",
    "    surrogates_c.append(result)\n",
    "\n",
    "n_found_c = sum(1 for s in surrogates_c if s is not None)\n",
    "print(f\"Found sim~0.60 surrogates: {n_found_c}/{len(samples_c)}\")\n",
    "\n",
    "# Evaluation loop\n",
    "results_c = []\n",
    "errors_c = 0\n",
    "start_c = time.time()\n",
    "\n",
    "CHECKPOINT_PATH_C = 'results/exp12/12_checkpoint_c.json'\n",
    "if os.path.exists(CHECKPOINT_PATH_C):\n",
    "    with open(CHECKPOINT_PATH_C) as f:\n",
    "        ckpt = json.load(f)\n",
    "    if ckpt.get('experiment') == '12_inv_c':\n",
    "        results_c = ckpt['results']\n",
    "        errors_c = ckpt['errors']\n",
    "        print(f\"Resumed: {len(results_c)} results\")\n",
    "\n",
    "start_idx_c = len(results_c) + errors_c\n",
    "\n",
    "for idx in tqdm(range(start_idx_c, len(samples_c)), desc=\"Inv C\",\n",
    "                initial=start_idx_c, total=len(samples_c)):\n",
    "    sample = samples_c[idx]\n",
    "    passage = sample['passage']\n",
    "    query = sample['query']\n",
    "    answer = sample['answer']\n",
    "    \n",
    "    answer_ids = tokenizer(answer, return_tensors='pt', add_special_tokens=False)['input_ids']\n",
    "    if answer_ids.shape[1] < 2:\n",
    "        continue\n",
    "    \n",
    "    if surrogates_c[idx] is None:\n",
    "        continue\n",
    "    \n",
    "    surr_query, surr_sim = surrogates_c[idx]\n",
    "    query_prompt = config.query_template.format(query=query)\n",
    "    \n",
    "    try:\n",
    "        result = {'idx': idx, 'surr_sim': surr_sim}\n",
    "        \n",
    "        # Bare + Oracle\n",
    "        oracle_prefix = f\"This document answers: {query}\"\n",
    "        bare_len, bare_cache, oracle_len, oracle_cache, _ = \\\n",
    "            build_matched_bare_and_truncated(oracle_prefix, passage, model, tokenizer, config)\n",
    "        result['nll_bare'] = score_answer_with_cache(\n",
    "            bare_cache, bare_len, query_prompt, answer, model, tokenizer, config)\n",
    "        result['nll_oracle'] = score_answer_with_cache(\n",
    "            oracle_cache, oracle_len, query_prompt, answer, model, tokenizer, config)\n",
    "        \n",
    "        # Template format\n",
    "        prefix_tmpl = f\"This document answers: {surr_query}\"\n",
    "        _, _, tl, tc, _ = build_matched_bare_and_truncated(prefix_tmpl, passage, model, tokenizer, config)\n",
    "        result['nll_template'] = score_answer_with_cache(tc, tl, query_prompt, answer, model, tokenizer, config)\n",
    "        \n",
    "        # Raw format (just the query)\n",
    "        _, _, rl, rc, _ = build_matched_bare_and_truncated(surr_query, passage, model, tokenizer, config)\n",
    "        result['nll_raw'] = score_answer_with_cache(rc, rl, query_prompt, answer, model, tokenizer, config)\n",
    "        \n",
    "        # Question format\n",
    "        prefix_q = f\"Question: {surr_query}\\n\\nPassage:\"\n",
    "        _, _, ql, qc, _ = build_matched_bare_and_truncated(prefix_q, passage, model, tokenizer, config)\n",
    "        result['nll_question'] = score_answer_with_cache(qc, ql, query_prompt, answer, model, tokenizer, config)\n",
    "        \n",
    "        # Instruction format\n",
    "        prefix_i = f\"Find information about: {surr_query}\\n\\n\"\n",
    "        _, _, il, ic, _ = build_matched_bare_and_truncated(prefix_i, passage, model, tokenizer, config)\n",
    "        result['nll_instruction'] = score_answer_with_cache(ic, il, query_prompt, answer, model, tokenizer, config)\n",
    "        \n",
    "        # Shuffled template\n",
    "        shuffled_q = shuffle_query_words(surr_query, rng_c)\n",
    "        prefix_st = f\"This document answers: {shuffled_q}\"\n",
    "        _, _, stl, stc, _ = build_matched_bare_and_truncated(prefix_st, passage, model, tokenizer, config)\n",
    "        result['nll_shuffled_template'] = score_answer_with_cache(stc, stl, query_prompt, answer, model, tokenizer, config)\n",
    "        \n",
    "        results_c.append(result)\n",
    "    except Exception as e:\n",
    "        errors_c += 1\n",
    "        if errors_c <= 3:\n",
    "            print(f\"\\n  Error: {e}\")\n",
    "        continue\n",
    "    finally:\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    if len(results_c) % 25 == 0:\n",
    "        with open(CHECKPOINT_PATH_C, 'w') as f:\n",
    "            json.dump({'experiment': '12_inv_c', 'results': results_c, 'errors': errors_c}, f)\n",
    "        elapsed = time.time() - start_c\n",
    "        print(f\"\\n  [{len(results_c)} done | {elapsed/60:.0f}m]\")\n",
    "\n",
    "print(f\"\\nInv C done. {len(results_c)} evaluated, {errors_c} errors.\")\n",
    "print(f\"Time: {(time.time()-start_c)/60:.1f} min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c608fdbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Investigation C: Analysis\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"INVESTIGATION C RESULTS: PREFIX FORMAT ABLATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "n_c = len(results_c)\n",
    "bare_c = np.array([r['nll_bare'] for r in results_c])\n",
    "\n",
    "formats = ['oracle', 'template', 'raw', 'question', 'instruction', 'shuffled_template']\n",
    "format_labels = ['Oracle', 'Template', 'Raw query', 'Question:', 'Instruction:', 'Shuffled template']\n",
    "\n",
    "print(f\"\\n{'Format':<25} {'N':>5} {'Mean NLL':>10} {'Win%':>8} {'Delta':>10} {'Cohen d':>10} {'p-value':>12}\")\n",
    "print(\"-\" * 85)\n",
    "print(f\"{'Bare':<25} {n_c:>5} {np.mean(bare_c):>10.4f}\")\n",
    "\n",
    "format_stats = {}\n",
    "for fmt, label in zip(formats, format_labels):\n",
    "    nlls = np.array([r[f'nll_{fmt}'] for r in results_c])\n",
    "    deltas = bare_c - nlls\n",
    "    wr = np.mean(deltas > 0) * 100\n",
    "    t, p = stats.ttest_rel(bare_c, nlls)\n",
    "    d = np.mean(deltas) / np.std(deltas, ddof=1) if np.std(deltas) > 0 else 0\n",
    "    format_stats[fmt] = {'mean_nll': float(np.mean(nlls)), 'win_rate': float(wr),\n",
    "                         'mean_delta': float(np.mean(deltas)), 'cohens_d': float(d), 'p_value': float(p)}\n",
    "    print(f\"{label:<25} {n_c:>5} {np.mean(nlls):>10.4f} {wr:>7.1f}% {np.mean(deltas):>+10.4f} {d:>10.3f} {p:>12.2e}\")\n",
    "\n",
    "# Key comparisons\n",
    "print(\"\\n--- Key Comparisons ---\")\n",
    "tmpl_nlls = np.array([r['nll_template'] for r in results_c])\n",
    "raw_nlls = np.array([r['nll_raw'] for r in results_c])\n",
    "shuf_nlls = np.array([r['nll_shuffled_template'] for r in results_c])\n",
    "\n",
    "t_tr, p_tr = stats.ttest_rel(tmpl_nlls, raw_nlls)\n",
    "print(f\"  Template vs Raw: t={t_tr:.3f}, p={p_tr:.4f}, template wins {np.mean(tmpl_nlls < raw_nlls)*100:.1f}%\")\n",
    "\n",
    "t_ts, p_ts = stats.ttest_rel(tmpl_nlls, shuf_nlls)\n",
    "print(f\"  Template vs Shuffled: t={t_ts:.3f}, p={p_ts:.4f}, template wins {np.mean(tmpl_nlls < shuf_nlls)*100:.1f}%\")\n",
    "\n",
    "# Visualization\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "x = range(len(format_labels))\n",
    "deltas_plot = [format_stats[f]['mean_delta'] for f in formats]\n",
    "colors_fmt = ['#c44e52', '#4c72b0', '#55a868', '#e377c2', '#8c564b', '#bcbd22']\n",
    "ax.bar(x, deltas_plot, color=colors_fmt)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(format_labels, rotation=30, ha='right')\n",
    "ax.set_ylabel('Mean Delta NLL vs Bare')\n",
    "ax.set_title('Prefix Format Comparison (sim~0.60 surrogates)')\n",
    "ax.axhline(0, color='gray', linestyle='--')\n",
    "for i, d in enumerate(deltas_plot):\n",
    "    ax.text(i, d + 0.001 if d >= 0 else d - 0.003, f'{d:.4f}', ha='center', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/exp12/12_investigation_c.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print('Saved: 12_investigation_c.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd21e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Investigation D: Ranking with Bootstrap CIs (N=1000)\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"INVESTIGATION D: RANKING WITH BOOTSTRAP CIs\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "samples_d = all_samples[:N_INV_D]\n",
    "print(f\"Using {len(samples_d)} samples\")\n",
    "\n",
    "# Embed and find surrogates\n",
    "print(\"Embedding target queries for Inv D...\")\n",
    "target_qs_d = [s['query'] for s in samples_d]\n",
    "target_embs_d = embed_model.encode(target_qs_d, batch_size=256)\n",
    "\n",
    "rng_d = np.random.RandomState(SEEDS['msmarco'] + 200)\n",
    "\n",
    "# Pre-select surrogates + distractors\n",
    "d_surrogates = []  # {sim_0.30: (q, sim), sim_0.60: ..., sim_0.80: ...}\n",
    "d_distractors = []  # [q1, q2, q3, q4]\n",
    "\n",
    "for i in tqdm(range(len(samples_d)), desc=\"D surrogates\"):\n",
    "    surr = {}\n",
    "    for sim_low, sim_high, bin_name in SIM_BINS_B:\n",
    "        result = find_surrogate_at_similarity(\n",
    "            target_qs_d[i], target_embs_d[i],\n",
    "            sim_low, sim_high, query_pool, pool_embeddings, rng_d\n",
    "        )\n",
    "        if result is not None:\n",
    "            surr[bin_name] = result\n",
    "    d_surrogates.append(surr)\n",
    "    \n",
    "    # Select 4 distractors: 2 low-sim + 2 medium-sim\n",
    "    sims_all = cosine_similarity([target_embs_d[i]], pool_embeddings)[0]\n",
    "    dists = []\n",
    "    for slo, shi, npick in [(0.1, 0.3, 2), (0.3, 0.5, 2)]:\n",
    "        mask = (sims_all >= slo) & (sims_all < shi)\n",
    "        cands = np.where(mask)[0]\n",
    "        if len(cands) >= npick:\n",
    "            chosen = rng_d.choice(cands, size=npick, replace=False)\n",
    "            dists.extend([query_pool[c] for c in chosen])\n",
    "        else:\n",
    "            dists.extend([query_pool[c] for c in cands[:npick]])\n",
    "    while len(dists) < 4:\n",
    "        dists.append(query_pool[rng_d.randint(0, len(query_pool))])\n",
    "    d_distractors.append(dists[:4])\n",
    "\n",
    "print(\"Surrogate and distractor selection done.\")\n",
    "\n",
    "# Evaluation loop\n",
    "results_d = []\n",
    "errors_d = 0\n",
    "start_d = time.time()\n",
    "\n",
    "CHECKPOINT_PATH_D = 'results/exp12/12_checkpoint_d.json'\n",
    "if os.path.exists(CHECKPOINT_PATH_D):\n",
    "    with open(CHECKPOINT_PATH_D) as f:\n",
    "        ckpt = json.load(f)\n",
    "    if ckpt.get('experiment') == '12_inv_d':\n",
    "        results_d = ckpt['results']\n",
    "        errors_d = ckpt['errors']\n",
    "        print(f\"Resumed: {len(results_d)} results\")\n",
    "\n",
    "start_idx_d = len(results_d) + errors_d\n",
    "\n",
    "for idx in tqdm(range(start_idx_d, len(samples_d)), desc=\"Inv D\",\n",
    "                initial=start_idx_d, total=len(samples_d)):\n",
    "    sample = samples_d[idx]\n",
    "    passage = sample['passage']\n",
    "    query = sample['query']\n",
    "    answer = sample['answer']\n",
    "    \n",
    "    answer_ids = tokenizer(answer, return_tensors='pt', add_special_tokens=False)['input_ids']\n",
    "    if answer_ids.shape[1] < 2:\n",
    "        continue\n",
    "    \n",
    "    distractors = d_distractors[idx]\n",
    "    all_queries = [query] + distractors  # correct is index 0\n",
    "    surrogates = d_surrogates[idx]\n",
    "    \n",
    "    try:\n",
    "        result = {'idx': idx}\n",
    "        \n",
    "        # Build caches for each condition\n",
    "        # bare + oracle + 3 sim levels\n",
    "        oracle_prefix = f\"This document answers: {query}\"\n",
    "        bare_len, bare_cache, oracle_len, oracle_cache, _ = \\\n",
    "            build_matched_bare_and_truncated(oracle_prefix, passage, model, tokenizer, config)\n",
    "        \n",
    "        cache_conditions = {\n",
    "            'bare': (bare_len, bare_cache),\n",
    "            'oracle': (oracle_len, oracle_cache),\n",
    "        }\n",
    "        \n",
    "        for _, _, bn in SIM_BINS_B:\n",
    "            if bn in surrogates:\n",
    "                sq, ss = surrogates[bn]\n",
    "                sp = f\"This document answers: {sq}\"\n",
    "                _, _, sl, sc, _ = build_matched_bare_and_truncated(sp, passage, model, tokenizer, config)\n",
    "                cache_conditions[bn] = (sl, sc)\n",
    "        \n",
    "        # Score all 5 queries under each cache condition\n",
    "        for cond_name, (clen, ccache) in cache_conditions.items():\n",
    "            scores = []\n",
    "            for q in all_queries:\n",
    "                qp = config.query_template.format(query=q)\n",
    "                nll = score_with_deepcopy(ccache, clen, qp, answer, model, tokenizer, config)\n",
    "                scores.append(nll)\n",
    "            # Rank: lower NLL = better match, correct is index 0\n",
    "            rank = int(np.argsort(scores).tolist().index(0)) + 1\n",
    "            result[f'scores_{cond_name}'] = scores\n",
    "            result[f'rank_{cond_name}'] = rank\n",
    "        \n",
    "        results_d.append(result)\n",
    "    except Exception as e:\n",
    "        errors_d += 1\n",
    "        if errors_d <= 3:\n",
    "            print(f\"\\n  Error: {e}\")\n",
    "        continue\n",
    "    finally:\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    if len(results_d) % 25 == 0:\n",
    "        with open(CHECKPOINT_PATH_D, 'w') as f:\n",
    "            json.dump({'experiment': '12_inv_d', 'results': results_d, 'errors': errors_d}, f)\n",
    "        elapsed = time.time() - start_d\n",
    "        print(f\"\\n  [{len(results_d)} done | {elapsed/60:.0f}m]\")\n",
    "\n",
    "print(f\"\\nInv D done. {len(results_d)} evaluated, {errors_d} errors.\")\n",
    "print(f\"Time: {(time.time()-start_d)/60:.1f} min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e85e667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Investigation D: Analysis with Bootstrap CIs\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"INVESTIGATION D RESULTS: RANKING WITH BOOTSTRAP CIs\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "n_d = len(results_d)\n",
    "\n",
    "# Compute MRR, Hit@1, Hit@3 for each condition\n",
    "cond_names_d = ['bare', 'oracle'] + [bn for _, _, bn in SIM_BINS_B]\n",
    "\n",
    "print(f\"\\n{'Condition':<15} {'N':>5} {'MRR':>10} {'MRR 95% CI':>22} {'Hit@1':>8} {'Hit@3':>8}\")\n",
    "print(\"-\" * 75)\n",
    "\n",
    "ranking_stats = {}\n",
    "for cond in cond_names_d:\n",
    "    valid = [r for r in results_d if f'rank_{cond}' in r]\n",
    "    if len(valid) < 10:\n",
    "        print(f\"{cond:<15} {len(valid):>5} -- insufficient data\")\n",
    "        continue\n",
    "    \n",
    "    ranks = np.array([r[f'rank_{cond}'] for r in valid])\n",
    "    rrs = 1.0 / ranks\n",
    "    mrr = float(np.mean(rrs))\n",
    "    hit1 = float(np.mean(ranks == 1))\n",
    "    hit3 = float(np.mean(ranks <= 3))\n",
    "    \n",
    "    # Bootstrap CIs\n",
    "    mrr_lo, mrr_hi, _ = bootstrap_ci(rrs, np.mean, n_boot=10000)\n",
    "    hit1_lo, hit1_hi, _ = bootstrap_ci((ranks == 1).astype(float), np.mean, n_boot=10000)\n",
    "    hit3_lo, hit3_hi, _ = bootstrap_ci((ranks <= 3).astype(float), np.mean, n_boot=10000)\n",
    "    \n",
    "    ranking_stats[cond] = {\n",
    "        'n': len(valid), 'mrr': mrr, 'mrr_ci': (mrr_lo, mrr_hi),\n",
    "        'hit1': hit1, 'hit1_ci': (hit1_lo, hit1_hi),\n",
    "        'hit3': hit3, 'hit3_ci': (hit3_lo, hit3_hi),\n",
    "    }\n",
    "    \n",
    "    print(f\"{cond:<15} {len(valid):>5} {mrr:>10.3f} [{mrr_lo:.3f}, {mrr_hi:.3f}]  {hit1:>7.1%} {hit3:>7.1%}\")\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for ax_idx, (metric, mlabel) in enumerate([('mrr', 'MRR'), ('hit1', 'Hit@1'), ('hit3', 'Hit@3')]):\n",
    "    ax = axes[ax_idx]\n",
    "    conds_plot = [c for c in cond_names_d if c in ranking_stats]\n",
    "    vals = [ranking_stats[c][metric] for c in conds_plot]\n",
    "    ci_los = [ranking_stats[c][f'{metric}_ci'][0] for c in conds_plot]\n",
    "    ci_his = [ranking_stats[c][f'{metric}_ci'][1] for c in conds_plot]\n",
    "    errs = [[v - lo for v, lo in zip(vals, ci_los)],\n",
    "            [hi - v for v, hi in zip(vals, ci_his)]]\n",
    "    \n",
    "    colors_d = ['#888888', '#c44e52'] + ['#4c72b0'] * len(SIM_BINS_B)\n",
    "    ax.bar(range(len(conds_plot)), vals, yerr=errs, color=colors_d[:len(conds_plot)],\n",
    "           capsize=4, edgecolor='black', linewidth=0.5)\n",
    "    ax.set_xticks(range(len(conds_plot)))\n",
    "    ax.set_xticklabels(conds_plot, rotation=30, ha='right')\n",
    "    ax.set_ylabel(mlabel)\n",
    "    ax.set_title(f'{mlabel} with 95% Bootstrap CIs')\n",
    "    ax.set_ylim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/exp12/12_investigation_d.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print('Saved: 12_investigation_d.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cee70a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Comprehensive Summary + Pre-Registered Verdicts\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"EXPERIMENT 12: COMPREHENSIVE SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# --- Criterion (a): Pearson r bootstrap 95% CI excludes 0 ---\n",
    "criterion_a = ci_excludes_zero\n",
    "print(f\"\\n(a) Pearson r = {r_pearson_a:.4f}, 95% CI = [{r_ci_lo:.4f}, {r_ci_hi:.4f}]\")\n",
    "print(f\"    CI excludes 0: {criterion_a}\")\n",
    "\n",
    "# --- Criterion (b): Bin-level Spearman rho > 0.7 ---\n",
    "criterion_b = rho_bins_a > 0.7\n",
    "print(f\"\\n(b) Bin-level Spearman rho = {rho_bins_a:.3f}\")\n",
    "print(f\"    rho > 0.7: {criterion_b}\")\n",
    "\n",
    "# --- Criterion (c): sim_0.60 beats shuffled_0.60 (p<0.05) ---\n",
    "criterion_c = False\n",
    "if len(valid_shuf) >= 10:\n",
    "    criterion_c = p_rs < 0.05 and wr_real_vs_shuf > 50\n",
    "    print(f\"\\n(c) sim_0.60 vs shuffled_0.60: p={p_rs:.6f}, real wins {wr_real_vs_shuf:.1f}%\")\n",
    "    print(f\"    Significant (p<0.05) and real wins: {criterion_c}\")\n",
    "\n",
    "# --- Overall verdict ---\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "if criterion_a and criterion_b and criterion_c:\n",
    "    verdict = \"CONFIRMED\"\n",
    "    print(\"VERDICT: CONFIRMED\")\n",
    "    print(\"All three pre-registered criteria met.\")\n",
    "elif criterion_a or criterion_b:\n",
    "    verdict = \"PARTIALLY CONFIRMED\"\n",
    "    print(\"VERDICT: PARTIALLY CONFIRMED\")\n",
    "    print(\"Some but not all criteria met.\")\n",
    "else:\n",
    "    verdict = \"REFUTED\"\n",
    "    print(\"VERDICT: REFUTED\")\n",
    "    print(\"No criteria met. Semantic quality gradient not confirmed at scale.\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# --- Cross-dataset summary ---\n",
    "print(\"\\n--- Cross-Dataset Replication ---\")\n",
    "for dname, bstats in [(\"MS MARCO\", bin_stats_a), (\"SQuAD v2\", squad_stats), (\"TriviaQA\", tqa_stats)]:\n",
    "    bins_used = SIM_BINS_A if dname == \"MS MARCO\" else SIM_BINS_B\n",
    "    if len(bstats) >= 2:\n",
    "        bs = [bstats[bn]['mean_sim'] for bn in [b[2] for b in bins_used] if bn in bstats]\n",
    "        bd = [bstats[bn]['mean_delta'] for bn in [b[2] for b in bins_used] if bn in bstats]\n",
    "        if len(bs) >= 3:\n",
    "            rho, p = stats.spearmanr(bs, bd)\n",
    "            print(f\"  {dname}: bin-level rho={rho:.3f}, p={p:.4f}\")\n",
    "        else:\n",
    "            print(f\"  {dname}: too few bins ({len(bs)})\")\n",
    "\n",
    "# --- Format ablation summary ---\n",
    "print(\"\\n--- Format Ablation ---\")\n",
    "best_fmt = max(format_stats.items(), key=lambda x: x[1]['mean_delta'])\n",
    "print(f\"  Best format: {best_fmt[0]} (delta={best_fmt[1]['mean_delta']:.4f})\")\n",
    "worst_fmt = min(format_stats.items(), key=lambda x: x[1]['mean_delta'])\n",
    "print(f\"  Worst format: {worst_fmt[0]} (delta={worst_fmt[1]['mean_delta']:.4f})\")\n",
    "\n",
    "# --- Ranking summary ---\n",
    "print(\"\\n--- Ranking ---\")\n",
    "if 'bare' in ranking_stats and 'oracle' in ranking_stats:\n",
    "    print(f\"  Bare MRR: {ranking_stats['bare']['mrr']:.3f} {ranking_stats['bare']['mrr_ci']}\")\n",
    "    print(f\"  Oracle MRR: {ranking_stats['oracle']['mrr']:.3f} {ranking_stats['oracle']['mrr_ci']}\")\n",
    "    for _, _, bn in SIM_BINS_B:\n",
    "        if bn in ranking_stats:\n",
    "            print(f\"  {bn} MRR: {ranking_stats[bn]['mrr']:.3f} {ranking_stats[bn]['mrr_ci']}\")\n",
    "\n",
    "# --- Control comparisons ---\n",
    "print(\"\\n--- Control Comparisons ---\")\n",
    "if len(valid_shuf) >= 10:\n",
    "    print(f\"  sim_0.60 vs shuffled: p={p_rs:.6f} (word order {'matters' if p_rs < 0.05 else 'does NOT matter'})\")\n",
    "if len(valid_lm) >= 10:\n",
    "    print(f\"  sim_0.60 vs length-matched random: p={p_rl:.6f} (similarity {'matters' if p_rl < 0.05 else 'does NOT matter'} beyond length)\")\n",
    "if len(valid_raw) >= 10:\n",
    "    print(f\"  template vs raw: p={p_tr:.4f} (template framing {'matters' if p_tr < 0.05 else 'does NOT matter'})\")\n",
    "print(f\"  Partial r (controlling prefix length): {r_partial:.4f}, p={p_partial:.2e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948bbd17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Save Results\n",
    "# ============================================================\n",
    "\n",
    "output = {\n",
    "    'metadata': {\n",
    "        'experiment': '12_definitive_semantic_signal',\n",
    "        'timestamp': datetime.datetime.now().isoformat(),\n",
    "        'model_name': config.model_name,\n",
    "        'seeds': SEEDS,\n",
    "        'n_inv_a': N_INV_A, 'n_inv_b': N_INV_B,\n",
    "        'n_inv_c': N_INV_C, 'n_inv_d': N_INV_D,\n",
    "    },\n",
    "    'investigation_a': {\n",
    "        'n_evaluated': len(results_a),\n",
    "        'skipped': skipped_a, 'errors': errors_a,\n",
    "        'bin_stats': bin_stats_a,\n",
    "        'oracle_win_rate': float(oracle_wr),\n",
    "        'pearson_r': float(r_pearson_a),\n",
    "        'pearson_p': float(p_pearson_a),\n",
    "        'pearson_r_ci': [r_ci_lo, r_ci_hi],\n",
    "        'spearman_rho': float(r_spearman_a),\n",
    "        'bin_spearman_rho': float(rho_bins_a),\n",
    "        'partial_r': float(r_partial),\n",
    "        'partial_p': float(p_partial),\n",
    "        'shuffled_p': float(p_rs) if len(valid_shuf) >= 10 else None,\n",
    "        'shuffled_real_wins': float(wr_real_vs_shuf) if len(valid_shuf) >= 10 else None,\n",
    "        'length_matched_p': float(p_rl) if len(valid_lm) >= 10 else None,\n",
    "        'raw_vs_template_p': float(p_tr) if len(valid_raw) >= 10 else None,\n",
    "        'results': results_a,\n",
    "    },\n",
    "    'investigation_b': {\n",
    "        'squad': {\n",
    "            'n_evaluated': len(results_squad),\n",
    "            'bin_stats': squad_stats,\n",
    "            'results': results_squad,\n",
    "        },\n",
    "        'triviaqa': {\n",
    "            'n_evaluated': len(results_tqa),\n",
    "            'bin_stats': tqa_stats,\n",
    "            'results': results_tqa,\n",
    "        },\n",
    "    },\n",
    "    'investigation_c': {\n",
    "        'n_evaluated': len(results_c),\n",
    "        'format_stats': format_stats,\n",
    "        'results': results_c,\n",
    "    },\n",
    "    'investigation_d': {\n",
    "        'n_evaluated': len(results_d),\n",
    "        'ranking_stats': {k: {kk: vv for kk, vv in v.items()} for k, v in ranking_stats.items()},\n",
    "        'results': results_d,\n",
    "    },\n",
    "    'verdict': {\n",
    "        'criterion_a': bool(criterion_a),\n",
    "        'criterion_b': bool(criterion_b),\n",
    "        'criterion_c': bool(criterion_c),\n",
    "        'overall': verdict,\n",
    "    },\n",
    "}\n",
    "\n",
    "output_path = 'results/exp12/12_results.json'\n",
    "with open(output_path, 'w') as f:\n",
    "    json.dump(output, f, indent=2, default=str)\n",
    "print(f\"Results saved to {output_path}\")\n",
    "print(f\"File size: {os.path.getsize(output_path) / 1e6:.1f} MB\")\n",
    "\n",
    "# Final summary figure\n",
    "fig, axes = plt.subplots(1, 4, figsize=(22, 5))\n",
    "\n",
    "# A: Quality gradient\n",
    "ax = axes[0]\n",
    "sp = [0.0]\n",
    "dp = [0.0]\n",
    "for _, _, bn in SIM_BINS_A:\n",
    "    if bn in bin_stats_a:\n",
    "        sp.append(bin_stats_a[bn]['mean_sim'])\n",
    "        dp.append(bin_stats_a[bn]['mean_delta'])\n",
    "sp.append(1.0)\n",
    "dp.append(float(np.mean(oracle_deltas)))\n",
    "ax.plot(sp, dp, 'o-', color='#4c72b0', linewidth=2, markersize=8)\n",
    "ax.axhline(0, color='gray', linestyle='--')\n",
    "ax.set_xlabel('Surrogate Similarity')\n",
    "ax.set_ylabel('Mean Delta NLL')\n",
    "ax.set_title(f'A: Quality Gradient (N={len(results_a)})\\nr={r_pearson_a:.4f} [{r_ci_lo:.4f},{r_ci_hi:.4f}]')\n",
    "\n",
    "# B: Cross-dataset\n",
    "ax = axes[1]\n",
    "for dname, bstats, marker, color in [\n",
    "    (\"MARCO\", bin_stats_a, 'o', '#4c72b0'),\n",
    "    (\"SQuAD\", squad_stats, 's', '#c44e52'),\n",
    "    (\"TQA\", tqa_stats, '^', '#55a868'),\n",
    "]:\n",
    "    bins_used = SIM_BINS_A if dname == \"MARCO\" else SIM_BINS_B\n",
    "    ss = [bstats[bn]['mean_sim'] for bn in [b[2] for b in bins_used] if bn in bstats]\n",
    "    dd = [bstats[bn]['mean_delta'] for bn in [b[2] for b in bins_used] if bn in bstats]\n",
    "    ax.plot(ss, dd, f'{marker}-', color=color, label=dname, markersize=8)\n",
    "ax.axhline(0, color='gray', linestyle='--')\n",
    "ax.set_xlabel('Surrogate Similarity')\n",
    "ax.set_ylabel('Mean Delta NLL')\n",
    "ax.set_title('B: Cross-Dataset')\n",
    "ax.legend()\n",
    "\n",
    "# C: Format ablation\n",
    "ax = axes[2]\n",
    "fmts_sorted = sorted(format_stats.items(), key=lambda x: -x[1]['mean_delta'])\n",
    "ax.barh(range(len(fmts_sorted)), [f[1]['mean_delta'] for f in fmts_sorted],\n",
    "        color='#4c72b0')\n",
    "ax.set_yticks(range(len(fmts_sorted)))\n",
    "ax.set_yticklabels([f[0] for f in fmts_sorted], fontsize=8)\n",
    "ax.set_xlabel('Mean Delta NLL')\n",
    "ax.set_title('C: Format Ablation')\n",
    "ax.axvline(0, color='gray', linestyle='--')\n",
    "\n",
    "# D: Ranking\n",
    "ax = axes[3]\n",
    "conds_d = [c for c in ['bare'] + [bn for _, _, bn in SIM_BINS_B] + ['oracle'] if c in ranking_stats]\n",
    "mrrs_d = [ranking_stats[c]['mrr'] for c in conds_d]\n",
    "errs_d = [[ranking_stats[c]['mrr'] - ranking_stats[c]['mrr_ci'][0] for c in conds_d],\n",
    "           [ranking_stats[c]['mrr_ci'][1] - ranking_stats[c]['mrr'] for c in conds_d]]\n",
    "ax.bar(range(len(conds_d)), mrrs_d, yerr=errs_d,\n",
    "       color=['#888888'] + ['#4c72b0']*(len(conds_d)-2) + ['#c44e52'], capsize=3)\n",
    "ax.set_xticks(range(len(conds_d)))\n",
    "ax.set_xticklabels(conds_d, rotation=30, ha='right', fontsize=8)\n",
    "ax.set_ylabel('MRR')\n",
    "ax.set_title('D: Ranking (95% CI)')\n",
    "ax.set_ylim(0, 1)\n",
    "\n",
    "plt.suptitle(f'Experiment 12: Verdict = {verdict}', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/exp12/12_summary.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print('Saved: 12_summary.png')\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"EXPERIMENT 12 COMPLETE. VERDICT: {verdict}\")\n",
    "print(f\"{'='*80}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
