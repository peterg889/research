{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model-Dependent Tests for lib/kv_cache.py\n",
    "\n",
    "These tests require a loaded model and run in a Jupyter kernel that already has\n",
    "the model in GPU memory. They verify:\n",
    "\n",
    "1. RoPE correction matches HuggingFace's actual rotary embedding\n",
    "2. RoPE roundtrip is identity in float64, bounded error in float16\n",
    "3. `apply_rope_roundtrip_noise` preserves BOS, introduces small noise\n",
    "4. `correct_rope_positions_with_bos` preserves BOS, modifies doc keys\n",
    "5. `score_answer_with_cache` NLL matches full-sequence forward pass\n",
    "6. Truncated+corrected keys match bare cache keys\n",
    "7. Truncated values differ from bare values (prefix priming works)\n",
    "8. `build_truncated_kv_cache_corrected` produces correct length\n",
    "9. `build_truncated_cache_variable_prefix` correctness\n",
    "10. BPE boundary effect detection\n",
    "11. Suffix cache passage portion matches bare cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import sys, os, torch, numpy as np\nsys.path.insert(0, '..')\n\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, DynamicCache\nfrom lib.config import ExperimentConfig\nfrom lib.kv_cache import (\n    _rotate_half, _build_rope_correction, _get_rope_theta, _ensure_dynamic_cache,\n    _get_cache_keys, _get_cache_values, _set_cache_keys, _set_cache_values,\n    build_kv_cache, extract_and_truncate_cache, extract_and_truncate_cache_with_bos,\n    correct_rope_positions, correct_rope_positions_with_bos,\n    build_hybrid_cache, swap_bos_entry, apply_rope_roundtrip_noise,\n    replace_values_at_layers, score_answer_with_cache,\n    build_truncated_kv_cache, build_truncated_kv_cache_corrected,\n    build_truncated_cache_variable_prefix, build_suffix_kv_cache,\n)\n\nconfig = ExperimentConfig(num_samples=10, seed=42)\n\n# Load model\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True, bnb_4bit_compute_dtype=torch.float16, bnb_4bit_quant_type='nf4',\n)\nmodel_name = config.model_name\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name, quantization_config=bnb_config, device_map='auto',\n)\nmodel.eval()\nNUM_LAYERS = model.config.num_hidden_layers\nHEAD_DIM = model.config.hidden_size // model.config.num_attention_heads\nROPE_THETA = _get_rope_theta(model.config)\n\nPASSAGE = (\n    'The Amazon rainforest produces approximately 20 percent of the world\\'s oxygen. '\n    'It covers over 5.5 million square kilometers and is home to roughly 10 percent '\n    'of all species on Earth.'\n)\nQUERY = 'how much oxygen does the amazon produce'\nANSWER = 'approximately 20 percent'\n\npassed = 0\nfailed = 0\ntotal = 0\n\ndef check(name, condition, detail=''):\n    global passed, failed, total\n    total += 1\n    if condition:\n        passed += 1\n        print(f'  PASS: {name}  {detail}')\n    else:\n        failed += 1\n        print(f'  FAIL: {name}  {detail}')\n\nprint(f'Model loaded: {model_name}')\nprint(f'Layers: {NUM_LAYERS}, Head dim: {HEAD_DIM}, RoPE theta: {ROPE_THETA}')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print('='*70)\nprint('TEST 1: RoPE correction matches HuggingFace rotary embedding')\nprint('='*70)\n\n# Get the model's actual rotary embedding module\nrotary_emb = model.model.rotary_emb\n\nS = 20  # offset\n# Generate cos/sin at specific positions using the model's own RoPE\ndummy_x = torch.zeros(1, 1, 1, HEAD_DIM, device=config.device, dtype=torch.float16)\npos_1 = torch.tensor([[1]], device=config.device)\npos_1_plus_S = torch.tensor([[1 + S]], device=config.device)\n\ncos_1, sin_1 = rotary_emb(dummy_x, pos_1)\ncos_1s, sin_1s = rotary_emb(dummy_x, pos_1_plus_S)\n\n# Apply HF RoPE at position 1 and 1+S to same pre-rope key\nkey_pre = torch.randn(1, 1, 1, HEAD_DIM, device=config.device, dtype=torch.float32)\nc1, s1 = cos_1.squeeze().float(), sin_1.squeeze().float()\nc1s, s1s = cos_1s.squeeze().float(), sin_1s.squeeze().float()\n\nkey_at_1 = key_pre * c1 + _rotate_half(key_pre) * s1\nkey_at_1_plus_S = key_pre * c1s + _rotate_half(key_pre) * s1s\n\n# Apply our correction(-S) to key_at_1+S => should recover key_at_1\ncos_corr, sin_corr = _build_rope_correction(S, HEAD_DIM, ROPE_THETA)\ncos_corr = cos_corr.to(device=config.device, dtype=torch.float32)\nsin_corr = sin_corr.to(device=config.device, dtype=torch.float32)\n\nrecovered = key_at_1_plus_S * cos_corr + _rotate_half(key_at_1_plus_S) * sin_corr\nmax_err = (recovered - key_at_1).abs().max().item()\n# Tolerance is 1e-3 because HF stores inv_freq as a buffer (computed once)\n# while we recompute from scratch — float32 accumulation differs slightly\ncheck('RoPE correction vs HF rotary_emb', max_err < 1e-3, f'max_err={max_err:.2e}')\n\n# Test multiple offsets\nfor S_test in [1, 5, 50, 200]:\n    pos_target = torch.tensor([[3]], device=config.device)\n    pos_shifted = torch.tensor([[3 + S_test]], device=config.device)\n    c_t, s_t = rotary_emb(dummy_x, pos_target)\n    c_s, s_s = rotary_emb(dummy_x, pos_shifted)\n    \n    k_t = key_pre * c_t.squeeze().float() + _rotate_half(key_pre) * s_t.squeeze().float()\n    k_s = key_pre * c_s.squeeze().float() + _rotate_half(key_pre) * s_s.squeeze().float()\n    \n    cc, sc = _build_rope_correction(S_test, HEAD_DIM, ROPE_THETA)\n    cc, sc = cc.to(device=config.device, dtype=torch.float32), sc.to(device=config.device, dtype=torch.float32)\n    rec = k_s * cc + _rotate_half(k_s) * sc\n    err = (rec - k_t).abs().max().item()\n    check(f'  offset={S_test}', err < 1e-3, f'max_err={err:.2e}')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print('='*70)\nprint('TEST 2: RoPE roundtrip identity (float64) and bounded error (float16)')\nprint('='*70)\n\nkeys_f64 = torch.randn(1, 8, 10, HEAD_DIM, dtype=torch.float64)\noffset = 15\n\n# Forward: RoPE(+S) — use correction(-S)\ncos_fwd, sin_fwd = _build_rope_correction(-offset, HEAD_DIM, ROPE_THETA)\ncos_fwd, sin_fwd = cos_fwd.double(), sin_fwd.double()\nrotated = keys_f64 * cos_fwd + _rotate_half(keys_f64) * sin_fwd\n\n# Inverse: correction(+S)\ncos_inv, sin_inv = _build_rope_correction(offset, HEAD_DIM, ROPE_THETA)\ncos_inv, sin_inv = cos_inv.double(), sin_inv.double()\nrecovered = rotated * cos_inv + _rotate_half(rotated) * sin_inv\n\nerr_f64 = (recovered - keys_f64).abs().max().item()\n# _build_rope_correction computes angles in float64 but returns float32,\n# so roundtrip through double has float32-level precision\ncheck('RoPE roundtrip identity (float64)', err_f64 < 1e-6, f'max_err={err_f64:.2e}')\n\n# Float16\nkeys_f16 = torch.randn(1, 8, 10, HEAD_DIM, dtype=torch.float16, device=config.device)\ncf, sf = _build_rope_correction(-offset, HEAD_DIM, ROPE_THETA)\ncf, sf = cf.to(device=config.device, dtype=torch.float16), sf.to(device=config.device, dtype=torch.float16)\nrotated_16 = keys_f16 * cf + _rotate_half(keys_f16) * sf\n\nci, si = _build_rope_correction(offset, HEAD_DIM, ROPE_THETA)\nci, si = ci.to(device=config.device, dtype=torch.float16), si.to(device=config.device, dtype=torch.float16)\nrecovered_16 = rotated_16 * ci + _rotate_half(rotated_16) * si\n\nerr_f16 = (recovered_16.float() - keys_f16.float()).abs().max().item()\ncheck('RoPE roundtrip float16: nonzero error', err_f16 > 0, f'err={err_f16:.6f}')\ncheck('RoPE roundtrip float16: bounded error', err_f16 < 0.1, f'err={err_f16:.6f}')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('='*70)\n",
    "print('TEST 3: apply_rope_roundtrip_noise')\n",
    "print('='*70)\n",
    "\n",
    "cache = DynamicCache()\n",
    "keys_before = []\n",
    "for li in range(NUM_LAYERS):\n",
    "    k = torch.randn(1, 8, 10, HEAD_DIM, device=config.device, dtype=torch.float16)\n",
    "    v = torch.randn(1, 8, 10, HEAD_DIM, device=config.device, dtype=torch.float16)\n",
    "    keys_before.append(k.clone())\n",
    "    cache.update(k, v, li)\n",
    "\n",
    "bos_before = [_get_cache_keys(cache, li)[:, :, 0:1, :].clone() for li in range(NUM_LAYERS)]\n",
    "apply_rope_roundtrip_noise(cache, offset=10, model=model)\n",
    "\n",
    "bos_preserved = all(\n",
    "    torch.allclose(_get_cache_keys(cache, li)[:, :, 0:1, :], bos_before[li], atol=0)\n",
    "    for li in range(NUM_LAYERS)\n",
    ")\n",
    "check('roundtrip_noise: BOS preserved', bos_preserved)\n",
    "\n",
    "max_errors = []\n",
    "for li in range(NUM_LAYERS):\n",
    "    doc_b = keys_before[li][:, :, 1:, :].float()\n",
    "    doc_a = _get_cache_keys(cache, li)[:, :, 1:, :].float()\n",
    "    max_errors.append((doc_a - doc_b).abs().max().item())\n",
    "\n",
    "mean_max = np.mean(max_errors)\n",
    "check('roundtrip_noise: nonzero perturbation', mean_max > 1e-5, f'mean_max_err={mean_max:.6f}')\n",
    "check('roundtrip_noise: bounded perturbation', mean_max < 0.1, f'mean_max_err={mean_max:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('='*70)\n",
    "print('TEST 4: correct_rope_positions_with_bos')\n",
    "print('='*70)\n",
    "\n",
    "# BOS preservation\n",
    "cache4 = DynamicCache()\n",
    "for li in range(NUM_LAYERS):\n",
    "    k = torch.randn(1, 8, 10, HEAD_DIM, device=config.device, dtype=torch.float16)\n",
    "    v = torch.randn(1, 8, 10, HEAD_DIM, device=config.device, dtype=torch.float16)\n",
    "    cache4.update(k, v, li)\n",
    "\n",
    "bos4 = [_get_cache_keys(cache4, li)[:, :, 0:1, :].clone() for li in range(NUM_LAYERS)]\n",
    "doc4 = [_get_cache_keys(cache4, li)[:, :, 1:, :].clone() for li in range(NUM_LAYERS)]\n",
    "\n",
    "correct_rope_positions_with_bos(cache4, offset=10, model=model)\n",
    "\n",
    "bos_ok = all(\n",
    "    torch.allclose(_get_cache_keys(cache4, li)[:, :, 0:1, :], bos4[li], atol=0)\n",
    "    for li in range(NUM_LAYERS)\n",
    ")\n",
    "check('correct_rope_with_bos: BOS preserved', bos_ok)\n",
    "\n",
    "doc_changed = all(\n",
    "    not torch.allclose(_get_cache_keys(cache4, li)[:, :, 1:, :], doc4[li], atol=1e-6)\n",
    "    for li in range(NUM_LAYERS)\n",
    ")\n",
    "check('correct_rope_with_bos: doc keys modified', doc_changed)\n",
    "\n",
    "# Zero offset = noop\n",
    "cache4b = DynamicCache()\n",
    "k4b = torch.randn(1, 8, 10, HEAD_DIM, device=config.device, dtype=torch.float16)\n",
    "v4b = torch.randn(1, 8, 10, HEAD_DIM, device=config.device, dtype=torch.float16)\n",
    "cache4b.update(k4b.clone(), v4b.clone(), 0)\n",
    "correct_rope_positions_with_bos(cache4b, offset=0, model=model)\n",
    "check('correct_rope_with_bos: offset=0 is noop',\n",
    "      torch.allclose(_get_cache_keys(cache4b, 0), k4b))\n",
    "\n",
    "# correct_rope_positions (no BOS) modifies ALL positions\n",
    "cache4c = DynamicCache()\n",
    "k4c = torch.randn(1, 8, 10, HEAD_DIM, device=config.device, dtype=torch.float16)\n",
    "cache4c.update(k4c.clone(), torch.randn_like(k4c), 0)\n",
    "correct_rope_positions(cache4c, offset=10, model=model)\n",
    "check('correct_rope (no BOS): modifies all positions including 0',\n",
    "      not torch.allclose(_get_cache_keys(cache4c, 0), k4c, atol=1e-6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print('='*70)\nprint('TEST 5: score_answer_with_cache NLL matches full forward pass')\nprint('='*70)\n\ncontext = f'Document:\\n{PASSAGE}'\nctx_len, cache5 = build_kv_cache(context, model, tokenizer, config)\nquery_prompt = config.query_template.format(query=QUERY)\nnll_cached = score_answer_with_cache(\n    cache5, ctx_len, query_prompt, ANSWER, model, tokenizer, config\n)\n\n# Full-sequence forward pass\nfull_text = context + query_prompt + ANSWER\nfull_ids = tokenizer(full_text, return_tensors='pt', add_special_tokens=True)['input_ids'].to(config.device)\nanswer_ids = tokenizer(ANSWER, return_tensors='pt', add_special_tokens=False)['input_ids'].to(config.device)\nanswer_len = answer_ids.shape[1]\n\nwith torch.no_grad():\n    outputs = model(input_ids=full_ids, attention_mask=torch.ones_like(full_ids), return_dict=True)\n\nanswer_start = full_ids.shape[1] - answer_len\nanswer_logits = outputs.logits[:, answer_start:-1, :]\nanswer_labels = full_ids[:, answer_start + 1:]\n\nloss_fct = torch.nn.CrossEntropyLoss(reduction='sum')\nnll_full = loss_fct(\n    answer_logits.contiguous().view(-1, answer_logits.size(-1)),\n    answer_labels.contiguous().view(-1)\n).item() / (answer_len - 1)\n\nrel_err = abs(nll_cached - nll_full) / max(abs(nll_full), 1e-8)\n# 4-bit quantized models may have small numerical differences between\n# cached and full forward passes due to non-deterministic matmul ordering\ncheck('NLL cached vs full', rel_err < 0.05,\n      f'cached={nll_cached:.6f}, full={nll_full:.6f}, rel_err={rel_err:.6f}')\n\n# Correct answer should have lower NLL\nnll_wrong = score_answer_with_cache(\n    cache5, ctx_len, query_prompt, 'purple elephants flying sideways',\n    model, tokenizer, config\n)\ncheck('correct < wrong answer NLL', nll_cached < nll_wrong,\n      f'correct={nll_cached:.4f}, wrong={nll_wrong:.4f}')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print('='*70)\nprint('TEST 6: Truncated+corrected keys match bare cache keys')\nprint('='*70)\n\nprefix_text = 'Some irrelevant prefix text here. '\ndocument_text = f'Document:\\n{PASSAGE}'\nprefix_with_sep = prefix_text + ' '\n\nprefix_enc = tokenizer(prefix_with_sep, return_tensors='pt', add_special_tokens=True)\nprefix_len = prefix_enc['input_ids'].shape[1]\n\nfull_context = prefix_with_sep + document_text\nfull_enc = tokenizer(full_context, return_tensors='pt', add_special_tokens=True)\nfull_ids = full_enc['input_ids'].to(config.device)\ndoc_len = full_ids.shape[1] - prefix_len\n\n# Extract exact doc tokens for bare cache\ndoc_token_ids = full_ids[:, prefix_len:]\nbos_id = full_ids[:, :1]\nbare_ids = torch.cat([bos_id, doc_token_ids], dim=1)\n\nwith torch.no_grad():\n    bare_out = model(input_ids=bare_ids, attention_mask=torch.ones_like(bare_ids),\n                     use_cache=True, return_dict=True)\n    full_out = model(input_ids=full_ids, attention_mask=torch.ones_like(full_ids),\n                     use_cache=True, return_dict=True)\n\nbare_cache = _ensure_dynamic_cache(bare_out.past_key_values)\ntrunc_cache = extract_and_truncate_cache_with_bos(full_out.past_key_values, doc_len)\noffset = prefix_len - 1\ncorrect_rope_positions_with_bos(trunc_cache, offset, model)\n\n# Layer 0: input is pure token embeddings (identical regardless of prefix),\n# so keys = RoPE(W_K * embed(token), pos). After correction, should match bare.\n# NOTE: 4-bit quantized matmul produces slightly different results for different\n# total sequence lengths, even for the same tokens. Tolerance reflects this.\nbk0 = _get_cache_keys(bare_cache, 0)[:, :, 1:, :].float()\ntk0 = _get_cache_keys(trunc_cache, 0)[:, :, 1:, :].float()\nerr_layer0 = (bk0 - tk0).abs().max().item()\ncheck('layer 0 corrected keys match bare (4-bit tol)', err_layer0 < 0.1,\n      f'max_err={err_layer0:.6f}')\n\n# Layers >0: hidden states differ because doc tokens attended to prefix tokens,\n# so pre-RoPE keys differ. RoPE correction only fixes positional encoding.\n# We expect INCREASING divergence with layer depth.\nmax_errors = []\nfor li in range(NUM_LAYERS):\n    bk = _get_cache_keys(bare_cache, li)[:, :, 1:, :].float()\n    tk = _get_cache_keys(trunc_cache, li)[:, :, 1:, :].float()\n    err = (bk - tk).abs().max().item()\n    max_errors.append(err)\n\ncheck('key divergence increases with depth',\n      max_errors[0] < max_errors[-1],\n      f'layer0={max_errors[0]:.6f}, last={max_errors[-1]:.6f}')\n\n# BOS keys should be identical (position 0, same input embedding, causal = no attention)\nbos_errs = []\nfor li in range(NUM_LAYERS):\n    bk_bos = _get_cache_keys(bare_cache, li)[:, :, 0:1, :].float()\n    tk_bos = _get_cache_keys(trunc_cache, li)[:, :, 0:1, :].float()\n    bos_errs.append((bk_bos - tk_bos).abs().max().item())\n\nmean_bos_err = np.mean(bos_errs)\ncheck('BOS keys match between bare and trunc', mean_bos_err < 1e-4,\n      f'mean_bos_err={mean_bos_err:.6f}')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('='*70)\n",
    "print('TEST 7: Values differ between bare and truncated (prefix priming)')\n",
    "print('='*70)\n",
    "\n",
    "divergences = []\n",
    "for li in range(NUM_LAYERS):\n",
    "    bv = _get_cache_values(bare_cache, li)[:, :, 1:, :].float()\n",
    "    tv = _get_cache_values(trunc_cache, li)[:, :, 1:, :].float()\n",
    "    l2 = torch.norm(bv - tv).item() / bv.numel()**0.5\n",
    "    divergences.append(l2)\n",
    "\n",
    "check('values differ (prefix priming works)', max(divergences) > 0.01,\n",
    "      f'min={min(divergences):.6f}, max={max(divergences):.6f}, mean={np.mean(divergences):.6f}')\n",
    "\n",
    "# Layer 0 values should differ least (minimal cross-attention effect)\n",
    "# Later layers should diverge more\n",
    "check('layer 0 divergence < last layer divergence',\n",
    "      divergences[0] < divergences[-1],\n",
    "      f'layer0={divergences[0]:.6f}, last={divergences[-1]:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('='*70)\n",
    "print('TEST 8: build_truncated_kv_cache_corrected output length')\n",
    "print('='*70)\n",
    "\n",
    "surrogate = 'how much oxygen'\n",
    "keep_len8, cache8 = build_truncated_kv_cache_corrected(\n",
    "    surrogate, PASSAGE, model, tokenizer, config\n",
    ")\n",
    "\n",
    "surr_prefix = f'This document may be relevant to queries like: {surrogate}\\n\\n'\n",
    "doc_text8 = f'Document:\\n{PASSAGE}'\n",
    "prefix_enc8 = tokenizer(surr_prefix, return_tensors='pt', add_special_tokens=True)\n",
    "full_enc8 = tokenizer(surr_prefix + doc_text8, return_tensors='pt', add_special_tokens=True)\n",
    "expected_doc_len = full_enc8['input_ids'].shape[1] - prefix_enc8['input_ids'].shape[1]\n",
    "\n",
    "check('build_truncated_kv_cache_corrected: keep_len', keep_len8 == 1 + expected_doc_len,\n",
    "      f'got={keep_len8}, expected={1+expected_doc_len}')\n",
    "\n",
    "# Check cache is usable\n",
    "nll8 = score_answer_with_cache(\n",
    "    cache8, keep_len8, config.query_template.format(query=QUERY),\n",
    "    ANSWER, model, tokenizer, config\n",
    ")\n",
    "check('build_truncated_kv_cache_corrected: valid NLL', np.isfinite(nll8) and nll8 > 0,\n",
    "      f'nll={nll8:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('='*70)\n",
    "print('TEST 9: build_truncated_cache_variable_prefix')\n",
    "print('='*70)\n",
    "\n",
    "prefix9 = 'Random prefix text here'\n",
    "keep9, cache9, prefix_tok_len9 = build_truncated_cache_variable_prefix(\n",
    "    prefix9, PASSAGE, model, tokenizer, config\n",
    ")\n",
    "prefix_enc9 = tokenizer(prefix9 + ' ', return_tensors='pt', add_special_tokens=True)\n",
    "check('variable_prefix: prefix_token_len',\n",
    "      prefix_tok_len9 == prefix_enc9['input_ids'].shape[1],\n",
    "      f'got={prefix_tok_len9}, expected={prefix_enc9[\"input_ids\"].shape[1]}')\n",
    "check('variable_prefix: keep_len > 1', keep9 > 1)\n",
    "\n",
    "# Different prefixes -> different caches\n",
    "_, cacheA, _ = build_truncated_cache_variable_prefix(\n",
    "    'Hello world', PASSAGE, model, tokenizer, config\n",
    ")\n",
    "_, cacheB, _ = build_truncated_cache_variable_prefix(\n",
    "    'Completely different text about quantum physics and black holes',\n",
    "    PASSAGE, model, tokenizer, config\n",
    ")\n",
    "v_a = _get_cache_values(cacheA, NUM_LAYERS - 1).float()\n",
    "v_b = _get_cache_values(cacheB, NUM_LAYERS - 1).float()\n",
    "min_len = min(v_a.shape[2], v_b.shape[2])\n",
    "diff9 = (v_a[:, :, :min_len, :] - v_b[:, :, :min_len, :]).abs().mean().item()\n",
    "check('variable_prefix: different prefixes -> different values', diff9 > 0.001,\n",
    "      f'mean_diff={diff9:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('='*70)\n",
    "print('TEST 10: BPE boundary effect detection')\n",
    "print('='*70)\n",
    "\n",
    "doc10 = 'Document:\\nThe quick brown fox'\n",
    "prefix10 = 'Some prefix text '\n",
    "\n",
    "ids_alone = tokenizer(doc10, return_tensors='pt', add_special_tokens=True)['input_ids'][0]\n",
    "prefix_ids10 = tokenizer(prefix10, return_tensors='pt', add_special_tokens=True)['input_ids'][0]\n",
    "full_ids10 = tokenizer(prefix10 + doc10, return_tensors='pt', add_special_tokens=True)['input_ids'][0]\n",
    "doc_ids_from_full = full_ids10[len(prefix_ids10):]\n",
    "ids_alone_no_bos = ids_alone[1:]\n",
    "\n",
    "match = torch.equal(ids_alone_no_bos, doc_ids_from_full)\n",
    "print(f'  BPE tokens match: {match}')\n",
    "print(f'    Alone (no BOS): {ids_alone_no_bos.tolist()[:10]}')\n",
    "print(f'    From full:      {doc_ids_from_full.tolist()[:10]}')\n",
    "if not match:\n",
    "    print('  -> BPE splits DIFFER. build_matched_caches is NECESSARY.')\n",
    "else:\n",
    "    print('  -> BPE splits match for this example (may not hold for all inputs).')\n",
    "\n",
    "# Test with a case more likely to show boundary effects\n",
    "doc10b = 'Document:\\nelectric'\n",
    "prefix10b = 'an '\n",
    "ids_alone_b = tokenizer(doc10b, return_tensors='pt', add_special_tokens=True)['input_ids'][0]\n",
    "prefix_ids_b = tokenizer(prefix10b, return_tensors='pt', add_special_tokens=True)['input_ids'][0]\n",
    "full_ids_b = tokenizer(prefix10b + doc10b, return_tensors='pt', add_special_tokens=True)['input_ids'][0]\n",
    "doc_from_full_b = full_ids_b[len(prefix_ids_b):]\n",
    "alone_no_bos_b = ids_alone_b[1:]\n",
    "\n",
    "match_b = torch.equal(alone_no_bos_b, doc_from_full_b)\n",
    "print(f'  Case 2 BPE match: {match_b}')\n",
    "print(f'    Alone: {alone_no_bos_b.tolist()}')\n",
    "print(f'    Full:  {doc_from_full_b.tolist()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print('='*70)\nprint('TEST 11: Suffix cache passage portion matches bare')\nprint('='*70)\n\n# First: check determinism by running bare cache twice with same length\nbare_len11a, bare_cache11a = build_kv_cache(PASSAGE, model, tokenizer, config)\nbare_len11b, bare_cache11b = build_kv_cache(PASSAGE, model, tokenizer, config)\nbare_cache11a = _ensure_dynamic_cache(bare_cache11a)\nbare_cache11b = _ensure_dynamic_cache(bare_cache11b)\n\ndet_k_err = 0\ndet_v_err = 0\nfor li in range(NUM_LAYERS):\n    k_err = (_get_cache_keys(bare_cache11a, li).float() - _get_cache_keys(bare_cache11b, li).float()).abs().max().item()\n    v_err = (_get_cache_values(bare_cache11a, li).float() - _get_cache_values(bare_cache11b, li).float()).abs().max().item()\n    det_k_err = max(det_k_err, k_err)\n    det_v_err = max(det_v_err, v_err)\n\nprint(f'  Determinism (same length): max_k_err={det_k_err:.2e}, max_v_err={det_v_err:.2e}')\ncheck('bare cache is deterministic (same length)', det_k_err < 1e-5 and det_v_err < 1e-5,\n      f'k={det_k_err:.2e}, v={det_v_err:.2e}')\n\n# Now test suffix causal invariance\nsuffix_len11, suffix_cache11 = build_suffix_kv_cache(\n    PASSAGE, 'What is the oxygen percentage?', model, tokenizer, config\n)\nsuffix_cache11 = _ensure_dynamic_cache(suffix_cache11)\n\nmax_k_err = 0\nmax_v_err = 0\nfor li in range(NUM_LAYERS):\n    bk = _get_cache_keys(bare_cache11a, li).float()\n    sk = _get_cache_keys(suffix_cache11, li)[:, :, :bare_len11a, :].float()\n    bv = _get_cache_values(bare_cache11a, li).float()\n    sv = _get_cache_values(suffix_cache11, li)[:, :, :bare_len11a, :].float()\n    k_err = (bk - sk).abs().max().item()\n    v_err = (bv - sv).abs().max().item()\n    max_k_err = max(max_k_err, k_err)\n    max_v_err = max(max_v_err, v_err)\n\n# 4-bit quantized matmul produces different results for different total\n# sequence lengths even with identical prefix tokens and causal masking.\n# This is a known bitsandbytes limitation — the dequantization + GEMM\n# batching differs. Tolerance of 0.15 accommodates this.\ncheck('suffix passage matches bare (4-bit causal invariance)',\n      max_k_err < 0.15 and max_v_err < 0.15,\n      f'max_k_err={max_k_err:.2e}, max_v_err={max_v_err:.2e}')\n\n# The errors should be small and uniform across layers (not growing)\n# since it's quantization noise, not a systematic error\nlayer_k_errs = []\nfor li in range(NUM_LAYERS):\n    bk = _get_cache_keys(bare_cache11a, li).float()\n    sk = _get_cache_keys(suffix_cache11, li)[:, :, :bare_len11a, :].float()\n    layer_k_errs.append((bk - sk).abs().max().item())\n\ncheck('suffix errors are uniform (not growing with depth)',\n      max(layer_k_errs) < 3 * np.mean(layer_k_errs),\n      f'mean={np.mean(layer_k_errs):.4f}, max={max(layer_k_errs):.4f}')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n' + '='*70)\n",
    "print(f'SUMMARY: {passed}/{total} passed, {failed}/{total} failed')\n",
    "print('='*70)\n",
    "if failed > 0:\n",
    "    print('\\n*** FAILURES DETECTED - review output above ***')\n",
    "else:\n",
    "    print('\\nAll tests passed.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}