{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Production Simulation: Generated vs Static Surrogate Routing\n",
    "\n",
    "## Objective\n",
    "\n",
    "This notebook simulates a production deployment comparing **three approaches** to KV cache priming:\n",
    "\n",
    "1. **Baseline**: KV cache from document only (no surrogate)\n",
    "2. **Generated Surrogates**: 5 document-specific queries generated per document\n",
    "3. **Static Surrogates**: 5 fixed queries (same for all documents)\n",
    "\n",
    "## Key Question\n",
    "\n",
    "**Is document-specific surrogate generation worth the compute cost?**\n",
    "\n",
    "Or can we achieve similar benefits with static intent-priming queries?\n",
    "\n",
    "## Experimental Conditions\n",
    "\n",
    "| Condition | Description | Cost |\n",
    "|-----------|-------------|------|\n",
    "| **Baseline** | KV cache from document only | Lowest |\n",
    "| **Static Routed** | 5 fixed queries, route to best match | Low |\n",
    "| **Generated Routed** | 5 doc-specific queries, route to best match | Higher |\n",
    "\n",
    "## Production Simulation\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────────────┐\n",
    "│  OFFLINE ANALYSIS (done once)                                          │\n",
    "│    - Define 5 generated surrogate templates (Doc2Query-inspired)       │\n",
    "│    - Define 5 static surrogate queries (fixed intent signals)          │\n",
    "└─────────────────────────────────────────────────────────────────────────┘\n",
    "                                    │\n",
    "                                    ▼\n",
    "┌─────────────────────────────────────────────────────────────────────────┐\n",
    "│  INDEXING TIME (per document)                                          │\n",
    "│    - Build 1 baseline cache (doc only)                                 │\n",
    "│    - Generate 5 doc-specific surrogates → build 5 caches               │\n",
    "│    - Use 5 static surrogates → build 5 caches                          │\n",
    "│    - Total: 11 caches per document                                     │\n",
    "└─────────────────────────────────────────────────────────────────────────┘\n",
    "                                    │\n",
    "                                    ▼\n",
    "┌─────────────────────────────────────────────────────────────────────────┐\n",
    "│  QUERY TIME                                                            │\n",
    "│    - User query arrives                                                │\n",
    "│    - Route to best-matching cache (by surrogate similarity)            │\n",
    "│    - Compare: Baseline vs Generated-Routed vs Static-Routed            │\n",
    "└─────────────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Step 1: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install transformers torch datasets tqdm scipy bitsandbytes accelerate matplotlib sentence-transformers -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports complete.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from datasets import load_dataset\n",
    "from tqdm.auto import tqdm\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import json\n",
    "from collections import Counter\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, List, Dict, Tuple\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "print(\"Imports complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Model: mistralai/Mistral-7B-Instruct-v0.2\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class ExperimentConfig:\n",
    "    \"\"\"Configuration for the production simulation experiment.\"\"\"\n",
    "    \n",
    "    # Model settings\n",
    "    model_name: str = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "    use_4bit: bool = True\n",
    "    \n",
    "    # Dataset settings\n",
    "    dataset_name: str = \"ms_marco\"\n",
    "    dataset_config: str = \"v1.1\"\n",
    "    dataset_split: str = \"validation\"\n",
    "    num_samples: int = 2500\n",
    "    min_passage_words: int = 50\n",
    "    max_passage_words: int = 300\n",
    "    \n",
    "    # Surrogate generation\n",
    "    surrogate_max_tokens: int = 45\n",
    "    \n",
    "    # Cache templates\n",
    "    baseline_cache_template: str = \"Document:\\n{document}\"\n",
    "    surrogate_cache_template: str = (\n",
    "        \"This document may be relevant to queries like: {surrogate}\\n\\n\"\n",
    "        \"Document:\\n{document}\"\n",
    "    )\n",
    "    query_template: str = \"\\n\\nQuery: {query}\\n\\nAnswer:\"\n",
    "    \n",
    "    # Embedding model\n",
    "    embedding_model_name: str = \"all-MiniLM-L6-v2\"\n",
    "    \n",
    "    # Random seed\n",
    "    seed: int = 42\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "config = ExperimentConfig()\n",
    "print(f\"Device: {config.device}\")\n",
    "print(f\"Model: {config.model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading language model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b04beee37c74401b2efef0f0cab5192",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/291 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language model loaded: 7,241,732,096 parameters\n",
      "\n",
      "Loading embedding model: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ee4b1fdbe434e4b8ec3e442c35f8bd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding model loaded.\n"
     ]
    }
   ],
   "source": [
    "# Set seeds and load models\n",
    "torch.manual_seed(config.seed)\n",
    "np.random.seed(config.seed)\n",
    "\n",
    "print(\"Loading language model...\")\n",
    "if config.use_4bit:\n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\"\n",
    "    )\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        config.model_name,\n",
    "        quantization_config=quantization_config,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "else:\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        config.model_name,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.model_name, trust_remote_code=True)\n",
    "tokenizer.padding_side = \"right\"\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "model.eval()\n",
    "print(f\"Language model loaded: {model.num_parameters():,} parameters\")\n",
    "\n",
    "print(f\"\\nLoading embedding model: {config.embedding_model_name}\")\n",
    "embed_model = SentenceTransformer(config.embedding_model_name)\n",
    "print(\"Embedding model loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## Step 2: Analyze MS MARCO to Identify Top Query Patterns\n",
    "\n",
    "Before defining our 5 surrogate templates, let's analyze the actual query distribution in MS MARCO to make informed choices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_query_patterns(dataset, num_queries: int = 5000) -> Dict:\n",
    "    \"\"\"\n",
    "    Analyze query patterns in MS MARCO to identify top query types.\n",
    "    \"\"\"\n",
    "    print(f\"Analyzing {num_queries} queries...\")\n",
    "    \n",
    "    # Collect queries\n",
    "    queries = []\n",
    "    for item in dataset:\n",
    "        if item.get('query'):\n",
    "            queries.append(item['query'].lower())\n",
    "        if len(queries) >= num_queries:\n",
    "            break\n",
    "    \n",
    "    # Analyze query starters (first word or first two words)\n",
    "    first_words = Counter()\n",
    "    first_two_words = Counter()\n",
    "    \n",
    "    # Query type patterns\n",
    "    patterns = {\n",
    "        'what_is': 0,        # \"what is\", \"what are\", \"what does\"\n",
    "        'how_to': 0,         # \"how to\", \"how do\", \"how does\"\n",
    "        'how_much': 0,       # \"how much\", \"how many\", \"how long\"\n",
    "        'who_when_where': 0, # \"who\", \"when\", \"where\"\n",
    "        'why': 0,            # \"why\"\n",
    "        'define': 0,         # \"define\", \"definition\", \"meaning\"\n",
    "        'cost_price': 0,     # \"cost\", \"price\", \"salary\", \"pay\"\n",
    "        'average': 0,        # \"average\"\n",
    "        'best': 0,           # \"best\"\n",
    "        'can_does': 0,       # \"can\", \"does\", \"is\"\n",
    "    }\n",
    "    \n",
    "    for query in queries:\n",
    "        words = query.split()\n",
    "        if words:\n",
    "            first_words[words[0]] += 1\n",
    "            if len(words) >= 2:\n",
    "                first_two_words[f\"{words[0]} {words[1]}\"] += 1\n",
    "        \n",
    "        # Categorize by pattern\n",
    "        if query.startswith(('what is', 'what are', 'what does', 'what do')):\n",
    "            patterns['what_is'] += 1\n",
    "        elif query.startswith(('how to', 'how do', 'how does')):\n",
    "            patterns['how_to'] += 1\n",
    "        elif query.startswith(('how much', 'how many', 'how long', 'how far')):\n",
    "            patterns['how_much'] += 1\n",
    "        elif query.startswith(('who ', 'when ', 'where ')):\n",
    "            patterns['who_when_where'] += 1\n",
    "        elif query.startswith('why'):\n",
    "            patterns['why'] += 1\n",
    "        elif any(w in query for w in ['define', 'definition', 'meaning of']):\n",
    "            patterns['define'] += 1\n",
    "        elif any(w in query for w in ['cost', 'price', 'salary', 'pay', 'earn']):\n",
    "            patterns['cost_price'] += 1\n",
    "        elif 'average' in query:\n",
    "            patterns['average'] += 1\n",
    "        elif query.startswith('best'):\n",
    "            patterns['best'] += 1\n",
    "        elif query.startswith(('can ', 'does ', 'is ', 'are ')):\n",
    "            patterns['can_does'] += 1\n",
    "    \n",
    "    return {\n",
    "        'total_queries': len(queries),\n",
    "        'first_words': first_words.most_common(20),\n",
    "        'first_two_words': first_two_words.most_common(20),\n",
    "        'patterns': patterns,\n",
    "        'sample_queries': queries[:20],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cell-8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MS MARCO dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since ms_marco couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'v1.1' at /home/petergrabowski_google_com/.cache/huggingface/datasets/ms_marco/v1.1/0.0.0/a47ee7aae8d7d466ba15f9f0bfac3b3681087b3a (last modified on Mon Jan 26 15:46:33 2026).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded: 10047 samples\n",
      "Analyzing 5000 queries...\n",
      "\n",
      "================================================================================\n",
      "MS MARCO QUERY PATTERN ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "Total queries analyzed: 5000\n",
      "\n",
      "--- Top 15 First Words ---\n",
      "  what             2041 (40.8%)\n",
      "  how               794 (15.9%)\n",
      "  where             197 (3.9%)\n",
      "  average           165 (3.3%)\n",
      "  is                140 (2.8%)\n",
      "  cost               95 (1.9%)\n",
      "  why                88 (1.8%)\n",
      "  when               76 (1.5%)\n",
      "  who                71 (1.4%)\n",
      "  can                70 (1.4%)\n",
      "  does               60 (1.2%)\n",
      "  which              41 (0.8%)\n",
      "  meaning            39 (0.8%)\n",
      "  do                 35 (0.7%)\n",
      "  are                30 (0.6%)\n",
      "\n",
      "--- Top 15 First Two Words ---\n",
      "  what is               1099 (22.0%)\n",
      "  how much               230 (4.6%)\n",
      "  how long               223 (4.5%)\n",
      "  what does              216 (4.3%)\n",
      "  what are               162 (3.2%)\n",
      "  how to                 141 (2.8%)\n",
      "  where is               107 (2.1%)\n",
      "  cost of                 56 (1.1%)\n",
      "  what kind               49 (1.0%)\n",
      "  average cost            48 (1.0%)\n",
      "  how many                48 (1.0%)\n",
      "  what type               40 (0.8%)\n",
      "  what causes             37 (0.7%)\n",
      "  meaning of              36 (0.7%)\n",
      "  how do                  31 (0.6%)\n",
      "\n",
      "--- Query Type Patterns ---\n",
      "  what_is               1507 (30.1%)\n",
      "  how_much               514 (10.3%)\n",
      "  cost_price             376 (7.5%)\n",
      "  who_when_where         344 (6.9%)\n",
      "  can_does               287 (5.7%)\n",
      "  define                 215 (4.3%)\n",
      "  how_to                 189 (3.8%)\n",
      "  why                     88 (1.8%)\n",
      "  average                 71 (1.4%)\n",
      "  best                     8 (0.2%)\n",
      "\n",
      "--- Sample Queries ---\n",
      "  - walgreens store sales average\n",
      "  - how much do bartenders make\n",
      "  - what is a furuncle boil\n",
      "  - what can urinalysis detect\n",
      "  - what is vitamin a used for\n",
      "  - what causes genetic alterations in normal cells\n",
      "  - cost to frame basement\n",
      "  - erudite divergent definition\n",
      "  - why is albumin normally absent in urine\n",
      "  - where was movie the birds filmed\n"
     ]
    }
   ],
   "source": [
    "# Load dataset and analyze\n",
    "print(\"Loading MS MARCO dataset...\")\n",
    "full_dataset = load_dataset(\n",
    "    config.dataset_name,\n",
    "    config.dataset_config,\n",
    "    split=config.dataset_split,\n",
    ")\n",
    "print(f\"Dataset loaded: {len(full_dataset)} samples\")\n",
    "\n",
    "# Analyze query patterns\n",
    "query_analysis = analyze_query_patterns(full_dataset, num_queries=5000)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MS MARCO QUERY PATTERN ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nTotal queries analyzed: {query_analysis['total_queries']}\")\n",
    "\n",
    "print(\"\\n--- Top 15 First Words ---\")\n",
    "for word, count in query_analysis['first_words'][:15]:\n",
    "    pct = count / query_analysis['total_queries'] * 100\n",
    "    print(f\"  {word:<15} {count:>5} ({pct:.1f}%)\")\n",
    "\n",
    "print(\"\\n--- Top 15 First Two Words ---\")\n",
    "for words, count in query_analysis['first_two_words'][:15]:\n",
    "    pct = count / query_analysis['total_queries'] * 100\n",
    "    print(f\"  {words:<20} {count:>5} ({pct:.1f}%)\")\n",
    "\n",
    "print(\"\\n--- Query Type Patterns ---\")\n",
    "sorted_patterns = sorted(query_analysis['patterns'].items(), key=lambda x: x[1], reverse=True)\n",
    "for pattern, count in sorted_patterns:\n",
    "    pct = count / query_analysis['total_queries'] * 100\n",
    "    print(f\"  {pattern:<20} {count:>5} ({pct:.1f}%)\")\n",
    "\n",
    "print(\"\\n--- Sample Queries ---\")\n",
    "for q in query_analysis['sample_queries'][:10]:\n",
    "    print(f\"  - {q}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## Step 3: Define the Top 5 Surrogate Query Templates\n",
    "\n",
    "Based on the Doc2Query philosophy and real MS MARCO user behavior analysis, we define 5 surrogate generation templates designed to capture the **vocabulary mismatch** problem:\n",
    "\n",
    "**The Core Insight**: Documents are written in formal, solution-oriented language, but users search with:\n",
    "- Informal keywords and fragments\n",
    "- Problem/symptom descriptions (not solution terms)\n",
    "- Misconceptions and negative framings\n",
    "- Messy, real-world language with typos and abbreviations\n",
    "\n",
    "| # | Template Type | Purpose | Example |\n",
    "|---|---------------|---------|---------|\n",
    "| 1 | **Target Natural Language Question** | Ideal grammatically-correct question the doc answers | \"What are the health benefits of green tea?\" |\n",
    "| 2 | **Keyword-ese Query** | How users actually type (keywords, no grammar) | \"green tea benefits weight loss\" |\n",
    "| 3 | **Symptom/Scenario Query** | The *problem* the user has, not the solution | \"feeling tired all the time natural remedies\" |\n",
    "| 4 | **Misconception/Negative Query** | What NOT to do, common myths | \"is green tea bad for you caffeine\" |\n",
    "| 5 | **Messy Real-World Query** | Abbreviations, slang, typos, urgency | \"best tea 4 energy need help asap\" |\n",
    "\n",
    "This approach ensures the document can be retrieved regardless of how the user phrases their query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cell-10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defined 5 Surrogate Templates (Doc2Query-inspired):\n",
      "======================================================================\n",
      "\n",
      "TARGET_QUESTION\n",
      "  Name: Target Natural Language Question\n",
      "  Purpose: The ideal, grammatically-correct question this document perfectly answers\n",
      "\n",
      "KEYWORD_QUERY\n",
      "  Name: Keyword-ese Query\n",
      "  Purpose: How users actually search: keyword strings without full sentences\n",
      "\n",
      "SYMPTOM_SCENARIO\n",
      "  Name: Symptom/Scenario Query\n",
      "  Purpose: The problem or symptom the user has, not the solution they need\n",
      "\n",
      "MISCONCEPTION_NEGATIVE\n",
      "  Name: Misconception/Negative Query\n",
      "  Purpose: Questions about what NOT to do, common myths, or concerns\n",
      "\n",
      "MESSY_REALWORLD\n",
      "  Name: Messy Real-World Query\n",
      "  Purpose: Abbreviations, slang, typos, urgency - how people really search\n"
     ]
    }
   ],
   "source": [
    "# Define the 5 surrogate generation templates\n",
    "# These are designed based on Doc2Query philosophy and real MS MARCO user behavior\n",
    "\n",
    "TOP_5_SURROGATE_TEMPLATES = {\n",
    "    'target_question': {\n",
    "        'name': 'Target Natural Language Question',\n",
    "        'description': 'The ideal, grammatically-correct question this document perfectly answers',\n",
    "        'prompt': (\n",
    "            \"You are helping index a document for search. Write the single most likely \"\n",
    "            \"natural language question that a user would ask that this document perfectly answers. \"\n",
    "            \"The question should be grammatically correct, clear, and specific. \"\n",
    "            \"Output only the question (5-12 words), nothing else.\\n\\n\"\n",
    "            \"Document:\"\n",
    "        ),\n",
    "    },\n",
    "    'keyword_query': {\n",
    "        'name': 'Keyword-ese Query',\n",
    "        'description': 'How users actually search: keyword strings without full sentences',\n",
    "        'prompt': (\n",
    "            \"You are helping index a document for search. Write a search query the way \"\n",
    "            \"real users type into Google: just keywords, no complete sentences, no question marks. \"\n",
    "            \"Think of someone quickly typing a few relevant words. \"\n",
    "            \"Output only the keyword query (3-6 words), nothing else.\\n\\n\"\n",
    "            \"Document:\"\n",
    "        ),\n",
    "    },\n",
    "    'symptom_scenario': {\n",
    "        'name': 'Symptom/Scenario Query',\n",
    "        'description': 'The problem or symptom the user has, not the solution they need',\n",
    "        'prompt': (\n",
    "            \"You are helping index a document for search. This document contains a solution or answer. \"\n",
    "            \"Write a query that describes the PROBLEM or SYMPTOM that would lead someone to need this document. \"\n",
    "            \"Focus on what the user is experiencing, not what they want to learn. \"\n",
    "            \"For example, if the doc is about 'fixing a leaky faucet', the query might be 'water dripping from sink handle'. \"\n",
    "            \"Output only the problem-focused query (4-10 words), nothing else.\\n\\n\"\n",
    "            \"Document:\"\n",
    "        ),\n",
    "    },\n",
    "    'misconception_negative': {\n",
    "        'name': 'Misconception/Negative Query',\n",
    "        'description': 'Questions about what NOT to do, common myths, or concerns',\n",
    "        'prompt': (\n",
    "            \"You are helping index a document for search. Write a query that reflects \"\n",
    "            \"a common misconception, concern, or 'what NOT to do' question related to this topic. \"\n",
    "            \"Think of someone who is worried, skeptical, or wants to avoid mistakes. \"\n",
    "            \"Examples: 'is X bad for you', 'X side effects', 'mistakes to avoid with X', 'X myths'. \"\n",
    "            \"Output only the concern/negative query (4-10 words), nothing else.\\n\\n\"\n",
    "            \"Document:\"\n",
    "        ),\n",
    "    },\n",
    "    'messy_realworld': {\n",
    "        'name': 'Messy Real-World Query',\n",
    "        'description': 'Abbreviations, slang, typos, urgency - how people really search',\n",
    "        'prompt': (\n",
    "            \"You are helping index a document for search. Write a messy, realistic search query \"\n",
    "            \"like someone would actually type in a hurry: use common abbreviations (info, govt, diff, etc), \"\n",
    "            \"internet slang, minor typos, or urgent language (help, asap, need, plz). \"\n",
    "            \"Make it feel like a real person typing quickly on their phone. \"\n",
    "            \"Output only the messy query (3-8 words), nothing else.\\n\\n\"\n",
    "            \"Document:\"\n",
    "        ),\n",
    "    },\n",
    "}\n",
    "\n",
    "print(\"Defined 5 Surrogate Templates (Doc2Query-inspired):\")\n",
    "print(\"=\"*70)\n",
    "for key, template in TOP_5_SURROGATE_TEMPLATES.items():\n",
    "    print(f\"\\n{key.upper()}\")\n",
    "    print(f\"  Name: {template['name']}\")\n",
    "    print(f\"  Purpose: {template['description']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2svwyz0y4bu",
   "metadata": {},
   "source": [
    "## Step 3b: Define 5 Static Surrogate Queries\n",
    "\n",
    "In addition to document-specific generated surrogates, we test **static queries** that are the same for every document. This serves as a control to measure:\n",
    "\n",
    "1. **How much value comes from document-specific generation** vs. generic intent priming\n",
    "2. **Whether simple intent signals** are sufficient to improve cache quality\n",
    "\n",
    "The static queries are designed to cover the main intent categories in MS MARCO:\n",
    "\n",
    "| # | Static Query | Intent Coverage |\n",
    "|---|--------------|-----------------|\n",
    "| 1 | \"What is this and what does it mean?\" | Definitional (what is, define, meaning) |\n",
    "| 2 | \"How do I do this step by step?\" | Procedural (how to, how do, instructions) |\n",
    "| 3 | \"How much does this cost or how long does it take?\" | Quantitative (how much, how many, cost, time) |\n",
    "| 4 | \"What are the key facts I need to know?\" | Factual (who, when, where, details) |\n",
    "| 5 | \"What problem does this solve?\" | Problem/Solution (why, troubleshooting, help)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ip0jh979i9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defined 5 Static Surrogate Queries:\n",
      "======================================================================\n",
      "\n",
      "STATIC_DEFINITIONAL\n",
      "  Query: \"What is this and what does it mean?\"\n",
      "  Covers: what is, define, meaning, explanation queries\n",
      "\n",
      "STATIC_PROCEDURAL\n",
      "  Query: \"How do I do this step by step?\"\n",
      "  Covers: how to, how do, instructions, guide queries\n",
      "\n",
      "STATIC_QUANTITATIVE\n",
      "  Query: \"How much does this cost or how long does it take?\"\n",
      "  Covers: how much, how many, cost, price, duration queries\n",
      "\n",
      "STATIC_FACTUAL\n",
      "  Query: \"What are the key facts I need to know?\"\n",
      "  Covers: who, when, where, what, factual detail queries\n",
      "\n",
      "STATIC_PROBLEM\n",
      "  Query: \"What problem does this solve?\"\n",
      "  Covers: why, troubleshooting, help, problem-solving queries\n"
     ]
    }
   ],
   "source": [
    "# Define 5 static surrogate queries (same for every document)\n",
    "# These cover the main intent categories without being document-specific\n",
    "\n",
    "STATIC_SURROGATE_QUERIES = {\n",
    "    'static_definitional': {\n",
    "        'name': 'Static: Definitional Intent',\n",
    "        'query': 'What is this and what does it mean?',\n",
    "        'covers': 'what is, define, meaning, explanation queries',\n",
    "    },\n",
    "    'static_procedural': {\n",
    "        'name': 'Static: Procedural Intent', \n",
    "        'query': 'How do I do this step by step?',\n",
    "        'covers': 'how to, how do, instructions, guide queries',\n",
    "    },\n",
    "    'static_quantitative': {\n",
    "        'name': 'Static: Quantitative Intent',\n",
    "        'query': 'How much does this cost or how long does it take?',\n",
    "        'covers': 'how much, how many, cost, price, duration queries',\n",
    "    },\n",
    "    'static_factual': {\n",
    "        'name': 'Static: Factual Intent',\n",
    "        'query': 'What are the key facts I need to know?',\n",
    "        'covers': 'who, when, where, what, factual detail queries',\n",
    "    },\n",
    "    'static_problem': {\n",
    "        'name': 'Static: Problem/Solution Intent',\n",
    "        'query': 'What problem does this solve?',\n",
    "        'covers': 'why, troubleshooting, help, problem-solving queries',\n",
    "    },\n",
    "}\n",
    "\n",
    "print(\"Defined 5 Static Surrogate Queries:\")\n",
    "print(\"=\"*70)\n",
    "for key, info in STATIC_SURROGATE_QUERIES.items():\n",
    "    print(f\"\\n{key.upper()}\")\n",
    "    print(f\"  Query: \\\"{info['query']}\\\"\")\n",
    "    print(f\"  Covers: {info['covers']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## Step 4: Core Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_surrogate_with_template(\n",
    "    doc_text: str,\n",
    "    template_prompt: str,\n",
    "    model: AutoModelForCausalLM,\n",
    "    tokenizer: AutoTokenizer,\n",
    "    config: ExperimentConfig\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Generate a surrogate query using a specific template.\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"{template_prompt}\\n\\nText:\\n{doc_text}\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(config.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=config.surrogate_max_tokens,\n",
    "            temperature=0.3,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    generated_tokens = outputs[0][inputs['input_ids'].shape[1]:]\n",
    "    surrogate = tokenizer.decode(generated_tokens, skip_special_tokens=True).strip()\n",
    "    surrogate = surrogate.strip('\"\\'')\n",
    "    surrogate = surrogate.split('\\n')[0].strip()\n",
    "    \n",
    "    return surrogate\n",
    "\n",
    "\n",
    "def generate_all_5_surrogates(\n",
    "    doc_text: str,\n",
    "    model: AutoModelForCausalLM,\n",
    "    tokenizer: AutoTokenizer,\n",
    "    config: ExperimentConfig\n",
    ") -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Generate all 5 surrogates for a document using the top 5 templates.\n",
    "    \"\"\"\n",
    "    surrogates = {}\n",
    "    for key, template in TOP_5_SURROGATE_TEMPLATES.items():\n",
    "        surrogates[key] = generate_surrogate_with_template(\n",
    "            doc_text, template['prompt'], model, tokenizer, config\n",
    "        )\n",
    "    return surrogates\n",
    "\n",
    "\n",
    "def compute_similarity(text1: str, text2: str, embed_model: SentenceTransformer) -> float:\n",
    "    \"\"\"Compute semantic similarity between two texts.\"\"\"\n",
    "    embeddings = embed_model.encode([text1, text2])\n",
    "    return float(cosine_similarity([embeddings[0]], [embeddings[1]])[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_kv_cache(\n",
    "    context: str,\n",
    "    model: AutoModelForCausalLM,\n",
    "    tokenizer: AutoTokenizer,\n",
    "    config: ExperimentConfig\n",
    ") -> Tuple[int, any]:\n",
    "    \"\"\"\n",
    "    Build a KV cache from the given context.\n",
    "    Returns (context_length, past_key_values).\n",
    "    \"\"\"\n",
    "    context_encoding = tokenizer(\n",
    "        context, return_tensors=\"pt\", add_special_tokens=True,\n",
    "        padding=False, truncation=False\n",
    "    )\n",
    "    context_ids = context_encoding['input_ids'].to(config.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            input_ids=context_ids,\n",
    "            attention_mask=torch.ones_like(context_ids),\n",
    "            use_cache=True,\n",
    "            return_dict=True\n",
    "        )\n",
    "    \n",
    "    return context_ids.shape[1], outputs.past_key_values\n",
    "\n",
    "\n",
    "def score_answer_with_cache(\n",
    "    past_key_values: any,\n",
    "    context_len: int,\n",
    "    query_prompt: str,\n",
    "    answer: str,\n",
    "    model: AutoModelForCausalLM,\n",
    "    tokenizer: AutoTokenizer,\n",
    "    config: ExperimentConfig\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Score an answer using a pre-built KV cache.\n",
    "    Returns mean NLL.\n",
    "    \"\"\"\n",
    "    # Tokenize query\n",
    "    query_encoding = tokenizer(\n",
    "        query_prompt, return_tensors=\"pt\", add_special_tokens=False,\n",
    "        padding=False, truncation=False\n",
    "    )\n",
    "    query_ids = query_encoding['input_ids'].to(config.device)\n",
    "    query_len = query_ids.shape[1]\n",
    "    \n",
    "    # Tokenize answer\n",
    "    answer_encoding = tokenizer(\n",
    "        answer, return_tensors=\"pt\", add_special_tokens=False,\n",
    "        padding=False, truncation=False\n",
    "    )\n",
    "    answer_ids = answer_encoding['input_ids'].to(config.device)\n",
    "    answer_len = answer_ids.shape[1]\n",
    "    \n",
    "    # Extend cache with query\n",
    "    combined_len = context_len + query_len\n",
    "    attention_mask = torch.ones((1, combined_len), device=config.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        query_outputs = model(\n",
    "            input_ids=query_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            past_key_values=past_key_values,\n",
    "            use_cache=True,\n",
    "            return_dict=True\n",
    "        )\n",
    "        extended_cache = query_outputs.past_key_values\n",
    "    \n",
    "    # Score answer\n",
    "    combined_len_final = context_len + query_len + answer_len\n",
    "    attention_mask_final = torch.ones((1, combined_len_final), device=config.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        answer_outputs = model(\n",
    "            input_ids=answer_ids,\n",
    "            attention_mask=attention_mask_final,\n",
    "            past_key_values=extended_cache,\n",
    "            use_cache=False,\n",
    "            return_dict=True\n",
    "        )\n",
    "    \n",
    "    # Compute NLL\n",
    "    logits = answer_outputs.logits\n",
    "    shift_logits = logits[:, :-1, :].contiguous()\n",
    "    shift_labels = answer_ids[:, 1:].contiguous()\n",
    "    shift_logits = shift_logits.view(-1, shift_logits.size(-1))\n",
    "    shift_labels = shift_labels.view(-1)\n",
    "    \n",
    "    loss_fct = torch.nn.CrossEntropyLoss(reduction='sum')\n",
    "    nll = loss_fct(shift_logits, shift_labels).item()\n",
    "    \n",
    "    num_scored = answer_len - 1\n",
    "    return nll / num_scored if num_scored > 0 else 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## Step 5: Load Evaluation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cell-15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering samples with answers...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0346e4a4c2b4f778f08f4644c90b34a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filtering:   0%|          | 0/10047 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 2500 samples\n",
      "\n",
      "Sample preview:\n",
      "  Query: what temperature should it be to plant grass seeds\n",
      "  Passage: Usually planted in the early fall, cool-season grass seeds prefer daytime temperatures ranging from 60 to 75 F. With this temperature range, the soil ...\n",
      "  Answer: Between 50deg and 65deg F\n"
     ]
    }
   ],
   "source": [
    "def count_words(text: str) -> int:\n",
    "    return len(text.split())\n",
    "\n",
    "\n",
    "def load_evaluation_samples(dataset, config: ExperimentConfig) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Load samples for evaluation with passage, query, and answer.\n",
    "    \"\"\"\n",
    "    print(\"Filtering samples with answers...\")\n",
    "    \n",
    "    filtered_samples = []\n",
    "    \n",
    "    for item in tqdm(dataset, desc=\"Filtering\"):\n",
    "        passages = item.get('passages', {})\n",
    "        passage_texts = passages.get('passage_text', [])\n",
    "        is_selected = passages.get('is_selected', [])\n",
    "        \n",
    "        query = item.get('query', '')\n",
    "        answers = item.get('answers', [])\n",
    "        well_formed = item.get('wellFormedAnswers', [])\n",
    "        \n",
    "        if not passage_texts or not query:\n",
    "            continue\n",
    "        \n",
    "        # Get best answer\n",
    "        if well_formed and len(well_formed) > 0 and well_formed[0] != '[]':\n",
    "            answer = well_formed[0]\n",
    "        elif answers and len(answers) > 0 and answers[0] != 'No Answer Present.':\n",
    "            answer = answers[0]\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        # Find valid passage\n",
    "        for i, passage in enumerate(passage_texts):\n",
    "            word_count = count_words(passage)\n",
    "            if config.min_passage_words <= word_count <= config.max_passage_words:\n",
    "                if is_selected and i < len(is_selected) and is_selected[i] == 1:\n",
    "                    filtered_samples.append({\n",
    "                        'passage': passage,\n",
    "                        'query': query,\n",
    "                        'answer': answer\n",
    "                    })\n",
    "                    break\n",
    "        \n",
    "        if len(filtered_samples) >= config.num_samples * 2:\n",
    "            break\n",
    "    \n",
    "    np.random.shuffle(filtered_samples)\n",
    "    filtered_samples = filtered_samples[:config.num_samples]\n",
    "    \n",
    "    print(f\"Selected {len(filtered_samples)} samples\")\n",
    "    return filtered_samples\n",
    "\n",
    "\n",
    "# Load samples\n",
    "samples = load_evaluation_samples(full_dataset, config)\n",
    "\n",
    "print(\"\\nSample preview:\")\n",
    "print(f\"  Query: {samples[0]['query']}\")\n",
    "print(f\"  Passage: {samples[0]['passage'][:150]}...\")\n",
    "print(f\"  Answer: {samples[0]['answer']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## Step 6: Test Surrogate Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cell-17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing surrogate generation with all 5 templates...\n",
      "================================================================================\n",
      "\n",
      "Document: Usually planted in the early fall, cool-season grass seeds prefer daytime temperatures ranging from 60 to 75 F. With this temperature range, the soil itself is usually between 50 and 65 F -- the right...\n",
      "\n",
      "Actual Query: what temperature should it be to plant grass seeds\n",
      "\n",
      "Generated Surrogates:\n",
      "------------------------------------------------------------\n",
      "\n",
      "Target Natural Language Question:\n",
      "  Surrogate: What is the ideal temperature range for planting cool-season grass seeds and why can warm-season grasses fail if planted during spring with cold temperatures?\n",
      "  Similarity to actual query: 0.8349\n",
      "\n",
      "Keyword-ese Query:\n",
      "  Surrogate: cool-season grass seeds, fall planting, 60-75 F temperature, germination, right environment, grass seeds failure, warm-season grasses, spring planting, cold snap, weed seeds.\n",
      "  Similarity to actual query: 0.6482\n",
      "\n",
      "Symptom/Scenario Query:\n",
      "  Surrogate: Grass seeds not germinating in cool soil (below 50°F)\n",
      "  Similarity to actual query: 0.7590\n",
      "\n",
      "Misconception/Negative Query:\n",
      "  Surrogate: What not to plant cool-season grasses in spring?\"\n",
      "  Similarity to actual query: 0.5399\n",
      "\n",
      "Messy Real-World Query:\n",
      "  Surrogate: Help me find cool-season grass temp range for planting, plz! ASAP. Germination fail if temp wrong, diff from warm-season seeds. Urgent!\n",
      "  Similarity to actual query: 0.7158\n",
      "\n",
      "Best matching template: target_question\n"
     ]
    }
   ],
   "source": [
    "# Test surrogate generation on first sample\n",
    "print(\"Testing surrogate generation with all 5 templates...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "test_sample = samples[0]\n",
    "print(f\"\\nDocument: {test_sample['passage'][:200]}...\")\n",
    "print(f\"\\nActual Query: {test_sample['query']}\")\n",
    "\n",
    "test_surrogates = generate_all_5_surrogates(\n",
    "    test_sample['passage'], model, tokenizer, config\n",
    ")\n",
    "\n",
    "print(\"\\nGenerated Surrogates:\")\n",
    "print(\"-\"*60)\n",
    "for key, surrogate in test_surrogates.items():\n",
    "    similarity = compute_similarity(surrogate, test_sample['query'], embed_model)\n",
    "    print(f\"\\n{TOP_5_SURROGATE_TEMPLATES[key]['name']}:\")\n",
    "    print(f\"  Surrogate: {surrogate}\")\n",
    "    print(f\"  Similarity to actual query: {similarity:.4f}\")\n",
    "\n",
    "# Find best match\n",
    "best_key = max(test_surrogates.keys(), \n",
    "               key=lambda k: compute_similarity(test_surrogates[k], test_sample['query'], embed_model))\n",
    "print(f\"\\nBest matching template: {best_key}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## Step 7: Run the Main Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_sample_with_top5_routing(\n",
    "    sample: Dict,\n",
    "    model: AutoModelForCausalLM,\n",
    "    tokenizer: AutoTokenizer,\n",
    "    embed_model: SentenceTransformer,\n",
    "    config: ExperimentConfig\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Evaluate a single sample with baseline, generated surrogates, and static surrogates.\n",
    "    \n",
    "    Returns detailed results including:\n",
    "    - Baseline NLL\n",
    "    - NLL for each of the 5 generated surrogate caches\n",
    "    - NLL for each of the 5 static surrogate caches\n",
    "    - Routed NLL (best-matched cache) for both generated and static\n",
    "    - Oracle NLL (best cache if we knew) for both generated and static\n",
    "    \"\"\"\n",
    "    passage = sample['passage']\n",
    "    query = sample['query']\n",
    "    answer = sample['answer']\n",
    "    \n",
    "    # ========== GENERATED SURROGATES (document-specific) ==========\n",
    "    generated_surrogates = generate_all_5_surrogates(passage, model, tokenizer, config)\n",
    "    \n",
    "    # Compute similarities to actual query for generated surrogates\n",
    "    generated_similarities = {\n",
    "        key: compute_similarity(surrogate, query, embed_model)\n",
    "        for key, surrogate in generated_surrogates.items()\n",
    "    }\n",
    "    \n",
    "    # ========== STATIC SURROGATES (same for all documents) ==========\n",
    "    static_surrogates = {key: info['query'] for key, info in STATIC_SURROGATE_QUERIES.items()}\n",
    "    \n",
    "    # Compute similarities to actual query for static surrogates\n",
    "    static_similarities = {\n",
    "        key: compute_similarity(surrogate, query, embed_model)\n",
    "        for key, surrogate in static_surrogates.items()\n",
    "    }\n",
    "    \n",
    "    # ========== BUILD CACHES ==========\n",
    "    \n",
    "    # Baseline cache (document only)\n",
    "    baseline_context = config.baseline_cache_template.format(document=passage)\n",
    "    baseline_len, baseline_cache = build_kv_cache(baseline_context, model, tokenizer, config)\n",
    "    \n",
    "    # Generated surrogate caches\n",
    "    generated_caches = {}\n",
    "    generated_lens = {}\n",
    "    for key, surrogate in generated_surrogates.items():\n",
    "        context = config.surrogate_cache_template.format(surrogate=surrogate, document=passage)\n",
    "        cache_len, cache = build_kv_cache(context, model, tokenizer, config)\n",
    "        generated_caches[key] = cache\n",
    "        generated_lens[key] = cache_len\n",
    "    \n",
    "    # Static surrogate caches\n",
    "    static_caches = {}\n",
    "    static_lens = {}\n",
    "    for key, surrogate in static_surrogates.items():\n",
    "        context = config.surrogate_cache_template.format(surrogate=surrogate, document=passage)\n",
    "        cache_len, cache = build_kv_cache(context, model, tokenizer, config)\n",
    "        static_caches[key] = cache\n",
    "        static_lens[key] = cache_len\n",
    "    \n",
    "    # ========== SCORE ANSWERS ==========\n",
    "    query_prompt = config.query_template.format(query=query)\n",
    "    \n",
    "    # Baseline\n",
    "    baseline_nll = score_answer_with_cache(\n",
    "        baseline_cache, baseline_len, query_prompt, answer,\n",
    "        model, tokenizer, config\n",
    "    )\n",
    "    \n",
    "    # Generated surrogate NLLs\n",
    "    generated_nlls = {}\n",
    "    for key in generated_surrogates.keys():\n",
    "        nll = score_answer_with_cache(\n",
    "            generated_caches[key], generated_lens[key], query_prompt, answer,\n",
    "            model, tokenizer, config\n",
    "        )\n",
    "        generated_nlls[key] = nll\n",
    "    \n",
    "    # Static surrogate NLLs\n",
    "    static_nlls = {}\n",
    "    for key in static_surrogates.keys():\n",
    "        nll = score_answer_with_cache(\n",
    "            static_caches[key], static_lens[key], query_prompt, answer,\n",
    "            model, tokenizer, config\n",
    "        )\n",
    "        static_nlls[key] = nll\n",
    "    \n",
    "    # ========== ROUTING DECISIONS ==========\n",
    "    \n",
    "    # Generated: route to highest-similarity surrogate\n",
    "    gen_routed_key = max(generated_similarities.keys(), key=lambda k: generated_similarities[k])\n",
    "    gen_routed_nll = generated_nlls[gen_routed_key]\n",
    "    gen_routed_similarity = generated_similarities[gen_routed_key]\n",
    "    \n",
    "    # Generated: oracle (lowest NLL)\n",
    "    gen_oracle_key = min(generated_nlls.keys(), key=lambda k: generated_nlls[k])\n",
    "    gen_oracle_nll = generated_nlls[gen_oracle_key]\n",
    "    \n",
    "    # Static: route to highest-similarity surrogate\n",
    "    static_routed_key = max(static_similarities.keys(), key=lambda k: static_similarities[k])\n",
    "    static_routed_nll = static_nlls[static_routed_key]\n",
    "    static_routed_similarity = static_similarities[static_routed_key]\n",
    "    \n",
    "    # Static: oracle (lowest NLL)\n",
    "    static_oracle_key = min(static_nlls.keys(), key=lambda k: static_nlls[k])\n",
    "    static_oracle_nll = static_nlls[static_oracle_key]\n",
    "    \n",
    "    return {\n",
    "        'query': query,\n",
    "        'answer_preview': answer[:50] + '...' if len(answer) > 50 else answer,\n",
    "        \n",
    "        # Generated surrogates\n",
    "        'generated_surrogates': generated_surrogates,\n",
    "        'generated_similarities': generated_similarities,\n",
    "        'generated_nlls': generated_nlls,\n",
    "        'gen_routed_key': gen_routed_key,\n",
    "        'gen_routed_nll': gen_routed_nll,\n",
    "        'gen_routed_similarity': gen_routed_similarity,\n",
    "        'gen_oracle_key': gen_oracle_key,\n",
    "        'gen_oracle_nll': gen_oracle_nll,\n",
    "        \n",
    "        # Static surrogates\n",
    "        'static_surrogates': static_surrogates,\n",
    "        'static_similarities': static_similarities,\n",
    "        'static_nlls': static_nlls,\n",
    "        'static_routed_key': static_routed_key,\n",
    "        'static_routed_nll': static_routed_nll,\n",
    "        'static_routed_similarity': static_routed_similarity,\n",
    "        'static_oracle_key': static_oracle_key,\n",
    "        'static_oracle_nll': static_oracle_nll,\n",
    "        \n",
    "        # Baseline\n",
    "        'baseline_nll': baseline_nll,\n",
    "        \n",
    "        # Deltas (positive = surrogate better)\n",
    "        'delta_gen_routed': baseline_nll - gen_routed_nll,\n",
    "        'delta_gen_oracle': baseline_nll - gen_oracle_nll,\n",
    "        'delta_static_routed': baseline_nll - static_routed_nll,\n",
    "        'delta_static_oracle': baseline_nll - static_oracle_nll,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_production_simulation(\n",
    "    samples: List[Dict],\n",
    "    model: AutoModelForCausalLM,\n",
    "    tokenizer: AutoTokenizer,\n",
    "    embed_model: SentenceTransformer,\n",
    "    config: ExperimentConfig\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Run the full production simulation experiment.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for i, sample in enumerate(tqdm(samples, desc=\"Running experiment\")):\n",
    "        try:\n",
    "            result = evaluate_sample_with_top5_routing(\n",
    "                sample, model, tokenizer, embed_model, config\n",
    "            )\n",
    "            result['sample_idx'] = i\n",
    "            results.append(result)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\nError on sample {i}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        if i % 20 == 0:\n",
    "            torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "RUNNING PRODUCTION SIMULATION EXPERIMENT\n",
      "================================================================================\n",
      "\n",
      "Samples: 2500\n",
      "\n",
      "Experimental Arms:\n",
      "  1. Baseline (document only)\n",
      "  2. Generated Surrogates (5 doc-specific queries, routed)\n",
      "  3. Static Surrogates (5 fixed queries, routed)\n",
      "\n",
      "This will take a while as we generate 5 surrogates and build 11 caches per sample...\n",
      "  - 1 baseline cache\n",
      "  - 5 generated surrogate caches\n",
      "  - 5 static surrogate caches\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b061db13daf74a5584e7821e8f9caf57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running experiment:   0%|          | 0/2500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run the experiment\n",
    "print(\"=\"*80)\n",
    "print(\"RUNNING PRODUCTION SIMULATION EXPERIMENT\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nSamples: {len(samples)}\")\n",
    "print(f\"\\nExperimental Arms:\")\n",
    "print(f\"  1. Baseline (document only)\")\n",
    "print(f\"  2. Generated Surrogates (5 doc-specific queries, routed)\")\n",
    "print(f\"  3. Static Surrogates (5 fixed queries, routed)\")\n",
    "print(\"\\nThis will take a while as we generate 5 surrogates and build 11 caches per sample...\")\n",
    "print(\"  - 1 baseline cache\")\n",
    "print(\"  - 5 generated surrogate caches\")\n",
    "print(\"  - 5 static surrogate caches\")\n",
    "\n",
    "results = run_production_simulation(\n",
    "    samples, model, tokenizer, embed_model, config\n",
    ")\n",
    "\n",
    "print(f\"\\nCompleted {len(results)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-22",
   "metadata": {},
   "source": [
    "## Step 8: Analyze Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_experiment_results(results: List[Dict]) -> Dict:\n",
    "    \"\"\"\n",
    "    Comprehensive analysis of experiment results for both generated and static surrogates.\n",
    "    \"\"\"\n",
    "    baseline_nlls = np.array([r['baseline_nll'] for r in results])\n",
    "    \n",
    "    # Generated surrogates\n",
    "    gen_routed_nlls = np.array([r['gen_routed_nll'] for r in results])\n",
    "    gen_oracle_nlls = np.array([r['gen_oracle_nll'] for r in results])\n",
    "    deltas_gen_routed = np.array([r['delta_gen_routed'] for r in results])\n",
    "    deltas_gen_oracle = np.array([r['delta_gen_oracle'] for r in results])\n",
    "    \n",
    "    # Static surrogates\n",
    "    static_routed_nlls = np.array([r['static_routed_nll'] for r in results])\n",
    "    static_oracle_nlls = np.array([r['static_oracle_nll'] for r in results])\n",
    "    deltas_static_routed = np.array([r['delta_static_routed'] for r in results])\n",
    "    deltas_static_oracle = np.array([r['delta_static_oracle'] for r in results])\n",
    "    \n",
    "    # Per-template analysis for GENERATED\n",
    "    gen_template_stats = {}\n",
    "    for key in TOP_5_SURROGATE_TEMPLATES.keys():\n",
    "        nlls = [r['generated_nlls'][key] for r in results]\n",
    "        deltas = [r['baseline_nll'] - r['generated_nlls'][key] for r in results]\n",
    "        times_routed = sum(1 for r in results if r['gen_routed_key'] == key)\n",
    "        times_oracle = sum(1 for r in results if r['gen_oracle_key'] == key)\n",
    "        \n",
    "        gen_template_stats[key] = {\n",
    "            'mean_nll': np.mean(nlls),\n",
    "            'mean_delta': np.mean(deltas),\n",
    "            'win_rate': np.mean([d > 0 for d in deltas]),\n",
    "            'times_routed': times_routed,\n",
    "            'times_oracle': times_oracle,\n",
    "        }\n",
    "    \n",
    "    # Per-template analysis for STATIC\n",
    "    static_template_stats = {}\n",
    "    for key in STATIC_SURROGATE_QUERIES.keys():\n",
    "        nlls = [r['static_nlls'][key] for r in results]\n",
    "        deltas = [r['baseline_nll'] - r['static_nlls'][key] for r in results]\n",
    "        times_routed = sum(1 for r in results if r['static_routed_key'] == key)\n",
    "        times_oracle = sum(1 for r in results if r['static_oracle_key'] == key)\n",
    "        \n",
    "        static_template_stats[key] = {\n",
    "            'mean_nll': np.mean(nlls),\n",
    "            'mean_delta': np.mean(deltas),\n",
    "            'win_rate': np.mean([d > 0 for d in deltas]),\n",
    "            'times_routed': times_routed,\n",
    "            'times_oracle': times_oracle,\n",
    "        }\n",
    "    \n",
    "    # Statistical tests\n",
    "    t_gen_routed, p_gen_routed = stats.ttest_rel(baseline_nlls, gen_routed_nlls)\n",
    "    t_gen_oracle, p_gen_oracle = stats.ttest_rel(baseline_nlls, gen_oracle_nlls)\n",
    "    t_static_routed, p_static_routed = stats.ttest_rel(baseline_nlls, static_routed_nlls)\n",
    "    t_static_oracle, p_static_oracle = stats.ttest_rel(baseline_nlls, static_oracle_nlls)\n",
    "    \n",
    "    # Generated vs Static comparison\n",
    "    t_gen_vs_static, p_gen_vs_static = stats.ttest_rel(gen_routed_nlls, static_routed_nlls)\n",
    "    \n",
    "    # Effect sizes\n",
    "    def cohens_d(diff):\n",
    "        return np.mean(diff) / np.std(diff, ddof=1) if np.std(diff) > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'n_samples': len(results),\n",
    "        \n",
    "        # Baseline\n",
    "        'mean_baseline_nll': np.mean(baseline_nlls),\n",
    "        'std_baseline_nll': np.std(baseline_nlls),\n",
    "        \n",
    "        # Generated surrogates\n",
    "        'mean_gen_routed_nll': np.mean(gen_routed_nlls),\n",
    "        'std_gen_routed_nll': np.std(gen_routed_nlls),\n",
    "        'mean_gen_oracle_nll': np.mean(gen_oracle_nlls),\n",
    "        'std_gen_oracle_nll': np.std(gen_oracle_nlls),\n",
    "        'win_rate_gen_routed': np.mean(deltas_gen_routed > 0),\n",
    "        'win_rate_gen_oracle': np.mean(deltas_gen_oracle > 0),\n",
    "        'mean_delta_gen_routed': np.mean(deltas_gen_routed),\n",
    "        'mean_delta_gen_oracle': np.mean(deltas_gen_oracle),\n",
    "        'median_delta_gen_routed': np.median(deltas_gen_routed),\n",
    "        'cohens_d_gen_routed': cohens_d(deltas_gen_routed),\n",
    "        't_stat_gen_routed': t_gen_routed,\n",
    "        'p_value_gen_routed': p_gen_routed,\n",
    "        't_stat_gen_oracle': t_gen_oracle,\n",
    "        'p_value_gen_oracle': p_gen_oracle,\n",
    "        'gen_template_stats': gen_template_stats,\n",
    "        \n",
    "        # Static surrogates\n",
    "        'mean_static_routed_nll': np.mean(static_routed_nlls),\n",
    "        'std_static_routed_nll': np.std(static_routed_nlls),\n",
    "        'mean_static_oracle_nll': np.mean(static_oracle_nlls),\n",
    "        'std_static_oracle_nll': np.std(static_oracle_nlls),\n",
    "        'win_rate_static_routed': np.mean(deltas_static_routed > 0),\n",
    "        'win_rate_static_oracle': np.mean(deltas_static_oracle > 0),\n",
    "        'mean_delta_static_routed': np.mean(deltas_static_routed),\n",
    "        'mean_delta_static_oracle': np.mean(deltas_static_oracle),\n",
    "        'median_delta_static_routed': np.median(deltas_static_routed),\n",
    "        'cohens_d_static_routed': cohens_d(deltas_static_routed),\n",
    "        't_stat_static_routed': t_static_routed,\n",
    "        'p_value_static_routed': p_static_routed,\n",
    "        't_stat_static_oracle': t_static_oracle,\n",
    "        'p_value_static_oracle': p_static_oracle,\n",
    "        'static_template_stats': static_template_stats,\n",
    "        \n",
    "        # Generated vs Static comparison\n",
    "        't_stat_gen_vs_static': t_gen_vs_static,\n",
    "        'p_value_gen_vs_static': p_gen_vs_static,\n",
    "        'gen_beats_static_rate': np.mean(gen_routed_nlls < static_routed_nlls),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze results\n",
    "analysis = analyze_experiment_results(results)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PRODUCTION SIMULATION RESULTS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nSamples analyzed: {analysis['n_samples']}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"ANSWER NLL BY CONDITION (lower = better)\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"{'Condition':<30} {'Mean NLL':>12} {'Std':>10}\")\n",
    "print(\"-\"*55)\n",
    "print(f\"{'Baseline (doc only)':<30} {analysis['mean_baseline_nll']:>12.4f} {analysis['std_baseline_nll']:>10.4f}\")\n",
    "print(\"-\"*55)\n",
    "print(f\"{'Generated Routed':<30} {analysis['mean_gen_routed_nll']:>12.4f} {analysis['std_gen_routed_nll']:>10.4f}\")\n",
    "print(f\"{'Generated Oracle':<30} {analysis['mean_gen_oracle_nll']:>12.4f} {analysis['std_gen_oracle_nll']:>10.4f}\")\n",
    "print(\"-\"*55)\n",
    "print(f\"{'Static Routed':<30} {analysis['mean_static_routed_nll']:>12.4f} {analysis['std_static_routed_nll']:>10.4f}\")\n",
    "print(f\"{'Static Oracle':<30} {analysis['mean_static_oracle_nll']:>12.4f} {analysis['std_static_oracle_nll']:>10.4f}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"WIN RATES vs BASELINE (positive delta = surrogate better)\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Generated Routed vs Baseline: {analysis['win_rate_gen_routed']*100:.1f}%\")\n",
    "print(f\"Generated Oracle vs Baseline: {analysis['win_rate_gen_oracle']*100:.1f}%\")\n",
    "print(f\"Static Routed vs Baseline:    {analysis['win_rate_static_routed']*100:.1f}%\")\n",
    "print(f\"Static Oracle vs Baseline:    {analysis['win_rate_static_oracle']*100:.1f}%\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"IMPROVEMENT OVER BASELINE (Delta = Baseline NLL - Surrogate NLL)\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"{'Condition':<25} {'Mean Delta':>12} {'Median':>10} {'Cohens d':>10}\")\n",
    "print(\"-\"*60)\n",
    "print(f\"{'Generated Routed':<25} {analysis['mean_delta_gen_routed']:>12.4f} {analysis['median_delta_gen_routed']:>10.4f} {analysis['cohens_d_gen_routed']:>10.4f}\")\n",
    "print(f\"{'Generated Oracle':<25} {analysis['mean_delta_gen_oracle']:>12.4f} {'-':>10} {'-':>10}\")\n",
    "print(f\"{'Static Routed':<25} {analysis['mean_delta_static_routed']:>12.4f} {analysis['median_delta_static_routed']:>10.4f} {analysis['cohens_d_static_routed']:>10.4f}\")\n",
    "print(f\"{'Static Oracle':<25} {analysis['mean_delta_static_oracle']:>12.4f} {'-':>10} {'-':>10}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"STATISTICAL SIGNIFICANCE\")\n",
    "print(f\"{'='*80}\")\n",
    "print(\"vs Baseline:\")\n",
    "print(f\"  Generated Routed: t={analysis['t_stat_gen_routed']:.4f}, p={analysis['p_value_gen_routed']:.6f}\")\n",
    "print(f\"  Static Routed:    t={analysis['t_stat_static_routed']:.4f}, p={analysis['p_value_static_routed']:.6f}\")\n",
    "print(f\"\\nGenerated vs Static:\")\n",
    "print(f\"  t={analysis['t_stat_gen_vs_static']:.4f}, p={analysis['p_value_gen_vs_static']:.6f}\")\n",
    "print(f\"  Generated beats Static: {analysis['gen_beats_static_rate']*100:.1f}% of samples\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"PER-TEMPLATE ANALYSIS: GENERATED SURROGATES\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"{'Template':<22} {'Win Rate':>10} {'Mean Delta':>12} {'Routed':>10} {'Oracle':>10}\")\n",
    "print(\"-\"*70)\n",
    "for key, s in sorted(analysis['gen_template_stats'].items(), \n",
    "                     key=lambda x: x[1]['mean_delta'], reverse=True):\n",
    "    print(f\"{key:<22} {s['win_rate']*100:>9.1f}% {s['mean_delta']:>12.4f} \"\n",
    "          f\"{s['times_routed']:>10} {s['times_oracle']:>10}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"PER-TEMPLATE ANALYSIS: STATIC SURROGATES\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"{'Template':<22} {'Win Rate':>10} {'Mean Delta':>12} {'Routed':>10} {'Oracle':>10}\")\n",
    "print(\"-\"*70)\n",
    "for key, s in sorted(analysis['static_template_stats'].items(), \n",
    "                     key=lambda x: x[1]['mean_delta'], reverse=True):\n",
    "    short_key = key.replace('static_', '')\n",
    "    print(f\"{short_key:<22} {s['win_rate']*100:>9.1f}% {s['mean_delta']:>12.4f} \"\n",
    "          f\"{s['times_routed']:>10} {s['times_oracle']:>10}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conclusions\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CONCLUSIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Generated surrogates\n",
    "print(\"\\n--- GENERATED SURROGATES (document-specific) ---\")\n",
    "if analysis['p_value_gen_routed'] < 0.05:\n",
    "    if analysis['mean_delta_gen_routed'] > 0:\n",
    "        print(\"SIGNIFICANTLY IMPROVES over baseline\")\n",
    "        print(f\"  - Win rate: {analysis['win_rate_gen_routed']*100:.1f}%\")\n",
    "        print(f\"  - Mean improvement: {analysis['mean_delta_gen_routed']:.4f} NLL\")\n",
    "        print(f\"  - Effect size: {analysis['cohens_d_gen_routed']:.4f}\")\n",
    "    else:\n",
    "        print(\"Performs WORSE than baseline\")\n",
    "else:\n",
    "    print(\"No significant difference from baseline\")\n",
    "\n",
    "# Static surrogates\n",
    "print(\"\\n--- STATIC SURROGATES (same for all documents) ---\")\n",
    "if analysis['p_value_static_routed'] < 0.05:\n",
    "    if analysis['mean_delta_static_routed'] > 0:\n",
    "        print(\"SIGNIFICANTLY IMPROVES over baseline\")\n",
    "        print(f\"  - Win rate: {analysis['win_rate_static_routed']*100:.1f}%\")\n",
    "        print(f\"  - Mean improvement: {analysis['mean_delta_static_routed']:.4f} NLL\")\n",
    "        print(f\"  - Effect size: {analysis['cohens_d_static_routed']:.4f}\")\n",
    "    else:\n",
    "        print(\"Performs WORSE than baseline\")\n",
    "else:\n",
    "    print(\"No significant difference from baseline\")\n",
    "\n",
    "# Generated vs Static comparison\n",
    "print(\"\\n--- GENERATED vs STATIC COMPARISON ---\")\n",
    "if analysis['p_value_gen_vs_static'] < 0.05:\n",
    "    if analysis['gen_beats_static_rate'] > 0.5:\n",
    "        print(f\"Generated SIGNIFICANTLY BETTER than Static\")\n",
    "        print(f\"  - Generated beats Static: {analysis['gen_beats_static_rate']*100:.1f}% of samples\")\n",
    "        improvement = analysis['mean_delta_gen_routed'] - analysis['mean_delta_static_routed']\n",
    "        print(f\"  - Additional improvement from generation: {improvement:.4f} NLL\")\n",
    "    else:\n",
    "        print(f\"Static SIGNIFICANTLY BETTER than Generated\")\n",
    "        print(f\"  - Static beats Generated: {(1-analysis['gen_beats_static_rate'])*100:.1f}% of samples\")\n",
    "else:\n",
    "    print(\"No significant difference between Generated and Static\")\n",
    "    print(f\"  - Generated beats Static: {analysis['gen_beats_static_rate']*100:.1f}% of samples\")\n",
    "\n",
    "# Oracle comparison\n",
    "print(\"\\n--- ORACLE UPPER BOUNDS ---\")\n",
    "print(f\"Generated Oracle improvement: {analysis['mean_delta_gen_oracle']:.4f} NLL\")\n",
    "print(f\"Static Oracle improvement:    {analysis['mean_delta_static_oracle']:.4f} NLL\")\n",
    "\n",
    "# Routing efficiency\n",
    "gen_efficiency = (analysis['mean_delta_gen_routed'] / analysis['mean_delta_gen_oracle'] * 100 \n",
    "                  if analysis['mean_delta_gen_oracle'] > 0 else 0)\n",
    "static_efficiency = (analysis['mean_delta_static_routed'] / analysis['mean_delta_static_oracle'] * 100 \n",
    "                     if analysis['mean_delta_static_oracle'] > 0 else 0)\n",
    "print(f\"\\nRouting efficiency (how close to oracle):\")\n",
    "print(f\"  Generated: {gen_efficiency:.1f}%\")\n",
    "print(f\"  Static:    {static_efficiency:.1f}%\")\n",
    "\n",
    "# Key insight\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"KEY INSIGHT\")\n",
    "print(\"=\"*80)\n",
    "gen_vs_static_delta = analysis['mean_delta_gen_routed'] - analysis['mean_delta_static_routed']\n",
    "if gen_vs_static_delta > 0.01:\n",
    "    print(f\"\\nDocument-specific generation provides {gen_vs_static_delta:.4f} additional NLL improvement.\")\n",
    "    print(\"The extra compute for generating surrogates IS worth it.\")\n",
    "elif gen_vs_static_delta < -0.01:\n",
    "    print(f\"\\nStatic queries actually perform better by {-gen_vs_static_delta:.4f} NLL!\")\n",
    "    print(\"Simple intent priming may be sufficient - generation overhead may not be needed.\")\n",
    "else:\n",
    "    print(f\"\\nGenerated and Static perform similarly (delta: {gen_vs_static_delta:.4f}).\")\n",
    "    print(\"Static queries may be a cost-effective alternative to generation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "\n",
    "# Data extraction\n",
    "baseline_nlls = [r['baseline_nll'] for r in results]\n",
    "gen_routed_nlls = [r['gen_routed_nll'] for r in results]\n",
    "gen_oracle_nlls = [r['gen_oracle_nll'] for r in results]\n",
    "static_routed_nlls = [r['static_routed_nll'] for r in results]\n",
    "static_oracle_nlls = [r['static_oracle_nll'] for r in results]\n",
    "\n",
    "deltas_gen_routed = [r['delta_gen_routed'] for r in results]\n",
    "deltas_static_routed = [r['delta_static_routed'] for r in results]\n",
    "\n",
    "# Plot 1: Box plot comparison - all conditions\n",
    "ax1 = axes[0, 0]\n",
    "bp = ax1.boxplot([baseline_nlls, gen_routed_nlls, gen_oracle_nlls, \n",
    "                   static_routed_nlls, static_oracle_nlls],\n",
    "                  labels=['Baseline', 'Gen\\nRouted', 'Gen\\nOracle', \n",
    "                          'Static\\nRouted', 'Static\\nOracle'],\n",
    "                  patch_artist=True)\n",
    "colors = ['lightcoral', 'lightgreen', 'darkgreen', 'lightskyblue', 'steelblue']\n",
    "for patch, color in zip(bp['boxes'], colors):\n",
    "    patch.set_facecolor(color)\n",
    "ax1.set_ylabel('Answer NLL (lower = better)')\n",
    "ax1.set_title('Answer Quality: All Conditions')\n",
    "\n",
    "# Plot 2: Delta distribution - Generated vs Static\n",
    "ax2 = axes[0, 1]\n",
    "ax2.hist(deltas_gen_routed, bins=25, alpha=0.7, label='Generated', color='green', edgecolor='black')\n",
    "ax2.hist(deltas_static_routed, bins=25, alpha=0.5, label='Static', color='blue', edgecolor='black')\n",
    "ax2.axvline(x=0, color='red', linestyle='--', linewidth=2, label='No improvement')\n",
    "ax2.axvline(x=np.mean(deltas_gen_routed), color='darkgreen', linestyle='-', linewidth=2)\n",
    "ax2.axvline(x=np.mean(deltas_static_routed), color='darkblue', linestyle='-', linewidth=2)\n",
    "ax2.set_xlabel('Delta (Baseline - Surrogate NLL)')\n",
    "ax2.set_ylabel('Frequency')\n",
    "ax2.set_title('Improvement Distribution\\n(Positive = Surrogate Better)')\n",
    "ax2.legend()\n",
    "\n",
    "# Plot 3: Generated vs Static scatter\n",
    "ax3 = axes[0, 2]\n",
    "ax3.scatter(deltas_static_routed, deltas_gen_routed, alpha=0.5, c='purple')\n",
    "max_val = max(max(deltas_gen_routed), max(deltas_static_routed))\n",
    "min_val = min(min(deltas_gen_routed), min(deltas_static_routed))\n",
    "ax3.plot([min_val, max_val], [min_val, max_val], 'k--', linewidth=1, label='y=x')\n",
    "ax3.set_xlabel('Static Delta (improvement)')\n",
    "ax3.set_ylabel('Generated Delta (improvement)')\n",
    "ax3.set_title(f'Generated vs Static\\n(Gen beats Static: {analysis[\"gen_beats_static_rate\"]*100:.1f}%)')\n",
    "ax3.legend()\n",
    "\n",
    "# Plot 4: Per-template performance - Generated\n",
    "ax4 = axes[1, 0]\n",
    "gen_templates = list(analysis['gen_template_stats'].keys())\n",
    "gen_deltas = [analysis['gen_template_stats'][t]['mean_delta'] for t in gen_templates]\n",
    "colors = ['green' if d > 0 else 'red' for d in gen_deltas]\n",
    "ax4.barh(gen_templates, gen_deltas, color=colors, alpha=0.7, edgecolor='black')\n",
    "ax4.axvline(x=0, color='black', linestyle='-', linewidth=1)\n",
    "ax4.set_xlabel('Mean Delta (improvement over baseline)')\n",
    "ax4.set_title('Generated Surrogate Templates')\n",
    "\n",
    "# Plot 5: Per-template performance - Static\n",
    "ax5 = axes[1, 1]\n",
    "static_templates = [k.replace('static_', '') for k in analysis['static_template_stats'].keys()]\n",
    "static_deltas = [analysis['static_template_stats'][k]['mean_delta'] \n",
    "                  for k in analysis['static_template_stats'].keys()]\n",
    "colors = ['green' if d > 0 else 'red' for d in static_deltas]\n",
    "ax5.barh(static_templates, static_deltas, color=colors, alpha=0.7, edgecolor='black')\n",
    "ax5.axvline(x=0, color='black', linestyle='-', linewidth=1)\n",
    "ax5.set_xlabel('Mean Delta (improvement over baseline)')\n",
    "ax5.set_title('Static Surrogate Templates')\n",
    "\n",
    "# Plot 6: Summary bar chart\n",
    "ax6 = axes[1, 2]\n",
    "conditions = ['Baseline', 'Gen Routed', 'Gen Oracle', 'Static Routed', 'Static Oracle']\n",
    "mean_nlls = [\n",
    "    analysis['mean_baseline_nll'],\n",
    "    analysis['mean_gen_routed_nll'],\n",
    "    analysis['mean_gen_oracle_nll'],\n",
    "    analysis['mean_static_routed_nll'],\n",
    "    analysis['mean_static_oracle_nll'],\n",
    "]\n",
    "colors = ['lightcoral', 'lightgreen', 'darkgreen', 'lightskyblue', 'steelblue']\n",
    "bars = ax6.bar(conditions, mean_nlls, color=colors, alpha=0.8, edgecolor='black')\n",
    "ax6.set_ylabel('Mean Answer NLL')\n",
    "ax6.set_title('Summary: Mean NLL by Condition')\n",
    "ax6.tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('production_simulation_results.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nFigure saved to: production_simulation_results.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show examples\n",
    "print(\"=\"*80)\n",
    "print(\"EXAMPLE RESULTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Best cases where Generated beats Static\n",
    "gen_vs_static_diff = [(r['delta_gen_routed'] - r['delta_static_routed'], r) for r in results]\n",
    "gen_vs_static_diff.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "print(\"\\n--- TOP 5: Generated BEATS Static ---\")\n",
    "for diff, r in gen_vs_static_diff[:5]:\n",
    "    print(f\"\\nGen-Static Delta: {diff:.4f}\")\n",
    "    print(f\"  Query: {r['query']}\")\n",
    "    print(f\"  Gen routed to: {r['gen_routed_key']} -> \\\"{r['generated_surrogates'][r['gen_routed_key']]}\\\"\")\n",
    "    print(f\"  Static routed to: {r['static_routed_key'].replace('static_', '')}\")\n",
    "    print(f\"  NLLs: Baseline={r['baseline_nll']:.3f} | Gen={r['gen_routed_nll']:.3f} | Static={r['static_routed_nll']:.3f}\")\n",
    "\n",
    "print(\"\\n--- TOP 5: Static BEATS Generated ---\")\n",
    "for diff, r in gen_vs_static_diff[-5:]:\n",
    "    print(f\"\\nGen-Static Delta: {diff:.4f}\")\n",
    "    print(f\"  Query: {r['query']}\")\n",
    "    print(f\"  Gen routed to: {r['gen_routed_key']} -> \\\"{r['generated_surrogates'][r['gen_routed_key']]}\\\"\")\n",
    "    print(f\"  Static routed to: {r['static_routed_key'].replace('static_', '')}\")\n",
    "    print(f\"  NLLs: Baseline={r['baseline_nll']:.3f} | Gen={r['gen_routed_nll']:.3f} | Static={r['static_routed_nll']:.3f}\")\n",
    "\n",
    "# Overall best improvements\n",
    "print(\"\\n--- TOP 5 OVERALL IMPROVEMENTS (any method) ---\")\n",
    "best_overall = sorted(results, key=lambda r: max(r['delta_gen_routed'], r['delta_static_routed']), reverse=True)\n",
    "for r in best_overall[:5]:\n",
    "    best_delta = max(r['delta_gen_routed'], r['delta_static_routed'])\n",
    "    best_method = \"Generated\" if r['delta_gen_routed'] > r['delta_static_routed'] else \"Static\"\n",
    "    print(f\"\\nBest Delta: {best_delta:.4f} ({best_method})\")\n",
    "    print(f\"  Query: {r['query']}\")\n",
    "    print(f\"  NLLs: Baseline={r['baseline_nll']:.3f} | Gen={r['gen_routed_nll']:.3f} | Static={r['static_routed_nll']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rdnqx3zzrzf",
   "metadata": {},
   "source": [
    "## Step 9: Additional Analysis (Based on Answer Quality Notebook Findings)\n",
    "\n",
    "The answer quality notebook found some surprising results that we should investigate here:\n",
    "\n",
    "1. **No correlation between surrogate-query similarity and improvement** - Does this hold for both generated and static?\n",
    "2. **Head vs Tail showed no difference** - Do generated and static differ in how they help head vs tail queries?\n",
    "3. **Coverage analysis** - What fraction of queries have a \"good\" surrogate match?\n",
    "4. **Per-sample winner analysis** - What characterizes samples where generated beats static (or vice versa)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tlshky87s4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis 9a: Correlation between surrogate quality and improvement\n",
    "# The answer quality notebook found NO correlation - does this hold here?\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"ANALYSIS 9a: DOES SURROGATE QUALITY PREDICT IMPROVEMENT?\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# For generated surrogates\n",
    "gen_best_similarities = [r['gen_routed_similarity'] for r in results]\n",
    "gen_deltas = [r['delta_gen_routed'] for r in results]\n",
    "\n",
    "gen_corr, gen_p = stats.pearsonr(gen_best_similarities, gen_deltas)\n",
    "print(f\"\\nGENERATED SURROGATES:\")\n",
    "print(f\"  Correlation (best surrogate similarity vs delta): r={gen_corr:.4f}, p={gen_p:.4f}\")\n",
    "\n",
    "# For static surrogates\n",
    "static_best_similarities = [r['static_routed_similarity'] for r in results]\n",
    "static_deltas = [r['delta_static_routed'] for r in results]\n",
    "\n",
    "static_corr, static_p = stats.pearsonr(static_best_similarities, static_deltas)\n",
    "print(f\"\\nSTATIC SURROGATES:\")\n",
    "print(f\"  Correlation (best surrogate similarity vs delta): r={static_corr:.4f}, p={static_p:.4f}\")\n",
    "\n",
    "# Interpretation\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"INTERPRETATION\")\n",
    "print(\"=\"*80)\n",
    "if gen_p > 0.05 and static_p > 0.05:\n",
    "    print(\"Neither shows significant correlation between surrogate quality and improvement.\")\n",
    "    print(\"This confirms the answer quality notebook finding: better-matching surrogates\")\n",
    "    print(\"don't necessarily lead to better results. The benefit may come from general\")\n",
    "    print(\"intent priming rather than specific query matching.\")\n",
    "elif gen_p < 0.05 and static_p > 0.05:\n",
    "    print(\"Generated shows correlation, Static does not.\")\n",
    "    print(\"Document-specific surrogates may capture something static queries miss.\")\n",
    "elif gen_p > 0.05 and static_p < 0.05:\n",
    "    print(\"Static shows correlation, Generated does not.\")\n",
    "    print(\"Static intent categories may better align with query improvement.\")\n",
    "else:\n",
    "    print(\"Both show significant correlation.\")\n",
    "    direction = \"positive\" if gen_corr > 0 else \"negative\"\n",
    "    print(f\"Better surrogate matching leads to {direction} improvement.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "q140wdemk58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis 9b: Head vs Tail - Do Generated and Static differ?\n",
    "# Stratify by how well the best surrogate matches the query\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"ANALYSIS 9b: HEAD VS TAIL QUERY PERFORMANCE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Use the BETTER of the two best similarities as the stratification criterion\n",
    "# (i.e., how \"matchable\" is this query to either method?)\n",
    "combined_best_sim = [max(r['gen_routed_similarity'], r['static_routed_similarity']) for r in results]\n",
    "median_sim = np.median(combined_best_sim)\n",
    "\n",
    "head_results = [r for r, sim in zip(results, combined_best_sim) if sim >= median_sim]\n",
    "tail_results = [r for r, sim in zip(results, combined_best_sim) if sim < median_sim]\n",
    "\n",
    "def analyze_head_tail(result_list, name):\n",
    "    gen_deltas = [r['delta_gen_routed'] for r in result_list]\n",
    "    static_deltas = [r['delta_static_routed'] for r in result_list]\n",
    "    gen_wins_vs_static = sum(1 for r in result_list if r['gen_routed_nll'] < r['static_routed_nll'])\n",
    "    \n",
    "    return {\n",
    "        'name': name,\n",
    "        'n': len(result_list),\n",
    "        'gen_win_rate': np.mean([d > 0 for d in gen_deltas]),\n",
    "        'gen_mean_delta': np.mean(gen_deltas),\n",
    "        'static_win_rate': np.mean([d > 0 for d in static_deltas]),\n",
    "        'static_mean_delta': np.mean(static_deltas),\n",
    "        'gen_beats_static': gen_wins_vs_static / len(result_list),\n",
    "    }\n",
    "\n",
    "head_analysis = analyze_head_tail(head_results, \"Head (high sim)\")\n",
    "tail_analysis = analyze_head_tail(tail_results, \"Tail (low sim)\")\n",
    "\n",
    "print(f\"\\nMedian best-surrogate similarity: {median_sim:.4f}\")\n",
    "print(f\"\\n{'Stratum':<18} {'N':>5} {'Gen Win%':>10} {'Gen Delta':>11} {'Static Win%':>12} {'Static Delta':>13} {'Gen>Static':>12}\")\n",
    "print(\"-\"*95)\n",
    "for a in [head_analysis, tail_analysis]:\n",
    "    print(f\"{a['name']:<18} {a['n']:>5} {a['gen_win_rate']*100:>9.1f}% {a['gen_mean_delta']:>11.4f} \"\n",
    "          f\"{a['static_win_rate']*100:>11.1f}% {a['static_mean_delta']:>13.4f} {a['gen_beats_static']*100:>11.1f}%\")\n",
    "\n",
    "# Key comparison: Does generated beat static differently in head vs tail?\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"KEY QUESTION: Does Generated have an advantage over Static for certain query types?\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nHead queries (well-matched): Generated beats Static {head_analysis['gen_beats_static']*100:.1f}% of the time\")\n",
    "print(f\"Tail queries (poorly-matched): Generated beats Static {tail_analysis['gen_beats_static']*100:.1f}% of the time\")\n",
    "\n",
    "if abs(head_analysis['gen_beats_static'] - tail_analysis['gen_beats_static']) > 0.1:\n",
    "    if head_analysis['gen_beats_static'] > tail_analysis['gen_beats_static']:\n",
    "        print(\"\\n-> Generated surrogates have MORE advantage on head queries.\")\n",
    "        print(\"   When queries are matchable, doc-specific generation helps more.\")\n",
    "    else:\n",
    "        print(\"\\n-> Generated surrogates have MORE advantage on tail queries.\")\n",
    "        print(\"   When queries are hard to match, doc-specific generation helps more.\")\n",
    "else:\n",
    "    print(\"\\n-> Generated vs Static advantage is similar for head and tail queries.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3auela11e4i",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis 9c: Coverage Analysis\n",
    "# What fraction of queries have a \"good\" surrogate match at various thresholds?\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"ANALYSIS 9c: COVERAGE - HOW WELL DO SURROGATES COVER THE QUERY SPACE?\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "thresholds = [0.4, 0.5, 0.6, 0.7, 0.8]\n",
    "\n",
    "print(f\"\\n{'Threshold':<12} {'Gen Coverage':>14} {'Static Coverage':>16} {'Either':>10} {'Both':>10}\")\n",
    "print(\"-\"*65)\n",
    "\n",
    "for thresh in thresholds:\n",
    "    gen_covered = sum(1 for r in results if r['gen_routed_similarity'] >= thresh)\n",
    "    static_covered = sum(1 for r in results if r['static_routed_similarity'] >= thresh)\n",
    "    either_covered = sum(1 for r in results if r['gen_routed_similarity'] >= thresh or r['static_routed_similarity'] >= thresh)\n",
    "    both_covered = sum(1 for r in results if r['gen_routed_similarity'] >= thresh and r['static_routed_similarity'] >= thresh)\n",
    "    \n",
    "    n = len(results)\n",
    "    print(f\"{thresh:<12} {gen_covered/n*100:>13.1f}% {static_covered/n*100:>15.1f}% \"\n",
    "          f\"{either_covered/n*100:>9.1f}% {both_covered/n*100:>9.1f}%\")\n",
    "\n",
    "# Analyze improvement by coverage\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"IMPROVEMENT BY COVERAGE (threshold=0.5)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "thresh = 0.5\n",
    "gen_high_cov = [r for r in results if r['gen_routed_similarity'] >= thresh]\n",
    "gen_low_cov = [r for r in results if r['gen_routed_similarity'] < thresh]\n",
    "static_high_cov = [r for r in results if r['static_routed_similarity'] >= thresh]\n",
    "static_low_cov = [r for r in results if r['static_routed_similarity'] < thresh]\n",
    "\n",
    "print(f\"\\nGENERATED:\")\n",
    "if gen_high_cov:\n",
    "    print(f\"  High coverage (sim >= {thresh}): N={len(gen_high_cov)}, \"\n",
    "          f\"win rate={np.mean([r['delta_gen_routed'] > 0 for r in gen_high_cov])*100:.1f}%, \"\n",
    "          f\"mean delta={np.mean([r['delta_gen_routed'] for r in gen_high_cov]):.4f}\")\n",
    "if gen_low_cov:\n",
    "    print(f\"  Low coverage  (sim <  {thresh}): N={len(gen_low_cov)}, \"\n",
    "          f\"win rate={np.mean([r['delta_gen_routed'] > 0 for r in gen_low_cov])*100:.1f}%, \"\n",
    "          f\"mean delta={np.mean([r['delta_gen_routed'] for r in gen_low_cov]):.4f}\")\n",
    "\n",
    "print(f\"\\nSTATIC:\")\n",
    "if static_high_cov:\n",
    "    print(f\"  High coverage (sim >= {thresh}): N={len(static_high_cov)}, \"\n",
    "          f\"win rate={np.mean([r['delta_static_routed'] > 0 for r in static_high_cov])*100:.1f}%, \"\n",
    "          f\"mean delta={np.mean([r['delta_static_routed'] for r in static_high_cov]):.4f}\")\n",
    "if static_low_cov:\n",
    "    print(f\"  Low coverage  (sim <  {thresh}): N={len(static_low_cov)}, \"\n",
    "          f\"win rate={np.mean([r['delta_static_routed'] > 0 for r in static_low_cov])*100:.1f}%, \"\n",
    "          f\"mean delta={np.mean([r['delta_static_routed'] for r in static_low_cov]):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57tackn93hx",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis 9d: Per-Sample Winner Analysis\n",
    "# What characterizes samples where Generated beats Static (and vice versa)?\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"ANALYSIS 9d: WHAT MAKES GENERATED BEAT STATIC (OR VICE VERSA)?\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Categorize samples\n",
    "gen_wins = [r for r in results if r['gen_routed_nll'] < r['static_routed_nll']]\n",
    "static_wins = [r for r in results if r['static_routed_nll'] < r['gen_routed_nll']]\n",
    "ties = [r for r in results if abs(r['gen_routed_nll'] - r['static_routed_nll']) < 0.01]\n",
    "\n",
    "print(f\"\\nSample breakdown:\")\n",
    "print(f\"  Generated wins: {len(gen_wins)} ({len(gen_wins)/len(results)*100:.1f}%)\")\n",
    "print(f\"  Static wins:    {len(static_wins)} ({len(static_wins)/len(results)*100:.1f}%)\")\n",
    "print(f\"  Ties (delta < 0.01): {len(ties)} ({len(ties)/len(results)*100:.1f}%)\")\n",
    "\n",
    "# Analyze characteristics\n",
    "def analyze_winner_group(group, name):\n",
    "    if not group:\n",
    "        return None\n",
    "    return {\n",
    "        'name': name,\n",
    "        'mean_gen_sim': np.mean([r['gen_routed_similarity'] for r in group]),\n",
    "        'mean_static_sim': np.mean([r['static_routed_similarity'] for r in group]),\n",
    "        'mean_query_len': np.mean([len(r['query'].split()) for r in group]),\n",
    "        'mean_baseline_nll': np.mean([r['baseline_nll'] for r in group]),\n",
    "    }\n",
    "\n",
    "gen_wins_stats = analyze_winner_group(gen_wins, \"Gen Wins\")\n",
    "static_wins_stats = analyze_winner_group(static_wins, \"Static Wins\")\n",
    "\n",
    "print(f\"\\n{'Characteristic':<25} {'Gen Wins':>15} {'Static Wins':>15}\")\n",
    "print(\"-\"*55)\n",
    "if gen_wins_stats and static_wins_stats:\n",
    "    print(f\"{'Mean Gen Similarity':<25} {gen_wins_stats['mean_gen_sim']:>15.4f} {static_wins_stats['mean_gen_sim']:>15.4f}\")\n",
    "    print(f\"{'Mean Static Similarity':<25} {gen_wins_stats['mean_static_sim']:>15.4f} {static_wins_stats['mean_static_sim']:>15.4f}\")\n",
    "    print(f\"{'Mean Query Length':<25} {gen_wins_stats['mean_query_len']:>15.1f} {static_wins_stats['mean_query_len']:>15.1f}\")\n",
    "    print(f\"{'Mean Baseline NLL':<25} {gen_wins_stats['mean_baseline_nll']:>15.4f} {static_wins_stats['mean_baseline_nll']:>15.4f}\")\n",
    "\n",
    "# Which template \"wins\" most often?\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"WINNING TEMPLATES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nWhen GENERATED wins, which template was routed to?\")\n",
    "gen_template_counts = {}\n",
    "for r in gen_wins:\n",
    "    key = r['gen_routed_key']\n",
    "    gen_template_counts[key] = gen_template_counts.get(key, 0) + 1\n",
    "for key, count in sorted(gen_template_counts.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"  {key}: {count} ({count/len(gen_wins)*100:.1f}%)\")\n",
    "\n",
    "print(\"\\nWhen STATIC wins, which template was routed to?\")\n",
    "static_template_counts = {}\n",
    "for r in static_wins:\n",
    "    key = r['static_routed_key'].replace('static_', '')\n",
    "    static_template_counts[key] = static_template_counts.get(key, 0) + 1\n",
    "for key, count in sorted(static_template_counts.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"  {key}: {count} ({count/len(static_wins)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qtzuh06q3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization for additional analyses\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Plot 1: Similarity vs Delta scatter (testing correlation)\n",
    "ax1 = axes[0, 0]\n",
    "ax1.scatter([r['gen_routed_similarity'] for r in results], \n",
    "            [r['delta_gen_routed'] for r in results], \n",
    "            alpha=0.5, c='green', label='Generated', s=30)\n",
    "ax1.scatter([r['static_routed_similarity'] for r in results], \n",
    "            [r['delta_static_routed'] for r in results], \n",
    "            alpha=0.5, c='blue', label='Static', s=30, marker='^')\n",
    "ax1.axhline(y=0, color='red', linestyle='--', linewidth=1)\n",
    "ax1.set_xlabel('Best Surrogate Similarity to Query')\n",
    "ax1.set_ylabel('Delta (Baseline - Surrogate NLL)')\n",
    "ax1.set_title(f'Does Surrogate Quality Predict Improvement?\\n(Gen r={gen_corr:.3f}, Static r={static_corr:.3f})')\n",
    "ax1.legend()\n",
    "\n",
    "# Plot 2: Coverage curves\n",
    "ax2 = axes[0, 1]\n",
    "thresholds = np.linspace(0.2, 0.9, 15)\n",
    "gen_coverage = [sum(1 for r in results if r['gen_routed_similarity'] >= t)/len(results) for t in thresholds]\n",
    "static_coverage = [sum(1 for r in results if r['static_routed_similarity'] >= t)/len(results) for t in thresholds]\n",
    "ax2.plot(thresholds, gen_coverage, 'g-', linewidth=2, label='Generated', marker='o')\n",
    "ax2.plot(thresholds, static_coverage, 'b-', linewidth=2, label='Static', marker='^')\n",
    "ax2.set_xlabel('Similarity Threshold')\n",
    "ax2.set_ylabel('Coverage (fraction of queries)')\n",
    "ax2.set_title('Coverage at Different Similarity Thresholds')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Head vs Tail comparison\n",
    "ax3 = axes[1, 0]\n",
    "x = np.arange(2)\n",
    "width = 0.35\n",
    "gen_rates = [head_analysis['gen_win_rate']*100, tail_analysis['gen_win_rate']*100]\n",
    "static_rates = [head_analysis['static_win_rate']*100, tail_analysis['static_win_rate']*100]\n",
    "bars1 = ax3.bar(x - width/2, gen_rates, width, label='Generated', color='green', alpha=0.7)\n",
    "bars2 = ax3.bar(x + width/2, static_rates, width, label='Static', color='blue', alpha=0.7)\n",
    "ax3.axhline(y=50, color='red', linestyle='--', linewidth=1, label='50% (no advantage)')\n",
    "ax3.set_xticks(x)\n",
    "ax3.set_xticklabels(['Head\\n(well-matched)', 'Tail\\n(poorly-matched)'])\n",
    "ax3.set_ylabel('Win Rate vs Baseline (%)')\n",
    "ax3.set_title('Head vs Tail: Win Rate by Query Matchability')\n",
    "ax3.legend()\n",
    "ax3.set_ylim(0, 100)\n",
    "\n",
    "# Plot 4: Winner breakdown pie chart\n",
    "ax4 = axes[1, 1]\n",
    "if len(gen_wins) > 0 and len(static_wins) > 0:\n",
    "    sizes = [len(gen_wins), len(static_wins), len(ties)]\n",
    "    labels = [f'Generated\\n({len(gen_wins)})', f'Static\\n({len(static_wins)})', f'Ties\\n({len(ties)})']\n",
    "    colors = ['lightgreen', 'lightskyblue', 'lightgray']\n",
    "    ax4.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)\n",
    "    ax4.set_title('Per-Sample Winner: Generated vs Static')\n",
    "else:\n",
    "    ax4.text(0.5, 0.5, 'Insufficient data', ha='center', va='center')\n",
    "    ax4.set_title('Per-Sample Winner')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('additional_analysis.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nFigure saved to: additional_analysis.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "output_data = {\n",
    "    'config': {\n",
    "        'model_name': config.model_name,\n",
    "        'num_samples': config.num_samples,\n",
    "        'generated_templates': {k: v['name'] for k, v in TOP_5_SURROGATE_TEMPLATES.items()},\n",
    "        'static_queries': {k: v['query'] for k, v in STATIC_SURROGATE_QUERIES.items()},\n",
    "    },\n",
    "    'analysis': {k: v for k, v in analysis.items() \n",
    "                 if k not in ['gen_template_stats', 'static_template_stats']},\n",
    "    'gen_template_stats': analysis['gen_template_stats'],\n",
    "    'static_template_stats': analysis['static_template_stats'],\n",
    "    'results': [\n",
    "        {\n",
    "            'sample_idx': i,\n",
    "            'query': r['query'],\n",
    "            'baseline_nll': r['baseline_nll'],\n",
    "            'gen_routed_nll': r['gen_routed_nll'],\n",
    "            'gen_oracle_nll': r['gen_oracle_nll'],\n",
    "            'gen_routed_key': r['gen_routed_key'],\n",
    "            'gen_oracle_key': r['gen_oracle_key'],\n",
    "            'static_routed_nll': r['static_routed_nll'],\n",
    "            'static_oracle_nll': r['static_oracle_nll'],\n",
    "            'static_routed_key': r['static_routed_key'],\n",
    "            'static_oracle_key': r['static_oracle_key'],\n",
    "            'delta_gen_routed': r['delta_gen_routed'],\n",
    "            'delta_static_routed': r['delta_static_routed'],\n",
    "        }\n",
    "        for i, r in enumerate(results)\n",
    "    ]\n",
    "}\n",
    "\n",
    "with open('production_simulation_results.json', 'w') as f:\n",
    "    json.dump(output_data, f, indent=2, default=str)\n",
    "\n",
    "print(\"Results saved to: production_simulation_results.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
