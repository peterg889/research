{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 07: Suffix Priming and Advanced Cache Strategies\n",
    "\n",
    "## Motivation\n",
    "\n",
    "Experiments 01-06 showed that **prefix priming** (surrogate before document, then truncate) produces a content-independent artifact: all prefixes help/hurt together (r=0.924 between gen_routed and shuffled deltas). The RoPE correction fixes positions but values carry content-independent contamination from the forward pass.\n",
    "\n",
    "**The suffix approach sidesteps this entirely.** In a causal model, appending a surrogate AFTER the document means document tokens never attend to the suffix. Document KV entries are byte-identical to bare. Any improvement must come from the query attending to suffix tokens that have \"read\" the full document — a clean semantic signal.\n",
    "\n",
    "## 18 Conditions\n",
    "\n",
    "- **Group A (2)**: Baselines — bare, bare_padded\n",
    "- **Group B (7)**: Suffix semantic isolation — gen_routed, gen_oracle, perfect, irrelevant, shuffled, rand_passage, rand_tokens\n",
    "- **Group C (3)**: Suffix format — raw, newline, multi_q\n",
    "- **Group D (3)**: Prefix comparison — pfx_trunc_routed, pfx_trunc_perfect, pfx_full_ctx\n",
    "- **Group E (3)**: Suffix routing pool — pool_cosine, pool_oracle, summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "l6hpgeovrn",
   "source": "## Experimental Notes\n\n- Motivated by Exp 06's finding that prefix contamination (the forward pass through the surrogate contaminates values, not just keys) is unavoidable with prefix placement.\n- The suffix approach guarantees byte-identical document KV entries because document tokens are processed before the suffix tokens, so their keys and values are unaffected.\n- **Sanity check confirmed:** document KV entries are identical between suffix-primed and bare-document caches.\n- **Result:** Content-independent effects persisted even with suffix placement, ruling out value contamination as the sole explanation.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import sys\n",
    "import os\n",
    "import copy\n",
    "import json\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "from typing import Dict, List, Any, Optional, Tuple\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from scipy import stats\n",
    "from scipy.stats import rankdata\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sys.path.insert(0, '.')\n",
    "\n",
    "from lib import (\n",
    "    ExperimentConfig,\n",
    "    build_kv_cache,\n",
    "    build_suffix_kv_cache,\n",
    "    score_answer_with_cache,\n",
    "    build_truncated_kv_cache_corrected,\n",
    "    generate_all_5_surrogates,\n",
    "    generate_summary,\n",
    "    compute_similarity,\n",
    "    load_evaluation_samples,\n",
    "    load_ms_marco,\n",
    "    TOP_5_SURROGATE_TEMPLATES,\n",
    "    STATIC_SURROGATE_QUERIES,\n",
    ")\n",
    "from lib.analysis import cohens_d\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Configuration — same seed=42 and filtering as exp 06\n",
    "config = ExperimentConfig(\n",
    "    num_samples=800,\n",
    "    min_passage_words=50,\n",
    "    max_passage_words=300,\n",
    "    surrogate_max_tokens=45,\n",
    "    surrogate_temperature=0.3,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "print(f\"Model: {config.model_name}\")\n",
    "print(f\"Samples requested: {config.num_samples}\")\n",
    "print(f\"Passage words: {config.min_passage_words}-{config.max_passage_words}\")\n",
    "print(f\"Device: {config.device}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "torch.manual_seed(config.seed)\n",
    "np.random.seed(config.seed)\n",
    "random.seed(config.seed)\n",
    "\n",
    "# Load model (4-bit quantized)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "print(f\"Loading {config.model_name}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "model.eval()\n",
    "print(f\"Model loaded. Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# Embedding model for routing\n",
    "embed_model = SentenceTransformer(config.embedding_model_name)\n",
    "print(f\"Embedding model loaded: {config.embedding_model_name}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "dataset = load_ms_marco(config)\n",
    "raw_samples = load_evaluation_samples(dataset, config, require_answer=True)\n",
    "print(f\"Raw samples after basic filtering: {len(raw_samples)}\")\n",
    "\n",
    "# Same additional filters as exp 06\n",
    "filtered_samples = []\n",
    "excluded_ratio = 0\n",
    "excluded_short_answer = 0\n",
    "\n",
    "for s in raw_samples:\n",
    "    if len(s['answer']) / max(len(s['passage']), 1) > 0.5:\n",
    "        excluded_ratio += 1\n",
    "        continue\n",
    "    answer_ids = tokenizer(s['answer'], return_tensors='pt', add_special_tokens=False)['input_ids']\n",
    "    if answer_ids.shape[1] < 2:\n",
    "        excluded_short_answer += 1\n",
    "        continue\n",
    "    filtered_samples.append(s)\n",
    "\n",
    "samples = filtered_samples\n",
    "print(f\"\\nFiltering stats:\")\n",
    "print(f\"  Excluded (answer/passage ratio > 0.5): {excluded_ratio}\")\n",
    "print(f\"  Excluded (answer < 2 tokens):          {excluded_short_answer}\")\n",
    "print(f\"  Remaining samples:                     {len(samples)}\")\n",
    "\n",
    "s = samples[0]\n",
    "print(f\"\\nExample sample:\")\n",
    "print(f\"  Query:   {s['query'][:100]}...\")\n",
    "print(f\"  Passage: {s['passage'][:100]}...\")\n",
    "print(f\"  Answer:  {s['answer'][:100]}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity Check: Suffix KV Identity\n",
    "\n",
    "Verify that document KV entries in a suffix cache are byte-identical to the bare cache.\n",
    "In a causal model, passage tokens cannot attend to suffix tokens, so their KV representations must be identical."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Sanity check: passage KV entries must be identical between bare and suffix caches\n",
    "test_passage = samples[0]['passage']\n",
    "test_suffix = \"What is the main topic discussed?\"\n",
    "\n",
    "bare_len, bare_cache = build_kv_cache(test_passage, model, tokenizer, config)\n",
    "sfx_len, sfx_cache = build_suffix_kv_cache(test_passage, test_suffix, model, tokenizer, config)\n",
    "\n",
    "print(f\"Bare cache length: {bare_len}\")\n",
    "print(f\"Suffix cache length: {sfx_len}\")\n",
    "print(f\"Suffix added {sfx_len - bare_len} tokens\")\n",
    "\n",
    "def _get_kv(cache, layer_idx):\n",
    "    \"\"\"Get (keys, values) tensors from a cache layer, compatible with all DynamicCache versions.\"\"\"\n",
    "    if hasattr(cache, 'key_cache'):\n",
    "        return cache.key_cache[layer_idx], cache.value_cache[layer_idx]\n",
    "    return cache.layers[layer_idx].keys, cache.layers[layer_idx].values\n",
    "\n",
    "# Compare first bare_len positions across all layers\n",
    "all_match = True\n",
    "n_layers = len(bare_cache)\n",
    "for layer_idx in range(n_layers):\n",
    "    bare_k, bare_v = _get_kv(bare_cache, layer_idx)\n",
    "    sfx_k, sfx_v = _get_kv(sfx_cache, layer_idx)\n",
    "\n",
    "    k_match = torch.equal(bare_k[:, :, :bare_len, :], sfx_k[:, :, :bare_len, :])\n",
    "    v_match = torch.equal(bare_v[:, :, :bare_len, :], sfx_v[:, :, :bare_len, :])\n",
    "\n",
    "    if not k_match or not v_match:\n",
    "        all_match = False\n",
    "        print(f\"  Layer {layer_idx}: keys_match={k_match}, values_match={v_match}\")\n",
    "        break\n",
    "\n",
    "if all_match:\n",
    "    print(f\"\\nPASSED: All {n_layers} layers verified — passage KV entries are byte-identical.\")\n",
    "    print(\"  Document tokens never attend to suffix. Causal isolation confirmed.\")\n",
    "else:\n",
    "    print(f\"\\nFAILED: Passage KV entries differ! Something is wrong.\")\n",
    "\n",
    "# Clean up\n",
    "del bare_cache, sfx_cache\n",
    "torch.cuda.empty_cache()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def shuffle_text(text: str, rng: random.Random) -> str:\n",
    "    words = text.split()\n",
    "    rng.shuffle(words)\n",
    "    return ' '.join(words)\n",
    "\n",
    "\n",
    "def generate_random_tokens(tokenizer, n_tokens: int, rng: random.Random) -> str:\n",
    "    vocab_size = tokenizer.vocab_size\n",
    "    random_ids = [rng.randint(100, vocab_size - 1) for _ in range(n_tokens)]\n",
    "    return tokenizer.decode(random_ids, skip_special_tokens=True)\n",
    "\n",
    "\n",
    "def get_irrelevant_query(samples: list, current_idx: int, rng: random.Random) -> str:\n",
    "    other_idx = current_idx\n",
    "    while other_idx == current_idx:\n",
    "        other_idx = rng.randint(0, len(samples) - 1)\n",
    "    return samples[other_idx]['query']\n",
    "\n",
    "\n",
    "def get_random_passage(samples: list, current_idx: int, rng: random.Random) -> str:\n",
    "    other_idx = current_idx\n",
    "    while other_idx == current_idx:\n",
    "        other_idx = rng.randint(0, len(samples) - 1)\n",
    "    return samples[other_idx]['passage']\n",
    "\n",
    "\n",
    "def deep_copy_cache(cache):\n",
    "    return copy.deepcopy(cache)\n",
    "\n",
    "\n",
    "def score_query_nll(cache, cache_len, query_text, model, tokenizer, config):\n",
    "    cache_copy = deep_copy_cache(cache)\n",
    "    query_ids = tokenizer(\n",
    "        query_text, return_tensors=\"pt\", add_special_tokens=False\n",
    "    )['input_ids'].to(config.device)\n",
    "    query_len = query_ids.shape[1]\n",
    "    if query_len < 2:\n",
    "        return float('inf')\n",
    "    attention_mask = torch.ones((1, cache_len + query_len), device=config.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            input_ids=query_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            past_key_values=cache_copy,\n",
    "            use_cache=True,\n",
    "            return_dict=True,\n",
    "        )\n",
    "    logits = outputs.logits\n",
    "    shift_logits = logits[:, :-1, :].contiguous().view(-1, logits.size(-1))\n",
    "    shift_labels = query_ids[:, 1:].contiguous().view(-1)\n",
    "    loss = torch.nn.CrossEntropyLoss(reduction='sum')\n",
    "    nll = loss(shift_logits, shift_labels).item()\n",
    "    return nll / (query_len - 1)\n",
    "\n",
    "\n",
    "print(\"Helper functions defined.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resume from Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "checkpoint_path = '07_checkpoint.json'\n",
    "results = []\n",
    "start_from = 0\n",
    "\n",
    "if os.path.exists(checkpoint_path):\n",
    "    with open(checkpoint_path) as f:\n",
    "        ckpt = json.load(f)\n",
    "    results = ckpt['results']\n",
    "    start_from = ckpt['n_done']\n",
    "    print(f\"Resuming from checkpoint: {start_from} samples already done.\")\n",
    "else:\n",
    "    print(\"No checkpoint found. Starting from scratch.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Verification\n",
    "\n",
    "Test all 18 conditions on one sample to verify everything works."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "test_sample = samples[0]\n",
    "test_idx = 0\n",
    "test_rng = random.Random(config.seed)\n",
    "passage = test_sample['passage']\n",
    "query = test_sample['query']\n",
    "answer = test_sample['answer']\n",
    "query_prompt = config.query_template.format(query=query)\n",
    "\n",
    "print(f\"Passage: {passage[:100]}...\")\n",
    "print(f\"Query:   {query}\")\n",
    "print(f\"Answer:  {answer[:80]}\")\n",
    "print()\n",
    "\n",
    "# --- Group A: Baselines ---\n",
    "# 1. Bare\n",
    "bare_len, bare_cache = build_kv_cache(passage, model, tokenizer, config)\n",
    "bare_nll = score_answer_with_cache(bare_cache, bare_len, query_prompt, answer, model, tokenizer, config)\n",
    "print(f\" 1. bare              NLL: {bare_nll:.4f}\")\n",
    "\n",
    "# 2. Bare padded (match suffix length with newlines)\n",
    "test_surrogates = generate_all_5_surrogates(passage, model, tokenizer, config)\n",
    "test_sims = {k: compute_similarity(v, query, embed_model) for k, v in test_surrogates.items()}\n",
    "routed_key = max(test_sims, key=test_sims.get)\n",
    "routed_surr = test_surrogates[routed_key]\n",
    "# Estimate suffix token count for padding\n",
    "sfx_text = \"\\n\\nRelated question: \" + routed_surr\n",
    "sfx_tok_len = len(tokenizer(sfx_text, add_special_tokens=False)['input_ids'])\n",
    "padding = \"\\n\" * sfx_tok_len\n",
    "padded_len, padded_cache = build_kv_cache(passage + padding, model, tokenizer, config)\n",
    "padded_nll = score_answer_with_cache(padded_cache, padded_len, query_prompt, answer, model, tokenizer, config)\n",
    "print(f\" 2. bare_padded       NLL: {padded_nll:.4f} (added {sfx_tok_len} newline tokens)\")\n",
    "\n",
    "# --- Group B: Suffix Semantic Isolation ---\n",
    "# 3. sfx_gen_routed\n",
    "sfx_len, sfx_cache = build_suffix_kv_cache(passage, routed_surr, model, tokenizer, config)\n",
    "sfx_gen_routed_nll = score_answer_with_cache(sfx_cache, sfx_len, query_prompt, answer, model, tokenizer, config)\n",
    "print(f\" 3. sfx_gen_routed    NLL: {sfx_gen_routed_nll:.4f}\")\n",
    "\n",
    "# 4. sfx_gen_oracle — score all 5\n",
    "sfx_gen_nlls = {}\n",
    "for k, surr in test_surrogates.items():\n",
    "    sl, sc = build_suffix_kv_cache(passage, surr, model, tokenizer, config)\n",
    "    sfx_gen_nlls[k] = score_answer_with_cache(sc, sl, query_prompt, answer, model, tokenizer, config)\n",
    "sfx_oracle_key = min(sfx_gen_nlls, key=sfx_gen_nlls.get)\n",
    "print(f\" 4. sfx_gen_oracle    NLL: {sfx_gen_nlls[sfx_oracle_key]:.4f} (key={sfx_oracle_key})\")\n",
    "\n",
    "# 5. sfx_perfect\n",
    "sl, sc = build_suffix_kv_cache(passage, query, model, tokenizer, config)\n",
    "sfx_perfect_nll = score_answer_with_cache(sc, sl, query_prompt, answer, model, tokenizer, config)\n",
    "print(f\" 5. sfx_perfect       NLL: {sfx_perfect_nll:.4f}\")\n",
    "\n",
    "# 6. sfx_irrel\n",
    "irrel_q = get_irrelevant_query(samples, test_idx, test_rng)\n",
    "sl, sc = build_suffix_kv_cache(passage, irrel_q, model, tokenizer, config)\n",
    "sfx_irrel_nll = score_answer_with_cache(sc, sl, query_prompt, answer, model, tokenizer, config)\n",
    "print(f\" 6. sfx_irrel         NLL: {sfx_irrel_nll:.4f}\")\n",
    "\n",
    "# 7. sfx_shuffled\n",
    "shuffled = shuffle_text(routed_surr, test_rng)\n",
    "sl, sc = build_suffix_kv_cache(passage, shuffled, model, tokenizer, config)\n",
    "sfx_shuffled_nll = score_answer_with_cache(sc, sl, query_prompt, answer, model, tokenizer, config)\n",
    "print(f\" 7. sfx_shuffled      NLL: {sfx_shuffled_nll:.4f}\")\n",
    "\n",
    "# 8. sfx_rand_passage\n",
    "rp = get_random_passage(samples, test_idx, test_rng)[:200]\n",
    "sl, sc = build_suffix_kv_cache(passage, rp, model, tokenizer, config)\n",
    "sfx_rand_passage_nll = score_answer_with_cache(sc, sl, query_prompt, answer, model, tokenizer, config)\n",
    "print(f\" 8. sfx_rand_passage  NLL: {sfx_rand_passage_nll:.4f}\")\n",
    "\n",
    "# 9. sfx_rand_tokens\n",
    "rt = generate_random_tokens(tokenizer, 20, test_rng)\n",
    "sl, sc = build_suffix_kv_cache(passage, rt, model, tokenizer, config)\n",
    "sfx_rand_tokens_nll = score_answer_with_cache(sc, sl, query_prompt, answer, model, tokenizer, config)\n",
    "print(f\" 9. sfx_rand_tokens   NLL: {sfx_rand_tokens_nll:.4f}\")\n",
    "\n",
    "# --- Group C: Suffix Format ---\n",
    "# 10. sfx_raw\n",
    "sl, sc = build_suffix_kv_cache(passage, routed_surr, model, tokenizer, config, separator=\"\")\n",
    "sfx_raw_nll = score_answer_with_cache(sc, sl, query_prompt, answer, model, tokenizer, config)\n",
    "print(f\"10. sfx_raw           NLL: {sfx_raw_nll:.4f}\")\n",
    "\n",
    "# 11. sfx_newline\n",
    "sl, sc = build_suffix_kv_cache(passage, routed_surr, model, tokenizer, config, separator=\"\\n\\n\")\n",
    "sfx_newline_nll = score_answer_with_cache(sc, sl, query_prompt, answer, model, tokenizer, config)\n",
    "print(f\"11. sfx_newline       NLL: {sfx_newline_nll:.4f}\")\n",
    "\n",
    "# 12. sfx_multi_q (all 5 generated)\n",
    "multi_q_text = \"\\n\".join(f\"Related question: {v}\" for v in test_surrogates.values())\n",
    "sl, sc = build_suffix_kv_cache(passage, multi_q_text, model, tokenizer, config, separator=\"\\n\\n\")\n",
    "sfx_multi_q_nll = score_answer_with_cache(sc, sl, query_prompt, answer, model, tokenizer, config)\n",
    "print(f\"12. sfx_multi_q       NLL: {sfx_multi_q_nll:.4f}\")\n",
    "\n",
    "# --- Group D: Prefix Comparison ---\n",
    "# 13. pfx_trunc_routed\n",
    "dl, c = build_truncated_kv_cache_corrected(routed_surr, passage, model, tokenizer, config)\n",
    "pfx_trunc_routed_nll = score_answer_with_cache(c, dl, query_prompt, answer, model, tokenizer, config)\n",
    "print(f\"13. pfx_trunc_routed  NLL: {pfx_trunc_routed_nll:.4f}\")\n",
    "\n",
    "# 14. pfx_trunc_perfect\n",
    "dl, c = build_truncated_kv_cache_corrected(query, passage, model, tokenizer, config)\n",
    "pfx_trunc_perfect_nll = score_answer_with_cache(c, dl, query_prompt, answer, model, tokenizer, config)\n",
    "print(f\"14. pfx_trunc_perfect NLL: {pfx_trunc_perfect_nll:.4f}\")\n",
    "\n",
    "# 15. pfx_full_ctx\n",
    "full_ctx = config.surrogate_cache_template.format(surrogate=routed_surr, document=passage)\n",
    "fl, fc = build_kv_cache(full_ctx, model, tokenizer, config)\n",
    "pfx_full_ctx_nll = score_answer_with_cache(fc, fl, query_prompt, answer, model, tokenizer, config)\n",
    "print(f\"15. pfx_full_ctx      NLL: {pfx_full_ctx_nll:.4f}\")\n",
    "\n",
    "# --- Group E: Suffix Routing Pool + Summary ---\n",
    "# 16. sfx_pool_cosine — build pool of suffix caches\n",
    "# (deferred to evaluate_sample)\n",
    "print(f\"16. sfx_pool_cosine   (tested in evaluate_sample)\")\n",
    "\n",
    "# 17. sfx_pool_oracle\n",
    "print(f\"17. sfx_pool_oracle   (tested in evaluate_sample)\")\n",
    "\n",
    "# 18. sfx_summary\n",
    "summary = generate_summary(passage, model, tokenizer, config)\n",
    "sl, sc = build_suffix_kv_cache(passage, summary, model, tokenizer, config, separator=\"\\n\\nSummary: \")\n",
    "sfx_summary_nll = score_answer_with_cache(sc, sl, query_prompt, answer, model, tokenizer, config)\n",
    "print(f\"18. sfx_summary       NLL: {sfx_summary_nll:.4f}\")\n",
    "print(f\"    Summary: {summary[:120]}...\")\n",
    "\n",
    "print(\"\\nAll conditions produce finite NLLs. Pipeline verified.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def evaluate_sample(\n",
    "    sample: Dict,\n",
    "    idx: int,\n",
    "    all_samples: List[Dict],\n",
    "    model,\n",
    "    tokenizer,\n",
    "    embed_model,\n",
    "    config: ExperimentConfig,\n",
    ") -> Optional[Dict]:\n",
    "    \"\"\"Evaluate a single sample across all 18 experimental conditions.\"\"\"\n",
    "    passage = sample['passage']\n",
    "    query = sample['query']\n",
    "    answer = sample['answer']\n",
    "    query_prompt = config.query_template.format(query=query)\n",
    "    rng = random.Random(config.seed + idx)\n",
    "\n",
    "    # ==================== GROUP A: BASELINES ====================\n",
    "\n",
    "    # 1. Bare passage\n",
    "    bare_len, bare_cache = build_kv_cache(passage, model, tokenizer, config)\n",
    "    bare_nll = score_answer_with_cache(\n",
    "        bare_cache, bare_len, query_prompt, answer, model, tokenizer, config\n",
    "    )\n",
    "\n",
    "    # ==================== GENERATE SURROGATES ====================\n",
    "    generated_surrogates = generate_all_5_surrogates(passage, model, tokenizer, config)\n",
    "    gen_similarities = {\n",
    "        k: compute_similarity(v, query, embed_model)\n",
    "        for k, v in generated_surrogates.items()\n",
    "    }\n",
    "    gen_routed_key = max(gen_similarities, key=gen_similarities.get)\n",
    "    routed_surr = generated_surrogates[gen_routed_key]\n",
    "\n",
    "    # 2. Bare padded (length control: match suffix token count with newlines)\n",
    "    sfx_text_for_len = \"\\n\\nRelated question: \" + routed_surr\n",
    "    sfx_tok_len = len(tokenizer(sfx_text_for_len, add_special_tokens=False)['input_ids'])\n",
    "    padding = \"\\n\" * sfx_tok_len\n",
    "    padded_len, padded_cache = build_kv_cache(passage + padding, model, tokenizer, config)\n",
    "    padded_nll = score_answer_with_cache(\n",
    "        padded_cache, padded_len, query_prompt, answer, model, tokenizer, config\n",
    "    )\n",
    "\n",
    "    # ==================== GROUP B: SUFFIX SEMANTIC ISOLATION ====================\n",
    "\n",
    "    # Score all 5 generated surrogates as suffixes\n",
    "    sfx_gen_nlls = {}\n",
    "    sfx_gen_cache_data = {}\n",
    "    for key, surrogate in generated_surrogates.items():\n",
    "        sl, sc = build_suffix_kv_cache(passage, surrogate, model, tokenizer, config)\n",
    "        sfx_gen_nlls[key] = score_answer_with_cache(\n",
    "            sc, sl, query_prompt, answer, model, tokenizer, config\n",
    "        )\n",
    "        sfx_gen_cache_data[key] = (sl, sc)\n",
    "\n",
    "    # 3. sfx_gen_routed\n",
    "    sfx_gen_routed_nll = sfx_gen_nlls[gen_routed_key]\n",
    "\n",
    "    # 4. sfx_gen_oracle\n",
    "    sfx_gen_oracle_key = min(sfx_gen_nlls, key=sfx_gen_nlls.get)\n",
    "    sfx_gen_oracle_nll = sfx_gen_nlls[sfx_gen_oracle_key]\n",
    "\n",
    "    # 5. sfx_perfect\n",
    "    sl, sc = build_suffix_kv_cache(passage, query, model, tokenizer, config)\n",
    "    sfx_perfect_nll = score_answer_with_cache(\n",
    "        sc, sl, query_prompt, answer, model, tokenizer, config\n",
    "    )\n",
    "\n",
    "    # 6. sfx_irrel\n",
    "    irrel_query = get_irrelevant_query(all_samples, idx, rng)\n",
    "    sl, sc = build_suffix_kv_cache(passage, irrel_query, model, tokenizer, config)\n",
    "    sfx_irrel_nll = score_answer_with_cache(\n",
    "        sc, sl, query_prompt, answer, model, tokenizer, config\n",
    "    )\n",
    "\n",
    "    # 7. sfx_shuffled\n",
    "    shuffled_text = shuffle_text(routed_surr, rng)\n",
    "    sl, sc = build_suffix_kv_cache(passage, shuffled_text, model, tokenizer, config)\n",
    "    sfx_shuffled_nll = score_answer_with_cache(\n",
    "        sc, sl, query_prompt, answer, model, tokenizer, config\n",
    "    )\n",
    "\n",
    "    # 8. sfx_rand_passage\n",
    "    rand_passage_text = get_random_passage(all_samples, idx, rng)[:200]\n",
    "    sl, sc = build_suffix_kv_cache(passage, rand_passage_text, model, tokenizer, config)\n",
    "    sfx_rand_passage_nll = score_answer_with_cache(\n",
    "        sc, sl, query_prompt, answer, model, tokenizer, config\n",
    "    )\n",
    "\n",
    "    # 9. sfx_rand_tokens\n",
    "    rand_tokens_text = generate_random_tokens(tokenizer, 20, rng)\n",
    "    sl, sc = build_suffix_kv_cache(passage, rand_tokens_text, model, tokenizer, config)\n",
    "    sfx_rand_tokens_nll = score_answer_with_cache(\n",
    "        sc, sl, query_prompt, answer, model, tokenizer, config\n",
    "    )\n",
    "\n",
    "    # ==================== GROUP C: SUFFIX FORMAT ====================\n",
    "\n",
    "    # 10. sfx_raw (no separator)\n",
    "    sl, sc = build_suffix_kv_cache(passage, routed_surr, model, tokenizer, config, separator=\"\")\n",
    "    sfx_raw_nll = score_answer_with_cache(\n",
    "        sc, sl, query_prompt, answer, model, tokenizer, config\n",
    "    )\n",
    "\n",
    "    # 11. sfx_newline (minimal separator)\n",
    "    sl, sc = build_suffix_kv_cache(passage, routed_surr, model, tokenizer, config, separator=\"\\n\\n\")\n",
    "    sfx_newline_nll = score_answer_with_cache(\n",
    "        sc, sl, query_prompt, answer, model, tokenizer, config\n",
    "    )\n",
    "\n",
    "    # 12. sfx_multi_q (all 5 generated)\n",
    "    multi_q_text = \"\\n\".join(f\"Related question: {v}\" for v in generated_surrogates.values())\n",
    "    sl, sc = build_suffix_kv_cache(passage, multi_q_text, model, tokenizer, config, separator=\"\\n\\n\")\n",
    "    sfx_multi_q_nll = score_answer_with_cache(\n",
    "        sc, sl, query_prompt, answer, model, tokenizer, config\n",
    "    )\n",
    "\n",
    "    # ==================== GROUP D: PREFIX COMPARISON ====================\n",
    "\n",
    "    # 13. pfx_trunc_routed (same surrogate, prefix placement, truncated+RoPE corrected)\n",
    "    dl, c = build_truncated_kv_cache_corrected(\n",
    "        routed_surr, passage, model, tokenizer, config\n",
    "    )\n",
    "    pfx_trunc_routed_nll = score_answer_with_cache(\n",
    "        c, dl, query_prompt, answer, model, tokenizer, config\n",
    "    )\n",
    "\n",
    "    # 14. pfx_trunc_perfect (actual query as prefix)\n",
    "    dl, c = build_truncated_kv_cache_corrected(\n",
    "        query, passage, model, tokenizer, config\n",
    "    )\n",
    "    pfx_trunc_perfect_nll = score_answer_with_cache(\n",
    "        c, dl, query_prompt, answer, model, tokenizer, config\n",
    "    )\n",
    "\n",
    "    # 15. pfx_full_ctx (prefix kept visible, no truncation)\n",
    "    full_ctx = config.surrogate_cache_template.format(\n",
    "        surrogate=routed_surr, document=passage\n",
    "    )\n",
    "    fl, fc = build_kv_cache(full_ctx, model, tokenizer, config)\n",
    "    pfx_full_ctx_nll = score_answer_with_cache(\n",
    "        fc, fl, query_prompt, answer, model, tokenizer, config\n",
    "    )\n",
    "\n",
    "    # ==================== GROUP E: SUFFIX ROUTING POOL ====================\n",
    "\n",
    "    # Build suffix pool: 5 generated + 5 static + bare = 11 caches\n",
    "    pool_caches = {}\n",
    "    pool_caches['bare'] = (bare_len, bare_cache)\n",
    "    for key in generated_surrogates:\n",
    "        pool_caches[f'gen_{key}'] = sfx_gen_cache_data[key]\n",
    "    for key, info in STATIC_SURROGATE_QUERIES.items():\n",
    "        sl, sc = build_suffix_kv_cache(passage, info['query'], model, tokenizer, config)\n",
    "        pool_caches[f'static_{key}'] = (sl, sc)\n",
    "\n",
    "    # Cosine similarities for pool routing\n",
    "    pool_sims = {}\n",
    "    pool_sims['bare'] = compute_similarity(passage, query, embed_model)\n",
    "    for k, surr in generated_surrogates.items():\n",
    "        pool_sims[f'gen_{k}'] = gen_similarities[k]\n",
    "    for k, info in STATIC_SURROGATE_QUERIES.items():\n",
    "        pool_sims[f'static_{k}'] = compute_similarity(info['query'], query, embed_model)\n",
    "\n",
    "    # Answer NLL for all pool caches\n",
    "    pool_answer_nlls = {}\n",
    "    for cache_key, (clen, cache_obj) in pool_caches.items():\n",
    "        if cache_key == 'bare':\n",
    "            pool_answer_nlls[cache_key] = bare_nll\n",
    "        elif cache_key.startswith('gen_'):\n",
    "            template_key = cache_key[4:]\n",
    "            pool_answer_nlls[cache_key] = sfx_gen_nlls[template_key]\n",
    "        else:\n",
    "            cache_copy = deep_copy_cache(cache_obj)\n",
    "            pool_answer_nlls[cache_key] = score_answer_with_cache(\n",
    "                cache_copy, clen, query_prompt, answer, model, tokenizer, config\n",
    "            )\n",
    "\n",
    "    # 16. sfx_pool_cosine\n",
    "    pool_cosine_key = max(pool_sims, key=pool_sims.get)\n",
    "    sfx_pool_cosine_nll = pool_answer_nlls[pool_cosine_key]\n",
    "\n",
    "    # 17. sfx_pool_oracle\n",
    "    pool_oracle_key = min(pool_answer_nlls, key=pool_answer_nlls.get)\n",
    "    sfx_pool_oracle_nll = pool_answer_nlls[pool_oracle_key]\n",
    "\n",
    "    # 18. sfx_summary\n",
    "    summary = generate_summary(passage, model, tokenizer, config)\n",
    "    sl, sc = build_suffix_kv_cache(passage, summary, model, tokenizer, config, separator=\"\\n\\nSummary: \")\n",
    "    sfx_summary_nll = score_answer_with_cache(\n",
    "        sc, sl, query_prompt, answer, model, tokenizer, config\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        'idx': idx,\n",
    "        'query': query,\n",
    "        'answer_len': len(answer),\n",
    "        'passage_len': len(passage),\n",
    "\n",
    "        # Group A\n",
    "        'bare_nll': bare_nll,\n",
    "        'padded_nll': padded_nll,\n",
    "\n",
    "        # Group B\n",
    "        'sfx_gen_routed_key': gen_routed_key,\n",
    "        'sfx_gen_routed_nll': sfx_gen_routed_nll,\n",
    "        'sfx_gen_routed_sim': gen_similarities[gen_routed_key],\n",
    "        'sfx_gen_oracle_key': sfx_gen_oracle_key,\n",
    "        'sfx_gen_oracle_nll': sfx_gen_oracle_nll,\n",
    "        'sfx_gen_nlls': sfx_gen_nlls,\n",
    "        'sfx_gen_similarities': gen_similarities,\n",
    "        'generated_surrogates': generated_surrogates,\n",
    "        'sfx_perfect_nll': sfx_perfect_nll,\n",
    "        'sfx_irrel_nll': sfx_irrel_nll,\n",
    "        'sfx_irrel_query': irrel_query,\n",
    "        'sfx_shuffled_nll': sfx_shuffled_nll,\n",
    "        'sfx_rand_passage_nll': sfx_rand_passage_nll,\n",
    "        'sfx_rand_tokens_nll': sfx_rand_tokens_nll,\n",
    "\n",
    "        # Group C\n",
    "        'sfx_raw_nll': sfx_raw_nll,\n",
    "        'sfx_newline_nll': sfx_newline_nll,\n",
    "        'sfx_multi_q_nll': sfx_multi_q_nll,\n",
    "\n",
    "        # Group D\n",
    "        'pfx_trunc_routed_nll': pfx_trunc_routed_nll,\n",
    "        'pfx_trunc_perfect_nll': pfx_trunc_perfect_nll,\n",
    "        'pfx_full_ctx_nll': pfx_full_ctx_nll,\n",
    "\n",
    "        # Group E\n",
    "        'pool_sims': pool_sims,\n",
    "        'pool_answer_nlls': pool_answer_nlls,\n",
    "        'sfx_pool_cosine_key': pool_cosine_key,\n",
    "        'sfx_pool_cosine_nll': sfx_pool_cosine_nll,\n",
    "        'sfx_pool_oracle_key': pool_oracle_key,\n",
    "        'sfx_pool_oracle_nll': sfx_pool_oracle_nll,\n",
    "        'sfx_summary_nll': sfx_summary_nll,\n",
    "        'summary_text': summary,\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"evaluate_sample() defined — 18 conditions.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Experiment"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "errors = 0\n",
    "start_time = time.time()\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"RUNNING EXPERIMENT 07: SUFFIX PRIMING AND ADVANCED CACHE STRATEGIES\")\n",
    "print(f\"Samples: {len(samples)}, Conditions per sample: 18\")\n",
    "if start_from > 0:\n",
    "    print(f\"Resuming from sample {start_from}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for idx in tqdm(range(start_from, len(samples)), desc=\"Evaluating\"):\n",
    "    sample = samples[idx]\n",
    "    try:\n",
    "        result = evaluate_sample(\n",
    "            sample, idx, samples, model, tokenizer, embed_model, config\n",
    "        )\n",
    "        if result is not None:\n",
    "            results.append(result)\n",
    "    except Exception as e:\n",
    "        errors += 1\n",
    "        if errors <= 5:\n",
    "            print(f\"\\n  Error on sample {idx}: {type(e).__name__}: {e}\")\n",
    "        continue\n",
    "\n",
    "    # Progress + checkpoint every 10 samples\n",
    "    if len(results) > 0 and len(results) % 10 == 0:\n",
    "        elapsed = time.time() - start_time\n",
    "        rate = elapsed / (len(results) - start_from) if len(results) > start_from else 1\n",
    "        remaining = rate * (len(samples) - idx - 1)\n",
    "\n",
    "        recent = results[-min(10, len(results)):]\n",
    "        bare_mean = np.mean([r['bare_nll'] for r in recent])\n",
    "        sfx_gen_mean = np.mean([r['sfx_gen_routed_nll'] for r in recent])\n",
    "        sfx_irrel_mean = np.mean([r['sfx_irrel_nll'] for r in recent])\n",
    "        wr_gen = np.mean([r['bare_nll'] - r['sfx_gen_routed_nll'] > 0 for r in recent]) * 100\n",
    "\n",
    "        if len(results) % 50 == 0:\n",
    "            print(\n",
    "                f\"\\n  [{len(results):>4d} done | {elapsed/60:.0f}m elapsed | ~{remaining/60:.0f}m left]\"\n",
    "                f\"\\n  Last batch: bare={bare_mean:.3f}  sfx_gen={sfx_gen_mean:.3f} ({wr_gen:.0f}% win)\"\n",
    "                f\"  sfx_irrel={sfx_irrel_mean:.3f}\"\n",
    "            )\n",
    "\n",
    "        # Checkpoint\n",
    "        with open(checkpoint_path, 'w') as f:\n",
    "            json.dump({\n",
    "                'n_done': len(results),\n",
    "                'n_errors': errors,\n",
    "                'elapsed': time.time() - start_time,\n",
    "                'results': results,\n",
    "            }, f, default=str)\n",
    "\n",
    "elapsed_total = time.time() - start_time\n",
    "print(f\"\\nDone. {len(results)} evaluated, {errors} errors.\")\n",
    "print(f\"Total time: {elapsed_total/60:.1f} minutes ({elapsed_total/len(results) if results else 0:.1f}s per sample)\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Primary Results\n",
    "\n",
    "Summary table: all 18 conditions vs bare baseline."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "bare_arr = np.array([r['bare_nll'] for r in results])\n",
    "n = len(results)\n",
    "\n",
    "conditions = [\n",
    "    (' 1. bare (BASELINE)',         'bare_nll'),\n",
    "    (' 2. bare_padded',             'padded_nll'),\n",
    "    (' 3. sfx_gen_routed',          'sfx_gen_routed_nll'),\n",
    "    (' 4. sfx_gen_oracle',          'sfx_gen_oracle_nll'),\n",
    "    (' 5. sfx_perfect',             'sfx_perfect_nll'),\n",
    "    (' 6. sfx_irrel',               'sfx_irrel_nll'),\n",
    "    (' 7. sfx_shuffled',            'sfx_shuffled_nll'),\n",
    "    (' 8. sfx_rand_passage',        'sfx_rand_passage_nll'),\n",
    "    (' 9. sfx_rand_tokens',         'sfx_rand_tokens_nll'),\n",
    "    ('10. sfx_raw',                 'sfx_raw_nll'),\n",
    "    ('11. sfx_newline',             'sfx_newline_nll'),\n",
    "    ('12. sfx_multi_q',             'sfx_multi_q_nll'),\n",
    "    ('13. pfx_trunc_routed',        'pfx_trunc_routed_nll'),\n",
    "    ('14. pfx_trunc_perfect',       'pfx_trunc_perfect_nll'),\n",
    "    ('15. pfx_full_ctx',            'pfx_full_ctx_nll'),\n",
    "    ('16. sfx_pool_cosine',         'sfx_pool_cosine_nll'),\n",
    "    ('17. sfx_pool_oracle',         'sfx_pool_oracle_nll'),\n",
    "    ('18. sfx_summary',             'sfx_summary_nll'),\n",
    "]\n",
    "\n",
    "print(\"=\" * 130)\n",
    "print(f\"ALL 18 CONDITIONS vs BARE BASELINE  (N = {n})\")\n",
    "print(\"Positive delta = better than bare baseline.  Sorted by mean NLL.\")\n",
    "print(\"=\" * 130)\n",
    "header = f\"{'Condition':<28} {'Mean NLL':>10} {'Std':>8} {'Delta':>10} {'Win%':>8} {'t-stat':>8} {'p-value':>12} {'Cohen d':>10}\"\n",
    "print(header)\n",
    "print(\"-\" * 130)\n",
    "\n",
    "rows = []\n",
    "for label, key in conditions:\n",
    "    arr = np.array([r[key] for r in results])\n",
    "    delta = bare_arr - arr\n",
    "    mean_nll = np.mean(arr)\n",
    "    std_nll = np.std(arr)\n",
    "    if key == 'bare_nll':\n",
    "        rows.append((mean_nll, label, std_nll, '--', '--', '--', '--', '--'))\n",
    "    else:\n",
    "        t, p = stats.ttest_rel(bare_arr, arr)\n",
    "        d = cohens_d(delta)\n",
    "        win_rate = np.mean(delta > 0) * 100\n",
    "        rows.append((mean_nll, label, std_nll, f\"{np.mean(delta):+.4f}\", f\"{win_rate:.1f}%\",\n",
    "                      f\"{t:.3f}\", f\"{p:.6f}\", f\"{d:.4f}\"))\n",
    "\n",
    "rows.sort(key=lambda x: x[0])\n",
    "for mean_nll, label, std_nll, *rest in rows:\n",
    "    vals = [f\"{mean_nll:>10.4f}\", f\"{std_nll:>8.4f}\"] + [f\"{v:>12}\" if i >= 4 else f\"{v:>10}\" for i, v in enumerate(rest)]\n",
    "    print(f\"{label:<28} {' '.join(vals)}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic Isolation Tests\n",
    "\n",
    "**The decisive test**: Correlation of per-sample deltas between `sfx_gen_routed` and controls.\n",
    "If r << 0.9 (unlike prefix r=0.924 in exp 06), the suffix effect is content-dependent."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "alpha = 0.01\n",
    "n_key_tests = 5\n",
    "bonferroni_alpha = alpha / n_key_tests\n",
    "\n",
    "print(\"=\" * 110)\n",
    "print(\"SEMANTIC ISOLATION: KEY PAIRWISE COMPARISONS (SUFFIX)\")\n",
    "print(f\"Bonferroni-corrected alpha = {alpha}/{n_key_tests} = {bonferroni_alpha:.4f}\")\n",
    "print(\"=\" * 110)\n",
    "\n",
    "# Pairwise tests\n",
    "pairwise = [\n",
    "    (\"sfx_gen_routed (3) vs bare (1)\",          'sfx_gen_routed_nll', 'bare_nll',\n",
    "     \"Does suffix priming help at all?\"),\n",
    "    (\"sfx_gen_routed (3) vs sfx_irrel (6)\",     'sfx_gen_routed_nll', 'sfx_irrel_nll',\n",
    "     \"DECISIVE: Semantic or structural?\"),\n",
    "    (\"sfx_gen_routed (3) vs sfx_shuffled (7)\",  'sfx_gen_routed_nll', 'sfx_shuffled_nll',\n",
    "     \"Does word order matter in suffix?\"),\n",
    "    (\"sfx_gen_routed (3) vs sfx_rand_pass (8)\", 'sfx_gen_routed_nll', 'sfx_rand_passage_nll',\n",
    "     \"Does topic relevance matter?\"),\n",
    "    (\"sfx_gen_routed (3) vs sfx_rand_tok (9)\",  'sfx_gen_routed_nll', 'sfx_rand_tokens_nll',\n",
    "     \"Does coherence matter?\"),\n",
    "]\n",
    "\n",
    "print(f\"\\n{'Comparison':<46} {'Mean A':>8} {'Mean B':>8} {'Delta':>8} {'t':>8} {'p':>12} {'d':>8} {'Sig?':>6}\")\n",
    "print(\"-\" * 110)\n",
    "\n",
    "for label, key_a, key_b, question in pairwise:\n",
    "    a = np.array([r[key_a] for r in results])\n",
    "    b = np.array([r[key_b] for r in results])\n",
    "    diff = b - a  # positive = A is better (lower NLL)\n",
    "    t, p = stats.ttest_rel(a, b)\n",
    "    d = cohens_d(diff)\n",
    "    sig = \"YES\" if p < bonferroni_alpha else \"no\"\n",
    "    print(f\"{label:<46} {np.mean(a):>8.4f} {np.mean(b):>8.4f} {np.mean(diff):>+8.4f} {t:>8.3f} {p:>12.6f} {d:>8.4f} {sig:>6}\")\n",
    "    print(f\"  Q: {question}\")\n",
    "\n",
    "# THE KEY TEST: Correlation of per-sample deltas\n",
    "print(f\"\\n{'=' * 110}\")\n",
    "print(\"CORRELATION OF PER-SAMPLE DELTAS (suffix vs prefix comparison)\")\n",
    "print(\"If suffix r << 0.9, the suffix effect is content-dependent (unlike prefix r=0.924)\")\n",
    "print(\"=\" * 110)\n",
    "\n",
    "sfx_gen_deltas = np.array([r['bare_nll'] - r['sfx_gen_routed_nll'] for r in results])\n",
    "sfx_shuf_deltas = np.array([r['bare_nll'] - r['sfx_shuffled_nll'] for r in results])\n",
    "sfx_irrel_deltas = np.array([r['bare_nll'] - r['sfx_irrel_nll'] for r in results])\n",
    "sfx_rand_p_deltas = np.array([r['bare_nll'] - r['sfx_rand_passage_nll'] for r in results])\n",
    "sfx_rand_t_deltas = np.array([r['bare_nll'] - r['sfx_rand_tokens_nll'] for r in results])\n",
    "\n",
    "correlations = [\n",
    "    (\"sfx_gen_routed vs sfx_shuffled\",     sfx_gen_deltas, sfx_shuf_deltas),\n",
    "    (\"sfx_gen_routed vs sfx_irrel\",        sfx_gen_deltas, sfx_irrel_deltas),\n",
    "    (\"sfx_gen_routed vs sfx_rand_passage\", sfx_gen_deltas, sfx_rand_p_deltas),\n",
    "    (\"sfx_gen_routed vs sfx_rand_tokens\",  sfx_gen_deltas, sfx_rand_t_deltas),\n",
    "]\n",
    "\n",
    "print(f\"\\n{'Delta correlation':<42} {'r':>8} {'p':>12}\")\n",
    "print(\"-\" * 65)\n",
    "for label, x, y in correlations:\n",
    "    r, p = stats.pearsonr(x, y)\n",
    "    print(f\"{label:<42} {r:>8.4f} {p:>12.6f}\")\n",
    "\n",
    "print(f\"\\nPrefix reference (exp 06): r=0.924 between gen_routed and shuffled deltas\")\n",
    "print(f\"Suffix result:             r={stats.pearsonr(sfx_gen_deltas, sfx_shuf_deltas)[0]:.4f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Suffix vs Prefix Comparison\n",
    "\n",
    "Paired comparison: same surrogate, different placement."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"=\" * 110)\n",
    "print(\"SUFFIX vs PREFIX: SAME SURROGATE, DIFFERENT PLACEMENT\")\n",
    "print(\"=\" * 110)\n",
    "\n",
    "comparisons = [\n",
    "    (\"sfx_gen_routed (3) vs pfx_trunc_routed (13)\", 'sfx_gen_routed_nll', 'pfx_trunc_routed_nll',\n",
    "     \"Suffix vs truncated prefix (same routed surrogate)\"),\n",
    "    (\"sfx_perfect (5) vs pfx_trunc_perfect (14)\",    'sfx_perfect_nll',    'pfx_trunc_perfect_nll',\n",
    "     \"Suffix vs prefix oracle (actual query)\"),\n",
    "    (\"sfx_gen_routed (3) vs pfx_full_ctx (15)\",      'sfx_gen_routed_nll', 'pfx_full_ctx_nll',\n",
    "     \"Suffix vs full-context prefix (strongest prefix)\"),\n",
    "]\n",
    "\n",
    "print(f\"\\n{'Comparison':<52} {'Mean A':>8} {'Mean B':>8} {'Delta':>8} {'t':>8} {'p':>12} {'d':>8}\")\n",
    "print(\"-\" * 110)\n",
    "\n",
    "for label, key_a, key_b, question in comparisons:\n",
    "    a = np.array([r[key_a] for r in results])\n",
    "    b = np.array([r[key_b] for r in results])\n",
    "    diff = b - a  # positive = A (suffix) is better\n",
    "    t, p = stats.ttest_rel(a, b)\n",
    "    d = cohens_d(diff)\n",
    "    print(f\"{label:<52} {np.mean(a):>8.4f} {np.mean(b):>8.4f} {np.mean(diff):>+8.4f} {t:>8.3f} {p:>12.6f} {d:>8.4f}\")\n",
    "    print(f\"  Q: {question}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format Sensitivity\n",
    "\n",
    "Conditions 3 vs 10 vs 11 — does the model need structural cues?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"=\" * 110)\n",
    "print(\"FORMAT SENSITIVITY: Does the suffix separator matter?\")\n",
    "print(\"=\" * 110)\n",
    "\n",
    "format_conditions = [\n",
    "    (\"sfx_gen_routed (3: 'Related question:')\", 'sfx_gen_routed_nll'),\n",
    "    (\"sfx_raw (10: no separator)\",               'sfx_raw_nll'),\n",
    "    (\"sfx_newline (11: just newlines)\",          'sfx_newline_nll'),\n",
    "    (\"sfx_multi_q (12: all 5 queries)\",          'sfx_multi_q_nll'),\n",
    "]\n",
    "\n",
    "print(f\"\\n{'Condition':<46} {'Mean NLL':>10} {'Delta vs bare':>14} {'Win%':>8} {'d':>8}\")\n",
    "print(\"-\" * 90)\n",
    "\n",
    "for label, key in format_conditions:\n",
    "    arr = np.array([r[key] for r in results])\n",
    "    delta = bare_arr - arr\n",
    "    t, p = stats.ttest_rel(bare_arr, arr)\n",
    "    d = cohens_d(delta)\n",
    "    wr = np.mean(delta > 0) * 100\n",
    "    print(f\"{label:<46} {np.mean(arr):>10.4f} {np.mean(delta):>+14.4f} {wr:>7.1f}% {d:>8.4f}\")\n",
    "\n",
    "# Pairwise format comparisons\n",
    "print(f\"\\nPairwise format comparisons:\")\n",
    "fmt_pairs = [\n",
    "    (\"Related question (3) vs Raw (10)\",     'sfx_gen_routed_nll', 'sfx_raw_nll'),\n",
    "    (\"Related question (3) vs Newline (11)\", 'sfx_gen_routed_nll', 'sfx_newline_nll'),\n",
    "    (\"Related question (3) vs Multi-Q (12)\", 'sfx_gen_routed_nll', 'sfx_multi_q_nll'),\n",
    "]\n",
    "for label, ka, kb in fmt_pairs:\n",
    "    a = np.array([r[ka] for r in results])\n",
    "    b = np.array([r[kb] for r in results])\n",
    "    t, p = stats.ttest_rel(a, b)\n",
    "    diff = np.mean(b - a)\n",
    "    print(f\"  {label}: delta={diff:+.4f}, t={t:.3f}, p={p:.6f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Suffix Pool Routing and Summary Condition"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"=\" * 110)\n",
    "print(\"SUFFIX POOL ROUTING\")\n",
    "print(\"=\" * 110)\n",
    "\n",
    "pool_conditions = [\n",
    "    (\"bare (baseline)\",      'bare_nll'),\n",
    "    (\"sfx_pool_cosine (16)\", 'sfx_pool_cosine_nll'),\n",
    "    (\"sfx_pool_oracle (17)\", 'sfx_pool_oracle_nll'),\n",
    "    (\"sfx_summary (18)\",     'sfx_summary_nll'),\n",
    "]\n",
    "\n",
    "print(f\"\\n{'Condition':<30} {'Mean NLL':>10} {'Delta vs bare':>14} {'Win%':>8} {'d':>8} {'p':>12}\")\n",
    "print(\"-\" * 90)\n",
    "\n",
    "for label, key in pool_conditions:\n",
    "    arr = np.array([r[key] for r in results])\n",
    "    delta = bare_arr - arr\n",
    "    if key == 'bare_nll':\n",
    "        print(f\"{label:<30} {np.mean(arr):>10.4f} {'--':>14} {'--':>8} {'--':>8} {'--':>12}\")\n",
    "    else:\n",
    "        t, p = stats.ttest_rel(bare_arr, arr)\n",
    "        d = cohens_d(delta)\n",
    "        wr = np.mean(delta > 0) * 100\n",
    "        print(f\"{label:<30} {np.mean(arr):>10.4f} {np.mean(delta):>+14.4f} {wr:>7.1f}% {d:>8.4f} {p:>12.6f}\")\n",
    "\n",
    "# Oracle ceiling comparison\n",
    "oracle_delta = np.mean(bare_arr - np.array([r['sfx_pool_oracle_nll'] for r in results]))\n",
    "print(f\"\\nSuffix pool oracle delta: {oracle_delta:+.4f}\")\n",
    "print(f\"(Compare to exp 06 prefix pool oracle delta: ~0.60)\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stratified Analysis by Difficulty"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "quartiles = np.percentile(bare_arr, [25, 50, 75])\n",
    "difficulty_bins = [\n",
    "    ('Q1 (easiest)', lambda x: x <= quartiles[0]),\n",
    "    ('Q2',           lambda x: quartiles[0] < x <= quartiles[1]),\n",
    "    ('Q3',           lambda x: quartiles[1] < x <= quartiles[2]),\n",
    "    ('Q4 (hardest)', lambda x: x > quartiles[2]),\n",
    "]\n",
    "\n",
    "print(\"=\" * 110)\n",
    "print(\"STRATIFIED ANALYSIS BY DIFFICULTY QUARTILE\")\n",
    "print(\"=\" * 110)\n",
    "\n",
    "key_conditions = [\n",
    "    ('sfx_gen_routed', 'sfx_gen_routed_nll'),\n",
    "    ('sfx_irrel',      'sfx_irrel_nll'),\n",
    "    ('sfx_shuffled',   'sfx_shuffled_nll'),\n",
    "    ('sfx_perfect',    'sfx_perfect_nll'),\n",
    "    ('pfx_trunc_rou',  'pfx_trunc_routed_nll'),\n",
    "]\n",
    "\n",
    "header = f\"{'Quartile':<16}\"\n",
    "for name, _ in key_conditions:\n",
    "    header += f\" {name:>16}\"\n",
    "print(header)\n",
    "print(\"-\" * 110)\n",
    "\n",
    "for bin_label, cond in difficulty_bins:\n",
    "    subset = [r for r in results if cond(r['bare_nll'])]\n",
    "    if not subset:\n",
    "        continue\n",
    "    row = f\"{bin_label:<16}\"\n",
    "    for _, key in key_conditions:\n",
    "        deltas = [r['bare_nll'] - r[key] for r in subset]\n",
    "        wr = np.mean([d > 0 for d in deltas]) * 100\n",
    "        row += f\" {wr:>15.1f}%\"\n",
    "    print(row)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "fig, axes = plt.subplots(3, 3, figsize=(22, 18))\n",
    "fig.suptitle('Experiment 07: Suffix Priming and Advanced Cache Strategies', fontsize=16, fontweight='bold')\n",
    "\n",
    "# --- (0,0) All suffix conditions bar chart ---\n",
    "ax = axes[0, 0]\n",
    "cond_order = [\n",
    "    ('Bare', 'bare_nll'),\n",
    "    ('Padded', 'padded_nll'),\n",
    "    ('Sfx\\nRouted', 'sfx_gen_routed_nll'),\n",
    "    ('Sfx\\nOracle', 'sfx_gen_oracle_nll'),\n",
    "    ('Sfx\\nPerfect', 'sfx_perfect_nll'),\n",
    "    ('Sfx\\nIrrel', 'sfx_irrel_nll'),\n",
    "    ('Sfx\\nShuf', 'sfx_shuffled_nll'),\n",
    "    ('Sfx\\nRndP', 'sfx_rand_passage_nll'),\n",
    "    ('Sfx\\nRndT', 'sfx_rand_tokens_nll'),\n",
    "]\n",
    "means = [np.mean([r[k] for r in results]) for _, k in cond_order]\n",
    "sems = [stats.sem([r[k] for r in results]) for _, k in cond_order]\n",
    "colors = ['#888888', '#aaaaaa', '#4c72b0', '#2a5298', '#55a868',\n",
    "          '#c44e52', '#dd8452', '#8c564b', '#7f7f7f']\n",
    "ax.bar(range(len(cond_order)), means, yerr=sems, color=colors, capsize=3)\n",
    "ax.set_xticks(range(len(cond_order)))\n",
    "ax.set_xticklabels([l for l, _ in cond_order], fontsize=7)\n",
    "ax.set_ylabel('Mean NLL')\n",
    "ax.set_title('Group A+B: Suffix Conditions')\n",
    "\n",
    "# --- (0,1) Semantic isolation: delta correlations scatter ---\n",
    "ax = axes[0, 1]\n",
    "sfx_gen_deltas = np.array([r['bare_nll'] - r['sfx_gen_routed_nll'] for r in results])\n",
    "sfx_shuf_deltas = np.array([r['bare_nll'] - r['sfx_shuffled_nll'] for r in results])\n",
    "r_val, _ = stats.pearsonr(sfx_gen_deltas, sfx_shuf_deltas)\n",
    "ax.scatter(sfx_gen_deltas, sfx_shuf_deltas, alpha=0.2, s=8, c='#4c72b0')\n",
    "lims = [min(sfx_gen_deltas.min(), sfx_shuf_deltas.min()), max(sfx_gen_deltas.max(), sfx_shuf_deltas.max())]\n",
    "ax.plot(lims, lims, 'r--', linewidth=1, alpha=0.5)\n",
    "ax.set_xlabel('Gen routed delta')\n",
    "ax.set_ylabel('Shuffled delta')\n",
    "ax.set_title(f'Semantic Isolation: r={r_val:.3f}\\n(prefix ref: r=0.924)')\n",
    "\n",
    "# --- (0,2) Suffix vs prefix ---\n",
    "ax = axes[0, 2]\n",
    "sfx_pfx_labels = ['Sfx\\nRouted', 'Pfx\\nTrunc', 'Pfx\\nFull']\n",
    "sfx_pfx_keys = ['sfx_gen_routed_nll', 'pfx_trunc_routed_nll', 'pfx_full_ctx_nll']\n",
    "sfx_pfx_means = [np.mean([r[k] for r in results]) for k in sfx_pfx_keys]\n",
    "sfx_pfx_sems = [stats.sem([r[k] for r in results]) for k in sfx_pfx_keys]\n",
    "sfx_pfx_colors = ['#4c72b0', '#dd8452', '#55a868']\n",
    "ax.bar(range(3), sfx_pfx_means, yerr=sfx_pfx_sems, color=sfx_pfx_colors, capsize=3)\n",
    "ax.axhline(np.mean(bare_arr), color='#888888', linestyle='--', linewidth=1, label='Bare')\n",
    "ax.set_xticks(range(3))\n",
    "ax.set_xticklabels(sfx_pfx_labels, fontsize=8)\n",
    "ax.set_ylabel('Mean NLL')\n",
    "ax.set_title('Suffix vs Prefix (same surrogate)')\n",
    "ax.legend(fontsize=7)\n",
    "\n",
    "# --- (1,0) Format sensitivity ---\n",
    "ax = axes[1, 0]\n",
    "fmt_labels = ['Related Q\\n(default)', 'Raw\\n(no sep)', 'Newline\\nonly', 'Multi-Q\\n(all 5)']\n",
    "fmt_keys = ['sfx_gen_routed_nll', 'sfx_raw_nll', 'sfx_newline_nll', 'sfx_multi_q_nll']\n",
    "fmt_means = [np.mean([r[k] for r in results]) for k in fmt_keys]\n",
    "fmt_sems = [stats.sem([r[k] for r in results]) for k in fmt_keys]\n",
    "fmt_colors = ['#4c72b0', '#c44e52', '#dd8452', '#55a868']\n",
    "ax.bar(range(4), fmt_means, yerr=fmt_sems, color=fmt_colors, capsize=3)\n",
    "ax.axhline(np.mean(bare_arr), color='#888888', linestyle='--', linewidth=1, label='Bare')\n",
    "ax.set_xticks(range(4))\n",
    "ax.set_xticklabels(fmt_labels, fontsize=7)\n",
    "ax.set_ylabel('Mean NLL')\n",
    "ax.set_title('Group C: Format Sensitivity')\n",
    "ax.legend(fontsize=7)\n",
    "\n",
    "# --- (1,1) Pool routing ---\n",
    "ax = axes[1, 1]\n",
    "pool_labels_viz = ['Bare', 'Pool\\nCosine', 'Pool\\nOracle', 'Summary']\n",
    "pool_keys_viz = ['bare_nll', 'sfx_pool_cosine_nll', 'sfx_pool_oracle_nll', 'sfx_summary_nll']\n",
    "pool_means_viz = [np.mean([r[k] for r in results]) for k in pool_keys_viz]\n",
    "pool_sems_viz = [stats.sem([r[k] for r in results]) for k in pool_keys_viz]\n",
    "pool_colors_viz = ['#888888', '#4c72b0', '#2a5298', '#55a868']\n",
    "ax.bar(range(4), pool_means_viz, yerr=pool_sems_viz, color=pool_colors_viz, capsize=3)\n",
    "ax.set_xticks(range(4))\n",
    "ax.set_xticklabels(pool_labels_viz, fontsize=8)\n",
    "ax.set_ylabel('Mean NLL')\n",
    "ax.set_title('Group E: Pool Routing + Summary')\n",
    "\n",
    "# --- (1,2) Win rate by difficulty quartile ---\n",
    "ax = axes[1, 2]\n",
    "q_labels_list = []\n",
    "q_sfx_gen_wr = []\n",
    "q_sfx_irrel_wr = []\n",
    "q_pfx_wr = []\n",
    "for label, cond in difficulty_bins:\n",
    "    subset = [r for r in results if cond(r['bare_nll'])]\n",
    "    if not subset:\n",
    "        continue\n",
    "    q_labels_list.append(label.replace(' ', '\\n'))\n",
    "    q_sfx_gen_wr.append(np.mean([r['bare_nll'] - r['sfx_gen_routed_nll'] > 0 for r in subset]) * 100)\n",
    "    q_sfx_irrel_wr.append(np.mean([r['bare_nll'] - r['sfx_irrel_nll'] > 0 for r in subset]) * 100)\n",
    "    q_pfx_wr.append(np.mean([r['bare_nll'] - r['pfx_trunc_routed_nll'] > 0 for r in subset]) * 100)\n",
    "\n",
    "x_pos = np.arange(len(q_labels_list))\n",
    "w = 0.25\n",
    "ax.bar(x_pos - w, q_sfx_gen_wr, w, label='Sfx Gen', color='#4c72b0')\n",
    "ax.bar(x_pos, q_sfx_irrel_wr, w, label='Sfx Irrel', color='#c44e52')\n",
    "ax.bar(x_pos + w, q_pfx_wr, w, label='Pfx Trunc', color='#dd8452')\n",
    "ax.axhline(50, color='black', linestyle='--', linewidth=0.8, alpha=0.3)\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels(q_labels_list, fontsize=7)\n",
    "ax.set_ylabel('Win Rate vs Bare (%)')\n",
    "ax.set_title('Win Rate by Difficulty')\n",
    "ax.legend(fontsize=7)\n",
    "ax.set_ylim(0, 100)\n",
    "\n",
    "# --- (2,0) Similarity vs suffix delta scatter ---\n",
    "ax = axes[2, 0]\n",
    "gen_sims = np.array([r['sfx_gen_routed_sim'] for r in results])\n",
    "gen_deltas_viz = sfx_gen_deltas\n",
    "r_sim, _ = stats.pearsonr(gen_sims, gen_deltas_viz)\n",
    "ax.scatter(gen_sims, gen_deltas_viz, alpha=0.2, s=8, c='#4c72b0')\n",
    "z = np.polyfit(gen_sims, gen_deltas_viz, 1)\n",
    "p_line = np.poly1d(z)\n",
    "x_range = np.linspace(gen_sims.min(), gen_sims.max(), 100)\n",
    "ax.plot(x_range, p_line(x_range), 'r-', linewidth=1.5)\n",
    "ax.axhline(0, color='black', linestyle='--', linewidth=0.8)\n",
    "ax.set_xlabel('Surrogate-Query Cosine Similarity')\n",
    "ax.set_ylabel('Suffix Delta NLL')\n",
    "ax.set_title(f'Similarity vs Suffix Delta (r={r_sim:.3f})')\n",
    "\n",
    "# --- (2,1) Per-template NLL (suffix) ---\n",
    "ax = axes[2, 1]\n",
    "gen_keys = list(TOP_5_SURROGATE_TEMPLATES.keys())\n",
    "gen_template_means = [np.mean([r['sfx_gen_nlls'][k] for r in results]) for k in gen_keys]\n",
    "gen_template_sems = [stats.sem([r['sfx_gen_nlls'][k] for r in results]) for k in gen_keys]\n",
    "ax.bar(range(len(gen_keys)), gen_template_means, yerr=gen_template_sems,\n",
    "       color='#4c72b0', capsize=3)\n",
    "ax.axhline(np.mean(bare_arr), color='#888888', linestyle='--', linewidth=1, label='Bare')\n",
    "ax.set_xticks(range(len(gen_keys)))\n",
    "ax.set_xticklabels([k.replace('_', '\\n') for k in gen_keys], fontsize=6)\n",
    "ax.set_ylabel('Mean NLL')\n",
    "ax.set_title('Per-Template: Suffix Generated')\n",
    "ax.legend(fontsize=7)\n",
    "\n",
    "# --- (2,2) Padded control ---\n",
    "ax = axes[2, 2]\n",
    "padded_delta = bare_arr - np.array([r['padded_nll'] for r in results])\n",
    "sfx_gen_delta = bare_arr - np.array([r['sfx_gen_routed_nll'] for r in results])\n",
    "bp = ax.boxplot([padded_delta, sfx_gen_delta], labels=['Padded\\n(length ctrl)', 'Sfx Gen\\nRouted'],\n",
    "                patch_artist=True)\n",
    "bp['boxes'][0].set_facecolor('#aaaaaa')\n",
    "bp['boxes'][1].set_facecolor('#4c72b0')\n",
    "for box in bp['boxes']:\n",
    "    box.set_alpha(0.7)\n",
    "ax.axhline(0, color='black', linestyle='--', linewidth=0.8)\n",
    "ax.set_ylabel('Delta NLL vs Bare')\n",
    "ax.set_title('Length Control: Padded vs Suffix')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('07_suffix_priming_results.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"Saved: 07_suffix_priming_results.png\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "os.makedirs('results', exist_ok=True)\n",
    "timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "output = {\n",
    "    'metadata': {\n",
    "        'experiment': '07_suffix_priming_experiment',\n",
    "        'description': (\n",
    "            'Suffix priming and advanced cache strategies: 18 conditions testing '\n",
    "            'suffix placement (passage + suffix), format sensitivity, prefix comparison, '\n",
    "            'suffix routing pool, and summary-as-suffix. Key test: semantic isolation '\n",
    "            'via delta correlations (comparing to prefix r=0.924).'\n",
    "        ),\n",
    "        'timestamp': datetime.datetime.now().isoformat(),\n",
    "        'model_name': config.model_name,\n",
    "        'num_samples_requested': config.num_samples,\n",
    "        'num_samples_evaluated': len(results),\n",
    "        'num_errors': errors,\n",
    "        'elapsed_seconds': elapsed_total,\n",
    "        'seed': config.seed,\n",
    "    },\n",
    "    'results': results,\n",
    "}\n",
    "\n",
    "# Save timestamped copy\n",
    "output_path_ts = f'results/07_suffix_priming_results_{timestamp}.json'\n",
    "with open(output_path_ts, 'w') as f:\n",
    "    json.dump(output, f, indent=2, default=str)\n",
    "print(f\"Timestamped results: {output_path_ts}\")\n",
    "\n",
    "# Save canonical copy\n",
    "output_path = '07_suffix_priming_results.json'\n",
    "with open(output_path, 'w') as f:\n",
    "    json.dump(output, f, indent=2, default=str)\n",
    "print(f\"Canonical results:   {output_path}\")\n",
    "print(f\"File size: {os.path.getsize(output_path) / 1e6:.1f} MB\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"AUTOMATED VERDICTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "alpha = 0.01\n",
    "bonferroni_alpha = alpha / 5\n",
    "verdicts = {}\n",
    "\n",
    "# H1: Does suffix priming help?\n",
    "sfx_gen_arr = np.array([r['sfx_gen_routed_nll'] for r in results])\n",
    "t, p = stats.ttest_rel(bare_arr, sfx_gen_arr)\n",
    "verdicts['H1: Suffix priming helps'] = {\n",
    "    'test': 'sfx_gen_routed (3) vs bare (1)',\n",
    "    'delta': float(np.mean(bare_arr - sfx_gen_arr)),\n",
    "    't': float(t), 'p': float(p),\n",
    "    'verdict': 'SUPPORTED' if p < bonferroni_alpha and np.mean(sfx_gen_arr) < np.mean(bare_arr) else 'NOT SUPPORTED'\n",
    "}\n",
    "\n",
    "# H2: Suffix effect is semantic (gen vs irrel)\n",
    "sfx_irrel_arr = np.array([r['sfx_irrel_nll'] for r in results])\n",
    "t, p = stats.ttest_rel(sfx_gen_arr, sfx_irrel_arr)\n",
    "verdicts['H2: Suffix semantic content matters'] = {\n",
    "    'test': 'sfx_gen_routed (3) vs sfx_irrel (6)',\n",
    "    'delta': float(np.mean(sfx_irrel_arr - sfx_gen_arr)),\n",
    "    't': float(t), 'p': float(p),\n",
    "    'verdict': 'SUPPORTED' if p < bonferroni_alpha and np.mean(sfx_gen_arr) < np.mean(sfx_irrel_arr) else 'NOT SUPPORTED'\n",
    "}\n",
    "\n",
    "# H3: Delta correlation is low (content-dependent, unlike prefix)\n",
    "sfx_gen_deltas_v = np.array([r['bare_nll'] - r['sfx_gen_routed_nll'] for r in results])\n",
    "sfx_shuf_deltas_v = np.array([r['bare_nll'] - r['sfx_shuffled_nll'] for r in results])\n",
    "r_corr, p_corr = stats.pearsonr(sfx_gen_deltas_v, sfx_shuf_deltas_v)\n",
    "verdicts['H3: Suffix effect is content-dependent (r < 0.5)'] = {\n",
    "    'test': 'Correlation of per-sample deltas: sfx_gen_routed vs sfx_shuffled',\n",
    "    'r': float(r_corr), 'p': float(p_corr),\n",
    "    'prefix_reference': 0.924,\n",
    "    'verdict': 'SUPPORTED' if r_corr < 0.5 else ('PARTIAL' if r_corr < 0.8 else 'NOT SUPPORTED')\n",
    "}\n",
    "\n",
    "# H4: sfx_perfect beats bare significantly\n",
    "sfx_perf_arr = np.array([r['sfx_perfect_nll'] for r in results])\n",
    "t, p = stats.ttest_rel(bare_arr, sfx_perf_arr)\n",
    "d = cohens_d(bare_arr - sfx_perf_arr)\n",
    "verdicts['H4: sfx_perfect beats bare (d > 0.3)'] = {\n",
    "    'test': 'sfx_perfect (5) vs bare (1)',\n",
    "    'delta': float(np.mean(bare_arr - sfx_perf_arr)),\n",
    "    'd': float(d), 't': float(t), 'p': float(p),\n",
    "    'verdict': 'SUPPORTED' if p < 0.01 and d > 0.3 else 'NOT SUPPORTED'\n",
    "}\n",
    "\n",
    "# H5: Suffix vs prefix — suffix is better or comparable\n",
    "pfx_arr = np.array([r['pfx_trunc_routed_nll'] for r in results])\n",
    "t, p = stats.ttest_rel(sfx_gen_arr, pfx_arr)\n",
    "verdicts['H5: Suffix >= prefix (same surrogate)'] = {\n",
    "    'test': 'sfx_gen_routed (3) vs pfx_trunc_routed (13)',\n",
    "    'sfx_mean': float(np.mean(sfx_gen_arr)),\n",
    "    'pfx_mean': float(np.mean(pfx_arr)),\n",
    "    'delta': float(np.mean(pfx_arr - sfx_gen_arr)),\n",
    "    't': float(t), 'p': float(p),\n",
    "    'verdict': 'SUPPORTED' if np.mean(sfx_gen_arr) <= np.mean(pfx_arr) else 'NOT SUPPORTED'\n",
    "}\n",
    "\n",
    "# Print\n",
    "for hyp, v in verdicts.items():\n",
    "    sig_marker = \"***\" if v.get('p', 1) < 0.01 else \"   \"\n",
    "    print(f\"\\n{sig_marker} {hyp}: {v['verdict']}\")\n",
    "    print(f\"    {v['test']}\")\n",
    "    if 'r' in v:\n",
    "        print(f\"    r={v['r']:.4f} (prefix ref: {v['prefix_reference']})\")\n",
    "    elif 'd' in v:\n",
    "        print(f\"    d={v['d']:.4f}, t={v['t']:.3f}, p={v['p']:.6f}\")\n",
    "    else:\n",
    "        print(f\"    delta={v.get('delta', 'N/A')}, t={v['t']:.3f}, p={v['p']:.6f}\")\n",
    "\n",
    "# Overall assessment\n",
    "print(f\"\\n{'=' * 80}\")\n",
    "print(\"OVERALL ASSESSMENT\")\n",
    "print(\"=\" * 80)\n",
    "n_supported = sum(1 for v in verdicts.values() if v['verdict'] == 'SUPPORTED')\n",
    "print(f\"Hypotheses supported: {n_supported}/{len(verdicts)}\")\n",
    "\n",
    "# Success criteria check\n",
    "print(f\"\\nSuccess criteria:\")\n",
    "h4_ok = verdicts['H4: sfx_perfect beats bare (d > 0.3)']['verdict'] == 'SUPPORTED'\n",
    "h2_ok = verdicts['H2: Suffix semantic content matters']['verdict'] == 'SUPPORTED'\n",
    "h3_ok = verdicts['H3: Suffix effect is content-dependent (r < 0.5)']['verdict'] == 'SUPPORTED'\n",
    "\n",
    "if h4_ok and h2_ok and h3_ok:\n",
    "    print(\"  STRONG SUCCESS: sfx_perfect helps AND gen_routed beats controls AND low correlation\")\n",
    "elif h4_ok and (h2_ok or h3_ok):\n",
    "    print(\"  PARTIAL SUCCESS: sfx_perfect helps, some semantic signal\")\n",
    "elif h4_ok:\n",
    "    print(\"  PARTIAL: sfx_perfect helps but no semantic separation\")\n",
    "else:\n",
    "    print(\"  FAILURE: suffix approach does not produce meaningful improvements\")"
   ],
   "outputs": [],
   "execution_count": null
  }
 ]
}