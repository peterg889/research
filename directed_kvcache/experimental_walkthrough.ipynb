{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro-cell",
   "metadata": {},
   "source": [
    "# Experimental Walkthrough: Understanding Surrogate KV Cache Priming\n",
    "\n",
    "This notebook walks through a single example from start to finish to build intuition for the experimental setup. We'll examine:\n",
    "\n",
    "1. **What is a data point?** - A passage, query, and answer from MS MARCO\n",
    "2. **What are static vs. generated surrogates?** - The two approaches we're comparing\n",
    "3. **How do we build KV caches?** - The caching mechanism at the heart of the experiment\n",
    "4. **How do we run generation/scoring?** - Using the cache to score answers\n",
    "5. **What is the evaluation metric?** - Negative Log-Likelihood (NLL) explained\n",
    "\n",
    "---\n",
    "\n",
    "## The Big Picture\n",
    "\n",
    "In production RAG systems, we retrieve a document and use it to answer user queries. The **KV cache** stores the model's internal representations of the document, allowing us to reuse computation across queries.\n",
    "\n",
    "**The core idea**: What if we \"prime\" the cache by prepending a surrogate query before the document? This might help the model understand what kind of questions it should expect.\n",
    "\n",
    "We compare two approaches:\n",
    "- **Static surrogates**: 5 fixed queries (same for every document), covering common intent categories\n",
    "- **Generated surrogates**: 5 document-specific queries, generated by an LLM\n",
    "\n",
    "```\n",
    "BASELINE CACHE:                    SURROGATE-PRIMED CACHE:\n",
    "┌─────────────────────┐            ┌─────────────────────────────────────┐\n",
    "│ Document: ...       │            │ This doc may answer: {surrogate}    │\n",
    "│                     │            │                                     │\n",
    "│ [KV Cache]          │            │ Document: ...                       │\n",
    "└─────────────────────┘            │                                     │\n",
    "                                   │ [KV Cache]                          │\n",
    "                                   └─────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-header",
   "metadata": {},
   "source": [
    "## Step 1: Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "install-deps",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (run once)\n",
    "!pip install transformers torch datasets bitsandbytes accelerate sentence-transformers -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "imports",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports complete.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from datasets import load_dataset\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Imports complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "load-models",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Model: mistralai/Mistral-7B-Instruct-v0.2\n",
      "\n",
      "Loading language model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b03c6f8e1d8f46d6b846cc94de080049",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/291 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language model loaded: 7,241,732,096 parameters\n",
      "\n",
      "Loading embedding model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3cff620c02140b094ae5ff11d64a913",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding model loaded.\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "\n",
    "# Load the language model (4-bit quantized for memory efficiency)\n",
    "print(\"\\nLoading language model...\")\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "tokenizer.padding_side = \"right\"\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "model.eval()\n",
    "print(f\"Language model loaded: {model.num_parameters():,} parameters\")\n",
    "\n",
    "# Load embedding model for similarity computation\n",
    "print(\"\\nLoading embedding model...\")\n",
    "embed_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "print(\"Embedding model loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 2: Understanding a Data Point from MS MARCO\n",
    "\n",
    "MS MARCO (Microsoft Machine Reading Comprehension) is a large-scale dataset for question answering. Each data point contains:\n",
    "\n",
    "- **Passage**: A document (typically a paragraph from the web)\n",
    "- **Query**: A natural language question from a real user\n",
    "- **Answer**: The correct answer to the question\n",
    "\n",
    "Let's load the dataset and examine one example in detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "load-dataset",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MS MARCO dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since ms_marco couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'v1.1' at /home/petergrabowski_google_com/.cache/huggingface/datasets/ms_marco/v1.1/0.0.0/a47ee7aae8d7d466ba15f9f0bfac3b3681087b3a (last modified on Mon Jan 26 15:46:33 2026).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded: 10047 samples\n"
     ]
    }
   ],
   "source": [
    "# Load MS MARCO validation set\n",
    "print(\"Loading MS MARCO dataset...\")\n",
    "dataset = load_dataset(\"ms_marco\", \"v1.1\", split=\"validation\")\n",
    "print(f\"Dataset loaded: {len(dataset)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "find-example",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found example!\n"
     ]
    }
   ],
   "source": [
    "def find_good_example(dataset, min_words=50, max_words=200):\n",
    "    \"\"\"\n",
    "    Find an example with:\n",
    "    - A passage of reasonable length\n",
    "    - A clear answer\n",
    "    - An interesting query\n",
    "    \"\"\"\n",
    "    for item in dataset:\n",
    "        passages = item.get('passages', {})\n",
    "        passage_texts = passages.get('passage_text', [])\n",
    "        is_selected = passages.get('is_selected', [])\n",
    "        \n",
    "        query = item.get('query', '')\n",
    "        answers = item.get('answers', [])\n",
    "        well_formed = item.get('wellFormedAnswers', [])\n",
    "        \n",
    "        if not passage_texts or not query:\n",
    "            continue\n",
    "        \n",
    "        # Get best answer (prefer well-formed answers)\n",
    "        if well_formed and len(well_formed) > 0 and well_formed[0] != '[]':\n",
    "            answer = well_formed[0]\n",
    "        elif answers and len(answers) > 0 and answers[0] != 'No Answer Present.':\n",
    "            answer = answers[0]\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        # Find the selected passage with reasonable length\n",
    "        for i, passage in enumerate(passage_texts):\n",
    "            word_count = len(passage.split())\n",
    "            if min_words <= word_count <= max_words:\n",
    "                if is_selected and i < len(is_selected) and is_selected[i] == 1:\n",
    "                    return {\n",
    "                        'passage': passage,\n",
    "                        'query': query,\n",
    "                        'answer': answer\n",
    "                    }\n",
    "    return None\n",
    "\n",
    "# Find our example\n",
    "example = find_good_example(dataset)\n",
    "print(\"Found example!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "show-example",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "EXAMPLE DATA POINT FROM MS MARCO\n",
      "================================================================================\n",
      "\n",
      "----------------------------------------\n",
      "PASSAGE (the document)\n",
      "----------------------------------------\n",
      "The average Walgreens salary ranges from approximately $15,000 per year for Customer Service Associate / Cashier to $179,900 per year for District Manager. Average Walgreens hourly pay ranges from approximately $7.35 per hour for Laboratory Technician to $68.90 per hour for Pharmacy Manager. Salary information comes from 7,810 data points collected directly from employees, users, and jobs on Indeed.\n",
      "\n",
      "----------------------------------------\n",
      "QUERY (what the user is asking)\n",
      "----------------------------------------\n",
      "walgreens store sales average\n",
      "\n",
      "----------------------------------------\n",
      "ANSWER (the correct response)\n",
      "----------------------------------------\n",
      "Approximately $15,000 per year.\n",
      "\n",
      "================================================================================\n",
      "Passage length: 59 words\n",
      "Query length: 4 words\n",
      "Answer length: 4 words\n"
     ]
    }
   ],
   "source": [
    "# Display the example in a clear format\n",
    "print(\"=\"*80)\n",
    "print(\"EXAMPLE DATA POINT FROM MS MARCO\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n\" + \"-\"*40)\n",
    "print(\"PASSAGE (the document)\")\n",
    "print(\"-\"*40)\n",
    "print(example['passage'])\n",
    "\n",
    "print(\"\\n\" + \"-\"*40)\n",
    "print(\"QUERY (what the user is asking)\")\n",
    "print(\"-\"*40)\n",
    "print(example['query'])\n",
    "\n",
    "print(\"\\n\" + \"-\"*40)\n",
    "print(\"ANSWER (the correct response)\")\n",
    "print(\"-\"*40)\n",
    "print(example['answer'])\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"Passage length: {len(example['passage'].split())} words\")\n",
    "print(f\"Query length: {len(example['query'].split())} words\")\n",
    "print(f\"Answer length: {len(example['answer'].split())} words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surrogate-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 3: Static vs. Generated Surrogates\n",
    "\n",
    "Now we define the two types of surrogate queries:\n",
    "\n",
    "### Static Surrogates\n",
    "These are **5 fixed queries** that cover the main intent categories. They're the same for every document:\n",
    "\n",
    "| # | Query | Intent Covered |\n",
    "|---|-------|----------------|\n",
    "| 1 | \"What is this and what does it mean?\" | Definitional |\n",
    "| 2 | \"How do I do this step by step?\" | Procedural |\n",
    "| 3 | \"How much does this cost or how long does it take?\" | Quantitative |\n",
    "| 4 | \"What are the key facts I need to know?\" | Factual |\n",
    "| 5 | \"What problem does this solve?\" | Problem/Solution |\n",
    "\n",
    "### Generated Surrogates\n",
    "These are **5 document-specific queries** generated by the LLM:\n",
    "\n",
    "| # | Type | Description |\n",
    "|---|------|-------------|\n",
    "| 1 | Target Question | The ideal question this doc answers |\n",
    "| 2 | Keyword Query | How users actually search (no grammar) |\n",
    "| 3 | Symptom Query | The problem/symptom leading to this doc |\n",
    "| 4 | Misconception Query | Concerns or \"what NOT to do\" questions |\n",
    "| 5 | Messy Query | Real-world typing: abbreviations, urgency |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "define-static",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STATIC SURROGATES (same for every document)\n",
      "============================================================\n",
      "\n",
      "DEFINITIONAL\n",
      "  Query: \"What is this and what does it mean?\"\n",
      "  Covers: what is, define, meaning, explanation\n",
      "\n",
      "PROCEDURAL\n",
      "  Query: \"How do I do this step by step?\"\n",
      "  Covers: how to, instructions, guide\n",
      "\n",
      "QUANTITATIVE\n",
      "  Query: \"How much does this cost or how long does it take?\"\n",
      "  Covers: how much, how many, cost, duration\n",
      "\n",
      "FACTUAL\n",
      "  Query: \"What are the key facts I need to know?\"\n",
      "  Covers: who, when, where, facts\n",
      "\n",
      "PROBLEM\n",
      "  Query: \"What problem does this solve?\"\n",
      "  Covers: why, troubleshooting, help\n"
     ]
    }
   ],
   "source": [
    "# Define the 5 static surrogate queries\n",
    "STATIC_SURROGATES = {\n",
    "    'definitional': {\n",
    "        'query': 'What is this and what does it mean?',\n",
    "        'covers': 'what is, define, meaning, explanation',\n",
    "    },\n",
    "    'procedural': {\n",
    "        'query': 'How do I do this step by step?',\n",
    "        'covers': 'how to, instructions, guide',\n",
    "    },\n",
    "    'quantitative': {\n",
    "        'query': 'How much does this cost or how long does it take?',\n",
    "        'covers': 'how much, how many, cost, duration',\n",
    "    },\n",
    "    'factual': {\n",
    "        'query': 'What are the key facts I need to know?',\n",
    "        'covers': 'who, when, where, facts',\n",
    "    },\n",
    "    'problem': {\n",
    "        'query': 'What problem does this solve?',\n",
    "        'covers': 'why, troubleshooting, help',\n",
    "    },\n",
    "}\n",
    "\n",
    "print(\"STATIC SURROGATES (same for every document)\")\n",
    "print(\"=\"*60)\n",
    "for name, info in STATIC_SURROGATES.items():\n",
    "    print(f\"\\n{name.upper()}\")\n",
    "    print(f\"  Query: \\\"{info['query']}\\\"\")\n",
    "    print(f\"  Covers: {info['covers']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "define-generated-templates",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GENERATED SURROGATE TEMPLATES (document-specific)\n",
      "============================================================\n",
      "\n",
      "TARGET_QUESTION: Target Question\n",
      "\n",
      "KEYWORD_QUERY: Keyword Query\n",
      "\n",
      "SYMPTOM_SCENARIO: Symptom Query\n",
      "\n",
      "MISCONCEPTION_NEGATIVE: Misconception Query\n",
      "\n",
      "MESSY_REALWORLD: Messy Query\n"
     ]
    }
   ],
   "source": [
    "# Define templates for generating document-specific surrogates\n",
    "GENERATED_TEMPLATES = {\n",
    "    'target_question': {\n",
    "        'name': 'Target Question',\n",
    "        'prompt': (\n",
    "            \"You are helping index a document for search. Write the single most likely \"\n",
    "            \"natural language question that a user would ask that this document perfectly answers. \"\n",
    "            \"The question should be grammatically correct, clear, and specific. \"\n",
    "            \"Output only the question (5-12 words), nothing else.\\n\\n\"\n",
    "            \"Document:\"\n",
    "        ),\n",
    "    },\n",
    "    'keyword_query': {\n",
    "        'name': 'Keyword Query',\n",
    "        'prompt': (\n",
    "            \"You are helping index a document for search. Write a search query the way \"\n",
    "            \"real users type into Google: just keywords, no complete sentences, no question marks. \"\n",
    "            \"Think of someone quickly typing a few relevant words. \"\n",
    "            \"Output only the keyword query (3-6 words), nothing else.\\n\\n\"\n",
    "            \"Document:\"\n",
    "        ),\n",
    "    },\n",
    "    'symptom_scenario': {\n",
    "        'name': 'Symptom Query',\n",
    "        'prompt': (\n",
    "            \"You are helping index a document for search. This document contains a solution or answer. \"\n",
    "            \"Write a query that describes the PROBLEM or SYMPTOM that would lead someone to need this document. \"\n",
    "            \"Focus on what the user is experiencing, not what they want to learn. \"\n",
    "            \"Output only the problem-focused query (4-10 words), nothing else.\\n\\n\"\n",
    "            \"Document:\"\n",
    "        ),\n",
    "    },\n",
    "    'misconception_negative': {\n",
    "        'name': 'Misconception Query',\n",
    "        'prompt': (\n",
    "            \"You are helping index a document for search. Write a query that reflects \"\n",
    "            \"a common misconception, concern, or 'what NOT to do' question related to this topic. \"\n",
    "            \"Think of someone who is worried, skeptical, or wants to avoid mistakes. \"\n",
    "            \"Output only the concern/negative query (4-10 words), nothing else.\\n\\n\"\n",
    "            \"Document:\"\n",
    "        ),\n",
    "    },\n",
    "    'messy_realworld': {\n",
    "        'name': 'Messy Query',\n",
    "        'prompt': (\n",
    "            \"You are helping index a document for search. Write a messy, realistic search query \"\n",
    "            \"like someone would actually type in a hurry: use common abbreviations, \"\n",
    "            \"internet slang, or urgent language (help, asap, need, plz). \"\n",
    "            \"Output only the messy query (3-8 words), nothing else.\\n\\n\"\n",
    "            \"Document:\"\n",
    "        ),\n",
    "    },\n",
    "}\n",
    "\n",
    "print(\"GENERATED SURROGATE TEMPLATES (document-specific)\")\n",
    "print(\"=\"*60)\n",
    "for name, info in GENERATED_TEMPLATES.items():\n",
    "    print(f\"\\n{name.upper()}: {info['name']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "generate-surrogates",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating document-specific surrogates...\n",
      "(This uses the LLM to create queries tailored to this specific passage)\n",
      "\n",
      "Generating Target Question... Done!\n",
      "Generating Keyword Query... Done!\n",
      "Generating Symptom Query... Done!\n",
      "Generating Misconception Query... Done!\n",
      "Generating Messy Query... Done!\n",
      "\n",
      "All surrogates generated!\n"
     ]
    }
   ],
   "source": [
    "def generate_surrogate(doc_text, template_prompt, max_tokens=45):\n",
    "    \"\"\"\n",
    "    Generate a single surrogate query for a document using a template.\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": f\"{template_prompt}\\n\\nText:\\n{doc_text}\"}\n",
    "    ]\n",
    "    \n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_tokens,\n",
    "            temperature=0.3,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    generated_tokens = outputs[0][inputs['input_ids'].shape[1]:]\n",
    "    surrogate = tokenizer.decode(generated_tokens, skip_special_tokens=True).strip()\n",
    "    surrogate = surrogate.strip('\"\\'')\n",
    "    surrogate = surrogate.split('\\n')[0].strip()  # Take first line only\n",
    "    \n",
    "    return surrogate\n",
    "\n",
    "# Generate all 5 surrogates for our example document\n",
    "print(\"Generating document-specific surrogates...\")\n",
    "print(\"(This uses the LLM to create queries tailored to this specific passage)\")\n",
    "print()\n",
    "\n",
    "generated_surrogates = {}\n",
    "for key, template in GENERATED_TEMPLATES.items():\n",
    "    print(f\"Generating {template['name']}...\", end=\" \")\n",
    "    generated_surrogates[key] = generate_surrogate(\n",
    "        example['passage'], \n",
    "        template['prompt']\n",
    "    )\n",
    "    print(\"Done!\")\n",
    "\n",
    "print(\"\\nAll surrogates generated!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "show-surrogates",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "COMPARISON: STATIC vs GENERATED SURROGATES\n",
      "================================================================================\n",
      "\n",
      "Actual user query: \"walgreens store sales average\"\n",
      "\n",
      "----------------------------------------\n",
      "STATIC SURROGATES (identical for all docs)\n",
      "----------------------------------------\n",
      "  definitional   : \"What is this and what does it mean?\"\n",
      "  procedural     : \"How do I do this step by step?\"\n",
      "  quantitative   : \"How much does this cost or how long does it take?\"\n",
      "  factual        : \"What are the key facts I need to know?\"\n",
      "  problem        : \"What problem does this solve?\"\n",
      "\n",
      "----------------------------------------\n",
      "GENERATED SURROGATES (specific to this document)\n",
      "----------------------------------------\n",
      "  Target Question     : \"What is the salary range for different positions at Walgreens, based on 7,810 data points?\"\n",
      "  Keyword Query       : \"Walgreens salary ranges\"\n",
      "  Symptom Query       : \"What is the average Walgreens salary or hourly pay for specific positions?\"\n",
      "  Misconception Query : \"What not to earn working at Walgreens: min. $15,000 or max. $179,900?\" (for someone who assumes there's a fixed salary bracket\"\n",
      "  Messy Query         : \"urgent: what's avg walgreens salary for dsmt plz? hourly pay pharm manager asap? data from 7.8k sources!\"\n"
     ]
    }
   ],
   "source": [
    "# Display both static and generated surrogates side by side\n",
    "print(\"=\"*80)\n",
    "print(\"COMPARISON: STATIC vs GENERATED SURROGATES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nActual user query: \\\"{example['query']}\\\"\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*40)\n",
    "print(\"STATIC SURROGATES (identical for all docs)\")\n",
    "print(\"-\"*40)\n",
    "for name, info in STATIC_SURROGATES.items():\n",
    "    print(f\"  {name:<15}: \\\"{info['query']}\\\"\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*40)\n",
    "print(\"GENERATED SURROGATES (specific to this document)\")\n",
    "print(\"-\"*40)\n",
    "for key, surrogate in generated_surrogates.items():\n",
    "    name = GENERATED_TEMPLATES[key]['name']\n",
    "    print(f\"  {name:<20}: \\\"{surrogate}\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "compute-similarities",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SEMANTIC SIMILARITY TO ACTUAL QUERY\n",
      "================================================================================\n",
      "\n",
      "Actual query: \"walgreens store sales average\"\n",
      "\n",
      "Higher similarity = surrogate is more semantically related to the actual query\n",
      "\n",
      "----------------------------------------\n",
      "STATIC SURROGATES\n",
      "----------------------------------------\n",
      "  definitional   : -0.0774\n",
      "  procedural     : 0.0368\n",
      "  quantitative   : 0.0101\n",
      "  factual        : -0.0916\n",
      "  problem        : 0.0027\n",
      "\n",
      "----------------------------------------\n",
      "GENERATED SURROGATES\n",
      "----------------------------------------\n",
      "  Target Question     : 0.5695\n",
      "  Keyword Query       : 0.7552\n",
      "  Symptom Query       : 0.6214\n",
      "  Misconception Query : 0.5299\n",
      "  Messy Query         : 0.4897\n",
      "\n",
      "================================================================================\n",
      "Best static match: procedural (sim=0.0368)\n",
      "Best generated match: Keyword Query (sim=0.7552)\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Compute semantic similarity between each surrogate and the actual query\n",
    "def compute_similarity(text1, text2):\n",
    "    \"\"\"Compute cosine similarity between two texts using embeddings.\"\"\"\n",
    "    embeddings = embed_model.encode([text1, text2])\n",
    "    return float(cosine_similarity([embeddings[0]], [embeddings[1]])[0][0])\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"SEMANTIC SIMILARITY TO ACTUAL QUERY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nActual query: \\\"{example['query']}\\\"\")\n",
    "print(\"\\nHigher similarity = surrogate is more semantically related to the actual query\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*40)\n",
    "print(\"STATIC SURROGATES\")\n",
    "print(\"-\"*40)\n",
    "static_similarities = {}\n",
    "for name, info in STATIC_SURROGATES.items():\n",
    "    sim = compute_similarity(info['query'], example['query'])\n",
    "    static_similarities[name] = sim\n",
    "    print(f\"  {name:<15}: {sim:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*40)\n",
    "print(\"GENERATED SURROGATES\")\n",
    "print(\"-\"*40)\n",
    "generated_similarities = {}\n",
    "for key, surrogate in generated_surrogates.items():\n",
    "    sim = compute_similarity(surrogate, example['query'])\n",
    "    generated_similarities[key] = sim\n",
    "    name = GENERATED_TEMPLATES[key]['name']\n",
    "    print(f\"  {name:<20}: {sim:.4f}\")\n",
    "\n",
    "# Find best matches\n",
    "best_static = max(static_similarities.items(), key=lambda x: x[1])\n",
    "best_generated = max(generated_similarities.items(), key=lambda x: x[1])\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"Best static match: {best_static[0]} (sim={best_static[1]:.4f})\")\n",
    "print(f\"Best generated match: {GENERATED_TEMPLATES[best_generated[0]]['name']} (sim={best_generated[1]:.4f})\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cache-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 4: Building KV Caches\n",
    "\n",
    "Now we'll see how the KV caches are constructed. A **KV (Key-Value) cache** stores the intermediate computations from the transformer's attention mechanism. When we process a context once, we can reuse these computations for subsequent queries.\n",
    "\n",
    "### Cache Templates\n",
    "\n",
    "```\n",
    "BASELINE CACHE:\n",
    "┌─────────────────────────────────┐\n",
    "│ Document:                       │\n",
    "│ {passage text}                  │\n",
    "└─────────────────────────────────┘\n",
    "\n",
    "SURROGATE-PRIMED CACHE:\n",
    "┌─────────────────────────────────────────────────┐\n",
    "│ This document may be relevant to queries like:  │\n",
    "│ {surrogate query}                               │\n",
    "│                                                 │\n",
    "│ Document:                                       │\n",
    "│ {passage text}                                  │\n",
    "└─────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "The hypothesis is that the surrogate-primed cache will be better prepared to answer queries similar to the surrogate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cache-templates",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CACHE TEMPLATES\n",
      "============================================================\n",
      "\n",
      "BASELINE TEMPLATE:\n",
      "----------------------------------------\n",
      "Document:\n",
      "[passage text]\n",
      "\n",
      "SURROGATE-PRIMED TEMPLATE:\n",
      "----------------------------------------\n",
      "This document may be relevant to queries like: [surrogate query]\n",
      "\n",
      "Document:\n",
      "[passage text]\n",
      "\n",
      "QUERY TEMPLATE (appended at query time):\n",
      "----------------------------------------\n",
      "\n",
      "\n",
      "Query: [user query]\n",
      "\n",
      "Answer:\n"
     ]
    }
   ],
   "source": [
    "# Define the cache templates\n",
    "BASELINE_TEMPLATE = \"Document:\\n{document}\"\n",
    "\n",
    "SURROGATE_TEMPLATE = (\n",
    "    \"This document may be relevant to queries like: {surrogate}\\n\\n\"\n",
    "    \"Document:\\n{document}\"\n",
    ")\n",
    "\n",
    "QUERY_TEMPLATE = \"\\n\\nQuery: {query}\\n\\nAnswer:\"\n",
    "\n",
    "print(\"CACHE TEMPLATES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nBASELINE TEMPLATE:\")\n",
    "print(\"-\"*40)\n",
    "print(BASELINE_TEMPLATE.format(document=\"[passage text]\"))\n",
    "\n",
    "print(\"\\nSURROGATE-PRIMED TEMPLATE:\")\n",
    "print(\"-\"*40)\n",
    "print(SURROGATE_TEMPLATE.format(surrogate=\"[surrogate query]\", document=\"[passage text]\"))\n",
    "\n",
    "print(\"\\nQUERY TEMPLATE (appended at query time):\")\n",
    "print(\"-\"*40)\n",
    "print(QUERY_TEMPLATE.format(query=\"[user query]\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "show-actual-contexts",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ACTUAL CACHE CONTEXTS FOR OUR EXAMPLE\n",
      "================================================================================\n",
      "\n",
      "----------------------------------------\n",
      "BASELINE CONTEXT\n",
      "----------------------------------------\n",
      "Document:\n",
      "The average Walgreens salary ranges from approximately $15,000 per year for Customer Service Associate / Cashier to $179,900 per year for District Manager. Average Walgreens hourly pay ranges from approximately $7.35 per hour for Laboratory Technician to $68.90 per hour for Pharmacy Manager. Salary information comes from 7,810 data points collected directly from employees, users, and jobs on Indeed.\n",
      "\n",
      "----------------------------------------\n",
      "STATIC SURROGATE CONTEXT (using 'procedural')\n",
      "----------------------------------------\n",
      "This document may be relevant to queries like: How do I do this step by step?\n",
      "\n",
      "Document:\n",
      "The average Walgreens salary ranges from approximately $15,000 per year for Customer Service Associate / Cashier to $179,900 per year for District Manager. Average Walgreens hourly pay ranges from approximately $7.35 per hour for Laboratory Technician to $68.90 per hour for Pharmacy Manager. Salary information comes from 7,810 data points collected directly from employees, users, and jobs on Indeed.\n",
      "\n",
      "----------------------------------------\n",
      "GENERATED SURROGATE CONTEXT (using 'Keyword Query')\n",
      "----------------------------------------\n",
      "This document may be relevant to queries like: Walgreens salary ranges\n",
      "\n",
      "Document:\n",
      "The average Walgreens salary ranges from approximately $15,000 per year for Customer Service Associate / Cashier to $179,900 per year for District Manager. Average Walgreens hourly pay ranges from approximately $7.35 per hour for Laboratory Technician to $68.90 per hour for Pharmacy Manager. Salary information comes from 7,810 data points collected directly from employees, users, and jobs on Indeed.\n"
     ]
    }
   ],
   "source": [
    "# Show the actual contexts for our example\n",
    "baseline_context = BASELINE_TEMPLATE.format(document=example['passage'])\n",
    "\n",
    "# Pick the best-matching surrogate for demonstration\n",
    "best_static_query = STATIC_SURROGATES[best_static[0]]['query']\n",
    "best_generated_query = generated_surrogates[best_generated[0]]\n",
    "\n",
    "static_context = SURROGATE_TEMPLATE.format(\n",
    "    surrogate=best_static_query, \n",
    "    document=example['passage']\n",
    ")\n",
    "\n",
    "generated_context = SURROGATE_TEMPLATE.format(\n",
    "    surrogate=best_generated_query,\n",
    "    document=example['passage']\n",
    ")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"ACTUAL CACHE CONTEXTS FOR OUR EXAMPLE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n\" + \"-\"*40)\n",
    "print(\"BASELINE CONTEXT\")\n",
    "print(\"-\"*40)\n",
    "print(baseline_context[:500] + \"...\" if len(baseline_context) > 500 else baseline_context)\n",
    "\n",
    "print(\"\\n\" + \"-\"*40)\n",
    "print(f\"STATIC SURROGATE CONTEXT (using '{best_static[0]}')\")\n",
    "print(\"-\"*40)\n",
    "print(static_context[:600] + \"...\" if len(static_context) > 600 else static_context)\n",
    "\n",
    "print(\"\\n\" + \"-\"*40)\n",
    "print(f\"GENERATED SURROGATE CONTEXT (using '{GENERATED_TEMPLATES[best_generated[0]]['name']}')\")\n",
    "print(\"-\"*40)\n",
    "print(generated_context[:600] + \"...\" if len(generated_context) > 600 else generated_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "build-cache-function",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function defined: build_kv_cache()\n",
      "\n",
      "This function:\n",
      "  1. Tokenizes the context\n",
      "  2. Runs a forward pass through the model\n",
      "  3. Returns the KV cache (stored attention states)\n"
     ]
    }
   ],
   "source": [
    "def build_kv_cache(context):\n",
    "    \"\"\"\n",
    "    Build a KV cache from the given context.\n",
    "    \n",
    "    This runs a forward pass through the model, storing the key and value\n",
    "    tensors from each attention layer. These can be reused for subsequent\n",
    "    queries, avoiding redundant computation.\n",
    "    \n",
    "    Returns:\n",
    "        context_length: Number of tokens in the context\n",
    "        past_key_values: The cached key-value pairs for all layers\n",
    "    \"\"\"\n",
    "    # Tokenize the context\n",
    "    context_encoding = tokenizer(\n",
    "        context, \n",
    "        return_tensors=\"pt\", \n",
    "        add_special_tokens=True,\n",
    "        padding=False, \n",
    "        truncation=False\n",
    "    )\n",
    "    context_ids = context_encoding['input_ids'].to(DEVICE)\n",
    "    \n",
    "    # Forward pass to build cache\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            input_ids=context_ids,\n",
    "            attention_mask=torch.ones_like(context_ids),\n",
    "            use_cache=True,  # This tells the model to return the KV cache\n",
    "            return_dict=True\n",
    "        )\n",
    "    \n",
    "    return context_ids.shape[1], outputs.past_key_values\n",
    "\n",
    "print(\"Function defined: build_kv_cache()\")\n",
    "print(\"\\nThis function:\")\n",
    "print(\"  1. Tokenizes the context\")\n",
    "print(\"  2. Runs a forward pass through the model\")\n",
    "print(\"  3. Returns the KV cache (stored attention states)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "build-caches",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building KV caches...\n",
      "\n",
      "Building baseline cache... Done! (107 tokens)\n",
      "Building static surrogate cache... Done! (127 tokens)\n",
      "Building generated surrogate cache... Done! (123 tokens)\n",
      "\n",
      "============================================================\n",
      "CACHE SUMMARY\n",
      "============================================================\n",
      "Baseline cache:           107 tokens\n",
      "Static surrogate cache:   127 tokens\n",
      "Generated surrogate cache:  123 tokens\n",
      "\n",
      "Extra tokens from surrogate priming: ~20 tokens\n"
     ]
    }
   ],
   "source": [
    "# Build the caches for our example\n",
    "print(\"Building KV caches...\")\n",
    "print()\n",
    "\n",
    "print(\"Building baseline cache...\", end=\" \")\n",
    "baseline_len, baseline_cache = build_kv_cache(baseline_context)\n",
    "print(f\"Done! ({baseline_len} tokens)\")\n",
    "\n",
    "print(\"Building static surrogate cache...\", end=\" \")\n",
    "static_len, static_cache = build_kv_cache(static_context)\n",
    "print(f\"Done! ({static_len} tokens)\")\n",
    "\n",
    "print(\"Building generated surrogate cache...\", end=\" \")\n",
    "generated_len, generated_cache = build_kv_cache(generated_context)\n",
    "print(f\"Done! ({generated_len} tokens)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CACHE SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Baseline cache:          {baseline_len:>4} tokens\")\n",
    "print(f\"Static surrogate cache:  {static_len:>4} tokens\")\n",
    "print(f\"Generated surrogate cache: {generated_len:>4} tokens\")\n",
    "print(f\"\\nExtra tokens from surrogate priming: ~{static_len - baseline_len} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "explain-cache",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "UNDERSTANDING THE KV CACHE STRUCTURE\n",
      "============================================================\n",
      "\n",
      "Cache type: DynamicCache\n",
      "Cache attributes: ['batch_repeat_interleave', 'batch_select_indices', 'crop', 'early_initialization', 'get_mask_sizes', 'get_max_cache_shape', 'get_seq_length', 'is_compileable', 'is_initialized', 'is_sliding', 'layer_class_to_replicate', 'layers', 'max_batch_size', 'max_cache_len', 'offload', 'offloading', 'prefetch', 'reorder_cache', 'reset', 'update']\n",
      "\n",
      "Note: Could not inspect cache internals (Cannot access cache internals)\n",
      "The cache has 32 layers.\n",
      "Each layer contains key and value tensors for the attention mechanism.\n",
      "\n",
      "------------------------------------------------------------\n",
      "KEY INSIGHT:\n",
      "------------------------------------------------------------\n",
      "The KV cache stores the attention states for every token in the context.\n",
      "When we add a query, we only need to compute attention for the new tokens,\n",
      "while reusing the cached states for the context. This is the efficiency gain.\n"
     ]
    }
   ],
   "source": [
    "# Explain the structure of the KV cache\n",
    "print(\"=\"*60)\n",
    "print(\"UNDERSTANDING THE KV CACHE STRUCTURE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check what type of cache we have and how to access it\n",
    "cache_type = type(baseline_cache).__name__\n",
    "print(f\"\\nCache type: {cache_type}\")\n",
    "\n",
    "# Try to access keys/values based on the cache type\n",
    "try:\n",
    "    # Try DynamicCache style access first\n",
    "    if hasattr(baseline_cache, 'key_cache') and baseline_cache.key_cache:\n",
    "        num_layers = len(baseline_cache.key_cache)\n",
    "        layer_0_key = baseline_cache.key_cache[0]\n",
    "        layer_0_value = baseline_cache.value_cache[0]\n",
    "        print(f\"The cache contains {num_layers} layers (one per transformer layer).\")\n",
    "    elif hasattr(baseline_cache, 'get_seq_length'):\n",
    "        # DynamicCache with different structure - iterate to get first layer\n",
    "        # Access via to_legacy_cache() if available\n",
    "        if hasattr(baseline_cache, 'to_legacy_cache'):\n",
    "            legacy_cache = baseline_cache.to_legacy_cache()\n",
    "            num_layers = len(legacy_cache)\n",
    "            layer_0_key = legacy_cache[0][0]\n",
    "            layer_0_value = legacy_cache[0][1]\n",
    "        else:\n",
    "            # Get the internal _data attribute or similar\n",
    "            print(f\"Cache attributes: {[a for a in dir(baseline_cache) if not a.startswith('_')]}\")\n",
    "            raise AttributeError(\"Cannot access cache internals\")\n",
    "        print(f\"The cache contains {num_layers} layers (one per transformer layer).\")\n",
    "    else:\n",
    "        # Tuple-style cache\n",
    "        num_layers = len(baseline_cache)\n",
    "        layer_0_key = baseline_cache[0][0]\n",
    "        layer_0_value = baseline_cache[0][1]\n",
    "        print(f\"The cache is a tuple of {num_layers} layers.\")\n",
    "    \n",
    "    print(\"Each layer stores key and value tensors for attention.\")\n",
    "    print(f\"\\nLayer 0 key shape:   {list(layer_0_key.shape)}\")\n",
    "    print(f\"Layer 0 value shape: {list(layer_0_value.shape)}\")\n",
    "\n",
    "    print(f\"\\nDimensions:\")\n",
    "    print(f\"  - Batch size: {layer_0_key.shape[0]}\")\n",
    "    print(f\"  - Num attention heads: {layer_0_key.shape[1]}\")\n",
    "    print(f\"  - Sequence length: {layer_0_key.shape[2]} (= context tokens)\")\n",
    "    print(f\"  - Head dimension: {layer_0_key.shape[3]}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nNote: Could not inspect cache internals ({e})\")\n",
    "    print(f\"The cache has {len(baseline_cache)} layers.\")\n",
    "    print(\"Each layer contains key and value tensors for the attention mechanism.\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"KEY INSIGHT:\")\n",
    "print(\"-\"*60)\n",
    "print(\"The KV cache stores the attention states for every token in the context.\")\n",
    "print(\"When we add a query, we only need to compute attention for the new tokens,\")\n",
    "print(\"while reusing the cached states for the context. This is the efficiency gain.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scoring-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 5: Running Generation/Scoring with the Cache\n",
    "\n",
    "Now we'll see how to use the KV cache to score an answer. The key metric is **Negative Log-Likelihood (NLL)**:\n",
    "\n",
    "$$\\text{NLL} = -\\frac{1}{N} \\sum_{i=1}^{N} \\log P(t_i | t_1, ..., t_{i-1}, \\text{context})$$\n",
    "\n",
    "Where:\n",
    "- $N$ is the number of tokens in the answer\n",
    "- $P(t_i | ...)$ is the probability the model assigns to token $t_i$ given all previous tokens\n",
    "\n",
    "**Lower NLL = Model is more confident in the answer = Better cache quality**\n",
    "\n",
    "### The Scoring Process\n",
    "\n",
    "```\n",
    "1. Start with pre-built cache (context already processed)\n",
    "2. Append the query to the cache\n",
    "3. For each token in the answer:\n",
    "   - Get model's probability for that token\n",
    "   - Compute log probability\n",
    "4. Average the negative log probabilities\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "scoring-function",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function defined: score_answer_with_cache()\n"
     ]
    }
   ],
   "source": [
    "def score_answer_with_cache(past_key_values, context_len, query_prompt, answer):\n",
    "    \"\"\"\n",
    "    Score an answer using a pre-built KV cache.\n",
    "    \n",
    "    The process:\n",
    "    1. Extend the cache by processing the query\n",
    "    2. Compute the probability of each token in the answer\n",
    "    3. Return the mean negative log-likelihood\n",
    "    \n",
    "    Lower NLL = model is more confident = better cache priming\n",
    "    \n",
    "    Args:\n",
    "        past_key_values: Pre-built KV cache from context\n",
    "        context_len: Number of tokens in the cached context\n",
    "        query_prompt: The query to append (formatted)\n",
    "        answer: The answer to score\n",
    "    \n",
    "    Returns:\n",
    "        Mean NLL for the answer tokens\n",
    "    \"\"\"\n",
    "    # Step 1: Tokenize the query\n",
    "    query_encoding = tokenizer(\n",
    "        query_prompt, \n",
    "        return_tensors=\"pt\", \n",
    "        add_special_tokens=False,\n",
    "        padding=False, \n",
    "        truncation=False\n",
    "    )\n",
    "    query_ids = query_encoding['input_ids'].to(DEVICE)\n",
    "    query_len = query_ids.shape[1]\n",
    "    \n",
    "    # Step 2: Tokenize the answer\n",
    "    answer_encoding = tokenizer(\n",
    "        answer, \n",
    "        return_tensors=\"pt\", \n",
    "        add_special_tokens=False,\n",
    "        padding=False, \n",
    "        truncation=False\n",
    "    )\n",
    "    answer_ids = answer_encoding['input_ids'].to(DEVICE)\n",
    "    answer_len = answer_ids.shape[1]\n",
    "    \n",
    "    # Step 3: Extend cache with query\n",
    "    combined_len = context_len + query_len\n",
    "    attention_mask = torch.ones((1, combined_len), device=DEVICE)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        query_outputs = model(\n",
    "            input_ids=query_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            past_key_values=past_key_values,  # Reuse the cache!\n",
    "            use_cache=True,\n",
    "            return_dict=True\n",
    "        )\n",
    "        extended_cache = query_outputs.past_key_values\n",
    "    \n",
    "    # Step 4: Score the answer\n",
    "    combined_len_final = context_len + query_len + answer_len\n",
    "    attention_mask_final = torch.ones((1, combined_len_final), device=DEVICE)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        answer_outputs = model(\n",
    "            input_ids=answer_ids,\n",
    "            attention_mask=attention_mask_final,\n",
    "            past_key_values=extended_cache,\n",
    "            use_cache=False,\n",
    "            return_dict=True\n",
    "        )\n",
    "    \n",
    "    # Step 5: Compute NLL\n",
    "    # The logits predict the NEXT token, so we shift\n",
    "    logits = answer_outputs.logits  # Shape: [1, answer_len, vocab_size]\n",
    "    shift_logits = logits[:, :-1, :].contiguous()  # All but last\n",
    "    shift_labels = answer_ids[:, 1:].contiguous()  # All but first\n",
    "    \n",
    "    # Flatten for cross-entropy\n",
    "    shift_logits = shift_logits.view(-1, shift_logits.size(-1))\n",
    "    shift_labels = shift_labels.view(-1)\n",
    "    \n",
    "    # Compute cross-entropy loss (= NLL)\n",
    "    loss_fct = torch.nn.CrossEntropyLoss(reduction='sum')\n",
    "    nll = loss_fct(shift_logits, shift_labels).item()\n",
    "    \n",
    "    # Average over number of tokens scored\n",
    "    num_scored = answer_len - 1\n",
    "    return nll / num_scored if num_scored > 0 else 0.0\n",
    "\n",
    "print(\"Function defined: score_answer_with_cache()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "score-example",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "SCORING THE ANSWER WITH DIFFERENT CACHES\n",
      "============================================================\n",
      "\n",
      "Query: \"walgreens store sales average\"\n",
      "Answer: \"Approximately $15,000 per year.\"\n",
      "\n",
      "Scoring with each cache...\n",
      "\n",
      "Scoring with baseline cache... NLL = 2.9583\n",
      "Scoring with static surrogate cache... NLL = 2.4583\n",
      "Scoring with generated surrogate cache... NLL = 2.7292\n"
     ]
    }
   ],
   "source": [
    "# Score the answer with each cache\n",
    "query_prompt = QUERY_TEMPLATE.format(query=example['query'])\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"SCORING THE ANSWER WITH DIFFERENT CACHES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nQuery: \\\"{example['query']}\\\"\")\n",
    "print(f\"Answer: \\\"{example['answer']}\\\"\")\n",
    "\n",
    "print(\"\\nScoring with each cache...\")\n",
    "\n",
    "print(\"\\nScoring with baseline cache...\", end=\" \")\n",
    "baseline_nll = score_answer_with_cache(\n",
    "    baseline_cache, baseline_len, query_prompt, example['answer']\n",
    ")\n",
    "print(f\"NLL = {baseline_nll:.4f}\")\n",
    "\n",
    "print(\"Scoring with static surrogate cache...\", end=\" \")\n",
    "static_nll = score_answer_with_cache(\n",
    "    static_cache, static_len, query_prompt, example['answer']\n",
    ")\n",
    "print(f\"NLL = {static_nll:.4f}\")\n",
    "\n",
    "print(\"Scoring with generated surrogate cache...\", end=\" \")\n",
    "generated_nll = score_answer_with_cache(\n",
    "    generated_cache, generated_len, query_prompt, example['answer']\n",
    ")\n",
    "print(f\"NLL = {generated_nll:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "show-results",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "RESULTS SUMMARY\n",
      "================================================================================\n",
      "\n",
      "----------------------------------------\n",
      "ANSWER NLL BY CACHE TYPE (lower = better)\n",
      "----------------------------------------\n",
      "Cache Type                            NLL     vs Baseline\n",
      "-------------------------------------------------------\n",
      "Baseline (document only)           2.9583               -\n",
      "Static Surrogate                   2.4583         +0.5000\n",
      "Generated Surrogate                2.7292         +0.2292\n",
      "\n",
      "----------------------------------------\n",
      "INTERPRETATION\n",
      "----------------------------------------\n",
      "\n",
      "Best cache for this example: Static Surrogate (NLL=2.4583)\n",
      "\n",
      "Static surrogate IMPROVES over baseline by 0.5000 NLL\n",
      "Generated surrogate IMPROVES over baseline by 0.2292 NLL\n",
      "\n",
      "Static beats Generated by 0.2708 NLL\n"
     ]
    }
   ],
   "source": [
    "# Display results with interpretation\n",
    "print(\"=\"*80)\n",
    "print(\"RESULTS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n\" + \"-\"*40)\n",
    "print(\"ANSWER NLL BY CACHE TYPE (lower = better)\")\n",
    "print(\"-\"*40)\n",
    "print(f\"{'Cache Type':<30} {'NLL':>10} {'vs Baseline':>15}\")\n",
    "print(\"-\"*55)\n",
    "print(f\"{'Baseline (document only)':<30} {baseline_nll:>10.4f} {'-':>15}\")\n",
    "print(f\"{'Static Surrogate':<30} {static_nll:>10.4f} {baseline_nll - static_nll:>+15.4f}\")\n",
    "print(f\"{'Generated Surrogate':<30} {generated_nll:>10.4f} {baseline_nll - generated_nll:>+15.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*40)\n",
    "print(\"INTERPRETATION\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "# Determine winner\n",
    "nlls = {\n",
    "    'Baseline': baseline_nll,\n",
    "    'Static Surrogate': static_nll,\n",
    "    'Generated Surrogate': generated_nll\n",
    "}\n",
    "winner = min(nlls.items(), key=lambda x: x[1])\n",
    "\n",
    "print(f\"\\nBest cache for this example: {winner[0]} (NLL={winner[1]:.4f})\")\n",
    "\n",
    "static_delta = baseline_nll - static_nll\n",
    "generated_delta = baseline_nll - generated_nll\n",
    "\n",
    "if static_delta > 0:\n",
    "    print(f\"\\nStatic surrogate IMPROVES over baseline by {static_delta:.4f} NLL\")\n",
    "else:\n",
    "    print(f\"\\nStatic surrogate WORSE than baseline by {-static_delta:.4f} NLL\")\n",
    "\n",
    "if generated_delta > 0:\n",
    "    print(f\"Generated surrogate IMPROVES over baseline by {generated_delta:.4f} NLL\")\n",
    "else:\n",
    "    print(f\"Generated surrogate WORSE than baseline by {-generated_delta:.4f} NLL\")\n",
    "\n",
    "if generated_nll < static_nll:\n",
    "    print(f\"\\nGenerated beats Static by {static_nll - generated_nll:.4f} NLL\")\n",
    "else:\n",
    "    print(f\"\\nStatic beats Generated by {generated_nll - static_nll:.4f} NLL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metric-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 6: Understanding the Evaluation Metric (NLL)\n",
    "\n",
    "Let's dig deeper into what NLL means and why we use it.\n",
    "\n",
    "### What is Negative Log-Likelihood?\n",
    "\n",
    "For each token in the answer, the model outputs a probability distribution over all possible next tokens. NLL measures how \"surprised\" the model is by the correct answer:\n",
    "\n",
    "- **Low NLL**: Model assigns high probability to the correct tokens → Confident, good prediction\n",
    "- **High NLL**: Model assigns low probability to the correct tokens → Uncertain, poor prediction\n",
    "\n",
    "### Why NLL Instead of Generated Text?\n",
    "\n",
    "1. **Deterministic**: Same input always gives same score (no sampling randomness)\n",
    "2. **Fine-grained**: Captures subtle differences in model confidence\n",
    "3. **Fast**: No need to generate full responses\n",
    "4. **Comparable**: Easy to compare across conditions statistically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "detailed-nll",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting token-level probabilities for deeper analysis...\n"
     ]
    }
   ],
   "source": [
    "# Let's look at token-by-token probabilities for deeper understanding\n",
    "def get_token_probabilities(past_key_values, context_len, query_prompt, answer):\n",
    "    \"\"\"\n",
    "    Get per-token probabilities for the answer.\n",
    "    Returns list of (token, probability, log_prob) tuples.\n",
    "    \"\"\"\n",
    "    # Tokenize\n",
    "    query_encoding = tokenizer(query_prompt, return_tensors=\"pt\", add_special_tokens=False)\n",
    "    query_ids = query_encoding['input_ids'].to(DEVICE)\n",
    "    query_len = query_ids.shape[1]\n",
    "    \n",
    "    answer_encoding = tokenizer(answer, return_tensors=\"pt\", add_special_tokens=False)\n",
    "    answer_ids = answer_encoding['input_ids'].to(DEVICE)\n",
    "    answer_len = answer_ids.shape[1]\n",
    "    \n",
    "    # Extend cache with query\n",
    "    combined_len = context_len + query_len\n",
    "    attention_mask = torch.ones((1, combined_len), device=DEVICE)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        query_outputs = model(\n",
    "            input_ids=query_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            past_key_values=past_key_values,\n",
    "            use_cache=True,\n",
    "            return_dict=True\n",
    "        )\n",
    "        extended_cache = query_outputs.past_key_values\n",
    "    \n",
    "    # Score answer\n",
    "    combined_len_final = context_len + query_len + answer_len\n",
    "    attention_mask_final = torch.ones((1, combined_len_final), device=DEVICE)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        answer_outputs = model(\n",
    "            input_ids=answer_ids,\n",
    "            attention_mask=attention_mask_final,\n",
    "            past_key_values=extended_cache,\n",
    "            use_cache=False,\n",
    "            return_dict=True\n",
    "        )\n",
    "    \n",
    "    # Get per-token probabilities\n",
    "    logits = answer_outputs.logits\n",
    "    probs = torch.softmax(logits, dim=-1)\n",
    "    \n",
    "    token_info = []\n",
    "    for i in range(answer_len - 1):\n",
    "        next_token_id = answer_ids[0, i + 1].item()\n",
    "        next_token = tokenizer.decode([next_token_id])\n",
    "        prob = probs[0, i, next_token_id].item()\n",
    "        log_prob = np.log(prob + 1e-10)\n",
    "        token_info.append((next_token, prob, -log_prob))\n",
    "    \n",
    "    return token_info\n",
    "\n",
    "print(\"Getting token-level probabilities for deeper analysis...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "show-token-probs",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TOKEN-BY-TOKEN PROBABILITY ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "Answer being scored: \"Approximately $15,000 per year.\"\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Token             Baseline Prob  Generated Prob       Winner\n",
      "----------------------------------------------------------------------\n",
      "'xim'                    0.0140          0.1338    Generated\n",
      "'ately'                  0.0008          0.0002     Baseline\n",
      "'$'                      0.0030          0.0065    Generated\n",
      "'1'                      0.9414          0.3926     Baseline\n",
      "'5'                      0.0933          0.0369     Baseline\n",
      "','                      0.3242          0.2129     Baseline\n",
      "'0'                      0.0064          0.0157    Generated\n",
      "'0'                      0.0306          0.0374    Generated\n",
      "'0'                      0.1865          0.1348     Baseline\n",
      "'per'                    0.0012          0.0004     Baseline\n",
      "'year'                   0.9922          0.9883     Baseline\n",
      "'.'                      0.0000          0.0000    Generated\n",
      "----------------------------------------------------------------------\n",
      "AVERAGE                  0.2161          0.1633\n"
     ]
    }
   ],
   "source": [
    "# Compare token probabilities between caches\n",
    "print(\"=\"*80)\n",
    "print(\"TOKEN-BY-TOKEN PROBABILITY ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nAnswer being scored: \\\"{example['answer']}\\\"\")\n",
    "\n",
    "baseline_tokens = get_token_probabilities(\n",
    "    baseline_cache, baseline_len, query_prompt, example['answer']\n",
    ")\n",
    "generated_tokens = get_token_probabilities(\n",
    "    generated_cache, generated_len, query_prompt, example['answer']\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(f\"{'Token':<15} {'Baseline Prob':>15} {'Generated Prob':>15} {'Winner':>12}\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "for (tok1, prob1, nll1), (tok2, prob2, nll2) in zip(baseline_tokens, generated_tokens):\n",
    "    winner = \"Generated\" if prob2 > prob1 else \"Baseline\" if prob1 > prob2 else \"Tie\"\n",
    "    tok_display = repr(tok1) if len(tok1.strip()) > 0 else \"' '\"\n",
    "    print(f\"{tok_display:<15} {prob1:>15.4f} {prob2:>15.4f} {winner:>12}\")\n",
    "\n",
    "print(\"-\"*70)\n",
    "avg_baseline = np.mean([x[1] for x in baseline_tokens])\n",
    "avg_generated = np.mean([x[1] for x in generated_tokens])\n",
    "print(f\"{'AVERAGE':<15} {avg_baseline:>15.4f} {avg_generated:>15.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "nll-explanation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "HOW NLL IS COMPUTED\n",
      "================================================================================\n",
      "\n",
      "For each token in the answer, we:\n",
      "1. Get the model's predicted probability for that token\n",
      "2. Take the negative log: -log(probability)\n",
      "3. Average across all tokens\n",
      "\n",
      "Example calculation (baseline cache):\n",
      "\n",
      "Token            Probability   -log(prob)\n",
      "------------------------------------------\n",
      "'xim'                 0.0140       4.2703\n",
      "'ately'               0.0008       7.1391\n",
      "'$'                   0.0030       5.8072\n",
      "'1'                   0.9414       0.0604\n",
      "'5'                   0.0933       2.3723\n",
      "...                      ...          ...\n",
      "------------------------------------------\n",
      "Mean NLL = 4.4645\n",
      "(This is the score we use to compare caches)\n"
     ]
    }
   ],
   "source": [
    "# Visual explanation of NLL\n",
    "print(\"=\"*80)\n",
    "print(\"HOW NLL IS COMPUTED\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\"\"\n",
    "For each token in the answer, we:\n",
    "1. Get the model's predicted probability for that token\n",
    "2. Take the negative log: -log(probability)\n",
    "3. Average across all tokens\n",
    "\n",
    "Example calculation (baseline cache):\n",
    "\"\"\")\n",
    "\n",
    "print(f\"{'Token':<15} {'Probability':>12} {'-log(prob)':>12}\")\n",
    "print(\"-\"*42)\n",
    "total_nll = 0\n",
    "for i, (tok, prob, nll) in enumerate(baseline_tokens[:5]):  # Show first 5\n",
    "    tok_display = repr(tok) if len(tok.strip()) > 0 else \"' '\"\n",
    "    print(f\"{tok_display:<15} {prob:>12.4f} {nll:>12.4f}\")\n",
    "    total_nll += nll\n",
    "\n",
    "if len(baseline_tokens) > 5:\n",
    "    print(f\"{'...':<15} {'...':>12} {'...':>12}\")\n",
    "\n",
    "mean_nll = np.mean([x[2] for x in baseline_tokens])\n",
    "print(\"-\"*42)\n",
    "print(f\"Mean NLL = {mean_nll:.4f}\")\n",
    "print(f\"(This is the score we use to compare caches)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "routing-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 7: The Routing Decision\n",
    "\n",
    "In practice, we have **5 surrogate caches** for each document (not just 1). At query time, we need to **route** the query to the best-matching cache.\n",
    "\n",
    "### The Routing Strategy\n",
    "\n",
    "1. Embed the user's query using a sentence embedding model\n",
    "2. Embed each of the 5 surrogates\n",
    "3. Compute cosine similarity between query and each surrogate\n",
    "4. Route to the cache with the highest similarity\n",
    "\n",
    "```\n",
    "User Query: \"what temperature should it be to plant grass seeds\"\n",
    "                         │\n",
    "                         ▼\n",
    "    ┌────────────────────────────────────────────────┐\n",
    "    │           Compute Similarity to:               │\n",
    "    │  Surrogate 1: sim=0.45  ─────────────────────┐ │\n",
    "    │  Surrogate 2: sim=0.62  ──────────────────┐  │ │\n",
    "    │  Surrogate 3: sim=0.83  ←── BEST MATCH! ──┼──┼ │\n",
    "    │  Surrogate 4: sim=0.31  ──────────────────┘  │ │\n",
    "    │  Surrogate 5: sim=0.55  ─────────────────────┘ │\n",
    "    └────────────────────────────────────────────────┘\n",
    "                         │\n",
    "                         ▼\n",
    "                 Use Cache #3\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "build-all-caches",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "BUILDING ALL SURROGATE CACHES\n",
      "============================================================\n",
      "\n",
      "Building 5 static surrogate caches...\n",
      "  definitional: 127 tokens\n",
      "  procedural: 127 tokens\n",
      "  quantitative: 130 tokens\n",
      "  factual: 128 tokens\n",
      "  problem: 124 tokens\n",
      "\n",
      "Building 5 generated surrogate caches...\n",
      "  Target Question: 142 tokens\n",
      "  Keyword Query: 123 tokens\n",
      "  Symptom Query: 134 tokens\n",
      "  Misconception Query: 162 tokens\n",
      "  Messy Query: 152 tokens\n",
      "\n",
      "Total caches built: 11 (1 baseline + 5 static + 5 generated)\n"
     ]
    }
   ],
   "source": [
    "# Build all 5 static and 5 generated caches\n",
    "print(\"=\"*60)\n",
    "print(\"BUILDING ALL SURROGATE CACHES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nBuilding 5 static surrogate caches...\")\n",
    "all_static_caches = {}\n",
    "all_static_lens = {}\n",
    "for name, info in STATIC_SURROGATES.items():\n",
    "    context = SURROGATE_TEMPLATE.format(surrogate=info['query'], document=example['passage'])\n",
    "    cache_len, cache = build_kv_cache(context)\n",
    "    all_static_caches[name] = cache\n",
    "    all_static_lens[name] = cache_len\n",
    "    print(f\"  {name}: {cache_len} tokens\")\n",
    "\n",
    "print(\"\\nBuilding 5 generated surrogate caches...\")\n",
    "all_generated_caches = {}\n",
    "all_generated_lens = {}\n",
    "for key, surrogate in generated_surrogates.items():\n",
    "    context = SURROGATE_TEMPLATE.format(surrogate=surrogate, document=example['passage'])\n",
    "    cache_len, cache = build_kv_cache(context)\n",
    "    all_generated_caches[key] = cache\n",
    "    all_generated_lens[key] = cache_len\n",
    "    print(f\"  {GENERATED_TEMPLATES[key]['name']}: {cache_len} tokens\")\n",
    "\n",
    "print(\"\\nTotal caches built: 11 (1 baseline + 5 static + 5 generated)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "routing-demo",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ROUTING DEMONSTRATION\n",
      "================================================================================\n",
      "\n",
      "User query: \"walgreens store sales average\"\n",
      "\n",
      "----------------------------------------\n",
      "STATIC SURROGATE ROUTING\n",
      "----------------------------------------\n",
      "Surrogate       Query                                         Similarity\n",
      "----------------------------------------------------------------------\n",
      "definitional    \"What is this and what does it mean?        \"    -0.0774 <-- ROUTE HERE\n",
      "procedural      \"How do I do this step by step?             \"     0.0368 <-- ROUTE HERE\n",
      "quantitative    \"How much does this cost or how long does i...\"     0.0101\n",
      "factual         \"What are the key facts I need to know?     \"    -0.0916\n",
      "problem         \"What problem does this solve?              \"     0.0027\n",
      "\n",
      "Routed to: procedural (similarity=0.0368)\n",
      "\n",
      "----------------------------------------\n",
      "GENERATED SURROGATE ROUTING\n",
      "----------------------------------------\n",
      "Type                 Surrogate                                Similarity\n",
      "------------------------------------------------------------------------\n",
      "Target Question      \"What is the salary range for differen...\"     0.5695 <--\n",
      "Keyword Query        \"Walgreens salary ranges               \"     0.7552 <--\n",
      "Symptom Query        \"What is the average Walgreens salary ...\"     0.6214\n",
      "Misconception Query  \"What not to earn working at Walgreens...\"     0.5299\n",
      "Messy Query          \"urgent: what's avg walgreens salary f...\"     0.4897\n",
      "\n",
      "Routed to: Keyword Query (similarity=0.7552)\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate the routing decision\n",
    "print(\"=\"*80)\n",
    "print(\"ROUTING DEMONSTRATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nUser query: \\\"{example['query']}\\\"\")\n",
    "\n",
    "# Static routing\n",
    "print(\"\\n\" + \"-\"*40)\n",
    "print(\"STATIC SURROGATE ROUTING\")\n",
    "print(\"-\"*40)\n",
    "print(f\"{'Surrogate':<15} {'Query':<45} {'Similarity':>10}\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "static_sims = {}\n",
    "for name, info in STATIC_SURROGATES.items():\n",
    "    sim = compute_similarity(info['query'], example['query'])\n",
    "    static_sims[name] = sim\n",
    "    query_preview = info['query'][:42] + \"...\" if len(info['query']) > 45 else info['query']\n",
    "    marker = \" <-- ROUTE HERE\" if sim == max(static_sims.values()) else \"\"\n",
    "    print(f\"{name:<15} \\\"{query_preview:<43}\\\" {sim:>10.4f}{marker}\")\n",
    "\n",
    "best_static_key = max(static_sims.keys(), key=lambda k: static_sims[k])\n",
    "print(f\"\\nRouted to: {best_static_key} (similarity={static_sims[best_static_key]:.4f})\")\n",
    "\n",
    "# Generated routing\n",
    "print(\"\\n\" + \"-\"*40)\n",
    "print(\"GENERATED SURROGATE ROUTING\")\n",
    "print(\"-\"*40)\n",
    "print(f\"{'Type':<20} {'Surrogate':<40} {'Similarity':>10}\")\n",
    "print(\"-\"*72)\n",
    "\n",
    "generated_sims = {}\n",
    "for key, surrogate in generated_surrogates.items():\n",
    "    sim = compute_similarity(surrogate, example['query'])\n",
    "    generated_sims[key] = sim\n",
    "    name = GENERATED_TEMPLATES[key]['name']\n",
    "    surrogate_preview = surrogate[:37] + \"...\" if len(surrogate) > 40 else surrogate\n",
    "    marker = \" <--\" if sim == max(generated_sims.values()) else \"\"\n",
    "    print(f\"{name:<20} \\\"{surrogate_preview:<38}\\\" {sim:>10.4f}{marker}\")\n",
    "\n",
    "best_generated_key = max(generated_sims.keys(), key=lambda k: generated_sims[k])\n",
    "print(f\"\\nRouted to: {GENERATED_TEMPLATES[best_generated_key]['name']} (similarity={generated_sims[best_generated_key]:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "full-comparison",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "FINAL COMPARISON: ROUTED CACHES\n",
      "================================================================================\n",
      "\n",
      "Query: \"walgreens store sales average\"\n",
      "Answer: \"Approximately $15,000 per year.\"\n",
      "\n",
      "------------------------------------------------------------\n",
      "RESULTS\n",
      "------------------------------------------------------------\n",
      "Method                    Routed To                        NLL\n",
      "------------------------------------------------------------\n",
      "Baseline                  (no routing)                  2.9583\n",
      "Static (routed)           procedural                    2.4583\n",
      "Generated (routed)        Keyword Query                 2.7292\n",
      "\n",
      "------------------------------------------------------------\n",
      "IMPROVEMENT OVER BASELINE\n",
      "------------------------------------------------------------\n",
      "Static:    +0.5000 NLL (better)\n",
      "Generated: +0.2292 NLL (better)\n",
      "\n",
      "============================================================\n",
      "WINNER FOR THIS EXAMPLE\n",
      "============================================================\n",
      "\n",
      "Static (procedural) wins with NLL = 2.4583\n"
     ]
    }
   ],
   "source": [
    "# Score with the routed caches and compare\n",
    "print(\"=\"*80)\n",
    "print(\"FINAL COMPARISON: ROUTED CACHES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Score with routed static cache\n",
    "routed_static_nll = score_answer_with_cache(\n",
    "    all_static_caches[best_static_key],\n",
    "    all_static_lens[best_static_key],\n",
    "    query_prompt,\n",
    "    example['answer']\n",
    ")\n",
    "\n",
    "# Score with routed generated cache\n",
    "routed_generated_nll = score_answer_with_cache(\n",
    "    all_generated_caches[best_generated_key],\n",
    "    all_generated_lens[best_generated_key],\n",
    "    query_prompt,\n",
    "    example['answer']\n",
    ")\n",
    "\n",
    "print(f\"\\nQuery: \\\"{example['query']}\\\"\")\n",
    "print(f\"Answer: \\\"{example['answer']}\\\"\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"RESULTS\")\n",
    "print(\"-\"*60)\n",
    "print(f\"{'Method':<25} {'Routed To':<25} {'NLL':>10}\")\n",
    "print(\"-\"*60)\n",
    "print(f\"{'Baseline':<25} {'(no routing)':<25} {baseline_nll:>10.4f}\")\n",
    "print(f\"{'Static (routed)':<25} {best_static_key:<25} {routed_static_nll:>10.4f}\")\n",
    "print(f\"{'Generated (routed)':<25} {GENERATED_TEMPLATES[best_generated_key]['name']:<25} {routed_generated_nll:>10.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"IMPROVEMENT OVER BASELINE\")\n",
    "print(\"-\"*60)\n",
    "static_improvement = baseline_nll - routed_static_nll\n",
    "generated_improvement = baseline_nll - routed_generated_nll\n",
    "\n",
    "print(f\"Static:    {static_improvement:+.4f} NLL {'(better)' if static_improvement > 0 else '(worse)'}\")\n",
    "print(f\"Generated: {generated_improvement:+.4f} NLL {'(better)' if generated_improvement > 0 else '(worse)'}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"WINNER FOR THIS EXAMPLE\")\n",
    "print(\"=\"*60)\n",
    "all_nlls = {\n",
    "    'Baseline': baseline_nll,\n",
    "    f'Static ({best_static_key})': routed_static_nll,\n",
    "    f'Generated ({GENERATED_TEMPLATES[best_generated_key][\"name\"]})': routed_generated_nll\n",
    "}\n",
    "winner = min(all_nlls.items(), key=lambda x: x[1])\n",
    "print(f\"\\n{winner[0]} wins with NLL = {winner[1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary: What We Learned\n",
    "\n",
    "### The Experimental Setup\n",
    "\n",
    "1. **Data**: MS MARCO passages with queries and answers\n",
    "2. **Caches**: 11 per document (1 baseline + 5 static + 5 generated)\n",
    "3. **Routing**: Use embedding similarity to pick best cache\n",
    "4. **Metric**: NLL (negative log-likelihood) - lower is better\n",
    "\n",
    "### Key Components\n",
    "\n",
    "| Component | Purpose |\n",
    "|-----------|----------|\n",
    "| Static Surrogates | 5 fixed intent queries (same for all docs) |\n",
    "| Generated Surrogates | 5 doc-specific queries (LLM generates) |\n",
    "| KV Cache | Stores attention states for reuse |\n",
    "| Routing | Matches query to best surrogate |\n",
    "| NLL Scoring | Measures answer probability |\n",
    "\n",
    "### The Research Question\n",
    "\n",
    "**Is document-specific surrogate generation worth the compute cost?**\n",
    "\n",
    "- Generated surrogates require an LLM call per document\n",
    "- Static surrogates are free (just fixed strings)\n",
    "- If they perform similarly, static is more cost-effective\n",
    "\n",
    "The full experiment runs this comparison across thousands of samples to get statistically significant results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "final-summary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "EXPERIMENT WALKTHROUGH COMPLETE\n",
      "================================================================================\n",
      "\n",
      "You've now seen each step of the experiment:\n",
      "\n",
      "1. DATA POINT: A passage, query, and answer from MS MARCO\n",
      "   - The passage is the document we're caching\n",
      "   - The query is what users ask\n",
      "   - The answer is the ground truth\n",
      "\n",
      "2. SURROGATES: Two approaches to \"prime\" the cache\n",
      "   - Static: 5 fixed intent queries (cheap, generic)\n",
      "   - Generated: 5 doc-specific queries (expensive, tailored)\n",
      "\n",
      "3. KV CACHE: Pre-computed attention states\n",
      "   - Built once per document (indexing time)\n",
      "   - Reused for all queries (query time)\n",
      "   - Surrogate priming adds context before the document\n",
      "\n",
      "4. SCORING: NLL measures answer quality\n",
      "   - Lower NLL = model more confident in answer\n",
      "   - Compare across cache types to measure improvement\n",
      "\n",
      "5. ROUTING: Pick the best surrogate cache\n",
      "   - Embed query and surrogates\n",
      "   - Route to highest-similarity match\n",
      "\n",
      "The full experiment runs this for 2500 samples to determine:\n",
      "- Do surrogates help at all?\n",
      "- Is generated better than static?\n",
      "- Is the extra compute worth it?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"EXPERIMENT WALKTHROUGH COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\"\"\n",
    "You've now seen each step of the experiment:\n",
    "\n",
    "1. DATA POINT: A passage, query, and answer from MS MARCO\n",
    "   - The passage is the document we're caching\n",
    "   - The query is what users ask\n",
    "   - The answer is the ground truth\n",
    "\n",
    "2. SURROGATES: Two approaches to \"prime\" the cache\n",
    "   - Static: 5 fixed intent queries (cheap, generic)\n",
    "   - Generated: 5 doc-specific queries (expensive, tailored)\n",
    "\n",
    "3. KV CACHE: Pre-computed attention states\n",
    "   - Built once per document (indexing time)\n",
    "   - Reused for all queries (query time)\n",
    "   - Surrogate priming adds context before the document\n",
    "\n",
    "4. SCORING: NLL measures answer quality\n",
    "   - Lower NLL = model more confident in answer\n",
    "   - Compare across cache types to measure improvement\n",
    "\n",
    "5. ROUTING: Pick the best surrogate cache\n",
    "   - Embed query and surrogates\n",
    "   - Route to highest-similarity match\n",
    "\n",
    "The full experiment runs this for 2500 samples to determine:\n",
    "- Do surrogates help at all?\n",
    "- Is generated better than static?\n",
    "- Is the extra compute worth it?\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e827a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
