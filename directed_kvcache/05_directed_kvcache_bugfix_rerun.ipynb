{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Directed KV Cache: Bug-Fix Rerun (Experiment 05)\n\n## Objective\n\nRerun of experiment 04 (`directed_kvcache_corrected_routing`) with three critical\nbug fixes in the library code. The experiment design and notebook code are identical\nto 04; only the underlying `lib/kv_cache.py` has changed.\n\n### Bug Fixes Applied\n\n1. **RoPE correction used wrong dimension pairing (CRITICAL)**\n   - The old code split keys into interleaved even/odd pairs (`keys[..., 0::2]`, `keys[..., 1::2]`)\n   - Mistral's HuggingFace implementation uses **half-split** pairing via `rotate_half`:\n     `x1 = x[..., :d/2]`, `x2 = x[..., d/2:]`\n   - The inverse rotation was applied to the wrong dimension pairs, effectively\n     scrambling keys instead of correcting positions\n   - Fixed: now uses `_rotate_half` + full `(head_dim,)` cos/sin matching HF exactly\n   - Validated: new tests apply HF's forward RoPE then our inverse, asserting identity\n\n2. **Truncated cache was missing the BOS token**\n   - `build_truncated_kv_cache_corrected` computed `doc_len` without BOS, so the\n     truncated cache contained `[Document, :, \\n, ...]` instead of `[<s>, Document, :, \\n, ...]`\n   - Baseline caches always started with BOS (`add_special_tokens=True`)\n   - Fixed: new `extract_and_truncate_cache_with_bos` preserves BOS (position 0) +\n     last N document tokens; `correct_rope_positions_with_bos` leaves BOS untouched\n\n3. **Tokenizer boundary mismatch**\n   - `doc_len` was computed by tokenizing the document text in isolation, but the\n     actual tokens differ at the join boundary (e.g., `▁Document` vs `Document`)\n   - Fixed: `doc_len` is now computed as `len(full_tokens) - len(prefix_tokens)`,\n     both tokenized with `add_special_tokens=True`, avoiding boundary artifacts\n\n### Expected Impact\n\nWith correct RoPE inversion, the truncated+corrected conditions should now properly\ntest whether surrogate-influenced KV entries improve downstream answer quality.\nPreviously these conditions were running with scrambled key vectors, making them\nequivalent to a slightly-noisy baseline.\n\n### Experimental Conditions (unchanged from 04, plus new routing-with-baseline)\n\n| Condition | Description |\n|-----------|-------------|\n| **Baseline** | Document cached with framing (`\"Document:\\n{document}\"`) |\n| **Bare Document** | Document cached without framing (raw text only) |\n| **Truncated + Corrected (5 gen + 5 static + perfect)** | Surrogate primed, truncated, RoPE corrected |\n| **Full Context (routed gen + routed static + perfect)** | Surrogate kept visible during inference |\n| **Full Context Random** | Random token prefix kept visible (positional control) |\n| **Routed / Oracle** | Best surrogate selected by similarity / by NLL |\n| **Routed-or-Baseline** | Router can pick the undirected baseline if no surrogate is a good match |"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import datetime\n",
    "from typing import Dict, List, Any, Optional\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ensure lib is importable\n",
    "sys.path.insert(0, '.')\n",
    "\n",
    "from lib import (\n",
    "    ExperimentConfig,\n",
    "    build_kv_cache,\n",
    "    score_answer_with_cache,\n",
    "    extract_and_truncate_cache,\n",
    "    build_truncated_kv_cache,\n",
    "    correct_rope_positions,\n",
    "    build_truncated_kv_cache_corrected,\n",
    "    generate_all_5_surrogates,\n",
    "    compute_similarity,\n",
    "    load_evaluation_samples,\n",
    "    load_ms_marco,\n",
    "    TOP_5_SURROGATE_TEMPLATES,\n",
    "    STATIC_SURROGATE_QUERIES,\n",
    "    analyze_experiment_results,\n",
    ")\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "config = ExperimentConfig(\n",
    "    num_samples=1000,\n",
    "    min_passage_words=50,\n",
    "    max_passage_words=300,\n",
    "    surrogate_max_tokens=45,\n",
    "    surrogate_temperature=0.3,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "print(f\"Model: {config.model_name}\")\n",
    "print(f\"Samples: {config.num_samples}\")\n",
    "print(f\"Passage words: {config.min_passage_words}-{config.max_passage_words}\")\n",
    "print(f\"Device: {config.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load Model, Tokenizer, and Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Set seeds\n",
    "torch.manual_seed(config.seed)\n",
    "np.random.seed(config.seed)\n",
    "\n",
    "# Load model (4-bit quantized)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    ")\n",
    "\n",
    "print(f\"Loading {config.model_name}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "model.eval()\n",
    "print(f\"Model loaded on {config.device}\")\n",
    "\n",
    "# Load embedding model for routing\n",
    "print(f\"Loading embedding model: {config.embedding_model_name}\")\n",
    "embed_model = SentenceTransformer(config.embedding_model_name)\n",
    "print(\"Embedding model loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_ms_marco(config)\n",
    "samples = load_evaluation_samples(dataset, config, require_answer=True)\n",
    "print(f\"Loaded {len(samples)} evaluation samples\")\n",
    "\n",
    "# Quick sanity check\n",
    "s = samples[0]\n",
    "print(f\"\\nExample sample:\")\n",
    "print(f\"  Query: {s['query'][:80]}...\")\n",
    "print(f\"  Passage: {s['passage'][:80]}...\")\n",
    "print(f\"  Answer: {s['answer'][:80]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Surrogate Templates\n",
    "\n",
    "We use 5 LLM-generated (document-specific) and 5 static (document-agnostic) surrogate queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"GENERATED SURROGATE TEMPLATES (document-specific, LLM-guided)\")\n",
    "print(\"=\" * 80)\n",
    "for key, tmpl in TOP_5_SURROGATE_TEMPLATES.items():\n",
    "    print(f\"  {key:30s} -> {tmpl['name']}\")\n",
    "\n",
    "print()\n",
    "print(\"=\" * 80)\n",
    "print(\"STATIC SURROGATE QUERIES (same for all documents)\")\n",
    "print(\"=\" * 80)\n",
    "for key, info in STATIC_SURROGATE_QUERIES.items():\n",
    "    print(f\"  {key:30s} -> \\\"{info['query']}\\\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Test on One Sample\n",
    "\n",
    "Verify the full pipeline works before running the main loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate surrogates for the first sample\n",
    "test_sample = samples[0]\n",
    "print(f\"Passage: {test_sample['passage'][:120]}...\")\n",
    "print(f\"Query:   {test_sample['query']}\")\n",
    "print(f\"Answer:  {test_sample['answer'][:80]}\")\n",
    "print()\n",
    "\n",
    "test_surrogates = generate_all_5_surrogates(\n",
    "    test_sample['passage'], model, tokenizer, config\n",
    ")\n",
    "print(\"Generated surrogates:\")\n",
    "for key, surr in test_surrogates.items():\n",
    "    sim = compute_similarity(surr, test_sample['query'], embed_model)\n",
    "    print(f\"  {key:30s} -> \\\"{surr}\\\" (sim={sim:.3f})\")\n",
    "\n",
    "print()\n",
    "print(\"Static surrogates:\")\n",
    "for key, info in STATIC_SURROGATE_QUERIES.items():\n",
    "    sim = compute_similarity(info['query'], test_sample['query'], embed_model)\n",
    "    print(f\"  {key:30s} -> \\\"{info['query']}\\\" (sim={sim:.3f})\")\n",
    "\n",
    "# Test baseline cache\n",
    "baseline_ctx = config.baseline_cache_template.format(document=test_sample['passage'])\n",
    "bl_len, bl_cache = build_kv_cache(baseline_ctx, model, tokenizer, config)\n",
    "query_prompt = config.query_template.format(query=test_sample['query'])\n",
    "bl_nll = score_answer_with_cache(\n",
    "    bl_cache, bl_len, query_prompt, test_sample['answer'],\n",
    "    model, tokenizer, config\n",
    ")\n",
    "print(f\"\\nBaseline NLL: {bl_nll:.4f}\")\n",
    "\n",
    "# Test perfect surrogate (actual query)\n",
    "pf_len, pf_cache = build_truncated_kv_cache_corrected(\n",
    "    test_sample['query'], test_sample['passage'], model, tokenizer, config\n",
    ")\n",
    "pf_nll = score_answer_with_cache(\n",
    "    pf_cache, pf_len, query_prompt, test_sample['answer'],\n",
    "    model, tokenizer, config\n",
    ")\n",
    "print(f\"Perfect surrogate NLL: {pf_nll:.4f}\")\n",
    "print(f\"Delta (positive = better): {bl_nll - pf_nll:.4f}\")\n",
    "\n",
    "# Test one generated surrogate\n",
    "first_key = list(test_surrogates.keys())[0]\n",
    "first_surr = test_surrogates[first_key]\n",
    "doc_len, corrected_cache = build_truncated_kv_cache_corrected(\n",
    "    first_surr, test_sample['passage'], model, tokenizer, config\n",
    ")\n",
    "corrected_nll = score_answer_with_cache(\n",
    "    corrected_cache, doc_len, query_prompt, test_sample['answer'],\n",
    "    model, tokenizer, config\n",
    ")\n",
    "print(f\"Generated surrogate NLL ({first_key}): {corrected_nll:.4f}\")\n",
    "print(f\"Delta (positive = better): {bl_nll - corrected_nll:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Per-Sample Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def generate_random_prefix(passage, tokenizer, config, seed):\n    \"\"\"Generate a random token prefix roughly matching a typical surrogate length.\"\"\"\n    np.random.seed(seed)\n    vocab_size = tokenizer.vocab_size\n    # Generate ~20 random token IDs (typical surrogate prefix length)\n    random_ids = np.random.randint(100, vocab_size, size=20)\n    random_text = tokenizer.decode(random_ids, skip_special_tokens=True)\n    return random_text\n\n\ndef evaluate_sample(\n    sample: Dict,\n    idx: int,\n    model: AutoModelForCausalLM,\n    tokenizer: AutoTokenizer,\n    embed_model: SentenceTransformer,\n    config: ExperimentConfig,\n) -> Optional[Dict]:\n    \"\"\"\n    Evaluate a single sample across all experimental conditions:\n\n    A. Baselines:\n       - baseline: \"Document:\\\\n{document}\"\n       - bare_doc: \"{document}\" (no framing)\n\n    B. Truncated + RoPE corrected:\n       - 5 generated surrogates\n       - 5 static surrogates\n       - perfect surrogate (actual query)\n\n    C. Full context (surrogate kept visible):\n       - routed generated surrogate (best by similarity)\n       - routed static surrogate (best by similarity)\n       - perfect surrogate (actual query)\n       - random prefix (positional control)\n\n    D. Routed-or-Baseline (truncated):\n       - Like B routing, but the undirected baseline cache is included as a\n         routing candidate. The baseline's similarity score is computed by\n         embedding the passage itself. If no surrogate is more similar to the\n         query than the passage is, the router falls back to the baseline cache.\n\n    Returns None if the sample should be skipped.\n    \"\"\"\n    passage = sample['passage']\n    query = sample['query']\n    answer = sample['answer']\n\n    # --- Guard: answer must tokenize to >= 2 tokens for a meaningful NLL ---\n    answer_ids = tokenizer(\n        answer, return_tensors='pt', add_special_tokens=False\n    )['input_ids']\n    if answer_ids.shape[1] < 2:\n        return None\n\n    query_prompt = config.query_template.format(query=query)\n\n    # ==================== A. BASELINES ====================\n\n    # A1: Standard baseline with framing\n    baseline_context = config.baseline_cache_template.format(document=passage)\n    baseline_len, baseline_cache = build_kv_cache(\n        baseline_context, model, tokenizer, config\n    )\n    baseline_nll = score_answer_with_cache(\n        baseline_cache, baseline_len, query_prompt, answer,\n        model, tokenizer, config\n    )\n\n    # A2: Bare document (no \"Document:\\n\" framing)\n    bare_len, bare_cache = build_kv_cache(passage, model, tokenizer, config)\n    bare_nll = score_answer_with_cache(\n        bare_cache, bare_len, query_prompt, answer,\n        model, tokenizer, config\n    )\n\n    # ==================== B. TRUNCATED + CORRECTED ====================\n\n    # B1: Perfect surrogate (actual query), truncated + corrected\n    perfect_doc_len, perfect_cache = build_truncated_kv_cache_corrected(\n        query, passage, model, tokenizer, config\n    )\n    perfect_trunc_nll = score_answer_with_cache(\n        perfect_cache, perfect_doc_len, query_prompt, answer,\n        model, tokenizer, config\n    )\n\n    # B2: Generated surrogates, truncated + corrected\n    generated_surrogates = generate_all_5_surrogates(passage, model, tokenizer, config)\n\n    generated_similarities = {\n        key: compute_similarity(surr, query, embed_model)\n        for key, surr in generated_surrogates.items()\n    }\n\n    generated_nlls = {}\n    for key, surrogate in generated_surrogates.items():\n        doc_len, corrected_cache = build_truncated_kv_cache_corrected(\n            surrogate, passage, model, tokenizer, config\n        )\n        nll = score_answer_with_cache(\n            corrected_cache, doc_len, query_prompt, answer,\n            model, tokenizer, config\n        )\n        generated_nlls[key] = nll\n\n    # B3: Static surrogates, truncated + corrected\n    static_surrogates = {key: info['query'] for key, info in STATIC_SURROGATE_QUERIES.items()}\n\n    static_similarities = {\n        key: compute_similarity(surr, query, embed_model)\n        for key, surr in static_surrogates.items()\n    }\n\n    static_nlls = {}\n    for key, surrogate in static_surrogates.items():\n        doc_len, corrected_cache = build_truncated_kv_cache_corrected(\n            surrogate, passage, model, tokenizer, config\n        )\n        nll = score_answer_with_cache(\n            corrected_cache, doc_len, query_prompt, answer,\n            model, tokenizer, config\n        )\n        static_nlls[key] = nll\n\n    # ==================== ROUTING (truncated) ====================\n\n    gen_routed_key = max(generated_similarities, key=generated_similarities.get)\n    gen_routed_nll = generated_nlls[gen_routed_key]\n    gen_routed_similarity = generated_similarities[gen_routed_key]\n\n    gen_oracle_key = min(generated_nlls, key=generated_nlls.get)\n    gen_oracle_nll = generated_nlls[gen_oracle_key]\n\n    static_routed_key = max(static_similarities, key=static_similarities.get)\n    static_routed_nll = static_nlls[static_routed_key]\n    static_routed_similarity = static_similarities[static_routed_key]\n\n    static_oracle_key = min(static_nlls, key=static_nlls.get)\n    static_oracle_nll = static_nlls[static_oracle_key]\n\n    # ==================== D. ROUTED-OR-BASELINE (truncated) ====================\n    # Include the undirected baseline as a routing candidate.\n    # The baseline's \"surrogate\" representation for similarity is the passage itself.\n    baseline_similarity = compute_similarity(passage, query, embed_model)\n\n    # Generated pool + baseline\n    if baseline_similarity >= gen_routed_similarity:\n        gen_routed_or_bl_nll = baseline_nll\n        gen_routed_or_bl_chose_baseline = True\n        gen_routed_or_bl_key = '__baseline__'\n    else:\n        gen_routed_or_bl_nll = gen_routed_nll\n        gen_routed_or_bl_chose_baseline = False\n        gen_routed_or_bl_key = gen_routed_key\n\n    # Static pool + baseline\n    if baseline_similarity >= static_routed_similarity:\n        static_routed_or_bl_nll = baseline_nll\n        static_routed_or_bl_chose_baseline = True\n        static_routed_or_bl_key = '__baseline__'\n    else:\n        static_routed_or_bl_nll = static_routed_nll\n        static_routed_or_bl_chose_baseline = False\n        static_routed_or_bl_key = static_routed_key\n\n    # Combined pool: all gen + all static + baseline\n    all_similarities = {}\n    all_similarities.update({f'gen_{k}': v for k, v in generated_similarities.items()})\n    all_similarities.update({f'static_{k}': v for k, v in static_similarities.items()})\n    all_similarities['__baseline__'] = baseline_similarity\n\n    all_nlls = {}\n    all_nlls.update({f'gen_{k}': v for k, v in generated_nlls.items()})\n    all_nlls.update({f'static_{k}': v for k, v in static_nlls.items()})\n    all_nlls['__baseline__'] = baseline_nll\n\n    combined_routed_key = max(all_similarities, key=all_similarities.get)\n    combined_routed_or_bl_nll = all_nlls[combined_routed_key]\n    combined_routed_or_bl_chose_baseline = (combined_routed_key == '__baseline__')\n\n    # ==================== C. FULL CONTEXT (surrogate kept visible) ====================\n\n    # C1: Full-context routed generated surrogate\n    gen_routed_surr = generated_surrogates[gen_routed_key]\n    full_gen_context = config.surrogate_cache_template.format(\n        surrogate=gen_routed_surr, document=passage\n    )\n    full_gen_len, full_gen_cache = build_kv_cache(\n        full_gen_context, model, tokenizer, config\n    )\n    full_gen_nll = score_answer_with_cache(\n        full_gen_cache, full_gen_len, query_prompt, answer,\n        model, tokenizer, config\n    )\n\n    # C2: Full-context routed static surrogate\n    static_routed_surr = static_surrogates[static_routed_key]\n    full_static_context = config.surrogate_cache_template.format(\n        surrogate=static_routed_surr, document=passage\n    )\n    full_static_len, full_static_cache = build_kv_cache(\n        full_static_context, model, tokenizer, config\n    )\n    full_static_nll = score_answer_with_cache(\n        full_static_cache, full_static_len, query_prompt, answer,\n        model, tokenizer, config\n    )\n\n    # C3: Full-context perfect surrogate (actual query)\n    full_perfect_context = config.surrogate_cache_template.format(\n        surrogate=query, document=passage\n    )\n    full_perfect_len, full_perfect_cache = build_kv_cache(\n        full_perfect_context, model, tokenizer, config\n    )\n    full_perfect_nll = score_answer_with_cache(\n        full_perfect_cache, full_perfect_len, query_prompt, answer,\n        model, tokenizer, config\n    )\n\n    # C4: Full-context random prefix (positional control)\n    random_prefix = generate_random_prefix(passage, tokenizer, config, seed=config.seed + idx)\n    full_random_context = config.surrogate_cache_template.format(\n        surrogate=random_prefix, document=passage\n    )\n    full_random_len, full_random_cache = build_kv_cache(\n        full_random_context, model, tokenizer, config\n    )\n    full_random_nll = score_answer_with_cache(\n        full_random_cache, full_random_len, query_prompt, answer,\n        model, tokenizer, config\n    )\n\n    return {\n        'query': query,\n        'answer_preview': answer[:50] + '...' if len(answer) > 50 else answer,\n\n        # A. Baselines\n        'baseline_nll': baseline_nll,\n        'bare_nll': bare_nll,\n\n        # B. Truncated + corrected\n        'perfect_nll': perfect_trunc_nll,\n        'delta_perfect': baseline_nll - perfect_trunc_nll,\n\n        'generated_surrogates': generated_surrogates,\n        'generated_similarities': generated_similarities,\n        'generated_nlls': generated_nlls,\n        'gen_routed_key': gen_routed_key,\n        'gen_routed_nll': gen_routed_nll,\n        'gen_routed_similarity': gen_routed_similarity,\n        'gen_oracle_key': gen_oracle_key,\n        'gen_oracle_nll': gen_oracle_nll,\n\n        'static_surrogates': static_surrogates,\n        'static_similarities': static_similarities,\n        'static_nlls': static_nlls,\n        'static_routed_key': static_routed_key,\n        'static_routed_nll': static_routed_nll,\n        'static_routed_similarity': static_routed_similarity,\n        'static_oracle_key': static_oracle_key,\n        'static_oracle_nll': static_oracle_nll,\n\n        # C. Full context\n        'full_gen_nll': full_gen_nll,\n        'full_static_nll': full_static_nll,\n        'full_perfect_nll': full_perfect_nll,\n        'full_random_nll': full_random_nll,\n\n        # D. Routed-or-Baseline\n        'baseline_similarity': baseline_similarity,\n        'gen_routed_or_bl_nll': gen_routed_or_bl_nll,\n        'gen_routed_or_bl_chose_baseline': gen_routed_or_bl_chose_baseline,\n        'gen_routed_or_bl_key': gen_routed_or_bl_key,\n        'static_routed_or_bl_nll': static_routed_or_bl_nll,\n        'static_routed_or_bl_chose_baseline': static_routed_or_bl_chose_baseline,\n        'static_routed_or_bl_key': static_routed_or_bl_key,\n        'combined_routed_or_bl_nll': combined_routed_or_bl_nll,\n        'combined_routed_or_bl_chose_baseline': combined_routed_or_bl_chose_baseline,\n        'combined_routed_or_bl_key': combined_routed_key,\n\n        # Deltas vs baseline (positive = better)\n        'delta_gen_routed': baseline_nll - gen_routed_nll,\n        'delta_gen_oracle': baseline_nll - gen_oracle_nll,\n        'delta_static_routed': baseline_nll - static_routed_nll,\n        'delta_static_oracle': baseline_nll - static_oracle_nll,\n        'delta_bare': baseline_nll - bare_nll,\n        'delta_full_gen': baseline_nll - full_gen_nll,\n        'delta_full_static': baseline_nll - full_static_nll,\n        'delta_full_perfect': baseline_nll - full_perfect_nll,\n        'delta_full_random': baseline_nll - full_random_nll,\n        'delta_gen_routed_or_bl': baseline_nll - gen_routed_or_bl_nll,\n        'delta_static_routed_or_bl': baseline_nll - static_routed_or_bl_nll,\n        'delta_combined_routed_or_bl': baseline_nll - combined_routed_or_bl_nll,\n    }"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Run the Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "skipped = 0\n",
    "errors = 0\n",
    "start_time = time.time()\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"RUNNING COMPREHENSIVE SURROGATE ROUTING EXPERIMENT\")\n",
    "print(f\"Samples: {len(samples)}\")\n",
    "print(f\"Per sample: 5 gen + 5 static + 1 perfect (truncated+corrected)\")\n",
    "print(f\"          + full-context gen/static/perfect/random + bare doc\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for idx, sample in enumerate(tqdm(samples, desc=\"Evaluating\")):\n",
    "    try:\n",
    "        result = evaluate_sample(sample, idx, model, tokenizer, embed_model, config)\n",
    "        if result is None:\n",
    "            skipped += 1\n",
    "            continue\n",
    "        results.append(result)\n",
    "    except Exception as e:\n",
    "        errors += 1\n",
    "        if errors <= 3:\n",
    "            print(f\"\\n  Error on sample {idx}: {type(e).__name__}: {e}\")\n",
    "        continue\n",
    "\n",
    "    # Progress report every 50 samples\n",
    "    if len(results) > 0 and len(results) % 50 == 0:\n",
    "        elapsed = time.time() - start_time\n",
    "        rate = elapsed / len(results)\n",
    "        remaining = rate * (len(samples) - idx - 1)\n",
    "\n",
    "        recent = results[-50:]\n",
    "        bl = np.mean([r['baseline_nll'] for r in recent])\n",
    "        br = np.mean([r['bare_nll'] for r in recent])\n",
    "        fg = np.mean([r['full_gen_nll'] for r in recent])\n",
    "        fr = np.mean([r['full_random_nll'] for r in recent])\n",
    "        gr = np.mean([r['gen_routed_nll'] for r in recent])\n",
    "        wr_fg = np.mean([r['delta_full_gen'] > 0 for r in recent]) * 100\n",
    "        wr_fr = np.mean([r['delta_full_random'] > 0 for r in recent]) * 100\n",
    "        wr_gt = np.mean([r['delta_gen_routed'] > 0 for r in recent]) * 100\n",
    "\n",
    "        print(\n",
    "            f\"\\n  [{len(results):>4d} done | {elapsed/60:.0f}m elapsed | ~{remaining/60:.0f}m left]\\n\"\n",
    "            f\"  Last 50: baseline={bl:.3f}  bare={br:.3f}\\n\"\n",
    "            f\"    Full-ctx: gen={fg:.3f} ({wr_fg:.0f}% win)  random={fr:.3f} ({wr_fr:.0f}% win)\\n\"\n",
    "            f\"    Trunc:    gen_routed={gr:.3f} ({wr_gt:.0f}% win)\"\n",
    "        )\n",
    "\n",
    "elapsed_total = time.time() - start_time\n",
    "print(f\"\\nDone. {len(results)} evaluated, {skipped} skipped (short answer), {errors} errors.\")\n",
    "print(f\"Total time: {elapsed_total/60:.1f} minutes ({elapsed_total/len(results):.1f}s per sample)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.analysis import print_analysis_summary\n",
    "\n",
    "analysis = analyze_experiment_results(results)\n",
    "print_analysis_summary(analysis)\n",
    "\n",
    "# Perfect surrogate analysis (not covered by analyze_experiment_results)\n",
    "perfect_nlls = np.array([r['perfect_nll'] for r in results])\n",
    "deltas_perfect = np.array([r['delta_perfect'] for r in results])\n",
    "baseline_nlls_arr = np.array([r['baseline_nll'] for r in results])\n",
    "\n",
    "t_perfect, p_perfect = stats.ttest_rel(baseline_nlls_arr, perfect_nlls)\n",
    "\n",
    "from lib.analysis import cohens_d\n",
    "d_perfect = cohens_d(deltas_perfect)\n",
    "\n",
    "print()\n",
    "print(\"=\" * 80)\n",
    "print(\"PERFECT SURROGATE (actual query as surrogate, truncated + RoPE corrected)\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"  Mean NLL:    {np.mean(perfect_nlls):.4f} ± {np.std(perfect_nlls):.4f}\")\n",
    "print(f\"  Mean Delta:  {np.mean(deltas_perfect):.4f} (positive = better than baseline)\")\n",
    "print(f\"  Win Rate:    {np.mean(deltas_perfect > 0)*100:.1f}%\")\n",
    "print(f\"  t-stat:      {t_perfect:.3f}\")\n",
    "print(f\"  p-value:     {p_perfect:.4f}\")\n",
    "print(f\"  Cohen's d:   {d_perfect:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-template breakdown\n",
    "print(\"=\" * 80)\n",
    "print(\"PER-TEMPLATE BREAKDOWN: GENERATED SURROGATES\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"{'Template':<30} {'Mean NLL':>10} {'Delta':>10} {'Win%':>8} {'Routed':>8} {'Oracle':>8}\")\n",
    "print(\"-\" * 80)\n",
    "for key, st in analysis['gen_template_stats'].items():\n",
    "    print(\n",
    "        f\"{key:<30} {st['mean_nll']:>10.4f} {st['mean_delta']:>10.4f}\"\n",
    "        f\" {st['win_rate']*100:>7.1f}% {st['times_routed']:>8d} {st['times_oracle']:>8d}\"\n",
    "    )\n",
    "\n",
    "print()\n",
    "print(\"=\" * 80)\n",
    "print(\"PER-TEMPLATE BREAKDOWN: STATIC SURROGATES\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"{'Template':<30} {'Mean NLL':>10} {'Delta':>10} {'Win%':>8} {'Routed':>8} {'Oracle':>8}\")\n",
    "print(\"-\" * 80)\n",
    "for key, st in analysis['static_template_stats'].items():\n",
    "    print(\n",
    "        f\"{key:<30} {st['mean_nll']:>10.4f} {st['mean_delta']:>10.4f}\"\n",
    "        f\" {st['win_rate']*100:>7.1f}% {st['times_routed']:>8d} {st['times_oracle']:>8d}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Effect sizes and significance\n",
    "print(\"=\" * 80)\n",
    "print(\"EFFECT SIZES\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Cohen's d (gen routed vs baseline):    {analysis['cohens_d_gen_routed']:.4f}\")\n",
    "print(f\"Cohen's d (static routed vs baseline): {analysis['cohens_d_static_routed']:.4f}\")\n",
    "print()\n",
    "print(f\"Generated beats static rate: {analysis['gen_beats_static_rate']*100:.1f}%\")\n",
    "print(f\"Gen vs Static p-value:       {analysis['p_value_gen_vs_static']:.4f}\")\n",
    "\n",
    "# Routing efficiency: how close does routing get to oracle?\n",
    "if analysis['mean_delta_gen_oracle'] != 0:\n",
    "    gen_efficiency = analysis['mean_delta_gen_routed'] / analysis['mean_delta_gen_oracle'] * 100\n",
    "    print(f\"\\nGenerated routing efficiency: {gen_efficiency:.1f}% of oracle\")\n",
    "if analysis['mean_delta_static_oracle'] != 0:\n",
    "    static_efficiency = analysis['mean_delta_static_routed'] / analysis['mean_delta_static_oracle'] * 100\n",
    "    print(f\"Static routing efficiency:    {static_efficiency:.1f}% of oracle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8b: Full-Context and Controls Analysis\n",
    "\n",
    "These conditions test whether the benefit of a surrogate prefix is **semantic** (content\n",
    "matters) or **positional** (any prefix helps). The `directed_kvcache_experiment` found\n",
    "that random prefixes helped as much as meaningful surrogates when kept in full context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": "# Full-context and control conditions analysis\nbaseline_arr = np.array([r['baseline_nll'] for r in results])\nbare_arr = np.array([r['bare_nll'] for r in results])\nfull_gen_arr = np.array([r['full_gen_nll'] for r in results])\nfull_static_arr = np.array([r['full_static_nll'] for r in results])\nfull_perfect_arr = np.array([r['full_perfect_nll'] for r in results])\nfull_random_arr = np.array([r['full_random_nll'] for r in results])\n\nconditions_extra = {\n    'Bare Doc (no framing)':    ('bare_nll',         'delta_bare'),\n    'Full-Ctx Gen (routed)':    ('full_gen_nll',     'delta_full_gen'),\n    'Full-Ctx Static (routed)': ('full_static_nll',  'delta_full_static'),\n    'Full-Ctx Perfect (query)': ('full_perfect_nll', 'delta_full_perfect'),\n    'Full-Ctx Random Prefix':   ('full_random_nll',  'delta_full_random'),\n    'Trunc+Corr Gen (routed)':  ('gen_routed_nll',   'delta_gen_routed'),\n    'Trunc+Corr Perfect':       ('perfect_nll',      'delta_perfect'),\n    'Trunc Gen+BL (routed)':    ('gen_routed_or_bl_nll',      'delta_gen_routed_or_bl'),\n    'Trunc Static+BL (routed)': ('static_routed_or_bl_nll',   'delta_static_routed_or_bl'),\n    'Trunc Combined+BL (rtd)':  ('combined_routed_or_bl_nll', 'delta_combined_routed_or_bl'),\n}\n\ncohens_d_label = \"Cohen's d\"\n\nprint('=' * 95)\nprint('ALL CONDITIONS vs BASELINE (sorted by Mean NLL)')\nprint('=' * 95)\nprint(f\"{'Condition':<30} {'Mean NLL':>10} {'Std':>8} {'Delta':>10} {'Win%':>8} {'t-stat':>8} {'p-value':>10} {cohens_d_label:>10}\")\nprint('-' * 95)\nbaseline_label = 'Baseline (Document:\\\\n{doc})'\nprint(f\"{baseline_label:<30} {np.mean(baseline_arr):>10.4f} {np.std(baseline_arr):>8.4f} {'--':>10} {'--':>8} {'--':>8} {'--':>10} {'--':>10}\")\nprint('-' * 95)\n\nrows = []\nfor label, (nll_key, delta_key) in conditions_extra.items():\n    nlls = np.array([r[nll_key] for r in results])\n    deltas = np.array([r[delta_key] for r in results])\n    t, p = stats.ttest_rel(baseline_arr, nlls)\n    d = cohens_d(deltas)\n    wr = np.mean(deltas > 0) * 100\n    rows.append((np.mean(nlls), label, np.std(nlls), np.mean(deltas), wr, t, p, d))\n\nrows.sort(key=lambda x: x[0])\nfor mean_nll, label, std, delta, wr, t, p, d in rows:\n    sig = '*' if p < 0.05 else ' '\n    print(f'{label:<30} {mean_nll:>10.4f} {std:>8.4f} {delta:>+10.4f} {wr:>7.1f}% {t:>8.2f} {p:>10.4f}{sig} {d:>10.4f}')\n\nprint()\nprint('KEY COMPARISON: Is the full-context benefit semantic or positional?')\nprint('-' * 80)\nfg_mean = np.mean(full_gen_arr)\nfr_mean = np.mean(full_random_arr)\nt_sem, p_sem = stats.ttest_rel(full_gen_arr, full_random_arr)\nprint(f'  Full-ctx generated (routed) NLL: {fg_mean:.4f}')\nprint(f'  Full-ctx random prefix NLL:      {fr_mean:.4f}')\nprint(f'  Paired t-test (gen vs random):   t={t_sem:.3f}, p={p_sem:.4f}')\nif p_sem < 0.05:\n    if fg_mean < fr_mean:\n        print('  -> Generated surrogate is SIGNIFICANTLY better than random prefix.')\n        print('     The benefit has a semantic component beyond just positional effects.')\n    else:\n        print('  -> Random prefix is SIGNIFICANTLY better than generated surrogate.')\n        print('     The benefit is purely positional; surrogate content may actually hurt.')\nelse:\n    print('  -> No significant difference. The benefit is likely positional, not semantic.')\n\n# --- Routed-or-Baseline analysis ---\nprint()\nprint('=' * 80)\nprint('ROUTED-OR-BASELINE: Can the router benefit from a baseline fallback?')\nprint('=' * 80)\n\ngen_bl_rate = np.mean([r['gen_routed_or_bl_chose_baseline'] for r in results]) * 100\nstatic_bl_rate = np.mean([r['static_routed_or_bl_chose_baseline'] for r in results]) * 100\ncombined_bl_rate = np.mean([r['combined_routed_or_bl_chose_baseline'] for r in results]) * 100\n\nprint(f'  Gen pool:      baseline chosen {gen_bl_rate:.1f}% of the time')\nprint(f'  Static pool:   baseline chosen {static_bl_rate:.1f}% of the time')\nprint(f'  Combined pool: baseline chosen {combined_bl_rate:.1f}% of the time')\n\n# Compare routed-or-baseline vs plain routed\nfor label, rob_key, plain_key in [\n    ('Gen',      'gen_routed_or_bl_nll',      'gen_routed_nll'),\n    ('Static',   'static_routed_or_bl_nll',   'static_routed_nll'),\n]:\n    rob_arr = np.array([r[rob_key] for r in results])\n    plain_arr = np.array([r[plain_key] for r in results])\n    t, p = stats.ttest_rel(plain_arr, rob_arr)\n    rob_wins = np.mean(rob_arr < plain_arr) * 100\n    print(f'\\n  {label}: routed-or-BL vs plain routed:')\n    print(f'    Mean NLL: {np.mean(rob_arr):.4f} vs {np.mean(plain_arr):.4f}')\n    print(f'    routed-or-BL wins {rob_wins:.1f}% (paired t={t:.3f}, p={p:.4f})')\n\n# Combined pool vs best individual pool\ncombined_arr = np.array([r['combined_routed_or_bl_nll'] for r in results])\ngen_rob_arr = np.array([r['gen_routed_or_bl_nll'] for r in results])\nt_comb, p_comb = stats.ttest_rel(gen_rob_arr, combined_arr)\nprint(f'\\n  Combined+BL vs Gen+BL:')\nprint(f'    Mean NLL: {np.mean(combined_arr):.4f} vs {np.mean(gen_rob_arr):.4f}')\nprint(f'    Combined wins {np.mean(combined_arr < gen_rob_arr)*100:.1f}% (p={p_comb:.4f})')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Stratified Analysis by Baseline Difficulty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Earlier experiments showed surrogates help most when baseline NLL is high.\n",
    "# Does the RoPE correction change this pattern?\n",
    "\n",
    "baseline_nlls = np.array([r['baseline_nll'] for r in results])\n",
    "quartiles = np.percentile(baseline_nlls, [25, 50, 75])\n",
    "\n",
    "bins = [\n",
    "    ('Q1 (easiest)', lambda x: x <= quartiles[0]),\n",
    "    ('Q2', lambda x: quartiles[0] < x <= quartiles[1]),\n",
    "    ('Q3', lambda x: quartiles[1] < x <= quartiles[2]),\n",
    "    ('Q4 (hardest)', lambda x: x > quartiles[2]),\n",
    "]\n",
    "\n",
    "print(\"=\" * 100)\n",
    "print(\"WIN RATE BY BASELINE DIFFICULTY QUARTILE\")\n",
    "print(\"=\" * 100)\n",
    "print(f\"{'Quartile':<20} {'N':>5} {'BL NLL':>8} {'Perfect':>10} {'Prf Win%':>10} {'Gen Rtd':>10} {'Gen Win%':>10} {'Stc Rtd':>10} {'Stc Win%':>10}\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "for label, cond in bins:\n",
    "    subset = [r for r in results if cond(r['baseline_nll'])]\n",
    "    if not subset:\n",
    "        continue\n",
    "    bl = np.mean([r['baseline_nll'] for r in subset])\n",
    "    pf = np.mean([r['perfect_nll'] for r in subset])\n",
    "    gr = np.mean([r['gen_routed_nll'] for r in subset])\n",
    "    sr = np.mean([r['static_routed_nll'] for r in subset])\n",
    "    pw = np.mean([r['delta_perfect'] > 0 for r in subset]) * 100\n",
    "    gw = np.mean([r['delta_gen_routed'] > 0 for r in subset]) * 100\n",
    "    sw = np.mean([r['delta_static_routed'] > 0 for r in subset]) * 100\n",
    "    print(f\"{label:<20} {len(subset):>5} {bl:>8.3f} {pf:>10.3f} {pw:>9.1f}% {gr:>10.3f} {gw:>9.1f}% {sr:>10.3f} {sw:>9.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Similarity-Delta Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Does a higher-similarity surrogate predict a bigger improvement?\n",
    "gen_sims = np.array([r['gen_routed_similarity'] for r in results])\n",
    "gen_deltas = np.array([r['delta_gen_routed'] for r in results])\n",
    "static_sims = np.array([r['static_routed_similarity'] for r in results])\n",
    "static_deltas = np.array([r['delta_static_routed'] for r in results])\n",
    "\n",
    "r_gen, p_gen = stats.pearsonr(gen_sims, gen_deltas)\n",
    "r_static, p_static = stats.pearsonr(static_sims, static_deltas)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"CORRELATION: SURROGATE-QUERY SIMILARITY vs NLL IMPROVEMENT\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Generated routed: r={r_gen:.3f}, p={p_gen:.4f}\")\n",
    "print(f\"Static routed:    r={r_static:.3f}, p={p_static:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": "fig, axes = plt.subplots(2, 4, figsize=(26, 11))\nfig.suptitle(\n    'Directed KV Cache: Bug-Fix Rerun (Exp 05)',\n    fontsize=14, fontweight='bold'\n)\n\n# --- Plot 1: All conditions NLL comparison ---\nax = axes[0, 0]\ncond_labels = ['Baseline', 'Bare Doc', 'Full\\nGen', 'Full\\nRandom', 'Full\\nPerfect',\n               'Trunc\\nGen Rtd', 'Trunc\\nPerfect', 'Gen+BL\\nRouted', 'Comb+BL\\nRouted']\ncond_data = [\n    np.array([r['baseline_nll'] for r in results]),\n    np.array([r['bare_nll'] for r in results]),\n    np.array([r['full_gen_nll'] for r in results]),\n    np.array([r['full_random_nll'] for r in results]),\n    np.array([r['full_perfect_nll'] for r in results]),\n    np.array([r['gen_routed_nll'] for r in results]),\n    np.array([r['perfect_nll'] for r in results]),\n    np.array([r['gen_routed_or_bl_nll'] for r in results]),\n    np.array([r['combined_routed_or_bl_nll'] for r in results]),\n]\ncond_colors = ['#cccccc', '#8c8c8c', '#4c72b0', '#c44e52', '#55a868',\n               '#8da0cb', '#a6d96a', '#e6ab02', '#b07aa1']\nbp = ax.boxplot(cond_data, labels=cond_labels, patch_artist=True)\nfor patch, c in zip(bp['boxes'], cond_colors):\n    patch.set_facecolor(c)\nax.set_ylabel('NLL')\nax.set_title('NLL by Condition')\nax.tick_params(axis='x', rotation=0, labelsize=6)\n\n# --- Plot 2: Full-context vs Truncated delta distributions ---\nax = axes[0, 1]\ndelta_full_gen = np.array([r['delta_full_gen'] for r in results])\ndelta_full_random = np.array([r['delta_full_random'] for r in results])\ndelta_trunc_gen = np.array([r['delta_gen_routed'] for r in results])\nax.hist(delta_full_gen, bins=50, alpha=0.5,\n        label=f'Full-ctx gen (win={np.mean(delta_full_gen>0)*100:.0f}%)', color='#4c72b0')\nax.hist(delta_full_random, bins=50, alpha=0.5,\n        label=f'Full-ctx random (win={np.mean(delta_full_random>0)*100:.0f}%)', color='#c44e52')\nax.hist(delta_trunc_gen, bins=50, alpha=0.5,\n        label=f'Trunc gen (win={np.mean(delta_trunc_gen>0)*100:.0f}%)', color='#8da0cb')\nax.axvline(0, color='black', linestyle='--', linewidth=0.8)\nax.set_xlabel('Delta NLL (positive = better than baseline)')\nax.set_ylabel('Count')\nax.set_title('Full-Context vs Truncated')\nax.legend(fontsize=7)\n\n# --- Plot 3: Semantic vs Positional scatter ---\nax = axes[0, 2]\nax.scatter(\n    [r['full_random_nll'] for r in results],\n    [r['full_gen_nll'] for r in results],\n    alpha=0.3, s=10, c='#4c72b0'\n)\nlims = [min(ax.get_xlim()[0], ax.get_ylim()[0]), max(ax.get_xlim()[1], ax.get_ylim()[1])]\nax.plot(lims, lims, 'k--', linewidth=0.8, alpha=0.5)\nax.set_xlabel('Full-Context Random NLL')\nax.set_ylabel('Full-Context Generated NLL')\nax.set_title('Semantic vs Positional\\n(below diagonal = gen wins)')\n\n# --- Plot 4: Routed-or-Baseline analysis ---\nax = axes[0, 3]\n# Show how often baseline is chosen, and the NLL improvement\nrob_labels = ['Gen\\nRouted', 'Gen+BL\\nRouted', 'Static\\nRouted', 'Static+BL\\nRouted', 'Combined\\n+BL']\nrob_means = [\n    np.mean([r['gen_routed_nll'] for r in results]),\n    np.mean([r['gen_routed_or_bl_nll'] for r in results]),\n    np.mean([r['static_routed_nll'] for r in results]),\n    np.mean([r['static_routed_or_bl_nll'] for r in results]),\n    np.mean([r['combined_routed_or_bl_nll'] for r in results]),\n]\nrob_colors = ['#8da0cb', '#e6ab02', '#dd8452', '#e6ab02', '#b07aa1']\nbl_mean = np.mean(baseline_arr)\nbars = ax.bar(range(len(rob_labels)), rob_means, color=rob_colors)\nax.axhline(bl_mean, color='#cccccc', linestyle='--', linewidth=1.5, label='Baseline')\nax.set_xticks(range(len(rob_labels)))\nax.set_xticklabels(rob_labels, fontsize=7)\nax.set_ylabel('Mean NLL')\nax.set_title('Routed-or-Baseline\\n(lower is better)')\nax.legend(fontsize=7)\n\n# Annotate baseline-chosen rate on +BL bars\ngen_bl_rate = np.mean([r['gen_routed_or_bl_chose_baseline'] for r in results]) * 100\nstatic_bl_rate = np.mean([r['static_routed_or_bl_chose_baseline'] for r in results]) * 100\ncombined_bl_rate = np.mean([r['combined_routed_or_bl_chose_baseline'] for r in results]) * 100\nfor i, rate in [(1, gen_bl_rate), (3, static_bl_rate), (4, combined_bl_rate)]:\n    ax.text(i, rob_means[i], f'BL:{rate:.0f}%', ha='center', va='bottom', fontsize=7, fontweight='bold')\n\n# --- Plot 5: Per-template mean delta (generated, truncated) ---\nax = axes[1, 0]\ngen_keys = list(analysis['gen_template_stats'].keys())\ngen_means = [analysis['gen_template_stats'][k]['mean_delta'] for k in gen_keys]\ngen_wins = [analysis['gen_template_stats'][k]['win_rate'] * 100 for k in gen_keys]\nbar_colors = ['#4c72b0' if d > 0 else '#c44e52' for d in gen_means]\nbars = ax.bar(range(len(gen_keys)), gen_means, color=bar_colors)\nax.set_xticks(range(len(gen_keys)))\nax.set_xticklabels([k.replace('_', '\\n') for k in gen_keys], fontsize=7)\nax.axhline(0, color='black', linestyle='--', linewidth=0.8)\nax.set_ylabel('Mean Delta NLL')\nax.set_title('Truncated Gen: Per-Template')\nfor i, (bar, wr) in enumerate(zip(bars, gen_wins)):\n    ax.text(i, bar.get_height(), f'{wr:.0f}%', ha='center', va='bottom', fontsize=7)\n\n# --- Plot 6: Per-template mean delta (static, truncated) ---\nax = axes[1, 1]\nstatic_keys = list(analysis['static_template_stats'].keys())\nstatic_means = [analysis['static_template_stats'][k]['mean_delta'] for k in static_keys]\nstatic_wins = [analysis['static_template_stats'][k]['win_rate'] * 100 for k in static_keys]\nbar_colors_s = ['#dd8452' if d > 0 else '#c44e52' for d in static_means]\nbars = ax.bar(range(len(static_keys)), static_means, color=bar_colors_s)\nax.set_xticks(range(len(static_keys)))\nax.set_xticklabels([k.replace('static_', '').replace('_', '\\n') for k in static_keys], fontsize=7)\nax.axhline(0, color='black', linestyle='--', linewidth=0.8)\nax.set_ylabel('Mean Delta NLL')\nax.set_title('Truncated Static: Per-Template')\nfor i, (bar, wr) in enumerate(zip(bars, static_wins)):\n    ax.text(i, bar.get_height(), f'{wr:.0f}%', ha='center', va='bottom', fontsize=7)\n\n# --- Plot 7: Win rate by baseline quartile (full-ctx vs truncated) ---\nax = axes[1, 2]\nq_labels = []\nq_full_gen_wr = []\nq_full_random_wr = []\nq_trunc_gen_wr = []\nq_gen_rob_wr = []\nfor label, cond in bins:\n    subset = [r for r in results if cond(r['baseline_nll'])]\n    if not subset:\n        continue\n    q_labels.append(label)\n    q_full_gen_wr.append(np.mean([r['delta_full_gen'] > 0 for r in subset]) * 100)\n    q_full_random_wr.append(np.mean([r['delta_full_random'] > 0 for r in subset]) * 100)\n    q_trunc_gen_wr.append(np.mean([r['delta_gen_routed'] > 0 for r in subset]) * 100)\n    q_gen_rob_wr.append(np.mean([r['delta_gen_routed_or_bl'] > 0 for r in subset]) * 100)\n\nx_pos = np.arange(len(q_labels))\nw = 0.2\nax.bar(x_pos - 1.5*w, q_full_gen_wr, w, label='Full-Ctx Gen', color='#4c72b0')\nax.bar(x_pos - 0.5*w, q_full_random_wr, w, label='Full-Ctx Random', color='#c44e52')\nax.bar(x_pos + 0.5*w, q_trunc_gen_wr, w, label='Trunc Gen', color='#8da0cb')\nax.bar(x_pos + 1.5*w, q_gen_rob_wr, w, label='Gen+BL Routed', color='#e6ab02')\nax.axhline(50, color='black', linestyle='--', linewidth=0.8, alpha=0.5)\nax.set_xticks(x_pos)\nax.set_xticklabels(q_labels, fontsize=8)\nax.set_ylabel('Win Rate %')\nax.set_title('Win Rate by Difficulty Quartile')\nax.legend(fontsize=7)\nax.set_ylim(0, 100)\n\n# --- Plot 8: Baseline similarity distribution and routing decisions ---\nax = axes[1, 3]\nbl_sims = np.array([r['baseline_similarity'] for r in results])\ngen_best_sims = np.array([r['gen_routed_similarity'] for r in results])\nchose_bl = np.array([r['gen_routed_or_bl_chose_baseline'] for r in results])\n\nax.scatter(bl_sims[~chose_bl], gen_best_sims[~chose_bl],\n           alpha=0.3, s=10, c='#8da0cb', label='Chose surrogate')\nax.scatter(bl_sims[chose_bl], gen_best_sims[chose_bl],\n           alpha=0.5, s=15, c='#e6ab02', label='Chose baseline', marker='x')\nlims = [min(ax.get_xlim()[0], ax.get_ylim()[0]), max(ax.get_xlim()[1], ax.get_ylim()[1])]\nax.plot(lims, lims, 'k--', linewidth=0.8, alpha=0.5)\nax.set_xlabel('Baseline (passage) similarity to query')\nax.set_ylabel('Best gen surrogate similarity to query')\nax.set_title('Routing Decision:\\nPassage vs Best Surrogate')\nax.legend(fontsize=7)\n\nplt.tight_layout()\nplt.savefig('05_bugfix_rerun_results.png', dpi=150, bbox_inches='tight')\nplt.show()\nprint('Saved: 05_bugfix_rerun_results.png')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 12: Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": "# Serialize results to JSON\noutput = {\n    'metadata': {\n        'experiment': '05_bugfix_rerun',\n        'description': (\n            'Rerun of experiment 04 with three bug fixes: '\n            '(1) RoPE correction now uses half-split pairing matching HF rotate_half, '\n            '(2) truncated cache preserves BOS token, '\n            '(3) tokenizer boundary mismatch fixed. '\n            '5 LLM-generated + 5 static surrogates per document, '\n            'with similarity-based routing and oracle selection.'\n        ),\n        'timestamp': datetime.datetime.now().isoformat(),\n        'model_name': config.model_name,\n        'num_samples_requested': config.num_samples,\n        'num_samples_evaluated': len(results),\n        'num_skipped': skipped,\n        'num_errors': errors,\n        'elapsed_seconds': elapsed_total,\n        'seed': config.seed,\n        'min_passage_words': config.min_passage_words,\n        'max_passage_words': config.max_passage_words,\n        'surrogate_max_tokens': config.surrogate_max_tokens,\n        'surrogate_temperature': config.surrogate_temperature,\n    },\n    'analysis': {\n        k: (v if not isinstance(v, np.floating) else float(v))\n        for k, v in analysis.items()\n        if k not in ('gen_template_stats', 'static_template_stats')\n    },\n    'gen_template_stats': {\n        k: {sk: float(sv) if isinstance(sv, np.floating) else sv for sk, sv in v.items()}\n        for k, v in analysis['gen_template_stats'].items()\n    },\n    'static_template_stats': {\n        k: {sk: float(sv) if isinstance(sv, np.floating) else sv for sk, sv in v.items()}\n        for k, v in analysis['static_template_stats'].items()\n    },\n    'results': results,\n}\n\noutput_path = '05_bugfix_rerun_results.json'\nwith open(output_path, 'w') as f:\n    json.dump(output, f, indent=2, default=str)\nprint(f\"Results saved to {output_path}\")\nprint(f\"File size: {os.path.getsize(output_path) / 1e6:.1f} MB\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 13: Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": "print('=' * 80)\nprint('CONCLUSIONS')\nprint('=' * 80)\n\nprint(f'\\nSamples evaluated: {len(results)} (skipped {skipped}, errors {errors})')\nprint(f'\\nBaseline (Document:\\\\n{{doc}}) mean NLL: {analysis[\"mean_baseline_nll\"]:.4f}')\n\n# Bare doc\nbare_mean = np.mean([r['bare_nll'] for r in results])\nbare_wr = np.mean([r['delta_bare'] > 0 for r in results]) * 100\nt_bare, p_bare = stats.ttest_rel(baseline_arr, np.array([r['bare_nll'] for r in results]))\nprint(f'\\n--- Bare Document (no framing) ---')\nprint(f'  Mean NLL: {bare_mean:.4f}  (win rate: {bare_wr:.1f}%, p={p_bare:.4f})')\n\n# Full context conditions\nfor label, nll_key in [('Full-Ctx Gen (routed)', 'full_gen_nll'),\n                        ('Full-Ctx Static (routed)', 'full_static_nll'),\n                        ('Full-Ctx Perfect (query)', 'full_perfect_nll'),\n                        ('Full-Ctx Random Prefix', 'full_random_nll')]:\n    arr = np.array([r[nll_key] for r in results])\n    delta = baseline_arr - arr\n    t, p = stats.ttest_rel(baseline_arr, arr)\n    wr = np.mean(delta > 0) * 100\n    print(f'\\n--- {label} ---')\n    print(f'  Mean NLL: {np.mean(arr):.4f}  (win rate: {wr:.1f}%, p={p:.4f})')\n\n# Truncated conditions\nperfect_mean = np.mean([r['perfect_nll'] for r in results])\nperfect_wr = np.mean([r['delta_perfect'] > 0 for r in results]) * 100\nprint(f'\\n--- Trunc+Corr Perfect (actual query) ---')\nprint(f'  Mean NLL: {perfect_mean:.4f}  (win rate: {perfect_wr:.1f}%, p={p_perfect:.4f})')\n\nprint(f'\\n--- Trunc+Corr Generated (routed) ---')\nprint(f'  Mean NLL: {analysis[\"mean_gen_routed_nll\"]:.4f}  (win rate: {analysis[\"win_rate_gen_routed\"]*100:.1f}%, p={analysis[\"p_value_gen_routed\"]:.4f})')\n\nprint(f'\\n--- Trunc+Corr Static (routed) ---')\nprint(f'  Mean NLL: {analysis[\"mean_static_routed_nll\"]:.4f}  (win rate: {analysis[\"win_rate_static_routed\"]*100:.1f}%, p={analysis[\"p_value_static_routed\"]:.4f})')\n\n# Routed-or-Baseline conditions\nfor label, nll_key, delta_key, bl_key in [\n    ('Gen+BL (routed)',      'gen_routed_or_bl_nll',      'delta_gen_routed_or_bl',      'gen_routed_or_bl_chose_baseline'),\n    ('Static+BL (routed)',   'static_routed_or_bl_nll',   'delta_static_routed_or_bl',   'static_routed_or_bl_chose_baseline'),\n    ('Combined+BL (routed)', 'combined_routed_or_bl_nll', 'delta_combined_routed_or_bl', 'combined_routed_or_bl_chose_baseline'),\n]:\n    arr = np.array([r[nll_key] for r in results])\n    deltas = np.array([r[delta_key] for r in results])\n    t, p = stats.ttest_rel(baseline_arr, arr)\n    wr = np.mean(deltas > 0) * 100\n    bl_rate = np.mean([r[bl_key] for r in results]) * 100\n    print(f'\\n--- {label} ---')\n    print(f'  Mean NLL: {np.mean(arr):.4f}  (win rate: {wr:.1f}%, p={p:.4f})')\n    print(f'  Baseline chosen: {bl_rate:.1f}% of samples')\n\n# Key interpretations\nfull_gen_mean = np.mean([r['full_gen_nll'] for r in results])\nfull_random_mean = np.mean([r['full_random_nll'] for r in results])\nt_sem, p_sem = stats.ttest_rel(\n    np.array([r['full_gen_nll'] for r in results]),\n    np.array([r['full_random_nll'] for r in results])\n)\n\nprint(f'\\n{\"=\" * 80}')\nprint('KEY FINDINGS')\nprint('=' * 80)\n\ntrunc_gen_mean = analysis['mean_gen_routed_nll']\nbl_mean = analysis['mean_baseline_nll']\n\nprint(f'\\n1. TRUNCATED + CORRECTED vs BASELINE:')\nif analysis['p_value_gen_routed'] < 0.05 and trunc_gen_mean < bl_mean:\n    print('   Truncated+corrected caches significantly beat baseline.')\nelse:\n    print('   Truncated+corrected caches do NOT significantly beat baseline.')\n    print('   RoPE correction neutralizes truncation damage but preserves no semantic benefit.')\n\nprint(f'\\n2. FULL-CONTEXT vs BASELINE:')\nt_fc, p_fc = stats.ttest_rel(baseline_arr, np.array([r['full_gen_nll'] for r in results]))\nif p_fc < 0.05 and full_gen_mean < bl_mean:\n    print(f'   Full-context generated surrogates significantly improve over baseline (p={p_fc:.4f}).')\nelse:\n    print(f'   Full-context generated surrogates do NOT significantly improve (p={p_fc:.4f}).')\n\nprint(f'\\n3. SEMANTIC vs POSITIONAL (full-ctx gen vs full-ctx random):')\nif p_sem < 0.05:\n    if full_gen_mean < full_random_mean:\n        print(f'   Generated > random (p={p_sem:.4f}): benefit has a SEMANTIC component.')\n    else:\n        print(f'   Random >= generated (p={p_sem:.4f}): benefit is POSITIONAL, not semantic.')\nelse:\n    print(f'   No significant difference (p={p_sem:.4f}): benefit appears POSITIONAL.')\n\nprint(f'\\n4. BARE DOCUMENT:')\nif p_bare < 0.05 and bare_mean < bl_mean:\n    print(f'   Bare doc (no framing) is significantly better than framed baseline (p={p_bare:.4f}).')\n    print('   The \"Document:\\\\n\" framing text itself slightly hurts performance.')\n\nprint(f'\\n5. ROUTED-OR-BASELINE (can the router benefit from a baseline fallback?):')\ngen_rob_mean = np.mean([r['gen_routed_or_bl_nll'] for r in results])\ngen_plain_mean = np.mean([r['gen_routed_nll'] for r in results])\nt_rob, p_rob = stats.ttest_rel(\n    np.array([r['gen_routed_nll'] for r in results]),\n    np.array([r['gen_routed_or_bl_nll'] for r in results])\n)\ngen_bl_rate = np.mean([r['gen_routed_or_bl_chose_baseline'] for r in results]) * 100\nif p_rob < 0.05 and gen_rob_mean < gen_plain_mean:\n    print(f'   Adding baseline fallback SIGNIFICANTLY improves routing (p={p_rob:.4f}).')\n    print(f'   Baseline chosen {gen_bl_rate:.1f}% of the time.')\n    print('   The router correctly identifies when no surrogate is helpful.')\nelif gen_bl_rate < 5:\n    print(f'   Baseline rarely chosen ({gen_bl_rate:.1f}%). Surrogates almost always win similarity contest.')\nelse:\n    print(f'   Baseline chosen {gen_bl_rate:.1f}% of the time, but no significant overall improvement (p={p_rob:.4f}).')\n\nprint('\\n' + '=' * 80)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cu124.m127",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/base-cu124:m127"
  },
  "kernelspec": {
   "display_name": "Python 3 (Local)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}