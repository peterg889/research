{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Directed KV Cache: Comprehensive Surrogate Routing Experiment\n",
    "\n",
    "## Objective\n",
    "\n",
    "Comprehensive evaluation of **surrogate-primed KV caches** across multiple strategies:\n",
    "truncated + RoPE-corrected caches, full-context caches, and key controls.\n",
    "\n",
    "### Motivation from Prior Experiments\n",
    "\n",
    "The `directed_kvcache_experiment` (200 samples, 15 conditions) produced these findings:\n",
    "\n",
    "1. **RoPE correction works mechanically** — it fully recovers from the catastrophic\n",
    "   damage of naive truncation (NLL 2.58 → 1.24), but corrected truncated caches\n",
    "   merely return to baseline, never beating it.\n",
    "2. **Full-context surrogates DO help** — keeping the surrogate visible during inference\n",
    "   significantly improves NLL (win rate ~71%, p < 1e-10).\n",
    "3. **Random prefix also helps** — a random token prefix kept in full context achieved\n",
    "   77% win rate, suggesting the benefit may be positional/structural rather than semantic.\n",
    "4. **Bare document (no \"Document:\\n\") is best** — removing the framing label gave the\n",
    "   lowest NLL overall.\n",
    "\n",
    "This experiment scales these findings to 500 samples with 5+5 surrogate routing to\n",
    "determine whether the patterns hold at scale and whether routing among surrogates\n",
    "captures additional value.\n",
    "\n",
    "### Experimental Conditions\n",
    "\n",
    "| Condition | Description |\n",
    "|-----------|-------------|\n",
    "| **Baseline** | Document cached with framing (`\"Document:\\n{document}\"`) |\n",
    "| **Bare Document** | Document cached without framing (raw text only) |\n",
    "| **Truncated + Corrected (5 gen + 5 static + perfect)** | Surrogate primed, truncated, RoPE corrected |\n",
    "| **Full Context (routed gen + routed static + perfect)** | Surrogate kept visible during inference |\n",
    "| **Full Context Random** | Random token prefix kept visible (positional control) |\n",
    "| **Routed / Oracle** | Best surrogate selected by similarity / by NLL |\n",
    "\n",
    "### Key Questions\n",
    "\n",
    "1. At 500-sample scale, do truncated+corrected caches ever beat baseline?\n",
    "2. Does full-context surrogate routing provide consistent improvement?\n",
    "3. Is the full-context improvement semantic (surrogate content matters) or positional\n",
    "   (any prefix helps equally)?\n",
    "4. Does the perfect surrogate condition separate these hypotheses?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch: 2.10.0+cu128\n",
      "CUDA available: True\n",
      "GPU: NVIDIA L4\n",
      "GPU Memory: 23.6 GB\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import datetime\n",
    "from typing import Dict, List, Any, Optional\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ensure lib is importable\n",
    "sys.path.insert(0, '.')\n",
    "\n",
    "from lib import (\n",
    "    ExperimentConfig,\n",
    "    build_kv_cache,\n",
    "    score_answer_with_cache,\n",
    "    extract_and_truncate_cache,\n",
    "    build_truncated_kv_cache,\n",
    "    correct_rope_positions,\n",
    "    build_truncated_kv_cache_corrected,\n",
    "    generate_all_5_surrogates,\n",
    "    compute_similarity,\n",
    "    load_evaluation_samples,\n",
    "    load_ms_marco,\n",
    "    TOP_5_SURROGATE_TEMPLATES,\n",
    "    STATIC_SURROGATE_QUERIES,\n",
    "    analyze_experiment_results,\n",
    ")\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: mistralai/Mistral-7B-Instruct-v0.2\n",
      "Samples: 1000\n",
      "Passage words: 50-300\n",
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "config = ExperimentConfig(\n",
    "    num_samples=1000,\n",
    "    min_passage_words=50,\n",
    "    max_passage_words=300,\n",
    "    surrogate_max_tokens=45,\n",
    "    surrogate_temperature=0.3,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "print(f\"Model: {config.model_name}\")\n",
    "print(f\"Samples: {config.num_samples}\")\n",
    "print(f\"Passage words: {config.min_passage_words}-{config.max_passage_words}\")\n",
    "print(f\"Device: {config.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load Model, Tokenizer, and Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading mistralai/Mistral-7B-Instruct-v0.2...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1195aaf0af4343cd9e96ebd2c1f23b67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/291 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on cuda\n",
      "Loading embedding model: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3aa228004197491ca90469692d51bcc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding model loaded.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Set seeds\n",
    "torch.manual_seed(config.seed)\n",
    "np.random.seed(config.seed)\n",
    "\n",
    "# Load model (4-bit quantized)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    ")\n",
    "\n",
    "print(f\"Loading {config.model_name}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "model.eval()\n",
    "print(f\"Model loaded on {config.device}\")\n",
    "\n",
    "# Load embedding model for routing\n",
    "print(f\"Loading embedding model: {config.embedding_model_name}\")\n",
    "embed_model = SentenceTransformer(config.embedding_model_name)\n",
    "print(\"Embedding model loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`trust_remote_code` is not supported anymore.\n",
      "Please check that the Hugging Face dataset 'microsoft/ms_marco' isn't based on a loading script and remove `trust_remote_code`.\n",
      "If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading microsoft/ms_marco dataset...\n",
      "Dataset loaded: 10047 samples\n",
      "Filtering samples...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "649c60e00cf143b0ad17222f4c5420d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filtering:   0%|          | 0/10047 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 1000 samples\n",
      "Loaded 1000 evaluation samples\n",
      "\n",
      "Example sample:\n",
      "  Query: what foods other than dairy products contain calcium...\n",
      "  Passage: • Almonds. Almonds contain more calcium than any other nut, and they are also go...\n",
      "  Answer: 1 Spinach. 2 Kale. 3 Okra. 4 Collards. 5 Soybeans. 6 White beans. 7 Some fish li\n"
     ]
    }
   ],
   "source": [
    "dataset = load_ms_marco(config)\n",
    "samples = load_evaluation_samples(dataset, config, require_answer=True)\n",
    "print(f\"Loaded {len(samples)} evaluation samples\")\n",
    "\n",
    "# Quick sanity check\n",
    "s = samples[0]\n",
    "print(f\"\\nExample sample:\")\n",
    "print(f\"  Query: {s['query'][:80]}...\")\n",
    "print(f\"  Passage: {s['passage'][:80]}...\")\n",
    "print(f\"  Answer: {s['answer'][:80]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Surrogate Templates\n",
    "\n",
    "We use 5 LLM-generated (document-specific) and 5 static (document-agnostic) surrogate queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "GENERATED SURROGATE TEMPLATES (document-specific, LLM-guided)\n",
      "================================================================================\n",
      "  target_question                -> Target Natural Language Question\n",
      "  keyword_query                  -> Keyword-ese Query\n",
      "  symptom_scenario               -> Symptom/Scenario Query\n",
      "  misconception_negative         -> Misconception/Negative Query\n",
      "  messy_realworld                -> Messy Real-World Query\n",
      "\n",
      "================================================================================\n",
      "STATIC SURROGATE QUERIES (same for all documents)\n",
      "================================================================================\n",
      "  static_definitional            -> \"What is this and what does it mean?\"\n",
      "  static_procedural              -> \"How do I do this step by step?\"\n",
      "  static_quantitative            -> \"How much does this cost or how long does it take?\"\n",
      "  static_factual                 -> \"What are the key facts I need to know?\"\n",
      "  static_problem                 -> \"What problem does this solve?\"\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"GENERATED SURROGATE TEMPLATES (document-specific, LLM-guided)\")\n",
    "print(\"=\" * 80)\n",
    "for key, tmpl in TOP_5_SURROGATE_TEMPLATES.items():\n",
    "    print(f\"  {key:30s} -> {tmpl['name']}\")\n",
    "\n",
    "print()\n",
    "print(\"=\" * 80)\n",
    "print(\"STATIC SURROGATE QUERIES (same for all documents)\")\n",
    "print(\"=\" * 80)\n",
    "for key, info in STATIC_SURROGATE_QUERIES.items():\n",
    "    print(f\"  {key:30s} -> \\\"{info['query']}\\\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Test on One Sample\n",
    "\n",
    "Verify the full pipeline works before running the main loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passage: • Almonds. Almonds contain more calcium than any other nut, and they are also good sources of fiber, folic acid, magnesi...\n",
      "Query:   what foods other than dairy products contain calcium\n",
      "Answer:  1 Spinach. 2 Kale. 3 Okra. 4 Collards. 5 Soybeans. 6 White beans. 7 Some fish li\n",
      "\n",
      "Generated surrogates:\n",
      "  target_question                -> \"What nuts are rich in calcium and which brands offer enriched imitation dairy products with added calcium?\" (sim=0.794)\n",
      "  keyword_query                  -> \"almonds, calcium-rich nuts, fiber, folic acid, magnesium, potassium, protein, imitation dairy products, enriched varieties, Edensoy, Rice Dream, Sil\" (sim=0.607)\n",
      "  symptom_scenario               -> \"User is looking for calcium-fortified non-dairy milk brands. (Query: \"Calcium-fortified non-dairy milk brands\")\" (sim=0.666)\n",
      "  misconception_negative         -> \"Is consuming almonds instead of dairy bad for calcium intake?\"\" (sim=0.733)\n",
      "  messy_realworld                -> \"Almonds: more calcium, fiber, plz name rich nut recipes, imitation dairy brands asap\"\" (sim=0.674)\n",
      "\n",
      "Static surrogates:\n",
      "  static_definitional            -> \"What is this and what does it mean?\" (sim=-0.163)\n",
      "  static_procedural              -> \"How do I do this step by step?\" (sim=-0.064)\n",
      "  static_quantitative            -> \"How much does this cost or how long does it take?\" (sim=-0.064)\n",
      "  static_factual                 -> \"What are the key facts I need to know?\" (sim=0.087)\n",
      "  static_problem                 -> \"What problem does this solve?\" (sim=-0.066)\n",
      "\n",
      "Baseline NLL: 2.0877\n",
      "Perfect surrogate NLL: 4.6667\n",
      "Delta (positive = better): -2.5789\n",
      "Generated surrogate NLL (target_question): 4.2982\n",
      "Delta (positive = better): -2.2105\n"
     ]
    }
   ],
   "source": [
    "# Generate surrogates for the first sample\n",
    "test_sample = samples[0]\n",
    "print(f\"Passage: {test_sample['passage'][:120]}...\")\n",
    "print(f\"Query:   {test_sample['query']}\")\n",
    "print(f\"Answer:  {test_sample['answer'][:80]}\")\n",
    "print()\n",
    "\n",
    "test_surrogates = generate_all_5_surrogates(\n",
    "    test_sample['passage'], model, tokenizer, config\n",
    ")\n",
    "print(\"Generated surrogates:\")\n",
    "for key, surr in test_surrogates.items():\n",
    "    sim = compute_similarity(surr, test_sample['query'], embed_model)\n",
    "    print(f\"  {key:30s} -> \\\"{surr}\\\" (sim={sim:.3f})\")\n",
    "\n",
    "print()\n",
    "print(\"Static surrogates:\")\n",
    "for key, info in STATIC_SURROGATE_QUERIES.items():\n",
    "    sim = compute_similarity(info['query'], test_sample['query'], embed_model)\n",
    "    print(f\"  {key:30s} -> \\\"{info['query']}\\\" (sim={sim:.3f})\")\n",
    "\n",
    "# Test baseline cache\n",
    "baseline_ctx = config.baseline_cache_template.format(document=test_sample['passage'])\n",
    "bl_len, bl_cache = build_kv_cache(baseline_ctx, model, tokenizer, config)\n",
    "query_prompt = config.query_template.format(query=test_sample['query'])\n",
    "bl_nll = score_answer_with_cache(\n",
    "    bl_cache, bl_len, query_prompt, test_sample['answer'],\n",
    "    model, tokenizer, config\n",
    ")\n",
    "print(f\"\\nBaseline NLL: {bl_nll:.4f}\")\n",
    "\n",
    "# Test perfect surrogate (actual query)\n",
    "pf_len, pf_cache = build_truncated_kv_cache_corrected(\n",
    "    test_sample['query'], test_sample['passage'], model, tokenizer, config\n",
    ")\n",
    "pf_nll = score_answer_with_cache(\n",
    "    pf_cache, pf_len, query_prompt, test_sample['answer'],\n",
    "    model, tokenizer, config\n",
    ")\n",
    "print(f\"Perfect surrogate NLL: {pf_nll:.4f}\")\n",
    "print(f\"Delta (positive = better): {bl_nll - pf_nll:.4f}\")\n",
    "\n",
    "# Test one generated surrogate\n",
    "first_key = list(test_surrogates.keys())[0]\n",
    "first_surr = test_surrogates[first_key]\n",
    "doc_len, corrected_cache = build_truncated_kv_cache_corrected(\n",
    "    first_surr, test_sample['passage'], model, tokenizer, config\n",
    ")\n",
    "corrected_nll = score_answer_with_cache(\n",
    "    corrected_cache, doc_len, query_prompt, test_sample['answer'],\n",
    "    model, tokenizer, config\n",
    ")\n",
    "print(f\"Generated surrogate NLL ({first_key}): {corrected_nll:.4f}\")\n",
    "print(f\"Delta (positive = better): {bl_nll - corrected_nll:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Per-Sample Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_prefix(passage, tokenizer, config, seed):\n",
    "    \"\"\"Generate a random token prefix roughly matching a typical surrogate length.\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    vocab_size = tokenizer.vocab_size\n",
    "    # Generate ~20 random token IDs (typical surrogate prefix length)\n",
    "    random_ids = np.random.randint(100, vocab_size, size=20)\n",
    "    random_text = tokenizer.decode(random_ids, skip_special_tokens=True)\n",
    "    return random_text\n",
    "\n",
    "\n",
    "def evaluate_sample(\n",
    "    sample: Dict,\n",
    "    idx: int,\n",
    "    model: AutoModelForCausalLM,\n",
    "    tokenizer: AutoTokenizer,\n",
    "    embed_model: SentenceTransformer,\n",
    "    config: ExperimentConfig,\n",
    ") -> Optional[Dict]:\n",
    "    \"\"\"\n",
    "    Evaluate a single sample across all experimental conditions:\n",
    "\n",
    "    A. Baselines:\n",
    "       - baseline: \"Document:\\\\n{document}\"\n",
    "       - bare_doc: \"{document}\" (no framing)\n",
    "\n",
    "    B. Truncated + RoPE corrected:\n",
    "       - 5 generated surrogates\n",
    "       - 5 static surrogates\n",
    "       - perfect surrogate (actual query)\n",
    "\n",
    "    C. Full context (surrogate kept visible):\n",
    "       - routed generated surrogate (best by similarity)\n",
    "       - routed static surrogate (best by similarity)\n",
    "       - perfect surrogate (actual query)\n",
    "       - random prefix (positional control)\n",
    "\n",
    "    Returns None if the sample should be skipped.\n",
    "    \"\"\"\n",
    "    passage = sample['passage']\n",
    "    query = sample['query']\n",
    "    answer = sample['answer']\n",
    "\n",
    "    # --- Guard: answer must tokenize to >= 2 tokens for a meaningful NLL ---\n",
    "    answer_ids = tokenizer(\n",
    "        answer, return_tensors='pt', add_special_tokens=False\n",
    "    )['input_ids']\n",
    "    if answer_ids.shape[1] < 2:\n",
    "        return None\n",
    "\n",
    "    query_prompt = config.query_template.format(query=query)\n",
    "\n",
    "    # ==================== A. BASELINES ====================\n",
    "\n",
    "    # A1: Standard baseline with framing\n",
    "    baseline_context = config.baseline_cache_template.format(document=passage)\n",
    "    baseline_len, baseline_cache = build_kv_cache(\n",
    "        baseline_context, model, tokenizer, config\n",
    "    )\n",
    "    baseline_nll = score_answer_with_cache(\n",
    "        baseline_cache, baseline_len, query_prompt, answer,\n",
    "        model, tokenizer, config\n",
    "    )\n",
    "\n",
    "    # A2: Bare document (no \"Document:\\n\" framing)\n",
    "    bare_len, bare_cache = build_kv_cache(passage, model, tokenizer, config)\n",
    "    bare_nll = score_answer_with_cache(\n",
    "        bare_cache, bare_len, query_prompt, answer,\n",
    "        model, tokenizer, config\n",
    "    )\n",
    "\n",
    "    # ==================== B. TRUNCATED + CORRECTED ====================\n",
    "\n",
    "    # B1: Perfect surrogate (actual query), truncated + corrected\n",
    "    perfect_doc_len, perfect_cache = build_truncated_kv_cache_corrected(\n",
    "        query, passage, model, tokenizer, config\n",
    "    )\n",
    "    perfect_trunc_nll = score_answer_with_cache(\n",
    "        perfect_cache, perfect_doc_len, query_prompt, answer,\n",
    "        model, tokenizer, config\n",
    "    )\n",
    "\n",
    "    # B2: Generated surrogates, truncated + corrected\n",
    "    generated_surrogates = generate_all_5_surrogates(passage, model, tokenizer, config)\n",
    "\n",
    "    generated_similarities = {\n",
    "        key: compute_similarity(surr, query, embed_model)\n",
    "        for key, surr in generated_surrogates.items()\n",
    "    }\n",
    "\n",
    "    generated_nlls = {}\n",
    "    for key, surrogate in generated_surrogates.items():\n",
    "        doc_len, corrected_cache = build_truncated_kv_cache_corrected(\n",
    "            surrogate, passage, model, tokenizer, config\n",
    "        )\n",
    "        nll = score_answer_with_cache(\n",
    "            corrected_cache, doc_len, query_prompt, answer,\n",
    "            model, tokenizer, config\n",
    "        )\n",
    "        generated_nlls[key] = nll\n",
    "\n",
    "    # B3: Static surrogates, truncated + corrected\n",
    "    static_surrogates = {key: info['query'] for key, info in STATIC_SURROGATE_QUERIES.items()}\n",
    "\n",
    "    static_similarities = {\n",
    "        key: compute_similarity(surr, query, embed_model)\n",
    "        for key, surr in static_surrogates.items()\n",
    "    }\n",
    "\n",
    "    static_nlls = {}\n",
    "    for key, surrogate in static_surrogates.items():\n",
    "        doc_len, corrected_cache = build_truncated_kv_cache_corrected(\n",
    "            surrogate, passage, model, tokenizer, config\n",
    "        )\n",
    "        nll = score_answer_with_cache(\n",
    "            corrected_cache, doc_len, query_prompt, answer,\n",
    "            model, tokenizer, config\n",
    "        )\n",
    "        static_nlls[key] = nll\n",
    "\n",
    "    # ==================== ROUTING (truncated) ====================\n",
    "\n",
    "    gen_routed_key = max(generated_similarities, key=generated_similarities.get)\n",
    "    gen_routed_nll = generated_nlls[gen_routed_key]\n",
    "    gen_routed_similarity = generated_similarities[gen_routed_key]\n",
    "\n",
    "    gen_oracle_key = min(generated_nlls, key=generated_nlls.get)\n",
    "    gen_oracle_nll = generated_nlls[gen_oracle_key]\n",
    "\n",
    "    static_routed_key = max(static_similarities, key=static_similarities.get)\n",
    "    static_routed_nll = static_nlls[static_routed_key]\n",
    "    static_routed_similarity = static_similarities[static_routed_key]\n",
    "\n",
    "    static_oracle_key = min(static_nlls, key=static_nlls.get)\n",
    "    static_oracle_nll = static_nlls[static_oracle_key]\n",
    "\n",
    "    # ==================== C. FULL CONTEXT (surrogate kept visible) ====================\n",
    "\n",
    "    # C1: Full-context routed generated surrogate\n",
    "    gen_routed_surr = generated_surrogates[gen_routed_key]\n",
    "    full_gen_context = config.surrogate_cache_template.format(\n",
    "        surrogate=gen_routed_surr, document=passage\n",
    "    )\n",
    "    full_gen_len, full_gen_cache = build_kv_cache(\n",
    "        full_gen_context, model, tokenizer, config\n",
    "    )\n",
    "    full_gen_nll = score_answer_with_cache(\n",
    "        full_gen_cache, full_gen_len, query_prompt, answer,\n",
    "        model, tokenizer, config\n",
    "    )\n",
    "\n",
    "    # C2: Full-context routed static surrogate\n",
    "    static_routed_surr = static_surrogates[static_routed_key]\n",
    "    full_static_context = config.surrogate_cache_template.format(\n",
    "        surrogate=static_routed_surr, document=passage\n",
    "    )\n",
    "    full_static_len, full_static_cache = build_kv_cache(\n",
    "        full_static_context, model, tokenizer, config\n",
    "    )\n",
    "    full_static_nll = score_answer_with_cache(\n",
    "        full_static_cache, full_static_len, query_prompt, answer,\n",
    "        model, tokenizer, config\n",
    "    )\n",
    "\n",
    "    # C3: Full-context perfect surrogate (actual query)\n",
    "    full_perfect_context = config.surrogate_cache_template.format(\n",
    "        surrogate=query, document=passage\n",
    "    )\n",
    "    full_perfect_len, full_perfect_cache = build_kv_cache(\n",
    "        full_perfect_context, model, tokenizer, config\n",
    "    )\n",
    "    full_perfect_nll = score_answer_with_cache(\n",
    "        full_perfect_cache, full_perfect_len, query_prompt, answer,\n",
    "        model, tokenizer, config\n",
    "    )\n",
    "\n",
    "    # C4: Full-context random prefix (positional control)\n",
    "    random_prefix = generate_random_prefix(passage, tokenizer, config, seed=config.seed + idx)\n",
    "    full_random_context = config.surrogate_cache_template.format(\n",
    "        surrogate=random_prefix, document=passage\n",
    "    )\n",
    "    full_random_len, full_random_cache = build_kv_cache(\n",
    "        full_random_context, model, tokenizer, config\n",
    "    )\n",
    "    full_random_nll = score_answer_with_cache(\n",
    "        full_random_cache, full_random_len, query_prompt, answer,\n",
    "        model, tokenizer, config\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        'query': query,\n",
    "        'answer_preview': answer[:50] + '...' if len(answer) > 50 else answer,\n",
    "\n",
    "        # A. Baselines\n",
    "        'baseline_nll': baseline_nll,\n",
    "        'bare_nll': bare_nll,\n",
    "\n",
    "        # B. Truncated + corrected\n",
    "        'perfect_nll': perfect_trunc_nll,\n",
    "        'delta_perfect': baseline_nll - perfect_trunc_nll,\n",
    "\n",
    "        'generated_surrogates': generated_surrogates,\n",
    "        'generated_similarities': generated_similarities,\n",
    "        'generated_nlls': generated_nlls,\n",
    "        'gen_routed_key': gen_routed_key,\n",
    "        'gen_routed_nll': gen_routed_nll,\n",
    "        'gen_routed_similarity': gen_routed_similarity,\n",
    "        'gen_oracle_key': gen_oracle_key,\n",
    "        'gen_oracle_nll': gen_oracle_nll,\n",
    "\n",
    "        'static_surrogates': static_surrogates,\n",
    "        'static_similarities': static_similarities,\n",
    "        'static_nlls': static_nlls,\n",
    "        'static_routed_key': static_routed_key,\n",
    "        'static_routed_nll': static_routed_nll,\n",
    "        'static_routed_similarity': static_routed_similarity,\n",
    "        'static_oracle_key': static_oracle_key,\n",
    "        'static_oracle_nll': static_oracle_nll,\n",
    "\n",
    "        # C. Full context\n",
    "        'full_gen_nll': full_gen_nll,\n",
    "        'full_static_nll': full_static_nll,\n",
    "        'full_perfect_nll': full_perfect_nll,\n",
    "        'full_random_nll': full_random_nll,\n",
    "\n",
    "        # Deltas vs baseline (positive = better)\n",
    "        'delta_gen_routed': baseline_nll - gen_routed_nll,\n",
    "        'delta_gen_oracle': baseline_nll - gen_oracle_nll,\n",
    "        'delta_static_routed': baseline_nll - static_routed_nll,\n",
    "        'delta_static_oracle': baseline_nll - static_oracle_nll,\n",
    "        'delta_bare': baseline_nll - bare_nll,\n",
    "        'delta_full_gen': baseline_nll - full_gen_nll,\n",
    "        'delta_full_static': baseline_nll - full_static_nll,\n",
    "        'delta_full_perfect': baseline_nll - full_perfect_nll,\n",
    "        'delta_full_random': baseline_nll - full_random_nll,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Run the Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "RUNNING COMPREHENSIVE SURROGATE ROUTING EXPERIMENT\n",
      "Samples: 1000\n",
      "Per sample: 5 gen + 5 static + 1 perfect (truncated+corrected)\n",
      "          + full-context gen/static/perfect/random + bare doc\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca29fdb668e649fb9f4674ac5fb6e866",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = []\n",
    "skipped = 0\n",
    "errors = 0\n",
    "start_time = time.time()\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"RUNNING COMPREHENSIVE SURROGATE ROUTING EXPERIMENT\")\n",
    "print(f\"Samples: {len(samples)}\")\n",
    "print(f\"Per sample: 5 gen + 5 static + 1 perfect (truncated+corrected)\")\n",
    "print(f\"          + full-context gen/static/perfect/random + bare doc\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for idx, sample in enumerate(tqdm(samples, desc=\"Evaluating\")):\n",
    "    try:\n",
    "        result = evaluate_sample(sample, idx, model, tokenizer, embed_model, config)\n",
    "        if result is None:\n",
    "            skipped += 1\n",
    "            continue\n",
    "        results.append(result)\n",
    "    except Exception as e:\n",
    "        errors += 1\n",
    "        if errors <= 3:\n",
    "            print(f\"\\n  Error on sample {idx}: {type(e).__name__}: {e}\")\n",
    "        continue\n",
    "\n",
    "    # Progress report every 50 samples\n",
    "    if len(results) > 0 and len(results) % 50 == 0:\n",
    "        elapsed = time.time() - start_time\n",
    "        rate = elapsed / len(results)\n",
    "        remaining = rate * (len(samples) - idx - 1)\n",
    "\n",
    "        recent = results[-50:]\n",
    "        bl = np.mean([r['baseline_nll'] for r in recent])\n",
    "        br = np.mean([r['bare_nll'] for r in recent])\n",
    "        fg = np.mean([r['full_gen_nll'] for r in recent])\n",
    "        fr = np.mean([r['full_random_nll'] for r in recent])\n",
    "        gr = np.mean([r['gen_routed_nll'] for r in recent])\n",
    "        wr_fg = np.mean([r['delta_full_gen'] > 0 for r in recent]) * 100\n",
    "        wr_fr = np.mean([r['delta_full_random'] > 0 for r in recent]) * 100\n",
    "        wr_gt = np.mean([r['delta_gen_routed'] > 0 for r in recent]) * 100\n",
    "\n",
    "        print(\n",
    "            f\"\\n  [{len(results):>4d} done | {elapsed/60:.0f}m elapsed | ~{remaining/60:.0f}m left]\\n\"\n",
    "            f\"  Last 50: baseline={bl:.3f}  bare={br:.3f}\\n\"\n",
    "            f\"    Full-ctx: gen={fg:.3f} ({wr_fg:.0f}% win)  random={fr:.3f} ({wr_fr:.0f}% win)\\n\"\n",
    "            f\"    Trunc:    gen_routed={gr:.3f} ({wr_gt:.0f}% win)\"\n",
    "        )\n",
    "\n",
    "elapsed_total = time.time() - start_time\n",
    "print(f\"\\nDone. {len(results)} evaluated, {skipped} skipped (short answer), {errors} errors.\")\n",
    "print(f\"Total time: {elapsed_total/60:.1f} minutes ({elapsed_total/len(results):.1f}s per sample)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.analysis import print_analysis_summary\n",
    "\n",
    "analysis = analyze_experiment_results(results)\n",
    "print_analysis_summary(analysis)\n",
    "\n",
    "# Perfect surrogate analysis (not covered by analyze_experiment_results)\n",
    "perfect_nlls = np.array([r['perfect_nll'] for r in results])\n",
    "deltas_perfect = np.array([r['delta_perfect'] for r in results])\n",
    "baseline_nlls_arr = np.array([r['baseline_nll'] for r in results])\n",
    "\n",
    "t_perfect, p_perfect = stats.ttest_rel(baseline_nlls_arr, perfect_nlls)\n",
    "\n",
    "from lib.analysis import cohens_d\n",
    "d_perfect = cohens_d(deltas_perfect)\n",
    "\n",
    "print()\n",
    "print(\"=\" * 80)\n",
    "print(\"PERFECT SURROGATE (actual query as surrogate, truncated + RoPE corrected)\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"  Mean NLL:    {np.mean(perfect_nlls):.4f} ± {np.std(perfect_nlls):.4f}\")\n",
    "print(f\"  Mean Delta:  {np.mean(deltas_perfect):.4f} (positive = better than baseline)\")\n",
    "print(f\"  Win Rate:    {np.mean(deltas_perfect > 0)*100:.1f}%\")\n",
    "print(f\"  t-stat:      {t_perfect:.3f}\")\n",
    "print(f\"  p-value:     {p_perfect:.4f}\")\n",
    "print(f\"  Cohen's d:   {d_perfect:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-template breakdown\n",
    "print(\"=\" * 80)\n",
    "print(\"PER-TEMPLATE BREAKDOWN: GENERATED SURROGATES\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"{'Template':<30} {'Mean NLL':>10} {'Delta':>10} {'Win%':>8} {'Routed':>8} {'Oracle':>8}\")\n",
    "print(\"-\" * 80)\n",
    "for key, st in analysis['gen_template_stats'].items():\n",
    "    print(\n",
    "        f\"{key:<30} {st['mean_nll']:>10.4f} {st['mean_delta']:>10.4f}\"\n",
    "        f\" {st['win_rate']*100:>7.1f}% {st['times_routed']:>8d} {st['times_oracle']:>8d}\"\n",
    "    )\n",
    "\n",
    "print()\n",
    "print(\"=\" * 80)\n",
    "print(\"PER-TEMPLATE BREAKDOWN: STATIC SURROGATES\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"{'Template':<30} {'Mean NLL':>10} {'Delta':>10} {'Win%':>8} {'Routed':>8} {'Oracle':>8}\")\n",
    "print(\"-\" * 80)\n",
    "for key, st in analysis['static_template_stats'].items():\n",
    "    print(\n",
    "        f\"{key:<30} {st['mean_nll']:>10.4f} {st['mean_delta']:>10.4f}\"\n",
    "        f\" {st['win_rate']*100:>7.1f}% {st['times_routed']:>8d} {st['times_oracle']:>8d}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Effect sizes and significance\n",
    "print(\"=\" * 80)\n",
    "print(\"EFFECT SIZES\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Cohen's d (gen routed vs baseline):    {analysis['cohens_d_gen_routed']:.4f}\")\n",
    "print(f\"Cohen's d (static routed vs baseline): {analysis['cohens_d_static_routed']:.4f}\")\n",
    "print()\n",
    "print(f\"Generated beats static rate: {analysis['gen_beats_static_rate']*100:.1f}%\")\n",
    "print(f\"Gen vs Static p-value:       {analysis['p_value_gen_vs_static']:.4f}\")\n",
    "\n",
    "# Routing efficiency: how close does routing get to oracle?\n",
    "if analysis['mean_delta_gen_oracle'] != 0:\n",
    "    gen_efficiency = analysis['mean_delta_gen_routed'] / analysis['mean_delta_gen_oracle'] * 100\n",
    "    print(f\"\\nGenerated routing efficiency: {gen_efficiency:.1f}% of oracle\")\n",
    "if analysis['mean_delta_static_oracle'] != 0:\n",
    "    static_efficiency = analysis['mean_delta_static_routed'] / analysis['mean_delta_static_oracle'] * 100\n",
    "    print(f\"Static routing efficiency:    {static_efficiency:.1f}% of oracle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8b: Full-Context and Controls Analysis\n",
    "\n",
    "These conditions test whether the benefit of a surrogate prefix is **semantic** (content\n",
    "matters) or **positional** (any prefix helps). The `directed_kvcache_experiment` found\n",
    "that random prefixes helped as much as meaningful surrogates when kept in full context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full-context and control conditions analysis\n",
    "baseline_arr = np.array([r['baseline_nll'] for r in results])\n",
    "bare_arr = np.array([r['bare_nll'] for r in results])\n",
    "full_gen_arr = np.array([r['full_gen_nll'] for r in results])\n",
    "full_static_arr = np.array([r['full_static_nll'] for r in results])\n",
    "full_perfect_arr = np.array([r['full_perfect_nll'] for r in results])\n",
    "full_random_arr = np.array([r['full_random_nll'] for r in results])\n",
    "\n",
    "conditions_extra = {\n",
    "    'Bare Doc (no framing)':    ('bare_nll',         'delta_bare'),\n",
    "    'Full-Ctx Gen (routed)':    ('full_gen_nll',     'delta_full_gen'),\n",
    "    'Full-Ctx Static (routed)': ('full_static_nll',  'delta_full_static'),\n",
    "    'Full-Ctx Perfect (query)': ('full_perfect_nll', 'delta_full_perfect'),\n",
    "    'Full-Ctx Random Prefix':   ('full_random_nll',  'delta_full_random'),\n",
    "    'Trunc+Corr Gen (routed)':  ('gen_routed_nll',   'delta_gen_routed'),\n",
    "    'Trunc+Corr Perfect':       ('perfect_nll',      'delta_perfect'),\n",
    "}\n",
    "\n",
    "print('=' * 95)\n",
    "print('ALL CONDITIONS vs BASELINE (sorted by Mean NLL)')\n",
    "print('=' * 95)\n",
    "print(f\"{'Condition':<30} {'Mean NLL':>10} {'Std':>8} {'Delta':>10} {'Win%':>8} {'t-stat':>8} {'p-value':>10} {\\\"Cohen's d\\\":>10}\")\n",
    "print('-' * 95)\n",
    "print(f\"{'Baseline (Document:\\\\n{doc})':<30} {np.mean(baseline_arr):>10.4f} {np.std(baseline_arr):>8.4f} {'--':>10} {'--':>8} {'--':>8} {'--':>10} {'--':>10}\")\n",
    "print('-' * 95)\n",
    "\n",
    "rows = []\n",
    "for label, (nll_key, delta_key) in conditions_extra.items():\n",
    "    nlls = np.array([r[nll_key] for r in results])\n",
    "    deltas = np.array([r[delta_key] for r in results])\n",
    "    t, p = stats.ttest_rel(baseline_arr, nlls)\n",
    "    d = cohens_d(deltas)\n",
    "    wr = np.mean(deltas > 0) * 100\n",
    "    rows.append((np.mean(nlls), label, np.std(nlls), np.mean(deltas), wr, t, p, d))\n",
    "\n",
    "rows.sort(key=lambda x: x[0])\n",
    "for mean_nll, label, std, delta, wr, t, p, d in rows:\n",
    "    sig = '*' if p < 0.05 else ' '\n",
    "    print(f'{label:<30} {mean_nll:>10.4f} {std:>8.4f} {delta:>+10.4f} {wr:>7.1f}% {t:>8.2f} {p:>10.4f}{sig} {d:>10.4f}')\n",
    "\n",
    "print()\n",
    "print('KEY COMPARISON: Is the full-context benefit semantic or positional?')\n",
    "print('-' * 80)\n",
    "fg_mean = np.mean(full_gen_arr)\n",
    "fr_mean = np.mean(full_random_arr)\n",
    "t_sem, p_sem = stats.ttest_rel(full_gen_arr, full_random_arr)\n",
    "print(f'  Full-ctx generated (routed) NLL: {fg_mean:.4f}')\n",
    "print(f'  Full-ctx random prefix NLL:      {fr_mean:.4f}')\n",
    "print(f'  Paired t-test (gen vs random):   t={t_sem:.3f}, p={p_sem:.4f}')\n",
    "if p_sem < 0.05:\n",
    "    if fg_mean < fr_mean:\n",
    "        print('  -> Generated surrogate is SIGNIFICANTLY better than random prefix.')\n",
    "        print('     The benefit has a semantic component beyond just positional effects.')\n",
    "    else:\n",
    "        print('  -> Random prefix is SIGNIFICANTLY better than generated surrogate.')\n",
    "        print('     The benefit is purely positional; surrogate content may actually hurt.')\n",
    "else:\n",
    "    print('  -> No significant difference. The benefit is likely positional, not semantic.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Stratified Analysis by Baseline Difficulty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Earlier experiments showed surrogates help most when baseline NLL is high.\n",
    "# Does the RoPE correction change this pattern?\n",
    "\n",
    "baseline_nlls = np.array([r['baseline_nll'] for r in results])\n",
    "quartiles = np.percentile(baseline_nlls, [25, 50, 75])\n",
    "\n",
    "bins = [\n",
    "    ('Q1 (easiest)', lambda x: x <= quartiles[0]),\n",
    "    ('Q2', lambda x: quartiles[0] < x <= quartiles[1]),\n",
    "    ('Q3', lambda x: quartiles[1] < x <= quartiles[2]),\n",
    "    ('Q4 (hardest)', lambda x: x > quartiles[2]),\n",
    "]\n",
    "\n",
    "print(\"=\" * 100)\n",
    "print(\"WIN RATE BY BASELINE DIFFICULTY QUARTILE\")\n",
    "print(\"=\" * 100)\n",
    "print(f\"{'Quartile':<20} {'N':>5} {'BL NLL':>8} {'Perfect':>10} {'Prf Win%':>10} {'Gen Rtd':>10} {'Gen Win%':>10} {'Stc Rtd':>10} {'Stc Win%':>10}\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "for label, cond in bins:\n",
    "    subset = [r for r in results if cond(r['baseline_nll'])]\n",
    "    if not subset:\n",
    "        continue\n",
    "    bl = np.mean([r['baseline_nll'] for r in subset])\n",
    "    pf = np.mean([r['perfect_nll'] for r in subset])\n",
    "    gr = np.mean([r['gen_routed_nll'] for r in subset])\n",
    "    sr = np.mean([r['static_routed_nll'] for r in subset])\n",
    "    pw = np.mean([r['delta_perfect'] > 0 for r in subset]) * 100\n",
    "    gw = np.mean([r['delta_gen_routed'] > 0 for r in subset]) * 100\n",
    "    sw = np.mean([r['delta_static_routed'] > 0 for r in subset]) * 100\n",
    "    print(f\"{label:<20} {len(subset):>5} {bl:>8.3f} {pf:>10.3f} {pw:>9.1f}% {gr:>10.3f} {gw:>9.1f}% {sr:>10.3f} {sw:>9.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Similarity-Delta Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Does a higher-similarity surrogate predict a bigger improvement?\n",
    "gen_sims = np.array([r['gen_routed_similarity'] for r in results])\n",
    "gen_deltas = np.array([r['delta_gen_routed'] for r in results])\n",
    "static_sims = np.array([r['static_routed_similarity'] for r in results])\n",
    "static_deltas = np.array([r['delta_static_routed'] for r in results])\n",
    "\n",
    "r_gen, p_gen = stats.pearsonr(gen_sims, gen_deltas)\n",
    "r_static, p_static = stats.pearsonr(static_sims, static_deltas)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"CORRELATION: SURROGATE-QUERY SIMILARITY vs NLL IMPROVEMENT\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Generated routed: r={r_gen:.3f}, p={p_gen:.4f}\")\n",
    "print(f\"Static routed:    r={r_static:.3f}, p={p_static:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 3, figsize=(20, 11))\n",
    "fig.suptitle(\n",
    "    'Directed KV Cache: Comprehensive Surrogate Routing Experiment',\n",
    "    fontsize=14, fontweight='bold'\n",
    ")\n",
    "\n",
    "# --- Plot 1: All conditions NLL comparison ---\n",
    "ax = axes[0, 0]\n",
    "cond_labels = ['Baseline', 'Bare Doc', 'Full\\nGen', 'Full\\nRandom', 'Full\\nPerfect',\n",
    "               'Trunc\\nGen Rtd', 'Trunc\\nPerfect']\n",
    "cond_data = [\n",
    "    np.array([r['baseline_nll'] for r in results]),\n",
    "    np.array([r['bare_nll'] for r in results]),\n",
    "    np.array([r['full_gen_nll'] for r in results]),\n",
    "    np.array([r['full_random_nll'] for r in results]),\n",
    "    np.array([r['full_perfect_nll'] for r in results]),\n",
    "    np.array([r['gen_routed_nll'] for r in results]),\n",
    "    np.array([r['perfect_nll'] for r in results]),\n",
    "]\n",
    "cond_colors = ['#cccccc', '#8c8c8c', '#4c72b0', '#c44e52', '#55a868', '#8da0cb', '#a6d96a']\n",
    "bp = ax.boxplot(cond_data, labels=cond_labels, patch_artist=True)\n",
    "for patch, c in zip(bp['boxes'], cond_colors):\n",
    "    patch.set_facecolor(c)\n",
    "ax.set_ylabel('NLL')\n",
    "ax.set_title('NLL by Condition')\n",
    "ax.tick_params(axis='x', rotation=0, labelsize=7)\n",
    "\n",
    "# --- Plot 2: Full-context vs Truncated delta distributions ---\n",
    "ax = axes[0, 1]\n",
    "delta_full_gen = np.array([r['delta_full_gen'] for r in results])\n",
    "delta_full_random = np.array([r['delta_full_random'] for r in results])\n",
    "delta_trunc_gen = np.array([r['delta_gen_routed'] for r in results])\n",
    "ax.hist(delta_full_gen, bins=50, alpha=0.5,\n",
    "        label=f'Full-ctx gen (win={np.mean(delta_full_gen>0)*100:.0f}%)', color='#4c72b0')\n",
    "ax.hist(delta_full_random, bins=50, alpha=0.5,\n",
    "        label=f'Full-ctx random (win={np.mean(delta_full_random>0)*100:.0f}%)', color='#c44e52')\n",
    "ax.hist(delta_trunc_gen, bins=50, alpha=0.5,\n",
    "        label=f'Trunc gen (win={np.mean(delta_trunc_gen>0)*100:.0f}%)', color='#8da0cb')\n",
    "ax.axvline(0, color='black', linestyle='--', linewidth=0.8)\n",
    "ax.set_xlabel('Delta NLL (positive = better than baseline)')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('Full-Context vs Truncated')\n",
    "ax.legend(fontsize=7)\n",
    "\n",
    "# --- Plot 3: Semantic vs Positional scatter ---\n",
    "ax = axes[0, 2]\n",
    "ax.scatter(\n",
    "    [r['full_random_nll'] for r in results],\n",
    "    [r['full_gen_nll'] for r in results],\n",
    "    alpha=0.3, s=10, c='#4c72b0'\n",
    ")\n",
    "lims = [min(ax.get_xlim()[0], ax.get_ylim()[0]), max(ax.get_xlim()[1], ax.get_ylim()[1])]\n",
    "ax.plot(lims, lims, 'k--', linewidth=0.8, alpha=0.5)\n",
    "ax.set_xlabel('Full-Context Random NLL')\n",
    "ax.set_ylabel('Full-Context Generated NLL')\n",
    "ax.set_title('Semantic vs Positional\\n(below diagonal = gen wins)')\n",
    "\n",
    "# --- Plot 4: Per-template mean delta (generated, truncated) ---\n",
    "ax = axes[1, 0]\n",
    "gen_keys = list(analysis['gen_template_stats'].keys())\n",
    "gen_means = [analysis['gen_template_stats'][k]['mean_delta'] for k in gen_keys]\n",
    "gen_wins = [analysis['gen_template_stats'][k]['win_rate'] * 100 for k in gen_keys]\n",
    "bar_colors = ['#4c72b0' if d > 0 else '#c44e52' for d in gen_means]\n",
    "bars = ax.bar(range(len(gen_keys)), gen_means, color=bar_colors)\n",
    "ax.set_xticks(range(len(gen_keys)))\n",
    "ax.set_xticklabels([k.replace('_', '\\n') for k in gen_keys], fontsize=7)\n",
    "ax.axhline(0, color='black', linestyle='--', linewidth=0.8)\n",
    "ax.set_ylabel('Mean Delta NLL')\n",
    "ax.set_title('Truncated Gen: Per-Template')\n",
    "for i, (bar, wr) in enumerate(zip(bars, gen_wins)):\n",
    "    ax.text(i, bar.get_height(), f'{wr:.0f}%', ha='center', va='bottom', fontsize=7)\n",
    "\n",
    "# --- Plot 5: Per-template mean delta (static, truncated) ---\n",
    "ax = axes[1, 1]\n",
    "static_keys = list(analysis['static_template_stats'].keys())\n",
    "static_means = [analysis['static_template_stats'][k]['mean_delta'] for k in static_keys]\n",
    "static_wins = [analysis['static_template_stats'][k]['win_rate'] * 100 for k in static_keys]\n",
    "bar_colors_s = ['#dd8452' if d > 0 else '#c44e52' for d in static_means]\n",
    "bars = ax.bar(range(len(static_keys)), static_means, color=bar_colors_s)\n",
    "ax.set_xticks(range(len(static_keys)))\n",
    "ax.set_xticklabels([k.replace('static_', '').replace('_', '\\n') for k in static_keys], fontsize=7)\n",
    "ax.axhline(0, color='black', linestyle='--', linewidth=0.8)\n",
    "ax.set_ylabel('Mean Delta NLL')\n",
    "ax.set_title('Truncated Static: Per-Template')\n",
    "for i, (bar, wr) in enumerate(zip(bars, static_wins)):\n",
    "    ax.text(i, bar.get_height(), f'{wr:.0f}%', ha='center', va='bottom', fontsize=7)\n",
    "\n",
    "# --- Plot 6: Win rate by baseline quartile (full-ctx vs truncated) ---\n",
    "ax = axes[1, 2]\n",
    "q_labels = []\n",
    "q_full_gen_wr = []\n",
    "q_full_random_wr = []\n",
    "q_trunc_gen_wr = []\n",
    "for label, cond in bins:\n",
    "    subset = [r for r in results if cond(r['baseline_nll'])]\n",
    "    if not subset:\n",
    "        continue\n",
    "    q_labels.append(label)\n",
    "    q_full_gen_wr.append(np.mean([r['delta_full_gen'] > 0 for r in subset]) * 100)\n",
    "    q_full_random_wr.append(np.mean([r['delta_full_random'] > 0 for r in subset]) * 100)\n",
    "    q_trunc_gen_wr.append(np.mean([r['delta_gen_routed'] > 0 for r in subset]) * 100)\n",
    "\n",
    "x_pos = np.arange(len(q_labels))\n",
    "w = 0.25\n",
    "ax.bar(x_pos - w, q_full_gen_wr, w, label='Full-Ctx Gen', color='#4c72b0')\n",
    "ax.bar(x_pos, q_full_random_wr, w, label='Full-Ctx Random', color='#c44e52')\n",
    "ax.bar(x_pos + w, q_trunc_gen_wr, w, label='Trunc Gen', color='#8da0cb')\n",
    "ax.axhline(50, color='black', linestyle='--', linewidth=0.8, alpha=0.5)\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels(q_labels, fontsize=8)\n",
    "ax.set_ylabel('Win Rate %')\n",
    "ax.set_title('Win Rate by Difficulty Quartile')\n",
    "ax.legend(fontsize=7)\n",
    "ax.set_ylim(0, 100)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('corrected_truncated_routing_results.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print('Saved: corrected_truncated_routing_results.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 12: Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Serialize results to JSON\n",
    "output = {\n",
    "    'metadata': {\n",
    "        'experiment': 'corrected_truncated_cache_routing',\n",
    "        'description': (\n",
    "            'Surrogate-primed KV cache with truncation and RoPE position correction. '\n",
    "            '5 LLM-generated + 5 static surrogates per document, '\n",
    "            'with similarity-based routing and oracle selection.'\n",
    "        ),\n",
    "        'timestamp': datetime.datetime.now().isoformat(),\n",
    "        'model_name': config.model_name,\n",
    "        'num_samples_requested': config.num_samples,\n",
    "        'num_samples_evaluated': len(results),\n",
    "        'num_skipped': skipped,\n",
    "        'num_errors': errors,\n",
    "        'elapsed_seconds': elapsed_total,\n",
    "        'seed': config.seed,\n",
    "        'min_passage_words': config.min_passage_words,\n",
    "        'max_passage_words': config.max_passage_words,\n",
    "        'surrogate_max_tokens': config.surrogate_max_tokens,\n",
    "        'surrogate_temperature': config.surrogate_temperature,\n",
    "    },\n",
    "    'analysis': {\n",
    "        k: (v if not isinstance(v, np.floating) else float(v))\n",
    "        for k, v in analysis.items()\n",
    "        if k not in ('gen_template_stats', 'static_template_stats')\n",
    "    },\n",
    "    'gen_template_stats': {\n",
    "        k: {sk: float(sv) if isinstance(sv, np.floating) else sv for sk, sv in v.items()}\n",
    "        for k, v in analysis['gen_template_stats'].items()\n",
    "    },\n",
    "    'static_template_stats': {\n",
    "        k: {sk: float(sv) if isinstance(sv, np.floating) else sv for sk, sv in v.items()}\n",
    "        for k, v in analysis['static_template_stats'].items()\n",
    "    },\n",
    "    'results': results,\n",
    "}\n",
    "\n",
    "output_path = 'corrected_truncated_routing_results.json'\n",
    "with open(output_path, 'w') as f:\n",
    "    json.dump(output, f, indent=2, default=str)\n",
    "print(f\"Results saved to {output_path}\")\n",
    "print(f\"File size: {os.path.getsize(output_path) / 1e6:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 13: Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('=' * 80)\n",
    "print('CONCLUSIONS')\n",
    "print('=' * 80)\n",
    "\n",
    "print(f'\\nSamples evaluated: {len(results)} (skipped {skipped}, errors {errors})')\n",
    "print(f'\\nBaseline (Document:\\\\n{{doc}}) mean NLL: {analysis[\"mean_baseline_nll\"]:.4f}')\n",
    "\n",
    "# Bare doc\n",
    "bare_mean = np.mean([r['bare_nll'] for r in results])\n",
    "bare_wr = np.mean([r['delta_bare'] > 0 for r in results]) * 100\n",
    "t_bare, p_bare = stats.ttest_rel(baseline_arr, np.array([r['bare_nll'] for r in results]))\n",
    "print(f'\\n--- Bare Document (no framing) ---')\n",
    "print(f'  Mean NLL: {bare_mean:.4f}  (win rate: {bare_wr:.1f}%, p={p_bare:.4f})')\n",
    "\n",
    "# Full context conditions\n",
    "for label, nll_key in [('Full-Ctx Gen (routed)', 'full_gen_nll'),\n",
    "                        ('Full-Ctx Static (routed)', 'full_static_nll'),\n",
    "                        ('Full-Ctx Perfect (query)', 'full_perfect_nll'),\n",
    "                        ('Full-Ctx Random Prefix', 'full_random_nll')]:\n",
    "    arr = np.array([r[nll_key] for r in results])\n",
    "    delta = baseline_arr - arr\n",
    "    t, p = stats.ttest_rel(baseline_arr, arr)\n",
    "    wr = np.mean(delta > 0) * 100\n",
    "    print(f'\\n--- {label} ---')\n",
    "    print(f'  Mean NLL: {np.mean(arr):.4f}  (win rate: {wr:.1f}%, p={p:.4f})')\n",
    "\n",
    "# Truncated conditions\n",
    "perfect_mean = np.mean([r['perfect_nll'] for r in results])\n",
    "perfect_wr = np.mean([r['delta_perfect'] > 0 for r in results]) * 100\n",
    "print(f'\\n--- Trunc+Corr Perfect (actual query) ---')\n",
    "print(f'  Mean NLL: {perfect_mean:.4f}  (win rate: {perfect_wr:.1f}%, p={p_perfect:.4f})')\n",
    "\n",
    "print(f'\\n--- Trunc+Corr Generated (routed) ---')\n",
    "print(f'  Mean NLL: {analysis[\"mean_gen_routed_nll\"]:.4f}  (win rate: {analysis[\"win_rate_gen_routed\"]*100:.1f}%, p={analysis[\"p_value_gen_routed\"]:.4f})')\n",
    "\n",
    "print(f'\\n--- Trunc+Corr Static (routed) ---')\n",
    "print(f'  Mean NLL: {analysis[\"mean_static_routed_nll\"]:.4f}  (win rate: {analysis[\"win_rate_static_routed\"]*100:.1f}%, p={analysis[\"p_value_static_routed\"]:.4f})')\n",
    "\n",
    "# Key interpretations\n",
    "full_gen_mean = np.mean([r['full_gen_nll'] for r in results])\n",
    "full_random_mean = np.mean([r['full_random_nll'] for r in results])\n",
    "t_sem, p_sem = stats.ttest_rel(\n",
    "    np.array([r['full_gen_nll'] for r in results]),\n",
    "    np.array([r['full_random_nll'] for r in results])\n",
    ")\n",
    "\n",
    "print(f'\\n{\"=\" * 80}')\n",
    "print('KEY FINDINGS')\n",
    "print('=' * 80)\n",
    "\n",
    "trunc_gen_mean = analysis['mean_gen_routed_nll']\n",
    "bl_mean = analysis['mean_baseline_nll']\n",
    "\n",
    "print(f'\\n1. TRUNCATED + CORRECTED vs BASELINE:')\n",
    "if analysis['p_value_gen_routed'] < 0.05 and trunc_gen_mean < bl_mean:\n",
    "    print('   Truncated+corrected caches significantly beat baseline.')\n",
    "else:\n",
    "    print('   Truncated+corrected caches do NOT significantly beat baseline.')\n",
    "    print('   RoPE correction neutralizes truncation damage but preserves no semantic benefit.')\n",
    "\n",
    "print(f'\\n2. FULL-CONTEXT vs BASELINE:')\n",
    "t_fc, p_fc = stats.ttest_rel(baseline_arr, np.array([r['full_gen_nll'] for r in results]))\n",
    "if p_fc < 0.05 and full_gen_mean < bl_mean:\n",
    "    print(f'   Full-context generated surrogates significantly improve over baseline (p={p_fc:.4f}).')\n",
    "else:\n",
    "    print(f'   Full-context generated surrogates do NOT significantly improve (p={p_fc:.4f}).')\n",
    "\n",
    "print(f'\\n3. SEMANTIC vs POSITIONAL (full-ctx gen vs full-ctx random):')\n",
    "if p_sem < 0.05:\n",
    "    if full_gen_mean < full_random_mean:\n",
    "        print(f'   Generated > random (p={p_sem:.4f}): benefit has a SEMANTIC component.')\n",
    "    else:\n",
    "        print(f'   Random >= generated (p={p_sem:.4f}): benefit is POSITIONAL, not semantic.')\n",
    "else:\n",
    "    print(f'   No significant difference (p={p_sem:.4f}): benefit appears POSITIONAL.')\n",
    "\n",
    "print(f'\\n4. BARE DOCUMENT:')\n",
    "if p_bare < 0.05 and bare_mean < bl_mean:\n",
    "    print(f'   Bare doc (no framing) is significantly better than framed baseline (p={p_bare:.4f}).')\n",
    "    print('   The \"Document:\\\\n\" framing text itself slightly hurts performance.')\n",
    "\n",
    "print('\\n' + '=' * 80)\n"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cu124.m127",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/base-cu124:m127"
  },
  "kernelspec": {
   "display_name": "Python 3 (Local)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
