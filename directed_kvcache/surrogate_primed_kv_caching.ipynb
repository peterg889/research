{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Surrogate-Primed KV Caching Experiment\n",
    "\n",
    "## Hypothesis\n",
    "Prepending a domain-specific \"surrogate query\" to a document before caching will produce KV states that are more receptive to the actual user query, compared to a generic system prompt.\n",
    "\n",
    "## Metric\n",
    "We measure **Conditional Perplexity Delta**:\n",
    "- `Delta = NLL(Query | Baseline Cache) - NLL(Query | Surrogate Cache)`\n",
    "- **Positive Delta** = Surrogate reduced surprise (success)\n",
    "- **Negative Delta** = Surrogate confused the model (failure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Configuration & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies if needed\n",
    "# !pip install transformers torch datasets tqdm scipy bitsandbytes accelerate matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from datasets import load_dataset\n",
    "from tqdm.auto import tqdm\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import json\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, List, Dict, Tuple\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ExperimentConfig:\n",
    "    \"\"\"Configuration for the Surrogate-Primed KV Caching experiment.\"\"\"\n",
    "    \n",
    "    # Model settings\n",
    "    model_name: str = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "    # Alternative: \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "    \n",
    "    # Quantization for memory efficiency (fits on T4/L4 GPU)\n",
    "    use_4bit: bool = True\n",
    "    \n",
    "    # Dataset settings\n",
    "    dataset_name: str = \"ms_marco\"\n",
    "    dataset_config: str = \"v1.1\"\n",
    "    dataset_split: str = \"validation\"\n",
    "    num_samples: int = 100\n",
    "    min_passage_words: int = 50\n",
    "    max_passage_words: int = 200\n",
    "    \n",
    "    # Surrogate generation settings\n",
    "    surrogate_temperature: float = 0.0  # Deterministic for reproducibility\n",
    "    surrogate_max_tokens: int = 15\n",
    "    \n",
    "    # Prompts\n",
    "    baseline_prompt: str = \"System: Read the following document carefully.\\n\\n\"\n",
    "    surrogate_generation_prompt: str = (\n",
    "        \"You are a search engine optimization expert. Read the following text. \"\n",
    "        \"Write a single, short search query that a user would type to find this text. \"\n",
    "        \"Output ONLY the query, nothing else.\"\n",
    "    )\n",
    "    surrogate_prefix_template: str = \"System: Answer the following query: {surrogate}\\n\\nDocument:\\n\"\n",
    "    \n",
    "    # Random seed for reproducibility\n",
    "    seed: int = 42\n",
    "    \n",
    "    # Device\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "config = ExperimentConfig()\n",
    "print(f\"Running on device: {config.device}\")\n",
    "print(f\"Model: {config.model_name}\")\n",
    "print(f\"4-bit quantization: {config.use_4bit}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(config.seed)\n",
    "np.random.seed(config.seed)\n",
    "\n",
    "# Load model and tokenizer\n",
    "print(\"Loading model and tokenizer...\")\n",
    "\n",
    "if config.use_4bit:\n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\"\n",
    "    )\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        config.model_name,\n",
    "        quantization_config=quantization_config,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "else:\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        config.model_name,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.model_name, trust_remote_code=True)\n",
    "\n",
    "# Ensure padding is on the right (critical for causal LM)\n",
    "tokenizer.padding_side = \"right\"\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model.eval()\n",
    "print(f\"Model loaded successfully. Parameters: {model.num_parameters():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Dataset Loading & Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(text: str) -> int:\n",
    "    \"\"\"Count words in a text string.\"\"\"\n",
    "    return len(text.split())\n",
    "\n",
    "\n",
    "def load_and_filter_dataset(config: ExperimentConfig) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Load MS MARCO dataset and filter passages by word count.\n",
    "    \n",
    "    Returns list of dicts with 'passage' and 'query' keys.\n",
    "    \"\"\"\n",
    "    print(f\"Loading {config.dataset_name} dataset...\")\n",
    "    \n",
    "    # Load MS MARCO\n",
    "    dataset = load_dataset(\n",
    "        config.dataset_name,\n",
    "        config.dataset_config,\n",
    "        split=config.dataset_split,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    print(f\"Total samples in {config.dataset_split}: {len(dataset)}\")\n",
    "    \n",
    "    # Filter and extract valid samples\n",
    "    filtered_samples = []\n",
    "    \n",
    "    for item in tqdm(dataset, desc=\"Filtering passages\"):\n",
    "        # MS MARCO has passages as a list; we take the first relevant one\n",
    "        passages = item.get('passages', {})\n",
    "        passage_texts = passages.get('passage_text', [])\n",
    "        is_selected = passages.get('is_selected', [])\n",
    "        \n",
    "        query = item.get('query', '')\n",
    "        \n",
    "        if not passage_texts or not query:\n",
    "            continue\n",
    "        \n",
    "        # Find a passage that meets word count criteria\n",
    "        for i, passage in enumerate(passage_texts):\n",
    "            word_count = count_words(passage)\n",
    "            if config.min_passage_words <= word_count <= config.max_passage_words:\n",
    "                # Prefer selected passages if available\n",
    "                if is_selected and i < len(is_selected) and is_selected[i] == 1:\n",
    "                    filtered_samples.append({\n",
    "                        'passage': passage,\n",
    "                        'query': query\n",
    "                    })\n",
    "                    break\n",
    "                elif not any(is_selected):  # No selection info, take first valid\n",
    "                    filtered_samples.append({\n",
    "                        'passage': passage,\n",
    "                        'query': query\n",
    "                    })\n",
    "                    break\n",
    "        \n",
    "        # Early stop if we have enough\n",
    "        if len(filtered_samples) >= config.num_samples * 2:  # Buffer for random selection\n",
    "            break\n",
    "    \n",
    "    # Randomly sample the required number\n",
    "    np.random.shuffle(filtered_samples)\n",
    "    filtered_samples = filtered_samples[:config.num_samples]\n",
    "    \n",
    "    print(f\"Selected {len(filtered_samples)} samples meeting criteria\")\n",
    "    return filtered_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and filter dataset\n",
    "samples = load_and_filter_dataset(config)\n",
    "\n",
    "# Preview a sample\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SAMPLE PREVIEW\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nPassage ({count_words(samples[0]['passage'])} words):\")\n",
    "print(samples[0]['passage'][:500] + \"...\" if len(samples[0]['passage']) > 500 else samples[0]['passage'])\n",
    "print(f\"\\nGround Truth Query: {samples[0]['query']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Surrogate Query Generator\n",
    "\n",
    "This function generates a \"surrogate query\" from a document. The surrogate mimics what a user might search for to find this document. \n",
    "\n",
    "**Critical**: The generator NEVER sees the ground truth query - it only sees the document (mimicking \"indexing time\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_surrogate(\n",
    "    doc_text: str,\n",
    "    model: AutoModelForCausalLM,\n",
    "    tokenizer: AutoTokenizer,\n",
    "    config: ExperimentConfig\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Generate a surrogate query for a document.\n",
    "    \n",
    "    The surrogate is what we hypothesize a user might search to find this document.\n",
    "    This function uses the model's chat template for proper instruction following.\n",
    "    \n",
    "    Args:\n",
    "        doc_text: The document/passage text\n",
    "        model: The language model\n",
    "        tokenizer: The tokenizer\n",
    "        config: Experiment configuration\n",
    "    \n",
    "    Returns:\n",
    "        Generated surrogate query string\n",
    "    \"\"\"\n",
    "    # Build the prompt using chat template for instruct models\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"{config.surrogate_generation_prompt}\\n\\nText:\\n{doc_text}\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Apply chat template\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(config.device)\n",
    "    \n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=config.surrogate_max_tokens,\n",
    "            temperature=config.surrogate_temperature if config.surrogate_temperature > 0 else None,\n",
    "            do_sample=config.surrogate_temperature > 0,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    # Decode only the generated tokens (exclude prompt)\n",
    "    generated_tokens = outputs[0][inputs['input_ids'].shape[1]:]\n",
    "    surrogate = tokenizer.decode(generated_tokens, skip_special_tokens=True).strip()\n",
    "    \n",
    "    return surrogate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test surrogate generation\n",
    "print(\"Testing surrogate generation...\")\n",
    "test_surrogate = generate_surrogate(samples[0]['passage'], model, tokenizer, config)\n",
    "print(f\"\\nDocument preview: {samples[0]['passage'][:200]}...\")\n",
    "print(f\"\\nGenerated Surrogate: '{test_surrogate}'\")\n",
    "print(f\"Ground Truth Query: '{samples[0]['query']}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Scoring Engine - KV Cache Surgery\n",
    "\n",
    "This is the critical component. We perform \"KV Cache Surgery\":\n",
    "\n",
    "1. **Prefill Phase**: Forward pass the context (prompt + document) to generate the KV cache\n",
    "2. **Decode Phase**: Forward pass the target query using the frozen KV cache\n",
    "3. **Scoring**: Compute NLL only on the target query tokens\n",
    "\n",
    "### Mathematical Foundation\n",
    "\n",
    "For a sequence `[context, target]`, the model computes:\n",
    "- Keys: `K = W_k @ [context, target]`  \n",
    "- Values: `V = W_v @ [context, target]`\n",
    "\n",
    "The KV cache stores `K_context` and `V_context` from the prefill. During decoding, attention is computed as:\n",
    "\n",
    "```\n",
    "Attention(Q_target, [K_context; K_target], [V_context; V_target])\n",
    "```\n",
    "\n",
    "By priming with different contexts, we change `K_context` and `V_context`, potentially making them more \"receptive\" to `Q_target`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_conditional_nll(\n",
    "    context_prefix: str,\n",
    "    document: str,\n",
    "    target_query: str,\n",
    "    model: AutoModelForCausalLM,\n",
    "    tokenizer: AutoTokenizer,\n",
    "    config: ExperimentConfig\n",
    ") -> Tuple[float, int]:\n",
    "    \"\"\"\n",
    "    Compute the Negative Log-Likelihood of target_query conditioned on\n",
    "    the KV cache generated from (context_prefix + document).\n",
    "    \n",
    "    This implements \"KV Cache Surgery\":\n",
    "    1. Prefill: Generate KV cache from context\n",
    "    2. Decode: Score target using the frozen cache\n",
    "    3. Loss: Compute NLL only on target tokens\n",
    "    \n",
    "    Args:\n",
    "        context_prefix: The prefix prompt (baseline or surrogate-based)\n",
    "        document: The document/passage text\n",
    "        target_query: The ground truth query to score\n",
    "        model: The language model\n",
    "        tokenizer: The tokenizer\n",
    "        config: Experiment configuration\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (nll_value, num_target_tokens)\n",
    "    \"\"\"\n",
    "    # =========================================================================\n",
    "    # STEP 1: Tokenize context and target separately\n",
    "    # =========================================================================\n",
    "    # Build full context string\n",
    "    context_str = context_prefix + document\n",
    "    \n",
    "    # Tokenize context (no padding - we process one sample at a time)\n",
    "    context_encoding = tokenizer(\n",
    "        context_str,\n",
    "        return_tensors=\"pt\",\n",
    "        add_special_tokens=True,  # Add BOS if model uses it\n",
    "        padding=False,\n",
    "        truncation=False\n",
    "    )\n",
    "    context_ids = context_encoding['input_ids'].to(config.device)\n",
    "    context_len = context_ids.shape[1]\n",
    "    \n",
    "    # Tokenize target (no special tokens - it continues from context)\n",
    "    target_encoding = tokenizer(\n",
    "        target_query,\n",
    "        return_tensors=\"pt\",\n",
    "        add_special_tokens=False,  # No BOS - continues from context\n",
    "        padding=False,\n",
    "        truncation=False\n",
    "    )\n",
    "    target_ids = target_encoding['input_ids'].to(config.device)\n",
    "    target_len = target_ids.shape[1]\n",
    "    \n",
    "    # =========================================================================\n",
    "    # STEP 2: Prefill - Generate KV cache from context\n",
    "    # =========================================================================\n",
    "    # Forward pass through context to build KV cache\n",
    "    # We don't need the logits from this pass, only the cache\n",
    "    with torch.no_grad():\n",
    "        prefill_outputs = model(\n",
    "            input_ids=context_ids,\n",
    "            attention_mask=torch.ones_like(context_ids),\n",
    "            use_cache=True,\n",
    "            return_dict=True\n",
    "        )\n",
    "        \n",
    "        # Extract the KV cache\n",
    "        # Shape per layer: (batch, num_heads, seq_len, head_dim)\n",
    "        past_key_values = prefill_outputs.past_key_values\n",
    "    \n",
    "    # =========================================================================\n",
    "    # STEP 3: Decode - Forward pass target with frozen KV cache\n",
    "    # =========================================================================\n",
    "    # CRITICAL: The attention mask must cover BOTH context AND target\n",
    "    # The model needs to know the full sequence length for position embeddings\n",
    "    # and attention masking\n",
    "    combined_len = context_len + target_len\n",
    "    attention_mask = torch.ones((1, combined_len), device=config.device)\n",
    "    \n",
    "    # Forward pass target tokens using the cached KV states\n",
    "    with torch.no_grad():\n",
    "        decode_outputs = model(\n",
    "            input_ids=target_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            past_key_values=past_key_values,\n",
    "            use_cache=False,  # Don't need to extend cache further\n",
    "            return_dict=True\n",
    "        )\n",
    "    \n",
    "    # =========================================================================\n",
    "    # STEP 4: Compute NLL on target tokens only\n",
    "    # =========================================================================\n",
    "    # decode_outputs.logits shape: (batch, target_len, vocab_size)\n",
    "    # For autoregressive loss, we predict token[i+1] from logits[i]\n",
    "    # So we use logits[:-1] to predict target_ids[1:]\n",
    "    \n",
    "    logits = decode_outputs.logits  # (1, target_len, vocab_size)\n",
    "    \n",
    "    # Shift: predict next token from current position\n",
    "    # logits[:, :-1, :] predicts target_ids[:, 1:]\n",
    "    shift_logits = logits[:, :-1, :].contiguous()  # (1, target_len-1, vocab)\n",
    "    shift_labels = target_ids[:, 1:].contiguous()  # (1, target_len-1)\n",
    "    \n",
    "    # Flatten for cross entropy\n",
    "    shift_logits = shift_logits.view(-1, shift_logits.size(-1))\n",
    "    shift_labels = shift_labels.view(-1)\n",
    "    \n",
    "    # Compute per-token cross entropy (NLL)\n",
    "    loss_fct = torch.nn.CrossEntropyLoss(reduction='sum')\n",
    "    nll = loss_fct(shift_logits, shift_labels).item()\n",
    "    \n",
    "    # Number of tokens scored (target_len - 1 due to shift)\n",
    "    num_scored_tokens = target_len - 1\n",
    "    \n",
    "    # Return mean NLL per token for comparability\n",
    "    mean_nll = nll / num_scored_tokens if num_scored_tokens > 0 else 0.0\n",
    "    \n",
    "    return mean_nll, num_scored_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the scoring engine\n",
    "print(\"Testing scoring engine...\")\n",
    "\n",
    "test_sample = samples[0]\n",
    "\n",
    "# Baseline condition\n",
    "baseline_nll, baseline_tokens = compute_conditional_nll(\n",
    "    context_prefix=config.baseline_prompt,\n",
    "    document=test_sample['passage'],\n",
    "    target_query=test_sample['query'],\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    config=config\n",
    ")\n",
    "\n",
    "# Surrogate condition\n",
    "surrogate = generate_surrogate(test_sample['passage'], model, tokenizer, config)\n",
    "surrogate_prefix = config.surrogate_prefix_template.format(surrogate=surrogate)\n",
    "\n",
    "surrogate_nll, surrogate_tokens = compute_conditional_nll(\n",
    "    context_prefix=surrogate_prefix,\n",
    "    document=test_sample['passage'],\n",
    "    target_query=test_sample['query'],\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    config=config\n",
    ")\n",
    "\n",
    "delta = baseline_nll - surrogate_nll\n",
    "\n",
    "print(f\"\\nBaseline NLL: {baseline_nll:.4f} ({baseline_tokens} tokens)\")\n",
    "print(f\"Surrogate NLL: {surrogate_nll:.4f} ({surrogate_tokens} tokens)\")\n",
    "print(f\"Delta (Baseline - Surrogate): {delta:.4f}\")\n",
    "print(f\"Result: {'Surrogate WINS' if delta > 0 else 'Baseline WINS'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Main Experiment Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(\n",
    "    samples: List[Dict],\n",
    "    model: AutoModelForCausalLM,\n",
    "    tokenizer: AutoTokenizer,\n",
    "    config: ExperimentConfig\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Run the full experiment comparing baseline vs surrogate-primed KV caching.\n",
    "    \n",
    "    Args:\n",
    "        samples: List of {passage, query} dicts\n",
    "        model: The language model\n",
    "        tokenizer: The tokenizer\n",
    "        config: Experiment configuration\n",
    "    \n",
    "    Returns:\n",
    "        List of result dicts with NLLs and deltas\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for i, sample in enumerate(tqdm(samples, desc=\"Running experiment\")):\n",
    "        passage = sample['passage']\n",
    "        query = sample['query']\n",
    "        \n",
    "        try:\n",
    "            # Generate surrogate query (only sees document, NOT ground truth query)\n",
    "            surrogate = generate_surrogate(passage, model, tokenizer, config)\n",
    "            \n",
    "            # Condition A: Baseline\n",
    "            baseline_nll, baseline_tokens = compute_conditional_nll(\n",
    "                context_prefix=config.baseline_prompt,\n",
    "                document=passage,\n",
    "                target_query=query,\n",
    "                model=model,\n",
    "                tokenizer=tokenizer,\n",
    "                config=config\n",
    "            )\n",
    "            \n",
    "            # Condition B: Surrogate-primed\n",
    "            surrogate_prefix = config.surrogate_prefix_template.format(surrogate=surrogate)\n",
    "            surrogate_nll, surrogate_tokens = compute_conditional_nll(\n",
    "                context_prefix=surrogate_prefix,\n",
    "                document=passage,\n",
    "                target_query=query,\n",
    "                model=model,\n",
    "                tokenizer=tokenizer,\n",
    "                config=config\n",
    "            )\n",
    "            \n",
    "            # Compute delta: positive means surrogate is better\n",
    "            delta = baseline_nll - surrogate_nll\n",
    "            \n",
    "            results.append({\n",
    "                'sample_idx': i,\n",
    "                'query': query,\n",
    "                'surrogate': surrogate,\n",
    "                'baseline_nll': baseline_nll,\n",
    "                'surrogate_nll': surrogate_nll,\n",
    "                'delta': delta,\n",
    "                'num_tokens': baseline_tokens,\n",
    "                'passage_preview': passage[:100] + '...' if len(passage) > 100 else passage\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\nError processing sample {i}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        # Clear GPU cache periodically\n",
    "        if i % 20 == 0:\n",
    "            torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the experiment\n",
    "print(f\"Running experiment on {len(samples)} samples...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "results = run_experiment(samples, model, tokenizer, config)\n",
    "\n",
    "print(f\"\\nCompleted {len(results)} samples successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Analysis & Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_results(results: List[Dict]) -> Dict:\n",
    "    \"\"\"\n",
    "    Compute summary statistics and perform statistical tests.\n",
    "    \n",
    "    Args:\n",
    "        results: List of result dicts from experiment\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary of analysis results\n",
    "    \"\"\"\n",
    "    deltas = np.array([r['delta'] for r in results])\n",
    "    baseline_nlls = np.array([r['baseline_nll'] for r in results])\n",
    "    surrogate_nlls = np.array([r['surrogate_nll'] for r in results])\n",
    "    \n",
    "    # Win rate: proportion where surrogate is better (delta > 0)\n",
    "    wins = np.sum(deltas > 0)\n",
    "    losses = np.sum(deltas < 0)\n",
    "    ties = np.sum(deltas == 0)\n",
    "    win_rate = wins / len(deltas)\n",
    "    \n",
    "    # Paired t-test: Are the NLLs significantly different?\n",
    "    t_stat, p_value = stats.ttest_rel(baseline_nlls, surrogate_nlls)\n",
    "    \n",
    "    # Effect size (Cohen's d for paired samples)\n",
    "    diff = baseline_nlls - surrogate_nlls\n",
    "    cohens_d = np.mean(diff) / np.std(diff, ddof=1) if np.std(diff) > 0 else 0\n",
    "    \n",
    "    # Wilcoxon signed-rank test (non-parametric alternative)\n",
    "    wilcoxon_stat, wilcoxon_p = stats.wilcoxon(baseline_nlls, surrogate_nlls, alternative='two-sided')\n",
    "    \n",
    "    analysis = {\n",
    "        'n_samples': len(results),\n",
    "        'win_rate': win_rate,\n",
    "        'wins': wins,\n",
    "        'losses': losses,\n",
    "        'ties': ties,\n",
    "        'mean_delta': np.mean(deltas),\n",
    "        'std_delta': np.std(deltas),\n",
    "        'median_delta': np.median(deltas),\n",
    "        'mean_baseline_nll': np.mean(baseline_nlls),\n",
    "        'mean_surrogate_nll': np.mean(surrogate_nlls),\n",
    "        't_statistic': t_stat,\n",
    "        'p_value': p_value,\n",
    "        'cohens_d': cohens_d,\n",
    "        'wilcoxon_stat': wilcoxon_stat,\n",
    "        'wilcoxon_p': wilcoxon_p,\n",
    "    }\n",
    "    \n",
    "    return analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute analysis\n",
    "analysis = analyze_results(results)\n",
    "\n",
    "# Print summary\n",
    "print(\"=\"*80)\n",
    "print(\"EXPERIMENT RESULTS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nSamples analyzed: {analysis['n_samples']}\")\n",
    "print(f\"\\n--- Win/Loss Record ---\")\n",
    "print(f\"Surrogate Wins:  {analysis['wins']} ({analysis['win_rate']*100:.1f}%)\")\n",
    "print(f\"Baseline Wins:   {analysis['losses']} ({(analysis['losses']/analysis['n_samples'])*100:.1f}%)\")\n",
    "print(f\"Ties:            {analysis['ties']}\")\n",
    "\n",
    "print(f\"\\n--- NLL Statistics ---\")\n",
    "print(f\"Mean Baseline NLL:  {analysis['mean_baseline_nll']:.4f}\")\n",
    "print(f\"Mean Surrogate NLL: {analysis['mean_surrogate_nll']:.4f}\")\n",
    "print(f\"Mean Delta:         {analysis['mean_delta']:.4f} (positive = surrogate better)\")\n",
    "print(f\"Std Delta:          {analysis['std_delta']:.4f}\")\n",
    "print(f\"Median Delta:       {analysis['median_delta']:.4f}\")\n",
    "\n",
    "print(f\"\\n--- Statistical Tests ---\")\n",
    "print(f\"Paired t-test: t={analysis['t_statistic']:.4f}, p={analysis['p_value']:.6f}\")\n",
    "print(f\"Wilcoxon test: W={analysis['wilcoxon_stat']:.4f}, p={analysis['wilcoxon_p']:.6f}\")\n",
    "print(f\"Cohen's d (effect size): {analysis['cohens_d']:.4f}\")\n",
    "\n",
    "significance_level = 0.05\n",
    "if analysis['p_value'] < significance_level:\n",
    "    print(f\"\\n*** RESULT: Statistically significant difference (p < {significance_level}) ***\")\n",
    "    if analysis['mean_delta'] > 0:\n",
    "        print(\"*** CONCLUSION: Surrogate-primed KV caching IMPROVES query prediction ***\")\n",
    "    else:\n",
    "        print(\"*** CONCLUSION: Surrogate-primed KV caching HURTS query prediction ***\")\n",
    "else:\n",
    "    print(f\"\\n*** RESULT: No statistically significant difference (p >= {significance_level}) ***\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Plot 1: Histogram of Deltas\n",
    "deltas = [r['delta'] for r in results]\n",
    "ax1 = axes[0]\n",
    "ax1.hist(deltas, bins=30, edgecolor='black', alpha=0.7, color='steelblue')\n",
    "ax1.axvline(x=0, color='red', linestyle='--', linewidth=2, label='Zero (no difference)')\n",
    "ax1.axvline(x=np.mean(deltas), color='green', linestyle='-', linewidth=2, label=f'Mean ({np.mean(deltas):.3f})')\n",
    "ax1.set_xlabel('Delta (Baseline NLL - Surrogate NLL)')\n",
    "ax1.set_ylabel('Frequency')\n",
    "ax1.set_title('Distribution of NLL Deltas\\n(Positive = Surrogate Better)')\n",
    "ax1.legend()\n",
    "\n",
    "# Plot 2: Baseline vs Surrogate NLL scatter\n",
    "ax2 = axes[1]\n",
    "baseline_nlls = [r['baseline_nll'] for r in results]\n",
    "surrogate_nlls = [r['surrogate_nll'] for r in results]\n",
    "ax2.scatter(baseline_nlls, surrogate_nlls, alpha=0.6, edgecolor='black', linewidth=0.5)\n",
    "min_val = min(min(baseline_nlls), min(surrogate_nlls))\n",
    "max_val = max(max(baseline_nlls), max(surrogate_nlls))\n",
    "ax2.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='y=x (no difference)')\n",
    "ax2.set_xlabel('Baseline NLL')\n",
    "ax2.set_ylabel('Surrogate NLL')\n",
    "ax2.set_title('Baseline vs Surrogate NLL\\n(Points below line = Surrogate better)')\n",
    "ax2.legend()\n",
    "\n",
    "# Plot 3: Box plot comparison\n",
    "ax3 = axes[2]\n",
    "box_data = [baseline_nlls, surrogate_nlls]\n",
    "bp = ax3.boxplot(box_data, labels=['Baseline', 'Surrogate'], patch_artist=True)\n",
    "bp['boxes'][0].set_facecolor('lightcoral')\n",
    "bp['boxes'][1].set_facecolor('lightgreen')\n",
    "ax3.set_ylabel('NLL')\n",
    "ax3.set_title('NLL Distribution by Condition')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('experiment_results.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nFigure saved to: experiment_results.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show examples: Best and worst cases\n",
    "sorted_results = sorted(results, key=lambda x: x['delta'], reverse=True)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"TOP 5 SURROGATE WINS (Largest Positive Delta)\")\n",
    "print(\"=\"*80)\n",
    "for r in sorted_results[:5]:\n",
    "    print(f\"\\nDelta: {r['delta']:.4f}\")\n",
    "    print(f\"  Ground Truth Query: {r['query']}\")\n",
    "    print(f\"  Generated Surrogate: {r['surrogate']}\")\n",
    "    print(f\"  Baseline NLL: {r['baseline_nll']:.4f} | Surrogate NLL: {r['surrogate_nll']:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TOP 5 BASELINE WINS (Largest Negative Delta)\")\n",
    "print(\"=\"*80)\n",
    "for r in sorted_results[-5:]:\n",
    "    print(f\"\\nDelta: {r['delta']:.4f}\")\n",
    "    print(f\"  Ground Truth Query: {r['query']}\")\n",
    "    print(f\"  Generated Surrogate: {r['surrogate']}\")\n",
    "    print(f\"  Baseline NLL: {r['baseline_nll']:.4f} | Surrogate NLL: {r['surrogate_nll']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save detailed results\n",
    "output_data = {\n",
    "    'config': {\n",
    "        'model_name': config.model_name,\n",
    "        'num_samples': config.num_samples,\n",
    "        'baseline_prompt': config.baseline_prompt,\n",
    "        'surrogate_prefix_template': config.surrogate_prefix_template,\n",
    "        'seed': config.seed,\n",
    "    },\n",
    "    'analysis': analysis,\n",
    "    'results': results\n",
    "}\n",
    "\n",
    "with open('experiment_results.json', 'w') as f:\n",
    "    json.dump(output_data, f, indent=2, default=str)\n",
    "\n",
    "print(\"Results saved to: experiment_results.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix: Ablation Studies\n",
    "\n",
    "Run additional experiments with different configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ablation_different_prompts(\n",
    "    samples: List[Dict],\n",
    "    model: AutoModelForCausalLM,\n",
    "    tokenizer: AutoTokenizer,\n",
    "    config: ExperimentConfig,\n",
    "    prompt_variants: Dict[str, str]\n",
    ") -> Dict[str, Dict]:\n",
    "    \"\"\"\n",
    "    Run ablation study with different surrogate prompt templates.\n",
    "    \n",
    "    Args:\n",
    "        samples: Dataset samples\n",
    "        model: Language model\n",
    "        tokenizer: Tokenizer\n",
    "        config: Base configuration\n",
    "        prompt_variants: Dict mapping variant name to surrogate prefix template\n",
    "    \n",
    "    Returns:\n",
    "        Dict mapping variant name to analysis results\n",
    "    \"\"\"\n",
    "    ablation_results = {}\n",
    "    \n",
    "    # Use a subset for ablation\n",
    "    ablation_samples = samples[:20]\n",
    "    \n",
    "    for variant_name, template in prompt_variants.items():\n",
    "        print(f\"\\nRunning ablation: {variant_name}\")\n",
    "        \n",
    "        # Temporarily modify config\n",
    "        original_template = config.surrogate_prefix_template\n",
    "        config.surrogate_prefix_template = template\n",
    "        \n",
    "        variant_results = run_experiment(ablation_samples, model, tokenizer, config)\n",
    "        variant_analysis = analyze_results(variant_results)\n",
    "        \n",
    "        ablation_results[variant_name] = {\n",
    "            'template': template,\n",
    "            'analysis': variant_analysis\n",
    "        }\n",
    "        \n",
    "        # Restore original\n",
    "        config.surrogate_prefix_template = original_template\n",
    "    \n",
    "    return ablation_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example ablation: different prompt phrasings\n",
    "# Uncomment to run\n",
    "\n",
    "# prompt_variants = {\n",
    "#     'query_focused': \"System: Answer the following query: {surrogate}\\n\\nDocument:\\n\",\n",
    "#     'question_focused': \"System: Answer this question: {surrogate}\\n\\nText:\\n\",\n",
    "#     'search_focused': \"System: User searched for: {surrogate}\\n\\nResult:\\n\",\n",
    "#     'minimal': \"{surrogate}\\n\\n\",\n",
    "# }\n",
    "\n",
    "# ablation_results = run_ablation_different_prompts(\n",
    "#     samples, model, tokenizer, config, prompt_variants\n",
    "# )\n",
    "\n",
    "# for name, data in ablation_results.items():\n",
    "#     print(f\"\\n{name}: Win Rate = {data['analysis']['win_rate']*100:.1f}%, Mean Delta = {data['analysis']['mean_delta']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Notes on Interpretation\n",
    "\n",
    "**If Win Rate > 50% and p < 0.05:**\n",
    "- Surrogate priming successfully \"steers\" the KV cache toward the query space\n",
    "- The document embeddings become more aligned with typical query patterns\n",
    "\n",
    "**If Win Rate < 50% or not significant:**\n",
    "- The generic prompt may already provide sufficient context\n",
    "- Surrogate generation quality may be a bottleneck\n",
    "- The effect may be domain-dependent\n",
    "\n",
    "**Potential confounds:**\n",
    "- Surrogate length differs from baseline prompt length (position effects)\n",
    "- Surrogate quality varies by passage complexity\n",
    "- Some queries may be too different from any reasonable surrogate"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
