{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7a8ebcf",
   "metadata": {},
   "source": [
    "# Experiment 06: Rigorous Semantic Priming Hypothesis Test\n",
    "\n",
    "## Core Question\n",
    "\n",
    "Does the **semantic content** of a surrogate query improve KV cache document representations\n",
    "after truncation, or does any prefix text work equally well?\n",
    "\n",
    "## Primary Baseline\n",
    "\n",
    "**Bare passage** — the raw passage text with no framing. Matches MS MARCO as closely as possible.\n",
    "\n",
    "## Experimental Conditions (16 per sample)\n",
    "\n",
    "| # | Condition | Group | Description |\n",
    "|---|-----------|-------|-------------|\n",
    "| 1 | Bare passage | A: Baseline | Raw passage, no framing |\n",
    "| 2 | Framed passage | A: Baseline | `\"Document:\\n{passage}\"` |\n",
    "| 3 | Generated (routed) | B: Truncated | Best of 5 by cosine similarity |\n",
    "| 4 | Generated (oracle) | B: Truncated | Best of 5 by actual answer NLL |\n",
    "| 5 | Perfect surrogate | B: Truncated | Actual MS MARCO query |\n",
    "| 6 | Irrelevant query | B: Truncated | Real query from different sample |\n",
    "| 7 | Shuffled surrogate | B: Truncated | Words randomly shuffled |\n",
    "| 8 | Random passage | B: Truncated | Different passage as prefix |\n",
    "| 9 | Random tokens | B: Truncated | Random vocab IDs decoded |\n",
    "| 10 | Full-ctx generated | C: Full Context | Surrogate kept visible |\n",
    "| 11 | Full-ctx irrelevant | C: Full Context | Wrong query kept visible |\n",
    "| 12 | Full-ctx random passage | C: Full Context | Random text kept visible |\n",
    "| 13 | Pool cosine | D: Routing | Best of 11 by cosine sim |\n",
    "| 14 | Pool query-NLL | D: Routing | Best of 11 by query NLL |\n",
    "| 15 | Pool combined | D: Routing | Rank-average of cosine + query NLL |\n",
    "| 16 | Pool oracle | D: Routing | Best of 11 by answer NLL |\n",
    "\n",
    "## Key Pairwise Comparisons\n",
    "\n",
    "| # | Comparison | Question |\n",
    "|---|-----------|----------|\n",
    "| 1 | Generated (3) vs Bare (1) | Does surrogate priming help after truncation? |\n",
    "| 2 | Generated (3) vs Irrelevant (6) | **DECISIVE**: Semantic or structural? |\n",
    "| 3 | Generated (3) vs Shuffled (7) | Does word order matter? |\n",
    "| 4 | Generated (3) vs Random passage (8) | Does any coherent text prime equally? |\n",
    "| 5 | Generated (3) vs Random tokens (9) | Does coherence matter at all? |\n",
    "| 6 | Framed (2) vs Bare (1) | Does framing help or hurt? |\n",
    "| 7 | Full-ctx gen (10) vs Full-ctx irrel (11) | Is full-context benefit semantic? |\n",
    "| 8 | Pool cosine (13) vs Pool query-NLL (14) | Which router is better? |\n",
    "| 9 | Pool oracle (16) vs Bare (1) | Ceiling for any routing strategy? |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31a0ee5",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09530ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import copy\n",
    "import json\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "from typing import Dict, List, Any, Optional, Tuple\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sys.path.insert(0, '.')\n",
    "\n",
    "from lib import (\n",
    "    ExperimentConfig,\n",
    "    build_kv_cache,\n",
    "    score_answer_with_cache,\n",
    "    build_truncated_kv_cache_corrected,\n",
    "    generate_all_5_surrogates,\n",
    "    compute_similarity,\n",
    "    load_evaluation_samples,\n",
    "    load_ms_marco,\n",
    "    TOP_5_SURROGATE_TEMPLATES,\n",
    "    STATIC_SURROGATE_QUERIES,\n",
    ")\n",
    "from lib.analysis import cohens_d\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46cd6b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "config = ExperimentConfig(\n",
    "    num_samples=800,\n",
    "    min_passage_words=50,\n",
    "    max_passage_words=300,\n",
    "    surrogate_max_tokens=45,\n",
    "    surrogate_temperature=0.3,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "print(f\"Model: {config.model_name}\")\n",
    "print(f\"Samples requested: {config.num_samples}\")\n",
    "print(f\"Passage words: {config.min_passage_words}-{config.max_passage_words}\")\n",
    "print(f\"Device: {config.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2fe375e",
   "metadata": {},
   "source": [
    "## Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44992659",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "torch.manual_seed(config.seed)\n",
    "np.random.seed(config.seed)\n",
    "random.seed(config.seed)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    ")\n",
    "\n",
    "print(f\"Loading {config.model_name}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "model.eval()\n",
    "print(f\"Model loaded on {config.device}\")\n",
    "\n",
    "print(f\"Loading embedding model: {config.embedding_model_name}\")\n",
    "embed_model = SentenceTransformer(config.embedding_model_name)\n",
    "print(\"Embedding model loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d5719e",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186db237",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_ms_marco(config)\n",
    "raw_samples = load_evaluation_samples(dataset, config, require_answer=True)\n",
    "print(f\"Raw samples after basic filtering: {len(raw_samples)}\")\n",
    "\n",
    "# Additional filters for this experiment\n",
    "filtered_samples = []\n",
    "excluded_ratio = 0\n",
    "excluded_short_answer = 0\n",
    "\n",
    "for s in raw_samples:\n",
    "    # Filter: answer/passage length ratio > 0.5\n",
    "    if len(s['answer']) / max(len(s['passage']), 1) > 0.5:\n",
    "        excluded_ratio += 1\n",
    "        continue\n",
    "    # Filter: answer tokenizes to < 2 tokens\n",
    "    answer_ids = tokenizer(s['answer'], return_tensors='pt', add_special_tokens=False)['input_ids']\n",
    "    if answer_ids.shape[1] < 2:\n",
    "        excluded_short_answer += 1\n",
    "        continue\n",
    "    filtered_samples.append(s)\n",
    "\n",
    "samples = filtered_samples\n",
    "print(f\"\\nFiltering stats:\")\n",
    "print(f\"  Excluded (answer/passage ratio > 0.5): {excluded_ratio}\")\n",
    "print(f\"  Excluded (answer < 2 tokens):          {excluded_short_answer}\")\n",
    "print(f\"  Remaining samples:                     {len(samples)}\")\n",
    "\n",
    "# Sanity check\n",
    "s = samples[0]\n",
    "print(f\"\\nExample sample:\")\n",
    "print(f\"  Query:   {s['query'][:100]}...\")\n",
    "print(f\"  Passage: {s['passage'][:100]}...\")\n",
    "print(f\"  Answer:  {s['answer'][:100]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d266b7c7",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd37e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_text(text: str, rng: random.Random) -> str:\n",
    "    \"\"\"Split text on whitespace, shuffle words, rejoin.\"\"\"\n",
    "    words = text.split()\n",
    "    rng.shuffle(words)\n",
    "    return ' '.join(words)\n",
    "\n",
    "\n",
    "def generate_random_tokens(tokenizer, n_tokens: int, rng: random.Random) -> str:\n",
    "    \"\"\"Sample random vocabulary IDs and decode as text.\"\"\"\n",
    "    vocab_size = tokenizer.vocab_size\n",
    "    random_ids = [rng.randint(100, vocab_size - 1) for _ in range(n_tokens)]\n",
    "    return tokenizer.decode(random_ids, skip_special_tokens=True)\n",
    "\n",
    "\n",
    "def get_irrelevant_query(samples: list, current_idx: int, rng: random.Random) -> str:\n",
    "    \"\"\"Get a query from a different sample.\"\"\"\n",
    "    other_idx = current_idx\n",
    "    while other_idx == current_idx:\n",
    "        other_idx = rng.randint(0, len(samples) - 1)\n",
    "    return samples[other_idx]['query']\n",
    "\n",
    "\n",
    "def get_random_passage(samples: list, current_idx: int, rng: random.Random) -> str:\n",
    "    \"\"\"Get a passage from a different sample.\"\"\"\n",
    "    other_idx = current_idx\n",
    "    while other_idx == current_idx:\n",
    "        other_idx = rng.randint(0, len(samples) - 1)\n",
    "    return samples[other_idx]['passage']\n",
    "\n",
    "\n",
    "def deep_copy_cache(cache):\n",
    "    \"\"\"Deep copy a DynamicCache so mutations don't affect the original.\"\"\"\n",
    "    return copy.deepcopy(cache)\n",
    "\n",
    "\n",
    "def score_query_nll(cache, cache_len: int, query_text: str, model, tokenizer, config) -> float:\n",
    "    \"\"\"Score query text NLL under a given cache.\n",
    "\n",
    "    The cache that best 'predicts' the query is likely best aligned for answering it.\n",
    "    Uses a deep copy of the cache to avoid mutation.\n",
    "    \"\"\"\n",
    "    cache_copy = deep_copy_cache(cache)\n",
    "\n",
    "    query_ids = tokenizer(\n",
    "        query_text, return_tensors=\"pt\", add_special_tokens=False\n",
    "    )['input_ids'].to(config.device)\n",
    "    query_len = query_ids.shape[1]\n",
    "\n",
    "    if query_len < 2:\n",
    "        return float('inf')\n",
    "\n",
    "    attention_mask = torch.ones((1, cache_len + query_len), device=config.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            input_ids=query_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            past_key_values=cache_copy,\n",
    "            use_cache=True,\n",
    "            return_dict=True\n",
    "        )\n",
    "\n",
    "    logits = outputs.logits\n",
    "    shift_logits = logits[:, :-1, :].contiguous().view(-1, logits.size(-1))\n",
    "    shift_labels = query_ids[:, 1:].contiguous().view(-1)\n",
    "\n",
    "    loss = torch.nn.CrossEntropyLoss(reduction='sum')\n",
    "    nll = loss(shift_logits, shift_labels).item()\n",
    "    return nll / (query_len - 1)\n",
    "\n",
    "\n",
    "def build_pool_caches(\n",
    "    passage: str,\n",
    "    generated_surrogates: Dict[str, str],\n",
    "    model, tokenizer, config\n",
    ") -> Dict[str, Tuple[int, Any]]:\n",
    "    \"\"\"Build all 11 pool caches: bare + 5 generated + 5 static.\n",
    "\n",
    "    Returns dict mapping cache_key -> (cache_len, cache_object).\n",
    "    Each cache is built fresh and NOT scored yet, so it can be reused.\n",
    "    \"\"\"\n",
    "    pool = {}\n",
    "\n",
    "    # Bare baseline cache\n",
    "    bare_len, bare_cache = build_kv_cache(passage, model, tokenizer, config)\n",
    "    pool['bare'] = (bare_len, bare_cache)\n",
    "\n",
    "    # 5 generated surrogate caches (truncated + corrected)\n",
    "    for key, surrogate in generated_surrogates.items():\n",
    "        doc_len, cache = build_truncated_kv_cache_corrected(\n",
    "            surrogate, passage, model, tokenizer, config\n",
    "        )\n",
    "        pool[f'gen_{key}'] = (doc_len, cache)\n",
    "\n",
    "    # 5 static surrogate caches (truncated + corrected)\n",
    "    for key, info in STATIC_SURROGATE_QUERIES.items():\n",
    "        doc_len, cache = build_truncated_kv_cache_corrected(\n",
    "            info['query'], passage, model, tokenizer, config\n",
    "        )\n",
    "        pool[f'static_{key}'] = (doc_len, cache)\n",
    "\n",
    "    return pool\n",
    "\n",
    "\n",
    "print(\"Helper functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42445821",
   "metadata": {},
   "source": [
    "## Pipeline Verification\n",
    "\n",
    "Test all 16 conditions on one sample to verify everything works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09aa2189",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample = samples[0]\n",
    "test_idx = 0\n",
    "test_rng = random.Random(config.seed)\n",
    "passage = test_sample['passage']\n",
    "query = test_sample['query']\n",
    "answer = test_sample['answer']\n",
    "query_prompt = config.query_template.format(query=query)\n",
    "\n",
    "print(f\"Passage: {passage[:100]}...\")\n",
    "print(f\"Query:   {query}\")\n",
    "print(f\"Answer:  {answer[:80]}\")\n",
    "print()\n",
    "\n",
    "# --- Group A: Baselines ---\n",
    "# 1. Bare passage\n",
    "bare_len, bare_cache = build_kv_cache(passage, model, tokenizer, config)\n",
    "bare_nll = score_answer_with_cache(bare_cache, bare_len, query_prompt, answer, model, tokenizer, config)\n",
    "print(f\"1. Bare passage NLL:      {bare_nll:.4f}\")\n",
    "\n",
    "# 2. Framed passage\n",
    "framed_ctx = config.baseline_cache_template.format(document=passage)\n",
    "framed_len, framed_cache = build_kv_cache(framed_ctx, model, tokenizer, config)\n",
    "framed_nll = score_answer_with_cache(framed_cache, framed_len, query_prompt, answer, model, tokenizer, config)\n",
    "print(f\"2. Framed passage NLL:    {framed_nll:.4f}\")\n",
    "\n",
    "# --- Group B: Truncated + RoPE Corrected ---\n",
    "# Generate surrogates\n",
    "test_surrogates = generate_all_5_surrogates(passage, model, tokenizer, config)\n",
    "test_sims = {k: compute_similarity(v, query, embed_model) for k, v in test_surrogates.items()}\n",
    "routed_key = max(test_sims, key=test_sims.get)\n",
    "print(f\"\\nGenerated surrogates:\")\n",
    "for k, v in test_surrogates.items():\n",
    "    marker = \" <-- routed\" if k == routed_key else \"\"\n",
    "    print(f\"  {k}: \\\"{v}\\\" (sim={test_sims[k]:.3f}){marker}\")\n",
    "\n",
    "# 3. Generated (routed)\n",
    "doc_len, cache = build_truncated_kv_cache_corrected(test_surrogates[routed_key], passage, model, tokenizer, config)\n",
    "gen_routed_nll = score_answer_with_cache(cache, doc_len, query_prompt, answer, model, tokenizer, config)\n",
    "print(f\"\\n3. Generated (routed) NLL: {gen_routed_nll:.4f}\")\n",
    "\n",
    "# 4. Generated (oracle) — need all 5 NLLs\n",
    "gen_nlls = {}\n",
    "for k, surr in test_surrogates.items():\n",
    "    dl, c = build_truncated_kv_cache_corrected(surr, passage, model, tokenizer, config)\n",
    "    gen_nlls[k] = score_answer_with_cache(c, dl, query_prompt, answer, model, tokenizer, config)\n",
    "oracle_key = min(gen_nlls, key=gen_nlls.get)\n",
    "print(f\"4. Generated (oracle) NLL: {gen_nlls[oracle_key]:.4f} (key={oracle_key})\")\n",
    "\n",
    "# 5. Perfect surrogate\n",
    "dl, c = build_truncated_kv_cache_corrected(query, passage, model, tokenizer, config)\n",
    "perfect_nll = score_answer_with_cache(c, dl, query_prompt, answer, model, tokenizer, config)\n",
    "print(f\"5. Perfect surrogate NLL:  {perfect_nll:.4f}\")\n",
    "\n",
    "# 6. Irrelevant query\n",
    "irrel_query = get_irrelevant_query(samples, test_idx, test_rng)\n",
    "dl, c = build_truncated_kv_cache_corrected(irrel_query, passage, model, tokenizer, config)\n",
    "irrel_nll = score_answer_with_cache(c, dl, query_prompt, answer, model, tokenizer, config)\n",
    "print(f\"6. Irrelevant query NLL:   {irrel_nll:.4f} (query: \\\"{irrel_query[:50]}\\\")\")\n",
    "\n",
    "# 7. Shuffled surrogate\n",
    "shuffled_text = shuffle_text(test_surrogates[routed_key], test_rng)\n",
    "dl, c = build_truncated_kv_cache_corrected(shuffled_text, passage, model, tokenizer, config)\n",
    "shuffled_nll = score_answer_with_cache(c, dl, query_prompt, answer, model, tokenizer, config)\n",
    "print(f\"7. Shuffled surrogate NLL: {shuffled_nll:.4f} (text: \\\"{shuffled_text[:50]}\\\")\")\n",
    "\n",
    "# 8. Random passage prefix\n",
    "rand_passage = get_random_passage(samples, test_idx, test_rng)\n",
    "dl, c = build_truncated_kv_cache_corrected(rand_passage[:200], passage, model, tokenizer, config)\n",
    "rand_passage_nll = score_answer_with_cache(c, dl, query_prompt, answer, model, tokenizer, config)\n",
    "print(f\"8. Random passage NLL:     {rand_passage_nll:.4f}\")\n",
    "\n",
    "# 9. Random tokens\n",
    "rand_tokens_text = generate_random_tokens(tokenizer, 20, test_rng)\n",
    "dl, c = build_truncated_kv_cache_corrected(rand_tokens_text, passage, model, tokenizer, config)\n",
    "rand_tokens_nll = score_answer_with_cache(c, dl, query_prompt, answer, model, tokenizer, config)\n",
    "print(f\"9. Random tokens NLL:      {rand_tokens_nll:.4f} (text: \\\"{rand_tokens_text[:50]}\\\")\")\n",
    "\n",
    "# --- Group C: Full Context ---\n",
    "# 10. Full-context generated (routed)\n",
    "full_gen_ctx = config.surrogate_cache_template.format(surrogate=test_surrogates[routed_key], document=passage)\n",
    "fl, fc = build_kv_cache(full_gen_ctx, model, tokenizer, config)\n",
    "full_gen_nll = score_answer_with_cache(fc, fl, query_prompt, answer, model, tokenizer, config)\n",
    "print(f\"\\n10. Full-ctx generated NLL: {full_gen_nll:.4f}\")\n",
    "\n",
    "# 11. Full-context irrelevant\n",
    "full_irrel_ctx = config.surrogate_cache_template.format(surrogate=irrel_query, document=passage)\n",
    "fl, fc = build_kv_cache(full_irrel_ctx, model, tokenizer, config)\n",
    "full_irrel_nll = score_answer_with_cache(fc, fl, query_prompt, answer, model, tokenizer, config)\n",
    "print(f\"11. Full-ctx irrelevant NLL: {full_irrel_nll:.4f}\")\n",
    "\n",
    "# 12. Full-context random passage\n",
    "full_rand_ctx = config.surrogate_cache_template.format(surrogate=rand_passage[:200], document=passage)\n",
    "fl, fc = build_kv_cache(full_rand_ctx, model, tokenizer, config)\n",
    "full_rand_passage_nll = score_answer_with_cache(fc, fl, query_prompt, answer, model, tokenizer, config)\n",
    "print(f\"12. Full-ctx random pass NLL: {full_rand_passage_nll:.4f}\")\n",
    "\n",
    "# --- Group D: Production Routing Pool ---\n",
    "print(\"\\nBuilding pool caches (11 total)...\")\n",
    "pool_caches = build_pool_caches(passage, test_surrogates, model, tokenizer, config)\n",
    "\n",
    "# Compute cosine similarities for pool\n",
    "pool_sims = {}\n",
    "pool_sims['bare'] = compute_similarity(passage, query, embed_model)\n",
    "for k, surr in test_surrogates.items():\n",
    "    pool_sims[f'gen_{k}'] = compute_similarity(surr, query, embed_model)\n",
    "for k, info in STATIC_SURROGATE_QUERIES.items():\n",
    "    pool_sims[f'static_{k}'] = compute_similarity(info['query'], query, embed_model)\n",
    "\n",
    "# Compute query NLL for pool (using deep copies)\n",
    "pool_query_nlls = {}\n",
    "query_text_for_nll = config.query_template.format(query=query)\n",
    "for cache_key, (clen, cache_obj) in pool_caches.items():\n",
    "    pool_query_nlls[cache_key] = score_query_nll(cache_obj, clen, query_text_for_nll, model, tokenizer, config)\n",
    "\n",
    "# Score answer NLL for all pool caches (for oracle + routed lookups)\n",
    "pool_answer_nlls = {}\n",
    "for cache_key, (clen, cache_obj) in pool_caches.items():\n",
    "    # Need deep copy since score_answer_with_cache mutates\n",
    "    cache_copy = deep_copy_cache(cache_obj)\n",
    "    pool_answer_nlls[cache_key] = score_answer_with_cache(\n",
    "        cache_copy, clen, query_prompt, answer, model, tokenizer, config\n",
    "    )\n",
    "\n",
    "# 13. Pool routed (cosine sim)\n",
    "pool_cosine_key = max(pool_sims, key=pool_sims.get)\n",
    "print(f\"\\n13. Pool cosine routed:   {pool_answer_nlls[pool_cosine_key]:.4f} (key={pool_cosine_key})\")\n",
    "\n",
    "# 14. Pool routed (query NLL) — lower NLL is better\n",
    "pool_qnll_key = min(pool_query_nlls, key=pool_query_nlls.get)\n",
    "print(f\"14. Pool query-NLL routed: {pool_answer_nlls[pool_qnll_key]:.4f} (key={pool_qnll_key})\")\n",
    "\n",
    "# 15. Pool routed (combined) — rank-average\n",
    "from scipy.stats import rankdata\n",
    "sim_values = np.array([pool_sims[k] for k in pool_caches.keys()])\n",
    "qnll_values = np.array([pool_query_nlls[k] for k in pool_caches.keys()])\n",
    "sim_ranks = rankdata(-sim_values)  # higher sim = better = lower rank\n",
    "qnll_ranks = rankdata(qnll_values)  # lower NLL = better = lower rank\n",
    "combined_ranks = (sim_ranks + qnll_ranks) / 2\n",
    "pool_keys_list = list(pool_caches.keys())\n",
    "pool_combined_key = pool_keys_list[np.argmin(combined_ranks)]\n",
    "print(f\"15. Pool combined routed:  {pool_answer_nlls[pool_combined_key]:.4f} (key={pool_combined_key})\")\n",
    "\n",
    "# 16. Pool oracle (answer NLL)\n",
    "pool_oracle_key = min(pool_answer_nlls, key=pool_answer_nlls.get)\n",
    "print(f\"16. Pool oracle:           {pool_answer_nlls[pool_oracle_key]:.4f} (key={pool_oracle_key})\")\n",
    "\n",
    "print(\"\\n--- Verification Summary ---\")\n",
    "all_nlls = [bare_nll, framed_nll, gen_routed_nll, gen_nlls[oracle_key], perfect_nll,\n",
    "            irrel_nll, shuffled_nll, rand_passage_nll, rand_tokens_nll,\n",
    "            full_gen_nll, full_irrel_nll, full_rand_passage_nll,\n",
    "            pool_answer_nlls[pool_cosine_key], pool_answer_nlls[pool_qnll_key],\n",
    "            pool_answer_nlls[pool_combined_key], pool_answer_nlls[pool_oracle_key]]\n",
    "print(f\"All 16 NLLs finite: {all(np.isfinite(x) for x in all_nlls)}\")\n",
    "print(f\"NLL range: [{min(all_nlls):.4f}, {max(all_nlls):.4f}]\")\n",
    "print(f\"Query NLL range: [{min(pool_query_nlls.values()):.4f}, {max(pool_query_nlls.values()):.4f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73768825",
   "metadata": {},
   "source": [
    "## Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1cdab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_sample(\n",
    "    sample: Dict,\n",
    "    idx: int,\n",
    "    all_samples: List[Dict],\n",
    "    model,\n",
    "    tokenizer,\n",
    "    embed_model,\n",
    "    config: ExperimentConfig,\n",
    ") -> Optional[Dict]:\n",
    "    \"\"\"Evaluate a single sample across all 16 experimental conditions.\"\"\"\n",
    "    passage = sample['passage']\n",
    "    query = sample['query']\n",
    "    answer = sample['answer']\n",
    "    query_prompt = config.query_template.format(query=query)\n",
    "    rng = random.Random(config.seed + idx)\n",
    "\n",
    "    # ==================== GROUP A: BASELINES ====================\n",
    "\n",
    "    # 1. Bare passage (PRIMARY BASELINE)\n",
    "    bare_len, bare_cache = build_kv_cache(passage, model, tokenizer, config)\n",
    "    bare_nll = score_answer_with_cache(\n",
    "        bare_cache, bare_len, query_prompt, answer, model, tokenizer, config\n",
    "    )\n",
    "\n",
    "    # 2. Framed passage\n",
    "    framed_ctx = config.baseline_cache_template.format(document=passage)\n",
    "    framed_len, framed_cache = build_kv_cache(framed_ctx, model, tokenizer, config)\n",
    "    framed_nll = score_answer_with_cache(\n",
    "        framed_cache, framed_len, query_prompt, answer, model, tokenizer, config\n",
    "    )\n",
    "\n",
    "    # ==================== GENERATE SURROGATES ====================\n",
    "\n",
    "    generated_surrogates = generate_all_5_surrogates(passage, model, tokenizer, config)\n",
    "    gen_similarities = {\n",
    "        k: compute_similarity(v, query, embed_model)\n",
    "        for k, v in generated_surrogates.items()\n",
    "    }\n",
    "    gen_routed_key = max(gen_similarities, key=gen_similarities.get)\n",
    "\n",
    "    # ==================== GROUP B: TRUNCATED + ROPE CORRECTED ====================\n",
    "\n",
    "    # Score all 5 generated surrogates (truncated)\n",
    "    gen_nlls = {}\n",
    "    for key, surrogate in generated_surrogates.items():\n",
    "        dl, c = build_truncated_kv_cache_corrected(\n",
    "            surrogate, passage, model, tokenizer, config\n",
    "        )\n",
    "        gen_nlls[key] = score_answer_with_cache(\n",
    "            c, dl, query_prompt, answer, model, tokenizer, config\n",
    "        )\n",
    "\n",
    "    # 3. Generated (routed by cosine sim)\n",
    "    gen_routed_nll = gen_nlls[gen_routed_key]\n",
    "\n",
    "    # 4. Generated (oracle by NLL)\n",
    "    gen_oracle_key = min(gen_nlls, key=gen_nlls.get)\n",
    "    gen_oracle_nll = gen_nlls[gen_oracle_key]\n",
    "\n",
    "    # 5. Perfect surrogate (actual query)\n",
    "    dl, c = build_truncated_kv_cache_corrected(query, passage, model, tokenizer, config)\n",
    "    perfect_nll = score_answer_with_cache(c, dl, query_prompt, answer, model, tokenizer, config)\n",
    "\n",
    "    # 6. Irrelevant query\n",
    "    irrel_query = get_irrelevant_query(all_samples, idx, rng)\n",
    "    dl, c = build_truncated_kv_cache_corrected(irrel_query, passage, model, tokenizer, config)\n",
    "    irrel_nll = score_answer_with_cache(c, dl, query_prompt, answer, model, tokenizer, config)\n",
    "\n",
    "    # 7. Shuffled surrogate (shuffle words of routed surrogate)\n",
    "    shuffled_text = shuffle_text(generated_surrogates[gen_routed_key], rng)\n",
    "    dl, c = build_truncated_kv_cache_corrected(shuffled_text, passage, model, tokenizer, config)\n",
    "    shuffled_nll = score_answer_with_cache(c, dl, query_prompt, answer, model, tokenizer, config)\n",
    "\n",
    "    # 8. Random passage prefix\n",
    "    rand_passage_text = get_random_passage(all_samples, idx, rng)[:200]\n",
    "    dl, c = build_truncated_kv_cache_corrected(rand_passage_text, passage, model, tokenizer, config)\n",
    "    rand_passage_nll = score_answer_with_cache(c, dl, query_prompt, answer, model, tokenizer, config)\n",
    "\n",
    "    # 9. Random tokens\n",
    "    rand_tokens_text = generate_random_tokens(tokenizer, 20, rng)\n",
    "    dl, c = build_truncated_kv_cache_corrected(rand_tokens_text, passage, model, tokenizer, config)\n",
    "    rand_tokens_nll = score_answer_with_cache(c, dl, query_prompt, answer, model, tokenizer, config)\n",
    "\n",
    "    # ==================== GROUP C: FULL CONTEXT ====================\n",
    "\n",
    "    # 10. Full-context generated (routed)\n",
    "    full_gen_ctx = config.surrogate_cache_template.format(\n",
    "        surrogate=generated_surrogates[gen_routed_key], document=passage\n",
    "    )\n",
    "    fl, fc = build_kv_cache(full_gen_ctx, model, tokenizer, config)\n",
    "    full_gen_nll = score_answer_with_cache(fc, fl, query_prompt, answer, model, tokenizer, config)\n",
    "\n",
    "    # 11. Full-context irrelevant query\n",
    "    full_irrel_ctx = config.surrogate_cache_template.format(\n",
    "        surrogate=irrel_query, document=passage\n",
    "    )\n",
    "    fl, fc = build_kv_cache(full_irrel_ctx, model, tokenizer, config)\n",
    "    full_irrel_nll = score_answer_with_cache(fc, fl, query_prompt, answer, model, tokenizer, config)\n",
    "\n",
    "    # 12. Full-context random passage\n",
    "    full_rand_ctx = config.surrogate_cache_template.format(\n",
    "        surrogate=rand_passage_text, document=passage\n",
    "    )\n",
    "    fl, fc = build_kv_cache(full_rand_ctx, model, tokenizer, config)\n",
    "    full_rand_passage_nll = score_answer_with_cache(\n",
    "        fc, fl, query_prompt, answer, model, tokenizer, config\n",
    "    )\n",
    "\n",
    "    # ==================== GROUP D: PRODUCTION ROUTING POOL ====================\n",
    "    # Build all 11 pool caches: bare + 5 gen + 5 static\n",
    "    pool_caches = build_pool_caches(passage, generated_surrogates, model, tokenizer, config)\n",
    "\n",
    "    # Cosine similarities for pool routing\n",
    "    pool_sims = {}\n",
    "    pool_sims['bare'] = compute_similarity(passage, query, embed_model)\n",
    "    for k, surr in generated_surrogates.items():\n",
    "        pool_sims[f'gen_{k}'] = gen_similarities[k]\n",
    "    for k, info in STATIC_SURROGATE_QUERIES.items():\n",
    "        pool_sims[f'static_{k}'] = compute_similarity(info['query'], query, embed_model)\n",
    "\n",
    "    # Query NLL for pool routing (deep copies to preserve caches)\n",
    "    query_text_for_nll = config.query_template.format(query=query)\n",
    "    pool_query_nlls = {}\n",
    "    for cache_key, (clen, cache_obj) in pool_caches.items():\n",
    "        pool_query_nlls[cache_key] = score_query_nll(\n",
    "            cache_obj, clen, query_text_for_nll, model, tokenizer, config\n",
    "        )\n",
    "\n",
    "    # Answer NLL for all pool caches (needed for oracle + lookup)\n",
    "    pool_answer_nlls = {}\n",
    "    for cache_key, (clen, cache_obj) in pool_caches.items():\n",
    "        cache_copy = deep_copy_cache(cache_obj)\n",
    "        pool_answer_nlls[cache_key] = score_answer_with_cache(\n",
    "            cache_copy, clen, query_prompt, answer, model, tokenizer, config\n",
    "        )\n",
    "\n",
    "    # 13. Pool routed (cosine sim)\n",
    "    pool_cosine_key = max(pool_sims, key=pool_sims.get)\n",
    "    pool_cosine_nll = pool_answer_nlls[pool_cosine_key]\n",
    "\n",
    "    # 14. Pool routed (query NLL) — lower is better\n",
    "    pool_qnll_key = min(pool_query_nlls, key=pool_query_nlls.get)\n",
    "    pool_qnll_nll = pool_answer_nlls[pool_qnll_key]\n",
    "\n",
    "    # 15. Pool routed (combined rank-average)\n",
    "    pool_keys_list = list(pool_caches.keys())\n",
    "    sim_values = np.array([pool_sims[k] for k in pool_keys_list])\n",
    "    qnll_values = np.array([pool_query_nlls[k] for k in pool_keys_list])\n",
    "    from scipy.stats import rankdata\n",
    "    sim_ranks = rankdata(-sim_values)\n",
    "    qnll_ranks = rankdata(qnll_values)\n",
    "    combined_ranks = (sim_ranks + qnll_ranks) / 2\n",
    "    pool_combined_key = pool_keys_list[int(np.argmin(combined_ranks))]\n",
    "    pool_combined_nll = pool_answer_nlls[pool_combined_key]\n",
    "\n",
    "    # 16. Pool oracle (answer NLL)\n",
    "    pool_oracle_key = min(pool_answer_nlls, key=pool_answer_nlls.get)\n",
    "    pool_oracle_nll = pool_answer_nlls[pool_oracle_key]\n",
    "\n",
    "    return {\n",
    "        'idx': idx,\n",
    "        'query': query,\n",
    "        'answer_len': len(answer),\n",
    "        'passage_len': len(passage),\n",
    "\n",
    "        # Group A: Baselines\n",
    "        'bare_nll': bare_nll,\n",
    "        'framed_nll': framed_nll,\n",
    "\n",
    "        # Group B: Truncated + RoPE corrected\n",
    "        'gen_routed_key': gen_routed_key,\n",
    "        'gen_routed_nll': gen_routed_nll,\n",
    "        'gen_routed_sim': gen_similarities[gen_routed_key],\n",
    "        'gen_oracle_key': gen_oracle_key,\n",
    "        'gen_oracle_nll': gen_oracle_nll,\n",
    "        'gen_nlls': gen_nlls,\n",
    "        'gen_similarities': gen_similarities,\n",
    "        'generated_surrogates': generated_surrogates,\n",
    "        'perfect_nll': perfect_nll,\n",
    "        'irrel_nll': irrel_nll,\n",
    "        'irrel_query': irrel_query,\n",
    "        'shuffled_nll': shuffled_nll,\n",
    "        'rand_passage_nll': rand_passage_nll,\n",
    "        'rand_tokens_nll': rand_tokens_nll,\n",
    "\n",
    "        # Group C: Full context\n",
    "        'full_gen_nll': full_gen_nll,\n",
    "        'full_irrel_nll': full_irrel_nll,\n",
    "        'full_rand_passage_nll': full_rand_passage_nll,\n",
    "\n",
    "        # Group D: Pool routing\n",
    "        'pool_sims': pool_sims,\n",
    "        'pool_query_nlls': pool_query_nlls,\n",
    "        'pool_answer_nlls': pool_answer_nlls,\n",
    "        'pool_cosine_key': pool_cosine_key,\n",
    "        'pool_cosine_nll': pool_cosine_nll,\n",
    "        'pool_qnll_key': pool_qnll_key,\n",
    "        'pool_qnll_nll': pool_qnll_nll,\n",
    "        'pool_combined_key': pool_combined_key,\n",
    "        'pool_combined_nll': pool_combined_nll,\n",
    "        'pool_oracle_key': pool_oracle_key,\n",
    "        'pool_oracle_nll': pool_oracle_nll,\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"evaluate_sample() defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65c5c7d",
   "metadata": {},
   "source": "## Resume from Checkpoint\n\nCheck for a previous partial run and optionally resume it."
  },
  {
   "cell_type": "code",
   "id": "j6zyn0bu7u",
   "source": "checkpoint_dir = 'checkpoints'\nos.makedirs(checkpoint_dir, exist_ok=True)\ncheckpoint_path = os.path.join(checkpoint_dir, '06_checkpoint.json')\n\nresumed_results = []\nresume_from_idx = 0\n\nif os.path.exists(checkpoint_path):\n    with open(checkpoint_path, 'r') as f:\n        ckpt = json.load(f)\n    n_done = ckpt.get('n_done', 0)\n    n_errors = ckpt.get('n_errors', 0)\n    last_idx = ckpt.get('last_idx', -1)\n    elapsed_prev = ckpt.get('elapsed', 0)\n    print(f\"Found checkpoint: {n_done} samples done, {n_errors} errors, {elapsed_prev/60:.1f}m elapsed.\")\n    print(f\"Last processed sample index: {last_idx}\")\n    print(f\"Checkpoint saved at: {datetime.datetime.fromtimestamp(os.path.getmtime(checkpoint_path))}\")\n\n    resume = input(\"Resume from checkpoint? [y/N]: \").strip().lower()\n    if resume == 'y':\n        resumed_results = ckpt['results']\n        resume_from_idx = last_idx + 1\n        print(f\"Resuming from sample index {resume_from_idx} with {len(resumed_results)} results loaded.\")\n    else:\n        print(\"Starting fresh. Checkpoint will be overwritten.\")\nelse:\n    print(\"No checkpoint found. Starting fresh.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170d8cbb",
   "metadata": {},
   "outputs": [],
   "source": "results = list(resumed_results)\nerrors = 0\nstart_time = time.time()\n\nprint(\"=\" * 80)\nprint(\"RUNNING EXPERIMENT 06: SEMANTIC PRIMING HYPOTHESIS TEST\")\nprint(f\"Samples: {len(samples)}, Conditions per sample: 16\")\nif resume_from_idx > 0:\n    print(f\"RESUMING from index {resume_from_idx} with {len(results)} results already collected.\")\nprint(\"=\" * 80)\n\nfor idx, sample in enumerate(tqdm(samples, desc=\"Evaluating\", initial=resume_from_idx)):\n    if idx < resume_from_idx:\n        continue\n\n    try:\n        result = evaluate_sample(\n            sample, idx, samples, model, tokenizer, embed_model, config\n        )\n        if result is not None:\n            results.append(result)\n    except Exception as e:\n        errors += 1\n        if errors <= 5:\n            print(f\"\\n  Error on sample {idx}: {type(e).__name__}: {e}\")\n        continue\n\n    # Checkpoint every 10 samples\n    if len(results) > 0 and len(results) % 10 == 0:\n        with open(checkpoint_path, 'w') as f:\n            json.dump({\n                'n_done': len(results),\n                'n_errors': errors,\n                'last_idx': idx,\n                'elapsed': time.time() - start_time,\n                'results': results,\n            }, f, default=str)\n\n    # Progress report every 50 samples\n    if len(results) > 0 and len(results) % 50 == 0:\n        elapsed = time.time() - start_time\n        rate = elapsed / (idx - resume_from_idx + 1)\n        remaining = rate * (len(samples) - idx - 1)\n        recent = results[-50:]\n        bare_mean = np.mean([r['bare_nll'] for r in recent])\n        gen_mean = np.mean([r['gen_routed_nll'] for r in recent])\n        irrel_mean = np.mean([r['irrel_nll'] for r in recent])\n        wr_gen = np.mean([r['bare_nll'] - r['gen_routed_nll'] > 0 for r in recent]) * 100\n        wr_irrel = np.mean([r['bare_nll'] - r['irrel_nll'] > 0 for r in recent]) * 100\n\n        print(\n            f\"\\n  [{len(results):>4d} done | {elapsed/60:.0f}m elapsed | ~{remaining/60:.0f}m left]\"\n            f\"\\n  Last 50: bare={bare_mean:.3f}  gen_routed={gen_mean:.3f} ({wr_gen:.0f}% win)\"\n            f\"  irrel={irrel_mean:.3f} ({wr_irrel:.0f}% win)\"\n        )\n\n# Final checkpoint\nwith open(checkpoint_path, 'w') as f:\n    json.dump({\n        'n_done': len(results),\n        'n_errors': errors,\n        'last_idx': len(samples) - 1,\n        'elapsed': time.time() - start_time,\n        'results': results,\n    }, f, default=str)\n\nelapsed_total = time.time() - start_time\nprint(f\"\\nDone. {len(results)} evaluated, {errors} errors.\")\nprint(f\"Total time: {elapsed_total/60:.1f} minutes ({elapsed_total/len(results) if results else 0:.1f}s per sample)\")"
  },
  {
   "cell_type": "markdown",
   "id": "3ec4ea3e",
   "metadata": {},
   "source": [
    "## Primary Results\n",
    "\n",
    "Summary table: all 16 conditions vs bare baseline (primary baseline)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d35cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "bare_arr = np.array([r['bare_nll'] for r in results])\n",
    "n = len(results)\n",
    "\n",
    "conditions = [\n",
    "    ('1. Bare passage (BASELINE)',   'bare_nll'),\n",
    "    ('2. Framed passage',            'framed_nll'),\n",
    "    ('3. Gen routed (cosine)',       'gen_routed_nll'),\n",
    "    ('4. Gen oracle (NLL)',          'gen_oracle_nll'),\n",
    "    ('5. Perfect surrogate',         'perfect_nll'),\n",
    "    ('6. Irrelevant query',          'irrel_nll'),\n",
    "    ('7. Shuffled surrogate',        'shuffled_nll'),\n",
    "    ('8. Random passage',            'rand_passage_nll'),\n",
    "    ('9. Random tokens',             'rand_tokens_nll'),\n",
    "    ('10. Full-ctx generated',       'full_gen_nll'),\n",
    "    ('11. Full-ctx irrelevant',      'full_irrel_nll'),\n",
    "    ('12. Full-ctx random passage',  'full_rand_passage_nll'),\n",
    "    ('13. Pool cosine',              'pool_cosine_nll'),\n",
    "    ('14. Pool query-NLL',           'pool_qnll_nll'),\n",
    "    ('15. Pool combined',            'pool_combined_nll'),\n",
    "    ('16. Pool oracle',              'pool_oracle_nll'),\n",
    "]\n",
    "\n",
    "print(\"=\" * 120)\n",
    "print(\"ALL 16 CONDITIONS vs BARE BASELINE (sorted by mean NLL)\")\n",
    "print(f\"N = {n} samples. Positive delta = better than bare baseline.\")\n",
    "print(\"=\" * 120)\n",
    "header = f\"{'Condition':<32} {'Mean NLL':>10} {'Std':>8} {'Delta':>10} {'Win%':>8} {'t-stat':>8} {'p-value':>12} {'Cohen d':>10}\"\n",
    "print(header)\n",
    "print(\"-\" * 120)\n",
    "\n",
    "rows = []\n",
    "for label, key in conditions:\n",
    "    arr = np.array([r[key] for r in results])\n",
    "    delta = bare_arr - arr\n",
    "    mean_nll = np.mean(arr)\n",
    "    std_nll = np.std(arr)\n",
    "    mean_delta = np.mean(delta)\n",
    "    win_rate = np.mean(delta > 0) * 100\n",
    "    if key == 'bare_nll':\n",
    "        rows.append((mean_nll, label, std_nll, '--', '--', '--', '--', '--'))\n",
    "    else:\n",
    "        t, p = stats.ttest_rel(bare_arr, arr)\n",
    "        d = cohens_d(delta)\n",
    "        rows.append((mean_nll, label, std_nll, f\"{mean_delta:+.4f}\", f\"{win_rate:.1f}%\",\n",
    "                      f\"{t:.3f}\", f\"{p:.6f}\", f\"{d:.4f}\"))\n",
    "\n",
    "rows.sort(key=lambda x: x[0])\n",
    "for mean_nll, label, std_nll, *rest in rows:\n",
    "    vals = [f\"{mean_nll:>10.4f}\", f\"{std_nll:>8.4f}\"] + [f\"{v:>12}\" if i >= 4 else f\"{v:>10}\" for i, v in enumerate(rest)]\n",
    "    print(f\"{label:<32} {' '.join(vals)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23df53ed",
   "metadata": {},
   "source": [
    "## Semantic Isolation Tests\n",
    "\n",
    "The decisive comparisons: does the semantic content of the surrogate matter?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f6d2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key pairwise comparisons with Bonferroni correction (5 key tests, alpha=0.01)\n",
    "alpha = 0.01\n",
    "n_key_tests = 5\n",
    "bonferroni_alpha = alpha / n_key_tests\n",
    "\n",
    "print(\"=\" * 100)\n",
    "print(\"SEMANTIC ISOLATION: KEY PAIRWISE COMPARISONS\")\n",
    "print(f\"Bonferroni-corrected α = {alpha}/{n_key_tests} = {bonferroni_alpha:.4f}\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "pairwise = [\n",
    "    (\"Gen routed (3) vs Bare (1)\",          'gen_routed_nll',   'bare_nll',\n",
    "     \"Does surrogate priming help after truncation?\"),\n",
    "    (\"Gen routed (3) vs Irrelevant (6)\",     'gen_routed_nll',   'irrel_nll',\n",
    "     \"DECISIVE: Semantic or structural?\"),\n",
    "    (\"Gen routed (3) vs Shuffled (7)\",       'gen_routed_nll',   'shuffled_nll',\n",
    "     \"Does word order matter?\"),\n",
    "    (\"Gen routed (3) vs Random passage (8)\", 'gen_routed_nll',   'rand_passage_nll',\n",
    "     \"Does any coherent text prime equally?\"),\n",
    "    (\"Gen routed (3) vs Random tokens (9)\",  'gen_routed_nll',   'rand_tokens_nll',\n",
    "     \"Does coherence matter at all?\"),\n",
    "]\n",
    "\n",
    "print(f\"\\n{'Comparison':<42} {'Mean A':>8} {'Mean B':>8} {'Delta':>8} {'t':>8} {'p':>12} {'d':>8} {'Sig?':>6}\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "for label, key_a, key_b, question in pairwise:\n",
    "    a = np.array([r[key_a] for r in results])\n",
    "    b = np.array([r[key_b] for r in results])\n",
    "    diff = b - a  # positive = A is better (lower NLL)\n",
    "    t, p = stats.ttest_rel(a, b)\n",
    "    d = cohens_d(diff)\n",
    "    sig = \"YES\" if p < bonferroni_alpha else \"no\"\n",
    "    print(f\"{label:<42} {np.mean(a):>8.4f} {np.mean(b):>8.4f} {np.mean(diff):>+8.4f} {t:>8.3f} {p:>12.6f} {d:>8.4f} {sig:>6}\")\n",
    "    print(f\"  Q: {question}\")\n",
    "\n",
    "# Additional comparison: framed vs bare\n",
    "print(f\"\\n{'='*100}\")\n",
    "print(\"ADDITIONAL: Framed (2) vs Bare (1)\")\n",
    "framed_arr = np.array([r['framed_nll'] for r in results])\n",
    "diff = bare_arr - framed_arr\n",
    "t, p = stats.ttest_rel(bare_arr, framed_arr)\n",
    "d = cohens_d(diff)\n",
    "print(f\"  Bare mean: {np.mean(bare_arr):.4f}, Framed mean: {np.mean(framed_arr):.4f}\")\n",
    "print(f\"  Delta (bare - framed): {np.mean(diff):+.4f}, t={t:.3f}, p={p:.6f}, d={d:.4f}\")\n",
    "if p < 0.05:\n",
    "    if np.mean(bare_arr) < np.mean(framed_arr):\n",
    "        print(\"  -> Bare is significantly better. 'Document:\\\\n' framing hurts.\")\n",
    "    else:\n",
    "        print(\"  -> Framed is significantly better. Framing helps.\")\n",
    "else:\n",
    "    print(\"  -> No significant difference between bare and framed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c05589",
   "metadata": {},
   "source": [
    "## Full-Context Controls\n",
    "\n",
    "Is the full-context benefit semantic or positional?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07940d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 100)\n",
    "print(\"FULL-CONTEXT: SEMANTIC vs POSITIONAL\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "full_comparisons = [\n",
    "    (\"Full-ctx gen (10) vs Full-ctx irrel (11)\",\n",
    "     'full_gen_nll', 'full_irrel_nll',\n",
    "     \"Is full-context benefit semantic?\"),\n",
    "    (\"Full-ctx gen (10) vs Full-ctx random pass (12)\",\n",
    "     'full_gen_nll', 'full_rand_passage_nll',\n",
    "     \"Does coherent-but-wrong text match generated?\"),\n",
    "    (\"Full-ctx irrel (11) vs Bare (1)\",\n",
    "     'full_irrel_nll', 'bare_nll',\n",
    "     \"Does any full-context prefix help?\"),\n",
    "    (\"Full-ctx random pass (12) vs Bare (1)\",\n",
    "     'full_rand_passage_nll', 'bare_nll',\n",
    "     \"Does random passage prefix help in full context?\"),\n",
    "]\n",
    "\n",
    "print(f\"\\n{'Comparison':<50} {'Mean A':>8} {'Mean B':>8} {'Delta':>8} {'t':>8} {'p':>12} {'Sig?':>6}\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "for label, key_a, key_b, question in full_comparisons:\n",
    "    a = np.array([r[key_a] for r in results])\n",
    "    b = np.array([r[key_b] for r in results])\n",
    "    diff = b - a\n",
    "    t, p = stats.ttest_rel(a, b)\n",
    "    d = cohens_d(diff)\n",
    "    sig = \"YES\" if p < 0.05 else \"no\"\n",
    "    print(f\"{label:<50} {np.mean(a):>8.4f} {np.mean(b):>8.4f} {np.mean(diff):>+8.4f} {t:>8.3f} {p:>12.6f} {sig:>6}\")\n",
    "    print(f\"  Q: {question}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f7bba9",
   "metadata": {},
   "source": [
    "## Production Routing Comparison\n",
    "\n",
    "Comparing routers: cosine similarity vs query-NLL vs combined vs oracle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9509b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 100)\n",
    "print(\"PRODUCTION ROUTING: ROUTER HEAD-TO-HEAD\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "routers = [\n",
    "    (\"Pool cosine (13)\",   'pool_cosine_nll',   'pool_cosine_key'),\n",
    "    (\"Pool query-NLL (14)\", 'pool_qnll_nll',    'pool_qnll_key'),\n",
    "    (\"Pool combined (15)\",  'pool_combined_nll', 'pool_combined_key'),\n",
    "    (\"Pool oracle (16)\",    'pool_oracle_nll',   'pool_oracle_key'),\n",
    "]\n",
    "\n",
    "# Summary stats\n",
    "print(f\"\\n{'Router':<25} {'Mean NLL':>10} {'Delta vs Bare':>14} {'Win% vs Bare':>14} {'Chose Bare%':>14}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for label, nll_key, key_key in routers:\n",
    "    nlls = np.array([r[nll_key] for r in results])\n",
    "    delta = bare_arr - nlls\n",
    "    win_rate = np.mean(delta > 0) * 100\n",
    "    chose_bare = np.mean([r[key_key] == 'bare' for r in results]) * 100\n",
    "    print(f\"{label:<25} {np.mean(nlls):>10.4f} {np.mean(delta):>+14.4f} {win_rate:>13.1f}% {chose_bare:>13.1f}%\")\n",
    "\n",
    "# Routing efficiency\n",
    "oracle_delta = np.mean(bare_arr - np.array([r['pool_oracle_nll'] for r in results]))\n",
    "print(f\"\\n--- Routing Efficiency (% of oracle's improvement) ---\")\n",
    "for label, nll_key, _ in routers[:3]:\n",
    "    delta = np.mean(bare_arr - np.array([r[nll_key] for r in results]))\n",
    "    efficiency = (delta / oracle_delta * 100) if oracle_delta != 0 else 0\n",
    "    print(f\"  {label}: {efficiency:.1f}%\")\n",
    "\n",
    "# Agreement with oracle\n",
    "print(f\"\\n--- Agreement with Oracle ---\")\n",
    "for label, _, key_key in routers[:3]:\n",
    "    agreement = np.mean([r[key_key] == r['pool_oracle_key'] for r in results]) * 100\n",
    "    print(f\"  {label}: {agreement:.1f}%\")\n",
    "\n",
    "# Head-to-head: cosine vs query-NLL\n",
    "cosine_nlls = np.array([r['pool_cosine_nll'] for r in results])\n",
    "qnll_nlls = np.array([r['pool_qnll_nll'] for r in results])\n",
    "t, p = stats.ttest_rel(cosine_nlls, qnll_nlls)\n",
    "cosine_wins = np.mean(cosine_nlls < qnll_nlls) * 100\n",
    "print(f\"\\n--- Cosine vs Query-NLL ---\")\n",
    "print(f\"  Cosine mean: {np.mean(cosine_nlls):.4f}, Query-NLL mean: {np.mean(qnll_nlls):.4f}\")\n",
    "print(f\"  Cosine wins {cosine_wins:.1f}%, t={t:.3f}, p={p:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf39d91",
   "metadata": {},
   "source": [
    "## Stratified Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e78f396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratify by difficulty (bare baseline NLL quartiles)\n",
    "quartiles = np.percentile(bare_arr, [25, 50, 75])\n",
    "difficulty_bins = [\n",
    "    ('Q1 (easiest)', lambda x: x <= quartiles[0]),\n",
    "    ('Q2',           lambda x: quartiles[0] < x <= quartiles[1]),\n",
    "    ('Q3',           lambda x: quartiles[1] < x <= quartiles[2]),\n",
    "    ('Q4 (hardest)', lambda x: x > quartiles[2]),\n",
    "]\n",
    "\n",
    "print(\"=\" * 120)\n",
    "print(\"STRATIFIED BY DIFFICULTY (bare baseline NLL quartiles)\")\n",
    "print(\"=\" * 120)\n",
    "print(f\"{'Quartile':<16} {'N':>5} {'Bare':>8} {'Framed':>8} {'GenRtd':>8} {'GenRtd%':>8} {'Irrel':>8} {'Irrel%':>8} {'Shuf':>8} {'RandP':>8} {'RandT':>8} {'Perf':>8}\")\n",
    "print(\"-\" * 120)\n",
    "\n",
    "for label, cond in difficulty_bins:\n",
    "    subset = [r for r in results if cond(r['bare_nll'])]\n",
    "    if not subset:\n",
    "        continue\n",
    "    n_sub = len(subset)\n",
    "    bare = np.mean([r['bare_nll'] for r in subset])\n",
    "    framed = np.mean([r['framed_nll'] for r in subset])\n",
    "    gen = np.mean([r['gen_routed_nll'] for r in subset])\n",
    "    gen_wr = np.mean([r['bare_nll'] - r['gen_routed_nll'] > 0 for r in subset]) * 100\n",
    "    irrel = np.mean([r['irrel_nll'] for r in subset])\n",
    "    irrel_wr = np.mean([r['bare_nll'] - r['irrel_nll'] > 0 for r in subset]) * 100\n",
    "    shuf = np.mean([r['shuffled_nll'] for r in subset])\n",
    "    rp = np.mean([r['rand_passage_nll'] for r in subset])\n",
    "    rt = np.mean([r['rand_tokens_nll'] for r in subset])\n",
    "    perf = np.mean([r['perfect_nll'] for r in subset])\n",
    "    print(f\"{label:<16} {n_sub:>5} {bare:>8.3f} {framed:>8.3f} {gen:>8.3f} {gen_wr:>7.1f}% {irrel:>8.3f} {irrel_wr:>7.1f}% {shuf:>8.3f} {rp:>8.3f} {rt:>8.3f} {perf:>8.3f}\")\n",
    "\n",
    "# Stratify by similarity bands\n",
    "print(f\"\\n{'='*100}\")\n",
    "print(\"STRATIFIED BY SURROGATE-QUERY SIMILARITY\")\n",
    "print(\"=\" * 100)\n",
    "sim_bins = [\n",
    "    ('Low (<0.4)',     lambda r: r['gen_routed_sim'] < 0.4),\n",
    "    ('Medium (0.4-0.7)', lambda r: 0.4 <= r['gen_routed_sim'] < 0.7),\n",
    "    ('High (>0.7)',    lambda r: r['gen_routed_sim'] >= 0.7),\n",
    "]\n",
    "\n",
    "print(f\"{'Band':<20} {'N':>5} {'Bare':>8} {'GenRtd':>8} {'Delta':>8} {'Win%':>8} {'Irrel':>8} {'IrrelDelta':>10}\")\n",
    "print(\"-\" * 80)\n",
    "for label, cond in sim_bins:\n",
    "    subset = [r for r in results if cond(r)]\n",
    "    if not subset:\n",
    "        print(f\"{label:<20} {'(no samples)':>5}\")\n",
    "        continue\n",
    "    bare_s = np.mean([r['bare_nll'] for r in subset])\n",
    "    gen_s = np.mean([r['gen_routed_nll'] for r in subset])\n",
    "    delta_s = np.mean([r['bare_nll'] - r['gen_routed_nll'] for r in subset])\n",
    "    wr = np.mean([r['bare_nll'] - r['gen_routed_nll'] > 0 for r in subset]) * 100\n",
    "    irrel_s = np.mean([r['irrel_nll'] for r in subset])\n",
    "    irrel_delta = np.mean([r['bare_nll'] - r['irrel_nll'] for r in subset])\n",
    "    print(f\"{label:<20} {len(subset):>5} {bare_s:>8.3f} {gen_s:>8.3f} {delta_s:>+8.3f} {wr:>7.1f}% {irrel_s:>8.3f} {irrel_delta:>+10.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a11ba93",
   "metadata": {},
   "source": [
    "## Correlation and Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16115539",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Correlation: surrogate-query similarity vs NLL delta\n",
    "gen_sims = np.array([r['gen_routed_sim'] for r in results])\n",
    "gen_deltas = np.array([r['bare_nll'] - r['gen_routed_nll'] for r in results])\n",
    "difficulty = bare_arr.copy()\n",
    "answer_lens = np.array([r['answer_len'] for r in results])\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"CORRELATION: SIMILARITY vs NLL DELTA\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "r_sim, p_sim = stats.pearsonr(gen_sims, gen_deltas)\n",
    "print(f\"Similarity vs Delta:  r={r_sim:.3f}, p={p_sim:.6f}\")\n",
    "\n",
    "r_diff, p_diff = stats.pearsonr(difficulty, gen_deltas)\n",
    "print(f\"Difficulty vs Delta:  r={r_diff:.3f}, p={p_diff:.6f}\")\n",
    "\n",
    "r_alen, p_alen = stats.pearsonr(answer_lens, gen_deltas)\n",
    "print(f\"Answer len vs Delta:  r={r_alen:.3f}, p={p_alen:.6f}\")\n",
    "\n",
    "# Regression: delta ~ similarity + difficulty + answer_length\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"REGRESSION: delta ~ similarity + difficulty + answer_length\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "X = np.column_stack([gen_sims, difficulty, answer_lens])\n",
    "y = gen_deltas\n",
    "reg = LinearRegression().fit(X, y)\n",
    "print(f\"R² = {reg.score(X, y):.4f}\")\n",
    "print(f\"Coefficients:\")\n",
    "for name, coef in zip(['similarity', 'difficulty', 'answer_length'], reg.coef_):\n",
    "    print(f\"  {name:>15}: {coef:+.4f}\")\n",
    "print(f\"  {'intercept':>15}: {reg.intercept_:+.4f}\")\n",
    "\n",
    "# Bootstrap 95% CIs (10k resamples)\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"BOOTSTRAP 95% CIs FOR KEY DELTAS (10,000 resamples)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "def bootstrap_ci(data, n_boot=10000, ci=0.95):\n",
    "    boot_means = np.array([\n",
    "        np.mean(np.random.choice(data, size=len(data), replace=True))\n",
    "        for _ in range(n_boot)\n",
    "    ])\n",
    "    lower = np.percentile(boot_means, (1 - ci) / 2 * 100)\n",
    "    upper = np.percentile(boot_means, (1 + ci) / 2 * 100)\n",
    "    return lower, upper\n",
    "\n",
    "np.random.seed(config.seed)\n",
    "for label, key in [\n",
    "    (\"Gen routed vs Bare\",     'gen_routed_nll'),\n",
    "    (\"Irrelevant vs Bare\",     'irrel_nll'),\n",
    "    (\"Shuffled vs Bare\",       'shuffled_nll'),\n",
    "    (\"Random passage vs Bare\", 'rand_passage_nll'),\n",
    "    (\"Random tokens vs Bare\",  'rand_tokens_nll'),\n",
    "    (\"Perfect vs Bare\",        'perfect_nll'),\n",
    "    (\"Pool oracle vs Bare\",    'pool_oracle_nll'),\n",
    "]:\n",
    "    deltas = np.array([r['bare_nll'] - r[key] for r in results])\n",
    "    lo, hi = bootstrap_ci(deltas)\n",
    "    print(f\"  {label:<28}: mean={np.mean(deltas):+.4f}  95% CI=[{lo:+.4f}, {hi:+.4f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce249729",
   "metadata": {},
   "source": [
    "## Per-Template Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a24ba57",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 100)\n",
    "print(\"PER-TEMPLATE BREAKDOWN: GENERATED SURROGATES (truncated)\")\n",
    "print(\"=\" * 100)\n",
    "print(f\"{'Template':<28} {'Mean NLL':>10} {'Delta':>10} {'Win%':>8} {'Routed':>8} {'Oracle':>8}\")\n",
    "print(\"-\" * 80)\n",
    "for key in TOP_5_SURROGATE_TEMPLATES.keys():\n",
    "    nlls = [r['gen_nlls'][key] for r in results]\n",
    "    deltas = [r['bare_nll'] - r['gen_nlls'][key] for r in results]\n",
    "    times_routed = sum(1 for r in results if r['gen_routed_key'] == key)\n",
    "    times_oracle = sum(1 for r in results if r['gen_oracle_key'] == key)\n",
    "    print(f\"{key:<28} {np.mean(nlls):>10.4f} {np.mean(deltas):>+10.4f} {np.mean([d > 0 for d in deltas])*100:>7.1f}% {times_routed:>8d} {times_oracle:>8d}\")\n",
    "\n",
    "print(f\"\\n{'='*100}\")\n",
    "print(\"PER-TEMPLATE BREAKDOWN: STATIC SURROGATES (in pool)\")\n",
    "print(\"=\" * 100)\n",
    "print(f\"{'Template':<28} {'Mean NLL':>10} {'Delta':>10} {'Win%':>8}\")\n",
    "print(\"-\" * 60)\n",
    "for key in STATIC_SURROGATE_QUERIES.keys():\n",
    "    nlls = [r['pool_answer_nlls'][f'static_{key}'] for r in results]\n",
    "    deltas = [r['bare_nll'] - r['pool_answer_nlls'][f'static_{key}'] for r in results]\n",
    "    print(f\"{key:<28} {np.mean(nlls):>10.4f} {np.mean(deltas):>+10.4f} {np.mean([d > 0 for d in deltas])*100:>7.1f}%\")\n",
    "\n",
    "# Routing efficiency per router\n",
    "print(f\"\\n{'='*100}\")\n",
    "print(\"POOL ROUTING: WHICH CACHES GET SELECTED?\")\n",
    "print(\"=\" * 100)\n",
    "for router_label, key_field in [(\"Cosine\", 'pool_cosine_key'), (\"Query-NLL\", 'pool_qnll_key'),\n",
    "                                  (\"Combined\", 'pool_combined_key'), (\"Oracle\", 'pool_oracle_key')]:\n",
    "    print(f\"\\n  --- {router_label} Router ---\")\n",
    "    from collections import Counter\n",
    "    counts = Counter(r[key_field] for r in results)\n",
    "    for k, c in counts.most_common():\n",
    "        print(f\"    {k:<30}: {c:>5} ({c/len(results)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feffdb6a",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afa9f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 3, figsize=(22, 18))\n",
    "fig.suptitle('Experiment 06: Semantic Priming Hypothesis Test', fontsize=16, fontweight='bold')\n",
    "\n",
    "# --- (0,0) All conditions bar chart ---\n",
    "ax = axes[0, 0]\n",
    "cond_order = [\n",
    "    ('Bare', 'bare_nll'),\n",
    "    ('Framed', 'framed_nll'),\n",
    "    ('Gen\\nRouted', 'gen_routed_nll'),\n",
    "    ('Gen\\nOracle', 'gen_oracle_nll'),\n",
    "    ('Perfect', 'perfect_nll'),\n",
    "    ('Irrel', 'irrel_nll'),\n",
    "    ('Shuffled', 'shuffled_nll'),\n",
    "    ('RandPass', 'rand_passage_nll'),\n",
    "    ('RandTok', 'rand_tokens_nll'),\n",
    "]\n",
    "means = [np.mean([r[k] for r in results]) for _, k in cond_order]\n",
    "sems = [stats.sem([r[k] for r in results]) for _, k in cond_order]\n",
    "colors = ['#888888', '#aaaaaa', '#4c72b0', '#2a5298', '#55a868',\n",
    "          '#c44e52', '#dd8452', '#8c564b', '#7f7f7f']\n",
    "bars = ax.bar(range(len(cond_order)), means, yerr=sems, color=colors, capsize=3)\n",
    "ax.set_xticks(range(len(cond_order)))\n",
    "ax.set_xticklabels([l for l, _ in cond_order], fontsize=7)\n",
    "ax.set_ylabel('Mean NLL')\n",
    "ax.set_title('Group A+B: Truncated Conditions')\n",
    "\n",
    "# --- (0,1) Semantic isolation: paired deltas ---\n",
    "ax = axes[0, 1]\n",
    "labels_iso = ['Gen\\nvs Bare', 'Gen\\nvs Irrel', 'Gen\\nvs Shuf', 'Gen\\nvs RndP', 'Gen\\nvs RndT']\n",
    "data_iso = [\n",
    "    np.array([r['bare_nll'] - r['gen_routed_nll'] for r in results]),\n",
    "    np.array([r['irrel_nll'] - r['gen_routed_nll'] for r in results]),\n",
    "    np.array([r['shuffled_nll'] - r['gen_routed_nll'] for r in results]),\n",
    "    np.array([r['rand_passage_nll'] - r['gen_routed_nll'] for r in results]),\n",
    "    np.array([r['rand_tokens_nll'] - r['gen_routed_nll'] for r in results]),\n",
    "]\n",
    "bp = ax.boxplot(data_iso, labels=labels_iso, patch_artist=True)\n",
    "box_colors = ['#4c72b0', '#c44e52', '#dd8452', '#8c564b', '#7f7f7f']\n",
    "for patch, c in zip(bp['boxes'], box_colors):\n",
    "    patch.set_facecolor(c)\n",
    "    patch.set_alpha(0.7)\n",
    "ax.axhline(0, color='black', linestyle='--', linewidth=0.8)\n",
    "ax.set_ylabel('NLL difference (positive = Gen better)')\n",
    "ax.set_title('Semantic Isolation Tests')\n",
    "\n",
    "# --- (0,2) Full-context comparisons ---\n",
    "ax = axes[0, 2]\n",
    "fc_labels = ['Full\\nGen', 'Full\\nIrrel', 'Full\\nRandP', 'Bare']\n",
    "fc_means = [np.mean([r[k] for r in results]) for k in ['full_gen_nll', 'full_irrel_nll', 'full_rand_passage_nll', 'bare_nll']]\n",
    "fc_sems = [stats.sem([r[k] for r in results]) for k in ['full_gen_nll', 'full_irrel_nll', 'full_rand_passage_nll', 'bare_nll']]\n",
    "fc_colors = ['#4c72b0', '#c44e52', '#8c564b', '#888888']\n",
    "ax.bar(range(4), fc_means, yerr=fc_sems, color=fc_colors, capsize=3)\n",
    "ax.set_xticks(range(4))\n",
    "ax.set_xticklabels(fc_labels, fontsize=8)\n",
    "ax.set_ylabel('Mean NLL')\n",
    "ax.set_title('Group C: Full Context')\n",
    "\n",
    "# --- (1,0) Pool routing comparison ---\n",
    "ax = axes[1, 0]\n",
    "pool_labels = ['Bare', 'Cosine', 'Q-NLL', 'Combined', 'Oracle']\n",
    "pool_keys = ['bare_nll', 'pool_cosine_nll', 'pool_qnll_nll', 'pool_combined_nll', 'pool_oracle_nll']\n",
    "pool_means = [np.mean([r[k] for r in results]) for k in pool_keys]\n",
    "pool_sems = [stats.sem([r[k] for r in results]) for k in pool_keys]\n",
    "pool_colors = ['#888888', '#4c72b0', '#55a868', '#dd8452', '#2a5298']\n",
    "ax.bar(range(5), pool_means, yerr=pool_sems, color=pool_colors, capsize=3)\n",
    "ax.set_xticks(range(5))\n",
    "ax.set_xticklabels(pool_labels, fontsize=8)\n",
    "ax.set_ylabel('Mean NLL')\n",
    "ax.set_title('Group D: Pool Routing')\n",
    "\n",
    "# --- (1,1) Routing efficiency ---\n",
    "ax = axes[1, 1]\n",
    "oracle_deltas = np.array([r['bare_nll'] - r['pool_oracle_nll'] for r in results])\n",
    "cosine_deltas = np.array([r['bare_nll'] - r['pool_cosine_nll'] for r in results])\n",
    "qnll_deltas = np.array([r['bare_nll'] - r['pool_qnll_nll'] for r in results])\n",
    "combined_deltas = np.array([r['bare_nll'] - r['pool_combined_nll'] for r in results])\n",
    "\n",
    "efficiencies = []\n",
    "for name, deltas in [('Cosine', cosine_deltas), ('Q-NLL', qnll_deltas), ('Combined', combined_deltas)]:\n",
    "    eff = np.mean(deltas) / np.mean(oracle_deltas) * 100 if np.mean(oracle_deltas) != 0 else 0\n",
    "    efficiencies.append((name, eff))\n",
    "\n",
    "ax.bar([e[0] for e in efficiencies], [e[1] for e in efficiencies],\n",
    "       color=['#4c72b0', '#55a868', '#dd8452'])\n",
    "ax.axhline(100, color='black', linestyle='--', linewidth=0.8, alpha=0.5)\n",
    "ax.set_ylabel('Routing Efficiency (%)')\n",
    "ax.set_title('Routing Efficiency (% of Oracle)')\n",
    "ax.set_ylim(0, 120)\n",
    "\n",
    "# --- (1,2) Win rate by difficulty quartile ---\n",
    "ax = axes[1, 2]\n",
    "q_labels_list = []\n",
    "q_gen_wr = []\n",
    "q_irrel_wr = []\n",
    "q_shuf_wr = []\n",
    "for label, cond in difficulty_bins:\n",
    "    subset = [r for r in results if cond(r['bare_nll'])]\n",
    "    if not subset:\n",
    "        continue\n",
    "    q_labels_list.append(label.replace(' ', '\\n'))\n",
    "    q_gen_wr.append(np.mean([r['bare_nll'] - r['gen_routed_nll'] > 0 for r in subset]) * 100)\n",
    "    q_irrel_wr.append(np.mean([r['bare_nll'] - r['irrel_nll'] > 0 for r in subset]) * 100)\n",
    "    q_shuf_wr.append(np.mean([r['bare_nll'] - r['shuffled_nll'] > 0 for r in subset]) * 100)\n",
    "\n",
    "x_pos = np.arange(len(q_labels_list))\n",
    "w = 0.25\n",
    "ax.bar(x_pos - w, q_gen_wr, w, label='Gen Routed', color='#4c72b0')\n",
    "ax.bar(x_pos, q_irrel_wr, w, label='Irrelevant', color='#c44e52')\n",
    "ax.bar(x_pos + w, q_shuf_wr, w, label='Shuffled', color='#dd8452')\n",
    "ax.axhline(50, color='black', linestyle='--', linewidth=0.8, alpha=0.3)\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels(q_labels_list, fontsize=7)\n",
    "ax.set_ylabel('Win Rate vs Bare (%)')\n",
    "ax.set_title('Win Rate by Difficulty')\n",
    "ax.legend(fontsize=7)\n",
    "ax.set_ylim(0, 100)\n",
    "\n",
    "# --- (2,0) Similarity vs delta scatter ---\n",
    "ax = axes[2, 0]\n",
    "ax.scatter(gen_sims, gen_deltas, alpha=0.2, s=8, c='#4c72b0')\n",
    "z = np.polyfit(gen_sims, gen_deltas, 1)\n",
    "p_line = np.poly1d(z)\n",
    "x_range = np.linspace(gen_sims.min(), gen_sims.max(), 100)\n",
    "ax.plot(x_range, p_line(x_range), 'r-', linewidth=1.5)\n",
    "ax.axhline(0, color='black', linestyle='--', linewidth=0.8)\n",
    "ax.set_xlabel('Surrogate-Query Cosine Similarity')\n",
    "ax.set_ylabel('Delta NLL (positive = better)')\n",
    "ax.set_title(f'Similarity vs Delta (r={r_sim:.3f})')\n",
    "\n",
    "# --- (2,1) Per-template NLL (generated) ---\n",
    "ax = axes[2, 1]\n",
    "gen_keys = list(TOP_5_SURROGATE_TEMPLATES.keys())\n",
    "gen_template_means = [np.mean([r['gen_nlls'][k] for r in results]) for k in gen_keys]\n",
    "gen_template_sems = [stats.sem([r['gen_nlls'][k] for r in results]) for k in gen_keys]\n",
    "bars = ax.bar(range(len(gen_keys)), gen_template_means, yerr=gen_template_sems,\n",
    "              color='#4c72b0', capsize=3)\n",
    "ax.axhline(np.mean(bare_arr), color='#888888', linestyle='--', linewidth=1, label='Bare baseline')\n",
    "ax.set_xticks(range(len(gen_keys)))\n",
    "ax.set_xticklabels([k.replace('_', '\\n') for k in gen_keys], fontsize=6)\n",
    "ax.set_ylabel('Mean NLL')\n",
    "ax.set_title('Per-Template: Generated')\n",
    "ax.legend(fontsize=7)\n",
    "\n",
    "# --- (2,2) Router agreement with oracle ---\n",
    "ax = axes[2, 2]\n",
    "router_names = ['Cosine', 'Query-NLL', 'Combined']\n",
    "router_keys = ['pool_cosine_key', 'pool_qnll_key', 'pool_combined_key']\n",
    "agreements = [np.mean([r[k] == r['pool_oracle_key'] for r in results]) * 100 for k in router_keys]\n",
    "ax.bar(router_names, agreements, color=['#4c72b0', '#55a868', '#dd8452'])\n",
    "ax.set_ylabel('Agreement with Oracle (%)')\n",
    "ax.set_title('Router Agreement with Oracle')\n",
    "ax.set_ylim(0, 100)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('06_semantic_priming_results.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"Saved: 06_semantic_priming_results.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db463a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Routing comparison figure\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "fig.suptitle('Production Routing Comparison', fontsize=14, fontweight='bold')\n",
    "\n",
    "# (0) Router NLL distributions\n",
    "ax = axes[0]\n",
    "router_data = [\n",
    "    np.array([r['pool_cosine_nll'] for r in results]),\n",
    "    np.array([r['pool_qnll_nll'] for r in results]),\n",
    "    np.array([r['pool_combined_nll'] for r in results]),\n",
    "    np.array([r['pool_oracle_nll'] for r in results]),\n",
    "]\n",
    "bp = ax.boxplot(router_data, labels=['Cosine', 'Q-NLL', 'Combined', 'Oracle'], patch_artist=True)\n",
    "for patch, c in zip(bp['boxes'], ['#4c72b0', '#55a868', '#dd8452', '#2a5298']):\n",
    "    patch.set_facecolor(c)\n",
    "    patch.set_alpha(0.7)\n",
    "ax.axhline(np.mean(bare_arr), color='#888888', linestyle='--', label='Bare mean')\n",
    "ax.set_ylabel('NLL')\n",
    "ax.set_title('Router NLL Distributions')\n",
    "ax.legend(fontsize=8)\n",
    "\n",
    "# (1) Cosine vs Query-NLL scatter\n",
    "ax = axes[1]\n",
    "ax.scatter(cosine_deltas, qnll_deltas, alpha=0.2, s=8, c='#4c72b0')\n",
    "lims = [min(ax.get_xlim()[0], ax.get_ylim()[0]), max(ax.get_xlim()[1], ax.get_ylim()[1])]\n",
    "ax.plot(lims, lims, 'k--', linewidth=0.8, alpha=0.5)\n",
    "ax.set_xlabel('Cosine Router Delta')\n",
    "ax.set_ylabel('Query-NLL Router Delta')\n",
    "ax.set_title('Cosine vs Query-NLL (above diag = Q-NLL better)')\n",
    "\n",
    "# (2) Cache selection heatmap\n",
    "ax = axes[2]\n",
    "from collections import Counter\n",
    "all_cache_keys = list(pool_caches.keys()) if 'pool_caches' in dir() else ['bare'] + [f'gen_{k}' for k in TOP_5_SURROGATE_TEMPLATES] + [f'static_{k}' for k in STATIC_SURROGATE_QUERIES]\n",
    "# Use first result to get pool keys\n",
    "all_cache_keys = list(results[0]['pool_answer_nlls'].keys())\n",
    "router_labels = ['Cosine', 'Q-NLL', 'Combined', 'Oracle']\n",
    "router_key_fields = ['pool_cosine_key', 'pool_qnll_key', 'pool_combined_key', 'pool_oracle_key']\n",
    "\n",
    "selection_matrix = np.zeros((len(router_labels), len(all_cache_keys)))\n",
    "for ri, rk in enumerate(router_key_fields):\n",
    "    counts = Counter(r[rk] for r in results)\n",
    "    for ci, ck in enumerate(all_cache_keys):\n",
    "        selection_matrix[ri, ci] = counts.get(ck, 0) / len(results) * 100\n",
    "\n",
    "im = ax.imshow(selection_matrix, cmap='YlOrRd', aspect='auto')\n",
    "ax.set_xticks(range(len(all_cache_keys)))\n",
    "ax.set_xticklabels([k.replace('gen_', 'G:').replace('static_', 'S:') for k in all_cache_keys],\n",
    "                    fontsize=6, rotation=45, ha='right')\n",
    "ax.set_yticks(range(len(router_labels)))\n",
    "ax.set_yticklabels(router_labels, fontsize=8)\n",
    "ax.set_title('Cache Selection Rate (%)')\n",
    "plt.colorbar(im, ax=ax)\n",
    "\n",
    "# Annotate cells with values > 5%\n",
    "for ri in range(len(router_labels)):\n",
    "    for ci in range(len(all_cache_keys)):\n",
    "        val = selection_matrix[ri, ci]\n",
    "        if val > 5:\n",
    "            ax.text(ci, ri, f'{val:.0f}', ha='center', va='center', fontsize=6,\n",
    "                    color='white' if val > 40 else 'black')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('06_routing_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"Saved: 06_routing_comparison.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d385980a",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0c5116",
   "metadata": {},
   "outputs": [],
   "source": "results_dir = 'results'\nos.makedirs(results_dir, exist_ok=True)\n\nrun_timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n\noutput = {\n    'metadata': {\n        'experiment': '06_semantic_priming_hypothesis_test',\n        'description': (\n            'Rigorous semantic priming hypothesis test with 16 conditions: '\n            'bare + framed baselines, 7 truncated variants (generated/oracle/perfect/'\n            'irrelevant/shuffled/random_passage/random_tokens), 3 full-context controls, '\n            'and 4 pool routing strategies (cosine/query-NLL/combined/oracle).'\n        ),\n        'timestamp': run_timestamp,\n        'model_name': config.model_name,\n        'num_samples_requested': config.num_samples,\n        'num_samples_evaluated': len(results),\n        'num_errors': errors,\n        'elapsed_seconds': elapsed_total,\n        'seed': config.seed,\n        'filters': {\n            'excluded_ratio': excluded_ratio,\n            'excluded_short_answer': excluded_short_answer,\n        },\n    },\n    'results': results,\n}\n\noutput_path = os.path.join(results_dir, f'06_semantic_priming_{run_timestamp}.json')\nwith open(output_path, 'w') as f:\n    json.dump(output, f, indent=2, default=str)\nprint(f\"Results saved to {output_path}\")\nprint(f\"File size: {os.path.getsize(output_path) / 1e6:.1f} MB\")\n\n# Also save a convenience symlink/copy as latest\nlatest_path = os.path.join(results_dir, '06_semantic_priming_latest.json')\nimport shutil\nshutil.copy2(output_path, latest_path)\nprint(f\"Copied to {latest_path}\")"
  },
  {
   "cell_type": "markdown",
   "id": "80c2a32d",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40639d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"AUTOMATED VERDICTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "alpha = 0.01\n",
    "bonferroni_alpha = alpha / 5\n",
    "\n",
    "verdicts = {}\n",
    "\n",
    "# H1: Does surrogate priming help?\n",
    "gen_arr = np.array([r['gen_routed_nll'] for r in results])\n",
    "t, p = stats.ttest_rel(bare_arr, gen_arr)\n",
    "verdicts['H1: Surrogate priming helps'] = {\n",
    "    'test': 'Gen routed (3) vs Bare (1)',\n",
    "    'bare_mean': float(np.mean(bare_arr)),\n",
    "    'gen_mean': float(np.mean(gen_arr)),\n",
    "    'delta': float(np.mean(bare_arr - gen_arr)),\n",
    "    't': float(t), 'p': float(p),\n",
    "    'significant': p < bonferroni_alpha,\n",
    "    'verdict': 'SUPPORTED' if p < bonferroni_alpha and np.mean(gen_arr) < np.mean(bare_arr) else 'NOT SUPPORTED'\n",
    "}\n",
    "\n",
    "# H2: Semantic content matters (gen vs irrelevant)\n",
    "irrel_arr = np.array([r['irrel_nll'] for r in results])\n",
    "t, p = stats.ttest_rel(gen_arr, irrel_arr)\n",
    "verdicts['H2: Semantic content matters'] = {\n",
    "    'test': 'Gen routed (3) vs Irrelevant (6)',\n",
    "    'gen_mean': float(np.mean(gen_arr)),\n",
    "    'irrel_mean': float(np.mean(irrel_arr)),\n",
    "    'delta': float(np.mean(irrel_arr - gen_arr)),\n",
    "    't': float(t), 'p': float(p),\n",
    "    'significant': p < bonferroni_alpha,\n",
    "    'verdict': 'SUPPORTED' if p < bonferroni_alpha and np.mean(gen_arr) < np.mean(irrel_arr) else 'NOT SUPPORTED'\n",
    "}\n",
    "\n",
    "# H3: Word order matters (gen vs shuffled)\n",
    "shuf_arr = np.array([r['shuffled_nll'] for r in results])\n",
    "t, p = stats.ttest_rel(gen_arr, shuf_arr)\n",
    "verdicts['H3: Word order matters'] = {\n",
    "    'test': 'Gen routed (3) vs Shuffled (7)',\n",
    "    'gen_mean': float(np.mean(gen_arr)),\n",
    "    'shuffled_mean': float(np.mean(shuf_arr)),\n",
    "    'delta': float(np.mean(shuf_arr - gen_arr)),\n",
    "    't': float(t), 'p': float(p),\n",
    "    'significant': p < bonferroni_alpha,\n",
    "    'verdict': 'SUPPORTED' if p < bonferroni_alpha and np.mean(gen_arr) < np.mean(shuf_arr) else 'NOT SUPPORTED'\n",
    "}\n",
    "\n",
    "# H4: Coherent text matters (gen vs random passage)\n",
    "rp_arr = np.array([r['rand_passage_nll'] for r in results])\n",
    "t, p = stats.ttest_rel(gen_arr, rp_arr)\n",
    "verdicts['H4: Topic relevance > any coherent text'] = {\n",
    "    'test': 'Gen routed (3) vs Random passage (8)',\n",
    "    'gen_mean': float(np.mean(gen_arr)),\n",
    "    'rand_passage_mean': float(np.mean(rp_arr)),\n",
    "    'delta': float(np.mean(rp_arr - gen_arr)),\n",
    "    't': float(t), 'p': float(p),\n",
    "    'significant': p < bonferroni_alpha,\n",
    "    'verdict': 'SUPPORTED' if p < bonferroni_alpha and np.mean(gen_arr) < np.mean(rp_arr) else 'NOT SUPPORTED'\n",
    "}\n",
    "\n",
    "# H5: Coherence matters (gen vs random tokens)\n",
    "rt_arr = np.array([r['rand_tokens_nll'] for r in results])\n",
    "t, p = stats.ttest_rel(gen_arr, rt_arr)\n",
    "verdicts['H5: Coherence matters'] = {\n",
    "    'test': 'Gen routed (3) vs Random tokens (9)',\n",
    "    'gen_mean': float(np.mean(gen_arr)),\n",
    "    'rand_tokens_mean': float(np.mean(rt_arr)),\n",
    "    'delta': float(np.mean(rt_arr - gen_arr)),\n",
    "    't': float(t), 'p': float(p),\n",
    "    'significant': p < bonferroni_alpha,\n",
    "    'verdict': 'SUPPORTED' if p < bonferroni_alpha and np.mean(gen_arr) < np.mean(rt_arr) else 'NOT SUPPORTED'\n",
    "}\n",
    "\n",
    "# H6: Full-context benefit is semantic\n",
    "full_gen_arr = np.array([r['full_gen_nll'] for r in results])\n",
    "full_irrel_arr = np.array([r['full_irrel_nll'] for r in results])\n",
    "t, p = stats.ttest_rel(full_gen_arr, full_irrel_arr)\n",
    "verdicts['H6: Full-context benefit is semantic'] = {\n",
    "    'test': 'Full-ctx gen (10) vs Full-ctx irrel (11)',\n",
    "    'gen_mean': float(np.mean(full_gen_arr)),\n",
    "    'irrel_mean': float(np.mean(full_irrel_arr)),\n",
    "    'delta': float(np.mean(full_irrel_arr - full_gen_arr)),\n",
    "    't': float(t), 'p': float(p),\n",
    "    'significant': p < 0.05,\n",
    "    'verdict': 'SUPPORTED' if p < 0.05 and np.mean(full_gen_arr) < np.mean(full_irrel_arr) else 'NOT SUPPORTED'\n",
    "}\n",
    "\n",
    "# H7: Query-NLL routing > cosine routing\n",
    "cosine_arr = np.array([r['pool_cosine_nll'] for r in results])\n",
    "qnll_arr = np.array([r['pool_qnll_nll'] for r in results])\n",
    "t, p = stats.ttest_rel(cosine_arr, qnll_arr)\n",
    "verdicts['H7: Query-NLL routing > Cosine routing'] = {\n",
    "    'test': 'Pool cosine (13) vs Pool query-NLL (14)',\n",
    "    'cosine_mean': float(np.mean(cosine_arr)),\n",
    "    'qnll_mean': float(np.mean(qnll_arr)),\n",
    "    'delta': float(np.mean(cosine_arr - qnll_arr)),\n",
    "    't': float(t), 'p': float(p),\n",
    "    'significant': p < 0.05,\n",
    "    'verdict': 'SUPPORTED' if p < 0.05 and np.mean(qnll_arr) < np.mean(cosine_arr) else 'NOT SUPPORTED'\n",
    "}\n",
    "\n",
    "# Print verdicts\n",
    "for hyp, v in verdicts.items():\n",
    "    sig_marker = \"***\" if v['significant'] else \"   \"\n",
    "    print(f\"\\n{sig_marker} {hyp}: {v['verdict']}\")\n",
    "    print(f\"    {v['test']}\")\n",
    "    print(f\"    t={v['t']:.3f}, p={v['p']:.6f}\")\n",
    "\n",
    "# Oracle ceiling\n",
    "oracle_arr = np.array([r['pool_oracle_nll'] for r in results])\n",
    "oracle_delta = np.mean(bare_arr - oracle_arr)\n",
    "t_oracle, p_oracle = stats.ttest_rel(bare_arr, oracle_arr)\n",
    "print(f\"\\n--- Oracle Ceiling ---\")\n",
    "print(f\"Pool oracle mean NLL: {np.mean(oracle_arr):.4f}\")\n",
    "print(f\"Delta vs bare: {oracle_delta:+.4f} (t={t_oracle:.3f}, p={p_oracle:.6f})\")\n",
    "print(f\"Win rate: {np.mean(bare_arr > oracle_arr)*100:.1f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}