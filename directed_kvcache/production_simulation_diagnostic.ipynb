{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": "# Production Simulation Diagnostic: Why Don't Surrogates Help?\n\n## Background\n\nPrevious experiments showed that surrogate-primed KV caches **hurt** performance in most cases:\n- Win rate vs baseline: only 9% for generated surrogates\n- Surrogates help ONLY when baseline is poor (NLL > 3.0)\n- When baseline is good, surrogates make predictions 3-4x worse\n\n## Definitions\n\n- **Baseline**: KV cache built from the document in isolation (no surrogate, no priming).\n  This is `Document:\\n{document}` processed through the model alone.\n- **Surrogate condition**: Any approach that incorporates a surrogate query during KV cache construction.\n  The surrogate may be generated, static, or even the actual query (perfect surrogate test).\n\n### Terminology Note: \"Oracle\" vs \"Perfect Surrogate\"\n\nIn **other notebooks** (e.g., `production_simulation_experiment.ipynb`), \"oracle\" refers to\n**best-of-N hindsight selection**: given N surrogate candidates, pick the one that happened to\nproduce the lowest NLL. This is oracle *selection*.\n\nIn **this notebook**, we test using the **actual query** in the surrogate slot. This is a\ndifferent concept — it's a *perfect surrogate* (the ideal surrogate you could generate if you\nknew the query in advance). We call this `perfect_surrogate` to avoid confusion with oracle\nselection used elsewhere.\n\n## Hypotheses to Test\n\n1. **Competing Query Signal**: The surrogate acts as a distractor that competes with the actual query\n2. **Template Framing Issue**: The specific phrasing of the surrogate template may be problematic\n3. **Attention Dilution**: Extra tokens dilute attention away from key document content\n\n## Experiments in This Notebook\n\nAll experiments compare against the **baseline (document in isolation)**:\n\n1. **Baseline Reproduction**: Confirm original results using shared library\n2. **Perfect Surrogate Test**: Use the ACTUAL query as surrogate (upper bound)\n3. **Template Ablation**: Test different surrogate framing approaches\n4. **Truncated Cache Comparison**: Does removing surrogate at inference help?\n5. **Poor Baseline Analysis**: Deep dive into cases where surrogates helped"
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install transformers torch datasets tqdm scipy bitsandbytes accelerate matplotlib sentence-transformers -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import json\n",
    "from typing import Dict, List, Tuple, Any\n",
    "from dataclasses import dataclass\n",
    "from tqdm.auto import tqdm\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from datasets import load_dataset\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Import from our shared library\n",
    "from lib import (\n",
    "    ExperimentConfig,\n",
    "    build_kv_cache,\n",
    "    score_answer_with_cache,\n",
    "    build_truncated_kv_cache,\n",
    "    TOP_5_SURROGATE_TEMPLATES,\n",
    "    STATIC_SURROGATE_QUERIES,\n",
    "    generate_surrogate_with_template,\n",
    "    generate_all_5_surrogates,\n",
    "    compute_similarity,\n",
    "    count_words,\n",
    ")\n",
    "\n",
    "print(\"Imports complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "config = ExperimentConfig(\n",
    "    num_samples=500,  # Enough for statistical power, fast enough for diagnostics\n",
    "    min_passage_words=50,\n",
    "    max_passage_words=300,\n",
    ")\n",
    "\n",
    "print(f\"Device: {config.device}\")\n",
    "print(f\"Model: {config.model_name}\")\n",
    "print(f\"Samples: {config.num_samples}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load models\n",
    "torch.manual_seed(config.seed)\n",
    "np.random.seed(config.seed)\n",
    "\n",
    "print(\"Loading language model...\")\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.model_name,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.model_name, trust_remote_code=True)\n",
    "tokenizer.padding_side = \"right\"\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "model.eval()\n",
    "print(f\"Language model loaded: {model.num_parameters():,} parameters\")\n",
    "\n",
    "print(f\"\\nLoading embedding model: {config.embedding_model_name}\")\n",
    "embed_model = SentenceTransformer(config.embedding_model_name)\n",
    "print(\"Embedding model loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "print(\"Loading MS MARCO dataset...\")\n",
    "full_dataset = load_dataset(\n",
    "    config.dataset_name,\n",
    "    config.dataset_config,\n",
    "    split=config.dataset_split,\n",
    ")\n",
    "print(f\"Dataset loaded: {len(full_dataset)} samples\")\n",
    "\n",
    "# Filter samples\n",
    "def load_samples_with_answers(dataset, config):\n",
    "    \"\"\"Load samples with passage, query, and answer.\"\"\"\n",
    "    filtered = []\n",
    "    for item in tqdm(dataset, desc=\"Filtering\"):\n",
    "        passages = item.get('passages', {})\n",
    "        passage_texts = passages.get('passage_text', [])\n",
    "        is_selected = passages.get('is_selected', [])\n",
    "        query = item.get('query', '')\n",
    "        answers = item.get('answers', [])\n",
    "        well_formed = item.get('wellFormedAnswers', [])\n",
    "        \n",
    "        if not passage_texts or not query:\n",
    "            continue\n",
    "        \n",
    "        # Get answer\n",
    "        if well_formed and len(well_formed) > 0 and well_formed[0] != '[]':\n",
    "            answer = well_formed[0]\n",
    "        elif answers and len(answers) > 0 and answers[0] != 'No Answer Present.':\n",
    "            answer = answers[0]\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        # Find valid passage\n",
    "        for i, passage in enumerate(passage_texts):\n",
    "            wc = count_words(passage)\n",
    "            if config.min_passage_words <= wc <= config.max_passage_words:\n",
    "                if is_selected and i < len(is_selected) and is_selected[i] == 1:\n",
    "                    filtered.append({'passage': passage, 'query': query, 'answer': answer})\n",
    "                    break\n",
    "        \n",
    "        if len(filtered) >= config.num_samples * 2:\n",
    "            break\n",
    "    \n",
    "    np.random.shuffle(filtered)\n",
    "    return filtered[:config.num_samples]\n",
    "\n",
    "samples = load_samples_with_answers(full_dataset, config)\n",
    "print(f\"\\nLoaded {len(samples)} samples\")\n",
    "print(f\"Sample query: {samples[0]['query']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## Experiment 1: Template Ablation Study\n",
    "\n",
    "Compare the **baseline (document in isolation)** against multiple surrogate conditions.\n",
    "\n",
    "Each surrogate condition uses a different template for how the surrogate is incorporated into the context.\n",
    "All are compared back to the same baseline: the document's KV cache computed in isolation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": "def evaluate_single_sample(\n    sample: Dict,\n    model,\n    tokenizer,\n    embed_model,\n    config: ExperimentConfig,\n    surrogate_templates: Dict[str, str],\n) -> Dict:\n    \"\"\"\n    Evaluate a sample: build baseline cache (document in isolation) and\n    compare against multiple surrogate conditions.\n    \n    The baseline is ALWAYS the document's KV cache computed in isolation:\n        \"Document:\\n{document}\"\n    \n    Each surrogate_template is a different way of incorporating a surrogate\n    into the context window. Templates can use:\n        {document} - the document text\n        {surrogate} - a generated surrogate query\n        {query} - the actual query (for oracle tests)\n    \"\"\"\n    passage = sample['passage']\n    query = sample['query']\n    answer = sample['answer']\n    query_prompt = config.query_template.format(query=query)\n    \n    results = {\n        'query': query,\n        'answer': answer[:50],\n        'passage_len': count_words(passage),\n    }\n    \n    # ===== BASELINE: Document KV cache in isolation =====\n    baseline_context = config.baseline_cache_template.format(document=passage)\n    baseline_len, baseline_cache = build_kv_cache(baseline_context, model, tokenizer, config)\n    baseline_nll = score_answer_with_cache(\n        baseline_cache, baseline_len, query_prompt, answer, model, tokenizer, config\n    )\n    results['baseline_nll'] = baseline_nll\n    results['baseline_len'] = baseline_len\n    \n    # ===== SURROGATE CONDITIONS =====\n    # Generate a surrogate once and reuse across templates that need it\n    surrogate = generate_surrogate_with_template(\n        passage,\n        TOP_5_SURROGATE_TEMPLATES['target_question']['prompt'],\n        model, tokenizer, config\n    )\n    results['generated_surrogate'] = surrogate\n    results['surrogate_similarity'] = compute_similarity(surrogate, query, embed_model)\n    \n    for name, template in surrogate_templates.items():\n        # Format context based on what placeholders the template uses\n        if '{surrogate}' in template and '{document}' in template:\n            context = template.format(document=passage, surrogate=surrogate)\n        elif '{query}' in template and '{document}' in template:\n            context = template.format(document=passage, query=query)\n        elif '{document}' in template:\n            context = template.format(document=passage)\n        else:\n            context = template\n        \n        # Build cache and score\n        cache_len, cache = build_kv_cache(context, model, tokenizer, config)\n        nll = score_answer_with_cache(\n            cache, cache_len, query_prompt, answer, model, tokenizer, config\n        )\n        results[f'{name}_nll'] = nll\n        results[f'{name}_len'] = cache_len\n    \n    return results"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": "# =====================================================================\n# SURROGATE CONDITIONS TO TEST\n# =====================================================================\n# Each is compared against the BASELINE (document KV cache in isolation).\n# The baseline is always: \"Document:\\n{document}\"\n#\n# NOTE: \"perfect_surrogate\" uses the ACTUAL query in the surrogate slot.\n# This is different from \"oracle selection\" in other notebooks, which means\n# picking the best-of-N surrogates in hindsight. See terminology note above.\n\nSURROGATE_CONDITIONS = {\n    # ----- Original surrogate template (from production simulation) -----\n    'surr_original': (\n        'This document may be relevant to queries like: {surrogate}\\n\\n'\n        'Document:\\n{document}'\n    ),\n    \n    # ----- Perfect surrogate: use the ACTUAL query (upper bound on surrogate quality) -----\n    'perfect_surrogate': (\n        'This document may be relevant to queries like: {query}\\n\\n'\n        'Document:\\n{document}'\n    ),\n    \n    # ----- Simpler framing (test if the verbose template is the problem) -----\n    'surr_simple_prefix': 'Query hint: {surrogate}\\n\\n{document}',\n    'surr_minimal': '{surrogate}\\n\\n{document}',\n    \n    # ----- Position test: surrogate AFTER document -----\n    'surr_suffix': 'Document:\\n{document}\\n\\nRelevant queries: {surrogate}',\n    \n    # ----- Control: bare document (no \"Document:\" label at all) -----\n    'bare_doc_no_label': '{document}',\n}\n\nprint(\"Baseline: Document KV cache in isolation\")\nprint(\"  Template: \\\"Document:\\\\n{document}\\\"\")\nprint()\nprint(\"Surrogate conditions to compare against baseline:\")\nfor name, template in SURROGATE_CONDITIONS.items():\n    print(f\"  {name}: \\\"{template[:60]}...\\\"\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": "# Run diagnostic experiment\nprint(f\"Running diagnostic experiment on {len(samples)} samples...\")\nprint(f\"Baseline: document KV cache in isolation\")\nprint(f\"Testing {len(SURROGATE_CONDITIONS)} surrogate conditions against baseline\")\n\ndiagnostic_results = []\n\nfor i, sample in enumerate(tqdm(samples, desc=\"Evaluating\")):\n    try:\n        result = evaluate_single_sample(\n            sample, model, tokenizer, embed_model, config,\n            SURROGATE_CONDITIONS\n        )\n        diagnostic_results.append(result)\n        \n        if (i + 1) % 50 == 0:\n            recent = diagnostic_results[-50:]\n            baseline_avg = np.mean([r['baseline_nll'] for r in recent])\n            perfect_avg = np.mean([r['perfect_surrogate_nll'] for r in recent])\n            print(f\"  [{i+1}] Baseline: {baseline_avg:.3f}, Perfect surrogate: {perfect_avg:.3f}\")\n            \n    except Exception as e:\n        print(f\"Error on sample {i}: {e}\")\n        continue\n\nprint(f\"\\nCompleted {len(diagnostic_results)} samples\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": "# Analyze results\nprint(\"=\" * 80)\nprint(\"ALL CONDITIONS vs BASELINE (document in isolation)\")\nprint(\"=\" * 80)\n\ncondition_names = list(SURROGATE_CONDITIONS.keys())\n\nprint(f\"\\n{'Condition':<25} {'Mean NLL':>10} {'Std':>10} {'vs Baseline':>12} {'Win Rate':>10}\")\nprint(\"-\" * 70)\n\nbaseline_nlls = np.array([r['baseline_nll'] for r in diagnostic_results])\n\n# Print baseline first\nprint(f\"{'BASELINE (doc only)':<25} {np.mean(baseline_nlls):>10.3f} {np.std(baseline_nlls):>10.3f} {'---':>12} {'---':>10}\")\nprint(\"-\" * 70)\n\nfor name in condition_names:\n    nlls = np.array([r[f'{name}_nll'] for r in diagnostic_results])\n    delta = baseline_nlls - nlls  # positive = this condition beats baseline\n    win_rate = np.mean(delta > 0)\n    \n    mean_delta = np.mean(delta)\n    sign = '+' if mean_delta > 0 else ''\n    \n    print(f\"{name:<25} {np.mean(nlls):>10.3f} {np.std(nlls):>10.3f} {sign}{mean_delta:>11.3f} {win_rate*100:>9.1f}%\")\n\nprint(\"\\n(Positive 'vs Baseline' = better than baseline, higher win rate = better)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": "# Statistical significance tests\nprint(\"\\n\" + \"=\" * 80)\nprint(\"STATISTICAL TESTS: Each condition vs BASELINE (doc in isolation)\")\nprint(\"=\" * 80)\n\nfor name in condition_names:\n    nlls = np.array([r[f'{name}_nll'] for r in diagnostic_results])\n    t_stat, p_value = stats.ttest_rel(baseline_nlls, nlls)\n    \n    direction = \"BETTER\" if t_stat > 0 else \"WORSE\"\n    sig = \"***\" if p_value < 0.001 else \"**\" if p_value < 0.01 else \"*\" if p_value < 0.05 else \"\"\n    \n    print(f\"{name:<25} t={t_stat:>7.3f}, p={p_value:.4f} {sig} ({direction} than baseline)\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## Experiment 2: Analysis by Baseline Quality\n",
    "\n",
    "Do surrogates help more when the baseline is poor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": "# Bin analysis by baseline quality\nprint(\"=\" * 80)\nprint(\"SURROGATE PERFORMANCE BY BASELINE QUALITY\")\nprint(\"(Baseline = document KV cache in isolation)\")\nprint(\"=\" * 80)\n\nbins = [\n    (0, 0.1, \"Perfect (0-0.1)\"),\n    (0.1, 0.5, \"Excellent (0.1-0.5)\"),\n    (0.5, 1.5, \"Good (0.5-1.5)\"),\n    (1.5, 3.0, \"Medium (1.5-3.0)\"),\n    (3.0, float('inf'), \"Poor (3.0+)\"),\n]\n\nfor name in ['perfect_surrogate', 'surr_original', 'surr_simple_prefix', 'bare_doc_no_label']:\n    print(f\"\\n--- {name} vs Baseline ---\")\n    print(f\"{'Baseline Quality':<20} {'N':>5} {'Avg Delta':>12} {'Win Rate':>10}\")\n    print(\"-\" * 50)\n    \n    for low, high, label in bins:\n        subset = [r for r in diagnostic_results if low <= r['baseline_nll'] < high]\n        if not subset:\n            continue\n        \n        deltas = [r['baseline_nll'] - r[f'{name}_nll'] for r in subset]\n        win_rate = np.mean([d > 0 for d in deltas])\n        \n        print(f\"{label:<20} {len(subset):>5} {np.mean(deltas):>+12.3f} {win_rate*100:>9.1f}%\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": "# Correlation analysis\nprint(\"\\n\" + \"=\" * 80)\nprint(\"CORRELATION: Baseline NLL vs Delta\")\nprint(\"(Does the surrogate condition help more when the baseline is worse?)\")\nprint(\"=\" * 80)\n\nfor name in condition_names:\n    deltas = np.array([r['baseline_nll'] - r[f'{name}_nll'] for r in diagnostic_results])\n    \n    # Filter out near-zero baselines for cleaner correlation\n    mask = baseline_nlls > 0.01\n    corr = np.corrcoef(baseline_nlls[mask], deltas[mask])[0, 1]\n    \n    interpretation = \"helps more when baseline worse\" if corr > 0.1 else \"helps more when baseline better\" if corr < -0.1 else \"no clear pattern\"\n    print(f\"{name:<25} r={corr:>6.3f} ({interpretation})\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": "## Experiment 3: Perfect Surrogate Deep Dive\n\nThe \"perfect surrogate\" test places the **actual query** into the surrogate slot.\nIf even this doesn't beat the baseline (document in isolation),\nthe problem is fundamental to prepending any query text, not surrogate quality.\n\nNote: This is different from \"oracle selection\" used in `production_simulation_experiment.ipynb`,\nwhich refers to picking the best-of-N surrogate candidates in hindsight."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": "# Detailed perfect surrogate analysis\nprint(\"=\" * 80)\nprint(\"PERFECT SURROGATE ANALYSIS\")\nprint(\"=\" * 80)\nprint(\"\\nQuestion: Does placing the ACTUAL query before the document help\")\nprint(\"compared to the baseline (document in isolation)?\")\n\nperfect_deltas = [r['baseline_nll'] - r['perfect_surrogate_nll'] for r in diagnostic_results]\n\nprint(f\"\\nUsing ACTUAL QUERY in surrogate position:\")\nprint(f\"  Mean delta vs baseline: {np.mean(perfect_deltas):+.4f}\")\nprint(f\"  Win rate vs baseline: {np.mean([d > 0 for d in perfect_deltas])*100:.1f}%\")\nprint(f\"  Median delta: {np.median(perfect_deltas):+.4f}\")\n\n# Cases where even perfect surrogate hurts\nperfect_hurt = [r for r in diagnostic_results if r['baseline_nll'] - r['perfect_surrogate_nll'] < -0.5]\nprint(f\"\\nCases where perfect surrogate HURT by >0.5 NLL vs baseline: {len(perfect_hurt)} ({len(perfect_hurt)/len(diagnostic_results)*100:.1f}%)\")\n\nif perfect_hurt:\n    print(\"\\nExamples where using exact query HURT vs document-in-isolation baseline:\")\n    for r in sorted(perfect_hurt, key=lambda x: x['baseline_nll'] - x['perfect_surrogate_nll'])[:5]:\n        delta = r['baseline_nll'] - r['perfect_surrogate_nll']\n        print(f\"  Query: {r['query'][:50]}...\")\n        print(f\"    Baseline: {r['baseline_nll']:.3f}, Perfect surrogate: {r['perfect_surrogate_nll']:.3f}, Delta: {delta:+.3f}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": "# Compare context lengths\nprint(\"\\n\" + \"=\" * 80)\nprint(\"CONTEXT LENGTH ANALYSIS\")\nprint(\"(How many extra tokens does each surrogate condition add vs baseline?)\")\nprint(\"=\" * 80)\n\nprint(f\"\\n{'Condition':<25} {'Mean Tokens':>12} {'Extra vs Baseline':>18}\")\nprint(\"-\" * 60)\n\nbaseline_lens = np.array([r['baseline_len'] for r in diagnostic_results])\nprint(f\"{'BASELINE (doc only)':<25} {np.mean(baseline_lens):>12.1f} {'---':>18}\")\n\nfor name in condition_names:\n    lens = np.array([r[f'{name}_len'] for r in diagnostic_results])\n    extra = np.mean(lens - baseline_lens)\n    print(f\"{name:<25} {np.mean(lens):>12.1f} {extra:>+17.1f}\")\n\n# Correlation between extra tokens and performance hit\nprint(\"\\nDoes more tokens = worse performance?\")\nfor name in ['perfect_surrogate', 'surr_original']:\n    lens = np.array([r[f'{name}_len'] for r in diagnostic_results])\n    deltas = np.array([r['baseline_nll'] - r[f'{name}_nll'] for r in diagnostic_results])\n    extra_tokens = lens - baseline_lens\n    corr = np.corrcoef(extra_tokens, deltas)[0, 1]\n    print(f\"  {name}: r={corr:.3f} (negative = more tokens hurts more)\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "## Experiment 4: Truncated Cache Test\n",
    "\n",
    "Test if removing the surrogate at inference time helps.\n",
    "This isolates whether the problem is:\n",
    "- (A) The surrogate competing with the query at inference time, or\n",
    "- (B) The surrogate fundamentally not helping document representations\n",
    "\n",
    "All conditions are compared against the same **baseline: document KV cache in isolation**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": "# Run truncated cache experiment on a subset\nprint(\"Running truncated cache experiment...\")\nprint(\"(Surrogate used during cache generation, then REMOVED before inference)\")\n\ntruncated_results = []\ntest_samples = samples[:100]  # Smaller subset for this diagnostic\n\nfor i, sample in enumerate(tqdm(test_samples, desc=\"Truncated cache test\")):\n    try:\n        passage = sample['passage']\n        query = sample['query']\n        answer = sample['answer']\n        query_prompt = config.query_template.format(query=query)\n        \n        # Baseline\n        baseline_context = f\"Document:\\n{passage}\"\n        baseline_len, baseline_cache = build_kv_cache(baseline_context, model, tokenizer, config)\n        baseline_nll = score_answer_with_cache(\n            baseline_cache, baseline_len, query_prompt, answer, model, tokenizer, config\n        )\n        \n        # Generate surrogate\n        surrogate = generate_surrogate_with_template(\n            passage,\n            TOP_5_SURROGATE_TEMPLATES['target_question']['prompt'],\n            model, tokenizer, config\n        )\n        \n        # Full context (surrogate visible at inference)\n        full_context = f\"This document may be relevant to queries like: {surrogate}\\n\\nDocument:\\n{passage}\"\n        full_len, full_cache = build_kv_cache(full_context, model, tokenizer, config)\n        full_nll = score_answer_with_cache(\n            full_cache, full_len, query_prompt, answer, model, tokenizer, config\n        )\n        \n        # Truncated (surrogate used for generation, then removed)\n        trunc_len, trunc_cache = build_truncated_kv_cache(\n            surrogate, passage, model, tokenizer, config\n        )\n        trunc_nll = score_answer_with_cache(\n            trunc_cache, trunc_len, query_prompt, answer, model, tokenizer, config\n        )\n        \n        # Perfect surrogate truncated (use actual query, then remove)\n        perfect_trunc_len, perfect_trunc_cache = build_truncated_kv_cache(\n            query, passage, model, tokenizer, config\n        )\n        perfect_trunc_nll = score_answer_with_cache(\n            perfect_trunc_cache, perfect_trunc_len, query_prompt, answer, model, tokenizer, config\n        )\n        \n        truncated_results.append({\n            'query': query,\n            'surrogate': surrogate,\n            'baseline_nll': baseline_nll,\n            'full_nll': full_nll,\n            'truncated_nll': trunc_nll,\n            'perfect_surrogate_truncated_nll': perfect_trunc_nll,\n        })\n        \n    except Exception as e:\n        print(f\"Error on sample {i}: {e}\")\n        continue\n\nprint(f\"\\nCompleted {len(truncated_results)} samples\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": "# Analyze truncated cache results\nprint(\"=\" * 80)\nprint(\"TRUNCATED CACHE RESULTS\")\nprint(\"(All conditions compared against baseline: document in isolation)\")\nprint(\"=\" * 80)\n\nconditions = [\n    ('baseline', 'baseline_nll', 'BASELINE: Document in isolation'),\n    ('full', 'full_nll', 'Full context (surrogate visible)'),\n    ('truncated', 'truncated_nll', 'Truncated (surrogate used then removed)'),\n    ('perfect_trunc', 'perfect_surrogate_truncated_nll', 'Perfect surr. truncated (query used then removed)'),\n]\n\nprint(f\"\\n{'Condition':<45} {'Mean NLL':>10} {'vs Baseline':>12} {'Win Rate':>10}\")\nprint(\"-\" * 80)\n\nbaseline_nlls_t = np.array([r['baseline_nll'] for r in truncated_results])\n\nfor name, key, desc in conditions:\n    nlls = np.array([r[key] for r in truncated_results])\n    delta = baseline_nlls_t - nlls\n    win_rate = np.mean(delta > 0)\n    \n    if name == 'baseline':\n        print(f\"{desc:<45} {np.mean(nlls):>10.3f} {'---':>12} {'---':>10}\")\n    else:\n        print(f\"{desc:<45} {np.mean(nlls):>10.3f} {np.mean(delta):>+11.3f} {win_rate*100:>9.1f}%\")\n\n# Key comparison: full vs truncated\nprint(\"\\n--- Key Comparison ---\")\nfull_nlls = np.array([r['full_nll'] for r in truncated_results])\ntrunc_nlls = np.array([r['truncated_nll'] for r in truncated_results])\n\nprint(f\"Does removing surrogate at inference help?\")\nprint(f\"  Truncated better than full context: {np.mean(trunc_nlls < full_nlls)*100:.1f}%\")\nprint(f\"  Mean improvement over full: {np.mean(full_nlls - trunc_nlls):+.3f}\")\n\nt_stat, p_value = stats.ttest_rel(full_nlls, trunc_nlls)\nprint(f\"  t={t_stat:.3f}, p={p_value:.4f}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-22",
   "metadata": {},
   "source": [
    "## Experiment 5: Deep Dive on Poor Baseline Cases\n",
    "\n",
    "Analyze the cases where baseline performs poorly - these are where surrogates might actually help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": "# Identify poor baseline cases\npoor_baseline = [r for r in diagnostic_results if r['baseline_nll'] > 3.0]\n\nprint(\"=\" * 80)\nprint(f\"POOR BASELINE CASES (NLL > 3.0): {len(poor_baseline)} samples\")\nprint(\"(These are cases where the document alone does not predict the answer well)\")\nprint(\"=\" * 80)\n\nif poor_baseline:\n    print(f\"\\n{'Condition':<25} {'Mean NLL':>10} {'vs Baseline':>12} {'Win Rate':>10}\")\n    print(\"-\" * 60)\n    \n    baseline_poor = np.array([r['baseline_nll'] for r in poor_baseline])\n    print(f\"{'BASELINE (doc only)':<25} {np.mean(baseline_poor):>10.3f} {'---':>12} {'---':>10}\")\n    \n    for name in condition_names:\n        nlls = np.array([r[f'{name}_nll'] for r in poor_baseline])\n        delta = baseline_poor - nlls\n        win_rate = np.mean(delta > 0)\n        \n        print(f\"{name:<25} {np.mean(nlls):>10.3f} {np.mean(delta):>+11.3f} {win_rate*100:>9.1f}%\")\n\n    # Show examples\n    print(\"\\n--- Examples of Poor Baseline Cases ---\")\n    for r in sorted(poor_baseline, key=lambda x: x['baseline_nll'], reverse=True)[:5]:\n        print(f\"\\nQuery: {r['query']}\")\n        print(f\"  Baseline NLL: {r['baseline_nll']:.3f}\")\n        print(f\"  Perfect surrogate NLL: {r['perfect_surrogate_nll']:.3f} (delta: {r['baseline_nll']-r['perfect_surrogate_nll']:+.3f})\")\n        print(f\"  Surrogate NLL: {r['surr_original_nll']:.3f} (delta: {r['baseline_nll']-r['surr_original_nll']:+.3f})\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-24",
   "metadata": {},
   "source": [
    "## Summary and Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": "print(\"=\" * 80)\nprint(\"SUMMARY OF FINDINGS\")\nprint(\"(Baseline = document KV cache computed in isolation)\")\nprint(\"=\" * 80)\n\n# Key metrics\nbaseline_mean = np.mean([r['baseline_nll'] for r in diagnostic_results])\nperfect_mean = np.mean([r['perfect_surrogate_nll'] for r in diagnostic_results])\nsurrogate_mean = np.mean([r['surr_original_nll'] for r in diagnostic_results])\nbare_mean = np.mean([r['bare_doc_no_label_nll'] for r in diagnostic_results])\n\nperfect_win = np.mean([r['baseline_nll'] > r['perfect_surrogate_nll'] for r in diagnostic_results])\nsurrogate_win = np.mean([r['baseline_nll'] > r['surr_original_nll'] for r in diagnostic_results])\n\nprint(f\"\\n1. BASELINE (document in isolation)\")\nprint(f\"   Template: \\\"Document:\\\\n{{document}}\\\"\")\nprint(f\"   Mean NLL: {baseline_mean:.3f}\")\nprint(f\"   Control (bare doc, no label): {bare_mean:.3f}\")\n\nprint(f\"\\n2. PERFECT SURROGATE (actual query placed in surrogate slot)\")\nprint(f\"   (NOT oracle selection — this tests the best possible surrogate content)\")\nprint(f\"   Mean NLL: {perfect_mean:.3f}\")\nprint(f\"   Win rate vs baseline: {perfect_win*100:.1f}%\")\nif perfect_win > 0.5:\n    print(f\"   -> Perfect surrogate beats baseline. Surrogate quality is the bottleneck.\")\nelse:\n    print(f\"   -> Even the actual query doesn't beat baseline.\")\n    print(f\"   -> Problem is fundamental to the approach, not surrogate quality.\")\n\nprint(f\"\\n3. GENERATED SURROGATE (original template)\")\nprint(f\"   Mean NLL: {surrogate_mean:.3f}\")\nprint(f\"   Win rate vs baseline: {surrogate_win*100:.1f}%\")\n\nprint(f\"\\n4. BY BASELINE QUALITY\")\npoor_wins = [r for r in diagnostic_results if r['baseline_nll'] > 3.0 and r['baseline_nll'] > r['perfect_surrogate_nll']]\ngood_wins = [r for r in diagnostic_results if r['baseline_nll'] < 1.0 and r['baseline_nll'] > r['perfect_surrogate_nll']]\npoor_total = len([r for r in diagnostic_results if r['baseline_nll'] > 3.0])\ngood_total = len([r for r in diagnostic_results if r['baseline_nll'] < 1.0])\nprint(f\"   Poor baseline (>3.0): Perfect surr. wins {len(poor_wins)}/{poor_total} ({len(poor_wins)/max(poor_total,1)*100:.1f}%)\")\nprint(f\"   Good baseline (<1.0): Perfect surr. wins {len(good_wins)}/{good_total} ({len(good_wins)/max(good_total,1)*100:.1f}%)\")\n\nif truncated_results:\n    print(f\"\\n5. TRUNCATED CACHE (surrogate removed after cache generation)\")\n    trunc_win = np.mean([r['baseline_nll'] > r['truncated_nll'] for r in truncated_results])\n    full_win = np.mean([r['baseline_nll'] > r['full_nll'] for r in truncated_results])\n    print(f\"   Full context win rate vs baseline: {full_win*100:.1f}%\")\n    print(f\"   Truncated cache win rate vs baseline: {trunc_win*100:.1f}%\")\n    if trunc_win > full_win:\n        print(f\"   -> Removing surrogate at inference helps!\")\n    else:\n        print(f\"   -> Truncation does not help vs baseline\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-26",
   "metadata": {},
   "outputs": [],
   "source": "# Save results\noutput = {\n    'config': {\n        'model_name': config.model_name,\n        'num_samples': len(diagnostic_results),\n        'conditions_tested': list(SURROGATE_CONDITIONS.keys()),\n        'baseline': 'Document:\\n{document} (document KV cache in isolation)',\n        'terminology_note': (\n            'perfect_surrogate = actual query used in surrogate slot (upper bound on surrogate quality). '\n            'This is different from \"oracle\" in production_simulation_experiment.ipynb, which means '\n            'best-of-N surrogate selection in hindsight.'\n        ),\n    },\n    'diagnostic_results': diagnostic_results,\n    'truncated_results': truncated_results,\n    'summary': {\n        'baseline_mean_nll': baseline_mean,\n        'perfect_surrogate_mean_nll': perfect_mean,\n        'perfect_surrogate_win_rate': perfect_win,\n        'surrogate_mean_nll': surrogate_mean,\n        'surrogate_win_rate': surrogate_win,\n    }\n}\n\nwith open('diagnostic_experiment_results.json', 'w') as f:\n    json.dump(output, f, indent=2, default=str)\n\nprint(\"Results saved to: diagnostic_experiment_results.json\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-27",
   "metadata": {},
   "outputs": [],
   "source": "# Visualization\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\nfig.suptitle(\"All conditions vs Baseline (document in isolation)\", fontsize=13, fontweight='bold')\n\n# Plot 1: NLL by condition\nax1 = axes[0, 0]\nall_names = ['baseline'] + condition_names\nall_means = [np.mean(baseline_nlls)] + [np.mean([r[f'{name}_nll'] for r in diagnostic_results]) for name in condition_names]\nall_stds = [np.std(baseline_nlls)] + [np.std([r[f'{name}_nll'] for r in diagnostic_results]) for name in condition_names]\ncolors = ['green'] + ['steelblue'] * len(condition_names)\nx = range(len(all_names))\nax1.bar(x, all_means, yerr=all_stds, capsize=3, alpha=0.7, color=colors)\nax1.set_xticks(x)\nax1.set_xticklabels(all_names, rotation=45, ha='right', fontsize=8)\nax1.set_ylabel('Mean NLL')\nax1.set_title('NLL by Condition (lower is better)')\nax1.axhline(y=all_means[0], color='r', linestyle='--', label='Baseline')\nax1.legend()\n\n# Plot 2: Win rate vs baseline\nax2 = axes[0, 1]\nwin_rates = []\nfor name in condition_names:\n    nlls = np.array([r[f'{name}_nll'] for r in diagnostic_results])\n    win_rates.append(np.mean(baseline_nlls > nlls) * 100)\nax2.bar(range(len(condition_names)), win_rates, alpha=0.7, color='steelblue')\nax2.set_xticks(range(len(condition_names)))\nax2.set_xticklabels(condition_names, rotation=45, ha='right', fontsize=8)\nax2.set_ylabel('Win Rate vs Baseline (%)')\nax2.set_title('Win Rate vs Baseline (doc in isolation)')\nax2.axhline(y=50, color='r', linestyle='--', label='50% (no effect)')\nax2.legend()\n\n# Plot 3: Baseline NLL vs Perfect Surrogate Delta\nax3 = axes[1, 0]\nperfect_deltas = [r['baseline_nll'] - r['perfect_surrogate_nll'] for r in diagnostic_results]\nax3.scatter(baseline_nlls, perfect_deltas, alpha=0.5, s=10)\nax3.axhline(y=0, color='r', linestyle='--')\nax3.set_xlabel('Baseline NLL (doc in isolation)')\nax3.set_ylabel('Delta (Baseline - Perfect Surrogate)')\nax3.set_title('Does perfect surrogate help more when baseline is worse?\\n(positive = perfect surrogate better than baseline)')\n\n# Plot 4: Distribution of deltas vs baseline\nax4 = axes[1, 1]\nsurrogate_deltas = [r['baseline_nll'] - r['surr_original_nll'] for r in diagnostic_results]\nax4.hist(perfect_deltas, bins=30, alpha=0.5, label='Perfect Surr. vs Baseline', color='blue')\nax4.hist(surrogate_deltas, bins=30, alpha=0.5, label='Gen. Surrogate vs Baseline', color='orange')\nax4.axvline(x=0, color='r', linestyle='--')\nax4.set_xlabel('Delta vs Baseline (positive = beats baseline)')\nax4.set_ylabel('Count')\nax4.set_title('Distribution of Improvement over Baseline')\nax4.legend()\n\nplt.tight_layout()\nplt.savefig('diagnostic_results.png', dpi=150, bbox_inches='tight')\nplt.show()\nprint(\"Saved: diagnostic_results.png\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}