{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Curriculum Learning Experiments\n",
    "\n",
    "**Research Question:** Does the order of training data affect language model learning performance?\n",
    "\n",
    "This notebook uses the streamlined curriculum learning framework to test 9 different curriculum strategies across reading level and semantic topic axes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸš€ Quick Start\n",
    "\n",
    "The new system is **dead simple** - just import and run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Streamlined curriculum learning system loaded!\n"
     ]
    }
   ],
   "source": [
    "# Import the streamlined system\n",
    "from config import debug_config, pilot_config, scientific_config\n",
    "from experiment import Experiment\n",
    "\n",
    "print(\"âœ… Streamlined curriculum learning system loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ§ª Quick Test (Debug Mode)\n",
    "\n",
    "Let's start with a quick test using debug configuration (1K samples, 2 epochs):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¬ Configuration:\n",
      "============================================================\n",
      "CURRICULUM LEARNING CONFIGURATION\n",
      "============================================================\n",
      "Scale: debug\n",
      "Model: bert-small\n",
      "Samples: 1,000\n",
      "Epochs: 2\n",
      "Runs per strategy: 1\n",
      "Batch size: 16\n",
      "Strategies: 2\n",
      "\n",
      "Model Parameters: 72.0M\n",
      "Hidden size: 512\n",
      "Layers: 8\n",
      "Attention heads: 8\n",
      "\n",
      "Total training steps: 248\n",
      "Estimated time: 0.0 hours\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Quick test with 2 strategies\n",
    "config = debug_config(\n",
    "    strategies=[\"random\", \"reading_level_easy_to_hard\"],\n",
    "    num_runs=1  # Just 1 run for speed\n",
    ")\n",
    "\n",
    "print(\"ğŸ”¬ Configuration:\")\n",
    "config.print_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¬ Data Pipeline initialized\n",
      "ğŸ“ Cache: cache\n",
      "ğŸ”‘ Cache key: 103e0d0a2927\n",
      "ğŸ“Š Evaluator initialized\n",
      "   Confidence level: 95.0%\n",
      "   Effect size threshold: 0.1\n",
      "ğŸ”¬ Experiment started: curriculum_debug_bert-small_20250724_084358\n",
      "ğŸ“Š Logs: results/curriculum_debug_bert-small_20250724_084358\n",
      "ğŸ”¬ Experiment initialized: curriculum_debug_bert-small\n",
      "ğŸ“Š Scale: debug\n",
      "ğŸš€ Device: cpu\n",
      "\n",
      "ğŸ”¬ Starting Curriculum Learning Experiment\n",
      "============================================================\n",
      "CURRICULUM LEARNING CONFIGURATION\n",
      "============================================================\n",
      "Scale: debug\n",
      "Model: bert-small\n",
      "Samples: 1,000\n",
      "Epochs: 2\n",
      "Runs per strategy: 1\n",
      "Batch size: 16\n",
      "Strategies: 2\n",
      "\n",
      "Model Parameters: 72.0M\n",
      "Hidden size: 512\n",
      "Layers: 8\n",
      "Attention heads: 8\n",
      "\n",
      "Total training steps: 248\n",
      "Estimated time: 0.0 hours\n",
      "============================================================\n",
      "\n",
      "ğŸ“Š Preparing data...\n",
      "âš¡ Loading from cache...\n",
      "ğŸ“Š Dataset: 1000 samples\n",
      "ğŸ“ˆ Reading levels: 0.3 - 20.0\n",
      "ğŸ·ï¸  Topics: 14 unique\n",
      "ğŸ“Š Dataset: 800 samples\n",
      "ğŸ“ˆ Reading levels: 0.3 - 20.0\n",
      "ğŸ·ï¸  Topics: 14 unique\n",
      "ğŸ“Š Dataset: 100 samples\n",
      "ğŸ“ˆ Reading levels: 6.2 - 20.0\n",
      "ğŸ·ï¸  Topics: 14 unique\n",
      "ğŸ“Š Dataset: 100 samples\n",
      "ğŸ“ˆ Reading levels: 3.8 - 20.0\n",
      "ğŸ·ï¸  Topics: 14 unique\n",
      "âœ… Data ready:\n",
      "   Train: 800 samples\n",
      "   Validation: 100 samples\n",
      "   Test: 100 samples\n",
      "   Topics: 14\n",
      "   Reading levels: 11.7 Â± 3.9\n",
      "\n",
      "ğŸ¯ Running strategy: random\n",
      "\n",
      "ğŸ¯ Training random (run 1/1)\n",
      "ğŸ§  CurriculumBERT initialized\n",
      "   Model: bert-small\n",
      "   Parameters: 41,274,682\n",
      "   Layers: 8\n",
      "   Hidden size: 512\n",
      "ğŸ“± Model loaded on: cpu\n",
      "ğŸ“š Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training random: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [02:10<00:00,  2.62s/it, loss=10.0505, acc=0.0376]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“š Epoch 2/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training random: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [02:11<00:00,  2.64s/it, loss=9.5774, acc=0.0510]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… random completed\n",
      "   Final loss: 9.5774\n",
      "   Final accuracy: 0.0510\n",
      "ğŸ“ˆ Recorded random: 100 steps\n",
      "\n",
      "ğŸ¯ Running strategy: reading_level_easy_to_hard\n",
      "\n",
      "ğŸ¯ Training reading_level_easy_to_hard (run 1/1)\n",
      "ğŸ§  CurriculumBERT initialized\n",
      "   Model: bert-small\n",
      "   Parameters: 41,274,682\n",
      "   Layers: 8\n",
      "   Hidden size: 512\n",
      "ğŸ“± Model loaded on: cpu\n",
      "ğŸ“š Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training reading_level_easy_to_hard: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [02:09<00:00,  2.60s/it, loss=10.0979, acc=0.0267]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“š Epoch 2/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training reading_level_easy_to_hard: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [02:11<00:00,  2.63s/it, loss=9.6681, acc=0.0526]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… reading_level_easy_to_hard completed\n",
      "   Final loss: 9.6681\n",
      "   Final accuracy: 0.0526\n",
      "ğŸ“ˆ Recorded reading_level_easy_to_hard: 100 steps\n",
      "\n",
      "ğŸ“Š Analyzing results...\n",
      "\n",
      "============================================================\n",
      "ğŸ”¬ CURRICULUM LEARNING RESULTS\n",
      "============================================================\n",
      "Strategies tested: 2\n",
      "Effective strategies: 0\n",
      "Success rate: 0.0%\n",
      "\n",
      "ğŸ† Best strategy: reading_level_easy_to_hard\n",
      "\n",
      "ğŸ’¡ Recommendations:\n",
      "   1. No curriculum strategies showed significant benefits over random ordering.\n",
      "   2. Consider: 1) Larger models, 2) More training data, 3) Different curriculum designs.\n",
      "============================================================\n",
      "ğŸ“„ Report saved: results/report_curriculum_debug_bert-small_20250724_084358.json\n",
      "âœ… Experiment completed: curriculum_debug_bert-small_20250724_084358\n",
      "â±ï¸  Runtime: 0.15 hours\n",
      "ğŸ’¾ Peak memory: 2901.8 MB\n"
     ]
    }
   ],
   "source": [
    "# Run the experiment\n",
    "experiment = Experiment(config)\n",
    "results = experiment.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š Results Analysis\n",
    "\n",
    "The system automatically generates comprehensive analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† EXPERIMENT RESULTS\n",
      "==================================================\n",
      "Strategies tested: 2\n",
      "Effective strategies: 0\n",
      "Success rate: 0.0%\n",
      "\n",
      "ğŸ’¡ Recommendations:\n",
      "   1. No curriculum strategies showed significant benefits over random ordering.\n",
      "   2. Consider: 1) Larger models, 2) More training data, 3) Different curriculum designs.\n"
     ]
    }
   ],
   "source": [
    "# View the results\n",
    "report = results['report']\n",
    "\n",
    "print(\"ğŸ† EXPERIMENT RESULTS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "summary = report['experiment_summary']\n",
    "print(f\"Strategies tested: {summary['total_strategies']}\")\n",
    "print(f\"Effective strategies: {summary['effective_strategies']}\")\n",
    "print(f\"Success rate: {summary['effectiveness_rate']*100:.1f}%\")\n",
    "\n",
    "print(\"\\nğŸ’¡ Recommendations:\")\n",
    "for i, rec in enumerate(report['recommendations'], 1):\n",
    "    print(f\"   {i}. {rec}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ˆ DETAILED RESULTS\n",
      "==================================================\n",
      "\n",
      "reading_level_easy_to_hard:\n",
      "  Final improvement: -0.9%\n",
      "  Effect size: 0.009\n",
      "  Significant: âŒ\n",
      "  Effective: âŒ\n"
     ]
    }
   ],
   "source": [
    "# Detailed results for each strategy\n",
    "print(\"ğŸ“ˆ DETAILED RESULTS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for strategy, details in report['detailed_results'].items():\n",
    "    print(f\"\\n{strategy}:\")\n",
    "    print(f\"  Final improvement: {details['final_improvement_%']:.1f}%\")\n",
    "    print(f\"  Effect size: {details['effect_size']:.3f}\")\n",
    "    print(f\"  Significant: {'âœ…' if details['is_significant'] else 'âŒ'}\")\n",
    "    print(f\"  Effective: {'âœ…' if details['is_effective'] else 'âŒ'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”¬ Scientific Experiment (Pilot Mode)\n",
    "\n",
    "Now let's run a more comprehensive experiment with 10K samples and 5 epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¬ Pilot Study Configuration:\n",
      "============================================================\n",
      "CURRICULUM LEARNING CONFIGURATION\n",
      "============================================================\n",
      "Scale: pilot\n",
      "Model: bert-small\n",
      "Samples: 10,000\n",
      "Epochs: 5\n",
      "Runs per strategy: 2\n",
      "Batch size: 32\n",
      "Strategies: 6\n",
      "\n",
      "Model Parameters: 72.0M\n",
      "Hidden size: 512\n",
      "Layers: 8\n",
      "Attention heads: 8\n",
      "\n",
      "Total training steps: 18,720\n",
      "Estimated time: 0.5 hours\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Pilot experiment with more strategies\n",
    "pilot_config_obj = pilot_config(\n",
    "    model_size=\"bert-small\",\n",
    "    strategies=[\n",
    "        \"random\",\n",
    "        \"reading_level_easy_to_hard\",\n",
    "        \"reading_level_hard_to_easy\", \n",
    "        \"topic_sequential\",\n",
    "        \"topic_largest_first\",\n",
    "        \"hybrid_reading_topic\"\n",
    "    ],\n",
    "    num_runs=2  # 2 runs for statistical reliability\n",
    ")\n",
    "\n",
    "print(\"ğŸ”¬ Pilot Study Configuration:\")\n",
    "pilot_config_obj.print_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸  This will take ~30-60 minutes depending on your hardware\n",
      "ğŸ’¡ Tip: Use GPU if available for faster training\n"
     ]
    }
   ],
   "source": [
    "# Run pilot experiment (this will take longer)\n",
    "print(\"âš ï¸  This will take ~30-60 minutes depending on your hardware\")\n",
    "print(\"ğŸ’¡ Tip: Use GPU if available for faster training\")\n",
    "\n",
    "# Uncomment to run:\n",
    "# pilot_experiment = Experiment(pilot_config_obj)\n",
    "# pilot_results = pilot_experiment.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ Custom Experiments\n",
    "\n",
    "The system is highly customizable. Here are some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“š Testing reading level curricula...\n"
     ]
    }
   ],
   "source": [
    "# Test specific hypothesis: Do reading level curricula work better?\n",
    "reading_level_config = debug_config(\n",
    "    strategies=[\n",
    "        \"random\",\n",
    "        \"reading_level_easy_to_hard\",\n",
    "        \"reading_level_hard_to_easy\",\n",
    "        \"reading_level_staged\"\n",
    "    ],\n",
    "    experiment_name=\"reading_level_focus\"\n",
    ")\n",
    "\n",
    "print(\"ğŸ“š Testing reading level curricula...\")\n",
    "# reading_experiment = Experiment(reading_level_config)\n",
    "# reading_results = reading_experiment.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§  Model size comparison configs ready\n",
      "ğŸ’¡ Tip: Run these to see if curriculum effects change with model size\n"
     ]
    }
   ],
   "source": [
    "# Test model size effects\n",
    "model_comparison_configs = []\n",
    "\n",
    "for model_size in [\"bert-mini\", \"bert-small\"]:\n",
    "    config = debug_config(\n",
    "        model_size=model_size,\n",
    "        strategies=[\"random\", \"reading_level_easy_to_hard\"],\n",
    "        experiment_name=f\"model_size_{model_size}\"\n",
    "    )\n",
    "    model_comparison_configs.append((model_size, config))\n",
    "\n",
    "print(\"ğŸ§  Model size comparison configs ready\")\n",
    "print(\"ğŸ’¡ Tip: Run these to see if curriculum effects change with model size\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š Individual Component Testing\n",
    "\n",
    "You can also test individual components of the system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¬ Data Pipeline initialized\n",
      "ğŸ“ Cache: cache\n",
      "ğŸ”‘ Cache key: f0933621bc13\n",
      "âš¡ Loading from cache...\n",
      "ğŸ“Š Dataset: 200 samples\n",
      "ğŸ“ˆ Reading levels: 1.9 - 20.0\n",
      "ğŸ·ï¸  Topics: 3 unique\n",
      "ğŸ“Š Dataset loaded: 200 samples\n",
      "ğŸ“ˆ Reading levels: 11.8 Â± 3.9\n",
      "ğŸ·ï¸  Topics: 3\n"
     ]
    }
   ],
   "source": [
    "# Test data pipeline\n",
    "from data_pipeline import DataPipeline\n",
    "from config import debug_config\n",
    "\n",
    "config = debug_config(num_samples=200)\n",
    "pipeline = DataPipeline(config)\n",
    "dataset = pipeline.load()\n",
    "\n",
    "print(f\"ğŸ“Š Dataset loaded: {len(dataset)} samples\")\n",
    "stats = dataset.get_statistics()\n",
    "print(f\"ğŸ“ˆ Reading levels: {stats['reading_level_stats']['mean']:.1f} Â± {stats['reading_level_stats']['std']:.1f}\")\n",
    "print(f\"ğŸ·ï¸  Topics: {stats['topic_stats']['num_topics']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ Testing curriculum strategies:\n",
      "\n",
      "random:\n",
      "  First 10 reading levels: ['11.8', '1.9', '13.9', '10.6', '9.1', '7.7', '14.9', '7.3', '20.0', '19.8']\n",
      "  Total samples: 200\n",
      "\n",
      "reading_level_easy_to_hard:\n",
      "  First 10 reading levels: ['1.9', '2.9', '3.7', '3.8', '4.2', '4.7', '5.4', '5.5', '5.5', '6.0']\n",
      "  Total samples: 200\n",
      "\n",
      "topic_sequential:\n",
      "  First 10 reading levels: ['10.5', '10.8', '7.4', '11.1', '12.4', '3.7', '13.1', '10.4', '13.3', '9.5']\n",
      "  Total samples: 200\n"
     ]
    }
   ],
   "source": [
    "# Test curriculum strategies\n",
    "strategies_to_test = ['random', 'reading_level_easy_to_hard', 'topic_sequential']\n",
    "\n",
    "print(\"ğŸ¯ Testing curriculum strategies:\")\n",
    "for strategy in strategies_to_test:\n",
    "    order = dataset.get_curriculum_order(strategy)\n",
    "    \n",
    "    # Show first 10 indices and their reading levels\n",
    "    first_10_indices = order[:10]\n",
    "    reading_levels = [dataset.reading_levels[i] for i in first_10_indices]\n",
    "    \n",
    "    print(f\"\\n{strategy}:\")\n",
    "    print(f\"  First 10 reading levels: {[f'{rl:.1f}' for rl in reading_levels]}\")\n",
    "    print(f\"  Total samples: {len(order)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§  CurriculumBERT initialized\n",
      "   Model: bert-small\n",
      "   Parameters: 41,274,682\n",
      "   Layers: 8\n",
      "   Hidden size: 512\n",
      "ğŸ“± Model loaded on: cpu\n",
      "ğŸ§  Model created:\n",
      "   Parameters: 41,274,682\n",
      "   Size: 157.5 MB\n",
      "   Stable: âœ…\n"
     ]
    }
   ],
   "source": [
    "# Test model creation\n",
    "from model import create_model\n",
    "\n",
    "model = create_model(config, device=\"cpu\")\n",
    "metrics = model.get_metrics()\n",
    "\n",
    "print(f\"ğŸ§  Model created:\")\n",
    "print(f\"   Parameters: {metrics.total_parameters:,}\")\n",
    "print(f\"   Size: {metrics.model_size_mb:.1f} MB\")\n",
    "print(f\"   Stable: {'âœ…' if metrics.is_stable else 'âŒ'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“ Research Tips\n",
    "\n",
    "**For Quick Iteration:**\n",
    "- Use `debug_config()` with 1-2 strategies\n",
    "- Set `num_runs=1` for speed\n",
    "- Use `bert-mini` model size\n",
    "\n",
    "**For Serious Research:**\n",
    "- Use `scientific_config()` for publication quality\n",
    "- Set `use_wandb=True` for experiment tracking\n",
    "- Use `bert-small` or `bert-base` models\n",
    "- Run multiple seeds/runs for statistical validity\n",
    "\n",
    "**Command Line Alternative:**\n",
    "```bash\n",
    "# Run from terminal for long experiments\n",
    "python experiment.py --scale=scientific --use-wandb\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“ˆ Next Steps\n",
    "\n",
    "1. **Start with debug experiments** to validate your setup\n",
    "2. **Run pilot studies** to identify promising strategies  \n",
    "3. **Scale to scientific experiments** for publication-ready results\n",
    "4. **Customize strategies** based on your specific research questions\n",
    "\n",
    "The streamlined system makes it easy to go from idea to results! ğŸš€"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ Full Scientific Experiment\n",
    "\n",
    "**Complete curriculum learning study with all 9 strategies at scientific scale**\n",
    "\n",
    "This section runs the comprehensive experiment designed for publication-quality results:\n",
    "- **50,000** training samples (scientific scale)\n",
    "- **15 epochs** for proper convergence\n",
    "- **All 9 curriculum strategies** tested\n",
    "- **5 runs per strategy** for statistical reliability\n",
    "- **Statistical analysis** with effect sizes and significance testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¬ FULL SCIENTIFIC EXPERIMENT CONFIGURATION\n",
      "============================================================\n",
      "============================================================\n",
      "CURRICULUM LEARNING CONFIGURATION\n",
      "============================================================\n",
      "Scale: scientific\n",
      "Model: bert-small\n",
      "Samples: 50,000\n",
      "Epochs: 15\n",
      "Runs per strategy: 5\n",
      "Batch size: 32\n",
      "Strategies: 9\n",
      "\n",
      "Model Parameters: 72.0M\n",
      "Hidden size: 512\n",
      "Layers: 8\n",
      "Attention heads: 8\n",
      "\n",
      "Total training steps: 1,054,350\n",
      "Estimated time: 29.3 hours\n",
      "============================================================\n",
      "\n",
      "âš ï¸  IMPORTANT:\n",
      "   â€¢ Total experiments: 45\n",
      "   â€¢ Estimated time: 22.5 hours\n",
      "   â€¢ This will use significant computational resources\n",
      "   â€¢ Results will be saved with experiment tracking\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Configure the full scientific experiment\n",
    "full_experiment_config = scientific_config(\n",
    "    model_size=\"bert-small\",\n",
    "    strategies=[\n",
    "        \"random\",                      # Baseline\n",
    "        \"reading_level_easy_to_hard\",  # Reading level curricula\n",
    "        \"reading_level_hard_to_easy\",\n",
    "        \"reading_level_staged\",\n",
    "        \"topic_sequential\",            # Topic-based curricula  \n",
    "        \"topic_interleaved\",\n",
    "        \"topic_largest_first\",\n",
    "        \"hybrid_reading_topic\",        # Hybrid approaches\n",
    "        \"hybrid_topic_reading\"\n",
    "    ],\n",
    "    experiment_name=\"full_scientific_study\",\n",
    "    use_wandb=True,  # Enable experiment tracking\n",
    "    num_runs=5       # 5 runs per strategy for statistical reliability\n",
    ")\n",
    "\n",
    "print(\"ğŸ”¬ FULL SCIENTIFIC EXPERIMENT CONFIGURATION\")\n",
    "print(\"=\" * 60)\n",
    "full_experiment_config.print_summary()\n",
    "\n",
    "total_experiments = len(full_experiment_config.strategies) * full_experiment_config.num_runs\n",
    "estimated_hours = total_experiments * 0.5  # Rough estimate\n",
    "\n",
    "print(f\"\\nâš ï¸  IMPORTANT:\")\n",
    "print(f\"   â€¢ Total experiments: {total_experiments}\")\n",
    "print(f\"   â€¢ Estimated time: {estimated_hours:.1f} hours\")\n",
    "print(f\"   â€¢ This will use significant computational resources\")\n",
    "print(f\"   â€¢ Results will be saved with experiment tracking\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Starting Full Scientific Curriculum Learning Experiment...\n",
      "ğŸ’¡ This will take several hours - consider running overnight\n",
      "\n",
      "ğŸ”¬ Data Pipeline initialized\n",
      "ğŸ“ Cache: cache\n",
      "ğŸ”‘ Cache key: 26e5a364c65c\n",
      "ğŸ“Š Evaluator initialized\n",
      "   Confidence level: 95.0%\n",
      "   Effect size threshold: 0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "  Â·Â·Â·Â·Â·Â·Â·Â·\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /Users/petergrabowski/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmythander889\u001b[0m (\u001b[33mmythander889-pxn\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/petergrabowski/work/research/wandb/run-20250724_085518-2a64xv61</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mythander889-pxn/curriculum-learning/runs/2a64xv61' target=\"_blank\">full_scientific_study_20250724_085256</a></strong> to <a href='https://wandb.ai/mythander889-pxn/curriculum-learning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mythander889-pxn/curriculum-learning' target=\"_blank\">https://wandb.ai/mythander889-pxn/curriculum-learning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mythander889-pxn/curriculum-learning/runs/2a64xv61' target=\"_blank\">https://wandb.ai/mythander889-pxn/curriculum-learning/runs/2a64xv61</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`trust_remote_code` is not supported anymore.\n",
      "Please check that the Hugging Face dataset 'wikitext' isn't based on a loading script and remove `trust_remote_code`.\n",
      "If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸŒ W&B logging enabled\n",
      "ğŸ”¬ Experiment started: full_scientific_study_20250724_085256\n",
      "ğŸ“Š Logs: results/full_scientific_study_20250724_085256\n",
      "ğŸ”¬ Experiment initialized: full_scientific_study\n",
      "ğŸ“Š Scale: scientific\n",
      "ğŸš€ Device: cpu\n",
      "ğŸ“Š Running comprehensive curriculum learning study...\n",
      "\n",
      "ğŸ”¬ Starting Curriculum Learning Experiment\n",
      "============================================================\n",
      "CURRICULUM LEARNING CONFIGURATION\n",
      "============================================================\n",
      "Scale: scientific\n",
      "Model: bert-small\n",
      "Samples: 50,000\n",
      "Epochs: 15\n",
      "Runs per strategy: 5\n",
      "Batch size: 32\n",
      "Strategies: 9\n",
      "\n",
      "Model Parameters: 72.0M\n",
      "Hidden size: 512\n",
      "Layers: 8\n",
      "Attention heads: 8\n",
      "\n",
      "Total training steps: 1,054,350\n",
      "Estimated time: 29.3 hours\n",
      "============================================================\n",
      "\n",
      "ğŸ“Š Preparing data...\n",
      "ğŸ”„ Creating dataset from scratch...\n",
      "ğŸ“š Loading raw datasets...\n",
      "  ğŸ“– WikiText (30000 samples)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`trust_remote_code` is not supported anymore.\n",
      "Please check that the Hugging Face dataset 'ag_news' isn't based on a loading script and remove `trust_remote_code`.\n",
      "If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    âœ… 30000 samples\n",
      "  ğŸ“° AG News (10000 samples)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`trust_remote_code` is not supported anymore.\n",
      "Please check that the Hugging Face dataset 'imdb' isn't based on a loading script and remove `trust_remote_code`.\n",
      "If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    âœ… 10000 samples\n",
      "  ğŸ¬ IMDB (10000 samples)...\n",
      "    âœ… 10000 samples\n",
      "ğŸ“Š Total: 50000 samples\n",
      "ğŸ“Š Computing reading levels...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading levels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [00:08<00:00, 6107.50it/s]\n",
      "2025-07-24 08:55:50,195 - BERTopic - Embedding - Transforming documents to embeddings.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§  Discovering topics with BERTopic...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e0da3ff64f642ab887cd9aa83c53eaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1563 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-24 09:01:15,108 - BERTopic - Embedding - Completed âœ“\n",
      "2025-07-24 09:01:15,109 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2025-07-24 09:01:32,476 - BERTopic - Dimensionality - Completed âœ“\n",
      "2025-07-24 09:01:32,477 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "2025-07-24 09:01:34,959 - BERTopic - Cluster - Completed âœ“\n",
      "2025-07-24 09:01:34,967 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2025-07-24 09:01:45,254 - BERTopic - Representation - Completed âœ“\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… 470 topics discovered\n",
      "âœ… Dataset created: 50000 samples, 470 topics\n",
      "ğŸ“Š Dataset: 50000 samples\n",
      "ğŸ“ˆ Reading levels: 0.0 - 20.0\n",
      "ğŸ·ï¸  Topics: 470 unique\n",
      "ğŸ“Š Dataset: 40000 samples\n",
      "ğŸ“ˆ Reading levels: 0.0 - 20.0\n",
      "ğŸ·ï¸  Topics: 470 unique\n",
      "ğŸ“Š Dataset: 5000 samples\n",
      "ğŸ“ˆ Reading levels: 0.0 - 20.0\n",
      "ğŸ·ï¸  Topics: 427 unique\n",
      "ğŸ“Š Dataset: 5000 samples\n",
      "ğŸ“ˆ Reading levels: 0.0 - 20.0\n",
      "ğŸ·ï¸  Topics: 402 unique\n",
      "âœ… Data ready:\n",
      "   Train: 40000 samples\n",
      "   Validation: 5000 samples\n",
      "   Test: 5000 samples\n",
      "   Topics: 470\n",
      "   Reading levels: 11.8 Â± 3.9\n",
      "\n",
      "ğŸ¯ Running strategy: random\n",
      "\n",
      "ğŸ¯ Training random (run 1/5)\n",
      "ğŸ§  CurriculumBERT initialized\n",
      "   Model: bert-small\n",
      "   Parameters: 41,274,682\n",
      "   Layers: 8\n",
      "   Hidden size: 512\n",
      "ğŸ“± Model loaded on: cpu\n",
      "ğŸ“š Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training random: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1250/1250 [2:34:24<00:00,  7.41s/it, loss=7.0558, acc=0.0664]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“š Epoch 2/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training random: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1250/1250 [2:29:15<00:00,  7.16s/it, loss=7.2572, acc=0.0511]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“š Epoch 3/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training random: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1250/1250 [2:28:18<00:00,  7.12s/it, loss=7.0546, acc=0.0444]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“š Epoch 4/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training random: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1250/1250 [2:29:47<00:00,  7.19s/it, loss=7.0157, acc=0.0349]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“š Epoch 5/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training random: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1250/1250 [2:41:08<00:00,  7.74s/it, loss=7.2473, acc=0.0289]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“š Epoch 6/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training random: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1250/1250 [2:39:06<00:00,  7.64s/it, loss=7.3569, acc=0.0440]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“š Epoch 7/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training random: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1250/1250 [2:34:41<00:00,  7.43s/it, loss=7.2128, acc=0.0322]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“š Epoch 8/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training random: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1250/1250 [2:36:07<00:00,  7.49s/it, loss=7.0601, acc=0.0288]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“š Epoch 9/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training random: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1250/1250 [10:01:09<00:00, 28.86s/it, loss=6.9942, acc=0.0493]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“š Epoch 10/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training random:  16%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                               | 201/1250 [21:44<1:53:28,  6.49s/it, loss=7.1965, acc=0.0443]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âŒ Error in random run 0: [Errno 2] No such file or directory: 'results/full_scientific_study_20250724_085256/metrics.jsonl'\n",
      "\n",
      "ğŸ¯ Training random (run 2/5)\n",
      "ğŸ§  CurriculumBERT initialized\n",
      "   Model: bert-small\n",
      "   Parameters: 41,274,682\n",
      "   Layers: 8\n",
      "   Hidden size: 512\n",
      "ğŸ“± Model loaded on: cpu\n",
      "ğŸ“š Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training random:   0%|                                                                                                                 | 0/1250 [00:06<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âŒ Error in random run 1: [Errno 2] No such file or directory: 'results/full_scientific_study_20250724_085256/metrics.jsonl'\n",
      "\n",
      "ğŸ¯ Training random (run 3/5)\n",
      "ğŸ§  CurriculumBERT initialized\n",
      "   Model: bert-small\n",
      "   Parameters: 41,274,682\n",
      "   Layers: 8\n",
      "   Hidden size: 512\n",
      "ğŸ“± Model loaded on: cpu\n",
      "ğŸ“š Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training random:   0%|                                                                                                                 | 0/1250 [00:06<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âŒ Error in random run 2: [Errno 2] No such file or directory: 'results/full_scientific_study_20250724_085256/metrics.jsonl'\n",
      "\n",
      "ğŸ¯ Training random (run 4/5)\n",
      "ğŸ§  CurriculumBERT initialized\n",
      "   Model: bert-small\n",
      "   Parameters: 41,274,682\n",
      "   Layers: 8\n",
      "   Hidden size: 512\n",
      "ğŸ“± Model loaded on: cpu\n",
      "ğŸ“š Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training random:   0%|                                                                                                                 | 0/1250 [00:06<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âŒ Error in random run 3: [Errno 2] No such file or directory: 'results/full_scientific_study_20250724_085256/metrics.jsonl'\n",
      "\n",
      "ğŸ¯ Training random (run 5/5)\n",
      "ğŸ§  CurriculumBERT initialized\n",
      "   Model: bert-small\n",
      "   Parameters: 41,274,682\n",
      "   Layers: 8\n",
      "   Hidden size: 512\n",
      "ğŸ“± Model loaded on: cpu\n",
      "ğŸ“š Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training random:   0%|                                                                                                                 | 0/1250 [00:06<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âŒ Error in random run 4: [Errno 2] No such file or directory: 'results/full_scientific_study_20250724_085256/metrics.jsonl'\n",
      "âš ï¸  No successful runs for random\n",
      "\n",
      "ğŸ¯ Running strategy: reading_level_easy_to_hard\n",
      "\n",
      "ğŸ¯ Training reading_level_easy_to_hard (run 1/5)\n",
      "ğŸ§  CurriculumBERT initialized\n",
      "   Model: bert-small\n",
      "   Parameters: 41,274,682\n",
      "   Layers: 8\n",
      "   Hidden size: 512\n",
      "ğŸ“± Model loaded on: cpu\n",
      "ğŸ“š Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training reading_level_easy_to_hard:   0%|                                                                                             | 0/1250 [00:06<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âŒ Error in reading_level_easy_to_hard run 0: [Errno 2] No such file or directory: 'results/full_scientific_study_20250724_085256/metrics.jsonl'\n",
      "\n",
      "ğŸ¯ Training reading_level_easy_to_hard (run 2/5)\n",
      "ğŸ§  CurriculumBERT initialized\n",
      "   Model: bert-small\n",
      "   Parameters: 41,274,682\n",
      "   Layers: 8\n",
      "   Hidden size: 512\n",
      "ğŸ“± Model loaded on: cpu\n",
      "ğŸ“š Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training reading_level_easy_to_hard:   0%|                                                                                             | 0/1250 [00:06<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âŒ Error in reading_level_easy_to_hard run 1: [Errno 2] No such file or directory: 'results/full_scientific_study_20250724_085256/metrics.jsonl'\n",
      "\n",
      "ğŸ¯ Training reading_level_easy_to_hard (run 3/5)\n",
      "ğŸ§  CurriculumBERT initialized\n",
      "   Model: bert-small\n",
      "   Parameters: 41,274,682\n",
      "   Layers: 8\n",
      "   Hidden size: 512\n",
      "ğŸ“± Model loaded on: cpu\n",
      "ğŸ“š Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training reading_level_easy_to_hard:   0%|                                                                                             | 0/1250 [00:06<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âŒ Error in reading_level_easy_to_hard run 2: [Errno 2] No such file or directory: 'results/full_scientific_study_20250724_085256/metrics.jsonl'\n",
      "\n",
      "ğŸ¯ Training reading_level_easy_to_hard (run 4/5)\n",
      "ğŸ§  CurriculumBERT initialized\n",
      "   Model: bert-small\n",
      "   Parameters: 41,274,682\n",
      "   Layers: 8\n",
      "   Hidden size: 512\n",
      "ğŸ“± Model loaded on: cpu\n",
      "ğŸ“š Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training reading_level_easy_to_hard:   0%|                                                                                             | 0/1250 [00:06<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âŒ Error in reading_level_easy_to_hard run 3: [Errno 2] No such file or directory: 'results/full_scientific_study_20250724_085256/metrics.jsonl'\n",
      "\n",
      "ğŸ¯ Training reading_level_easy_to_hard (run 5/5)\n",
      "ğŸ§  CurriculumBERT initialized\n",
      "   Model: bert-small\n",
      "   Parameters: 41,274,682\n",
      "   Layers: 8\n",
      "   Hidden size: 512\n",
      "ğŸ“± Model loaded on: cpu\n",
      "ğŸ“š Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training reading_level_easy_to_hard:   0%|                                                                                             | 0/1250 [00:06<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âŒ Error in reading_level_easy_to_hard run 4: [Errno 2] No such file or directory: 'results/full_scientific_study_20250724_085256/metrics.jsonl'\n",
      "âš ï¸  No successful runs for reading_level_easy_to_hard\n",
      "\n",
      "ğŸ¯ Running strategy: reading_level_hard_to_easy\n",
      "\n",
      "ğŸ¯ Training reading_level_hard_to_easy (run 1/5)\n",
      "ğŸ§  CurriculumBERT initialized\n",
      "   Model: bert-small\n",
      "   Parameters: 41,274,682\n",
      "   Layers: 8\n",
      "   Hidden size: 512\n",
      "ğŸ“± Model loaded on: cpu\n",
      "ğŸ“š Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training reading_level_hard_to_easy:   0%|                                                                                             | 0/1250 [00:06<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âŒ Error in reading_level_hard_to_easy run 0: [Errno 2] No such file or directory: 'results/full_scientific_study_20250724_085256/metrics.jsonl'\n",
      "\n",
      "ğŸ¯ Training reading_level_hard_to_easy (run 2/5)\n",
      "ğŸ§  CurriculumBERT initialized\n",
      "   Model: bert-small\n",
      "   Parameters: 41,274,682\n",
      "   Layers: 8\n",
      "   Hidden size: 512\n",
      "ğŸ“± Model loaded on: cpu\n",
      "ğŸ“š Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training reading_level_hard_to_easy:   0%|                                                                                             | 0/1250 [00:06<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âŒ Error in reading_level_hard_to_easy run 1: [Errno 2] No such file or directory: 'results/full_scientific_study_20250724_085256/metrics.jsonl'\n",
      "\n",
      "ğŸ¯ Training reading_level_hard_to_easy (run 3/5)\n",
      "ğŸ§  CurriculumBERT initialized\n",
      "   Model: bert-small\n",
      "   Parameters: 41,274,682\n",
      "   Layers: 8\n",
      "   Hidden size: 512\n",
      "ğŸ“± Model loaded on: cpu\n",
      "ğŸ“š Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training reading_level_hard_to_easy:   0%|                                                                                             | 0/1250 [00:06<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âŒ Error in reading_level_hard_to_easy run 2: [Errno 2] No such file or directory: 'results/full_scientific_study_20250724_085256/metrics.jsonl'\n",
      "\n",
      "ğŸ¯ Training reading_level_hard_to_easy (run 4/5)\n",
      "ğŸ§  CurriculumBERT initialized\n",
      "   Model: bert-small\n",
      "   Parameters: 41,274,682\n",
      "   Layers: 8\n",
      "   Hidden size: 512\n",
      "ğŸ“± Model loaded on: cpu\n",
      "ğŸ“š Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training reading_level_hard_to_easy:   0%|                                                                                             | 0/1250 [00:06<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âŒ Error in reading_level_hard_to_easy run 3: [Errno 2] No such file or directory: 'results/full_scientific_study_20250724_085256/metrics.jsonl'\n",
      "\n",
      "ğŸ¯ Training reading_level_hard_to_easy (run 5/5)\n",
      "ğŸ§  CurriculumBERT initialized\n",
      "   Model: bert-small\n",
      "   Parameters: 41,274,682\n",
      "   Layers: 8\n",
      "   Hidden size: 512\n",
      "ğŸ“± Model loaded on: cpu\n",
      "ğŸ“š Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training reading_level_hard_to_easy:   0%|                                                                                             | 0/1250 [00:06<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âŒ Error in reading_level_hard_to_easy run 4: [Errno 2] No such file or directory: 'results/full_scientific_study_20250724_085256/metrics.jsonl'\n",
      "âš ï¸  No successful runs for reading_level_hard_to_easy\n",
      "\n",
      "ğŸ¯ Running strategy: reading_level_staged\n",
      "\n",
      "ğŸ¯ Training reading_level_staged (run 1/5)\n",
      "ğŸ§  CurriculumBERT initialized\n",
      "   Model: bert-small\n",
      "   Parameters: 41,274,682\n",
      "   Layers: 8\n",
      "   Hidden size: 512\n",
      "ğŸ“± Model loaded on: cpu\n",
      "ğŸ“š Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training reading_level_staged:   0%|                                                                                                   | 0/1250 [00:07<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âŒ Error in reading_level_staged run 0: [Errno 2] No such file or directory: 'results/full_scientific_study_20250724_085256/metrics.jsonl'\n",
      "\n",
      "ğŸ¯ Training reading_level_staged (run 2/5)\n",
      "ğŸ§  CurriculumBERT initialized\n",
      "   Model: bert-small\n",
      "   Parameters: 41,274,682\n",
      "   Layers: 8\n",
      "   Hidden size: 512\n",
      "ğŸ“± Model loaded on: cpu\n",
      "ğŸ“š Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training reading_level_staged:   0%|                                                                                                   | 0/1250 [00:07<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âŒ Error in reading_level_staged run 1: [Errno 2] No such file or directory: 'results/full_scientific_study_20250724_085256/metrics.jsonl'\n",
      "\n",
      "ğŸ¯ Training reading_level_staged (run 3/5)\n",
      "ğŸ§  CurriculumBERT initialized\n",
      "   Model: bert-small\n",
      "   Parameters: 41,274,682\n",
      "   Layers: 8\n",
      "   Hidden size: 512\n",
      "ğŸ“± Model loaded on: cpu\n",
      "ğŸ“š Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training reading_level_staged:   0%|                                                                                                   | 0/1250 [00:08<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âŒ Error in reading_level_staged run 2: [Errno 2] No such file or directory: 'results/full_scientific_study_20250724_085256/metrics.jsonl'\n",
      "\n",
      "ğŸ¯ Training reading_level_staged (run 4/5)\n",
      "ğŸ§  CurriculumBERT initialized\n",
      "   Model: bert-small\n",
      "   Parameters: 41,274,682\n",
      "   Layers: 8\n",
      "   Hidden size: 512\n",
      "ğŸ“± Model loaded on: cpu\n",
      "ğŸ“š Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training reading_level_staged:   0%|                                                                                                   | 0/1250 [00:07<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âŒ Error in reading_level_staged run 3: [Errno 2] No such file or directory: 'results/full_scientific_study_20250724_085256/metrics.jsonl'\n",
      "\n",
      "ğŸ¯ Training reading_level_staged (run 5/5)\n",
      "ğŸ§  CurriculumBERT initialized\n",
      "   Model: bert-small\n",
      "   Parameters: 41,274,682\n",
      "   Layers: 8\n",
      "   Hidden size: 512\n",
      "ğŸ“± Model loaded on: cpu\n",
      "ğŸ“š Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training reading_level_staged:   0%|                                                                                                   | 0/1250 [00:07<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âŒ Error in reading_level_staged run 4: [Errno 2] No such file or directory: 'results/full_scientific_study_20250724_085256/metrics.jsonl'\n",
      "âš ï¸  No successful runs for reading_level_staged\n",
      "\n",
      "ğŸ¯ Running strategy: topic_sequential\n",
      "\n",
      "ğŸ¯ Training topic_sequential (run 1/5)\n",
      "ğŸ§  CurriculumBERT initialized\n",
      "   Model: bert-small\n",
      "   Parameters: 41,274,682\n",
      "   Layers: 8\n",
      "   Hidden size: 512\n",
      "ğŸ“± Model loaded on: cpu\n",
      "ğŸ“š Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training topic_sequential:   0%|                                                                                                       | 0/1250 [00:07<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âŒ Error in topic_sequential run 0: [Errno 2] No such file or directory: 'results/full_scientific_study_20250724_085256/metrics.jsonl'\n",
      "\n",
      "ğŸ¯ Training topic_sequential (run 2/5)\n",
      "ğŸ§  CurriculumBERT initialized\n",
      "   Model: bert-small\n",
      "   Parameters: 41,274,682\n",
      "   Layers: 8\n",
      "   Hidden size: 512\n",
      "ğŸ“± Model loaded on: cpu\n",
      "ğŸ“š Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training topic_sequential:   0%|                                                                                                       | 0/1250 [00:07<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âŒ Error in topic_sequential run 1: [Errno 2] No such file or directory: 'results/full_scientific_study_20250724_085256/metrics.jsonl'\n",
      "\n",
      "ğŸ¯ Training topic_sequential (run 3/5)\n",
      "ğŸ§  CurriculumBERT initialized\n",
      "   Model: bert-small\n",
      "   Parameters: 41,274,682\n",
      "   Layers: 8\n",
      "   Hidden size: 512\n",
      "ğŸ“± Model loaded on: cpu\n",
      "ğŸ“š Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training topic_sequential:   0%|                                                                                                       | 0/1250 [00:07<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âŒ Error in topic_sequential run 2: [Errno 2] No such file or directory: 'results/full_scientific_study_20250724_085256/metrics.jsonl'\n",
      "\n",
      "ğŸ¯ Training topic_sequential (run 4/5)\n",
      "ğŸ§  CurriculumBERT initialized\n",
      "   Model: bert-small\n",
      "   Parameters: 41,274,682\n",
      "   Layers: 8\n",
      "   Hidden size: 512\n",
      "ğŸ“± Model loaded on: cpu\n",
      "ğŸ“š Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training topic_sequential:   0%|                                                                                                       | 0/1250 [00:07<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âŒ Error in topic_sequential run 3: [Errno 2] No such file or directory: 'results/full_scientific_study_20250724_085256/metrics.jsonl'\n",
      "\n",
      "ğŸ¯ Training topic_sequential (run 5/5)\n",
      "ğŸ§  CurriculumBERT initialized\n",
      "   Model: bert-small\n",
      "   Parameters: 41,274,682\n",
      "   Layers: 8\n",
      "   Hidden size: 512\n",
      "ğŸ“± Model loaded on: cpu\n",
      "ğŸ“š Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training topic_sequential:   0%|                                                                                                       | 0/1250 [00:07<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âŒ Error in topic_sequential run 4: [Errno 2] No such file or directory: 'results/full_scientific_study_20250724_085256/metrics.jsonl'\n",
      "âš ï¸  No successful runs for topic_sequential\n",
      "\n",
      "ğŸ¯ Running strategy: topic_interleaved\n",
      "\n",
      "ğŸ¯ Training topic_interleaved (run 1/5)\n",
      "ğŸ§  CurriculumBERT initialized\n",
      "   Model: bert-small\n",
      "   Parameters: 41,274,682\n",
      "   Layers: 8\n",
      "   Hidden size: 512\n",
      "ğŸ“± Model loaded on: cpu\n",
      "ğŸ“š Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training topic_interleaved:   0%|                                                                                                      | 0/1250 [00:07<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âŒ Error in topic_interleaved run 0: [Errno 2] No such file or directory: 'results/full_scientific_study_20250724_085256/metrics.jsonl'\n",
      "\n",
      "ğŸ¯ Training topic_interleaved (run 2/5)\n",
      "ğŸ§  CurriculumBERT initialized\n",
      "   Model: bert-small\n",
      "   Parameters: 41,274,682\n",
      "   Layers: 8\n",
      "   Hidden size: 512\n",
      "ğŸ“± Model loaded on: cpu\n",
      "ğŸ“š Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training topic_interleaved:   0%|                                                                                                      | 0/1250 [00:08<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âŒ Error in topic_interleaved run 1: [Errno 2] No such file or directory: 'results/full_scientific_study_20250724_085256/metrics.jsonl'\n",
      "\n",
      "ğŸ¯ Training topic_interleaved (run 3/5)\n",
      "ğŸ§  CurriculumBERT initialized\n",
      "   Model: bert-small\n",
      "   Parameters: 41,274,682\n",
      "   Layers: 8\n",
      "   Hidden size: 512\n",
      "ğŸ“± Model loaded on: cpu\n",
      "ğŸ“š Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training topic_interleaved:   0%|                                                                                                      | 0/1250 [00:07<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âŒ Error in topic_interleaved run 2: [Errno 2] No such file or directory: 'results/full_scientific_study_20250724_085256/metrics.jsonl'\n",
      "\n",
      "ğŸ¯ Training topic_interleaved (run 4/5)\n",
      "ğŸ§  CurriculumBERT initialized\n",
      "   Model: bert-small\n",
      "   Parameters: 41,274,682\n",
      "   Layers: 8\n",
      "   Hidden size: 512\n",
      "ğŸ“± Model loaded on: cpu\n",
      "ğŸ“š Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training topic_interleaved:   0%|                                                                                                      | 0/1250 [00:06<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âŒ Error in topic_interleaved run 3: [Errno 2] No such file or directory: 'results/full_scientific_study_20250724_085256/metrics.jsonl'\n",
      "\n",
      "ğŸ¯ Training topic_interleaved (run 5/5)\n",
      "ğŸ§  CurriculumBERT initialized\n",
      "   Model: bert-small\n",
      "   Parameters: 41,274,682\n",
      "   Layers: 8\n",
      "   Hidden size: 512\n",
      "ğŸ“± Model loaded on: cpu\n",
      "ğŸ“š Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training topic_interleaved:   0%|                                                                                                      | 0/1250 [00:07<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âŒ Error in topic_interleaved run 4: [Errno 2] No such file or directory: 'results/full_scientific_study_20250724_085256/metrics.jsonl'\n",
      "âš ï¸  No successful runs for topic_interleaved\n",
      "\n",
      "ğŸ¯ Running strategy: topic_largest_first\n",
      "\n",
      "ğŸ¯ Training topic_largest_first (run 1/5)\n",
      "ğŸ§  CurriculumBERT initialized\n",
      "   Model: bert-small\n",
      "   Parameters: 41,274,682\n",
      "   Layers: 8\n",
      "   Hidden size: 512\n",
      "ğŸ“± Model loaded on: cpu\n",
      "ğŸ“š Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training topic_largest_first:   0%|                                                                                                    | 0/1250 [00:06<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âŒ Error in topic_largest_first run 0: [Errno 2] No such file or directory: 'results/full_scientific_study_20250724_085256/metrics.jsonl'\n",
      "\n",
      "ğŸ¯ Training topic_largest_first (run 2/5)\n",
      "ğŸ§  CurriculumBERT initialized\n",
      "   Model: bert-small\n",
      "   Parameters: 41,274,682\n",
      "   Layers: 8\n",
      "   Hidden size: 512\n",
      "ğŸ“± Model loaded on: cpu\n",
      "ğŸ“š Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training topic_largest_first:   0%|                                                                                                    | 0/1250 [00:06<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âŒ Error in topic_largest_first run 1: [Errno 2] No such file or directory: 'results/full_scientific_study_20250724_085256/metrics.jsonl'\n",
      "\n",
      "ğŸ¯ Training topic_largest_first (run 3/5)\n",
      "ğŸ§  CurriculumBERT initialized\n",
      "   Model: bert-small\n",
      "   Parameters: 41,274,682\n",
      "   Layers: 8\n",
      "   Hidden size: 512\n",
      "ğŸ“± Model loaded on: cpu\n",
      "ğŸ“š Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training topic_largest_first:   0%|                                                                                                    | 0/1250 [00:06<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âŒ Error in topic_largest_first run 2: [Errno 2] No such file or directory: 'results/full_scientific_study_20250724_085256/metrics.jsonl'\n",
      "\n",
      "ğŸ¯ Training topic_largest_first (run 4/5)\n",
      "ğŸ§  CurriculumBERT initialized\n",
      "   Model: bert-small\n",
      "   Parameters: 41,274,682\n",
      "   Layers: 8\n",
      "   Hidden size: 512\n",
      "ğŸ“± Model loaded on: cpu\n",
      "ğŸ“š Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training topic_largest_first:   0%|                                                                                                    | 0/1250 [00:06<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âŒ Error in topic_largest_first run 3: [Errno 2] No such file or directory: 'results/full_scientific_study_20250724_085256/metrics.jsonl'\n",
      "\n",
      "ğŸ¯ Training topic_largest_first (run 5/5)\n",
      "ğŸ§  CurriculumBERT initialized\n",
      "   Model: bert-small\n",
      "   Parameters: 41,274,682\n",
      "   Layers: 8\n",
      "   Hidden size: 512\n",
      "ğŸ“± Model loaded on: cpu\n",
      "ğŸ“š Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training topic_largest_first:   0%|                                                                                                    | 0/1250 [00:06<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âŒ Error in topic_largest_first run 4: [Errno 2] No such file or directory: 'results/full_scientific_study_20250724_085256/metrics.jsonl'\n",
      "âš ï¸  No successful runs for topic_largest_first\n",
      "\n",
      "ğŸ¯ Running strategy: hybrid_reading_topic\n",
      "\n",
      "ğŸ¯ Training hybrid_reading_topic (run 1/5)\n",
      "ğŸ§  CurriculumBERT initialized\n",
      "   Model: bert-small\n",
      "   Parameters: 41,274,682\n",
      "   Layers: 8\n",
      "   Hidden size: 512\n",
      "ğŸ“± Model loaded on: cpu\n",
      "ğŸ“š Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training hybrid_reading_topic:   0%|                                                                                                   | 0/1250 [00:06<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âŒ Error in hybrid_reading_topic run 0: [Errno 2] No such file or directory: 'results/full_scientific_study_20250724_085256/metrics.jsonl'\n",
      "\n",
      "ğŸ¯ Training hybrid_reading_topic (run 2/5)\n",
      "ğŸ§  CurriculumBERT initialized\n",
      "   Model: bert-small\n",
      "   Parameters: 41,274,682\n",
      "   Layers: 8\n",
      "   Hidden size: 512\n",
      "ğŸ“± Model loaded on: cpu\n",
      "ğŸ“š Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training hybrid_reading_topic:   0%|                                                                                                   | 0/1250 [00:06<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âŒ Error in hybrid_reading_topic run 1: [Errno 2] No such file or directory: 'results/full_scientific_study_20250724_085256/metrics.jsonl'\n",
      "\n",
      "ğŸ¯ Training hybrid_reading_topic (run 3/5)\n",
      "ğŸ§  CurriculumBERT initialized\n",
      "   Model: bert-small\n",
      "   Parameters: 41,274,682\n",
      "   Layers: 8\n",
      "   Hidden size: 512\n",
      "ğŸ“± Model loaded on: cpu\n",
      "ğŸ“š Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training hybrid_reading_topic:   0%|                                                                                                   | 0/1250 [00:06<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âŒ Error in hybrid_reading_topic run 2: [Errno 2] No such file or directory: 'results/full_scientific_study_20250724_085256/metrics.jsonl'\n",
      "\n",
      "ğŸ¯ Training hybrid_reading_topic (run 4/5)\n",
      "ğŸ§  CurriculumBERT initialized\n",
      "   Model: bert-small\n",
      "   Parameters: 41,274,682\n",
      "   Layers: 8\n",
      "   Hidden size: 512\n",
      "ğŸ“± Model loaded on: cpu\n",
      "ğŸ“š Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training hybrid_reading_topic:   0%|                                                                                                   | 0/1250 [00:06<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âŒ Error in hybrid_reading_topic run 3: [Errno 2] No such file or directory: 'results/full_scientific_study_20250724_085256/metrics.jsonl'\n",
      "\n",
      "ğŸ¯ Training hybrid_reading_topic (run 5/5)\n",
      "ğŸ§  CurriculumBERT initialized\n",
      "   Model: bert-small\n",
      "   Parameters: 41,274,682\n",
      "   Layers: 8\n",
      "   Hidden size: 512\n",
      "ğŸ“± Model loaded on: cpu\n",
      "ğŸ“š Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training hybrid_reading_topic:   0%|                                                                                                   | 0/1250 [00:06<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âŒ Error in hybrid_reading_topic run 4: [Errno 2] No such file or directory: 'results/full_scientific_study_20250724_085256/metrics.jsonl'\n",
      "âš ï¸  No successful runs for hybrid_reading_topic\n",
      "\n",
      "ğŸ¯ Running strategy: hybrid_topic_reading\n",
      "\n",
      "ğŸ¯ Training hybrid_topic_reading (run 1/5)\n",
      "ğŸ§  CurriculumBERT initialized\n",
      "   Model: bert-small\n",
      "   Parameters: 41,274,682\n",
      "   Layers: 8\n",
      "   Hidden size: 512\n",
      "ğŸ“± Model loaded on: cpu\n",
      "ğŸ“š Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training hybrid_topic_reading:   0%|                                                                                                   | 0/1250 [00:06<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âŒ Error in hybrid_topic_reading run 0: [Errno 2] No such file or directory: 'results/full_scientific_study_20250724_085256/metrics.jsonl'\n",
      "\n",
      "ğŸ¯ Training hybrid_topic_reading (run 2/5)\n",
      "ğŸ§  CurriculumBERT initialized\n",
      "   Model: bert-small\n",
      "   Parameters: 41,274,682\n",
      "   Layers: 8\n",
      "   Hidden size: 512\n",
      "ğŸ“± Model loaded on: cpu\n",
      "ğŸ“š Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training hybrid_topic_reading:   0%|                                                                                                   | 0/1250 [00:06<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âŒ Error in hybrid_topic_reading run 1: [Errno 2] No such file or directory: 'results/full_scientific_study_20250724_085256/metrics.jsonl'\n",
      "\n",
      "ğŸ¯ Training hybrid_topic_reading (run 3/5)\n",
      "ğŸ§  CurriculumBERT initialized\n",
      "   Model: bert-small\n",
      "   Parameters: 41,274,682\n",
      "   Layers: 8\n",
      "   Hidden size: 512\n",
      "ğŸ“± Model loaded on: cpu\n",
      "ğŸ“š Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training hybrid_topic_reading:   0%|                                                                                                   | 0/1250 [00:06<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âŒ Error in hybrid_topic_reading run 2: [Errno 2] No such file or directory: 'results/full_scientific_study_20250724_085256/metrics.jsonl'\n",
      "\n",
      "ğŸ¯ Training hybrid_topic_reading (run 4/5)\n",
      "ğŸ§  CurriculumBERT initialized\n",
      "   Model: bert-small\n",
      "   Parameters: 41,274,682\n",
      "   Layers: 8\n",
      "   Hidden size: 512\n",
      "ğŸ“± Model loaded on: cpu\n",
      "ğŸ“š Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training hybrid_topic_reading:   0%|                                                                                                   | 0/1250 [00:06<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âŒ Error in hybrid_topic_reading run 3: [Errno 2] No such file or directory: 'results/full_scientific_study_20250724_085256/metrics.jsonl'\n",
      "\n",
      "ğŸ¯ Training hybrid_topic_reading (run 5/5)\n",
      "ğŸ§  CurriculumBERT initialized\n",
      "   Model: bert-small\n",
      "   Parameters: 41,274,682\n",
      "   Layers: 8\n",
      "   Hidden size: 512\n",
      "ğŸ“± Model loaded on: cpu\n",
      "ğŸ“š Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training hybrid_topic_reading:   0%|                                                                                                   | 0/1250 [00:06<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âŒ Error in hybrid_topic_reading run 4: [Errno 2] No such file or directory: 'results/full_scientific_study_20250724_085256/metrics.jsonl'\n",
      "âš ï¸  No successful runs for hybrid_topic_reading\n",
      "\n",
      "ğŸ“Š Analyzing results...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'results/full_scientific_study_20250724_085256/events.log'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Run all strategies with full statistical analysis\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mğŸ“Š Running comprehensive curriculum learning study...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m full_results = \u001b[43mfull_experiment\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mâœ… Full scientific experiment completed!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     17\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mğŸ“ Results saved to: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfull_experiment.tracker.log_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/research/experiment.py:290\u001b[39m, in \u001b[36mExperiment.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    287\u001b[39m report = \u001b[38;5;28mself\u001b[39m.evaluator.generate_report()\n\u001b[32m    289\u001b[39m \u001b[38;5;66;03m# Log final results\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m290\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtracker\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlog_experiment_results\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreport\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    292\u001b[39m \u001b[38;5;66;03m# Print summary\u001b[39;00m\n\u001b[32m    293\u001b[39m \u001b[38;5;28mself\u001b[39m._print_summary(report)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/research/tracking.py:151\u001b[39m, in \u001b[36mlog_experiment_results\u001b[39m\u001b[34m(self, results)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/research/tracking.py:181\u001b[39m, in \u001b[36m_log_event\u001b[39m\u001b[34m(self, event_type, data)\u001b[39m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'results/full_scientific_study_20250724_085256/events.log'"
     ]
    }
   ],
   "source": [
    "# RUN THE FULL SCIENTIFIC EXPERIMENT\n",
    "# WARNING: This cell will take several hours to complete\n",
    "# Uncomment the lines below to run the full experiment\n",
    "\n",
    "print(\"ğŸš€ Starting Full Scientific Curriculum Learning Experiment...\")\n",
    "print(\"ğŸ’¡ This will take several hours - consider running overnight\")\n",
    "print()\n",
    "\n",
    "# Create and run the experiment\n",
    "full_experiment = Experiment(full_experiment_config)\n",
    "\n",
    "# Run all strategies with full statistical analysis\n",
    "print(\"ğŸ“Š Running comprehensive curriculum learning study...\")\n",
    "full_results = full_experiment.run()\n",
    "\n",
    "print(\"âœ… Full scientific experiment completed!\")\n",
    "print(f\"ğŸ“ Results saved to: {full_experiment.tracker.log_dir}\")\n",
    "\n",
    "# Save results summary\n",
    "results_summary = {\n",
    "    'config': full_experiment_config._serialize_config(full_experiment_config),\n",
    "    'results': full_results\n",
    "}\n",
    "\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "summary_file = f\"results/full_scientific_study_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "with open(summary_file, 'w') as f:\n",
    "    json.dump(results_summary, f, indent=2, default=str)\n",
    "\n",
    "print(f\"ğŸ“‹ Summary saved to: {summary_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“Š Scientific Results Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive analysis of full scientific results\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "print(\"ğŸ“ˆ SCIENTIFIC RESULTS ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Extract detailed results\n",
    "report = full_results['report']\n",
    "summary = report['experiment_summary']\n",
    "\n",
    "print(f\"ğŸ”¬ EXPERIMENT SUMMARY\")\n",
    "print(f\"   Total strategies tested: {summary['total_strategies']}\")\n",
    "print(f\"   Effective strategies: {summary['effective_strategies']}\")\n",
    "print(f\"   Success rate: {summary['effectiveness_rate']*100:.1f}%\")\n",
    "print(f\"   Baseline: {summary['baseline']}\")\n",
    "\n",
    "print(f\"\\nğŸ† BEST STRATEGIES\")\n",
    "best = report['best_strategies']\n",
    "print(f\"   Best by performance: {best['by_final_performance']}\")\n",
    "print(f\"   Best by convergence: {best['by_convergence']}\")\n",
    "print(f\"   All effective: {best['effective_list']}\")\n",
    "\n",
    "# Detailed strategy analysis\n",
    "print(f\"\\nğŸ“Š DETAILED STRATEGY ANALYSIS\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "strategy_data = []\n",
    "for strategy, details in report['detailed_results'].items():\n",
    "    improvement = details['final_improvement_%']\n",
    "    effect_size = details['effect_size']\n",
    "    significant = details['is_significant']\n",
    "    effective = details['is_effective']\n",
    "    \n",
    "    strategy_data.append({\n",
    "        'Strategy': strategy,\n",
    "        'Improvement_%': improvement,\n",
    "        'Effect_Size': effect_size,\n",
    "        'Significant': 'âœ…' if significant else 'âŒ',\n",
    "        'Effective': 'âœ…' if effective else 'âŒ'\n",
    "    })\n",
    "    \n",
    "    print(f\"{strategy:25} | {improvement:6.1f}% | {effect_size:6.3f} | {significant:1} | {effective:1}\")\n",
    "\n",
    "print(\"-\" * 60)\n",
    "print(\"Columns: Strategy | Improvement% | Effect Size | Significant | Effective\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# 1. Strategy Performance Comparison\n",
    "df = pd.DataFrame(strategy_data)\n",
    "ax1 = axes[0, 0]\n",
    "bars = ax1.bar(range(len(df)), df['Improvement_%'], \n",
    "               color=['green' if x == 'âœ…' else 'red' for x in df['Effective']])\n",
    "ax1.set_title('Strategy Performance vs Baseline', fontsize=14, fontweight='bold')\n",
    "ax1.set_ylabel('Improvement over Random (%)')\n",
    "ax1.set_xticks(range(len(df)))\n",
    "ax1.set_xticklabels(df['Strategy'], rotation=45, ha='right')\n",
    "ax1.axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Effect Sizes\n",
    "ax2 = axes[0, 1]\n",
    "bars2 = ax2.bar(range(len(df)), df['Effect_Size'],\n",
    "                color=['green' if x == 'âœ…' else 'red' for x in df['Significant']])\n",
    "ax2.set_title('Effect Sizes (Cohen\\'s d)', fontsize=14, fontweight='bold')\n",
    "ax2.set_ylabel('Effect Size')\n",
    "ax2.set_xticks(range(len(df)))\n",
    "ax2.set_xticklabels(df['Strategy'], rotation=45, ha='right')\n",
    "ax2.axhline(y=0.2, color='orange', linestyle='--', alpha=0.7, label='Small effect')\n",
    "ax2.axhline(y=0.5, color='blue', linestyle='--', alpha=0.7, label='Medium effect')  \n",
    "ax2.axhline(y=0.8, color='purple', linestyle='--', alpha=0.7, label='Large effect')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Learning Curves (if available)\n",
    "ax3 = axes[1, 0]\n",
    "if 'training_curves' in full_results:\n",
    "    curves = full_results['training_curves']\n",
    "    for strategy, curve_data in curves.items():\n",
    "        if strategy in ['random', 'reading_level_easy_to_hard', 'topic_sequential']:\n",
    "            steps = curve_data['steps']\n",
    "            losses = curve_data['losses']\n",
    "            ax3.plot(steps, losses, label=strategy, linewidth=2)\n",
    "    \n",
    "    ax3.set_title('Learning Curves (Selected Strategies)', fontsize=14, fontweight='bold')\n",
    "    ax3.set_xlabel('Training Steps')\n",
    "    ax3.set_ylabel('Loss')\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "else:\n",
    "    ax3.text(0.5, 0.5, 'Learning curves not available', \n",
    "             ha='center', va='center', transform=ax3.transAxes)\n",
    "    ax3.set_title('Learning Curves', fontsize=14, fontweight='bold')\n",
    "\n",
    "# 4. Strategy Categories Analysis\n",
    "ax4 = axes[1, 1] \n",
    "categories = {\n",
    "    'Reading Level': ['reading_level_easy_to_hard', 'reading_level_hard_to_easy', 'reading_level_staged'],\n",
    "    'Topic-Based': ['topic_sequential', 'topic_interleaved', 'topic_largest_first'], \n",
    "    'Hybrid': ['hybrid_reading_topic', 'hybrid_topic_reading'],\n",
    "    'Baseline': ['random']\n",
    "}\n",
    "\n",
    "category_performance = {}\n",
    "for cat, strategies in categories.items():\n",
    "    cat_improvements = [df[df['Strategy'] == s]['Improvement_%'].iloc[0] \n",
    "                       for s in strategies if s in df['Strategy'].values]\n",
    "    if cat_improvements:\n",
    "        category_performance[cat] = np.mean(cat_improvements)\n",
    "\n",
    "if category_performance:\n",
    "    bars4 = ax4.bar(category_performance.keys(), category_performance.values(),\n",
    "                    color=['skyblue', 'lightgreen', 'orange', 'gray'])\n",
    "    ax4.set_title('Average Performance by Strategy Category', fontsize=14, fontweight='bold')\n",
    "    ax4.set_ylabel('Average Improvement (%)')\n",
    "    ax4.axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/scientific_experiment_analysis.png', dpi=300, bbox_inches='tight')\n",
    "print(\"ğŸ“Š Visualizations saved to: results/scientific_experiment_analysis.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“ Generate Research Paper Draft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a research paper draft based on results\n",
    "from datetime import datetime\n",
    "\n",
    "def generate_paper_draft(results, config):\n",
    "    \"\"\"Generate research paper draft from experimental results\"\"\"\n",
    "    \n",
    "    report = results['report']\n",
    "    summary = report['experiment_summary']\n",
    "    best = report['best_strategies']\n",
    "    \n",
    "    paper = f\"\"\"\n",
    "# The Effect of Curriculum Learning on BERT Pre-training: A Comprehensive Study\n",
    "\n",
    "## Abstract\n",
    "\n",
    "We investigate whether the order of training data presentation affects language model learning performance through a comprehensive study of 9 curriculum learning strategies applied to BERT pre-training. Using {config.num_samples:,} samples across {config.num_epochs} epochs with {config.num_runs} runs per strategy, we find that {summary['effective_strategies']} out of {summary['total_strategies']} strategies show statistically significant improvements over random data ordering.\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "Curriculum learning, the idea of presenting training examples in a meaningful order, has shown promise across various machine learning domains. However, its application to large-scale language model pre-training remains understudied. This work provides the first comprehensive evaluation of curriculum learning strategies for BERT-style masked language modeling.\n",
    "\n",
    "## 2. Methods\n",
    "\n",
    "### 2.1 Experimental Setup\n",
    "- **Model**: {config.model_size} ({config.hidden_size}d, {config.num_hidden_layers} layers)\n",
    "- **Data**: {config.num_samples:,} samples from WikiText, AG News, and IMDB\n",
    "- **Training**: {config.num_epochs} epochs, batch size {config.batch_size}\n",
    "- **Evaluation**: {config.num_runs} runs per strategy for statistical reliability\n",
    "\n",
    "### 2.2 Curriculum Strategies\n",
    "\n",
    "We evaluated 9 curriculum strategies across 3 main categories:\n",
    "\n",
    "**Reading Level Curricula:**\n",
    "- Easy-to-hard: Present texts in increasing Flesch-Kincaid complexity\n",
    "- Hard-to-easy: Present texts in decreasing complexity  \n",
    "- Staged: Complete each difficulty tier before advancing\n",
    "\n",
    "**Topic-based Curricula:**\n",
    "- Sequential: Present each BERTopic cluster completely\n",
    "- Interleaved: Evenly distribute samples across topics\n",
    "- Largest-first: Present largest topic clusters first\n",
    "\n",
    "**Hybrid Approaches:**\n",
    "- Reading-then-topic: Reading level primary, topic secondary\n",
    "- Topic-then-reading: Topic primary, reading level secondary\n",
    "\n",
    "## 3. Results\n",
    "\n",
    "### 3.1 Overall Effectiveness\n",
    "\"\"\"\n",
    "\n",
    "    # Add specific results\n",
    "    effective_strategies = best['effective_list']\n",
    "    if effective_strategies:\n",
    "        paper += f\"**{len(effective_strategies)} strategies showed significant benefits:**\\n\"\n",
    "        for strategy in effective_strategies[:3]:  # Top 3\n",
    "            if strategy in report['detailed_results']:\n",
    "                details = report['detailed_results'][strategy]\n",
    "                improvement = details['final_improvement_%']\n",
    "                effect_size = details['effect_size']\n",
    "                paper += f\"- **{strategy}**: {improvement:.1f}% improvement (d={effect_size:.3f})\\\\n\"\n",
    "    else:\n",
    "        paper += \"**No strategies showed statistically significant improvements over random ordering.**\\\\n\"\n",
    "    \n",
    "    paper += f\"\"\"\n",
    "\n",
    "### 3.2 Best Performing Strategy\n",
    "\"\"\"\n",
    "    \n",
    "    if best['by_final_performance']:\n",
    "        best_strategy = best['by_final_performance']\n",
    "        if best_strategy in report['detailed_results']:\n",
    "            details = report['detailed_results'][best_strategy]\n",
    "            paper += f\"**{best_strategy}** achieved the best final performance with {details['final_improvement_%']:.1f}% improvement over random baseline (Cohen's d = {details['effect_size']:.3f}).\"\n",
    "    else:\n",
    "        paper += \"No single strategy demonstrated clear superiority over the random baseline.\"\n",
    "\n",
    "    paper += f\"\"\"\n",
    "\n",
    "### 3.3 Statistical Analysis\n",
    "- Significance testing: Welch's t-test (Î± = 0.05)\n",
    "- Effect size: Cohen's d \n",
    "- Multiple comparisons: Bonferroni correction applied\n",
    "- Statistical power: {summary['effectiveness_rate']*100:.1f}% of strategies showed significant effects\n",
    "\n",
    "## 4. Discussion\n",
    "\n",
    "### 4.1 Key Findings\n",
    "\"\"\"\n",
    "\n",
    "    # Add interpretation based on results\n",
    "    if summary['effective_strategies'] > 0:\n",
    "        paper += f\"\"\"\n",
    "Our results demonstrate that curriculum learning can provide meaningful improvements to language model pre-training, with {summary['effective_strategies']} strategies showing statistically significant benefits. This suggests that the order of data presentation during pre-training is indeed important for learning efficiency.\n",
    "\"\"\"\n",
    "    else:\n",
    "        paper += f\"\"\"\n",
    "Contrary to expectations, our results show no significant benefit from curriculum learning strategies over random data ordering. This may indicate that:\n",
    "1. BERT's masked language modeling objective is robust to data ordering\n",
    "2. Larger models or longer training may be required to observe curriculum effects  \n",
    "3. Alternative curriculum designs may be more effective\n",
    "\"\"\"\n",
    "\n",
    "    paper += \"\"\"\n",
    "### 4.2 Implications for Practice\n",
    "\"\"\"\n",
    "    \n",
    "    # Add recommendations from the report\n",
    "    if report.get('recommendations'):\n",
    "        for i, rec in enumerate(report['recommendations'][:3], 1):\n",
    "            paper += f\"{i}. {rec}\\\\n\"\n",
    "\n",
    "    paper += f\"\"\"\n",
    "\n",
    "## 5. Limitations and Future Work\n",
    "- Single model architecture (BERT-small)\n",
    "- Limited training duration ({config.num_epochs} epochs)\n",
    "- Specific curriculum design choices\n",
    "- Focus on masked language modeling objective only\n",
    "\n",
    "Future work should explore curriculum learning with larger models, longer training regimes, and alternative curriculum designs based on linguistic properties beyond reading level and topic.\n",
    "\n",
    "## 6. Conclusion\n",
    "\n",
    "This comprehensive study of curriculum learning for BERT pre-training {\"demonstrates\" if summary['effective_strategies'] > 0 else \"suggests limited evidence for\"} the effectiveness of strategic data ordering. {\"The reading-level and topic-based curricula show promise\" if summary['effective_strategies'] > 0 else \"While no strategies showed significant benefits, this provides valuable negative results\"} for the field and establishes a rigorous experimental framework for future curriculum learning research in language models.\n",
    "\n",
    "---\n",
    "*Generated automatically from experimental results on {datetime.now().strftime('%Y-%m-%d')}*\n",
    "\"\"\"\n",
    "    \n",
    "    return paper\n",
    "\n",
    "# Generate the paper\n",
    "paper_draft = generate_paper_draft(full_results, full_experiment_config)\n",
    "\n",
    "# Save to file\n",
    "paper_file = f\"results/curriculum_learning_paper_draft_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md\"\n",
    "with open(paper_file, 'w') as f:\n",
    "    f.write(paper_draft)\n",
    "\n",
    "print(\"ğŸ“ RESEARCH PAPER DRAFT GENERATED\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"ğŸ“„ Saved to: {paper_file}\")\n",
    "print()\n",
    "print(\"ğŸ” Paper Preview:\")\n",
    "print(\"-\" * 40)\n",
    "print(paper_draft[:1000] + \"...\" if len(paper_draft) > 1000 else paper_draft)\n",
    "print(\"-\" * 40)\n",
    "print(f\"ğŸ“ Full paper length: {len(paper_draft):,} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ¯ Research Recommendations\n",
    "\n",
    "Based on the full scientific experiment results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final recommendations and next steps\n",
    "print(\"ğŸ¯ RESEARCH RECOMMENDATIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Extract recommendations from results\n",
    "recommendations = report.get('recommendations', [])\n",
    "for i, rec in enumerate(recommendations, 1):\n",
    "    print(f\"{i}. {rec}\")\n",
    "\n",
    "print(f\"\\nğŸ”¬ EXPERIMENTAL INSIGHTS\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Analyze which axis worked better\n",
    "reading_strategies = [s for s in report['detailed_results'].keys() if 'reading_level' in s]\n",
    "topic_strategies = [s for s in report['detailed_results'].keys() if 'topic' in s]\n",
    "hybrid_strategies = [s for s in report['detailed_results'].keys() if 'hybrid' in s]\n",
    "\n",
    "def get_avg_improvement(strategies):\n",
    "    if not strategies:\n",
    "        return 0\n",
    "    improvements = [report['detailed_results'][s]['final_improvement_%'] for s in strategies]\n",
    "    return np.mean(improvements)\n",
    "\n",
    "reading_avg = get_avg_improvement(reading_strategies)  \n",
    "topic_avg = get_avg_improvement(topic_strategies)\n",
    "hybrid_avg = get_avg_improvement(hybrid_strategies)\n",
    "\n",
    "print(f\"ğŸ“š Reading Level Strategies: {reading_avg:.1f}% average improvement\")\n",
    "print(f\"ğŸ·ï¸  Topic-Based Strategies: {topic_avg:.1f}% average improvement\")  \n",
    "print(f\"ğŸ”— Hybrid Strategies: {hybrid_avg:.1f}% average improvement\")\n",
    "\n",
    "best_axis = max([(\"Reading Level\", reading_avg), (\"Topic-Based\", topic_avg), (\"Hybrid\", hybrid_avg)], \n",
    "                key=lambda x: x[1])\n",
    "print(f\"ğŸ† Best Axis: {best_axis[0]} ({best_axis[1]:.1f}% improvement)\")\n",
    "\n",
    "print(f\"\\nğŸ’¡ FUTURE RESEARCH DIRECTIONS\")\n",
    "print(\"-\" * 35)\n",
    "print(\"1. ğŸ“ˆ Scale to BERT-Base or larger models\")\n",
    "print(\"2. â±ï¸  Extended training (25+ epochs)\")  \n",
    "print(\"3. ğŸ¯ Custom curriculum designs based on linguistic features\")\n",
    "print(\"4. ğŸ”„ Multi-phase curriculum learning\")\n",
    "print(\"5. ğŸŒ Cross-domain curriculum transfer\")\n",
    "print(\"6. ğŸ“Š Real-world downstream task evaluation\")\n",
    "\n",
    "print(f\"\\nâœ… EXPERIMENT COMPLETED SUCCESSFULLY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"ğŸ“ All results saved to: {full_experiment.tracker.log_dir}\")\n",
    "print(f\"ğŸ“Š Visualizations: results/scientific_experiment_analysis.png\")\n",
    "print(f\"ğŸ“„ Paper draft: {paper_file}\")\n",
    "print(f\"ğŸ“‹ Summary: {summary_file}\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
