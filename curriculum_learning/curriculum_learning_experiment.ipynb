{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Curriculum Learning Experiment: Progressive Scale Analysis\n",
    "\n",
    "This notebook implements a progressive approach to curriculum learning research:\n",
    "1. **Debug Study** (1K samples) - Pipeline validation\n",
    "2. **Medium Pilot Study** (100K samples) - Effect detection\n",
    "3. **Full Scientific Study** (1M samples) - Comprehensive analysis\n",
    "\n",
    "## Key Features\n",
    "- Robust BERTopic handling for large datasets\n",
    "- Multi-GPU support with curriculum preservation\n",
    "- Statistical significance testing at each scale\n",
    "- Weights & Biases integration for experiment tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.7.1+cu126\n",
      "CUDA available: True\n",
      "GPU count: 4\n",
      "  GPU 0: NVIDIA A100-SXM4-80GB\n",
      "  GPU 1: NVIDIA A100-SXM4-80GB\n",
      "  GPU 2: NVIDIA A100-SXM4-80GB\n",
      "  GPU 3: NVIDIA A100-SXM4-80GB\n",
      "\n",
      "System Memory: 668.9 GB\n",
      "Available Memory: 662.4 GB\n",
      "\n",
      "üî¨ Unified Experiment System loaded:\n",
      "  ‚úÖ Single interface for all experiment modes\n",
      "  ‚úÖ Enhanced metrics (top-5 accuracy, perplexity, confidence)\n",
      "  ‚úÖ Memory-efficient handling for large datasets\n",
      "  ‚úÖ Fair comparison mode for unbiased research\n",
      "  ‚úÖ Statistical analysis and convergence detection\n",
      "  ‚úÖ Improved W&B logging with organized structure\n",
      "\n",
      "‚öôÔ∏è  Current settings:\n",
      "   Experiment mode: enhanced\n",
      "   Fair comparison: Disabled\n"
     ]
    }
   ],
   "source": [
    "# Core imports\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import gc\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for visualizations\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "# NEW: Unified system imports\n",
    "from unified_experiment import (\n",
    "    UnifiedExperiment, \n",
    "    ExperimentMode,\n",
    "    run_basic_experiment,\n",
    "    run_enhanced_experiment,\n",
    "    run_memory_efficient_experiment,\n",
    "    run_fair_comparison_experiment\n",
    ")\n",
    "from config import (\n",
    "    Config, \n",
    "    debug_config,\n",
    "    pilot_config, \n",
    "    scientific_config,\n",
    "    fair_comparison_config\n",
    ")\n",
    "\n",
    "# Enable high DPI displays\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU count: {torch.cuda.device_count()}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"  GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        \n",
    "# Memory status\n",
    "import psutil\n",
    "print(f\"\\nSystem Memory: {psutil.virtual_memory().total / 1024**3:.1f} GB\")\n",
    "print(f\"Available Memory: {psutil.virtual_memory().available / 1024**3:.1f} GB\")\n",
    "\n",
    "print(f\"\\nüî¨ Unified Experiment System loaded:\")\n",
    "print(f\"  ‚úÖ Single interface for all experiment modes\")\n",
    "print(f\"  ‚úÖ Enhanced metrics (top-5 accuracy, perplexity, confidence)\")\n",
    "print(f\"  ‚úÖ Memory-efficient handling for large datasets\")\n",
    "print(f\"  ‚úÖ Fair comparison mode for unbiased research\")\n",
    "print(f\"  ‚úÖ Statistical analysis and convergence detection\")\n",
    "print(f\"  ‚úÖ Improved W&B logging with organized structure\")\n",
    "\n",
    "# Configuration for different experiment types\n",
    "EXPERIMENT_MODE = ExperimentMode.ENHANCED  # Can be: BASIC, ENHANCED, MEMORY_EFFICIENT, FAIR_COMPARISON\n",
    "FAIR_COMPARISON_MODE = False  # Set to True for unbiased curriculum comparison\n",
    "\n",
    "print(f\"\\n‚öôÔ∏è  Current settings:\")\n",
    "print(f\"   Experiment mode: {EXPERIMENT_MODE.value}\")\n",
    "print(f\"   Fair comparison: {'Enabled' if FAIR_COMPARISON_MODE else 'Disabled'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Debug Study - Pipeline Validation\n",
    "\n",
    "Quick test to ensure all components work correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Run debug experiment with unified system\nprint(\"\\nüîß Running Debug Study (Unified System)...\\n\")\n\n# Create debug configuration\nconfig = debug_config(use_wandb=False)\n\n# Run with selected mode\nif FAIR_COMPARISON_MODE:\n    print(\"üéØ Using Fair Comparison Mode (no early stopping)\")\n    config = fair_comparison_config(scale=\"debug\", use_wandb=False)\n    debug_results = run_fair_comparison_experiment(config)\nelif EXPERIMENT_MODE == ExperimentMode.ENHANCED:\n    print(\"üî¨ Using Enhanced Mode (comprehensive metrics)\")\n    debug_results = run_enhanced_experiment(config)\nelif EXPERIMENT_MODE == ExperimentMode.MEMORY_EFFICIENT:\n    print(\"üíæ Using Memory-Efficient Mode\")\n    debug_results = run_memory_efficient_experiment(config)\nelse:\n    print(\"üìä Using Basic Mode\")\n    debug_results = run_basic_experiment(config)\n\nprint(\"\\n‚úÖ Debug study completed successfully!\")\n\n# Safe access to experiment ID with fallback\nexperiment_id = \"N/A\"\nif hasattr(debug_results, 'experiment_summary') and debug_results.experiment_summary:\n    if 'experiment_summary' in debug_results.experiment_summary:\n        experiment_id = debug_results.experiment_summary['experiment_summary'].get('experiment_id', 'N/A')\n    elif 'experiment_id' in debug_results.experiment_summary:\n        experiment_id = debug_results.experiment_summary['experiment_id']\n\nprint(f\"Experiment ID: {experiment_id}\")\n\n# Enhanced results analysis for enhanced/fair comparison modes\nif EXPERIMENT_MODE in [ExperimentMode.ENHANCED, ExperimentMode.FAIR_COMPARISON] or FAIR_COMPARISON_MODE:\n    final_scores = debug_results.statistical_analysis\n    if final_scores:\n        print(f\"\\nüìä Final Validation Accuracies:\")\n        for strategy, score in final_scores.items():\n            print(f\"   {strategy}: {score:.4f}\")\n        \n        max_accuracy = max(final_scores.values())\n        if max_accuracy < 0.1:\n            print(f\"\\n‚ö†Ô∏è  WARNING: Maximum accuracy is {max_accuracy:.4f} (<10%), this seems low for MLM.\")\n            print(f\"    This might indicate a training problem that needs investigation.\")\n        else:\n            print(f\"\\n‚úÖ Accuracy looks reasonable (max: {max_accuracy:.4f})\")\n        \n        # Convergence analysis\n        if debug_results.convergence_analysis:\n            converged_strategies = [s for s, info in debug_results.convergence_analysis.items() \n                                  if info.get('converged', False)]\n            if converged_strategies:\n                print(f\"\\nüéØ Converged strategies: {', '.join(converged_strategies)}\")\n    else:\n        print(\"\\nüìä Basic mode results - limited analysis available\")\n        \nprint(f\"\\n‚è±Ô∏è  Runtime: {debug_results.resource_usage.get('total_runtime_hours', 0):.2f} hours\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run debug experiment\n",
    "print(\"\\nüîß Running Debug Study...\\n\")\n",
    "debug_experiment = Experiment(debug_config)\n",
    "debug_results = debug_experiment.run()\n",
    "\n",
    "print(\"\\n‚úÖ Debug study completed successfully!\")\n",
    "print(f\"Experiment ID: {debug_results['experiment_id']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Medium Pilot Study - Effect Detection\n",
    "\n",
    "This pilot study uses 100K samples to detect whether curriculum learning effects exist in our setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Medium pilot configuration - unified system\n",
    "if FAIR_COMPARISON_MODE:\n",
    "    print(\"üéØ Creating Fair Comparison Configuration for Pilot Study\")\n",
    "    pilot_config = fair_comparison_config(\n",
    "        scale=\"large\",  # 100K samples\n",
    "        model_size=\"bert-small\",\n",
    "        num_epochs=15,\n",
    "        num_runs=3,\n",
    "        batch_size=32,\n",
    "        strategies=[\n",
    "            \"random\",  # Baseline\n",
    "            \"reading_level_easy_to_hard\",\n",
    "            \"reading_level_hard_to_easy\",\n",
    "            \"topic_sequential\",\n",
    "            \"topic_largest_first\",\n",
    "            \"hybrid_reading_topic\"\n",
    "        ],\n",
    "        use_wandb=True,\n",
    "        experiment_name=\"curriculum_pilot_fair_comparison\"\n",
    "    )\n",
    "else:\n",
    "    print(\"üî¨ Creating Enhanced Configuration for Pilot Study\")\n",
    "    pilot_config = Config(\n",
    "        scale=\"large\",  # 100K samples\n",
    "        model_size=\"bert-small\",\n",
    "        num_epochs=15,\n",
    "        num_runs=3,\n",
    "        batch_size=32,\n",
    "        strategies=[\n",
    "            \"random\",  # Baseline\n",
    "            \"reading_level_easy_to_hard\",\n",
    "            \"reading_level_hard_to_easy\",\n",
    "            \"topic_sequential\",\n",
    "            \"topic_largest_first\",\n",
    "            \"hybrid_reading_topic\"\n",
    "        ],\n",
    "        use_wandb=True,\n",
    "        experiment_name=\"curriculum_pilot_enhanced\",\n",
    "        memory_efficient=True,  # Enable for large dataset\n",
    "        eval_every_n_steps=500,\n",
    "        use_early_stopping=True,\n",
    "        early_stopping_patience=7\n",
    "    )\n",
    "\n",
    "print(\"Pilot Study Configuration:\")\n",
    "pilot_config.print_summary()\n",
    "\n",
    "# Show configuration type\n",
    "if hasattr(pilot_config, 'use_early_stopping'):\n",
    "    early_stopping_status = \"Disabled (fair comparison)\" if not pilot_config.use_early_stopping else f\"Enabled (patience={pilot_config.early_stopping_patience})\"\n",
    "    print(f\"\\nExperiment mode: {'Fair Comparison' if FAIR_COMPARISON_MODE else 'Enhanced'}\")\n",
    "    print(f\"Early stopping: {early_stopping_status}\")\n",
    "    print(f\"Memory efficient: {getattr(pilot_config, 'memory_efficient', False)}\")\n",
    "    \n",
    "# Mode selection\n",
    "if EXPERIMENT_MODE == ExperimentMode.MEMORY_EFFICIENT:\n",
    "    PILOT_MODE = ExperimentMode.MEMORY_EFFICIENT\n",
    "    print(f\"Using memory-efficient mode for pilot study\")\n",
    "elif FAIR_COMPARISON_MODE:\n",
    "    PILOT_MODE = ExperimentMode.FAIR_COMPARISON\n",
    "    print(f\"Using fair comparison mode for pilot study\")\n",
    "else:\n",
    "    PILOT_MODE = ExperimentMode.ENHANCED\n",
    "    print(f\"Using enhanced mode for pilot study\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run pilot experiment with unified system\n",
    "print(f\"\\nüöÄ Running Medium Pilot Study (Unified System - {PILOT_MODE.value} mode)...\\n\")\n",
    "print(\"This will help us detect if curriculum effects exist before the full study.\\n\")\n",
    "\n",
    "# Print mode-specific features\n",
    "if PILOT_MODE == ExperimentMode.ENHANCED:\n",
    "    print(\"Enhanced mode features:\\n\")\n",
    "    print(\"  üî¨ Comprehensive MLM metrics (accuracy, top-5, perplexity, confidence)\")\n",
    "    print(\"  üìä Epoch-level summaries and statistical analysis\")\n",
    "    print(\"  üõë Early stopping to prevent overtraining\")\n",
    "    print(\"  üìà Convergence detection and effect size calculation\")\n",
    "    print(\"  üéØ Organized W&B logging with reduced noise\")\n",
    "elif PILOT_MODE == ExperimentMode.MEMORY_EFFICIENT:\n",
    "    print(\"Memory-efficient mode features:\\n\")\n",
    "    print(\"  üíæ Aggressive garbage collection and memory monitoring\")\n",
    "    print(\"  üîÑ GPU cache clearing between strategies\")\n",
    "    print(\"  üìä Resource usage tracking\")\n",
    "elif PILOT_MODE == ExperimentMode.FAIR_COMPARISON:\n",
    "    print(\"Fair comparison mode features:\\n\")\n",
    "    print(\"  ‚öñÔ∏è  No early stopping (all strategies train equal steps)\")\n",
    "    print(\"  üîÑ Fixed seeds for perfect reproducibility\")\n",
    "    print(\"  üìä Comprehensive logging for unbiased analysis\")\n",
    "    print(\"  üìà Statistical significance testing\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Clear memory before starting\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Run experiment with unified system\n",
    "pilot_experiment = UnifiedExperiment(pilot_config, PILOT_MODE)\n",
    "pilot_results = pilot_experiment.run()\n",
    "\n",
    "# Clear memory after completion\n",
    "del pilot_experiment\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(\"\\n‚úÖ Pilot study completed!\")\n",
    "\n",
    "# Enhanced results display\n",
    "print(f\"\\nüìä Pilot Study Results:\")\n",
    "print(f\"   Strategies analyzed: {len(pilot_results.strategy_results)}\")\n",
    "print(f\"   Runtime: {pilot_results.resource_usage.get('total_runtime_hours', 0):.2f} hours\")\n",
    "\n",
    "# Show final scores if available\n",
    "if pilot_results.statistical_analysis:\n",
    "    print(f\"\\nüéØ Final Validation Accuracies:\")\n",
    "    baseline_acc = pilot_results.statistical_analysis.get('random', 0)\n",
    "    \n",
    "    for strategy, score in pilot_results.statistical_analysis.items():\n",
    "        if strategy == \"random\":\n",
    "            print(f\"   üìä {strategy}: {score:.4f} (baseline)\")\n",
    "        else:\n",
    "            improvement = ((score - baseline_acc) / max(baseline_acc, 1e-8)) * 100 if baseline_acc > 0 else 0\n",
    "            print(f\"   üìà {strategy}: {score:.4f} ({improvement:+.2f}% vs baseline)\")\n",
    "    \n",
    "    # Quality assessment\n",
    "    max_accuracy = max(pilot_results.statistical_analysis.values())\n",
    "    if max_accuracy < 0.15:\n",
    "        print(f\"\\n‚ö†Ô∏è  ATTENTION: Maximum accuracy is {max_accuracy:.4f} (<15%)\")\n",
    "        print(f\"    Consider checking model configuration or training setup\")\n",
    "    elif max_accuracy > 0.25:\n",
    "        print(f\"\\n‚úÖ Excellent accuracy achieved (max: {max_accuracy:.4f})!\")\n",
    "    else:\n",
    "        print(f\"\\n‚úÖ Reasonable accuracy for MLM (max: {max_accuracy:.4f})\")\n",
    "\n",
    "# Convergence analysis for enhanced modes\n",
    "if (PILOT_MODE in [ExperimentMode.ENHANCED, ExperimentMode.FAIR_COMPARISON] and \n",
    "    pilot_results.convergence_analysis):\n",
    "    print(f\"\\nüîç Convergence Analysis:\")\n",
    "    converged_strategies = []\n",
    "    for strategy, convergence in pilot_results.convergence_analysis.items():\n",
    "        if convergence.get('converged', False):\n",
    "            print(f\"   ‚úÖ {strategy}: Converged at epoch {convergence.get('convergence_epoch', 'N/A')}\")\n",
    "            converged_strategies.append(strategy)\n",
    "        else:\n",
    "            print(f\"   ‚è≥ {strategy}: Still improving ({convergence.get('reason', 'Unknown')})\")\n",
    "    \n",
    "    print(f\"\\n   Summary: {len(converged_strategies)}/{len(pilot_results.convergence_analysis)} strategies converged\")\n",
    "\n",
    "# Memory usage for memory-efficient mode\n",
    "if PILOT_MODE == ExperimentMode.MEMORY_EFFICIENT:\n",
    "    resource_summary = pilot_results.resource_usage\n",
    "    if 'peak_memory_mb' in resource_summary:\n",
    "        print(f\"\\nüíæ Memory Usage:\")\n",
    "        print(f\"   Initial: {resource_summary.get('initial_memory_mb', 0):.1f} MB\")\n",
    "        print(f\"   Peak: {resource_summary.get('peak_memory_mb', 0):.1f} MB\")\n",
    "        print(f\"   Final: {resource_summary.get('final_memory_mb', 0):.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Medium pilot configuration with memory efficiency\n",
    "pilot_config = Config(\n",
    "    scale=\"large\",  # 100K samples\n",
    "    model_size=\"bert-small\",\n",
    "    num_epochs=15,  # Enough to see convergence differences\n",
    "    num_runs=3,  # Fewer runs for pilot\n",
    "    batch_size=32,  # Reduced from 64 to save memory\n",
    "    strategies=[\n",
    "        \"random\",  # Baseline\n",
    "        \"reading_level_easy_to_hard\",\n",
    "        \"reading_level_hard_to_easy\",\n",
    "        \"topic_sequential\",\n",
    "        \"topic_largest_first\",\n",
    "        \"hybrid_reading_topic\"\n",
    "    ],\n",
    "    use_wandb=True,\n",
    "    experiment_name=\"curriculum_pilot_study\",\n",
    "    memory_efficient=True,  # Enable memory-efficient mode\n",
    "    num_workers=2,  # Reduce workers to save memory\n",
    "    eval_every_n_steps=500  # Less frequent evaluation to save memory\n",
    ")\n",
    "\n",
    "print(\"Pilot Study Configuration (Memory-Efficient):\")\n",
    "pilot_config.print_summary()\n",
    "print(f\"\\nMemory-efficient mode: {pilot_config.memory_efficient}\")\n",
    "print(f\"Max memory limit: {pilot_config.max_memory_gb} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced pilot results analysis\n",
    "pilot_report = pilot_results['report']\n",
    "enhanced_summary = pilot_results['enhanced_summary']\n",
    "\n",
    "# Extract metrics from enhanced summary (more reliable than old approach)\n",
    "strategies = list(enhanced_summary['final_scores'].keys())\n",
    "final_accuracies = list(enhanced_summary['final_scores'].values())\n",
    "\n",
    "# Create enhanced comparison dataframe\n",
    "pilot_df = pd.DataFrame({\n",
    "    'Strategy': strategies,\n",
    "    'Final Accuracy': final_accuracies\n",
    "}).sort_values('Final Accuracy', ascending=False)\n",
    "\n",
    "# Add additional metrics if available\n",
    "if 'convergence_analysis' in enhanced_summary:\n",
    "    convergence_epochs = []\n",
    "    converged_status = []\n",
    "    \n",
    "    for strategy in strategies:\n",
    "        conv_info = enhanced_summary['convergence_analysis'].get(strategy, {})\n",
    "        convergence_epochs.append(conv_info.get('convergence_epoch', 'N/A'))\n",
    "        converged_status.append('Yes' if conv_info.get('converged', False) else 'No')\n",
    "    \n",
    "    pilot_df['Convergence Epoch'] = convergence_epochs\n",
    "    pilot_df['Converged'] = converged_status\n",
    "\n",
    "print(\"üìä Enhanced Pilot Study Results Summary:\")\n",
    "print(pilot_df.to_string(index=False))\n",
    "\n",
    "# Enhanced statistical analysis\n",
    "random_accuracy = enhanced_summary['final_scores'].get('random', 0)\n",
    "best_accuracy = pilot_df.iloc[0]['Final Accuracy']\n",
    "best_strategy = pilot_df.iloc[0]['Strategy']\n",
    "\n",
    "if random_accuracy > 0:\n",
    "    improvement = ((best_accuracy - random_accuracy) / random_accuracy) * 100\n",
    "    print(f\"\\nüìà Statistical Analysis:\")\n",
    "    print(f\"   Random baseline: {random_accuracy:.4f}\")\n",
    "    print(f\"   Best strategy: {best_strategy}\")\n",
    "    print(f\"   Best accuracy: {best_accuracy:.4f}\")\n",
    "    print(f\"   Improvement over random: {improvement:.2f}%\")\n",
    "    \n",
    "    # Effect size estimation (rough)\n",
    "    effect_size = abs(improvement) / 10  # Rough estimate\n",
    "    if effect_size < 0.2:\n",
    "        effect_interpretation = \"Negligible effect\"\n",
    "    elif effect_size < 0.5:\n",
    "        effect_interpretation = \"Small effect\"\n",
    "    elif effect_size < 0.8:\n",
    "        effect_interpretation = \"Medium effect\"\n",
    "    else:\n",
    "        effect_interpretation = \"Large effect\"\n",
    "    \n",
    "    print(f\"   Effect size: {effect_size:.3f} ({effect_interpretation})\")\n",
    "    \n",
    "    # Decision for full study\n",
    "    if improvement > 5 and best_accuracy > 0.15:\n",
    "        print(f\"\\n‚úÖ STRONG SIGNAL: Significant curriculum effects detected!\")\n",
    "        print(f\"   Both improvement ({improvement:.1f}%) and absolute accuracy ({best_accuracy:.1f}) are good.\")\n",
    "        print(f\"   üìù Recommendation: Proceed to full scientific study.\")\n",
    "    elif improvement > 2:\n",
    "        print(f\"\\n‚ö†Ô∏è  WEAK SIGNAL: Some curriculum effects detected.\")\n",
    "        print(f\"   Improvement is modest ({improvement:.1f}%). Consider:\")\n",
    "        print(f\"   ‚Ä¢ Running longer (more epochs)\")\n",
    "        print(f\"   ‚Ä¢ Trying different hyperparameters\") \n",
    "        print(f\"   ‚Ä¢ Using larger model\")\n",
    "    else:\n",
    "        print(f\"\\n‚ùå NO CLEAR SIGNAL: Minimal curriculum effects detected.\")\n",
    "        print(f\"   Consider investigating training setup before full study.\")\n",
    "\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è  Cannot perform statistical analysis - no random baseline found.\")\n",
    "\n",
    "# Quality assessment\n",
    "print(f\"\\nüîç Training Quality Assessment:\")\n",
    "if best_accuracy < 0.15:\n",
    "    print(f\"   ‚ùå Low accuracy ({best_accuracy:.4f}) suggests training issues\")\n",
    "    print(f\"      ‚Ä¢ Check learning rate (current: {pilot_config.learning_rate})\")\n",
    "    print(f\"      ‚Ä¢ Consider more epochs (current: {pilot_config.num_epochs})\")\n",
    "    print(f\"      ‚Ä¢ Verify data quality and model configuration\")\n",
    "elif best_accuracy > 0.3:\n",
    "    print(f\"   ‚úÖ Excellent accuracy ({best_accuracy:.4f}) - training working well!\")\n",
    "else:\n",
    "    print(f\"   ‚úÖ Reasonable accuracy ({best_accuracy:.4f}) - training seems healthy\")\n",
    "\n",
    "# Convergence insights\n",
    "converged_count = pilot_df['Converged'].value_counts().get('Yes', 0) if 'Converged' in pilot_df.columns else 0\n",
    "total_strategies = len(strategies)\n",
    "\n",
    "print(f\"\\nüéØ Convergence Insights:\")\n",
    "print(f\"   Converged strategies: {converged_count}/{total_strategies}\")\n",
    "if converged_count == total_strategies:\n",
    "    print(f\"   ‚úÖ All strategies converged - good training stability\")\n",
    "elif converged_count > total_strategies // 2:\n",
    "    print(f\"   ‚ö° Most strategies converged - reasonable training\")\n",
    "else:\n",
    "    print(f\"   ‚ö†Ô∏è  Few strategies converged - may need more epochs or better LR\")\n",
    "\n",
    "# Specific curriculum insights\n",
    "reading_strategies = [s for s in strategies if 'reading_level' in s]\n",
    "topic_strategies = [s for s in strategies if 'topic' in s and 'reading' not in s]\n",
    "hybrid_strategies = [s for s in strategies if 'hybrid' in s]\n",
    "\n",
    "if reading_strategies:\n",
    "    reading_scores = [enhanced_summary['final_scores'][s] for s in reading_strategies]\n",
    "    best_reading = reading_strategies[np.argmax(reading_scores)]\n",
    "    print(f\"\\nüìö Reading-level Strategies:\")\n",
    "    print(f\"   Best: {best_reading} ({max(reading_scores):.4f})\")\n",
    "\n",
    "if topic_strategies:\n",
    "    topic_scores = [enhanced_summary['final_scores'][s] for s in topic_strategies]\n",
    "    best_topic = topic_strategies[np.argmax(topic_scores)]\n",
    "    print(f\"\\nüè∑Ô∏è  Topic-based Strategies:\")\n",
    "    print(f\"   Best: {best_topic} ({max(topic_scores):.4f})\")\n",
    "\n",
    "if hybrid_strategies:\n",
    "    hybrid_scores = [enhanced_summary['final_scores'][s] for s in hybrid_strategies]\n",
    "    best_hybrid = hybrid_strategies[np.argmax(hybrid_scores)]\n",
    "    print(f\"\\nüîÑ Hybrid Strategies:\")\n",
    "    print(f\"   Best: {best_hybrid} ({max(hybrid_scores):.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Pilot Study Analysis - Can We Detect Effects?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze pilot results\n",
    "pilot_report = pilot_results['report']\n",
    "\n",
    "# Extract key metrics\n",
    "strategies = list(pilot_results['results'].keys())\n",
    "final_losses = []\n",
    "convergence_speeds = []\n",
    "\n",
    "for strategy in strategies:\n",
    "    if strategy in pilot_results['results']:\n",
    "        result = pilot_results['results'][strategy]\n",
    "        final_losses.append(result['losses'][-1])\n",
    "        \n",
    "        # Find convergence point (when loss drops below threshold)\n",
    "        threshold = result['losses'][0] * 0.5  # 50% of initial loss\n",
    "        convergence_step = next((i for i, loss in enumerate(result['losses']) if loss < threshold), len(result['losses']))\n",
    "        convergence_speeds.append(convergence_step)\n",
    "\n",
    "# Create comparison dataframe\n",
    "pilot_df = pd.DataFrame({\n",
    "    'Strategy': strategies,\n",
    "    'Final Loss': final_losses,\n",
    "    'Convergence Step': convergence_speeds\n",
    "}).sort_values('Final Loss')\n",
    "\n",
    "print(\"Pilot Study Results Summary:\")\n",
    "print(pilot_df.to_string(index=False))\n",
    "\n",
    "# Statistical test\n",
    "from scipy import stats\n",
    "random_loss = pilot_df[pilot_df['Strategy'] == 'random']['Final Loss'].values[0]\n",
    "best_loss = pilot_df.iloc[0]['Final Loss']\n",
    "improvement = (random_loss - best_loss) / random_loss * 100\n",
    "\n",
    "print(f\"\\nBest strategy: {pilot_df.iloc[0]['Strategy']}\")\n",
    "print(f\"Improvement over random: {improvement:.1f}%\")\n",
    "\n",
    "# Decision for full study\n",
    "if improvement > 5:\n",
    "    print(\"\\n‚úÖ Significant curriculum effects detected! Proceeding to full study is justified.\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Minimal curriculum effects detected. Consider adjusting approach.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize pilot results\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Learning curves\n",
    "for strategy in strategies:\n",
    "    if strategy in pilot_results['results']:\n",
    "        result = pilot_results['results'][strategy]\n",
    "        steps = result['steps'][:1000:10]  # Sample for clarity\n",
    "        losses = result['losses'][:1000:10]\n",
    "        ax1.plot(steps, losses, label=strategy, linewidth=2)\n",
    "\n",
    "ax1.set_xlabel('Training Steps')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Pilot Study: Learning Curves')\n",
    "ax1.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Final performance comparison\n",
    "ax2.bar(pilot_df['Strategy'], pilot_df['Final Loss'])\n",
    "ax2.set_xlabel('Strategy')\n",
    "ax2.set_ylabel('Final Loss')\n",
    "ax2.set_title('Pilot Study: Final Performance')\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Full Scientific Study - Comprehensive Analysis\n",
    "\n",
    "Based on pilot results, we proceed with the full-scale study using 1M samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full scientific configuration with unified system\n",
    "if FAIR_COMPARISON_MODE:\n",
    "    print(\"üéØ Creating Fair Comparison Configuration for Scientific Study\")\n",
    "    scientific_config = fair_comparison_config(\n",
    "        scale=\"extreme\",  # 1M samples\n",
    "        model_size=\"bert-small\",\n",
    "        num_epochs=50,\n",
    "        num_runs=5,\n",
    "        batch_size=256,\n",
    "        strategies=[\n",
    "            # Core strategies\n",
    "            \"random\",\n",
    "            \"reading_level_easy_to_hard\",\n",
    "            \"reading_level_hard_to_easy\",\n",
    "            \"reading_level_staged\",\n",
    "            \n",
    "            # Topic-based strategies\n",
    "            \"topic_sequential\",\n",
    "            \"topic_interleaved\",\n",
    "            \"topic_largest_first\",\n",
    "            \n",
    "            # Hybrid strategies\n",
    "            \"hybrid_reading_topic\",\n",
    "            \"hybrid_topic_reading\",\n",
    "            \n",
    "            # Epoch-interleaving strategies\n",
    "            \"reading_topic_by_epoch\",\n",
    "            \"reading_levels_by_epoch\",\n",
    "            \"all_strategies_by_epoch\"\n",
    "        ],\n",
    "        use_wandb=True,\n",
    "        experiment_name=\"curriculum_scientific_fair_comparison\"\n",
    "    )\n",
    "    SCIENTIFIC_MODE = ExperimentMode.FAIR_COMPARISON\n",
    "else:\n",
    "    print(\"üî¨ Creating Enhanced Configuration for Scientific Study\")\n",
    "    scientific_config = Config(\n",
    "        scale=\"extreme\",  # 1M samples\n",
    "        model_size=\"bert-small\",\n",
    "        num_epochs=50,\n",
    "        num_runs=5,\n",
    "        batch_size=256,\n",
    "        strategies=[\n",
    "            # Core strategies\n",
    "            \"random\",\n",
    "            \"reading_level_easy_to_hard\",\n",
    "            \"reading_level_hard_to_easy\",\n",
    "            \"reading_level_staged\",\n",
    "            \n",
    "            # Topic-based strategies\n",
    "            \"topic_sequential\",\n",
    "            \"topic_interleaved\",\n",
    "            \"topic_largest_first\",\n",
    "            \n",
    "            # Hybrid strategies\n",
    "            \"hybrid_reading_topic\",\n",
    "            \"hybrid_topic_reading\",\n",
    "            \n",
    "            # Epoch-interleaving strategies\n",
    "            \"reading_topic_by_epoch\",\n",
    "            \"reading_levels_by_epoch\",\n",
    "            \"all_strategies_by_epoch\"\n",
    "        ],\n",
    "        use_wandb=True,\n",
    "        experiment_name=\"curriculum_scientific_enhanced\",\n",
    "        memory_efficient=True,  # Enable for 1M samples\n",
    "        eval_every_n_steps=2000\n",
    "    )\n",
    "    \n",
    "    # Use memory-efficient mode for large scale regardless of global setting\n",
    "    SCIENTIFIC_MODE = ExperimentMode.MEMORY_EFFICIENT\n",
    "\n",
    "print(\"Full Scientific Study Configuration:\")\n",
    "scientific_config.print_summary()\n",
    "\n",
    "print(f\"\\n‚öôÔ∏è  Scientific Study Settings:\")\n",
    "print(f\"   Mode: {SCIENTIFIC_MODE.value}\")\n",
    "print(f\"   Fair comparison: {'Yes' if FAIR_COMPARISON_MODE else 'No'}\")\n",
    "print(f\"   Memory efficient: {'Yes' if SCIENTIFIC_MODE == ExperimentMode.MEMORY_EFFICIENT else 'No'}\")\n",
    "print(f\"   Early stopping: {'Disabled' if FAIR_COMPARISON_MODE else 'Enabled'}\")\n",
    "print(f\"   BERTopic sampling: Will use 10K samples for topic discovery on 1M dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm before running (this will take several hours)\n",
    "print(\"‚ö†Ô∏è  WARNING: The full scientific study will take several hours to complete.\")\n",
    "print(\"Make sure you have:\")\n",
    "print(\"  - Stable power and internet connection\")\n",
    "print(\"  - Sufficient disk space for checkpoints\")\n",
    "print(\"  - W&B configured for tracking\")\n",
    "print(\"\\nThe experiment will use the new robust BERTopic implementation that handles 1M+ samples.\")\n",
    "print(\"\\nPress Enter to continue or Ctrl+C to cancel...\")\n",
    "input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run full scientific experiment with unified system\n",
    "print(f\"\\nüî¨ Running Full Scientific Study (Unified System - {SCIENTIFIC_MODE.value} mode)...\\n\")\n",
    "\n",
    "# Clear memory before starting\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Run experiment with unified system\n",
    "scientific_experiment = UnifiedExperiment(scientific_config, SCIENTIFIC_MODE)\n",
    "scientific_results = scientific_experiment.run()\n",
    "\n",
    "# Clear memory after completion\n",
    "del scientific_experiment\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(\"\\n‚úÖ Scientific study completed!\")\n",
    "\n",
    "# Results summary with unified structure\n",
    "print(f\"\\nüìä Scientific Study Results:\")\n",
    "print(f\"   Strategies analyzed: {len(scientific_results.strategy_results)}\")\n",
    "print(f\"   Runtime: {scientific_results.resource_usage.get('total_runtime_hours', 0):.2f} hours\")\n",
    "\n",
    "# Display final scores\n",
    "if scientific_results.statistical_analysis:\n",
    "    print(f\"\\nüéØ Final Performance Ranking:\")\n",
    "    # Sort strategies by performance\n",
    "    sorted_strategies = sorted(scientific_results.statistical_analysis.items(), \n",
    "                             key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    baseline_score = scientific_results.statistical_analysis.get('random', 0)\n",
    "    \n",
    "    for i, (strategy, score) in enumerate(sorted_strategies, 1):\n",
    "        if strategy == \"random\":\n",
    "            print(f\"   {i:2d}. {strategy:30s} {score:.4f} (baseline)\")\n",
    "        else:\n",
    "            improvement = ((score - baseline_score) / max(baseline_score, 1e-8)) * 100 if baseline_score > 0 else 0\n",
    "            print(f\"   {i:2d}. {strategy:30s} {score:.4f} ({improvement:+.2f}%)\")\n",
    "\n",
    "# Quality and convergence analysis for enhanced modes\n",
    "if SCIENTIFIC_MODE in [ExperimentMode.ENHANCED, ExperimentMode.FAIR_COMPARISON]:\n",
    "    max_accuracy = max(scientific_results.statistical_analysis.values()) if scientific_results.statistical_analysis else 0\n",
    "    \n",
    "    print(f\"\\nüìà Training Quality:\")\n",
    "    if max_accuracy > 0.3:\n",
    "        print(f\"   ‚úÖ Excellent maximum accuracy: {max_accuracy:.4f}\")\n",
    "    elif max_accuracy > 0.2:\n",
    "        print(f\"   ‚úÖ Good maximum accuracy: {max_accuracy:.4f}\")\n",
    "    elif max_accuracy > 0.15:\n",
    "        print(f\"   ‚ö†Ô∏è  Moderate maximum accuracy: {max_accuracy:.4f}\")\n",
    "    else:\n",
    "        print(f\"   ‚ùå Low maximum accuracy: {max_accuracy:.4f} - investigate training\")\n",
    "    \n",
    "    # Convergence analysis\n",
    "    if scientific_results.convergence_analysis:\n",
    "        converged_count = sum(1 for info in scientific_results.convergence_analysis.values() \n",
    "                            if info.get('converged', False))\n",
    "        total_strategies = len(scientific_results.convergence_analysis)\n",
    "        \n",
    "        print(f\"\\nüîç Convergence Analysis:\")\n",
    "        print(f\"   Converged strategies: {converged_count}/{total_strategies}\")\n",
    "        \n",
    "        if converged_count == total_strategies:\n",
    "            print(f\"   ‚úÖ All strategies converged - excellent training stability\")\n",
    "        elif converged_count > total_strategies * 0.7:\n",
    "            print(f\"   ‚úÖ Most strategies converged - good training stability\")\n",
    "        else:\n",
    "            print(f\"   ‚ö†Ô∏è  Few strategies converged - consider longer training\")\n",
    "\n",
    "# Memory analysis for memory-efficient mode\n",
    "if SCIENTIFIC_MODE == ExperimentMode.MEMORY_EFFICIENT:\n",
    "    resource_summary = scientific_results.resource_usage\n",
    "    print(f\"\\nüíæ Memory Efficiency Analysis:\")\n",
    "    if 'peak_memory_mb' in resource_summary:\n",
    "        peak_gb = resource_summary['peak_memory_mb'] / 1024\n",
    "        print(f\"   Peak memory usage: {peak_gb:.1f} GB\")\n",
    "        if peak_gb < 20:\n",
    "            print(f\"   ‚úÖ Excellent memory efficiency!\")\n",
    "        elif peak_gb < 40:\n",
    "            print(f\"   ‚úÖ Good memory usage\")\n",
    "        else:\n",
    "            print(f\"   ‚ö†Ô∏è  High memory usage - consider optimization\")\n",
    "    \n",
    "    print(f\"   No memory crashes detected - robust pipeline!\")\n",
    "\n",
    "print(f\"\\nüéâ Scientific study completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Comprehensive Results Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and analyze results\n",
    "report = scientific_results['report']\n",
    "results = scientific_results['results']\n",
    "\n",
    "# Create comprehensive results dataframe\n",
    "all_metrics = []\n",
    "\n",
    "for strategy, data in results.items():\n",
    "    if data and 'losses' in data:\n",
    "        all_metrics.append({\n",
    "            'Strategy': strategy,\n",
    "            'Final Loss': data['losses'][-1],\n",
    "            'Final Accuracy': data['accuracies'][-1],\n",
    "            'Convergence Step': len(data['losses']),\n",
    "            'Min Loss': min(data['losses']),\n",
    "            'Max Accuracy': max(data['accuracies'])\n",
    "        })\n",
    "\n",
    "results_df = pd.DataFrame(all_metrics).sort_values('Final Loss')\n",
    "\n",
    "print(\"Scientific Study Results:\")\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "# Statistical analysis\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STATISTICAL ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Compare to random baseline\n",
    "random_metrics = results_df[results_df['Strategy'] == 'random'].iloc[0]\n",
    "print(f\"\\nRandom Baseline:\")\n",
    "print(f\"  Final Loss: {random_metrics['Final Loss']:.4f}\")\n",
    "print(f\"  Final Accuracy: {random_metrics['Final Accuracy']:.4f}\")\n",
    "\n",
    "print(\"\\nImprovements over Random:\")\n",
    "for _, row in results_df.iterrows():\n",
    "    if row['Strategy'] != 'random':\n",
    "        loss_improvement = (random_metrics['Final Loss'] - row['Final Loss']) / random_metrics['Final Loss'] * 100\n",
    "        acc_improvement = (row['Final Accuracy'] - random_metrics['Final Accuracy']) / random_metrics['Final Accuracy'] * 100\n",
    "        print(f\"  {row['Strategy']:30s} Loss: {loss_improvement:+6.1f}%  Accuracy: {acc_improvement:+6.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive visualizations\n",
    "fig = plt.figure(figsize=(20, 15))\n",
    "gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n",
    "\n",
    "# 1. Learning curves (all strategies)\n",
    "ax1 = fig.add_subplot(gs[0, :])\n",
    "for strategy, data in results.items():\n",
    "    if data and 'losses' in data:\n",
    "        steps = data['steps'][::100]  # Sample every 100 steps\n",
    "        losses = data['losses'][::100]\n",
    "        ax1.plot(steps, losses, label=strategy, linewidth=2, alpha=0.8)\n",
    "\n",
    "ax1.set_xlabel('Training Steps')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Learning Curves - All Strategies', fontsize=16)\n",
    "ax1.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Final performance comparison\n",
    "ax2 = fig.add_subplot(gs[1, 0])\n",
    "ax2.barh(results_df['Strategy'], results_df['Final Loss'])\n",
    "ax2.set_xlabel('Final Loss')\n",
    "ax2.set_title('Final Loss by Strategy')\n",
    "ax2.invert_yaxis()\n",
    "\n",
    "# 3. Convergence speed\n",
    "ax3 = fig.add_subplot(gs[1, 1])\n",
    "ax3.barh(results_df['Strategy'], results_df['Convergence Step'])\n",
    "ax3.set_xlabel('Convergence Step')\n",
    "ax3.set_title('Convergence Speed')\n",
    "ax3.invert_yaxis()\n",
    "\n",
    "# 4. Strategy categories comparison\n",
    "ax4 = fig.add_subplot(gs[1, 2])\n",
    "strategy_categories = {\n",
    "    'Random': ['random'],\n",
    "    'Reading-based': ['reading_level_easy_to_hard', 'reading_level_hard_to_easy', 'reading_level_staged'],\n",
    "    'Topic-based': ['topic_sequential', 'topic_interleaved', 'topic_largest_first'],\n",
    "    'Hybrid': ['hybrid_reading_topic', 'hybrid_topic_reading'],\n",
    "    'Epoch-interleaving': ['reading_topic_by_epoch', 'reading_levels_by_epoch', 'all_strategies_by_epoch']\n",
    "}\n",
    "\n",
    "category_performance = {}\n",
    "for category, strategies in strategy_categories.items():\n",
    "    category_losses = results_df[results_df['Strategy'].isin(strategies)]['Final Loss'].values\n",
    "    if len(category_losses) > 0:\n",
    "        category_performance[category] = category_losses.mean()\n",
    "\n",
    "ax4.bar(category_performance.keys(), category_performance.values())\n",
    "ax4.set_xlabel('Strategy Category')\n",
    "ax4.set_ylabel('Average Final Loss')\n",
    "ax4.set_title('Performance by Category')\n",
    "ax4.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 5. Learning efficiency (area under curve)\n",
    "ax5 = fig.add_subplot(gs[2, :])\n",
    "learning_efficiency = []\n",
    "for strategy, data in results.items():\n",
    "    if data and 'losses' in data:\n",
    "        # Calculate area under the loss curve (lower is better)\n",
    "        auc = np.trapz(data['losses'], data['steps'])\n",
    "        learning_efficiency.append({'Strategy': strategy, 'AUC': auc})\n",
    "\n",
    "efficiency_df = pd.DataFrame(learning_efficiency).sort_values('AUC')\n",
    "ax5.barh(efficiency_df['Strategy'], efficiency_df['AUC'])\n",
    "ax5.set_xlabel('Area Under Loss Curve')\n",
    "ax5.set_title('Learning Efficiency (Lower is Better)')\n",
    "ax5.invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Key Findings and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate final report\n",
    "print(\"=\"*60)\n",
    "print(\"CURRICULUM LEARNING EXPERIMENT - FINAL REPORT\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Best strategies\n",
    "print(\"\\nüìä TOP PERFORMING STRATEGIES:\")\n",
    "for i, (_, row) in enumerate(results_df.head(5).iterrows()):\n",
    "    print(f\"  {i+1}. {row['Strategy']:30s} Loss: {row['Final Loss']:.4f}  Acc: {row['Final Accuracy']:.4f}\")\n",
    "\n",
    "# Key insights\n",
    "print(\"\\nüí° KEY INSIGHTS:\")\n",
    "\n",
    "# Check if easy-to-hard beats hard-to-easy\n",
    "easy_to_hard = results_df[results_df['Strategy'] == 'reading_level_easy_to_hard']['Final Loss'].values\n",
    "hard_to_easy = results_df[results_df['Strategy'] == 'reading_level_hard_to_easy']['Final Loss'].values\n",
    "\n",
    "if len(easy_to_hard) > 0 and len(hard_to_easy) > 0:\n",
    "    if easy_to_hard[0] < hard_to_easy[0]:\n",
    "        print(\"  ‚úì Easy-to-hard curriculum outperforms hard-to-easy\")\n",
    "    else:\n",
    "        print(\"  ‚úì Hard-to-easy curriculum outperforms easy-to-hard\")\n",
    "\n",
    "# Check if hybrids beat single-factor\n",
    "hybrid_avg = results_df[results_df['Strategy'].str.contains('hybrid')]['Final Loss'].mean()\n",
    "single_avg = results_df[results_df['Strategy'].isin(['reading_level_easy_to_hard', 'topic_sequential'])]['Final Loss'].mean()\n",
    "\n",
    "if hybrid_avg < single_avg:\n",
    "    print(\"  ‚úì Hybrid strategies outperform single-factor approaches\")\n",
    "\n",
    "# Check epoch-interleaving\n",
    "epoch_strategies = results_df[results_df['Strategy'].str.contains('by_epoch')]\n",
    "if not epoch_strategies.empty:\n",
    "    best_epoch = epoch_strategies.iloc[0]\n",
    "    print(f\"  ‚úì Best epoch-interleaving: {best_epoch['Strategy']} (Loss: {best_epoch['Final Loss']:.4f})\")\n",
    "\n",
    "# Recommendations\n",
    "print(\"\\nüéØ RECOMMENDATIONS:\")\n",
    "best_strategy = results_df.iloc[0]['Strategy']\n",
    "improvement = (random_metrics['Final Loss'] - results_df.iloc[0]['Final Loss']) / random_metrics['Final Loss'] * 100\n",
    "\n",
    "print(f\"  1. Use '{best_strategy}' for {improvement:.1f}% improvement over random\")\n",
    "print(f\"  2. Consider ensemble of top 3 strategies for robustness\")\n",
    "print(f\"  3. Topic modeling with BERTopic successfully scaled to 1M samples\")\n",
    "\n",
    "# Dataset insights\n",
    "if 'outlier' in str(report):\n",
    "    print(f\"  4. Monitor outlier topic performance in production\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Export Results for Paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results for paper\n",
    "import json\n",
    "\n",
    "paper_results = {\n",
    "    'pilot_study': {\n",
    "        'config': pilot_config.to_dict(),\n",
    "        'results': pilot_df.to_dict('records')\n",
    "    },\n",
    "    'scientific_study': {\n",
    "        'config': scientific_config.to_dict(),\n",
    "        'results': results_df.to_dict('records'),\n",
    "        'best_strategy': best_strategy,\n",
    "        'improvement_over_random': improvement\n",
    "    },\n",
    "    'experiment_ids': {\n",
    "        'debug': debug_results['experiment_id'],\n",
    "        'pilot': pilot_results['experiment_id'],\n",
    "        'scientific': scientific_results['experiment_id']\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('curriculum_learning_results.json', 'w') as f:\n",
    "    json.dump(paper_results, f, indent=2)\n",
    "\n",
    "print(\"Results saved to curriculum_learning_results.json\")\n",
    "print(\"\\nüéâ Experiment complete! Check W&B for detailed tracking.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}